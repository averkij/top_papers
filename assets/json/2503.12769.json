{
    "paper_title": "ViSpeak: Visual Instruction Feedback in Streaming Videos",
    "authors": [
        "Shenghao Fu",
        "Qize Yang",
        "Yuan-Ming Li",
        "Yi-Xing Peng",
        "Kun-Yu Lin",
        "Xihan Wei",
        "Jian-Fang Hu",
        "Xiaohua Xie",
        "Wei-Shi Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on offline video understanding. Instead, streaming video understanding poses great challenges to recent models due to its time-sensitive, omni-modal and interactive characteristics. In this work, we aim to extend the streaming video understanding from a new perspective and propose a novel task named Visual Instruction Feedback in which models should be aware of visual contents and learn to extract instructions from them. For example, when users wave their hands to agents, agents should recognize the gesture and start conversations with welcome information. Thus, following instructions in visual modality greatly enhances user-agent interactions. To facilitate research, we define seven key subtasks highly relevant to visual modality and collect the ViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation. Further, we propose the ViSpeak model, which is a SOTA streaming video understanding LMM with GPT-4o-level performance on various streaming video understanding benchmarks. After finetuning on our ViSpeak-Instruct dataset, ViSpeak is equipped with basic visual instruction feedback ability, serving as a solid baseline for future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 9 6 7 2 1 . 3 0 5 2 : r ViSpeak: Visual Instruction Feedback in Streaming Videos Shenghao Fu1,2,4,, Qize Yang2,, Yuan-Ming Li1,4, Yi-Xing Peng1,2,4, Kun-Yu Lin1,4, Xihan Wei2, Jian-Fang Hu1,4, Xiaohua Xie1,4,5*, Wei-Shi Zheng1,3,4,6 1School of Computer Science and Engineering, Sun Yat-sen University, China; 2Tongyi Lab, Alibaba Group; 3Peng Cheng Laboratory, China; 4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China; 5Guangdong Province Key Laboratory of Information Security Technology, China; 6Pazhou Laboratory (Huangpu), China fushh7@mail2.sysu.edu.cn, qize.yqz@alibaba-inc.com, xiexiaoh6@mail.sysu.edu.cn, wszheng@ieee.org ViSpeak: https://github.com/HumanMLLM/ViSpeak ViSpeak-Bench: https://github.com/HumanMLLM/ViSpeak-Bench"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on offline video understanding. Instead, streaming video understanding poses great challenges to recent models due to its time-sensitive, omni-modal and interactive characteristics. In this work, we aim to extend the streaming video understanding from new perspective and propose novel task named Visual Instruction Feedback in which models should be aware of visual contents and learn to extract instructions from them. For example, when users wave their hands to agents, agents should recognize the gesture and start conversations with welcome information. Thus, following instructions in visual modality greatly enhances user-agent interactions. To facilitate research, we define seven key subtasks highly relevant to visual modality and collect the ViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation. Further, we propose the ViSpeak model, which is SOTA streaming video understanding LMM with GPT-4o-level performance on various streaming video understanding benchmarks. After finetuning on our ViSpeak-Instruct dataset, ViSpeak is equipped with basic visual instruction feedback ability, serving as solid baseline for future research. 1. Introduction Recent Large Video Language Models [4, 17, 23, 34, 66, 67] excel at fine-grained spatial perception, long-term temporal reasoning, and comprehensive spatiotemporal under- *: Corresponding authors are Xiaohua Xie and Wei-Shi Zheng. : Equal Contribution. Work was done when Shenghao Fu and Yi-Xing Peng were interns at Alibaba. standing. In the offline setting, the entire video is provided and the complete context can be modeled. However, in streaming video understanding, models can not access the entire video. Video content continuously arrives, and the model must make decisions based on the information available so far while continuously processing incoming future data, which poses great challenges to recent LMMs. Three key differences exist between streaming and offline video understanding: First, the question answers in streaming video understanding are time-sensitive. The answers for the same question What is happening now? vary at different timestamps and the model should output the answer at proper time. Second, steaming videos are always accompanied by streaming audios, making problems as omni-modal ones. Third but most importantly, streaming video understanding is distinguished by its interaction characteristic. The interaction characteristic encompasses three folds: 1) non-awakening interaction where users can interact with agents at any time, 2) interruption where users can stop the answer or change the topic at any time, and 3) proactive output where agents can also express their mind at proper time. Despite its significance, the interaction characteristic has been largely overlooked by the community. MMDuet [49] and Dispider [40] conducted preliminary explorations on proactive output to point out specific event when it occurs based on user prompt. However, the prompts do not always exist, especially for an unintentional event or during communications. VITA [16] uses dual models to decide when to respond to instructions in audio but it can not respond to visual contents. In this work, we dive deeper into the interaction characteristic of streaming video understanding and introduce new task named Visual Instruction Feedback to explore Figure 1. Examples of some actions in Visual Instruction Feedback task, which are Visual Wake-Up, Visual Reference, Visual Interruption, and Visual Termination in order. The content in parentheses is displayed by body language instead of text or speech. the instructions in the visual modality. We restrict the feedback primarily in conversational scenarios and define it as kind of feedback towards visual contents to provide in-time interaction with users and necessary assistance effectively. In this task, we select seven representative subtasks, including: 1) Visual Wake-Up: users use body language to start the conversation, 2) Anomaly Warning: agents provide intime warnings or advice based on accidental events, 3) Gesture Understanding: agents respond to gestures from humans in conversations, 4) Visual Reference: users use body language to refer to specific object, 5) Visual Interruption: users use body language to stop agents speaking, 6) Humor Reaction: agents share feedback to funny things with users, and 7) Visual Termination: users use body language to end the conversation. Examples are shown in Figure 1. To facilitate exploration, we collect the ViSpeak-Bench benchmark containing 1,000 videos and 1,000 QA pairs and the ViSpeak-Instruct training dataset containing 34k samples. As shown in Table 1, ViSpeak-Bench is the first comprehensive benchmark to evaluate the ability to respond to instructions in visual modality. However, to the best of our knowledge, none of the opensourced models can perform the Visual Instruction Feedback task even after finetuning on our dataset, especially for the visual interruption subtask, as they adopt turntaking chat template and the agent will fully express its mind without interruption before analyzing new user inputs. Thus, we propose the ViSpeak model which is finetuned from an existing omni-model using novel threestage finetuning procedure. In the first template alignment stage, we adapt the offline model to streaming input template while preserving the original offline capacities. The template supports taking the users input and the models responses as inputs at the same time, making the two input streams fully time-aligned. This template also supports interruption when the model is speaking. In the second streaming finetuning stage, we enhance the models streaming question-answering ability and proactive output ability. The resulting model achieves SOTA performance on the StreamingBench [29] and OVO-Bench [27], achieving 62.00 and 61.08 overall scores, separately, which are comparable with GPT-4o. Finally, we finetune the model on our collected ViSpeak-Instruct dataset which serves as solid baseline for the Visual Instruction Feedback task. In summary, our contributions are three folds: 1. We propose novel streaming video understanding task named Visual Instruction Feedback, which requires the model to actively respond to visual contents. This task greatly enhances human-agent interactions. 2. To support exploration, we manually collect the ViSpeak-Bench benchmark and the ViSpeak-Instruct training dataset. We also provide some analysis based on the evaluation results of existing models. 3. We also propose strong baseline ViSpeak for the new task, which is finetuned from an existing omni-modal model with three-stage finetuning. ViSpeak not only preserves offline understanding capacities but also achieves SOTA performance on streaming video understanding benchmarks. 2. Related Work 2.1. Large Multi-Modal Model Recent Large Multi-modal Models (LMMs) have rapidly evolved from image understanding models [3, 30, 31, 44] to video understanding models [4, 23, 66, 67] and even to omni-modal understanding models [16, 17, 26, 34, 68]. With high-quality instruction turning data [3, 4, 46], improved training recipes [23, 46], and well-designed model architecture [28, 33, 44, 54], recent LMMs achieve finegrained multi-modal alignment and extend their abilities of comprehensive image-level understanding [23, 47], finegrained region perception [18, 21, 22, 48], long-term temporal reasoning [39, 64, 65], timestamp awareness [32, 45] and even human mind or emotional understanding [20, 51, 57, 68]. Although great progress has been made, recent video LMMs are primarily focused on offline videos where the entire video is provided for understanding. Benchmark ActivityNet-QA [60] NExT-QA [50] MVBench [25] Video-MME [15] ET-Bench [32] StreamingBench [29] ViSpeak-Bench #Videos 800 1,000 3,641 900 7,002 900 1,000 #QA Pairs Time 8,000 8,564 4,000 2,700 7,289 4,500 1,000 Streaming PO Visual Instruct"
        },
        {
            "title": "Anno\nManual\nAuto\nAuto\nManual\nManual\nMixed\nMixed",
            "content": "Table 1. Comparison between ViSpeak-Bench and other video benchmarks. Time means the dataset is time-sensitive. PO denotes the dataset to evaluate the proactive output ability. 2.2. Streaming Video Understanding In practical human-agent interactions, LMMs should process streaming videos, which has drawn great attention in recent years. Many streaming video understanding benchmarks [27, 29, 53, 58] have been proposed, which have simultaneously spurred the development of many streaming video LMMs. VideoLLM-online [2], as pioneer model, proposes LIVE framework to process streaming videos, which uses Streaming Loss to learn when to speak. Subsequently, different models focus on different challenges in streaming video understanding. Flash-VStream [62] and IXC2.5-OL [64] propose some memory mechanisms to handle long context in streaming videos. Dispider [40] and MMDuet [49] focus on proactive output, the former disentangling perception, decision, and reaction while the latter introduces two additional heads. Mini-Omni2 [52] and VITA 1.5 [17] pay more attention to end-to-end realtime speech interaction. STREAMCHAT [53] and StreamingChat [58] aim to tackle the multi-round conversation problem. In this work, we extend the streaming video understanding problem from new perspective and introduce new Visual Instruction Feedback task. 3. Visual Instruction Feedback Task 3.1. Task Definition In this work, we define new task named Visual Instruction Feedback for streaming video understanding. Formally, we define the feedback as kind of feedback towards visual contents to provide in-time interaction with users and necessary assistance effectively. We also restrict the feedback primarily to conversational scenarios. In this task, users may not provide explicit instructions in text or audio format. The agent should analyze visual inputs and express its mind accordingly. Assume video stream X[0,+) with infinite length. An action A[t1,t2] or an event E[t1,t2] may appear at any time [t1, t2] and the model should recognize them and provide feedback within limited time span [t1, t2 +T ]. As the task focuses on conversational scenarios, the agent should provide responses from second-person perspective. According to the definition, we summarize seven key subtasks. Before conversations: 1. Visual Wake-Up (VW). Unlike keyword-based (like Siri) or VAD-based [16, 64] wake-up, visual wake-up Subtask Visual Wake-Up Anomaly Warning Gesture Understanding Visual Reference Visual Interruption Humor Reaction Visual Termination ViSpeak-Bench #Videos 100 200 200 200 100 100 100 1,000 #QA Pairs 100 200 200 200 100 100 100 1,000 QA Type Open-Ended Open-Ended Open-Ended Multi-Choice Open-Ended Open-Ended Open-Ended Table 2. ViSpeak-Bench benchmark statistics. ViSpeak-Bench contains 7 subtasks with 1,000 videos and 1,000 QA pairs. needs the model response to salutations from users. 2. Anomaly Warning (AW). In this task, models need to identify accidental events (e.g. fighting, explosion) or unintentional actions (e.g. falling down) and provide intime warnings, advice, or help. During conversations: 3. Gesture Understanding (GU). Gestures play vital role in conversations, even serving as short response from users, like OK, GOOD, ONE, TWO. Models should understand human gestures and provide the corresponding feedback. 4. Visual Reference (VR). In many cases, it is difficult to describe an object or where the object is precisely, but it can be done by pointing it out with the fingers, such as What is this. Models should identify which object is referenced and answer questions from uses. 5. Visual Interruption (VI). When users are not satisfied with the models response or want to change the topic, they may interrupt the model with some body language, like the stop gesture. Models should stop generating the remaining responses when receiving these signals. 6. Humor Reaction (HR). Humor understanding is one of the key abilities of humans. Reacting properly to funny things provides necessary emotional value to users. 7. Visual Termination (VT). Visual termination is the action to end conversations. Although the actions in wakeup and termination may be the same (i.e. wave hands), they can be classified by the context where the action at the beginning of conversations is visual wake-up, otherwise visual termination. Models should be aware of contexts and start or end conversations properly. Although there are many other scenarios where agents should talk to users actively, for example, sign language (we exclude it due to the technical complexity and its variability across the world), we believe the subtasks above cover common scenarios in daily life. Some examples are shown in Figure 1. More visualizations are shown in Supplement. 3.2. Dataset Construction Video Collection and Annotation. We collect videos from both open-sourced datasets and our self-collected datasets. For open-sourced datasets, we use anomaly videos in Holmes-VAU [63] and unintentional videos in OOPS [13] for Anomaly Warning and HumorQA in FunQA [51] for Humor Reaction. All the datasets above are annotated with timestamps and event descriptions. We simply use GPT-4o to rewrite the annotations in conversational tone to simulate conversations. For Gesture Understanding, we select 10 common gestures from Jester [35], each with 400 videos. For other subtasks, we manually record the videos by ourselves. To ensure diversity, we recruit team of 610 people (346 men and 264 women) with an age ranging from 10 to 70 years old from 5 provinces. For each kind of subtask, we carefully designed diverse conversation scripts and instructed participants to follow these scripts during filming to simulate human-computer interaction scenarios. Videos are recorded in various environments, including homes, offices, factories, warehouses, supermarkets, wild, and many others. In summary, we collect 1,185 videos for Visual Interruption, 4,689 videos for Visual Reference, 1,188 videos for Visual Wake-Up and Termination, and 1,507 videos for Gesture Understanding. For each video, we manually annotate the accurate timestamps for each body language, making the videos suitable for streaming video understanding. The corresponding scripts for each video are used as the annotations. For the Gesture Understanding subtask, in addition to the 10 gestures in Jester [35], we further add 10 gestures commonly used during conversations, with total of 20 gestures. We also design 5 gestures for Visual Interruption. More details can be found in Supplement. Dataset Enhancement. Although we have made great efforts to make the dataset large and diverse, the self-collected data still cannot cover infinite scenarios in the real world, especially for gesture understanding. To alleviate the problem, we augment the dataset with some offline video understanding datasets. The motivation is that the model can improve its social intelligence from the perspective of bystander, simulating child, who observes the conversations between adults. The offline data have no timestamp annotations and questions are appended at the end of the video. Specifically, we select SMILE [20] for understanding funny things and IntentQA [24] and Social-IQ [61] for learning body languages. Further, we manually review the videos in Social-IQ [61] dataset and re-annotate some common body languages lasting longer than 1 second in conversations. For each action, we point out what action it is and why the speaker did the action in the context, which helps the model study the meaning of common body language in the wild. In summary, 678 videos with 1,861 annotations are collected. Examples can be found in Supplement. Quality Verification. While recording videos, we will provide guidelines to participants to ensure the quality of the raw videos. The videos are then sent for human annotation. Low-quality data will be rejected and re-recorded. Some well-performed annotators will conduct spot checks on the annotation results. If significant quality issues are identiSubtask Visual Wake-Up"
        },
        {
            "title": "Humor Reaction",
            "content": "Visual Termination ViSpeak-Instruct Data Source self-collected data OOPS [13] HIVAU [63] Jester [35] self-collected data Social-IQ [61] IntentQA [24] SocialIQA [41] self-collected data self-collected data self-collected data FunQA [51] SMILE [20] self-collected data"
        },
        {
            "title": "Data Type\nonline\nonline\nonline\nonline\nonline\noffline\noffline\noffline\noffline\nonline\nonline\nonline\noffline\nonline",
            "content": "#Samples Ratio 0.03 0.09 0.09 0.12 0.12 0.06 0.15 0.02 0.03 0.15 0.03 0.06 0.03 0.03 1 1k 3k 3k 4k 4k 2k 5k 0.5k 1k 5k 1k 2k 1k 1k 34k Table 3. Task and sample distribution in ViSpeak-Instruct. fied during annotation, the corresponding annotator will undergo retraining, and the data will be re-annotated. Data Partition and Dataset Statistics. With the collected data above, we manually select some representative videos to construct the ViSpeak-Bench evaluation dataset. For Visual Wake-Up, Visual Termination and Visual Interruption, we select 100 videos for each subtask with actions lasting 2 seconds. For Visual Reference, we carefully select 200 videos with multiple objects. The referenced object may appear at any location within the frame and is not necessarily positioned at the center. We formulate this subtask as multi-choice problem during evaluation and manually annotate each video with three other confusing options which are also displayed in the video, ensuring the answer is highly related to visual reference. For Humor Reaction, we select 100 humorous videos in FunQA that are only relevant to visual content. For Gesture Understanding and Anomaly Warning, we randomly select 200 videos for testing. The remaining data are contained in ViSpeak-Instruct and used for training. The statistics of ViSpeak-Bench and ViSpeakInstruct are summarized in Table 2 and Table 3. Evaluation Metrics. In our task, we evaluate both the timing accuracy Tacc of the models feedback (i.e., Time Accuracy) and the quality score of its response text (i.e., Text Score), and then derive an overall score O. For Time Accuracy, the model should response within the ground-truth time span [t1, t2 + ], where is the time margin we set. For subtask s, the accuracy of Tres is measured based on whether it falls within this time window. acc ="
        },
        {
            "title": "1\nN s",
            "content": "N (cid:88) i=1 I(cid:0)T (i) res [t1, t2 + ](cid:1), (1) where is the number of questions in each subtask. For Text Score, the output of the model should accurately reflect the actions or events within the video and be consistent with historical context. The response must be positive and supportive, providing assistance to the user when necessary. Thus, we use the dialogue history and ground truth Figure 2. ViSpeak is an omni-modality LMM with multiple encoders and LLM. To support streaming video analysis, ViSpeak takes two input streams as inputs, one for user inputs and one for self-generated outputs. Two streams will be combined into single one before sending to LLM. An informative head is trained for visual proactive output. as references, designing different prompts for different subtasks. We use GPT-4o as the judge model, scoring the responses on scale from 0 to 5 (see Supplement for more details). Note that, for the visual reference task, we use multi-choice questions for evaluation and rescale the score for this task to range from [0,5], while its response time accuracy is always set to 1. Finally, the overall score is calculated as: ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) s=1 acc s, (2) where is the number of subtasks. 4. The ViSpeak Model 4.1. Model Architecture In order to accomplish the Visual Instruction Feedback task, we design model named ViSpeak as shown in Figure 2, which is an omni-model with an image encoder to extract image or video features, an audio encoder for encoding both audios and music, and large language model to integrate multimodal features and conduct analyses to fulfill the relevant instructions. However, the turn-taking chat template with explicit role control is not suitable for interruption. Inspired by Moshi [10], we design two-stream chat template, one for user inputs and one for agent historical outputs, so that the model can continuously process user inputs while outputting the next tokens. Thus, the model can adjust outputs based on upcoming inputs. Two input streams are combined into single one before sending to the LLM. By default, we use linear layer to predict the weights for weighted sum of the two streams. In the ablation study part, we have tried many kinds of combination methods and found that they perform similarly. Further, we segment the streaming inputs from users into multiple fragments, such as extracting one frame per second from the video and dividing the audio into 1-second snippets, subsequently organizing these segments in chronological order. Each segment is appended with special <seg> token and the LLM can only start to speak from it. To differentiate the response towards different kinds of instructions, answers for text, audio, and visual instructions start with , , and separately, following VITA [16]. For Visual Interruption, the model can simply output Stop! at <seg> token to stop generation. When to speak is key problem in streaming processing. We find that using the original language model head (next token prediction) is sufficient to handle text-answering problems, i.e. the model can always output token at the end of text segment. However, visual proactive output is more challenging task and next token prediction can not manage the turn-taking problem well. Thus, we train another informative head to predict when to speak following MMDuet [49], which is binary classification head to predict speaking or not. When the prediction score is above predefined threshold, the model will respond to visual instruction. In this work, we do not take the turn-taking problem for audio modality into account for simplicity. 4.2. Three-Stage Finetuning Recipe Directly training strong streaming model from scratch is resource-demanding. Thus we begin with well-pretrained Method Params Frames Omni OP CR CS Real-Time Visual Understanding TR EU PR SU ATP ACP CT All ER Omni-Source Understanding SD SCU MA Contextual Understanding All ACU MCU SQA PO All Overall Proprietary MLLMs Gemini 1.5 pro [43] GPT-4o [19] Claude-3.5-sonnet LLaVA-OneVision [23] MiniCPM-V [59] InternVL-V2 [6] Qwen2-VL [47] LLaVA-Next-Video [66] Video-LLaMA2 [8] Ola [34] VITA 1.5 [17] Flash-VStream [62] VideoLLM-online [2] IXC2.5-OL [64] Dispider [40] ViSpeak (Ours, s2) ViSpeak (Ours, s3) - - - 7B 8B 8B 7B 32B 7B 7B 7B 7B 8B 7B 7B 7B 7B - - - 32 32 16 1 fps 64 32 64 16 - 2 fps 64 1 fps 1 fps 1 fps 83.43 80.66 82.45 82.83 78.20 73.84 75.75 80.11 59.95 61.58 74.11 25.89 39.07 82.83 74.92 79.84 79.84 77.94 76.98 73. 77.34 71.88 65.63 79.69 71.09 60.16 71.09 78.13 43.57 40.06 73.77 75.53 88.28 71.09 89.24 86.67 82.43 83.23 84.18 78.80 76.58 80.70 62.97 67.19 80.76 24.91 34.49 78.66 74.10 83.28 81.39 81.65 73.81 82. 83.33 83.99 82.03 79.08 80.72 60.46 62.09 77.12 23.87 31.05 82.95 73.08 81.05 78.76 79.17 75.95 76.39 72.05 75.16 71.43 74.53 71.43 54.66 62.73 73.91 27.33 45.96 72.50 74.44 76.40 74.53 83.92 85.48 85. 74.77 75.39 72.90 75.08 73.21 46.11 51.71 64.17 13.08 32.40 76.01 59.92 75.08 70.09 49.22 43.09 47.62 74.87 65.99 67.88 60.32 70.66 60.73 71.10 67.14 65.44 65.16 63.17 48.16 53.82 66. 68.29 56.50 63.01 65.85 59.35 46.75 52.03 58.54 83.93 75.00 61.68 Open-Source Video MLLMs 73.15 72.22 73.15 74.07 62.96 41.67 60.19 66.67 41.97 47.15 42.49 41.97 36.79 34.72 17.62 33.68 Open-Source Streaming MLLMs 48.70 27.89 58.85 45.80 34.20 27.98 23.87 42.49 71.59 62.16 77.34 71.39 25.20 34.16 60.67 62.91 65.85 64.23 18.52 31.48 61.11 76.14 70.37 63. 77.39 74.54 74.04 74.27 72.43 70.11 71.15 69.83 52.58 56.16 68.20 23.23 35.99 73.79 67.63 74.36 70.44 52.40 53.60 39.60 41.20 42.00 44.80 40.80 41.60 43.60 40.80 44.00 25.91 31.20 - 35.46 42.80 47. 50.80 32.40 35.60 26.10 27.71 28.11 25.30 24.50 23.29 27.20 26.80 24.90 26.51 - 25.26 35.20 56.40 80.40 49.00 34.40 43.20 40.40 47.20 41.20 44.40 35.20 23.60 42.80 25.60 24.10 - 38.57 61.20 61. 87.60 68.80 56.00 52.80 50.80 50.80 55.60 56.40 41.60 43.20 56.80 28.40 32.00 - 43.34 74.80 81.20 67.80 50.95 41.40 40.83 40.23 42.73 40.73 41.73 35.92 33.70 42.60 26.00 28.45 - 35.66 53.50 61. 52.80 50.40 36.00 35.08 37.50 35.08 34.27 34.27 28.23 30.40 31.60 24.80 24.19 - 39.62 38.80 49.20 42.40 42.80 43.20 30.40 27.20 27.20 26.40 28.80 26.00 22.80 32.80 25.20 29.20 - 27.65 36.80 36. 59.20 52.40 34.80 30.00 40.00 42.80 44.40 44.00 21.20 31.20 36.40 26.80 30.80 - 34.80 44.00 39.20 45.10 56.86 64.71 29.55 22.22 40.91 22.73 18.18 0.00 11.20 23.60 1.96 3.92 - 25.34 38.80 50. 51.06 49.06 39.70 31.68 34.09 35.40 34.24 34.58 23.54 23.90 31.10 24.12 26.55 - 33.61 39.60 43.90 70.26 64.31 60.06 58.56 57.80 57.28 57.20 56.73 43.30 44.00 54.27 24.04 32.48 - 53.12 62.00 62. Table 4. Performance on StreamingBench [29]. Results for ViSpeak trained after the second and third stage are reported."
        },
        {
            "title": "OCR ACR",
            "content": "Real-Time Visual Perception STU"
        },
        {
            "title": "ATR",
            "content": "- - 87.25 69.13 66.97 65.14 80.17 65.52 54.49 50."
        },
        {
            "title": "FPD",
            "content": "Avg. Proprietary MLLMs 68.32 68.32 67.39 63.68 70.77 63."
        },
        {
            "title": "HLD",
            "content": "Avg. Forward Active Responding Avg."
        },
        {
            "title": "REC",
            "content": "68.59 49.83 75.68 70.95 52.69 55.38 62.32 58.72 35.53 27.58 74.24 73. 61.67 59.40 57.15 53.40 Gemini 1.5 pro [43] GPT-4o [19] Qwen2-VL [47] Qwen2-VL [47] LLaVA-Next-Video [66] LLaVA-OneVision [23] InternVL-V2 [6] LongVU [42] VITA 1.5 [17] Flash-VStream [62] VideoLLM-online [2] ViSpeak (Ours, s2) - - 72B 7B 7B 7B 8B 7B 7B 7B 8B 7B 64 64 64 64 16 1 fps 16 1 fps 2 fps 1 fps 72.48 69.13 69.80 67.11 68.46 55.70 74.50 25.50 8.05 75. 56.88 53.21 59.63 58.72 58.72 49.54 60.55 32.11 23.85 58.72 77.59 63.79 66.38 69.83 68.97 59.48 70.69 29.31 12.07 71.55 Open-Source Video MLLMs 51.52 44.44 51.18 52.53 43.10 43.10 46. 61.41 60.87 61.41 60.33 55.98 63.04 58.70 74.26 66.34 72.28 71.29 67.33 68.32 63.37 65.81 60.65 63.34 62.79 60.73 57.40 63.53 52.25 50.56 50.56 49.44 44.94 48.31 53.37 Open-Source Streaming MLLMs 33.71 14.04 51.12 29.86 20.79 66.28 28.80 21.20 66. 29.70 45.54 74.26 36.36 22.22 59.93 73.65 66.89 64.19 58.78 61.49 66.22 54.05 33.78 18.80 48.65 63.44 34.41 9.68 23.66 27.41 9.14 24.19 5.91 12.18 63. 62.87 48.58 41.68 44.99 44.00 39.49 41.46 25.35 17.73 57.52 37.68 30.09 34.10 24.79 25.79 16.62 37.54 5.44 - 33.81 60.10 65.66 67.57 66.93 57.55 69.00 60.73 67.25 - 68. 45.00 50.83 60.83 60.83 52.92 60.00 62.08 60.00 - 60.42 47.59 48.86 54.17 50.85 45.42 48.54 53.45 44.23 - 54."
        },
        {
            "title": "Overall",
            "content": "65.25 58.58 58.76 52.70 53.06 52.88 50.05 48.48 55.49 33.15 - 61.08 Table 5. Performance of various MLLMs on OVO-Bench [27]. Results for ViSpeak trained after the second stage are reported. omni-modal offline model [17] and adopt three-stage finetuning recipe to train the model. In the first template alignment stage, we adapt the offline model to our streaming input template with the goal of not compensating for its offline multi-modal understanding ability. In this stage, we select 300k text data from Magie [55], 665k image data from ShareGPT4V [3], 1,335k video data from LLaVA-Video [67], 410k audio data from LibriSpeech [37] and WavCaps [36], and 121k crossmodality data from Ola [34], with total of 2.7M data for training. To save computation, we compress the data by concatenating short samples to longer one, resulting in 2.0M data. To further enhance the cross-modality feature alignment, we use the audio in video data when available. We further use the CosyVoice2 [12] Text-to-Speech (TTS) method to change small part of text questions in image and video data into speech following VITA [16]. To make sure the speech is rich in diversity, we select the voice of 5,962 speakers in VoxCeleb2 [9] as the condition for CosyVoice2 to synthesize speech. The training starts with tuning the projector with one quarter of the data and then training the projector and the LLM with LoRA and all data. In the second streaming finetuning stage, we enhance the models streaming question-answering ability and proactive output ability. Thus the data should be annotated with timestamps. In this stage, we use 81k data from MMDuet [49] with temporal video grounding task, dense captioning task, and multi-answer question answering task, 42k data from ET-Instruct [32] for temporal action localization task and referred video captioning task, and 42k data from EgoTimeQA [11] for general question answering task. We also sample 500k offline data in stage 1 to enrich the dataset. Finally, the training dataset comprises 657k samples. The informative head is trained at this stage. Finally, we finetune the model on our collected ViSpeakInstruct dataset, giving the model the ability to mine the instructions in the visual modality and respond to users actively. The resulting model ViSpeak serves as solid baseline on ViSpeak-Bench. 5. Experiment 5.1. Implementation Detail ViSpeak is finetuned from VITA 1.5 [17] due to its high performance on omni-modal data and early open-resourcing, which uses Qwen2.5 7B [56] as the LLM and InternViT300M-448px [7] as the visual encoder. The audio encoder is designed by VITA itself and has 341M parameters. In the first stage, we first employ learning rate of 5e-4 and batch size 256 for MLP adapter pre-training and learning rate of 1e-4 and batch size 128 for LLM LoRA fintuning. The number of tokens for each image is 256 and the maximum number of images per video is 16. In the second and third stages, the training configurations are the same as"
        },
        {
            "title": "Streaming",
            "content": "Human (Avg) Human (Max) Gemini 1.5 pro [43] GPT-4o [19] InternVL-2.5 [5] Qwen2.5-VL [1] Qwen2.5-VL [1] VITA 1.5 [17] Ola [34] FlashVstream [62] Dispider [40] ViSpeak (Ours, s3) - - - - 8B 7B 72B 7B 7B 7B 7B 7B - - - - 16 1 fps 1 fps 1 fps 1 fps 1 fps 16 1 fps - - - - AW 70.00 70.00 VI 100.00 100.00 46.00 48. 60.00 82.00 41.50 42.50 44.50 18.00 27.00 34.00 38.50 56.50 55.50 78.00 81.00 46.00 67.00 16.00 70.00 72.00 HR 90.00 100.00 Time Accuracy (%) VW 92.00 100.00 VT 96.00 100.00 Proprietary MLLMs 48.00 100. 84.00 99.00 85.00 96.00 Open-Source Video MLLMs 46.00 31.00 77.00 40.00 44.00 48.00 44.00 83.00 72.00 85.00 91.00 49.00 69.00 33.00 100.00 79.00 96.00 95.00 91.00 88.00 89.00 75.00 69.00 93.00 GU 98.80 100.00 97.00 99. 99.50 98.50 93.00 97.50 98.50 99.50 99.50 99.00 All 91.13 95.00 70.00 87.50 68.42 71.67 79.58 56.42 65.75 50.92 70.17 80.42 VR 4.80 5.00 3.03 3. 2.93 2.34 3.15 2.40 2.95 1.75 2.50 3.75 AW VI 4.58 2.45 5.00 2.71 Text Score HR VW VT 5.00 5.00 3.06 5.00 5.00 3.62 2.34 2.27 2.16 2.31 2.64 2.08 1.81 1.63 1.75 2.63 2.93 3. 3.67 2.31 3.36 0.57 2.67 1.31 4.06 3.84 1.36 1.71 0.74 1.32 1.00 0.85 0.55 0.67 0.91 1.07 4.66 5.00 3.05 5.00 5.00 4.57 4.71 4.88 0.61 4.95 4.68 4. 4.81 3.91 5.00 4.49 3.67 4.61 2.49 3.15 GU 2.85 3.19 2.07 2.22 1.26 1.02 1.50 1.18 1.52 0.70 2.07 3.36 All 3.96 4.22 3.01 3. 2.66 2.60 3.09 2.31 2.55 2.22 2.06 3."
        },
        {
            "title": "Overall",
            "content": "3.69 4.01 2.19 2.99 1.98 2.25 2.62 1.54 1.86 1.24 1.63 2.76 Table 6. Performance of various MLLMs on ViSpeak-Bench. Results for ViSpeak trained after the third stage are reported. For human evaluation, we invite 5 participants which are not received relevant training to answer 20% randomly selected questions and we report their average scores and the maximum scores on each subtask. those in stage 1 finetuning. However, as streaming video always lasts for few minutes, we further downsample the image for each frame by factor of 2, resulting in 64 tokens per image, and increase the maximum number of images per video to 64 accordingly, to extend the context. By default, videos are sampled at 1 fps. All experiments are conducted on 32 NVIDIA L20 GPUs and the max context length is set to 6,200 due to resource limit. The threshold for the informative head is set to 0.35 for all experiments. 5.2. Streaming Video Understanding Benchmarks In this subsection, we select two large-scale comprehensive streaming video understanding benchmarks for evaluation. Performance on StreamingBench. StreamingBench [29] is designed for evaluating real-time visual understanding, omni-source understanding and contextual understanding, which is comprehensive benchmark for streaming video understanding. It has 18 tasks, 900 videos and 4,500 QA pairs. As shown in Table 4, our ViSpeak model achieves SOTA performance among open-sourced models with only 7B parameters. And the performance is also comparable with GPT-4o, which is well-known model for its omnisource understanding and interactive ability. And the performance of ViSpeak on omni-source understanding is even higher than GPT-4o (61.60 vs 50.95), demonstrating its outstanding omni-modal comprehensive understanding. Further, with our informative head, our model can speak proactively and get 38.80 scores on PO tasks, while other models should change the proactive problem to an offline one. After the stage 3 finetuning, the proactive output ability is further enhanced and gets 50.80 scores. Performance on OVO-Bench. OVO-Bench [27] is designed for evaluating the backward tracing ability, the realtime visual perception ability and the forward active responding ability, which evaluates the models streaming video understanding capability from different dimension compared with StreamingBench [29]. It has 12 tasks, 644 videos and 2,800 QA pairs. As shown in Table 5, our ViSpeak model also achieves SOTA performance among opensourced models and the performance is even higher than that of GPT-4o, showing great ability to handle timesensitive characteristics in video streaming understanding. 5.3. ViSpeak-Bench On ViSpeak-Bench, we evaluate both representative proprietary and open-source MLLMs. We also conduct human evaluation as reference. Results are shown in Table 6. For human evaluation, we find that in most cases, humans are able to provide appropriate responses at suitable time and achieve the highest score. But the scores on AW, HR, and GU are relatively low. For Anomaly Warning and Humor Reaction, participants overlook some details in their descriptions, leading to reduction in scores. And participants sometimes fail to accurately describe the gestures depicted in the videos in the gesture understanding subtask. During testing MLLMs, we observed existing models perform poorly when not given explicit prompts to indicate the exact expected response type, because these models are unaware they are in conversational scenario. To ensure the reliability of the evaluation of these models, we provide clear prompts for different subtasks (see Supplement for more details). With explicit prompts, all models achieve stable performance. Some observations are concluded as follows: a) Due to its great interactive ability, GPT-4o performs best among all models. b) Within opensourced offline models, Qwen2.5-VL[1] performs best and larger model can get more reasonable responses. c) For open-sourced omni-modality models Ola [34] and VITA 1.5 [17], their performances in both time accuracy and text score are inferior to models like InternVL-2.5 [5] and Qwen2.5-VL [1], possibly because they prioritize omnimodality, resulting in weaker focus on visual understanding. d) For streaming video LMMs, FlashVstream [62] and Dispider [40] still underperform Qwen2.5-VL. We find that FlashVstream tends to speak aggressively, always prior to the actions or events, especially for VI and VT in which the actions are not at the beginning of the video. Additionally, we also use the same prompts for evaluating MMDuet [49] Method MME 2353.5 (1728.9/624.6) VITA 1.5 Adaptive Sum 2237.0 (1636.3/600.7) 2283.4 (1685.5/597.9) Linear 2292.8 (1691.4/601.4) Add MVBench Video-MME 53.95 54.12 52.95 54.27 58 55 56 55 Table 7. Ablation studies on input stream combination methods and VideoLLM-online [2], but we find they can not follow the instructions and simply describe the video, e.g. You look at the camera., which is possibly due to their selected training datasets. In contrast, without explicit prompts, ViSpeak achieves the highest scores among open-source models, owing to fine-tuning from our strong streaming model. However, we observed that the performance on the anomaly warning and humor reaction subtasks is relatively low, as these tasks exhibit considerable variability in real-world scenarios, and understanding humor is difficult for MLLMs without reasoning ability. 5.4. Ablation Study Effect of different combination methods to combine two input streams. In the ViSpeak model, we propose to use two-stream chat template to support interactions within streaming videos. Two input streams (one for the user and one for the agent) are combined into single one before sending to LLM. We design three types of combination methods: Adaptive Sum, Linear and Add. The Add method directly adds two streams into single one along the feature channel dimension. The Linear method first concatenates two streams along the feature channel dimension and then uses linear layer to reduce the dimension. The Adaptive Sum method first predicts weight for each stream and then weighted adds two streams. The intuition behind Adaptive Sum is that two inputs may not have equal importance at specific timestamp. When models are generating responses, they may focus more on their previous output tokens, whereas they pay more attention to user input tokens otherwise. In these experiments, we select MME [14] for image understanding ability evaluation and Video-MME [15] and MVBench [25] for the evaluation of video understanding capacity. As shown in Table 7, after the first template alignment stage, the model can maintain its offline data understanding ability, achieving performance comparable to our baseline VITA 1.5. Further, we find that different combination methods also perform similarly and we use the Adaptive Sum method by default. Effect of different designs to control visual proactive output. In this work, we jointly train an informative head with the LLM to control visual proactive output. In TaIn Exp (a) to (c), we ble 8, we ablate different designs. first train the model except the informative head. Then, we freeze the LLM and train the informative head in Exp (b) Exp (a) (b) (c) (d)"
        },
        {
            "title": "Head\nLM\ninform\ninform\ninform",
            "content": "Joint Token <seg> <seg> Visual Visual"
        },
        {
            "title": "All",
            "content": "73.88 51.70 37.70 60.91 74.36 53. 39.60 62.00 PO 30.00 34.80 36.00 38.80 Table 8. Ablation studies on the design of visual proactive speaking control. Performances on StreamingBench are reported. Head denotes using language modeling head or informative head for prediction. Joint denotes whether the head is finetuned with LLM. Token means which token is used for prediction. Dataset ViSpeak-Instruct w/o offline data HR (Text Score) GU (Text Score) Overall 1.07 1.02 3.36 3.17 2.76 2.70 Table 9. Ablation studies on the offline data in ViSpeak-Instruct. Performance on ViSpeak-Bench are reported. Model ViSpeak (s1) ViSpeak (s2) ViSpeak (s3) MME MVBench Video-MME 2237.0 2051.1 2181. 54.12 49.53 53.97 55 58 60 StreamingBench ViSpeak-Bench - 62.00 62.58 - - 2.76 Table 10. Performance on different benchmarks across different training stages. Results in purple are reported in above tables. and (c). We find that using the language modeling head for proactive control gets limited performance with only 30.00 scores. Training an informative head following MMDuet [49] on the frozen LLM can get 34.80 scores. We further find that the last visual token in segment contains more visual cues than the <seg> token so training the informative head based on the visual token can further improve the proactive output score to 36.00. Since the LLM in Exp (a) to (c) are frozen, the performance of other tasks in StreamingBench is the same across these experiments. In Exp (d), we jointly train the informative head with LLM and get the highest proactive output performance. We find that other tasks in StreamingBench are also improved by co-training. We speculate that the informative head makes the model aware of the action boundary thus improving the performance on other tasks. Effect of the different dataset composition of ViSpeakInstruct. Since there are many kinds of gestures in conversations and the gesture in different contexts has different meanings. To enhance the gesture understanding ability, we use some offline data during training, as well as for humor reactions. As shown in Table 9, using offline data can increase the generalization ability and get higher scores. Comparisons on the performance across the model in different training stages. In this work, we adopt threestage finetuning recipe by first finetuning an offline model to SOTA streaming model and then finetuning for the Visual Instruction Feedback task. In Table 10, we find the model can effectively preserve the ability learned from previous stages while progressively learning new skills, demonstrating the superiority of our training recipe. 6. Conclusion In this work, we extend the streaming video understanding problem with new Visual Instruction Feedback task, which requires the model to respond to visual contents actively. To facilitate research, we define seven key subtasks and collect the ViSpeak-Bench for evaluation and the ViSpeak-Instruct for training. To solve this problem, we first adapt an offline omni-modal LMM to our designed chat template, and then finetuning it to get SOTA streaming LMM. This model is evaluated on two comprehensive streaming benchmarks and gets GPT-4o-level performance. Finally, we finetune the SOTA streaming model on ViSpeak-Instruct and get the ViSpeak model which serves as strong baseline on ViSpeak-Bench for future research. We hope our work can provide deeper insights into streaming video understanding and human-agent interaction."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 7 [2] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. In CVPR, 2024. 3, 6, 8 [3] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV, 2024. 2, 6 [4] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. In NeurIPS, 2024. 1, 2 [5] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 7 [6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 6 [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. 6 [8] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [9] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. arXiv preprint Voxceleb2: Deep speaker recognition. arXiv:1806.05622, 2018. 6 [10] Alexandre Defossez, Laurent Mazare, Manu Orsini, Amelie Royer, Patrick Perez, Herve Jegou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for realtime dialogue. arXiv preprint arXiv:2410.00037, 2024. 5 [11] Shangzhe Di and Weidi Xie. Grounded question-answering in long egocentric videos. In CVPR, 2024. 6 [12] Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. [13] Dave Epstein, Boyuan Chen, and Carl Vondrick. Oops! predicting unintentional action in video. In CVPR, 2020. 3, 4, 12 [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 8 [15] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 3, 8 [16] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. 1, 2, 3, 5, 6 [17] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level arXiv preprint real-time vision and speech interaction. arXiv:2501.01957, 2025. 1, 2, 3, 6, 7 [18] Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, and Wei-Shi Zheng. Llmdet: Learning strong open-vocabulary object detectors under the supervision of large language models. In CVPR, 2025. 2 [19] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 6, 7, 12 [20] Lee Hyun, Kim Sung-Bin, Seungju Han, Youngjae Yu, and Tae-Hyun Oh. Smile: Multimodal dataset for understanding laughter in video with language models. arXiv preprint arXiv:2312.09818, 2023. 2, 4, [21] Qing Jiang, Yuqin Yang, Yuda Xiong, Yihao Chen, Zhaoyang Zeng, Tianhe Ren, Lei Zhang, et al. Chatrex: Taming multimodal llm for joint perception and understanding. arXiv preprint arXiv:2411.18363, 2024. 2 [22] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, 2024. 2 [23] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2, 6 [24] Jiapeng Li, Ping Wei, Wenjuan Han, and Lifeng Fan. Intentqa: Context-aware video intent reasoning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1196311974, 2023. 4, 12 [25] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024. 3, 8 [26] Yadong Li, Jun Liu, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Da Pan, et al. Baichuan-omni-1.5 technical report. arXiv preprint arXiv:2501.15368, 2025. [27] Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, et al. Ovo-bench: How far is your videollms from real-world online video understanding? In CVPR, 2025. 2, 3, 6, 7, 13 [28] Yuan-Ming Li, An-Lan Wang, Kun-Yu Lin, Yu-Ming Tang, Ling-An Zeng, Jian-Fang Hu, and Wei-Shi Zheng. Techcoach: Towards technical keypoint-aware descriptive action coaching. arXiv preprint arXiv:2411.17130, 2024. 2 [29] Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, and Maosong Sun. Streamingbench: Assessing the gap for mllms to achieve streaming video understanding. arXiv preprint arXiv:2411.03628, 2024. 2, 3, 6, 7, 13 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2 [31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 2 [32] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. Et bench: Towards open-ended eventlevel video-language understanding. In NeurIPS, 2024. 2, 3, [33] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatialIn ICLR, temporal understanding at arbitrary resolution. 2025. 2 [34] Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment. arXiv preprint arXiv:2502.04328, 2025. 1, 2, 6, 7 [35] Joanna Materzynska, Guillaume Berger, Ingo Bax, and Roland Memisevic. The jester dataset: large-scale video dataset of human gestures. In ICCV, 2019. 4, 12 [36] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark Plumbley, Yuexian Zou, and Wenwu Wang. Wavcaps: chatgpt-assisted weaklylabelled audio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. 6 [37] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In ICASSP, 2015. 6, 16 [38] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Instruction tuning with gpt-4. arXiv and Jianfeng Gao. preprint arXiv:2304.03277, 2023. 13 [39] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video In NeurIPS, understanding with large language models. 2024. 2 [40] Rui Qian, Shuangrui Ding, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Dispider: Enabling video llms with active real-time interaction via disentangled perception, decision, and reaction. In CVPR, 2025. 1, 3, 6, 7 [41] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. 4, 12 [42] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 6 [43] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6, [44] An-Lan Wang, Bin Shan, Wei Shi, Kun-Yu Lin, Xiang Fei, Guozhi Tang, Lei Liao, Jingqun Tang, Can Huang, and WeiShi Zheng. Pargo: Bridging vision-language with partial and global views. In AAAI, 2025. 2 [45] Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu Huang. Grounded-videollm: Sharpening fine-grained temarXiv poral grounding in video large language models. preprint arXiv:2410.03290, 2024. 2 [46] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. 2 [47] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 6 [48] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. In ECCV, 2024. 2 [49] Yueqian Wang, Xiaojun Meng, Yuxuan Wang, Jianxin Liang, Jiansheng Wei, Huishuai Zhang, and Dongyan Zhao. Videollm knows when to speak: Enhancing time-sensitive video comprehension with video-text duet interaction format. arXiv preprint arXiv:2411.17991, 2024. 1, 3, 5, 6, 7, [64] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. Internlm-xcomposer2. 5-omnilive: comprehensive multimodal system for long-term streaming video and audio interactions. arXiv preprint arXiv:2412.09596, 2024. 2, 3, 6 [65] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 2 [66] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 1, 2, 6 [67] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 1, 2, 6 [68] Jiaxing Zhao, Qize Yang, Yixing Peng, Detao Bai, Shimin Yao, Boyuan Sun, Xiang Chen, Shenghao Fu, Xihan Wei, Liefeng Bo, et al. Humanomni: large vision-speech language model for human-centric video understanding. arXiv preprint arXiv:2501.15111, 2025. 2 [50] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, 2021. [51] Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, and Ziwei Liu. Funqa: Towards surprising video comprehension. In ECCV, 2024. 2, 4, 12 [52] Zhifei Xie and Changqiao Wu. Mini-omni2: Towards opensource gpt-4o with vision, speech and duplex capabilities. arXiv preprint arXiv:2410.11190, 2024. 3 [53] Haomiao Xiong, Zongxin Yang, Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Jiawen Zhu, and Huchuan Lu. Streaming video understanding and multi-round interaction with memoryenhanced knowledge. In ICLR, 2025. 3 [54] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Slowfast-llava: strong training-free baseDehghan. arXiv preprint line for video large language models. arXiv:2407.15841, 2024. 2 [55] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. 6 [56] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [57] Qize Yang, Detao Bai, Yi-Xing Peng, and Xihan Wei. Omniemotion: Extending video mllm with detailed face and audio modeling for multimodal emotion analysis. arXiv preprint arXiv:2501.09502, 2025. 2 [58] Zhenyu Yang, Yuhang Hu, Zemin Du, Dizhan Xue, Shengsheng Qian, Jiahong Wu, Fan Yang, Weiming Dong, and Changsheng Xu. Svbench: benchmark with temporal multi-turn dialogues for streaming video understanding. In ICLR, 2025. 3 [59] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 6 [60] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI, 2019. 3 [61] Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency. Social-iq: question answerIn CVPR, ing benchmark for artificial social intelligence. 2019. 4, 12 [62] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memorybased real-time understanding for long video streams. arXiv preprint arXiv:2406.08085, 2024. 3, 6, [63] Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Xiaonan Huang, Changxin Gao, Shanjun Zhang, Li Yu, and Nong Sang. Holmes-vau: Towards long-term video anomaly understanding at any granularity. arXiv preprint arXiv:2412.06171, 2024. 3, 4, 12 License CC BY-NC-SA 4.0 CC BY-NC-SA 4.0 Datasets OOPS [13] FunQA [51] SocialIQA [41] MIT MIT HIVAU [63] Social-IQ [61] MIT N/A IntentQA [24] N/A Jester [35] N/A SMILE [20] Table 11. License of source datasets in ViSpeak-Bench and ViSpeak-Instruct. Figure 3. Statistics on participants who recorded videos. The participants comprised nearly equal numbers of males and females, with ages ranging from 10 to 70 years. 7. More Details for ViSpeak-Bench and ViSpeak-Instruct 7.1. Licenses The self-collected videos in our ViSpeak-Bench and ViSpeak-Instruct are provided to the community under CC BY-NC-SA 4.0 license. By downloading our dataset from our website or other sources, the user agrees to adhere to the terms of CC BY-NC-SA 4.0 and licenses of other source datasets. Licenses of the source datasets are listed in Table 11. 7.2. Participants in Collecting Videos To collect the ViSpeak-Bench and ViSpeak-Instruct datasets, we recruit team of 610 people (346 men and 264 women) with an age ranging from 10 to 70 years old from 5 provinces, summarized in Figure 3. We obtained signed agreements from each participant to ensure that their data can be utilized by the community. 7.3. Prompts for Dataset Construction During the data collection procedure, we use GPT-4o [19] to reformulate the responses and generate conversation scripts. Since original datasets have high-quality annotations, we directly use these annotations as conditions, which greatly decreases the difficulty for GPT-4o to translate. The reformulated responses contain two parts: the first one is what action or event happens and the second part is some reasonable responses toward the action or event. Prompts for Anomaly Warning and Humor Reaction are displayed as follows."
        },
        {
            "title": "The prompt for reformulating the responses in\nHIVAU dataset",
            "content": "Suppose you are helpful AI chatbot that will give the user some advice based on any anomalous situations. You should first identify whether an anomaly event exists. If it does, give the user some advice in sentence and in conversational tone assuming the event has actually happened. The output should be in dict format, like {anomaly: 0, advice: None} or {anomaly: 1, advice: Your advice}, where 0 indicates no anomaly event and 1 indicates an anomaly event. Description: {caption} Output:"
        },
        {
            "title": "The prompt for reformulating the responses in\nOOPS dataset",
            "content": "Suppose you are helpful AI chatbot that will give the user some advice based on the given unintentional situations. Assume you have seen the situation and remind the user. You should first describe the situation and give the user some advice in sentence and in conversational tone. Description: {caption} Output:"
        },
        {
            "title": "The prompt for reformulating the responses in\nFunQA dataset",
            "content": "Change the input to conversational tone as if you are talking to someone about the scene you are watching now. Do not output imaginary contents. Description: {caption} Output: For the Gesture Understanding task, we manually select 10 common gestures from the Jester [35] dataset and the other 10 gestures collected by ourselves. Gestures from Jester are Swiping Right, Swiping Down, Swiping Left, Swiping Up, Pulling Hand In, Pushing Hand Away, Zooming Out With Full Hand, Zooming In With Full Hand, Thumb Down, and Thumb Up. Our self-collected gestures are Zero, One, Two, Three, Four, Five, Victory, Finger Heart 1, Finger Heart 2, and OK. Note that many gestures are similar, for example Two is similar to Victory, Three is similar to OK. The meaning of gestures varies in different contexts. Thus, we use GPT-4o to generate wide variety of scripts for video recording. The prompt is shown below. we used are as follows: For Gesture Understanding, the prompts are:"
        },
        {
            "title": "The prompt for generating gesture understanding\nscripts",
            "content": "Suppose you are talking to user. Your task is to generate reasonable conversation context for gesture from the user. For example, suppose the gesture is number 5, reasonable context is {human: Can you share something with me?, gpt: was just looking at how many hours you usually spend on your hobbies each week. How many do you think it is?, human: number 5, gpt: Your gesture is the number 5. Thats great! It sounds like you really dedicate some solid time to your hobbies. What do you enjoy doing the most during those hours?}. In the first round, the user start the conversation. Then, in the second round, you should start with various topics. The input gesture in the third round is the feedback from the user. After receiving the feedback from the user, you should first point out the gesture and then generate friendly or helpful feedback in the last round. Note that conversations should be unrelated to specific environment, but it should be highly reasonable to perform the gesture in this context. You should answer the question following the template in the example. Gesture: {gesture} Output: For scripts in the visual termination and visual interruption tasks, we select QA pairs from GPT-4-LLM [38], with long ones for Visual Interruption and short ones for Visual Termination. Step 1: Youre watching video. At this moment in the video, is there any gesture being made in the video? You can only answer yes or no. Step 2: What gesture did the person in the video make, and what does it signify when considering the context of the preceding conversation? For Visual Wake-Up, the prompts are: Step 1: Youre watching video. At this moment in the video, is there any gesture/action being made in the video? You can only answer yes or no. Step 2: When you see greeting gesture, what should you respond to me? Directly output your response. For Visual Termination, the prompts are: Step 1: Youre watching video. At this moment in the video, is there any gesture/action being made in the video? You can only answer yes or no. Step 2: When you see the goodbye gesture, what should you respond to me? Directly output your response. For Visual Interruption, the prompts are: Step 1: Youre watching video. At this moment in the video, is there any gesture/action being made in the video? You can only answer yes or no. Step 2: When you see the body language or gesture that indicates interruption, you should say stop. What should you respond to me now? Directly output your response. For Anomaly Warning, the prompts are: Step 1: Youre watching video. At this moment in the video, is there anything unusual happening in the video? You can only answer yes or no. Step 2: What unusual events occur in this video, and what 7.4. Evaluation Method for Offline Models on is your suggestion based on these observations? ViSpeak-Bench Since most existing Large Video Language Models are offline models, we change the proactive output problems in our ViSpeak-Bench into offline ones following StreamingBench [29] and OVO-Bench [27]. The evaluation is broken into two-step evaluation. In the first step, we will inquire the model whether it is an appropriate time to provide response iteratively at each timestamp to find an appropriate time for response. The sub-video from the beginning till now is used as if it is full video. In the second step, the model generates the actual responses based on the context up to now. Further, since existing offline models are not finetuned on our ViSpeak-Instruct, they can not generate proper responses without explicit prompts and the prompts For Humor Reaction, the prompts are: Step 1: Youre watching video. At this moment in the video, is there anything funny happening in the video? You can only answer yes or no. Step 2: What interesting events occurred in the video, and why? 7.5. Evaluation Prompts for ViSpeak-Bench As most of questions in our ViSpeak-Bench are open-ended questions. Thus, we utilize GPT-4o as the judge for evaluation. The prompts for each subtask are shown as follows. The prompt for Visual Wake-Up"
        },
        {
            "title": "The prompt for Visual Interruption",
            "content": "Determine if the GPT text expresses greeting intent. Use 0-5 scale where 0 indicates no greeting signals and 5 shows explicit welcoming language. 5 = Explicit welcoming phrases (Hello!,How can assist?) 4 = Contextual opening (Matching time-based greetings like Good morning) 3 = Service-oriented initiation (What can help with?) 2 = Neutral conversational starters (Shall we begin?) 1 = Implicit opening cues (Single welcoming emoji) 0 = No greeting/dialogue initiation detected DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. DO NOT INCLUDE ANY MARKDOWN FORMAT. Only provide the score and reason in JSON format. For example, {score: 3, reason: this is reason for the score}"
        },
        {
            "title": "The prompt for Visual Termination",
            "content": "Determine if the GPT text contains intent to conclude the conversation. Use 0-5 scale where 0 indicates no termination signals and 5 shows explicit closure intent. 5 = Contains explicit closure phrases (e.g., Goodbye, End of discussion) 4 = Implies conclusion with summary + sign-off (e.g., In summary...feel free to ask more) 3 = Neutral statement with closing tendency (e.g., Let me know if needed) 2 = Ambiguous transitional phrasing (e.g., Anyway...) 1 = Contains weak closure cues (e.g., single emoji/period) 0 = No termination indicators, continues dialog flow DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. DO NOT INCLUDE ANY MARKDOWN FORMAT. Only provide the score and reason in JSON format. For example, {score: 3, reason: this is reason for the score} Determine if the GPT text contains cessation intent (stopping speaking/concluding). Use 0-5 scale where 0 indicates continued engagement and 5 shows explicit termination. 5 = Explicit acknowledgment (Ill stop here) + closure 4 = Implicit cessation recognition (Unless you need more...) 3 = Passive-agreement cessation (As requested, ending now) 2 = Ambiguous continuation (...but) after stop cue 1 = Partial acknowledgment without closure 0 = Complete continuation disregard DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. DO NOT INCLUDE ANY MARKDOWN FORMAT. Only provide the score and reason in JSON format. For example, {score: 3, reason: this is reason for the score}"
        },
        {
            "title": "The prompt for Anomaly Warning",
            "content": "Evaluate video anomaly response from GPT with the the following metric, total score of 5 points, with separate scores for Description Consistency between ground truth and Advice Rationality: taking into account Description Consistency between Ground Truth: 3 = Core elements match without errors 2 = Core elements match with minor errors 1 = Superficial match 0 = Key element errors or contradictory Advice Rationality: 2 = Actionable & Safe & Logically sound 1 = Generally appropriate 0 = Dangerous/hallucinated DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. DO NOT INCLUDE ANY MARKDOWN FORMAT. Only provide the Total score and reason in JSON format. For example, {description: 3, advice: 2, reason: this is reason for the score}"
        },
        {
            "title": "The prompt for Humor Reaction",
            "content": "7.6. Examples of Each Subtask Evaluate alignment between Ground Truth and GPT Text regarding humorous event descriptions. 5 = Perfect match in humor and delivery 4 = Preserves main humor, but with minor changes to the story or details 3 = Only partial humor retention with some deviations 2 = Only partial humor retention and some important parts are missing 1 = Superficial similarity only 0 = No comedic correlation DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. DO NOT INCLUDE ANY MARKDOWN FORMAT. Only provide the score and reason in JSON format. For example, {score: 3, reason: this is reason for the score}"
        },
        {
            "title": "The prompt for Gesture Understanding",
            "content": "Evaluate gesture response from GPT with the following metric, taking into account the total score of 5 points, with separate scores for gesture recognition and contextual appropriateness of the response: Gesture recognition: 3 = Precise gesture identification 2 = Ambiguous gesture reference 1 = No explicit mention of gestures 0 = Hallucinated/non-existent gesture Contextual appropriateness: 2 = Natural integration with dialogue 1 = Generic but relevant response 0 = Irrelevant/contradictor response [Dialogue History] provided for context [Gesture] is the ground truth [Contextual Reference Text] as reference, but does not have to match exactly DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. DO NOT INCLUDE ANY MARKDOWN FORMAT. Only provide the Total score and reason in JSON format. For example, {description: 3, advice: 2, reason: this is reason for the score} From Figure 4 to Figure 10, we visualize some samples in each task, each of which is annotated with accurate timestamps and referenced response. We also visualize the outputs from our ViSpeak model. 7.7. Examples of Self-Annotated Gesture Understanding Data In Figure 11, we visualize some examples of self-annotated gesture understanding data. Each sample is annotated with two questions: the first one is to ask what the gesture is and the second one is to ask the meaning of the gesture. Gestures in natural conversations greatly enhance the diversity of our dataset. 7.8. Evaluation of ViSpeak on Visual Interruption Since recent LMMs can not be interrupted by visual instructions, we actually do not evaluate their ability to be interrupted. As illustrated in Section 7.4, we simplify the problem to recognize the stop gesture. But when evaluating our ViSpeak, we use the following methods to evaluate the ability to be interrupted. Taking Figure 6 as an example, we assume that the question from the user arises at 00:06. Then, we directly use the long reply from the annotation files as responses to prevent the model-generated replies from being too short to be interrupted. We replace the predicted token in the next token prediction with the token in the long reply until token is predicted on <seg> token, which means the model is interrupted. 7.9. Failure Case and Analysis In Figure 12, we visualize some failure cases of ViSpeak and mainly summarize them into three parts. First, ViSpeak may respond at an improper time. In the first example, there is nothing special in the video but ViSpeak begins to speak at 00:11 with some hallucinations. And ViSpeak may also ignore some actions and events. Second, ViSpeak may not understand the visual content in the video. As shown in the second video, the cat is actually in toilet but ViSpeak mistakenly recognizes the toilet as box thus failing to get the actual humor. In addition, ViSpeak may also not be aware of the context of the conversation. In the third example, the agent has asked the user about the feeling, not the number. But ViSpeak mistakenly recognizes the gesture as number 4. Improvements in the future could solve the problems above to get more intelligent agent. 8. Limitation 1) Due to the difficulty of the task and resource constraint, the diversity and scale of ViSpeak-Instruct are now relatively smaller than other well-known instruction following datasets. Expanding dataset size, collecting more diverse videos, and enriching more valuable sub-tasks are left for future work. 2) Second, due to computation constraints, ViSpeak is only trained with 6k context. We believe longer context will enhance users experience. And memory mechanism can equip the model with the long-term streaming video understanding ability. 3) Further, since we divide an integral audio into multiple small segments, we find the Automatic Speech Recognition (ASR) ability of ViSpeak degrades lot, getting only 18.4 WER on LibriSpeech [37]. Training with more audio data can possibly mitigate the problem. But we find ViSpeak still achieves SOTA performance on Omni-Source Understanding tasks of StreamingBench. Figure 4. Examples of Visual Wake-Up in ViSpeak-Bench and the corresponding output by ViSpeak. Figure 5. Examples of Visual Termination in ViSpeak-Bench and the corresponding output by ViSpeak. The first round conversation is used as context. Figure 6. Examples of Visual Interruption in ViSpeak-Bench and the corresponding output by ViSpeak. The first round conversation is used as context. Figure 7. Examples of Gesture Understanding in ViSpeak-Bench and the corresponding output by ViSpeak. The first round conversation is used as context. Figure 8. Examples of Anomaly Warning in ViSpeak-Bench and the corresponding output by ViSpeak. Figure 9. Examples of Humor Reaction in ViSpeak-Bench and the corresponding output by ViSpeak. Figure 10. Examples of Visual Reference in ViSpeak-Bench and the corresponding output by ViSpeak. Figure 11. Examples of our self-annotated data for gesture understanding. Figure 12. Examples of failure cases. The Time Mistake denotes the model responds at an improper time. The Content Mistake denotes the model fails to understand the visual content in the video. The Context Mistake means the model is unaware of the context of the conversation."
        }
    ],
    "affiliations": [
        "Guangdong Province Key Laboratory of Information Security Technology, China",
        "Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
        "Pazhou Laboratory (Huangpu), China",
        "Peng Cheng Laboratory, China",
        "School of Computer Science and Engineering, Sun Yat-sen University, China",
        "Tongyi Lab, Alibaba Group"
    ]
}