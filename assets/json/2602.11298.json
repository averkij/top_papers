{
    "paper_title": "Voxtral Realtime",
    "authors": [
        "Alexander H. Liu",
        "Andy Ehrenberg",
        "Andy Lo",
        "Chen-Yo Sun",
        "Guillaume Lample",
        "Jean-Malo Delignon",
        "Khyathi Raghavi Chandu",
        "Patrick von Platen",
        "Pavankumar Reddy Muddireddy",
        "Rohin Arora",
        "Sanchit Gandhi",
        "Sandeep Subramanian",
        "Soham Ghosh",
        "Srijan Mishra",
        "Abhinav Rastogi",
        "Alan Jeffares",
        "Albert Jiang",
        "Alexandre Sablayrolles",
        "Amélie Héliou",
        "Andrew Bai",
        "Angele Lenglemetz",
        "Anmol Agarwal",
        "Anton Eliseev",
        "Antonia Calvi",
        "Arjun Majumdar",
        "Baptiste Bout",
        "Baptiste Rozière",
        "Baudouin De Monicault",
        "Benjamin Tibi",
        "Clémence Lanfranchi",
        "Connor Chen",
        "Corentin Barreau",
        "Corentin Sautier",
        "Cyprien Courtot",
        "Darius Dabert",
        "Diego de las Casas",
        "Elliot Chane-Sane",
        "Enguerrand Paquin",
        "Faruk Ahmed",
        "Federico Baldassarre",
        "Gabrielle Berrada",
        "Gaëtan Ecrepont",
        "Gauthier Guinet",
        "Genevieve Hayes",
        "Georgii Novikov",
        "Giada Pistilli",
        "Guillaume Martin",
        "Gunjan Dhanuka",
        "Gunshi Gupta",
        "Han Zhou",
        "Indraneel Mukherjee",
        "Irene Zhang",
        "Jaeyoung Kim",
        "Jan Ludziejewski",
        "Jason Rute",
        "Joachim Studnia",
        "John Harvill",
        "Jonas Amar",
        "Josselin Somerville Roberts",
        "Julien Tauran",
        "Karmesh Yadav",
        "Kartik Khandelwal",
        "Kush Jain",
        "Laurence Aitchison",
        "Léonard Blier",
        "Lingxiao Zhao",
        "Louis Martin",
        "Lucile Saulnier",
        "Luyu Gao",
        "Maarten Buyl",
        "Manan Sharma",
        "Margaret Jennings",
        "Marie Pellat",
        "Mark Prins",
        "Mathieu Poirée",
        "Mathilde Guillaumin",
        "Matthieu Dinot",
        "Matthieu Futeral",
        "Maxime Darrin",
        "Maximilian Augustin",
        "Mert Unsal",
        "Mia Chiquier",
        "Nathan Grinsztajn",
        "Neha Gupta",
        "Olivier Bousquet",
        "Olivier Duchenne",
        "Patricia Wang",
        "Paul Jacob",
        "Paul Wambergue",
        "Paula Kurylowicz",
        "Philomène Chagniot",
        "Pierre Stock",
        "Piotr Miłoś",
        "Prateek Gupta",
        "Pravesh Agrawal",
        "Quentin Torroba",
        "Ram Ramrakhya",
        "Rishi Shah",
        "Romain Sauvestre",
        "Roman Soletskyi",
        "Rosalie Millner",
        "Sagar Vaze",
        "Samuel Humeau",
        "Siddharth Gandhi",
        "Sumukh Aithal",
        "Szymon Antoniak",
        "Teven Le Scao",
        "Théo Cachet",
        "Theo Simon Sorg",
        "Thibaut Lavril",
        "Thomas Chabal",
        "Thomas Foubert",
        "Thomas Robert",
        "Thomas Wang",
        "Tim Lawson",
        "Tom Bewley",
        "Tom Edwards",
        "Tyler Wang",
        "Valeriia Nemychnikova",
        "Van Phung",
        "Vedant Nanda",
        "Victor Jouault",
        "Virgile Richard",
        "Vladislav Bataev",
        "Wassim Bouaziz",
        "Wen-Ding Li",
        "William Marshall",
        "Xinghui Li",
        "Xingran Guo",
        "Xinyu Yang",
        "Yannic Neuhaus",
        "Yihan Wang",
        "Zaccharie Ramzi",
        "Zhenlin Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to a large-scale dataset spanning 13 languages. At a delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license."
        },
        {
            "title": "Start",
            "content": "We introduce Voxtral Realtime, natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing new causal audio encoder and Ada RMS-Norm for improved delay conditioning. We scale pretraining to large-scale dataset spanning 13 languages. At delay of 480ms, Voxtral Realtime achieves performance on par with Whisper, the most widely deployed offline transcription system. We release the model weights under the Apache 2.0 license. Webpage: Model weights: https://mistral.ai/news/voxtral-transcribe-2 https://huggingface.co/mistralai/Voxtral-Mini-4B-Realtime-2602 6 2 0 2 1 ] . [ 1 8 9 2 1 1 . 2 0 6 2 : r Figure 1: Voxtral Realtime approaches offline accuracy at sub-second latency. Macro-average word errorrate (WER) vs. delay on the FLEURS multilingual benchmark for realtime and offline models. Lower is better. At 480 ms delay, Voxtral Realtime is competitive with Scribe v2 Realtime, the leading realtime API model, as well as Whisper, the most popular open-source offline model. It surpasses both baselines at 960 ms delay, approaching the performance of Voxtral Mini Transcribe V2, state-of-the-art offline transcription model."
        },
        {
            "title": "Introduction",
            "content": "Automatic speech recognition (ASR) systems achieve strong performance in offline settings [Radford et al., 2023, Liu et al., 2025], where the entire audio input is available before transcription begins. However, many real-world applicationssuch as voice assistants, live captioning, and interactive speech interfacesrequire transcriptions to be produced in real time while audio is streaming, under strict latency constraints. Bridging the gap between offline transcription quality and real-time streaming remains central challenge in speech recognition [Graves, 2012, Zeghidour et al., 2025]. common approach to streaming adapts offline models by processing audio in short chunks as it arrives [Macháˇcek et al., 2023]. While effective at moderate latencies, this strategy has fundamental limitation: offline models are typically trained with access to bidirectional acoustic context (and often full-sequence conditioning), whereas streaming system must emit tokens before future audio is available. This traininginference mismatch becomes increasingly severe as latency is reduced, and often leads to degraded accuracy in low-delay regimes and out-of-distribution settings. Native streaming architectures address this by reformulating the learning problem so that each output token is predicted using only past inputs and bounded amount of lookahead, making latency tunable constraint. This requires (i) an explicit alignment between input audio and output text (e.g., wordor frame-level alignments), and (ii) an architecture that processes new audio incrementally as it arrives. common instantiation is neural transducer (RNN-T) with streaming encoder that limits right context through chunking, memory, and caching [Graves, 2012, Shi et al., 2020, Chen et al., 2021, Noroozi et al., 2024]. Delayed Streams Modeling (DSM) [Zeghidour et al., 2025] follows the same alignmentbased principle, but replaces the transducer with decoder-only model over aligned audio and text streams, enabling simpler designs that leverage pre-trained language decoders. DSM approaches offline accuracy at high delay settings. However, achieving offline-level performance at sub-second latencyparticularly in multilingual and multi-domain settingshas remained an open challenge. We address this challenge by introducing Voxtral Realtime, 4B parameter natively streaming ASR model that supports 13 languages. Concretely, our contributions are: causal audio encoder trained from scratch with modern architectural choices (RMSNorm, SwiGLU, RoPE, sliding window attention). An adaptive RMS-Norm (Ada RMS-Norm) mechanism in the decoder, enabling single model to operate at any delay that is multiple of 80 ms. Pretraining at scale on large-scale dataset spanning 13 languages, enabling robust generalization across languages and domains. At delay of 480 ms, Voxtral Realtime achieves performance competitive with Whisper [Radford et al., 2023] and ElevenLabs Scribe v2 Realtime [ElevenLabs Team, 2026]. At higher delay settings (e.g., 960 ms), it matches or surpasses strong offline baselines such as Voxtral Mini Transcribe V2 on several English and multilingual benchmarks [Mistral AI Team, 2026]. These results demonstrate that offlinelevel transcription quality can be achieved within fully streaming framework at sub-second latency. We release the resulting model as open weights under the Apache 2.0 license. The remainder of this report details the model architecture, training and inference methodology, and empirical evaluations that support these findings."
        },
        {
            "title": "2 Modeling",
            "content": "Voxtral Realtime is Transformer-based streaming ASR model that follows the stream-synchronous design of DSM. The model comprises (i) causal audio encoder, (ii) temporal adapter that downsamples encoder frames, and (iii) Transformer decoder that generates text autoregressively. At each stream step, the decoder consumes fused representation obtained by summing the currentstep audio embedding with the embedding of the most recently generated text token. The overall architecture is summarized in Figure 2, with model dimensions outlined in Table 1. 2 Figure 2: Voxtral Realtime architecture and decoding scheme for target delay τ = 80 ms. Voxtral Realtime consists of causal audio encoder to embed the input audio stream, an MLP adapter layer to temporally downsample the audio embeddings, and text decoder to auto-regressively generate the output text stream. The downsampled audio embeddings from the adapter and the embeddings of previously generated tokens have the same frame-rate of 12.5Hz, with each frame representing 80ms of audio. These are summed and processed by the text decoder, which predicts one token per frame. The decoder emits padding token [P] while waiting for sufficient acoustic evidence. Once word is acoustically complete and the target delay τ has elapsed, word-boundary token [W] is emitted to initiate generation, followed by the corresponding subword tokens. Table 1: Voxtral Realtime configuration. For the decoder, we use grouped-query attention (GQA) [Ainslie et al., 2023]; the number in parentheses indicates KV heads. Sliding window sizes are in frames (encoder) and tokens (decoder). Component"
        },
        {
            "title": "Total",
            "content": "32 1 26 1280 12804 3072 3072 32 32 (8 KV) 750 8192 970M 25M 3.4B 4.4B"
        },
        {
            "title": "2.1 Audio Encoder",
            "content": "Audio encoders for offline ASR systems are typically trained with bidirectional attention [Baevski et al., 2020, Radford et al., 2023], since the full audio signal is available. However, in real-time settings the encoder must produce representations causally, attending only to current and past inputs. Therefore, we define causal audio encoder architecture and train it from scratch. The waveform is converted to log-Mel spectrogram [Davis and Mermelstein, 1980] with 128 Mel bins and hop length of 160 samples (10 ms at 16 kHz). Features are processed by causal convolutional stem with 2x temporal downsampling, followed by stack of causal self-attention layers. The encoder emits one embedding every 20 ms (50 Hz). The convolutional stem induces finite history dependency: the output at step depends on the previous four input frames (two kernel-3 convolutions, including strided downsampling layer). During streaming inference, we maintain 4-frame history buffer to compute the current encoder state exactly. In the Transformer backbone, we adopt RMSNorm, SwiGLU and RoPEarchitectural choices that have been shown to improve training stability and downstream performance [Zhang and Sennrich, 2019, Shazeer, 2020, Su et al., 2023, Touvron et al., 2023]. The self-attention uses sliding window of 3 Table 2: Whisper vs. Voxtral Realtime encoder architectures. Voxtral Realtime is fully causal encoder that leverages modern architectural choices with 750 frame sliding window."
        },
        {
            "title": "FFN",
            "content": "Pos-Enc Window Params (M)"
        },
        {
            "title": "Fixed\nSliding",
            "content": ""
        },
        {
            "title": "Causal",
            "content": "750 frames (15 at 50 Hz) [Child et al., 2019, Beltagy et al., 2020], bounding memory while enabling unbounded streaming. The differences in relation to the Whisper encoder are summarized in Table 2."
        },
        {
            "title": "2.2 Adapter Layer",
            "content": "To reduce the effective sequence length processed by the language decoder, we insert lightweight adapter layer between the audio encoder and the decoder. This adapter consists of single MLP that temporally downsamples the encoder outputs, reducing computational cost in the language decoder while preserving relevant acoustic information. Following Voxtral [Liu et al., 2025], we apply downsampling factor of 4x, resulting in an effective frame rate of 12.5 Hz. Therefore, each downsampled audio embedding represents 80 ms of audio."
        },
        {
            "title": "2.3 Language Decoder",
            "content": "The language decoder is decoder-only Transformer that operates synchronously with the adapter stream, closely following the delayed-streams decoding scheme of DSM [Zeghidour et al., 2025]. At each adapter step (80 ms), the model performs one autoregressive decoding step. The emitted token can be either text token or non-emitting placeholder. The placeholder allows the model to wait when acoustic evidence is insufficient, deferring text emission until the target delay has elapsed. This mechanism enables the model to learn emission timing end-to-end, without external voice activity decection (VAD) or forced alignments. The construction of training targets is described in Section 3. We condition the decoder on target streaming delay τ , which specifies minimum offset between acoustic evidence and the earliest time at which corresponding text tokens may be produced, using an Adaptive RMSNorm (AdaRMSNorm) mechanism. τ is embedded as sinusoidal embedding and projected using small MLP with GELU activation to vector g(τ ) Rd, where is the model dimension. This conditioning is injected additively in the normalized space on the feed-forward branch of every Transformer block; the attention branch remains unconditioned. Specifically, given hidden states x, block computes: rattn = Attn(RMSNorm(x)), = + rattn, rffn = FFN(RMSNorm(h) (1.0 + g(τ ))) , = + rffn. To minimize the additional parameter count, we use an inner-dimension of 32 for the MLP in g(τ ), which introduces 5M extra parameters for the 4.4B model. In Section 6.1, we show that this form of additive conditioning is more effective than alternative time-conditioning strategies. The decoder uses sliding window attention with left-context of 8192 tokens. Together with the encoders sliding window, this supports arbitrarily long streams with bounded memory."
        },
        {
            "title": "3.1 Target Construction",
            "content": "Training streaming ASR model requires supervision that aligns continuous audio stream with discrete text stream. We leverage (audio, text, word-level timestamps) tuples to build frame-synchronous target sequences for the language decoder. In addition to the base subword vocabulary, we introduce two special symbols: padding token [P] and word-boundary token [W]. Training targets are constructed such that the decoder emits exactly one token per downsampled audio frame (80 ms). For frames in which no text emission is 4 warrantedeither because no word is currently underway or because the current word is acoustically incompletethe target token is [P]. Once word has been fully observed and the specified target delay has elapsed, [W] token is emitted to mark the onset of text generation, followed by the subword tokens corresponding to the word itself. When consecutive words share the same emission frame, no additional [W] token is inserted; the subword tokens of the next word follow directly. We demonstrate in Section 6.2 that this grouping is crucial for retaining the text-modeling capabilities of the language decoder. This target construction induces an implicit alignment between the audio and text streams. The emission frame of the [W] token defines grouping point that associates segment of the audio stream with the subsequent text tokens. The model learns this alignment end-to-end from data, without relying on forced alignments or explicit decoding policies. At inference time, the same learned mechanism governs whether the decoder emits non-emitting token or initiates text generation at each audio frame."
        },
        {
            "title": "3.2 Delay Sampling",
            "content": "During training, the target delay τ is sampled uniformly from 80 ms to 2400 ms in 80 ms increments (i.e., 1 to 30 adapter frames). This exposes the model to range of latency-accuracy tradeoffs, enabling single model to operate at any delay within this range at inference time via the Ada RMS-Norm conditioning mechanism (Section 2.3)."
        },
        {
            "title": "3.3 Optimization",
            "content": "We initialize the encoder and adapter randomly and the decoder from Ministral 3B [Liu et al., 2026]. Training proceeds in two phases: 1. Encoder warm-up (5% of training): The decoder is frozen and only the encoder and adapter are trained. This prevents the randomly initialized encoder from destabilizing the pretrained decoder representations before it has learned to produce useful audio embeddings. 2. End-to-end (95% of training): The full model is trained jointly. We use the AdamW optimizer [Loshchilov and Hutter, 2019] with batch size of 370 hours. For the encoder warm-up, we use learning rate of 4 104, and for the end-to-end phase 6 105. We observed that logit magnitudes in the language decoder grew unboundedly over training. Since we tie the language modeling head and text embedding matrices, this caused text embedding norms to grow proportionally, while audio embedding norms steadily diminished. The resulting imbalance caused the model to increasingly rely on the text stream and ignore audio. We address this by applying z-loss penalty on the logit norm [de Brébisson and Vincent, 2016, Chowdhery et al., 2022], which encourages the softmax normalizer to remain close to zero. This allows the audio and text embedding norms to converge to stable values."
        },
        {
            "title": "Inference and Serving in vLLM",
            "content": "While low theoretical transcription delay is critical, practical deployments must maintain low latency under realistic conditions (e.g., batching, concurrency, and network overhead). In collaboration with the library authors and community contributors, we contribute realtime serving to the vLLM framework [Kwon et al., 2023] by combining (i) paged-attention backend for temporally heterogeneous encoder/decoder KV caches, (ii) resumable streaming sessions that preserve KV state across incremental updates, and (iii) WebSocket-based realtime endpoint for incremental audio ingestion and token output streaming. Together, these features enable serving Voxtral Realtime with low operational complexity while achieving high throughput."
        },
        {
            "title": "4.1 Paged Attention with Temporally Heterogeneous KV Caches",
            "content": "Voxtral Realtime requires maintaining two KV caches during inferenceone for the audio encoder and one for the language decodereach with different frame rate. Specifically, the encoder operates at 50 Hz and the language decoder at 12.5 Hz, the difference due to = 4 temporal pooling applied 5 by the adapter. Thus, one decoder step corresponds to four new encoder KV positions. Standard paged-attention implementations assume single, uniform KV-position increment per step, which leads to inconsistent block indexing unless the metadata is adapted. To support this efficiently, we implement custom attention-metadata backend that stretches the encoder-side KV-cache block size by the pooling factor (p = 4) and applies the same scaling to the associated indexing metadata (sequence lengths and query offsets), while expanding the slot mapping so that each original slot ID maps to contiguous range of slots. This keeps vLLMs paged-attention indexing consistent across the encoder and decoder and allows both KV caches to share unified paged-attention allocation, preserving the performance benefits of vLLMs optimized KV paging."
        },
        {
            "title": "4.2 Asynchronous Streaming Input with Resumable Requests",
            "content": "Most serving frameworks assume the full input is available before decoding begins, which prevents true realtime operation when input arrives continuously. In vLLM, incremental generation is enabled via resumable requests: streaming session persists an anchor request whose KV blocks are reused across incremental updates, so newly arrived input can be appended while reusing previously computed KV states. In our deployment, we pipeline I/O and compute: while the server buffers the next 80 ms audio increment, it concurrently performs one-token decoding step, so the next update can be appended and processed immediately when the chunk arrives. To stream output tokens while audio continues to arrive, vLLM pairs its existing async output generator with new async input generator, enabling full-duplex streaming (ingest and emit concurrently) rather than turn-based send-then-decode loop. schematic is provided in Appendix A.2."
        },
        {
            "title": "4.3 WebSocket-Based Realtime API",
            "content": "To make streaming sessions accessible in production, we contribute realtime WebSocket API to vLLM. The API provides bidirectional endpoint for incremental audio ingestion and output token streaming. Clients append audio chunks to an input buffer and periodically commit increments; the server converts these events into resumable session updates for the vLLM engine and streams back token deltas over the same persistent connection with low per-message overhead."
        },
        {
            "title": "5 Results",
            "content": "We evaluate Voxtral Realtime across English and multilingual benchmarks, comparing against offline systems, realtime APIs, and open-source streaming models. Figure 1 illustrates the latencyaccuracy trade-off of Voxtral Realtime on the FLEURS multilingual benchmark over 13 languages. Full results for each language are presented in Table 7. At delay of 480 ms, Voxtral Realtime approaches the accuracy of Scribe v2 Realtime, the industryleading realtime API model, as well as Whisper [Radford et al., 2023], the most widely adopted offline ASR system. Increasing the delay to 960 ms further closes the gap, with Voxtral Realtime surpassing past both Scribe v2 Realtime and Whisper. At higher delay of 2400 ms, the model continues to improve, achieving accuracy within 1% of Voxtral Mini Transcribe V2, state-of-the-art offline transcription model. Table 3 reports macro-average WER across four benchmark categories: English short-form, English long-form, FLEURS, and Mozilla Common Voice (MCV). English results are macro-averaged across tasks, while FLEURS and MCV results are averaged across languages. Full results for each task are presented in Appendix A.1. Across all benchmark categories, Voxtral Realtime substantially outperforms existing open-source streaming baselines at comparable latencies. Prior natively streaming approaches such as DSM achieve competitive accuracy only at substantially higher delays, while Nemotron Streaming [Noroozi et al., 2024] exhibits more limited latencyaccuracy trade-off and reduced robustness, particularly in long-form settings. In contrast, Voxtral Realtime consistently improves as latency increases and maintains strong performance across distributions. Notably, Voxtral Realtime supports 13 languages, while other recent open-source models such as DSM support only English and French. 6 Table 3: Macro-average WER (%) across benchmark categories.. English Short and Long results are averaged across tasks; MCV and FLEURS results are averaged across languages. The definition of \"target delay\" differs across Realtime APIs and is function of the audio input. Hence, we omit the delay for the APIs. Bold indicates the best realtime result. indicates that the multilingual task is unsupported for mono or bilingual model."
        },
        {
            "title": "Offline",
            "content": "Whisper Voxtral Mini Transcribe V"
        },
        {
            "title": "Realtime API",
            "content": "GPT-4o mini Transcribe Scribe v2 Realtime Realtime Open-Source DSM 1B En-Fr DSM 2.6B En"
        },
        {
            "title": "Voxtral Realtime",
            "content": "Delay (ms) En-Short En-Long FLEURS MCV WER (%) 500 2500 560 240 480 960 2400 8.39 7.27 7.93 7.33 12.26 8.11 9.59 9.41 9.95 8.47 7.94 7. 7.97 7.11 7.97 7.43 13.83 7.72 14.29 13.02 9.29 7.73 7.13 6.93 8.23 5. 7.95 8.34 10.80 8.72 7.70 6.73 14.25 8.07 12.85 20. 19.22 15.24 11.99 10.47 Taken together, these results demonstrate that Voxtral Realtime achieves offline-level transcription quality at sub-second latency with natively streaming architecture."
        },
        {
            "title": "6 Analysis",
            "content": "In this Section, we ablate three design choices: construction scheme, and the degree of left-padding. the delay-conditioning mechanism, the target"
        },
        {
            "title": "6.1 Ada RMS-Norm",
            "content": "There are multiple strategies available to incorporate delay conditioning into the model. DSM sums sinusoidal delay embedding with the combined audio-text embedding. Alternatively, special tokens can be injected in the text stream to indicate the target delay, though this requires repeating the token at sliding-window boundaries. third approach using Ada RMS-Norm injects the delay into the decoders residual stream (Section 2.3). Figure 3 plots WER results for three languages in the FLEURS dataset for the three conditioning methods outlined above. Summing and special tokens perform comparably, whereas Ada RMS-Norm leads to faster convergence and lower overall WER."
        },
        {
            "title": "6.2 Word Grouping",
            "content": "In Section 3.1, we describe the construction of the target tokens during training. Figure 4 compares two target schemes: inserting word-boundary token [W] between consecutive words in the same emission frame, or grouping them without boundary. Grouping results in much faster convergence and lower overall WERs. It preserves the subword sequences seen during language model pretraining, allowing the decoder to retain its learned text distributions. 7 (a) English (b) French (c) German Figure 3: Ablation of delay-conditioning mechanisms. Word error-rate on three languages from the FLEURS dataset as function of training progress. Ada RMS-Norm consistently improves convergence speed and final accuracy compared to alternative conditioning strategies. (a) English (b) French (c) German Figure 4: Ablation of target construction schemes. Word error-rate on three languages from the FLEURS dataset as function of training progress. Inserting single word-boundary token [W] per-group better preserves the capabilities of the pre-trained language decoder than inserting [W] per-word."
        },
        {
            "title": "6.3 Left-Padding",
            "content": "During inference, we explored inserting additional left-padding before the first audio frame. This does not affect streaming delay, as it only increases the size of the prefill step. We pad the audio stream with zeros (equivalent to silence) and the text stream with the corresponding number of [P] tokens. Table 4 shows the WER results across the four benchmark categories as the amount of padding is increased. Increasing the left-padding from 0 to 16 frames improves results across task categories. Further increasing to 32 frames yields additional gains all bar MCV. We hypothesize that left-padding introduces initial tokens that serve similar role to attention sinks [Xiao et al., 2024], but leave investigating this to future works."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced Voxtral Realtime, natively streaming speech transcription model that incorporates causal encoder, Ada RMS-Norm conditioning, and training pattern that leverage the pre-trained capabilities of the language decoder. By achieving near-offline performance at sub-second latency, Voxtral Realtime enables practical real-time applications such as live transcription, voice assistants, and interactive speech interfaces without sacrificing accuracy or language coverage. We release Voxtral Realtime as open weights under the Apache 2.0 license to support further research and deployment of high-quality streaming ASR systems."
        },
        {
            "title": "Core contributors",
            "content": "Alexander H. Liu, Andy Ehrenberg, Andy Lo, Chen-Yo Sun, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, Rohin Arora, Sanchit Gandhi, Sandeep Subramanian, Soham Ghosh, Srijan Mishra. 8 Table 4: Effect of left-padding on transcription accuracy. Macro-average WER (%) across benchmark categories for different degrees of left-padding. Padding Frames En-Short En-Long FLEURS MCV 0 16 32 9.10 8.53 8. 10.98 7.93 7.73 9.06 8.87 8.72 16.03 15.12 15."
        },
        {
            "title": "Contributors",
            "content": "Abhinav Rastogi, Alan Jeffares, Albert Jiang, Alexandre Sablayrolles, Amélie Héliou, Andrew Bai, Angele Lenglemetz, Anmol Agarwal, Anton Eliseev, Antonia Calvi, Arjun Majumdar, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Benjamin Tibi, Clémence Lanfranchi, Connor Chen, Corentin Barreau, Corentin Sautier, Cyprien Courtot, Darius Dabert, Diego de las Casas, Elliot Chane-Sane, Enguerrand Paquin, Faruk Ahmed, Federico Baldassarre, Gabrielle Berrada, Gaëtan Ecrepont, Gauthier Guinet, Genevieve Hayes, Georgii Novikov, Giada Pistilli, Guillaume Martin, Gunjan Dhanuka, Gunshi Gupta, Han Zhou, Indraneel Mukherjee, Irene Zhang, Jaeyoung Kim, Jan Ludziejewski, Jason Rute, Joachim Studnia, John Harvill, Jonas Amar, Josselin Somerville Roberts, Julien Tauran, Karmesh Yadav, Kartik Khandelwal, Kush Jain, Laurence Aitchison, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Maarten Buyl, Manan Sharma, Margaret Jennings, Marie Pellat, Mark Prins, Mathieu Poirée, Mathilde Guillaumin, Matthieu Dinot, Matthieu Futeral, Maxime Darrin, Maximilian Augustin, Mert Unsal, Mia Chiquier, Nathan Grinsztajn, Neha Gupta, Olivier Bousquet, Olivier Duchenne, Patricia Wang, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Philomène Chagniot, Pierre Stock, Piotr Miłos, Prateek Gupta, Pravesh Agrawal, Quentin Torroba, Ram Ramrakhya, Rishi Shah, Romain Sauvestre, Roman Soletskyi, Rosalie Millner, Sagar Vaze, Samuel Humeau, Siddharth Gandhi, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Théo Cachet, Theo Simon Sorg, Thibaut Lavril, Thomas Chabal, Thomas Foubert, Thomas Robert, Thomas Wang, Tim Lawson, Tom Bewley, Tom Edwards, Tyler Wang, Valeriia Nemychnikova, Van Phung, Vedant Nanda, Victor Jouault, Virgile Richard, Vladislav Bataev, Wassim Bouaziz, Wen-Ding Li, William Marshall, Xinghui Li, Xingran Guo, Xinyu Yang, Yannic Neuhaus, Yihan Wang, Zaccharie Ramzi, Zhenlin Xu."
        },
        {
            "title": "7.1 Acknowledgements",
            "content": "We would like to thank Joshua Deng, Yu Luo from Meta AI, and Nick Hill, Nicolò Lucchesi, Chen Zhang, Cyrus Leung, and Roger Wang from the vLLM team for their support and contributions in integrating Voxtral Realtime to the vLLM framework. We are grateful to Salvatore Sanfilippo, Awni Hannun, Prince Canuma, TrevorS and the open-source community for their contributions of Voxtral Realtime to additional frameworks."
        },
        {
            "title": "References",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints, 2023. URL https://arxiv.org/abs/2305.13245. Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: Framework for Self-Supervised Learning of Speech Representations, 2020. URL https://arxiv.org/abs/ 2006.11477. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-Document Transformer, 2020. URL https://arxiv.org/abs/2004.05150. Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, Mingjie Jin, Sanjeev Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Yujun Wang, Zhao You, and Zhiyong Yan. GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio. arXiv e-prints, art. arXiv:2106.06909, June 2021. Xie Chen, Yu Wu, Zhenghao Wang, Shujie Liu, and Jinyu Li. Developing real-time streaming transformer transducer for speech recognition on large-scale dataset. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 59045908. IEEE, 2021. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating Long Sequences with Sparse Transformers, 2019. URL https://arxiv.org/abs/1904.10509. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling Language Modeling with Pathways, 2022. URL https://arxiv.org/abs/2204.02311. Steven Davis and Paul Mermelstein. Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences. IEEE Transactions on Acoustics, Speech, and Signal Processing, 28(4):357366, 1980. Alexandre de Brébisson and Pascal Vincent. The Z-loss: shift and scale invariant classification loss belonging to the Spherical Family, 2016. URL https://arxiv.org/abs/1604.08859. Miguel Del Rio, Natalie Delworth, Ryan Westerman, Michelle Huang, Nishchal Bhandari, Joseph Palakapilly, Quinten McNamara, Joshua Dong, Piotr Zelasko, and Miguel Jetté. Earnings-21: Practical Benchmark for ASR in the Wild. In Proc. Interspeech 2021, pages 34653469, 2021. doi: 10.21437/Interspeech.2021-1915. Miguel Del Rio, Peter Ha, Quinten McNamara, Corey Miller, and Shipra Chandra. Earnings-22: Practical Benchmark for Accents in the Wild. arXiv e-prints, art. arXiv:2203.15591, March 2022. ElevenLabs Team. Introducing Scribe v2, January 2026. URL https://elevenlabs.io/blog/i ntroducing-scribe-v2. Accessed: 2026-02-06. J.J. Godfrey, E.C. Holliman, and J. McDaniel. SWITCHBOARD: telephone speech corpus for research and development. In [Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 517520 vol.1, 1992. doi: 10.1109/IC ASSP.1992.225858. Alex Graves. Sequence Transduction with Recurrent Neural Networks, 2012. URL https://arxi v.org/abs/1211.3711. François Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, and Yannick Estève. TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation, page 198208. Springer International Publishing, 2018. ISBN 9783319995793. doi: 10.1007/97 8-3-319-99579-3_21. URL http://dx.doi.org/10.1007/978-3-319-99579-3_21. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Alexander H. Liu, Andy Ehrenberg, Andy Lo, Clément Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, Sanchit Gandhi, Soham Ghosh, Srijan Mishra, Thomas Foubert, Abhinav Rastogi, Adam Yang, Albert Q. Jiang, Alexandre Sablayrolles, Amélie Héliou, Amélie Martin, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Clémence Lanfranchi, Darius Dabert, Devendra Singh Chaplot, Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gabrielle Berrada, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jason Rute, Jean-Hadrien Chabran, Jessica Chudnovsky, Joachim Studnia, Joep Barmentlo, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Karmesh Yadav, Kartik Khandelwal, Kush Jain, Lélio Renard Lavaud, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Matthieu Dinot, Maxime Darrin, Maximilian Augustin, Mickaël Seznec, Neha Gupta, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Rémi Delacourt, Romain Sauvestre, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Shashwat Dalal, Siddharth Gandhi, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Robert, Thomas Wang, Timothée Lacroix, Tom Bewley, Valeriia Nemychnikova, Victor Paltz, Virgile Richard, Wen-Ding Li, William Marshall, Xuanyu Zhang, Yihan Wan, and Yunhao Tang. Voxtral, 2025. URL https://arxiv.org/abs/2507.13264. Alexander H. Liu, Kartik Khandelwal, Sandeep Subramanian, Victor Jouault, Abhinav Rastogi, Adrien Sadé, Alan Jeffares, Albert Jiang, Alexandre Cahill, Alexandre Gavaudan, Alexandre Sablayrolles, Amélie Héliou, Amos You, Andy Ehrenberg, Andy Lo, Anton Eliseev, Antonia Calvi, Avinash Sooriyarachchi, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Clémence Lanfranchi, Corentin Barreau, Cyprien Courtot, Daniele Grattarola, Darius Dabert, Diego de las Casas, Elliot Chane-Sane, Faruk Ahmed, Gabrielle Berrada, Gaëtan Ecrepont, Gauthier Guinet, Georgii Novikov, Guillaume Kunsch, Guillaume Lample, Guillaume Martin, Gunshi Gupta, Jan Ludziejewski, Jason Rute, Joachim Studnia, Jonas Amar, Joséphine Delas, Josselin Somerville Roberts, Karmesh Yadav, Khyathi Chandu, Kush Jain, Laurence Aitchison, Laurent Fainsin, Léonard Blier, Lingxiao Zhao, Louis Martin, Lucile Saulnier, Luyu Gao, Maarten Buyl, Margaret Jennings, Marie Pellat, Mark Prins, Mathieu Poirée, Mathilde Guillaumin, Matthieu Dinot, Matthieu Futeral, Maxime Darrin, Maximilian Augustin, Mia Chiquier, Michel Schimpf, Nathan Grinsztajn, Neha Gupta, Nikhil Raghuraman, Olivier Bousquet, Olivier Duchenne, Patricia Wang, Patrick von Platen, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Quentin Torroba, Romain Sauvestre, Roman Soletskyi, Rupert Menneer, Sagar Vaze, Samuel Barry, Sanchit Gandhi, Siddhant Waghjale, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Teven Le Scao, Théo Cachet, Theo Simon Sorg, Thibaut Lavril, Thiziri Nait Saada, Thomas Chabal, Thomas Foubert, Thomas Robert, Thomas Wang, Tim Lawson, Tom Bewley, Tom Bewley, Tom Edwards, Umar Jamil, Umberto Tomasini, Valeriia Nemychnikova, Van Phung, Vincent Maladière, Virgile Richard, Wassim Bouaziz, Wen-Ding Li, William Marshall, Xinghui Li, Xinyu Yang, Yassine El Ouahidi, Yihan Wang, Yunhao Tang, and Zaccharie Ramzi. Ministral 3, 2026. URL https://arxiv.org/abs/2601.08584. Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization, 2019. URL https: //arxiv.org/abs/1711.05101. 11 Dominik Macháˇcek, Raj Dabre, and Ondˇrej Bojar. Turning Whisper into Real-Time Transcription In Sriparna Saha and Herry Sujaini, editors, Proceedings of the 13th International System. Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: System Demonstrations, pages 17 24, Bali, Indonesia, November 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.ijcnlp-demo.3. Mistral AI Team. Voxtral Transcribe 2, February 2026. URL https://mistral.ai/news/voxt ral-transcribe-2. Accessed: 2026-02-06. Vahid Noroozi, Somshubra Majumdar, Ankur Kumar, Jagadeesh Balam, and Boris Ginsburg. Stateful Conformer with Cache-based Inference for Streaming Automatic Speech Recognition, 2024. URL https://arxiv.org/abs/2312.17279. Patrick K. ONeill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid Noroozi, Yuekai Zhang, Oleksii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko, Keenan Freyberg, Michael D. Shulman, Boris Ginsburg, Shinji Watanabe, and Georg Kucsko. SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition. In Proc. Interspeech 2021, pages 14341438, 2021. doi: 10.21437/Interspeech.2021-1860. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210, 2015. doi: 10.1109/ICASSP.2015.7178964. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust Speech Recognition via Large-Scale Weak Supervision. In International Conference on Machine Learning, pages 2849228518. PMLR, 2023. Noam Shazeer. GLU Variants Improve Transformer, 2020. URL https://arxiv.org/abs/2002 .05202. Yangyang Shi, Yongqiang Wang, Chunyang Wu, Ching feng Yeh, Julian Chan, Frank Zhang, Duc Le, and Michael L. Seltzer. Emformer: Efficient Memory Transformer Based Acoustic Model ICASSP 2021 - 2021 IEEE International for Low Latency Streaming Speech Recognition. Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 67836787, 2020. URL https://api.semanticscholar.org/CorpusID:224818050. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding, 2023. URL https://arxiv.org/abs/2104.0 9864. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models, 2023. URL https://arxiv.org/abs/2302.13971. Emmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha, Jon Barker, and Ricard Marxer. An Analysis of Environment, Microphone and Data Simulation Mismatches in Robust Speech Recognition. Comput. Speech Lang., 46(C):535557, nov 2017. doi: 10.1016/j.csl.2016.11.005. URL https://doi.org/10.1016/j.csl.2016.11.005. ISSN 0885-2308. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. VoxPopuli: Large-Scale Multilingual Speech In ProCorpus for Representation Learning, Semi-Supervised Learning and Interpretation. ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 9931003, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.80. URL https://aclanthology.org/2021.acl-long.80. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient Streaming Language Models with Attention Sinks, 2024. URL https://arxiv.org/abs/2309.17453. 12 Neil Zeghidour, Eugene Kharitonov, Manu Orsini, Václav Volhejn, Gabriel de Marmiesse, Edouard Grave, Patrick Pérez, Laurent Mazaré, and Alexandre Défossez. Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling, 2025. URL https://arxiv.org/abs/2509.08753. Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.ne urips.cc/paper_files/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Pap er.pdf."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Speech Recognition Results Table 5 shows task-breakdown of short-form English speech recognition results for LibriSpeech Test Clean [Panayotov et al., 2015], LibriSpeech Test Other, GigaSpeech [Chen et al., 2021], VoxPopuli [Wang et al., 2021], SwitchBoard [Godfrey et al., 1992], CallHome, CHiME-4 [Vincent et al., 2017], SPGISpeech [ONeill et al., 2021], TED-LIUM [Hernandez et al., 2018] and Earnings-22 [Del Rio et al., 2022]. Table 5: English Short-Form WER (%) results. We report scores for LibriSpeech Test Clean (LS-C), LibriSpeech Test Other (LS-O), GigaSpeech (GS), VoxPopuli (VP), SwitchBoard (SB), CallHome (CH), CHiME-4 (C-4), SPGISPeech (SPGI), TED-LIUM (TED) and Earnings-22 (E22). 1 4 Model Offline Whisper Voxtral Mini Transcribe V2 Realtime API GPT-4o mini Transcribe Scribe v2 Realtime Realtime Open-Source DSM 1B En-Fr DSM 2.6B En Nemotron Streaming Voxtral Realtime Delay (ms) LS-C LS-O GS VP SB CH C-4 SPGI TED E22 AVG 500 2500 560 1120 240 480 960 2400 1.84 1.60 3.66 3.24 11.60 10. 9.58 6.81 13.14 11.54 14.58 12.74 10.88 10.42 3.15 1.74 3.83 3. 11.63 10.67 8.39 7.27 1.94 1.75 4.48 4.01 10.67 10.29 6.79 6. 11.15 11.54 15.54 11.87 11.08 11.59 2.87 2.04 4.07 2.63 10.69 11. 7.93 7.33 3.64 1.71 2.42 2.38 2.49 2.08 1.96 1.82 11.44 4.46 5.12 4. 7.15 5.54 4.59 4.03 12.09 10.39 11.87 11.84 12.10 11.05 10.51 10.60 11.51 6.51 7.09 7. 10.52 7.87 7.23 7.06 12.46 12.53 13.82 13.35 13.26 11.90 11.44 11.55 16.62 12.02 17.30 17. 14.66 13.59 13.34 13.44 28.84 16.75 18.64 17.37 18.07 15.00 13.17 12.18 4.63 1.95 2.64 2. 3.31 1.96 2.36 2.11 4.58 3.08 4.50 4.50 4.53 3.96 3.55 3.57 16.76 11.72 12.48 12. 13.39 11.71 11.24 10.80 12.26 8.11 9.59 9.41 9.95 8.47 7.94 7.72 For English long-form, we report Meanwhile [Radford et al., 2023] and the long-form version of TED-LIUM. We also take the one-hour long earnings calls from Earnings-21 [Del Rio et al., 2021] and Earnings-22 [Del Rio et al., 2022], and segment them into shorter, 10 minute variants. Tables 7 and 8 show the per-language breakdown of error-rate scores for the FLEURS and Mozilla Common Voice benchmarks respectively. Table 6: English Long-Form WER (%) results. We report scores for Meanwhile (MW), Earnings-21 (E21), Earnings-22 (E22), and TED-LIUM (TED). 1 5 Model Offline Whisper Voxtral Mini Transcribe V2 Realtime API GPT-4o mini Transcribe Scribe v2 Realtime Realtime Open-Source DSM 1B En-Fr DSM 2.6B En Nemotron Streaming Voxtral Realtime Delay (ms) MW E22 TED AVG 500 560 1120 240 480 960 2400 5.80 4.08 9.88 9.81 13.07 11.69 3.11 2. 7.97 7.11 5.21 3.62 9.92 10.72 12.58 13.22 4.17 2.18 7.97 7. 7.36 5.29 8.25 7.43 5.76 5.05 4.14 4.03 14.58 10.52 20.92 18.75 12.56 10.46 9.86 9. 21.43 12.18 23.53 21.65 14.84 12.46 11.63 11.31 11.92 2.89 4.46 4.25 4.00 2.94 2.86 2. 13.83 7.72 14.29 13.02 9.29 7.73 7.13 6.93 Table 7: FLEURS error-rate results by language. We report scores for Arabic (ar), German (de), English (en), Spanish (es), French (fr), Hindi (hi), Italian (it), Japanese (ja), Korean (ko), Dutch (nl), Portuguese (pt), Russian (ru), and Chinese (zh). For Chinese and Japanese we report character error-rate (CER). For all other languages, we report WER. 1 6 Model Offline Whisper Voxtral Mini Transcribe V2 Realtime API GPT-4o mini Transcribe Scribe v2 Realtime Realtime Open-Source DSM 1B En-Fr DSM 2.6B En Nemotron Streaming Voxtral Realtime Delay (ms) ar de en es fr hi it ja ko nl pt ru zh AVG 500 2500 560 1120 240 480 960 2400 15.44 13.54 5.46 3. 4.00 3.32 2.81 2.63 5.55 4.32 28.87 10.33 2.71 2.17 4.97 4. 14.30 12.29 5.87 4.78 3.90 3.56 5.13 4.75 7.94 7.30 8.23 5. 13.99 19.53 4.07 4.31 3.65 3.54 3.41 3.23 5.84 5.12 8.39 12. 2.82 2.33 9.89 10.92 19.46 11.90 6.00 6.72 5.04 3.75 5.30 7. 15.43 16.82 7.95 8.34 9.56 16.31 6.11 6.11 5.72 23.95 22.53 20.32 14.71 8.15 6.19 4.87 4.15 5.91 4.90 4.34 4.05 4.59 3.31 2.98 2.71 8.00 6.42 5.68 5.23 14.26 12.88 11.82 10.73 4.41 3.27 2.46 2. 15.17 9.59 6.80 5.50 17.56 15.74 14.90 14. 9.23 7.07 6.76 5.91 7.51 5.03 4.57 3. 7.87 6.02 5.56 5.41 13.84 10.45 8.99 8.48 10.80 8.72 7.70 6.73 Table 8: Mozilla Common Voice error-rate results by language. For Chinese and Japanese we report CER. For all other languages, we report WER. For fairness, we omit Arabic from the macro-average in Tables 3 and 4, since all models score in excess of 45%. 1 7 Model Offline Whisper Voxtral Mini Transcribe V2 Realtime API GPT-4o mini Transcribe Scribe v2 Realtime Realtime Open-Source DSM 1B En-Fr DSM 2.6B En Nemotron Streaming Voxtral Realtime Delay (ms) ar de en es fr hi it ja ko nl pt ru zh AVG 500 2500 560 1120 240 480 960 2400 50.58 46.06 6.25 4. 22.91 8.61 5.66 3.93 11.33 7.21 46.75 10.26 6.81 4.15 15.80 12. 20.86 20.29 5.83 4.38 7.17 6.58 6.76 5.18 14.88 9.04 14.25 8. 51.06 60.60 6.05 16.60 10.89 19.43 5.54 15.93 9.77 15.74 23.90 35. 5.75 14.04 18.53 24.70 32.90 26.98 7.89 9.06 9.70 19.77 8.49 13. 14.81 38.97 12.85 20.85 34.93 24.29 18.27 12.33 11.92 55.10 48.64 48.68 50.35 11.13 8.70 6.85 5.66 19.63 15.18 12.49 10.51 9.05 6.05 5.12 4.56 14.51 11.51 9.80 9.05 20.05 17.22 15.04 13.19 10.62 7.80 6.05 4.96 27.25 20.87 16.77 15.45 33.47 31.37 27.24 25.26 11.86 8.97 6.36 5.18 15.27 11.25 8.00 7. 13.90 11.03 8.65 7.64 43.93 32.92 21.51 17.00 19.22 15.24 11.99 10.47 A.2 vLLM Realtime Inference Figure 5: Voxtral streaming session via vLLM resumable requests. session is created with an anchor request that includes the initial buffered audio (e.g., the first τ ms plus padding tokens to enforce the target delay) and runs one-token decoder step. Each subsequent update is sent as resumable request that appends the next 80 ms audio chunk together with the previously emitted token ID, allowing the engine to reuse cached KV states and emit the next token incrementally. This requestdecodeupdate loop enables low-latency, continuous transcription with full-duplex streaming-input/streaming-output."
        }
    ],
    "affiliations": [
        "Mistral AI"
    ]
}