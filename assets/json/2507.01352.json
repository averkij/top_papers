{
    "paper_title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy",
    "authors": [
        "Chris Yuhao Liu",
        "Liang Zeng",
        "Yuzhen Xiao",
        "Jujie He",
        "Jiacai Liu",
        "Chaojie Wang",
        "Rui Yan",
        "Wei Shen",
        "Fuxiang Zhang",
        "Jiacheng Xu",
        "Yang Liu",
        "Yahui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B parameters, trained on a carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The Skywork-Reward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality."
        },
        {
            "title": "Start",
            "content": "2025-07-04 Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy Chris Yuhao Liu Liang Zeng Yuzhen Xiao Jujie He Jiacai Liu Chaojie Wang Rui Yan Wei Shen Fuxiang Zhang Jiacheng Xu Yang Liu Yahui Zhou 2050 Research, Skywork AI https://huggingface.co/Skywork https://modelscope.cn/organization/Skywork https://github.com/SkyworkAI/Skywork-Reward-V"
        },
        {
            "title": "Abstract",
            "content": "Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, suite of eight reward models ranging from 0.6B to 8B parameters, trained on carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The SkyworkReward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality. 5 2 0 2 3 ] . [ 2 2 5 3 1 0 . 7 0 5 2 : r Figure 1: benchmark performance comparison of the top three models among the eight in the SkyworkReward-V2 series, against previous state-of-the-art reward models across seven major benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Reward models (RMs) have become critical components in Reinforcement Learning from Human Feedback (RLHF) pipelines (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Dong et al., 2024a; Lambert, 2025; Schulman et al., 2017; Wang, 2025), now standard in Large Language Model (LLM) post-training (Tie et al., 2025). Recent advancements in LLM reasoning capabilities (Jaech et al., 2024; Guo et al., 2025; Xu et al., 2025; Chen et al., 2025a) and Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024a) have sparked interest in policy optimization via rule-based rewards (Luo et al., 2025c; Wen et al., 2025; Team, 2025b;a; Luo et al., 2025b; He et al., 2025b). These reward functions typically verify whether answers match ground truth for math problems or pass unit tests for coding tasks, and can include fine-grained rules for verifiable outputs (Bercovich et al., 2025; Ma et al., 2025). However, complex human preferences often cannot be captured through simple rules, limiting the effectiveness of rule-based approaches in advancing general preference learning. Thus, the challenge of modeling nuanced, sophisticated, and sometimes conflicting human preferences through effective reward models remains largely unresolved. To model human preferences, previous works have curated various datasets (Cui et al., 2023; Wang et al., 2024f; 2025c; Dong et al., 2024a; Xu et al., 2024; Park et al., 2024; Lambert et al., 2024a; OLMo et al., 2024; Teknium, 2023) with prompts drawn from diverse sources. These efforts employ automatic methods (Cui et al., 2023; Xu et al., 2024) or human annotators (Wang et al., 2024f; 2025c) to generate preference pairs, enabling preference learning in pairwise contrastive manner (Bradley & Terry, 1952; Ouyang et al., 2022). Beyond dataset construction, some works aim to improve reward modeling via inductive biases in enhanced loss functions (Liu et al., 2024b; Cai et al., 2024; Yang et al., 2024b; Wang et al., 2024f; Zhang et al., 2024) or modified model architectures (Wang et al., 2024a; Chen et al., 2025b; Dorka, 2024). To evaluate progress in reward modeling, RewardBench (Coste et al., 2023) was released as the first benchmark for RMs. As reward models evolve, scores on RewardBench have begun to saturate (Wang et al., 2024a; Park et al., 2024; Wang et al., 2024c; Liu et al., 2024b; Shiwen et al., 2024; Wang et al., 2024b;e), but multiple studies (Frick et al., 2024; Zhou et al., 2024; Song et al., 2025; Wen et al., 2024) have argued that such saturated scores are weak indicators of real progress. These studies highlight weak (or even inverse) correlations between RewardBench scores and downstream task performance (e.g., best-of-N or policy training). In this work, we focus exclusively on the singular goal of enhancing the data quality of existing preference data, to advance the development of open reward models. We introduce SynPref-40M, large-scale preference dataset comprising 40 million preference pairs. We design two-stage preference data curation pipeline (Figure 3) that (1) combines human verification under stringent protocol for quality assurance (Section 3.2), (2) and employs human-preference-guided LLM judges for scalability (Section 3.3). The pipeline also involves iterative training of reward model, which continuously incorporates feedback from human labels and retrieves preference data where the RM itself performs poorly, to enable further learning. Our pipeline yields 26 million carefully curated preference pairs, which we use to develop and train Skywork-Reward-V2, our second-generation reward model series, consisting of eight high-performing reward models, ranging from 0.6B to 8B parameters. Through comprehensive evaluations on seven major RM benchmarks (Lambert et al., 2024b; Frick et al., 2024; Zhou et al., 2024; Liu et al., 2024c; Tan et al., 2024; Malik et al., 2025), we demonstrate that the Skywork-Reward-V2 series, trained using only the Bradley-Terry objective (Bradley & Terry, 1952), achieves state-of-the-art performance, with our 8B reward model outperforming all existing open reward models across all seven benchmarks by significant margin (Figure 1 and table 1). We also demonstrate Skywork-Reward-V2s superior performance across multiple critical dimensions, including general human preferences, objective correctness, resistance to stylistic biases, safety, and best-of-N scaling (Section 4.2). Through data ablations, we show that the success of SynPref-40M is driven not only by its scale but also by its high quality (Section 4.3). Our method-wise ablations confirm the importance of human annotation, LLM annotation guided by human preferences, and our carefully designed and rigorously implemented annotation protocols (Section 4.4). We outline our main contributions as follows: We collect and curate SynPref-40M, which, to the best of our knowledge, is the largest curated preference mixture to date. We release Skywork-Reward-V2, series of eight state-of-the-art reward models ranging from 0.6B to 8B parameters, which achieve top rankings on seven major reward model benchmarks, demonstrating strong performance across diverse evaluation dimensions. We propose preference data curation pipeline that combines human verification for quality with LLM-as-a-Judge, guided by human preferences for scalability. Ablation studies demonstrate the importance of each component in the pipeline. 2 Figure 2: Left: Comparison of the performance of 31 top open reward models on RewardBench (Lambert et al., 2024b) and their average scores across seven newer benchmarks (Frick et al., 2024; Zhou et al., 2024; Liu et al., 2024c; Tan et al., 2024; Gureja et al., 2024). Right: Pearson correlation scores across seven reward model benchmarks."
        },
        {
            "title": "2 The brittleness of current open reward models",
            "content": "In this section, we begin with comprehensive assessment of existing open reward models. We then present the results and examine potential shortcomings of the status quo. Single-benchmark evaluation leads to potential over-optimization. RewardBench (Lambert et al., 2024b) is dataset for pairwise preference evaluation in chat, safety, and reasoning, and has become the standard benchmark for assessing reward models. However, several subsequent studies (Frick et al., 2024; Zhou et al., 2024; Wen et al., 2024) argue that scores on RewardBench (Li et al., 2024) do not directly correlate with downstream performance and, in some cases, exhibit an inverse relationship. This aligns with our own evaluation results in Figure 2, suggesting potential over-optimization. We advocate for benchmarks that either (1) involve more challenging evaluation methods (e.g., best-of-N) or (2) demonstrate stronger correlations with downstream performance. comprehensive evaluation suite exposes over-optimization. Based on the above criteria, in addition to RewardBench, we select several other benchmarks that span multiple evaluation dimensions. Specifically, we include PPE Preference and Correctness (Frick et al., 2024) to assess both real human preferences and unambiguous correctness; RMB (Zhou et al., 2024) for its challenging best-of-N evaluation; RMBench (Liu et al., 2024c) to evaluate robustness to content variation and style bias; and JudgeBench (Tan et al., 2024), which evaluates preference pairs drawn from difficult, real-world LLM evaluation datasets, such as LiveCodeBench (Jain et al., 2024). Finally, we include the newly released RewardBench v2 (Malik et al., 2025), which enforces global best-of-N evaluation and extremely difficult capability assessments (e.g., distinguishing highly similar responses and reward margin requirements). detailed description of these benchmarks is provided in Appendix B.1. We present the main results in Figure 2, comparing RewardBench scores with average scores across the seven newer benchmarks, and report Pearson correlations among all benchmarks. Our findings are as follows: The average score on newer benchmarks shows minimal improvement, even as RewardBench scores saturate. This suggests potential over-optimization to narrow set of preferences encoded by RewardBench, further supported by the weak correlations with other benchmarks shown in the right plot of Figure 2. Alternative loss functions or model modifications fail to yield consistent gains (Yang et al., 2024b; Dorka, 2024; Lou et al., 2024; Zhang et al., 2024; Liu et al., 2025), and in many cases degrade performance. This is evident in the left plot of Figure 2, where Skywork derivatives fine-tuned from the Skywork-Reward model or trained on the same data outperform the original Skywork-Reward models (Liu et al., 2024b) on this benchmark, but underperform them on others. Among the top 20 models on RewardBench, 16 directly or indirectly use the same base model (Liu et al., 2024b) or are fine-tuned on highly similar training data, indicating stagnant progress in both open preference datasets and reward models since September 2024. 3 Figure 3: two-stage preference data curation pipeline. Stage 1 (top) involves human-AI synergistic curation and runs iteratively. Stage 2 (bottom) scales data curation automatically using reward model consistency checks, eliminating the need for further human supervision."
        },
        {
            "title": "3 Scaling preference data curation via human-guided AI feedback",
            "content": "3.1 Pipeline overview In this section, we present two-stage preference data curation pipeline (Figure 3) that combines human verification for quality assurance with annotations from human-preference-guided LLM judges to achieve scalability. In Stage 1, human and LLM annotators label gold and silver preference data, respectively. Humans follow strict verification protocol, while LLMs use preference-aware annotation scheme conditioned on human preference labels. reward model is first trained on the silver data and evaluated against the gold data to identify its shortcomings. We then employ mechanism to select similar preference samples where the current reward model performs poorly, which are re-annotated to train the next iteration of the RM. This process is repeated over multiple iterations. In Stage 2, we combine the reward model from Stage 1 with gold reward model trained exclusively on verified human data to guide data selection through consistency-based mechanism. Since this stage requires no human supervision, it enables scaling to millions of preference data pairs. This is depicted as Stage 2 in the lower part of Figure 3."
        },
        {
            "title": "3.2 Stage 1: small-scale human-in-the-loop curation",
            "content": "Seed preference data initialization. We begin by collecting available preference data to form an unverified pool, Dun. For each pair in this pool, given the 3-tuple (x, yw, yl) comprising the conversation x, the chosen (winning) response yw, and the rejected (losing) response yl we collect LLM-generated preference attributes a. Each attribute set is 5-tuple consisting of: (1) task category, (2) preference objectivity, (3) controversiality, (4) desired attributes, and (5) annotation guideline. Task category, objectivity, and controversiality serve as metadata to ensure annotation diversity across scenarios. The desired attributes describe the qualities users seek in good responses, while the annotation guideline provides instance-specific, context-dependent criteria for determining the preference label. Human verification and annotation protocol. We initialize with small, high-quality, and diverse set of preference pairs1 as the seed data. Using the generated preference attributes, human annotators perform strict verification following predefined protocol (Appendix D.2). At high level, the protocol outlines core principles and practices, as well as specific guidelines tailored to each task category, objectivity type, and controversiality level. For example, it permits the use of external tools such as search 1In our case, we use the Skywork-Reward-Preference-80K-v0.2 dataset. 4 engines, frontier LLM assistants, and domain-specialized LLMs (e.g., for math or code) to aid in labeling. However, full reliance on LLMs for labeling is strictly prohibited. This rigorous process yields the seed dataset Dseed, where the human-verified portion is denoted as Dgold (for validation), and the LLM-verified portion as Dsilver (for training). We provide further annotation details and insights in Appendix D. Step 1: Reward model training and evaluation. We initialize pointwise Bradley-Terry reward model (Bradley & Terry, 1952; Ouyang et al., 2022) and train it on Dsilver. We select the best current reward model checkpoint θ based on validation accuracy on Dgold. For each (x, yw, yl), we collect its prediction = σ(rθ(x, yw) rθ(x, yl)). Step 2: Error-driven adaptive preference retrieval. Instead of relying solely on human-annotated data to increase data volume, we leverage LLM annotators via an adaptive retrieval mechanism (Ram et al., 2023) to collect representative samples aligned with human preferences. This mechanism selects new examples from the unverified pool based on both the preference attributes and the reward models predictions. For each pairwise instance, we compute the embedding (Sturua et al., 2024) of (x, a) and retrieve the top-k similar items. Intuitively, we prioritize preference data that resemble instances where the reward model errs or shows low confidence. We set the retrieval upper bound kmax = 8 and use dynamic rule to determine k: = (cid:26)kmax, kmax (1 p), if 0.5 if > 0.5 (incorrect prediction) (correct prediction) Step 3: Preference-aware labeling. Using the retrieved examples with human labels, we employ group of strong LLMs to aggregate final judgments using self-consistency (Wang et al., 2022). First, we perform intra-model aggregation via self-consistency, then merge results across models to mitigate potential bias from any single model. For all LLM annotations, responses are labeled as Candidate 1 and Candidate 2, with their order randomized in the prompt. While pointwise scoring (He et al., 2025a; Liu et al., 2025) has shown greater effectiveness, it is not applicable here due to our reliance on both human and LLM annotators, making it impractical to enforce shared standard. Finally, human-labeled samples are added to Dgold, and LLM-labeled samples to Dsilver. Throughout Stage 1, we iteratively perform Steps 1, 2, and 3. After each iteration, we use an internal human-labeled validation set for sanity checking. However, scores from this sanity check serve only as reference; pipeline execution does not depend on them. 3.3 Stage 2: large-scale automatic curation of preference data in-the-wild We now scale up to tens of millions of in-the-wild preference data pairs. However, annotating the entire dataset even automatically can be prohibitively costly and unnecessary. Below, we describe two consistency-based filtering strategies to determine which data points warrant further verification. Preference consistency with the best reward model. Inspired by Kim et al. (2024) and Liu et al. (2024b), we adopt filtering strategy that excludes all pairs with confidence greater than 0.5 (i.e., correct) under the current best reward model. For the remaining data, we apply the same adaptive preference retrieval and human-preference-guided LLM annotation from Section 3.2, without involving human verifiers. Preference consistency with the gold reward model. We train separate gold reward model using all cumulative human-verified samples to approximate the true human preference distribution. From the unverified pool, we retain only those pairs whose original chosen-rejected labels are consistent with (1) the gold reward model and (2) either the LLM judges or the current best reward model. Approximately 5 million preference pairs passed through this consistency mechanism without requiring attribute generation or additional labeling. To leverage the discarded pool, we also experiment with recycling the discarded data by simply flipping the chosen-rejected order, which incurs no additional annotation or computational overhead."
        },
        {
            "title": "4 Experimental results",
            "content": "In this section, we first present the main results of reward model performance in Section 4.2. We then conduct additional ablations on both data (Section 4.3) and method (Section 4.4) to demonstrate the effectiveness of our approach. 4.1 Reward model training We train all models in the Skywork-Reward-V2 series using the Llama 3.1 and 3.2 series (Grattafiori et al., 2024) and the Qwen3 (Yang et al., 2025) collection as backbones. We choose model backbones with no 5 Model RewardBench RewardBench v2 PPE Pref PPE Corr RMB RM-Bench JudgeBench Avg. Open Reward Models Llama-3-OffsetBias-RM-8B (Park et al., 2024) ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024a) Internlm2-20b-reward (Cai et al., 2024) Skywork-Reward-Llama-3.1-8B-v0.2 (Liu et al., 2024b) LDL-Reward-Gemma-2-27B-v0.1 Skywork-Reward-Gemma-2-27B-v0.2 (Liu et al., 2024b) Llama-3.1-Nemotron-70B (Wang et al., 2024f) INF-ORM-Llama3.1-70B (Yang et al., 2024b) 89.0 90.4 90.2 93.1 95.0 94.3 93.9 95.1 64.8 66.5 56.3 71.8 72.5 75.3 76.7 76.5 59.2 60.6 61.0 62.2 62.4 63.6 64.2 64.2 LLM-as-a-Judge & Generative Reward Models GPT-4o (Hurst et al., 2024) Claude-3.5-Sonnet (Anthropic, 2024) DeepSeek-GRM-27B (Liu et al., 2025) DeepSeek-GRM-27B (w/ MetaRM) (Liu et al., 2025) RM-R1-Qwen-Instruct-32B (Chen et al., 2025c) RM-R1-DeepSeek-Distill-Qwen-32B (Chen et al., 2025c) EvalPlanner (Llama-3.1-70B) (Saha et al., 2025) EvalPlanner (Llama-3.3-70B) (Saha et al., 2025) J1-Llama-8B (Whitehouse et al., 2025) J1-Llama-8B (Maj@32) (Whitehouse et al., 2025) J1-Llama-70B (Whitehouse et al., 2025) J1-Llama-70B (Maj@32) (Whitehouse et al., 2025) Skywork-Reward-V2-Qwen3-0.6B Skywork-Reward-V2-Qwen3-1.7B Skywork-Reward-V2-Qwen3-4B Skywork-Reward-V2-Qwen3-8B Skywork-Reward-V2-Llama-3.2-1B Skywork-Reward-V2-Llama-3.2-3B Skywork-Reward-V2-Llama-3.1-8B Skywork-Reward-V2-Llama-3.1-8B-40M 86.7 84.2 88.5 90.4 92.9 90.9 93.9 93.8 85.7 - 93.3 - 85.2 90.3 93.4 93.7 89.9 93.0 96.4 97.8 64.9 64.7 - - - - - - - - - - Our Reward Models 61.3 68.3 75.5 78.2 64.3 74.7 84.1 86.5 67.7 67.3 65.3 67.2 - - - - 60.3 60.6 66.3 67.0 65.3 67.6 69.5 70.6 66.6 69.1 77.3 79.8 64.1 60.6 63.0 62.5 63.9 61.9 63.2 64.4 - - 60.4 63.2 - - - - 59.2 61.9 72.9 73.7 68.3 70.5 74.7 75.1 67.4 72.1 83.4 87. 57.8 64.6 62.9 66.6 67.9 69.4 64.9 70.5 73.8 70.6 69.0 70.3 73.0 69.8 - - - - - - 74.5 78.1 80.6 81.2 76.7 80.5 86.4 89.3 71.3 69.3 68.3 72.1 71.1 70.0 72.2 73.8 - - - - 79.1 83.9 80.0 82.1 73.4 - 82.7 - 74.4 78.7 81.6 82.6 76.4 81.1 92.8 96. 63.5 59.7 64.3 62.9 64.2 66.5 65.8 70.2 59.8 64.8 - - - - 50.9 56.6 42.0 - 60.0 - 67.6 72.9 69.3 73.4 65.0 69.2 80.0 83.4 67.1 67.4 66.6 70.2 71.0 71.6 71.6 73.5 - - - - - - - - - - - - 70.9 75.2 77.8 79.3 72.3 77.1 85.7 88. Table 1: Reward model performance assessed on seven benchmarks. Bold numbers indicate the best performance among all models, while underlined numbers represent the second best. Entries marked with - indicate either (1) that the specific models score is not included in the original benchmark or (2) that the model is unreleased. complete evaluation is provided in Table 5. more than 8B parameters for both training and usability considerations. Specifically, from the Llama 3 series, we employ Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, and Llama-3.2-1B-Instruct. For Qwen3, we consider sizes of 0.6B, 1.7B, 4B, and 8B. It is evident from the first generation of Skywork-Reward and RewardBench v2 (Malik et al., 2025) that using larger model backbones, such as 70B, results in greater gains. However, we do not consider them for this generation due to the training cost (on 26 to 40 million preference pairs) and the ease of use in actual RLHF settings. All reward models are trained with maximum context length of 16K tokens, which encompasses the majority of the samples in our data mixture to avoid truncation. We conduct all early-stage experiments with varying learning rates based on model size, using linear decay schedule and small batch size of 256, for 1 epoch, following the hyperparameters in Lambert et al. (2024a). For all final model training runs, we adopt the hyperparameters from Wang et al. (2025a), with large global batch size of 10,240 and constant learning rate schedule. We observe no change in final performance; however, using large batch size significantly improves convergence time, saving approximately 35% of total training compute. We train all reward models in the Skywork-Reward-V2 series exclusively on the 26 million curated subset. As an experimental release, Skywork-Reward-V2-Llama-3.1-8B-40M is trained using 26 million selected preference pairs and an additional 14 million pairs with the chosen-rejected order flipped to achieve optimal performance. 4.2 comprehensive evaluation of the Skywork-Reward-V2 series Here, we present the main evaluation results and analysis based on seven reward model benchmarks: RewardBench (Lambert et al., 2024b), RewardBench v2 (Malik et al., 2025), PPE Preference (Frick et al., 2024), PPE Correctness (Frick et al., 2024), RMB (Zhou et al., 2024), RM-Bench (Liu et al., 2024c), and JudgeBench (Tan et al., 2024). General preferences. We report full benchmark results for the current top-performing reward models, LLM-as-a-Judges, and generative reward models in Table 1. Across all seven benchmarks, the SkyworkReward-V2 series of models outperform not only much larger ones (i.e., 70B) but also the emerging class of generative reward models (Liu et al., 2025; Chen et al., 2025c). We interpret this as strong evidence that SynPref-40M captures wide range of preferences, enabling more robust preference learning across multiple dimensions simultaneously. Meanwhile, Skywork-Reward-V2 highlights the importance of data quality relative to the strength of the base models. Even at scale of 1.7B parameters, reward model can outperform 70B model on all benchmarks except for RewardBench and RewardBench v2, effectively 6 Model GPT-4o Claude-3.5-Sonnet DeepSeek-R1 o1-preview o3-mini o3-mini (low) o3-mini (medium) o3-mini (high) Skywork-Reward-V2-Qwen3-0.6B Skywork-Reward-V2-Qwen3-1.7B Skywork-Reward-V2-Qwen3-4B Skywork-Reward-V2-Qwen3-8B Skywork-Reward-V2-Llama-3.2-1B Skywork-Reward-V2-Llama-3.2-3B Skywork-Reward-V2-Llama-3.1-8B Skywork-Reward-V2-Llama-3.1-8B-40M Knowledge Reasoning Math Coding Avg. 50.6 62.3 59.1 66.2 58.4 63.0 62.3 67.5 62.3 66.9 66.9 70.1 61.0 64.3 76.6 79.9 54.1 66.3 82.7 79.6 62.2 69.4 86.7 89.8 66.3 69.4 64.3 67.3 66.3 65.3 75.5 78.6 75.0 66.1 80.4 85.7 82.1 83.4 85.7 87.5 82.1 83.9 80.4 82.1 73.2 87.5 89.3 89. 59.5 64.3 92.9 85.7 78.6 83.3 92.9 100 59.5 71.4 66.7 73.8 59.5 59.5 78.6 85.7 59.8 64.8 78.8 79.3 70.3 74.8 81.9 86.2 67.6 72.9 69.5 73.4 65.0 69.2 80.0 83.4 Table 2: Performance comparison of our RMs with state-of-the-art LLM-as-a-Judges and reasoning models on JudgeBench (Tan et al., 2024). Model Helpfulness (BoN) Harmlessness (BoN) Avg. Skywork-Reward-Llama-3.1-8B-v0.2 Skywork-Reward-Gemma-2-27B-v0.2 DeepSeek-GRM-27B DeepSeek-GRM-27B + MetaRM RM-R1-DeepSeek-Distill-Qwen-32B RM-R1-Qwen-Instruct-32B Qwen2-72B-Instruct GPT-4o-2024-05-03 Skywork-Reward-V2-Qwen3-0.6B Skywork-Reward-V2-Qwen3-1.7B Skywork-Reward-V2-Qwen3-4B Skywork-Reward-V2-Qwen3-8B Skywork-Reward-V2-Llama-3.2-1B Skywork-Reward-V2-Llama-3.2-3B Skywork-Reward-V2-Llama-3.1-8B Skywork-Reward-V2-Llama-3.1-8B-40M 60.5 63.1 63.9 64.2 62.0 63.6 64.5 63.9 68.4 72.0 74.7 76.5 68.0 74.4 82.3 86.2 56.8 59.9 58.0 58.0 61.8 68.2 64.9 68. 69.1 72.2 75.1 75.8 73.2 76.2 82.8 86.6 58.7 61.5 61.0 61.1 61.9 65.9 64.7 66.1 68.8 72.1 74.9 76.2 70.6 75.3 82.6 86.4 Table 3: Performance comparison of reward models based on Best-of-N accuracy for Helpfulness and Harmlessness in RMB (Zhou et al., 2024). bridging the model size gap. Correctness preferences. For objective correctness evaluation, we primarily consider JudgeBench (Tan et al., 2024) and PPE Correctness (Frick et al., 2024). To effectively measure progress, we directly compare our reward models with leading LLMs and reasoning models that top the JudgeBench leaderboard  (Table 2)  . Note that JudgeBench uses weighted average score across all samples, whereas we compute the average score across the four categories to maintain consistency with all other benchmarks. While our reward models underperform state-of-the-art reasoning and coding models on average, they outperform all leading models on knowledge tasks by significant margin. Notably, Skywork-Reward-V2-Llama-3.23B achieves math performance equivalent to o3-mini (high), while Skywork-Reward-V2-Llama-3.1-8B outperforms o3-mini (high) in this category. For PPE Correctness, we compare our model against existing reward models using the Best-of-N evaluation (Figure 4) in the following paragraph. Figure 4: Best-of-N scaling curves of reward models across five tasks on PPE Correctness (Frick et al., 2024). Best-of-N accuracy and scaling. We evaluate our RMs on the BoN splits from RMB (Zhou et al., 2024) and PPE Correctness Preference (Frick et al., 2024). As shown in Table 3, our RMs demonstrate strong Best-ofN (BoN) capability in both helpfulness and harmlessness. All eight RMs outperform GPT-4o, the previous 7 Model Easy Normal Hard Avg. Skywork-Reward-Llama-3.1-8B-v0.2 Skywork-Reward-Gemma-2-27B-v0.2 ArmoRM-Llama3-8B-v0.1 Nemotron-340B-Reward LDL-Reward-Gemma-2-27B-v0.1 Llama-3-OffsetBias-RM-8B Internlm2-20b-reward Llama-3.1-Nemotron-70B INF-ORM-Llama3.1-70B 70.5 88.9 80.4 81.0 92.4 83.9 79.4 92.2 92. 90.3 Skywork-Reward-V2-Qwen3-0.6B 93.0 Skywork-Reward-V2-Qwen3-1.7B 92.1 Skywork-Reward-V2-Qwen3-4B 91.9 Skywork-Reward-V2-Qwen3-8B 91.3 Skywork-Reward-V2-Llama-3.2-1B 91.5 Skywork-Reward-V2-Llama-3.2-3B Skywork-Reward-V2-Llama-3.1-8B 97.0 Skywork-Reward-V2-Llama-3.1-8B-40M 97.6 74.2 71.9 71.5 71.4 75.2 73.2 74.2 76.5 80.0 78.0 83.4 84.7 85.7 79.9 84.1 95.0 96.9 49.3 42.1 55.8 56.1 45.5 56.9 62.8 47.8 54.0 54.8 59.7 67.9 70.1 57.8 67.8 86.5 93.5 64.7 67.6 69.2 69.5 71.0 71.3 72.1 72.2 75. 74.4 78.7 81.6 82.6 76.3 81.1 92.8 96.0 Figure 5: Performance comparison of fine-grained difficulty-level scores on RM-Bench (Liu et al., 2024c). Model Factuality Precise IF Math Safety Focus Ties Avg. Skywork-Reward-Llama-3.1-8B URM-LLama-3.1-8B Skywork-Reward-Gemma-2-27B-v0.2 claude-3-7-sonnet-20250219 Skywork-Reward-Gemma-2-27B llama-3.1-70B-Instruct-RM-RB2 INF-ORM-Llama3.1-70B claude-opus-4-20250514 QRM-Gemma-2-27B gemini-2.5-flash-preview-04-17 LMUnit-llama3.1-70b LMUnit-qwen2.5-72b Skywork-Reward-V2-Qwen3-0.6B Skywork-Reward-V2-Qwen3-1.7B Skywork-Reward-V2-Qwen3-4B Skywork-Reward-V2-Qwen3-8B Skywork-Reward-V2-Llama-3.2-1B Skywork-Reward-V2-Llama-3.2-3B Skywork-Reward-V2-Llama-3.1-8B Skywork-Reward-V2-Llama-3.1-8B-40M 69.9 68.8 76.7 73.3 73.7 81.3 74.1 82.7 78.5 65.7 84.6 87.2 58.2 65.8 77.3 79.8 60.9 76.2 84.6 87.9 42.5 45.0 37.5 54.4 40.3 41.9 41.9 41.9 37.2 55.3 48.8 54.4 40.0 45.0 46.2 49.1 45.6 45.6 66.2 67.8 62.8 63.9 67.2 75.0 70.5 69.9 69.9 74.9 69.9 81.1 71.6 72. 71.6 72.7 73.2 77.0 59.6 69.4 77.6 83.1 93.3 91.8 96.9 90.3 94.2 88.4 96.4 89.5 95.8 90.9 90.7 91.3 84.4 89.1 92.2 94.0 87.3 93.1 96.7 97.3 96.2 97.6 91.7 92.1 93.2 86.5 90.3 86.2 95.4 86.7 97.0 96.8 79.4 88.5 96.6 96.4 89.3 96.0 98.4 99.2 74.1 76.5 81.8 67.2 82.6 88.3 86.2 83.7 83.2 83.4 90.6 90. 34.0 48.7 67.4 72.9 43.1 67.7 81.2 83.9 73.1 73.9 75.3 75.4 75.8 76.1 76.5 76.5 76.7 77.2 80.5 82.1 61.3 68.3 75.5 78.2 64.3 74.7 84.1 86.5 Figure 6: Performance comparison of our RMs with the top 12 RMs on RewardBench v2 (Malik et al., 2025). state-of-the-art, by margin of up to 20 points. We further present BoN curves for five challenging tasks in PPE Correctness in Figure 4. Skywork-Reward-V2-Llama-3.1-8B and Skywork-Reward-V2-Llama-3.18B-40M show superior scaling, outperforming all other models evaluated. Among all BoN scaling curves, all Skywork-Reward-V2 variants exhibit positive scaling (i.e., performance continues to improve as increases), except for our 1.7B variant in GPQA. We further confirm the strengths of Skywork-Reward-V2 in BoN capability on RewardBench v2 (Li et al., 2024) (Figure 6), which requires precise best-of-N selection globally across the dataset. Resistance against style biases. Using RM-Bench (Liu et al., 2024c), we assess the ability of reward models to judge substance under varying stylistic differences between chosen and rejected responses. As shown in Figure 5, most baseline models exhibit significant performance gaps across the three stylistic conditions, indicating high sensitivity to such biases. This is particularly evident for INF-ORM-Llama3.170B, with gap of 36 points between Normal and Hard accuracy. In contrast, Skywork-Reward-V2 models outperform all baselines not only in absolute scores across all three categories but also in maintaining much smaller performance differences. We also observe rapidly shrinking gap as model size increases. These results suggest that training on SynPref-40M leads to more debiased representations of preferences. Superiority in advanced capabilities. On RewardBench v2, Skywork-Reward-V2 further demonstrates superior capability in precise instruction following, including assessing whether models response adheres to specific instructions in the prompt. Notably, all existing reward models score below 50 in this category. In contrast, Skywork-Reward-V2-Llama-3.1-8B and Skywork-Reward-V2-Llama-3.1-8B40M outperform strong proprietary models like Claude-3.7-Sonnet and Gemini-2.5-Flash-Preview-04-17, and generative reward models that utilize rubrics (Saad-Falcon et al., 2024), through learning pure representation of preferences. We also observe significant increase in the Factuality score, likely due to the volume of SynPref-40M and the richness of the information it contains. 8 Figure 7: Left: Reward model score progress throughout the entire curation pipeline, including three data ablations: original data, filtered data, and filtered data with corrected preference pairs, based on an early version of SynPref-40M with only 16 million preference pairs. Right: The average score of the final training run of Llama-3.1-8B-BTRM, preliminary version of Skywork-Reward-V2-Llama-3.1-8B. The Avg. Score indicates the averaged RM score across RewardBench, PPE Preference, PPE Correct, RMB, RM-Bench, and JudgeBench. 4.3 Ablation studies on data quantity and quality We further examine the effect of data quantity and quality through performance trends across our pipeline, based on an early version of SynPref-40M with only 16 million preference pairs. Preference data scaling does not hold for uncurated data. (quality and quantity) In the left plot of Figure 7, we show that increasing the amount of uncurated data results in minimal performance gains. During Stage 2, training on an additional 12 million preference pairs fails to surpass the performance of the initial seed model. In contrast, with curated data, we observe consistent performance improvements as more data is added, with the most significant gains occurring in Stage 2 where the largest volume of curated data is introduced. Notably, this result partially aligns with findings in concurrent work (Wang et al., 2025a), which specifically demonstrates that subjective preference learning does not exhibit scaling behavior, whereas objective preferences do. Data curation enables preference correction. (quality) We further demonstrate that our data curation process not only selects high-quality data for training but also identifies low-quality or incorrect preferences, which are placed in discarded pool during training. By recycling this discarded data simply flipping the chosen and rejected responses we achieve consistent performance gains across all stages and iterations, as illustrated by the orange curve in Figure 7. As result, Skywork-RewardV2-Llama-3.1-8B-40M benefits from the inclusion of preference data even with flipped chosen-rejected responses. Training on 1.8% of 16M mixture outperforms previous SOTA open RM (70B) at the 8B scale. (quality and quantity) In the right plot of Figure 7, we report the average RM score across six benchmarks (excluding RewardBench v2 (Malik et al., 2025), which had not been released at the time) during training. Using only 1.8% (roughly 290K samples) of the full training set surpasses the previous state-of-the-art. This underscores that our data mixture excels not only in scale but also in quality. 4.4 Ablation studies on annotation method In this section, we conduct method-wise ablation studies to examine the importance of key components in our data curation pipeline. Although it is not feasible to perform ablations across the entire pipeline due to the long annotation interval and its recursive nature, we focus exclusively on iteration 1 of Stage 1. This setting allows for both rapid and highly controlled experiments, demonstrating how curated data contributes to reward model improvement within single curation cycle. 4.4.1 Pipeline-level ablations Setup. We begin with the filtered seed dataset and examine five settings: (1) direct training on unverified data (i.e., no curation), (2) simple LLM curation only, (3) both human and LLM curation, and (4) incorporating adaptively retrieved examples into LLM curation. These components collectively represent one iteration of Stage 1 in Figure 3. Finding 1: Simple LLM curation barely improves RM quality. As shown in Figure 8, simple LLM 9 curation increases the final RM score by only 0.1 potentially within the error margin of optimization randomness. Given that much in-the-wild preference data is synthetically labeled (Cui et al., 2023; Dong et al., 2024a; Lambert et al., 2024a) by LLMs, this result aligns with our findings in Figure 7, where scaling uncurated preference data yields negligible gains. potential factor may be the limited capabilities or annotation quality of the LLM judges used in our study (Ye et al., 2024; Chen et al., 2024). Finding 2: Human curation is crucial to data quality. From Figure 8, we observe that the largest improvement comes from human curation, with relative gain of 2.3 points over the seed RM baseline. This highlights the need for scalable methods of collecting human preference data and showcases the strength of our approach, which requires only modest amount of human annotation. In Section 4.4.2, we further analyze which component of the human curation pipeline contributes most significantly. Finding 3: Adaptive retrieval boosts LLM curation quality. Given access to human-curated gold data, adding similar gold examples to the LLM annotation prompt improves RM quality. This technique results in 0.9-point gain compared to raw LLM annotation in the human curation variant. While the improvement is smaller than with direct human curation, this method is simple, scalable, and incurs minimal overhead, making it an attractive tool for enhancing LLM annotation. Figure 8: Ablations over different curation variants. 4.4.2 Human annotation ablations Setup. We now focus specifically on the most impactful component: human curation. We evaluate three variants: (1) raw human curation, where annotators are shown only the conversation history and two responses, (2) human curation with LLM-generated preference attributes, and (3) human curation following our full annotation protocol (i.e., with external tools such as search engines and frontier LLMs). To control for memorization, the same annotators label three distinct subsets of preference data sampled with similar distributions over task category, objectivity, and controversiality. Before running the ablation, we train reward models on each of the three subsets and confirm they yield similar final performancewithin maximum of 0.6 points difference. This reduces the influence of intrinsic data quality as confounding factor, ensuring controlled experiments. All other components remain unchanged from our final method. As in Section 4.4.1, we begin from an RM trained on the filtered seed data. Human annotation with additional information and tools boosts annotation quality. As shown in Table 4, all forms of human curation improve the quality of the seed RM. Raw annotation based solely on the conversation and two responses results in 0.4-point gain. Adding preference attributes (task category, objectivity, controversiality, desired attributes, and annotation guidelines) yields larger 1.1-point gain. Incorporating our full annotation protocol including access to external tools leads to the best final performance, with 3.2-point improvement, validating the effectiveness of our human curation process. Table 4: RM scores on three human annotation setups. w/ Raw Curation w/ Pref Attributes w/ Verification Protocol 71.0 71.4 (+0.4) 72.1 (+1.1) 74.2 (+3.2) Avg. Score Seed RM Method"
        },
        {
            "title": "5 Related work",
            "content": "Preference data annotation. Traditional preference data annotation relies heavily on human annotators (Liu et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a; Hurst et al., 2024; Touvron et al., 2023a;b), which is both costly and inefficient and sometimes even noisy (Daniels-Koch & Freedman, 2022). To improve scalability, recent work now collectively referred to as RLAIF (Bai et al., 2022b) has proposed various forms of automatic annotation using strong LLMs (Bai et al., 2022b; Lee et al., 2023; Burns et al., 2023; Cui et al., 2023; Guo et al., 2024; Yuan et al.; Prasad et al., 2024; Pace et al., 2024; Lambert et al., 2024a; He et al., 2025a), in some cases even outperforming human annotators (Gilardi et al., 2023; Ding et al., 2022). Our approach combines the strengths of both paradigms: we enhance human annotation using external tools and frontier LLMs, while also guiding LLM-based annotation with human-verified labels. Among related work, the most relevant are Kim et al. (2025) and He et al. (2024). Kim et al. (2025) leverages small set of human-labeled seed data to iteratively refine an LLM policy via self-improvement (Rafailov et al., 2023); in contrast, we iteratively incorporate gold human preference labels to augment LLM annotation within structured data curation framework. 10 He et al. (2024) employs an iterative process that pseudo-labels unlabeled preference pairs and retains only high-confidence examples, without human annotators. Our work bridges the gap between human and LLM-based annotation by integrating them into principled and scalable framework, enabling high-quality preference data at scale. In addition, our approach to human verification via preference attributes is similar to LMUnit (Saad-Falcon et al., 2024), which decomposes requirements based on context and conducts automatic unit tests on assistant responses using LLMs. The paradigm of reward models. The reward model paradigm has evolved rapidly. Initially based on the Bradley-Terry (BT) model (Bradley & Terry, 1952; Liu et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a; Wang, 2025), early reward models were trained to maximize the score difference between pairwise responses. During inference, these models produce scalar score indicating the relative quality of response compared to alternatives given the same prompt. Later, RewardBench (Lambert et al., 2024b) introduced the first taxonomy of reward models, categorizing them into (1) sequence classifiers, (2) direct preference optimization (DPO) models with implicit rewards, (3) generative models, and (4) custom classifiers. Most BT-based models fall under the sequence classifier category, while generative models primarily include LLM-as-a-Judge approaches. DPO models, by contrast, rely on implicit rewards derived from the DPO objective (Rafailov et al., 2023). This taxonomy was further elaborated in Liu et al. (2024b) and has since been adopted by subsequent works (Zhong et al., 2025; Zang et al., 2025; Wang et al., 2025b). With the emergence of generative reward models (Liu et al., 2025; Chen et al., 2025c; Saha et al., 2025; Guo et al., 2025), Liu et al. (2025) proposed new categorization based on the form of reward generation and scoring patterns, highlighting differences in input flexibility and inference-time scalability. The reward generation forms include scalar, semi-scalar, and generative outputs, while scoring patterns are categorized as pointwise or pairwise. Beyond these major paradigms, Sun et al. (2024) introduces an alternative approach that trains reward models using an order consistency objective. This reframes reward modeling as binary classification task and has been shown to outperform the Bradley-Terry model in the presence of annotation noise. Strong open reward models and preference datasets. At the time of writing, there are already 166 reward models on the RewardBench v1 leaderboard (Lambert et al., 2024b), most of which are open-weight. The top-ranking models are primarily from the Skywork-Reward series (Liu et al., 2024b) and their derivatives, trained using either the same base models (Dorka, 2024; Lou et al., 2024) or datasets (Yang et al., 2024b; Shiwen et al., 2024; Lou et al., 2024; Zhang et al., 2024; Yang et al., 2024c). Their training data primarily consist of unfiltered human preferences and automatically curated synthetic data (Liu et al., 2024b). Another line of high-performing reward models includes FsfairX and ArmoRM (Dong et al., 2024b; 2023; Wang et al., 2024a), trained on Preference 700K (Dong et al., 2024b), dataset composed of preference data aggregated from eight diverse sources. The ArmoRM variant extends FsfairX with multi-dimensional reward head, enabling it to generate reward signals for fine-grained aspects of response quality. The InternLM2-Reward series (Cai et al., 2024) also presents strong models across different sizes, trained on large-scale collection of 2.4 million closed-source preference pairs, with focus on both English and Chinese data. Recently, the release of RewardBench v2 (Malik et al., 2025) introduced set of seven reward models trained on various Llama-3.1 checkpoints (i.e., different sizes and base models). Among these, the 70B variant is one of the top-performing models on the benchmark. Right before our release, we noticed two generative reward models from the LMUnit series (Saad-Falcon et al., 2024) that topped the RewardBench v2 leaderboard. These models use rubrics as unit tests, which are much more robust than reward models based on discriminative classifiers. Their strength is further reflected by their hgigh scores in Factuality and Ties categories. Our reward models leverage both the Skywork-Reward dataset and Preference 700K in the Seed and Stage 1 phases, respectively forming the foundation for improvements in later stages."
        },
        {
            "title": "6 Conclusion and Future Outlook",
            "content": "In this work, we introduce SynPref-40M, preference data mixture comprising 40 million preference pairs (26 million curated), and Skywork-Reward-V2, series of eight state-of-the-art reward models designed for versatility across wide range of tasks. SynPref-40M is constructed through two-stage curation pipeline that synergistically combines human supervision for quality with human-guided LLM judges for scalability. Built on this preference data mixture, we present the Skywork-RewardV2 series collection of eight strong reward models ranging from 0.6B to 8B parameters. Across seven major reward model benchmarks, models in the Skywork-Reward-V2 series achieve state-of-theart performance, demonstrating strong capabilities in capturing general human preferences, objective correctness, resistance to style biases, safety, and best-of-N scaling. Our small 1.7B variant surpasses the best existing 70B reward model on average, while our 8B variant ranks first on all seven benchmarks among all open reward models. We also conduct extensive ablation studies on both the data and the curation method to validate the effectiveness of our approach. We believe this work advances open reward models and, more broadly, RLHF research, representing significant step forward that will 11 accelerate open progress in the field. To this end, we have concluded our efforts in scaling preference data. While additional data can still be incorporated for further improvements, we hope to shift our focus in the future to other underexplored areas, such as alternative training techniques and modeling objectives. Meanwhile, given recent developments in the field, it is clear that reward models and reward shaping have become the most if not the only critical components of todays LLM training pipeline. This applies to both RLHF with general preference learning and behavior steering, and RLVR with tasks involving math, coding, or general reasoning, as well as agent-based learning. Looking ahead, we envision future in which reward models or more broadly, unified reward systems form the foundation of AI infrastructure, not merely as evaluators of behavior or correctness, but as the very compass by which intelligent systems navigate complexity, align with human values, and continually evolve toward ever more meaningful objectives."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude 3.5 sonnet model card addendum. https://www.anthropic.com/news/claude-3-5 -sonnet, 2024. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, Eric Chung, and Chris Alexiuk. Llama-nemotron: Efficient reasoning models, 2025. URL https://arxiv.org/abs/2505.00949. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report, 2024. Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or llms as the judge? study on judgement biases. arXiv preprint arXiv:2402.10669, 2024. 13 Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025a. Shikai Chen, Jin Yuan, Yang Zhang, Zhongchao Shi, Jianping Fan, Xin Geng, and Yong Rui. Ldl-rewardgemma-2-27b-v0.1. https://huggingface.co/ShikaiChen/LDL-Reward-Gemma-2-27B-v0.1, 2025b. Label Distribution Learning for Reward Modeling. Tech report forthcoming. Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025c. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai feedback. arXiv preprint arXiv:2310.01377, 2023. Oliver Daniels-Koch and Rachel Freedman. The expertise problem: Learning from specialized feedback. arXiv preprint arXiv:2211.06519, 2022. Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Shafiq Joty, Boyang Li, and Lidong Bing. Is gpt-3 good data annotator? arXiv preprint arXiv:2212.10450, 2022. Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024a. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. Transactions on Machine Learning Research, 2024b. URL https://arxiv.org/abs/2405.07863. Nicolai Dorka. Quantile regression for distributional reward models in rlhf. arXiv preprint arXiv:2409.10164, 2024. Evan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. How to evaluate reward models for rlhf. arXiv preprint arXiv:2410.14872, 2024. Fabrizio Gilardi, Meysam Alizadeh, and Mael Kubli. Chatgpt outperforms crowd workers for textannotation tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024. Srishti Gureja, Lester James Miranda, Shayekh Bin Islam, Rishabh Maheshwary, Drishti Sharma, Gusti Winata, Nathan Lambert, Sebastian Ruder, Sara Hooker, and Marzieh Fadaee. M-rewardbench: Evaluating reward models in multilingual settings. arXiv preprint arXiv:2410.15522, 2024. Bingxiang He, Wenbin Zhang, Jiaxi Song, Cheng Qian, Zixuan Fu, Bowen Sun, Ning Ding, Haiwen Hong, Longtao Huang, Hui Xue, et al. Air: systematic analysis of annotations, instructions, and response pairs in preference dataset. arXiv preprint arXiv:2504.03612, 2025a. 14 Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025b. Yifei He, Haoxiang Wang, Ziyan Jiang, Alexandros Papangelis, and Han Zhao. Semi-supervised reward modeling via iterative self-training. arXiv preprint arXiv:2409.06903, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Dongyoung Kim, Kimin Lee, Jinwoo Shin, and Jaehyung Kim. Spread preference annotation: Direct preference judgment for efficient llm alignment. In The Thirteenth International Conference on Learning Representations, 2025. Joongwon Kim, Anirudh Goyal, Aston Zhang, Bo Xiong, Rui Hou, Melanie Kambadur, Dhruv Mahajan, Hannaneh Hajishirzi, and Liang Tan. systematic examination of preference learning through the lens of instruction-following. arXiv preprint arXiv:2412.15282, 2024. Nathan Lambert. Reinforcement learning from human feedback. arXiv preprint arXiv:2504.12501, 2025. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024a. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024b. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. Vlrewardbench: challenging benchmark for vision-language generative reward models. arXiv preprint arXiv:2411.17451, 2024. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024b. Fei Liu et al. Learning to summarize from human feedback. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 583592, 2020. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Rm-bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184, 2024c. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inferencetime scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025. 15 Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie, and Junge Zhang. Uncertainty-aware reward model: Teaching reward models to know what is unknown. arXiv preprint arXiv:2410.00847, 2024. Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, et al. Large language model agent: survey on methodology, applications and challenges. arXiv preprint arXiv:2503.21460, 2025a. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site/DeepCod er-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51, 2025b. Notion Blog. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/DeepScaleR-Surpassin g-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025c. Notion Blog. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652, 2025. Saumya Malik, Valentina Pyatkin, Sander Land, Jacob Morrison, Noah Smith, Hannaneh Hajishirzi, and Nathan Lambert. Rewardbench 2: Advancing reward model evaluation. arXiv preprint arXiv:2506.01937, 2025. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. OpenAI. OpenAI o3 and o4-mini System Card, April 2025. URL https://cdn.openai.com/pdf/2221c 875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Alizee Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. West-of-n: Synthetic preference generation for improved reward modeling. arXiv preprint arXiv:2401.12086, 2024. Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators, 2024. Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, and Jane Yu. Self-consistency preference optimization. arXiv preprint arXiv:2411.04109, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:13161331, 2023. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 35053506, 2020. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. 16 Paul ottger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263, 2023. Jon Saad-Falcon, Rajan Vivek, William Berrios, Nandita Shankar Naik, Matija Franklin, Bertie Vidgen, Amanpreet Singh, Douwe Kiela, and Shikib Mehri. Lmunit: Fine-grained evaluation with natural language unit tests. arXiv preprint arXiv:2412.13091, 2024. Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, and Tianlu Wang. Learning to plan & reason for evaluation with thinking-llm-as-a-judge. arXiv preprint arXiv:2501.18099, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Tu Shiwen, Zhao Liang, Chris Yuhao Liu, Liang Zeng, and Yang Liu. Skywork critic model series, 2024. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael unther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Andreas Koukounas, Nan Wang, and Han Xiao. jina-embeddings-v3: Multilingual embeddings with task lora, 2024. URL https://arxiv.org/abs/24 09.10173. Hao Sun, Yunyi Shen, and Jean-Francois Ton. Rethinking bradley-terry models in preference-based reward modeling: Foundations, theory, and alternatives. arXiv preprint arXiv:2411.04991, 2024. Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. Judgebench: benchmark for evaluating llm-based judges. arXiv preprint arXiv:2410.12784, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025a. URL https: //qwenlm.github.io/blog/qwq-32b/. TinyR1 Team. Superdistillation achieves near-r1 performance with just 5parameters., 2025b. URL https://huggingface.co/qihoo360/TinyR1-32B-Preview. Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5. Guiyao Tie, Zeli Zhao, Dingjie Song, Fuyang Wei, Rong Zhou, Yurou Dai, Wen Yin, Zhejian Yang, Jiangyue Yan, Yao Su, et al. survey on post-training of large language models. arXiv preprint arXiv:2503.06072, 2025. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Binghai Wang, Runji Lin, Keming Lu, Le Yu, Zhenru Zhang, Fei Huang, Chujie Zheng, Kai Dang, Yang Fan, Xingzhang Ren, et al. Worldpm: Scaling human preference modeling. arXiv preprint arXiv:2505.10527, 2025a. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024a. Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, and Shafiq Joty. Direct judgement preference optimization. 2024b. 17 Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024c. Xiaokun Wang, Chris, Jiangbo Pei, Wei Shen, Yi Peng, Yunzhuo Hao, Weijie Qiu, Ai Jian, Tianyidan Xie, Xuchen Song, Yang Liu, and Yahui Zhou. Skywork-vl reward: An effective reward model for multimodal understanding and reasoning, 2025b. URL https://arxiv.org/abs/2505.07263. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024d. Zhilin Wang. Reward model evaluation in june 2025. Jun 2025. URL https://zhilin123.github.io/bl og/2025/reward/. Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. Helpsteer2-preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257, 2024e. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024f. Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, and Oleksii Kuchaiev. Dedicated feedback and edit models empower inference-time scaling for open-ended general-domain tasks. arXiv preprint arXiv:2503.04378, 2025c. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. Xueru Wen, Jie Lou, Yaojie Lu, Hongyu Lin, Xing Yu, Xinyu Lu, Ben He, Xianpei Han, Debing Zhang, and Le Sun. Rethinking reward model evaluation: Are we barking up the wrong tree? arXiv preprint arXiv:2410.05584, 2024. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 2024. Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, and Swarnadeep Saha. J1: Incentivizing thinking in llm-as-a-judge via reinforcement learning. arXiv preprint arXiv:2505.10320, 2025. xAI. Grok 2 beta release. https://x.ai/blog/grok-2, 2024. xAI. Grok 3 beta the age of reasoning agents. https://x.ai/news/grok-3, 2025. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Minghao Yang, Chao Qu, and Xiaoyu Tan. Inf-orm-llama3.1-70b, 2024b. URL [https://huggingface. co/infly/INF-ORM-Llama3.1-70B](https://huggingface.co/infly/INF-ORM-Llama3.1-70B). 18 Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. Regularizing hidden states enables learning generalizable reward model for llms. In Advances in Neural Information Processing Systems, 2024c. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al. Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint arXiv:2410.02736, 2024. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees, 2024. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models, 2024. URL https://arxiv. org/abs/2401.10020. Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. Internlm-xcomposer2. 5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025. Yifan Zhang, Ge Zhang, Yue Wu, Kangping Xu, and Quanquan Gu. General preference modeling with preference representations for aligning language models. arXiv preprint arXiv:2410.02197, 2024. Jialun Zhong, Wei Shen, Yanzeng Li, Songyang Gao, Hua Lu, Yicheng Chen, Yang Zhang, Wei Zhou, Jinjie Gu, and Lei Zou. comprehensive survey of reward models: Taxonomy, applications, challenges, and future. arXiv preprint arXiv:2504.12328, 2025. Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, et al. Rmb: Comprehensively benchmarking reward models in llm alignment. arXiv preprint arXiv:2410.09893, 2024. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November 2023. URL https://starling.cs.berkeley.edu/."
        },
        {
            "title": "A Limitations",
            "content": "Human preferences are inherently diverse and often conflicting, especially for prompts without single correct answer. Even when ground-truth answers exist, individuals may differ in their preferences based on factors such as writing style, tone, level of detail, or the relative weighting of helpfulness versus harmlessness. single reward model may not fully capture this complexity and may inherently favor certain response types over others. Future work could explore personalized reward models or context-dependent training paradigms to better reflect the multifaceted nature of human preference. Our observation regarding performance improvements from re-annotated discarded data is purely empirical. Due to budget constraints, we did not conduct further verification to rigorously assess this pool. As result, the re-annotated data may include noisy preferences or judgments that are not broadly representative or that fall outside the scope of current evaluation benchmarks. thorough investigation of this flipped pool is left for future work. Meanwhile, we would like to clarify that not all discarded preference pairs are incorrect or useless. Since our pipeline still uses LLMs and trained reward models to filter data, which is not fully interpretable, biases and modeling errors are inherently unavoidable. Studying why and how examples are removed during the process, as well as their actual usefulness for reward modeling and RLHF, could be valuable research direction. Our annotation protocol differs in implementation from most existing approaches, where human annotators provide their own preferences. In contrast, our protocol is more constrained: it instructs annotators to follow predefined desired attributes and annotation guidelines for each sample. While this structured approach promotes consistency, it also reduces flexibility and may not fully capture minority preferences. This limitation arises because, for certain subjective preferences, it is often infeasible to determine which response is better-even on relative scale. Finally, the success of our approach relies heavily on human annotation; we did not observe satisfactory results from fully automatic curation alone. This raises the question of whether current-generation LLMs are capable of supporting high-quality, fully automatic data labeling. Due to inference costs and API limitations, we were unable to scale automatic curation to the latest frontier models with strong reasoning capabilities. We consider this promising direction for future exploration, particularly given the central role these LLMs already play in supporting human annotation within our pipeline."
        },
        {
            "title": "B Reward model benchmarks and evaluation results",
            "content": "B.1 Reward model benchmarks RewardBench. RewardBench (Lambert et al., 2024b) is the first benchmark released for evaluating reward models. It includes 2,985 evaluation samples from 23 data sources, categorized into four main groups: chat, chat-hard, safety, and reasoning. The evaluation uses pairwise comparison accuracy, where reward model generates scores for both the chosen and rejected responses. prediction is correct if the score for the chosen response exceeds that of the rejected one. Final accuracy is computed as weighted average within each category and then averaged across categories. noted limitation is that the chosen-rejected pairs are constructed using semi-automatic methods and manually validated, though the authors do not detail the validation process. They also acknowledge potential spurious correlations in the reasoning subsets and the absence of correlation analysis between RewardBench scores and downstream performance. PPE Preference and Correctness. PPE (Frick et al., 2024) includes two datasets for evaluating reward models: PPE Preference and PPE Correctness. PPE Preference consists of 16K human-labeled preference pairs from Chatbot Arena, targeting real human preferences. PPE Correctness is derived from challenging benchmarks with ground-truth answers, allowing direct verification of preference pairs. Included benchmarks are MATH (Hendrycks et al., 2021), MBPP (Austin et al., 2021), MMLU-Pro (Wang et al., 2024d), IFEVAL (Zhou et al., 2023), and GPQA (Rein et al., 2024). Each prompt yields 32 LLM responses, enabling both pairwise and best-of-N evaluations. The authors demonstrate strong correlation between PPE scores and downstream RLHF performance, making it reliable benchmark for real-world reward model evaluation. RMB. RMB (Zhou et al., 2024) is comprehensive benchmark covering 49 real-world task categories under both helpfulness and harmlessness. Like PPE Correctness, it supports pairwise and best-of-N evaluations. Preference pairs are generated synthetically, with GPT-4 providing pointwise ratings based on query-specific principles. Human verification is used to ensure dataset quality. RMB shows strong positive correlation with downstream performance across several benchmarks. 20 RM-Bench. Unlike other benchmarks that focus on general preference evaluation, RM-Bench (Liu et al., 2024c) specifically tests reward models ability to discern nuanced response differences and resist style biases. It includes four categories: Chat, Math, Code, and Safety. Prompts are sourced from benchmarks such as AlpacaEval (Li et al., 2023), HumanEval (Muennighoff et al., 2023), MATH (Hendrycks et al., 2021), and XSTest (R ottger et al., 2023). Response pairs are minimally different (e.g., word-level changes introducing factual errors) and generated with controlled style. RM-Bench defines three difficulty levels: (1) easy, where style mismatches may mislead the model; (2) normal, with matched stylistic quality; and (3) hard, where content is decisive despite stylistically superior distractor. JudgeBench. JudgeBench (Tan et al., 2024) is correctness-focused benchmark originally designed for LLM-based judges. Due to its pairwise format, it naturally supports pointwise reward model evaluation. It includes subsets such as MMLU-Pro (Wang et al., 2024d) (knowledge), LiveBench (White et al., 2024) (math and reasoning), and LiveCodeBench (Jain et al., 2024). RewardBench v2. RewardBench v2 (Li et al., 2024) is the second version of the original RewardBench (Lambert et al., 2024b), featuring substantially more difficult and realistic evaluation data. It assembles new human-generated prompts (in contrast to prior benchmarks which reuse downstream prompts), grouped into diverse and multi-skill classification tasks. On average, existing reward models score around 20 points lower on RewardBench 2 compared to its predecessor. RewardBench v2 also shows stronger correlation with downstream performance both during RL fine-tuning (e.g., PPO) and best-of-N inference sampling compared to earlier RM benchmarks. B.2 Full evaluation results In Table 5, we present the complete evaluation results for all the reward models considered. We categorize them into Bradley-Terry reward models, LLM-as-Judges, and the new paradigm of generative reward models (Liu et al., 2025). Across all seven benchmarks discussed in the main body of the paper, our reward models trained on SynPref-40M outperform all previous models on average. Model RewardBench RewardBench v2 PPE Pref PPE Corr RMB RM-Bench JudgeBench Avg. Bradley-Terry Reward Models GRM-gemma2-2B-rewardmodel-ft (Yang et al., 2024c) RM-Mistral-7B (Dong et al., 2023) Eurus-RM-7b (Yuan et al., 2024) BTRM Qwen2 7b 0613 Internlm2-7b-reward (Cai et al., 2024) FsfairX-LLaMA3-RM-v0.1 (Dong et al., 2023) internlm2-1 8b-reward (Cai et al., 2024) ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024a) Llama-3-OffsetBias-RM-8B (Park et al., 2024) QRM-Llama3.1-8B-v2 (Dorka, 2024) GRM-llama3-8B-distill (Yang et al., 2024c) QRM-Llama3.1-8B (Dorka, 2024) GRM-Llama3-8B-rewardmodel-ft (Yang et al., 2024c) URM-LLaMa-3.1-8B (Lou et al., 2024) Skywork-Reward-Llama-3.1-8B (Liu et al., 2024b) Skywork-Reward-Llama-3.1-8B-v0.2 (Liu et al., 2024b) Starling-RM-34B (Zhu et al., 2023) QRM-Gemma-2-27B (Dorka, 2024) Internlm2-20b-reward (Cai et al., 2024) Skywork-Reward-Gemma-2-27B (Liu et al., 2024b) Llama-3.1-Nemotron-70B (Wang et al., 2024e) LDL-Reward-Gemma-2-27B-v0.1 Skywork-Reward-Gemma-2-27B-v0.2 (Liu et al., 2024b) INF-ORM-Llama3.1-70B (Yang et al., 2024b) GPT-4o (Hurst et al., 2024) Claude-3.5-Sonnet (Anthropic, 2024) DeepSeek-GRM-27B (Liu et al., 2025) DeepSeek-GRM-27B (w/ MetaRM) (Liu et al., 2025) RM-R1-Qwen-Instruct-32B (Chen et al., 2025c) RM-R1-DeepSeek-Distill-Qwen-32B (Chen et al., 2025c) EvalPlanner (Llama-3.1-70B) (Saha et al., 2025) EvalPlanner (Llama-3.3-70B) (Saha et al., 2025) J1-Llama-8B (Whitehouse et al., 2025) J1-Llama-8B (Maj@32) (Whitehouse et al., 2025) J1-Llama-70B (Whitehouse et al., 2025) J1-Llama-70B (Maj@32) (Whitehouse et al., 2025) Skywork-Reward-V2-Qwen3-0.6B Skywork-Reward-V2-Qwen3-1.7B Skywork-Reward-V2-Qwen3-4B Skywork-Reward-V2-Qwen3-8B Skywork-Reward-V2-Llama-3.2-1B Skywork-Reward-V2-Llama-3.2-3B Skywork-Reward-V2-Llama-3.1-8B Skywork-Reward-V2-Llama-3.1-8B-40M 88.5 80.9 83.3 83.6 87.6 84.7 82.0 90.4 89.0 93.1 86.2 93.1 91.5 92.9 92.5 93.1 80.8 94.4 90.2 93.8 93.9 95.0 94.3 95.1 59.7 59.6 58.1 57.4 53.4 62.9 39.0 66.5 64.8 70.7 58.9 70.7 67.7 73.9 73.1 71.8 45.5 76.7 56.3 75.8 76.7 72.5 75.3 76. 59.7 61.8 59.6 61.8 62.1 63.1 57.3 60.6 59.2 57.2 63.2 60.6 62.1 60.2 62.1 62.2 62.8 52.3 61.0 60.3 64.2 62.4 63.6 64.2 LLM-as-a-Judges & Generative Reward Models 86.7 84.2 88.5 90.4 92.9 90.9 93.9 93.8 85.7 - 93.3 - 85.2 90.3 93.4 93.7 89.9 93.0 96.4 97.8 64.9 64.7 - - - - - - - - - - Our Reward Models 61.3 68.3 75.5 78.2 64.3 74.7 84.1 86.5 67.7 67.3 65.3 67.2 - - - - 60.3 60.6 66.3 67.0 65.3 67.6 69.5 70.6 66.6 69.1 77.3 79.8 58.5 56.4 60.5 58.4 60.0 61.1 53.6 60.6 64.1 60.3 62.8 60.5 60.0 60.4 60.3 62.5 57.5 54.8 63.0 60.1 63.2 63.9 61.9 64.4 - - 60.4 63.2 - - - - 59.2 61.9 72.9 73.7 68.3 70.5 74.7 75.1 67.4 72.1 83.4 87. 68.0 66.6 65.5 61.5 67.1 70.2 54.2 64.6 57.8 61.1 68.8 64.7 70.2 65.7 69.2 66.6 72.0 53.4 62.9 69.5 64.9 67.9 69.4 70.5 73.8 70.6 69.0 70.3 73.0 69.8 - - - - - - 74.5 78.1 80.6 81.2 76.7 80.5 86.4 89.3 66.2 66.9 69.0 69.4 67.1 70.5 66.2 69.3 71.3 72.5 70.3 72.8 69.9 72.0 71.8 72.1 67.1 65.9 68.3 68.5 72.2 71.1 70.0 73.8 - - - - 79.1 83.9 80.0 82.1 73.4 - 82.7 - 74.4 78.7 81.6 82.6 76.4 81.1 92.8 96. 63.5 62.1 58.4 63.8 59.4 59.9 59.0 59.7 63.5 62.6 63.3 63.8 62.3 64.1 62.0 62.9 63.8 57.5 64.3 65.2 65.8 64.2 66.5 70.2 59.8 64.8 - - - - 50.9 56.6 42.0 - 60.0 - 67.6 72.9 69.3 73.4 65.0 69.2 80.0 83.4 66.3 64.9 64.9 65.1 65.2 67.5 58.8 67.4 67.1 68.2 67.6 69.5 69.1 69.9 70.1 70.2 64.2 65.0 66.6 70.4 71.6 70.9 71.6 73.5 - - - - - - - - - - - - 70.9 75.2 77.8 79.3 72.3 77.1 85.7 88. Table 5: Open reward model performance on seven reward model benchmarks."
        },
        {
            "title": "C Dataset Processing details",
            "content": "C.1 Pre-processing, deduplication, and decontamination For pre-processing, we perform simple structural check to remove preference pairs in which either the chosen or rejected response contains None as content. This ensures valid formatting of the conversation. To eliminate potential duplicates within or across datasets, we perform global deduplication across all available data sources at the time. Specifically, for each chosen-rejected pair, we represent the sample using the tuple (conversation history, chosen response, rejected response) and discard any duplicates. The conversation history includes all prior user and assistant turns, while the chosen and rejected responses refer to the assistants final turn. To ensure decontamination from benchmark data, we remove any instances that share at least one 13-gram overlap with (first-turn) prompt from any of the evaluation benchmarks. For this, we employ decontamination script previously used to clean preference datasets against RewardBench data2."
        },
        {
            "title": "D Annotation details",
            "content": "D.1 LLM preference attributes labeling Before the verification and annotation process, our preference labels are generated from combination of API and local models, including GPT-4o (Hurst et al., 2024), DeepSeek-V3 (Liu et al., 2024a), Llama-3.370B-Instruct (Grattafiori et al., 2024), Llama-3.1-70B-Instruct (Grattafiori et al., 2024), Qwen2.5-72B-Instruct (Yang et al., 2024a), Qwen2.5-32B-Instruct, Qwen3-32B (Yang et al., 2025), Qwen3-14B (Yang et al., 2025). D.2 Human verification and annotation protocol LLM usage during human verification. During our human annotation pipeline, annotators are allowed to use external tools such as search engine or frontier LLMs, including GPT-4o (Hurst et al., 2024), all o-series models (Jaech et al., 2024; OpenAI, 2025), Gemini (2.0 Flash, 2.5 Flash, 2.5 Pro) (Team et al., 2023), Claude (3.5-Sonnet and 3.7-Sonnet) (Anthropic, 2024), and Grok (2 and 3) (xAI, 2024; 2025), DeepSeek-V3 (Liu et al., 2024a), and DeepSeek-R1 (Guo et al., 2025). However, we design strict guidelines for using these tool, and specify detailed guidelines for different tasks, objectivity type, and controversiality level. Batched pre-verification. To speed up annotation, we prioritize preference pairs labeled as objective, and pre-verify them with LLMs in batched way. Specifically, we use set of query templates embedded with the conversation with single response, and the LLM provides final judgment of correct or incorrect. During human annotation, the annotator still reads the response in general. This drastically improves efficiency, because annotators no longer need to interact with the LLM for verification and annotation. Verification and annotation priority. During our initial inspection of the data pool, we found that many preference data pairs contain extremely ambiguous preference signals, even with the provided attributes. In some conversations, the user asks vague questions, and both assistant responses seek clarification, differing only in phrasing. As result, we use the preference attributes to prioritize annotating objective and low-controversiality preferences. If an annotator cannot determine the preference relationship from the pairwise data, we skip the LLM annotation process and discard it. In the later stages of the project, we recognized that potentially more valuable approach is to use LLMs to label the differences between the two candidate responses and prioritize the annotation of these samples. However, due to the high inference cost associated with millions of samples, we will continue with our original approach in this work and leave this for future research. D.3 LLM-as-a-judge labeling For LLM-as-a-Judge labeling, we employ the same verification and annotation guideline used by human annotators but remove all sentences mentioning LLM usage and the use of web search for those without web browsing capabilities. Toward the end of the guideline, we provide at most eight concatenated pairwise instances and their corresponding preference attributes, and the target pairwise instance for labeling. 2https://gist.github.com/natolambert/1aed306000c13e0e8c5bc17c1a5dd300 22 D.4 Lessons learned from verifying and annotating human preferences in-the-wild While we initially include in-house human annotators, the authors also participate in the later stages of the annotation process. Here, we share the lessons we learned and some discussions from our annotation efforts. 1. LLMs can effectively automate certain types of annotation. For conversations involving reasoning tasks such as math problems or coding questions, LLMs are more efficient and reliable than human annotators. Human annotators may not be experts in all types of math and coding problems. We emphasize using cutting-edge models for this purpose, particularly those with advanced reasoning capabilities. Our inspection of early annotations reveals that different LLMs exhibit strong annotation bias. This bias arises from various sources, including scenarios with multiple or no ground-truth answers, which are highly context-dependent, and those requiring external knowledge. We believe this issue can be mitigated in the era of agents (Luo et al., 2025a), given their ability to perform web searches or conduct deeper research. 2. Human preferences are complicated, even for humans. During annotation, we consistently encountered preference pairs that were ambiguous, subjective, or context-dependent making it difficult even for trained annotators to confidently determine which response was better. Factors like subtle tone differences, varying expectations around informativeness or safety, and individual annotator biases introduced uncertainty into this process. This highlights key challenge in reward modeling: even with structured annotation protocols and strong preference attributes, some preferences are inherently ill-defined or non-universal. This problem stems from the concept of human preferences and their diversity. It also raises the question of whether single reward model can effectively capture this diverse range of human preferences. This view is also shared by recent blog post (Wang, 2025). 3. Learning clear and aligned preferences significantly enhances reward models. Our experiments demonstrate that when reward models are trained on preference data that is well-structured, verified, and guided by clear annotation protocols, their performance improves substantially across all evaluation benchmarks. We hypothesize that this may be due to the significantly higher requirement for constructing preference pairs in the benchmark dataset. While we do not have quantitative results, reviewing the preference pairs presented in multiple test sets reveals strong preference signal. This also highlights fundamental flaw in the design of todays preference data: although the response pairs are provided, the actual difference between them - the core indication of preference - is ignored. This raises concerns about what reward models, or any other types of models that provide reward signal, actually learn from underspecified responses."
        },
        {
            "title": "E Training Details and Hyperparameters",
            "content": "We primarily adhere to the hyperparameter choices outlined in Lambert et al. (2024a) and Wang et al. (2025a). During the development phase, we adjust the learning rates according to the model size, using 1e-6 for all 8B models and 4e-6 for all other sizes. All models are trained with global batch size of 256 and linear learning rate decay, using warmup schedule for only 1 epoch, with maximum token length of 16,384. For all final training runs, we switch to learning rate of 3e-6 and large global batch size of 10,240 for all models, following Wang et al. (2025a), due to its faster convergence and negligible impact on performance. All models are trained using 64 H800 GPUs with DeepSpeed ZeRO Stage 1 (Rasley et al., 2020)."
        }
    ],
    "affiliations": [
        "2050 Research, Skywork AI"
    ]
}