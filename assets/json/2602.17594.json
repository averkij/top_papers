{
    "paper_title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
    "authors": [
        "Lance Ying",
        "Ryan Truong",
        "Prafull Sharma",
        "Kaiya Ivy Zhao",
        "Nathan Cloos",
        "Kelsey R. Allen",
        "Thomas L. Griffiths",
        "Katherine M. Collins",
        "José Hernández-Orallo",
        "Phillip Isola",
        "Samuel J. Gershman",
        "Joshua B. Tenenbaum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 1 ] . [ 1 4 9 5 7 1 . 2 0 6 2 : r AI GAMESTORE: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games Lance Ying1,2, Ryan Truong2, Prafull Sharma1 , Kaiya Ivy Zhao1, Nathan Cloos1, Kelsey R. Allen3, Thomas L. Griffiths4, Katherine M. Collins1,4,5, José Hernández-Orallo5,6, Phillip Isola1, Samuel J. Gershman2, Joshua B. Tenenbaum1 1MIT 2Harvard University 3University of British Columbia 4 Princeton University 5University of Cambridge 6Universitat Politècnica de València (cid:109): https://aigamestore.org"
        },
        {
            "title": "Abstract",
            "content": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that more promising way to evaluate human-like general intelligence in AI systems is through particularly strong form of general game playing: studying how and how well they play and learn to play all conceivable human games, in comparison to human players with the same level of experience, time, or other resources. We define human game to be game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy the Multiverse of Human Games. Taking first step towards this vision, we introduce the AI GAMESTORE, scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier visionlanguage models (VLMs) on short episodes of play. The best models achieved less than 10% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with set of next steps for building out the AI GAMESTORE as practical way to measure and drive progress toward human-like general intelligence in machines."
        },
        {
            "title": "Introduction",
            "content": "Human intelligence has evolved to solve problems in an open world often problems we have never encountered before. Such general problem solving has enabled our species to survive and flourish in complex and ever-changing environments. The quest to understand human intelligence and replicate it in machine form has captivated philosophers, scientists and engineers for centuries (Turing, 1950; Newell et al., 1972; Chater et al., 2018). Only in recent years, however, has Artificial Intelligence (AI) reached the point where machine minds are serious near-term possibility. These developments have sparked heated discussion and debate about the prospect of Artificial General Preprint. Intelligence (AGI) (Bubeck et al., 2023; Mitchell, 2024; Chen et al., 2026). Debates about AGI aside, there is broad interest in assessing to what extent and in what ways machines are approaching the cognitive versatility and proficiency of well-educated adult (Hendrycks et al., 2025), or the capacity to learn and execute any cognitive task that typical human can, as efficiently and effectively as human can. Evaluating whether an AI system has achieved level of generality that is not narrowed by the testing setting is profoundly difficult. Traditional AI benchmarks often focus on assessing performance in isolated, albeit complex, domains, such as strategic board games like Chess or Go (Campbell et al., 2002; Silver et al., 2016), language understanding (Wang et al., 2018), answering domain-specific questions (Phan et al., 2025), solving mathematical problems (Cobbe et al., 2021) or completing coding tasks (Jimenez et al., 2023). Consequently, these benchmarks, while measuring important facets of intelligence, only assess fragments of the vast landscape of human capabilities and activities, and thus fail to capture the generality of intelligent human behavior. While recent work has attempted to build large collection of benchmarks to evaluate large language models (Srivastava et al., 2023), these still only cover tiny subset of tasks that humans can solve. Performance on these tasks remains poor indicator of how well models can thrive in an open-ended world (Xing et al., 2024; Li et al., 2025; Collins and Tenenbaum, 2026). The sheer breadth and depth of human activities pose significant challenge for creating any single, meaningful point of comparison between human and machine intelligence. Building one benchmark that encompasses all real-world human activities is simply impractical. How then can we design evaluation paradigms that truly test the generality, adaptability, and integrated cognitive capabilities of human intelligence, rather than just measuring performance on predefined narrow task spaces? In this paper, we argue for and prototype way to address this challenge, with new twist on classic approach. We propose that promising way to assess human-level and human-like general intelligence capabilities in AI systems is by studying how and how well they play and learn to play all conceivable human games, and comparing their game play to the behavior of wide and representative range of human adults. This differs from studying how well AI systems play any one game at high level of expertise, as well as other formulations of general game playing based on an unbounded distribution of all computable environments or even all conceivable games (Legg and Hutter, 2007). We require that the games could conceivably be created and enjoyed by some broad segment of the human population. We refer to this space of games as the Multiverse of Human Games. The Multiverse of Human Games offers uniquely comprehensive and objective testbed for machine general intelligence, rooted in why humans play in the first place. Games are powerful cultural artifacts, designed to be effective miniatures and abstractions of real-world human activities, problems, challenges, enterprises, and dynamics for training and preparing humans for adaptation and problem solving in the real world. Games cover nearly every human skill and interest, from strategic planning and resource management (strategy games) to social interaction and deception (social deduction games), pattern recognition (puzzle games), and navigating complex physical environments (video games). To excel in the space of all conceivable human games is to possess diverse set of cognitive capabilities, needed for surviving and flourishing in the world humans inhabit. For the purpose of measurement, we believe that focus on games humans enjoy covers range of capabilities that are neither too trivial nor too demanding, while capturing at least some of the diversity in human cognition. While the Multiverse of Human Games is appealing in principle as setting to evaluate machine general intelligence, there are significant challenges to making this idea practical. Most fundamentally, working with actual human games limits us to just finite subset of the games people could possibly have created, and relatively small, closed subset if we restrict ourselves to just popular games that have been produced and played widely. There would also be substantial technical hurdles to working with commercially produced digital games, including interface heterogeneity, intellectual property constraints, and the pervasive risk of data contamination within training corpora. Here we take first concrete step towards the full Multiverse vision by proposing the AI GAMESTORE, platform that leverages large language models (LLMs) to source and adapt diverse games from popular digital marketplaces into standardized, containerized evaluation game environments. The pipeline then uses human-in-the-loop system to refine such games and create novel variants of the generated games, thus creating never-ending game benchmark for evaluating machine general 2 intelligence. As proof of concept, we curated 100 games and conducted comprehensive comparative analysis between frontier vision-language models (VLMs) and 106 human participants. Our results reveal significant performance gap, with state-of-the-art models achieving less than 30% of the human baseline on average, while taking 15-20x more time to compute than humans. Notably, we observe that current models struggle primarily with environments requiring robust world-model acquisition and long-term memory. In summary, we make the following contributions in this paper: 1. We introduce and argue for studying AI capabilities in the Multiverse of Human Games, as promising route to measuring human-like general intelligence in machines. 2. We propose the AI GAMESTORE, tractable, scalable and open-ended evaluation platform that aims to sample new games from the Multiverse of Human Games. 3. We generate first suite of such games on AI GAMESTORE, sourced from top charts of Apple App Store and Steam. 4. We evaluate popular frontier vision language models against human play and learning on the first two minutes of each game, highlighting how the AI GAMESTORE reveals cognitive capability gaps of current models as well as how the platform should be extended to more fully realize its potential for assessing human-like learning and thinking in AI."
        },
        {
            "title": "2 Measuring General Intelligence with the Multiverse of Human Games",
            "content": "A central argument of this paper is that the space of all conceivable human games provides uniquely comprehensive set of tasks for evaluating machine general intelligence. By human games, we mean games that humans intentionally have designed for themselves or other humans to play. These games are by definition enjoyable and learnable by (at least some) people. The space of all conceivable human games is infinite and open-ended: these are all the games that humans could create and that other humans could enjoy. By the Multiverse of Human Games, we mean not only this space but also the associated distribution of how likely they are to be created and spread by humans. We propose that promising way to evaluate how well machines are approaching human-level general intelligence is by testing how well machine can learn to play representative samples of the Multiverse of Human Games relative to typical human player when given the same gameplay budget. This paradigm builds on long and rich tradition of using games and general game playing to study intelligence (Cleveland, 1907; Genesereth et al., 2005; Schaul et al., 2011; OpenAI, 2016; Hernández-Orallo et al., 2017; Perez-Liebana et al., 2019; ?; Hafner, 2021; Allen et al., 2024; Paglieri et al., 2024; Collins et al., 2025b; Ying et al., 2025; Li et al., 2025; Wang et al., 2025; Lehrach et al., 2025; Magne et al., 2026; Google, 2026; ARC Prize, 2026) (For more extended discussion of this and other related work, please see Appendix A.) Our aim here is to extend these efforts with formulation and technical approach that scales to the full space and distribution of all games designed or to be designed by humans although in this paper we can implement only first, very small step towards this goal. In the following sections, we will discuss why the Multiverse of Human Games is good way to evaluate truly general intelligence as well as the practical challenges and strategies for operationalizing this evaluation paradigm with the AI GAMESTORE . 2.1 Why is this good measurement of general intelligence? The proposition that the set of all conceivable human games serves as robust proxy for human-like general intelligence is rooted in the teleology of play itself: humans design, engage in, and propagate games to prepare themselves for the multifaceted challenges that they are likely to encounter in dynamic environments and habitats. Play is foundational part of human cognition (Chu and Schulz, 2020). Human cognitive development is characterized by an inherent propensity for play behavior; children and adults alike frequently engage in the spontaneous generation of arbitrary goals, rules, and constraints. This is exemplified by the way individuals gamify mundane environmentstransforming simple walk into challenge to avoid specific pavement patterns or imposing complex rules on existing activities to modulate difficulty. Furthermore, children exhibit frequent pretend-play, using their imagination to act out make-believe scenarios, take on different roles (like doctor or superhero), or use objects to represent Figure 1: Many games are abstractions of real-world activities. They are inspired by diverse and concrete activities in human enterprise, and they prepare agents to adapt to similar problems that arise in the real-world. something else (like block as phone) (Lillard et al., 2013). Play behavior is not unique to the human lineage, and has been documented across diverse species, from the complex social romping of primates to the object manipulation observed in corvids and cetaceans (Smith, 1982; Burghardt et al., 2024). This mounting body of evidence suggests that play serves as phylogenetically conserved mechanism for learning. By inventing and navigating hypothetical problems, agents can refine their cognitive capabilities and problem-solving skills to increase their fitness. Empirical research substantiates this evolutionary perspective, demonstrating that game play is robustly associated with measurable improvements in executive functions, spatial reasoning, and attentional control (Spence and Feng, 2010; Feng et al., 2007). Furthermore, games serve as vital cultural artifacts (Chu et al., 2024). They are not merely pastimes but are sophisticated vehicles for cultural transmission. By abstracting and containerizing real-world complexities ranging from the strategic planning in multi-party conflicts to the nuanced social dynamics of RPG games humanity has collectively created curriculum for training and preparing individuals for surviving and adapting in the open world (Figure 1). Games often pass down through generations and spread across cultures: invented thousands of years ago, Go and Chess are still enjoyed by millions of players today; the Olympic Games, which first emerged from the cradle of ancient Greek civilization, have evolved into grand cross-cultural human enterprise with events practiced and watched by people around the world. Consequently, the space of all conceivable human games represents distilled, concentrated library of the essential skills required to navigate the world that humans live in. To excel in this space of games is, by design, to exhibit the core tenets of human-like general intelligence. 4 Figure 2: Comparison between different space of games discussed in the paper. The yellow rectangle represents the full space of all computable games. In the paper, we introduce the Multiverse of Human games (orange), which collectively demand large space of cognitive capabilities that are found in average humans. We argue that this space is good proxy for human-like general intelligence. Then the space of all digital games on gaming platform (green) covers subset of that space. Among the these digital games, AI GAMESTORE (blue) aims to sample from all digital games but the initial 100 games only cover small restricted space. 2.2 Practical challenges of evaluating general intelligence with human games While we argue the Multiverse of Human Games is good testbed for human-like general intelligence in machines, such space of games is unbounded. To create practical evaluation suite, we may start with all games on any digital gaming platforms. This would create finite set of well-defined instances while still covering broad set of activities that require diverse cognitive skills (See Figure 2). However, the immense scale and heterogeneity of all millions of digital games on gaming platforms still makes it challenging to implement as direct, monolithic benchmark today. The technical challenges of providing universal interface and evaluation framework for these digital games are formidable. We list few major roadblocks below: 1. Copyright and licensing restrictions: The majority of commercial games are protected by proprietary licenses and intellectual property (IP) laws, preventing their use in public AI benchmarks without complex, costly, and often unattainable agreements with developers and publishers. 2. Platform heterogeneity: Games are built on diverse engines (Unity, Unreal, custom), operating systems, and APIs. Creating single, universal evaluation platform or standardized interface capable of normalizing input and state across thousands of structurally varied titles is formidable software engineering challenge. 3. Human data collection and privacy: Obtaining high-quality human gameplay data for evaluating models is restricted by user privacy regulations (EULAs) and typically unwillingness of game companies to share data logs. 4. Latency for real-time games: Many games on the gaming platforms require rapid player response (e.g. action games with live combats). Todays commercially available AI models, especially with thinking enabled, all have long latency for each API call. The model would trivially fail at these games if they are queried to play the existing real-time games as is. 5. Dataset contamination risk: Because AI model developers frequently do not disclose their training data, it is impossible to verify which games in the benchmark have already been seen by the model. Model developers can also train their models on vast space of digital 5 games to perform well on such benchmark that consists of actual games on digital gaming platforms. This risk of data contamination invalidates the evaluation as measure of general intelligence. While we encourage the industry to overcome these challenges and develop large-scale evaluation benchmarks based on diverse and representative human games, we also need more practical, yet still highly dynamic approach to approximate this ideal space for evaluation purposes. In this paper, we propose an alternative approach towards this vision by developing the AI GAMESTORE. The AI GAMESTORE is designed as meta-benchmark for evaluating AI systems on diverse set of human-designed games and facilitate their comparison against human performance. However, instead of using the original set of games on gaming platforms, it uses an automated pipeline to source and rebuild synthetic games that represent standardized versions suitable for tractable and rigorous testing of AI models and comparison with human behavior. It serves as concrete, albeit simplified, realization of the digital games benchmark concept we proposed above. We will introduce the construction and experiments on the AI GAMESTORE in the sections below."
        },
        {
            "title": "3 The AI GAMESTORE",
            "content": "The AI GameStore offers unified sandbox environment designed for evaluating models on standardized adaptations of popular digital games (See Figure 4). Unlike many existing generative game evaluation frameworks (e.g. Cobbe et al. 2020, Verma et al. 2025), AI GAMESTORE aims to source and generate human games that are designed and enjoyed by humans. To do so, the AI GAMESTORE utilizes scalable, semi-automated pipeline for the procedural generation and refinement of evaluation tasks, as visualized in Figure 3. This pipeline operates across four distinct stages: Stage 1: Sourcing and suitability filtering High-quality game candidates are harvested from existing gaming platforms and subjected to multi-stage filter based on player engagement metrics and an LLM-driven suitability scoring system. Stage 2: Game generation and refinement Utilizing filtered game descriptions, an LLM generates functional p5.js codebase. This draft undergoes automated unit testing via simulated play to ensure mechanical stability and basic responsiveness to input. The functional code is subjected to secondary refinement phase where human participants provide natural language feedback to correct mechanical issues, increase playability and propose novel gameplay variations. Stage 3: Game annotation and profiling To characterize the latent cognitive demands of the benchmark, each finalized game is subjected to human annotation process. Expert annotators evaluate the tasks across multi-dimensional cognitive taxonomy using 0-5 scale. These profiles allow for the disentanglement of complex model behaviors by mapping performance failures to specific cognitive demands, ensuring that the AI GAMESTORE serves as both benchmark and diagnostic tool for understanding machine intelligence. Stage 4: Model evaluation In the final stage, both human players and various AI models interact with the games through gameplay interface. AI models are integrated via specialized harness to ensure standardized interaction. The resulting game output is used to compute aggregate human vs model performance as well as deeper analysis on models cognitive capabilities. This hybrid approach is highly efficient; the end-to-end process of generating and refining new game with human-in-the-loop can be completed in approximately 30 minutes on average. The pipeline achieves significant scalability by integrating recruited human participants (especially through online crowdsourcing platforms), enabling the continual expansion of the benchmark suite. This framework creates living evaluation suite: 1) as new games emerge on digital marketplaces, they can be ingested to generate fresh evaluation tasks, and 2) by having human participants creating novel variants of existing games, large space of human games can be generated scalably even with few game concepts. This ensures the evaluation platform is more robust to overfitting and saturation, as new games can continually probe model capabilities. 6 Figure 3: The AI GAMESTORE pipeline consists of four core stages: a) Game Sourcing and Selection: Popular games are harvested from digital marketplaces (Apple App Store and Steam) and filtered based on player ratings and reviews. An LLM then scores these candidates against specific suitability criteriasuch as playability within minutes and the ability to produce quantifiable metricsto identify the most viable games for adaptation. b) Game Generation and Refinement: Using game descriptions and requirements, an LLM generates an initial game (Version 0). This version undergoes automated refinement via simulated play and error-checking, followed by humanin-the-loop refinement, where human participants play the game and give feedback to improve the game until its fun and playable. This process generates base game that corresponds to the original game and novel variants with modified or added mechanics. c) Game Annotations and Profiling: The final generated games are played by humans who annotate them based on rubric of cognitive capabilities (e.g., planning, working memory, and reasoning). These annotations enable in-depth analysis on AI models cognitive capabilities. d) Model Evaluation: AI models and human players interact with the games through standardized interface. We then compute models performance normalized against humans and perform capability analysis. 7 Figure 4: Examples of popular digital games on the Apple App Store and their adapted AI GAMESTORE versions. We present four example games, capturing diverse genre and cognitive capability. The top half of each example shows pair of original game and the its corresponding version on the AI GAMESTORE. The bottom half shows the annotations of the cognitive demand for the game. The games and example play videos can be accessed on http://aigamestore.org. (VP = Visual Processing; ST = Spatial-temporal Coordination; ME = Memory; PL = Planning; WM = World Model Learning; PH = Physical Reasoning; SO = Social Reasoning. 3.1 Sourcing games from digital game marketplaces The AI GAMESTORE draws its content from popular digital game marketplaces, specifically focusing on games from the Apple App Store and Steam. We choose platforms like the Apple App Store as primary source for several key reasons: 1) these gaming platforms are immensely popular with hundreds of millions of monthly active users from all over the world; 2) the games are created by large diverse cohort of game developers with diverse themes and concepts; 3) new games are constantly being developed and released on these platforms, which mitigates saturation; and 4) these gaming platforms maintain rigorous review system that screens out poorly designed games. We first sampled 7,500 games from these platforms with diverse genres and themes (see Appendix for details). We then filtered the games based on their popularity and diversity. We retained the games that have at least 10,000 reviews and an average ratings over 4.5 out of 5. We then used Gemini 2.5 Flash to score each game base on their suitability to be converted into game on the AI GAMESTORE. We prompted the LLM to score the game based on few criteria, including 1) whether the game can be reasonably played within few minutes, 2) can be expressed in p5.js, 3) can have quantifiable metric for evaluating the performance, and 4) does not require extensive game-specific knowledge (e.g. poker). The LLM judge was asked to output suitability score out of 100 and give its explanation. After filtering the games, we retained 100 such games for generation. 3.2 Game adaptation and construction The game construction pipeline works as follows (see Figure 3). Inspired by recent work on LLMpowered game generation (Todd et al., 2024; Nasir et al., 2024; Kanervisto et al., 2025), we prompt an LLM to generate game based on the description of the game from the filtered dataset. We used CLAUDE-SONNET-4.5 for all game generation. To facilitate practical and scalable model evaluation, 8 we designed detailed game specs for AI GAMESTORE games including the format, metrics, and technical implementation (e.g. all games must be written in JavaScript, can be paused, have scoring metric, have multiple levels, etc). This ensures that all games can be expressed and run on web portal and be scored for comparing across models and human players. The detailed design spec can be found in Appendix C. To ensure games are playable, engaging and capture the core mechanics of the original game, we implement sophisticated iteration and refinement pipeline. The iteration for each AI GAMESTORE game is divided into two stages. In the first stage, we ask an LLM to generate simple game test script based on the game source code in JavaScript. Then the script is run to simulate different actions in the game and detect game bugs. Upon catching the bugs or broken mechanics, the LLM model is asked to fix the bug until the game passes all the tests. Then in the second stage, the game is further refined with human in the loop. The refinement takes place over customized interface (see Appendix D). The human player plays to evaluate the game and gives additional feedback to the LLM to improve the games. The refinement loop continues until the human player determines that good base game is generated that mimics the original game and is playable and sufficiently engaging judged by the human player. Each refinement step takes about 2 minutes. On average, this process took 4.7 refinement steps for all 100 generated games. Additionally, human player may generate variants of each game by proposing novel mechanics using the same refinement interface. An example of this process is shown in Figure 12. This allows many valid variant games to be generated per source game and mitigates benchmark saturation. 3.3 Game annotation and profiling To better characterize the distribution of cognitive capabilities measured by the AI GAMESTORE games, we annotated each game based on the cognitive capabilities needed to succeed in each game. We selected few commonly used cognitive capabilities categories as shown in Table 1. This covers many cognitive capabilities proposed in previous literature (Zhou et al., 2025; Hendrycks et al., 2025; Ying et al., 2025) and incorporates additional categories (Spatial-temporal Coodination and World Model Learning) that are valuable for studying reasoning in the dynamic contexts we study here. For each capability, we annotated the games on 6 point scale where 0 indicates the capability is absent and 5 indicates that the game requires extremely sophisticated capability. Detailed rubrics can be found in Appendix F. Each game was annotated independently by three annotators from our author team, based on the rubrics. The annotators then deliberated to resolve the disagreement. Overall we find significant diversity in the 100 games generated by our pipeline. Many games require multiple sophisticated cognitive capabilities. We show some examples in Figure 4. 3.4 Model evaluation The final stage of the AI GAMESTORE pipeline involves evaluating various AI models against human players to quantify their performance and latent capabilities. To ensure fair and standardized comparison, both human players and AI models interact with the games through unified interface. While humans use standard computer inputs, models are integrated via specialized evaluation harness that enables them to indicate specific actions to perform at each step. Although many different types of harnesses have been proposed for interactive gameplay (Zhang et al., 2025), we describe one possible implementation in Section 4 and encourage researchers to develop and test alternative harnesses to interface with our games. Models and humans are provided with the same interaction budget (e.g. number of seconds), allowing for direct performance comparison against average human baselines. By leveraging our cognitive demand annotations for each game, we can probe why models fail at specific tasks and perform fine-grained analysis of their cognitive strengths and weaknesses. Detailed descriptions of the specific models tested, the prompting strategies used in the harness, and the resulting performance metrics across our 100-game corpus are provided in Section 4. 9 Cognitive Capability Visual Processing (VP) Description and Examples Requires counting or matching object properties like shape and size. Example: Connect-3 games require identifying and matching tiles of the same color. Spatial-temporal Coordination (ST) Requires well-timed and precise actions to navigate visual scene. Example: In Flappy Bird, the player must time sequence of flaps precisely to pass through barriers. Memory (ME) Planning (PL) World Model Learning (WM) Physical Reasoning (PH) Social Reasoning (SO) Requires retrieving information from previous frames for current or future actions. Example: Navigating game with partial map view requires integrating information across frames. Requires simulating many steps ahead and evaluating future outcomes. Example: Water Sort requires calculating sequence of pours to reach the goal state. Involves inferring hidden game mechanics not explicitly provided in the description through active gameplay. Example: Baba Is You requires players to discover how manipulating text blocks physically alters the games logic and rules. Requires mental physics simulation. Example: Angry Birds requires adjusting angles and power based on simulated trajectory and impact. Requires reasoning about the intentions, beliefs, and plans of other agents. Example: In Counter Strike, the player must reason about enemys line of sight and pathing. Table 1: Cognitive capability categories for annotating AI GAMESTORE games. Each capability is annotated on 6 points scale. Detailed rubrics for each can be found in Appendix F. 3.5 Game release and updates Finally, new games can be continually generated and incorporated into our game evaluation suite. To minimize the overfitting and saturation concerns that affect any benchmark, we initially make only 10 of the games public, for demonstration and open experimentation by the community. The remaining 90 games in this first version of the benchmark constitute private test set. The 10 public games are available on our website, along with instructions for how to evaluate models and mechanisms for community submitted models to be tested privately on the full set of 100 games. The public games are chosen to be representative of the full suite, in terms of objective difficulty for current models, capabilities tested, and subjective fun and challenge ratings, We describe some summary statistics for the public and full game sets in Appendix G. Using the aforementioned pipeline, AI GAMESTORE will continue to source and generate more games, providing living and continually evolving benchmark for evaluating AI models."
        },
        {
            "title": "4 Model and Human Experiments",
            "content": "In this section, we report the results of our benchmarking study testing state-of-the-art AI models and human participants on the 100 curated AI GAMESTORE games described in the previous section. 4.1 Human gameplay experiment In order to compare models performance on each game relative to humans, we recruited 106 human participants from Prolific (mean age = 38.81, 58 male, 46 female, 2 non-binary) to play each game over customized game interface. This human study was conducted under an MIT IRB-approved protocol. Each human player was randomly assigned to play 10 games in sequence, and was given two minutes (120s) to play each game. At the end of each game, players were asked to rate using two slider scales how fun and how challenging the game was (0 = not at all fun/challenging, 100 = extremely fun / challenging). On average participants find the games to be moderately fun and challenging (See Appendix G). Throughout their two minute playtime, the game score was collected every 30 frames and their gameplay video and complete action sequences were recorded. 10 4.2 Model gameplay experiment We evaluate seven frontier vision language models (VLMs), including GPT-5.2, GPT-5-MINI, GEMINI-2.5-PRO, GEMINI-2.5-FLASH, CLAUDE-OPUS-4.5, QWEN-3-VL-32B, and LLAMA-4MAVERICK. Each model is run three times on each game, under the default configurations for that model (i.e., the default temperature and auto mode for thinking budget) and the model performance and runtime is averaged across three runs. While ideally the models should be able to play the games as humans do over web interface, todays AI models cannot interact with the games the same way humans do. For instance, many models have long response time for each API call much beyond normal human response time, which would make real-time interaction impractical. Therefore, we built lightweight harness for AI models to interact with the game environment, inspired by previous work on prompting AI agents to play video games (Zhang et al., 2025). In our harness (Figure 13), we pause the game every second to query the model to elicit five lists of actions to perform in the next second, with each action list corresponding to 0.2 second segment of gameplay. Upon receiving the model response, the game is resumed and the actions are applied. The loop continues until the game is won or it reaches 2 minutes of game play (120 API calls). Many possible game harnesses can be used for the models to interact with our games, and we encourage future research to use different techniques for prompting the models to play the games on the AI GAMESTORE, as well as other AI agent architectures. The prompt of each API call consists of five parts: game description and control, models scratchpad, actions performed and game screenshots since the last API call, and prompt with available actions in the current step. The scratchpad serves as the models memory. It records the summary of the gameplay history, and anything the model wishes to record for future gameplay. The scratchpad is updated at each model API call. The model response contains three components: 1) an updated scratchpad, 2) list of actions to perform for the next second, and 3) its reasoning and explanation for the actions provided. We provide additional information for the harness in Appendix H."
        },
        {
            "title": "5 Results",
            "content": "We report the geometric mean of each models human-relative performance across all 100 games in Figure 5. Following prior work comparing AI models with humans on large suites of video games (Tsividis et al., 2021), we chose to report the geometric mean because we are interested in how models perform across many tasks relative to typical human players, and these scores can be heavily skewed.1 To account for the heterogeneous scoring scales across the game suite, we normalize all model scores by the median human performance in each game, where the human median for each game is set to 100, and we cap model scores between 1 to 10000 before computing geometric means: Normalized Score = clip 100 (cid:18) Raw Game Score Human Median Score (cid:19) , 1, 10000 . While the evaluated models demonstrate the ability to navigate and interact with most game environments, substantial performance gap remains between AI agents and human participants. State-of-the-art models like GPT-5.2, GEMINI-2.5-PRO, and CLAUDE-OPUS-4.5, all achieve geometric mean scores of less than 10% of the human baseline (see Figure 5). The differences between the top 6 models are not statistically significant (p < 0.05). The distribution of model normalized scores across all 100 games is shown in Figure 6. All models exhibit distinct bimodal distribution. First, for roughly two-thirds of the games, they are able to make some progress. They can even surpass the median human score on handful of games; these tend to be easy, casual games that require some visual processing but few or no other cognitive capabilities, and players can earn high scores with simple strategy that is executed very quickly (often more quickly than most humans do). For most of the games which models make progress on, however, they score significantly worse than median humans between 10-30% of the median human level while taking much longer to reason about each move. Then there is second category 1Using medians to aggregate model scores across games is another alternative with similar properties, and leads to similar numerical results (see Figure 6). 11 Figure 5: Performance score (top) and runtime comparison (bottom) between human players and VLMs on 100 games. We normalized all model scores against human median scores for each game (i.e. human median = 100), and then report the geometric mean of normalized scores across 100 games. The best scoring model, GPT-5.2, reaches only 8.5 out of 100 on the human-relative scale. Additionally, humans play each game for 120s, whereas models are significantly slower to complete 120 API calls, requiring more than 10 times longer (> 1200s) to finish most games, and averaging around 12-18 times longer. Error bars indicate 95 % confidence intervals. Figure 6: Model score distribution across all 100 games. The normalized scores are capped between 1 and 10,000 where the human median for each game is 100. All models show bimodal distribution across games, scoring close to 0 on significant portion of the games, while performing factor of 3 or 4 times worse than typical human players on the remaining games (and occasionally outperforming human averages on just few games). The horizontal line at the top of each plot shows the interquartile range, with the diamond and the square on each line showing median and geometric mean scores, respectively, across 100 games. 12 Figure 7: Geometric means of model scores grouped by cognitive demand for each game, with different difficulty levels as measured by demand thresholds. For each difficulty level and each cognitive capability, game is included in the calculation of models score if the game was judged to demand that capability at the given demand level, or higher. For instance, the first group of scores (labeled Visual Processing) in the top plot (Demand 1) shows model performance on all games that require at least level 1 capability in Visual Processing. Overall, models tend to struggle especially with games that challenge Memory, Planning and World Model Learning. Error bars indicate 95% confidence interval. The number of games for each cognitive capability is shown in brackets. of games, roughly 30-40% for all models, where they struggle fundamentally and fail to achieve any meaningful progress at all and obtain less than 1% of the median human score. While these analyses give some indication of the gap between humans and AI models for this sample of games, assessing model performance based on its relation to the human median performance provides very limited visibility into what makes different games challenging for models. To gain more insight into model behavior, we break down the model performance by their cognitive capability profiles (Figure 7), which shows the geometric mean of model performance on games under each cognitive capability. While AI models have made significant strides, they consistently perform below the human baseline (normalized to 100) across all tested capabilities. Notably, Memory, Planning and World Model Learning represent the most significant bottlenecks. This suggests that models still struggle with maintaining information across timesteps even with scratchpad, and solve problems in ambiguous domains with underspecified rules and mechanics. Additionally, model performance decreases significantly on games that require planning and social reasoning as games demand more sophisticated levels of these capabilities. While our results show that todays frontier VLMs struggle with the majority of the games, one possible explanation is that the harness is not allowing the model to react quickly enough to the game, as the model is queried 5 sets of actions every game second. We computed the model performance on games that do not require fast reaction time (demand score 2 for Spatial Temporal Coordination). These typically include puzzle games and turn-based strategy games. We found little difference in aggregate model performance. (See Appendix I). 13 Figure 8: Geometric mean of model performance as function of the number of different cognitive capacities game challenges. When models perform close to median human levels, it is typically only for the few games that place high demand on just one cognitive capability. As games demand more distinct cognitive capabilities, performance for all models relative to humans drops significantly. Error bars indicate 95% confidence interval. Figure 9: Model and human median cumulative score trajectories on 10 public games. The first four games correspond to the examples shown in Figure 4. Scores are based on the maximum score achieved up to each time point (so they are always nondecreasing), and measured each second of game play for the first 120s of play. All scores are normalized to the maximum score of the median human player (human median = 100). The variation in model behavior shown across these games is representative of what we observe across the full set of 100 games: AI models make some progress on most games but typically much more slowly than humans, and for number of games they completely fail to make progress at all. We also assessed how model performance varies based on the the number of capabilities present in the game. Models generally struggle on games that require more cognitive capabilities (Fig 8). This indicates that success in many games require models to not only have sufficient competence in each cognitive capability, but also integrate them to solve the problem. Figure 9 shows the median cumulative score trajectories for models and humans on the 10 public games we released on our website. While humans are able to make steady progress on all of these games, models behave differently. For the majority of games, models make early progress but then stagnate or advance much more slowly than human players (e.g. Games 1, 4, and 9). And in significant fraction of games, all models fail to make any progress at all (e.g. Game 6 and 10). Closing this gap is an important near-term goal for building general human-like and human-level AI: Systems should be able to make rapid and steady progress on any new task that typical human learners can, at roughly the pace of human learners. 14 Lastly, we also evaluated the average runtime of all models relative to human players (Figure 5). Models all take significantly longer than humans to complete each game. This is because the models spend few minutes thinking, in addition to typically few seconds of response latency per query; as result, many models spend at least 20 minutes on the game, while humans play the games within 2 minutes. To achieve genuine human-level general intelligence, future AI models should strive for real-time integrated systems that aim to play games in human-like way, performing perception, thinking, decision-making simultaneously and within strict time budget."
        },
        {
            "title": "6 Discussion and Future Directions",
            "content": "In this paper, we argue that promising way to measure and understand the advance of general human-like intelligence in machines is by evaluating how AI systems play on the full space of possible human games. Leveraging LLMs to construct samples drawn from the vast and diverse world of digital human games, we proposed the AI GAMESTORE as the basis for never-ending meta-benchmark for evaluating current and future AI models. Our current implementaiton is just first step towards operationalizing this idea in scalable benchmark, based initially on small slice of some of the simplest games in this enormous space. We focused here on the setting of rapid learning of relatively simple but novel games, and human players engaging mostly for fun (as opposed to material reward). Even in this simple setting, our findings reveal stark divergence between state-of-the-art visionlanguage models (VLMs) and human players: Leading frontier AI models achieved on average less than 10% of median human players scores across 100 games while taking much longer than human players do to think and act. But the AI GAMESTORE serves not only as quantitative benchmark with leaderboard; it is also diagnostic tool for identifying missing capabilities of machine cognition. While frontier models like GPT-5.2, CLAUDE-OPUS-4.5, and GEMINI 2.5 PRO exhibit sophisticated linguistic and visual reasoning capabilities, they appear to fall far short of human cognition when confronted with tasks requiring storing and retrieving episodic information, learning new world models, and long-horizon planning and especially on tasks that challenge number of these capacities at the same time. We believe these tasks are also some of the most representative of where AI will increasingly be challenged in the real world, and where human standards of safety, robustness, and trustworthiness will most matter to the humans who will be engaging with and affected by these AI systems. In its current state, the AI GAMESTORE is still relatively primitive, and limited in the capacities it can assess and the insights it can yield. Nonetheless, we see this work as proof of concept that it is practical and valuable to build scalable and open-ended AI evaluation approach based on synthetic versions and variants of actual human games. To advance this approach towards truly comprehensive framework for evaluating general human-like intelligence, as indicated in Figure 2, our future work must extend the AI GAMESTORE in many directions including the following: Expanding game diversity While our initial corpus covers broad spectrum of cognitive tasks, we aim to increase both the quantity and the diversity of games. Many games do not pose significant challenges for certain capabilities. For instance, most games we have utilize relatively naive non-player character (NPC), which does not mentalize over the players mental states and adapt its strategies. As result, the games do not test complex social reasoning (e.g. recursive mentalizing). Future AI GAMESTORE games will introduce environments that require agents to engage in sophisticated multi-agent interaction, drawing inspiration from frameworks like Melting Pot (Leibo et al., 2021). In particular, we aim to support games where multiple AI agents can interact with each other or with human players in collaborative and competitive settings. Generating complex and challenging games The current suite consists primarily of easy, short-duration or casual games that can be learned almost instantly. While effective for testing immediate reasoning and rapid learning capabilities, these do not capture the most important long-horizon and multi-timescale activities humans engage in. We intend to develop methods for generating more sophisticated, longer timescale games characterized by more 15 complex scenarios, tasks, and storylines that might require hours of gameplay. Such environments will force models to maintain state across vast temporal windows, necessitating transition from reactive agents to those capable of forming complex world models and tracking large volume of information in its memory. Automated level generation While we find that todays frontier LLMs are capable of generating playable casual games, they often struggle with coming up with interesting levels. The levels generated by the models are often too trivial or impossible. In our pipeline, we have human players giving natural language feedback on how the LLMs may improve the levels, but to make the game generation pipeline more scalable, future work can build more sophisticated testing and iterating pipeline to procedurally generate interesting and challenging game scenarios and control their difficulty levels (e.g. Todd et al. 2023), potentially based on richer computational understanding of what makes games fun to people (Collins et al., 2025a). Deeper capability-oriented analysis While we offer some preliminary experiments on how our games can be used for evaluating model capabilities, more games are needed to support more advanced analysis, especially because the games we have are still not sufficiently challenging or reveealing for many aspects of human cognition. Quantifying model capabilities in interactive environments also remains significant methodological challenge. Because single game often tests composite of skills e.g., failure in Angry Birds could stem from poor physical reasoning, visual noise, or motor coordination traditional scoring is insufficient for skill disentanglement. While we present some preliminary analysis on model capability, we need more sophisticated ability profiling techniques, such as measurement layouts (Burden et al., 2024) or the ADeLe methodology (Zhou et al., 2025) adapted for sequential decisionmaking. This would allow us to estimate latent model capabilities across overlapping skill sets. In sum, while our current platform is only modest beginning towards operationalizing the full multiverse of human games vision, we hope it serves as an example and catalyst for building more general, scalable, open-ended AI model evaluations and small step towards the development of general-purpose agents capable of interacting intuitively and flexibly, and safely and robustly, with human beings in human world."
        },
        {
            "title": "7 Acknowledgments",
            "content": "This work is funded in part by AFOSR, ONR through the Science of AI, and MURI programs, Schmidt AI2050 Fellowship to JBT, and the Siegel Family Quest for Intelligence at MIT. Research was also sponsored by the Department of the Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Department of the Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. JHOs research is supported by OpenAIs grant to the AI Progress through the Lens of Predictable AI Ecosystems programme, which is based at the Leverhulme Centre for the Future of Intelligence at the University of Cambridge. KMC acknowledges support from the NSF SBE SPRF, Kings College Cambridge, and the Cambridge Trust."
        },
        {
            "title": "References",
            "content": "Allen, K., Brändle, F., Botvinick, M., Fan, J. E., Gershman, S. J., Gopnik, A., Griffiths, T. L., Hartshorne, J. K., Hauser, T. U., Ho, M. K., et al. (2024). Using games to understand the mind. Nature Human Behaviour, pages 19. ARC Prize (2026). Arc-agi-3: The first interactive reasoning benchmark. Accessed: February 12, 2026. 16 Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. (2023). Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712. Burden, J., Cheke, L., Hernández-Orallo, J., Tešic, M., and Voudouris, K. (2024). Measurement layouts for capability-oriented AI evaluation. AAAI 2024 Tutorial. Presented at the 38th AAAI Conference on Artificial Intelligence (AAAI-24). Burghardt, G. M., Pellis, S. M., Schank, J. C., Smaldino, P. E., Vanderschuren, L. J., and Palagi, E. (2024). Animal play and evolution: Seven timely research issues about enigmatic phenomena. Neuroscience & Biobehavioral Reviews, 160:105617. Campbell, M., Hoane Jr, A. J., and Hsu, F.-h. (2002). Deep blue. Artificial intelligence, 134(1-2):57 83. Chater, N., Felin, T., Funder, D. C., Gigerenzer, G., Koenderink, J. J., Krueger, J. I., Noble, D., Nordli, S. A., Oaksford, M., Schwartz, B., et al. (2018). Mind, rationality, and cognition: An interdisciplinary debate. Psychonomic Bulletin & Review, 25(2):793826. Chen, E. K., Belkin, M., Bergen, L., and Danks, D. (2026). Does AI already have human-level intelligence? the evidence is clear. Nature, 650(8100):3640. Chu, J. and Schulz, L. E. (2020). Play, curiosity, and cognition. Annual Review of Developmental Psychology, 2(1):317343. Chu, J., Tenenbaum, J. B., and Schulz, L. E. (2024). In praise of folly: flexible goals and human cognition. Trends in Cognitive Sciences, 28(7):628642. Cleveland, A. A. (1907). The psychology of chess and of learning to play it. The American Journal of Psychology, 18(3):269308. Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. (2020). Leveraging procedural generation to In International conference on machine learning, pages benchmark reinforcement learning. 20482056. PMLR. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Collins, K. M. and Tenenbaum, J. B. (2026). Expert-level test is head-scratcher for AI. Collins, K. M., Todd, G., Wong, L., Zhang, C., Togelius, J., Weller, A., Chu, J., Griffiths, T., and Tenenbaum, J. (2025a). Generation and evaluation in the human invention process through the lens of game design. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 47. Collins, K. M., Zhang, C. E., Wong, L., da Costa, M. B., Todd, G., Weller, A., Cheyette, S. J., Griffiths, T. L., and Tenenbaum, J. B. (2025b). People use fast, flat goal-directed simulation to reason about novel problems. arXiv preprint arXiv:2510.11503. Feng, J., Spence, I., and Pratt, J. (2007). Playing an action video game reduces gender differences in spatial cognition. Psychological science, 18(10):850855. Genesereth, M., Love, N., and Pell, B. (2005). General game playing: Overview of the aaai competition. AI magazine, 26(2):6262. Gobet, F., Retschitzki, J., and de Voogt, A. (2004). Moves in mind: The psychology of board games. Psychology Press. Google (2026). Advancing AI benchmarking with game arena. Google DeepMind Blog. Accessed: February 12, 2026. Hafner, D. (2021). arXiv:2109.06780. Benchmarking the spectrum of agent capabilities. arXiv preprint Hendrycks, D., Song, D., Szegedy, C., Lee, H., Gal, Y., Brynjolfsson, E., Li, S., Zou, A., Levine, L., Han, B., et al. (2025). definition of agi. arXiv preprint arXiv:2510.18212. Hernández-Orallo, J., Baroni, M., Bieger, J., Chmait, N., Dowe, D. L., Hofmann, K., MartínezPlumed, F., Strannegård, C., and Thórisson, K. R. (2017). new AI evaluation cosmos: Ready to play the game? AI Magazine, 38(3):6669. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. (2023). Swebench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770. Kanervisto, A., Bignell, D., Wen, L. Y., Grayson, M., Georgescu, R., Valcarcel Macua, S., Tan, S. Z., Rashid, T., Pearce, T., Cao, Y., et al. (2025). World and human action models towards gameplay ideation. Nature, 638(8051):656663. Legg, S. and Hutter, M. (2007). Universal intelligence: definition of machine intelligence. Minds and machines, 17(4):391444. Lehrach, W., Hennes, D., Lazaro-Gredilla, M., Lou, X., Wendelken, C., Li, Z., Dedieu, A., GrauMoya, J., Lanctot, M., Iscen, A., et al. (2025). Code world models for general game playing. arXiv preprint arXiv:2510.04542. Leibo, J. Z., Dueñez-Guzman, E. A., Vezhnevets, A., Agapiou, J. P., Sunehag, P., Koster, R., Matyas, J., Beattie, C., Mordatch, I., and Graepel, T. (2021). Scalable evaluation of multi-agent reinforcement learning with melting pot. In International conference on machine learning, pages 61876199. PMLR. Li, Y., Lin, C., Nasir, M. U., Bontrager, P., Liu, J., and Togelius, J. (2025). Gvgai-llm: Evaluating large language model agents with infinite games. arXiv preprint arXiv:2508.08501. Lillard, A. S., Lerner, M. D., Hopkins, E. J., Dore, R. A., Smith, E. D., and Palmquist, C. M. (2013). The impact of pretend play on childrens development: review of the evidence. Psychological bulletin, 139(1):1. Magne, L., Awadalla, A., Wang, G., Xu, Y., Belofsky, J., Hu, F., Kim, J., Schmidt, L., Gkioxari, G., Kautz, J., et al. (2026). Nitrogen: An open foundation model for generalist gaming agents. arXiv preprint arXiv:2601.02427. Mitchell, M. (2024). Debates on the nature of artificial general intelligence. Nasir, M. U., James, S., and Togelius, J. (2024). Word2world: Generating stories and worlds through large language models. arXiv preprint arXiv:2405.06686. Newell, A., Simon, H. A., et al. (1972). Human problem solving, volume 104. Prentice-hall Englewood Cliffs, NJ. OpenAI (2016). Universe. https://openai.com/index/universe/. Archived content; Accessed: December 3, 2025. Paglieri, D., Cupiał, B., Coward, S., Piterbarg, U., Wolczyk, M., Khan, A., Pignatelli, E., Kucinski, Ł., Pinto, L., Fergus, R., et al. (2024). Balrog: Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543. Perez-Liebana, D., Liu, J., Khalifa, A., Gaina, R. D., Togelius, J., and Lucas, S. M. (2019). General video game ai: multitrack framework for evaluating agents, games, and content generation algorithms. IEEE Transactions on Games, 11(3):195214. Phan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Zhang, C. B. C., Shaaban, M., Ling, J., Shi, S., et al. (2025). Humanitys last exam. arXiv preprint arXiv:2501.14249. Schaul, T., Togelius, J., and Schmidhuber, J. (2011). Measuring intelligence through games. arXiv preprint arXiv:1109.1314. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489. 18 Smith, P. K. (1982). Does play matter? functional and evolutionary aspects of animal and human play. Behavioral and brain sciences, 5(1):139155. Spence, I. and Feng, J. (2010). Video games and spatial cognition. Review of general psychology, 14(2):92104. Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. (2023). Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on machine learning research. Todd, G., Earle, S., Nasir, M. U., Green, M. C., and Togelius, J. (2023). Level generation through large language models. In Proceedings of the 18th International Conference on the Foundations of Digital Games, pages 18. Todd, G., Padula, A. G., Stephenson, M., Piette, É., Soemers, D. J., and Togelius, J. (2024). Gavel: Generating games via evolution and language models. Advances in Neural Information Processing Systems, 37:110723110745. Tsividis, P. A., Loula, J., Burga, J., Foss, N., Campero, A., Pouncy, T., Gershman, S. J., and Tenenbaum, J. B. (2021). Human-level reinforcement learning through theory-based modeling, exploration, and planning. arXiv preprint arXiv:2107.12544. Turing, A. M. (1950). Computing machinery and intelligence. Mind, LIX(236):433460. Verma, V., Huang, D., Chen, W., Klein, D., and Tomlin, N. (2025). Measuring general intelligence with generated games. arXiv preprint arXiv:2505.07215. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. (2018). Glue: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP workshop BlackboxNLP: Analyzing and interpreting neural networks for NLP, pages 353355. Wang, Z., Li, X., Ye, Y., Fang, J., Wang, H., Liu, L., Liang, S., Lu, J., Wu, Z., Feng, J., et al. (2025). Game-tars: Pretrained foundation models for scalable generalist multimodal game agents. arXiv preprint arXiv:2510.23691. Warrier, A., Nguyen, D., Naim, M., Jain, M., Liang, Y., Schroeder, K., Yang, C., Tenenbaum, J. B., Vollmer, S., Ellis, K., et al. (2025). Benchmarking world-model learning. arXiv preprint arXiv:2510.19788. Xing, M., Zhang, R., Xue, H., Chen, Q., Yang, F., and Xiao, Z. (2024). Understanding the weakness of large language model agents within complex android environment. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 60616072. Ying, L., Collins, K. M., Sharma, P., Colas, C., Zhao, K. I., Weller, A., Tavares, Z., Isola, P., Gershman, S. J., Andreas, J. D., et al. (2025). Assessing adaptive world models in machines with novel games. arXiv preprint arXiv:2507.12821. Zhang, A. L., Griffiths, T. L., Narasimhan, K. R., and Press, O. (2025). Videogamebench: Can vision-language models complete popular video games? arXiv preprint arXiv:2505.18134. Zhou, L., Pacchiardi, L., Martínez-Plumed, F., Collins, K. M., Moros-Daval, Y., Zhang, S., Zhao, Q., Huang, Y., Sun, L., Prunty, J. E., et al. (2025). General scales unlock AI evaluation with explanatory and predictive power. arXiv preprint arXiv:2503.06378."
        },
        {
            "title": "A Discussion of Related Work",
            "content": "A.1 AI evaluation Measuring progress toward general intelligence has traditionally relied on curated benchmarks targeting specific tasks and cognitive domains, including language understanding, reasoning, mathematics, and programming. Large multi-task benchmark suites such as BIG-bench (Srivastava et al., 2023), GLUE (Wang et al., 2018), SWE-bench (Jimenez et al., 2023), and domain-focused reasoning datasets (Phan et al., 2025) enable standardized comparison across models but remain fundamentally static. Recent work attempts to broaden evaluation through capability-oriented frameworks and large collections of tasks. Cognitive taxonomies and measurement layouts aim to disentangle latent skills underlying model performance (Burden et al., 2024; Zhou et al., 2025), while interactive reasoning benchmarks in game-like environments such as ARC-AGI-3 (ARC Prize, 2026), Balrog (Paglieri et al., 2024), and AutumnBench (Warrier et al., 2025) introduce dynamic task structures. However, as models increasingly optimize directly for these benchmarks, concerns about saturation, contamination, and narrow capability measurement have been raised (Collins and Tenenbaum, 2026; Hendrycks et al., 2025). Our work builds on these directions by proposing an evaluation paradigm that continually generates new tasks that cover broad spectrum of human activities and skills. Rather than expanding static datasets, the AI GAMESTORE introduces living meta-benchmark grounded in diverse real human-designed games. A.2 General game playing Games have long served as central testbed for studying and measuring intelligence due to their well-defined rules, measurable performance, and diversity of cognitive demands (Schaul et al., 2011; ?; Gobet et al., 2004). Early milestones demonstrated superhuman performance in individual games such as chess and Go through search and reinforcement learning methods (Campbell et al., 2002; Silver et al., 2016). Recognizing the limitations of highly specialized systems trained for single environments, Genesereth et al. (2005) introduced the General Game Playing (GGP) paradigm, which evaluates agents across multiple games without game-specific engineering. Subsequent platforms such as GVGAI (Perez-Liebana et al., 2019) expanded this paradigm by providing diverse rule systems and environments that emphasize adaptability, transfer, and rapid learning. More recent work extends general game playing and evaluation to foundation models and LLM-based agents on diverse sets of video games (Zhang et al., 2025; Li et al., 2025; Wang et al., 2025; Lehrach et al., 2025). The Multiverse of Human Games and AI GAMESTORE extend this line of work by shifting the focus from synthetic environment families to the distribution of games created and played by humans. We argue that this constitutes stronger notion of general game playing: the objective is not merely to perform well across larger collection of tasks, but to acquire the capacity to learn and play the full space of human-designed games under human-like constraints. A.3 LLM-guided game generation Large language models have recently enabled automated generation of interactive environments, including code-based games, simulated worlds, and reinforcement learning tasks. Prior work demonstrates that LLMs can translate natural language descriptions into playable environments (Nasir et al., 2024), support procedural content generation such as level design (Todd et al., 2023), and generate full games through evolutionary or iterative pipelines (Todd et al., 2024). Hybrid approaches combine language models with world models or action models to support gameplay ideation and environment construction (Kanervisto et al., 2025). Generated environments have also been proposed as evaluation tools for evaluating AI models (Cobbe et al., 2020; Verma et al., 2025). While these approaches can generate large quantity of tasks, fully automated generation introduces challenges related to playability, representativeness, and evaluation validity. Generated tasks may lack meaningful structure, drift away from human-relevant activities, or fail to capture realistic cognitive demands. AI GAMESTORE builds on this line of work by using existing popular game concepts from gaming platforms and combining LLM-based generation with human-in-the-loop refinement. The pipeline anchors game generation in existing human game concepts that people enjoy and systematically 20 produces variants that preserve playability, diversity, and evaluative relevance. This positions LLMguided generation as mechanism for constructing scalable, open-ended evaluation suites that remain aligned with the distribution of human games that people create and enjoy."
        },
        {
            "title": "B Sourcing Games",
            "content": "For our first version of the AI GAMESTORE, we first sourced from the Apple AppStore Top 100 games for each of 5 game categories (action, adventure, casual, puzzle, board) in 15 different countries (USA, China, UK, Japan, South Korea, South Africa, Mexico, India, France, Germany, Turkey, Saudi Arabia, Vietnam, Australia, Brazil). In total that resulted in 7500 games. In addition, we sourced top 500 Indie games from Steam (Steam does not maintain per-country top chart). The final set of 100 games show diverse genre, as shown in Figure 10. While action games represent the largest cohort, the evaluation suite also features many puzzle and board games. Figure 10: Genre distribution of the final curated dataset. The chart displays the frequency of games across 14 distinct game categories listed under the games. Action and Casual are the most common categories."
        },
        {
            "title": "C Game Generation Spec",
            "content": "We require all games written in JavaScript with p5.js. Games may use three.js and matter.js for 3D effects and physics simulations. This enable us to express wide range of game mechanics with reasonably complex graphics. We require all games to be playable exclusively through keyboard presses. While we envision future versions of AI GAMESTORE to have games that require cursor movements, we restrict the output to key presses for the current AI GAMESTORE games such that the action space of the AI model is more limited. This then becomes multiple choice question for the model at each step. On the other hand, it is non-trivial to prompt models to use the cursor by outputting cursor trajectories. All games can be paused and resumed. This enables us to query AI models for actions in real-time games. While we argue that true generally intelligent model should be able to play the games as humans do, todays AI models have high latency for each API call, which limits the number of decisions they can make in each second. All games must include scoring mechanism, and the scores should increase as the player makes progress over the game. All games should have multiple levels with progressing degree of difficulty. This ensures that the game is neither too easy nor too hard, such that we can better quantify the performance of the players to inform comparison. For games where players can die or fail, there should be sufficient lives provided to allow players to learn and improve throughout the game session. Players can also reset the games at any moment in case the goal becomes no longer attainable."
        },
        {
            "title": "D Game Refinement",
            "content": "The interface for generating and iteratively refining games is depicted in Figure 9. In the central screen, the game is displayed, allowing for real-time interaction. human player can play the game in Human Mode or initiate automated testing. On the bottom right, the player can select target LLMsuch as Claude 4.5 Sonnetto refine the game. The human player provides natural language feedback to describe specific issues or requested features in the Feedback text area, and click Apply Fix to trigger the automated code refinement, which will re-render the game for the player. The iteration and refinement process continues until the player is satisfied with the game. In our game generation, all game iteration and refinement was performed by the coauthors on this paper, but we believe this can be easily scaled up by recruiting online participants to play and refine the games as next steps for generating AI GAMESTORE games. Figure 11: Interface for human-in-the-loop refinement"
        },
        {
            "title": "E Novel Game Variants Generation",
            "content": "Multiple novel variants of the game can be generated by augmenting the game mechanics using the interface above. This allows the AI GAMESTORE to generate large quantity of test games from few source games. In addition, we can carefully control the cognitive capability demand for each variant by manipulating the mechanics, which allows for more targeted model evaluation. As proof of concept, we recruited two human game players (both are undergraduate students at MIT) to play 30 of the base games and propose novel variant for each. We show an example in Figure 12. These games are not used in our model evaluation. Figure 12: Generating novel variants of games. As proof of concept, we recruited human players to play the base games and propose novel mechanics for variants in natural language, which is then implemented by an LLM to generate novel game variant. We show an example above: the game objective is to control the mouse to get all the cheeses while avoiding the cat. In the base game, the cat moves on one of the platforms. In the novel variant, the cat can move through space to chase the mouse but has an isovist view. This enables us to quickly generate large quantity of interesting games to evaluate AI models. At the same time, these novel variants enable us to control the demand profile of each game and stress test models. In the example above, the novel variant requires more sophisticated theory of mind reasoning and planning capabilities than the base game."
        },
        {
            "title": "F Game Annotation Rubrics",
            "content": "In this section we show the rubrics for annotating the cognitive demands for each game. These rubrics are inspired by previous work by Zhou et al. (2025). The use of rubrics in natural language for annotating several capabilities of task has several advantages: (1) It gives an interpretable, human-readable definition of the capability the construct under consideration; (2) It extracts the demand levels for each game in way that is independent from other games and from any player (human or machine); (3) It assumes no independence or any degree of correlation between the capabilities, allowing for flexibility in changes in the catalog of capabilities or observed correlations, without affecting the existing measures; and (4) It potentially allows for annotation automation (Zhou et al., 2025) using multimodal LLMs. Spatial-Temporal Coordination (ST) This criterion assesses the precision, timing, and sensorimotor integration necessary to navigate visual scene and interact with dynamic elements. It measures the demand for real-time responsiveness and the synchronization of motor outputs with moving visual stimuli. Level None (0) Description and Example The task is static or turn-based. There is no timing demand, and performance remains unaffected by the speed of execution. E.g., digital version of Solitaire or text-based RPG. 23 Very Low (1) Low (2) Intermediate (3) High (4) Very High (5) The task demands only basic, non-urgent interactions. Timing is extremely forgiving, and movement is slow, predictable, or linear. E.g., Clicking on large, slow-moving button that stays on screen for several seconds. Reactive timing is necessary as the user must respond to stimulus. Mistakes in timing are easily corrected and do not significantly impede overall progress. E.g., Navigating character through wide corridor with no obstacles. Success depends on well-timed actions to navigate environments with moderate complexity. Performance relies on hitting specific windows of opportunity or maintaining consistent rhythm. E.g., Flappy Bird or basic platforming jumps. Multi-dimensional coordination is essential. The user must manage speed, trajectory, and timing simultaneously across multiple moving targets or obstacles. E.g., fast-paced bullet hell shooter or 3D platformer needing mid-air adjustments. Frame-perfect precision and exceptional reflex integration are fundamental. Success depends on maintaining complex sequences of movement under high-velocity conditions where errors measured in milliseconds result in total failure. E.g., Professional-level F1 racing simulations or high-level competitive fighting games. Visual Processing (VP) This criterion assesses the ability to identify, match, and categorize objects based on visual properties. It progresses from simple detection of presence to the complex parsing of cluttered, 3D, or partially occluded environments. Level None (0) Very Low (1) Low (2) Description and Example The task does not rely on visual differentiation. Information is conveyed through other modalities such as audio or text. The task involves detecting the presence or absence of one or few objects and their general location. E.g., Detecting white square on black background. Simple parsing of object identity and properties is sufficient. The user can distinguish between few distinct shapes or colors, and high precision is not vital for success. E.g., Sorting large, colored blocks into matching bins. Intermediate (3) The task involves identifying and matching multiple objects based on precise propertiessuch as shape, size, color, and texturein 2D environment. E.g., Match-3 games like Bejeweled or Candy Crush. High (4) Very High (5) Successful navigation involves identifying objects despite occlusion, perspective shifts, or complex patterns in crowded environment. E.g., Hidden Object game with significant clutter and overlapping items. High-level visual inference is necessary. The user must parse complex, 3D, or partially observable scenes to make educated guesses about object identity and position using minimal visual cues. E.g., Identifying an enemys position in tactical shooter by seeing only sliver of shadow. Memory (ME) This criterion assesses the demand for retrieving and integrating information from previous states to inform current or future actions. It spans from immediate sensory persistence to the long-term synthesis of vast amounts of data. Level None (0) Very Low (1) Low (2) Intermediate (3) High (4) Very High (5) Description and Example The task is entirely reactive. All information needed to succeed is present in the current view or frame. E.g., simple reaction test where you click when light turns green. Successful completion involves holding basic information from the immediate past to inform future actions. However, this information is not critical for the success of the player. E.g., Remembering which direction character was just walking to continue the path. Remembering multiple pieces of information for short duration, often few seconds, is necessary. E.g., standard Match Two card memory game. The task involves integrating information across longer horizon, such as building mental map of small local area from exploration. E.g., Navigating maze where only the immediate surroundings are visible (Fog of War). Tracking multiple types of information across long durationssuch as inventory states, quest locations, and character statusis essential. E.g., Keeping track of multiple resource counts and enemy positions in RealTime Strategy (RTS) game. The retention and synthesis of vast, heterogeneous datasets across long timeframes are vital. This often involves recalling specific narrative details or mechanics encountered prior to solving current problems. E.g., An RPG where information from hours before affects the current decisionmaking. World Model Learning (WM) This criterion assesses the ability to infer hidden mechanics, rules, or causal relationships through active experimentation and hypothesis testing. Level None (0) Very Low (1) Low (2) Description and Example All rules and mechanics are explicitly stated, standardized, or immediately obvious. No discovery is necessary. E.g., standard Tic-Tac-Toe game. The environment is highly familiar. Minimal trial-and-error is needed to understand basic mechanics, leaving little uncertainty about how the system works. E.g., basic maze game with multiple colored exits. The environment contains few novel mechanics that can be easily discovered through casual interaction. E.g., Learning that specific type of button can unlock certain doors. 25 Intermediate (3) Success involves inferring hidden mechanics within limited search space. The user must use simple trial-and-error to understand how different elements interact. E.g., Learning to unlock door in particular, unusual way. High (4) Very High (5) The task presents novel system, prompting the user to discover unfamiliar mechanics through deliberate exploration and observation of patterns. E.g., Understanding complex crafting recipes in survival game without manual. The environment is highly novel and potentially counter-intuitive. Success depends on setting up epistemic goalsdeliberate experiments to validate or invalidate hypotheses about how the world works. E.g., Games like Outer Wilds, where the player must scientifically deduce physical laws to progress. Planning (PL) This criterion assesses the requirement for simulating future states and evaluating the outcomes of sequence of actions. It measures the depth of the search tree and the complexity of managing branching possibilities. Level None (0) Very Low (1) Low (2) Description and Example Actions are purely reactive or instinctive, as there is no advantage to thinking ahead. E.g., Whack-a-Mole. Thinking exactly one step ahead is necessary to avoid immediate negative outcomes. E.g., Moving out of the way of slow-moving projectile. Planning short, linear sequences (23 steps) with clear, singular goal is necessary for progress. E.g., Moving piece in Checkers to set up single jump. Intermediate (3) Simulating several steps ahead and evaluating few possible future outcomes are necessary to reach specific goal state. E.g., Solving Water Sort puzzle or medium-difficulty Sudoku. High (4) Very High (5) Deep search is fundamental requirement. The player must account for multiple branching possibilities and plan many moves into the future, often anticipating an opponents counter-moves. E.g., High-level Chess or Go. Strategic, multi-objective planning in dynamic or stochastic environments is essential. The user must balance long-term goals against immediate threats while accounting for uncertainty. E.g., Grand Strategy games where economic, military, and diplomatic plans are managed simultaneously. Physical Reasoning (PH) This criterion assesses the mental simulation of physical properties, such as gravity, trajectory, momentum, and material interactions. It measures the ability to predict how objects will behave according to physical laws. 26 Level None (0) Very Low (1) Description and Example The task follows abstract or symbolic rules with no physical components. E.g., crossword puzzle or math quiz. Basic awareness of physical laws such as solidity is sufficient for the task (e.g., characters cannot walk through walls). E.g., Navigating top-down RPG where walls block movement. Low (2) Understanding simple linear movement or basic gravity is necessary. E.g., Dropping an object and knowing it will fall straight down. Intermediate (3) High (4) Very High (5) The task requires simulating object trajectories. The user must make precise predictions of how an object will move through space to interact with other objects. E.g., Simple Angry Birds levels with few structures Reasoning about complex physical interactions with few basic variablessuch as leverage, friction, momentum transfer, or basic fluid dynamicsis necessary. E.g., Angry Birds with complex structures that may interact with each other The simultaneous integration of multiple physical variables (wind, mass, elasticity, torque) is necessary to predict outcomes in highly dynamic environments. E.g., Building and flying rocket in Kerbal Space Program. Social Reasoning (SO) This criterion assesses the cognitive demands associated with mind modeling and social cognition. The levels progress from tasks needing no mind modeling to those requiring reasoning about how the beliefs, desires, and emotions of multiple agents interact. Level None (0) Very Low (1) Low (2) Intermediate (3) Description and Example The task does not involve mind modeling or social cognition. It may not involve other agents, or if it does, interacting with them is not necessary for success. E.g., Solving Sudoku puzzle. Performance is improved through the detection of other agents. These agents often dont move or act in goal-directed way (e.g. periodic movements). Reasoning about observed behavior or attributing mental states to others is not necessary for good performance. E.g. platformer game where an enemy moves horizontally on particular platform. This task requires some basic intuition about the behaviour of others, but only minimal levels of mental state attribution. Good performance might be based on developing accurate associations between others responses and the stimuli that caused them. Note, this reasoning need not be explicit. This task goes beyond simple state-behaviour associations and involves attributing cognitive or affective states (i.e., mentalising). That is, it involves inferring and representing specific mental properties about others (they believe the moon landing was hoax, they want glass of water). The task may not, however, require explicit reasoning about these mental states (i.e., full-blown theory of mind). E.g. Recognizing that someone using rock to crack open coconut is trying to get to the food inside. 27 High (4) Very High (5) This task requires full theory of mind to be solved effectively. It requires not only the attribution of mental states to others, but explicit reasoning about those states. It may also require the integration of social knowledge and heuristics about normal agentic behaviour to accurately predict future behaviour. Importantly, this task also requires clear distinction between selfand other-related representations. E.g. typical false belief task (e.g. Sally Anne test) This task requires exceptional mind modelling and social cognition abilities. It goes beyond generating intuitive theories about another agent within dyadic interaction, and instead requires the combination of multiple theories of mind corresponding to the intentions, emotions, and beliefs of range of different agents. Expanding the scope of mind-modelling and social cognition to include multiple agents would enable more sophisticated forms of collaborative action. Tasks at this level may require an understanding of the complex networks and hierarchies that form within social groups E.g. Leading negotiation between multiple stakeholders where each party has different beliefs about others intentions and bottom lines, while managing the complex emotional dynamics between opposing personalities."
        },
        {
            "title": "G Comparing Public and All Games",
            "content": "We evaluate the representativeness of the 10 publicly released games against the full 100-game dataset. Overall, the comparison shows that the public subset serves as high-fidelity proxy for the broader benchmark, as evidenced by the comparable mean ratings for human-centric metrics such as \"Funness\" and \"Challengingness\" as well as model performance. Metric / Model 10 Public Games All 100 Games Participant Ratings (Mean SD) Average Funness Rating Average Challengingness Rating Model Performance (Geom Mean [95% CI]) GPT-5.2 Claude-Opus-4.5 Gemini-2.5-Pro Gemini-2.5-Flash GPT-5-mini Llama-4-Maverick Qwen-3-VL-32B 51.41 9.54 56.32 7.69 52.37 13.17 59.33 13.89 6.71 [2.67, 16.22] 5.91 [2.06, 16.90] 8.99 [4.08, 18.87] 6.27 [2.27, 14.05] 5.01 [1.67, 13.92] 4.74 [2.22, 9.12] 2.53 [1.16, 5.97] 8.26 [5.93, 11.28] 7.74 [5.50, 10.68] 7.49 [5.36, 10.28] 7.07 [5.07, 9.69] 6.13 [4.44, 8.39] 5.91 [4.26, 7.80] 4.68 [3.39, 6.41] Cognitive Demands (Mean SD) Visual Processing (VP) Spatial Temporal Coordination (ST) Memory (ME) Planning (PL) Word Model Learning (WM) Physical Reasoning (PH) Social Reasoning (SO) 2.40 0.70 2.10 1.52 0.60 0.97 2.20 1.23 0.80 1.32 1.30 0.95 0.90 0.99 2.23 0.63 2.04 1.54 0.20 0.72 0.95 1.17 0.52 1.06 0.96 1.20 0.50 0.81 Table 9: Comparing the 10 public games vs all 100 games on the AI GAMESTORE. Values for Model Performance are scaled by 100 to represent the percentage relative to the human median (100)."
        },
        {
            "title": "H Model Experiment Details",
            "content": "In each API call, the model is prompted to give list of actions to perform. The model is given list of possible keys to choose from. For each api call, the model is instructed to return list of 5 objects, where each object is list of action strings. Each object corresponds to the agent actions for 0.2 second period. The list of action strings indicate which keys are pressed for this 0.2 second period. 28 If multiple action strings are included, then all mentioned keys are pressed. For each key press, the model can also indicate regular key press (\"DOWN\") or hold key (\"HOLD_DOWN\"). regular key press indicates that the action is applied once, whereas HOLD option would indicate the action is continuously applied for the whole 0.2 second period. The model can always return \"RETRY\" to restart the level. Figure 13: The model evaluation harness for evaluating AI models on AI GAMESTORE games. The Model Prompt provides the AI with the current game state via screenshots, along with game descriptions, \"scratchpad\" for maintaining internal state, and history of previous actions and rationales. The model then processes this information to generate Model Response, which includes an updated scratchpad, determined sequence of actions, and rationale for those choices. These actions are then applied to the game environment, and the game is paused to capture the new state, completing the cycle for the next interaction step. Prompt for VLM actions at each gameplay step You are professional video game player tasked to win video game. You will read the description of the game and your previous actions and game state. You will then provide actions for the next 5 steps (Each step lasts for 0.2 seconds). [INSERT GAME DESCRIPTION, SCRATCHPAD, PREVIOUS ACTIONS and SCREENSHOTS] ** Output :** 1. Provide brief reasoning behind your actions ( < 10 sentences ) . 2. Output exactly 5 lists of actions . Each list represents 0.2 second time segment . - Each segment can contain : [ NOOP ] ( do nothing ) , single action like [ UP ] , or multiple simultaneous actions like [ UP , LEFT ] - Instant actions ( applied once at the start of the segment ) : \" UP \" , \" DOWN \" , \" LEFT \" , \" RIGHT \" , \" SPACE \" - Continuous actions ( held for the entire 0.2 seconds ) : \" HOLD_UP \" , \" HOLD_DOWN \" , \" HOLD_LEFT \" , \" HOLD_RIGHT \" , \" HOLD_SPACE \" - You can mix instant and continuous actions in the same segment , . . , [ UP , HOLD_LEFT ] applies UP once and holds LEFT for 0.2 - You can use \" \" to restart the game if it ends . Feel free to restart as many time as you want . ** Format your response as follows :** 29 < reasoning > [ INSERT YOUR THINKING ] </ reasoning > < keys > [[\" NOOP \"] , [\" HOLD_UP \" , \" HOLD_LEFT \"] , [\" NOOP \"] , [\" HOLD_UP \"] , [\" DOWN \"]] </ keys > < scratchpad > Provide scratchpad of your current understanding of the game state , your plan , and any important observations . This will be included in future API calls to help maintain context . </ scratchpad >"
        },
        {
            "title": "I Additional Experimental Results",
            "content": "The model median normalized score and geometric mean score across all 100 games are shown in Table 10. Model Median [95% CI] Geom Mean [95% CI] Gemini-2.5-Pro GPT-5.2 Claude-Opus-4.5 Gemini-2.5-Flash Llama-4-Maverick GPT-5-mini Qwen-3-VL-32B 12.40 [4.78, 18.10] 10.70 [5.57, 17.09] 8.63 [4.02, 17.86] 7.32 [4.03, 13.72] 6.50 [3.94, 11.31] 6.36 [2.01, 14.97] 3.50 [0.00, 7.41] 8.99 [4.08, 18.87] 6.71 [2.67, 16.22] 5.91 [2.06, 16.90] 6.27 [2.27, 14.05] 4.74 [2.22, 9.12] 5.01 [1.67, 13.92] 2.53 [1.16, 5.97] Table 10: Model median vs geometric mean performance on all 100 scores. Figure 14 show models performance on games that require low spatio-temporal coordination (demand score 2) in Figure 14. We did not observe any significant changes to top models performance (e.g. GPT-5.2 or GEMINI-2.5-PRO). This indicates that model failure is not solely due to slow reaction time for games. Figure 14: Model performance on games that require low spatial temporal coordination. We find that the results are not significantly different from the aggregate model performance."
        }
    ],
    "affiliations": [
        "Harvard University",
        "MIT",
        "Princeton University",
        "Universitat Politècnica de València",
        "University of British Columbia",
        "University of Cambridge"
    ]
}