{
    "paper_title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
    "authors": [
        "Chin-Yang Lin",
        "Cheng Sun",
        "Fu-En Yang",
        "Min-Hung Chen",
        "Yen-Yu Lin",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, a robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) a robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: https://linjohnss.github.io/longsplat/"
        },
        {
            "title": "Start",
            "content": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos Fu-En Yang2 Chin-Yang Lin1 Cheng Sun2 Min-Hung Chen2 Yen-Yu Lin1 Yu-Lun Liu1 1National Yang Ming Chiao Tung University 2NVIDIA Research 5 2 0 2 9 ] . [ 1 1 4 0 4 1 . 8 0 5 2 : r Figure 1. LongSplat achieves robust novel view synthesis from casually captured long videos without provided camera poses. Our approach jointly optimizes camera poses and 3D Gaussian Splatting, producing accurate and visually coherent reconstructions even under challenging conditions."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction LongSplat addresses critical challenges in novel view synthesis (NVS) from casually captured long videos characterized by irregular camera motion, unknown camera poses, and expansive scenes. Current methods often suffer from pose drift, inaccurate geometry initialization, and severe memory limitations. To address these issues, we introduce LongSplat, robust unposed 3D Gaussian Splatting framework featuring: (1) Incremental Joint Optimization that concurrently optimizes camera poses and 3D Gaussians to avoid local minima and ensure global consistency; (2) robust Pose Estimation Module leveraging learned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that converts dense point clouds into anchors based on spatial density. Extensive experiments on challenging benchmarks demonstrate that LongSplat achieves state-of-the-art results, substantially improving rendering quality, pose accuracy, and computational efficiency compared to prior approaches. Project page: https://linjohnss.github.io/longsplat/ High-quality 3D reconstruction and novel view synthesis (NVS) are essential for applications such as virtual reality, augmented reality, virtual tourism, and cultural heritage preservation. They also play crucial role in video editing tasks like stabilization, visual effects, and digital mapping for real estate or pedestrian-level navigation. With the widespread availability of smartphones and action cameras, casually captured videos have emerged as significant source of 3D content. Unlike professionally acquired datasets, casual videos present challenging characteristics: irregular camera trajectories, long sequences spanning hundreds or thousands of frames, and the absence of reliable camera poses or precise geometric priors. Addressing novel view synthesis (NVS) from casually captured videos poses two critical challenges: robust camera pose estimation over extended trajectories and efficient representation of large-scale scenes. Traditional methods rely on accurate poses from Structure-from-Motion (SfM) preprocessing, yet as shown in Fig. 2, pipelines like COLMAP [51] 1 frequently fail in casual settings. COLMAP-free methods, such as CF-3DGS [14], often encounter severe memory constraints, limiting their effectiveness for large-scale scenarios. Similarly, methods like LocalRF [39] struggle with complex camera trajectories, resulting in fragmented reconstructions. Foundation models like MASt3R [27] provide fast initial estimates but suffer inaccuracies and drift in long videos, severely affecting reconstruction quality. To address these limitations, we introduce LongSplat, robust unposed 3D Gaussian Splatting (3DGS) [22] framework designed specifically for casual long videos. As illustrated in Fig. 1, LongSplat achieves accurate novel view synthesis without relying on provided camera poses. LongSplat departs from traditional pipelines by jointly optimizing camera poses and 3DGS in unified framework. It integrates correspondence-guided Pose Estimation Module with 3DGS geometry and photometric refinements to improve pose accuracy, even under large-scale and unstructured camera motion. Furthermore, an efficient Octree Anchor Formation mechanism converts dense point clouds into anchors based on spatial density, significantly reducing memory usage while retaining detailed scene structures. These components work together in an incremental joint optimization strategy that avoids local minima and ensures global geometric consistency across extensive sequences. Extensive experiments on challenging datasets, including Tanks and Temples, Free, and Hike datasets, demonstrate that LongSplat consistently outperforms existing approaches, significantly improving rendering quality and pose accuracy. Compared to conventional methods shown in Fig. 2, LongSplat produces clearer and more coherent reconstructions, effectively addressing pose drift and memory limitations and substantially advancing the state-of-the-art. The main contributions of our work are: An incremental joint optimization approach for simultaneous camera pose and 3DGS reconstruction, reducing local minima and ensuring global consistency. robust pose estimation module leveraging learned 3D priors for accurate camera pose estimation. An adaptive Octree Anchor Formation strategy that significantly reduces memory usage while preserving reconstruction quality. 2. Related Work Novel View Synthesis. Novel View Synthesis (NVS) generates new perspectives from captured images, evolving from early pixel interpolation methods [8] to depth-based warping techniques [28] and 3D reconstruction-based rendering [6, 12]. Various representations have been explored, including planes [17, 18], meshes [20, 48, 49], point clouds [68, 75], and multi-plane images [29, 58, 77]. Neural Radiance Fields (NeRF) [40] revolutionized photorealistic rendering, with subsequent improvements in antiFigure 2. Novel view synthesis for casual long videos. Existing methods encounter significant challenges when reconstructing scenes from casually captured long videos: COLMAP [51] fails due to incorrect camera pose estimation, CF-3DGS [14] suffers from out-of-memory issues, LocalRF [39] struggles with complex trajectories, and MASt3R [27]+Scaffold-GS [36] provides inaccurate poses leading to degraded rendering quality. In contrast, LongSplat robustly handles these challenges, yielding accurate camera poses and high-quality novel view synthesis without memory constraints. aliasing [24, 74], reflectance [1, 59], sparse view training [24, 43, 67, 69], faster training [41, 45, 50], and rendering speed [15, 34, 54, 72]. Recent works have extended NeRF to few-shot scenarios without learned priors [32], domain-specific applications such as autonomous driving environments [52], and dynamic scenes with human pose variations [38]. Point-based methods [22, 37, 68, 75], particularly 3D Gaussian Splatting (3DGS) [22], enable real-time rendering through explicit representations. Recent advances have extended 3DGS capabilities to dynamic specular scenes with physically-based rendering [13], developed compression techniques for efficient storage and transmission [73], and improved robustness for unconstrained image scenarios [19]. However, most approaches still rely on pre-computed camera parameters from SfM [16, 32, 42, 51, 56]. Unposed Novel View Synthesis. Recent work has aimed to eliminate camera estimation preprocessing. i-NeRF [71] predicts camera poses using pre-trained NeRF. NeRFmm [65] jointly optimizes NeRF and camera poses for forward-facing scenes, with SiNeRF [66] offering improvements. BARF [31] and GARF [10] address gradient inconsistency through coarse-to-fine positional encoding but require good initialization. Advanced approaches [5, 9, 35, 39] leverage pre-trained networks for geometric priors, with NoPe-NeRF [5] incorporating monocular depth priors and CF-3DGS [14] using progressive optimization. Recent methods have improved robustness in joint optimization of camera poses and scene geometry using decomposed low-rank tensorial representations [7] and dynamic radiance fields [35]. These methods typically assume small pose perturbations [10, 31], limited camera motion [65, 66], or additional priors [5, 11, 21, 39], struggling with challenging trajectories, like Free dataset[26, 44]. Large-scale Novel View Synthesis. Extending NVS to largescale environments introduces memory and computational 2 Figure 3. Overview of the LongSplat framework. Given casually captured long video without known poses, LongSplat incrementally reconstructs the scene through tightly coupled pose estimation and 3D Gaussian Splatting. (a) Initialization converts MASt3R [27] global aligned point cloud into an octree-anchored 3DGS. (b) Global Optimization jointly refines all camera poses and 3D Gaussians for global consistency. (c) Pose estimation estimates each new frame pose via correspondence-guided PnP, applies photometric refinement, and updates octree anchors using unprojected points. If PnP fails, fallback triggers global re-optimization to recover. (d) Incremental Optimization alternates between Local Optimization within visibility-adapted window and periodic Global Optimization to propagate consistent updates across frames. (e) All optimization stages leverage unified objective composed of photometric loss, depth loss, and reprojection loss to ensure accurate geometry and appearance reconstruction. challenges that NeRFs implicit global representation struggles with. Recent research employs scene partitioning strategies for managing large scenes [23, 55, 57]. Progressive optimization techniques have been developed for robust view synthesis in large-scale scenes from casually captured videos [39]. At the same time, MVS-based approaches have been enhanced to handle generalizable view synthesis at scale [53]. For indoor environments, methods like GenRC [30] enable room-scale 3D reconstruction from sparse image collections. 3DGS offers explicit representation advantages through Gaussian primitive. VastGaussian [33] divides scenes into separately optimized blocks[22]. Scaffold-GS [36] introduces anchor-based Gaussian representation with fixed-resolution grids, though it requires SfM initialization. Octree-GS [47] implements fixed-level octrees with preset resolutions but similarly depends on SfM. Unlike these approaches, our method dynamically adjusts voxel size based on point cloud density, without dependency on SfM, and addresses unposed, large-scale, casually captured videos through adaptive Octree Anchor Formation. Casual Long Videos. Casual long videos present unique challenges: free-moving trajectories, lack of pose information, and continuously expanding scenes. LocalRF [39] addresses these through progressive localized field construction but suffers from slow training and fragmentation under irregular camera movements. 3D Foundation Models [60], including DUSt3R [63], MASt3R [27], Fast3r [70], and CUT3R [62], estimate poses and geometry directly but accumulate errors in long sequences. LongSplat treats foundation model outputs as soft priors, jointly optimizing them with 3D Gaussian Splatting while progressively correcting poses and geometry through combined PnP and optimization strategies. 3. Method LongSplat reconstructs long video sequences with unknown camera poses and unconstrained trajectories through fully incremental pipeline based on octree-anchored 3D Gaussian Splatting. The process begins with octree anchor formation, where per-frame dense point clouds are structured into an adaptive representation. Next, camera poses are estimated and refined using correspondence-guided initialization and photometric alignment. Finally, the reconstruction alternates between local optimization, which updates Gaussians within visibility-adapted window, and global refinement, which ensures long-term consistency. This design allows LongSplat to robustly handle long, unconstrained trajectories while adapting to scene complexity and minimizing drift. 3.1. Preliminaries Gaussian Splatting. 3D Gaussian Splatting (3DGS) [22] represents the scene as set of 3D Gaussians, each defined by center µ R3, covariance matrix Σ, color, scale, rotation, and opacity. The covariance is factorized into rotation SO(3) and diagonal scale matrix S, giving: G(x) = 1 2 (xµ)Σ1(xµ), Σ = RSSR. (1) This parameterization allows each Gaussian to adaptively capture local scene geometry. To render the scene, each Gaussian is projected onto the image plane using the camera pose , resulting in 2D Gaussian with covariance Σ2D = JW ΣW , where is the Jacobian of the projective transformation. The final rendered color and depth are computed via alpha blending: 3 Figure 4. Visualization of our proposed Octree Anchor Formation strategy. Given an initial sparse voxelized point cloud, we iteratively perform density-guided adaptive voxel splitting and pruning. Voxels with point cloud density (ρ) exceeding threshold are split, while those with density below the threshold are pruned. Repeated across multiple octree levels, this adaptive octree anchor design significantly reduces memory usage, allowing efficient representation and rendering of large-scale scenes. = (cid:88) i=1 i1 (cid:89) ci αi (cid:0)1 αj (cid:1), = j=1 (cid:88) i=1 i1 (cid:89) (cid:0)1 αj (cid:1), di αi j=1 (2) where ci and αi denote the color and opacity of the i-th Gaussian, respectively. di denotes the depth value along the ray at the Gaussians center. Anchor-based 3D Gaussian Splatting. To enhance memory efficiency and robustness in large scenes, Scaffold-GS [36] introduces the anchor-based 3DGS representation. Instead of directly maintaining individual Gaussians, the scene is first divided into sparse voxels, each acting as an anchor. From each anchor, Gaussians are initialized with positions relative to the anchor center: {µ0, µ1, . . . , µk1} = xv + {O0, O1, . . . , Ok1} lv, (3) where xv denotes the anchor position, {Oi} are offset vectors, and lv is scaling factor. Each Gaussians opacity, rotation, scale, and color are decoded from an anchor feature through lightweight MLPs. For opacity, the formulation is: {α0, α1, . . . , αk1} = Fα( ˆfv, vc, ˆdv), (4) where Fα is an MLP taking the anchor feature ˆfv, the relative view distance vc, and the view direction ˆdv as inputs. Anchor Initialization. In traditional Scaffold-GS, initial anchors are derived from sparse SfM point clouds. Points are voxelized to form anchor centers: ϵ, }, = {v = ϵ where is the SfM point cloud and ϵ is the voxel size. Each anchor holds local feature, managing its associated Gaussians. This design ensures structured densification and pruning, adapting Gaussian density to scene complexity and improving both memory and rendering efficiency. (5) 3.2. Octree Anchor Formation In large-scale casual video settings, memory efficiency and scene adaptability are essential. Our Octree Anchor Formation dynamically adjusts spatial resolution based on observed geometry, enabling scalable and redundant-free anchor management. LongSplat constructs structured anchors from MASt3Rs per-frame dense point clouds using an adaptive octree (Fig. 3 (a)). Unlike Scaffold-GS, which relies Figure 5. Detailed illustration of our camera pose estimation. (a) PnP initialization: Given correspondences between the predicted 3D anchor points from frame Ti1 and the 2D keypoints detected in frame Ti we employ PnP with RANSAC to robustly estimate an initial camera pose. (b) Pose refinement: The estimated pose is further refined by rasterizing the 3DGS scene and iteratively minimizing reprojection error to enhance pose accuracy. (c) Anchor unprojection: Newly observed regions are detected via an occlusion mask, computed by forward-warping the previous frames rendered depth. These regions are unprojected into 3D and converted into anchors via Octree Anchor Formation. on fixed-resolution grid, we progressively subdivide space based on local point density. Each point cloud = {pi} is voxelized into sparse grid at resolution ϵ0. Voxels exceeding density threshold τsplit split into 8 smaller voxels: ϵl+1 = 1 2 ϵl. (6) This process repeats up to maximum level L. Low-density voxels (density ρv < τprune) are removed to reduce redundancy  (Fig. 4)  . Each anchor inherits spatial scale proportional to its voxel size, ensuring coarse anchors for sparsely observed areas and finer anchors for detailed regions: sv ϵv. (7) To further prevent unnecessary duplication, newly generated anchors are compared to existing ones. If significant spatial overlap exists, the new anchor is discarded. This density-adaptive, duplication-free octree formation ensures compact memory usage while preserving adaptive resolution across scenes. 3.3. Pose Estimation module Accurate and robust camera pose estimation is essential for consistent reconstruction in unposed long video settings. We estimate each pose using 2D-3D correspondences derived from MASt3R, followed by photometric refinement against the current 3D Gaussian scene to maintain coherence across evolving 3D structures (Fig. 3 (c)). For each new frame t, MASt3R provides 2D corresponi)} between frame and 1, allowing backdences {(xi, projection of matched points xi to 3D via: Xi = Dt1(xi) 1 xi. (8) These 2D-3D correspondences {(x i, Xi)} are used to solve the initial pose Tt via PnP (Fig. 5 (a)), followed by photometric refinement that minimizes (Fig. 5 (b)): 4 proposed Octree Anchor Formation (Fig. 3 (a). When camera intrinsics are unavailable, we directly adopt MASt3Rs estimated focal length. Global Optimization. After initialization, we jointly optimize all 3D Gaussian parameters and camera poses across all processed frames (Fig. 3 (b)). This global optimization ensures geometric consistency across the entire sequence, reducing accumulated pose drift and local misalignments. Frame Insertion and Pose Estimation. As new frames arrive, we estimate their poses using the correspondenceguided PnP initialization and refinement strategy described in Sec. 3.3. If PnP fails due to insufficient feature correspondences or poor initialization, we trigger fallback mechanism that re-optimizes all past frames globally before retrying pose estimation. This iterative fallback enhances robustness under challenging motion or weak texture (Fig. 3 (c)). Local Optimization with Visibility-Adaptive Window. Once the pose is estimated, we optimize only the Gaussians visible in the new frames frustum, while constraining them with observations from nearby frames in dynamically selected visibility-adapted local window  (Fig. 6)  . Covisibility between frames is measured by: IoU(t, t) = V(t) V(t) V(t) V(t) , (12) where V(t) denotes the set of Gaussians visible in frame t. Frames with covisibility below threshold τ are excluded from the window. This adaptive mechanism ensures local Gaussians are consistently supervised by reliable multi-view constraints, balancing efficiency and accuracy. Final Global Refinement. In the final step, final global refinement jointly optimizes all Gaussians and camera poses over the sequence. This final pass further improves both rendering quality and long-range pose consistency. Depth and Reprojection Losses. To provide additional supervision in newly revealed regions, where multi-view observations are insufficient, we introduce two regularization terms. monocular depth loss encourages rendered depth to match MASt3Rs scale-aligned depth prior: Ldepth = Drendered DMASt3R2. Additionally, keypoint reprojection loss enforces alignment between projected 3D keypoints and their 2D observations: (13) Lreprojection = (cid:88) π(Xk) uk2, (14) where π() denotes projection using the current pose. Total Loss. Throughout the entire incremental reconstruction pipeline, each processed frame is optimized using the following objective: Ltotal = Lphoto + λdepthLdepth + λreprojectionLreprojection, (15) This combined loss applies to both local and global optimization stages, ensuring coherent multi-view, robust pose refinement, and stable geometry reconstruction across the evolving scene. Figure 6. Illustration of our Visibility-Adapted Local Window strategy for local optimization. To ensure balanced training of the 3D Gaussians, we dynamically define the optimization window based on anchor visibility overlap. Specifically, we compute the Intersection-over-Union (IoU) of visible anchors between consecutive views. Suppose the visibility IoU is below certain threshold (a). In that case, the local optimization window is adjusted by removing the earliest frame, iteratively repeating until suitable window with IoU above the threshold is found (b). This approach ensures balanced training coverage and enhances local reconstruction details during optimization (c). Lphoto = (cid:88) pΩ It(p) ˆIt(p)2, (9) where It is the observed frame and ˆIt is the rendering using the current 3DGS. This ensures the pose aligns with the evolving scene. To correct MASt3Rs depth scale drift, we compute scale factor ˆst by comparing the rendered depth Dt1 and MASt3Rs aligned depth Dalign : ˆst = Dt1, Dalign , Dalign Dalign . (10) This rescaled depth ensures consistent scale across frames. As the camera moves, newly visible regions are detected via an occlusion mask Mocc, derived by forward-warping Dt1 to frame and comparing it to the rescaled depth DMASt3R (Fig. 5 (c)). Newly visible pixels are unprojected into 3D using: pi = DMASt3R t,ui K1ui. (11) These new points are converted into octree anchors using the Octree Anchor Formation described in Sec. 3.2, with overlapping anchors removed to avoid redundancy (Fig. 5 (c)). This process incrementally expands the scene while maintaining structural regularity. 3.4. Incremental Joint Optimization To handle casually captured long videos, LongSplat adopts progressive incremental optimization framework that alternates between per-frame local reconstruction and crossframe global consistency refinement. Initialization. We begin with small set of initial frames. Camera poses and dense point clouds for these frames are estimated using MASt3R [27], followed by converting the point cloud into an initial octree-anchored 3DGS using the Table 1. Quantitative comparison on the Free dataset [61] across various baseline methods. Methods such as CF-3DGS [14] frequently encounter out-of-memory issues, denoted by -. Our method consistently outperforms all baselines across diverse scenes, delivering superior rendering quality and robustness, especially in challenging environments characterized by complex camera trajectories and varied geometric structures. *: Initialized with MASt3R poses, then jointly optimized. Scenes COLMAP [51] + F2-NeRF [61] COLMAP [51] + Scaffold-GS [36] MASt3R [27] + Scaffold-GS [36] MASt3R [27] + Scaffold-GS [36]* CF-3DGS [14] NoPe-NeRF [5] LocalRF [39] Ours PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 23.44 Grass Hydrant 23.75 24.34 Lab 28.05 Pillar 26.03 Road 25.10 Sky 28.14 Stair Avg. 25.55 0.58 0.74 0.83 0.79 0.80 0.86 0.84 0.78 0.45 0.28 0.26 0.23 0.27 0.24 0.22 0. 26.75 26.66 28.27 31.75 30.45 28.34 32.13 29.19 0.82 0.86 0.92 0.90 0.92 0.92 0.93 0.90 0.20 0.12 0.10 0.12 0.10 0.12 0.10 0. 22.65 23.22 20.66 23.95 24.23 23.26 23.35 23.05 0.61 0.71 0.74 0.70 0.73 0.80 0.71 0.72 0.34 0.21 0.25 0.28 0.25 0.22 0.30 0. 25.06 25.68 22.42 22.88 25.05 25.37 24.46 24.42 0.79 0.83 0.80 0.67 0.78 0.88 0.79 0.79 0.21 0.12 0.18 0.24 0.27 0.14 0.28 0. - - - 14.55 - - 13.41 13.98 - - - 0.40 - - 0.41 0.41 - - - 0.66 - - 0.63 0. 16.39 17.94 17.42 18.88 17.48 16.18 19.14 17.63 0.27 0.43 0.52 0.44 0.44 0.51 0.47 0.44 0.81 0.66 0.63 0.75 0.79 0.65 0.69 0. 18.84 19.19 17.22 22.98 20.68 18.76 23.55 20.17 0.35 0.48 0.55 0.59 0.54 0.60 0.66 0.54 0.60 0.48 0.47 0.49 0.56 0.46 0.38 0. 26.16 24.69 27.11 30.44 27.73 28.07 31.00 27.88 0.80 0.79 0.87 0.88 0.84 0.91 0.89 0.85 0.22 0.18 0.15 0.16 0.20 0.13 0.16 0. Figure 7. Qualitative comparison on the Free dataset [61]. We compare our method with state-of-the-art approaches including NoPeNeRF [5], LocalRF [39], CF-3DGS [14], and MASt3R [27] combined with Scaffold-GS [36]. CF-3DGS fails due to memory constraints (OOM), and other baseline methods exhibit artifacts or blurry reconstructions. In contrast, our method produces results closest to the ground truth, demonstrating clearer details, accurate geometry, and visually consistent rendering, particularly under challenging scene structures and complex camera trajectories. *: Initialized with MASt3R poses, then jointly optimized. 4. Experiments 4.1. Experimental Setup Datasets. We evaluate LongSplat on three challenging realworld datasets with varying difficulty levels: Tanks and Temples [25] (Standard): Eight scenes with smooth, forward-facing camera trajectories, evaluated at full resolution. Every 8th frame is used for testing. Free Dataset [61] (Moderate): Seven handheld videos featuring complex, unconstrained trajectories with multiple foreground objects, evaluated at 1/2 resolution. Frequent scene changes make memory-efficient 3D representation essential. Every 8th frame is tested. Hike Dataset [39] (Hard): Long videos with hundreds to thousands of frames, complex trajectories, and detailed geometry, evaluated at 1/4 resolution. The scale and duration demand adaptive memory management. Every 10th frame is used for testing. Evaluation Metrics. We evaluate novel view synthesis quality using PSNR, SSIM [64], and LPIPS [76]. Pose accuracy is measured with Absolute Trajectory Error (ATE) and Relative Pose Error (RPE), using COLMAP poses as ground truth. We also report model size, training time, and FPS to assess computational efficiency. Table 2. Quantitative evaluation of camera pose estimation accuracy on the Free dataset [61]. Our method achieves superior performance across most scenes, significantly reducing pose errors compared to state-of-the-art approaches. *: Initialized with MASt3R poses, then jointly optimized. Method RPEt RPEr ATE MASt3R [27] + Scaffold-GS [36] MASt3R [27] + Scaffold-GS [36]* CF-3DGS [14] NoPe-NeRF [5] LocalRF [39] Ours 0.162 0.083 0.234 6.231 0.754 0.028 0.265 0.176 3.442 4.822 7.086 0.103 0.013 0.008 0.022 0.576 0.035 0.004 Baselines. We compare LongSplat with COLMAP-based methods (COLMAP [51]+F2-NeRF [61] / 3DGS [22] / Scaffold-GS [36]) and unposed methods (NoPe-NeRF [5], LocalRF [39], CF-3DGS [14]). Additionally, we evaluate naıve baseline combining MASt3Rs [27] predicted point cloud and poses with Scaffold-GS. During training, camera poses are either fixed (MASt3R + Scaffold-GS) or jointly optimized (MASt3R + Scaffold-GS*). Implementation Details. We implement LongSplat based on Scaffold-GS [36], using its learning rate schedule and growing/pruning rules. Each anchor emits Gaussians predicted by lightweight 2-layer MLP. The initial sparse voxel 6 Table 4. Quantitative evaluation on the Hike dataset [39]. Our method consistently outperforms baselines across diverse scenes with complex trajectories and extended sequences, highlighting LongSplats robustness and superior scene representation capability. CF-3DGS [14] encounters OOM in all scenes and is thus omitted."
        },
        {
            "title": "Method",
            "content": "PSNR SSIM LPIPS MASt3R [27] + Scaffold-GS [36] MASt3R [27] + Scaffold-GS [36]* LocalRF [39] Ours 17.30 17.90 23.56 25.39 0.42 0.44 0.68 0. 0.52 0.50 0.29 0.19 Table 5. Ablation on training components. Removing pose estimation, global optimization, or local optimization significantly degrades performance, highlighting each modules importance. Our full method achieves the best rendering quality and pose accuracy. Method PSNR SSIM w/o Pose estimation w/o Global optimization w/o Local optimization w/o Refinement Ours 20.19 20.50 25.94 26.08 27.88 0.56 0.58 0.77 0.80 0.85 LPIPS RPEt RPEr ATE 0.71 0.01 0.01 0.01 0.00 0.42 0.12 0.06 0.04 0.03 0.51 0.41 0.28 0.25 0.17 2.71 0.50 0.31 0.22 0. Free Dataset. We evaluate LongSplat on the challenging Free dataset, achieving superior reconstruction quality as shown in Tab. 1 and Fig. 7. Competing methods like CF3DGS often face OOM issues, while LocalRF produces fragmented geometry and pose drift. Although MASt3R + Scaffold-GS avoids OOM errors, its inaccurate global pose estimates from MASt3R result in blurred renderings and structural distortions. Our method also achieves consistently lower pose errors than baselines, as shown quantitatively in Tab. 2 and visually in Fig. 8. Hike Dataset. We evaluate LongSplat on the challenging Hike dataset, achieving state-of-the-art reconstruction quality as shown in Tab. 4 and Fig. 10. Competing methods like CF-3DGS often fail due to OOM issue, LocalRF produces lower-quality reconstructions, and MASt3R struggles with long outdoor trajectories, resulting in poor reconstruction quality. 4.3. Ablation Studies Training Components. To analyze the contribution of each training component, we individually disable them and evaluate performance. As shown in Tab. 5, removing pose estimation severely harms reconstruction quality and increases pose errors (ATE: 0.71). Omitting global or local optimization also reduces performance. Our full method achieves the highest quality and minimal pose errors. Local Window Sizes. We analyze the effect of local window size on reconstruction and pose accuracy in Tab. 6. Small fixed-size windows (e.g., 1 frame) lack sufficient constraints, causing fragmentation and higher errors. Our visibility-adapted window achieves the best balance, yielding the highest reconstruction quality and lowest pose drift. Anchor Unprojection Strategies. We compare our adaptive octree anchor formation to (1) per-pixel initialization, (2) Figure 8. Visualization of camera trajectories on Free dataset [61]. CF-3DGS [14] encounters OOM and fails for long sequences, whereas our method reliably estimates accurate, stable trajectories, demonstrating superior robustness. Table 3. Quantitative evaluation of novel view synthesis quality on the Tanks and Temples dataset [25]. Our proposed LongSplat consistently surpasses existing methods across multiple scenes. Method PSNR SSIM LPIPS RPEt RPEr ATE COLMAP+3DGS [22] MASt3R [27] + Scaffold-GS [36] MASt3R [27] + Scaffold-GS [36]* NoPe-NeRF [5] CF-3DGS [14] Ours 30.21 28.67 30.92 26.34 31.28 32.83 0.92 0.79 0.90 0.74 0.93 0.94 0.10 0.21 0.13 0.39 0.09 0.08 0.166 0.047 0.080 0.041 00.032 0.168 0.103 0.038 0.069 0. 0.006 0.005 0.006 0.004 0.003 Figure 9. Qualitative comparison on the Tanks and Temples dataset [25]. NoPe-NeRF [5] produces visibly blurred results with inaccurate geometries, while CF-3DGS [14], despite better sharpness, fails to reconstruct fine details accurately. In contrast, our LongSplat method achieves superior rendering quality, closely matching the ground truth with sharper textures, more accurate geometry, and consistent lighting. grid size is 0.1. Camera poses are optimized via differentiable CUDA-accelerated rasterizer, parameterized with quaternions and translation vectors. We use 400 local, 900 global, and 10,000 refinement iterations, starting with three initial frames. The octree density thresholds for splitting and removal start at 10 and 5, progressively increasing with depth. Visibility IoU threshold is set to 0.2. All experiments are conducted on single NVIDIA RTX 4090. 4.2. Comparisons Tanks and Temples. We evaluate LongSplat on the Tanks and Temples dataset [25], standard benchmark for novel view synthesis. As shown in Tab. 3, LongSplat achieves state-of-the-art rendering quality (avg. PSNR: 32.83 dB) and superior camera pose estimation accuracy (lowest ATE and RPE). Qualitative results in Fig. 9 confirm sharper textures, accurate geometry, and better visual consistency compared to baselines. Please refer to the supplementary material for the full quantitative evaluation table for each scene. Figure 10. Qualitative results on the Hike dataset [39]. Compared to existing methods such as LocalRF [39] and MASt3R [27]+ScaffoldGS [36], our approach significantly improves visual clarity and reconstruction fidelity, accurately capturing complex details and textures in challenging scenes captured during long, casual outdoor trajectories. Notably, our method better preserves structural details and reduces artifacts, demonstrating enhanced robustness and visual quality. *: Initialized with MASt3R poses, then jointly optimized. Table 6. Ablation on local window sizes. Fixed small windows (e.g., 1-frame or 5-frame) or global optimization degrades reconstruction quality and pose accuracy. Our visibility-adaptive window dynamically selects optimal context, achieving the best balance of local detail and global consistency. Window size 1-frame (Minimum Window) 5-frame (Fixed Window) All Frames (Global Optimize) Ours (Visibility-Adaptive) PSNR SSIM LPIPS RPEt RPEr ATE 0.01 26.58 0.01 26.90 0.08 26.15 0.00 27.88 0.21 0.18 0.28 0. 0.80 0.82 0.78 0.85 0.05 0.04 0.06 0.03 0.23 0.22 0.26 0.17 (a) ATE (b) RPEt Figure 11. Robustness analysis on camera pose estimation (Free dataset [61]). We plot cumulative error distributions for ATE, RPE translation, and rotation. Our method consistently achieves lower errors compared to existing methods, demonstrating superior robustness and reduced pose drift. (c) RPEr Table 7. Ablation on anchor unprojection strategies. Our Adaptive Octree method achieves the best rendering quality and lowest perceptual errors, significantly reducing memory usage (7.92 compression) compared to baselines. Method PSNR SSIM LPIPS Size (MB) Compress Per-pixel Unprojection (Dense) Fixed-size Voxel Unprojection Naive Densification Ours (Adaptive Octree) 22.47 26.99 25.73 27.88 0.69 0.81 0.75 0.85 0.35 0.18 0.31 0.17 799 591 63 101 1.00x 1.35x 12.66x 7.92x Table 8. Comparison of training efficiency on the Free dataset. Our method significantly reduces training time and achieves dramatically higher throughput (FPS) while simultaneously maintaining compact model size compared to state-of-the-art approaches. Method FPS Training time Size (MB) NoPe-NeRF [5] LocalRF [39] CF-3DGS [14] Ours 0.29 1.17 9.81 281.71 36 hr 14 hr 2 hr 1 hr 7 1080 1966 101 fixed-resolution voxels, and (3) naıve densification in Tab. 7. Our method achieves superior reconstruction quality with significantly reduced memory usage (7.92 compression). Training Efficiency. We evaluate the computational efficiency of LongSplat (Tab. 8), which achieves 281.71 FPS and trains in just 1 hour on an NVIDIA RTX 4090, nearly 30 faster than LocalRF. Our method also significantly re8 duces the model size to approximately 101 MB. Robustness Analysis of Camera Pose Estimation. We further analyze robustness by comparing cumulative error distributions for ATE and RPE (translation and rotation) in Fig. 11. LongSplat achieves consistently lower errors than baselines, effectively minimizing drift and maintaining stable trajectories, highlighting the advantage of our incremental optimization and robust tracking. 5. Conclusion We present LongSplat, robust unposed 3D Gaussian Splatting framework for casual long videos. It integrates incremental joint optimization, robust tracking module, and adaptive octree anchors, significantly improving pose accuracy, reconstruction quality, and memory efficiency. Extensive experiments confirm that LongSplat consistently outperforms state-of-the-art approaches. Future work includes handling dynamic scenes and enhancing pose estimation robustness. Limitations. LongSplat shares common limitations of unposed reconstruction methods, assuming static scenes and fixed intrinsics, making it unsuitable for dynamic objects or varying focal lengths. Acknowledgements. This work was supported by NVIDIA Taiwan AI Research & Development Center (TRDC). This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 1122222-E-A49-004-MY2 and 113-2628-E-A49-023-. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan."
        },
        {
            "title": "References",
            "content": "[1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhofer, Johannes Kopf, Matthew OToole, and Changil Kim. Hyperreel: High-fidelity 6-dof video with rayconditioned sampling. In CVPR, 2023. 2 [2] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In ICCV, 2021. 2 [3] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022. [4] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Zip-nerf: Anti-aliased arXiv preprint Srinivasan, and Peter Hedman. grid-based neural arXiv:2304.06706, 2023. 2 radiance fields. [5] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior. In CVPR, 2023. 2, 6, 7, 8, 15, 18 [6] Chris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. Unstructured lumigraph rendering. In SIGGRAPH, 2001. 2 [7] Bo-Yu Chen, Wei-Chen Chiu, and Yu-Lun Liu. Improving robustness for joint optimization of camera pose and decomposed low-rank tensorial radiance fields. In AAAI, 2024. 2 [8] Shenchang Eric Chen and Lance Williams. View interpolation for image synthesis. In SIGGRAPH, 1993. 2 [9] Zezhou Cheng, Carlos Esteves, Varun Jampani, Abhishek Kar, Subhransu Maji, and Ameesh Makadia. Lu-nerf: Scene and pose estimation by synchronizing local unposed nerfs. arXiv preprint arXiv:2306.05410, 2023. 2 [10] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and Simon Lucey. Garf: Gaussian activated radiance fields for high fidelity reconstruction and pose estimation. arXiv eprints, 2022. 2 [11] Wenyan Cong, Kevin Wang, Jiahui Lei, Colton Stearns, Yuanhao Cai, Dilin Wang, Rakesh Ranjan, Matt Feiszli, Leonidas Guibas, Zhangyang Wang, Weiyao Wang, and Zhiwen Fan. Videolifter: Lifting videos to 3d with fast hierarchical stereo alignment, 2025. [12] Paul Debevec, Camillo Taylor, and Jitendra Malik. Modeling and rendering architecture from photographs: hybrid geometry-and image-based approach. In SIGGRAPH, 1996. 2 [13] Cheng-De Fan, Chen-Wei Chang, Yi-Ruei Liu, Jie-Ying Lee, Jiun-Long Huang, Yu-Chee Tseng, and Yu-Lun Liu. Spectromotion: Dynamic 3d reconstruction of specular scenes. In CVPR, 2025. 2 9 [14] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In CVPR, 2024. 2, 6, 7, 8, 14, 15, 17, 18 [15] Stephan Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In ICCV, 2021. 2 [16] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. 2003. 2 [17] Derek Hoiem, Alexei Efros, and Martial Hebert. Automatic photo pop-up. In ACM SIGGRAPH 2005 Papers, 2005. 2 [18] Youichi Horry, Ken-Ichi Anjyo, and Kiyoshi Arai. Tour into the picture: using spidery mesh interface to make animation In Proceedings of the 24th annual from single image. conference on Computer graphics and interactive techniques, 1997. 2 [19] Hao-Yu Hou, Chia-Chi Hsu, Yu-Chen Huang, Mu-Yi Shen, Wei-Fang Sun, Cheng Sun, Chia-Che Chang, Yu-Lun Liu, and Chun-Yi Lee. 3d gaussian splatting with grouped uncertainty for unconstrained images. In ICASSP, 2025. 2 [20] Ronghang Hu, Nikhila Ravi, Alex Berg, and Deepak Pathak. Worldsheet: Wrapping the world in 3d sheet for view synthesis from single image. In ICCV, 2020. [21] Bo Ji and Angela Yao. Sfm-free 3d gaussian splatting via hierarchical training. arXiv preprint arXiv:2412.01553, 2024. 2 [22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM TOG, 2023. 2, 3, 6, 7, 15 [23] Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, and George Drettakis. hierarchical 3d gaussian representation for real-time rendering of very large datasets. ACM TOG, 2024. 3 [24] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf: Ray entropy minimization for few-shot neural volume rendering. In CVPR, 2022. 2 [25] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM TOG, 2017. 6, 7, 15, 18 [26] Johannes Kopf, Michael F. Cohen, and Richard Szeliski. Firstperson hyper-lapse videos. ACM TOG, 2014. 2 [27] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In ECCV, 2024. 2, 3, 5, 6, 7, 8, 14, 15, 18, 19, 20 [28] Du-Hsiu Li, Hsueh-Ming Hang, and Yu-Lun Liu. Virtual view synthesis using backward depth warping algorithm. In PCS, 2013. 2 [29] Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, and Gim Hee Lee. Mine: Towards continuous depth mpi with nerf for novel view synthesis. In ICCV, 2021. 2 [30] Ming-Feng Li, Yueh-Feng Ku, Hong-Xuan Yen, Chi Liu, YuLun Liu, Albert YC Chen, Cheng-Hao Kuo, and Min Sun. Genrc: Generative 3d room completion from sparse image collections. In ECCV, 2024. 3 [31] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In ICCV, 2021. [32] Chin-Yang Lin, Chung-Ho Wu, Chang-Han Yeh, Shih-Han Yen, Cheng Sun, and Yu-Lun Liu. Frugalnerf: Fast convergence for few-shot novel view synthesis without learned priors. CVPR, 2025. 2 [33] Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, et al. Vastgaussian: Vast 3d gaussians for large scene reconstruction. In CVPR, 2024. 3 [34] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. In NeurIPS, 2020. 2 [35] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In CVPR, 2023. 2 [36] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In CVPR, 2024. 2, 3, 4, 6, 7, 8, 14, 15, 18, 19, 20 [37] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713, 2023. [38] Caoyuan Ma, Yu-Lun Liu, Zhixiang Wang, Wu Liu, Xinchen Liu, and Zheng Wang. Humannerf-se: simple yet effective approach to animate humannerf with diverse poses. In CVPR, 2024. 2 [39] Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang, Changil Kim, Min Kim, and Johannes Kopf. Progressively optimized local radiance fields for robust view synthesis. In CVPR, 2023. 2, 3, 6, 7, 8, 14, 15, 18, 19, 20 [40] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 2021. 2 [41] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM TOG, 2022. 2 [42] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. Orb-slam: versatile and accurate monocular slam system. IEEE transactions on robotics, 2015. 2 [43] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In CVPR, 2022. [44] Yunlong Ran, Yanxu Li, Qi Ye, Yuchi Huo, Zechun Bai, Jiahao Sun, and Jiming Chen. Ct-nerf: Incremental optimizing neural radiance field and poses with complex trajectory. arXiv preprint arXiv:2404.13896, 2024. 2 [45] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In ICCV, 2021. 2 [46] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In ICCV, 2021. 12, 13 [47] Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898, 2024. 3 [48] Gernot Riegler and Vladlen Koltun. Free view synthesis. In ECCV, 2020. [49] Gernot Riegler and Vladlen Koltun. Stable view synthesis. In CVPR, 2021. 2 [50] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022. 2 [51] Johannes Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In CVPR, 2016. 1, 2, 6 [52] Mu-Yi Shen, Chia-Chi Hsu, Hao-Yu Hou, Yu-Chen Huang, Wei-Fang Sun, Chia-Che Chang, Yu-Lun Liu, and Chun-Yi Lee. Driveenv-nerf: Exploration of nerf-based autonomous driving environment for real-world performance validation. arXiv preprint arXiv:2403.15791, 2024. 2 [53] Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, and Yu-Lun Liu. Boostmvsnerfs: Boosting mvs-based nerfs to generalizable view synthesis in large-scale scenes. In ACM SIGGRAPH 2024 Conference Papers, 2024. [54] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In CVPR, 2022. 2 [55] Teppei Suzuki. Fed3dgs: Scalable 3d gaussian splatting with federated learning. arXiv preprint arXiv:2403.11460, 2024. 3 [56] Takafumi Taketomi, Hideaki Uchiyama, and Sei Ikeda. Visual slam algorithms: survey from 2010 to 2016. IPSJ Transactions on Computer Vision and Applications, 2017. 2 [57] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul Srinivasan, Jonathan Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In CVPR, 2022. 3 [58] Richard Tucker and Noah Snavely. Single-view view synthesis with multiplane images. In CVPR, 2020. 2 [59] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan Barron, and Pratul Srinivasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. In CVPR, 2022. 2 [60] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. [61] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu, Taku Komura, Christian Theobalt, and Wenping Wang. F2nerf: Fast neural radiance field training with free camera trajectories. In CVPR, 2023. 6, 7, 8, 15, 17, 18 [62] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. arXiv preprint arXiv:2501.12387, 2025. 3 [63] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 3 [64] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 2004. 6 10 [65] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and NeRF: Neural radiance Victor Adrian Prisacariu. fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021. [66] Yitong Xia, Hao Tang, Radu Timofte, and Luc Van Gool. Sinerf: Sinusoidal neural radiance fields for joint pose estimation and scene reconstruction. 2022. 2 [67] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang. Sinnerf: Training neural radiance fields on complex scenes from single image. In ECCV, 2022. 2 [68] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Pointbased neural radiance fields. In CVPR, 2022. 2 [69] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In CVPR, 2023. 2 [70] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. arXiv preprint arXiv:2501.13928, 2025. 3 [71] Lin Yen-Chen, Pete Florence, Jonathan Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting neural radiance fields for pose estimation. In IROS, 2021. 2 [72] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In ICCV, 2021. [73] Yu-Ting Zhan, Cheng-Yuan Ho, Hebi Yang, Yi-Hsin Chen, Jui Chiu Chiang, Yu-Lun Liu, and Wen-Hsiao Peng. Cat-3dgs: context-adaptive triplane approach to ratearXiv preprint distortion-optimized 3dgs compression. arXiv:2503.00357, 2025. 2 [74] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv:2010.07492, 2020. 2 [75] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differentiable point-based radiance fields for efficient view synthesis. In SIGGRAPH Asia 2022 Conference Papers, 2022. 2 [76] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6 [77] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. 2018. 2 A. Implementation Details Table 9. Qualtitative comparison on the CO3Dv2 dataset [46] We implement LongSplat using PyTorch. Our rendering and 3D Gaussian updates are accelerated using CUDA and cuDNN. Camera pose optimization is performed using differentiable rendering, while the PnP initialization leverages OpenCVs solver with RANSAC. All experiments run on NVIDIA 4090 GPUs. A.1. LongSplat Algorithm: Pseudo-Code The LongSplat pipeline incrementally reconstructs scene from casually captured long video, without known poses, by tightly coupling pose estimation and 3D Gaussian Splatting. The workflow can be summarized in the following pseudo-code: Algorithm 1: LONGSPLAT: Incremental 3DGS Input: RGB frames {It}T Output: 3DGS G, camera poses {Pt}T /* Initialization (Dt, Ct, Pt) t=1 t=1 MASt3R Global Alignment(I1...Ninit ) OctreeAnchorFormation(G, Dt, Pt) /* Incremental Joint Optimization for Ninit to do GlobalOptimize(G, {P1..t1}, Kg) (Dt, Ct) MASt3R(It) Pt PnP RANSAC(Ct, G) if Pt = FAIL then fallback to end PoseRefine(G, Pt, It) AnchorUnprojection(G, Dt, Pt) VisibilityWindow(t) LocalOptimize(G, {Pk}kW , Kℓ) end /* Final Global Refinement GlobalRefinement(G, {P1..T }, Kr) return (G, {Pt}T t=1) */ */ */ B. Additional Experiments B.1. CO3Dv2 Benchmark Evaluation. We report the results on CO3Dv2 [46] in Fig. 12 and Table 9. LongSplat surpasses CF-3DGS and HT-3DGS in all image and pose metrics, confirming the methods robustness on this more challenging benchmark. B.2. Comparison between COLMAP and"
        },
        {
            "title": "LongSplat on the Hike Dataset",
            "content": "We compare LongSplat with standard COLMAP-based reconstruction pipeline on our Hike dataset. This dataset poses extreme challenges for incremental SfM due to vegetation 12 Dataset CO3Dv2 Method CF-3DGS HT-3DGS Ours PSNR 26.61 28.34 32. SSIM 0.79 0.84 0.91 LPIPS 0.29 0.30 0.17 ATE 0.014 0.017 0.005 RPEt 0.218 0.058 0.023 RPEr 0.374 0.314 0.096 Table 10. Pose Accuracy on Hike Dataset."
        },
        {
            "title": "Hike dataset",
            "content": "ATE RPEt RPEr MASt3R + Scaffold-GS MASt3R + Scaffold-GS* LocalRF Ours 0.006 0.006 0.004 0.002 0.009 0.009 0.011 0.003 0.292 0.221 0.211 0.128 Table 11. Qualitative comparison with HT-3DGS. Dataset Method PSNR SSIM LPIPS ATE RPEt RPEr Success Rate Tanks & Temples Free Hike HT-3DGS Ours HT-3DGS Ours HT-3DGS Ours 33.53 32.83 13.75 27.88 OOM 25.39 0.96 0.94 0.39 0.85 OOM 0. 0.07 0.08 0.65 0.17 0.00 0.00 0.02 0.00 0.04 0.03 0.34 0. 0.07 0.07 4.41 0.10 OOM 0.19 OOM OOM OOM 0.21 0.01 0.00 8/8 8/8 6/7 7/ 0/12 12/12 occlusion, textureless surfaces, and long trajectories. The quantitative results in Table 13 show that LongSplat consistently outperforms COLMAP in both rendering quality and pose estimation accuracy. This highlights the advantage of our octree-anchored Gaussian formulation combined with learned 3D priors. B.3. Pose Accuracy on Hike Dataset. COLMAP poses are noisy on several Hike videos, so we use the 6 stable sequences (forest2, indoor, university1-4) as references to compute pose accuracy in Table 10. LongSplat achieves the lowest errors, beating all baselines. B.4. Comparison between HT-3DGS and LongSplat We report the comparison with HT-3DGS in Table 11 and Fig. 13. HT-3DGS runs only on T&T (33.53 dB), but falls to 13.75 dB on Free and runs OOM on Hike. LongSplat remains stable across all datasets. This confirms our SOTA claim for long, casually captured videos. B.5. Ablation on Using MASt3R Relative Poses To demonstrate the importance of our proposed pose estimation pipeline, we conduct an ablation replacing LongSplats correspondence-guided PnP with directly using MASt3Rs relative pose estimates. As shown in Fig. 14, this leads to degraded novel view synthesis quality and larger pose errors, especially in long sequences. This confirms that raw MASt3R poses alone are insufficient for high-quality incremental reconstruction. Figure 12. Qualitative comparison on the CO3Dv2 dataset [46] Figure 13. Qualitative comparison with HT-3DGS Table 12. Ablation on training loss. Method PSNR SSIM w/o 2d correspondence loss w/o depth loss Ours 26.54 26.74 27.88 0.80 0.82 0.85 LPIPS RPEt RPEr ATE 0.007 0.011 0. 0.253 0.246 0.103 0.049 0.076 0.028 0.24 0.22 0.17 B.6. Ablation on training loss We report the ablation study on training loss in Table 12. Removing individual losses degrades performance. Our full method achieves the best rendering quality and pose accuracy. C. Complete Quantitative Evaluation C.1. Tanks and Temples We provide full quantitative results on the Tanks and Temples benchmark in Tabs. 14 and 15. LongSplat consistently outperforms baselines in both rendering quality and pose estimation accuracy, demonstrating its effectiveness even in indoor and urban scenes with varied scales and complexities. C.2. Free dataset We provide full quantitative results on the Free dataset benchmark in Tab. 16. LongSplat consistently outperforms baselines in both rendering quality and pose estimation accuracy, demonstrating its effectiveness even in indoor and urban scenes with varied scales and complexities. C.3. Hike dataset Hike dataset benchmark in Tab. 13. LongSplat consistently outperforms baselines in both rendering quality and pose estimation accuracy, demonstrating its effectiveness even 13 in challenging indoor and urban scenes with varied scales and complexities. Notably, in scenarios where COLMAP fails to reconstruct due to long trajectories or low-texture regions, LongSplat maintains high-quality results, preserving structural details and ensuring stable pose estimation. D. Additional Visual Comparisons D.1. Visual Comparison on Ablation Study Fig. 15 shows the visual impact of removing key training components. Both trajectory estimation and novel view synthesis degrade severely when global optimization, local optimization, or final refinement is removed, emphasizing their importance. D.2. Additional Trajectory Results We include additional visualizations of camera trajectories estimated by LongSplat. As shown in Fig. 16, our method reconstructs stable, drift-free trajectories even in long and complex sequences. D.3. Additional Tanks and Temples Results We provide additional qualitative comparisons on the Tanks and Temples benchmark. LongSplat produces sharper and more visually consistent results across diverse scenes, demonstrating strong generalization across both indoor and outdoor environments. D.4. Additional Free Dataset Results Additional qualitative comparisons on the Free dataset are shown in Fig. 18. Our method preserves more fine details, produces fewer artifacts, and achieves sharper novel view synthesis than all baselines. Figure 14. Visual comparisons on ablation MASt3R relative pose. Table 13. Quantitative evaluation on the Hike dataset [39]. Our method consistently outperforms baselines across diverse scenes with complex trajectories and extended sequences, highlighting LongSplats robustness and superior scene representation capability. CF3DGS [14] encounters OOM in all scenes and is thus omitted. Scenes COLMAP + Scaffold-GS [36] MASt3R [27] + Scaffold-GS [36] MASt3R [27] + Scaffold-GS [36] LocalRF [39] Ours PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 20.12 28.35 - 20.77 - 23.46 28.85 - forest1 forest2 forest3 garden1 garden2 garden3 indoor playground university1 25.36 university2 27.25 university3 26.98 university4 25.03 0.55 0.89 - 0.67 - 0.73 0.90 - 0.78 0.87 0.89 0.82 0.44 0.14 - 0.28 - 0.23 0.19 - 0.27 0.13 0.13 0. 17.68 20.91 9.54 13.09 13.21 11.82 23.64 19.31 19.38 20.27 18.59 20.23 0.30 0.53 0.15 0.23 0.19 0.13 0.81 0.49 0.47 0.58 0.51 0.61 0.64 0.36 0.70 0.75 0.75 0.64 0.33 0.40 0.53 0.36 0.39 0.39 17.54 21.11 9.62 14.84 15.67 11.89 24.64 19.73 19.62 20.72 19.31 20.13 0.34 0.54 0.15 0.27 0.26 0.13 0.83 0.52 0.48 0.60 0.57 0.61 0.55 0.35 0.70 0.72 0.74 0.64 0.31 0.38 0.52 0.35 0.35 0. 19.12 27.23 17.05 22.11 23.34 23.33 30.17 22.29 25.22 24.56 23.23 25.08 0.45 0.84 0.38 0.66 0.61 0.67 0.91 0.63 0.71 0.75 0.73 0.79 0.41 0.15 0.59 0.28 0.33 0.27 0.17 0.28 0.32 0.23 0.23 0.22 23.86 27.87 19.59 24.12 24.35 24.01 30.62 24.30 25.50 26.82 25.57 27.00 0.79 0.88 0.62 0.80 0.74 0.75 0.92 0.78 0.79 0.85 0.86 0.88 Avg 25.13 0.79 0.22 17.30 0.42 0. 17.90 0.44 0.50 23.56 0.68 0. 25.39 0.81 0.21 0.11 0.31 0.19 0.25 0.23 0.17 0.18 0.24 0.15 0.13 0.12 0.19 D.5. Additional Hike Dataset Results Finally, we present more qualitative results on the Hike dataset in Fig. 19, Fig. 20. LongSplat reconstructs complex natural scenes with higher visual quality, capturing vegetation, terrain, and large-scale geometry with remarkable accuracy. 14 Table 14. Quantitative evaluation of novel view synthesis quality on the Tanks and Temples dataset [25]. Our proposed LongSplat consistently surpasses existing methods across multiple challenging scenes. COLMAP+3DGS [22] NoPe-NeRF [5] CF-3DGS [14] Ours Scenes PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS 29.93 Church 31.08 Barn Museum 34.47 27.93 Family Horse 20.91 Ballroom 34.48 32.64 Francis 30.20 Ignatius 0.93 0.95 0.96 0.92 0.77 0.96 0.92 0.93 0.09 0.07 0.05 0.11 0.23 0.04 0.15 0.08 25.17 26.35 26.77 26.01 27.64 25.33 29.48 23. 0.73 0.69 0.76 0.74 0.84 0.72 0.80 0.61 0.39 0.44 0.35 0.41 0.26 0.38 0.38 0.47 30.23 31.23 29.91 31.27 33.94 32.47 32.72 28.43 0.93 0.90 0.91 0.94 0.96 0.96 0.91 0.90 0.11 0.10 0.11 0.07 0.05 0.07 0.14 0.09 30.96 32.57 33.78 33.67 33.42 32.80 33.80 31. 0.93 0.92 0.95 0.96 0.96 0.95 0.92 0.94 Avg. 30.21 0.92 0.10 26. 0.74 0.39 31.28 0.93 0.09 32. 0.94 0.10 0.09 0.06 0.06 0.06 0.06 0.15 0.07 0.08 Table 15. Quantitative evaluation of camera pose estimation accuracy on the Tanks and Temples dataset [25]. Our method achieves consistently low errors across diverse scenes, outperforming CF-3DGS and NoPe-NeRF, especially in terms of global trajectory accuracy (ATE) and local translation consistency (RPEt). CF-3DGS NoPe-NeRF Ours Scenes ATE RPEr RPEt ATE RPEr RPEt ATE RPEr RPEt 0.002 0.018 Church Barn 0.003 0.034 Museum 0.005 0.215 0.002 0.024 Family Horse 0.003 0.057 Ballroom 0.003 0.024 0.006 0.154 Francis 0.005 0.032 Ignatius 0.008 0.008 0.008 0.034 0.004 0.032 0.052 0.020 0.202 0.022 0.001 0.015 0.112 0.003 0.017 0.037 0.002 0.018 0.029 0.005 0.009 0.033 0.002 0.005 0.034 0.001 0.048 0.046 0.004 0.061 0.207 0.001 0.046 0.047 0.002 0.043 0.179 0.001 0.046 0.041 0.002 0.053 0.057 0.009 0.213 0.026 0.002 0. 0.011 0.025 0.025 0.021 0.086 0.021 0.036 0.032 Avg. 0.004 0.069 0.041 0.006 0.038 0.080 0.003 0.068 0. Table 16. Quantitative evaluation of camera pose estimation accuracy on the Free dataset [61]. - indicates methods that encountered out-of-memory issues. Our method consistently achieves superior performance across most scenes, significantly reducing pose errors compared to state-of-the-art approaches. *: Initialized with MASt3R poses, then jointly optimized. Scenes MASt3R [27] + Scaffold-GS [36] MASt3R [27] + Scaffold-GS [36]* CF-3DGS [14] NoPe-NeRF [5] LocalRF [39] Ours ATE RPEr RPEt ATE RPEr RPEt ATE RPEr RPEt ATE RPEr RPEt ATE RPEr RPEt ATE RPEr RPEt Grass 0.038 Hydrant 0.013 0.009 Lab 0.003 Pillar 0.013 Road 0.010 Sky 0.006 Stair 0.554 0.168 0.294 0.225 0.153 0.203 0.260 0.559 0.145 0.175 0.024 0.088 0.091 0. 0.002 0.013 0.009 0.003 0.013 0.010 0.006 0.152 0.165 0.265 0.199 0.159 0.197 0.247 0.016 0.144 0.178 0.016 0.088 0.090 0.050 - - - 0.023 - - 0.021 - - - 4.744 - - 2.139 - - - 0.328 - - 0. 0.431 0.480 0.533 0.576 0.584 0.807 0.624 9.333 4.068 2.623 4.176 4.087 6.661 2.809 3.044 5.844 5.774 2.013 6.045 9.775 11.120 0.008 6.026 0.056 8.487 0.060 4.405 0.041 3.553 0.025 0.023 9.798 0.031 11.075 6.257 0.612 1.068 1.072 0.526 0.699 0.894 0.563 0.000 0.013 0.004 0.001 0.005 0.002 0. 0.058 0.111 0.217 0.066 0.080 0.114 0.078 0.002 0.069 0.067 0.003 0.036 0.017 0.001 Avg. 0.013 0.265 0. 0.008 0.198 0.083 0.019 4.365 0. 0.576 4.822 6.231 0.035 7.086 0. 0.004 0.103 0.028 15 Figure 15. Visual comparisons on ablation studies. The top row shows the camera trajectory estimation and novel view synthesis results when different training components are removed, demonstrating the importance of each proposed module. Removing global optimization, local optimization, or final refinement significantly degrades pose accuracy and reconstruction quality. The bottom row evaluates different settings for the visibility-adapted local window size. Too small window leads to unstable geometry and pose drift, while too large window dilutes local visibility priors, slowing convergence. LongSplat achieves the best balance using the proposed adaptive window. Figure 16. Visualization of camera trajectories on Free dataset [61]. CF-3DGS [14] encounters OOM and fails for long sequences, whereas our method reliably estimates accurate, stable trajectories, demonstrating superior robustness. 17 Figure 17. More Qualitative comparison on the Tanks and Temples dataset [25]. NoPe-NeRF [5] produces visibly blurred results with inaccurate geometries, while CF-3DGS [14], despite better sharpness, fails to reconstruct fine details accurately. In contrast, our LongSplat method achieves superior rendering quality, closely matching the ground truth with sharper textures, more accurate geometry, and consistent lighting. Figure 18. More Qualitative comparison on the Free dataset [61]. We compare our method with state-of-the-art approaches including NoPe-NeRF [5], LocalRF [39], CF-3DGS [14], and MASt3R [27] combined with Scaffold-GS [36]. CF-3DGS fails due to memory constraints (OOM), and other baseline methods exhibit artifacts or blurry reconstructions. In contrast, our method produces results closest to the ground truth, demonstrating clearer details, accurate geometry, and visually consistent rendering, particularly under challenging scene structures and complex camera trajectories. *: Initialized with MASt3R poses, then jointly optimized. 18 Figure 19. Qualitative results on the Hike dataset [39]. Compared to existing methods such as LocalRF [39] and MASt3R [27]+ScaffoldGS [36], our approach significantly improves visual clarity and reconstruction fidelity, accurately capturing complex details and textures in challenging scenes captured during long, casual outdoor trajectories. Notably, our method better preserves structural details and reduces artifacts, demonstrating enhanced robustness and visual quality. *: Initialized with MASt3R poses, then jointly optimized. 19 Figure 20. More Qualitative results on the Hike dataset [39]. Compared to existing methods such as LocalRF [39] and MASt3R [27]+Scaffold-GS [36], our approach significantly improves visual clarity and reconstruction fidelity, accurately capturing complex details and textures in challenging scenes captured during long, casual outdoor trajectories. Notably, our method better preserves structural details and reduces artifacts, demonstrating enhanced robustness and visual quality. *: Initialized with MASt3R poses, then jointly optimized."
        }
    ],
    "affiliations": [
        "NVIDIA Research",
        "National Yang Ming Chiao Tung University"
    ]
}