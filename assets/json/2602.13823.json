{
    "paper_title": "Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings",
    "authors": [
        "Haonan Jiang",
        "Yuji Wang",
        "Yongjie Zhu",
        "Xin Lu",
        "Wenyu Qin",
        "Meng Wang",
        "Pengfei Wan",
        "Yansong Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 1 ] . [ 1 3 2 8 3 1 . 2 0 6 2 : r Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings Haonan Jiang1,2, Yuji Wang1,2, Yongjie Zhu2, Xin Lu2, Wenyu Qin2, Meng Wang2, Pengfei Wan2, and Yansong Tang1 1Tsinghua Shenzhen International Graduate School, Tsinghua University 2Kling Team, Kuaishou Technology {jiang-hn24@mails, yuji-wan24@mails, tang.yansong@sz}.tsinghua.edu.cn {zhuyongjie, luxin09, qinwenyu, wangmeng46, wanpengfei}@kuaishou.com Abstract. Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the models fine-grained matching capability as well as its generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing practical and efficient solution for reasoning-driven UME development. Project page. Keywords: Multimodal Embedding Generative Reasoning Reinforcement Learning"
        },
        {
            "title": "Introduction",
            "content": "Multimodal embedding, as core supporting technology for cross-modal tasks, has been widely applied to numerous important directions such as image-text : Equal Contribution. Work done during an internship at Kuaishou Technology. : Project Leader. : Corresponding Author. 2 H. Jiang, Y. Wang et al. retrieval, video moment localization, and visual document understanding [26, 36]. Traditional multimodal embedding methods adopt dual-encoder architectures, such as CLIP [43], BLIP [34], and SigLIP [68]. These methods demonstrate weaker ability in bridging the gap between different modalities compared with Multimodal Large Language Models (MLLMs) [2, 3, 31, 32, 51]. Additionally, MLLMs benefit from their strong multimodal understanding and instructionfollowing capabilities, enabling them to adapt to diverse and complex task requirements. Therefore, an increasing body of literature [18, 26, 41, 46, 64] proves that MLLMs can be used to learn Universal Multimodal Embedding (UME) that captures general-purpose content similarity. Meanwhile, evaluation benchmarks such as the Multimodal Embedding Benchmark (MMEB) [26] and its upgraded version MMEB-V2 [41] have addressed this academic demand for UME research, covering 78 instruction-aware tasks across three modalities. Fig. 1: Multimodal embedding optimization via Embedder-Guided Reinforcement Learning (EG-RL). (a) Frameworks evolution. (b) Reasoning enhancement with RL-optimized evidential Traceability CoT (T-CoT). (c) Comparison of multi-task performance. Currently, the majority of MLLM-powered embedding methods belong to discriminative embedding models [29, 39, 72]. These models typically extract embedding features directly from the final hidden states of input tokens, which fails to fully leverage the generative capabilities and reasoning potential inherent in MLLMs. Consequently, recent studies have begun to explore the integration of generative reasoning into UME tasks, as shown in Figure 1. For instance, approaches like UME-R1 [30] unify discriminative and generative embeddings through textual Chain-of-Thoughts (CoTs) generated by the MLLM Embedder. However, the simultaneous optimization of contrastive loss and next-token prediction objectives can result in conflicting gradients, leading to suboptimal Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 3 performance [8, 30]. In contrast, the decoupled Reasoner-Embedder paradigm proposed by TTE [13] aims to alleviate this problem by decoupling the two processes, leveraging pre-trained MLLMs to generate offline CoT reasoning to improve embedding quality while training only the Embedder. Nevertheless, the CoTs generated by the Reasoner of TTE are not specifically designed for embedding, as they are not trained together with the Embedder. This misalignment may introduce noise and even lead to hallucinations. Moreover, relying only on textual reasoning fails to fully leverage the MLLM Embedders potential to process multimodal signals, whose rich representations could significantly enhance retrieval performance. Such insufficiency of multimodal cues leads to notable embedding alignment bias, where critical visual-spatial cues and video-temporal signals are not fully captured in retrieval, resulting in less effective cross-modal matching and restricted generalization on complex real-world multimodal tasks. To address the above issues, this paper proposes reasoning-driven decoupled UME framework. This framework leverages the Embedder-Guided Reinforcement Learning (EG-RL) algorithm to optimize the CoTs generated by the Reasoner, using our novel process reward oriented to the alignment between query and target and verifiable outcome reward for retrieval. Firstly, we construct dataset that initially trains the Embedder to generate high-quality embeddings conditioned on the sequence of preceding input and CoT tokens. The trained Embedder acts as reward model and provides stable and reliable reward signals. Secondly, inspired by the region-aware paradigm [15, 45, 47, 76] that makes the model focus on the region of interest (RoI), we propose the evidential Traceability CoT (TCoT) that explicitly guides the Embedder to focus on task-related information, effectively filter out redundant visual elements, and integrate modality-specific critical cues to adapt to long-text retrieval, coarse-grained semantic matching, and fine-grained alignment for robust performance across heterogeneous tasks. The main contributions of this paper are summarized as follows: 1. Embedder-Guided Reinforcement Learning. We propose novel decoupled RL framework where the Embedder guides the Reasoner to optimize CoT trajectories for specific embedding tasks. This approach resolves key conflict between generative and embedding objectives, ensures the Reasoners output greatly improves retrieval quality, and addresses core challenge of adapting general CoTs to embedding tasks. 2. Evidential Traceability CoT for Embedding. We further extend CoT reasoning to complex multimodal scenarios, integrating explicit visual localization information, video keyframes, and text keywords into detailed inference trajectories. This design enables the model to focus on core retrieval-related information and effectively mitigate the negative impact of redundant multimodal and text data on overall embedding alignment performance. 3. Efficient Performance Improvement Across Multiple Benchmarks. Under computationally constrained settings, the framework proposed in this paper outperforms state-of-the-art generative embedding models on both MMEB-V2 [41] and video retrieval UVRB [20] benchmark datasets, and achieves exceptional performance across diverse combinatorial scenarios. 4 H. Jiang, Y. Wang et al."
        },
        {
            "title": "2.1 Universal Multimodal Embedding",
            "content": "Constructing robust multimodal representations is core challenge in multimodal learning. Pioneering models such as CLIP [43] and ALIGN [24] adopt dualencoder architecture and learn effective representations through contrastive learning on large-scale image-text paired data. However, they struggle to handle interleaved image-text inputs, and their text encoders lack sufficient capacity to understand complex textual content. To address this issue, researchers leverage Multimodal Large Language Models (MLLMs) to build embedding models [26,36, 39, 41, 72, 78], capitalizing on their strong multimodal comprehension capabilities to enhance learning performance. Existing works focus on different aspects: VLM2Vec [26] transforms MLLMs into embedding models via contrastive learning and achieves outstanding performance on unconventional retrieval tasks such as visual question answering and localization; MM-Embed [36] explores using off-the-shelf MLLMs as zeroshot rerankers to optimize retrieval results; LamRA [39] unifies the multimodal retrieval paradigm through two-stage retrieval training and joint reranking. MegaPairs [78] and GME [72] address the modality imbalance problem with automated pipelines; LLaVE [29] and UniME [18] focus on hard negative sample mining. Recent studies focus on instruction-aware representations: MMEB [26] and MMEB-V2 [41] construct comprehensive evaluation benchmark covering 78 tasks. UME-R1 [30] first introduces reasoning mechanisms, yet simultaneous optimization of dual components via Reinforcement Learning (RL) leads to conflicts, and redundant Chain-of-Thought (CoT) trajectories dilute representations. TTE [13] adopts decoupled architecture, but its Reasoner is misaligned with retrieval tasks, resulting in task-irrelevant outputs. In this paper, we propose decoupled RL framework enabling separate optimization of dual components and generating retrieval-relevant reasoning trajectories through dual reward mechanism, addressing the aforementioned challenges."
        },
        {
            "title": "2.2 Multimodal Reasoning with Reinforcement Learning",
            "content": "MLLMs [2, 31, 38, 51] extend the capabilities of Large Language Models (LLMs) to the multimodal domain, achieving promising results across diverse tasks including visual question answering [3, 17, 38, 50], visual grounding [12, 28, 33, 57], and keyframe extraction [35, 52, 69, 80]. Early works [23, 53, 59, 61, 74] completed reasoning tasks using standardized CoT prompts. Since DeepSeek-R1 [19] proposed the Group Relative Policy Optimization (GRPO) RL algorithm, numerous recent works have optimized RL algorithms [65, 75] and enhanced the reasoning capabilities of MLLMs [6, 14, 44, 56, 60]. GRIT [15] interleaves bounding box coordinates with textual reasoning chains and designs an RL scheme based on the GRPO algorithm, enabling efficient training with dual robust rewards and no additional annotated data. GroundR1 [6] proposes an RL framework to achieve grounded visual reasoning without Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 5 extra annotations, guiding response generation through dual rewards to improve reasoning reliability and interpretability. BRPO [11] uses Intersection over Union (IoU)-based rewards to guide models to autonomously generate visual-text reflections, combined with visual token mechanism to mitigate the problems of visual attention dilution and hallucinations. DeepEyes [76] adopts end-to-end RL to induce models to develop the ability of thinking with images, improving performance on various reasoning tasks. TreeVGR [47] proposes the TreeBench benchmark and the TreeVGR training paradigm, enhancing visual grounding reasoning capabilities by jointly supervising localization and reasoning via RL. Inspired by grounding reasoning, this paper further proposes evidential Traceability CoT (T-CoT), which constructs structured multimodal reasoning chains by extracting bounding boxes of images, keyframes of videos, and keywords of text. This method enables the model to focus on the core regions of retrieval tasks, thereby improving embedding quality."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "We focus on the universal multimodal retrieval task, where given query (text, image, or interleaved text-image modalities) and candidate set Ω = {cn}N n=1, the goal is to retrieve the most relevant candidate from Ω. To learn discriminative multimodal embeddings, we adopt contrastive learning with the InfoNCE loss [42], as shown in Figure 2(a). For query qi, its positive target t+ , and its negative target set = {t }j=i, the InfoNCE loss optimizes the model to maximize the similarity between qi and t+ while minimizing the similarities to all . The loss is defined as: LInfoNCE = 1 (cid:80)N i=1 log (cid:18) exp cos(hqi ,h (cid:19) (cid:19) )/τ + (cid:18) exp cos(hqi ,h )/τ + +(cid:80) T exp(cos(hqi ,ht )/τ) (1) where hqi and ht are embeddings of qi and target (extracted as the last-layer hidden states of the last token from vision-language model), cos(, ) denotes cosine similarity, and τ is the temperature hyperparameter."
        },
        {
            "title": "3.2 Data Construction",
            "content": "To support the training of reasoning-driven universal multimodal embeddings, we construct high-quality multimodal dataset following sampling-annotationfiltering-splitting pipeline, as illustrated in Figure 2(a). The dataset integrates diverse modalities (image, video, visual document) and ensures alignment between reasoning trajectories and retrieval objectives through strict quality control. We first curate the initial data pool by adopting stratified sampling strategy across three core sources, and referencing the data paradigm of VLM2Vec-V2 [41]: 6 H. Jiang, Y. Wang et al. Fig. 2: Overview of the proposed data synthesis and EG-RL framework. (a) Data Construction generates T-CoT annotations for query-positive pairs, filters and splits the dataset to enable contrastive and reinforcement learning, laying the groundwork for reasoning-aware embedding. (b) Embedder-Guided Reinforcement Learning finetunes the MLLM with process-outcome reward function, encouraging T-CoT trajectories that yield more discriminative and beneficial generative embeddings. (1) Image-centric tasks from MMEB-train [26], covering image classification, Question Answering, retrieval, and grounding; (2) Video-language instruction data from LLaVA-Hound [71], including video captioning, QA, and retrieval; (3) Visual document retrieval data from ViDoRe [16] and VisRAG [66]. Next, we perform evidential Traceability Chain-of-Thought (T-CoT) annotation for all query-positive pairs. Each T-CoT follows structured threepart format: (1) <thinking> extracts modality-specific cues (text keywords via text_keywords, image spatial locations via bbox_2d, video critical moments via key_frames); (2) <rethink> refines reasoning logic to focus on key retrieval retrieval-relevant aspects; (3) the final answer summarizes core retrieval-relevant information. We design task-specific prompts for annotation, ensuring T-CoT aligns with diverse multimodal retrieval scenarios. Following annotation, we perform strict CoT-guided relevance filtering to eliminate noisy samples. With custom-designed judgment prompt, we assess whether the T-CoTs of queries and their positive samples are clearly irrelevant or contradictory to the task description. We retain only samples labeled No, meaning those that are relevant and not contradictory, for contrastive learning. Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 7 This filtering step effectively mitigates noise interference in contrastive learning. The initial dataset contains 2.22 million samples, and 1.83 million are preserved after filtering, yielding an retention rate of approximately 80%. Approximately 20% of the filtered-out samples are uniformly sampled and used in the reinforcement learning stage, as these hard examples are valuable for model exploration in reinforcement learning. In addition, we assign training weights to different datasets based on their task importance and data quality. The constructed dataset exhibits three features: (1) modal diversity, covering three modalities of text, image, and video; (2) reasoning alignment, where TCoT explicitly integrates multimodal cues and retrieval-related logic, avoiding informational redundancy; (3) quality assurance, as rigorous filtering and weighted sampling ensure that the dataset is free of significant noise and balanced across tasks. This dataset lays solid foundation for the two-stage training, enabling Embedder to learn reasoning-aware representations and the Reasoner to optimize the generation of retrieval-centric T-CoTs through reinforcement learning."
        },
        {
            "title": "3.3 Embedder-Guided Reinforcement Learning",
            "content": "To address the misalignment between generative reasoning and embedding objectives, we propose decoupled reinforcement learning framework in which the pre-trained Embedder provides supervision to the Reasoner. This framework optimizes the generation of T-CoT to prioritize retrieval-relevant multimodal cues, leveraging dual-guidance reward mechanism and Group Relative Policy Optimization (GRPO) [19]. Figure 2(b) illustrates the workflow of this stage. EG-RL Framework Design. First, we fully train an Embedder using the InfoNCE loss to equip it with robust embedding capabilities. Our RL framework maintains strict separation between two components: the Reasoner, which is responsible for generating T-CoT, and the Embedder, which is frozen after contrastive training. This decoupling ensures three key benefits: (1) targeted optimization of reasoning without disrupting the Embedders learned discriminative capabilities; (2) stable reward signals from the frozen Embedder that consistently evaluate T-CoT quality based on embedding alignment; and (3) flexible integration of multi-source rewards to internalize both retrieval and reranking knowledge. The Reasoner takes multimodal queries as input and outputs structured T-CoT, which integrates three critical cues: text keywords, image bounding boxes, and video keyframes. Additionally, we recrop the content within the bounding boxes and keyframes based on T-CoT to achieve multimodal reasoning-aware embeddings. This structured reasoning is then concatenated with the original input to form the Embedders input, denoted as I: = [xtext, ximg, xvid, T-CoT(x), <emb>] (2) In this equation, <emb> is special token whose hidden state is extracted as the final embedding, and the evaluation of this embedding by the Embedder directly guides the Reasoners policy update. Reward Function with Process and Outcome Guidance. We design three-component reward function to align T-CoT generation with embedding 8 H. Jiang, Y. Wang et al. quality, combining format compliance, outcome-level retrieval effectiveness, and process-level T-CoT alignment: Format Reward (Rformat): This reward ensures T-CoT strictly follows the predefined template (<thinking> <rethink> <answer>) and includes all required multimodal cues. Reward 1 for full compliance, 0 otherwise, guaranteeing T-CoT output interpretability and compatibility with the Embedder module. Embedder-Guided Outcome Reward (Routcome): This reward measures how T-CoT improves embedding alignment by jointly assessing the ranking accuracy of positive samples and the similarity margin between positive and hard negative samples. The margin is softmax-weighted average of negative similarities scaled by temperature parameter. For query qi with positive }j=i, embeddings are eqi = πe(qi, oq target t+ ) and etj = πe(tj, ot are T-CoT outputs for query and target. The reward for oq and in-batch negatives {t j), where oq is defined as: and ot Routcome(oq ) = Acck(eqi, t+ ) (cid:16) sim(eq, et+ ) Eτ (cid:2)sim(eqi, et )(cid:3)(cid:17) , (3) where Acck(eqi, t+ ) denotes the top-k retrieval accuracy, which measures whether t+ is among the top-k ranked targets when sorted by cosine similarity to eqi; sim(, ) denotes the cosine similarity between normalized embeddings; and Eτ [] stands for the softmax-weighted average of cosine similarities between eqi and embeddings of in-batch negative targets. Additionally, we compute Routcome symmetrically for positive targets: taking t+ as the anchor, qi as its positive query, and other in-batch queries as negatives to calculate Routcome for ot i. This symmetric computation enforces consistent embedding alignment in both query-to-target and target-to-query directions. Optimizing this reward optimizes T-CoT with embedding learning as the core objective, enhancing its discriminative ability across samples. T-CoT Process Reward (Rprocess): We employ an independent pretrianed VLM Discriminator for listwise comparison to align T-CoT outputs of queries and targets. Let qcot be the querys T-CoT output and {cj j=1 the T-CoT outputs of in-batch candidate targets, comprising positive samples from multiple rollouts of the querys data pair and negative samples. After shuffling, the index set of ground-truth positives is denoted P. To mitigate position bias, we feed qcot and shuffled {cj cot}m j=1 to as paired inputs. The reward quantifies alignment via Ds selection correctness, formally defined as: cot}m Rprocess(oi) = (cid:40) 1, 0, if D(cid:0)qcot, {cj otherwise, cot}m j= (cid:1) P, (4) where oi denotes the T-CoT generation outcome of the i-th sample, and D(, ) outputs the index of the candidate T-CoT most aligned with qcot in the shuffled candidate set. reward of 1 indicates that correctly selects positive T-CoT from the ground-truth set, signifying well-aligned query-target T-CoT pairs; reward of 0 means fails to select any positive T-CoT, indicating misalignment between query and target T-CoT outputs. We compute Rprocess symmetrically in Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 9 the reverse direction, moving from positive targets to their corresponding queries, to ensure consistent embedding alignment across both directions. This reward guides the Reasoner module to align query and target T-CoT outputs. Since T-CoT is the intermediate process for embedding generation, T-CoT alignment quantified by selection correctness directly improves final embedding quality. This design avoids extra inference steps while ensuring embeddings inherit discriminative power from well-aligned T-CoT outputs. The total reward is weighted combination of these three components: Rtotal = αRformat + βRprocess + γRoutcome, (5) where α, β, γ 0 are weighting coefficients that balance the contributions of the reward components. Detailed hyperparameters are shown in Sup. B. Policy Optimization with GRPO. We adopt GRPO to optimize the Reasoners policy, and use group-based rewards to stabilize the training process. For each query-target pair S, where denotes the training sample set of query-target pairs. We sample = 8 candidate T-CoT sequences {oi}G according to the old policy πθold. The optimization objective is defined as: i=1 Lgrpo = qS, {oi}πθold (cid:34) (cid:16) 1 (cid:80)G i=1 min(rθ(oi)Ai, clip(rθ(oi), 1 ϵ, 1 + ϵ) Ai) βDKL(πθπref) (cid:35) (cid:17) , (6) where rθ(oi) = πθ(oiq)/πθold(oiq) denotes importance ratio, ϵ is the clipping threshold of importance ratio, β is hyperparameter weighting the KL divergence term, πref denotes the reference policy model before optimization, and Ai = (ri µr)/σr represents advantage, where µr = mean({r1, ..., rG}) and σr = std({r1, ..., rG}). This optimization ensures the Reasoner learns to generate TCoT sequences improving embedding quality, aligning reasoning trajectories with retrieval objectives while maintaining computational efficiency."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "We train Qwen3-VL-2B [2] and Qwen3-VL-4B [2] using the DeepSpeed Zero2 optimization framework, adopting sub-batch strategy following VLM2Vec [26]. The models are trained for 2 epochs with an initial learning rate of 1e-4 and weight decay of 0.01; the batch size is set to 512 for the 2B model and 256 for the 4B model, and we employ LoRA [21] for fine-tuning. For reinforcement learning, Qwen3-VL-8B [2] is trained using the GRPO algorithm [19] for 1 epoch, with batch size of 256, learning rate of 3e-6, and typical GRPO hyperparameters. Detailed hyperparameters are shown in Sup. B."
        },
        {
            "title": "4.2 Baselines and Datasets",
            "content": "We compare with representative multimodal embedding models with diverse architectures, modalities and scales. These baselines cover mainstream image, 10 H. Jiang, Y. Wang et al. video and visual document retrieval, ensuring thorough and fair evaluation. We evaluate against GME [73], ColPali [16], VLM2Vec [26], LamRA [39], CAFe [64], VLM2Vec-V2 [41], UME-R1 [30], InternVideo2 [55], Unite [18], and GVE [20]. For the training phase, we followed the training data paradigm of VLM2VecV2 [41] and constructed training dataset from three sources: video-language instruction data from LLaVA-Hound [71], visual document retrieval data from ViDoRe [16] and VisRAG [66], and image-based vision-language task data from MMEB-train [26]. Detailed dataset construction is provided in Sup. C. We evaluate our model on two comprehensive benchmarks: MMEB-V2 (Massive Multimodal Embedding Benchmark) [41]: It is comprehensive and robust benchmark consisting of 78 diverse tasks across three core visual modalities (image, video, and visual document). MMEB-V2 extends the original MMEB [26] by introducing five additional meta-tasks focused specifically on video and visual document understanding, bringing the total to nine metatasks. We adopted Hit@1 as the evaluation metric for image and video tasks, and NDCG@5 [22] for visual document tasks. UVRB (Universal Video Retrieval Benchmark) [20]: It is suite of 16 datasets designed to diagnose critical capability gaps in video retrieval across tasks and domains. UVRB explicitly measures multi-dimensional generalization over textual, composite, and visual retrieval tasks, as well as across coarse-grained, fine-grained, and long-context scenarios. We reported Mean Average Precision (mAP) as the primary evaluation metric for all UVRB tasks. Table 1: Comparison of performance between baselines and our method on MMEB-V2. CLS: classification, QA: question answering, RET: retrieval, GD: grounding, MRET: moment retrieval, VDR: ViDoRe, VR: VisRAG, OOD: out-of-domain. The highest and second-highest values are highlighted in bold and underline. Model Image Video VisDoc All CLS QA RET GD Overall CLS QA RET MRET Overall VDRv1 VDRv2 VR OOD Overall # of Datasets 10 10 4 36 5 5 5 18 10 4 6 4 78 Baseline Models 26.7 37.8 21.6 ColPali-V1.3-3B [16] 40.3 11.5 48.1 40.3 34.9 42.0 25.6 54.4 29.9 66.9 55.5 GME-2B [73] 37.4 50.4 28.4 GME-7B [73] 57.7 34.7 71.2 59.3 39.3 42.6 24.3 LamRA-2VL-7B [39] 59.2 26.5 70.0 62.7 32.9 42.6 23.2 LamRA-2.5V-7B [39] 51.7 34.1 66.9 56.7 33.4 30.5 20.6 58.7 49.3 65.0 72.9 VLM2Vec-2B [26] 39.1 30.0 29.0 VLM2Vec-7B [26] 62.7 56.9 69.4 82.2 39.3 34.3 28.8 VLM2Vec-V2-2B [41] 62.9 56.3 69.5 77.3 45.9 33.9 27.6 VLM2Vec-V2-7B [41] 65.7 61.5 70.0 85.2 35.8 58.7 34.4 63.6 61.7 69.1 87.6 CAFe-7B [64] 64.8 62.8 67.6 77.2 UME-R1-2B [30] 44.3 51.2 32.9 67.1 69.2 71.9 84.9 71.3 48.6 60.7 38.2 UME-R1-7B [30] 34.9 51.9 56.0 54.1 52.4 59.7 65.5 64.9 68.1 67.6 66.6 25.5 32.4 38.2 34.6 37.6 33.0 40.6 38.5 39.3 39.5 39.7 39.3 28.2 33.9 38.6 35.2 33.7 29.0 34.0 34.9 36.4 42.4 42.2 47. 83.6 86.1 89.4 22.0 56.3 49.8 56.9 75.5 78.8 70.7 72.4 75.7 Ours 71.0 72.7 81.1 43.1 82.5 43.1 44.4 52.0 54.1 54.0 55.6 85.0 44.4 75.2 57.8 40.4 11.5 47.4 33.3 47.0 13.5 52.3 9.4 58.0 44.9 61.2 52.6 60.6 49.6 60.1 46.2 64.5 50. 37.4 21.0 58.2 40.1 51.8 33.5 59.1 38.1 79.4 39.4 82.7 42.1 79.5 38.1 79.2 37.2 83.7 37.6 23.9 50.2 41.6 46.4 65.4 69.3 63.9 63.9 67.1 Embed-RL-2B Embed-RL-4B 62.8 67.9 68.6 90.4 57.0 55.9 45.1 49.4 69.2 63.7 70.5 71.3 91.4 70.1 57.6 58.4 45.1 49.5 52.1 53. 79.9 80.2 52.0 53.4 84.6 65.7 66.8 84.9 67.1 74.7 68.1 74.1 Embed-RL: RL for Reasoning-Driven Multimodal Embeddings"
        },
        {
            "title": "4.3 Main Results",
            "content": "Model CG FG LC Table 2: Video retrieval performance on UVRB. Domain dimensions: Coarse-grained (CG), Finegrained (FG), Long-context (LC). The best and second-best results are marked in bold and underline. Table 1 compares the comprehensive performance of our proposed Embed-RL models with various baseline approaches on the MMEB-V2 benchmark. Our Embed-RL models achieve superior performance directly compared to all baseline models. Specifically, Embed-RL-4B attains the best overall score of 68.1, outperforming the strong next top baseline UME-R1-7B by 3.6. Embed-RL-2B follows closely with an overall score of 66.8, also surpassing all baseline variants. Across different modalities, our models show clear notable advantages: in the Image modality, Embed-RL-4B achieves the best GD performance of 91.4, while EmbedRL-2B ranks second with 90.4; in the Video modality, both Embed-RL-2B and Embed-RL-4B outperform baselines in overall score (52.1 and 53.0 respectively), and video RET with 45.1; in the VisDoc modality, our models deliver significant improvements in OOD, where Embed-RL-4B reaches 67.1 and Embed-RL-2B 65.7, far exceeding prior baseline performances. These results demonstrate the effectiveness of our proposed approach across diverse visual modalities and task types. More scores are provided in Sup. D. InternVideo2-6B [55] 50.4 41.7 42.3 49.8 50.2 76.2 VLM2Vec-V2 [41] 51.8 50.7 78.8 GME-7B [73] 54.1 53.9 74.6 Unite-7B [27] 55.2 54.1 76.4 GVE-3B [20] 59.1 54.6 86.9 60.7 55.6 86. Embed-RL-2B Embed-RL-4B In the broader field of video retrieval, benefiting from the effective ability of our T-CoT to locate retrieval keywords or keyframes, our model exhibits significant advantages in CG, FG, and LC retrieval. As shown in Table 2, which presents detailed video retrieval performance of different models on the UVRB dataset across the three domains, our Embed-RL models consistently excel: 4B tops CG at 60.7 and FG at 55.6, while 2B leads LC at 86.9, both outperforming existing all baselines. Detailed scores on UVRB are reported in Sup. G. Additionally, Figure 3 presents the visualization of our T-CoT on text, image and video; we crop bbox and keyframe to achieve multi-modal CoT input, and our T-CoT accurately locates retrieval needs to improve retrieval performance."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "To dissect the contribution of each component in our framework, we conduct ablation experiments on MMEB-V2, where Embed-RL-2B serves as the full model. Analysis of the reward model in the RL Stage. As shown in Table 3, removing the entire RL stage leads to an overall performance decline of 1.5, dropping the score from 66.8 to 65.3. This result verifies that RL fine-tuning is indispensable for embedding alignment. Omitting weighted negative sampling, core module of our contrastive reward mechanism, brings performance reduction of 0.3 from 66.8 to 66.5. This outcome emphasizes the components key function in prioritizing hard negative examples to strengthen discriminative embedding 12 H. Jiang, Y. Wang et al. Fig. 3: Example visualization of our reasoning-driven embedding framework on multimodal retrieval tasks. The figure shows the evidential Traceability CoT reasoning process for video and visual document retrieval. learning. The process reward is formulated to reward logical reasoning steps and align query T-CoT with target T-CoT. It contributes 0.8 to the overall performance, as its exclusion lowers the score from 66.8 to 66.0. This component shows the most notable influence on video tasks, where performance falls from 52.1 to 51.3. This trend reveals that video understanding strongly depends on step-by-step reasoning and further reflects the critical role of the process reward in T-CoT alignment. Additionally, the outcome reward is built to reward correct final predictions. It contributes 1.0 to the overall performance, and its removal reduces the total score from 66.8 to 65.8. This reward ensures that the reasoning process remains consistent with the objective of the target task. Impact of Reasoning Components on T-CoT.As shown in Table 4, removing the reasoning process while retaining the answer part leads to an overall performance decrease of 1.3 from 66.8 to 65.5. Image grounding and video moment retrieval see notable drops, falling from 69.2 to 67.9 and 52.1 to 50.5 respectively, which highlights the importance of multimodal evidence tracking for fine-grained alignment tasks. Removing multimodal cues leads to performance reduction of 1.0 from 66.8 to 65.8, validating the necessity of extracting multimodal cues through bounding boxes and keyframes to enhance alignment between multimodal representations and retrieval objectives. Most critically, using only raw input without T-CoT causes catastrophic decrease in overall performance of 6.6, from 66.8 to 60.2. The greatest impact appears on video tasks, where performance falls from 52.1 to 43.7. This dramatic decline demonstrates the necessity of high-quality evidential Traceability CoT for retrieval accuracy, as it enables the Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 13 model to decompose complex retrieval and understanding tasks into manageable steps and improve cross-modal embedding quality. Table 3: Ablation Study on Reward Components in EG-RL stage. Table 4: Ablation Study on Reasoning Components in T-CoT. Model Image Video VisDoc All Model Image Video VisDoc All Embed-RL-2B 69. 52.1 74.1 66.8 Embed-RL-2B 69.2 52. 74.1 66.8 w/o EG-RL w/o weighted negative w/o process reward w/o outcome reward 68.0 68.9 68.3 68.1 50.1 51.7 51.3 51.2 72.7 73.9 73.5 73. 65.3 66.5 66.0 65.8 w/o reasoning w/o multimodal cues w/ raw input 67.9 68.1 60.4 50.5 51.4 43.7 73.1 73.3 72.4 65.5 65.8 60. Ablation Study on Models Discriminative Ability for Candidates. We define the top-ranked candidates with the highest similarity (excluding positive samples) as highly similar candidate samples. On this basis, we study how optimizing the reasoner with EG-RL improves the models ability to distinguish between similar candidates. Specifically, we calculate the difference between the similarity of the query to the most similar candidate and that to the second-most similar candidate on each dataset, both before and after RL. This difference measures whether the model assigns significantly higher similarity to positive samples than to other highly similar candidates. As shown in Figure 4, we observe that on different datasets across three modalities, the radar chart obtained after RL prominently encloses the one obtained before RL. This indicates that the computed similarity difference becomes larger after RL, widening the gap between the querys similarity to the top-ranked candidate and the second-ranked candidate. It demonstrates that the models ability to discriminate between similar candidates is effectively enhanced. Meanwhile, the bar chart shows that the model achieves consistent overall improvement on three-modal datasets. This verifies that optimizing T-CoT with RL strengthens the models general ability to distinguish between different candidates. Fig. 4: Similarity difference = sim(query, top1) sim(query, top2) before and after EG-RL. Here, sim(, ) denotes cosine similarity of normalized embeddings, top1 is the most similar positive candidate and top2 the second-most similar. This metric quantifies the models discriminative ability over similar candidates on multimodal datasets. 14 H. Jiang, Y. Wang et al. Ablation on traceable evidence count and retrieval metrics. We also analyze the relationship between the number of traceable evidence pieces and the retrieval metrics across datasets before and after reinforcement learning. For images and visdoc data we count the change in the number of bounding boxes. For video data we count the change in the number of keyframes. We observe that after reinforcement learning the T-CoT generated by the Reasoner tends to produce more bounding boxes. For the video modality the model tends to focus on fewer keyframes. The retrieval metrics show consistent improvements and the curves after reinforcement learning lie entirely above those before reinforcement learning. For image modality, the model captures more visual evidence to improve reasoning accuracy and recall.For video modality, it focuses on critical frames and performs precise keyframe extraction and temporal localization to identify key content. The changes are particularly pronounced on samples involving multi object localization and multi person relationship reasoning. More visualizations are presented in Sup. L. Fig. 5: Relationship between traceable evidence counts and retrieval metrics across datasets. Hit@1 is employed for Image and Video; NDCG@5 is used for VisDoc. Bounding box counts are shown for Image and VisDoc, while keyframe counts for Video."
        },
        {
            "title": "5 Conclusion",
            "content": "This work addresses key limitations of generative universal multimodal embedding (UME) methods: chain-of-thought (CoT) remains text-only, resulting in poor retrieval relevance, while joint optimization of generative and embedding objectives gives rise to gradient conflicts that impede cross-modal matching. We propose Embed-RL, reasoning-driven UME model built upon Embedder-Guided RL (EG-RL), which serves as decoupled reinforcement learning framework that integrates multimodal evidential Traceability CoT (T-CoT) and retrieval-oriented dual-reward mechanism to enable precise reasoning-embedding alignment. Extensive experiments on the MMEB-V2 and UVRB benchmarks demonstrate that Embed-RL outperforms state-of-the-art counterparts within computational constraints, achieving significant improvements in cross-modal retrieval and out-of-domain generalization tasks. This work shows that targeted reasoning optimization can substantially enhance multimodal embeddings, providing an efficient solution for reasoning-driven UME and valuable insights into the integration of generative reasoning with multimodal representation learning. Embed-RL: RL for Reasoning-Driven Multimodal Embeddings"
        },
        {
            "title": "References",
            "content": "1. Anne Hendricks, L., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.: Localizing moments in video with natural language. In: Proceedings of the IEEE international conference on computer vision. pp. 58035812 (2017) 2. Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., Zhu, K.: Qwen3-vl technical report. arXiv preprint arXiv:2511.21631 (2025) 3. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al.: Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 (2025) 4. Bolya, D., Huang, P.Y., Sun, P., Cho, J.H., Madotto, A., Wei, C., Ma, T., Zhi, J., Rajasegaran, J., Rasheed, H., et al.: Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181 (2025) 5. Cai, Q., Liang, H., Dong, H., Qiang, M., An, R., Han, Z., Zhu, Z., Cui, B., Zhang, W.: Lovr: benchmark for long video retrieval in multimodal contexts. arXiv preprint arXiv:2505.13928 (2025) 6. Cao, M., Zhao, H., Zhang, C., Chang, X., Reid, I., Liang, X.: Ground-r1: Incentivizing grounded visual reasoning via reinforcement learning. arXiv preprint arXiv:2505.20272 (2025) 7. Chai, W., Song, E., Du, Y., Meng, C., Madhavan, V., Bar-Tal, O., Hwang, J.N., Xie, S., Manning, C.D.: Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051 (2024) 8. Chen, H., Liu, H., Luo, Y., Wang, L., Yang, N., Wei, F., Dou, Z.: Moca: Modalityaware continual pre-training makes better bidirectional multimodal embeddings. arXiv preprint arXiv:2506.23115 (2025) 9. Chen, H., Wang, L., Yang, N., Zhu, Y., Zhao, Z., Wei, F., Dou, Z.: mme5: Improving multimodal multilingual embeddings via high-quality synthetic data. arXiv preprint arXiv:2502.08468 (2025) 10. Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., Jitsev, J.: Reproducible scaling laws for contrastive language-image learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 28182829 (2023) 11. Chu, X., Chen, X., Wang, G., Tan, Z., Huang, K., Lv, W., Mo, T., Li, W.: Qwen look again: Guiding vision-language reasoning models to re-attention visual information. arXiv preprint arXiv:2505.23558 (2025) 12. Chung, J., Kim, J., Kim, S., Lee, J., Kim, M.S., Yu, Y.: Dont look only once: Towards multimodal interactive reasoning with selective visual revisitation. arXiv preprint arXiv:2505.18842 (2025) 13. Cui, X., Cheng, J., Chen, H.y., Shukla, S.N., Awasthi, A., Pan, X., Ahuja, C., Mishra, S.K., Yang, Y., Xiao, J., et al.: Think then embed: Generative context improves multimodal embedding. arXiv preprint arXiv:2510.05014 (2025) 14. Duan, C., Fang, R., Wang, Y., Wang, K., Huang, L., Zeng, X., Li, H., Liu, X.: Gotr1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning. arXiv preprint arXiv:2505.17022 (2025) 16 H. Jiang, Y. Wang et al. 15. Fan, Y., He, X., Yang, D., Zheng, K., Kuo, C.C., Zheng, Y., Narayanaraju, S.J., Guan, X., Wang, X.E.: Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879 (2025) 16. Faysse, M., Sibille, H., Wu, T., Omrani, B., Viaud, G., Hudelot, C., Colombo, P.: Colpali: Efficient document retrieval with vision language models. arXiv preprint arXiv:2407.01449 (2024) 17. Geng, X., Xia, P., Zhang, Z., Wang, X., Wang, Q., Ding, R., Wang, C., Wu, J., Zhao, Y., Li, K., et al.: Webwatcher: Breaking new frontier of vision-language deep research agent. arXiv preprint arXiv:2508.05748 (2025) 18. Gu, T., Yang, K., Feng, Z., Wang, X., Zhang, Y., Long, D., Chen, Y., Cai, W., Deng, J.: Breaking the modality barrier: Universal embedding learning with multimodal llms. arXiv preprint arXiv:2504.17432 (2025) 19. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al.: Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025) 20. Guo, Z., Li, M., Zhang, Y., Long, D., Xie, P., Chu, X.: Towards universal video retrieval: Generalizing video embedding via synthesized multimodal pyramid curriculum. arXiv preprint arXiv:2510.27571 (2025) 21. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank adaptation of large language models. ICLR 1(2), 3 (2022) 22. Järvelin, K., Kekäläinen, J.: Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems (TOIS) 20(4), 422446 (2002) 23. Ji, D., Zhu, L., Gao, S., Xu, P., Lu, H., Ye, J., Zhao, F.: Tree-of-table: Unleashing the power of llms for enhanced large-scale table understanding. arXiv preprint arXiv:2411.08516 (2024) 24. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy text supervision. In: International conference on machine learning. pp. 49044916. PMLR (2021) 25. Jiang, T., Song, M., Zhang, Z., Huang, H., Deng, W., Sun, F., Zhang, Q., Wang, D., Zhuang, F.: E5-v: Universal embeddings with multimodal large language models. arXiv preprint arXiv:2407.12580 (2024) 26. Jiang, Z., Meng, R., Yang, X., Yavuz, S., Zhou, Y., Chen, W.: Vlm2vec: Training vision-language models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160 (2024) 27. Kong, F., Zhang, J., Liu, Y., Zhang, H., Feng, S., Yang, X., Wang, D., Tian, Y., Zhang, F., Zhou, G., et al.: Modality curation: Building universal embeddings for advanced multimodal information retrieval. arXiv preprint arXiv:2505.19650 (2025) 28. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J.: Lisa: Reasoning segmentation via large language model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 95799589 (2024) 29. Lan, Z., Niu, L., Meng, F., Zhou, J., Su, J.: Llave: Large language and vision embedding models with hardness-weighted contrastive learning. arXiv preprint arXiv:2503.04812 (2025) 30. Lan, Z., Niu, L., Meng, F., Zhou, J., Su, J.: Ume-r1: Exploring reasoning-driven generative multimodal embeddings. arXiv preprint arXiv:2511.00405 (2025) 31. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., et al.: Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 (2024) Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 17 32. Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z., Li, C.: Llava-nextinterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895 (2024) 33. Li, G., Xu, J., Zhao, Y., Peng, Y.: Dyfo: training-free dynamic focus visual search for enhancing lmms in fine-grained visual understanding. In: Proceedings of the Computer Vision and Pattern Recognition Conference. pp. 90989108 (2025) 34. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In: International conference on machine learning. pp. 1973019742. PMLR (2023) 35. Liao, Z., Xie, Q., Zhang, Y., Kong, Z., Lu, H., Yang, Z., Deng, Z.: Improved visual-spatial reasoning via r1-zero-like training. arXiv preprint arXiv:2504.00883 (2025) 36. Lin, S.C., Lee, C., Shoeybi, M., Lin, J., Catanzaro, B., Ping, W.: Mm-embed: Universal multimodal retrieval with multimodal llms. arXiv preprint arXiv:2411.02571 (2024) 37. Lin, Z., Cen, S., Jiang, D., Karhade, J., Wang, H., Mitra, C., Ling, T., Huang, Y., Liu, S., Chen, M., et al.: Towards understanding camera motions in any video. arXiv preprint arXiv:2504.15376 (2025) 38. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural information processing systems 36, 3489234916 (2023) 39. Liu, Y., Zhang, Y., Cai, J., Jiang, X., Hu, Y., Yao, J., Wang, Y., Xie, W.: Lamra: Large multimodal model as your advanced retrieval assistant. In: Proceedings of the Computer Vision and Pattern Recognition Conference. pp. 40154025 (2025) 40. Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing 508, 293304 (2022) 41. Meng, R., Jiang, Z., Liu, Y., Su, M., Yang, X., Fu, Y., Qin, C., Chen, Z., Xu, R., Xiong, C., et al.: Vlm2vec-v2: Advancing multimodal embedding for videos, images, and visual documents. arXiv preprint arXiv:2507.04590 (2025) 42. Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018) 43. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 87488763. PmLR (2021) 44. Su, Z., Li, L., Song, M., Hao, Y., Yang, Z., Zhang, J., Chen, G., Gu, J., Li, J., Qu, X., et al.: Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617 (2025) 45. Su, Z., Xia, P., Guo, H., Liu, Z., Ma, Y., Qu, X., Liu, J., Li, Y., Zeng, K., Yang, Z., et al.: Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918 (2025) 46. Thirukovalluru, R., Meng, R., Liu, Y., Su, M., Nie, P., Yavuz, S., Zhou, Y., Chen, W., Dhingra, B., et al.: Breaking the batch barrier (b3) of contrastive learning via smart batch mining. arXiv preprint arXiv:2505.11293 (2025) 47. Wang, H., Li, X., Huang, Z., Wang, A., Wang, J., Zhang, T., Zheng, J., Bai, S., Kang, Z., Feng, J., et al.: Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology. arXiv preprint arXiv:2507.07999 (2025) 48. Wang, J., Wang, C., Huang, K., Huang, J., Jin, L.: Videoclip-xl: Advancing long description understanding for video clip models. arXiv preprint arXiv:2410.00741 (2024) 18 H. Jiang, Y. Wang et al. 49. Wang, J., Yuan, L., Zhang, Y., Sun, H.: Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634 (2024) 50. Wang, P., Ling, H.: Svqa-r1: Reinforcing spatial reasoning in mllms via viewconsistent reward optimization. arXiv preprint arXiv:2506.01371 (2025) 51. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al.: Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024) 52. Wang, Q., Liu, J., Liang, J., Jiang, Y., Zhang, Y., Chen, J., Zheng, Y., Wang, X., Wan, P., Yue, X., et al.: Vr-thinker: Boosting video reward models through thinking-with-image reasoning. arXiv preprint arXiv:2510.10518 (2025) 53. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., Zhou, D.: Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022) 54. Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Li, X., Chen, G., Chen, X., Wang, Y., et al.: Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942 (2023) 55. Wang, Y., Li, K., Li, X., Yu, J., He, Y., Chen, G., Pei, B., Zheng, R., Wang, Z., Shi, Y., et al.: Internvideo2: Scaling foundation models for multimodal video understanding. In: European Conference on Computer Vision. pp. 396416. Springer (2024) 56. Wang, Y., Liu, W., Niu, J., Zhang, H., Tang, Y.: Vg-refiner: Towards tool-refined referring grounded reasoning via agentic reinforcement learning. arXiv preprint arXiv:2512.06373 (2025) 57. Wang, Y., Xu, H., Liu, Y., Li, J., Tang, Y.: Sam2-love: Segment anything model 2 in language-aided audio-visual scenes. In: Proceedings of the Computer Vision and Pattern Recognition Conference. pp. 2893228941 (2025) 58. Wei, C., Chen, Y., Chen, H., Hu, H., Zhang, G., Fu, J., Ritter, A., Chen, W.: Uniir: Training and benchmarking universal multimodal information retrievers. In: European Conference on Computer Vision. pp. 387404. Springer (2024) 59. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35, 2482424837 (2022) 60. Wu, M., Yang, J., Jiang, J., Li, M., Yan, K., Yu, H., Zhang, M., Zhai, C., Nahrstedt, K.: Vtool-r1: Vlms learn to think with images via reinforcement learning on multimodal tool use. arXiv preprint arXiv:2505.19255 (2025) 61. Xu, G., Jin, P., Wu, Z., Li, H., Song, Y., Sun, L., Yuan, L.: Llava-cot: Let vision language models reason step-by-step. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 20872098 (2025) 62. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: large video description dataset for bridging video and language. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 52885296 (2016) 63. Xu, Y., Li, X., Yang, Y., Meng, D., Huang, R., Wang, L.: Carebench: fine-grained benchmark for video captioning and retrieval (2025), https://arxiv.org/abs/ 2501.00513 64. Yu, H., Zhao, Z., Yan, S., Korycki, L., Wang, J., He, B., Liu, J., Zhang, L., Fan, X., Yu, H.: Cafe: Unifying representation and generation with contrastive-autoregressive finetuning. arXiv preprint arXiv:2503.19900 (2025) 65. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al.: Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 (2025) Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 19 66. Yu, S., Tang, C., Xu, B., Cui, J., Ran, J., Yan, Y., Liu, Z., Wang, S., Han, X., Liu, Z., et al.: Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint arXiv:2410.10594 (2024) 67. Yuan, H., Ni, J., Liu, Z., Wang, Y., Zhou, J., Liang, Z., Zhao, B., Cao, Z., Dou, Z., Wen, J.R.: Momentseeker: task-oriented benchmark for long-video moment retrieval. arXiv preprint arXiv:2502.12558 (2025) 68. Zhai, X., Mustafa, B., Kolesnikov, A., Beyer, L.: Sigmoid loss for language image pretraining. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 1197511986 (2023) 69. Zhang, H., Gu, X., Li, J., Ma, C., Bai, S., Zhang, C., Zhang, B., Zhou, Z., He, D., Tang, Y.: Thinking with videos: Multimodal tool-augmented reinforcement learning for long video reasoning. arXiv preprint arXiv:2508.04416 (2025) 70. Zhang, K., Luan, Y., Hu, H., Lee, K., Qiao, S., Chen, W., Su, Y., Chang, M.W.: Magiclens: Self-supervised image retrieval with open-ended instructions. arXiv preprint arXiv:2403.19651 (2024) 71. Zhang, R., Gui, L., Sun, Z., Feng, Y., Xu, K., Zhang, Y., Fu, D., Li, C., Hauptmann, A.G., Bisk, Y., et al.: Direct preference optimization of video large multimodal models from language model reward. In: Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp. 694717 (2025) 72. Zhang, X., Zhang, Y., Xie, W., Li, M., Dai, Z., Long, D., Xie, P., Zhang, M., Li, W., Zhang, M.: Gme: Improving universal multimodal retrieval by multimodal llms. arXiv preprint arXiv:2412.16855 (2024) 73. Zhang, X., Zhang, Y., Xie, W., Li, M., Dai, Z., Long, D., Xie, P., Zhang, M., Li, W., Zhang, M.: Bridging modalities: Improving universal multimodal retrieval by multimodal large language models. In: Proceedings of the Computer Vision and Pattern Recognition Conference. pp. 92749285 (2025) 74. Zhang, X., Du, C., Pang, T., Liu, Q., Gao, W., Lin, M.: Chain of preference optimization: Improving chain-of-thought reasoning in llms. Advances in Neural Information Processing Systems 37, 333356 (2024) 75. Zheng, C., Liu, S., Li, M., Chen, X.H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al.: Group sequence policy optimization. arXiv preprint arXiv:2507.18071 (2025) 76. Zheng, Z., Yang, M., Hong, J., Zhao, C., Xu, G., Yang, L., Shen, C., Yu, X.: Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362 (2025) 77. Zhou, J., Liu, Z., Liu, Z., Xiao, S., Wang, Y., Zhao, B., Zhang, C.J., Lian, D., Xiong, Y.: Megapairs: Massive data synthesis for universal multimodal retrieval. arXiv preprint arXiv:2412.14475 (2024) 78. Zhou, J., Xiong, Y., Liu, Z., Liu, Z., Xiao, S., Wang, Y., Zhao, B., Zhang, C.J., Lian, D.: Megapairs: Massive data synthesis for universal multimodal retrieval. In: Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 1907619095 (2025) 79. Zhu, B., Lin, B., Ning, M., Yan, Y., Cui, J., HongFa, W., Pang, Y., Jiang, W., Zhang, J., Li, Z., Zhang, C.W., Li, Z., Liu, W., Yuan, L.: Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. In: The Twelfth International Conference on Learning Representations (2024) 80. Zhu, L., Chen, Q., Shen, X., Cun, X.: Vau-r1: Advancing video anomaly understanding via reinforcement fine-tuning. arXiv preprint arXiv:2505.23504 (2025) 20 H. Jiang, Y. Wang et al. Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings"
        },
        {
            "title": "A Additional details",
            "content": "In this supplementary material, we elaborate on further insights, provide detailed derivations, and include additional qualitative results to enhance the comprehensiveness of our work."
        },
        {
            "title": "B Training Details",
            "content": "B.1 Contrastive Learning We trained the Qwen3-VL-2B-Instruct [2] and Qwen3-VL-4B-Instruct [2] models using the DeepSpeed Zero2 optimization framework. Key training hyperparameters were summarized as follows: The training process was conducted for 2 epochs with batch size of 512 for Qwen3-VL-2B-Instruct and 256 for Qwen3VL-4B-Instruct. Following the sub-batch training scheme in VLM2Vec [26], we adopted sub-batch training strategy that ensured samples in each sub-batch are drawn from the same dataset, where the sub-batch size was set to 256 for Qwen3-VL-2B-Instruct and 128 for Qwen3-VL-4B-Instruct. We set the initial learning rate to 1e-4, using cosine learning rate scheduler with 10 warm-up steps and weight decay of 0.01. We employed Low-Rank Adaptation (LoRA) [21] for fine-tuning: the rank and scaling factor α were set to 64 and 128 for Qwen3VL-2B-Instruct, and 96 and 192 for Qwen3-VL-4B-Instruct, respectively. For comparison, UME-R1 [30] was trained with batch size of 1024 and TTE [13] was trained with 8192, indicating that our training scale was significantly smaller than both methods. Notably, our experimental results could be further scaled up with more abundant computational resources. B.2 Embedder-Guided Reinforcement Learning In the reinforcement learning stage, we trained the Qwen3-VL-8B-Instruct [2] Reasoner with the GRPO [19]. We adopted in-batch negative contrastive rewards for optimization, with GRPO hyperparameters set as group size = 8, clipping parameter ε = 0.2, and KL-divergence coefficient β = 0.01. We set batch size to 256, learning rate to 3e-6, and trained the model for one epoch. We also restricted each step to samples from the same dataset to avoid overly simple negative samples affecting optimization. For the embedder-guided outcome reward Routcome, two core hyperparameters balance retrieval accuracy and similarity margin calculation: the top-k parameter Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 21 for retrieval accuracy Acck and the temperature parameter τ for softmax-weighted negative sampling. Top-k Retrieval Accuracy k: We set the parameter in Acck(eqi, t+ ) to 8. Specifically, Acc8(eqi, t+ ) checks whether all 8 T-CoT rollouts of query-target pair rank in the top-8 by cosine similarity to eqi, which captures the embedding alignment consistency critical for stable GRPO reward optimization. Temperature Parameter (τ ): The temperature τ scales the softmax weights for hard negative sampling in the similarity margin calculation, which is defined as: Eτ (cid:2)sim(eqi, et )(cid:3) = (cid:80) j=i exp (cid:18) sim(eqi ,e (cid:19) ) τ (cid:18) sim(eqi ,e sim(eqi, et (cid:19) ) ) . (7) (cid:80) j=i exp τ We set τ = 0.5 for all experiments, value chosen to emphasize hard negatives while avoiding overfitting to noisy negative samples. For process reward, we employ an independent Qwen3-VL-8B-Instruct [2] as the pretrained VLM Discriminator D. It performs listwise comparison to align query T-CoT outputs with corresponding target outputs. The discriminator selects the candidate T-CoT that best matches the query. Selection correctness quantifies alignment quality and forms the process reward signal. We set α = 0.05, β = 0.8, and γ = 0.2 for the total reward Rtotal to balance the contributions of format, process, and outcome rewards respectively. B.3 Multimodel Vision Processing For visual input processing, we set specific pixel constraints for images and videos to balance computational efficiency and feature fidelity: For images: MIN_PIXELS = 1283232 and MAX_PIXELS = 7683232. For videos: VIDEO_MIN_PIXELS = 1283232, VIDEO_MAX_PIXELS = 300 32 32, and VIDEO_TOTAL_PIXELS = 300 32 32 8. Video frame sampling hyperparameters were fixed as FRAME_FACTOR = 2, FPS = 2.0, FPS_MIN_FRAMES = 8 and FPS_MAX_FRAMES = 8. For multimodal cropping, we first convert the relative coordinates of bounding boxes, which are scaled to the range 01000, into the original image coordinates, then conduct cropping on the raw image. Keyframes corresponding to the sampled frames are re-extracted and concatenated between the </thinking>and <rethink>tokens. 22 H. Jiang, Y. Wang et al."
        },
        {
            "title": "C Detailed Dataset Construction",
            "content": "C.1 Data Sources and Initial Sampling Strategy To enable effective multi-modal task training, we adopt the training data paradigm of VLM2Vec-V2 [41] and build comprehensive dataset from three core sources: (1) video-language instruction data (LLaVA-Hound [71]), (2) visual document retrieval data (ViDoRe [16] and VisRAG [66]), and (3) image-based vision task data (MMEB-train [26]). We apply stratified sampling strategy across data modalities to ensure balanced coverage: Image-based datasets: Maximum 50,000 samples per dataset Document-based datasets: Maximum 100,000 samples per dataset Video-based datasets: Maximum 300,000 samples per dataset Full sampling is used if the original dataset size is smaller than the above maximum. As shown in Table 5, we presented the exact number of samples selected for each dataset in our experiments. Table 5: Statistics of Initial Sampling and CoT-guided Filtering. Dataset Initial Samples Filtered Samples Retention Ratio Weight Modality A-OKVQA CIRR ChartQA DocVQA HatefulMemes ImageNet-1K InfographicsVQA MSCOCO MSCOCO-i2t MSCOCO-t2i N24News NIGHTS OK-VQA SUN397 VOC2007 Visual7W VisDial VisualNews-i2t VisualNews-t2i WebQA Caption Retrieval Video QA Video Retrieval ViDoRe VisRAG Image-based Video-based Document-based Total 50,000 50,000 50,000 50,000 25,500 50,000 50,000 50,000 50,000 50,000 50,000 47,823 27,027 50,000 23,532 50,000 50,000 50,000 50,000 50,000 300,000 300,000 300,000 100,000 100,000 1,123,882 900,000 200,000 2,223,882 37,929 35,085 39,512 47,401 16,572 44,409 40,746 26,429 46,596 43,173 30,320 43,167 19,900 45,864 20,454 41,677 34,652 34,364 28,684 43,910 Image-based (MMEB-train) 75.86% 70.17% 79.02% 94.80% 64.99% 88.82% 81.49% 52.86% 93.19% 86.35% 60.64% 90.26% 73.63% 91.73% 86.92% 83.35% 69.30% 68.73% 57.37% 87.82% Video-based (LLaVA-Hound) 94.57% 91.30% 86.80% 283,721 273,906 260,410 Document-based 83,964 60,266 865,074 819,037 144,230 1,828,341 83.96% 60.27% 76.97% 91.02% 72.12% 82.21% Text-Image Text Text-Image Text Text-Image Text Text-Image Text Text-Image Text Text-Image Text 0.26 0.43 Text-Image Text-Image 0.35 0.84 0.30 2.25 0.31 3.78 Text-Image Text-Image 2.58 2.32 1.65 0.23 Text-Image Text-Image 0.25 0.22 0.24 1.68 3.75 2.91 3.49 0.23 Text-Image Text Text-Image Text Text-Image Text Text-Image Text Text Text-Image Text-Image Text Text Text-Image Text Text-Image Text-Image Text Text Text-Image Text-Image Text 5.27 4.38 5. 5.0 6.0 - - - - Video Text Video-Text Text Text Video Text-Image Text Text Image Image-centric Video-centric Document-centric Multimodal Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 23 We exclude three classification datasets (HatefulMemes, N24News, VOC2007) from the first-stage contrastive learning. This stage only employs contrastive loss under single-dataset sub-batch constraint. Their limited number of categories results in false negatives and noise with large sub-batches, which impairs representation learning. This explains the models weak performance on image classification, as it never encounters these samples during training. C.2 CoT-guided Relevance Filtering We generate chain-of-thought (CoT) annotations for queries and positive samples using Qwen3-VL-8B [2], and further conduct strict relevance filtering via custom prompt with the same model to discard annotations that are irrelevant to or conflict with the query task. To mitigate noise in contrastive learning, only samples labeled \"No\" are retained. Table 5 lists the sample size, retention ratio, training weight, and modality of each dataset. CoT Relevance and Conflict Judgment Prompt Your task is to determine whether the content of query_cot and pos_cot are obviously irrelevant or obviously conflicting based on the given qry (task description). Rules: 1. Only output single word: \"Yes\" (if obviously irrelevant/conflicting) or \"No\" (if relevant and not conflicting) 2. \"Obviously irrelevant\": The content of query_cot and pos_cot have no logical connection to each other or to the qry task 3. \"Obviously conflicting\": The core conclusions/key information of query_cot and pos_cot are mutually contradictory 4. Only judge \"obvious\" cases if the relevance is ambiguous, output \"No\" C.3 RL Dataset Sampling Using the high-quality CoT-filtered dataset, we apply equidistant sampling to construct the reinforcement learning (RL) training set, ensuring uniform distribution across challenging sub-datasets, as shown in Table 6. Table 6: Reinforcement Learning Dataset Construction and Sampling Strategy. Sampled Dataset RL Samples Modality 1,000 A-OKVQA 2,000 llavahound video retrieval 2,000 ViDoRe colpali train set 1,000 VisualNews-t2i VisualNews-i2t 1,000 VisRAG-Ret-Train-In-domain-data 2,000 2,000 CIRR 1,000 ChartQA 1,000 OK-VQA 2,000 llavahound qa 2,000 llavahound caption retrieval 1,000 Visual7W 1,000 N24News Text-Image Text Text Video Text-Image Text Text Text-Image Text-Image Text Text Image Text-Image Text-Image Text-Image Text Text-Image Text Video-Text Text Video Text Text-Image Text Text-Image Text Total 19,000 Multimodal 24 H. Jiang, Y. Wang et al. Detailed Scores of MMEB-V2 We report the detailed metrics for our MMEB-V2 dataset [41], as shown in Table 7 and Table 8. The highest and second-highest values are highlighted in bold and underlined, respectively. Table 7: Detailed results of baselines and Embed-RL on full MMEB-v2 benchmark. Video and Visual Doc results are shown on the next table. ColPali v1.3 [16] GME-7B [73] VLM2Vec-7B [26] VLM2Vec-V2-2B [41] CAFe-7B [64] UME-R1-2B [30] UME-R1-7B [30] Embed-RL-2B Embed-RL-4B Avg - All (78 tasks) Avg - Image (36 tasks, Hit@1) Avg - Video (18 tasks, Hit@1) Avg - Visdoc (24 tasks, NDCG@5) I-CLS (10) I-QA (10) I-RET (12) I-VG (4) V-CLS (5) V-QA (5) V-RET (5) V-MR (3) VD-Vidore-V1 (10) VD-Vidore-V2 (4) VD-VisRAG (6) VD-OOD (4) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country211 OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA VisDial CIRR VisualNews_t2i VisualNews_i2t MSCOCO_t2i MSCOCO_i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS MSCOCO RefCOCO RefCOCO-Matching Visual7W-Pointing 44.4 34.9 28.2 71. 40.3 11.5 48.1 40.3 26.7 37.8 21.6 25.5 83.6 52.0 81.1 43.1 42.4 25.5 50.6 69.8 56.1 27.5 14.9 64.6 45.6 6.0 9.4 6.6 11.3 5.0 5.7 6.1 16.3 27.6 8.3 18.8 41.2 8.2 50.1 47.6 59.2 49.9 65.5 53.8 5.9 80.5 50.0 64.7 36.7 64.5 3.9 56.1 57.8 56.0 38.4 75. 57.7 34.7 71.2 59.3 37.4 50.4 28.4 37.0 89.4 55.6 85.0 44.4 64.6 50.5 53.6 80.3 69.5 39.1 41.2 83.9 69.0 24.8 33.2 21.0 41.4 20.3 17.8 22.2 28.0 39.0 76.9 46.8 60.8 54.9 79.7 83.6 71.2 57.7 67.6 91.4 37.8 78. 75.1 96.0 31.4 60.9 78.4 66.5 52.3 65.5 33.7 46.4 62.7 56.9 69.4 82.2 39.1 30.0 29.0 38.9 56.9 9.4 59.1 38.1 80.1 79.7 69.7 80.7 77.4 37.4 58.1 73.9 40.1 29.8 56.8 47.3 89.7 60.0 56.9 52.7 38.5 39.9 55.1 71.6 81.9 51. 80.5 81.2 77.2 73.9 67.6 88.3 17.1 62.3 66.5 85.7 75.7 87.6 84.6 81.0 58.0 64.9 34.6 65.4 62.9 56.3 69.5 77.3 39.3 34.3 28.8 36.8 75.7 45.1 79.6 39.6 80.8 72.9 56.3 85.0 71.0 35.9 47.4 89.3 65.2 25.2 51.5 43.6 90.1 58.8 47.4 52.9 38.2 43.3 64.9 72.2 82.7 57.5 74.5 78.2 75.3 71.4 68.6 90.6 19.5 66.9 64.3 84.1 67.1 87.1 85.8 69. 60.6 67.6 42.4 63.9 63.6 61.7 69.1 87.6 35.8 58.7 34.4 39.5 70.7 49.6 79.5 38.1 77.3 83.2 78.7 89.8 79.9 45.0 55.2 88.0 22.5 16.7 67.3 63.8 79.2 53.3 48.8 52. 65.4 43.8 65.7 76.8 82.7 60.4 69.5 79.4 75.4 73.1 66.7 89.3 39.0 61.2 60.8 71.3 84.7 89.4 83.0 93.2 60.1 66.6 42.2 63.9 64.8 62.8 67.6 77.2 44.3 51.0 32.9 39.7 72.4 46.2 79.2 37. 75.3 81.1 75.2 80.0 79.4 42.6 50.4 88.7 52.0 23.4 62.4 51.1 92.2 67.7 64.9 54.1 42.7 46.8 67.3 78.6 76.6 53.7 71.7 74.2 75.1 68.9 67.2 90.0 17.1 62.0 66.9 88.0 69.5 83.3 84.4 71.5 64.5 71.3 47.5 67.1 67.1 69.2 71.9 84.9 48.6 60.7 38.2 39.3 75.7 50.5 83.7 37. 80.4 82.3 79.0 90.8 80.3 46.8 53.9 90.1 42.3 25.0 71.7 58.7 93.8 79.2 75.1 55.2 53.7 51.6 69.3 83.5 80.7 55.3 76.8 82.0 78.3 71.4 68.1 90.9 23.4 72.5 71.4 92.0 72.7 91.4 91.1 84. 66.8 69.2 52.1 74.1 62.8 67.9 68.6 90.4 57.0 55.9 45.1 49.4 79.9 52.0 84.6 65.7 78.0 44.9 65.0 78.7 75.4 43.9 59.2 88.5 74.8 20.0 61.4 54.7 92.4 76.7 80.7 52.7 57.3 54.5 64.9 83.8 81.5 47.6 71.9 73. 79.4 75.3 66.3 89.3 24.0 68.9 61.4 84.5 92.9 94.9 85.8 88.0 68.1 71.2 53.0 74.7 63.7 70.5 71. 91.4 57.6 58.4 45.1 49.5 80.2 53.4 84.9 67.1 79.5 48.3 66.2 79.5 79.2 43.1 58.1 88.2 75.4 19.4 67.3 59.3 94.3 77. 80.9 55.3 61.6 56.2 68.5 84.3 84.9 61.2 73.7 73.9 78.9 76.3 66.4 90.5 31.9 69.6 60.7 87.4 93.6 95.9 88.0 87.9 Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 25 Table 8: Detailed results of baselines and Embed-RL on Video and Visual Doc of MMEB-v2 benchmark. ColPali v1.3 [16] GME-7B [73] VLM2Vec-7B [26] VLM2Vec-V2-2B [41] CAFe-7B [64] UME-R1-2B [30] UME-R1-7B [30] Embed-RL-2B Embed-RL-4B K700 SmthSmthV2 HMDB51 UCF101 Breakfast MVBench Video-MME NExTQA EgoSchema ActivityNetQA DiDeMo MSR-VTT MSVD VATEX YouCook2 QVHighlight Charades-STA MomentSeeker ViDoRe_arxivqa ViDoRe_docvqa ViDoRe_infovqa ViDoRe_tabfquad ViDoRe_tatdqa ViDoRe_shiftproject ViDoRe_artificial_intelligence ViDoRe_energy ViDoRe_government_reports ViDoRe_healthcare_industry ViDoRe_esg_reports_human_labeled_v2 ViDoRe_biomedical_lectures_v2_multilingual ViDoRe_economics_reports_v2_multilingual ViDoRe_esg_reports_v2_multilingual VisRAG_ArxivQA VisRAG_ChartQA VisRAG_MP-DocVQA VisRAG_SlideVQA VisRAG_InfoVQA VisRAG_PlotQA ViDoSeek-page ViDoSeek-doc MMLongBench-page MMLongBench-doc 23.4 25.1 24.8 49.4 10.9 33.7 30.6 35.2 38.4 51.3 22.8 17.6 45.4 16.7 5.3 19. 29.0 27.6 81.7 56.6 84.9 86.9 70.9 75.1 95.7 94.7 93.6 95.9 51.3 54.7 49.0 52.9 80.9 72.3 82.0 85.1 83.5 79.3 38.1 87.5 27.1 80.4 39.7 30.6 47.9 54.7 14.3 46.6 39.2 53.6 46.8 65.6 26.4 31.8 49.7 24.9 9.1 59.5 14.0 37.4 86. 57.5 91.6 94.6 74.1 96.8 99.6 95.3 98.8 99.3 63.4 49.5 54.2 55.4 87.4 86.1 89.7 92.6 88.6 76.5 32.6 90.3 36.9 85. 35.5 32.1 42.2 61.8 23.8 28.5 27.8 20.3 21.8 51.4 29.3 34.5 46.7 25.5 9.0 57.7 19.8 39.3 60.2 34.7 70.4 78.2 27.6 38.6 67.7 60.4 61.8 69.9 6.8 5.1 13.9 11.9 52.6 57.7 60.6 54.7 66.0 62.7 16.3 69.4 0.4 28.8 38.0 42.8 40.9 60.0 14.8 33.7 30.7 20.9 34.0 52.3 30.4 28.3 48.1 26.5 10.6 49.4 20.2 40.8 80.6 44.9 83.7 89.2 43.8 60.8 88.5 86.5 85.0 92.2 45.6 44.3 43.0 46.6 76.9 83.7 88.1 84.1 82.3 75.9 29.1 79.0 15.8 63.0 40.1 35.8 46.9 39.6 16.6 48.9 46.0 62.4 60.0 76.0 37.8 36.5 56.4 32.0 9.5 58.4 18.7 41. 73.3 38.3 80.6 80.7 37.8 52.0 86.0 84.8 85.0 88.4 50.7 50.9 54.3 42.3 74.0 82.7 75.1 87.6 87.9 69.4 22.5 73.8 13.3 42.6 35.8 44.1 54.4 67.2 20.1 49.9 41.7 59.9 45.4 57.8 32.4 34.3 55.4 29.9 12.7 57.5 20.4 41.2 73.9 37.9 76.2 86.1 40.6 66.8 85.9 83.3 82.6 90.8 50.2 46.2 45.7 42.7 74.3 86.0 75.6 87.1 84.4 68.0 21.2 75.9 11.9 39.7 42.8 50.4 58.3 70.0 21.5 58.2 47.3 69.6 52. 76.0 40.0 38.9 60.8 32.6 18.5 54.9 21.9 41.1 73.6 41.1 80.8 90.2 46.7 65.0 89.5 85.7 89.8 94.3 50.4 50.7 57.8 43.2 80.5 85.0 83.4 91.5 89.2 72.7 21.3 75.3 12.3 41.3 55.8 56.7 56.7 79.3 36.7 50.8 47.1 53.9 53.0 74.8 45.3 45.7 67.2 43.6 23.5 70.7 26. 50.9 86.1 45.7 86.8 94.5 54.6 70.7 94.0 86.7 89.0 91.1 56.9 51.0 53.0 46.9 84.9 88.3 79.1 92.3 90.0 73.0 82.0 82.6 47.7 50.3 56.8 59.5 60.1 78.5 33.0 55.9 50.5 58.2 52.8 74.4 46.8 46.2 65.8 43.4 23. 73.6 25.0 49.9 88.7 47.5 86.9 94.7 54.8 69.0 91.6 88.1 90.7 90.4 59.8 50.1 53.9 49.7 86.9 88.5 79.3 92.6 89.6 72.4 84.4 82. 51.0 50.7 26 H. Jiang, Y. Wang et al. Detailed Scores of MMEB-V1 We also report our performance on MMEB-V1 [26], including both in-domain and out-of-domain performance, as shown in Table 9. Table 9: Results on the MMEB-V1 benchmark (consisting of 36 image embedding tasks). IND and OOD denote the in-distribution and out-of-distribution datasets, respectively. The highest and second-highest values are highlighted in bold and underline. Model Per Meta-Task Score Average Score Classification VQA Retrieval Grounding IND OOD Overall # of Datasets 10 12 4 20 16 CLIP [43] BLIP2 [34] SigLIP [68] OpenCLIP [10] UniIR (BLIPFF) [58] UniIR (CLIPSF) [58] Magiclens [70] 42.8 27.0 40.3 47.8 42.1 44.3 38.8 Baseline Models 9.1 4.2 8.4 10.9 15.0 16.2 8.3 53.0 33.9 31.6 52.3 60.1 61.8 35.4 51.8 47.0 59.5 53.3 62.2 65.3 26. 37.1 38.7 25.3 25.1 32.3 38.0 39.3 40.2 44.7 40.4 47.1 41.7 31.0 23.7 37.8 25.2 34.8 39.7 42.8 44.7 27.8 E5-V [25] VLM2Vec-2B [26] VLM2Vec-7B [26] VLM2Vec-V2 [41] MMRet-7B [78] CAFe-V1-7B [64] CAFe-V2-7B [64] mmE5-11B [9] LLaVE-2B [29] LLaVE-7B [29] UniME-4B [18] UniME-7B [18] UME-R1-2B [30] UME-R1-7B [30] Embed-RL-2B Embed-RL-4B MLLM-based Baseline Models 21.8 59.0 62.6 62.9 56.0 65.2 63.6 67.6 62.1 65.7 54.8 66.8 64.8 67. 62.8 63.7 4.9 49.4 57.8 56.3 57.4 65.6 61.7 62.8 60.2 65.4 55.9 66.6 62.8 69.2 11.5 65.4 69.9 69.5 69.9 70.0 69.1 70.9 65.2 70.9 64.5 70.6 67.6 71.9 Ours 67.9 70.5 68.6 71. 19.0 73.4 81.7 77.3 83.6 91.2 87.6 89.7 84.9 91.9 81.8 90.9 77.2 84.9 13.3 14.9 11.5 60.1 66.0 52.6 65.8 72.2 57.8 64.9 68.8 59.9 64.1 68.0 59.1 69.8 75.8 62.4 67.6 72.8 61.1 69.8 72.3 66.7 65.2 69.4 59.8 70.3 75.0 64.4 64.2 68.2 52.7 70.7 74.6 65.8 71.5 60.4 66.6 76.1 65.1 71.3 90.4 91.4 71.9 65.9 69.2 74.3 67.3 71.2 Embed-RL: RL for Reasoning-Driven Multimodal Embeddings"
        },
        {
            "title": "F Prompt for synthesizing multimodal chain of thought",
            "content": "To enable precise guidance for visual reasoning and retrieval tasks, we design hierarchically structured prompting system that instructs models to execute visual analysis tasks across text, image, and video modalities. This system consists of two scenario-specialized core modules, which impose constraints on reasoning logic, output formatting, and evidence anchoring, and adheres to framework of two-round reasoning with fixed-format output to guarantee the consistency and accuracy of the generated results. F.1 Basic Visual Reasoning Prompts Such prompts guide the model to complete basic reasoning based on inputs, supporting various visual tasks, and are divided into 4 items according to inputs and objectives: Text-to-Image Retrieval Reasoning Prompts: Focus on text-to-image retrieval, extract key visual concepts, anchor textual evidence to output JSON keyword list, adapt to subtasks, and output results in fixed format. Image Reasoning Prompts: For image-based tasks, anchor visual evidence to output the 2D bounding box coordinates of key elements, locate core features, and complete reasoning and answer output in accordance with the process. Video Reasoning Prompts: Adapt to video sequence tasks, output 1-based key frame indices based on frame visual evidence, identify core frames, and generate results following fixed process. Text-to-Video Retrieval Reasoning Prompts: For text-to-video retrieval, extract visual concepts containing temporal dynamics, output JSON keyword list, and standardize result output according to subtasks. Text-to-Image Retrieval Visual Reasoning Prompt You are visual reasoning assistant specialized in texttoimage retrieval. Given text description and task, analyze the text content to determine the key visual concepts needed for retrieving matching images. Rules: 1. Keep reasoning concise and grounded in textual evidence. Limit each step to 12 sentences. 2. Base your reasoning solely on the textual content and the task description. 3. Rephrase the final answer to preserve its exact meaning, changing only wording/phrasing if needed. In your thinking process, you must extract and output key visual concepts from the text description. 4. Use JSON format with key text_keywords to specify the important keywords as list. 5. First, think between <thinking> and </thinking> while output necessary keywords from the text in JSON with key text_keywords. Then, based on the thinking contents, rethink between <rethink> and </rethink>. Finally, output the answer within <answer>...</answer>. 6. Your thought process should adapt to the task type: For captionbased retrieval (e.g., find image from caption): Extract key visual elements, objects, scenes, and relationships. For news retrieval (e.g., find news image from headline): Identify key people, locations, events, and contextual elements. For dialoguebased retrieval (e.g., find image from conversation): Summarize visual attributes, actions, and scene details. For questionbased retrieval (e.g., find factual image): Identify the key concepts and relationships. Now, process the following input: TASK/QUESTION: {question} 28 H. Jiang, Y. Wang et al. Image-based Visual Reasoning Prompt You are visual reasoning assistant. Given an image and task description or question, analyze the image stepbystep to produce the required output. The task may involve image retrieval, classification, question answering, or object identification. Rules: 1. Keep reasoning concise and grounded in visual evidence. Limit each step to 12 sentences. 2. Base your reasoning solely on the visual content of the image and the task description. 3. Rephrase the final answer to preserve its exact meaning, changing only wording/phrasing if needed. In your thinking process, you must output coordinates for the key visual element(s) relevant to 4. answering the question. Use JSON format with key bbox_2d to specify the bounding box as [x1, y1, x2, y2]. For multiple elements, use list of bboxes: [[x1, y1, x2, y2], [x1, y1, x2, y2]]. 5. First, think between <thinking> and </thinking> while output necessary coordinates needed to answer the question in JSON with key bbox_2d. Then, based on the thinking contents and coordinates, rethink between <rethink> and </rethink>. Finally, output the answer within < answer>...</answer>. 6. Your thought process should adapt to the task: For retrieval (e.g., find similar image): Identify and locate key visual elements that define the match. For classification (e.g., scene, object, domain): Locate distinguishing visual features that belong to the class. For question answering: Locate the visual or textual clues in the image that lead to the answer. For object identification/segmentation: Provide the objects location and boundaries. Now, process the following input: IMAGE: {image} TASK/QUESTION: {question} Video Sequence Visual Reasoning Prompt You are video reasoning assistant. Given video sequence (multiple frames) and task description, analyze the video content stepbystep to produce the required output. The task may involve video captioning, video question answering, or video retrieval. Rules: 1. Keep reasoning concise and grounded in visual evidence from the video frames. Limit each step to 12 sentences. 2. Base your reasoning solely on the visual content of the video frames and the task description. 3. Rephrase the final answer to preserve its exact meaning, changing only wording/phrasing if needed. In your thinking process, you must identify and output key frames from the video sequence that are 4. most relevant to answering the question. Use JSON format with key key_frames to specify the frame indices as list (using 1based indexing). 5. First, think between <thinking> and </thinking> while output necessary key frame indices in JSON with key key_frames. Then, based on the thinking contents, rethink between <rethink> and </ rethink>. Finally, output the answer within <answer>...</answer>. 6. Your thought process should adapt to the task type: For video captioning/description: Identify frames that show main events, transitions, or key moments in the video sequence. For video question answering: Locate frames that contain the visual evidence needed to answer the specific question. For video retrieval: Identify frames that represent the core content or distinguishing features of the video. Now, process the following input: VIDEO FRAMES: {video} TASK/QUESTION: {question} Text-to-Video Retrieval Visual Reasoning Prompt You are visual reasoning assistant specialized in texttovideo retrieval. Given video description and task, analyze the text content to determine the key visual concepts needed for retrieving matching video clips or keyframes. Rules: 1. Keep reasoning concise and grounded in textual evidence. Limit each step to 12 sentences. 2. Base your reasoning solely on the textual content and the task description. 3. Rephrase the final answer to preserve its exact meaning, changing only wording/phrasing if needed. Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 29 4. In your thinking process, you must extract and output key visual concepts from the text description. Use JSON format with key text_keywords to specify the important keywords as list. 5. First, think between <thinking> and </thinking> while output necessary keywords from the text in JSON with key text_keywords. Then, based on the thinking contents, rethink between <rethink> and </rethink>. Finally, output the answer within <answer>...</answer>. 6. Your thought process should adapt to the task type: For videobased retrieval (e.g., find video from description): Extract key visual elements, objects, scenes, actions, temporal sequences, and relationships. For scenebased retrieval (e.g., find video from scene description): Identify key people, locations, events, contextual elements, and temporal progression. For actionbased retrieval (e.g., find video from action sequence): Summarize visual attributes, actions, scene details, and temporal dynamics. Now, process the following input: TASK/QUESTION: {question} F.2 Positive Sample Verification Reasoning Prompts These prompts analyze the rationale behind positive samples to support model training and verification, and are categorized into three types according to the positive sample modality: Text Positive Sample Analysis Prompts: Anchor textual evidence to output JSON-formatted keyword list, generate text summary through two rounds of reasoning, and clarify the rationale for labeling the sample as positive. Image Positive Sample Analysis Prompts: Infer image content in conjunction with the target task, output bounding box coordinates of key elements, generate an image summary through two rounds of reasoning, and clarify the core features and grounding basis. Video Positive Sample Analysis Prompts: Anchor video evidence to output 1-indexed key frame indices, generate structured video analysis through two rounds of reasoning, and clarify the core rationale for positive sample validity. Positive Text Output Visual Reasoning Analysis Prompt You are visual reasoning assistant. Given task description and positive text output, analyze the text to determine the key concepts that make it the correct output. Rules: 1. Keep reasoning concise and grounded in the textual evidence. Limit each step to 12 sentences. 2. Use the task description to understand the context of the output. 3. In your thinking process, you must extract and output key concepts from the text. Use JSON format with key text_keywords to specify the important keywords as list. 4. First, think between <thinking> and </thinking> while output necessary keywords from the text in JSON with key text_keywords. Then, based on the thinking contents, rethink between <rethink> and </rethink>. Finally, output brief description of the text within <answer>...</answer>. Now, process the following input: POSITIVE TEXT OUTPUT: {pos_text} Positive Image Output Visual Reasoning Analysis Prompt You are visual reasoning assistant. Given task description and positive image output (with optional text template), analyze the task to determine what the image should contain and locate key regions. Rules: 1. Keep reasoning concise and grounded in the task description. Limit each step to 12 sentences. 2. Use the task description to infer what the target image should look like. 30 H. Jiang, Y. Wang et al. 3. In your thinking process, you must output coordinates for the key visual element(s) relevant to answering the question. If no question is specified, output the most important element. Use JSON format with key bbox_2d to specify the bounding box as [x1, y1, x2, y2]. For multiple elements, use list of bboxes: [[x1, y1, x2, y2], [x1, y1, x2, y2]]. 4. First, think between <thinking> and </thinking> while output necessary coordinates in JSON with key bbox_2d. Then, based on the thinking contents, rethink between <rethink> and </rethink >. Finally, output brief description of the image within <answer>...</answer>. Now, process the following input: POSITIVE TEXT OUTPUT: {pos_text} POSITIVE IMAGE OUTPUT: {pos_image_description} Positive Video Output Visual Reasoning Analysis Prompt You are visual reasoning assistant for video. Given task description and positive video output, analyze the output to determine the key moments or concepts that make it correct for the task. Rules: 1. Keep reasoning concise and grounded in the provided video output evidence. Limit each step to 1 sentences. 2. Use the task description to understand the context of the output. 3. In your thinking process, you must identify and output the indices of the most relevant or representative frames from the described video sequence. Use JSON format with key key_frames to specify the frame indices as list (using 1based indexing). 4. First, think between <thinking> and </thinking> while outputting necessary key frame indices in JSON with key key_frames. Then, based on the thinking contents, rethink between <rethink> and </rethink>. Finally, output brief analysis of the video output within <answer>...</answer >. Now, process the following input: POSITIVE TEXT OUTPUT: {pos_text} POSITIVE VIDEO OUTPUT: {pos_video_output} Embed-RL: RL for Reasoning-Driven Multimodal Embeddings"
        },
        {
            "title": "Retrieval",
            "content": "To fully validate the cross-dimensional generalization capability of our proposed model in complex and diverse retrieval scenarios, we conduct systematic performance evaluation on the Universal Video Retrieval Benchmark (UVRB) [20]. This benchmark consists of 16 datasets targeting distinct core capabilities, comprehensively covering multiple retrieval paradigms (textual, composed, and visual retrieval) and diverse semantic scenarios (coarse-grained, fine-grained, and longcontext retrieval). It thus enables accurate quantification of the models universal adaptation capacity across heterogeneous retrieval tasks. Experimental results demonstrate that our model outperforms counterparts with equivalent parameter scales, maintains consistent performance advantage, and achieves the optimal comprehensive capability among models of the same parameter scale. Specifically, in core retrieval dimensions: (1) it attains state-of-the-art performance in coarse-grained semantic retrieval tasks and the second-best result in fine-grained semantic understanding scenarios; (2) it secures the optimal and second-best performances in spatial fine-grained perception (object/appearance recognition) and temporal fine-grained perception (motion/dynamics capture) subtasks, respectively. These results fully highlight the models robust capability in multi-dimensional semantic understanding and spatiotemporal feature extraction. G.1 Video Retrieval Performance on UVRB Datasets Baselines. Following the experimental settings of GVE [20], we evaluate 16 representative baselines spanning diverse architectures, parameter scales, and training data compositions. These baselines are categorized into two groups: (1) traditional CLIP-based embedding models, including CLIP4Clip [40], ViCLIP [54], VideoCLIP-XL [48], LanguageBind [79], and the InternVideo2 (1B/6B) [55]; (2) recent MLLM-based embedding models, including GVE-2B/7B [20], GME-2B/7B [73], Unite-2B/7B [27], VLM2Vec-V2 [41], BGE-VL [77], UniME-7B [18], and B3-7B [46]. Datasets. We adopt the Universal Video Retrieval Benchmark (UVRB) [20], which assesses model universality via 16 test datasets targeting distinct core abilities. UVRB covers diverse retrieval scenarios, with datasets categorized as: coarse-grained retrieval (MSRVTT [62], DiDeMo [1], CRB-G [63]); fine-grained retrieval (spatial: CRB-S [63]/VDC-O [7], temporal: CRB-T [63]/CMRB [37], partially relevant: DREAM-E [49]/LoVR-Theme2Clip [5]/PEV-K [4]); longcontext retrieval (LoVR-V [5], VDC-D [7]); and composed query retrieval (MSTI/MS-TV adapted from MomentSeeker [67], MSRVTT-I2V [62], LoVR-C2V [5]). Metrics. Following GVE [20], we adopt Recall@1 (R@1) as the primary metric, which measures the accuracy of identifying the most relevant item. For challenging datasets with ambiguous queries (e.g., CMRB, LoVR-TH), we additionally report Recall@10 (R@10) to reflect performance on top-k retrieval. For 32 H. Jiang, Y. Wang et al. Table 10: Performance of video retrieval on UVRB datasets: AVG values represent the average across 16 datasets, with the highest score in each column bolded and the second-highest underlined. Metrics include R@1 (Recall@1), R@10 (Recall@10) and P@1 (Precision@1). The highest and second-highest values are highlighted in bold and underline. Model AVG MSRVTT DiDeMo CRB-G CRB-S VDC-O CRB-T CMRB DREAM-E 39.0 CLIP4Clip [40] 35.2 ViCLIP [54] 49.1 VideoCLIP-XL [48] 48.7 LanguageBind [79] InternVideo2-1B [55] 40.4 InternVideo2-6B [55] 42.7 48.8 GME-2B [73] 48.0 Unite-2B [27] 50.8 VLM2Vec-V2 [41] 44.3 BGE-VL [77] 52.1 UniME-7B [18] 51.1 B3-7B [46] 53.0 GME-7B [73] 53.8 Unite-7B [27] 54.4 GVE-3B [20] 57.3 GVE-7B [20] Embed-RL-2B Embed-RL-4B 58.7 60. R@1 33.3 38.6 44.3 47.9 44.9 48.5 39.0 36.7 33.0 33.7 35.1 28.2 43.6 43.9 43.1 46.4 43.7 44.0 R@1 R@1 R@ R@1 29.7 30.6 40.3 42.1 40.4 41.8 30.3 29.8 29.9 31.8 33.5 35.0 37.7 38.6 37.6 43.3 51.1 44.7 82.8 71.6 58.6 60.8 69.0 69.9 82.8 69.0 81.5 81.5 74.0 79.8 85.0 86.5 49.7 43.7 83.9 68.7 56.8 61.2 71.8 72.3 84.3 68.8 82.7 82.5 76.7 80.4 84.6 84.7 62.0 53.0 73.5 75.9 64.4 65.0 71.5 72.7 77.5 63.9 74.3 76.8 73.1 75.3 78.6 79.4 42.2 46. 91.4 92.1 89.7 89.9 84.6 85.9 R@1 28.9 34.9 48.7 46.6 47.0 45.5 40.0 40.9 41.0 35.9 47.6 41.5 44.2 47.2 49.6 53.9 49.4 53. R@10 R@1 28.0 22.9 27.4 29.0 35.5 34.6 29.8 28.4 28.6 22.5 31.7 31.2 30.4 35.1 36.3 39.8 36.5 38.2 19.1 23.5 26.3 28.0 24.2 27.1 24.0 22.3 22.8 21.2 29.3 21.6 27.4 27.9 28.0 30.2 31.8 32. Model CLIP4Clip [40] ViCLIP [54] VideoCLIP-XL [48] LanguageBind [79] InternVideo2-1B [55] InternVideo2-6B [55] GME-2B [73] Unite-2B [27] VLM2Vec-V2 [41] BGE-VL [77] UniME-7B [18] B3-7B [46] GME-7B [73] Unite-7B [27] GVE-3B [20] GVE-7B [20] Embed-RL-2B Embed-RL-4B LoVR-TH PEV-K LoVR-V VDC-D MS-TI MS-TV MSRVTT-I2V LoVR-C2V P@1 R@10 R@ R@1 R@1 R@1 R@1 P@1 33.8 20.2 43.9 42.5 29.8 30.2 44.6 44.5 49.2 38.7 50.4 46.2 52.3 55.5 52.2 54. 56.5 57.9 17.9 7.5 22.9 30.3 2.6 8.6 35.4 35.5 32.4 18.4 32.3 38.7 39.6 44.0 33.0 41.3 33.6 31.9 36.0 23.0 38.0 54.0 28.0 33.0 53.0 57.0 61.0 55.0 48.0 59.0 71.0 62.0 61.0 68.0 80.0 77.0 56.6 39.5 82.0 67.9 48.5 51.6 83.9 79.2 91.3 72.2 84.7 85.3 86.5 87.1 91.8 94. 93.8 95.2 17.3 28.3 23.0 22.8 26.5 23.5 35.0 25.0 27.5 30.3 31.0 27.5 34.8 27.8 34.0 34.3 19.3 15.8 18.3 24.3 22.3 23.3 23.0 20.5 34.0 23.3 25.0 23.3 30.5 26.5 33.3 23.0 26.8 28.0 21.0 21.0 92.4 84.6 86.1 82.7 79.4 86.8 82.7 86.3 84.1 77.9 86.7 88.4 86.0 88.3 89.1 89. 89.1 87.9 50.3 43.3 40.3 46.3 36.8 45.2 36.6 44.5 38.5 46.5 53.7 47.1 37.0 44.8 40.3 41.5 51.4 49.0 Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 33 MS-TI and MS-TV (containing multiple positive candidates), we use Precision@1 (P@1) as the key metric. Performance. Table 10 presents the video retrieval performance of diverse models on the Universal Video Retrieval Benchmark (UVRB) [20] datasets. Our Embed-RL-2B and Embed-RL-4B models outperform all baselines in the average score (AVG), with Embed-RL-4B achieving the highest AVG (0.602). Both models secure the first or second place on key datasets across different retrieval paradigms, including coarse-grained retrieval (DiDeMo [1], CRB-G [63]) and fine-grained/long-context retrieval (CRB-S [63], VDC-O/VDC-D [7]), which verifies their superior multi-dimensional video retrieval capability. G.2 Capability Characterization of the UVRB Evaluation Metrics The Universal Video Retrieval Benchmark (UVRB) adopts unweighted arithmetic means for all metric calculations to ensure fair comparison across heterogeneous datasets, and comprehensively evaluates video retrieval models from three orthogonal dimensions (Tasks, Domains, Sub-domains). UVRB covers 16 datasets that are exhaustively partitioned into non-overlapping categories according to Tasks, Domains and Sub-domains, as detailed in Table 11. This partition is the foundation for quantifying distinct model capabilities in different retrieval scenarios. Table 11: Detailed Partition of Datasets in the Universal Video Retrieval Benchmark (UVRB) Across Tasks, Domains, and Sub-domains Partition DTXT DCMP DVIS DCG DFG DLC DS DT DPR Content {MSRVTT, DiDeMo, CRB-G, CRB-S, VDC-O, CRB-T, CMRB, DREAM-E, LoVR-TH, PEV-K, LoVR-V, VDC-D} {MS-TI, MS-TV} {MSRVTT-I2V, LoVR-C2V} {MSRVTT, DiDeMo, CRB-G} {CRB-S, VDC-O, CRB-T, CMRB, DREAM-E, LoVR-TH, PEV-K} {LoVR-V, VDC-D} {CRB-S, VDC-O} {CRB-T, CMRB} {DREAM-E, LoVR-TH, PEV-K} Based on the above dataset partition, Table 12 defines the calculation rules and corresponding capability characterization for each evaluation dimension. All metrics are computed as unweighted arithmetic means of corresponding datasets (denoted as D), with results rounded to three decimal places. The three core dimensions are defined as follows: 34 H. Jiang, Y. Wang et al. 1. Task dimension: Distinguishes retrieval paradigms by query formats (textual, composed, visual), reflecting cross-modal alignment ability for different query types; 2. Domain dimension: Assesses model performance across different levels of semantic granularity (coarse-grained, fine-grained, long-context), measuring generalization on retrieval tasks involving short/long context and high/lowlevel semantics; 3. Sub-domain dimension: Further decomposes fine-grained retrieval into three sub-tasks (spatial, temporal, partially relevant), pinpointing model strengths and weaknesses in fine-grained understanding. The overall AVG score is the arithmetic mean of the three task columns (TXT, CMP, VIS) and three domain columns (CG, FG, LC). This score aggregates model performance across core retrieval paradigms, rather than taking raw averages over datasets. Table 12: Calculation Rules and Capability Characterization for Model Evaluation on UVRB  (Table 13)  . All Metrics Are Computed as Unweighted Arithmetic Means of Corresponding Datasets (Denoted as D), with Results Rounded to Three Decimal Places. Dimension Level Column Name Capability Characterization Tasks Tasks Tasks Domains Domains Domains Sub-domains Sub-domains Sub-domains TXT CMP VIS CG FG LC PR Text-to-video retrieval (12 datasets) Composed retrieval (text+image/video, 2 datasets) Visual-to-video retrieval (image/video, 2 datasets) Coarse-grained semantic retrieval (3 datasets) Fine-grained semantic retrieval (7 datasets) Long-context retrieval (long text/video, 2 datasets) Spatial fine-grained (object/appearance, 2 datasets) Temporal fine-grained (motion/dynamics, 2 datasets) Partially relevant retrieval (3 datasets) Overall AVG Aggregated task and domains performance (core retrieval paradigms) Table 13 presents the video retrieval performance of mainstream models on UVRB, characterized by the above-defined abilities (Tasks, Domains, and Sub-domains). The AVG score is the arithmetic mean of performance across textual (TXT), composed (CMP), and visual (VIS) retrieval tasks, aggregating model performance across core retrieval paradigms (rather than raw averages over datasets). Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 35 Table 13: Video Retrieval Performance of Models on UVRB by Ability Dimensions (Tasks, Domains, Sub-domains). The AVG score denotes the mean performance across textual (TXT), composed (CMP), and visual (VIS) retrieval tasks. Domains involve coarse-grained (CG), fine-grained (FG), and long-context (LC) retrieval, while subdomains include spatial (S), temporal (T), and partially relevant (PR) retrieval. The highest and second-highest values are highlighted in bold and underline."
        },
        {
            "title": "Domains",
            "content": "Sub-domains"
        },
        {
            "title": "41.6\nCLIP4Clip [40]\n37.5\nViCLIP [54]\n51.0\nVideoCLIP-XL [48]\n50.8\nLanguageBind [79]\nInternVideo2-1B [55] 42.0\nInternVideo2-6B [55] 44.5\n41.6\nGME-2B [73]\n50.7\nUnite-2B [27]\n53.8\nVLM2Vec-V2 [41]\n48.0\nBGE-VL [77]\n54.2\nUniME-7B [18]\n53.8\nB3-7B [46]\n56.2\nGME-7B [73]\n55.9\nUnite-7B [27]\n57.1\nGVE-3B [20]",
            "content": "T PR 17.8 71.4 38.0 36.0 46.3 55.9 28.5 23.6 40.1 26.3 64.0 38.0 31.5 31.3 48.4 28.9 17.1 33.6 22.7 63.2 55.8 49.3 60.0 78.7 38.1 31.0 55.0 23.1 64.5 53.9 47.9 61.0 72.3 37.8 33.6 54.3 24.8 58.1 48.0 40.3 38.3 60.6 41.3 18.9 42.2 44.8 22.0 66.0 50.4 41.7 42.3 63.1 40.0 22.0 53.9 34.5 59.7 46.1 47.1 68.5 71.6 34.9 34.7 24.2 65.4 45.5 47.1 68.1 72.5 34.7 34.1 53.6 26.3 61.3 49.8 50.2 76.2 80.9 34.8 34.8 58.7 26.8 62.2 44.8 40.6 63.6 66.4 29.2 26.1 49.7 30.8 70.2 50.0 51.8 66.4 78.5 39.6 37.3 56.1 27.0 67.8 48.2 50.5 72.2 79.7 36.4 35.5 57.0 34.1 61.5 51.8 50.7 78.8 74.9 37.3 39.8 60.4 25.4 66.6 54.1 53.9 74.6 77.9 41.2 42.5 60.9 30.4 64.7 55.2 54.1 76.4 81.6 43.0 37.7 61.9 Embed-RL-2B Embed-RL-4B 20.1 70.3 59.1 54.6 86.9 87.2 43.0 40.6 58.7 61.1 58.5 62.0 18.4 70.3 60.7 55.6 86.1 87.9 46.0 40.6 36 H. Jiang, Y. Wang et al."
        },
        {
            "title": "H Training Trajectory Dynamics",
            "content": "This section details dynamic training trajectories of 2B and 4B Embed-RL models via core metrics. H.1 Training Metrics We track key metrics throughout the reinforcement learning phase, as shown in Figure 6. We observe that the entropy declines gradually and then plateaus, while the response length increases steadily with ongoing training. Meanwhile, the reward exhibits fluctuating upward trend, which is attributed to the effects of in-batch reward sample sampling as the discrepancies between individual samples far outweigh the inherent growth of the reward itself. Fig. 6: Key RL-phase metrics of Embedder-Guided RL (entropy, response length, reward). Additionally, we track two core training metrics for 2B and 4B-scale EmbedRL models during contrastive learning: contrastive training loss and gradient norm, as presented in Figure 7. Based on our training experience, neither an excessively large nor an overly small converged loss is favorable.An overly large converged loss suggests that the model fails to correctly discriminate positive and negative samples, whereas an excessively small one indicates that in-batch negative samples are too easily distinguished, leaving the model unable to learn effective discriminative information.Proper adjustment of the sampling ratio and sub-batch size is thus required to ensure the model converges correctly. Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 37 Fig. 7: Contrastive training loss and gradient norm of 2B and 4B scale Embed-RL models."
        },
        {
            "title": "I Efficiency and Latency",
            "content": "The proposed evidential Traceability CoT (T-CoT) demonstrates prominent efficiency advantages and negligible latency overhead compared with traditional generative embedding methods. For any multimodal retrieval target, T-CoT only needs to be generated once offline, and the derived multimodal embedding vector can be directly stored in the retrieval database. This is different from generative embedding approaches that require on-the-fly reasoning chain and embedding generation for each query, which leads to repeated computational costs. Designed to be targeted and concise, T-CoT only extracts core retrievalrelated multimodal cues such as text keywords, image bounding boxes and video keyframes while abandoning redundant content. This ensures it does not significantly increase single embedding inference latency. Additionally, the stable semantic representation of T-CoT-based embeddings allows for long-term caching and reuse in subsequent tasks without frequent re-generation or updates. This further reduces inference latency and computational consumption in largescale scenarios and makes the framework more suitable for practical industrial deployment. 38 H. Jiang, Y. Wang et al."
        },
        {
            "title": "J Limitations",
            "content": "This work has several notable limitations. First, the weight coefficients of the multi-component reward function are empirically set for simplicity, lacking an adaptive optimization mechanism for diverse multimodal tasks, which may lead to suboptimal performance in specific scenarios. Second, the constructed dataset excludes partial classification tasks, resulting in relatively weak performance on image classification subtasks; we recommend designing additional loss for classification tasks to avoid false negatives while adapting to large-batch contrastive loss. Finally, we have not applied any hard negative sample mining or curriculum learning strategies, which are expected to further enhance the models discriminative capability and training stability if incorporated."
        },
        {
            "title": "K Exploratory Perspectives",
            "content": "In numerous practical systems such as Multimodal Content Understanding, Recommendation Systems (RS), and Retrieval-Augmented Generation (RAG), high-quality Universal Multimodal Embeddings serve as the core foundation supporting downstream tasks. The quality of embeddings directly determines the performance ceiling of downstream taskswhether it is the matching accuracy of recommendation systems, the answer correctness of RAG systems, or the semantic alignment performance of cross-modal tasks, all are closely related to it. In existing solutions, to extract richer semantic information, multi-dimensional feature fusion is often performed through techniques such as semantic segmentation, image and video fine-grained understanding, and user profile analysis. However, this approach typically incurs additional computational and storage overhead, and the feature fusion module suffers from high design complexity. To address this, we strive to deeply internalize key information capture capabilities, including core visual region localization, video keyframe extraction, and core semantic keyword mining, into the embedding large model itself. This exploration aims to simplify the model structure of feature fusion, reduce the computational and storage costs of semantic vectors, and simultaneously enable embedding results to naturally incorporate task-relevant core semantic information. Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 39 Comparative Examples of T-CoT Before and After Embedder-Guided RL Figures 8 to 14 present additional comparative examples of T-CoT before and after the application of Embedder-Guided Reinforcement Learning. These examples fully demonstrate that after RL optimization, the Reasoner achieves more accurate localization of key regions and exhibits improved embedding quality, which further validates the effectiveness of the proposed EG-RL approach in enhancing the performance of T-CoT. Fig. 8: Example 1 of T-CoT Before and After EG-RL. 40 H. Jiang, Y. Wang et al. Fig. 9: Example 2 of T-CoT Before and After EG-RL. Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 41 Fig. 10: Example 3 of T-CoT Before and After EG-RL. 42 H. Jiang, Y. Wang et al. Fig. 11: Example 4 of T-CoT Before and After EG-RL. Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 43 Fig. 12: Example 5 of T-CoT Before and After EG-RL. 44 H. Jiang, Y. Wang et al. Fig. 13: Example 6 of T-CoT Before and After EG-RL. Embed-RL: RL for Reasoning-Driven Multimodal Embeddings 45 Fig. 14: Example 7 of T-CoT Before and After EG-RL."
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology",
        "Tsinghua Shenzhen International Graduate School, Tsinghua University"
    ]
}