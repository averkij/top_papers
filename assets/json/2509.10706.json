{
    "paper_title": "Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson Method",
    "authors": [
        "Chin-Yun Yu",
        "György Fazekas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automatic differentiation through digital signal processing algorithms for virtual analogue modelling has recently gained popularity. These algorithms are typically more computationally efficient than black-box neural networks that rely on dense matrix multiplications. Due to their differentiable nature, they can be integrated with neural networks and jointly trained using gradient descent algorithms, resulting in more efficient systems. Furthermore, signal processing algorithms have significantly fewer parameters than neural networks, allowing the application of the Newton-Raphson method. This method offers faster and more robust convergence than gradient descent at the cost of quadratic storage. This paper presents a method to emulate analogue levelling amplifiers using a feed-forward digital compressor with parameters optimised via the Newton-Raphson method. We demonstrate that a digital compressor can successfully approximate the behaviour of our target unit, the Teletronix LA-2A. Different strategies for computing the Hessian matrix are benchmarked. We leverage parallel algorithms for recursive filters to achieve efficient training on modern GPUs. The resulting model is made into a VST plugin and is open-sourced at https://github.com/aim-qmul/4a2a."
        },
        {
            "title": "Start",
            "content": "Presented at the AES International Conference on Artiﬁcial Intelligence and Machine Learning for Audio 2025 September 810, London, UK 5 2 0 2 2 1 ] . e [ 1 6 0 7 0 1 . 9 0 5 2 : r This paper was peer-reviewed as complete manuscript for presentation at this conference. This paper is available in the AES E-Library (http://www.aes.org/e-lib), all rights reserved. Reproduction of this paper, or any portion thereof, is not permitted without direct permission from the Journal of the Audio Engineering Society. Sound Matching an Analogue Levelling Ampliﬁer Using the Newton-Raphson Method Chin-Yun Yu1 and György Fazekas1 1Centre for Digital Music, Queen Mary University of London, London, UK Correspondence should be addressed to Chin-Yun Yu (chin-yun.yu@qmul.ac.uk)"
        },
        {
            "title": "ABSTRACT",
            "content": "Automatic differentiation through digital signal processing algorithms for virtual analogue modelling has recently gained popularity. These algorithms are typically more computationally efﬁcient than black-box neural networks that rely on dense matrix multiplications. Due to their differentiable nature, they can be integrated with neural networks and jointly trained using gradient descent algorithms, resulting in more efﬁcient systems. Furthermore, signal processing algorithms have signiﬁcantly fewer parameters than neural networks, allowing the application of the Newton-Raphson method. This method offers faster and more robust convergence than gradient descent at the cost of quadratic storage. This paper presents method to emulate analogue levelling ampliﬁers using feed-forward digital compressor with parameters optimised via the Newton-Raphson method. We demonstrate that digital compressor can successfully approximate the behaviour of our target unit, the Teletronix LA-2A. Different strategies for computing the Hessian matrix are benchmarked. We leverage parallel algorithms for recursive ﬁlters to achieve efﬁcient training on modern GPUs. The resulting model is made into VST plugin and is open-sourced."
        },
        {
            "title": "1.1 Problem Deﬁnition",
            "content": "Controlling the dynamic range of an audio signal is fundamental task in audio engineering. Audio effects such as compressors, limiters, and expanders are designed to achieve this goal and are often used in music production, broadcasting, and live sound reinforcement. Analogue compressors developed in the early days have iconic sounds, such as the popular Teletronix LA-2A, which is known for its smooth and musical compression. Considerable interest has been devoted to replicating their characteristics in the digital world, known as virtual analogue (VA) modelling [1]. Let us denote an audio stimulus as x(t) and its discrete version as = [x(0), x(T ), . . . , x((N 1)T )](cid:62) RN with R+ being the sampling period. We want to model target analogue compressor δ : R, speciﬁcally, its behaviour to the stimulus x(t) as y(t) = δ (x(s) t). We denote = [y(0), y(T ), . . . , y((N 1)T )](cid:62) RN as the sampled compressed signal. Given controllable base model : RN RM RN parametrised by parameters, we aim to ﬁnd the optimal parameters θ RM that minimises the distance between the output ˆy = (x, θ ) and according to distance function : RN RN R+."
        },
        {
            "title": "Yu and Fazekas",
            "content": "Analogue Levelling Ampliﬁer Emulation"
        },
        {
            "title": "1.2 Related Work",
            "content": "In recent years, neural networks (NNs) with various architectures have become popular base models for . Hawley et al. [2] are pioneers in this approach, using an U-Net-like autoencoder with skip connections to model the LA-2A end-to-end. Steinmetz and Reiss [3] proposed temporal convolutional networks (TCNs) to enhance efﬁciency while maintaining large receptive ﬁelds of the networks. Different conditioning strategies have been explored to improve the modelling ability [4, 5]. More recent works explored the use of statespace models for VA modelling [6, 7]. The choice of architecture is crucial since compressors are used in real-time applications, and the computational cost is concern. Despite impressive results, the black-box nature of NNs makes the learnt parameters difﬁcult to interpret and limits creative control. Among previous works, Wright and Välimäki [8] proposed grey-box approach that combines NNs and signal processing components. feed-forward compressor is used as with lightweight Gated Recurrent Unit (GRU) at the end to model time-varying non-linear make-up gain. This approach achieves competitive results while requiring only 5% of arithmetic operations compared to single GRU. The learnt control parameters also give us better understanding of the target units behaviour. Yu et al. [9] further improve their training speed by implementing specialised kernel for backpropagating the gradients through the ballistics module of the compressor."
        },
        {
            "title": "1.3 Motivation",
            "content": "Both [8] and [9] use gradient descent optimisation to ﬁnd the optimal feed-forward compressor parameters for VA modelling. Gradient descent is widely used in training neural networks instead of more robust methods like Newton-Raphson (NR) due to its simplicity and lower storage cost, favouring methods with many parameters (M (cid:29) 106). However, since the feedforward compressor has only handful of parameters (M < 10), the quadratic storage cost of NR is feasible on modern computers. NR converges faster and more robustly than gradient descent when the objective function is convex around the solution and twice differentiable. This motivates us to explore the NR method for VA modelling of analogue compressors. Although NR has been used to solve differential equations in white-box models, such as wave digital ﬁlters [10] and the physical modelling of musical instruments [11], to the authors knowledge, it has not been used for grey-box modelling of analogue compressors."
        },
        {
            "title": "1.4 Contributions",
            "content": "In this article, we formulate VA modelling as sound matching problem, aiming to ﬁnd the optimal parameters of digital compressor with the closest match to the target sound. We verify the feasibility of using the NR method for modelling the LA-2A compressor. Moreover, we further accelerate the backpropagation method from [9] on modern GPUs using parallel algorithms for recursive ﬁlters. The learnt mapping from the circuits peak reduction to the parameters is presented, providing an interpretable and intuitive way to control the compressor. We made the resulting model into VST plugin, open-sourced under the MPL-2.0 license1, for validation and as viable music production tool."
        },
        {
            "title": "2 Methodology",
            "content": "Continuing the work from [8, 9], we use feed-forward compressor as the base model for its efﬁciency and high modelling capability. We will discuss the choice of distance function in Section 2.2. Since the input signal is ﬁxed during optimisation, we use fx : RM RN to denote the curried given x, to simplify the later derivative notations."
        },
        {
            "title": "2.1 Feed-Forward Compressor ( f )",
            "content": "We adopt torchcomp [9], differentiable implementation of the feed-forward compressor from [12]. digital compressor is deﬁned as: ˆy[n] = x[n]g[n] = x[n]g (s[m], θ n) , (1) where : Rn+1 R+ is the gain reduction function with parameters θ and s[m] is the side-chain signal. The design is feed-forward when s[m] = x[m] [13]. We remove the root mean square (RMS) level detector from the design for two reasons. Firstly, Table 7 in [9] shows the converged RMS smoothing coefﬁcient is close to one, implying the smoothing effect is negligible. Secondly, we found that when running the NR method, the converged D(ˆy, y) is slightly lower without it. possible explanation is that the extra degree of freedom 1github.com/aim-qmul/4a2a AES International Conference on Artiﬁcial Intelligence and Machine Learning for Audio, London, UK 2025 September 810 Page 2 of"
        },
        {
            "title": "Yu and Fazekas",
            "content": "Analogue Levelling Ampliﬁer Emulation introduced by the RMS ﬁlter creates local minimum that traps the NR optimisation, phenomenon we will discuss in Section 2.2. The resulting compressor consists of ﬁve parameters (M = 5): threshold (CT ), ratio (R), attack time (tat ), release time (trt ), and make-up gain (γ). Threshold and make-up gain are expressed in decibels, and the attack/release times are in milliseconds. Threshold controls the level at which the compressor starts to work, and the ratio determines the amount of gain reduction. The make-up gain adjusts the output level to compensate for the gain reduction. The attack and release time control the ballistics of the gain reduction, which is deﬁned as: Fig. 1: Block diagram of the feed-forward compressor. AT/RT refer to the ballistics control in Eq. ( 2). function : RM R. It updates the parameter θ by the following equation: θ (cid:55) θ [2L (θ )]1L (θ ). (5) g[n] = (cid:40) αat ˆg[n] + (1 αat )g[n 1] αrt ˆg[n] + (1 αrt )g[n 1] if ˆg[n] < g[n 1], otherwise, αat/rt = 1 exp (cid:18) (cid:19) . 2200T tat/rt Here (θ ) = D( fx(θ ), y), and (θ ) RM and 2L (θ ) RMM are the gradient and Hessian matrix of at θ , respectively. NR leverages second-order information from the Hessian; thus, it guarantees convergence to local minimum and converges faster than gradient descent, especially when the initial point is close to the minimum. (2) (3) ˆg[n] is the gain reduction before applying the ballistics. Equation ( 2) is recursive smoothing ﬁlter, and the attack and release times determine how fast the ﬁlter responds to the loudness change. The whole signal ﬂow is shown in Fig. 1. Both threshold and make-up gain can be any real numbers, but not for the other parameters. We also want to bound the parameters to some reasonable range that is common in practice. To enforce these constraints, we deﬁne the parameter vector as θ = {CT, γ, ˆR, ˆαat , ˆαrt } R5 and get (cid:110) λ (u) { ˆR, ˆαat , ˆαrt }, {R, αat , αrt } = (cid:111) λ (u) = θmin(u) + (cid:0)θmax(u) θmin(u)(cid:1)σ (u) , (4) where σ : [0, 1] is the monotonic sigmoid function, and θmin : and θmax : return the lower and upper bounds of the parameters, respectively."
        },
        {
            "title": "2.2 Newton-Raphson Optimisation",
            "content": "Newton-Raphson is fundamental optimisation method that ﬁnds the minimum of smooth and convex NR has two requirements that need to be satisﬁed: 1. must be convex around the optimal θ . 2. must be twice differentiable. We validate 1) empirically by observing the convergence behaviour, which we will discuss more in Section 4.4. For 2), we set the distance function to be the square distance D( fx(θ ), y) = ( fx(θ ) y)(cid:62)( fx(θ ) y), which is twice differentiable. Regarding the differentiability of the feed-forward compressor , although torchcomp computes the sub-gradients for operations that are not differentiable everywhere (the absolute and minimum functions in Fig. 1 and the if-else statement in Eq. ( 2)), it is shown that they are accurate enough for ﬁrst-order optimisation. Since its gradient backpropagation function is also differentiable, we can quickly evaluate its second-order derivatives within automatic differentiation frameworks like PyTorch [14]."
        },
        {
            "title": "3 Hessian Computation",
            "content": "torchcomp implements the Vector-Jacobian product (VJP) [15, 16] (cid:55) v(cid:62) fx(θ ) of the compressor, which lets us compute the VJP of the loss function (cid:55) v(cid:62)L (θ ) using backpropagation where AES International Conference on Artiﬁcial Intelligence and Machine Learning for Audio, London, UK 2025 September 810 Page 3 of"
        },
        {
            "title": "Yu and Fazekas",
            "content": "Analogue Levelling Ampliﬁer Emulation (θ ) = fx(θ )D( fx(θ ), y)θ fx(θ ). We get the gradient by setting = 1. The gradients β [n]L (θ ) can be computed using chain rule: To get the Hessian matrix 2L (θ ), note that the Hessian matrix is the Jacobian of (θ ). Given an identity matrix RMM, if we pass the ith column of into the VJP of : RM (cid:55) RM, which is (cid:55) v(cid:62)2L (θ ), the result will be the ith row of the Hessian 2L (θ ); computing the entire Hessian equals passing every column of to the VJP function in parallel (I(cid:62)2L (θ )). Although we can get the Hessian using VJP twice (which is known as reverse-over-reverse mode) as we show above, the second (sometimes also called outer) Jacobian can also be computed with Jacobian-Vector product (JVP) (cid:55) h(θ )v [17]. This is called forwardover-reverse mode where = . Since the Jacobian of h(θ ) is square matrix, computing it by passing the basis vectors, either from the right or the left, has to be repeated the same number of times. The efﬁciency thus depends on the computational costs of VJP and JVP of : RM RM, although in practice, the forward-over-reverse strategy is usually preferred. For fair comparison, we derive the JVP of the function (cid:55) v(cid:62) fx(θ ), speciﬁcally, the backpropagation of the ballistics module in Eq. ( 2), as it involves recursive ﬁlters and is non-trivial to implement in PyTorch."
        },
        {
            "title": "3.1 Forward-Mode Gradients for Time-Varying",
            "content": "One-Pole Filters To get the gradients of Eq. ( 2), let us rewrite it as time-varying one-pole ﬁlter: , rt at α 1ζ [n] β [n] = 1 α ζ [n] g[n] = (1 β [n]) ˆg[n], + β [n] (cid:124)(cid:123)(cid:122)(cid:125) multiplier = g[n] (cid:124)(cid:123)(cid:122)(cid:125) input , g[n 1] (cid:124) (cid:123)(cid:122) (cid:125) previous output g[n] (cid:124)(cid:123)(cid:122)(cid:125) output (6) (7) (8) where ζ [n] {0, 1} is the attack and release phase indicator function. We colour-coded Eq. ( 8) to emphasise the recursions as it will show up multiple times in the derivations, and we have special treatment for it in Section 4.2. According to [9], backpropagating g[n]L (θ ) through Eq. ( 8) can be expressed as passing through timevarying one-pole ﬁlter in reversed time: g[n]L (θ ) = g[n]L (θ ) + β [n + 1] g[n+1]L (θ ). (10) (θ ), and ˆg[n]L (θ ) β [n]L (θ ) = g[n]L (θ )(g[n 1] ˆg[n]). (θ ), αrt The gradients αat are trivial to compute given Eq. ( 6) and Eq. ( 7). Backpropagation of g[n]L (θ ) simplify (θ ) as for brevity) through Eq. ( 9) can be done in the same way as in Eq. ( 8): g[n]L (θ ) = g[n]L (θ ) + β [n]g[n1]L (θ ), (11) (we β [n+1]L (θ ) = g[n]L (θ ) g[n+1]L (θ ). (12) For forward propagation (JVP) on Eq. ( 9), let us use Eq. ( 8) as an example. Since in the forward mode the Jacobians are multiplied from right to left (opposite to the backpropagation), we need to apply Eq. ( 9) and Eq. ( 10) in reverse order and traverse the computational graph from the outputs back to the inputs. This also means applying recursive ﬁlters in the opposite direction. Given forwarded gradients θ g[n] and θ β [n], the forward version of Eq. ( 9) is: θ g[n] = [θ g[n] + θ β [n] (g[n 1] ˆg[n])] + β [n]θ g[n 1], (13) which is just changing the input of Eq. ( 8) to θ g[n] + θ β [n](g[n 1] ˆg[n]). Applying the same process to Eq. ( 9), we can get the forward version of Eq. ( 11) similar to Eq. ( 13) as: θ g[n]L (θ ) = [θ g[n]L (θ ) + θ β [n] g[n+1]L (θ )] + β [n + 1]θ g[n+1]L (θ ). (14) Comparing the VJP of Eq. ( 9) (Eq. ( 11) and Eq. ( 12) combined) to its JVP (Eq. ( 14)), we see that the JVP has one more addition than the VJP. Nevertheless, the extra addition does not occur within the recursion, and modern computers are very good at vectorised atomic addition, so the computation time is often negligible. In summary, in this section, we show that propagating the gradients of time-varying one-pole ﬁlter in forward mode equals ﬁltering the gradients with the same ﬁlter2. The resulting Eq. ( 13) is required for the JVP of the inner Jacobian (cid:55) (θ )v, while Eq. ( 14) is required for the JVP of the outer Jacobian (cid:55) h(θ )v. (9) 2This result generalises to higher order all-pole ﬁlters as well. AES International Conference on Artiﬁcial Intelligence and Machine Learning for Audio, London, UK 2025 September 810 Page 4 of"
        },
        {
            "title": "4.1 Dataset",
            "content": "We use the recordings from SignalTrain [18] as it is the largest curated analogue compressor dataset known to the authors and has been utilised in many previous works [3, 8, 9, 5]. It contains paired recordings sampled at 44.1 kHz from LA-2A. Technically, the LA-2A is levelling ampliﬁer (LA) and does not have the parameters mentioned in Section 2.1. It has only one knob, called peak reduction, that controls the compression. The peak reduction was sampled from 0 to 100 with spacing of 5, and the gain value was ﬁxed for the compressor and limiter modes. We select recordings labelled with 3c in the ﬁle name, which use an identical 20 min audio stimulus consisting of both musical and synthetic sounds for all conﬁgurations."
        },
        {
            "title": "4.2 Acceleration Techniques",
            "content": "We implement everything in PyTorch. To accelerate the training process, we split the audio into 12 chunks with 1 overlap, so fx, which is inherently sequential in contrast to fx, can be parallelised at the batch level. The overlap parts are used as warm-up, and the loss is computed only on each chunks last 11 s. Following [8], 10.995z1 to and ˆy we apply pre-emphasis ﬁlter prior to computing the loss to reduce low-frequency noise. 1z1 Furthermore, we identify that lot of the operations are one-pole ﬁlters (Eq. ( 8)), and parallel algorithm exists for these ﬁlters called parallel associative scan [19]. The idea is to convert the recursion into form like s[0] s[1] . . . s[n] where is associative, then the scan algorithm can be applied. We use the CUDA implementation from [20] to parallelise the coloured equations (except Eq. ( 8)) and the pre-emphasis ﬁlter on the GPU."
        },
        {
            "title": "4.3 Analysis on Computational Cost",
            "content": "PyTorch has two implementations for computing the Hessian. One lies under the autograd module, which is the default implementation and has been around since PyTorch 1.5. It only has full coverage for reverse-overreverse (hereafter referred to as rev-rev) mode. The other implementation is the func module, introduced in PyTorch 2.0, which is relatively new but allows for Analogue Levelling Ampliﬁer Emulation the ﬂexible composition of forward and reverse mode Jacobians. We evaluate all the four combination strategies for the Hessian 2L (θ ) using func and compare them to the default autograd implementation. Table 1: The memory cost (in MB) and runtime (in milliseconds) of different hessian computation strategies in PyTorch. autograd rev-rev func rev-rev fwd-rev rev-fwd fwd-fwd Memory Time 1066 26. 2358 28.1 3534 28.1 6306 64.7 5364 38.9 We report the computational cost in Table 1, measured on an RTX 3060 with batch size of 16. From Table 1, we see that the rev-rev and fwd-rev modes are equally fast, which implies that the extra addition we mentioned in Section 3 does not impact the performance. The revfwd mode is the slowest and most memory-consuming. We see that fwd-rev requires more memory than rev-rev, probably due to non-optimal memory utilisation for the forward gradients in our implementation3. The table also shows inherent overhead in memory and time for the func module, as the numbers are all bigger than those of autograd. This is likely because this feature is still in the Beta stage and has not been optimised enough. Based on the above ﬁndings, we choose the autograd implementation. All the computations for each conﬁguration can be ﬁtted on single RTX 3060 with 12GB of VRAM, and no mini-batching is required. The resulting optimisation takes roughly just 3.5 for one NR update. Moreover, training on the entire dataset takes less than 20 min, instead of hours as reported in [8]."
        },
        {
            "title": "4.4 Optimisation Strategy",
            "content": "We perform the damped NR method with backtracking line search [21]. In each step, the parameters are updated as θ (cid:55) θ τν, where τ [0, 1] is the step size and ν is the solution of 2L (θ )ν = (θ ). We use an equation solver to avoid performing matrix inversion for better numerical stability. Starting from τ = 1, we divide τ by 2 until the ArmijoGoldstein 3In our ﬁlter implementation, only one dimension of the tensor is treated as batch dimension. The straightforward way to call the JVP times in parallel using the same function is to duplicate the parameters times along the batch dimension. AES International Conference on Artiﬁcial Intelligence and Machine Learning for Audio, London, UK 2025 September 810 Page 5 of"
        },
        {
            "title": "Yu and Fazekas",
            "content": "Analogue Levelling Ampliﬁer Emulation condition [22] (θ τν) (θ ) ατL (θ )(cid:62)ν is satisﬁed with α = 0.0001. If the Hessian matrix is not positive semi-deﬁnite (which implies negative curvature), we randomly sample new direction orthogonal to ν and repeat the process. We ﬁnd different optimal parameters for each sampled peak reduction. Since the NR method is sensitive to the initialisation, we start from 100 peak reduction which has the heaviest compression so it is easier to ﬁt, and then use the previous result to initialise the subsequent peak reduction, e.g., 100 95 90, etc. This ensures the starting point for each conﬁguration is as close to the solution as possible. The initial values are determined empirically based on early experiments and shared beliefs of the target circuit, which are CT = 36 dB, γ = 0 dB, = 4, tat = 1 ms, and trt = 200 ms. We restrict the range of the parameters to [1, 20], tat [0.1, 100] ms, and trt [10, 1000] ms. The optimisation converges in less than 10 iterations in most cases. We stopped the process at 40 peak reduction, as the NR method could not ﬁnd consistent solution for lower peak reductions. The optimisation encounters negative curvature structure frequently 4. We investigated the data and found that the compression is less noticeable in this range and is surpassed by other non-linearities that feed-forward compressor cannot capture. For other peak reductions, the NR method converges to the same solution in multiple different initialisations, verifying that (θ ) is mostly convex around the optimal solution."
        },
        {
            "title": "4.5 Results",
            "content": "...The average compression ratio is always set at roughly 3:1, while the average Attack time is 10 milliseconds, and the Release time is about 60 milliseconds for 50% of the release, and anywhere from 1 to 15 seconds for the rest. 5 The learnt conﬁgurations are shown in Fig. 2. The mappings to the parameters are nearly identical between the compressor and limiter modes, except for the ratio, which is slightly higher in the limiter mode but exhibits similar trend. The ratio is typically around 4:1 in compressor mode, slightly deviating from the manufacturers stated ratio of 3:1. The attack and release times vary exponentially with the peak reduction, rather than ﬁxed values. Since there is only one release time parameter, it does not capture the documented twostage release behaviour of the LA-2A, but an average of them. ) ( h h 10 30 40 ) ( G - M 1 0.8 0.6 0.4 0.2 0 40 (a) (c) ) ( T t 30 20 10 0 (b) (d) ) ( T e 60 100 500 400 300 200 60"
        },
        {
            "title": "Peak Reduction",
            "content": "(e) 7 6 5 4 a 3 40 50 60 70 Peak Reduction 80 Compressor Limiter 90 100 Fig. 2: The mapping from LA-2A peak reduction to the compressors parameters."
        },
        {
            "title": "5 Evaluation",
            "content": "We compare our compressor (denoted as 4A-2A and 4 stands for feed-forward) with the following baselines: the ofﬁcial UAD LA-2A plugin 6, the emulation plugin CA-2A by Cakewalk 7, the emulation plugin CLA-2A by Waves 8, and the grey-box compressor from [8]. We evaluate 4A-2A using its training data, as the ﬁve-parameter model cannot fully represent the LA-2A and no overﬁtting is observed on this dataset 6www.uaudio.com/uad-plugins/compressors-limiters/teletronix4For peak reductions over 40, this rarely happens (it happens la-2a-tube-compressor few times when random initialisation is used). 5www.uaudio.com/blog/la-2a-collection-tips-tricks 7legacy.cakewalk.com/Products/CA-2A 8www.waves.com/plugins/cla-2a-compressor-limiter AES International Conference on Artiﬁcial Intelligence and Machine Learning for Audio, London, UK 2025 September 810 Page 6 of"
        },
        {
            "title": "Yu and Fazekas",
            "content": "Analogue Levelling Ampliﬁer Emulation with more complex models [8]. We evaluate performance using the Error-to-Signal Ratio (ESR) and the Loudness Dynamic Range difference (LDR). ESR is deﬁned as (yˆy)(cid:62)(yˆy) . LDR is the difference between the LDR [23] of the target signal and the predicted signal. The same pre-emphasis ﬁlter is applied to all audio before evaluations. y(cid:62)y To calculate LDR given y, we ﬁrst compute the log ratio between the short-term and long-term RMS envelopes as Ly[n] = 10 log10 RMSshort(y[n]) RMSlong(y[n]) . (15) The integration time for the short-term RMS is set to 50 ms and the long-term RMS is set to 3 where the details can be found in [23]. We deﬁne LDR(y) as LDR(y) = (cid:115)"
        },
        {
            "title": "1\nN",
            "content": "N1 n=0 Ly[n]2, (16) which tells how much the loudness varies locally (microdynamics) in y. LDR is simply LDR(ˆy) LDR(y) so if LDR > 0, it means ˆy is less compressed than and vice versa. Since we do not know what setup and gain staging were used to model the commercial plugins, the plugins input and output gains must be tuned. We manually adjust the input gain so that the sum of its LDR across peak reductions is close to zero, ensuring its compression curve matches the SignalTrain dataset. We then adjust the output gain so that its overall ESR is minimised. We render the audio through plugins using pedalboard [24]. We choose the Model 5 conﬁguration for the greybox compressor, their best-performing model. It is architecture-wise very similar to our 4A-2A, but the make-up gain γ is replaced with GRU. Thus, we follow their setup to train GRU on the same training set but pre-processed by 4A-2A with γ = 0 dB. We set the batch size to 26 (13 conﬁgurations two modes) and the sequence length to 12288. The training converges after around eight epochs. We stop the training at the 15th epoch and pick the checkpoint with the lowest loss. The process takes less than 30 minutes on an RTX 3060 Laptop GPU. We denote this model as 4A-2A-G. The evaluation results are shown in Fig. 3. 4A-2A-G has the lowest ESR and is close to those reported in [8]. 4A-2A shows comparable performance to the ofﬁcial UAD plugin. At around 75 peak reduction, the LA-2A behaves most like the feed-forward compressor, thus having the lowest ESR. Beyond this point, the 4A-2As ESR increases, and the errors are more for the limiter mode. For the commercial plugins, their curves are less smooth and have few apparent spikes. Among them, the CA-2A has the highest ESR in general and even reaches over 40 % at 100 peak reduction. Regarding their LDR proﬁles, we see that CA-2A varies the most with maximum mismatch of 1 dB at 100 peak reduction. UAD and CLA-2A exhibit similar patterns of difference, which can be divided into three stages: from 40 to 60 (peak reduction), their compressions are lighter than the ground truth; then, the compressions become heavier from 60 to 80 and then return to lighter levels beyond 80. 4A-2A and 4A-2A-G have their LDR very close to zero thus out-performing others, where 4A-2A-G being the closest. UAD 4A-2A CLA-2A 4A-2A-G CA-2A"
        },
        {
            "title": "Limiter",
            "content": ") % ( 30 20 5 0 1 0 20 60 80 100 0 20 40 80 100 ) ( 0.5 0 0.5 0 60 40 20 Peak Reduction 80 100 60 40 20 Peak Reduction 80 100 Fig. 3: ESR and LDR of all the evaluated models under different peak reduction and mode."
        },
        {
            "title": "6 Application: the 4A-2A Plugin",
            "content": "We implement our 4A-2A compressor as real-time audio plugin using JUCE [25], providing it as creAES International Conference on Artiﬁcial Intelligence and Machine Learning for Audio, London, UK 2025 September 810 Page 7 of"
        },
        {
            "title": "Yu and Fazekas",
            "content": "Analogue Levelling Ampliﬁer Emulation ative tool for audio engineers. In the plugin interface  (Fig. 4)  , we have ﬁve sliders that control the parameters of the underlying compressor. At the bottom, there is one slider for peak reduction and switch for the mode. The peak reduction slider is linked to the above compressors parameters, mapping the peak reduction to them using the learnt mappings from Fig. 2. The mode switch toggles between the compressor and limiter proﬁles. The user can adjust the peak reduction and observe the compressors behaviour in real-time. If they want to ﬁne-tune the compressor, they can adjust the parameters manually."
        },
        {
            "title": "Compressor",
            "content": "7.49 7.56 7."
        },
        {
            "title": "Limiter",
            "content": "7.76 7.84 7.92 10 8 10 8 6 ) % ( E ) % ( 45 55 75"
        },
        {
            "title": "Peak Reduction",
            "content": "Optimal Linear Spline Fig. 5: Comparison of different interpolation methods. The horizontal lines are the averages. GRU model resembles the 4A-2A-G baseline, realtime implementation of the Model 5 emulator from [8], with the feed-forward part being trained using the NR method, outperforming all commercial plugins in our evaluation. Comparing the residuals with or without the GRU make-up gain shown in Fig. 6, the most noticeable difference is in the high-frequency range (> 4 kHz). The GRU successfully captures the non-linearities of these analogue circuits, which cannot be modelled by simple digital compressor, and spreads out the errors across frequencies."
        },
        {
            "title": "7 Discussion",
            "content": "The NR method effectively ﬁnds unique solution to match the LA-2As response to the square distance criterion. There is room for improvement, as LA-2A is technically feedback compressor; therefore, the signal ﬂows of 4A-2A and LA-2A are not precisely matched. By using digital feedback compressor (s[n] = ˆy[n1]) and modelling the two-stage release behaviour explicitly, we can potentially improve the performance. However, efﬁciently backpropagating gradients through the feedback path is challenging and requires custom implementations. In contrast, our grey-box sound-matching Fig. 4: The GUI of the 4A-2A plugin. The peak reductions in SignalTrain are sampled on grid. Interpolation is required for continuous mapping [40, 100] R5. To evaluate the interpolation error, we exclude peak reductions {45, 55, . . . , 95} and use the remaining 7 points to interpolate them using either linear or cubic spline interpolation. Their ESR is shown in Fig. 5. Linear interpolation and spline interpolation perform the worst at 65 and 95 peak reduction, respectively. On average, the spline interpolation has slightly higher ESR than the linear interpolation. We suppose this ﬁnding generalises to denser grids and choose the linear interpolation for the plugin. We made the GRU model from 4A-2A-G into standalone plugin. The GRU model is very lightweight, consisting of only one layer with eight hidden units. We wrapped the GRU model as PyTorch script module using Neutone [26], which can be loaded into their Neutone FX plugin 9. Cascading the 4A-2A and the 9github.com/Neutone/neutone_sdk AES International Conference on Artiﬁcial Intelligence and Machine Learning for Audio, London, UK 2025 September 810 Page 8 of"
        },
        {
            "title": "Yu and Fazekas",
            "content": "Analogue Levelling Ampliﬁer Emulation Fig. 6: The spectrograms of drum loop from the SignalTrain and the residuals (y ˆy) of the models predictions with 75 peak reductions operated in compressor mode. approach can be generalised to any dynamic range controller circuit, and our trial on the LA-2A already yields satisfactory results. Due to the variations between analogue circuits, the inferior plugin performance we see in Section 5 is expected. Their modelled LA-2A might behave slightly differently from the SignalTrain one. It is interesting to note that the UAD and CLA-2A have similar ESR/LDR patterns, which could be due to similar target circuits or modelling approaches. We evaluate the entire Hessian matrix, as the number of parameters is smaller. However, for more complex effects and longer effect chains, the Hessian can become too large to ﬁt in memory. Methods like conjugate gradients using Hessian-vector product (cid:55) 2L (θ )v can be used to solve the equation 2L (θ )ν = (θ ) without explicitly computing the Hessian [17, 27]. Nevertheless, increasing the number of parameters also raises the possibility of encountering more local minima, making it harder to ﬁnd the optimal solution."
        },
        {
            "title": "8 Conclusion",
            "content": "This paper presented method to emulate the LA2A optical compressor using feed-forward digital compressor with parameters optimised via the NewtonRaphson method. Our approach successfully turned the LA-2As response into interpretable parameters that can be used for creative control. The efﬁcient implementation of ballistics and the parallel associative scan algorithm for gradient computation signiﬁcantly accelerate optimisation. Forward-mode gradient propagation through the compressor is derived to benchmark different strategies for computing the Hessian. Our ﬁndings suggest that the Newton-Raphson method is promising approach for the grey-box modelling of audio compressors, providing foundation for further research and development in this area. Further improvements could be made by using feedback design and explicitly modelling the LA-2As two-stage release behaviour. Future work could extend NewtonRaphson optimisation to other analogue compressors or non-linear audio effects. Another direction could be reducing the computational costs of the Hessian matrix for more complex signal chains."
        },
        {
            "title": "Acknowledgements",
            "content": "The ﬁrst author is research student supported jointly by UKRI (grant number EP/S022694/1) and Queen Mary University of London."
        },
        {
            "title": "References",
            "content": "[1] Eichas, F., Gerat, E., and Zölzer, U., Virtual analog modeling of dynamic range compression systems, in AES 142nd Convention, 2017. [2] Hawley, S. H., Colburn, B., and Mimilakis, S. I., Proﬁling Audio Compressors with Deep Neural Networks, in AES 147th Convention, 2019. [3] Steinmetz, C. J. and Reiss, J. D., Efﬁcient neural networks for real-time modeling of analog dynamic range compression, in AES 152nd Convention, 2022. [4] Comunità, M., Steinmetz, C. J., Phan, H., and Reiss, J. D., Modelling black-box audio effects with time-varying feature modulation, in Proc. IEEE ICASSP, pp. 15, 2023. AES International Conference on Artiﬁcial Intelligence and Machine Learning for Audio, London, UK 2025 September 810 Page 9 of"
        },
        {
            "title": "Yu and Fazekas",
            "content": "Analogue Levelling Ampliﬁer Emulation [5] Yeh, Y.-T., Hsiao, W.-Y., and Yang, Y.-H., Hyper Recurrent Neural Network: Condition Mechanisms for Black-box Audio Effect Modeling, in Proc. DAFx, pp. 97104, 2024. [6] Simionato, R. and Fasciani, S., Comparative Study of Recurrent Neural Networks for Virtual Analog Audio Effects Modeling, arXiv preprint arXiv:2405.04124, 2024. [7] Simionato, R. and Fasciani, S., Modeling TimeVariant Responses of Optical Compressors with Selective State Space Models, Journal of the Audio Engineering Society, 73, pp. 144165, 2025. [8] Wright, A. and Välimäki, V., Grey-box modelling of dynamic range compression, in Proc. DAFx, pp. 304311, 2022. [9] Yu, C.-Y., Mitcheltree, C., Carson, A., Bilbao, S., Reiss, J. D., and Fazekas, G., Differentiable All-pole Filters for Time-varying Audio Systems, in Proc. DAFx, pp. 345352, 2024. [10] Bernardini, A., Bozzo, E., Fontana, F., and Sarti, A., Wave Digital Newton-Raphson Method for Virtual Analog Modeling of Audio Circuits with Multiple One-Port Nonlinearities, IEEE/ACM TASLP, 29, pp. 21622173, 2021. [11] Bilbao, S., Torin, A., and Chatziioannou, V., Numerical Modeling of Collisions in Musical Instruments, Acta Acustica united with Acustica, 101(1), pp. 155173, 2015. [12] Zölzer, U., DAFX: Digital Audio Effects, chapter Nonlinear Processing, pp. 110112, John Wiley & Sons, 2011, ISBN 0470665998. [13] Giannoulis, D., Massberg, M., and Reiss, J. D., Digital dynamic range compressor designA tutorial and analysis, Journal of the Audio Engineering Society, 60(6), pp. 399408, 2012. [14] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S., PyTorch: An imperative style, high-performance deep learning library, in Proc. NeurIPS, pp. 8026 8037, 2019. [15] Rirsch, M. W. and Smale, S., Differential equations, dynamical systems, and linear algebra, ACADEMIC PRESS. INC., 1974. [16] Robeyns, M., Of VJPs and JVPs, https://maximerobeyns.com/of_ vjps_and_jvps, 2023, [Accessed: 2025-0217]. [17] Dagréou, M., Ablin, P., Vaiter, S., and How to compute Hessianin ICLR Blogposts, 2024, Moreau, T., vector products? https://iclr-blogposts.github. io/2024/blog/bench-hvp/. [18] Colburn, B. and Hawley, S., SignalTrain LA2A Dataset, 2020, doi:10.5281/zenodo.3824876. [19] Blelloch, G. E., Preﬁx Sums and Their Applications, Technical Report CMU-CS-90-190, School of Computer Science, Carnegie Mellon University, 1990. [20] Martin, E. and Cundy, C., Parallelizing Linear Recurrent Neural Nets Over Sequence Length, in ICLR, 2018. [21] Moré, J. J. and Sorensen, D. C., Newtons method, Technical report, Argonne National Lab.(ANL), Argonne, IL (United States), 1982. [22] Armijo, L., Minimization of functions having Lipschitz continuous ﬁrst partial derivatives, Paciﬁc Journal of Mathematics, 16(1), pp. 1 3, 1966. [23] Nercessian, S., McClellan, R., and Lukin, A., direct microdynamics adjusting processor with matching paradigm and differentiable implementation, in Proc. DAFx, 2022. [24] Sobot, P., Pedalboard, 2021, doi:10.5281/ zenodo.7817838. [25] Raw Material Software Limited, JUCE: Jules Utility Class Extensions, https://juce. com/, 2025, [Accessed: 2025-02-16]. [26] Fyfe, A. and Mitcheltree, C., Neutone-Real-time AI audio plugin for DAWs, in Audio Developer Conference, 2022. [27] Hestenes, M. R. and Stiefel, E., Methods of conjugate gradients for solving linear systems, Journal of research of the National Bureau of Standards, 49, pp. 409435, 1952. AES International Conference on Artiﬁcial Intelligence and Machine Learning for Audio, London, UK 2025 September 810 Page 10 of"
        }
    ],
    "affiliations": [
        "Centre for Digital Music, Queen Mary University of London, London, UK"
    ]
}