{
    "paper_title": "Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers",
    "authors": [
        "Xingyue Huang",
        "Rishabh",
        "Gregor Franke",
        "Ziyi Yang",
        "Jiamu Bai",
        "Weijie Bai",
        "Jinhe Bi",
        "Zifeng Ding",
        "Yiqun Duan",
        "Chengyu Fan",
        "Wendong Fan",
        "Xin Gao",
        "Ruohao Guo",
        "Yuan He",
        "Zhuangzhuang He",
        "Xianglong Hu",
        "Neil Johnson",
        "Bowen Li",
        "Fangru Lin",
        "Siyu Lin",
        "Tong Liu",
        "Yunpu Ma",
        "Hao Shen",
        "Hao Sun",
        "Beibei Wang",
        "Fangyijie Wang",
        "Hao Wang",
        "Haoran Wang",
        "Yang Wang",
        "Yifeng Wang",
        "Zhaowei Wang",
        "Ziyang Wang",
        "Yifan Wu",
        "Zikai Xiao",
        "Chengxing Xie",
        "Fan Yang",
        "Junxiao Yang",
        "Qianshuo Ye",
        "Ziyu Ye",
        "Guangtao Zeng",
        "Yuwen Ebony Zhang",
        "Zeyu Zhang",
        "Zihao Zhu",
        "Bernard Ghanem",
        "Philip Torr",
        "Guohao Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across a diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LoongBench, a curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LoongEnv, a modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LoongBench on a broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct a comprehensive analysis of synthetic data generated by LoongEnv, examining correctness, difficulty, and diversity. Code and documentation are available at https://github.com/camel-ai/loong."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 9 5 0 3 0 . 9 0 5 2 : r Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers Xingyue Huang*, Rishabh*, Gregor Franke*, Ziyi Yang*, Jiamu Bai, Weijie Bai, Jinhe Bi, Zifeng Ding, Yiqun Duan, Chengyu Fan, Wendong Fan, Xin Gao, Ruohao Guo, Yuan He, Zhuangzhuang He, Xianglong Hu, Neil Johnson, Bowen Li, Fangru Lin, Siyu Lin, Tong Liu, Yunpu Ma, Hao Shen, Hao Sun, Beibei Wang, Fangyijie Wang, Hao Wang, Haoran Wang, Yang Wang, Yifeng Wang, Zhaowei Wang, Ziyang Wang, Yifan Wu, Zikai Xiao, Chengxing Xie, Fan Yang, Junxiao Yang, Qianshuo Ye, Ziyu Ye, Guangtao Zeng, Yuwen Ebony Zhang, Zeyu Zhang, Zihao Zhu, Bernard Ghanem, Philip Torr, Guohao Li CAMEL-AI.org Equal Contribution, Corresponding author, Authors listed here are in alphabetical order"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LOONGBENCH, curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LOONGENV, modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LOONGBENCH on broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct comprehensive analysis of synthetic data generated by LOONGENV, examining correctness, difficulty, and diversity. Code and documentation are available at https://github.com/camel-ai/loong. Date: September 4, 2025 Correspondence: Guohao Li at guohao.li@eigent.ai Project Page: https://github.com/camel-ai/loong"
        },
        {
            "title": "1 Introduction",
            "content": "Recent Large Reasoning Models such as DeepSeek-R1 [1] and o3 [2] have demonstrated that the general reasoning capabilities of LLMs greatly improve when base models undergo post-training with Reinforcement Learning (RL) with verifiable reward [35]. Mathematics and programming [3, 6] have particularly benefited from this approach, as these domains can be verified quite easily, allowing accurate interpretation of LLM responses and effective comparison to the ground truth on semantic level. This idea that the ease of verification is crucial to improving domain-specific capabilities has become widely accepted in the research community [79]. Another critical prerequisite which is often overlooked is the abundance of high-quality datasets, featuring questions paired with verified correct answers in the domains of Maths and Coding [6, 10]. These curated datasets provided the necessary signal for models to learn to construct coherent Chains-of-Thought (CoTs) [6], leading reliably to correct answers. However, many other domains also require reliable reasoning, such as logic, graph theory, physics, and finance. These domains lack comparable datasets [4, 1113], and human-supervised data production at scale is prohibitively expensive [5, 13, 14]. Without abundant correct answers to learn from, models cannot easily acquire domain-specific reasoning patterns. This raises crucial question: Can similar reasoning performance be achieved in domains beyond maths and programming? Approach. We present the Loong Project: an open framework for scaling synthetic data generation with verifiable supervision across diverse set of reasoning-centric domains. The framework comprises two key components: 1. LOONGBENCH, meticulously curated seed dataset comprising 8,729 examples across 12 reasoningintensive domains, each accompanied by executable code and verified answers. 2. LOONGENV, modular and versatile synthetic data generation environment, capable of generating diverse and semantically verifiable question-answer pairs using various automated generation strategies. As illustrated in Figure 1, the overall agent-environment loop operates as follows: First, given collection of seed datasets, our generator produces synthetic data points consisting of automatically generated questions and corresponding executable codes that answer these questions. Second, these codes are executed within the environment to yield synthetic answers. Third, trainable agent is prompted to solve the synthetic questions by generating natural language CoT responses. Finally, verifier compares the agents CoT-derived answer to the code-generated answer. This setup will enable large-scale reinforcement learning with minimal human supervision while preserving semantic correctness through automated verification in the future. Contributions. Our main contributions are: We introduce LOONGBENCH, high-quality seed dataset of 8,729 examples spanning 12 reasoningintensive domains, each paired with executable code and semantically verified answers. Figure 1 Agent-Environment Loop We develop LOONGENV, synthetic data generation environment that supports multiple generation strategies to produce diverse and verifiable question-answer pairs. We benchmark LOONGBENCH across diverse suite of large language models-including both opensource and proprietary, general-purpose and reasoning-specialized models-to establish baseline performance and identify domain-specific challenges. We conduct detailed analysis of the synthetic data generated by LOONGENV, evaluating it in terms of semantic correctness, question difficulty, and diversity."
        },
        {
            "title": "Loong Project",
            "content": "The Loong Project is focused on scaling up synthetic data generation with verification mechanisms across broad spectrum of domains. We believe that generating synthetic data is essential not just to overcome the lack of datasets in under-represented fields, but also to strengthen reasoning abilities in areas like mathematics and programming by making more training examples available. Our system relies on multi-agent setup that starts with seed dataset and expands it by generating new questions and corresponding answers. These synthetic questions are then passed to model under training, which attempts to answer them. Verifiers are then employed to compare the models answers with the pre-generated and pre-computed solutions, checking for semantic agreement. At the heart of our approach is simple idea: an LLM equipped with code interpreter is often far more reliable when solving complex tasks than one that relies solely on natural language reasoning. This idea is backed by how many scientific disciplines operate in practice: whether its physics, neurophysiology, 3 Table 1 Statistics of the LOONGBENCH by domains. Domain Main Dependency Advanced Maths Advanced Physics Chemistry Computational Biology Finance Board Game Graph & Discrete Maths Logic Mathematical Programming Medicine Security & Safety Programming sympy sympy, numpy rdkit, numpy - QuantLib - networkx python-constraint gurobipy, cvxpy, pyscipopt, statsmodel medcalc-bench cryptography, gmpy2, pycryptodome - Size 1,611 429 3,076 51 235 926 178 130 76 916 516 585 economics, or computational biology, code-based solutions are standard way to formulate and solve domain-specific problems. To achieve our goals, two key components are required: high-quality seed dataset that spans multiple domains, and modular environment capable of generating synthetic questions and answers in structured, verifiable manner. LOONGBENCH provides the foundational data for our framework. It consists of 8,729 carefully curated examples covering 12 diverse, reasoning-intensive domains. Each example is annotated with executable code and paired with semantically verified answers. These seed examples ensure coverage of domain-specific patterns while maintaining correctness and diversity, offering reliable basis for downstream synthetic data generation and benchmarking. Complementing LOONGBENCH is LOONGENV, flexible and extensible synthetic data generation environment. LOONGENV takes the seed examples from LOONGBENCH and uses various strategies, including few-shot prompting, self-instruction, and programmatic transformations, to generate new question-answer pairs. It also executes associated code to produce verifiable answers, which are used to supervise and evaluate model performance. This environment is domain-agnostic and modular, supporting plug-and-play verifiers and generation policies across reasoning domains."
        },
        {
            "title": "2.1 LOONGBENCH: Human-Vetted Seed Datasets Across Multiple Domains",
            "content": "We begin by manually collecting domain-specific datasets consisting of questions and ground truth answers. Each question in the seed dataset is ensured to be solvable using code. If available, we also record the code that leads to the ground truth. The purpose of the seed dataset is not to be large-scale dataset to use directly for training, but as means to bootstrap the synthetic data generation process by seeding the generative process of the LLM. To ensure broad coverage across diverse dataset types, we first collected 8,729 data points spanning 12 different domains to construct LOONGBENCH. Detailed statistics of the LOONGBENCHare provided in Table 1. In particular, every data point in the seed set contains: natural language question, 4 verified final answer, and an accompanying rationale in the form of executable python code. corresponding metadata, including license, source, domain, required dependencies, name, contributor, creation date, difficulty level, and relevant tags. We start by introducing, in detail, how we collect the seed datapoints for each domain. Advanced Math To construct high-quality seed dataset for advanced mathematical problem-solving, we begin by selecting problems from the training split of the MATH [10] dataset, focusing on those labeled with difficulty levels 4 or 5. For each selected problem, we prompt the o3-mini model [2] to generate corresponding SymPy code intended to solve it. We then filter out any outputs that fail to produce executable code or yield incorrect solutions. The remaining SymPy programs are executed to produce numerical or symbolic results, which are then compared against the ground truth answers using MathVerifier1a tool designed to parse and evaluate LaTeX-formatted mathematical expressions. Only those samples with verified correct solutions are retained and added to the seed dataset. We ended up collecting 1,611 problems. Advanced Physics We select the physics problems from Scibench [15] and Olympiadbench [16]. We asked o3-mini [2] to generate sympy code which solves the problem. We execute the code and compare the result with the ground truth answer. The answer has two parts: numerical value and unit. Unit conversion and dynamic tolerance are used in the verification process. unit conversion: If the response unit doesnt match with the ground truth unit, perform unit conversion to match them (e.g. 1km v.s. 1000m, 200, 000 vs 2 105). We then compare the converted numerical value and unit to the ground truth ones. dynamic tolerance: Adjust relative tolerance when comparing the numerical results if needed (the default tolerance is 0.01, but sometimes the ground truth answer is given in lower tolerance, e.g. when ground truth is 1.3e + 02, we should allow the answers to sit within difference of 0.05e + 02) For each problem, we perform maximum of three attempts to generate the correct rationale. The correct ones are included in the seed dataset. Chemistry We extracted examples from the ChemistryQA dataset available on HuggingFace 2, which contains wide range of chemistry reasoning question-answer pairs. We selected examples from this dataset that present conceptually clear and computationally relevant problems. When the original examples provided concise and understandable questions with well-formatted answers, we retained them directly. For examples where the format was unclear or the answer presentation lacked structure, we reformulated them using the o3-mini [2] model. This model allowed us to generate well-structured questions that emphasize general chemistry reasoning, such as stoichiometry, thermodynamics, or molecular structure, while avoiding references to implementation-specific details. For instance, an original question about molar mass calculation was rewritten as: What is the molar mass of sulfuric acid (H2SO4)? This process resulted in 3076 seed data points, each aligned with core chemistry concepts and designed to support both human and model-based reasoning. 1https://github.com/camel-ai/camel/blob/master/camel/verifiers/math_verifier.py 2https://huggingface.co/datasets/avaliev/ChemistryQA 5 Computational Biology We collected datasets from the Biochemistry, Genetics, Molecular Biology, and Microbiology domains under the Biology category on the General Reasoning website3. These datasets were then preprocessed using custom script to merge and transform the raw data into unified question-answering format. We employed GPT-4o [17] to generate code-based solutions for each question. Subsequently, we filtered out instances where the generated code could not be executed or failed to address the problem correctly. To ensure the quality and validity of the resulting dataset, we further utilized GPT-4o-mini [17] to evaluate whether: (1) the generated code genuinely solved the problem rather than merely paraphrasing it; (2) the execution result of the code was semantically consistent with the ground truth answer; (3) the code included necessary computational steps instead of directly hardcoding the output. To more accurately evaluate whether the results of code execution align with the ground truth, we employed rule-based regular expression matching for both. Finally, we retained the question-answer data that showed consistent matches. Only samples that passed all these criteria were included in the final seed dataset used for our experiments. We ended up collecting 51 samples. Finance We focus on extracting examples from variety of QuantLib [18] resources, including the official documentation4, online tutorials5, and the FinAI Financial Reasoning Dataset on Hugging Face6. From these, we identified instances that contain well-defined financial modeling problems and accompanying code examples. When the original examples included clearly presented questions and answers, we retained them directly. For those that lacked clarity or structure, we used the o3-mini model [2] to reformulate them into precise question formats that emphasize financial reasoning and conceptual understanding, while avoiding low-level implementation details. For instance, an example involving bond pricing using QuantLib was rewritten into the question: What is the clean price of fixed-rate bond with face value of $1000, 5% annual coupon, and 10 years to maturity if the yield is 4.5%? This process yielded curated set of seed data points suitable for financial reasoning tasks, each paired with corresponding Python-based solution. Board Game We adopt Blackjack as the canonical imperfect-information board game and generate interaction data through hybrid pipeline that combines reproducible simulation with quality-controlled traces derived from Agent Pro [19]. All fresh episodes are played in the RLCARD environment [20] against two fixed baseline opponentsDeep Q-Network (DQN) [21] and Deep Monte-Carlo (DMC) [22]so that the strategic background remains constant. We execute games until we collect 50 distinct losing rounds; these serve as the seed corpus for policy-level reflection. Following the protocol of Zhang et al. [19], every state-action pair is augmented with (i) natural-language rationale extracted from an expert Blackjack playbook and (ii) behaviour guideline produced by the reflection mechanism. To suppress hallucination and retain only strategically sound trajectories, we run perfect-information MonteCarlo roll-outs [23] from each decision point to estimate the true probability of winning for every legal action. trajectory is kept only if the chosen action belongs to the top-k actions ranked by this oracle probability (with k=1 in all experiments). This filtering step aligns the sampled behaviour with optimal-play statistics 3https://gr.inc 4quantlib-python-docs.readthedocs.io 5http://gouthamanbalaraman.com/blog/quantlib-python-tutorials-with-examples.html 6https://huggingface.co/TheFinAI 6 and yields high-fidelity corpus for subsequent safety and alignment analysis. We ended up generating 926 questions as described. Graph & Discrete Math We focus on extracting examples from the documentation of networkx7 [24] (abbreviated as nx), widely used library for network science. We began by inspecting all 896 functions under nx.algorithms. From these, we identified 370 functions that included usage examples and extracted the corresponding code snippets as rationales. To ensure reproducibility, we filtered out examples involving multiple valid outputs or random sampling. The remaining examples were rewritten using GPT-4o-mini [17] to formulate questions that reflect the exact outcome of the code in general graph-theoretic terms, avoiding references to implementation details. For instance, the code snippet nx.is_k_regular(nx.Graph([(1, 2), (2, 3), (3, 4), (4, 1)]), k=3) was transformed into the question: For graph defined by the edges connecting nodes as follows: (1, 2), (2, 3), (3, 4), and (4, 1), is the graph 3-regular? This process yielded 178 seed data points, each corresponding to distinct algorithm implementation in the networkx library. All data points were then manually checked to ensure that the code executes correctly and produces the intended result. Logic We currently have two types of constraint satisfaction problems (CSPs): Einsteins Puzzle8 and Analytical Reasoning questions from the Law School Admission Test (AR-LSAT)9. Einsteins Puzzle is deductive reasoning task where set of positions must each be assigned unique combination of items across several categories, guided by set of interrelated clues (constraints). Solutions can be represented as dictionaries, with categories as keys and ordered lists of items as values. To generate these puzzles, we define pool of 10 possible categories (e.g., Job, Beverage, Food) and 25 items per category (e.g., Accountant and Doctor belong to Job). For each instance, we randomly select subset of categories and items, enumerate all possible permutations, and iteratively add constraints until only one valid solution remains. The final puzzle is constructed using natural language template that integrates the scenario description, sampled categories, items, and constraints. AR-LSAT questions assess logical reasoning within structured system of relationships by requiring valid conclusions based on set of rules and conditions. For example, problem might involve scheduling presentations for Mary, John, and Alice on Monday, Tuesday, and Wednesday, with constraints such as John presenting after Mary. The task is to determine which conclusions follow logically. We use questions collected from [25] as data points. Since both Einsteins Puzzle and AR-LSAT are CSPs, we employ GPT-4.1-mini [26] to generate Python code using the python-constraint library10, ensuring each problem is solvable and has unique solution. Mathematical Programming The mathematical programming domains focus on solving optimization problems for an objective function that is subject to constraints. We mainly obtain questions directly from 7https://networkx.org/ 8https://en.wikipedia.org/wiki/Zebra_Puzzle 9https://www.lsac.org/lsat 10https://pypi.org/project/python-constraint/ the existing tutorials from Gurobipy11 [27], PySCIPOpt12 [28], CVXPY13 [29], and Statsmodels14 [30]. For all these datasets, we began by reviewing all official example notebooks provided in their corresponding documentation and identified subset that met our criteria for practical relevance and technical clarity. Specifically, we selected notebooks that included detailed real-world problem descriptions, utilized data from identifiable sources, and were compatible with the latest Jupyter environment. From these, we curated collection of problems where the accompanying data were either regenerated using GPT-4o [17]guided by the original sourcesor partially retained to preserve solvability. Markdown formatting issues were corrected, and scripts were streamlined by removing non-essential outputs such as extraneous print statements and visualizations. Logical inconsistencies and errors within the solution code were minimally edited, and final answers were marked using the boxed notation for clarity. Medicine We constructed our dataset from the MedCalc-Bench benchmark15 through the following procedure. We start by merging the patient note and question columns into unified question field, and designating the Ground Truth Answer column as the final answer. To ensure diversity across categories, we sampled up to 30 entries for each unique Calculator Name. For calculators with fewer than 30 available entries, we included all available data. This yielded total of 1,192 examples. We then improved the official tool code [31] by adding docstrings and performing additional code optimizations. The entire codebase was then consolidated into Python package and also stored in single text file. We prompted GPT-4.1 [26] to generate code (as rationale) capable of solving each question using functions from the medcalc-bench package. We filtered the data by discarding any question-answer pairs for which the generated rationale code failed to execute successfully. This filtering step resulted in 1,117 valid examples. We then compared the outputs of the executed rationales with the corresponding final answer fields using rule-based regular expression matching system. Through this process, we identified 916 question-answer pairs with matched outputs. Security & Safety We focused on extracting representative examples from existing cryptographic CaptureThe-Flag (CTF) problems and their associated explanations. Specifically, we manually curated problems from the CTF-Wiki16, which encompasses 3 major categories and 44 subcategories of cryptographic challenges. For each subcategory, we ensured coverage by including at least 3-5 representative problems. To diversify and scale the dataset, we systematically replaced the plaintext and ciphertext in the original problems to generate multiple variants, ensuring each subcategory included at least 5 distinct instances. Where applicable, we used GPT-4o [17] to rewrite the textual components of the problems, making them more natural while preserving the cryptographic core. For each problem, we also collected the corresponding solution logic and reference implementation. With the assistance of GPT-4o [17], we manually refined these into coherent rationales that explain the step-by-step solution process. All solution scripts were executed in Python interpreter, and their outputs were compared against the official flags or expected results. Any examples with mismatched outputs were discarded. The 11https://github.com/Gurobi/modeling-examples 12https://github.com/scipopt/PySCIPOpt 13https://www.cvxpy.org/examples/ 14https://www.statsmodels.org/stable/examples/index.html 15https://ncbi/MedCalc-Bench-v1.0 16https://ctf-wiki.org/crypto/introduction/ 8 final dataset consists of problem statements, rationale code, and verified outputs for each data point. All entries were manually reviewed to ensure correctness, clarity, and reproducibility. Programming The dataset taken from the LeetCode dataset [32] and is constructed through two-stage rollout procedure aimed at building code critic modelan LLM designed to identify and fix errors in solutions to competitive programming problems. Each data point consists of programming problem, an initial candidate solution, an automatic correctness judgment, and potential correction. In the first round rollout, an initial candidate solution is generated using Claude-3.5-Sonnet [33] for each programming task. These candidate solutions then go through second round evaluation where DeepSeek-R1 [1]. This model acts as an automated code critic: it first determines whether the initial solution is correct, outputting binary true/false label. If the solution is found to be incorrect, the model attempts to revise it by producing corrected version that better aligns with the problem specification."
        },
        {
            "title": "2.2 LOONGENV: Modular Synthetic Generation with Verifiable Supervision",
            "content": "LOONGENV is modular, black-box synthetic data generator seeded with high-quality dataset, such as LOONGBENCH. Given this seed, it can generate an unbounded number of question-answer pairs that expand the training distribution in controllable and verifiable manner. The generator is abstracted from downstream modules, enabling the flexible integration of various prompting strategies and agent behaviors. We support both simple prompting methods and complex multi-agent generation workflows. This section details the generation process and experimental configurations. Question Synthesis We explore three strategies for generating synthetic questions from seed examples: Few-shot prompting [34]: We provide few seed QA pairs as demonstrations and prompt the model to generate new problems in similar style. This serves as the simplest generation baseline. Self-Instruct [35]: An instruction-tuned model is recursively prompted to generate increasingly diverse and structured prompts. Evol-Instruct [36]: This approach evolves seed questions through mutation operations such as generalization, specification, and complexity scaling. Answer Synthesis For every generated question, we generate corresponding answer using coder agent, which generates the corresponding code and tries to execute it to obtain the results. While we do not assume the correctness of these synthetic answers, code execution enables us to produce grounded numerical outputs in many domains. Notably, we do not use the raw synthetic answers directly for training. Verifiers To ensure high-quality generated data, we incorporate verification mechanism that filters out incorrect synthetic solutions generated by our pipeline. To do this effectively, we validate synthetic answers using two independent approaches: Deriving one solution directly through the Generators code execution. Independently generating another solution via natural-language Chain-of-Thought (CoT) reasoning. If these independent solutions agree, its highly likely that the answer is correct. Although rare, theres still possibility of false positives (both 9 approaches incorrectly agreeing). However, given the fundamentally different methods involved, we believe this will not occur often enough to be detrimental to model training. Each environment also includes verifier that semantically compares the LLM response with the synthetic answer, ensuring they are effectively equivalent. This verification step is crucial for accurately filtering semantic equivalences, significantly reducing false negatives (cases where semantically correct answers would otherwise be wrongly rejected). Note that while our current setup relies on the LLM-as-a-judge paradigm, where large language models are used to assess the correctness of solutions, we ultimately aim to develop domain-specific verifiers (e.g., for mathematics or programming), which are both more efficient and more reliable. Future direction In future work, we plan to use this verification framework to support reinforcement learning (RL). Specifically, the CoT-generating agent, the model we ultimately aim to train, can receive positive rewards only when its final answer is semantically verified to match the trusted synthetic answer. This setup enables Reinforcement Learning from Verified Rewards (RLVR) paradigm, where the agent learns exclusively from high-confidence, semantically aligned supervision."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate the capabilities of state-of-the-art language models on LOONGBENCH and synthetic datasets produced by LOONGENV. Specifically, we aim to answer the following questions: How well models perform across diverse reasoning domains on LOONGBENCH? How reliably can they generate executable and semantically valid solutions using LOONGENV? How do different prompting strategies affect the diversity and difficulty of the generated tasks?"
        },
        {
            "title": "3.1 Setup",
            "content": "Dataset and Models. We evaluate the reasoning and problem-solving capabilities of range of state-of-theart language models across 12 domains provided as LOONGBENCH: Advanced Math, Advanced Physics, Chemistry, Computational Biology, Finance, Game, Graph & Discrete Math, Logic, Mathematical Programming, Medicine, Security & Safety, and Programming. Each domain consists of curated problems sourced from high-quality benchmarks, academic competitions, and domain-specific datasets, as described in Section 2.1. We include both openand closed-source language models in our evaluation. Proprietary models such as GPT4.1-mini [26], o3-mini [2], Grok-3 [37], and Claude-3.7-Sonnet [38] provide strong baselines from leading commercial providers. In parallel, we incorporate high-performing open-source models like DeepSeek-r1 [1] and Qwen3-8B [39], both of which demonstrate competitive performance on reasoning-intensive tasks. Implementation. All models are evaluated using their publicly available APIs or checkpoints, with consistent temperature, top-k, and top-p settings where applicable. For fairness, we disable function/tool calling unless stated otherwise, and restrict outputs to single response per prompt without retries. We set the max-token for all experiments to 409617. We use single NVIDIA H100 80GB GPU for inference with the 17Some questions in mathematical programming thus go out of this window and the result would be incomplete. 10 Table 2 Benchmarking accuracy across domains and model categories. The best model is highlighted in bold, and the second best is underlined. Domain Advanced Maths Advance Physics Chemistry Computational Biology Finance Game Graph Logic Math. Programming Medicine Security Programming GPT4.1-mini o3-mini Grok-3 Claude-3.7 DeepSeek-r Qwen3-8B 91.4 71.8 75.2 90.2 23.8 92.0 80.9 65.4 11.8 59.6 25.6 98.6 97.4 75.3 79.5 88.2 24.3 96.0 82.0 61.6 9.2 46.3 11.2 100.0 92.3 69.0 71.2 96.1 19.1 93.0 80.1 55.4 6.4 50.7 22.3 91.5 79.3 63.9 80.7 90.2 22.0 95.1 73.6 46.9 13.2 54.1 4.7 97.4 96.7 77.4 74.7 88.2 24.3 97.3 83.7 62.3 10.5 52.6 28.7 98. 79.2 59.2 79.7 86.2 12.8 43.2 62.9 39.2 10.0 28.4 7.9 81.7 open-sourced model. Our development is based on the CAMEL framework [40], and the codebase supporting this work is publicly available at https://github.com/camel-ai/loong. Evaluation. Accuracy is measured as the percentage of correctly answered problems per domain. We employ LLM-as-judge [41], using GPT4.1-mini [26], to assess correctness due to the complex nature of answers, which may vary in format across different domains. The judge accounts for symbolic equivalence where applicable, ensuring that mathematically correct but differently expressed answers are not penalized."
        },
        {
            "title": "3.2 Benchmarking LOONGBENCH",
            "content": "We report the benchmarking results in Table 2. The results reveal key trends regarding domain difficulty, model specialization, and the performance gap between openand closed-source systems. We highlight three central findings below. well-calibrated spectrum of difficulty. The twelve domains in our benchmark exhibit wide range of difficulty levels. For example, the Mathematical Programming domain yields accuracies as low as 10%, indicating substantial unresolved complexity, whereas the Programming domain is nearly saturated, with models like o3-mini achieving 100% accuracy. Other domains, such as Logic, Graph & Discrete Math, and Chemistry, distribute evenly across the middle range. This balance ensures the benchmark is broadly discriminative: it avoids ceiling effects for strong models while remaining accessible for lower-capacity systems, making it robust testbed for both evaluation and ablation studies. Reasoning-optimized models consistently outperform. We observe that models explicitly tuned or pretrained for reasoning, particularly o3-mini and DeepSeek-r1, achieve top scores across the majority of domains. Notably, DeepSeek-r1 consistently reached top-2 in 8 out of 12 datasets, whereas o3-mini also reached 6 out of 12. These results suggest that the benchmark requires more than surface-level pattern matching or factual recall: it emphasizes structured, multi-step reasoning, which is better captured by models with strong CoT or planning capabilities. 11 Open-source models lag in reasoning-heavy domains. Although open models like DeepSeek-r1 and Qwen3-8B perform competitively in certain areas, clear performance gap remains in the most reasoningintensive domains. For instance, in the Game and Logic domains, Qwen3-8B trails o3-mini by 50 and 22 percentage points, respectively. These discrepancies highlight two key challenges: (i) current open-source systems underperform on strategy-based and logic-heavy tasks, and (ii) this benchmark suite is well-positioned to reveal such fine-grained capability gaps, offering concrete targets for future model development and alignment in the open-source community."
        },
        {
            "title": "3.3 Synthesizing Data with LOONGENV",
            "content": "We use LOONGENV to generate synthetic data under three instruction paradigms: Few-shot prompting [34], Self-instruct [35], and Evol-instruct [36]. For each setting, we initialize the generator with seed examples from LOONGBENCH and use multi-agent workflow to synthesize new data: 1. question synthesis agent generates new natural language question based on the seed data. 2. separate code generation agent is then prompted to produce an executable program that answers the generated question. We then evaluate the quality of the generated samples in two stages: Executability Check: We run each generated code snippet in sandboxed Python environment and measure the fraction that executes without error. This provides proxy for functional correctness and yields the execution success rate. Verification via Judge Agent: For each generated question-code pair, we prompt judge agent to assess two criteria: (1) whether the question is well-formed and meaningful, and (2) whether the generated code correctly solves the posed problem. The fraction of samples that fail either criterion determines the rejection rate. This two-stage evaluation process provides systematic measure of both functional correctness and semantic fidelity of the generated question-code pairs. Implementation. We use GPT-4.1-mini [26] as both the question synthesis agent and the code generation agent across all experiments. For each domain and each generation strategies, we randomly selected from the seed dataset and generated 100 synthetic questions. For verification, we employ DeepSeek-R1 [1] as the judge agent. All other settings remain consistent with the experimental setup described in the previous section. 3.3.1 Execution and Verification Outcomes Figure 2 summarizes the execution outcomes of synthetic data generated by LOONGENV under three prompting strategies-FewShot, Self-Instruct, and Evol-Instruct-across two domains: Logic and Physics. We categorize outcomes into three classes: Pass (code executed and the judge approved the answer), Judge-Rejected (code executed but result disagreed with the judges answer), and Not Executable (code failed to run). We observe that in the Logic domain, FewShot prompting yields high pass rate (92.6%) with very few failures, while Self-Instruct exhibits much higher rejection rate (44.8%) and Evol-Instruct produces large 12 ) % ( t r 80 60 40 20 0 Logic Physics FewShot Self-Ins Evol-Ins FewShot Self-Ins Evol-Ins Pass Judge-Rejected Not executable Figure 2 Execution outcome breakdown across synthetic data generation strategies for Logic and Physics domains. r m n C 1 0.9 0.8 0.7 0.97 0. 0.88 0.77 0.91 0.8 FewShot Evol Instruct Self Instruct Average Cosine Similarity Max Cosine Similarity Figure 3 Seed-generated pairwise cosine similarity on the Advanced Physics domain across synthetic data generation strategies. portion of non-executable code (55.0%). In contrast, for the Physics domain, both FewShot and Self-Instruct maintain high pass rates (93.9% and 82.0% respectively), but Evol-Instruct again suffers from significantly reduced executability and semantic agreement, with 29.8% judged as incorrect and 14.0% failing to execute. These results highlight trade-off between prompting complexity and generation reliability: while FewShot prompting offers the most stable pipeline with the highest overall pass rates, Evol-Instruct, despite its higher rejection and execution failure rates, remains highly valuable from training perspective. Its ability to synthesize more diverse and challenging reasoning tasks makes it especially well-suited for building robust models. As we demonstrate in the later sections, Evol-Instruct more effectively captures edge cases and reasoning depth that are essential for meaningful generalization. 13 (a) Few-shot (b) Self-Instruct (c) Evol-Instruct Figure 4 t-SNE projection of embedding space for seed vs. generated problems on the Advanced Physics across different generation strategies. Generated examples (orange) and seed samples (blue) cluster with varying degrees of overlap, indicating distributional proximity and diversity. (a) Few-shot (b) Self-Instruct (c) Evol-Instruct Figure 5 Cosine similarity distribution between seed and generated questions on the Advanced Physics domain for different generation strategies. 3.3.2 Diversity We assess the semantic diversity of generated questions in the Advanced Physics domain by comparing their embeddings to those of the seed questions using cosine similarity and t-SNE visualization. Figure 3 reports the average and maximum cosine similarity between 100 seed-generated question pairs under each strategy, while Figures 4 and 5 provide qualitative breakdown of embedding proximity using t-SNE and distributional skew. Overall, we observe that Few-Shot prompting tends to produce questions that are more lexically distinct from the seeds, as reflected by its lower average similarity (0.77) and more dispersed t-SNE distribution. However, these questions often remain close to the original in structure and complexity, representing surface-level variation. In contrast, Self-Instruct generates questions that are both lexically and semantically more diverse, often drifting farther from the seed set in the t-SNE space. This suggests bias toward novelty, albeit sometimes at the expense of coherence or executability (see correctness analysis). Interestingly, Evol-Instruct generates questions that appear much closer to the seed questions semantically, as suggested by the high average (0.90+) and maximum cosine similarities, along with tight clustering in the tSNE plots. This pattern may indicate that Evol-Instruct tends to apply transformations, such as generalization, 14 Table 3 Average accuracy (%) on Advanced Physics domain across synthetic data generation strategies Model Few-shot Self-Instruct Evol-Instruct Seed Dataset GPT4.1-mini DeepSeek-r1 92.0 93.2 83.0 87.4 62.0 70.3 71.8 77.4 specification, or rephrasing, that preserve the underlying semantics while increasing complexity. These observations suggest potential trade-off between surface-level lexical variation and semantic alignment: while Few-Shot prompting yields more lexically distinct outputs, Evol-Instruct may generate structurally richer and semantically coherent examples. We hypothesize that such properties could be beneficial for capturing deeper reasoning patterns and edge cases, which are valuable for robust model training. 3.3.3 Difficulty Finally, we proceed to measure the difficulty level of the generated datasets. Here, we again focus on Advanced Physics domain and assess the difficulty of generated questions by measuring the accuracy of two models: GPT4.1-mini and DeepSeek-r1. Table 3 presents model accuracies on questions generated via each strategy in the Advanced Physics domain. We observe that both GPT4.1-mini and DeepSeek-r1 perform best on Few-Shot generated data, achieving 92.0% and 93.2% accuracy respectively. Accuracy slightly drops on Self-Instruct data (83.0% and 87.4%), and significantly declines on Evol-Instruct questions (62.0% and 70.3%). Interestingly, despite the Evol-Instruct examples being more semantically similar to the seed questions (Figure 3), their lower model accuracy suggests that they are substantially harder to solve. This supports our earlier hypothesis that Evol-Instruct tends to preserve core semantics while increasing reasoning complexity, possibly through abstract transformations or compound formulations."
        },
        {
            "title": "4 Related Work",
            "content": "Post-training for LLM Early efforts in aligning large language models with human preferences focused on fine-tuning via human feedback. InstructGPT demonstrated that reinforcement learning from human feedback (RLHF) can make smaller models more helpful, honest, and harmless than much larger base models [42], building on earlier work that optimized from pairwise preferences [43] and fine-tuned models to match human judgments [44]. Recent approaches pursue more stable or efficient alignment techniques. Direct Preference Optimization (DPO) reframes reward modeling as supervised objective [45], while Preference Ranking Optimization (PRO) models task-aware ranks [46] and RRHF bypasses RL entirely [47]. Other efforts include training helpful assistants via RLHF [42], improving summarization through feedback [48], and formalizing verifiable reward learning [49]. Co-evolutionary training with unit testers [50], single-example reward tuning [51], autoregressive search [3], cross-domain RLVR [52], and prolonged fine-tuning [53] further extend the design space. Synthetic data generation In recent years, there has been surge of interest in using language models to synthesize their own training data. Self-Instruct introduced pipeline in which models generate and train 15 on synthetic instructions to improve alignment [54]. WizardLM [36] and WizardCoder [55] extend this with Evol-Instruct, automatically evolving instruction complexity for general and code tasks, respectively. The Flan Collection [56] curates diverse instructional tasks and data augmentation strategies to boost zeroshot generalization, while LIMA [57] shows that only small amount of high-quality prompts can yield strong performance. Super-NaturalInstructions [58] aggregates over 1600 NLP tasks to study generalization to unseen instructions. Beyond instruction generation, more systematic data generation frameworks have emerged. DataGen provides unified pipeline with modules for controllability, diversity, and factuality [59]. comprehensive survey reviews challenges and advances in LLM-based synthetic data generation for text and code [60]. Reinforcement Learning with Verifiable Reward (RLVR) RLVR combines reinforcement learning with automatic, verifiable reward signals, often programmatic correctness or agreement with auxiliary tools, to scale alignment and reasoning. GRPO formalizes reward dynamics [49], while one-shot RLVR shows that even single example can substantially improve reasoning [51]. Satori proposes Chain-of-Action-Thought framework [3], and ProRL shows that prolonged optimization enables emergent capabilities [53]. RLVR has also been applied to instruction following [52], co-evolutionary coding setups [50], and multi-domain generalization. These efforts enable scalable, automatic reward supervision in open-ended environments [3, 4953]."
        },
        {
            "title": "5 Conclusion",
            "content": "We present Loong, modular framework for aligning LLMs via synthetic data generation and verifiable reward supervision. Our approach is driven by two key insights: effective alignment requires diverse, domain-specific data and scalable, annotation-free reward mechanisms. Our contributions are fourfold: (1) LOONGBENCH, seed dataset of 8,729 examples across 12 reasoningintensive domains with executable code and verified answers; (2) LOONGENV, flexible environment enabling diverse synthetic data generation strategies; (3) comprehensive benchmarking of open-source and proprietary models to assess domain generalization; and (4) in-depth analysis of generated data quality in terms of correctness, diversity, and complexity. Together, these components form cohesive framework for studying alignment at scale. Our results demonstrate that structured synthetic generation and verification can yield diverse and challenging synthetic datasets. key future direction is leveraging LOONGENV to support RLVR with synthetically generated questions. We also plan to extend LOONGENVwith tool-augmented generation and formal abstraction, and scale LOONGBENCHto cover multilingual and multimodal tasks."
        },
        {
            "title": "References",
            "content": "[1] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501.12948. [2] OpenAI. o3-mini large language model. https://openai.com/index/openai-o3-mini/, 2025. Accessed: 2025-05-15. [3] Maohao Shen, Guangtao Zeng, Zhenting Qi, et al. Satori: Reinforcement learning with chain-of-action-thought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508, 2025. [4] Miao Peng, Nuo Chen, Zongrui Suo, and Jia Li. Rewarding graph reasoning process makes llms more generalized reasoners, 2025. [5] Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, and Andreas Kpf. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards, 2025. [6] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2022. URL https: //arxiv.org/abs/2201.11903. arXiv preprint. [7] Sebastian Raschka. The state of reinforcement learning for llm reasoning. blog post, May 2025, 2025. [8] Anonymous. Automatic post-training via reinforcement learning with verifiable rewards, 2025. [9] Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm reasoning across all domains, 2025. [10] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [11] Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, and Junxian He. Synlogic: Synthesizing verifiable reasoning data at scale for learning logical reasoning, 2025. [12] Zijian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, and Michael Qizhe Shieh. Synthrl: Scaling visual reasoning with verifiable data synthesis, 2025. [13] Zhuohan Xie, Dhruv Sahnan, Debopriyo Banerjee, Georgi Georgiev, Rushil Thareja, Hachem Madmoun, Jinyan Su, Aaryamonvikram Singh, Yuxia Wang, Rui Xing, Fajri Koto, Haonan Li, Ivan Koychev, Tanmoy Chakraborty, Salem Lahlou, Veselin Stoyanov, and Preslav Nakov. Finchain: symbolic benchmark for verifiable chain-of-thought financial reasoning, 2025. [14] Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu, Dezhi Chen, Yun Chen, Zuo Bai, and Liwen Zhang. Fin-r1: large language model for financial reasoning through reinforcement learning, 2025. [15] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. In Proceedings of the Forty-First International Conference on Machine Learning, 2024. [16] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. 17 [17] OpenAI. Gpt-4o: Openais new multimodal flagship model. https://openai.com/index/hello-gpt -4o/, 2024. Accessed: 2025-05-22. [18] David Duarte. Quantlib-python object building documentation. https://quantlib-python-docs.re adthedocs.io/, 2020. Accessed: 2025-05-22. [19] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve via policy-level reflection and optimization. arXiv preprint arXiv:2402.17574, 2024. [20] Daochen Zha, Kwei-Herng Lai, Songyi Huang, Yuanpu Cao, Keerthana Reddy, Juan Vargas, Alex Nguyen, Ruzhe Wei, Junyu Guo, and Xia Hu. Rlcard: platform for reinforcement learning in card games. In IJCAI, 2020. [21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, 2015. [22] Da Zha, Jun Zhang, Chao Zhou, Zhen Xu, and Xia Hu. Douzero: Mastering doudizhu with self-play deep reinforcement learning. In Proceedings of the 38th International Conference on Machine Learning, pages 12333 12343. PMLR, 2021. [23] Jrme Arjonilla, Abdallah Saffidine, and Tristan Cazenave. Perfect information monte carlo with postponing reasoning. In 2024 IEEE Conference on Games (CoG), page 18. IEEE, August 2024. doi: 10.1109/cog60054.2 024.10645574. URL http://dx.doi.org/10.1109/CoG60054.2024.10645574. [24] Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. Exploring network structure, dynamics, and function using networkx. In Gal Varoquaux, Travis Vaught, and Jarrod Millman, editors, Proceedings of the 7th Python in Science Conference, pages 11 15, Pasadena, CA USA, 2008. [25] Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. Ar-lsat: Investigating analytical reasoning of text. arXiv preprint arXiv:2104.06598, 2021. [26] OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025. Accessed: 2025-05-22. [27] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2024. URL https://www.gurobi.com. [28] Stephen Maher, Matthias Miltenberger, Joo Pedro Pedroso, Daniel Rehfeldt, Robert Schwarz, and Felipe Serrano. PySCIPOpt: Mathematical programming in python with the SCIP optimization suite. In Mathematical Software ICMS 2016, pages 301307. Springer International Publishing, 2016. doi: 10.1007/978-3-319-42432-3_37. [29] Steven Diamond, Eric Chu, and Stephen Boyd. CVXPY: Python-embedded modeling language for convex optimization, version 0.2. http://cvxpy.org/, May 2014. [30] Skipper Seabold and Josef Perktold. statsmodels: Econometric and statistical modeling with python. In 9th Python in Science Conference, 2010. [31] Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina Applebaum, Zain Anwar, Maame SarfoGyamfi, Conrad Safranek, Abid Anwar, Andrew Zhang, Aidan Gilson, Maxwell Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, and Zhiyong Lu. Medcalc-bench: Evaluating large language models for medical calculations, 2024. URL https://arxiv.org/abs/2406.12036. [32] Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu. Leetcodedataset: temporal dataset for robust evaluation and efficient training of code llms, 2025. URL https://arxiv.org/abs/2504.14655. 18 [33] Anthropic. Introducing claude 3.5 sonnet, June 2024. URL https://www.anthropic.com/news/cla ude-3-5-sonnet. Accessed: 2025-06-04. [34] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901, 2020. URL https: //proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64 a-Paper.pdf. [35] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long.754/. [36] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. URL https://arxiv.org/abs/2304.12244. [37] xAI. Grok 3 beta the age of reasoning agents, 2025. URL https://x.ai/news/grok-3. Accessed: 2025-05-22. [38] Anthropic. Claude 3.7 sonnet: Hybrid reasoning model announcement, February 2025. URL https://www. anthropic.com/news/claude-3-7-sonnet. Accessed: 2025-05-22. [39] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [40] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [41] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. survey on llm-as-a-judge, 2025. URL https://arxiv.org/abs/2411.15594. [42] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 35: 2773027744, 2022. [43] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement In Advances in Neural Information Processing Systems (NeurIPS), pages learning from human preferences. 42994307, 2017. [44] Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. CoRR, abs/1909.08593, 2019. 19 [45] Rafael Rafailov et al. Direct preference optimization: Your language model is secretly reward model. arXiv preprint arXiv:2305.18290, 2023. [46] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023. [47] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023. [48] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel Ziegler, Ryan Lowe, et al. Learning to summarize from human feedback. Advances in Neural Information Processing Systems (NeurIPS), 33:30083021, 2020. [49] Youssef Mroueh. Reinforcement learning with verifiable rewards: Grpos effective loss, dynamics, and success amplification. arXiv preprint arXiv:2503.06639, 2025. [50] Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, and Mengdi Wang. Co-evolving llm coder and unit tester via reinforcement learning. arXiv preprint arXiv:2506.03136, 2025. [51] Yiping Wang, Qing Yang, Zhiyuan Zeng, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. [52] Yi Su, Dian Yu, Linfeng Song, et al. Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. [53] Mingjie Liu, Shizhe Diao, Ximing Lu, et al. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025. [54] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. [55] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. [56] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023. [57] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023. [58] Yizhong Wang, Swaroop Mishra, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In EMNLP, 2022. [59] Yue Huang, Siyuan Wu, Chujie Gao, et al. Datagen: Unified synthetic dataset generation via large language models. In International Conference on Learning Representations (ICLR), 2025. [60] Mihai Nadas, Laura Diosan, and Andreea Tomescu. Synthetic data generation using large language models: Advances in text and code. arXiv preprint arXiv:2503.14023, 2025."
        },
        {
            "title": "A System Prompt for Benchmarking Experiments",
            "content": "We use tailored prompts to standardize the generation and evaluation processes across all models tested in our benchmark. These prompts are described below. Solver System Prompt You are an {{DOMAIN}} solver. wrapped in boxed{} statement. boxed{frac{1}{4}}, incorrect: It is imperative that you end with your answer It should only be the final answer, e.g., correct: boxed{x = frac{1}{4}}. LLM-as-Judge System Prompt You will be provided with GROUND Your task is the following: You are an answer judge. TRUTH ANSWER, and with an entire answer from an LLM output. there should be boxed{} statement containing an answer. Your task is to compare this answer to the GROUND TRUTH ANSWER. You should only focus on the boxed statement at the end of the LLM response, disregard any reasoning steps before it. You should end your response with your judgement wrapped in boxed{} statement. Wrap your evaluation result as boxed{1} if the provided LLM answer is mathematically equivalent (EVEN IF THE FORMAT IS DIFFERENT!!) as boxed{0} if the provided LLM answer is either not existent or not mathematically equivalent. to the GROUND TRUTH ANSWER. Wrap it At the end of the LLM output, These prompts ensure consistency in both generation and evaluation phases and are critical to maintaining reproducibility and fairness in our benchmarking pipeline."
        }
    ],
    "affiliations": [
        "CAMEL-AI.org",
        "eigent.ai"
    ]
}