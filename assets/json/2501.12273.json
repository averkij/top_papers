{
    "paper_title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement",
    "authors": [
        "Maosong Cao",
        "Taolin Zhang",
        "Mo Li",
        "Chuyu Zhang",
        "Yunxin Liu",
        "Haodong Duan",
        "Songyang Zhang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become a significant bottleneck, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research."
        },
        {
            "title": "Start",
            "content": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement Maosong Cao1,, Taolin Zhang1,2,, Mo Li1,2, Chuyu Zhang1, Yunxin Liu2 Haodong Duan1,, Songyang Zhang1,,, Kai Chen1, 1Shanghai AI Laboratory 2Tsinghua University Datasets:https://hf.co/datasets/internlm/Condor-SFT-20K Github:https://github.com/InternLM/Condor 5 2 0 2 1 2 ] . [ 1 3 7 2 2 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The quality of Supervised Fine-Tuning (SFT) data plays critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become significant bottleneck, necessitating greater reliance on synthetic training data. In this work, we introduce Condor, novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and SelfReflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) are experiencing rapid advancements, with proprietary models such as like GPT (Achiam et al., 2023) and Gemini (Team et al., 2023), alongside open-source counterparts such as LLaMA (Dubey et al., 2024), Qwen (Yang et al., 2024), Mistral (Jiang et al., 2023a), and Deepseek (Liu et al., 2024a) evolving at an unprecedented pace. However, this rapid iteration comes with significant challenge: the depletion of existing high-quality data for Supervised Fine-Tuning (SFT). Moreover, the internet is 1This work is done when Taolin Zhang and Chuyu Zhang are on internship at Shanghai AI Laboratory, means equal contribution, means corresponding author, means project lead. Figure 1: Comparison between Official Models and Condor-Refined Models. The performances are obtained from CompassJudger-1-32B. Different families of popular LLMs benefit greatly from the Condor Refine engine. increasingly inundated with synthetic data of varying and often questionable quality, making it ever more difficult to construct and filter higher-quality training data for LLMs. Empirical studies on SFT training have conclusively shown that both data quality and quantity play crucial roles in enhancing model performance (Shen, 2024). While scaling laws suggest that models can achieve extraordinary capabilities when trained on large datasets, high-quality data can yield comparable results even at smaller scales. This highlights the importance of generating substantial amounts of high-quality data to further enhance large-scale model capabilities. Recent research introduce various methods for synthesizing SFT training data (Wang et al., 2024a; Ding et al., 2023; Xu et al., 2023; Yuan et al., 2024; Tang et al., 2024). However, these approaches face several limitations: they often rely heavily on existing datasets for seed prompts, lack prompt diversity, and cannot guarantee the quality of the generated data. To address these challenges, we introduce Condor, an automated two-stage pipeline for synthetic data generation that effectively leverages existing world knowledge. Our approach begins with constructing comprehensive World Knowledge Tree by prompting LLM to generate tags, and then creating complete tag chain from coarse to fine through an auto-regressive pipeline. After generating diverse questions and original responses using tags, we then employ Self-Reflection Refinement to obtain synthesized SFT data with more higher-quality replies. Condor further generates extensive and high-quality SFT data by integrating up-to-date world knowledge tags. In this way, Condor not only ensures the relevance and diversity of the content but also maintains scalability in data synthesis. We conduct experiments on multiple models using the data synthesized by Condor, as shown in Figure 1, the results indicate that the data synthesized by Condor significantly improves the subjective chat capabilities of the models across the board. To summarize, our contributions are as follows: We propose Condor Void, novel approach incorporating knowledge inspiration strategy based on the World Knowledge Tree, enabling models to acquire rich world knowledge for generating diverse questions. We further introduce Condor Refine, selfreflection mechanism that allows models to iteratively refine their responses during the construction phase, resulting in higher-quality responses. We investigate the scaling and self-iteration techniques for Condor datasets, offering valuable insights and potential pathways for the scalable expansion of synthetic data."
        },
        {
            "title": "2 Related Work",
            "content": "synthetic data can significantly enhance model performance, low-quality synthetic data may lead to model degradation or collapse. WizardLM (Xu et al., 2023) introduces an EvolInstruct scheme, which expands an initial question dataset by generating new questions through techniques such as deep thinking, adding constraints, and reconstruction. However, the quality and diversity of the generated data are strongly influenced by the characteristics of the original dataset. Similarly, Self-Reward (Yuan et al., 2024) builds on seed dataset, enabling the model to generate questions via few-shot prompting and produce multiple corresponding responses. These responses are subsequently evaluated by reward model and labeled as training data for reinforcement learning with human feedback (RLHF). Magpie(Xu et al., 2024) explores leveraging models themselves to generate synthetic data by utilizing chat templates as prompts to directly produce dialogues. However, this approach is not universally applicable to all model types, and therefore lacks generalization ability. Additionally, the synthetic data generated often lacks stability, containing significant noise that requiring extensive filtering to maintain quality. Agent-based data synthesis methods, such as MATRIX-Gen (Tang et al., 2024), simulate multiple world scenarios using multiagent framework and collect dialogue data from interactions within these scenarios. While effective in some cases, this method is time-consuming, labor-intensive, and limited by the need for multiple models to operate concurrently. In contrast to these methods, Condor eliminates the dependence on seed dataset, requiring only tags as seeds for expansion and generation. This enables exceptional diversity and scalability in data production. Furthermore, Condor handles all data generation tasks with single model, significantly reducing costs while maintaining high efficiency."
        },
        {
            "title": "2.2 LLM Self-Iteration",
            "content": "With the rapid advancement of Large Language Models (LLMs) (Achiam et al., 2023; Team et al., 2023; Dubey et al., 2024; Yang et al., 2024; Jiang et al., 2023a; Liu et al., 2024a) and Large MultiModality Models (LMMs) (Radford et al., 2021; Zhu et al., 2023; Zhang et al., 2024a,b; Liu et al., 2024b; Chen et al., 2024), the demand for highquality training data continues to grow. As result, synthetic data generation has become increasingly crucial for model development. While high-quality"
        },
        {
            "title": "For",
            "content": "Recent studies have explored methods for large language models (LLMs) to improve themselves inthrough iterative self-enhancement. stance, I-SHEEP (Liang et al., 2024) and SelfInstruct (Wang et al., 2023) utilize seed data and LLMs to generate new instructions. However, these methods often produce instructions that closely resemble those in the seed dataset, limiting the diversity and novelty of the generated data. (Tao et al., 2024) categorizes existing self-iteration methFigure 2: The Pipeline of Condor Engine. The pipeline is mainly divided into (1) Data synthesis: LLM is inspired from the World Knowledge Tree with different task diversity and difficulty to construct the original question-answer dataset DV ; (2) Data refinement: LLM is utilized to reflect on the synthesized data and generate critiques for refinement and improve the quality of responses, producing the dataset DR. ods, highlighting works such as self-instruct (Wang et al., 2023), self-improving (Huang et al., 2022), and self-training (Gulcehre et al., 2023). Specifically, the self-improving framework proposed by (Huang et al., 2022) enhances the reasoning abilities of LLMs without requiring labeled data by generating multiple reasoning paths and answers, then selecting high-confidence answers through self-consistency mechanism. Reinforced Self-Training (ReST) (Gulcehre et al., 2023) achieves self-iteration by generating multiple outputs, scoring them with reward model, and fine-tuning the LLM using the highestscoring samples. While effective, this approach depends on an external reward model to provide additional knowledge. Other notable works, such as Reflexion (Shinn et al., 2023), Self-Refine (Madaan et al., 2023), and Refiner (Paul et al., 2024), propose self-iteration techniques where models iteratively refine their answers based on feedback from specific tasks. However, these methods are primarily designed for specific domains like mathematics, limiting their ability for generalization. By contrast, Condor achieves self-improvement on wide range of human-preference datasets solely using the models own capabilities, enabling broader applicability while maintaining scalability and efficiency."
        },
        {
            "title": "3.1 Pipeline Overview",
            "content": "As shown in Figure 2, the pipeline of Condor is divided into two stages: data synthesis and data refinement. In the data synthesis stage, we begin by introducing the World Knowledge Tree, which serves as foundation of tags for data generation. Next, we apply task and difficulty expansion to enhance the diversity and complexity of questions under each tag, leading to the creation of the initial synthetic QA dataset DV . In the data refinement stage, we employ Self-Reflection Refinement strategy, enabling the model to iteratively optimize the generated responses and yield DR. Notably, we utilize single model during the entire pipeline for better efficiency."
        },
        {
            "title": "3.2 Condor Void",
            "content": "In the data synthesis stage, we generate the synthetic data based on existing world knowledge. This stage consists of two key components: (1) World Knowledge Tree and (2) Q&A pair generation. The dataset produced from this stage is referred to as DV ."
        },
        {
            "title": "3.2.1 World Knowledge Tree",
            "content": "To construct comprehensive multi-level tag system that encapsulates diverse aspects of world knowledge, we propose methodology that enhances the models capacity for inspiration and knowledge stimulation. We define set of root tags, i.e., = {r1, r2, . . . , rn}, representing broad themes such as marriage, entertainment, and artificial intelligence. Using large language models (LLMs), we expand each root tag into set of detailed leaf tags, denoted as Li = {li1, li2, . . . , lim}. This hierarchical structure forms the foundation of the World Knowledge Tree, , which can be expressed as union of subtrees rooted at each ri: = (cid:83)n i=1{(ri, Li)}. To ensure the tree remains reflective of realworld themes and contexts, we augment it with additional tags collected from trending topics on platforms such as Zhihu and Reddit, denoted as = {s1, s2, . . . , sk}. This creates an enriched tree, = (cid:83)n i=1{(ri, Li Si)}, where Si ensures each root branch is contextually informed and relevant. Moreover, the World Knowledge Tree is designed to be dynamic, adapting over time with an update function (T , t) that that continuously integrates new information. This robust, evolving framework, comprising over 8,400 tags, is pivotal for maintaining the current and applicable nature of knowledge required for the iteration of LLMs. 3.3 Condor Refine In this stage, we enable the model to engage in self-reflection and critical examination of its own responses, followed by revisiting the questions to generate higher-quality replies. The resulting refined dataset is referred as DR. The initial QA pairs contain responses directly generated by the model, which are inherently limited by the models current capabilities. Therefore, we implement data refinement to further improve the quality of the responses. We carefully develop set of fixed templates that guide the model in identifying both strengths and weaknesses in its responses, leading to specific suggestions for improvement. Subsequently, we utilize these selfgenerated improvement suggestions to prompt the model to produce enhanced responses. The model is tasked with preserving the effective elements of its original responses while addressing identified weaknesses to generate superior responses. Complete examples of QA pairs that have undergone the entire Condor pipeline are provided in the Appendix. Finally, we obtain 200k high-quality refinement datasets from DV , i.e., DR 200, 000."
        },
        {
            "title": "4.1 Training and Evaluation Settings",
            "content": "After acquiring the seed tags, we employ various methods to enhance the diversity of the generated questions. First, we identify and summarize the most common chat scenarios in human interactions with LLMs, such as daily conversation, creative tasks, and role-playing (the total 7 tasks with detailed descriptions are provided in Figure A.2). The model is then required to deeply engage in specific task under the corresponding tag theme to generate questions. This approach enhances the diversity of the generated questions and encourages knowledge extrapolation, allowing the model to explore themes it may not have frequently encounteredor not encountered at allin particular scenario. To further expand the range of questions, we implement difficulty control, prompting the model to adapt to three difficulty levels and generate questions of varying complexity. This step ensures greater diversity and richness in the generated data, contributing to more well-rounded and comprehensive dataset. Finally, we obtain Condor Void datasets with approximately 200k Q&A pairs, i.e., DV 200, 000. Training Settings. We use xTuner (Contributors, 2023b) as the training framework. To ensure fairness in comparison, we set the initial learning rate to 2e-5 and train for 3 epochs in all experiments. Unless specifically stated, we start training from the base model and compare it with the official RLHF model. Settings. We Evaluation use OpenCompass (Contributors, 2023a) and employ greedy inference to uniformly evaluate all models, ensuring fairness across various datasets. We selected 8 human-preference benchmarks for evaluation the chat capabilities of the models. We evaluate the results and reports the average normalized score at the percentage scale. Additionally, due to the high cost of conducting subjective evaluations with paid API model, we use GPT4o as the Judge Model for main results only, and we judge with the open-source CompassJudger-1-32B (Cao et al., 2024) in ablation study and scaling experiments. Furthermore, we also select range of knowledgebased Q&A datasets for groundtruth-based evaluations. Datasets Score Range Qwen2.5-7B-Instruct Qwen2.5-7B-Base Qwen2.5-7B-Instruct Official Condor Void Condor Refine Condor Void Condor Refine CompassArena (Contributors, 2023a) FoFo (Xia et al., 2024) AlignBenchv1.1 (Liu et al., 2023) AlpacaEvalv2 (Dubois et al., 2024) ArenaHard (Li et al., 2024) FollowBench (Jiang et al., 2023b) MTBench101 (Bai et al., 2024) WildBench (Lin et al., 2024) Average 0100 01 010 010 0100 01 010 -100100 0100 33.8 0.52 6.22 34.66 53.65 0.84 8.60 16.71 58.02 32.43 0.41 6.04 32.84 42.44 0.85 8.22 14.09 54.09 36.13 0.45 6.20 44.10 60.87 0.83 8.37 30.13 60. 35.17 0.50 6.24 38.76 53.56 0.88 8.41 21.54 59.14 40.12 0.47 6.20 44.60 61.53 0.85 8.43 32.70 61.29 Table 1: Performance Comparison on Human-preference Benchmarks Judged by GPT4o-0806. The results demonstrate that Condor Refine significantly enhances the subjective chat capability of models. The Average Score is calculated by normalizing the scores for each dataset to percentage scale and then taking their average. Datasets BoolQ (Clark et al., 2019) GPQA Diamond (Rein et al., 2023) Math (Hendrycks et al., 2021) GSM8K (Cobbe et al., 2021) GaokaoBench (Zhang et al., 2024c) MMLU-Pro (Wang et al., 2024b) DROP (Dua et al., 2019) IFEval loose (Zhou et al., 2023) HumanEval (Chen et al., 2021) Race-high (?) MBPP (Austin et al., 2021) CMO (Contributors, 2023a) Average Qwen2.5-7B-Instruct Qwen2.5-7B-Base Qwen2.5-7B-Instruct Official Condor Void Condor Refine Condor Void Condor Refine 86.06 35.35 83.21 92.27 60.42 54.70 80.81 81.70 89.63 85.05 74.32 22.60 70. 87.83 34.34 82.31 91.21 63.87 52.15 79.31 81.70 89.02 84.88 74.32 25.00 70.50 86.82 35.86 81.91 90.75 63.64 50.70 78.50 79.85 85.98 83.56 73.54 28.85 70.00 87.34 35.35 82.67 91.43 60.69 54.73 81.53 83.36 87.20 85.16 74.32 23.08 70.57 86.33 35.35 82.47 91.96 60.13 53.02 80.46 80.96 86.59 84.28 74.71 20.19 69.70 Table 2: Performance Comparison on Knowledge-Based Benchmarks. The score range across all the tasks are 0100. When trained on high-quality SFT data, the differences in the models performance on knowledge-intensive question-answering tasks are minimal."
        },
        {
            "title": "4.2 Data Synthesis Settings",
            "content": "Only one model is required for data synthesis and data refinement throughout the entire pipeline. In the main experiments, we use 200k data synthesized by Qwen2.5-72B for model training. Additionally, for SOTA comparisons and selfinteraction experiments, we also generate 200k data points using Qwen2.5-7B to test the effectiveness of the Condor pipeline."
        },
        {
            "title": "4.3 Main Results",
            "content": "Using Condor data generated by Qwen2.5-72BInstruct, we first train both the Base and Instruct versions of Qwen2.5-7B and compare their performance against the official model on both Chat and Knowledge benchmarks. Human-preference Evaluation. We select several widely-used human-preference benchmarks and calculate the final normalized average scores on these datasets (detailed calculation logic is provided in AppendixA.1). We use GPT-4o as the Judge Model, and the average results are reported in Table 1. As shown in the table, the results of training the Base model with DR surpassed the official model on almost all human-perference benchmarks. It is noteworthy that this stage only employs SFT and does not include RLHF as applied in the official model, highlighting the significant potential of Condors constructed data in enhancing the models subjective chat capabilities. Furthermore, continue tuning on the Instruct model further improves performance, demonstrating that the data synthesized by Condor complements the original models capabilities and can enhance humanpreference performance without introducing conflicts. To further verify these results, we also conducted evaluations using CompassJudger-132B (Cao et al., 2024). As shown in Figure 3, evaluations using both Judge Models reached consistent conclusion. Knowledge Performance Evaluation. We further conduct experiments on several knowledgebased benchmarks to assess the models objective question-answering capabilities. The detailed results are presented in Table 2. It is worth noting that our synthetic data doesnt specifically include knowledge-based QAs like the official models, yet it achieves comparable or even superior results. its efficiency and effectiveness. 4.4 Ablation Study We further investigate the impact of model type, size, and question difficulty on Condor training through series of ablation experiments using Condor data generated by Qwen2.5-72B-Instruct. For the type ablation, we evaluate models of different families, including Qwen, InternLM, and Llama. For the size ablation, we test Qwen2.5 at 7B, 14B, and 32B parameter scales. Additionally, in the difficulty ablation, we train models with datasets containing questions of varying difficulty levels. In all experiments, we utilize CompassJudger as judger for subjective evaluation. Model Type. From Table 4, it is evident that training on DR consistently improves performance across different models. Almost all models demonstrate significant enhancements on all subjective evaluation datasets. Compared to the official model, Qwen2.5-7B shows an improvement more than 6% (56.9% to 63.3%) after training on Condor Refine. InternLM2 and Llama exhibit an even larger improvement by about 10%. These results highlight that the quality of the base models plays crucial role in determining the conversational performance of the final Instruct model. Model Size. We further evaluate the adaptability of models of different sizes to DR. Table 5 show results aligning with the observations from Table 4. Interestingly, we find that the improvement in performance for the 14B and 32B models is notably greater compared to the 7B variant. This suggests that larger models demonstrate greater capacity to learn and benefit from the refined data and achieve better performance. Task Difficulty. Table 6 presents the performance of models trained with Condor data of varying difficulty levels. The results show that tasks of increasing difficulty contribute to greater performance improvements, with more challenging tasks yielding higher average scores. Additionally, combining all three difficulty levels during training further enhances performance, resulting in an average improvement of 0.34%."
        },
        {
            "title": "The performance of models within the Condor\npipeline improves with respect to the increasing",
            "content": "2We apply the SFT to an internal version of InternLM2.57B, which is enhanced with instruction data focused on mathematics and coding. Figure 3: Average Score on Human-preference Benchmarks. CompassJudger-1-32B and GPT-4o provide consistent trend in scoring. Method Avg Sub Score Data Size Synth Model Tulu v2 (Ivison et al., 2023) Evol Instruct (Xu et al., 2023) WildChat (Zhao et al., 2024) Magpie (Xu et al., 2024) Qwen2.5-7B-Instruct Condor 7B Condor 72B 24.65 34.33 39.53 55.67 58.02 58.38 60.03 326k 70k 520k 1M 500k+ 200k 200k Mixed GPT GPT Mixed Qwen2.5-72B Unknown Qwen2.5-7B Qwen2.5-72B Table 3: Comparison Results with Other Data Synthesis Methods. We adopt SFT data generated by various approaches to tune Qwen2.5-7B-Base and use GPT4o as the judge model. The outcome indicates that our dataset not only enhances the models human-preference performance but also preserves its objective knowledge QA capabilities. Additionally, the results demonstrate that the models chat ability and knowledge capability are largely independent. The knowledge capability primarily relies on the foundational base models abilities, whereas the chat ability can be independently improved through SFT and RLHF. Comparing with the State-of-the-art. We generate two sets of Condor data using Qwen2.5-7B and Qwen2.5-72B and compare them with existing state-of-the-art synthetic datasets. We use the data obtained from various data synthesis methods to train starting from the Qwen2.5-7B-Base, and evaluate the model with GPT-4o judge. As shown in Table 3, Condor data generated by both the 7B and 72B models significantly outperforms other data synthesis methods and even surpasses the official model after RLHF. This observation indicates that self-improvement is achievable even at the 7B scale, which will be further analyzed in detail in Section 4.6. Moreover, Condor requires only single model for the entire data synthesis and refinement pipeline with and mach smaller volume of data than most synthetic datasets, highlighting Datasets CompassArena FoFo AlignBenchv1.1 AlpacaEvalv2 ArenaHard FollowBench MTBench101 WildBench Average Qwen2.5-7B Internlm2.5-7B Llama3.1-8B Official Condor Refine Official Condor Refine Official Condor Refine 34.95 0.45 6.16 32.42 54.72 0.86 8.37 15.69 56. 43.62 0.48 6.26 60.00 61.37 0.84 8.18 31.13 63.27 34.03 0.40 5.64 26.34 16.19 0.81 8.00 -13.37 47.19 42.92 0.41 5.79 55.53 36.02 0.79 8.07 17.71 56.44 9.23 0.37 4.66 24.1 31.33 0.83 8.18 -2.24 45.18 20.48 0.50 5.29 47.33 45.73 0.84 8.19 20.45 55.24 Table 4: Performance Comparison across Different Types of Models. Condor Refine shows strong adaptability over different types of models. Datasets CompassArena FoFo AlignBenchv1.1 AlpacaEvalv2 ArenaHard FollowBench MTBench101 WildBench Average Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Official Condor Refine Official Condor Refine Official Condor Refine 34.95 0.45 6.16 32.42 54.72 0.86 8.37 15.69 56.90 43.62 0.48 6.26 60.00 61.37 0.84 8.18 31.13 63.27 35.78 0.55 6.56 33.66 71.00 0.92 8.43 23.42 62.38 55.05 0.58 6.66 71.18 74.95 0.90 8.30 40.80 71.03 40.18 0.59 6.78 35.53 76.82 0.94 8.49 22.46 64.87 60.18 0.63 6.88 74.41 86.83 0.92 8.33 45.84 75. Table 5: Performance Comparison across Different Model Sizes. Qwen2.5-7B, 14B, and 32B all benefit from Condor Refine by significant margin. Different level Exp. Easy Exp. Medium Exp. Hard Exp. All Easy Data Medium Data Hard Data CompassArena FoFo AlignBenchv1.1 AlpacaEvalv2 ArenaHard FollowBench MTBench101 WildBench Average Score 42.22 0.45 6.18 58.76 58.55 0.83 8.18 26.98 61.71 38.95 0.46 6.22 56.89 61.02 0.86 8.17 29.41 62.14 43.62 0.48 6.26 60.00 61.37 0.84 8.18 31.13 63.27 43.07 0.47 6.16 58.51 61.81 0.84 8.16 31.53 62.93 Table 6: Ablations on Different Levels of Task Difficulty. Harder tasks yield greater improvements. amount of training data, tags, and tasks. To investigate these relationships in detail, we conduct comprehensive experiments, as outlined below. Training Data Amount. We visualize the relative performance under different sampling proportions of training data in Figure 4. Surprisingly, we find that the model retains high percentage of its performance even when trained on only small proportion of the data. Additionally, while some performance fluctuations are observed on specific benchmarks due to the relatively smaller data sizes the overall average score consistently improves as the total amount of training data scales up. Number of WKT Tags and Tasks. To better understand the impact of tags and tasks in the Condor pipeline, we conduct ablation experiments by incrementally increasing the proportions of Tasks and WKT Tags. As shown in Figure 5, while there are some fluctuations, the fitted average performance consistently improves with the increasing number of tags and tasks, indicating that both factors contribute positively to enhancing the models humanpreference capabilities. Furthermore, the influence of tasks is observed to be more significant than that of tags, as evidenced by the larger performance improvements associated with adding tasks. Detailed scores are provided in Tables 10 and 11."
        },
        {
            "title": "4.6 Self Iteration",
            "content": "We conduct self-iteration experiments using Condor data generated by Qwen2.5-7B and Qwen2.572B to evaluate whether single model can enhance its chat capabilities by itself. We train the Base model and then report human-preference perDatasets 7B-IT 7B-Condor 72B-IT 72B-Condor CompassArena FoFo AlignBenchv1.1 AlpacaEvalv2 ArenaHard FollowBench MTBench101 WildBench Average 33.80 0.52 6.22 34.66 53.65 0.84 8.60 16.71 58. 32.83 0.45 6.02 41.37 60.39 0.84 8.38 19.78 58.38 43.70 0.66 6.94 47.64 85.67 0.92 8.45 45.23 70.14 48.73 0.60 6.98 56.15 84.86 0.91 8.44 47.75 71.12 Table 7: Condor Self Iteration Experiments on Qwen 7B and 72B. Here, \"IT\" refers to the Instruct model after official RLHF. We use GPT-4o as the judge. Figure 4: Performance Comparison under Different Proportions of Training Data. \"Average\" represents the average score across 8 benchmarks, and the results show clear and consistent improvement as the size of the training data increases (Detailed scores in Table 9). Figure 6: Qwen2.5-7B Performance improvements on Various Sub-Dimensions. Condor Refine consistently improves the model among all the sub-dimensions. shown in Figure 6, the model demonstrates improvement across all sub-capabilities, with the most significant score increases observed in the tasks of Creation, QA, and Chat. These areas align closely with the core aspects of the models humanpreference capability, highlighting Condors effectiveness in enhancing key conversational skills. Figure 5: Performance Results under Different Proportion of WKT Tags and defined Tasks. Tasks have more significant influence than tags, as evidenced by the significant drop with small proportion of tasks. formance scores across 8 benchmarks. As shown in Table 7, both the 7B and 72B models achieve self-improvement even compared with the RLHF models. Notably, the improvement for the 72B model is slightly greater than that of the 7B model, suggesting that larger, more capable models are better equipped to achieve self-improvement."
        },
        {
            "title": "5.1 Which capability is improved the most?",
            "content": "We evaluate the scores for each benchmark across different sub-capability dimensions and observe improvements in sub-capabilities after Condor training (detailed information in Appendix A.2). As"
        },
        {
            "title": "5.2 How large is the synthetic data coverage?",
            "content": "We extract the embeddings of questions from Condor and Magpie and use t-SNE for dimensionality reduction and visualization. To ensure fair and balanced comparison, we randomly sample 200k questions from each dataset for this analysis. Note that according to Magpies claimed method, it is capable of feedbacking the models own training data, which means that the data distribution obtained by Magpie is somewhat close to the models original SFT data. As shown in Figure 7, Condor has similar or even broader distribution compared to Magpie, which indicates that the questions generated by Condor have good diversity. are still many experiments that require further exploration, such as the use of multi-round iterative synthetic data and how to further enhance the diversity of the synthetic data. Additionally, the hallucinations produced by LLMs in synthetic data could also become potential risk. These issues need to be further addressed and improved, which will in turn enhance the quality of the synthetic data. Figure 7: t-SNE visualization of Condor and Magpie. Condor shows great alignment with Magpie, highlighting its large diversity and generalization capabilities. 5.3 What does SFT actually enhance? comprehensive analysis of the experimental results reveals important insights, particularly from the knowledge-based evaluation results  (Table 2)  . The findings indicate that the SFT phase has minimal impact on the models intrinsic knowledge capabilities. This suggests that the vast majority of knowledge acquisition occurs during the pretraining phase. These observations suggest that the SFT phase of Condor contributes very little to enhancing the models foundational knowledge. Instead, its primary focus is on refining the models ability to utilize existing knowledge to answer questions effectively, which can be achieved with relatively small amount of high-quality data."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose Condor, two-stage data synthesis engine to generate high quality data for supervised fine-tuning. Extensive experiments demonstrate that with the high quality data generated by Condor, the performance of the fine-tuned model surpasses many existing methods and the official RLHF models using small amount of synthetic data. We also explore the scaling of synthetic data and self-iteration experiments, demonstrating that models can achieve self-iteration through synthetic data."
        },
        {
            "title": "7 Limitations",
            "content": "Despite the significant improvements brought by Condor in human preference performance, there"
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, et al. 2024. Mt-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv preprint arXiv:2402.14762. Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. 2024. Compassjudger-1: All-in-one judge model helps arXiv preprint model evaluation and evolution. arXiv:2410.16256. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. 2024. Internvl: Scaling up vision foundation models and aligning for In Proceedings of generic visual-linguistic tasks. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. Preprint, arXiv:1905.10044. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. OpenCompass Contributors. 2023a. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. XTuner Contributors. 2023b. Xtuner: toolkit for efficiently fine-tuning llm. https://github.com/ InternLM/xtuner. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. Preprint, arXiv:1903.00161. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. 2023. Reinforced self-training (rest) for language modeling. Preprint, arXiv:2308.08998. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. Preprint, arXiv:2103.03874. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. Preprint, arXiv:2210.11610. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah Smith, Iz Beltagy, et al. 2023. Camels in changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023a. Mistral 7b. arXiv preprint arXiv:2310.06825. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2023b. Followbench: multi-level fine-grained constraints following benchmark for large language models. arXiv preprint arXiv:2310.20410. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939. Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng Liu, Chenghua Lin, Lei Ma, Wenhao Huang, and Jiajun Zhang. 2024. I-sheep: Self-alignment of llm from scratch through an iterative self-enhancement paradigm. Preprint, arXiv:2408.08072. Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. 2024. Wildbench: Benchmarking llms with challenging tasks from real users in the wild. arXiv preprint arXiv:2406.04770. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. 2024a. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024b. Visual instruction tuning. Advances in neural information processing systems, 36. Shuo Tang, Xianghe Pang, Zexi Liu, Bohan Tang, Rui Ye, Xiaowen Dong, Yanfeng Wang, and Siheng Chen. 2024. Synthesizing post-training data for llms through multi-agent simulation. arXiv preprint arXiv:2410.14251. Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. 2024. survey on self-evolution of large language models. Preprint, arXiv:2404.14387. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Gemini: Ke Wang, Jiahui Zhu, Minjie Ren, Zeming Liu, Shiwei Li, Zongye Zhang, Chenkai Zhang, Xiaoyu Wu, Qiqi Zhan, Qingjie Liu, et al. 2024a. survey on data synthesis and augmentation for large language models. arXiv preprint arXiv:2410.12896. Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. 2023. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. Preprint, arXiv:2212.10560. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. Preprint, arXiv:2303.17651. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Refiner: Reasoning feedBoi Faltings. 2024. back on intermediate representations. Preprint, arXiv:2304.01904. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. Preprint, arXiv:2311.12022. Ming Shen. 2024. Rethinking data selection for supervised fine-tuning. arXiv preprint arXiv:2402.06094. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024b. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Preprint, arXiv:2406.01574. Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and Caiming Xiong. 2024. Fofo: benchmark to evaluate llms format-following capability. arXiv preprint arXiv:2402.18667. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Preprint, arXiv:2303.11366. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. arXiv 2024. Self-rewarding language models. preprint arXiv:2401.10020. Taolin Zhang, Sunan He, Tao Dai, Zhi Wang, Bin Chen, and Shu-Tao Xia. 2024a. Vision-language pretraining with object contrastive learning for 3d scene In Proceedings of the AAAI Conunderstanding. ference on Artificial Intelligence, volume 38, pages 72967304. Taolin Zhang, Jinpeng Wang, Hang Guo, Tao Dai, Bin Chen, and Shu-Tao Xia. 2024b. Boostadapter: Improving test-time adaptation via regional bootstrapping. arXiv preprint arXiv:2410.15430. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. 2024c. Evaluating the performance of large language models on gaokao benchmark. Preprint, arXiv:2305.12474. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. Preprint, arXiv:2311.07911. Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 2023. 3d-vista: Pretrained transformer for 3d vision and text alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29112921."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Detailed Calculation Method for the Subjective Evaluation Scores. Since the metrics for statistical scoring of various subjective chat datasets are not the same, and the scoring ranges are not all 0-100, we use the following mapping relationships when calculating the average subjective chat score for the model: The scoring ranges for CompassArena, AlpacaEvalv2, and ArenaHard are 0-100, and no special treatment is needed when calculating the overall mean score. For FoFo and Followbench, the scoring range is 0-1, and we multiply by 100 when calculating the overall mean score. For AlignBenchv1.1 and MTBench101, the scoring range is 0-10, and we multiply by 10 when calculating the overall mean score. For WildBench, the scoring range is -100 to 100, and we add 100 to the score and then divide by 2 for mapping. The final calculated total Average score is the mean score of these datasets after they have been mapped to the 0-100 range. A.2 Detailed Information for Sub-capabilities Improvement of Condor Based on the subscore for each capability dimension provided by each dataset, we aggregated the models scores by dimension, thereby obtaining the models scores on each capability dimension across these subjective test datasets. Specifically, according to the different subscores for various capability dimensions provided by different datasets, we aggregated the scores in the following manner: Math: The math and reasoning capabilities of model, we aggregate the sub-score from AlignBench, CompassArena. Task: The task problem svoling capability of model, we aggregate from AlignBench, ArenaHard and MTBench101. Creation: The ability of the model to create various types of content as required is aggregated from AlignBench, CompassArena, MTBench101, and WildBench. Role-play: The role-playing capability of the model is aggregated from AlignBench, AlpacaEval, and WildBench. QA: The knowledge-based question-answering capability of the model is aggregated from AlignBench and CompassArena. Chat: The daily chat capability of the model is aggregated from AlignBench, AlpacaEval, ArenaHard, MTBench101, and WildBench. IF: The instruction following capability of the model is aggregated from FoFo and FollowBench. Language: The language understanding and processing capability of the model is aggregated from AlignBench and CompassArena. Datasets CompassArena FoFo AlignBenchv1.1 AlpacaEvalv2 ArenaHard FollowBench MTBench101 WildBench Average Qwen2.5-7B-Instruct Qwen2.5-7B-Base Qwen2.5-7B-Instruct Official Condor Void Condor Refine Condor Void Condor Refine 34.95 0.45 6.16 32.42 54.72 0.86 8.37 15.69 56.9 33.78 0.38 6.08 33.04 43.35 0.86 8.12 13.82 54.01 43.62 0.48 6.26 60.00 61.37 0.84 8.18 31.13 63.27 38.17 0.50 6.32 43.98 57.33 0.83 8.28 18.52 59.67 46.68 0.48 6.32 63.85 63.30 0.85 8.24 32.99 64. Table 8: Detailed Results for Condor on Subjective Evaluation Datasets Judged by CompassJudger-1-32B. Proportion Datasets CompassArena FoFo AlignBenchv1.1 AlpacaEvalv2 ArenaHard FollowBench MTBench101 WildBench Average 1% 2.5% 5% 10% 25% 50% 100% 20.02 0.18 5.28 22.36 42.00 0.80 6.71 0.44 44.07 40.80 0.41 6.14 56.65 56.99 0.76 8.01 29.23 59. 43.35 0.42 6.07 55.53 59.27 0.80 8.16 28.54 60.79 41.82 0.47 6.15 58.88 59.56 0.82 8.17 29.2 62.09 40.22 0.46 6.20 60.62 63.17 0.82 8.18 28.79 62.50 42.98 0.48 6.27 56.89 61.64 0.82 8.22 30.41 62.68 43.62 0.48 6.26 60.00 61.37 0.84 8.18 31.13 63.27 Table 9: Performance Comparison under Different Proportions of Training Data. Here, 100% indicates the use of the full dataset, which is approximately 200K synthetic data for training, while the other proportions involve random sampling of the full dataset based on the given percentage. Proportion Datasets CompassArena FoFo AlignBenchv1.1 AlpacaEvalv2 ArenaHard FollowBench MTBench101 WildBench Average 12.50% 25% 37.50% 50% 62.50% 75% 87.50% 100% 42.57 0.45 6.14 60.12 62.19 0.80 8.18 28.83 62.08 40.98 0.47 6.18 55.65 62.43 0.86 8.20 27.96 62. 41.87 0.49 6.18 55.03 60.81 0.85 8.15 29.70 62.43 42.25 0.48 6.24 56.89 60.01 0.83 8.17 28.97 62.3 42.88 0.50 6.25 56.77 59.73 0.87 8.14 27.15 62.98 41.75 0.46 6.19 57.76 60.11 0.83 8.18 28.89 62.07 43.5 0.48 6.14 58.14 60.99 0.81 8.20 29.25 62.46 43.62 0.48 6.26 60.00 61.37 0.84 8.18 31.13 63. Table 10: Performance of the model after training with different numbers of tags in the training data. The total number of Chinese tags is 4249, and the total number of English tags is 4296. We randomly sampled the Chinese and English tags in proportion and then combined the sampled Chinese and English data for training to ensure balance of tags between the two languages. Datasets CompassArena FoFo AlignBenchv1.1 AlpacaEvalv2 ArenaHard FollowBench MTBench101 WildBench Average Task Number 1 3 4 5 6 7 42.83 0.42 6.18 56.65 62.6 0.81 8.11 33.07 61. 42.62 0.42 6.28 58.26 59.40 0.83 8.24 33.69 62.16 41.1 0.44 6.24 59.75 60.86 0.76 8.21 31.38 61.46 42.63 0.44 6.15 56.65 60.61 0.75 8.18 31.38 60.89 42.85 0.49 6.20 57.64 61.60 0.78 8.15 29.33 62.04 43.88 0.46 6.16 58.51 62.91 0.82 8.19 30.94 62.81 43.62 0.48 6.26 60.00 61.37 0.84 8.18 31.13 63. Table 11: Performance of the model after training with different numbers of tasks in the training data. All the results in the table were obtained by testing the Qwen2.5-7B-Base model after training on the relevant datasets, with the number of tasks increasing by inclusion. The first task is Role-Playing, the second task is Daily Chat, the third task is Domain Knowledge Q&A, the fourth task is Given Material Processing, the fifth task is Response Format Control, the sixth task is View, and the seventh task is Creation. Prompt for Question Synthesis Now we need to create high-quality SFT data for LLM training, so we need you to produce batch of such data. You only need to create Questions. will give you theme and some examples of SFT data Questions. You need to create three Questions of different difficulty levels based on this new theme. Your Questions must meet the following requirements: 1. You must strictly create only three Questions at time. These three Questions must be in the domain of [[domain]] and the Questions should align with the given theme of [[theme]]. 2. The Questions you create must have context and sufficient information; they should not be abrupt and directly ask the question. 3. Your reply must strictly follow the format below. Your Questions need to be included between [Question Start] and [Question End], and the difficulty level should be indicated at the beginning, as in the following format: [Easy][Question Start]Question[Question End] [Medium][Question Start]Question[Question End] [Hard][Question Start]Question[Question End] 4. Your Questions of different difficulty levels should be distinct and actually reflect the different levels of difficulty. Here are some examples of high-quality SFT data Questions for [[domain]]: list] Now its your turn. Please provide the three Questions of different difficulty levels you created about the theme of [[theme]] for [[domain]], according to the requirements. Do not be confined by the theme; your Questions only need to be related to the theme. You can use your rich imagination, but note that you cannot copy the expression from the examples; you must have your own new expression: [example"
        },
        {
            "title": "Prompt for Response Critic",
            "content": "There is now users question and models response. You need to write critique for this response, pointing out the strengths and weaknesses of the models answer to help the model improve its response. Your critique must strictly adhere to the following format: [Critique Start] [Strength Start]Strength[Strength End] [Weakness Start]Weakness[Weakness End] [Suggestion Start]Suggestion[Suggestion End] [Critique End] That is, you must place the strength, weakness, and suggestion in their respective sections. Here is the users question and the models response: [dialogue] Now its your turn. Please provide your Critique as required: Condor Void Example - User Question was watching documentary about mapping ancient cities, and it got me thinking about how maps have evolved over time. Have you ever seen an old map and marveled at how different it looks from todays maps? Whats the oldest map youve ever seen? Condor Void Example - Origin Answer As digital AI assistant, dont have personal experiences, emotions, or visual interactions, so havent \"seen\" maps in the classical sense. dont have physical presence, and my interactions are limited to text-based conversations. However, Id be delighted to take you on journey through the evolution of maps and highlight some notable examples of ancient and historic maps! The Evolution of Maps 1. Prehistory ( 20,000 BCE - 3,000 BCE): Early maps were often simple, symbolic, and focused on specific areas, like the Turin Papyrus Map ( 1160 BCE), which depicts the location of gold mines in ancient Egypt. 2. Ancient Civilizations ( 3,000 BCE - 500 CE): - Babylonian Map of the World ( 600 BCE): clay tablet showing the world as flat disk, surrounded by an ocean. - Ptolemys World Map ( 150 CE): more accurate, yet still flawed, representation of the known world, using longitude and latitude. 3. Medieval Period ( 500 - 1500 CE): - Mappa Mundi ( 1300 CE): symbolic, Christian-centric map, often displaying Jerusalem at the center. 4. Age of Exploration ( 1500 - 1700 CE): - Waldseemüller Map (1507 CE): One of the earliest maps to depict the New World, naming America after Amerigo Vespucci. 5. Modern Cartography ( 1700 CE - present): - Triangulation and Surveying: Led to more accurate mapping of territories. - Aerial Photography and Satellite Imaging (20th century): Revolutionized mapmaking with highresolution, globally consistent data. - Digital Maps (late 20th century - present): Enabled interactive, dynamically updated, and highly accurate maps, like Google Maps. Some of the Oldest Known Maps - Cave of El Castillo Map ( 14,000 BCE): simple, prehistoric map etched into cave wall in Spain, showing the location of nearby water source. - The Bedolina Map ( 5,000 BCE): rock carving in Italy, depicting valley with topographical features. - The Nubian Map ( 2,000 BCE): An ancient Egyptian map, showing the location of Nubian gold mines and territories. Marveling at Old Maps While may not have personal experiences, can certainly help you appreciate the historical context, artistic value, and cartographic innovations of old maps. If youd like to discuss specific map or era in more detail, feel free to ask! Which aspect of old maps fascinates you the most? Is it: A) The artistic and symbolic representations B) The historical context and cultural influences C) The cartographic innovations and technological advancements D) Something else (please share!) Condor Refine Example - Reflection [Strength Start] Strengths: 1.Comprehensive Response: The model provides an exhaustive overview of the evolution of maps, covering various historical periods, notable examples, and technological advancements. 2.Structured Formatting: The use of clear headings, numbered lists, and concise bullet points enhances readability and facilitates easy understanding of complex information. 3.Engagement Initiator: The response ends with an interactive element, encouraging user participation and potentially leading to more in-depth discussion. 4.Honesty About Limitations: The model transparently acknowledges its limitations as digital AI assistant, setting clear expectations for the user. [Strength End] [Weakness Start] Weaknesses: 1.Length and Information Overload: The response is lengthy and packed with numerous examples, which might overwhelm the user. Some points could be elaborated upon in subsequent interactions rather than all at once. 2.Lack of Visual Enhancement: Despite discussing maps, the response is text-only. Incorporating images, diagrams, or even suggesting external visual resources could significantly enhance the users understanding and engagement. 3.Initial Acknowledgement Could Be Brief: While honesty about the models limitations is appreciated, the initial acknowledgement could be more concise to quickly transition to the more engaging and informative sections. 4.Question Response Mismatch: The user inquired about the oldest map the model has \"seen,\" which the model addresses by stating its inability to see. However, the model could more directly address the implicit curiosity about old maps by initially highlighting one or two of the oldest known maps before delving into the broader evolution. [Weakness End] [Suggestion Start] Suggestions for Improvement: 1.Tiered Information Disclosure: Initially provide brief overview of the evolution of maps and highlight 2-3 of the oldest known maps. Offer the user the option to explore specific eras or types of maps in more detail, facilitating more interactive and paced information exchange. 2.Incorporate Visual Aids or References: Suggest reputable online resources or include descriptions that encourage users to visually explore the mentioned maps, enhancing their understanding of cartographic developments. 3.Refine the Initial Limitation Disclosure: Condense the initial acknowledgement to single sentence, swiftly moving the focus to the engaging content (e.g., \"As text-based AI, Ill guide you through the fascinating evolution of maps, highlighting notable examples and innovations.\"). [Suggestion End] Condor Refine Example - Refined Answer Discovering the Ancient Roots of Modern Maps As text-based AI, Ill guide you through the fascinating evolution of maps, highlighting notable examples and innovations. Lets Start with the Oldest Known Maps: 1.Cave of El Castillo Map ( 14,000 BCE): Etched into Spanish cave wall, this simple map shows the location of nearby water source. 2.The Bedolina Map ( 5,000 BCE): rock carving in Italy, depicting valley with topographical features. Visual Exploration Suggestion: Visit the British Museums Online Collection or Google Arts & Culture to see high-resolution images of these ancient maps.** Curious About the Evolution of Maps? Heres concise overview. Feel free to pick any era to explore in more depth: - Prehistory ( 20,000 BCE - 3,000 BCE): Simple, symbolic maps (e.g., Turin Papyrus Map) - Ancient Civilizations ( 3,000 BCE - 500 CE): Notable examples include Babylonian Map of the World and Ptolemys World Map - Medieval to Modern ( 500 - present): Advances from Mappa Mundi to Triangulation, Aerial Photography, and Digital Maps like Google Maps Which Era or Aspect of Old Maps Would You Like to Explore Further? A) Ancient symbolic representations B) Medieval navigational charts C) The leap to digital cartography D) Something specific (please share, and Ill delve deeper!) Detailed explanation of total 7 chat tasks [Task 1-Role Playing] Engage in simulated conversations or behaviors by assuming different roles to explore various perspectives or scenarios. [Task 2-Daily Chat] Participate in casual conversations, including greetings, small talk, and sharing everyday experiences. [Task 3-Domain QA] Provide specialized and accurate answers to questions within specific domain or field. [Task 4-Given Material Processing] Analyze, process, and summarize given texts, data, or other materials. [Task 5-Response Format Control] Adjust the format, style, and expression of responses according to specified requirements. [Task 6-View] Offer personal opinions, insights, or perspectives on particular topic. [Task 7-Creation] Generate new content such as articles, stories, poetry, designs, etc. Comparison Example - User Question Is the ATF violation in of itself against the American people? Comparison Example - Qwen2.5-7B-Instruct Response The term \"ATF\" typically refers to the Bureau of Alcohol, Tobacco, Firearms and Explosives, federal agency within the United States Department of Justice. The ATF is responsible for reducing crime and preventing violence by reducing the illegal flow and improper use of firearms, alcohol, tobacco, and explosives. The ATF itself is not violation against the American people. Rather, it is an agency that operates under the laws and regulations established by the U.S. government to enforce laws related to alcohol, tobacco, firearms, and explosives. The agencys actions are generally aimed at preventing crime and ensuring public safety. However, like any government agency, the ATF can be subject to criticism and controversy. Some criticisms have been raised regarding the agencys enforcement practices, the impact of its regulations on law-abiding citizens, and concerns about civil liberties. These criticisms do not imply that the agency itself is violation, but rather that its actions or policies may be subject to scrutiny and debate. If you have specific concerns or questions about the ATFs actions or policies, it would be helpful to provide more context or details so that more accurate and informed response can be given. Comparison Example - Qwen2.5-7B-Condor-Refine Response The term \"ATF\" typically refers to the Bureau of Alcohol, Tobacco, Firearms and Explosives, federal agency in the United States responsible for enforcing federal laws related to the manufacture, possession, and use of alcohol, tobacco, firearms, and explosives. The ATF is not violation against the American people; rather, it is an agency that operates within the legal framework established by the U.S. Congress to enforce specific laws. However, the ATFs operations and the laws it enforces can be subjects of debate and criticism. For example, some argue that the agencys enforcement of gun control laws infringes on the Second Amendment rights of law-abiding citizens. Others may criticize the agency for its handling of certain cases or its budget and resource allocation. Additionally, there have been instances where the ATF has been involved in controversial operations, such as the Ruby Ridge and Waco incidents, which have fueled public debate and criticism. Its important to note that the ATFs role is to enforce the laws as they are written, and the laws themselves are subject to interpretation and debate. The agencys actions are generally aimed at reducing crime and ensuring public safety, but the effectiveness and fairness of these actions are often subjects of public and political discourse. If you have specific concerns or questions about the ATFs operations or the laws it enforces, it might be helpful to look at the broader context of U.S. law and policy, including the legislative history and the ongoing debates surrounding gun control and public safety. For more detailed information, you can refer to official ATF publications, such as the ATFs annual reports, or consult reputable news sources and legal analyses.\""
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Tsinghua University"
    ]
}