{
    "paper_title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting",
    "authors": [
        "Atin Pothiraj",
        "Elias Stengel-Eskin",
        "Jaemin Cho",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recognizing and reasoning about occluded (partially or fully hidden) objects is vital to understanding visual scenes, as occlusions frequently occur in real-world environments and act as obstacles for spatial comprehension. To test models' ability to reason about multiple occluded objects, we introduce a novel task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which requires a model to count objects arranged in a pattern by inferring how the pattern continues behind an occluder (an object which blocks parts of the scene). CAPTURe requires both recognizing visual patterns and reasoning, making it a useful testbed for evaluating vision-language models (VLMs) on whether they understand occluded patterns and possess spatial understanding skills. By requiring models to reason about occluded objects, CAPTURe also tests VLMs' ability to form world models that would allow them to fill in missing information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually filtered images of real objects in patterns and (2) CAPTURe-synthetic, a controlled diagnostic with generated patterned images. We evaluate four strong VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models struggle to count on both occluded and unoccluded patterns. Crucially, we find that models perform worse with occlusion, suggesting that VLMs are also deficient in inferring unseen spatial relationships: even the strongest VLMs like GPT-4o fail to count with occlusion. In contrast, we find that humans achieve very little error on CAPTURe. We also find that providing auxiliary information of occluded object locations increases performance, underscoring that the model error comes both from an inability to handle occlusion as well as difficulty counting in images."
        },
        {
            "title": "Start",
            "content": "CAPTURE: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting"
        },
        {
            "title": "Atin Pothiraj",
            "content": "Elias Stengel-Eskin"
        },
        {
            "title": "UNC Chapel Hill",
            "content": "{atin, esteng, jmincho, mbansal}@cs.unc.edu 5 2 0 2 1 2 ] . [ 1 5 8 4 5 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recognizing and reasoning about occluded (partially or fully hidden) objects is vital to understanding visual scenes, as occlusions frequently occur in real-world environments and act as obstacles for spatial comprehension. To test models ability to reason about multiple occluded objects, we introduce novel task, Counting Amodally for Patterns Through Unseen REgions (CAPTURE), which requires model to count objects arranged in pattern by inferring how the pattern continues behind an occluder (an object which blocks parts of the scene). CAPTURE requires both recognizing visual patterns and reasoning, making it useful testbed for evaluating vision-language models (VLMs) on whether they understand occluded patterns and possess spatial understanding skills. By requiring models to reason about occluded objects, CAPTURE also tests VLMs ability to form world models that would allow them to fill in missing information. CAPTURE consists of two parts: (1) CAPTUREreal, with manually filtered images of real objects in patterns and (2) CAPTUREsynthetic, controlled diagnostic with generated patterned images. We evaluate four strong VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2VL) on CAPTURE, finding that models struggle to count on both occluded and unoccluded patterns. Crucially, we find that models perform worse with occlusion, suggesting that VLMs are also deficient in inferring unseen spatial relationships: even the strongest VLMs like GPT-4o fail to count with occlusion. In contrast, we find that humans achieve very little error on CAPTURE. We also find that providing auxiliary information of occluded object locations increases performance, underscoring that the model error comes both from an inability to handle occlusion as well as difficulty counting in images.1 1. Introduction Inferring what lies behind different objects in occluded scenes is crucial for human perception, as it allows us to 1Code and data: https://github.com/atinpothiraj/CAPTURe Figure 1. CAPTURE example with an output from GPT4-o. While people can easily infer the missing number of cups and correctly reason over occluded patterns, models generally struggle to reason over these occluded scenes. maintain coherent understanding of our environment even when parts are hidden. The human visual system accomplishes this by integrating past experiences, context, and sensory inputs to reconstruct incomplete scenes [19, 27, 30, 44]. Meanwhile, recent advancements in vision-language models (VLMs) especially in terms of visual and spatial reasoning raise the question of whether these systems can perform similar inferential tasks. One way of measuring such capabilities is through amodal completion the task of inferring the invisible parts of partially occluded objects; here, vision-only models are typically evaluated via dense prediction tasks like object segmentation and image inpainting [5]. However, this format is not well-suited for assessing VLMs, whose outputs consist of text tokens rather than pixel-level predictions. This raises critical question: how can we quantify the ability of VLMs to form spatial world modeling [17] in the presence of occlusion. To address this, we introduce CAPTURE, Counting Amodally for Patterns Through Unseen REgions, novel benchmark that tests VLMs world modeling and spatial reasoning abilities through the task of amodal count1 ing, where models are prompted to count occluded objects by amodally completing pattern. CAPTURE focuses on counting as it provides an objective and easy-to-verify output by comparing predicted counts with ground truth values. Moreover, patterned objects appear in variety of realworld domains, especially in man-made environments like parking lots, cities, and warehouses, where counting objects is often required. Fig. 1 illustrates the CAPTURE task. We show VLM an image where objects are placed in regular pattern (e.g., 4x4 grid) with some objects occluded, and ask the model to count the total number of objects in the image assuming that the pattern continues behind the occlusion. The task requires handling occlusion, pattern recognition, and counting skills that exist in humans from fairly young age [27, 30, 44], thus humans can easily answer such questions indeed, we find that people can complete CAPTURE tasks with almost no error. CAPTURE consists of two subsets: CAPTUREreal and CAPTUREsynthetic. As shown in Fig. 2, CAPTUREreal contains real-world images and tests the ability of models to perform amodal counting in naturalistic contexts, while CAPTUREsynthetic allows us to analyze specific factors by controlling different variables like color, shape, and number of objects. All images in CAPTURE contain pattern of objects and manually annotated occluding black box covering some objects. CAPTUREreal contains 924 images with diverse range of settings and objects, covering 92 different object types, while CAPTUREsyntheticcontains 1250 images across multiple attribute classes. By combining vision encoders with large language models (LLMs), VLMs have the potential to reason in zeroshot way about visual inputs. To put this ability to the test and measure VLMs ability to reason about missing visual information, we evaluate four strong recent VLMs (GPT4o, InternVL2, Molmo, and Qwen2VL) on CAPTURE. Our experiment results (Sec. 4) show that models generally struggle with the multiple aspects of task, with high error rates on both CAPTUREreal and CAPTUREsynthetic for occluded and unoccluded images. In contrast, we find that humans can perform the task easily: whereas model performance deteriorates as more objects in images are occluded, humans complete the task almost perfectly. We also compare VLMs to vision-only model trained to count visible objects; while this model generally outperforms VLMs, its error is directly tied to the number of occluded objects the more objects are occluded, the higher its error will be. By objectively measuring VLMs spatial reasoning capabilities under occlusion, CAPTURE highlights an unexpected weakness in VLMs. We analyze this weakness by providing the model with additional clues and information. Specifically, we test to what degree the VLMs failure stems from an inability to integrate visual information by providing it with text-based representation of the visible objects in the image in the form of object coordinates; here, VLMs perform substantially better, indicating that their poor performance on CAPTURE stems partly from an inability to count objects in images, rather than an inability to count more generally. This aligns with previous work, which similarly finds that VLMs struggle to count in images [22, 33, 41]. We also test the degree to which VLM errors stem from an inability to form world model by providing it with auxiliary information (the coordinates of the occluded objects in text, or inpainting of occluded regions). We find that VLMs perform substantially better with this auxiliary information, suggesting that VLMs are partly limited by their inability to imagine the missing visual information. Addressing these gaps is critical for VLMs to function effectively in real-world scenarios, where visual reasoning often involves occlusions whether counting stadium seats, components on production lines, or buildings in neighborhoods. We hope that our work will foster future research on improving the world modeling capabilities of VLMs. 2. CAPTURE 2.1. Task Overview Input/output formulation. CAPTURE tests VLMs in their ability to reason over occluded scenarios, recognize patterns, and count visible and occluded objects. VLMs already achieve high accuracy in classifying single, occluded objects [20]. Thus, we also argue that VLMs have the potential to perform well on CAPTUREs challenging task because their proficiency in handling occlusion ought to enable them to recognize occluded objects and reason accordingly. All images in CAPTURE contain pattern. This makes the task solvable for models and people if the objects were not placed in pattern, it would be unreasonable to expect models to infer the position of the occluded objects. For example, given an image of random pile of coins with region occluded, it is not easy to infer whether the occluded region simply contains no coins or contains roughly the same amount as the rest of the pile. For this task, the patterns considered are all regular and fairly small patterns, e.g. grids, circles, triangles, and other regular shapes see Fig. 2 for further examples. The last step of CAPTURE is counting, asking the model to provide an objectively measurable output. In addition to VLMs, we also test COUNTGD [3], state-of-the-art object detection-based counting method, finding that it fails to account for the occluded scenario, as its training entails solely predicting the visible, unoccluded objects in the image. Metric. We use symmetric mean percent error (sMAPE) as the primary metric. sMAPE is given by: sMAPE = 100 1 (cid:88) i=1 yi ˆyi yi + ˆyi (1) Figure 2. Example images with GPT-4o responses to CAPTUREreal and CAPTUREsynthetic occluded splits. where yi represents the actual values, ˆyi represents the predicted values, and is the number of observations. sMAPE is capped at 100%, providing fixed range. This makes sMAPE ideal for challenging tasks like ours, as we can penalize responses that fail to produce an answer with max error of 100%. For justification of sMAPE over other metrics, see Appendix A.1. 2.2. Dataset CAPTUREreal. We introduce set of real images with patterns to test amodal counting in naturalistic settings. The original images and annotations come from the FSC-147 dataset [37], diverse counting dataset with manual annotations for the number of target objects and all object bounding boxes in each image. FSC-147 contains diverse array of objects, with 6146 real-world images across 147 object categories. We filter FSC-147 for images that contain identifiable and regular patterns of objects and manually overlay black box to occlude some objects, resulting in 924 images. Filtering is first performed by GPT-4o and then manually verified; we also manually verify that determining objects despite the occlusion is feasible. For each example, we maintain both an occluded version and an unoccluded version. Additional details on how we create CAPTUREreal can be found in Appendix B. CAPTUREsynthetic. While CAPTUREreal makes CAPTURE more applicable to real-world scenarios, each image is unique, making the data less controlled and challenging to draw clear conclusions about model performance. Images without background distractors, texture variance, and other potential visual obstacles provide more controlled version of the task. Therefore, we create CAPTUREsynthetic to examine the task in fully controlled environment. CAPTUREsynthetic comprises 1250 images of simple objects in patterns, where different variables are held constant or changed. We vary the following features: 1. Object count: varies from 5 to 15. 2. Object: can be either dots or squares. 3. Arrangement/shape: can be rectangle, circle, or pyramid (where feasible based on object count). 4. Location: we consider five positions on the page: center, top-left, top-right, bottom-left, or bottom-right. 5. Color: we randomly choose one of 5 colors for all the objects in an image. The CAPTUREsynthetic data is split similarly to the CAPTUREreal data; each configuration has variant with an overlaid occluding box and one without. 2.3. Statistics and Examples Fig. 2 shows examples from CAPTUREreal and CAPTUREsynthetic paired with their corresponding answers from GPT-4o and their ground truth answers. These examples show the range of objects and patterns present in the dataset, and highlight the feasibility of the task for human. Tab. 1 reports summary statistics for CAPTURE, including the number of images and object types, as well as the mean number of occluded and total objects in both splits of CAPTURE. The number of objects in CAPTUREreal is shown in Fig. 3, where most images have between 0 and 30 objects. On CAPTUREsynthetic, the maximum number of objects is 15, and CAPTUREsynthetic images generally have 1-6 occluded objects (shown in Fig. 4, as further occlusion could make the count unresolvable). 3. Experiment Setup 3.1. Models We experiment with GPT-4o [28], Intern-VL2-Llama3-8B [9, 10], and Qwen2-VL-7B [40] for their high scores on other VLM tasks [29]. We add Molmo 7B-D [13], because of its ability to point and count, giving it potential advantage on CAPTURE. Specifically, Molmo is trained on millions of examples that directly ground text to 3 # Images # Object Types Avg. Occluded Obj. Avg. Total Obj. Strengths CAPTUREreal CAPTUREsynthetic 924 92 13.97 61.45 1250 2 2.73 10.00 Diverse Objects/Settings Naturalistic Realistic Context Confounder-free Controllable Attributes Uniformly Distributed Table 1. Statistics and strengths for CAPTURE splits. Figure 3. # of objects in CAPTUREreal images. Figure 4. # of occluded objects in CAPTUREsynthetic images. 2D coordinates (or points) in images. This allows Molmo to directly point to image coordinates, also enabling it to count more easily by pointing to several objects. All the VLMs feature different language backbone and vision encoder to provide broad coverage of model architectures. To evaluate models, we provide the model with the name of the specific object to be counted and the explicit instruction to count fully visible objects and objects behind the occluding box (in the occluded images). For each model, we test ten prompts on validation set of 100 images, selecting the best prompt for each model in each dataset section (CAPTUREreal/CAPTUREsynthetic) and for each environment (occluded/unoccluded). We provide the selected prompts in Appendix D. 3.2. Answer Generation and Extraction Given the complex nature of CAPTURE, we allow models to generate open-ended responses and then subsequently extract answers. Further details (including max number of tokens) can be found in Appendix A.2. 4 Answer extraction. Empirically, we found that constraining the output to specific format for ease of analysis negatively impacted benchmark performance. Therefore, we instead prompt models to generate freely and extract the final output number using separate answer extractor based on Llama 3.1 8B [1]. This answer extractor takes as input the output from the model and is prompted to extract single number representing the final answer. The answer extractor also identifies if an output failed to converge on singular number answer and assigns label to these examples. We mark such incomplete/incoherent model generations as skipped questions and when calculating the error later, these responses are assigned the worst possible sMAPE score (100%). The answer extractor outputs were manually verified on 1000 outputs, where the extractor was found to have 100% accuracy. Human and object detection baselines. We also report the performance of humans and recent counting model (COUNTGD [3]) as baselines to establish point of reference for model performance. To confirm that humans can perform the CAPTURE task, we provided 100 randomlyselected occluded examples each from the CAPTUREreal and CAPTUREsynthetic subsets to 3 undergraduate students with no prior knowledge of the task. 4. Results and Analysis 4.1. Main Results on CAPTUREreal Models consistently struggle with counting and perform worse on occluded images. We run the VLMs on the occluded and unoccluded versions of CAPTURE to discern whether occlusion significantly impacts model performance. Tab. 2 shows that all models struggle with counting generally, performing poorly on both splits. Moreover, we see that every model performs better on the unoccluded images. On average, the models perform 6.28% worse in CAPTUREreal occluded images and 4.85% worse in CAPTUREsynthetic occluded images (in terms of absolute sMAPE), indicating increased difficulty from standard counting task. The best model for both splits, GPT-4o, has an error rate of 14.75% on CAPTUREreal and lower error rate of 5.90 on synthetic. Across both the real and synthetic split, GPT-4os error increases with occlusion, by 1.41% on the real data and 3.81% on the synthetic split. Interestingly, despite its fine-tuning on counting tasks, Molmo exhibits sizable error rate of 32.5% on CAPTUREreal occluded images. The high error rates of VLMs indicate limited capabilities in visual understanding under occlusions, pattern recognition, and counting. We further analyze the source of these errors with oracle experiments in Sec. 4.3. Humans complete the task with almost no error. Tab. 3, evaluated on 100-example subset of each split, confirms Error (%) [] Model GPT-4o InternVL2 Molmo Qwen2VL Avg. of 4 VLMs CAPTUREreal CAPTUREsynthetic Original w/ Occlusion () Original w/ Occlusion () 13.34 26.17 25.90 18.96 21.09 14.75 (+1.41) 32.90 (+6.73) 32.49 (+6.59) 29.33 (+10.37) 27.37 (+6.28) 5.90 16.44 8.40 6.63 9.34 9.71 (+3.81) 17.57 (+1.13) 17.73 (+9.33) 11.74 (+5.11) 14.19 (+4.85) Table 2. Results across VLMs on all splits of CAPTURE, with average error for each column. Metric: sMAPE (lower is better). Model (Baseline) Human (VLMs) GPT-4o InternVL2 Molmo Qwen2VL Avg. of 4 VLMs Error (%) [] CAPTUREreal CAPTUREsynthetic 3. 14.09 29.74 33.68 29.80 26.83 0.92 8.40 24.98 17.24 9.39 15.00 Table 3. Human baseline vs VLMs on CAPTUREreal and CAPTUREsynthetic (occluded split). Metric: sMAPE (lower is better). that humans complete the task with ease despite occlusion, with an sMAPE of 3.79% on CAPTUREreal and 0.92% on CAPTUREsynthetic. On the same subset of examples, models performed 7 times worse on CAPTUREreal and 16 times worse on CAPTUREsynthetic than humans, underscoring the gap between VLM and humans in this task. Object detection-based baseline outperforms VLMs. We attempt the task with strong object detection-based model to highlight that standard counting approach will experience greater loss going from unoccluded to occluded environments, as it cannot capture any occluded objects, i.e. cannot reason. We choose COUNTGD [3], the top solution for unoccluded counting on FSC-147, on which it was trained. Because we draw our images from FSC-147s train and test sets, and COUNTGD trains on FSC-147, we only evaluate COUNTGD on the subset of our data sourced from the FSC-147 test set, consisting of 149 images. We find that COUNTGD deteriorates by 7.19% on occluded images, increasing from 3.15% sMAPE to 10.34% as observed in Fig. 5. As expected, COUNTGD outperforms all of the VLMs on the unoccluded split as it is trained for counting on FSC-147. COUNTGD also outperforms the VLMs on the occluded split, reinforcing that only counting the visible objects is hard-to-beat baseline. However, Figure 5. VLM vs. VLM + CountGD hybrid on questions from the CAPTUREreal (occluded split) that are not in COUNTGD training set. Metric: sMAPE (lower is better). the drop in performance with occlusion is greater than the average VLMs drop, highlighting disadvantage of nonreasoning solutions on CAPTURE: their error is necessarily tied directly to the number of occluded objects and they cannot address the task on their own, whereas VLM might be able to infer missing objects via reasoning. Hybrid VLM counting systems improve performance. Finding that COUNTGD is far better at counting visible objects than VLMs, we leverage the advantage that COUNTGD has by feeding its visible object count information to the VLMs as part of the prompt. As expected, Fig. 5 illustrates that there is considerable decrease in error when CountGD and the VLMs are combined into hybrid system. However, overall the hybrid system still performs worse than COUNTGD alone, indicating VLMs are still subpar even at counting just occluded objects (this is further reinforced by Appendix C.2). 4.2. Effect of Data Factors on VLM Performance Here, we use the CAPTUREsynthetic data (which can be controlled precisely and for which we have metadata) to examine which features correlate with model performance. We test the effect of the following variables on final performance: (1) Increasing the number of occluded objects; (2) Varying the pattern. We also investigate whether models are biased towards certain numbers, whether models can classify patterns, and to what degree models can predict the number of occluded objects only (rather than the total). 5 Figure 6. Effect of number of total objects in the image and number of occluded objects on sMAPE from CAPTUREsynthetic (occluded split). Metric: sMAPE (lower is better). Models perform worse when more dots are occluded. In Fig. 6 (right), we observe that error increases with respect to the number of occluded dots. However, Fig. 6 (left) also shows that performance is less affected by the total number of dots. This suggests that the task difficulty is more closely correlated with the difficulty of occlusion i.e. the difficulty of the world modeling task rather than the complexity of the pattern. However, there are models that do not follow this trend, e.g. GPT-4o, which has lower error on certain numbers. We investigate this further below. Models are more likely to predict certain numbers. To examine where models frequently err, we generated confusion matrix for every model based on CAPTUREsynthetic results (shown in Appendix C.4). We find that models often over-predict numbers associated with common counts in real life: GPT-4o tends to predict numbers like 8, 9, 10, and 12, which are all non-prime numbers (i.e. can be arranged into grid) and common groupings of objects, e.g. 12 is common grouping (dozens) and allows arrangements into 3x4 or 2x6 grids. InternVL and Qwen2VL over-predict 5 and 10, aligning with how humans conceptualize numbers. Indeed, Coupland [12] found that numbers 5, 10, 20, and other round numbers appear disproportionally more in online texts. Molmo shows no correlation with these factors, possibly because of its unique point and count ability. Figure 7. Effect of pattern type in CAPTUREsynthetic (occluded split) on sMAPE. Metric: sMAPE (lower is better). Model GPT-4o InternVL2 Molmo Qwen2VL Avg. of 4 VLMs Accuracy (%) [] Original w/ Occlusion () 84.00 68.52 80.70 88.35 80.39 78.52 (-5.48) 47.48 (-21.04) 65.22 (-15.48) 86.43 (-1.92) 69.41 (-10.98) Table 4. TUREsynthetic. Metric: accuracy (higher is better). VLM accuracy in identifying pattern in CAPPerformance depends on pattern type. The controllability of CAPTUREsynthetic allows us to measure the effect of pattern type on performance. In Fig. 7, we find that model performance differs across shapes with some regularity: objects arranged in circle generally have lower sMAPE than other shapes, across all models. Qwen2VL has an especially large decrease in error when given circular arrangements compared to rectangles or triangles. Models can identify patterns. To determine how much model errors can be attributed to lack of pattern recognition ability, we formulate separate task where models must recognize the pattern in the image on CAPTUREsynthetic. Here, we frame the task as multiple-choice task, asking the model to select from the pattern types available (rectangle, triangle, or circle). Table 4 illustrates that all perform substantially better than random at this task, with most models except InternVL2 achieving accuracy above 80% in the unoccluded setting. As expected, the patterns were easier to identify in unoccluded scenarios, with models suffering 10.95% accuracy drop on average in the occluded setting. Notably, GPT-4o and Qwen2VL have fairly small drop in performance, suggesting they are generally able to capture the pattern even in the presence of occlusion. 6 Figure 8. Example image and text inputs for experiments with auxiliary information experiments (Sec. 4.3). Blue eyes indicate objects for which the All Object Coordinate Oracle or Visible Object Coordinate Oracle extracts coordinates. Brighter part of image represents the area which Inpainting Pipeline fills in. Example prompts shown in italics. Blue eye overlays and faded parts of images are for demonstration purposes and are not passed with the image. Model Original w/ Occlusion Oracle Information Predicted Information + All Coordinates () + Visible () + Inpainting () GPT-4o InternVL2 Qwen2VL Avg. of 3 VLMs 13.34 26.17 18.96 19.49 14.75 32.90 29.33 25. 2.93 (-11.82) 17.48 (-15.42) 9.62 (-19.71) 9.20 (-5.55) 25.13 (-7.77) 17.70 (-11.63) 15.89 (+1.14) 31.12 (-1.78) 22.64 (-6.69) 10.01 (-15.65) 17.34 (-8.32) 23.22 (-2.44) Table 5. Effect of auxiliary information on occluded CAPTUREreal. = (Auxiliary Information) (w/ Occlusion). Metric: sMAPE. 4.3. Analysis with Auxiliary Information In Sec. 4.1, we see that models broadly struggle with amodal counting. Here, we seek to disentangle whether this problem results from failure to reason, the absence of world model, or both, by giving VLMs two different types of auxiliary information: oracle information and predicted information. Oracle information is ground truth and is directly pulled from CAPTUREs metadata, e.g., object locations. Predicted information generates new information from completely separate model and gives it to the VLM. This information is not ground truth and is sourced from an external model, such as an image inpainting model, rather than the VLM. By giving the model auxiliary information in the form of reasoning and spatial clues, we can establish how much of each models error results from an inability to handle occlusion rather than an inability to recognize and count visible objects. Oracle setup. We test two oracles for CAPTUREreals occluded split based on its constituent subtasks: counting the visible objects and inferring/counting occluded objects. Both oracles provide the VLM with text-based coordinates of objects in the image, simplifying the visual task by assuming the VLM effectively has perfect visual system that can recognize and localize objects in the image. The first oracle, the Visible Object Coordinate Oracle, gives VLM the coordinates of all unoccluded objects (encoded as text, as seen in Fig. 8) and instructs the model to estimate the number of occluded objects, then count the number of visible object coordinates, and add the two. In other words, the model is given oracle information about what objects are visible, thus also revealing key information about the pattern. The second oracle, the All Object Coordinate Oracle, instead gives the model the coordinates of all objects. Here, the model only needs to count the coordinates in the prompt, eliminating the need to reason on the visual input.2 An example of the oracle inputs can be seen in Fig. 8. Prediction setup. In this setting, we provide the VLM with an external world model representation predicted by another model. Specifically, we develop the Inpainting Pipeline to fill in the occluded region via diffusion-based inpainting model and pass the inpainted image to the VLMs. For the inpainting model, we choose FLUX.1-Fill [dev], 2Note that Molmo is excluded in these tests because it contains prompt limit that would truncate the list of coordinates. 7 whose backbone FLUX.1 [dev] [21] is top public model in the Text to Image Model Arena [7]. An example input to VLM can be seen on the far-right of Fig. 8. Providing visible or all object coordinates improves performance substantially. The results in Tab. 2 indicate that models struggle on CAPTURE, which requires identifying pattern and counting both visible and occluded objects. Moreover, models generally struggle with counting even in unoccluded settings. Both oracles simplify the counting task: All Object Coordinate Oracle reduces the task to simply counting coordinates with no reasoning involved, and Visible Object Coordinate Oracle similarly simplifies the task for visible objects, while still requiring inferring occluded objects. Additionally, under Visible Object Coordinate Oracle, recognizing the pattern shifts from visual reasoning task to an augmented math problem. Instead of visually reasoning about where objects are located, the VLM considers what patterns the coordinates could make. Translating this task into text problem results in an average increase of 15% with all objects coordinate oracle; the errors LLMs make here are due to an inability to count in the text prompt, as opposed to weaknesses in handling occlusion (since all object coordinates are given), and the strongest model, GPT-4o, achieves minimal error here. We also obtain an average increase of 8% with the visible objects coordinate oracle (shown in Tab. 5), possibly because it allows the more powerful LLM backbone (which is far larger than the vision model in all models tested) to take over and complete the counting portion of the task. Taken together, these results suggest that there is much room for improvement in visual world modeling beyond text-based reasoning of VLMs. Providing diffusion-based inpainting improves performance marginally. Similar to the object coordinate oracles, the Inpainting Pipeline (rightmost columns in Fig. 8 and Tab. 5) eliminates the need for world modeling and provides VLMs with an approximation of the image behind the occluder. With the inpainted images, VLM error decreases by almost 2% for InternVL2 and 7% for Qwen2VL compared to the original occluded images. GPT-4os error increases on inpainted images by small margin; we hypothesize that this may be because GPT-4o has one of the better world models (based on its superior performance), and thus does not improve further with the inpainted images. Moreover, every VLM still falls short of unoccluded image performance, indicating that the diffusion model is not perfect world model. Qualitatively, we find that the inpainting model does sometimes fail to output the correct pattern behind the occluder. 5. Related Work Spatial reasoning in visual question answering. Past work measures the spatial reasoning capabilities of VLMs in the form of visual question answering (VQA) [4, 16] benchmarks. SpartQA [26] asks VLMs to identify the spatial relation (e.g., above, behind, left of) between objects in synthetically created 2D images from NLVR [39]. More recent benchmarks test similar spatial relation understanding with real images [2, 24, 36]. While this past work asks models to provide text description for relation between two fully-observed objects, CAPTURE measures the world modeling from partially observed scene, thus requiring the handling of occlusion, pattern recognition, and counting. Amodal completion. Occlusions are common in natural scenes, and vision solutions for amodal completion have made significant progress infilling the occluded portions of objects [6, 38, 45]. The amodal completion task has evolved from simply completing shape to filling in appearance (e.g., texture, color, etc.) to finally dealing with fine-grained order perception (where the model is given multiple stacked layers of occluded objects) [5]. Specifically in Qiu and Di [34], VLMs classify the hidden objects and extract fine details from occluded items. CAPTURE, however, presents unique category of patterned amodal counting which requires inferring completely occluded objects based on pattern rather than inferring occluded object wholes based on object parts. In other words, previous work by has only attempted tasks that require amodal completion for one object at time [31, 38, 45], whereas CAPTURE involves inferring information about multiple objects. Multi-object amodal completion is crucial because in visual environments with lots of objects, it is likely that entire groups of objects can be occluded. Moreover, the output space of CAPTURE is language (rather than filling pixels). Counting with vision-and-language models. Within the task of counting, the most similar application to CAPTURE is dense counting, where the objects to be counted occlude each other. There are many practical applications of such task, like counting microscopic cells on crowded slide [8], determining crop yields from densely-packed fields [42], or crowd counting [14, 43, 46]. Liang et al. [23] attempted to improve crowd counting with an augmented CLIP [35], i.e. also using VLMs for counting. Additionally, Jenkins et al. [18] created benchmark that does involve amodal counting, presenting an occluded 3D counting task where models must count objects on retail shelves. However, our work differs in many ways, as Jenkins et al. [18] only count retail shelves and processes LiDAR scans as an additional input. More broadly, dense counting focuses on overlapping objects rather than on counting objects that are arranged into 8 patterns, which is the focus of CAPTURE. [7] Artificial Analysis. Text to image model arena, 2025. Ac6. Conclusion We introduced CAPTURE, novel benchmark for amodal counting that measures spatial reasoning capabilities under occlusion. CAPTURE is designed to assess VLMs ability to form robust world model and use that model for visual reasoning skills under occlusion. By testing counting, we cast the problem as measurable task with an objective correct answer that also has real-world utility as VLMs become more broadly adopted. Our results suggest that VLMs struggle to combine reasoning, counting, and world modeling with low performance on occluded and unoccluded images. Our analysis indicates that models improve substantially with oracle information about visible objects (simplifying the reasoning/counting tasks) and predicted information about the occluded objects (also simplifying world modeling), pointing to possible directions of model improvement."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by DARPA ECOLE Program No. HR00112390060, NSF-CAREER Award 1846185, NSF-AI Engage Institute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, Microsoft Accelerate Foundation Models Research (AFMR) grant program, and Bloomberg Data Science PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "[1] AI@Meta. Llama 3.1 model card. Github Model Card, 2024. 4 [2] Haider Al-Tahan, Quentin Garrido, Randall Balestriero, Diane Bouchacourt, Caner Hazirbas, and Mark Ibrahim. Unibench: Visual reasoning requires rethinking visionlanguage beyond scaling. arXiv preprint arXiv:2408.04810, 2024. 8 [3] Niki Amini-Naieni, Tengda Han, and Andrew Zisserman. Countgd: Multi-modal open-world counting. arXiv preprint arXiv:2407.04619, 2024. 2, 4, 5 [4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 8 [5] Jiayang Ao, Qiuhong Ke, and Krista Ehinger. Image amodal completion: survey. Computer Vision and Image Understanding, 229:103661, 2023. 1, 8 [6] Jiayang Ao, Yanbei Jiang, Qiuhong Ke, and Krista Ehinger. Open-world amodal appearance completion. arXiv preprint arXiv:2411.13019, 2024. 8 9 cessed: April 10, 2025. 8 [8] Soumen Bera. Partially occluded object detection and counting. In Proceedings of the 2015 Third International Conference on Computer, Communication, Control and Information Technology (C3IT), pages 16. IEEE, 2015. 8 [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Internvl: Scaling up vision foundation models and Dai. aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. [10] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 3 [11] Davide Chicco, Matthijs Warrens, and Giuseppe Jurman. The coefficient of determination r-squared is more informative than smape, mae, mape, mse and rmse in regression analysis evaluation. Peerj computer science, 7:e623, 2021. 11 [12] Nikolas Coupland. How frequent are numbers? Language & Communication, 31(1):2737, 2011. 6 [13] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 3 [14] Zheyi Fan, Zihao Song, Di Wu, and Yixuan Zhu. Multibranch segmentation-guided attention network for crowd counting. Journal of Visual Communication and Image Representation, 97:103964, 2023. [15] Benito Flores. pragmatic view of accuracy measurement in forecasting. Omega, 14(2):9398, 1986. 11 [16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 8 [17] David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018. 1 [18] Porter Jenkins, Kyle Armstrong, Stephen Nelson, Siddhesh Gotad, Stockton Jenkins, Wade Wilkey, and Tanner Watts. Countnet3d: 3d computer vision approach to infer counts of occluded objects. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 30083017, 2023. 8 [19] Gaetano Kanizsa, Paolo Legrenzi, and Paolo Bozzi. Organization in vision : essays on gestalt perception. Praeger, 1979. [20] Kaleb Kassaw, Francesco Luzi, Leslie Collins, and Jordan Malof. Are deep learning models robust to partial object occlusion in visual recognition tasks? arXiv preprint arXiv:2409.10775, 2024. 2 [21] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 8 [36] Navid Rajabi and Jana Kosecka. Gsr-bench: benchmark for grounded spatial reasoning evaluation via multimodal llms. arXiv preprint arXiv:2406.13246, 2024. 8 [37] Viresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh In Proceedings of Hoai. Learning to count everything. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 33943403, 2021. 3 [38] Kaziwa Saleh, Sandor Szenasi, and Zoltan Vamossy. Mask guided gated convolution for amodal content completion. In 2024 IEEE 22nd Jubilee International Symposium on Intelligent Systems and Informatics (SISY), pages 000321000326. IEEE, 2024. [39] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217 223, Vancouver, Canada, 2017. Association for Computational Linguistics. 8 [40] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 [41] Wei-Yao Wang, Zhao Wang, Helen Suzuki, and Yoshiyuki Kobayashi. Seeing is understanding: Unlocking causal attention into modality-mutual attention for multimodal llms. arXiv preprint arXiv:2503.02597, 2025. 2 [42] Yiding Wang, Yuxin Qin, and Jiali Cui. Occlusion robust wheat ear counting algorithm based on deep learning. Frontiers in Plant Science, 12:645899, 2021. 8 [43] Yongjie Wang, Feng Wang, and Dongyang Huang. Dualbranch counting method for dense crowd based on selfattention mechanism. Expert Systems with Applications, 236:121272, 2024. 8 [44] Karen Wynn. Childrens understanding of counting. Cognition, 36(2):155193, 1990. 1, 2 [45] Katherine Xu, Lingzhi Zhang, and Jianbo Shi. Amodal completion via progressive mixed context diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90999109, 2024. 8 [46] Lifang Zhou, Songlin Rao, Weisheng Li, Bo Hu, and Bo Sun. Multi-branch progressive embedding network for crowd counting. Image and Vision Computing, page 105140, 2024. 8 [22] Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating vision-language models on natural adversarial samples. arXiv preprint arXiv:2410.14669, 2024. 2 [23] Dingkang Liang, Jiahao Xie, Zhikang Zou, Xiaoqing Ye, Wei Xu, and Xiang Bai. Crowdclip: Unsupervised crowd In Proceedings of counting via vision-language model. the IEEE/CVF conference on computer vision and pattern recognition, pages 28932903, 2023. 8 [24] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 2023. [25] Baraka Jacob Maiseli. Optimum design of chamfer masks using symmetric mean absolute percentage error. EURASIP Journal on Image and Video Processing, 2019(1):74, 2019. 11 [26] Roshanak Mirzaee and Hossein Rajaby. Spartqa: textual question answering benchmark for spatial reasoning. In The 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL2021), 2021. 8 [27] Ingrid Olson, Christopher Gatenby, Hoi-Chung Leung, Pawel Skudlarski, and John Gore. Neuronal representation of occluded objects in the human brain. Neuropsychologia, 42(1):95104, 2004. 1, 2 [28] OpenAI. Hello gpt-4o, 2024. 3 [29] OpenCompass Team. Openvlm leaderboard. https:// huggingface.co/spaces/opencompass/open_ vlm_leaderboard, 2024. Accessed: 2024-11-13. 3 [30] Yumiko OTSUKA, So KANAZAWA, and Masami YAMAGUCHI. Development of modal and amodal completion in infants. Perception (London. Print), 35(9):12511264, 2006. 1, 2 [31] Ege Ozguroglu, Ruoshi Liu, Dıdac Surıs, Dian Chen, Achal Dave, Pavel Tokmakov, and Carl Vondrick. pix2gestalt: In 2024 Amodal segmentation by synthesizing wholes. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 39313940. IEEE Computer Society, 2024. 8 [32] Max Peeperkorn, Tom Kouwenhoven, Dan Brown, and Anna Jordanous. Is temperature the creativity parameter of large language models? arXiv preprint arXiv:2405.00492, 2024. 11 [33] Muhammad Fetrat Qharabagh, Mohammadreza Ghofrani, and Kimon Fountoulakis. Lvlm-count: Enhancing the counting ability of large vision-language models. arXiv preprint arXiv:2412.00686, 2024. [34] Wenmo Qiu and Xinhan Di. Occ-mllm: Empowering multimodal large language model for the understanding of occluded objects. arXiv preprint arXiv:2410.01261, 2024. 8 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021."
        },
        {
            "title": "Appendix",
            "content": "A. Implementation Details A.1. Metric Details We use symmetric mean percent error (sMAPE) as the primary metric for our benchmarks due to its resistance to bias for under/over predictions and small/large ground truths [25]. The standard metric to use for counting benchmark is mean average error (MAE). MAE is popular, but heavily penalizes predictions that deviate by small margin from big ground truths, highlighting the necessity for metric that gives equal weighting to all questions. Mean average percent error (MAPE) initially seems appealing but is disproportionally inflated for small ground truths and is biased towards overpredictions. Mean square error (MSE) and root mean square error (RMSE) are also commonly used but are very sensitive to outliers because they square the error. Intuitively, performing well on almost all questions and poorly on small subset should score better than consistently being wrong. Among commonly-used metrics, sMAPE is the only metric that evaluates performance in relation to the distribution of ground truth elements [11]. There are two common definitions [15] for sMAPE, but we use the one that scales to 100%. sMAPE is given by: sMAPE = 100 1 (cid:88) i=1 yi ˆyi yi + ˆyi (2) where yi represents the actual values, ˆyi represents the predicted values, and is the number of observations. sMAPE is capped at 100%, providing finite scoring range. This feature is ideal for challenging tasks like ours, as it penalizes model responses that fail to produce an answer. A.2. Output Tokens To maximize the models chance at success, we allocate high number of output tokens such that it can generate rationale and output. This varies per model. We give 4000 tokens to InternVL2, 2000 tokens to Molmo, and 8192 tokens to Qwen2VL. For GPT-4o, we use the default of 4096 tokens. B. CAPTURE Dataset Creation Details While FSC-147 is strong starting point, it cannot immediately be adapted to our task. In order to make the task of amodal counting solvable, our dataset requires images with patterns in them, such that person (or model) can infer how the pattern would continue and thus predict the total number accurately. For questions to be solvable, the datasets images must be filtered down to represent patterns model or person could recognize. Our filtering process follows two stages. First, we prompt GPT-4o to determine whether the objects were arranged in pattern. If the model responded with no, the images were immediately discarded. If the model output was yes, the log probability of the token is stored. Empirically, we found that higher log probability values (i.e. higher confidence scores) corresponded to more welldefined patterns in the image. Thus, we use the log probabilities for filtering. Specifically, let Pyes be the log probability of the yes token and denote the threshold for determining how welldefined pattern is.3 To filter the images based on pattern rigidity, we apply the following condition: ePyes . This inequality yields 991 images from the original dataset (16.12%). Next, we manually filter each of the selected images to ensure that the they indeed contain patterns and feature countable number of objects, excluding 34 images. Afterward, we manually place fair occluding box in each image, i.e. box that leaves sufficient portions of the pattern visible, such that the pattern can still be inferred from the unoccluded portions of the image. Occluding boxes were also chosen with varying positions and sizes in the image. C. Additional Analysis C.1. Temperature Backoff To improve VLM performance on CAPTURE, we address trend that we established during early testing. Most of the time, the VLM fails by reaching an incorrect answer. Sometimes, however, our benchmark can cause VLMs to produce long and irrelevant response that strays from the original prompt. When an irrelevant response is given, we treat the model as skipping the question and assign it the worst possible sMAPE score (100%). To reduce the number of skipped questions, we introduce temperature backoff, which iteratively decreases the sampling temperature. Because the answer extractor can immediately identify when an output is incoherent, we can regenerate the response with lower temperature to get the model to answer the task properly. Lower temperature increases coherence in VLMs [32], enhancing their chances of maintaining relevance to the prompt. Therefore, temperature backoff gives VLMs better chance at achieving higher scores. Each time the answer extractor returns an empty answer, we reduce the temperature by 0.1 (starting from 1.0) until reaching temperature of 0.0, at which point the example is skipped. Models perform better with temperature backoff. We introduced temperature backoff to reduce model incoherence, and it performed fairly well. As shown in Tab. 6 3We set = 0.9999 based on manual evaluation, finding it resulted in fewer false positives. Model Real Synthetic sMAPE () Occluded Unoccluded Occluded Unoccluded Original w/ backoff () Original w/ backoff () Original w/ backoff () Original w/ backoff () GPT-4o InternVL2 Molmo Qwen2VL Avg. of 4 VLMs 14.75 32.90 32.49 29. 27.37 14.39 (0.36) 32.37 (0.53) 28.17 (4.32) 28.47 (0.86) 25.85 (1.52) 13.34 26.17 25.90 18.96 21.09 12.57 (0.77) 27.09 (+0.92) 21.23 (4.67) 19.40 (+0.44) 20.07 (1.02) 9.71 17.57 17.73 11.74 14.19 9.23 (0.48) 16.24 (1.33) 15.85 (1.88) 11.51 (0.23) 13.21 (0.98) 5.90 16.44 8.40 6. 9.34 5.93 (+0.03) 15.59 (0.85) 2.88 (5.52) 6.66 (+0.03) 7.76 (1.58) Table 6. Comparison of models on CAPTURE across four scenarios (CAPTUREreal vs. CAPTUREsynthetic, Occluded vs. Unoccluded). Original indicates no backoff; w/ backoff indicates applying backoff, with = (w/ backoff ) (Original). Negative values indicate an improvement."
        },
        {
            "title": "Model",
            "content": "GPT-4o InternVL2 Molmo Qwen2VL Avg. of 4 VLMs Error (%) [] 26.13 75.82 96.79 32.89 57.91 Table 7. VLM sMAPE for only counting the occluded objects in CAPTUREreal. Metric: sMAPE (lower is better). (bottom), this method slightly improves performance across each model, resulting in an average error reduction of 5.78% in CAPTUREreal and 5.45% in CAPTUREsynthetic. Temperature backoff essentially allows the model to reattempt the question if it fails to respond to the prompt. Similar to previous results, positive results from reattempts highlight VLMs weak reasoning abilities. C.2. Only Occluded Object Counting Models struggle at counting occluded objects. Given that models seem to be able to identify patterns, we separately test whether models can count only the occluded objects in an image, which would be the next piece of reasoning needed to achieve accurate an accurate total. Here, as Tab. 7 demonstrates, the models perform especially poorly in this task, with high error rates across all models. Therefore, we can conclude that occlusion and counting are uniquely difficult for the VLMs, and that the drop in performance between unoccluded and occluded settings in Tab. 2 is more likely due to poor ability to count occluded objects rather than to determine the pattern. Qwen2VL for its confidence in the answer. For the second method, we generate 20 responses for every question in our VQA and calculate the confidence as the percentage of times the most common answer was generated. These results can be seen in Fig. 10 and Fig. 9 respectively. In both reliability curves, there is slight trend that the models confidence is negatively correlated with the error, which is the desired outcome. In CAPTUREreal, however, the correlation is much stronger. While the models are somewhat calibrated (with generally lower confidence on higher-error examples, there are still outliers in prompted confidence for CAPTUREreal occluded and sampled confidence for CAPTUREsynthetic occluded. This indicates that not only do the models perform worse under occlusion, they can also be overconfident. Figure 9. Reliability curve of prompted confidence vs. sMAPE. C.3. Uncertainty/Confidence C.4. Confusion Matrix: Predicted vs. Ground Models are overconfident in occluded settings. We test the uncertainty with two different methods of obtaining confidence on Qwen2VL. In the first method, we prompt"
        },
        {
            "title": "Truth Counts",
            "content": "We generate confusion matrices for every model on CAPTUREreal occluded images to investigate whether models 12 Prompt TUREsynthetic occluded split. for GPT-4o and Molmo on CAPYour task is to count objects in the image. Assume the pattern of [dot shape]s continues behind the black box. First, state what the pattern is, then give your final count. Prompt for InternVL2 and Qwen2VL on CAPTUREsynthetic occluded split. Count the exact number of [dot shape]s in the image. Assume the pattern of [dot shape]s continues behind any black box. Provide the total number of [dot shape]s as if the black box were not there. Only count [dot shape]s that are visible within the frame (or would be visible without the occluding box). If [dot shape]s are partially in the frame (i.e. if any part of [dot shape]s are visible), count it. If the [dot shape]s would be partially in the frame without the occluding box, count it. Prompt for GPT-4o on CAPTUREreal unoccluded split. Count the exact number of [object] in the image. Assume the pattern of [object] continues behind any black box. Provide the total number of [object] as if the black box were not there. Prompt for InternVL2 on CAPTUREreal unoccluded split. Your task is to count objects in the image. First, state what the pattern is, then give your final count. Prompt for Molmo on CAPTUREreal unoccluded split. Count the exact number of [object] in the image. Only count [object] that are visible within the frame. If [object] are partially in the frame (i.e. if any part of [object] are visible), count it. 13 Figure 10. Reliability curve of sampled confidence vs. sMAPE. are biased to predict certain numbers. The y-axis represents the ground truth values and the x-axis represents the models answers. Fig. 11 demonstrates that GPT-4o, InternVL2, and Qwen2VL are more likely to predict numbers that occur more frequently in text. D. VLM Prompts For each setting, we use 100-example validation set to select the best prompt, which we report below. Prompt for GPT-4o, InternVL2, and Qwen2VL on CAPTUREreal occluded split. Count the exact number of [object] in the image. Assume the pattern of [object] continues behind any black box. Provide the total number of [object] as if the black box were not there. Only count [object] that are visible within the frame (or would be visible without the occluding box). If [object] are partially in the frame (i.e. if any part of [object] are visible), count it. If the [object] would be partially in the frame without the occluding box, count it. Molmo: Your task is to count objects in the image. Assume the pattern of [object] continues behind the black box. First, state what the pattern is, then give your final count. Prompt for Molmo on CAPTUREreal occluded split. Your task is to count objects in the image. Assume the pattern of [object] continues behind the black box. First, state what the pattern is, then give your final count. Figure 11. Confusion matrix: predicted vs. ground truth counts for CAPTUREreals occluded split. Prompt for Qwen2VL on CAPTUREreal unoccluded split. Prompt for GPT-4o on CAPTUREsynthetic unoccluded split. Count the exact number of [object] in the image. Assume the pattern of [object] continues behind any black box. Provide the total number of [object] as if the black box were not there. Only count [object] that are visible within the frame (or would be visible without the occluding box). If [object] are partially in the frame (i.e. if any part of [object] are visible), count it. If the [object] would be partially in the frame without the occluding box, count it. Your task is to count objects in the image. First, state what the pattern is, then give your final count. Prompt for InternVL2 on CAPTUREsynthetic unoccluded split. Count the exact number of [dot shape]s in the image. Only count [dot shape]s that are visible within the frame. If [dot shape]s are partially in the frame (i.e. if any part of [dot shape]s are visible), count it. Prompt for Molmo on CAPTUREsynthetic unoccluded split. Count the exact number of [dot shape]s in the image. Only count [dot shape]s that are visible within the frame. Prompt for Qwen2VL on CAPTUREsynthetic unoccluded split. Count the exact number of [dot shape]s in the image. Assume the pattern of [dot shape]s continues behind any black box. Provide the total number of [dot shape]s as if the black box were not there. Only count [dot shape]s that are visible within the frame (or would be visible without the occluding box). If [dot if any part of shape]s are partially in the frame (i.e. [dot shape]s are visible), count it. If the [dot shape]s would be partially in the frame without the occluding box, count it."
        }
    ],
    "affiliations": [
        "cs.unc.edu"
    ]
}