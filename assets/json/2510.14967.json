{
    "paper_title": "Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents",
    "authors": [
        "Guoqing Wang",
        "Sunhao Dai",
        "Guangze Ye",
        "Zeyu Gan",
        "Wei Yao",
        "Yong Deng",
        "Xiaofeng Wu",
        "Zhenzhe Ying"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency."
        },
        {
            "title": "Start",
            "content": "INFORMATION GAIN-BASED POLICY OPTIMIZATION: SIMPLE AND EFFECTIVE APPROACH FOR MULTITURN LLM AGENTS Guoqing Wang1*, Sunhao Dai2*, Guangze Ye3*, Zeyu Gan2, Wei Yao2, Yong Deng1, Xiaofeng Wu1, and Zhenzhe Ying1 2Renmin University of China 3Individual Author 1Ant Group 5 2 0 2 6 ] . [ 1 7 6 9 4 1 . 0 1 5 2 : r GitHub: https://github.com/GuoqingWang1/IGPO"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gainbased Policy Optimization (IGPO), simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policys probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the models own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both indomain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language model (LLM)based agents are increasingly endowed with the ability to interact with external environments through tool use (Zhang et al., 2025a; Huang et al., 2025; Li et al., 2025c), capability often regarded as critical step toward building general-purpose autonomous intelligent systems (Gutierrez et al., 2023; Qu et al., 2025). For example, web search (Zhang et al., 2025b; Qi et al., 2024), one of the most fundamental tools, enables agents to access up-to-date largescale knowledge that substantially improves their capacity to solve complex, knowledge-intensive tasks (Ning et al., 2025). Through iterative interaction with the external environment, agents can gradually acquire missing information and refine their reasoning toward solving the target query. To equip general-purpose LLMs with such agentic capabilities, early efforts primarily relied on prompt-based workflows (Li et al., 2025b; Wang et al., 2024a; Zheng et al., 2024), which allowed tool use without additional training but often suffered from poor generalization. More recent studies have explored supervised fine-tuning (SFT) (Wang et al., 2024b) and reinforcement learning (RL) (Jin et al., 2025; Song et al., 2025a; Zheng et al., 2025b) to explicitly incentivize tool use, achieving markedly better performance. In particular, Group Relative Policy Optimization (GRPO) (Shao et al., 2024)style methods have emerged as the dominant approach for training agentic LLMs. In this paradigm, group of rollouts is generated for each query under the current *Equal contribution. 1 policy, and outcome-based rewards, typically defined by the correctness of the final answer against the ground truth, are used to construct group-relative advantages that drive policy optimization. Despite their simplicity and effectiveness on relatively easy tasks, outcome rewards suffer from an inherent limitation: they are sparse (Zhang et al., 2025c), since supervision is provided only at the final answer. This sparsity becomes particularly detrimental in multi-turn agentic settings, where long trajectories exacerbate the problem in two ways. First, sparse rewards frequently lead to advantage collapse: when sampled rollouts yield the same answer (e.g., all wrong or all right), all rollouts in the group receive identical outcome rewards, yielding zero group-relative advantages. As shown in Figure 1, substantial portion of training iterations suffer from this issue, especially for smaller models, which struggle more with complex queries. Second, outcome-only supervision fails to provide fine-grained credit assignment. In multi-turn scenarios, later turns are tightly dependent on earlier ones: reasoning or tool call of the current turn may be correct but rendered useless by prior mistakes, or conversely, early successes may be negated by subsequent errors. Such dependencies are easily obscured under outcome-only rewards, particularly in multi-hop tasks that require long-horizon reasoning. Figure 1: Proportion of zero-advantage groups during trainingIGPO vs. GRPO on Qwen2.5-7B/3B-Instruct. Several recent approaches have attempted to mitigate these issues by introducing process-level rewards. One line of work leverages external oracle knowledge or reward models to judge intermediate steps (Wang et al., 2025; Feng et al., 2025), but this strategy is costly to obtain and risks introducing additional bias. Another line relies on Monte Carlo simulations to estimate step values (Wang et al., 2023; Zuo et al., 2025; Zhang et al., 2025c), yet these methods suffer from high variance unless large number of samples are collected. Overall, both directions face challenges in scalability and fail to provide simple and stable supervision, underscoring the need for an intrinsic and reliable process-level reward design. To address these challenges, we propose Information-Gain-based Policy Optimization (IGPO), simple but effective reinforcement learning framework that provides stable and intrinsic supervision for multi-turn agent training. The key intuition is to model each agentenvironment interaction turn as an incremental process of acquiring information about the ground truth. Specifically, at every turn, IGPO computes the policys probability of producing the correct answer and defines the turn-level reward as the marginal increase in this probability compared to the previous state. This information gain reward offers ground-truth-aware feedback at every turn, in contrast to outcome rewards that only supervise the final answer. While turn-level rewards ensure dense and stable supervision, the outcome reward remains essential to anchor training to the final task objective. To combine these strengths, IGPO also integrates the outcome reward with the sequence of turn-level rewards, forming dense reward trajectory for each rollout. To further stabilize training, we normalize rewards within groups and propagate them with discounted accumulation, enabling turn-level advantage estimation that captures long-horizon dependencies. Finally, IGPO optimizes the policy with GRPO-style surrogate objective, replacing rollout-level advantages with our turn-level ones. To evaluate the effectiveness of IGPO, we conduct extensive experiments on both in-domain and out-of-domain benchmarks with search-based agents. Results show that IGPO consistently outperforms strong baselines, delivering substantial gains in both answer accuracy and sample efficiency. Our main contributions can be summarized as follows: (1) We analyze the phenomenon of advantage collapse in outcome-rewardbased optimization, and reveal the inefficiency of existing processlevel rewards due to reliance on external knowledge or high-variance estimation. (2) We propose IGPO, simple yet effective policy optimization framework that leverages turn-level information gain to provide dense, ground-truth-aware supervision while preserving outcome-level alignment. (3) Comprehensive experiments demonstrate that IGPO outperforms strong baselines across multiple benchmarks and significantly improves sample efficiency, especially for smaller models."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "In this section, we present the standard multi-turn agentic RL pipeline, illustrated with search agent as representative example."
        },
        {
            "title": "2.1 TASK FORMULATION",
            "content": "Let = {(q, a)} denote dataset of questionanswer pairs, and let represent an external tool (e.g., web search engine). The goal of the agent is to solve question by generating rollout = (τ1, τ2, . . . , τT ) through iterative interaction with the environment via tool E, where is the total number of interaction turns. The last turn τT is the answer turn that outputs rationale-thenfinal answer sequence, while all previous turns involve reasoning and tool interaction. Specifically, for < , each turn τt is defined as triple consisting of [think], [tool call], and [tool response]. The [think] step compels the agent to reason explicitly before acting, and each reasoning process is wrapped in <think></think> tag following the DeepSeek-R1 setting (Guo et al., 2025). The [tool call] step invokes the external tool by producing structured request, typically JSON-formatted and wrapped in dedicated tag (e.g., <search>search query</search> for web search). The [tool response] step then returns structured outputs from E, such as webpage snippets with titles, URLs, and text when using web search engine tool, enclosed in <tool response>retrieved documents</tool response> tags. the agent generates its answer within the <answer></answer> tag, and this content is extracted as the trajectorys final prediction ˆa, which is expected to correctly address the input query q. This agent-environment interaction is illustrated at the bottom of Figure 2. In the final turn, after [think] step, 2.2 AGENTIC REINFORCEMENT LEARNING PIPELINE Policy Optimization. Agentic RL typically adopts policy-gradient methods to optimize the agent policy πθ. common approach is Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which removes the need for an explicit critic by normalizing returns within each sampled group of rollouts. Formally, given an actor model πθ, group of rollouts {oi}G i=1 is sampled from old policy πθold( q) for each input (q, a) D. The policy is then optimized by maximizing the clipped surrogate objective with KL regularization: JGRPO(θ) = E(q,a)D, {oi}πθold (q) (cid:34) 1 (cid:88) i=1 1 oi oi (cid:88) t= (cid:32) min πθ(oi,t q, oi,<t) πθold(oi,t q, oi,<t) (cid:98)Ai, clip (cid:18) πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) , 1 ϵ, 1 + ϵ (cid:19) (cid:33) (cid:35) β DKL(πθ πref ) , (cid:98)Ai (1) std(r1,r2, ,rG) where (cid:98)Ai = rimean(r1,r2, ,rG) is the normalized group-relative advantage for the i-th rollout and ri is the outcome reward of the i-th rollout. ϵ is the clipping ratio, and β controls the KL penalty that regularizes the updated policy toward the reference model πref . During optimization, gradients are applied only to decision tokens (reasoning, tool calls, answers), while tool responses from the external environment are masked out. Reward. During training, the agent receives scalar reward for each rollout o, which provides the optimization signal. Prior work usually adopts an outcome-based answer reward combined with format penalty: (cid:40) F1(ˆa, a) = 2 ˆaa ˆa+a [0, 1], if the output is in valid format, = λfmt, otherwise, (2) where ˆa is the predicted final answer for each rollout, is the ground-truth answer, and F1(ˆa, a) [0, 1] denotes the word-level F1 score between the two. If the output violates the required schema (e.g., missing tags or malformed JSON), negative constant λfmt < 0 is assigned as penalty. Thus, the outcome reward provides correctness signal aligned with evaluation metrics, while the format penalty enforces the structural validity of outputs. 3 Figure 2: The training pipeline of IGPO. (Upper) Turn-level information gain rewards are computed by measuring changes in ground-truth probability and combined with the outcome reward to derive discounted advantages. (Lower) Each rollout contains at most 1 interaction turns, where each turn includes reasoning step, tool call, and the returned tool response, followed by final answer turn. During optimization, the loss on tool response is masked out. 3 INFORMATION GAIN-BASED POLICY OPTIMIZATION In this section, we first illustrate our motivation and then provide detailed introduction to our proposed information gain-based policy optimization, whose overall framework is shown in Figure 2. 3.1 MOTIVATION While outcome-based reinforcement learning has been effective in single-turn tasks, directly extending it to multi-turn agentic settings such as search agents faces critical limitations. In the standard GRPO framework (Eq. 1), each rollout oi receives scalar reward ri computed from the final answer ˆai. For complex queries, however, it is often the case that all rollouts fail to produce the correct answer, resulting in uniformly zero rewards; conversely, for simple queries, all rollouts may produce the same correct answer, leading to the same issue. In these cases, the normalized group-relative advantages { (cid:98)Ai} collapse to near zero, and the entire sample provides almost no learning signal. We refer to this phenomenon as advantage collapse. Moreover, such outcome-only supervision lacks fine-grained credit assignment across turns. In multi-turn scenarios, later decisions critically depend on earlier ones: tool call may be effective yet rendered useless by prior retrieval errors, or early reasoning may be correct but overshadowed by subsequent mistakes. Such dependencies are obscured under single outcome rewards, making it difficult for the policy to distinguish productive reasoning from uninformative or misleading turns. To mitigate this issue, we introduce Information-Gain-based Policy Optimization (IGPO). The key idea is to exploit the multi-turn structure of agentic rollouts and treat each turn as an opportunity to acquire additional evidence toward the ground truth. At every turn, IGPO measures the increase in the policys confidence of generating the correct answer, which we defined as the information gain of this turn and uses this as the turn-level reward. By rewarding turn-level information gain, IGPO supplies denser and more fine-grained supervision, especially at early training stages. We further present theoretical analysis in Appendix A, which intuitively explains why IGPO effectively addresses the limitations of sparse outcome rewards in multi-turn scenarios. Since the information gain is defined with respect to the ground-truth answer and computed under teacher forcing, it always produces valid signal, ensuring that every sample contributes to learning even when no rollout produces fully correct final answer. 4 3.2 INFORMATION GAIN-BASED TURN-LEVEL REWARD Turn-level Reward. We view multi-turn agentenvironment interaction as process of incrementally acquiring information about the ground truth. To capture this intuition, we propose an intrinsic information gain-based reward. At each turn, we evaluate the policys probability of generating the ground-truth answer and define the reward as the difference between consecutive states. We call this the information gain reward, as it measures the marginal increase in posterior probability mass assigned to the ground truth induced by the current turn. Formally, let = (a1, . . . , aL) denote the ground-truth answer tokens. For the t-th turn in the i-th rollout, the probability of under the current policy πθ is computed as πθ(a q, oi,t) = exp"
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) j=1 log πθ(aj q, oi,t, a<j) , (3) where oi,t denotes the prefix of rollout oi up to turn t. Then the immediate reward * for turn is ri,t = IG(a q, oi,t) = πθ(a q, oi,t) πθ(a q, oi,t1), 1 < T. (4) In practice, the ground-truth answer is wrapped in the same schema as predicted answer to ensure consistency with rollout formatting, e.g., <think>Now theres enough information to answer</think><answer>Ground Truth a</answer>. This turn-level reward has two desirable properties: (1) Ground-truth awareness: the reward increases when the action raises the policys confidence in the correct answer, and decreases otherwise; (2) Dense supervision: the reward is defined for every sample, even when no rollout yields correct answer, thereby alleviating reward sparsity and avoiding advantage collapse. Integrating Outcome and Turn-level Rewards. For each rollout oi = (τi,1, . . . , τi,T ) where the last turn τi,T is the answer turn producing ˆai, we can construct length-T reward vector ri = (ri,1, ri,2, . . . , ri,T ). For < , the turn reward is the information gain ri,t = IG(a q, oi,t) defined in Section 3.2. For the answer turn = , the reward ri,T follows the outcome-based formulation in Eq. 2. This yields dense per-turn supervision signal that combines intrinsic information gains for intermediate turns with final extrinsic correctness signal at the answer turn. 3.3 POLICY OPTIMIZATION WITH TURN-LEVEL ADVANTAGE Turn-level Advantage Estimation. Given rollout oi = (τi,1, . . . , τi,T ), each turn τi,t is associated with reward ri,t as defined in Section 3.2. To make rewards comparable across turns and trajectories, we first aggregate all rewards in the group: = { ri,t : = 1, . . . , G, = 1, . . . , }, and apply group-wise z-normalization: Ai,t = ri,t mean(R) std(R) . (5) (6) While Ai,t captures the relative quality of each turn, it only reflects immediate effects and ignores the impact of current decisions on future turns. To incorporate such long-horizon dependencies, we compute discounted cumulative advantage to propagate outcome signals backward to earlier turns: (cid:101)Ai,t = (cid:88) k=t γ ktAi,k, (7) where γ (0, 1] is the discount factor. During optimization, (cid:101)Ai,t is assigned to all decision tokens produced in turn t, while raw tool responses from the external environment are masked out. This yields dense and future-aware supervision signal for policy learning. *Due to its log-prob origin, we apply stop-gradient to the information gainbased reward. 5 Policy Optimization. With the discounted turn-level advantages { (cid:101)Ai,j} defined above, we optimize the agent policy using clipped surrogate objective with KL regularization, following the same structure as GRPO but with finer-grained credit assignment. Formally, the IGPO objective is JIGPO(θ) = E(q,a)D, {oi}πθold (q) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:32) min πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) (cid:101)Ai, t, clip (cid:18) πθ(oi,t q, oi,<t) πθold(oi,t q, oi,<t) (cid:19) , 1 ϵ, 1 + ϵ (cid:101)Ai, (cid:33) (cid:35) β DKL(πθ πref ) , (8) where ϵ is the clipping threshold, β controls the KL penalty strength, and maps token oi,t to its originating turn. During optimization, only decision tokens (reasoning, tool calls, and answers) receive gradient updates, while raw tool responses are masked out. To further substantiate the simplicity and implementability of the proposed IGPO, we provide an algorithmic flow comparison between IGPO and GRPO in Appendix E."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Datasets & Metrics. To evaluate the effectiveness of our proposed IGPO, we conduct experiments on both in-domain (ID) and out-of-domain (OOD) QA benchmarks in an agentic search setting. Following previous work (Zheng et al., 2025b; Deng et al., 2025), the ID setting includes four widely used datasets: NQ (Kwiatkowski et al., 2019), TQ (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and 2Wiki (Ho et al., 2020), while the OOD setting includes three datasets: MusiQue (Trivedi et al., 2022), Bamboogle (Press et al., 2022), and PopQA (Mallen et al., 2022). We report word-level F1 as the evaluation metric, which is computed as the harmonic mean of precision and recall between the predicted and reference answers. Baselines. To directly verify IGPOs superiority on agentic search tasks, we compare it against set of competitive baselines: (1) Prompt-based methods: CoT (Wei et al., 2022), CoT+RAG (Gao et al., 2023), and Search-o1 (Li et al., 2025b), which represent the baseline performance of LLMs without further training on search tasks. (2) Outcome-reward RL-based methods: Search-r1-base/Instruct (Jin et al., 2025), R1-searcher (Song et al., 2025a), and DeepResearcher (Zheng et al., 2025b), the representative search agents with outcome-based reward RL, yielding marked performance gains. (3) Step-reward RL-based methods: StepSearch-base/instruct (Wang et al., 2025), ReasoningRAG (Zhang et al., 2025c), and GiGPO (Feng et al., 2025), which are the latest approaches exploring step-reward RL in search-agent settings. To further validate IGPOs effectiveness, we also compare it against the following commonly used RL algorithms under the same configuration: PPO (Schulman et al., 2017), widely used actor-critic algorithm that requires an additional value model, and critic-free methods Reinforce++ (Hu, 2025), RLOO (Kool et al., 2019; Ahmadian et al., 2024), GRPO (Shao et al., 2024), and GSPO(Zheng et al., 2025a) which perform advantage estimation over trajectory groups or batchs. Implementation Details. We use Qwen2.5-7B-Instruct (Qwen et al., 2025) as our backbone model. The training is conducted using the verl (Sheng et al., 2025) framework. The discounted factor γ is set to 1 with no future tuning. At each training step, we sample 32 prompts, and sample 16 rollouts for each prompt. The maximum dialogue turns are set to 10. For the environment, we use the google search API as our tool. The settings of our experiments are consistent with DeepResearcher (Zheng et al., 2025b). For the other baselines in Table 1, we directly copy their reported results. All RL training methods (including ours and the baselines) use exactly the same hyperparameter configurations. The training and inference prompt templates are shown in Appendix F. Please refer to Appendix for more details. 4.2 OVERALL PERFORMANCE The overall performance comparison between IGPO and the baseline methods is presented in Table 1 and Table 2. Based on these results, we can draw the following key observations: 6 Table 1: Main results of IGPO compared with different agentic RL baselines across seven datasets."
        },
        {
            "title": "Method",
            "content": "NQ TQ HotpotQA 2Wiki Musique Bamboogle PopQA Avg. In-domain Out-of-domain Prompt-based CoT CoT+RAG Search-o1 19.8 45.6 42.0 68.9 32.4 58. Outcome-reward RL-based Search-r1-base Search-r1-instruct R1-searcher DeepResearcher 45.4 71.9 33.1 44.7 35.4 73.1 39.6 78.4 Step-reward RL-based - StepSearch-base - StepSearch-instruct - ReasoningRAG GiGPO IGPO 46.4 64.7 46.7 80.1 - - - 24.4 37.1 33. 55.9 45.7 44.8 52.8 49.3 50.2 48.9 41.6 57.2 26.4 24.4 30.9 44.6 43.4 59.4 59.7 45.0 43.1 50.4 43.6 68.2 8.5 10.0 14. 26.7 26.5 22.8 27.1 32.4 31.2 20.6 18.9 31.4 22.1 25.4 46.6 56.5 45.0 64.8 71.0 57.3 53.4 45.5 68.9 74.9 17.0 46.9 38. 43.2 43.0 42.7 48.5 - - 46.2 46.1 52.5 23.4 36.4 36.4 49.2 40.2 49.0 53.9 46.0 44.5 42.3 47.2 58.7 Table 2: Main results of IGPO compared with different RL baselines across seven datasets. In-domain Out-of-domain"
        },
        {
            "title": "Method",
            "content": "RLOO PPO GRPO Reinforce++ GSPO IGPO NQ 40.7 38.7 40.3 34.3 41.5 46.7 TQ HotpotQA 2Wiki"
        },
        {
            "title": "Musique Bamboogle",
            "content": "PopQA Avg. 72.5 75.4 77.0 67.5 77.7 80.1 49.6 48.6 48.9 45.9 46.3 57.2 55.0 59.7 57.7 54.5 60.1 68.2 24.8 26.2 25.0 23.7 25.4 31.4 62.2 63.4 65.1 61.2 67.6 74. 43.1 48.7 49.6 44.3 45.4 52.5 49.7 51.5 51.9 47.3 52.0 58.7 Training-based methods consistently outperform prompt-based baselines. As shown in Table 1, all reinforcement learningbased methods, whether outcomeor step-reward driven, achieve substantially higher performance than all prompt-based approaches. This confirms that explicit policy optimization is essential for developing effective LLM-based agents, as opposed to relying on zero-shot prompting alone. Existing step-reward methods yield competitive but unstable improvements compared to outcome-reward RL methods. While step-reward baselines occasionally surpass outcome-reward ones on specific datasets (e.g., StepSearch on Musique), their overall performance still lags behind the strongest outcome-reward methods such as DeepResearcher. This suggests that existing stepreward designs, although able to provide intermediate guidance, often suffer from noisy or weak supervision signals that limit their generalizability. IGPO achieves the best overall performance across both in-domain and out-of-domain datasets. Our IGPO outperforms all baselines, with an average score of 58.7, clear margin over the best method (+4.8 over DeepResearcher). This improvement is attributed to IGPOs information gain-based reward design, which assigns intrinsic, ground-truth-aware credit at every turn while preserving the outcome reward at the answer step. By avoiding advantage collapse and improving sample efficiency, IGPO delivers robust gains across both in-domain and out-of-domain datasets. IGPO consistently outperforms other RL algorithms. Beyond task-specific baselines, Table 2 shows that IGPO also achieves the highest overall score among standard RL methods, surpassing RLOO, PPO, Reinforce++, and GSPO. Unlike these methods, which rely solely on sparse outcome 7 Table 3: Ablation results of IGPO on Qwen2.5-3B/7B-Instruct with different reward designs. IGPO (w/ F1) corresponds to using only outcome rewards, reducing to standard GRPO. Method NQ TQ HotpotQA 2Wiki Musique Bamboogle PopQA Avg. In-domain Out-of-domain Qwen2.5-3B-Instruct 31.0 29.1 40.5 IGPO (w/ F1) IGPO (w/ IG) IGPO (w/ F1+IG) Qwen2.5-7B-Instruct 40.3 37.5 46. IGPO (w/ F1) IGPO (w/ IG) IGPO (w/ F1+IG) 55.6 53.6 69.4 77.0 75.0 80.1 27.5 27.9 46.8 48.9 51.0 57.2 29.4 36.5 48. 57.7 61.0 68.2 12.1 17.5 23.1 25.0 28.6 31.4 35.7 44.7 57.9 65.1 69.6 74.9 34.9 31.3 47. 49.6 47.1 52.5 32.3 34.4 47.6 51.9 52.8 58.7 (a) NQ (b) TQ (c) HotpotQA (d) 2Wiki (e) Musique (f) Bamboogle (g) PopQA Figure 3: Training curves on Qwen2.5-7B-Instruct with different reward designs. rewards, IGPO incorporates turn-level advantages to provide denser and more stable supervision, leading to stronger generalization and more efficient training. 4.3 ABLATION STUDY We further conduct ablation experiments to assess the contribution of different reward components. As shown in Table 3, we observe: First, using only information gain (IG) turn-based reward or only outcome reward (F1) yields clearly inferior results compared to the full combination. This highlights the complementary roles of turn-level and outcome-level supervision: the outcome reward enforces alignment with the final task objective but suffers from severe sparsity, whereas the information gain reward offers dense and stable guidance for intermediate steps. Second, IGPO with only IG achieves performance comparable to or even exceeding that of standard GRPO (i.e., IGPO w/ F1). This demonstrates that IGPOs information gain reward is not subject to reward hacking. Usually, without outcome supervision, unstable reward designs would quickly collapse. In contrast, our IGPO remains robust because its turn-level signals are intrinsically defined and grounded in the ground truth. Third, the improvements are particularly pronounced on the smaller 3B model. Compared to standard GRPO, IGPO improves the 3B model by +15.3 points (32.3 47.6) and the 7B model by +6.8 points (51.9 58.7). This larger benefit on the 3B model arises because advantage collapse is more severe for weaker models that struggle to directly produce correct answers (Figure 1), making them especially reliant on dense reward signals. In such cases, the information gain reward helps prune noisy reasoning paths and reinforce rollouts that progressively approach the ground truth. Figure 4: Mean reduction in ground-truth answer entropy from the initial query (Turn 0) to the last non-answer turn (T 1) during training. Figure 5: Token Efficiency: average performance with respect to the number of tokens used for gradient updates. Finally, IGPO demonstrates consistently faster and more stable learning dynamics. As shown in Figure 3, IGPO steadily outperforms its two ablated variants throughout training across all seven datasets. The curves highlight two advantages: (i) IGPO converges to higher F1 scores, confirming the benefit of combining intrinsic turn-level reward and outcome rewards, and (ii) IGPO maintains stable improvements over steps, indicating robustness against reward sparsity and noisy supervision. These results further validate that IGPO provides dense and reliable training signals, thereby improving both training efficiency and final performance. 4.4 IN-DEPTH ANALYSIS Ground-truth Entropy Reduction. To better understand how IGPO improves training dynamics, we measure the change in ground-truth answer entropy from the initial query (Turn 0) to the last non-answer turn (T 1). As shown in Figure 4, IGPO consistently achieves larger entropy reduction than GRPO throughout training. This indicates that the information gain reward effectively encourages intermediate steps to move the policy closer to the ground-truth answer distribution. In contrast, outcome-based supervision in GRPO provides no guidance for intermediate turns, resulting in weaker entropy reduction. These results highlight that IGPOs turn-level supervision translates into more confident and grounded reasoning trajectories. Token Efficiency. We further compare IGPO and GRPO in terms of token efficiency, i.e., the performance improvement per token used for gradient updates. As shown in Figure 5, performance In increases more rapidly under IGPO, and the gap over GRPO widens as training progresses. other words, IGPO achieves stronger performance with fewer tokens, indicating that its turn-level rewards deliver denser and more informative gradients than outcome-only supervision. This finding is consistent with the training dynamics observed in Figure 3, where IGPO not only converges faster but also maintains stable advantage throughout optimization. Such improvements in token efficiency are particularly valuable in agentic RL, where training data is scarce and expensive to obtain, making efficient use of every gradient update critical factor for scaling. The case study and additional analyses are provided in Appendix D. Beyond empirical effectiveness, our theoretical analysis in Appendix shows that maximizing turnlevel information gain constrains error accumulation in multi-turn scenarios. Thus, IGPO not only alleviates advantage collapse but also reduces error accumulation in long-horizon agentic tasks."
        },
        {
            "title": "5 RELATED WORK",
            "content": "The recent success of reinforcement learning (RL) methods in large reasoning models (Chen et al., 2025a) such as OpenAI o1 (Jaech et al., 2024) and DeepSeek R1 (Guo et al., 2025) has established RL as central tool for enhancing large language models (LLMs)-based agents to solve more complex tasks. growing body of work has explored different RL algorithms such as PPO (Schulman et al., 2017), Reinforce++ (Hu, 2025), GRPO (Shao et al., 2024), RLOO (Kool et al., 2019; Ahmadian et al., 2024), DAPO (Yu et al., 2025), and GSPO (Zheng et al., 2025a). These methods have been particularly effective in improving the capabilities of LLM-based agents (Li et al., 2025a). 9 Building on these advances, an important line of research has focused on applying RL to searchbased agents (Deng et al., 2025; Dai et al., 2025b;a). Early efforts such as DeepRetrieval (Jiang et al., 2025) demonstrated the feasibility of end-to-end optimization by applying PPO with retrievaloriented metrics (e.g., recall) as rewards. Subsequent works, including Search-R1 (Jin et al., 2025), DeepResearcher (Zheng et al., 2025b), and ReSearch (Chen et al., 2025b), extended this paradigm to multi-turn reasoning and search. R1-Searcher (Song et al., 2025a) and R1-Searcher++ (Song et al., 2025b) further introduced two-stage RL strategies, separately strengthening the ability to interact with retrieval systems and to utilize retrieved information effectively. However, in multi-turn scenarios, outcome-only rewards remain sparse and often fail to provide sufficient guidance, leading to unstable optimization and inefficient sample utilization. Recent studies have explored step-wise or process-level rewards that assign credit to intermediate actions. ReasonRAG (Zhang et al., 2025c) adopted Monte Carlo Tree Search (MCTS) to approximate the value of each step. StepSearch (Wang et al., 2025) leveraged memory vector of retrieved documents, supervising intermediate steps based on their maximum similarity to ground-truth evidence. GiGPO (Feng et al., 2025) introduced anchor-based grouping to estimate relative advantages for actions originating from the same anchor state. While these methods provide denser supervision than outcomeonly rewards, they either rely on external oracle knowledge or suffer from limited stability and scalability, leaving room for more intrinsic and generalizable process-level reward designs."
        },
        {
            "title": "6 CONCLUSION, LIMITATIONS AND FUTURE WORK",
            "content": "In this work, we propose IGPO, simple and effective reinforcement learning framework for training multi-turn LLM-based agents. By providing intrinsic, ground-truth-aware supervision at every turn while preserving alignment with the final objective, IGPO delivers dense and stable training signals. Extensive experiments across in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines, achieving higher accuracy and better sample efficiency, particularly for smaller models where sparse rewards are most problematic. However, our approach still relies on the availability of ground-truth answers, which limits its applicability in open-ended settings. In future work, we plan to extend IGPO to broader agentic scenarios beyond search, including tasks without explicit supervision."
        },
        {
            "title": "REFERENCES",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Kedi Chen, Dezhao Ruan, Yuhao Dan, Yaoting Wang, Siyu Yan, Xuecheng Wu, Yinqi Zhang, Qin Chen, Jie Zhou, Liang He, Biqing Qi, Linyang Li, Qipeng Guo, Xiaoming Shi, and Wei Zhang. survey of inductive reasoning for large language models, 2025a. URL https://arxiv.org/ abs/2510.10182. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, Fan Yang, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025b. Yuqin Dai, Guoqing Wang, Yuan Wang, Kairan Dou, Kaichen Zhou, Zhanwei Zhang, Shuo Yang, Fei Tang, Jun Yin, Pengyu Zeng, et al. Evinote-rag: Enhancing rag models via answer-supportive evidence notes. arXiv preprint arXiv:2509.00877, 2025a. Yuqin Dai, Shuo Yang, Guoqing Wang, Yong Deng, Zhanwei Zhang, Jun Yin, Pengyu Zeng, Zhenzhe Ying, Changhua Meng, Can Yi, et al. Careful queries, credible results: Teaching rag models advanced web search tools with reinforcement learning. arXiv preprint arXiv:2508.07956, 2025b. Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, Yang Qin, Yuan Wang, Quanxing Zha, Sunhao Dai, and Changhua Meng. Atom-searcher: Enhancing agentic deep research via fine-grained atomic thought reward. arXiv preprint arXiv:2508.12800, 2025. 10 Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. Zeyu Gan, Yun Liao, and Yong Liu. Rethinking external slow-thinking: From snowball errors to probability of correct reasoning. In Forty-second International Conference on Machine Learning, 2025. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1), 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. Carlos Gutierrez, Anthony Aguirre, Risto Uuk, Claire Boine, and Matija Franklin. proposal for definition of general purpose artificial intelligence systems. Digital Society, 2(3):36, 2023. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 66096625, 2020. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, et al. Deep research agents: systematic examination and roadmap. arXiv preprint arXiv:2506.18096, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han. Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning. arXiv preprint arXiv:2503.00223, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get baseline for free! 2019. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Long Li, Jiaran Hao, Jason Klein Liu, Zhijian Zhou, Xiaoyu Tan, Wei Chu, Zhe Wang, Shirui Pan, Chao Qu, and Yuan Qi. The choice of divergence: neglected key to mitigating diversity collapse in reinforcement learning with verifiable reward. arXiv preprint arXiv:2509.07430, 2025a. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025b. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025c. 11 Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 7, 2022. Liangbo Ning, Ziran Liang, Zhuohang Jiang, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei, Shanru Lin, Hui Liu, Philip Yu, et al. survey of webagents: Towards next-generation In Proceedings of the 31st ACM ai agents for web automation with large foundation models. SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pp. 61406150, 2025. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and JiRong Wen. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8):198343, 2025. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, pp. 12791297. ACM, March 2025. doi: 10.1145/3689031.3696075. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025a. Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher++: Incentivizing the dynamic knowledge acquisition of llms via reinforcement learning. arXiv preprint arXiv:2505.17005, 2025b. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, et al. Searching for best practices in retrieval-augmented generation. arXiv preprint arXiv:2407.01219, 2024a. Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, and Yichao Wu. Stepsearch: Igniting llms search ability via step-wise proximal policy optimization. arXiv preprint arXiv:2505.15107, 2025. 12 Ziting Wang, Haitao Yuan, Wei Dong, Gao Cong, and Feifei Li. Corag: cost-constrained retrieval optimization system for retrieval-augmented generation. arXiv preprint arXiv:2411.00744, 2024b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning for llms: survey. arXiv preprint arXiv:2509.02547, 2025a. Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, et al. From web search towards agentic deep research: Incentivizing search with reasoning agents. arXiv preprint arXiv:2506.18959, 2025b. Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, et al. Process vs. outcome reward: Which is better for agentic rag reinforcement learning. arXiv preprint arXiv:2505.14069, 2025c. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025a. Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, et al. Openresearcher: Unleashing ai for accelerated scientific research. arXiv preprint arXiv:2408.06941, 2024. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025b. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "A THEORETICAL ANALYSIS",
            "content": "The theoretical analysis here provides an intuitive support for the efficacy of our proposed method by addressing the limitations of sparse outcome rewards in multi-turn agents. Specifically, the theory establishes crucial link: maximizing the process reward (IGPOs objective) is equivalent to minimizing the upper bound on the undesirable accumulation of snowball errors during the reasoning process. This minimization, in turn, systematically lowers the theoretical minimum for the final answer error rate, thus providing fundamental guarantee that IGPOs dense, turn-level signals lead to more confident and successful reasoning trajectories. Notations. Let Efinal be the event that the agents generated final answer does not match the ground truth answer. Its probability is denoted by P(Efinal), i.e., the error rate. For each turn , denote the observed response [think], [tool call] as Rt. We also posit that there is an unobservable, abstract thinking step It that underlies the generation of Rt. Let R(t) process be the process reward, which is dense reward signal received at each turn of the interaction. It is defined as the information gain about the ground truth answer, which is calculated as the increase in the log-probability of the correct answer from the previous state to the current state. Then, the total process reward Rtotal = (cid:80)T 1 process] is the cumulative sum of all process rewards over complete trajectory or episode. t=1 The expectation is taken over the thinking step and observed response. The training objective of the policy is to maximize this total reward. Definition A.1 (Snowball Error in Multi-turn Agentic RL). Consistent with Gan et al. (2025), we define the information loss at turn as the conditional entropy Ent(ItRt). Consider the nontrivial case where Ent(ItRt) is bounded. The cumulative snowball error up to turn is the sum of these losses: E[R(t) Ent<T (IR) 1 (cid:88) t= Ent(ItRt) (9) This quantity measures the aggregate uncertainty and ambiguity accumulated throughout the reasoning trajectory before the final answer is produced. Next, we connect the cumulative snowball error to the agents final performance. It indicates the fundamental limitation of multi-turn agentic RL pipeline caused by snowball error. Lemma A.2 (Lower bound of error rate). The probability of final answer error, P(Efinal), is lowerbounded by the cumulative snowball error accumulated during the reasoning process: P(Efinal) = Ω (cid:18) Ent<T (IR) (cid:19) Cconst. (10) where Cconst is small positive constant. Proof Sketch. This result is strongly motivated by Theorem 3.3 from Gan et al. (2025). We treat the generation of the final answer at turn as the final step of multi-step reasoning process. The quality of this final step is conditioned on the information accumulated over the previous 1 turns. The theorem from (Gan et al., 2025) states that the error probability of any step is lower-bounded by the average snowball error accumulated up to that point. Applying this principle to the final step (t = ) yields the stated result. Assumption A.3 (Monotonic Reward-Information Loss Link). The expected process reward at any turn H, E[R(t) process], is monotonically non-increasing with respect to the information loss at that turn, Ent(ItRt). We assume there exists bounded and monotonically non-increasing convex function : R+ such that: E[R(t) processIt, Rt] (Ent(ItRt)) . (11) Remark. As the information loss Ent(ItRt) at turn increases, the expected total information loss tends to decreases and asymptotically approaches relatively small value, which is characterized by the convex nature of function . 14 This assumption leads to the following result, demonstrating that optimizing for process rewards implicitly constrains the accumulation of snowball errors. We first formalize the intuition that clearer reasoning step (lower information loss) is prerequisite for high-quality query, which in turn yields higher expected process reward. Theorem A.4 (Process Reward as Bound on Snowball Error). Under Assumption A.3, the expected cumulative snowball error is upper bounded by E[Ent<T (IR)] = O(1) Ω (Rtotal) . (12) Theorem A.4 establishes that maximizing the process reward is mathematically coupled with minimizing an upper bound on the cumulative snowball error. The combination of Theorem A.4 and Lemma A.2 provides complete, end-to-end theoretical justification for the efficacy of our proposed process reward mechanism. The logical chain is as follows: Maximizing the process reward (our algorithms objective) forces the agent to minimize an upper bound on the cumulative snowball error (Theorem A.4). Minimizing the cumulative snowball error, in turn, lowers the theoretical minimum for the final error rate, thereby systematically increasing the probability of task success (Lemma A.2). In conclusion, the turn-level process reward is not merely an engineering heuristic; it is theoretically grounded mechanism that fundamentally addresses the problem of error accumulation in multi-step reasoning. By providing dense, immediate signal for reasoning clarity, it transforms the intractable problem of sparse-reward, long-horizon exploration into series of manageable, shorthorizon sub-problems, each aimed at maximizing immediate information gain. This explains the significant gains in training efficiency and final performance observed in our experiments."
        },
        {
            "title": "B PROOF FOR THEORETICAL ANALYSIS",
            "content": "B.1 PROOF OF LEMMA A.2 Proof. We achieve this by applying Theorem 3.3 from Gan et al. (2025) to the final decision-making step of the agent. In particular, P(Efinal) Ent<T (IR) 1 C1 log(Afinal 1) , (13) where Afinal is the cardinality of the final answer space and C1 is small positive constant analogous to Entb(et) in Gan et al. (2025). Since log(Afinal1) and C1 are constant, simplifies to form that is asymptotically dominated by the variable term. Therefore, the right-hand side of the inequality can be expressed in terms of the lower bound symbol Ω as Ω Cconst, which completes the proof. (cid:16) Ent<T (IR) 1 log(Afinal1) (cid:17) Ent<T (IR) 1 C1 B.2 PROOF OF THEOREM A.4 Proof. According to the nature of and the fact that there exist constants Cmax and β such that for all non-negative bounded x, there holds (x) Cmax βx. Therefore, by taking the expectation over Assumption A.3 and summing across all turns from = 1 to 1, we have Rtotal = 1 (cid:88) t=1 E[R(t) process] 1 (cid:88) t= 1 (cid:88) t=1 E[f (Ent(ItRt))] E[Cmax β Ent(ItRt)] = (T 1)Cmax β 1 (cid:88) t=1 E[Ent(ItRt)] = (T 1)Cmax β E[Ent<T (IR)]. Rearranging terms yields the final result."
        },
        {
            "title": "C MORE IMPLEMENTATION DETAILS",
            "content": "All our training experiments are conducted on 8 NVIDIA A100-80G GPUs. The detailed hyperparameter settings are provided in Table 4. Unless otherwise specified, all experiments are based on this configuration. Table 4: Training hyperparameters. Training hyperparameters Training Batch Size Mini-Batch Size Infer Tensor Model Parallel Size Sequence Parallel Size Max Prompt Length Max Response Length Actor Learning Rate Rollout Temperature Rollout Group Size Max Turn Call Turns KL-Divergence loss coefficient Value 32 32 1 4 30767 2000 1e-6 1.0 16 10 0."
        },
        {
            "title": "D MORE DISCUSSION AND EXPERIMENTAL ANALYSIS",
            "content": "D.1 COMPARISON WITH OTHER PROCESS-REWARD METHODS In addition to its obvious performance advantages, we also conduct deeper analysis of IGPOs superiority in terms of algorithmic characteristics compared to other process-reward-based agentic RL algorithms. We first introduce other existing process-reward-based agentic RL algorithms: ReasoningRAG. The main contribution of this work is the proposal of step-level data labeling strategy based on MCTS. Subsequently, the DPO algorithm is used to optimize the agents policy on the labeled step-level dataset. The main limitations of this method are: (1) the data labeling process relies on MCTS, which is inefficient, and when the number of samples is insufficient, it is difficult to accurately estimate the value of each step; (2) the off-policy optimization based on DPO is less effective than on-policy algorithms. StepSearch. StepSearch constructs turn-level supervision signals by pre-defining golden search keywords and golden tool responses, and adopts an on-policy optimization approach. Although it shifts from off-policy to on-policy, the annotation process is resource-intensive and prone to annotator bias (whether from humans or LLMs). GiGPO. GiGPO introduces step-level grouping strategy based on anchor states and performs fine-grained advantage estimation within each step-level group. Although this provides novel solution, it essentially still relies on the Monte Carlo assumption. When the number of anchor states is insufficient, it is often difficult to accurately estimate their value, which in turn leads to biased advantage estimation. The proposed IGPO effectively addresses the aforementioned limitations. Starting from the onpolicy GRPO setting (where rollout data are used for single parameter update), it employs an information-gainbased incremental reward construction strategy that requires no annotation and does not rely on Monte Carlo. Moreover, the incorporation of ground-truth awareness substantially reduces bias. Table 5 provides detailed comparison highlighting the advantages of IGPO over other algorithms. D.2 CASE STUDY 16 Table 5: Comparison between various process reward methods."
        },
        {
            "title": "Algorithm\nReasoningRAG\nStepSearch\nGiGPO\nIGPO",
            "content": "On-Policy Explicit Labeling-Free Monte CarloFree"
        },
        {
            "title": "No\nYes\nNo\nYes",
            "content": "Introduces No Bias Sample-size Dependent No Sample-size Dependent Yes Figure 6: Case study showing scenario where the final answer is incorrect but contains single correct retrieval turn. IGPO provides process reward for this turn, improving token utilization. 17 Figure 7: Case study illustrating situation where the first round of retrieval failed, but the second and third rounds successfully located the correct evidence and produced the right answer. In this case, IGPO imposes penalty on the erroneous retrieval in the first round."
        },
        {
            "title": "E COMPARISON BETWEEN GRPO AND IGPO",
            "content": "Algorithm 1 illustrates the algorithmic flow of IGPO (right) and GRPO (left). The key steps corresponding to each algorithm are highlighted with the same color font to visually highlight the differences: yellow for reward calculation, green for advantage estimation, blue for advantage accumulation and assignment, and purple for policy optimization. In terms of reward calculation, IGPO constructs dense turn-level rewards through incremental information gain. For advantage estimation, both IGPO and GRPO use Group Normalization. Regarding advantage accumulation and allocation, GRPO directly assigns the outcome-based advantage to all tokens of the current output, while IGPO further computes the cumulative discounted advantage, capturing long-horizon information and performing turn-level reward assignment. In policy optimization, IGPO achieves more efficient and effective optimization by maximizing the turn-level cumulative discounted advantages. Algorithm 1 GRPO vs. IGPO GRPO Require: initial policy πθinit ; hyperparameters ϵ, β, µ task prompts D; IGPO Require: initial policy πθinit; task prompts D; max turns H; hyperparameters ϵ, β, γ, µ πref πθ for step = 1, . . . , do 1: πθ πθinit 2: for iteration = 1, . . . , do 3: 4: 5: 6: 7: i=1 πθold( q) Sample batch Db from πθold πθ For each Db, sample outputs {yi}G Compute outcome reward {ri}G the final answer in each yi Compute each yis advantage {Ai}G via group normalization of {ri}G (Eq.in Sec 2.2) Assign Ai to all tokens of yi for GRPO iter = 1, . . . , µ do i=1 from i= i=1 Update πθ by maximizing the GRPO objective (Eq. 1) 8: 9: 10: 11: 12: end for 13: 14: 15: end for end for πref πθ for step = 1, . . . , do 1: πθ πθinit 2: for iteration = 1, . . . , do 3: 4: 5: 6: 7: Sample batch Db from πθold πθ For each Db, sample outputs {yi}G i=1 πθold( q) for iteration = 1, . . . , do if < then Compute the infomation gainbased turn-level rewards {ri,t}G for each yi (Eq. 4) i=1 else Compute the final-turn rewards {ri,T }G i=1 based on the answer in each yi (Eq. 2) the per turn advantages i=1 in yi via group normali=1 (Eq. 6) end if end for Compute {Ai,1tT }G ization of {ri,1tT }G Compute the per turn cumulative discounted advantages { ˆAi,1tT }G i=1 in each yi (Eq. 7), then assign them to the tokens in each turn for IGPO iteration = 1, . . . , µ do Update πθ by maximizing the IGPO objective (Eq. 8) 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for 19: 20: 21: end for end for 19 PROMPT TEMPLATE USED IN OUR EXPERIMENTS. Our prompt follows the style of DeepResearcher Zheng et al. (2025b), and the same template is used for training, validation, and testing. The prompt template is shown in the Figure 8, where {today} represents the current date to ensure the relevance of the models response. {{ tool.name }}: {{ tool.description }} indicates the available tools, while the #Rollout section controls the models output format. The #Tools section provides the model with the tool invocation method. Figure 8: Prompt template used in our experiments."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Individual Author",
        "Renmin University of China"
    ]
}