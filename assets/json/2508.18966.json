{
    "paper_title": "USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning",
    "authors": [
        "Shaojin Wu",
        "Mengqi Huang",
        "Yufeng Cheng",
        "Wenxu Wu",
        "Jiahe Tian",
        "Yiming Luo",
        "Fei Ding",
        "Qian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing literature typically treats style-driven and subject-driven generation as two disjoint tasks: the former prioritizes stylistic similarity, whereas the latter insists on subject consistency, resulting in an apparent antagonism. We argue that both objectives can be unified under a single framework because they ultimately concern the disentanglement and re-composition of content and style, a long-standing theme in style-driven research. To this end, we present USO, a Unified Style-Subject Optimized customization model. First, we construct a large-scale triplet dataset consisting of content images, style images, and their corresponding stylized content images. Second, we introduce a disentangled learning scheme that simultaneously aligns style features and disentangles content from style through two complementary objectives, style-alignment training and content-style disentanglement training. Third, we incorporate a style reward-learning paradigm denoted as SRL to further enhance the model's performance. Finally, we release USO-Bench, the first benchmark that jointly evaluates style similarity and subject fidelity across multiple metrics. Extensive experiments demonstrate that USO achieves state-of-the-art performance among open-source models along both dimensions of subject consistency and style similarity. Code and model: https://github.com/bytedance/USO"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 6 6 9 8 1 . 8 0 5 2 : r USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning Shaojin Wu Mengqi Huang Yufeng Cheng Wenxu Wu"
        },
        {
            "title": "Jiahe Tian Yiming Luo",
            "content": "Fei Ding Qian He UXO Team, Intelligent Creation Lab, ByteDance"
        },
        {
            "title": "Abstract",
            "content": "Existing literature typically treats style-driven and subject-driven generation as two disjoint tasks: the former prioritizes stylistic similarity, whereas the latter insists on subject consistency, resulting in an apparent antagonism. We argue that both objectives can be unified under single framework because they ultimately concern the disentanglement and re-composition of content and style, long-standing theme in style-driven research. To this end, we present USO, Unified Style-Subject Optimized customization model. First, we construct large-scale triplet dataset consisting of content images, style images, and their corresponding stylized content images. Second, we introduce disentangled learning scheme that simultaneously aligns style features and disentangles content from style through two complementary objectives, style-alignment training and contentstyle disentanglement training. Third, we incorporate style reward-learning paradigm denoted as SRL to further enhance the models performance. Finally, we release USO-Bench, the first benchmark that jointly evaluates style similarity and subject fidelity across multiple metrics. Extensive experiments demonstrate that USO achieves state-of-the-art performance among open-source models along both dimensions of subject consistency and style similarity. Code and model: https://github.com/bytedance/USO Date: August 27, 2025 Project Page: https://bytedance.github.io/USO/ Correspondence: Shaojin Wu at wushaojin@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "The significant advancements in image generation over the past years have greatly improved generative controllability, fundamentally changing how humans create images, i.e., whether through abstract textual descriptions, specific visual reference images, or both. Research on leveraging both textual and visual conditions has attracted increasing interest, giving rise to numerous real-world tasks such as style-driven generation and subject-driven generation. While textual conditions are typically explicit, visual conditions are inherently noisy, as images intrinsically embody rich spectrum of features (e.g., style, appearance), of which only specific one is relevant to specific task. * Corresponding author Project lead. 1 Figure 1 Showcase of the versatile abilities of the USO model. Prompts are in Table 6. 2 Figure 2 Illustration of our motivation. By jointly disentangling content and style across tasks, we unify style-driven and subject-driven generation within single framework. For instance, style-driven generation requires only the style feature from the reference images, whereas other features constitute noise. Therefore, fundamental and long-standing challenge in these tasks is to accurately include all required features from the reference image while simultaneously excluding other noisy ones, e.g., including only the style in style-driven generation or only the subjects appearance in subject-driven generation. Extensive efforts in the literature have been dedicated to disentangling different features in visual conditions (i.e., reference images). On the one hand, in the realm of style-driven generation, DEADiff [23] employs QFormer to selectively query only the style features from reference images. CSGO [37] constructs contentstyle-stylized triplets to facilitate style-content decoupling during training. StyleStudio [16] introduces style-based classifier-free guidance (SCFG) to enable selective control over stylistic elements and to mitigate the influence of irrelevant features. On the other hand, subject-driven generation methods primarily focus on disentangling subject appearance features or constructing more effectively disentangled paired data. For example, RealCustom [10, 19] proposes dual-inference framework that selectively incorporates subjectrelevant features into subject-specific regions. UNO [34] leverages the in-context capabilities of DiT to progressively improve both the quality of paired data and the model itself. To conclude, existing methods primarily focus on task-specific disentanglement by designing tailored datasets or model architectures for each individual task, thereby performing disentanglement in an isolated, single-task context. In this study, we argue that more comprehensive and precise disentanglement approach should fully account for the coupling and complementarity between different generation tasks. Each task should not only learn which features to include, but, more importantly, also learn which features to exclude, i.e., features that are often required by other tasks. Therefore, learning to include certain features in one task inherently informs and enhances the process of learning to exclude those same features in complementary task, and vice versa. For example, style-driven generation aims to incorporate stylistic features while excluding subject appearance features, whereas subject-driven generation does the exact opposite. The ability to learn and include subject appearance features in subject-driven generation can, in turn, help style-driven generation more effectively learn to exclude these features, thereby improving disentanglement for both tasks. In conclusion, we believe 3 that jointly modeling complementary tasks enables mutually reinforcing disentanglement process, leading to more precise separation of relevant and irrelevant features for each task. Based on the above motivation, we propose novel cross-task co-disentanglement paradigm to unify subject-driven and style-driven generation, and, more importantly, to mutually enhance the performance of both tasks, as illustrated in Figure 2. Specifically, this co-disentanglement paradigm is implemented through subject-for-style data curation framework and style-for-subject model training framework. The subject-for-style framework first utilizes state-of-the-art subject model to generate high-quality style data, while the style-for-subject framework subsequently trains more effective subject model under the guidance of style rewards and disentangled training. Technically, on the one hand, for the subject-for-style data curation framework, we build upon state-of-the-art subject-driven model [34] and further develop both stylization expert and de-stylization expert to curate stylized and non-stylized images. This process ultimately constructs triplet data pairs in the form of <style reference, de-stylized subject reference, stylized subject result> for subsequent model training. On the other hand, for the style-for-subject model training framework, we propose Unified Style-Subject Optimized (USO) customization model, which introduces progressive style alignment and style-subject disentanglement training, both supervised by style reward. Our contributions are summarized as follows: Concepts: We point out that existing style-driven and subject-driven methods focus solely on isolated disentanglement within each task, neglecting their potential complementarity and thus leading to suboptimal disentanglement. For the first time, we propose novel cross-task co-disentanglement paradigm that unifies style-driven and subject-driven tasks, enabling mutual enhancement and achieving significant performance improvements for both. Technique: We present novel cross-task triplet curation framework that bridges style-driven and subjectdriven generation. Building on this, we introduce USO, unified customization architecture that incorporates progressive style-alignment training, contentstyle disentanglement training, and style reward learning paradigm to further promote cross-task disentanglement. We further release USO-Bench, to the best of our knowledge, the first benchmark tailored for evaluating cross-task customization. Performance: Extensive evaluations on USO-Bench and DreamBench [25] show that USO achieves stateof-the-art results on subject-driven, style-driven, and joint style-subject-driven tasks, attaining the highest CLIP-T, DINO, and CSD scores. USO can handle individual tasks and their free-form combinations while exhibiting superior subject consistency, style fidelity, and text controllability as shown in Figure 1."
        },
        {
            "title": "2.1 Style Transfer",
            "content": "Style Transfer aims to apply the style in the reference image to the given content image or fully generated image. Early work like adaptive instance normalization [11] achieved impressive style transfer results with layout-preserved results by simply using pre-trained network as the style encoder and well-designed injection modules. The recent powerful text-to-image generation base models, like Stable Diffusion [4, 21] and FLUX [14], along with style transfer plugins built upon them, have significantly improved the convenience and effectiveness of performing this task. Several are even training-free, like StyleAlign [35] and StylePrompt [12] which transfer the style via simple query-key swapping in the specific self-attention layers. Other training-based methods can theoretically achieve better fitting and style transfer performance, but they also raise concerns of content leakage. IP-adapter [39] and DEADiff [23] demonstrate the style transfer ability with new decoupled cross-attention layer trained with coupled data, and overcome the content leakage by decreasing the injection weights in inference-time. InstanceStyle [28], StyleShot [7] and B-lora [5] provide more detailed time-aware and layer-aware injection strategies to disentangle the style and content feature injections. However, those disentanglement analyses are tied to the specific model architecture and hard to migrate. 4 Figure 3 Illustration of our proposed cross-task triplet curation framework, which systematically generates layoutpreserved and layout-shifted triplets."
        },
        {
            "title": "2.2 Subject-Driven generation",
            "content": "Subject-driven generation refers to generating images of the same subject conditioned on text instruction and reference images of given subjects. Dreambooth [25] and IP-Adapter [39] turn UNet-based text-toimage model into subject-driven model by parameter-efficient tuning or newly introduced attention plug-in. Recently, popular image-generation foundation models have shifted from UNet-based architectures to transformer-based ones. The inherent in-context learning capabilities of transformers have greatly enriched research on subject-driven generation. ICLoRA [9], OmniControl [27], UNO [34], and FLUX.1 Kontext [15] use shared attention between the generated image and reference image to train text-to-image DiT into subject-driven variant. It is worth noting that some of them have extended the reference subject to other types. OmniControl [27] supports layout control image as reference, UNO [34] supports multiple reference images input, and DreamO [20] can work for simple style transfer. They have indicated that various types of reference-guided generation can be unified within the DiT in-context framework. This further prompts the question of whether jointly addressing different tasks in this setting could lead to mutual benefits across them."
        },
        {
            "title": "3.1 Preliminary",
            "content": "Latent diffusion models [21, 24] have evolved from UNet-based architectures to DiT-based designs, with steadily improving foundational capabilities. MM-DiT [4, 14] further elevates image-generation quality, spawning numerous downstream applications and unlocking greater controllability in text-to-image generation [34]. It incorporates multi-modal attention mechanism that can be seamlessly extended to an in-context generation framework: the conditioned tokens are directly concatenated with the text prompt and the noisy latent, yielding the formulation: Attention ([c, zt, zc]) = softmax (cid:19) (cid:18) QK V, (1) where = [c, zt, zc] denotes the concatenation of text tokens, noisy latent, and condition tokens. This allows both representations to function within their own respective spaces while still taking the other into account."
        },
        {
            "title": "3.2 Cross-Task Triplet Curation Framework",
            "content": "This section details the construction of cross-task triplets for USO training. Although prior works [29, 37] have explored triplet generation, they retain the original layout, preventing any pose or spatial re-arrangement 5 Figure 4 Illustration of the training framework of USO. USO unifies subject-driven and style-driven generation in two stages: Stage 1 aligns SigLIP embeddings via style-alignment training to yield style-capable model; Stage 2 disentangles the conditional encoders and trains on triplets to enable the joint conditional generation. Finally, style-reward learning paradigm supervises both stages to yield stronger unified model. of the subject. To jointly enable subject-driven and style-driven generation beyond simple instruction-based edits, we curate new USO dataset expressly designed for this unified objective. Figure 3 provides an overview of USO dataset. Our co-disentanglement paradigm starts from subjectfor-style data curation framework. Among many possible tasks, subject-driven (i.e., UNO-1M [34]) and instruction-based editing (i.e., X2I2 [32]) datasets are comparatively easy to collect at scale, enabling targeted task-specific corpora. In particular, subject-driven data emphasizes learning from content cues while preserving subject identity and consistency; instruction-based editing bridges styles by preserving spatial layout and transferring appearance between realistic and stylized domains in both directions. These resources naturally support training domain-specialist models and, through deliberate dataset design, induce the capabilities we care about (e.g., extracting task-relevant features conditioned on image type). Guided by these insights, we curate 200k stylized image pairs sourced from publicly licensed datasets and augmented with samples synthesized by state-of-the-art text-to-image models. Using these data, we trained two complementary experts on top of the leading customization framework UNO [34]: (1) stylized expert model that performs style-driven generation conditioned on style-reference image, producing new subject rendered in the target style (I ref from Itgt), and (2) de-stylization expert model that inverts stylized image to photorealistic counterpart, allowing either flexible layout shifts or preservation (I ref via the stylization Each curated stylized image serves as the target Itgt. We synthesize its style reference via the de-stylization expert. Following [34], VLM-based filter enforces expert and its content reference ref style similarity between Itgt and . This yields two kinds of and subject consistency between Itgt and triplets, shown in Figure 3: layout-preserved and layout-shifted. Unlike prior work [29, 37], which focuses solely on style-driven generation and confines itself to layout-preserved triplets, our cross-task triplets achieve deeper contentstyle disentanglement across tasks and are used to train USO. from Itgt). ref ref ref"
        },
        {
            "title": "3.3 Unified Customization Framework (USO)",
            "content": "In this section, we describe how we unify two tasks that have traditionally been treated separately, style-driven and subject-driven generation, into single model. Each task demands the model to master distinct knowledge: the former emphasizes style similarity, while the latter insists on subject consistency. By excelling at both simultaneously, the model naturally disentangles content from style, long-standing focus of style-driven 6 generation, which in turn boosts the quality of stylization and customization. Beyond merely preserving layout during style-driven generation, the model can now freely recombine any subject with any style."
        },
        {
            "title": "3.3.1 Style Alignment Training.",
            "content": "As illustrated in Figure 4, We start from pre-trained text-to-image (T2I) model and fine-tune it into stylized variant via our proposed style-alignment training. Unlike prior in-context generation approaches that rely solely on VAE E() to encode the conditioned image Iref , we argue that style is more abstract cue demanding richer semantic information. Therefore, we employ the semantic encoder SigLIP instead of the VAE to process the reference style image . While subject-driven or identity-preserving tasks typically emphasize high-level semantics, style-driven tasks must simultaneously handle two extremes: high-level semantics to accommodate large geometric deformations (e.g., 3-D cartoon styles) and low-level details to reproduce subtle brushstrokes (e.g., pencil sketches). Following recent works like [41], we introduce lightweight Hierarchical Projector MProj() to project multi-scale, fine-grained visual features zs from the extracted SigLIP embeddings {ci}N , where represents the layer indices of SigLIP. This process can be formulated as: ref i=1 Specifically, we assign the style tokens zs the same positional indices as the text tokens c, then feed the fused multimodal sequence z1 into the DiT model as its input: zs = Concatenate(MProj({ci}N i=1)), (2) z1 = Concatenate(zs, c, zt), (3) In this stage, we freeze all parameters except the Hierarchical Projector, enabling the extracted style features to be rapidly aligned with the native textual distribution. Consequently, the pretrained T2I model is converted into stylized variant capable of accepting style-reference images as conditional input. 3.3.2 Content-Style Disentanglement Training. Building on Stage 1, we introduce subject conditioning in Stage 2 as shown in Figure 4. Following recent paradigms [27, 34], the content image is encoded into pure conditional tokens zc by frozen VAE encoder E(). We formulate USO as multi-image conditioned model, yet explicitly disentangle content and style features via separate encoders. This design alleviates content leakage, where extraneous style-image details undesirably appear in the output, and also helps the model learn to exclude undesired features for the specific task, as introduced in Section 1. ref During training, the Hierarchical Projector remains frozen while the DiT parameters are unfrozen. Content tokens receive positional indices via UnoPE [34] in its diagonal layout. The final multimodal input sequence z2 is thus expressed as: z2 = Concatenate(zs, c, zt, zc), Consequently, USO can directly handle both subject-driven and style-driven tasks on the proposed triplet dataset. (4) Compared with prior open-source style-driven methods, most of which either (i) rigidly preserve the content layout while altering its style [29] or (ii) retain layout via an external ControlNet at the cost of subject consistency with the content image [23, 28]-USO removes these constraints. Trained on our triplet data, it freely re-positions the subject from the content image into any scene while re-rendering it in the style of the reference image. 3.3.3 Style Reward Learning Beyond the standard flow-matching objective, we introduce Style Reward Learning (SRL) to explicitly disentangle style from content during optimization. Flow-matching pre-trains the model by minimizing the L2 distance between the predicted velocity vθ(xt, t) and the true velocity vt = dαt dt ϵ. Building on this, we denote the training objective as LPre, which can be computed as: LPre = Ex0,t,ϵ[w(t)vθ vt2] dt x0 + dσt (5) 7 Algorithm 1 Style Reward Learning (SRL) with Flow Matching Require: Customization model net with pretrained parameters θ; pretrain loss LPre; reward loss LSRL; reward model MRM; balancing coefficient λ; noise-schedule steps ; SRL fine-tuning interval [ts, te]; dataset are reference content and style images = {(y, I0, (Section 3.2) 1: for (y, I0, 2: ref)}, is prompt, I0 is target image and ref) // calculate pretrain loss with Equation (5) ref , ref, ref ref) do ref, ref, LPre netθ(y, I0, U(ts, te) // pick random time step in [ts, te] xT (0, I) for τ = T, . . . , + 1 do ˆvτ no-grad(netθ(y, xτ , xτ 1 xτ ˆvτ // reverse-step update ref, ref)) ref) ref, end for ˆvt netθ(y, xt, ˆI0 decode(xt ˆvtt) // predict original image LSRL MRM( ˆI0, LPre + λLSRL θ θ η θL // update model parameters via gradient descent (η is learning rate) ref) // calculate SRL loss with negative reward with Equation (6) 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for where w(t) is weighting function, vθ is neural network parameterized by θ, and αt = 1 t, σt = are continuous-time coefficients with [0, = 1]. The sampling process is from = with xT (0, I) and stops at = 0, solving the PF-ODE via dxt = vθ(xt, t)dt. Extending ReFL [38] from T2I generation to the reference-to-image setting, where generation is conditioned on both an image reference and and its corresponding text prompt, SRL alternates between computing reward score and back-propagating the reward signal. As shown in Figure 4, we define the reward score as the style similarity between the reference style image and the generated stylized image Iθ, measured by either VLM-based filter or the CSD model MRM() [26, 37]. The reward loss is defined as: ref LSRL = EyiY [ϕ(MRM(yi, Iθ(yi)))] (6) where = {yi}n image generated by the diffusion model with parameters θ corresponding to prompt y. is the prompt set, ϕ maps reward scores to per-sample loss values, and Iθ denotes the i=1 The final objective combines both losses: = LPre + λLSRL, λ = 0 before step S, λ = 1 thereafter. (7) As shown in Algorithm 1, we present the detailed SRL algorithm. The entire process comprises gradient-free inference followed by reward-backward step. Figure 5 Qualitative comparison with different methods on subject-driven generation. Figure 6 Qualitative comparison with different methods on style-driven generation. 9 Figure 7 Qualitative comparison with different methods on identity-driven generation. Figure 8 Qualitative comparison with different methods on style-subject-driven generation."
        },
        {
            "title": "4.1 Experiments Setting\nUSO Unified Benchmark. To enable a comprehensive evaluation, we introduce USO-Bench, a unified benchmark\nbuilt from 50 content images (20 human-centric, 30 object-centric) paired with 50 style references. We\nfurther craft 30 subject-driven prompts that span pose variation, descriptive stylization, and instructive\nstylization, along with 30 style-driven prompts. We generate four images per prompt for both subject-driven\nand style-driven tasks, and a single image for the combined style-subject-driven task. This yields 6000\nsamples for subject-driven generation, 7040 for style-driven generation, and 29500 for the combined task; full\nconstruction details are provided in the supplementary material.",
            "content": "Subject-driven generation Style-driven generation CLIP-I DINO CLIP-T CSD Model RealCustom++ [10] RealGeneral [18] UNO [34] BAGEL [3] OmniGen2 [32] FLUX.1 Kontext dev [15] Qwen-Image Edit [31] DEADiff [23] InstantStyle-XL [28] CSGO [37] StyleStudio [16] DreamO [20] USO (Ours) 0.314 0.485 0.605 0.516 0.475 0.579 0.544 - - - - 0.615 0.732 0.789 0.741 0.723 0.775 0.756 - - - - 0.303 0.275 0.264 0.298 0.302 0.287 0.302 - - - - 0.588 0.787 0.623 0. 0.280 0.288 - - - - - - - 0.462 0.540 0.452 0.348 0.454 0. CLIP-T - - - - - - - 0.274 0.276 0.272 0.282 0.278 0.282 Table 1 Quantitative results for subject-driven and style-driven generation on USO-Bench. Model StyleID [2] OmniStyle [29] CSD CLIP-T 0.407 0.365 0.230 0.229 USO (Ours) 0.495 0.283 Table 2 Quantitative results for style-subject-driven driven generation on USO-Bench. Evaluation Metrics. For quantitative evaluation, we assess each task along three dimensions: (1) subject consistency, measured by the cosine similarity of CLIP-I and DINO embeddings following [34]; (2) style similarity, reported via the CSD score [26] for both style-driven and style-subject-driven generation, following [37]; and (3) textimage alignment, evaluated with CLIP-T across all three tasks. Comparative Methods. As unified customization framework, USO is evaluated against both task-specific and unified baselines. For subject-driven generation, we benchmark RealCustom++ [19], RealGeneral [18], UNO [34], OmniGen2 [32], BAGEL [3], FLUX.1 Kontext dev [15], and Qwen-Image Edit [31]. For style-driven generation, we compare StyleStudio [16], DreamO [20], CSGO [37], InstantStyle [28], and DEADiff [23]. For the joint style-subject-driven setting with dual conditioning, we compare OmniStyle [29] and StyleID [2]. We also compared with InfiniteYou [13] to further demonstrate the positive effect of our proposed method on identity tasks."
        },
        {
            "title": "4.2 Experimental Results\nSubject-Driven Generation. As shown in Figure 5, the first two columns demonstrate that USO simultaneously\nsatisfies both descriptive and instructive style edits while maintaining high subject consistency. In contrast,\ncompeting methods either fail to apply the style or lose the subject. The last two columns further illustrate\nUSO’s strength in preserving human appearance and identity; it adheres strictly to the textual prompt and\nalmost perfectly retains facial and bodily features, whereas other approaches fall short. When the prompt is\n“The man is reading a book in a cafe”, FLUX.1 Kontext dev [15] achieves decent facial similarity but carries\ncopy-paste risks. In Figure 7 we compare with task-specific identity-preserving methods; USO produces\nmore realistic, non-plastic results with higher identity consistency. As reported in Table 1, USO significantly\noutperforms prior work, achieving the highest DINO and CLIP-I scores and a leading CLIP-T score.",
            "content": "Style-Driven Generation. Figure 6 shows that USO outperforms task-specific baselines in preserving the original style, including global color palettes and painterly brushwork. In the last two columns, given highly abstract references such as material textures or Pixar-style renderings, USO handles them almost flawlessly while prior methods struggle, demonstrating the generalization power of our cross-task co-disentanglement. Quantitatively, Table 1 confirms that USO achieves the highest CSD and CLIP-T scores among all style-driven approaches. Style-Subject-Driven Generation. As illustrated in Figure 8, we evaluate USO on both layout-preserved and layout-shifted scenarios. When the input prompt is empty, USO not only preserves the original layout of the content reference but also delivers the strongest style adherence. In the last two columns, under more complex prompt, USO simultaneously preserves the subject and identity consistency, matches the reference style, and aligns with the text, while other methods lag markedly and merely adhere to the text. Table 2 corroborates these observations, showing USO achieves the highest CSD and CLIP-T scores and substantially outperforms all baselines. User Study. We further conduct an online user-study questionnaire to compare state-of-the-art subject-driven and style-driven methods. Questionnaires were distributed to both domain experts and non-experts, who ranked the best results for each task. (1) Subject-driven tasks were evaluated on text fidelity, visual appeal, subject consistency, and overall quality. (2) Style-driven tasks were judged on text fidelity, visual appeal, style similarity, and overall quality. As shown in Figure 9, our USO achieves top performance on both tasks, validating the effectiveness of our cross-task co-disentanglement and showcasing its capability to deliver state-of-the-art results. Figure 9 Radar charts of user evaluation of methods for subject-driven and style-driven generation on different dimensions."
        },
        {
            "title": "5.1 Effect of Style Reward Learning (SRL).\nFor style-driven task: As shown in Figure 10, the last three columns reveal a clear boost in style similarity\nfor both style-driven and style-subject-driven tasks; the stroke textures and painting style closely match the\nreference images, confirming the effectiveness of our style reward learning.",
            "content": "For subject-driven task: In the first three and final columns of Figure 10, we observe notable improvement in subject and identity consistency, with more uniform details and higher facial similarity. As shown in Table 3, removing SRL leads to sharp drop in the CSD score and simultaneous declines in CLIP-I and CLIP-T. Notably, we rely solely on style reward and introduce no identity-specific data; nevertheless, the unified model benefits in content consistency. By sharpening the models ability to extract and retain desired features, SRL yields an overall improvement across all tasks, strongly validating our motivation. Beyond gains in subject and identity fidelity, we observe noticeable enhancement in aesthetic quality (e.g., texture as in VMix [33]) and marked reduction in the plastic artifact, which is an issue that has long challenged text-to-image generation [33]. Through SRL training, the model exhibits emerging properties even in tasks not explicitly targeted during training. Figure 10 Ablation study of SRL. The blue boxes denote content reference and the purple boxes denote style reference. Prompts are \"A toy with mountain in the background.\", \"The man on the beach.\", \"The woman is skateboarding on the street.\", \"A beautiful woman.\", \"A beautiful woman.\", \"The woman gave an impassioned speech on the podium.\" from left to right. Table 3 Ablation study of different components proposed in USO. Table 4 Ablation study of different projector in USO. Model Subject-driven Style-subject-driven CLIP-I CLIP-T CSD CLIP-T USO (Ours) w/o SRL w/o SAT w/o DE 0.623 0.620 0.621 0. 0.288 0.284 0.275 0.269 0.495 0.413 0.409 0.382 0.283 0.280 0.280 0.277 Model resampler (depth=1) resampler, unfreeze siglip mlp (depth=1) mlp, unfreeze siglip hierarchical projector 0.279 CSD CLIP-T 0.336 0.155 0.277 0. 0.288 0.284 0.288 0.284 0."
        },
        {
            "title": "5.2 Effect of Style Alignment Training (SAT).",
            "content": "Removing SAT and instead jointly fine-tuning both SigLIP and DiT from scratch degrades CLIP-T on subject-driven tasks and lowers CSD on style-subject-driven tasks ( Table 3). Qualitatively, Figure 11 shows 13 Figure 11 Ablation study of USO. Zoom in for details. that the oil-painting style of the cheetah example becomes noticeably weaker."
        },
        {
            "title": "5.3 Effect of Disentangled Encoder (DE).",
            "content": "Replacing the disentangled encoders with single VAE for both style and content images harms nearly every metric ( Table 3). Visually, the cheetah reverts to more photorealistic appearance, while the mans identity suffers marked loss ( Figure 11)."
        },
        {
            "title": "5.4 Effect of Hierarchical Projector.",
            "content": "Table 4 shows that the hierarchical projector yields the highest CSD and leading CLIP-T score, substantially benefiting style-alignment training."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present USO, unified framework capable of subject-driven, style-driven, and joint stylesubject-driven generation. We introduce cross-task co-disentanglement paradigm that first constructs systematic triplet-curation pipeline, then applies progressive style-alignment and contentstyle disentanglement training on the curated triplets. Additionally, we propose style-reward learning paradigm to further boost performance. To comprehensively evaluate our method, we construct USO-Bench, unified benchmark that provides both task-specific and joint evaluation for existing approaches. Finally, extensive experiments demonstrate that USO sets new state-of-the-art results on subject-driven, style-driven, and their joint style-subject-driven tasks, exhibiting superior subject consistency, style fidelity, and text controllability."
        },
        {
            "title": "References",
            "content": "[1] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William Cohen. Re-imagen: Retrieval-augmented text-toimage generator. arXiv preprint arXiv:2209.14491, 2022. [2] Jiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. Style injection in diffusion: training-free approach for adapting large-scale diffusion models for style transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 87958805, 2024. [3] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [4] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [5] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Cohen-Or. Implicit style-content separation using b-lora. In European Conference on Computer Vision, pages 181198. Springer, 2024. [6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [7] Junyao Gao, Yanchen Liu, Yanan Sun, Yinhao Tang, Yanhong Zeng, Kai Chen, and Cairong Zhao. Styleshot: snapshot on any style. arXiv preprint arXiv:2407.01414, 2024. [8] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [9] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. [10] Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom: narrowing real text word for real-time open-domain text-to-image customization. In CVPR, pages 74767485, 2024. [11] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. [12] Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, and Youngjung Uh. Visual style prompting with swapping self-attention. arXiv preprint arXiv:2402.12974, 2024. [13] Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, and Xin Lu. Infiniteyou: Flexible photo recrafting while preserving your identity. arXiv preprint arXiv:2503.16418, 2025. [14] Black Forest Labs. Flux: Official inference repository for flux.1 models, 2024. URL https://github.com/ black-forest-labs/flux. Accessed: 2025-02-07. [15] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. [16] Mingkun Lei, Xue Song, Beier Zhu, Hao Wang, and Chi Zhang. Stylestudio: Text-driven style transfer with selective control of style elements. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2344323452, 2025. [17] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36:3014630166, 2023. [18] Yijing Lin, Mengqi Huang, Shuhan Zhuang, and Zhendong Mao. Realgeneral: Unifying visual generation via temporal in-context learning with video models. arXiv preprint arXiv:2503.10406, 2025. [19] Zhendong Mao, Mengqi Huang, Fei Ding, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom++: Representing images as real-word for real-time customization. arXiv preprint arXiv:2408.09744, 2024. 15 [20] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. [21] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. URL https://openreview.net/forum?id=di52zR8xgf. [22] Senthil Purushwalkam, Akash Gokul, Shafiq Joty, and Nikhil Naik. Bootpig: Bootstrapping zero-shot personalized image generation capabilities in pretrained diffusion models. arXiv preprint arXiv:2401.13974, 2024. [23] Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, and Yongdong Zhang. Deadiff: An efficient stylization diffusion model with disentangled representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86938702, 2024. [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. [25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. [26] Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. arXiv preprint arXiv:2404.01292, 2024. [27] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 3, 2024. [28] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. [29] Ye Wang, Ruiqi Liu, Jiang Lin, Fei Liu, Zili Yi, Yilin Wang, and Rui Ma. Omnistyle: Filtering high quality style transfer data at scale. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 78477856, 2025. [30] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In CVPR, pages 1594315953, 2023. [31] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [32] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [33] Shaojin Wu, Fei Ding, Mengqi Huang, Wei Liu, and Qian He. Vmix: Improving text-to-image diffusion model with cross-attention mixing control. arXiv preprint arXiv:2412.20800, 2024. [34] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. [35] Zongze Wu, Yotam Nitzan, Eli Shechtman, and Dani Lischinski. Stylealign: Analysis and applications of aligned stylegan models. arXiv preprint arXiv:2110.11323, 2021. [36] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [37] Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, and Zechao Li. Csgo: Content-style composition in text-to-image generation. arXiv preprint arXiv:2408.16766, 2024. [38] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. [39] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. 16 [40] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [41] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In CVPR, pages 80698078, 2024. 17 USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning F.1 Experiments Setting F.1. Implementation Details."
        },
        {
            "title": "Appendix",
            "content": "We begin with FLUX.1 dev [14] and the SigLIP [40] pretrained model. For style alignment stage, we train ref , Itgt} for 23, 000 steps at batch size 16, learning rate 8e 5, resolution 768 and reward steps on pairs {I = 16, 000. For content-style disentanglement stage, we train on triplets {I ref , Itgt} for 21, 000 steps at batch size 64, learning rate 8e 5, resolution 1024 and reward steps = 18, 000. LoRA [8] rank 128 is used throughout. ref , Figure 12 Examples of USO-Bench. F.1.2 Details of USO-Bench. USO-Bench is built to evaluate subject-driven, style-driven, and joint style-subject-driven generation. As shown in Figure 12, each subject-driven sample uses three prompt types: descriptive, instructive-stylization, and descriptive-stylization. By pairing these prompts with style-reference images from style-driven tasks, we obtain style-subject-driven samples via their Cartesian product. The resulting prompts are further split into layout-shifted and layout-preserved variants. F.2 More Results F.2.1 Quantitative Evaluation on DreamBench [25]. To further assess USO, we evaluate it on DreamBench [25] in addition to USO-Bench. Following UNO [34], we generate six images per prompt, yielding 4,500 image groups across all subjects. As shown in Table 5, USO achieves the highest CLIP-I and DINO scores, and with CLIP-T score of 0.317, it trails the top result (0.318) by only narrow margin. These results demonstrate USOs superior subject consistency among state-of-the-art methods. 18 Method DINO CLIP-I CLIP-T Oracle(reference images) Textual Inversion [6] DreamBooth [25] BLIP-Diffusion [17] ELITE [30] Re-Imagen [1] BootPIG[22] SSR-Encoder[41] RealCustom++ [10, 19] OmniGen [36] OminiControl [27] FLUX.1 IP-Adapter UNO [34] USO (Ours) 0.774 0.569 0.668 0.670 0.647 0.600 0.674 0.612 0.702 0.693 0.684 0.582 0.760 0.777 0.885 0.780 0.803 0.805 0.772 0.740 0.797 0.821 0.794 0.801 0.799 0.820 0.835 0.838 - 0.255 0.305 0.302 0.296 0.270 0.311 0.308 0.318 0.315 0.312 0.288 0.304 0.317 Table 5 Quantitative results for single-subject driven generation on Dreambench [25]. Scenarios Subject/Identity Driven Generation Prompt (1) \"The girl is riding bike in the street.\" (2) \"The man is driving car in the street.\" (3) \"A sophisticated gentleman exuding confidence. He is dressed in 1990s brown plaid jacket with high collar, paired with dark grey turtleneck. His trousers are tailored and charcoal in color, complemented by sleek leather belt. The background showcases an elegant library with bookshelves, marble fireplace, and warm lighting, creating refined and cozy atmosphere. His relaxed posture and casual hand-in-pocket stance add to his composed and stylish demeanor\" (4) \"The woman is reading book in cafe.\" Subject/Identity Driven Stylization Style Driven Generation Multi-style Driven Generation Style-subject Driven Generation (Layout-preserved) Style-subject Driven Generation (Layout-shifted) (1) \"Sketch style, bowl with mountain in the background.\" (2) \"Illustration style, dog on the beach.\" (3) \"Transform to Picassos style of work, Cubism.\" (4) \"Ghibli style, The woman rides deer in the forest.\" (1)\"A shark.\" (2) \"Small boat in the lake.\" (3) \"A beautiful woman.\" (4) \"The top chef is stir-frying in the kitchen.\" (1) \"A beautiful woman.\" (2) \"A duck, with words read \"USO\", \"inspires creativity\".\" (3) \"A man.\" (1) \"\" (2) \"\" (3) \"\" (1) \"A toy in the jungle.\" (2) \"A cat on the beach.\" (3) \"The woman gave an impassioned speech on the podium.\" Table 6 Text prompts used in Figure 1. F.2.2 Additional Results. We present additional qualitative results from USO: From Figures 13 to 16, USO demonstrates the ability to extract task-relevant content features while maintaining subject consistency across diverse textual promptscapabilities that prior work typically treats as isolated tasks (e.g., subject-driven generation, instruction-based stylized editing, and identity preservation). In Figures 17 and 18, USO exhibits high stylistic fidelity, capturing both fine-grained characteristics (e.g., brushwork and material textures) and abstract artistic stylesfar beyond simple color transfer. In Figures 19 and 20, USO freely combines arbitrary subjects with arbitrary styles, supporting both layout-preserving and layout-shifting generations. Figure 13 More results on subject-driven generation. 20 Figure 14 More results on subject-driven generation. 21 Figure 15 More results on identity-driven generation. Figure 16 More results on identity-driven generation. 23 Figure 17 More results on style-driven generation. 24 Figure 18 More results on style-driven generation. Figure 19 More results on style-subject-driven generation. We set prompt to empty for layout-preserved generation. 26 Figure 20 More results on style-subject-driven generation. USO supports any subject combined with any style in any scenario."
        }
    ],
    "affiliations": [
        "UXO Team, Intelligent Creation Lab, ByteDance"
    ]
}