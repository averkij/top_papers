{
    "paper_title": "A Preliminary Study for GPT-4o on Image Restoration",
    "authors": [
        "Hao Yang",
        "Yan Yang",
        "Ruikun Zhang",
        "Liyuan Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an autoregressive architecture, has demonstrated unprecedented performance in image generation. In this work, we investigate its potential impact on the image restoration community. We present the first systematic evaluation of GPT-4o across diverse restoration tasks. Our experiments reveal that, although restoration outputs from GPT-4o are visually appealing, they often suffer from pixel-level structural fidelity when compared to ground-truth images. Common issues are variations in image proportions, shifts in object positions and quantities, and changes in viewpoint.To address it, taking image dehazing, derainning, and low-light enhancement as representative case studies, we show that GPT-4o's outputs can serve as powerful visual priors, substantially enhancing the performance of existing dehazing networks. It offers practical guidelines and a baseline framework to facilitate the integration of GPT-4o into future image restoration pipelines. We hope the study on GPT-4o image restoration will accelerate innovation in the broader field of image generation areas. To support further research, we will release GPT-4o-restored images from over 10 widely used image restoration datasets."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 1 2 6 5 0 . 5 0 5 2 : r Preliminary Study for GPT-4o on Image Restoration Hao Yang1, Yan Yang2, Ruikun Zhang1, Liyuan Pan1 1Beijing Institute of Technology, 2Australian National University hao.yang@bit.edu.cn Our dataset and code are publicly available at https://github.com/noxsine/GPT_Restoration"
        },
        {
            "title": "Abstract",
            "content": "OpenAIs GPT-4o model, integrating multi-modal inputs and outputs within an autoregressive architecture, has demonstrated unprecedented performance in image generation. In this work, we investigate its potential impact on the image restoration community. We present the first systematic evaluation of GPT-4o across diverse restoration tasks. Our experiments reveal that, although restoration outputs from GPT-4o are visually appealing, they often suffer from pixel-level structural fidelity when compared to ground-truth images. Common issues are variations in image proportions, shifts in object positions and quantities, and changes in viewpoint. To address it, taking image dehazing, derainning, and low-light enhancement as representative case studies, we show that GPT-4os outputs can serve as powerful visual priors, substantially enhancing the performance of existing dehazing networks. It offers practical guidelines and baseline framework to facilitate the integration of GPT-4o into future image restoration pipelines. We hope the study on GPT-4o image restoration will accelerate innovation in the broader field of image generation areas. To support further research, we will release GPT-4o-restored images from over 10 widely used image restoration datasets."
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models have made groundbreaking progress in visual generation [27, 46, 7, 42, 19]. Among them, OpenAIs GPT-4o [29] stands out for its ability to interpret complex visual and textual inputs to generate semantically accurate, visually realistic images. Meanwhile, image restoration can be naturally formulated as conditional image generation task [14, 38], where degraded images serve as visual conditioning inputs. By providing an appropriate prompt, GPT-4os generative capabilities can be directed toward image restoration. This capability represents major leap forward in multimodal generation and has prompted renewed consideration of its role in image restoration tasks (see Fig. 1). Traditionally, image restoration methods rely on degradation-specific network architectures designed to achieve high performance on individual tasks, such as image denoising [15, 30], image deblurring [28, 49, 2, 5, 45], image super-resolution [26, 10], image inpainting [41, 9], and image dehazing [40, 33, 4, 50]. While these methods are effective within their respective domains, they often lack flexibility and exhibit poor generalization across diverse degradation types. Although some recent efforts have explored unified, all-in-one frameworks capable of handling multiple restoration tasks within single model [44, 31, 6, 16], such approaches have yet to demonstrate scalability or consistent performance across varied restoration scenarios. Given its powerful visual generation and semantic understanding capabilities, GPT-4o naturally emerges as potential foundation model for all-in-one image restoration [16]. In this work, we conduct the first systematic investigation of GPT-4o in the context of image restoration, uncovering both its promising strengths and current limitations. Building on these insights, we further explore Preprint. Under review. (a) (b) (c) (d) Figure 1: Image restoration results of GPT-4o on real world degradation. The first row and second row are degraded inputs and the restored outputs, respectively. (a)-(d) correspond to low-light conditions, heavy noise, motion blur, and dense haze, respectively. simple baseline approach that leverages GPT-4o as plug-and-play component to enhance the performance of existing restoration networks. The study is organized into three parts. (i) Restoration Capability of GPT-4o: We evaluate GPT-4o on eight diverse image restoration tasks through both quantitative and qualitative analysis. While the restored images are visually appealing (as reflected by CLIP-IQA scores), they often suffer from lack of pixel-level structural fidelity, as indicated by lower PSNR scores even compared with the degraded image (e.g., 12.89 dB vs. 21.58 dB). (ii) Failure Cases: Although GPT-4o generally preserves overall image semantics, it often fails to maintain pixel-level structural fidelity. This is primarily due to three limitations: distortion of image proportions, inaccuracies in object positioning and quantity, and inconsistencies in viewpoint reconstruction, which are often critical for low-level image restoration tasks. (iii) Baseline: Although GPT-4o performs poorly in preserving pixel-level structural fidelity, its visually pleasing outputs can serve as strong visual priors. We propose lightweight post-processing baseline that leverages GPT-4os outputs to enhance the performance of image restorations."
        },
        {
            "title": "2 Restoration of Diverse Degradation",
            "content": "Fig. 2 and Fig. 3 present the restoration results of GPT-4o on eight representative types of realworld degradation. The degraded images are collected from datasets related to deraining [11, 37], desnowing [24], dehazing [50, 8], low-light enhancement [23, 25], motion deblurring [28, 32, 51], defocus deblurring [43, 20, 2], underwater image enhancement [36, 22, 13], and denoising [21, 1], as well as from web sources. For the real-world images with ground truth shown in Fig. 3, we report quantitative metrics including PSNR [12] and CLIP-IQA [35]. The first metric evaluates pixel-level structural fidelity, while the latter two assess perceptual quality. Overall, GPT-4o delivers visually compelling restorations across wide range of image restoration tasks, showcasing its versatility. For example, in deraining and desnowing, it effectively removes occlusions like rain streaks and snow buildup, restoring clean scenes with preserved fine details in trees, pedestrians, and vehicles. These results highlight GPT-4os potential not only in task-specific restoration but also as unified foundation model for general-purpose low-level vision restoration. However, as shown in Fig. 4, while GPT-4o achieves high CLIP-IQA scores (indicating strong perceptual quality), its PSNR values are often lower than even those of the degraded input. This reveals significant limitation: poor preservation of pixel-level structural fidelity, which is critical for many practical restoration applications."
        },
        {
            "title": "3 Failure Cases",
            "content": "We analyze several representative cases to further investigate the pixel-level structural fidelity issues present in GPT-4os restoration results. 2 Rain Snow Haze Low-Light Motion blur Defocus blur Underwater Noise Figure 2: Image restoration results of GPT-4o on real-world degraded images without ground truth. Each vertical pair shows degraded input image (top) and its corresponding restored output (bottom), with the type of degradation labeled beside each pair. Figure 3: Image restoration results of GPT-4o on real-world degraded images with available ground truth. Each triplet consists of the ground truth image, the degraded input, and the corresponding restored output, with the type of degradation labeled beside each set. We display the PSNR and CLIP-IQA scores below each image, reflecting perceptual quality and pixel-level structural fidelity, respectively. Variations in Image Proportions. As shown in the left part of Fig. 4, GPT-4o fails to preserve the original aspect ratio during restoration, leading to noticeable geometric distortions. Such inconsisten3 (a) (b) (c) Figure 4: Failure cases. (a) Variations in image proportions. (b) Shifts in object positions and quantities. (c) Changes in viewpoint. cies disrupt visual coherence and can be detrimental to downstream tasks that depend on accurate spatial representation. Shifts in Object Positions and Quantities. In the middle example of Fig. 4, GPT-4o exhibits poor control over object presence and placement. For instance, it inadvertently removes roadside tree, despite no instruction to modify the scene content. This highlights key challenge in maintaining structural and semantic consistency for low-level vision tasks within multimodal generation frameworks. Changes in Viewpoint. On the right side of Fig. 4, GPT-4o applies slight scaling and cropping, which alters the original camera viewpoint. As result, certain scene elements, such as swing set in the lower-left corner, are partially or entirely lost. Such viewpoint shifts can undermine restoration reliability, especially when precise scene reconstruction is required. While GPT-4o demonstrates impressive generative capabilities and generalization across diverse image restoration tasks, it exhibits notable limitations in maintaining geometric consistency, accurate object placement, and stable viewpoints for achieving high pixel-level structure fidelity. These shortcomings can be critical in applications where spatial precision is essential. Addressing them will be vital for advancing the reliability of multimodal models in low-level image restoration tasks."
        },
        {
            "title": "4 A Baseline Solution",
            "content": "We take image dehazing as test case and explore baseline network, as plug-in-and-play model, that post-processes GPT-4os restoration outputs to improve pixel-level structural fidelity. Implementation Details. We adopt Restormer [47] enhanced with Deformable Convolutions [52] as our post-processing network. The degraded image is fed into the model, and the restoration output from GPT-4o is fused at the feature level via element-wise addition. The prompt used to instruct GPT-4o for image restoration is: Please remove the {degradation type} from the image. The processed image should remain aligned with the input image. The network is trained using the charbonnier loss [17] for the O-Haze dataset [3] (40 training and 5 testing images), Rain800 dataset [48] (485 training and 15 testing images), and LOL [39] (700 training and 100 testing images). All experiments are conducted using NVIDIA RTX 4090 GPUs and implemented in PyTorch. Training is performed using the Adam optimizer [18] with an initial learning rate of 2 104, decayed via cosine annealing schedule. We use batch size of 2, and input images are randomly cropped into 256 256 patches. Standard data augmentation techniques, including random horizontal flipping and random rotation, are applied. The network is trained for total of 150,000 iterations. Results. We compare two baselines: (1) the direct restoration output from GPT-4o, and (2) standard Restormer model trained to restore directly from the degraded image. Qualitative and quantitative results are presented in Fig. 5 and Tab. 1, respectively. The proposed pipeline using GPT-4o outputs as visual priors achieves significantly higher scores in perceptual quality metrics (e.g., 0.566 in CLIP-IQA on the O-Haze dataset), indicating improved visual appeal. At the same time on the 4 Degraded Ground Truth GPT-4o Baseline Ours Figure 5: Comparisons on the Rain800 [48], LOL [39], and O-HAZE [3] datasets. Rows 12 show results on Rain800 dataset, Rows 34 are results on LOL dataset, and Rows 56 are for O-HAZE dataset. GPT-4o denotes the image restoration results generated by GPT-4o. Baseline refers to the restoration results without using GPT-4o priors, while Ours indicates the enhanced restoration results guided by GPT-4o priors. Table 1: PSNR, SSIM, and CLIP-IQA on O-Haze, Rain800, and LOL datasets. Method O-Haze Rain800 LOL PSNR SSIM CLIP-IQA PSNR SSIM CLIP-IQA PSNR SSIM CLIP-IQA GPT-4o [29] Baseline [47] Ours 13.13 20.86 22. 0.133 0.794 0.801 0.757 0.540 0.566 12.44 28.63 29.19 0.296 0.881 0.893 0.812 0.612 0.628 12.13 21.28 22. 0.387 0.807 0.831 0.706 0.470 0.495 O-Haze dataset, it achieves comparable performance in pixel-level structural metrics (e.g., 22.08 in PSNR), demonstrating that the enhancement in visual quality does not come at the expense of structural fidelity. We present visual comparison in Fig. 5. The first column shows the degraded hazy input images, the fourth column displays the restoration results from the baseline Restormer (without GPT-4o guidance), and the last column presents the outputs of our proposed method incorporating aligned GPT-4o priors. Across variety of challenging scenes, our method consistently produces clearer restorations with reduced haze artifacts compared to the baseline. For example, in the outdoor bench scene, our approach successfully recovers realistic color tones and fine details in the bricks and benches, whereas the baseline result appears desaturated and lacks contrast. Similarly, in the forest pathway scene, our method restores distant foliage and pathway textures with enhanced sharpness and color fidelity. Consistent improvements are also observed on deraining and low-light enhancement tasks, further demonstrating the effectiveness of our method. These improvements highlight the effectiveness of integrating GPT-4o-generated priors to enhance restoration quality."
        },
        {
            "title": "Degraded",
            "content": "Gemini 2.0 GPT-4o Figure 6: Comparison between GPT-4o and Gemini 2.0 on image restoration tasks."
        },
        {
            "title": "5 Discussion",
            "content": "We further compare GPT-4o with another state-of-the-art multimodal model, Gemini 2.0 [34], on image restoration in Fig. 6. GPT-4o consistently delivers clearer and more structurally faithful restorations than Gemini 2.0. Notably, GPT-4o better preserves fine details such as thin object boundaries and texture continuity, whereas Gemini 2.0 occasionally introduces artifacts or overly smooths subtle scene elements. These observations suggest that GPT-4o currently provides superior visual fidelity for restoration-oriented generation tasks. However, both models exhibit minor misalignments at the pixel level, reinforcing the need for alignment mechanisms when integrating generative priors into low-level vision pipelines. Furthermore, there is notable difference in computational efficiency: GPT-4o requires an average of 82 seconds per image, while Gemini 2.0 completes the same task in just 15 seconds. This highlights practical trade-off between restoration quality and inference speed that must be considered for real-world deployment."
        },
        {
            "title": "6 Conclusion",
            "content": "In this study, we present the first systematic evaluation of GPT-4o for image restoration across diverse degradations. While GPT-4o demonstrates impressive generative capabilities and produces visually appealing results, it often struggles with maintaining pixel-level structural fidelity manifested through issues such as geometric distortions, object misplacement, and viewpoint inconsistencies. We identify these limitations through detailed analysis and demonstrate that GPT-4o outputs can serve as strong visual priors to enhance restoration quality when paired with lightweight post-processing network. This approach effectively combines GPT-4os perceptual strengths with traditional restoration models, leading to improved structural fidelity without compromising visual appeal. We hope our findings 6 offer valuable insights into the effective use of large multimodal models for image restoration and inspire future research in this emerging direction."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael Brown. high-quality denoising dataset for smartphone cameras. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 16921700, 2018. [2] Abdullah Abuolaim and Michael Brown. Defocus deblurring using dual-pixel data. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 111126. Springer, 2020. [3] Codruta Ancuti, Cosmin Ancuti, Radu Timofte, and Christophe De Vleeschouwer. O-haze: dehazing benchmark with real hazy and haze-free outdoor images. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 754762, 2018. [4] Codruta Ancuti, Cosmin Ancuti, Mateu Sbert, and Radu Timofte. Dense-haze: benchmark for image dehazing with dense-haze and haze-free images. In 2019 IEEE international conference on image processing (ICIP), pages 10141018. IEEE, 2019. [5] Sunghyun Cho and Seungyong Lee. Fast motion deblurring. In ACM SIGGRAPH Asia 2009 papers, pages 18. 2009. [6] Yuning Cui, Syed Waqas Zamir, Salman Khan, Alois Knoll, Mubarak Shah, and Fahad Shahbaz Khan. Adair: Adaptive all-in-one image restoration via frequency mining and modulation. arXiv preprint arXiv:2403.14614, 2024. [7] Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. Enabling multimodal generation on clip via vision-language knowledge distillation. arXiv preprint arXiv:2203.06386, 2022. [8] Wei Dong, Han Zhou, Ruiyi Wang, Xiaohong Liu, Guangtao Zhai, and Jun Chen. Dehazedct: Towards effective non-homogeneous dehazing via deformable convolutional transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64056414, 2024. [9] Omar Elharrouss, Noor Almaadeed, Somaya Al-Maadeed, and Younes Akbari. Image inpainting: review. Neural Processing Letters, 51:20072028, 2020. [10] Sina Farsiu, Dirk Robinson, Michael Elad, and Peyman Milanfar. Fast and robust multiframe super resolution. IEEE transactions on image processing, 13(10):13271344, 2004. [11] Xueyang Fu, Borong Liang, Yue Huang, Xinghao Ding, and John Paisley. Lightweight pyramid networks for image deraining. IEEE transactions on neural networks and learning systems, 31(6):17941807, 2019. [12] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 23662369. IEEE, 2010. [13] Md Jahidul Islam, Youya Xia, and Junaed Sattar. Fast underwater image enhancement for improved visual perception. IEEE Robotics and Automation Letters, 5(2):32273234, 2020. [14] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 11251134, 2017. [15] Bo Jiang, Jinxing Li, Yao Lu, Qing Cai, Huaibo Song, and Guangming Lu. Eficient image denoising using deep learning: brief survey. Information Fusion, page 103013, 2025. [16] Junjun Jiang, Zengyuan Zuo, Gang Wu, Kui Jiang, and Xianming Liu. survey on all-in-one image restoration: Taxonomy, evaluation and future trends. arXiv preprint arXiv:2410.15067, 2024. [17] Kui Jiang, Zhongyuan Wang, Peng Yi, Chen Chen, Baojin Huang, Yimin Luo, Jiayi Ma, and Junjun Jiang. Multi-scale progressive fusion network for single image deraining. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83468355, 2020. [18] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [19] Ning Li, Jingran Zhang, and Justin Cui. Have we unified image generation and understanding yet? an empirical study of gpt-4os image generation ability. arXiv preprint arXiv:2504.08003, 2025. [20] Yu Li, Yaling Yi, Xinya Shu, Dongwei Ren, Qince Li, and Wangmeng Zuo. Learning dual-pixel alignment for defocus deblurring. Neurocomputing, 616:128880, 2025. [21] Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, and Liangqiong Qu. Residual denoising diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27732783, 2024. [22] Risheng Liu, Xin Fan, Ming Zhu, Minjun Hou, and Zhongxuan Luo. Real-world underwater enhancement: Challenges, benchmarks, and solutions under natural light. IEEE transactions on circuits and systems for video technology, 30(12):48614875, 2020. [23] Xiaoning Liu, Zongwei Wu, Ao Li, Florin-Alexandru Vasluianu, Yulun Zhang, Shuhang Gu, Le Zhang, Ce Zhu, Radu Timofte, Zhi Jin, et al. Ntire 2024 challenge on low light image enhancement: Methods and results. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65716594, 2024. [24] Yun-Fu Liu, Da-Wei Jaw, Shih-Chia Huang, and Jenq-Neng Hwang. Desnownet: Context-aware deep network for snow removal. IEEE Transactions on Image Processing, 27(6):30643073, 2018. [25] Yuen Peng Loh and Chee Seng Chan. Getting to know low-light images with the exclusively dark dataset. Computer Vision and Image Understanding, 178:3042, 2019. [26] Zhisheng Lu, Juncheng Li, Hong Liu, Chaoyan Huang, Linlin Zhang, and Tieyong Zeng. Transformer for single image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 457466, 2022. [27] Gary Marcus, Ernest Davis, and Scott Aaronson. very preliminary analysis of dall-e 2. arXiv preprint arXiv:2204.13807, 2022. [28] Seungjun Nah, Sanghyun Son, Suyoung Lee, Radu Timofte, Kyoung Mu Lee, Liangyu Chen, Jie Zhang, Xin Lu, Xiaojie Chu, Chengpeng Chen, et al. Ntire 2021 challenge on image deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 149165, 2021. [29] OpenAI. Gpt-4o technical report, 2024. [30] Tongyao Pang, Huan Zheng, Yuhui Quan, and Hui Ji. Recorrupted-to-recorrupted: Unsupervised deep learning for image denoising. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20432052, 2021. [31] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Shahbaz Khan. Promptir: Prompting for all-in-one image restoration. Advances in Neural Information Processing Systems, 36:7127571293, 2023. [32] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho. Real-world blur dataset for learning and benchmarking deblurring algorithms. In Computer visionECCV 2020: 16th European conference, glasgow, UK, August 2328, 2020, proceedings, part XXV 16, pages 184201. Springer, 2020. [33] Yuda Song, Zhuqing He, Hui Qian, and Xin Du. Vision transformers for single image dehazing. IEEE Transactions on Image Processing, 32:19271941, 2023. [34] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [35] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, pages 25552563, 2023. [36] Mingjie Wang, Keke Zhang, Hongan Wei, Weiling Chen, and Tiesong Zhao. Underwater image quality optimization: Researches, challenges, and future trends. Image and Vision Computing, page 104995, 2024. [37] Tianyu Wang, Xin Yang, Ke Xu, Shaozhe Chen, Qiang Zhang, and Rynson WH Lau. Spatial attentive single-image deraining with high quality real rain dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1227012279, 2019. [38] Xintao Wang, Yuchen Cui, Shuhang Gu, Chen Chen, and Chen Change Loy. Image restoration via conditional diffusion models. arXiv preprint arXiv:2111.13564, 2021. [39] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, 2018. 8 [40] Haiyan Wu, Yanyun Qu, Shaohui Lin, Jian Zhou, Ruizhi Qiao, Zhizhong Zhang, Yuan Xie, and Lizhuang Ma. Contrastive learning for compact single image dehazing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1055110560, 2021. [41] Hanyu Xiang, Qin Zou, Muhammad Ali Nawaz, Xianfeng Huang, Fan Zhang, and Hongkai Yu. Deep learning for image inpainting: survey. Pattern Recognition, 134:109046, 2023. [42] Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025. [43] Hao Yang, Liyuan Pan, Yan Yang, Richard Hartley, and Miaomiao Liu. Ldp: Language-driven dual-pixel image defocus deblurring network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2407824087, 2024. [44] Hao Yang, Liyuan Pan, Yan Yang, and Wei Liang. Language-driven all-in-one adverse weather removal. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24902 24912, 2024. [45] Yan Yang, Liyuan Pan, Liu Liu, and Miaomiao Liu. K3dn: Disparity-aware kernel estimation for dualpixel defocus deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1326313272, 2023. [46] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [47] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57285739, 2022. [48] He Zhang, Vishwanath Sindagi, and Vishal Patel. Image de-raining using conditional generative adversarial network. IEEE transactions on circuits and systems for video technology, 30(11):39433956, 2019. [49] Kaihao Zhang, Wenqi Ren, Wenhan Luo, Wei-Sheng Lai, Bj√∂rn Stenger, Ming-Hsuan Yang, and Hongdong Li. Deep image deblurring: survey. International Journal of Computer Vision, 130(9):21032130, 2022. [50] Ruikun Zhang, Hao Yang, Yan Yang, Ying Fu, and Liyuan Pan. Lmhaze: Intensity-aware image dehazing with large-scale multi-intensity real haze dataset. In Proceedings of the 6th ACM International Conference on Multimedia in Asia, pages 11, 2024. [51] Qian Zhao, Hao Yang, Dongming Zhou, and Jinde Cao. Rethinking image deblurring via cnn-transformer multiscale hybrid architecture. IEEE Transactions on Instrumentation and Measurement, 72:115, 2022. [52] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better results. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 93089316, 2019."
        }
    ],
    "affiliations": [
        "Australian National University",
        "Beijing Institute of Technology"
    ]
}