{
    "paper_title": "Sparsified State-Space Models are Efficient Highway Networks",
    "authors": [
        "Woomin Song",
        "Jihoon Tack",
        "Sangwoo Mo",
        "Seunghyuk Oh",
        "Jinwoo Shin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State-space models (SSMs) offer a promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose a simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, a hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose a novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences. Code is available at https://github.com/woominsong/Simba."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 8 9 6 0 2 . 5 0 5 2 : r Published in Transactions on Machine Learning Research (03/2025) Sparsified State-Space Models are Efficient Highway Networks Woomin Song Korea Advanced Institute of Science & Technology (KAIST) Jihoon Tack Korea Advanced Institute of Science & Technology (KAIST) Sangwoo Mo University of Michigan, Ann Arbor Seunghyuk Oh Korea Advanced Institute of Science & Technology (KAIST) Jinwoo Shin Korea Advanced Institute of Science & Technology (KAIST) Reviewed on OpenReview: https: // openreview. net/ forum? id= G1p0YwrX8X woomin.song@kaist.ac.kr jihoontack@kaist.ac.kr swmo@umich.edu seunghyukoh@kaist.ac.kr jinwoos@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "State-space models (SSMs) offer promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences. Code is available at https://github.com/woominsong/Simba."
        },
        {
            "title": "1 Introduction",
            "content": "State-space models (SSMs) (Gu et al., 2022b; Gu & Dao, 2023) offer promising architecture for sequence modeling, efficiently handling sequences using linear recurrence structures. Thanks to this efficiency, SSMs have shown potential as an alternative to Transformers (Vaswani et al., 2017), which use the self-attention mechanism, incurring high computational costs for long sequences. In particular, Mamba (Gu & Dao, 2023) has recently demonstrated that SSMs can scale up to billions of parameters and show comparable performance with Transformers in various domains (Zhu et al., 2024; Li et al., 2024a;b). After their success, numerous works have aimed to enhance SSMs and Mamba further. One popular approach involves hybrid models combining Transformers and SSMs (Lieber et al., 2024; Poli et al., 2024). This approach assists the models in retaining past information through the global memory of Transformers (Vardasbi et al., 1 Published in Transactions on Machine Learning Research (03/2025) (a) Conceptual illustration of Simba (b) Performance highlights Figure 1: Simba: hierarchical sparsification of SSMs via token pruning. (a) We found that tokens in state-space models (SSMs) are highly redundant, especially in the upper layers. Motivated by this observation, we propose hierarchically sparsifying pre-trained SSMs by progressively pruning tokens across layers. This results in models with trapezoidal shape, featuring sparse upper layers that act like highways, enhancing efficiency and information flow of the original SSM. (b) We highlight results comparing Simba-2.8b with Mamba and Pythia, all with the same number of FLOPS. We report the mean accuracy over 6 NLP benchmarks and perplexity on the PG-19 dataset with 2k context, following the setups in Section 4. Simba outperforms both models in accuracy and perplexity. 2023; Jelassi et al., 2024). However, this compromises the efficiency of SSMs by reintroducing expensive selfattention. Instead of sacrificing efficiency, we explore an alternative direction to improve SSMs by comparing models on fixed computational budget. Specifically, we investigate sparsification of large pre-trained SSMs, known to yield better models than training small ones from scratch (Frankle & Carbin, 20189). To this end, we first analyze the behavior of tokens in pre-trained SSMs. We observe that tokens in SSMs are highly redundant, as they are gradually updated over sequences. This redundancy is particularly noticeable in the upper layers. Furthermore, dense recurrence operations over the redundant tokens block the delivery of past information, potentially harming the contextual understanding of SSMs. Inspired by this, we propose Simba, simple yet effective method to sparsify SSMs through token pruning (i.e., it is training-free). Our core idea is to sparsify SSMs into hierarchical form, enforcing more sparsity in upper layers than in lower layers. As result, the upper layers behave as highways to transmit past information, enabling efficient inference and facilitating the information flow across long sequences. Figure 1a illustrates the visual concept of our approach, obtaining trapezoidal-shaped sparsified network. To implement this, we propose novel token pruning criterion for SSMs. Specifically, our score measures the global influence of each token on the final output by reformulating SSM equations to accumulate the effect from local recurrences. This approach can also be viewed as an SSM extension of attention-based token pruning criteria used for Transformers (Goyal et al., 2020). We found that our criterion outperforms intuitive baselines, such as uniform pruning of tokens with even intervals. Our experiments show that Simba, obtained by sparsifying Mamba without any fine-tuning, significantly outperforms Mamba using the same number of FLOPS in various tasks. For instance, Simba consistently achieves better FLOPS-accuracy curves on 6 NLP benchmarks, including Lambada (Paperno et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC-Challenge (Clark et al., 2018), ARCEasy (Clark et al., 2018), and WinoGrande (Sakaguchi et al., 2021). Here, Simba obtained from Mamba-2.8b performs on par with the original Mamba-2.8b, despite using similar FLOPS to Mamba-1.4b, as highlighted in Figure 1b. For example, it achieves an average accuracy of 62.5% for 6 downstream NLP tasks, improving 58.8% of Mamba-1.4b. 2 Published in Transactions on Machine Learning Research (03/2025) We also demonstrate the language modeling ability of Simba by measuring perplexity on the PG-19 dataset (Rae et al., 2019) across different context lengths. Like the NLP benchmarks, Simba achieves better perplexity than Mamba using the same number of FLOPS. More importantly, Simba performs robustly over long sequences exceeding the pre-trained context length, such as twice longer than the trained length, unlike Mamba, which significantly deteriorates with length extrapolation. This supports the idea that the highway structures in Simba facilitate long sequence modeling. We further investigate the effect of highways in Simba. Somewhat unexpectedly, we found that Simba performed better than its original unpruned Mamba in some of our experiments, potentially benefiting from the highways created at the upper layers. To further investigate the positive effect of highways, we examine the information flow across layers by assessing the influence of the sequence tokens on the final output. We observe that Mamba relies on tokens near the end across all layers, while Simba also focuses on earlier tokens at the upper layers, showcasing the role of highways."
        },
        {
            "title": "2 Related work",
            "content": "State-space models (SSMs) are powerful architecture for sequence modeling, integrating concepts from classic control theory (Kalman, 1960) with recurrent neural networks (Elman, 1990). The key idea of SSMs is to employ linear recurrence (Katharopoulos et al., 2020; Gu et al., 2022b;a; Mehta et al., 2023; Smith et al., 2023; Fu et al., 2023; Orvieto et al., 2023; Poli et al., 2023; Peng et al., 2023; Sun et al., 2023; De et al., 2024), enabling efficient parallel inference and effective training, unlike Transformers using self-attention (Vaswani et al., 2017), which requires quadratic computation over the sequence length. As result, SSMs have shown success in handling long sequences (Tay et al., 2021). Recently, Mamba (Gu & Dao, 2023) further scaled up SSMs through selection mechanism and hardware-aware algorithm, showing the potential of SSMs in challenging tasks such as language, audio, and video (Zhu et al., 2024; Li et al., 2024a;b). We aim to further improve Mamba through network sparsification, efficiently utilizing fixed computational budget. Sparsifying networks have been widely studied, primarily for training efficient models (Han et al., 2016). Most prior work focused on weight pruning, which removes unnecessary edges in weight matrices (Zhu & Gupta, 2017; Gale et al., 2019; Park et al., 2020; Lee et al., 2021). However, while weight pruning reduces model size, it does not enhance inference speed due to the batch computation nature of GPUs. To address this, structured pruning removes entire blocks at once, such as channels in CNNs (Li et al., 2017) or attention heads in Transformers (Michel et al., 2019). On the other hand, some works focused on the performance benefits of sparsified networks, suggesting that sparsifying large networks yields better models than training small models from scratch, known as lottery tickets (Frankle & Carbin, 20189; Li et al., 2020). Our work relates to structured pruning, as removing tokens speeds up computation and reduces memory usage, and to lottery tickets, as it also improves performance. Token pruning (or merging) is widely applied in Transformers to reduce heavy computation over long sequences (Goyal et al., 2020; Kim et al., 2022; Liu et al., 2022; Bolya et al., 2023; Ke et al., 2024; Shang et al., 2024). In particular, HOMER (Song et al., 2024) has shown that hierarchical token pruning not only reduces computation but also enhances long context understanding by condensing global information into sparse tokens in the upper layers. Our work shares similar spirit with HOMER but has notable differences. First, we explore token pruning for SSMs, unlike prior works focused on Transformers. To this end, we propose novel token pruning criterion based on the global importance of tokens to the final output, derived from reformulating SSM equations to accumulate local recurrences. Second, by targeting SSMs, our hierarchical pruning scheme offers novel interpretation of highway networks, connecting SSMs with classical recurrent networks with long-term memory. As result, our token pruning approach enhances both the inference efficiency and information flow of SSMs. Highway networks have been proposed to bypass information loss from dense computation through local residuals (He et al., 2016) or long skip connections (Srivastava et al., 2015; Zilly et al., 2017; Huang et al., 2017; Ronneberger et al., 2015). Highways, also termed long-term memory, were used as standard approach for sequence modeling before global computation methods like self-attention gained popularity (Hochreiter & Schmidhuber, 1997; Chung et al., 2014; Feichtenhofer et al., 2019). However, integrating highways with SSMs proved challenging due to the linear recurrence structure of SSMs, not easily combined with skip connections. 3 Published in Transactions on Machine Learning Research (03/2025) Instead of explicitly using such modules in SSM architectures, we introduce simple and effective way to integrate highways by pruning dense token connections from the upper layers. Tree RNNs have been explored for processing hierarchical data structures, such as word, sentence, and paragraph hierarchies in language (Hihi & Bengio, 1995; Wang et al., 2019). Despite aligning with human intuition, most methods were unsuccessful due to complex architectures, while simple linear sequence modeling demonstrates its power (Achiam et al., 2023). Our token pruning naturally incorporates this hierarchical structure into SSMs, while favoring the success and scalability of linear sequence modeling."
        },
        {
            "title": "3 Simba: Hierarchical sparsification for state-space models",
            "content": "In Section 3.1, we review the mathematical formula of state-space models (SSMs) and discuss observations on the token redundancy of Mamba. In Section 3.2, we describe our proposed hierarchical sparsification approach for SSMs and explain the token pruning criteria."
        },
        {
            "title": "3.1 Motivation: Hierarchy in SSM token redundancy",
            "content": "Structured state-space model (S4) (Gu et al., 2022b) is family of recently proposed SSMs. In its continuous form, the SSM updates the state h(t) using the input x(t) and produces the output y(t) according to Eq. (1) where A, B, and refer to the parameters of SSMs. It discretizes the update rules for discrete sequences, as shown in Eq. (2). h(t) = Ah(t) + Bx(t), y(t) = Ch(t) ht = Aht1 + Bxt, yt = Cht (1) (2) Mamba (Gu & Dao, 2023) further improves this formulation using an input selectivity mechanism, creating the matrices A, B, and dependent on the input xt. Thus, the state update equation can be rewritten as follows, where At, Bt, and Ct denote the input-dependent matrices created using xt. ht = Atht1 + Btxt, yt = Ctht (3) Hierarchy in token redundancy. As the state update depends solely on the input and the immediate previous states, the model must compress all previous information into the state of each token. Thus, the states of SSM tokens are likely highly redundant, as states at similar positions would compress similar set of information. To verify this, we visualize the token redundancy of Mamba in Figure 2 by measuring the cosine similarity between adjacent tokens across the network layers. Our analysis indicates that tokens in the upper layers tend to exhibit more redundancy than those in the lower layers. One possible explanation is that SSMs process information in hierarchical manner: lower layers focus on local information, while upper layers focus on global information. This aligns with findings from attention visualizations of Mamba models Ali et al. (2024), where lower layers emphasize diagonal elements in the attention map, while upper layers highlight lower-triangular elements. Since upper layer states encapsulate global information, states at similar positions contain similar information, resulting in higher redundancy. Figure 2: Token redundancy of SSMs has hierarchical structure. We measure the cosine similarity between adjacent tokens of the Mamba-2.8b model across layers, averaged over documents from the PG-19 test dataset. The tokens are highly redundant, especially in the upper layers. 4 Published in Transactions on Machine Learning Research (03/2025)"
        },
        {
            "title": "3.2 Hierarchical sparsification for SSMs",
            "content": "We introduce Simba, simple yet effective sparsification approach that can be directly applied to any pre-trained state-space models (SSMs) in plug-and-play manner. This introduces an efficient highway for effective sequential modeling. Based on prior motivation, Simba sparsifies the full SSM in hierarchical manner: it sparsifies redundant tokens in upper layers while preserving the local information captured in lower layers, guided by our novel token importance criteria. Hierarchical sparsification through token pruning. To achieve sparsity in SSMs in hierarchical manner, we propose token pruning at each layer with specified rate. Implementing this pruning technique at particular layer automatically decreases the computational burden on subsequent layers, as pruned tokens are no longer propagated to the next layer. Consequently, by sequentially applying token pruning from lower to upper layers, the pruning ratio consistently increases during propagation, thereby introducing hierarchical sparsification to SSMs. Token pruning criterion. We propose novel token importance score for SSMs to prune the least important token from each layer. To achieve this, we leverage the influence that each token has on the final tokens output. This calculation is straightforward due to the linear recurrence property of SSMs. Formally, for given token sequence (x1, , xT ) of length and the final token output yT of the layer, we estimate the influence of token xt at position by considering the updated output y(t) (obtained by removing xt from yT ), as follows: yT (t) := yT y(t) T = CT ! Ak Brxr Y ! Ak Brxr CT r=1 = CT k=r+1 ! Ak Btxt. r=1,r=t k=r+ (4) k=t+1 Here, we found that the influence measure yT (t) aggregated with the max pooling (or the ℓ2 norm) serves as an effective pruning criterion, denoted as s(t) := max(yT (t)); we use max pooling throughout the paper as it was slightly better than ℓ2 norm. Also, we empirically observed that excluding the bias term when computing Ak in fully connected layers was beneficial, as it potentially helps normalize token importance scores across different channels before applying max pooling, resulting in more consistent pruning decisions. Based on the proposed score s(t), we prune each layer by removing the tokens with the lowest scores to obtain sparsified SSM. The pruning process is performed per input sequence during inference and is based solely on token importance scores computed for that specific input. As result, no external pruning data is required to determine the pruning pattern. Pruning schedule. The pruning schedule aims to balance the trade-off between efficiency and performance. Upper layers, with their ability to model global context, are better equipped to identify important tokens. Conversely, pruning tokens at earlier layers offers more significant computational savings. Therefore, we propose linear pruning schedule, inspired by prior work on Transformers (Bolya et al., 2023), where the number of active tokens is linearly reduced across all layers, resulting in trapezoidal-shaped network after sparsification. Highways in sparsified SSMs. Long-term dependency is well-known challenge for recurrent models (Hochreiter & Schmidhuber, 1997). Dense recurrence operations tend to attenuate previous information, restricting the information flow across distant tokens. Our sparsification scheme addresses this issue by reducing the number of recurrence operations. As unimportant tokens are pruned during sparsification, upper layers can selectively focus on processing more important information without being burdened by dense recurrence operations on redundant tokens. This highway effect enables our pruning scheme to not only enhance inference efficiency but also facilitate the information flow across distant tokens (Section 4.3). Published in Transactions on Machine Learning Research (03/2025)"
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we demonstrate the performance of Simba on diverse tasks. In Section 4.1, we evaluate its performance on 6 NLP benchmarks, consistently showing superior performance compared to dense models with equivalent computational resources. In Section 4.2, we assess the language modeling ability of Simba by measuring perplexity conditioned on various context lengths. In Section 4.3, we further investigate the highway effects of Simba. Finally in Section 4.4, we perform ablation studies on token pruning criteria and different pruning ratios, along with simple fine-tuning experiment. Common setups and baselines. We primarily apply our sparsification method to pre-trained Mamba models of various scales. For our method, we implement linear pruning schedule that preserves 10% of the tokens at the final layer unless specified otherwise. The Simba models are generated by sparsifying Mamba models without any fine-tuning, following plug-and-play approach. We conduct performance comparisons of Simba against Mamba (Gu & Dao, 2023) and Pythia (Biderman et al., 2023) models that utilize similar amount of computation. In NLP tasks consisting of prompt and label, we exclusively apply sparsification to the prompts to ensure an accurate evaluation of the label logits. The final token of the prompt remains unpruned, as its output is utilized for computing the label logits. Moreover, to accommodate the task structure, the token importance score is computed with respect to the final token of the prompt. For additional details, refer to Appendix A. (a) Lambada (b) HellaSwag (c) PIQA (d) Arc-Easy (e) Arc-Challenge (f) WinoGrande Figure 3: Performance on NLP Benchmarks. We visualize the FLOPs-accuracy curve of Mamba, Pythia, and Simba models of various scales on 6 NLP benchmarks. Across all benchmarks, Simba consistently outperforms the baselines using the same number of FLOPs. 6 Published in Transactions on Machine Learning Research (03/2025)"
        },
        {
            "title": "4.1 NLP benchmarks",
            "content": "In this section, we assess the language understanding capability of Simba by evaluating six downstream NLP tasks. Specifically, we present the performance and computational efficiency of Simba on the Lambada (Paperno et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC-Challenge (Clark et al., 2018), ARC-Easy (Clark et al., 2018), and WinoGrande (Sakaguchi et al., 2021) benchmarks. Consistent with Mamba (Gu & Dao, 2023), we report accuracy normalized by sequence length for HellaSwag and ARC-Challenge, and accuracy for the other datasets. All evaluations use the LM evaluation harness from EleutherAI (Gao et al., 2021). We report evaluation accuracy and computational cost for each benchmark in Figure 3. As evident from the results, Simba provides the best accuracy-efficiency trade-off, consistently outperforming other models with the same number of FLOPs. This demonstrates that Simba can successfully make dense Mamba models sparse, advancing the frontier of the accuracy-efficiency trade-off. We provide the full results for all benchmarks in Table 4 of the Appendix. 4.2 Language modeling (a) Mamba-130m and Simba-370m (b) Mamba-370m and Simba-790m (c) Mamba-790m and Simba-1.4b (d) Mamba-1.4b and Simba-2.8b Figure 4: Language modeling ability. We measure the FLOPs-perplexity curves on the PG-19 test dataset. Simba models are compared against Mamba models that use similar computation. Simba not only outperforms Mamba with the same computation but also shows decreasing perplexity after its pre-trained context limit of 2k tokens. In this section, we evaluate the language modeling ability of Simba by measuring perplexity on long documents. Specifically, we measure the perplexity of short document snippets sampled from PG-19 dataset (Rae et al., 2019), conditioned on varying amounts of context. We keep the 100-token snippet fixed for all experiments to ensure that all perplexity measurements are done on the same set of tokens. We report perplexity values conditioned on varying contexts in Figure 4. The results show that Simba consistently shows improved perplexity with similar computation, outperforming the dense Mamba models. We provide the full results, including Pythia, in Table 4 of the Appendix. 7 Published in Transactions on Machine Learning Research (03/2025) Long context capability. An intriguing observation is that Simba, unlike Mamba models, exhibits decreasing perplexity even after surpassing its pre-trained context limit of 2k tokens. Context limits, defining the maximum number of tokens model can handle, often result in catastrophic performance drop if exceeded (Song et al., 2024). While SSMs are generally more resilient to this issue, the results indicate increasing perplexity for Mamba models with longer contexts, suggesting ineffective utilization of additional context. In contrast, Simba consistently demonstrates decreasing perplexity even with extended contexts, highlighting its adeptness at leveraging extra information. This benefit is likely attributed to the highways formed in the upper layers due to extensive sparsification. The performance drop with longer inputs typically results from distribution shift in the training data, as the model never saw the long input length during training. For dense models like Mamba, this shift affects all network layers. Conversely, sparse models like Simba process significantly fewer tokens in the upper layers, thanks to extensive sparsification. Consequently, the upper layers remain unaffected by distribution shifts in input lengths, processing inputs more effectively. In summary, our experiments demonstrate that Simba not only outperforms Mamba with the same computation but also exhibits superior long-context handling abilities, suggesting the benefits of highways. 4.3 Sparsified SSMs as highway networks In this section, we further investigate the highway effects of Simba. First, we identify some scenarios where Simba models perform better than the original dense models despite using fewer FLOPs, potentially benefiting from the highways. Second, we examine the information flow in the model, showing that highways assist in obtaining information from earlier tokens, unlike dense SSMs, which over-rely on later tokens. Table 1: Comparsion between same model scales. We compare Simba and Mamba on NLP benchmarks: Lambada (Lbd.), HellaSwag (HS), PIQA, Arc-Easy (Arc-E), Arc-Challenge (Arc-C), and WinoGrande (WG). We use moderate pruning ratio for Simba, leaving 70% of the tokens at the final layer. Bold denotes the best results, showing that Simba often improves Mamba while using fewer FLOPS. Model Scale Model Dim. FLOPs (x1e12) Lbd. acc. () HS acc. () PIQA acc. () Arc-E acc. () Arc-C acc. () WG acc. () Avg. acc. () Mamba Simba (ours) 130m 130m 768 768 Mamba Simba (ours) 370m 1024 370m 1024 Mamba Simba (ours) 790m 1536 790m 0.48 0.39 1.38 1.15 2.94 2.47 31.84 32.43 46.61 47.09 51.33 51. 34.88 35.05 46.46 46.69 55.12 54.93 64.36 64.42 69.59 69.64 72.47 72. 50.76 50.38 59.05 59.51 65.49 65.87 23.72 24.32 27.47 27.65 32.76 32. 49.80 49.88 55.80 55.49 57.54 57.22 42.56 42.75 50.83 51.01 55.78 55. Comparison under same model sizes. In the previous experiments, we mainly compare models with the same number of FLOPS. Here, we provide an additional comparison between the dense and sparse models that have the same scale. Specifically, we evaluate Simba models with more moderate pruning ratio, using linear pruning schedule with 70% of tokens remaining at the final layer. We benchmark Mamba and Simba models using the 6 NLP benchmarks, following the setup in Section 4.1. We provide the evaluation results in Table 1. Somewhat unexpectedly, we found that Simba models sometimes perform even better than the original model, indicating the possible benefits of highways created at the upper layers. The gain is more evident for smaller models, possibly because dense recurrence operations are more harmful to smaller state sizes, so highways provide more benefits. Highways facilitate information flow from early tokens. We investigate how information flows through the dense and sparse SSM layers. Specifically, we measure the influence of tokens at each position on the final token, using our token importance score in Eq. (4) but normalized to equalize the contribution of each input document, i.e., s(t)/yT 2. See Appendix A.2 for more details. 8 Published in Transactions on Machine Learning Research (03/2025) (a) Layer (b) Layer 13 (c) Layer 26 (d) Layer 38 (e) Layer 51 (f) Layer 64 Figure 5: Information flow across layers. We visualize the information flow using the normalized token influence score. We compare Mamba-2.8b and Simba-2.8b, averaging scores over the PG-19 test dataset samples. The information flow of Simba flattens at the upper layers, indicating better information flow from early tokens. We illustrate the results in Figure 5. Mamba demonstrates consistent information flow pattern across all layers, with tokens near the end exerting more significant influence on the final token. Simba displays similar trend in the lower layers. However, the slope flattens in the upper layers, indicating that they function as highways, facilitating the flow of information from early tokens. 4.4 Ablation study and analysis This section presents ablation studies on token pruning criteria and different pruning ratios. We also perform simple fine-tuning experiment to further improve the performance of Simba. Pruning criteria. We compare our proposed pruning criteria in Eq. (4) with two baselines: Random, which chooses tokens from random positions, and Uniform, which chooses tokens from evenly distributed intervals. We visualize the efficiency-performance trade-off of Mamba2.8b and Simba-2.8b models on the Arc-Challenge dataset in Figure 6. Random pruning significantly hurts performance, while Uniform pruning forms strong baseline, highlighting the necessity of proper token selection. Simba further improves upon Uniform pruning by considering token influence. 9 Figure 6: Ablation study on pruning criteria. Performance of sparsified Mamba-2.8b models on the Arc-Challenge dataset (Clark et al., 2018) evaluated using different pruning criteria across various pruning ratios. Our proposed token influence score in Eq. (4) performs the best, even remaining robust under severe sparsification. We also report the performance of the dense Mamba1.4b and Mamba-2.8b models for comparison. Published in Transactions on Machine Learning Research (03/2025) Table 2: Fine-tuning results. We compare the language modeling perplexity on the test split of the PG-19 dataset, measured with different context lengths. The fine-tuned Simba model consistently outperforms its training-free counterpart. Model Mamba Simba (training-free) Simba (fine-tuned) Mamba Scale 130m 370m 370m 370m FLOPs (x1e12) 0.48 0.68 0.68 1.38 Within Context 0.5k 1k 1.5k 2k 2.5k Extrapolation 3.5k 3k 2.943 2.727 2.723 2.917 2.686 2.678 2.907 2.662 2.658 2.899 2.650 2.645 2.899 2.643 2. 2.900 2.633 2.630 2.901 2.630 2.626 4k 2.905 2.625 2.621 2.645 2. 2.607 2.600 2.599 2.601 2.604 2. Pruning ratio. We compare the performance of Simba using different sparsity levels, with linear pruning schedules leaving 90%, 80%, 70%, 60%, 50%, 30%, and 10% of tokens at the final layer. Figure 6 presents the performance curves for different pruning criteria. Simba is robust to extreme sparsity, retaining performance even when pruning 90% of tokens at the final layer. Fine-tuning. Although our method can be applied to pre-trained SSMs without training, we investigate if the performance can be improved with further fine-tuning. To this end, we perform simple fine-tuning experiment, further training the Mamba-370m model with MiniPile dataset (Kaddour, 2023), which is subset of the pre-training dataset (the Pile (Gao et al., 2020)) used for training Mamba. We provide the detailed training configurations in Appendix A.3. Following the setup in Section 4.2, we evaluate the language modeling perplexity of 100 tokens using the PG-19 dataset, conditioned on varying amounts of context. We report the results in Table 2. For all context lengths, the fine-tuned Simba model consistently outperforms the training-free Simba model, suggesting that fine-tuning can further improve the performance of sparsified SSMs."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose Simba, which sparsifies pre-trained SSMs into hierarchical form through token pruning. Simba outperforms Mamba with the same number of FLOPS in both accuracy on downstream NLP benchmarks and language modeling perplexity. Additionally, our pruning scheme creates highways in the upper layers, enhancing length extrapolation for long sequences and facilitating the information flow across distant tokens. We hope Simba inspires broad community, including state-space models, sparse and efficient networks, and classic recurrent networks with highways. Limitations. Our paper mainly focused on applying Simba to the pre-trained Mamba without adjustment. However, token pruning incurs distribution shifts from the original models, and further fine-tuning could reduce this misalignment. We demonstrate that simple fine-tuning can improve the performance of small models within fixed computational budget in Section 4.4. However, more sophisticated fine-tuning scheme tailored for sparse SSMs could be investigated. Also, while sparsifying large model provides high performance at reduced inference-time computation, preparing large model would require higher pre-training cost compared to using smaller, dense model. Broader Impact Statement Our paper studies sequence models, with broad applications such as language, audio, and video generation. As our method enhances the efficiency and efficacy of these models, it holds the potential to impact broader audience in generative AI. Hence, users of our method and sequence models should carefully read and follow the guidance from the community (Bai et al., 2022)."
        },
        {
            "title": "6 Acknowledgements",
            "content": "This work was conducted by Center for Applied Research in Artificial Intelligence(CARAI) grant funded by Defense Acquisition Program Administration(DAPA) and Agency for Defense Development(ADD) (UD230017TD), and partially supported by Institute for Information & communications Technology Pro10 Published in Transactions on Machine Learning Research (03/2025) motion(IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190075 Artificial Intelligence Graduate School Program (KAIST); RS-2022-II220959, Few-shot Learning of Causal Inference in Vision and Language for Decision Making)."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. arXiv preprint arXiv:2403.01590, 2024. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https://huggingface.co/spaces/ HuggingFaceH4/open_llm_leaderboard, 2023. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, 2023. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. International Conference on Learning Representations, 2023. Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. Jeffrey Elman. Finding structure in time. Cognitive science, 14(2):179211, 1990. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In IEEE International Conference on Computer Vision, 2019. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. International Conference on Learning Representations, 20189. Daniel Fu, Tri Dao, Khaled Saab, Armin Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models. In International Conference on Learning Representations, 2023. Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019. 11 Published in Transactions on Machine Learning Research (03/2025) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. framework for few-shot language model evaluation. Version v0. 0.1. Sept, pp. 8, 2021. Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakaravarthy, Yogish Sabharwal, and Ashish Verma. Power-bert: Accelerating bert inference via progressive word-vector elimination. In International Conference on Machine Learning, 2020. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022a. Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022b. Song Han, Huizi Mao, and William Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. International Conference on Learning Representations, 2016. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016. Salah Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. Advances in Neural Information Processing Systems, 1995. Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Weinberger. Densely connected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. Samy Jelassi, David Brandfonbrener, Sham Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. arXiv preprint arXiv:2402.01032, 2024. Jean Kaddour. The minipile challenge for data-efficient language models. arXiv preprint arXiv:2304.08442, 2023. Rudolph Emil Kalman. new approach to linear filtering and prediction problems. 1960. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, 2020. Tsung-Wei Ke, Sangwoo Mo, and Yu Stella. Learning hierarchical image segmentation for recognition and by recognition. In International Conference on Learning Representations, 2024. Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin. Layer-adaptive sparsity for the magnitude-based pruning. International Conference on Learning Representations, 2021. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. International Conference on Learning Representations, 2017. 12 Published in Transactions on Machine Learning Research (03/2025) Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model for efficient video understanding. arXiv preprint arXiv:2403.06977, 2024a. Shufan Li, Harkanwar Singh, and Aditya Grover. Mamba-nd: Selective state space modeling for multidimensional data. arXiv preprint arXiv:2402.05892, 2024b. Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez. Train big, then compress: Rethinking model size for efficient training and inference of transformers. In International Conference on Machine Learning, 2020. Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. arXiv preprint arXiv:2202.07800, 2022. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. Xiangcheng Liu, Tianyi Wu, and Guodong Guo. Adaptive sparse vit: Towards learnable adaptive token pruning by fully exploiting self-attention. arXiv preprint arXiv:2209.13802, 2022. Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In International Conference on Learning Representations, 2023. Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in Neural Information Processing Systems, 2019. Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, 2023. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. Sejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin. Lookahead: far-sighted alternative of magnitudebased pruning. International Conference on Learning Representations, 2020. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, 2023. Michael Poli, Armin Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. Jack Rae, Anna Potapenko, Siddhant Jayakumar, Chloe Hillier, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/ 1911.05507. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer Assisted Intervention, 2015. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. 13 Published in Transactions on Machine Learning Research (03/2025) Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In International Conference on Learning Representations, 2023. Woomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, and Jinwoo Shin. Hierarchical context merging: Better long context understanding for pre-trained llms. International Conference on Learning Representations, 2024. Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: benchmark for efficient transformers. In International Conference on Learning Representations, 2021. Alexandre Torres-Leguet. mamba.py: simple, hackable and efficient mamba implementation in pure pytorch and mlx., 2024. URL https://github.com/alxndrTL/mamba.py. Ali Vardasbi, Telmo Pessoa Pires, Robin Schmidt, and Stephan Peitz. State spaces arent enough: Machine translation needs attention. arXiv preprint arXiv:2304.12776, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Yau-Shian Wang, Hung-Yi Lee, and Yun-Nung Chen. Tree transformer: Integrating tree structures into self-attention. Conference on Empirical Methods in Natural Language Processing, 2019. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017. Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutnık, and Jürgen Schmidhuber. Recurrent highway networks. In International Conference on Machine Learning, 2017. 14 Published in Transactions on Machine Learning Research (03/2025)"
        },
        {
            "title": "A Detailed setups",
            "content": "A.1 NLP benchmarks Here, we provide the details for downstream experiments in Section 4.1. The six benchmarks were chosen according to the experiment setup of (Gu & Dao, 2023). The metrics are also selected accordingly, where we measure answer perplexity and accuracy for Lambada, accuracy for PIQA, ARC-Easy, and Winogrande, and normalized accuracy for HellaSwag and Arc-Challenge. All measurements use the LM evaluation harness from EleutherAI (Gao et al., 2021). Following the widely adopted practice (Beeching et al., 2023), we evaluate the models downstream performance using few-shot prompts. Following the setup in the open LLM leaderboard (Beeching et al., 2023), we evaluate the model with 10-shot prompts for HellaSwag, 25-shot prompts for ARC-Easy and Arc-Challenge, and 5-shot prompts for WinoGrande. For benchmarks not considered in the leaderboard, we evaluate the models with 5-shot prompts. For the visualization in Figure 3, we measure FLOPs separately for each benchmark and each model. First, we measure each benchmarks mean prompt and answer length, as shown in Table 3. Then, we measure FLOPs by forwarding an input that matches the mean lengths. Table 3: Downstream task details. We provide the details of our downstream evaluations in Section 4.1, including the number of few-shot prompts, average prompt lengths, average answer lengths, and metrics used for evaluation. Dataset Few-shot Prompt Length Answer Length Prompts (avg.) (avg.) Lambada (Paperno et al., 2016) HellaSwag (Zellers et al., 2019) PIQA (Bisk et al., 2020) ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) WinoGrande (Sakaguchi et al., 2021) 5 10 5 25 25 5 507.44 877.25 229.53 913.81 913.81 143. 1.47 29.86 22.82 5.00 5.00 5.67 Metrics ppl./ acc. acc_norm acc. acc. acc_norm acc. A.2 Information flow visualization Here, we provide the details for our information flow visualization experiments in Figure 5. We measure the normalized token influence score s(t)/yT 2 across all documents from the PG-19 test set, truncated at 1000 tokens. For all samples, we gather the influence score into five bins according to the position of the tokens. We report the average influence scores for each bin. A.3 Fine-tuning with Simba We provide the details for our fine-tuning experiments in Section 4.4. Loss design. The language modeling loss cannot be directly applied to sparsified models because the output logit shape does not match the label shape due to pruning. We apply the language modeling loss only to the available output logits, which are trained to predict the next token in the input sequence. We also add the standard language modeling loss computed using dense forwarding for stable training. Training. We train the model for 400 steps on the MiniPile dataset (Kaddour, 2023), subset of the Pile dataset (Gao et al., 2020) with similar data distribution. We use AdamW optimizer with learning rate of 5e-5. We schedule the learning rate with linear warmup for 10% of the total training steps and cosine learning rate decay for the remaining steps. We randomly select the pruning ratio between 0% and 90% for each sample. 15 Published in Transactions on Machine Learning Research (03/2025)"
        },
        {
            "title": "B Additional results",
            "content": "The tables below present the full values reported in our experiments. B.1 Detailed results for downstream evaluations Table 4: The full results table corresponding to Figure 3. Bold denotes the best results. Model Scale Simba (ours) 130m Pythia Mamba Simba (ours) Pythia Mamba Simba (ours) Pythia Mamba Simba (ours) Pythia Mamba Simba (ours) Pythia Mamba 160m 130m 370m 410m 370m 790m 1b 790m 1.4b 1.4b 1.4b 2.8b 2.8b 2.8b FLOPs (x1e12) Lbd. perp. () Lbd. acc. () HS acc. () PIQA acc. () Arc-E acc. () Arc-C acc. () WG acc. () Avg. acc. () 0.23 0.66 0.53 0.75 1.86 1.51 1.67 4.27 3.24 2. 6.19 5.60 6.07 12.21 11.31 31.63 > 102 35.81 16.79 25.65 12.34 10.10 16.01 9.44 8. 10.35 7.31 6.29 7.69 5.80 36.28 22.21 31.84 45.10 38.22 46.61 51.84 42.27 51.33 55. 49.46 56.43 60.06 54.93 60.85 35.29 30.42 34.88 46.72 40.95 46.46 54.82 47.92 55.12 58. 53.54 59.60 65.70 60.85 66.73 64.64 62.46 64.36 69.59 68.06 69.59 72.25 71.44 72.47 73. 70.89 74.05 75.46 74.37 76.12 49.12 45.29 50.76 58.84 56.73 59.05 65.40 60.65 65.49 65. 65.32 66.50 71.21 68.10 71.97 24.66 22.95 23.72 28.07 26.45 27.47 32.42 29.61 32.76 34. 33.02 36.18 38.99 36.26 39.93 49.96 50.36 49.80 55.41 53.51 55.80 58.17 52.88 57.54 58. 57.38 60.14 63.61 60.93 63.69 43.32 38.95 42.56 50.62 47.32 50.83 55.82 50.80 55.78 57. 54.94 58.82 62.51 59.24 63.21 B.2 Detailed results for perplexity evaluations Table 5: The full results table corresponding to Figure 4. Bold denotes the best results. Model Scale Simba (ours) 130m Pythia Mamba Simba (ours) Pythia Mamba Simba (ours) Pythia Mamba Simba (ours) Pythia Mamba Simba (ours) Pythia Mamba 160m 130m 370m 410m 370m 790m 1b 790m 1.4b 1.4b 1.4b 2.8b 2.8b 2.8b FLOPs (x1e12) Within Context 0.5k 1k 1.5k 2k 2.5k Extrapolation 3.5k 3k 4k 0.21 0.60 0.48 0. 1.69 1.38 1.52 3.88 2.94 2.68 5.63 5.09 5.52 11.11 10.28 3.060 3. 2.986 2.974 2.961 2.956 2.948 2. 3.166 2.943 2.727 2.750 2.645 2.543 2.607 2.486 2.495 2.513 2.389 2.380 2.382 2.281 3.134 2.917 2. 2.713 2.616 2.509 2.573 2.457 2.466 2.474 2.363 2.330 2.345 2.254 3.128 2.907 2.662 2.702 2.607 2. 2.560 2.445 2.443 2.465 2.354 2.317 2.332 2.242 3.120 2.899 2.650 2.691 2.600 2.477 2.559 2.437 2. 2.456 2.348 2.304 2.326 2.235 3.195 2.899 2.643 2.923 2.599 2.474 5.665 2.436 2.413 3.003 2.347 2. 6.689 2.236 6.333 2.900 2.633 7.611 2.601 2.470 6.582 2.443 2.404 6.696 2.352 2.289 7.275 2. 7.883 2.901 2.630 8.918 2.604 2.468 6.817 2.458 2.396 7.321 2.358 2.279 7.266 2.314 7.986 2.905 2. 9.314 2.617 2.471 7.003 2.497 2.394 7.775 2.372 2.284 7.244 2.547 16 Published in Transactions on Machine Learning Research (03/2025)"
        },
        {
            "title": "C Additional information",
            "content": "C.1 Compute resources All experiments are done on RTX-3090 or RTX-2080 GPUs. All FLOPs reported in the paper indicate the computation required for running single forward pass, so the number of data samples must be multiplied to calculate the amount of computation required for the full experiments. We further acknowledge that the full research project required more computing than the experiments reported in the paper, including the preliminary experiments and failed experiments. C.2 License for existing assets Here, we provide the license for all datasets and models used in our experiments. Apache 2.0 license is applied for the pretrained Mamba models, Pythia models, PG-19 dataset, and WinoGrande dataset. CC BY 4.0 license is applied for the Lambada dataset. CC BY-SA 4.0 license is applied for ARC-Challenge and ARC-Easy datasets. MIT license is applied for the PIQA dataset, HellaSwag dataset, and MiniPile dataset."
        },
        {
            "title": "D Further Discussion and Analysis",
            "content": "D.1 Inference speed analysis Throughout the paper, we have used FLOPs as the primary metric for evaluating efficiency. In this section, we present actual inference time measurements to demonstrate that Simba achieves tangible real-world speedups. Specifically, we compare the inference time of Simba and Mamba models at two scales: 790M and 2.8B parameters. To ensure fair comparison, we base our evaluation on the PyTorch implementation of Mambas parallel scan operation (Torres-Leguet, 2024), with optimizations to eliminate redundant operations involved in score computation. These modifications allow us to measure the relative gains provided by Simba, and we expect comparable improvements with hardware-optimized implementations, given appropriate optimizations. The results are summarized in Table 6, which reports the average inference time for both Mamba and Simba across various context lengths. Table 6: Inference time for various input lengths. All measurements are taken on 8 RTX 2080 Ti GPUs. We also report the percentage of the speedup. Model Scale 1k Context Length 2k 3k 4k Mamba Simba Mamba Simba 790m 790m 0.598 (14.2% speedup) 0.683 1.356 1.115 (21.6% speedup) 2.438 1.724 (41.4% speedup) 2.709 2.164 (25.2% speedup) 2.8b 2.8b 1.608 1.070 (50.3% speedup) 3.294 2.041 (61.4% speedup) 5.819 3.217 (80.9% speedup) 6.469 4.023 (60.8% speedup) The results indicate that Simba delivers substantial real-world efficiency improvements, achieving up to an 80% speedup for Mamba-2.8B models. We believe that these gains could be further enhanced by leveraging existing techniques, such as performing token pruning at specific layers rather than at every layer (Liang et al., 2022). This represents promising direction for future exploration. D.2 Application to chunked forwarding While originally designed for standard prefill scenarios where all tokens are forwarded at once, Simba can be seamlessly adapted to chunked prefill scenarios. In this context, long input sequences are divided into fixed-size chunks for sequential processing. To maintain computational efficiency, token pruning can be performed locally within each chunk. 17 Published in Transactions on Machine Learning Research (03/2025) This can be done by calculating token eviction scores based on the influence of tokens on the last token of each chunk, instead of the last token of the entire input sequence. This localized scoring approach ensures that pruning decisions remain contextually relevant within each chunk, while still preserving the overall efficiency benefits of sparsification. By adopting this method, Simba can achieve similar efficiency gains in chunked prefill scenarios as in standard prefill settings. D.3 Analysis for performance improvement under same scale Table 7: Performance on downstream tasks. We evaluate Simba and Mamba models on 6 NLP benchmarks (Lambada, HellaSwag, PIQA, Arc-Easy, Arc-Challenge, and WinoGrande) and report the averaged accuracy. We also report the performance difference between the two approaches with the same model scale. State size corresponds the size of the state tensor corresponding to single token, at single SSM layer. All experiment setup follows Table 1. Model Scale 130m 370m 790m 1.4b State Size 24k 32k 48k 64k Mamba Simba Difference 42.56 50.83 55.78 55.80 51.01 42.75 +0.19 +0.18 +0.02 58.82 58.21 -0.60 2.8b 80k 63.21 62.86 -0.35 While token pruning is often associated with decrease in performance compared to dense models, we observed that moderate pruning can enhance performance under certain conditions in Section 4.3. In this section, we conduct further analysis on this phenomenon. In Table 7, we compare the averaged accuracy of Mamba and Simba models across 6 NLP benchmarks considered throughout the paper. Noticably, the performance gains are more evident for smaller models with smaller state sizes. We hypothesize that this occurs because smaller state sizes have limited capacity to retain information and are more prone to forgetting in dense recurrence operations. By introducing highway-like connections through our sparsification method in the upper layers, Simba could mitigate these issues by improving the information flow across distant tokens. While these results are promising, we believe that further exploration in this direction will be essential to fully understand the underlying mechanisms and to optimize the use of token pruning across broader range of settings. D.4 Comparison with Transformers Transformers excel at modeling long-range dependencies, as their attention mechanism gives each token direct access to all previous tokens. However, this capability comes at high computational cost, with complexity growing quadratically with input length. In contrast, SSMs process sequences with linear complexity by compressing all prior information into single state and updating it sequentially. While efficient, this design restricts information flow, as tokens can only access the immediate previous state. Over time, dense recurrence operations degrade earlier information, leading to loss of long-range dependencies. Simba mitigates the information flow issue in SSMs by sparsifying tokens in upper layers, reducing the number of recurrence steps. This approach alleviates information degradation while preserving the computational efficiency of SSMs. By balancing efficiency and improved long-range information flow, Simba narrows the gap between SSMs and Transformers. 18 Published in Transactions on Machine Learning Research (03/2025) D.5 Potential failure cases One potential failure case for Simba arises in scenarios where tokens containing essential information are pruned, leading to loss of critical context. While this issue would be less severe in comparison to Transformerbased models due to the inherently compressed nature of SSMs, where information from pruned tokens is retained in the states of subsequent tokens, it may still impact the models ability to effectively utilize that information. This limitation could result in performance degradation compared to the dense model, particularly in tasks requiring precise retention of long-range dependencies or complex reasoning."
        }
    ],
    "affiliations": [
        "Korea Advanced Institute of Science & Technology (KAIST)",
        "University of Michigan, Ann Arbor"
    ]
}