{
    "paper_title": "Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decoder",
    "authors": [
        "Jingchao Wang",
        "Zhijian Wu",
        "Dingjiang Huang",
        "Yefeng Zheng",
        "Hong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reference Expression Segmentation (RES) aims to segment image regions specified by referring expressions and has become popular with the rise of multimodal large models (MLLMs). While MLLMs excel in semantic understanding, their token-generation paradigm struggles with pixel-level dense prediction. Existing RES methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight pipelines that sacrifice accuracy. To address the trade-off between performance and cost, we specifically propose MLLMSeg, a novel framework that fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. Besides, we propose a detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully integrates the detail-related visual feature with the semantic-related feature output by the large language model (LLM) of MLLM. Finally, we establish a light-weight mask decoder with only 34M network parameters that optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. Extensive experiments demonstrate that our method generally surpasses both SAM-based and SAM-free competitors, striking a better balance between performance and cost. Code is available at https://github.com/jcwang0602/MLLMSeg."
        },
        {
            "title": "Start",
            "content": "Unlocking the Potential of MLLMs in Referring Expression Segmentation via Light-weight Mask Decoder Jingchao Wang1*, Zhijian Wu3*, Dingjiang Huang1, Yefeng Zheng3, Hong Wang2 1School of Data Science and Engineering, East China Normal University 2School of Life Science and Technology, Xian Jiaotong University 3Medical Artificial Intelligence Laboratory, Westlake University jcwang@stu.ecnu.edu.cn, wuzhijian@westlake.edu.cn, djhuang@dase.ecnu.edu.cn, zhengyefeng@westlake.edu.cn, hongwang01@xjtu.edu.cn 5 2 0 2 7 ] . [ 2 7 0 1 4 0 . 8 0 5 2 : r Abstract Reference Expression Segmentation (RES) aims to segment image regions specified by referring expressions and has become popular with the rise of multimodal large models (MLLMs). While MLLMs excel in semantic understanding, their token-generation paradigm struggles with pixellevel dense prediction. Existing RES methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight pipelines that sacrifice accuracy. To address the trade-off between performance and cost, we specifically propose MLLMSeg, novel framework that fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. Besides, we propose detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully integrates the detailrelated visual feature with the semantic-related feature output by the large language model (LLM) of MLLM. Finally, we establish light-weight mask decoder with only 34M network parameters that optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. Extensive experiments demonstrate that our method generally surpasses both SAM-based and SAM-free competitors, striking better balance between performance and cost. Code is available at https://github.com/jcwang0602/MLLMSeg."
        },
        {
            "title": "Introduction",
            "content": "Referring Expression Segmentation (RES) lies at the intersection of computer vision and natural language processing (Wang et al. 2025a). Given an input image, the goal of the RES task is to segment an object or region designated by natural language expression. Multimodal large language models (MLLMs) have recently demonstrated strong capabilities in understanding and reasoning across diverse modalities, particularly vision and language. Benefiting from large-scale pretraining and instruction tuning, MLLMs excel at various tasks, such as visual question answering, captioning, referring expression comprehension, and reasoning-based multimodal interaction. However, despite their strong capability in semantic *These authors contributed equally. Corresponding author Figure 1: Performance of MLLMSeg on Referring Expression Segmentation (RES), showing consistent improvement over state-of-the-art methods. understanding, MLLMs inherently operate in token generation paradigm, which is well-suited for producing text sequences but not for generating dense and fine-grained pixellevel predictions (Ren et al. 2024). Recently, in order to adapt MLLMs to the RES task for pixel-wise mask prediction, researchers have proposed two main paradigms: Segment Anything Model (SAM) (Kirillov et al. 2023)-based and SAM-free pipelines. Specifically, SAM-based methods generally utilize MLLMs to generate prompts, which are then fed to the SAM mask decoder for pixel-level prediction. For instance, LISA (Lai et al. 2024) utilized MLLMs to generate prompt tokens for SAM mask generation, and GSVA (Xia et al. 2024) introduced multiple [SEG] tokens and [REJ] token to deal with the Generalized Referring Expression Segmentation (GRES) task for multi-object segmentation and empty target rejection. Although achieving promising performance, the introduction of SAM would inevitably bring many network parameters, causing large storage cost (i.e., 632M). To alleviate this issue, the SAM-free pipeline has emerged. For example, in UFO (Tang et al. 2025), the authors reformulated the segmentation procedure as embedding retrieval by calculating the dot-product similarity between mask tokens and image features extracted by MLLMs. Text4Seg (Lan et al. 2025) introduced new text-as-mask paradigm that casts image segmentation as text generation problem, eliminating the need for additional decoders. Although these SAM-free approaches avoid introducing additional network parameters, the insufficient feature propagation generally causes limited performance improvement (refer to Fig. 1). To strike better balance between performance and cost, in this paper, we specifically propose novel approach, called MLLMSeg, in order to finely unleash the potential of MLLMs in addressing the RES task. Specifically, we carefully analyze that the visual feature extracted by the visual encoder of MLLM inherently contains rich detail information, which are useful for the fine-grained segmentation prediction. Inspired by this analysis, we propose to directly utilize the shallow detail-related visual feature without introducing an extra visual encoder, like SAM visual encoder in LISA and GSVA. Then we construct detail enhancement and semantically consistent feature fusion module (DSFF) as well as flexible upsampling mechanism, which fully integrates the detail-related visual feature with the semanticrelated feature output by the large language model (LLM) of MLLM. Finally, we establish light-weight mask decoder with only 34M network parameters, which optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. As shown in Figure 1, our MLLMSeg achieves comprehensive performance superiority over contemporary methods under identical training conditions, achieving better trade-off between performance and cost. Our main contributions are three-fold: We specifically propose novel framework, called MLLMSeg, for adapting MLLM to deal with the referring expression segmentation task. The main characteristic of our proposed method lies in that it does not require the introduction of large-scale, pre-trained basic segmentation model like SAM, but only uses light-weight mask decoder to achieve comparable performance. We propose Detail-enhanced and Semantically consistent Feature Fusion module (DSFF), that can efficiently inject the shallow detail-related visual feature extracted by the visual encoder of MLLM into the deep semanticrelated feature extracted by the LLM of MLLM, and then boost the accurate mask prediction. Comprehensive experimental results demonstrate that our proposed MLLMSeg can always achieve the average SOTA performance across multiple tasks, including Referring Expression Segmentation (RES), Referring Expression Comprehension (REC), and Generalized Referring Expression Segmentation (GRES)"
        },
        {
            "title": "2.1 Referring Expression Segmentation",
            "content": "Referring Expression Segmentation (RES) is popular task at the intersection of computer vision and natural language processing, aiming at segmenting the corresponding target object or region in an image based on given natural language description (Referring Expression). CEFNet(Feng et al. 2021) proposes an encoder fusion network that transforms the visual encoder into multimodal feature learning network and utilizes language to progressively refine these multimodal features. LST(Ya Jing 2021) tackles referring image segmentation by decoupling it into referring object prior prediction and fine mask generation, achieving superior performance through explicit position prior modeling. In order to improve cross-modal alignment, LAVT(Yang et al. 2022) introducing early fusion of linguistic and visual features within the intermediate layers of vision Transformer encoder, enabling stronger multi-modal context modeling and more accurate segmentation without relying on heavy cross-modal decoders."
        },
        {
            "title": "Segmentation",
            "content": "Referring Expression Segmentation (RES) has garnered significant attention in recent years. However, most traditional approaches impose strong pre-defined constraints on the task, often excluding expressions that refer to no target or multiple targets. To address these limitations, Chang Liu et al.(Liu, Ding, and Jiang 2023) introduced novel task, Generalized Referring Expression Segmentation (GRES), along with large-scale benchmark dataset, gRefCOCO. They also proposed baseline method, ReLA, which explicitly models the relationships between different image regions and linguistic components. Leveraging the strong inference capabilities of MLLMs, GSVA(Xia et al. 2024) introduces multiple [SEG] tokens and novel [REJ] token, achieving effective multi-object segmentation as well as the rejection of empty targets."
        },
        {
            "title": "2.3 Multi-modal Large Language Models",
            "content": "MLLMs (Zhang et al. 2025) have made significant progress by aligning powerful visual encoders with Large Language Models (LLMs). However, most existing MLLMs are limited to text-only outputs, which restricts their applicability in more diverse and complex multimodal scenarios. LISA(Lai et al. 2024) introduces SEG tokens to leverage the reasoning abilities of MLLMs and address the challenge of segmentation ambiguity. However, LISA struggles with the unique challenges posed by Generalized Referring Expression Segmentation (GRES), primarily due to the rigidity of its SEG token design. To overcome this limitation, GSVA(Xia et al. 2024) proposes more flexible approach by introducing multiple SEG tokens along with novel REJ token, enabling effective multi-object segmentation and the rejection of expressions with no corresponding target. While these methods achieve impressive results with powerful SAMs, they also incur significant parameters. Figure 2: Overview of the proposed MLLMSeg, where the detailed structure of light-weight mask decoder is shown in Figure 3."
        },
        {
            "title": "3.1 Model Architecture\nFigure 2 illustrates the entire network architecture of our\nproposed MLLMSeg, which mainly consists of a multi-\nmodal large language model (MLLM) and a light-weight\nmask decoder. Here the MLLM is composed of a vision en-\ncoder and a large language model, which is utilized to ex-\ntract visual features and generate proxy tokens as [SEG] and\n[REJ]. The proposed light-weight mask decoder is used to\ngenerate segmentation masks based on visual features and\nproxy tokens. Please note that we follow GSVA (Xia et al.\n2024) and adopt multiple [SEG] tokens for multi-target seg-\nmentation and a [REJ] token for empty target rejection.\nHowever, unlike GSVA which is built on the heavy-weight\nSAM, we carefully design a light-weight masked decoder\nwhich is jointly trained with multi-task language models\n(MLLMs) to achieve the accurate segmentation prediction.",
            "content": "Multimodal Large Language Model. Given an input imimg R4484483 and the text description 0 age 0 text that specifies the target to be segmented, the MLLM first feeds them to visual encoder Fen and tokenizer, respectively, expressed as: img = Reshape(Fen(T 0 1 text = Tokenizer(T 0 1 text), img)), (1) where Reshape() means converting the 2-dimensional spatial dimensions of the visual features into one-dimensional sequence, which is consistent with the shape of the text sequence. 1 text are the detail-related visual tokens embedding and the text tokens, respectively. img and 1 Taking the concatenation of the visual token 1 text as the input sequence llm img, and the text token 1 input, and then feeding it into the large language model (LLM) Fllm, through the auto-regessive processing, we can get the final output token llm output. The process can be formulated as: img, 1 input), input = Concat(T 1 llm output = Fllm(T llm llm img, 2 [T 2 text] = Split(T llm output), text), (2) (3) (4) img and the semantic-related text token where Concat() denotes the concatenation procedure along the length dimension of the token sequence and Split() denotes dividing the output llm output of the MLLM into two parts for the subsequent processing, i.e., the semantic-related visual token 2 text. 2 text contains several [SEG] Tokens and [REJ] Tokens. Following GSVA, several special tokens, [SEG] and [REJ], are appended to the vocabulary to provide MLLM segmentation and identify empty object capabilities. MLLM learns to predict the [SEG] in the output sequence to represent the target to be segmented and [REJ] to represent the target specified in the user instruction that is not in the image. After the processing by MLLM, the image token 1 extracted by the visual encoder Fen, the image token 2 img output by the LLM Fllm, and the special token [SEG] Tseg are fed into the subsequent light-weight mask decoder for segmentation prediction. img Light-weight Mask Decoder. To adapt MLLMs to the RES task, the existing work, such as, LISA and GSVA, directly adopts the SAM for segmentation prediction, causing are helpful for fine-grained segmentation, we specifically design Semantic-consistent and Detail-enhanced Feature Fusion (DSFF) module to fuse the detail-related visual tokens 1 img and the compressed semantic-related visual tokens 3 img, and then we can obtain the refined visual feature 0 ds as: ds = DSFF(T 1 img, 3 img). (6) where DSFF() will be described in detail in Section 3.2. Attributed to our DSFF, 0 ds is rich in details and consistent with the semantics of natural language expression. The merit will be validated in Figure 4 and Table 4 below. In order to inject the fused visual feature 0 mentation token 1 introduce cross-attention mechanism, formulated as: ds into the segseg for boosting the mask prediction, we 1 ds = ϕout(Softmax( QK seg), = ϕV (T 1 ds), = ϕK(T 1 )V ), = ϕQ(T 0 seg), (7) where ϕQ(), ϕK(), and ϕV () are linear projection operations for obtaining the query, key, and value, respectively. ϕout() represents linear projection layer. Then by subsequently passing 1 ds to series of operations, we can get the final mask Mout, expressed as: Mout = ϕk5(PixelShuffle(ϕk3(T 1 ds))), (8) where PixelShuffle() denotes the pixel shuffle layer (Shi et al. 2016), ϕk3() denotes convolutional layer with kernel size 3 3, and ϕk5() denotes convolutional layer with kernel size 5 5. Please note that in the visual encoder Fen, the size of the original image is reduced from 448 448 to 32 32, and visual token represents 14 14 patch."
        },
        {
            "title": "Feature Fusion",
            "content": "In this subsection, we will provide the detailed design about the network module DSFF() in Eq. (6). Considering that the shallow detail-related visual feature 1 img is independently extracted by the visual encoder, to enhance features pertinent to the users prompt, we fuse visual detail and semantic features via cross-attention, using 1 as the Query and 3 img as the Key and Value, as: img Tvl = ϕvl(Softmax( QK )V ), (9) = ϕQ(T 1 img), = ϕK(T 3 img), = ϕV (T 3 img). As validated in Figure 4, the shallow feature 1 img presents more details, but is not related to the text description, due to the non-directional nature of the visual encoder during feature extraction. Additionally, the feature 3 img output by the LLM lacks fine-grained details, and the abstract semantic makes visual interpretation challenging. However, after the cross-attention fusion procedure, the modelled feature Tvl is highly relevant to the users prompt, which also validates the role of the introduction of 1 img. Figure 3: Overview of the light-weight mask decoder with detail-enhanced and semantic-consistent feature fusion module (DSFF). Figure 4: Visualization of 1 img from the visual encoder, 2 img from the large language model, and Tvl from the crossattention of 1 img and 2 img. large cost. To alleviate this issue, we carefully design brand-new and novel light-weight mask decoder with Detailenhanced and Semantic-consistent Feature Fusion module (DSFF), which optimally leverages detailed spatial features from the visual encoder and semantic features from the large language model to achieve precise mask prediction. Specifically, for the light-weight design, as shown in Figimg and the special seg are first compressed through linure 3, the semantic-related visual token 2 segmentation token 0 ear projection, expressed as: img = ϕ1(T 2 3 img), 1 seg = ϕ2(T 0 seg), (5) img, 1 where 3 seg, and ϕ() denotes linear layer for reducing the number of channels of features, increasing the information density, and reducing the computational complexity. Inspired by the fact that the shallow visual feature 1 img extracted by the visual encoder contains rich details which Methods LISA(ft)(Lai et al. 2024) GSVA(ft)(Xia et al. 2024) AnyRef(ft)(He et al. 2024) PixelLM(Ren et al. 2024) VisionLLM v2 (Wu et al. 2024) Text4SegInternVL2(Lan et al. 2025) Text4SegInternVL2(Lan et al. 2025) M²SA(Jang et al. 2025) SegAgentLLaVA(Zhu et al. 2025) SegAgentQwen (Zhu et al. 2025) UFOInternVL2(Tang et al. 2025) VRS-HQ (Gong et al. 2025) MLLMSegInternVL2 (Ours) MLLMSegInternVL2.5 (Ours) w/ SAM RefCOCO RefCOCO+ Val 74.9 77.2 76.9 73.0 76.6 74.7 79.2 74.0 79.2 78.0 78.0 73.5 79.2 81.0 TestA TestB 72.3 79.1 73.5 78.9 74.2 79.9 68.2 76.5 74.3 79.3 71.6 77.4 75.6 81.7 69.7 76.8 75.7 81.4 75.0 80.3 75.6 79.7 69.5 77.5 77.5 81.0 78.7 82.4 Val 65.1 65.9 70.3 66.3 64.5 68.5 72.8 63.1 71.5 70.9 72.3 61.7 73.9 76.4 TestA TestB 58.1 70.8 59.8 69.6 61.8 73.5 58.3 71.7 61.5 69.8 62.9 73.6 66.5 77.9 56.1 67.2 65.4 76.7 65.8 75.5 66.6 76.8 54.3 67.6 68.6 77.6 72.5 79. RefCOCOg Test Val 70.6 67.9 73.3 72.7 70.7 70.0 70.5 69.3 71.2 70.7 71.6 70.7 75.3 74.0 68.3 67.0 74.9 74.8 74.6 74.5 74.3 73.7 67.5 66.7 78.6 77.3 80.8 79.9 Avg. 69.9 71.4 72.2 69.2 71.0 71.4 75.4 67.8 75.0 74.3 74.6 67.3 76.7 78.9 Table 1: Referring Expression Segmentation results (cIoU) on RefCOCO (+/g) datasets. The models we compared are all MLLMs (8B) based methods and the best results are marked in bold. Concurrently, we expect to further unleash the power of 1 img and enable the detail-related information 1 img to further boost the segmentation. To this end, apart from the cross-attention operation, we spatially upsample 3 img from 16 16 to 32 32 to make its size align with the size of 1 img for the concatenation fusion. Specifically, inspired by DySample (Liu et al. 2023), we first feed 3 img with the size of 1024 16 16 to linear layer and pixel shuffling layer in order to generate an offset tensor R(24)1616, where the value 2 in the first dimension denotes the and coordinate offsets, as: = PixelShuffle(ϕ(T img)), (10) In order to avoid aliasing and overlapping between neighboring sampling points, we apply fixed scope factor α = 0.25 to constrain the offset magnitude: = + α O, (11) where is the original sampling grid. The high-resolution output 4 img is obtained by sampling the input 3 img at the dynamically predicted positions using the built-in differentiable grid sample function in PyTorch: img = grid sample(T 3 4 img, S), (12) In order to retain as much valid information as possible, we concatenate 1 img, and Tvl in the channel dimension, and then compress the channels using linear layer to obtain 0 ds in Eq. (6), as: img, 4 ds = ϕ(Concat(T 1 img, 4 img, Tvl)). (13) Remark: As seen, through the cross-attention mechanism and the upsampled-based alignment procedure, we fully inject the shallow detail-related feature extracted by the visual encoder into the output of LLM. It is this way of fully utilizing features that makes it possible to design lightweight mask decoder. Therefore, even if we do not adopt heavy SAM, our method is still expected to achieve satisfactory performance, which will be verified by experiments."
        },
        {
            "title": "3.3 Training Objectives\nThe model is trained end-to-end using the text generation\nloss Ltext and the segmentation loss Lmask, defined as:",
            "content": "Ltext = CE(T llm gt ), Lmask = BCE(Mout, Mgt) + DICE(Mout, Mgt), output, llm (14) where CE is Cross-Entropy, BCE is per-pixel binary (Lai et al. 2024), DICE is the cross-entropy (BCE) DICE (Rezatofighi et al. 2019), llm and Mgt are the gt ground-truth. The total training loss is: = λtextLtext + λmaskLmask, (15) where λtext = 1.0 and λmask = 1.0."
        },
        {
            "title": "4.1 Settings\nDatasets. Follow previous works (Lan et al. 2025), we\ntrain on RefCOCO series (Kazemzadeh et al. 2014; Mao\net al. 2016) for RES and REC tasks, the train split of gRe-\nfCOCO(Liu, Ding, and Jiang 2023) for the GRES task.\nCrucially, unlike prior works (e.g., LISA (Lai et al. 2024),\nGSVA (Xia et al. 2024)) that require extensive mixed-dataset\npretraining followed by fine-tuning on RefCOCO-family\ndatasets, our approach achieves state-of-the-art performance\nwithout any pretraining phase.",
            "content": "Evaluation Metrics. For RES, we use cIoU (Xia et al. 2024) as the evaluation metric, which computes the overall intersection over union across the dataset. For REC, as in previous work (Lai et al. 2024), we consider the prediction correct if IoU is greater than 0.5 (Prec@0.5). For GRES, we use both gIoU and cIoU metrics. In line with the setup from GRES (Liu, Ding, and Jiang 2023), and gIoU measures the mean IoU per instance. To handle expressions that refer to no target, we employ No-target-accuracy (N-acc.)(Liu, Ding, and Jiang 2023), defined as the proportion of correctly Methods Qwen2-VL(Wang et al. 2024) InternVL2(Chen et al. 2024c) LISA(ft)(Lai et al. 2024) GSVA(ft)(Xia et al. 2024) PixelLM(Ren et al. 2024) VisionLLM v2 (Wu et al. 2024) InternVL2.5(Chen et al. 2024b) Qwen2.5-VL(Bai et al. 2025) Text4SegInternVL2(Lan et al. 2025) Text4SegInternVL2(Lan et al. 2025) UFOInternVL2(Tang et al. 2025) HiMTokInternVL2.5(Wang et al. 2025b) MLLMSegInternVL2 (Ours) MLLMSegInternVL2.5 (Ours) w/ SAM RefCOCO RefCOCO+ Val 91.7 87.1 85.4 86.3 89.8 87.9 90.3 90.0 88.3 90.3 91.4 92.9 92.0 93.5 TestA TestB 87.3 93.6 80.7 91.1 82.6 88.8 83.8 89.2 86.4 92.2 84.3 91.2 85.9 94.5 85.4 92.5 85.8 91.4 87.5 93.4 88.2 93.8 89.3 94.7 89.0 94.0 90.3 95.0 Val 85.8 79.8 74.2 72.8 83.2 77.6 85.2 84.2 83.5 85.2 85.7 87.6 87.4 89.4 TestA TestB 79.5 90.5 71.4 87.9 68.4 79.5 68.0 78.8 78.9 87.0 70.2 83.8 78.8 91.5 76.9 89.1 77.9 88.2 79.5 89.9 79.7 90.7 81.5 91.5 81.8 90.7 85.2 92.3 RefCOCOg Test Val 87.8 87.3 82.7 82.7 80.4 79.3 81.8 81.6 86.0 84.6 84.1 82.9 87.6 86.7 87.2 87.2 82.5 82.4 85.4 85.4 87.4 86.8 89.0 88.5 88.9 89.0 91.3 91.1 Avg. 87.9 82.9 79.8 80.3 86.0 82.8 87.6 86.6 85.0 87.1 88.0 89.4 89.1 91.0 Table 2: Referring Expression Comprehension results (Prec@0.5) on RefCOCO (+/g) datasets. The models we compared are all MLLMs (8B) based methods and the best results are marked in bold. Methods LISA(Lai et al. 2024) LISA(ft)(Lai et al. 2024) GSVA(Xia et al. 2024) GSVA(ft)(Xia et al. 2024) LaSagnA* (Wei et al. 2024) PSALM*(Zhang et al. 2024) SAM4MLLM(Chen et al. 2024a) Text4SegInternVL2(Lan et al. 2025) Text4SegInternVL2(Lan et al. 2025) MLLMSegInternVL2 (Ours) MLLMSegInternVL2.5 (Ours) w/ SAM Validation Set gIoU cIoU N-acc 2.71 38.7 32.2 54.7 61.8 61.6 56.5 61.7 63.3 62.4 63.3 66.5 - 38.1 32.4 - 42.0 43.3 - 67.8 71.9 - 65.6 71.8 - 69.1 74.4 71.1 70.0 73.2 73.2 71.6 75. Test Set gIoU cIoU N-acc 6.37 52.6 48.5 50.0 68.5 66.3 63.5 69.2 70.1 65.3 69.9 71.1 - 50.4 47.3 - 52.4 54.5 - 72.2 74.2 - 70.0 71.2 - 73.8 75.1 68.5 75.4 75.2 72.4 76.9 77.0 Test Set gIoU cIoU N-acc 44.8 39.7 60.6 58.8 60.3 61.3 60.5 62.2 42.1 38.9 50.6 52.5 63.4 65.3 62.5 64.2 66.6 67.3 67.1 67.9 68.5 69.7 5.0 51.9 58.4 60.6 - - - - - 63.1 65.5 Table 3: Generalized Referring Expression Segmentation results on the gRefCOCO dataset. * indicates zero-shot performance. The models we compared are all MLLMs (8B) based methods, and the best results are marked in bold. identified empty-target cases. In such cases, correct prediction is assigned gIoU of 1.0 and is excluded from cIoU computation; incorrect predictions receive gIoU of 0.0 and contribute to the union in cIoU. Implementation Details. In this paper, we use InternVL28B (Chen et al. 2024c) and InternVL2.5-8B (Chen et al. 2024b) as our foundation models. In training, we use batch size of 32, running on 4 NVIDIA A100 GPUs for 50k iterations. The multimodal large language model we use is InternVL-8B. The AdamW optimizer and cosine annealing schedule are employed, with learning rate of 4e-5. For efficient training, we employ LoRA with rank of 64, freezing the visual encoder and LLM while keeping only the lora parameter of LLM and the mask decoder trainable. For RES tasks, the entire training process takes about 20 hours. After RES, we continued to fine-tune the model on the GRES task for 20k iterations, which took roughly 8 hours. Further experimental details can be found in Appendix A."
        },
        {
            "title": "4.2 Main Results",
            "content": "Referring Expression Segmentation. To comprehensively evaluate MLLMSegs capabilities, we conduct rigorous benchmarking on standard RES tasks using RefCOCO, RefCOCO+, and RefCOCOg datasets, following established protocols: UNC (Kazemzadeh et al. 2014) partitions for RefCOCO/RefCOCO+ and UMD (Mao et al. 2016) split for RefCOCOg. As shown in Table 1, MLLMSegInternVL2 achieves significant performance gainsoutperforming Text4SegInternVL2 with SAM by 1.3% cIoU and surpassing SAM-free UFOInternVL2(only use features from the LLM output of MLLM for mask prediction) by 2.1% cIoU. Furthermore, our MLLMSegInternVL2.5 variant establishes state-of-the-art leadership across all RES benchmarks. Appendix shows more qualitative results. Expression Comprehension. Table Referring 2 shown the REC results of MLLMSeg. Compared to Text4SegInternVL2 with SAM, MLLMSegInternVL2 achieved 3.9% improvement in Prec@0.5. Compared to HiMTok based on InternVL2.5 (Chen et al. 2024b), MLLMImage Features Feature Mask Decoder Fusion #Params RefCOCO RefCOCO+ Val TestA TestB Val TestA TestB RefCOCOg Test Val Avg. img 1 2 img 1 img, 2 img, 2 1 img img img 1 1 img img, 2 1 img, 2 1 img img None None Concat DSFF None None Concat DSFF Referring Expression Segmentation 17.56M 21.78M 22.83M 34.39M 79.7 79.5 80.4 81.0 81.6 81.0 82.0 82.4 77.4 78.0 78.3 78. 75.1 75.1 75.4 76.4 Referring Expression Comprehension 17.56M 21.78M 22.83M 34.39M 92.6 92.8 93.1 93.5 94.6 94.4 95.1 95.0 89.3 90.5 90.3 90. 88.6 88.7 88.8 89.4 78.8 77.8 78.6 79.1 92.4 91.7 92.6 92.3 70.4 70.9 70.7 72.5 83.5 84.4 83.2 85.2 78.5 78.8 79.3 79. 90.7 90.3 90.9 91.1 79.4 79.6 80.3 80.8 91.2 90.5 90.6 91.3 77.6 77.6 78.1 78.9 90.4 90.4 90.6 91.0 Table 4: Performance comparison of different image feature fusion strategies on RES and REC across RefCOCO (+/g). Models LLMs MLLMSegInternVL-1B internlm2 5-7b-chat MLLMSegInternVL-2B Qwen2.5-3B-Instruct MLLMSegInternVL-4B internlm2 5-1 8b-chat MLLMSegInternVL-8B Qwen2.5-0.5B-Instruct MLLMSegInternVL-1B internlm2 5-7b-chat MLLMSegInternVL-2B Qwen2.5-3B-Instruct MLLMSegInternVL-4B internlm2 5-1 8b-chat MLLMSegInternVL-8B Qwen2.5-0.5B-Instruct RefCOCO Val TestA TestB 77.3 79.3 79.5 81.0 Val Referring Expression Segmentation 71.4 79.9 73.6 81.0 75.3 81.4 76.4 82.4 Referring Expression Comprehension 83.4 92.1 86.3 93.6 87.9 93.9 89.4 95.0 74.9 77.1 77.7 78.7 85.9 88.1 88.8 90.3 89.1 91.3 92.1 93. RefCOCO+ TestA TestB RefCOCOg Test Val Avg. 78.4 77.5 78.8 79.1 88.7 90.6 91.5 92. 66.4 69.4 70.8 72.5 78.5 81.6 82.6 85.2 75.1 76.5 78.1 79.9 87.4 89.1 89.4 91.1 75.9 78.4 78.4 80.8 88.1 89.9 89.8 91. 74.9 76.6 77.5 78.9 86.7 88.8 89.5 91.0 Table 5: Performance comparison of different foundation MLLMs on RES and REC across RefCOCO (+/g). SegInternVL2.5 achieved 1.6% improvement in Prec@0.5. Compared to the currently widely used multimodal large models (IntrernvL2.5, Qwen2.5-VL), MLLMSeg achieves performance improvement of over 3%. In summary, our MLLMSegInternVL2.5 achieves overall leadership in REC. Generalized Referring Expression Segmentation. To evaluate the performance of MLLMSeg on the GRES task, we utilize the gRefCOCO dataset (Liu, Ding, and Jiang 2023). The dataset is partitioned into Train, Val, Test-A, and Test-B subsets, following the UNC split convention from RefCOCO (Yu et al. 2016). Evaluation is conducted on the Val, Test-A, and Test-B subsets. Table 3 shows the GRES results of MLLMSeg. Compared with LISA and GSVA with SAM, our method achieves significant improvements in gIoU, cIoU, and recognition accuracy for empty targets. Compared with Text4Seg based on InternVL2, our approach also achieved significant leadership in gIoU and cIoU."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Effectiveness of DSFF. To validate the effectivess of our DSFF, we conducted experiments based on InternVL2.5 (Chen et al. 2024b). Specifically, we evaluated models utilizing image features from three configurations: solely the visual encoder, solely the LLM, and jointly from both the visual encoder and the LLM. The results, reported in Table 4, demonstrate that using either 1 img (from the visual encoder) or 2 img (from the LLM) in isolation for mask prediction yields comparable performance on the RES and REC tasks. However, combining features from both sources allows the detailed and semantic information to complement each other, resulting in measurable performance gain. Crucially, upon integrating DSFF, the detailed and semantic features are effectively fused, leading to further performance improvements. Remarkably, with 34M parameters, our approach achieves performance comparable to or exceeding that of MLLMs equipped with SAM (632M parameters). These experimental results conclusively demonstrate the effectiveness of our DSFF. Performance at different model scales. To evaluate the effectiveness of our proposed lightweight mask decoder across MLLMs of varying scales, we conducted experiments using InternVL2.5 with 1B, 2B, and 4B base models. As shown in Table 5, while performance on the RES and REC tasks exhibits modest degradation when reducing the model size from 8B to 4B, the overall accuracy remains consistently high. Critically, even with further reductions to 2B and 1B parameter scales, the model maintains acceptable performance without suffering catastrophic performance drops, demonstrating robustness against significant parameter reduction. These results underscore the stability of our proposed method and highlight its suitability for deployment in resource-constrained scenarios."
        },
        {
            "title": "5 Conclusion\nIn this work, to finely adapt MLLMs to deal with the RES\ntask, we specifically propose MLLMSeg, a novel frame-\nwork that fully utilizes the inherent fine-grained cues within\nMLLMs for RES. The proposed detail enhancement and se-\nmantically consistent feature fusion module, coupled with\na light-weight mask decoder, enables effective integration\nof semantic understanding from LLM with high-resolution\nvisual cues extracted by the inherent visual encoder. Experi-\nments demonstrate that MLLMSeg achieves competing per-\nformance with only 34M network parameters, establishing a\nnew paradigm for efficient and accurate RES.",
            "content": "References Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Chen, Y.-C.; Li, W.-H.; Sun, C.; Wang, Y.-C. F.; and Chen, C.-S. 2024a. Sam4mllm: Enhance multi-modal large language model for referring expression segmentation. In European Conference on Computer Vision, 323340. Springer. Chen, Z.; Wang, W.; Cao, Y.; Liu, Y.; Gao, Z.; Cui, E.; Zhu, J.; Ye, S.; Tian, H.; Liu, Z.; et al. 2024b. Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling. arXiv preprint arXiv:2412.05271. Chen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong, W.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024c. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821. Feng, G.; Hu, Z.; Zhang, L.; and Lu, H. 2021. Encoder fusion network with co-attention embedding for referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15506 15515. Gong, S.; Zhuge, Y.; Zhang, L.; Yang, Z.; Zhang, P.; and The Devil is in Temporal Token: High Lu, H. 2025. arXiv preprint Quality Video Reasoning Segmentation. arXiv:2501.08549. He, J.; Wang, Y.; Wang, L.; Lu, H.; He, J.-Y.; Lan, J.-P.; Luo, B.; and Xie, X. 2024. Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1398013990. Jang, D.; Cho, Y.; Lee, S.; Kim, T.; and Kim, D.-S. 2025. Mmr: large-scale benchmark dataset for multi-target and multi-granularity reasoning segmentation. arXiv preprint arXiv:2503.13881. Kazemzadeh, S.; Ordonez, V.; Matten, M.; and Berg, T. 2014. Referitgame: Referring to objects in photographs In Proceedings of the 2014 conferof natural scenes. ence on empirical methods in natural language processing (EMNLP), 787798. Kirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.; Gustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.- In Proceedings of the Y.; et al. 2023. Segment anything. IEEE/CVF International Conference on Computer Vision, 40154026. Lai, X.; Tian, Z.; Chen, Y.; Li, Y.; Yuan, Y.; Liu, S.; and Jia, J. 2024. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 95799589. Lan, M.; Chen, C.; Zhou, Y.; Xu, J.; Ke, Y.; Wang, X.; Feng, L.; and Zhang, W. 2025. Text4Seg: Reimagining Image SegIn The Thirteenth Internamentation as Text Generation. tional Conference on Learning Representations. Liu, C.; Ding, H.; and Jiang, X. 2023. Gres: Generalized referring expression segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2359223601. Liu, W.; Lu, H.; Fu, H.; and Cao, Z. 2023. Learning In Proceedings of to upsample by learning to sample. the IEEE/CVF international conference on computer vision, 60276037. Mao, J.; Huang, J.; Toshev, A.; Camburu, O.; Yuille, A. L.; and Murphy, K. 2016. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, 11 20. Ren, Z.; Huang, Z.; Wei, Y.; Zhao, Y.; Fu, D.; Feng, J.; and Jin, X. 2024. Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2637426383. Rezatofighi, H.; Tsoi, N.; Gwak, J.; Sadeghian, A.; Reid, I.; and Savarese, S. 2019. Generalized intersection over union: metric and loss for bounding box regression. In CVPR. Shi, W.; Caballero, J.; Huszar, F.; Totz, J.; Aitken, A. P.; Bishop, R.; Rueckert, D.; and Wang, Z. 2016. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, 18741883. Tang, H.; Xie, C.; Wang, H.; Bao, X.; Weng, T.; Li, P.; Zheng, Y.; and Wang, L. 2025. Ufo: unified approach to fine-grained visual perception via open-ended language interface. arXiv preprint arXiv:2503.01342. Wang, J.; Wang, H.; Zhang, W.; Ji, K.; Huang, D.; and Progressive Language-guided Visual Zheng, Y. 2025a. Learning for Multi-Task Visual Grounding. arXiv preprint arXiv:2504.16145. Wang, P.; Bai, S.; Tan, S.; Wang, S.; Fan, Z.; Bai, J.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Fan, Y.; Dang, K.; Du, M.; Ren, X.; Men, R.; Liu, D.; Zhou, C.; Zhou, J.; and Lin, J. 2024. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191. Wang, T.; Cheng, C.; Wang, L.; Chen, S.; and Zhao, W. 2025b. Himtok: Learning hierarchical mask tokens for image segmentation with large multimodal model. arXiv preprint arXiv:2503.13026. Wei, C.; Tan, H.; Zhong, Y.; Yang, Y.; and Ma, L. 2024. LaSagnA: Language-based Segmentation Assistant for Complex Queries. arXiv preprint arXiv:2404.08506. Wu, J.; Zhong, M.; Xing, S.; Lai, Z.; Liu, Z.; Chen, Z.; Wang, W.; Zhu, X.; Lu, L.; Lu, T.; et al. 2024. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. Advances in Neural Information Processing Systems, 37: 6992569975. Xia, Z.; Han, D.; Han, Y.; Pan, X.; Song, S.; and Huang, G. 2024. Gsva: Generalized segmentation via multimodal In Proceedings of the IEEE/CVF large language models. Conference on Computer Vision and Pattern Recognition, 38583869. Ya Jing, W. W. L. W. L. L. T. T., Tao Kong. 2021. Locate Then Segment: Strong Pipeline for Referring Image Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 98589867. Yang, Z.; Wang, J.; Tang, Y.; Chen, K.; Zhao, H.; and Torr, P. H. 2022. Lavt: Language-aware vision transformer In Proceedings of the for referring image segmentation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1815518165. Yu, L.; Poirson, P.; Yang, S.; Berg, A. C.; and Berg, T. L. 2016. Modeling context in referring expressions. In European conference on computer vision, 6985. Springer. Zhang, T.; Li, X.; Huang, Z.; Li, Y.; Lei, W.; Deng, X.; Chen, S.; Ji, S.; and Feng, J. 2025. Pixel-sail: Single transformer for pixel-grounded understanding. arXiv preprint arXiv:2504.10465. Zhang, Z.; Ma, Y.; Zhang, E.; and Bai, X. 2024. Psalm: Pixelwise segmentation with large multi-modal model. In European Conference on Computer Vision, 7491. Springer. Zhu, M.; Tian, Y.; Chen, H.; Zhou, C.; Guo, Q.; Liu, Y.; Yang, M.; and Shen, C. 2025. Segagent: Exploring pixel understanding capabilities in mllms by imitating human annotator trajectories. In Proceedings of the Computer Vision and Pattern Recognition Conference, 36863696."
        }
    ],
    "affiliations": [
        "Medical Artificial Intelligence Laboratory, Westlake University",
        "School of Data Science and Engineering, East China Normal University",
        "School of Life Science and Technology, Xian Jiaotong University"
    ]
}