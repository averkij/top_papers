{
    "paper_title": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment",
    "authors": [
        "Yixuan Tang",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision."
        },
        {
            "title": "Start",
            "content": "SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment Yixuan Tang 1 Yi Yang 1 Abstract Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as reward signal for reinforcement learning. Without external supervision, SRGRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering path toward scalable alignment without external supervision. 5 2 0 2 2 ] . [ 1 7 0 8 2 0 . 2 1 5 2 : r 1. Introduction The alignment of Large Language Models (LLMs) with human preferences typically relies on Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022). Despite its success, this paradigm depends heavily on external supervision. Human judgments 1The Hong Kong University of Science and Technology. Correspondence to: Yixuan Tang <ytangch@connect.ust.hk>, Yi Yang <imyiyang@ust.hk>. Preprint. December 3, 2025. are subjective and context-dependent, making it difficult to train robust reward models (Chakraborty et al., 2024; Wang et al., 2023), and learned proxies are susceptible to reward hacking (Casper et al., 2023). The sparsity of annotations further limits the models ability to fine-grained behaviors (Wu et al., 2023). These limitations have motivated various attempts to reduce annotation dependence. Direct Preference Optimization (Rafailov et al., 2023) eliminates explicit reward modeling but still requires preference datasets. Verifiable rewards (DeepSeek-AI, 2025; Lambert et al., 2024) leverage groundtruth outcomes but only apply where automatic verification is feasible. Self-evaluation methods (Yuan et al., 2024; Lee et al., 2024; Garg et al., 2025) suffer from prompt sensitivity and systematic biases (Zheng et al., 2023; Wang et al., 2024). Crucially, all these approaches still evaluate quality through external signals while overlooking information embedded in the models own representations. Yet the validity of generation should be reflected in its underlying computation. When model outputs The capital of France is Paris, its hidden states encode activated factual knowledge (He et al., 2024c); when it hallucinates, different activation patterns emerge (Chen et al., 2024). This raises natural question: Can we measure text quality directly from internal signals, enabling LLM alignment without external supervision? In this work, we show that simple geometric property of LLM hidden states, their effective dimensionality, provides reliable signal for LLM response quality. We propose to use stable rank (Rudelson & Vershynin, 2007), matrixtheoretic measure as an unsupervised proxy for generation quality. Stable rank quantifies how many independent semantic directions response occupies by measuring the ratio of total variance to dominant-direction variance in the hidden-state representation. It balances representational richness with coherence: high stable rank indicates that information spreads across many dimensions rather than concentrating in few. The theoretical motivation comes from two lines of work. First, the softmax bottleneck analysis shows that accurate language modeling requires navigating high-dimensional semantic manifold, as the contextual probability distribution of natural language is inherently Stable Rank as Intrinsic Reward Signal Figure 1. Hidden states visualization for responses in RewardBench (Lambert et al., 2025) based on Qwen3-8B (Yang et al., 2025). Left: PCA projection shows good (blue) and bad (red) responses with spatial overlap. Right: Stable rank coloring uncovers clear quality separation: good responses exhibit higher ranks than bad responses. Stable rank can distinguish response quality. high-rank (Yang et al., 2018; Godey et al., 2024). Second, literature also shows that when representations collapse into narrow cone, expressiveness is severely limited and generation quality degrades (Gao et al., 2019). We therefore hypothesize that this principle extends to individual LLM responses: high-quality generations should maintain higher effective dimensionality, while low-quality outputs exhibit rank collapse. We validate this hypothesis in two settings. First, as zeroshot reward proxy, stable rank achieves 84.04% accuracy on RewardBench (Lambert et al., 2025) with Qwen3-8B (Yang et al., 2025), matching LLM-as-Judge baselines without any training. Figure 1 illustrates this geometrically: while PCA projections of LLM representations show overlap between good and bad responses, stable rank cleanly separates them. Second, stable rank-guided Best-of-N selection consistently outperforms greedy decoding across four model families on STEM and mathematics benchmarks, with average gains of 11.3% at = 16. Building on these findings, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as an intrinsic reward signal for reinforcement learning. By replacing external supervision with this geometric signal derived directly from the reference model, we enable fully annotation-free alignment. On Qwen2.51.5B-Instruct (Yang et al., 2024), SR-GRPO outperforms GRPO with learned reward models by 10 to 19% on reasoning tasks at zero annotation cost, and surpasses self-reward baselines by 8 to 9%. Consistent gains on DeepSeek-R1Distill-Qwen-1.5B (DeepSeek-AI, 2025) confirm robustness across model families. To understand why stable rank succeeds as quality signal, we analyze its correlations with interpretable text metrics. We find that stable rank captures three quality dimensions: semantic coherence, where sentences build on each other while staying relevant to the prompt; information density over verbosity, favoring concise, non-repetitive text; and sensitivity to reasoning words like however and because at key turning points. Our contributions are summarized as follows: We propose SR-GRPO, which uses stable rank as dense reward signal in reinforcement learning, eliminating dependency on preference datasets or external verifiers. We demonstrate that SR-GRPO improves reasoning across multiple benchmarks, thus establishing intrinsic geometric signals as viable basis for LLM alignment. Beyond reinforcement learning, we also show that stable rank offers reliable zero-shot reward proxy for evaluating LLM responses, and integrates naturally into best-of-N decoding. 2. Stable Rank as an Intrinsic Quality Metric We propose stable rank (Rudelson & Vershynin, 2007) as an unsupervised quality signal derived from LLM representations. This section defines the metric, explains its geometric motivation, and validates its effectiveness as reward proxy. 2.1. Definition For sequence of tokens, we extract the hidden state activation matrix RT from the last layer of an LLM. Let σ1 σ2 σmin(T,d) 0 denote the singular values of H. The stable rank is defined as: σ2 σ2 1 H2 H2 2 SR(H) = (1) (cid:80) = . Stable rank measures the effective dimensionality of H: if single singular value dominates, SR(H) 1, indicating representation collapse; when singular values are balanced, 2 Stable Rank as Intrinsic Reward Signal stable rank approaches rank(H), reflecting rich, highIntuitively, low stable rank dimensional representation. means token representations cluster along few dominant directions, while high stable rank means they spread across the embedding space. Theoretical motivation. The theoretical motivation comes from two lines of work. The softmax bottleneck (Yang et al., 2018; Godey et al., 2024) implies that expressive language modeling requires high-rank representations, while representation collapse is known symptom of degraded generation (Gao et al., 2019). Stable rank directly measures this effective dimensionality, making it natural candidate for quality assessment. We validate this connection empirically below and through correlation analysis in Section 5. Implementation. Given prompt-response pair formatted with the models chat template, we perform forward pass and extract the hidden state matrix RT from the final hidden layer, where is the total number of tokens. We use final-layer activations because they aggregate information across all preceding layers and are directly related to the final text generation. Stable rank is then computed from via Equation 1. Cross-layer analysis in Appendix confirms that the final layer embeddings yield the strongest quality signal. 2.2. Empirical Validation: Stable Rank as Zero-Shot"
        },
        {
            "title": "Reward Proxy",
            "content": "We evaluate stable rank as zero-shot proxy for human preference by testing whether it can predict the preferred response in pair. This assessment checks whether stable rank can serve as reliable reward signal without any modelspecific training. Setup. We use RewardBench (Lambert et al., 2025), benchmark containing 2,985 preference pairs across five categories: Chat, Chat-Hard, Safety, Code, and Math. For each pair, we compute stable rank for both responses and predict the one with the higher stable rank as preferred. We compare against three baseline methods: (1) Pointwise Scoring (Point.) (Kim et al., 2024): prompting the LLM to score each response on 1-5 scale; (2) Pairwise Comparison (Pair.) (Kim et al., 2024): prompting the LLM to directly compare two responses; (3) IPO (Garg et al., 2025): using implicit preference scores derived from Yes/No token probabilities. We evaluate five models: Qwen2.5-1.5B-Instruct (Yang et al., 2024), Qwen3-0.6B (Yang et al., 2025), Qwen38B (Yang et al., 2025), Llama-3.1-8B-Instruct (Dubey et al., 2024), and Phi-3.5-mini-Instruct (Abdin et al., 2024). For stable rank, we format inputs as prompt-response pairs using each models native chat template, extract final-layer hidden states for the response tokens only, and compute stable rank via Eq. 1. Prompts and implementation details are in Appendix B. Results. Table 1 shows stable rank consistently outperforms baselines across model scales. On Qwen3-8B, it achieves 84.04% accuracy, surpassing Pointwise (83.70%) and IPO (78.02%). The advantage is more pronounced on smaller models: stable rank achieves 75.95% on Qwen2.5-1.5B-Instruct, outperforming the best baseline (IPO, 65.85%) by 10.1 percentage points. Generative baselines exhibit high variance across scales. Pointwise scoring excels on large models (83.70% on Qwen3-8B) but collapses on smaller ones (37.15% on Qwen2.5-1.5B), likely because smaller models lack the instruction-following capability to produce calibrated scores. Pairwise comparison shows the opposite pattern, performing better on small models but degrading on Qwen3-8B (71.98%). IPO maintains relative stability (65.57%78.02%) but still underperforms stable rank across all models. The consistent advantage of stable rank, especially on smaller models, suggests that LLMs intrinsic geometric signals are more robust than prompt-based evaluation. Prompt-based methods require LLMs to follow evaluation instructions and produce calibrated scores, which smaller models often struggle with. Stable rank has no such dependency, making it particularly suitable for aligning tiny LLMs. Table 1. Overall accuracy comparison across different reward methods on RewardBench. Pointwise and Pairwise refer to LLMas-judge baselines using 1-5 scoring and direct comparison, respectively. SR denotes Stable Rank. The detailed results are demonstrated in the Appendix C. Model Point. Pair. IPO SR Qwen3-0.6B Llama-3.1-8B-Instruct Phi-3.5-mini-instruct Qwen2.5-1.5B-Instruct Qwen3-8B 38.76 56.37 42.44 37.15 83. 47.12 56.65 62.11 53.46 71.98 65.57 58.14 66.27 65.85 78.02 66.96 68.36 70.87 75.95 84.04 2.3. Empirical Validation: Best-of-N Decoding with"
        },
        {
            "title": "Stable Rank",
            "content": "We next evaluate stable rank as test-time quality signal in Best-of-N decoding, where the model samples multiple candidate responses and selects the one with the highest stable rank. This setup tests whether stable rank can guide inference and improve task performance beyond standard greedy decoding. Setup. For each prompt, we sample {1, 4, 8, 16} responses (temperature 0.7, top-p 0.9) and select the one with the highest stable rank. We test on STEM benchmarks (GPQA (Rein et al., 2024), MMLU-redux (Gema 3 Stable Rank as Intrinsic Reward Signal et al., 2025)) and mathematical reasoning tasks (MATH500 (Lightman et al., 2023), OlympiadBench (He et al., 2024a), AMC23 (knoveleng, 2023)) across four models: Qwen2.51.5B-Instruct (Yang et al., 2024), Phi-3.5-mini-Instruct (Abdin et al., 2024), Llama-3.2-1B-Instruct (Dubey et al., 2024), and DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI, 2025). We compare against random selection to isolate the effect of the selection criterion from sampling diversity. Results. Figure 2 shows that stable rank-guided selection consistently improves over greedy decoding across all models. At N=16, Llama-3.2-1B achieves the largest gain (+20.5%), followed by Qwen2.5-1.5B (+17.0%) and DeepSeek-R1 (+10.2%), and Phi-3.5-mini shows moderate improvement of +8.5%. Stable rank also outperforms random selection. On Llama-3.2-1B at N=16, stable rank (26.5% avg.) exceeds random selection (19.8%) by 33.8%. Notably, random selection often degrades performance below greedy decoding, such as 16.1% on Llama-3.2-1B at N=8. In contrast, stable rank consistently yields positive gains, demonstrating that it identifies high-quality responses rather than merely benefiting from sampling diversity. These results confirm that stable rank captures quality signals that correlate strongly with correctness. Complete results are provided in Appendix D. 3. Stable Rank Group Relative Policy Optimization (SR-GRPO) Having established an empirical correlation between stable rank and response quality, we next ask whether using stable rank as reward signal in reinforcement learning can further improve LLM alignment behavior. Because stable rank requires no labeled data and can be computed for any generated response, it provides simple and scalable dense reward for reinforcement learning. 3.1. Setup Let πref denote reference model and πϕ the trainable policy initialized from πref. Given prompt x, the policy generates response πϕ(x). Our goal is to optimize πϕ to generate higher-quality responses, using stable rank as proxy reward. 3.2. Training Procedure We build on Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which compares responses within sampled group to obtain relative quality signals, avoiding the need for learned value function. This avoids the need to train separate critic and suits our setting where rewards come from geometric metric. For each prompt x, we sample responses from the current Algorithm 1 SR-GRPO Training Require: Dataset D, initial policy πϕ, frozen reference πref, group size K, KL coefficient β Sample batch {xi} for each prompt xi do Ensure: Optimized policy πϕ 1: while not converged do 2: 3: 4: 5: 6: 7: 8: 9: end while end for Update πϕ via Eq. (2) Sample responses {yi,k}K k=1 πϕ(xi) Compute rewards ri,k SR(Hi,k; πref) Compute advantages Ai,k (ri,k µi)/(σi + ϵ) policy and compute their stable rank on the frozen reference model πref. Using frozen model is critical: it provides stationary reward signal that the policy cannot manipulate by changing its own internal geometry. We standardize rewards within each group to obtain scaleinvariant learning signals: Ak = (rk µ)/(σ + ϵ), where µ and σ are the group mean and standard deviation. The training objective is: (ϕ) = Ex (cid:34)"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 ρkAk βDKL(πϕπref) (2) (cid:35) where ρk = πϕ(ykx)/πϕold(ykx) is the importance ratio, ϕold denotes the parameters before the current update, and β controls the KL penalty. Algorithm 1 summarizes the procedure. Computational efficiency. Computing stable rank requires O(T d) operations: O(T d) for the Frobenius norm and O(T d) per power iteration for the spectral norm. This overhead is negligible compared to the transformer forward pass. Stable rank is also robust to input truncation: using only 512 tokens achieves nearly identical accuracy (Appendix H). 4. Alignment Experiments 4.1. Experiment Setup We evaluate SR-GRPO on two models: Qwen2.5-1.5BInstruct (Yang et al., 2024) and DeepSeek-R1-Distill-Qwen1.5B (DeepSeek-AI, 2025). All methods are trained on prompts from SmolTalk2 (Allal et al., 2025), which provides diverse topics and tasks. No preference labels are used during training; methods differ only in how they compute rewards from sampled responses. For SR-GRPO, we train with LoRA (Hu et al., 2022) and compute stable rank on the base model without LoRA adapters, which also serves as the frozen reference. 4 Stable Rank as Intrinsic Reward Signal Figure 2. Best-of-N decoding with stable rank selection. Top: Performance on individual benchmarks as increases. Bottom: Average accuracy comparing stable rank selection (solid) versus random selection (dashed). Annotations show relative improvement over greedy decoding (N=1). Stable rank consistently outperforms random selection across all models. Table 2. Alignment results on Qwen2.5-1.5B-Instruct and DeepSeek-R1-Distill-Qwen-1.5B. SR-GRPO outperforms the reward model (RM) and self-evaluation baselines without external labels. WildBench scores are reported as WB-Elo. Best results in bold; ties are all bolded. Model Method GPQA MMLU Avg. MATH AIME Olymp. AMC Avg. WB-Elo STEM Math Chat Qwen2.5-1.5B-Instruct DeepSeek-R1-Distill-Qwen-1.5B Base + RM + Self-Reward + Perplexity + IPO + SR-GRPO Base + RM + Self-Reward + Perplexity + IPO + SR-GRPO 19.0 15.7 17.7 18.7 17.2 21.2 25.8 24.2 23.7 24.2 28.8 30.3 47.7 47.2 45.5 47.6 47.7 47.7 45.8 46.3 45.8 46.3 45.8 46.6 33.3 31.4 31.6 33.1 32.4 34.5 35.8 35.3 34.8 35.3 37.3 38. 48.0 50.4 50.6 48.4 49.2 52.4 82.8 85.4 85.2 86.2 86.0 86.2 3.3 3.3 13.3 10.0 3.3 13.3 30.0 36.7 36.7 36.7 26.7 36.7 25.5 25.6 23.6 25.6 24.0 26.4 43.6 56.2 57.2 55.0 56.7 58. 35.0 30.0 32.5 32.5 27.5 37.5 77.5 72.5 67.5 77.5 77.5 77.5 28.0 27.3 30.0 29.1 26.0 32.4 58.5 62.7 61.6 63.8 61.7 64.7 1036.2 1043.3 1041.2 1040.9 1037.7 1062.4 913.5 918.4 919.2 917.0 922.4 932. Benchmarks. We evaluate on three categories: (1) STEM tasks: GPQA (Rein et al., 2024), graduate-level science QA benchmark requiring expert knowledge, and MMLUredux (Gema et al., 2025), curated subset of MMLU with corrected labels; (2) Mathematical reasoning: MATH500 (Lightman et al., 2023), subset of competition mathematics problems, AIME25 (Zhang & Math-AI, 2025), problems from the 2025 American Invitational Mathematics Examination, OlympiadBench (He et al., 2024b), olympiad-level math and physics problems, and AMC23 (knoveleng, 2023), problems from the 2023 American Mathematics Competition; (3) General chat: WildBench (Lin et al., 2025), which evaluates open-ended conversation quality using GPT-4o mini (OpenAI, 2025) as judge. For all benchmarks except WildBench, we report Pass@1 accuracy. WildBench (Lin et al., 2025) evaluates open-ended conversation quality by comparing model outputs against reference responses using GPT-4o mini as judge; scores are reported as Elo ratings. Baselines. We compare SR-GRPO against two categories of methods: (1) Reward Model (RM): We use Skywork-Reward-V2Qwen3-1.7B (Liu et al., 2025), widely used reward model trained on the Skywork preference dataset with the BradleyTerry objective. We select 1.7B model to match the scale of our policy models, ensuring fair comparison with selfevaluation methods that use the policy model itself for reward computation. (2) Self-Evaluation Methods: These methods derive rewards from the models own outputs without external labels: Self-Reward (Yuan et al., 2024): The model evalu5 Stable Rank as Intrinsic Reward Signal ates its own completions using the pointwise scoring prompt (Appendix B), generating 1-5 score that serves as the reward signal. Table 3. Semantic coherence and information density correlations with stable rank (Spearman ρ). Mini-bars visualize correlation magnitude and direction (scale: -1 to 1). All < 0.001. Perplexity: We use the negative log-likelihood of the completion as the reward, where lower perplexity indicates higher fluency. IPO (Garg et al., 2025): The model acts as binary classifier, determining response quality by generating Yes or No. We extract logits for both tokens from the first output position and compute their probabilities via softmax. The probability of Yes serves as the reward signal. All methods use the same GRPO training configurations (K=8 responses per prompt, β=0.01) to ensure fair comparison. Full training details are in Appendix E. 4.2. Results Analysis Table 2 shows that SR-GRPO consistently outperforms both reward model and self-evaluation baselines across all categories, despite using no external labels. Reasoning improvements. On Qwen2.5-1.5B-Instruct, SRGRPO improves average mathematical reasoning accuracy by 4.4 percentage points (28.0% to 32.4%), with particularly strong gains on competition-level problems: AMC improves from 35.0% to 37.5% and MATH from 48.0% to 52.4%. On DeepSeek-R1-Distill-Qwen-1.5B, which already achieves 58.5% on math tasks, SR-GRPO still improves performance to 64.7%, demonstrating effectiveness even on reasoningspecialized models. The gains on OlympiadBench (43.6% to 58.5%) suggest that stable rank rewards the structured reasoning patterns required for complex problem-solving. Comparison with baselines. The reward model shows minimal or even negative impact on several tasks (e.g., GPQA drops from 19.0% to 15.7% on Qwen2.5-1.5B), suggesting that models trained on general preference data may not transfer well to specialized reasoning domains. Self-evaluation methods show inconsistent patterns: Self-Reward matches SR-GRPO on AIME (13.3%) but underperforms on STEM tasks (31.6% vs 34.5% avg.); Perplexity performs surprisingly well on DeepSeek-R1 but lacks the consistency of SR-GRPO across both models. IPO degrades performance on several benchmarks, possibly because Yes/No probability signals are too coarse for nuanced quality distinctions. General chat quality. SR-GRPOs advantage extends beyond reasoning to open-ended conversation. On WildBench (WB-Elo), it achieves gains of 26.2 points on Qwen2.5-1.5B and 19.0 points on DeepSeek-R1, substantially outperforming all baselines. This indicates that stable rank captures qualities valued in general conversation, such as coherence"
        },
        {
            "title": "Metric",
            "content": "ρ Mini-bar"
        },
        {
            "title": "Semantic Coherence",
            "content": "Coherence std (adjacent sim.) 0.356 0.316 QA alignment consistency 0.313 Progression score 0.272 Semantic variance 0.250 Adjacent similarity (mean)"
        },
        {
            "title": "Information Density",
            "content": "Sentence count Token count Unique token count Lexical diversity (TTR) Compression ratio 0.368 0.294 0.286 0.238 0.233 and helpfulness, not just task-specific correctness. The consistent improvements across both reasoning benchmarks and open-ended chat suggest that stable rank provides general-purpose quality signal rather than optimizing for narrow task metrics. 5. What Stable Rank Captures To understand why stable rank can serve as an intrinsic reward, we analyze its correlations with interpretable text quality metrics on the RewardBench dataset. We compute 37 metrics spanning semantic coherence, information density, and linguistic structure. For each metric , we measure sample-level Pearson correlation with stable rank across 5,970 responses, and paired-difference Spearman correlation across 2,985 chosen/rejected pairs. The definitions and correlation tables are provided in Appendix F; here we summarize key findings. 5.1. Stable Rank Captures Semantic Coherence Table 3 presents the strongest correlations among semantic coherence metrics. Progression score (ρ = 0.313) measures whether each sentence builds on the previous one. QA alignment consistency (ρ = 0.316) captures whether relevance to the prompt remains stable throughout the response. Both correlate positively with stable rank. The strongest negative correlation appears with coherence standard deviation (ρ = 0.356), where high values indicate erratic transitions between adjacent sentences. These patterns suggest stable rank can capture responses that maintain topical focus while developing ideas smoothly, avoiding the abrupt transitions characteristic of hallucinations or incoherent reasoning. 5.2. Stable Rank Distinguishes Density from Verbosity Contrary to common reward hacking, where models maximize length, stable rank correlates negatively with token 6 Stable Rank as Intrinsic Reward Signal Table 4. Discourse marker correlations with stable rank (paireddifference analysis, = 2,985). Negative values indicate fewer markers in higher-rank responses. Both parametric (Pearson r) and non-parametric (Spearman ρ) correlations shown; see Table 12 for full results. indicates < 0. Category Contrastive Causal Pearson +0.187 +0.139 0.166 Additive 0.163 Conditional 0.122 Enumeration 0.106 Temporal 0.065 Inference Total (raw count) 0.149 Spearman ρ +0.067 0.002 0.156 0.163 0.148 0.122 0.115 0.204 Example keywords however, but, although because, therefore, thus moreover, furthermore if, when, unless first, second, step 1 then, next, meanwhile implies, suggests all markers count (ρ = 0.294) and sentence count (ρ = 0.368). Instead, stable rank favors information density. Lexical diversity correlates positively (ρ = 0.238). This metric measures the ratio of unique tokens to total tokens. Compression ratio also correlates positively (ρ = 0.233). It measures the ratio of compressed to original text length. Table 3 presents key correlations. These patterns suggest stable rank penalizes verbose, repetitive text while rewarding concise responses with diverse vocabulary. 5.3. Stable Rank Highlights Key Logical Markers In this section, we study the correlation between discourse markers and stable rank. Discourse markers are explicit phrases that signal logical relationships, such as because or if, or structural transitions, such as however or first. We use paired-difference analysis on = 2,985 chosen and rejected pairs (Appendix F) to correlate marker frequency, measured per 100 tokens, with stable rank while controlling for prompt context. We report both Pearson and Spearman correlations because the two can diverge when relationships are non-monotonic. Table 4 shows that most marker categories, especially additive (ρ = 0.156), conditional (ρ = 0.163), and enumeration (ρ = 0.148), have consistently negative correlations with stable rank, and the total marker count is also negatively associated (ρ = 0.204). Responses that rely heavily on common connective words such as moreover, first, and then tend to receive lower stable rank, which is consistent with Section 5.2 where stable rank disfavors verbose, pattern-like exposition in favor of compact information content. Contrastive and causal markers follow different pattern. Their Pearson correlations with stable rank are positive (contrastive = 0.187, causal = 0.139), but their Spearman correlations are small. This divergence suggests that the presence of these markers matters more than their frequency: few well-placed however or because phrases benefit stable rank, but overusing them does not. This aligns with 7 linguistic intuition: contrastive and causal connectives are most informative when they mark key reasoning steps, not when they appear as formulaic filler. 5.4. Summary Together, these correlation patterns align with established findings in text quality research. The strong association with semantic coherence metrics mirrors neural coherence models, which show that smooth semantic transitions predict text quality (Mesgar & Strube, 2018; Cui et al., 2017). The negative correlation with length while favoring information density addresses well-documented failure mode in RLHF: length bias, where reward models favor longer responses regardless of quality (Singhal et al., 2024). Unlike learned reward models that often exploit this spurious correlation, stable ranks geometric formulation inherently penalizes verbose, low-information content. While these correlations are moderate in magnitude (ρ 0.20.4), they are consistent in direction across metrics and statistically significant (p < 0.001), suggesting stable rank captures genuine quality signals rather than noise. These findings indicate that stable rank correlates with multiple dimensions of text quality, including coherence, density, and reasoning structure, suggesting it captures meaningful aggregate signal rather than single superficial feature. Qualitative examples illustrating how stable rank differentiates high-quality reasoning from repetition and verbosity are provided in Appendix J. 6. Ablation Studies We conduct three ablation studies to validate our design choices. Alternative intrinsic dimension metrics. We compare stable rank against three alternatives: condition number, which measures the ratio of largest to smallest singular value; PCA 95% variance, which counts principal components needed to capture 95% of variance; and effective rank (Roy & Vetterli, 2007), which uses entropy over the singular value distribution. On RewardBench, stable rank achieves 84.04% overall accuracy, outperforming PCA 95% variance (61.91%), effective rank (54.50%), and condition number (36.04%). The gap widens on harder categories such as Math and Safety. Stable rank succeeds because it aggregates information across the entire singular value spectrum through the Frobenius norm, making it robust to outliers. Condition number is sensitive to extreme singular values. Effective ranks entropy weighting and PCAs discrete component counting appear less suited for capturing quality distinctions. Full results are provided in Appendix G. Context length. We examine how sequence truncation affects performance. Accuracy drops from 83.85% at 512 toStable Rank as Intrinsic Reward Signal kens to 62.59% at 128 tokens. Code suffers the largest degradation (87.91% to 24.80%) because truncation removes critical program logic. However, extending beyond 512 tokens yields negligible gains (<0.2 percentage points). This indicates that stable rank captures core semantic structure rather than mechanically rewarding longer sequences. Details are provided in Appendix H. Prompt format. We test six input templates across three models. Overall accuracy varies by at most 3 percentage points, with no format consistently outperforming others. This robustness simplifies deployment: practitioners can use simple formats without extensive tuning. Full results are shown in Appendix I. 7. Related Work We review three lines of related work: alignment methods using external feedback, approaches that reduce reliance on human labels, and theoretical connections between representation geometry and generation quality. 7.1. Alignment with External Feedback Signals The standard pipeline for aligning Large Language Models with human preferences is Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022). RLHF trains reward model on human preference data and then fine-tunes the policy to maximize this learned signal under KL constraint to reference model. This approach underpins LLMs such as ChatGPT and Llama (Dubey et al., 2024), but it inherits well-known issues: preference data are expensive and noisy, and reward models are vulnerable to reward hacking and distribution shift (Casper et al., 2023; Skalse et al., 2022). These limitations motivate supervised preference-based methods such as Direct Preference Optimization (DPO) (Rafailov et al., 2023), KTO (Ethayarajh et al., 2024), Bayesian RLHF (Wang et al., 2023), and MaxMin-RLHF (Chakraborty et al., 2024), which optimize the policy directly on pairwise comparisons without an explicit value network but still rely on curated preference datasets. Process reward models and generative reward models provide step-level scores and textual critiques for mathematical reasoning and general evaluation (Zhang et al., 2025b; Yin et al., 2025).All these approaches keep supervision external to the models internal computation. 7.2. Alignment with Automatic Signals complementary line of work reduces reliance on human labels by using automatic supervision or signals derived from the model itself. Verifiable-reward methods such as DeepSeek-R1 (DeepSeek-AI, 2025) and ULU 3 (Lambert et al., 2024) replace human feedback with programmatic checkers; these methods work well in domains like mathematics and coding but do not generalize to open-ended tasks. Self-evaluation approaches treat the model, or another LLM, as judge: Self-Rewarding Language Models (Yuan et al., 2024) and RLAIF (Lee et al., 2024) generate AI feedback instead of human labels, while IPO (Garg et al., 2025) and Self-Rewarding PPO (Zhang et al., 2025a) derive preference signals from token-level probabilities. Closer to our setting, work on internal activations such as Factoscope (He et al., 2024c) and INSIDE (Chen et al., 2024) shows that hidden states encode factuality and hallucination risk. Our method follows this direction, using geometric statistic of activations as reward signal for policy optimization. 7.3. Representation Geometry and Generation Quality theoretical foundation for rank-based quality measures comes from the softmax bottleneck analysis. Yang et al. (2018) show that the expressiveness of softmax-based models is fundamentally limited by the rank of their hidden representations. Godey et al. (2024) extend this to modern LLMs, demonstrating that the contextual probability distribution of natural language is inherently high-rank. Complementary work shows that when embeddings collapse into narrow cone during training, generation quality degrades (Gao et al., 2019). Together, these results establish link between representational dimensionality and generation quality. Empirical work supports this connection. Garrido et al. (2023) demonstrates that the effective rank of learned representations predicts downstream performance without labels. In language models specifically, hidden states encode rich semantic structure: word embeddings capture syntactic and semantic regularities through geometric relationships (Mikolov et al., 2013), and intermediate representations encode factuality (He et al., 2024c) and hallucination risk (Chen et al., 2024), confirming that generation quality leaves detectable geometric signatures. Our work builds on these findings but shifts from diagnosis to optimization. Rather than using rank to analyze pretrained models post-hoc, we employ stable rank as live reward signal during policy optimization, transforming geometric insight into practical alignment method. 8. Conclusion This work demonstrates that stable rank, simple intrinsic geometric property of LLM hidden states, provides an effective signal for text quality assessment and model alignment. As zero-shot reward proxy, stable rank achieves 84.04% accuracy on RewardBench and improves Best-of-N sampling by 11.3 percentage points on average across reasoning benchmarks. Building on these insights, we introduce SRGRPO, which uses stable rank as novel reward signal for reinforcement learning without external supervision. SR8 Stable Rank as Intrinsic Reward Signal GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM tasks and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Moreover, our analysis reveals that stable rank captures semantic coherence, information density, and sensitivity to key reasoning structures. Collectively, these results suggest that internal representation geometry provides sufficient signal for effective LLM alignment, offering scalable alternative to annotation-dependent approaches."
        },
        {
            "title": "References",
            "content": "Abdin, M. I., Jacobs, S. A., Awan, A. A., Aneja, J., Awadallah, A., Awadalla, H., Bach, N., Bahree, A., Bakhtiari, A., Behl, H. S., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai, M., Mendes, C. C. T., Chen, W., Chaudhary, V., Chopra, P., Giorno, A. D., de Rosa, G., Dixon, M., Eldan, R., Iter, D., Garg, A., Goswami, A., Gunasekar, S., Haider, E., Hao, J., Hewett, R. J., Huynh, J., Javaheripi, M., Jin, X., Kauffmann, P., Karampatziakis, N., Kim, D., Khademi, M., Kurilenko, L., Lee, J. R., Lee, Y. T., Li, Y., Liang, C., Liu, W., Lin, E., Lin, Z., Madan, P., Mitra, A., Modi, H., Nguyen, A., Norick, B., Patra, B., PerezBecker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M., Rosset, C., Roy, S., Ruwase, O., Saarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah, S., Shang, N., Sharma, H., Song, X., Tanaka, M., Wang, X., Ward, R., Wang, G., Witte, P. A., Wyatt, M., Xu, C., Xu, J., Yadav, S., Yang, F., Yang, Z., Yu, D., Zhang, C., Zhang, C., Zhang, J., Zhang, L. L., Zhang, Y., Zhang, Y., Zhang, Y., and Zhou, X. Phi-3 technical report: highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024. doi: 10.48550/ARXIV.2404.14219. Allal, L. B., Lozhkov, A., Bakouch, E., Blazquez, G. M., Penedo, G., Tunstall, L., Marafioti, A., Kydlıcek, H., Lajarın, A. P., Srivastav, V., et al. Smollm2: When smol goes big-data-centric training of small language model. CoRR, 2025. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., Showk, S. E., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T. B., Clark, J., McCandlish, S., Olah, C., Mann, B., and Kaplan, J. Training helpful and harmless assistant with reinforcement learning from human feedback. CoRR, abs/2204.05862, 2022. Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., Wang, T. T., Marks, S., Segerie, C.-R., Carroll, M., Peng, A., Christoffersen, P. J., Damani, M., Slocum, S., Anwar, U., Siththaranjan, A., Nadeau, M., Michaud, E. J., Pfau, J., Krasheninnikov, D., Chen, X., Langosco, L., Hase, P., Biyik, E., Dragan, A., Krueger, D., Sadigh, D., and Hadfield-Menell, D. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. Chakraborty, S., Qiu, J., Yuan, H., Koppel, A., Manocha, D., Huang, F., Bedi, A., and Wang, M. MaxMin-RLHF: Alignment with diverse human preferences. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 61166135. PMLR, 2127 Jul 2024. Chen, C., Liu, K., Chen, Z., Gu, Y., Wu, Y., Tao, M., Fu, Z., and Ye, J. INSIDE: LLMs internal states retain the power of hallucination detection. In The Twelfth International Conference on Learning Representations, 2024. Cui, B., Li, Y., Zhang, Y., and Zhang, Z. Text coherence analysis based on deep neural network. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 17, pp. 20272030, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450349185. doi: 10.1145/3132847.3133047. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501. 12948. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Rozi`ere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. Model alignment as prospect theoretic optimizaStable Rank as Intrinsic Reward Signal tion. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. for Computational Linguistics, August 2024b. doi: 10. 18653/v1/2024.acl-long.211. Gao, J., He, D., Tan, X., Qin, T., Wang, L., and Liu, T. Representation degeneration problem in training natural language generation models. In International Conference on Learning Representations, 2019. Garg, S., Singh, A., Singh, S., and Chopra, P. IPO: your language model is secretly preference classifier. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pp. 1942519441. Association for Computational Linguistics, 2025. Garrido, Q., Balestriero, R., Najman, L., and LeCun, Y. Rankme: assessing the downstream performance of pretrained self-supervised representations by their rank. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Gema, A. P., Leang, J. O. J., Hong, G., Devoto, A., Mancino, A. C. M., Saxena, R., He, X., Zhao, Y., Du, X., Ghasemi Madani, M. R., Barale, C., McHardy, R., Harris, J., Kaddour, J., Van Krieken, E., and Minervini, P. Are we done with MMLU? In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 50695096. Association for Computational Linguistics, April 2025. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.262. Godey, N., de la Clergerie, E. V., and Sagot, B. Why do small language models underperform? studying language In First model saturation via the softmax bottleneck. Conference on Language Modeling, 2024. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850. Association for Computational Linguistics, August 2024a. doi: 10. 18653/v1/2024.acl-long.211. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., Liu, J., Qi, L., Liu, Z., and Sun, M. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850. Association He, J., Gong, Y., Lin, Z., Wei, C., Zhao, Y., and Chen, K. LLM factoscope: Uncovering LLMs factual discernment through measuring inner states. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 1021810230. Association for Computational Linguistics, August 2024c. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Kim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J., Welleck, S., Neubig, G., Lee, M., Lee, K., and Seo, M. Prometheus 2: An open source language model specialized in evaluating other language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 43344353. Association for Computational Linguistics, November 2024. doi: 10.18653/v1/ 2024.emnlp-main.248. knoveleng. Amc-23. Hugging Face Dataset, 2023. https://huggingface.co/datasets/ URL knoveleng/AMC-23. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tulu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV. 2411.15124. Lambert, N., Pyatkin, V., Morrison, J., Miranda, L. J. V., Lin, B. Y., Chandu, K. R., Dziri, N., Kumar, S., Zick, T., Choi, Y., Smith, N. A., and Hajishirzi, H. Rewardbench: Evaluating reward models for language modeling. In Findings of the Association for Computational Linguistics: NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pp. 17551797. Association for Computational Linguistics, 2025. Lee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J., Lu, K., Bishop, C., Hall, E., Carbune, V., Rastogi, A., and Prakash, S. RLAIF vs. RLHF: scaling reinforcement learning from human feedback with AI feedback. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. 10 Stable Rank as Intrinsic Reward Signal Lin, B. Y., Deng, Y., Chandu, K., Ravichander, A., Pyatkin, V., Dziri, N., Bras, R. L., and Choi, Y. Wildbench: Benchmarking LLMs with challenging tasks from real users in the wild. In The Thirteenth International Conference on Learning Representations, 2025. Liu, C. Y., Zeng, L., Xiao, Y., He, J., Liu, J., Wang, C., Yan, R., Shen, W., Zhang, F., Xu, J., Liu, Y., and Zhou, Y. Skywork-reward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint arXiv:2507.01352, 2025. Mesgar, M. and Strube, M. neural local coherence model for text quality assessment. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 43284339, Brussels, Belgium, OctoberNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1464. Mikolov, T., Yih, W.-t., and Zweig, G. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 746751, Atlanta, Georgia, June 2013. Association for Computational Linguistics. OpenAI. Gpt-4o mini: more efficient multimodal model, 2025. URL https://openai.com/blog/ gpt-4o-mini. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Roy, O. and Vetterli, M. The effective rank: measure of effective dimensionality. In 2007 15th European signal processing conference, pp. 606610. IEEE, 2007. Rudelson, M. and Vershynin, R. Sampling from large matrices: An approach through geometric functional analysis. J. ACM, 54(4):21es, July 2007. ISSN 0004-5411. doi: 10.1145/1255443.1255449. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ ARXIV.2402.03300. Singhal, P., Goyal, T., Xu, J., and Durrett, G. long way to go: Investigating length correlations in RLHF. In First Conference on Language Modeling, 2024. Skalse, J., Howe, N. H. R., Krasheninnikov, D., and Krueger, D. Defining and characterizing reward hacking. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22. Curran Associates Inc., 2022. ISBN 9781713871088. Wang, J., Wang, H., Sun, S., and Li, W. Aligning language models with human preferences via bayesian approach. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23. Curran Associates Inc., 2023. Wang, P., Li, L., Chen, L., Cai, Z., Zhu, D., Lin, B., Cao, Y., Kong, L., Liu, Q., Liu, T., and Sui, Z. Large language In Proceedings of the models are not fair evaluators. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 94409450. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.511. Wu, Z., Hu, Y., Shi, W., Dziri, N., Suhr, A., Ammanabrolu, P., Smith, N. A., Ostendorf, M., and Hajishirzi, H. Finegrained human feedback gives better rewards for language model training. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23. Curran Associates Inc., 2023. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report. 11 Stable Rank as Intrinsic Reward Signal Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV. 2412.15115. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report. CoRR, abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505.09388. Yang, Z., Dai, Z., Salakhutdinov, R., and Cohen, W. W. Breaking the softmax bottleneck: high-rank RNN language model. In International Conference on Learning Representations, 2018. Yin, Z., Sun, Q., Zeng, Z., Cheng, Q., Qiu, X., and Huang, X. Dynamic and generalizable process reward modeling. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 42034233, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-889176-251-0. doi: 10.18653/v1/2025.acl-long.212. Yuan, W., Pang, R. Y., Cho, K., Li, X., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. Zhang, Q., Qiu, L., Hong, I., Xu, Z., Liu, T., Li, S., Zhang, R., Li, Z., Li, L., Yin, B., Zhang, C., Chen, J., Jiang, H., and Zhao, T. Self-rewarding PPO: Aligning large language models with demonstrations only. In Second Conference on Language Modeling, 2025a. Zhang, Y. and Math-AI, T. American invitational mathematics examination (aime) 2025, 2025. Zhang, Z., Zheng, C., Wu, Y., Zhang, B., Lin, R., Yu, B., Liu, D., Zhou, J., and Lin, J. The lessons of developing process reward models in mathematical reasoning. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 1049510516. Association for Computational Linguistics, July 2025b. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.547. Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, Judging llm-as-aH., Gonzalez, J. E., and Stoica, I. In Advances judge with mt-bench and chatbot arena. in Neural Information Processing Systems 36: Annual Stable Rank as Intrinsic Reward Signal Figure 3. Cross-layer stable rank performance on RewardBench across different model families. We evaluate the stable rank computed from each transformer layer as reward proxy on RewardBenchs six subcategories. Final layers (rightmost) consistently achieve the highest accuracy across models and categories, validating our choice to use final-layer activations in SR-GRPO. A. Cross-Layer Stable Rank Analysis We analyze how stable rank performance varies across transformer layers by computing R(l) 2 at each layer and evaluating it as reward proxy on RewardBench (Lambert et al., 2025). We test three model families: Qwen3-8B (36 layers) (Yang et al., 2025), Qwen2.5-1.5B-Instruct (28 layers) (Yang et al., 2024), and Phi-3.5-mini-instruct (32 layers) (Abdin et al., 2024), measuring accuracy on five subcategories: Chat, Chat Hard, Safety, Code, and Math. SR(y) = Hl2 /Hl Final layers encode quality signals most effectively. Figure 3 shows that stable rank performance varies dramatically across layers. Early and middle layers (indices 5-25) achieve near-random performance (50% accuracy) across all categories, indicating that stable rank computed from these layers carries minimal quality information. In contrast, final layers (indices 28-32 depending on the model) exhibit sharp performance improvements, with accuracy rising to 70-85% on most categories. This pattern holds consistently across all three model families, suggesting that semantic abstraction in deeper layers is crucial for stable rank to capture response quality. Task-dependent layer sensitivity. While final layers universally outperform earlier layers, we observe task-specific patterns. On Chat and Chat Hard categories, performance plateaus across middle layers before rising sharply at the end, suggesting that conversational quality signals emerge primarily in final layers. For Safety, we observe an interesting U-shape on Qwen3-8B and Phi-3.5-mini, with both very early (layer 0-2) and final layers achieving reasonable performance. This may indicate that safety-related features are encoded at multiple levels of abstraction. Math and Code categories show the most dramatic improvements in final layers, with Qwen2.5-1.5B achieving near-perfect accuracy (95-98%) on Math using layer 27, highlighting that complex reasoning quality is best captured by the deepest semantic representations. Implications for SR-GRPO. These findings strongly support our design choice in SR-GRPO to extract stable rank from the final transformer layer. The consistent performance advantage of final-layer stable rank (70-85% accuracy) over middle layers (45-50% accuracy) demonstrates that quality-indicative geometric structure emerges primarily in deep semantic representations. This validates that maximizing final-layer stable rank during RL optimization directly targets the representational regime where quality signals are most pronounced, enabling effective annotation-free alignment. 13 Stable Rank as Intrinsic Reward Signal B. Baseline Implementation Details In this section, we provide the prompts and implementation details for all the baselines. B.1. Generative Baselines In our experiments, we have two generative baselines: (1) Pointwise Scoring (Kim et al., 2024) and (2) Pairwise Comparison (Kim et al., 2024). B.1.1. POINTWISE SCORING PROMPT For the Pointwise Scoring baseline, we use the following prompt structure to score each response on 1-5 scale: ###Task Description: An instruction (might include an Input inside it), response to evaluate, reference answer that gets score of 5, and score rubric representing evaluation criteria are given. 1. Write detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing feedback, write score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as follows: \"Feedback: (write feedback for criteria) [RESULT] (an integer number between 1 and 5)\" 4. Please do not generate any other opening, closing, and explanations. ###The instruction to evaluate: {instruction} ###Response to evaluate: {response} ###Reference Answer (Score 5): {reference_answer} ###Score Rubrics: {criteria} Score 1: {score_description_1} Score 2: {score_description_2} Score 3: {score_description_3} Score 4: {score_description_4} Score 5: {score_description_5} ###Feedback: We extract the numerical rating from the models output by parsing the [RESULT] token and compare scores to determine the preferred response. The reference answer and score rubrics are adapted from RewardBenchs provided materials. B.1.2. PAIRWISE COMPARISON PROMPT For the Pairwise Comparison baseline, we prompt the model to directly compare two responses: ###Task Description: An instruction (might include an Input inside it), response to evaluate, and score rubric representing evaluation criteria are given. 1. Write detailed feedback that assess the quality of two responses strictly based on the given score rubric, not evaluating in general. 2. After writing feedback, choose better response between Response and Response B. You should refer to the score rubric. 3. The output format should look as follows: \"Feedback: (write feedback for criteria) [RESULT] (A or B)\" 4. Please do not generate any other opening, closing, and explanations. 14 Stable Rank as Intrinsic Reward Signal ###Instruction: {instruction} ###Response A: {response_a} ###Response B: {response_b} ###Score Rubric: {criteria} ###Feedback: We parse the models output to extract the preference (A or B) from the [RESULT] token. To account for positional bias, we evaluate each pair twice with responses swapped and only count predictions where both orderings agree. B.2. IPO Implementation We follow the evaluation protocol from IPO (Garg et al., 2025) for RewardBench. IPO computes implicit preference scores by prompting the model with task-specific Yes/No verification questions and extracting token probabilities from the final logits. B.2.1. TASK-SPECIFIC EVALUATION PROMPTS We use category-specific verification prompts adapted from the IPO codebase. All prompts follow the format: [verification prompt] User: {instruction} Response: {response}. We automatically assign categories based on subset names: Code tasks contain hepor code, Math tasks contain mathor prm, Safety tasks contain refusals-, xstest-, or donotanswer, and all others default to Chat. The prompt structure varies by task: Code: Below is coding-related question along with response containing code. Your task is to verify whether the provided code is correct, functional, and aligned with the requirements of the question. Answer with just Yes/No Math: Below is math question along with response containing solution. Your task is to verify whether the solution is correct, the reasoning is logical, and the final answer is accurate. Answer with just Yes/No Safety: Inspect the response to ensure it is safe and appropriate for the question below. Answer Yes/No. Chat (default): Below is question, along with response to that question. Your task is to verify whether the response is helpful, factually correct, logically sound, and contextually appropriate. Answer with just Yes/No B.2.2. SCORE COMPUTATION Given response to instruction x, we tokenize the verification prompt and perform forward pass to obtain logits ℓ RV at the final position. We extract logits for Yes and No tokens (including variants with leading spaces), apply softmax normalization, and define the IPO score as: IPO(yx) = exp(ℓYes) exp(ℓYes) + exp(ℓNo) (3) The response with higher IPO(yx) is predicted as preferred. This approach differs from standard likelihood-based methods by explicitly prompting for binary quality judgments rather than computing sequence log-probabilities. C. RewardBench Results We show the complete performance comparison across different reward methods for RewardBench in Table 5. 15 Table 5. Complete performance comparison across different reward methods for RewardBench. Pointwise Scoring uses an LLM to grade each response on 1-5 scale independently, while Pairwise Comparison directly evaluates two responses. Stable Rank as Intrinsic Reward Signal Model Method Chat Chat Hard Safety Code Math Overall Qwen2.5 1.5B-Instruct Qwen3 0.6B Qwen3 8B Llama-3.1-Instruct 8B Phi-3.5-instruct mini Pointwise Scoring Pairwise Comparison IPO Stable Rank Pointwise Scoring Pairwise Comparison IPO Stable Rank Pointwise Scoring Pairwise Comparison IPO Stable Rank Pointwise Scoring Pairwise Comparison IPO Stable Rank Pointwise Scoring Pairwise Comparison IPO Stable Rank 37.61 56.14 70.48 87.28 37.54 49.01 54.99 55. 80.54 77.15 55.40 87.22 51.73 55.52 56.07 70.63 32.01 77.10 49.32 65.35 38.36 50.37 70.93 85.70 39.10 43.27 63.23 72.87 81.44 71.80 85.93 84. 49.29 63.47 60.56 76.50 37.88 62.74 66.48 84.60 32.05 47.59 48.28 66.54 48.24 49.00 48.14 56.44 87.43 75.84 86.18 76.20 64.10 57.17 74.75 43. 57.08 54.28 54.03 61.82 43.29 48.78 80.08 73.78 38.51 46.23 72.46 79.26 86.19 73.17 91.67 88.41 65.04 54.77 65.55 75.81 48.57 52.43 64.43 88. 34.45 64.43 59.51 66.44 30.42 48.09 89.04 71.14 82.89 61.96 70.92 84.34 51.67 52.34 33.78 74.94 36.68 63.98 97.09 54.59 37.15 53.46 65.85 75. 38.76 47.12 65.57 66.96 83.70 71.98 78.02 84.04 56.37 56.65 58.14 68.36 42.44 62.11 66.27 70.87 D. Best-of-N Complete Results We provide complete Best-of-N decoding results comparing stable rank selection against random selection across four models and five benchmarks. For each configuration, we sample candidate responses and either select randomly (Random@N) or choose the response with highest stable rank (Best@N). We report two relative metrics: Rand. measures improvement over random selection at the same N, and @1 measures improvement over greedy decoding (N=1). Table 6 shows that stable rank selection consistently outperforms random selection across all models and sampling budgets. The advantage is most pronounced on smaller models: Llama-3.2-1B achieves +33.8% relative improvement over random selection at N=16, while random selection actually degrades performance below greedy decoding (9.9%). This degradation occurs because random selection may choose low-quality samples from the candidate pool, whereas stable rank reliably identifies better responses. Notably, the gains from stable rank selection increase with for most models, suggesting that larger candidate pools provide more opportunities for quality differentiation. On math-heavy benchmarks (MATH500, OlympiadBench), stable rank selection yields particularly strong improvements, with DeepSeek-R1 improving from 82.8% to 86.8% on MATH500 and from 43.6% to 59.7% on OlympiadBench at N=16. E. SR-GRPO Training Details We provide the training configurations for SR-GRPO experiments on both models. All experiments are conducted on 4NVIDIA H800 GPUs. Qwen2.5-1.5B-Instruct. We train for 400 steps using LoRA with rank r=16 and α=32, targeting all attention and MLP projection layers. We use learning rate of 106 with cosine scheduling and 10% warmup. Training uses per-device batch size of 4 with gradient accumulation steps of 8 (effective batch size 128), mixed precision (bfloat16), and gradient checkpointing for memory efficiency. The maximum prompt and completion lengths are both set to 4096 tokens. DeepSeek-R1-Distill-Qwen-1.5B. We train for 300 steps using the same LoRA configuration (r=16, α=32). Due to the models longer reasoning traces, we use per-device batch size of 2 with gradient accumulation steps of 16 (effective batch size 128). The maximum prompt length is set to 8192 and maximum completion length to 4096 tokens. All other 16 Table 6. Complete Best-of-N decoding results (%). We compare stable rank selection (SR) against random selection (Rand.) at each N. : relative improvement of SR over Rand., computed as (SR Rand.)/Rand. 100%. Stable Rank as Intrinsic Reward Signal Benchmark Qwen2.5-1.5B Rand. SR Phi-3.5-mini SR Rand. Llama-3.2-1B DeepSeek-R1-1.5B Rand. Rand. SR SR 4 8 16 GPQA MMLU-Re. MATH Olymp. AMC23 Avg. GPQA MMLU-Re. MATH Olymp. AMC Avg. GPQA MMLU-Re. MATH Olymp. AMC23 Avg. 19.2 46.6 51.0 23.6 27.5 33.6 20.3 47.7 55.6 28.4 40. 38.4 23.2 60.3 49.8 18.2 20.0 34.3 23.7 61.2 50.8 20.0 25.0 36.2 21.2 30.0 23.8 9.3 15. 19.9 20.7 32.6 25.0 12.1 22.5 22.6 +14.4% +5.4% +13.6% 19.7 46.6 55.2 22.8 25.0 33.9 21.2 47.9 57.8 29.0 40.0 39.2 27.3 59.5 50.0 17.8 20.0 34. 30.8 61.4 52.4 19.6 25.0 37.8 21.2 31.7 22.2 9.6 7.5 18.4 25.3 32.6 28.8 12.2 25.0 24. +15.7% +8.4% +34.2% 20.2 46.7 53.4 21.0 40.0 36.3 22.2 48.2 59.6 32.4 42. 41.0 27.8 60.1 49.6 18.7 17.5 34.7 29.3 62.1 52.8 21.8 27.5 38.7 22.2 31.7 22.2 10.4 12. 19.8 32.8 32.5 29.2 12.9 25.0 26.5 25.8 45.9 80.0 51.9 80.0 56.7 25.8 46.1 83.0 52.6 75. 56.5 22.5 46.9 82.2 55.1 82.5 57.8 26.2 46.0 84.0 54.8 82.5 58.7 +3.6% 26.8 48.4 83.4 55.0 85.0 59.7 +5.7% 27.3 47.2 86.8 59.7 82.5 60.7 +13.0% +11.4% +33.8% +5.0% hyperparameters remain identical to the Qwen configuration. Stable Rank Computation. For reward calculation, we format the prompt and completion using the models chat template. We then compute hidden states from the last transformer layer using the reference model. To prevent reward hacking, we temporarily disable LoRA adapters during stable rank computation, ensuring rewards are derived from the frozen base models representations rather than the adapting policy. Hidden states are extracted only for non-padding tokens, and stable rank is computed using the method described in Section 2. Common Settings. Both models are trained on the SmolTalk2 (Allal et al., 2025) preference dataset using paged AdamW optimizer with 8-bit quantization. We set weight decay to 0, maximum gradient norm to 1.0, and LoRA dropout to 0.1. Rewards are not normalized across batches. For GRPO-specific hyperparameters, we use the default values from the HuggingFace TRL library: group size = 8, KL penalty coefficient β = 0.04, and numerical stability constant ϵ = 108 for advantage standardization. F. Experimental Details for Metric Analysis We analyze the relationship between stable rank and text quality using the RewardBench dataset (Lambert et al., 2025), which contains 2,985 prompts, each with one chosen and one rejected response (N = 5,970 responses). This appendix describes our statistical methodology, defines all metrics, and reports the complete correlation results. F.1. Statistical Methodology We employ two complementary analyses to characterize the relationship between stable rank and text metrics: Sample-level correlation. We treat each of the = 5,970 responses as an independent observation. For metric and stable rank S, we compute Pearson correlation = corr(M, S) and Spearman rank correlation ρ. This measures whether higher stable rank globally associates with higher (or lower) metric values. We apply this method to semantic coherence, information density, and basic structural metrics. Stable Rank as Intrinsic Reward Signal Table 7. SR-GRPO training configurations. Hyperparameter Qwen2.5-1.5B DeepSeek-R1-1.5B Training steps Batch size (per device) Gradient accumulation Effective batch size Learning rate Warmup ratio Group size (K) KL coefficient (β) LoRA rank (r) LoRA alpha (α) Max prompt length Max completion length Precision 400 4 8 128 106 0.1 8 0.04 16 32 4096 4096 bfloat 300 2 16 128 106 0.1 8 0.04 16 32 8192 4096 bfloat16 Paired-difference analysis. Linguistic markers (discourse and logical connectives) exhibit strong context dependence: some prompts naturally require more explanation markers than others, making absolute counts across different prompts statistically noisy. To control for this variation, we work at the pair level. For each prompt with chosen response yc and rejected response yr, we compute differences Mi = (yc) (yr), Si = S(yc) S(yr), (4) and correlate with across all = 2,985 pairs. This measures whether, within the same context, the response with higher stable rank tends to use more (positive correlation) or fewer (negative correlation) of given linguistic feature. F.2. Metric Definitions We measure text quality along three axes: semantic coherence, information density, and linguistic structure. Tables 810 define the metrics formally. For response y, let denote its token sequence (Ntokens tokens), s1, . . . , sn its sentences with embeddings v1, . . . , vn (from all-MiniLM-L6-v2 (Reimers & Gurevych, 2019)), and vp the prompt embedding. F.3. Implementation Details Sentence embeddings. We use the all-MiniLM-L6-v2 (Reimers & Gurevych, 2019) model from the sentence-transformers library, which produces 384-dimensional embeddings. The model applies mean pooling over token embeddings and L2-normalization. Sentence tokenization. We tokenize responses into sentences using NLTK 1, with regex-based fallback (splitting on sentence boundaries while handling common abbreviations). Each response contains sentences indexed 1, . . . , n. Syllable counting. For readability metrics, we count syllables using the pyphen 2 library (dictionary-based hyphenation for English). Complex words are defined as having 3 syllables following standard practice. Marker detection. Logical and discourse markers are detected via case-insensitive substring matching. For each marker category, we report: (1) raw count, (2) count normalized per sentence, and (3) count normalized per 100 tokens as (c/Ntokens) 100. F.4. Correlation Results Tables 11 and 12 report Pearson (r) and Spearman (ρ) correlations with p-values. Positive correlations indicate that higher metric values associate with higher stable rank; negative correlations indicate the opposite. 1https://www.nltk.org/ 2https://pyphen.org/ 18 Stable Rank as Intrinsic Reward Signal Table 8. Semantic coherence metrics. All metrics use sentence embeddings v1, . . . , vn R384 from all-MiniLM-L6-v2. Metric Formula Interpretation Intra-response coherence 1 i=1 cos(vi, vi+1) 1 Progression score n1 where = {(i, j) > + 1} (cid:80)n1 (cid:80) (i,j)P cos(vi, vj) Adjacent similarity sadj = 1 n1 (cid:80)n1 i=1 cos(vi, vi+1). Coherence mean sadj = 1 n1 (cid:80)n1 i=1 cos(vi, vi+1) Coherence minimum mini[1,n1] cos(vi, vi+1) Semantic density (cid:80) 1 (n 2) 1i<jn[1 cos(vi, vj)] Semantic variance 1 384 (cid:80)384 d=1 Var([v1,d, . . . , vn,d]) Topic jumps Count of pairs where cos(vi, vi+1) < µ 2σ, with µ = sadj and σ = std({cos(vi, vi+1)}) Promptresponse alignment (using prompt embedding vp) QA alignment Compute ai = cos(vp, vi) for = 1, . . . , n. Report max(ai), mean(ai), min(ai). QA consistency max(0, 1 std({ai}n i=1)) Measures forward momentum by comparing adjacent vs non-adjacent sentence similarity. Higher values indicate natural progression where each sentence builds on the previous one. Captures smooth local transitions between consecutive sentences. Statistics characterize different aspects of coherence stability. Minimum detects weak links. Average cosine similarity between consecutive sentences. Higher values indicate smoother local transitions throughout the response. Minimum adjacent similarity, identifying the weakest link in the argument chain. Low values indicate at least one abrupt topic shift or incoherent transition. Average pairwise cosine distance. Higher values indicate scattered content; lower values indicate tightly focused responses. Measures spread across each of the 384 embedding dimensions, then averages. High variance signals erratic topic shifts. Detects abrupt drops in adjacent similarity that fall significantly below baseline. Identifies sudden coherence breaks. Measures how well each response sentence aligns with the prompt. Max, mean, and min capture peak relevance, overall relevance, and worst-case relevance. Measures stable relevance throughout generation. Lower std means the response maintains consistent focus without drifting. 19 Stable Rank as Intrinsic Reward Signal Table 9. Information density and diversity metrics. These capture lexical richness and redundancy. Metric Formula / Computation Interpretation Lexical variety Typetoken ratio Moving-avg TTR n-gram diversity /Ntokens where = set of unique tokens Measures vocabulary richness. Higher ratios indicate more diverse word usage and richer language expression. Vi with window size i=1 (cid:80)Ntokensw+ 1 Ntokensw+1 = 100, sliding by 1 token For {2, 3, 4} : # unique n-grams quence # total n-grams over token seStabilized diversity measure that reduces length bias. Averages TTR over local windows for robust estimates. Detects phrase-level repetition. Lower diversity indicates phrase reuse; higher diversity suggests varied expression. Repetition rate Fraction of 10-token windows appearing 2 times Measures sustained redundant phrases. Captures copy-paste patterns or looping behavior. Information content and redundancy Compression ratio zlib.compress(y) Proxy for information density via compressibility. Higher ratios (closer to 1.0) indicate less compressible text; lower ratios suggest redundancy. Text entropy = (cid:80) frequency p(c) log2 p(c) where p(c) is character Character-level Shannon entropy. Measures inherent randomness and information content. Model confidence (using Qwen2.5-7B) Perplexity exp (cid:16) 1 Ntokens (cid:80)Ntokens i=1 log p(ti t<i) (cid:17) Measures how surprising the text is to the base LM. Lower perplexity indicates more natural language patterns. Model uncertainty 1 Ntokens tropy (cid:80)Ntokens i=1 H(p(ti+1 ti)) where is enAverage next-token entropy. Higher values indicate less confidence, more random token choices. F.5. Key Findings The correlation analysis reveals three main patterns. First, stable rank shows strongest positive associations with semantic coherence metrics: progression score (ρ = 0.313) and QA alignment consistency (ρ = 0.316). Responses maintaining smooth topic flow and stable prompt alignment achieve higher stable rank. Conversely, coherence standard deviation exhibits strong negative correlation (ρ = 0.356), indicating that inconsistent adjacent similarities reduce stable rank. Second, stable rank distinguishes information density from verbosity. While absolute measures like token count (ρ = 0.294) and sentence count (ρ = 0.368) correlate negatively, efficiency metrics show positive associations: lexical diversity (ρ = 0.238) and compression ratio (ρ = 0.233). The negative correlation with sentence count is particularly revealing: responses with many short sentences fail to build rich semantic representations. Combined with positive correlation with average sentence length (ρ = 0.085), this suggests stable rank rewards well-developed sentences over fragmented text. Third, discourse markers reveal nuanced reasoning patterns in paired-difference analysis. Most marker categories show negative correlations: additive markers (ρ = 0.156), conditional markers (ρ = 0.163), and enumeration patterns (ρ = 0.148), with total marker count negatively associated (ρ = 0.204). Responses relying heavily on common connective words tend to receive lower stable rank, consistent with favoring compact information content over explicit signaling. However, contrastive and causal markers follow different pattern: positive Pearson correlations (contrastive = 0.187, causal = 0.139) but weak Spearman correlations (contrastive ρ = 0.067, causal ρ 0). These markers often appear at branch points in arguments or when cause-effect relations are introduced, corresponding to deeper reasoning structure than simple additive or enumerative markers. The weak Spearman correlations indicate that stable rank is sensitive to whether such markers are present but does not increase monotonically with frequency, consistent with the view that contrastive and causal connectives are most informative when they highlight key reasoning steps rather than being used uniformly as stylistic choice. 20 Table 10. Linguistic structure metrics. Logical and discourse markers indicate explicit reasoning patterns; structural elements capture formatting and organization; readability scores measure text complexity. Stable Rank as Intrinsic Reward Signal Category Subcategory Representative Keywords Logical Discourse Causal Conditional Inference Comparison because, therefore, thus, consequently, leads to if, when, unless, provided that, assuming implies, suggests, indicates, proves, demonstrates similar to, different from, versus, compared to Contrastive Additive Temporal Exemplification however, but, although, nevertheless, conversely moreover, also, furthermore, in addition, besides then, next, subsequently, meanwhile, eventually for example, such as, namely, specifically, e.g. Structural Enumeration Formatting first, second, step 1, finally, to begin, in conclusion Markdown headers (#), numbered lists (1.), bullets (-), code blocks Normalization Raw, per-sent, per-100-tok Raw, per-sent, per-100-tok Raw, per-sent, per-100-tok Raw, per-sent, per-100-tok Raw, per-sent, per-100-tok Raw, per-sent, per-100-tok Raw, per-sent, per-100-tok Raw, per-sent, per-100-tok Raw, per-sent, per-100-tok Raw, per-sent, per-100-tok G. Ablation: Comparison with Other Intrinsic Dimension Metrics While stable rank has shown strong performance as reward signal, other intrinsic dimension metrics exist that measure the effective dimensionality of neural representations. We compare stable rank against three alternative metrics: condition number, PCA-based 95% variance dimension, and effective rank. All metrics are computed on the final hidden layer of Qwen3-8B(Yang et al., 2025) using the same RewardBench evaluation protocol. Metrics. We evaluate four intrinsic dimension metrics: Stable rank: sr(X) = X2 /X2 2, the ratio of squared Frobenius norm to squared spectral norm. Effective rank (Roy & Vetterli, 2007): er(X) = exp(H(σ)), where H(σ) is the Shannon entropy of the normalized singular value distribution σi = σi/ (cid:80) σj. Condition number: κ(X) = σmax/σmin, the ratio of largest to smallest singular value. We use 1/κ as the score so that higher values indicate better quality. PCA 95% variance: The number of principal components needed to capture 95% of the variance in the hidden states. Results. Table 13 shows that stable rank outperforms all alternative metrics across every category, achieving 84.04% overall accuracy compared to 61.91% for PCA 95% variance, 54.50% for effective rank, and 36.04% for condition number. The gap is largest on challenging categories: stable rank reaches 84.34% on Math while effective rank and condition number fall below 13%, and 76.20% on Safety while alternatives remain below 46%. Among baseline metrics, PCA 95% variance performs best overall but struggles on Math (27.96%) and Safety (45.45%). Effective rank shows moderate Code performance (76.12%) but fails on Math (12.75%). Condition number performs poorly across all categories, suggesting that extreme singular value ratios are unreliable quality indicators. Analysis. Stable rank outperforms alternatives due to two key properties. First, it aggregates information across the entire singular value spectrum through the Frobenius norm, making it robust to outliers that affect condition number. Second, it balances representational richness (Frobenius norm) with coherence (spectral norm): high-quality responses activate diverse semantic dimensions in structured manner, while low-quality responses either collapse into low-dimensional representations or exhibit erratic noise. The entropy weighting in effective rank and discrete counting in PCA 95% variance appear less suited for capturing these quality distinctions. H. Ablation: Context Window Size We investigate how context window size affects stable rank performance on RewardBench. For each prompt-response pair, we truncate the combined sequence to maximum length in {128, 512, 1024, 2048, 4096} tokens before computing stable 21 Stable Rank as Intrinsic Reward Signal Table 11. Sample-level correlations between stable rank and text metrics (N = 5, 970). Progression score (r = 0.299) and QA consistency (r = 0.258) show strongest positive associations; coherence std (r = 0.298) and total markers (r = 0.281) show strongest negative associations. All < 0.001 except where noted. Metric Pearson Spearman ρ Semantic coherence and alignment Progression score Coherence minimum QA alignment consistency Avg adjacent similarity Avg nonadjacent similarity Coherence mean 0.299 0.273 0.258 0.250 0.259 0.233 Semantic density Semantic variance Coherence std 0.250 0.272 0.298 Information density and diversity Lexical diversity (TTR) Moving-avg TTR Compression ratio 2-gram diversity 3-gram diversity 4-gram diversity 0.217 0.133 0.125 0.127 0.099 0.075 Repetition rate Text entropy Unique tokens Length and structure Length (tokens) Sentence count Has headers Avg sentence length Code blocks (per sent) Complex word ratio 0.015 0.066 0.221 0.211 0.218 0.231 0.139 0.078 0.093 <0.001 <0.001 <0.001 <0.001 <0.001 <0. <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 0.241 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 0.313 0.299 0.316 0.267 0.278 0. 0.252 0.302 0.356 0.238 0.139 0.233 0.189 0.188 0.173 0.109 0.145 0.286 0.294 0.368 0.276 0.085 0.072 0.185 <0.001 <0.001 <0.001 <0.001 <0.001 <0. <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 Table 12. Paired-difference correlations between marker differences and stable-rank differences (N = 2, 985 pairs). Positive correlation means chosen responses have more markers when stable rank is higher. Most markers show negative associations, indicating chosen responses use fewer markers. Marker Type (per 100 tokens) Pearson Spearman ρ Positive associations Contrastive discourse Causal logical Logical markers (total) Total markers Negative associations Comparison logical Exemplification discourse Inference logical Temporal discourse Enumeration structural Additive discourse Conditional logical Total markers (raw) <0.001 <0.001 <0.001 0.034 0.061 0.002 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 0.067 0.002 0.042 0.063 0.078 0.084 0.115 0.122 0.148 0.156 0.163 0.204 <0.001 0.926 0.024 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0. 0.187 0.139 0.077 0.039 0.034 0.057 0.065 0.106 0.122 0.166 0.163 0.149 22 Table 13. RewardBench accuracy (%) using different intrinsic dimension metrics as reward proxies (Qwen3-8B, final layer). Higher is better. Stable Rank as Intrinsic Reward Signal Metric Chat Chat Hard Safety Code Math Overall Condition number PCA 95% variance Effective rank Stable rank 75.96 91.23 87.72 87. 31.49 74.90 59.51 84.02 31.32 45.45 36.40 76.20 28.46 70.02 76.12 12.98 27.96 12. 88.41 84.34 36.04 61.91 54.50 84.04 rank on the final layer of Qwen3-8B (Yang et al., 2025). RewardBench examples have mean length of 281 tokens, with 75% under 512 tokens. Table 14. Effect of maximum input length on RewardBench accuracy across categories (Qwen3-8B, final-layer stable rank). Max length Chat Chat Hard Safety Code Math Overall 128 512 1024 2048 4096 78.25 86.89 86.88 87.23 87.23 63.07 84.38 84.02 84.02 84.02 76.37 76.20 76.20 76.20 76. 24.80 87.91 88.41 88.41 88.41 70.47 83.89 84.34 84.34 84.34 62.59 83.85 83.97 84.04 84.04 Results. Table 14 shows that stable rank accuracy degrades at short context lengths but saturates quickly. At 128 tokens, overall accuracy drops to 62.59%, 21 percentage point decrease from 512 tokens (83.85%). Code suffers most severely, falling from 87.91% to 24.80%, because truncation removes critical program logic. However, extending beyond 512 tokens yields minimal gains: increasing to 4096 tokens improves accuracy by only 0.2 percentage points. Analysis. The rapid saturation indicates that stable rank captures semantic content rather than mechanically rewarding longer sequences. Short windows (128 tokens) harm performance by cutting off meaningful content, but once the window covers the core reasoning structure (around 512 tokens for this dataset), additional context provides diminishing returns. Safety performance remains constant at 76.20% for all lengths 512, suggesting that refusal quality is determined early in the response. Practical Recommendations. 512-token window provides an effective operating point for datasets with similar length distributions. For longer-form tasks, we recommend scaling the window to cover at least the 75th percentile of sequence lengths to avoid truncation artifacts. I. Ablation: Input Prompt Format We investigate whether input prompt format affects stable rank performance. Since stable rank is computed on hidden states, the concatenation structure could influence the resulting geometry. We test six formats ranging from minimal (no prefix) to structured (User/Assistant tags). (1) {prompt}nn{response} (no prefix) (2) {prompt}nnResponse:{response} (3) {prompt}nnAssistant:{response} (4) User:{prompt}nnAssistant:{response} (5) Question:{prompt}nnAnswer:{response} (6) {prompt}nnAnswer:{response} 23 Stable Rank as Intrinsic Reward Signal Table 15. Effect of prompt format on RewardBench accuracy. Format indices correspond to the list above. Overall accuracy varies by at most 3 percentage points across formats for each model. Model Fmt Chat Hard Safety Code Math Overall Phi-3.5-mini-instruct Qwen2.5-1.5B-Instruct Qwen3-0.6B 1 2 3 4 5 1 2 3 4 5 6 1 2 3 4 5 6 82.79 82.14 78.39 77.07 81.19 84.56 88.33 85.21 86.96 87.28 86.30 87.28 83.67 82.98 82.28 80.54 83.67 83.30 86.36 84.70 85.60 87.55 85.46 84. 87.08 86.25 85.41 86.60 84.22 85.70 80.04 80.94 81.84 78.85 81.01 80.11 53.99 57.17 62.63 62.07 59.13 52.82 64.60 65.27 67.00 69.32 65.47 66.54 51.39 50.64 47.62 48.72 47.77 49.14 82.72 87.91 89.43 89.53 87.91 87. 72.66 74.19 74.39 75.71 74.39 73.78 76.83 75.71 76.83 76.93 78.05 76.22 44.30 40.94 40.94 45.86 51.23 46.09 63.09 64.65 61.74 52.80 68.46 66.44 49.44 48.99 43.18 44.30 44.52 41.61 70.03 70.57 71.40 72.42 72.99 71. 75.15 75.11 75.10 74.34 75.77 75.95 68.27 67.85 66.35 65.87 67.00 66.08 Results. Table 15 shows that stable rank is robust to format variation. Overall accuracy varies by at most 3 percentage points across formats for each model, and no single format consistently outperforms others. Category-level variations are larger (e.g., Safety on Phi-3.5-mini ranges from 52.82% to 62.63%) but show no systematic pattern across models, indicating that format effects are model-specific rather than fundamental to stable rank. Practical Recommendations. The robustness to prompt format simplifies deployment: practitioners can use simple formats without extensive tuning. We recommend using the models native chat template when available, or minimal format like {prompt}nnAnswer:{response} otherwise. J. Stable Rank Example Prompts To complement the quantitative analysis in Section 5, we provide examples illustrating how stable rank differentiates high-quality responses from two common failure modes: catastrophic repetition and redundant verbosity. Catastrophic Repetition. The Image Processing example in Table 16 shows response that begins coherently but enters an infinite repetition loop (Image > Image > ...), collapsing into degenerate low-dimensional representation. This failure directly relates to coherence standard deviation, which has the strongest negative correlation with stable rank (ρ = 0.356). Geometrically, repetition concentrates variance into single direction, reducing the effective dimensionality that stable rank measures. Redundant Verbosity. The Math Problem example shows response that correctly solves the problem but continues with unnecessary elaboration (However..., Alternatively...). While the content is not incorrect, it dilutes information density. This aligns with our finding that stable rank correlates negatively with token count (ρ = 0.294) and total discourse markers (ρ = 0.204), while correlating positively with compression ratio (ρ = 0.233). The high stable rank response terminates precisely at the answer, maintaining focused information flow. 24 Stable Rank as Intrinsic Reward Signal Table 16. Qualitative Comparison of Model Responses. High stable rank models produce coherent and accurate instructions, whereas low stable rank models suffer from severe degeneration, exhibiting infinite repetition loops (e.g., in Image Processing) and logical inconsistencies (e.g., in Math Problems). Example Query Response Image Processing can see that in case of PNG you just save the image as is, But still want to control its size/quality Math Problem Cities and are 45 miles apart. Alicia bikes towards at 18 miles per hour. Beth bikes toward at 12 miles per hour. How many miles from City will they be when they meet? High Stable Rank: When working with PNG files, there are several ways to control their size and quality: 1. Use ImageMagick or Other Command-Line Tools: convert input.png -resize 100x100 output.png (resizes) and convert input.png -quality 85 output.png (compresses). 2. Use JavaScript Libraries: Libraries like canvas allow you to manipulate and resize images. (Includes the full JavaScript canvas code snippet) 3. Use Online Tools: There are online tools like OnlineImageResize. 4. Use WebP: Consider using WebP, which is more efficient... convert input.png -quality 85 output.webp. Low Stable Rank: PNG (Portable Network Graphics) is lossless image format... If you need to control the size or quality... Steps to Control PNG Size or Quality: 1. Using Photoshop: - Open the PNG image. - Go to Image > Image Size to adjust the size... - Under the Image Size dialog box, you can set the Resolution to value... - To change the quality, go to Image > Adjustments > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > Image > ... High Stable Rank: To solve this problem, we can follow these steps: 1. Calculate their combined speed: 18 + 12 = 30 miles per hour. 2. Determine the time it takes for them to meet: Time = Distance/Speed = 45/30 = 1.5 hours. 3. Calculate the distance covered by Alicia: Distance = 18 1.5 = 27 miles. Therefore, the final answer is: 27 . Low Stable Rank: To solve this problem, we need to determine the distance covered by Alicia and Beth... 1. Combined speed is 18 + 12 = 30 miles per hour. 2. Time it takes for them to meet: 45 miles/30 mph = 1.5 hours. 4. Distance covered by Alicia: 18 mph 1.5 = 27 miles. 6. However, this distance is the distance from the starting point of City A... Alternatively, we can take simpler approach: Since the distance between the cities is 45 miles... The shortest and simplest way to solve this is by considering the proportion... Alicia covers proportion of 18/(18 + 12) = 18/30 = 0.6 of the distance."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology"
    ]
}