{
    "paper_title": "Revisiting Text Ranking in Deep Research",
    "authors": [
        "Chuan Meng",
        "Litu Ou",
        "Sean MacAvaney",
        "Jeff Dalton"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external evidence, and reason over it. Despite search's essential role in deep research, black-box web search APIs hinder systematic analysis of search components, leaving the behaviour of established text ranking methods in deep research largely unclear. To fill this gap, we reproduce a selection of key findings and best practices for IR text ranking methods in the deep research setting. In particular, we examine their effectiveness from three perspectives: (i) retrieval units (documents vs. passages), (ii) pipeline configurations (different retrievers, re-rankers, and re-ranking depths), and (iii) query characteristics (the mismatch between agent-issued queries and the training queries of text rankers). We perform experiments on BrowseComp-Plus, a deep research dataset with a fixed corpus, evaluating 2 open-source agents, 5 retrievers, and 3 re-rankers across diverse setups. We find that agent-issued queries typically follow web-search-style syntax (e.g., quoted exact matches), favouring lexical, learned sparse, and multi-vector retrievers; passage-level units are more efficient under limited context windows, and avoid the difficulties of document length normalisation in lexical retrieval; re-ranking is highly effective; translating agent-issued queries into natural-language questions significantly bridges the query mismatch."
        },
        {
            "title": "Start",
            "content": "Chuan Meng The University of Edinburgh Edinburgh, United Kingdom chuan.meng@ed.ac.uk Sean MacAvaney University of Glasgow Glasgow, United Kingdom sean.macavaney@glasgow.ac.uk Litu Ou The University of Edinburgh Edinburgh, United Kingdom litu.ou@ed.ac.uk Jeff Dalton The University of Edinburgh Edinburgh, United Kingdom jeff.dalton@ed.ac.uk 6 2 0 2 5 ] . [ 1 6 5 4 1 2 . 2 0 6 2 : r Abstract Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external evidence, and reason over it. Despite searchs essential role in deep research, black-box web search APIs hinder systematic analysis of search components, leaving the behaviour of established text ranking methods in deep research largely unclear. To fill this gap, we reproduce selection of key findings and best practices for IR text ranking methods in the deep research setting. In particular, we examine their effectiveness from three perspectives: (i) retrieval units (documents vs. passages), (ii) pipeline configurations (different retrievers, re-rankers, and re-ranking depths), and (iii) query characteristics (the mismatch between agent-issued queries and the training queries of text rankers). We perform experiments on BrowseComp-Plus, deep research dataset with fixed corpus, evaluating 2 open-source agents, 5 retrievers, and 3 re-rankers across diverse setups. We find that agent-issued queries typically follow web-search-style syntax (e.g., quoted exact matches), favouring lexical, learned sparse, and multi-vector retrievers; passage-level units are more efficient under limited context windows, and avoid the difficulties of document length normalisation in lexical retrieval; re-ranking is highly effective; translating agent-issued queries into natural-language questions significantly bridges the query mismatch. CCS Concepts Information systems Information retrieval. Keywords Text ranking, Retrieval, Re-ranking, Deep research"
        },
        {
            "title": "1 Introduction\nText ranking is a core part of information retrieval (IR) [22, 28,\n35, 38, 39, 53, 63]. It aims to produce an ordered list of texts re-\ntrieved from a corpus in response to a query [26]. Text ranking\nmethods have been studied across diverse settings, from single-\nhop [4, 21] and multi-hop search [15, 60] to reasoning-intensive\nsearch [40, 44] and retrieval-augmented generation (RAG) [3, 32, 45].\nRecently, the IR community has witnessed the emergence of deep\nresearch [42] scenarios. Deep research aims to answer multi-hop,\nreasoning-intensive queries that require extensive exploration of",
            "content": "the open web and are difficult for both humans and large language models (LLMs) [50]. To tackle this task, growing number of studies build agents that interact with live web search APIs to obtain information [23, 27, 64]. Such agents typically use LLMs as their decision-making core and perform multiple rounds of chain-ofthought (CoT) reasoning [51] and search invocations [18, 24, 43]. Motivation. Despite the essential role of search in deep research, how existing text ranking methods perform in this setting remains poorly understood. Most prior studies rely on black-box live web search APIs [56, 64], which lack transparency, thereby hindering systematic analysis and clear understanding of the contribution of search components [6, 17]. Although recent work [6] builds deep research dataset with fixed document corpus and human-verified relevance judgments, it evaluates only two retrievers on the dataset. Research goal. In this paper, we examine to what extent established findings and best practices in text ranking methods are generalisable to the deep research setup. We identify three research gaps in text ranking for deep research that have received limited attention. First, passage-level information units have received little attention in deep research. Most existing studies build deep research agents that search and read at the document level (i.e., full web pages). Because feeding full web pages to LLM-based agents quickly exhausts the context window, prior work [6, 41] typically returns truncated documents to agents, which may remove relevant content and lead to information loss. Although prior work introduces an additional full-document read tool that agents can invoke to access complete documents [6], it adds system complexity. There is strong motivation to explore passage-level units in deep research: (i) their concise nature makes them more efficient under limited context-window budgets; (ii) they allow agents to access any relevant segments within document, avoiding information loss from document truncation; (iii) passages can enhance lexical retrieval by avoiding the difficulties of document length normalisation [19]; and (iv) large body of neural retrievers has been developed for passage retrieval [11, 12, 22, 39], but it remains unclear how well they perform in deep research. To address this gap, we ask RQ1: To what extent are existing retrievers effective in deep research under passage-level and document-level retrieval units? In this RQ, we revisit key findings that neural retrievers often underperform lexical methods (e.g., BM25 [38]) on out-of-domain data [46], and that learned sparse and multi-vector dense (a.k.a. late-interaction) retrievers generalise better than single-vector dense retrievers on out-of-domain data [11, 46]. Second, our understanding of re-ranking in deep research remains limited. Re-ranking plays an important role in lifting relevant documents to the top of ranked lists in traditional search settings [34]. It remains unclear whether widely-used re-ranking methods [28, 35, 53] provide consistent benefits when the content consumer is an LLM-based agent rather than human user. Despite recent work [41] examining one specific re-ranking method with single retriever, systematic evaluation across various ranking configurations is still lacking in deep research. To address this gap, we ask RQ2: To what extent is re-ranking stage effective in deep research under different initial retrievers, re-ranker types, and re-ranking cutoffs? In this RQ, we revisit established findings that re-ranking effectively improves ranking performance [28, 34, 46], deeper reranking generally produces better results [29], and reasoning-based re-rankers outperform their non-reasoning counterparts [53, 59]. Third, the potential mismatch between agent-issued queries and the queries used to train existing text ranking methods remains underexplored. Many text ranking methods in the IR community are trained on natural-language-style questions, such as those in MS MARCO [4]. However, agent-issued queries may not align with the queries these methods expect. Prior studies show that queryformat mismatch between training and inference can significantly hurt neural retrieval quality [31, 65]. It remains unclear how such potential mismatch affects the performance of existing text ranking methods in deep research. To address this gap, we ask RQ3: To what extent does the mismatch between agent-issued queries and the training queries used for text ranking methods affect their performance? Experiments. We perform experiments on BrowseComp-Plus [6], deep research dataset that provides fixed document corpus and human-verified relevance judgments.1 To ensure broad coverage, we use widely-used retrievers spanning 4 main paradigms in modern IR: lexical-based sparse (BM25 [38]), learned sparse (SPLADEv3 [22]), single-vector dense (RepLLaMA [28], Qwen3-Embed [63]), and multi-vector dense retrievers (ColBERTv2 [39]). For re-ranking, we select methods that trade off effectiveness and efficiency at 3 operational points: relatively inexpensive re-ranker (monoT53B [35]), an LLM-based re-ranker (RankLLaMA-7B [28]), and CoT-based reasoning re-ranker (Rank1-7B [53]), which generates additional reasoning tokens. We use two open-source LLM-based agents: gpt-oss-20b [1] and GLM-4.7-Flash (30B) [62]. Findings. For RQ1, our findings are fivefold: (i) The concise nature of passage-level units enables more search and reasoning iterations before reaching context-window limits, resulting in higher answer accuracy than document-level units (without full-document reader), particularly for the gpt-oss-20b agent that has shorter context windows. (ii) BM25 on the passage corpus outperforms neural retrievers in most cases (gpt-oss-20b with BM25 achieves the highest accuracy of 0.572 across all retrieval settings in our study). We find agent-issued queries tend to follow web-search style with keywords, phrases, and quotation marks for exact matching (see Table 5), favouring lexical retrievers such as BM25. (iii) On the document corpus, BM25 performs worst under the parameter settings used in prior work [6]. We find that these parameters lack proper document-length normalisation. With document-oriented 1 To the best of our knowledge, at the time of writing, this is the only publicly available deep research dataset providing this setup. Chuan Meng et al. parameters, however, BM25 becomes highly competitive. This highlights BM25s sensitivity to length normalisation and the advantage of passage-level units, which reduce reliance on document-length normalisation. (iv) learned sparse [22] and multi-vector dense [39] retrievers with only millions of parameters generalise better to web-search-style queries than 7B/8B single-vector dense models. (v) Enabling full-document reader on truncated documents generally improves answer accuracy and reduces search calls, suggesting that the reader complements truncated inputs. In contrast, adding the reader to the passage corpus slightly degrades performance, likely because passage retrieval already provides access to any segments within document, rendering the reader redundant. For RQ2, re-ranking consistently improves ranking effectiveness and answer accuracy while reducing search calls, confirming its important role in deep research. These gains are further amplified by deeper re-ranking depths and stronger initial retrievers. Notably, the BM25monoT5-3B pipeline with gpt-oss-20b achieves the best results in our work, reaching 0.716 recall and 0.689 accuracy. Despite using only 20B agent with BM25 and 3B re-ranker, this setup approaches the 0.701 accuracy of GPT-5based agent (Table 1 in [6]). The reasoning-based Rank1 [53] shows no clear advantage over non-reasoning methods, as it often misinterprets the intent of keyword-rich web-search queries, limiting the benefits of reasoning. For RQ3, we propose query-to-question (Q2Q) method to translate agent-issued web search queries into natural-language questions (similar to MS MARCO-style questions [4]), significantly improving neural retrieval and re-ranking performance. This indicates that the mismatch between agent-issued queries and queries used for training neural rankers can severely degrade neural ranking effectiveness. Mitigating this traininginference query mismatch is therefore critical for improving neural rankers in deep research. Contributions. Our main contributions are as follows: To the best of our knowledge, we are the first to reproduce comprehensive set of text ranking methods in the context of deep research. We construct passage corpus for the recent deep research dataset BrowseComp-Plus. Experiments across 2 open-source agents, 5 retrievers, and 3 rerankers reveal the effectiveness of the following components in deep research: passage-level information units, retrievers suited to web-search-style queries, re-ranking, and mitigation of the traininginference query mismatch in neural ranking. We open-source our code, data, and all agent-generated traces of reasoning and search calls at https://github.com/ChuanMeng/ text-ranking-in-deep-research. Following the BrowseComp-Plus protocol, the traces are released in an encrypted format and can be locally decrypted for post-hoc analyses of agent behaviour."
        },
        {
            "title": "2 Task definition\nText ranking has been extensively studied in ad-hoc search, which\ntypically follows a single-shot paradigm over user-issued queries.\nGiven a query ğ‘ğ‘¢ issued by a user and a corpus of documents\nğ¶ = {ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘› } with ğ‘› documents, the goal of a text ranking\nmethod ğ‘“ is to return a ranked list ğ· âŠ† ğ¶ of size ğ‘˜, i.e., ğ· = ğ‘“ (ğ‘ğ‘¢, ğ¶).\nText ranking in deep research. We follow the ReAct [61] para-\ndigm, widely used in recent work [6, 55], to define text ranking in",
            "content": "Revisiting Text Ranking in Deep Research deep research. Given query ğ‘ğ‘¢ issued by user, deep research agent ğ´ takes ğ‘ğ‘¢ as input and performs multiple iterations of reasoning and search before producing the final answer ğ‘. At iteration ğ‘¡, the agent generates reasoning trace ğ‘ ğ‘¡ that determines whether to output the final answer ğ‘ or invoke search to gather information. If the agent decides to invoke search, it issues search query ğ‘ğ‘¡ to text ranking method ğ‘“ , which returns ranked list ğ·ğ‘¡ : ğ‘ğ‘¡ = ğ´(ğ‘ğ‘¢, ğ‘ 0, ğ‘0, ğ·0, . . . , ğ‘ ğ‘¡ ), ğ·ğ‘¡ = ğ‘“ (ğ‘ğ‘¡ ), (1) where ğ‘ 0, ğ‘0 are the reasoning trace and query generated by the agent at the initial iteration, respectively; ğ·0 is the ranked list returned by ğ‘“ in response to ğ‘0. The ranked list ğ·ğ‘¡ with ğ‘˜ documents is then fed back to the agent to produce the reasoning trace at the next iteration ğ‘¡ + 1: ğ‘ ğ‘¡ +1 = ğ´(ğ‘ğ‘¢, ğ‘ 0, ğ‘0, ğ·0, . . . , ğ‘ ğ‘¡, ğ‘ğ‘¡, ğ·ğ‘¡ ). Note that some agents may perform multiple consecutive search invocations or reasoning steps; for simplicity, the above definition assumes alternating reasoning and search steps. (2)"
        },
        {
            "title": "3.1 Research questions and experimental design\nRQ1 To what extent are existing retrievers effective in deep re-\nsearch under passage-level and document-level retrieval units?",
            "content": "To address RQ1, we reproduce widely-used retrievers from different categories (see Section 3.2.2) in deep research, and compare their performance on passage and document corpora. BrowseCompPlus [6], the dataset used in this study, provides only document corpus; we construct passage corpus (see Section 3.2.4). When using document-level retrieval units, prior work [6, 41] typically feeds truncated retrieved documents to agents to avoid exhausting the context window; Chen et al. [6] further introduces full-document reader tool that allows agents to access complete documents when needed. Accordingly, we evaluate three settings: (i) agents retrieve documents but read truncated versions; (ii) same as (i), but with full-document reader tool; and (iii) agents retrieve and read passages. In addition, we consider setting in which agents retrieve and read passages, and can choose to invoke the full-document reader to read the source document of retrieved passage. RQ2 To what extent is re-ranking stage effective in deep research under different initial retrievers, re-ranker types, and re-ranking cut-offs? To address RQ2, we reproduce widely-used re-rankers (see Section 3.2.2), and evaluate text ranking pipelines under various configurations, including different initial retrievers, different re-rankers, and different re-ranking depths. RQ3 To what extent does the mismatch between agent-issued queries and the training queries used for text ranking methods affect their performance? To address RQ3, we compare the performance of text ranking methods using agent-issued search queries with their performance using natural-language questions similar to those in MS MARCO [4], on which many neural rankers are trained. We propose query-toquestion (Q2Q) method, which translates agent-issued web search queries into natural-language questions; see Section 3.2.5 for details."
        },
        {
            "title": "3.2 Experimental setup\n3.2.1 Deep research agents. We use two LLMs as deep research\nagents with model sizes that are feasible to a broad range of research\ngroups. Specifically, we follow [6] to use gpt-oss-20b [1], and use\nthe recently released GLM-4.7-Flash [62]. Both LLMs have been\ntrained to invoke web search, which is crucial for deep research.\nâ€¢ gpt-oss-20b [1] is an OpenAIâ€™s open-weight LLM. It is pre-\ntrained and subsequently post-trained for reasoning (i.e., using\nchain-of-thought) and tool use. For tool use, the model is trained\nto interact with the web via search and web page opening actions.\nIt supports a maximum context window and output length of\n131,072 tokens.",
            "content": "GLM-4.7-Flash (30B) [62] is an open-source LLM developed by Z.ai. It has pre-training stage, followed by mid-training phase (improves coding & reasoning) and post-training phase. During post-training, the model is explicitly trained for web search via reinforcement learning (RL). It supports context window of 202,752 tokens and maximum output length of 128,000 tokens. Note that our work focuses on reproducing text ranking methods in the deep research scenario; considering other LLMs as deep research agents or scaling sizes is beyond the scope of this paper. 3.2.2 Text ranking methods to be reproduced. We employ set of widely-used and representative text-ranking methods in the IR community, including retrievers and re-rankers. Retrievers. We use widely-used retrievers spanning 4 main paradigms in modern IR: lexical-based sparse, learned sparse, single-vector dense, and multi-vector dense retrievers: BM25 [38] is lexical retriever based on vocabulary-level vectors using the bag-of-words approach for queries and documents. SPLADE-v3 [22] is learned sparse retriever that trains BERT [10] to predict sparse vectors over BERTs vocabulary list for queries and documents. RepLLaMA [28] is single-vector dense retriever; it fine-tunes Llama 2 [47] using LoRA [16] to produce single embedding vector for each query and document. It appends an end-of-sequence token to each query or document input and uses the hidden state of the last model layer corresponding to this token as the embedding. Its embedding dimension is 4,096. Qwen3-Embed-8B [63] is single-vector dense retriever. It belongs to the Qwen3-Embedding series (0.6B, 4B, and 8B), and we use the largest variant. The series features fine-tuning of Qwen3 LLMs [57] on synthetic training data across multiple domains and languages, generated by the Qwen3 models themselves. It follows RepLLaMA to obtain query and document embeddings and shares the same embedding dimension. ColBERTv2 [39] is multi-vector retriever. It trains BERT [10] to produce embeddings for each token in the query and the document, and models relevance as the sum of the maximum similarities between each query vector and all document vectors. Note that the training data and input length configuration of Qwen3Embed-8B differ substantially from those of other neural retrievers (SPLADE-v3, ColBERTv2, and RepLLaMA): (i) Qwen3-Embed-8B is Chuan Meng et al. Table 1: Statistics of the BrowseComp-Plus dataset [6]. Length is measured in tokens using the Qwen3 [57] tokeniser. # Avg. Len. Corpus Type # Items Avg. Item Len. the gold documents and documents supporting intermediate reasoning steps. On average, each query has 2.9 gold, 6.1 evidence, and 76.28 negative documents. Table 1 reports detailed statistics. 830 132.19 Document Passage 100,195 2,772,255 7,845.55 279.64 trained on fully synthetic data generated by Qwen3, whereas the others share the same training dataset, namely the training data of the MS MARCO V1 passage ranking corpus [4]; (ii) Qwen3-Embed-8B is trained to support document lengths of up to 32,000 tokens, whereas the other neural ones are trained for passage-level inputs. Re-rankers. We select methods representing three effectiveness efficiency trade-offs: relatively inexpensive model (monoT5-3B [35]), an LLM-based re-ranker (RankLLaMA-7B [28]), and CoT-based reasoning re-ranker (Rank1-7B [53]), which requires reasoning token generation. All of them are pointwise re-rankers, which independently assign relevance score given query and document: monoT5-3B [35] is non-reasoning-based re-ranker that finetunes T5 [37] to output either true or false to indicate relevance. The probability assigned to the true token is used as the relevance score. monoT5 is available in base (220M), large (770M), and 3B variants; we use the 3B model. RankLLaMA-7B [28] is non-reasoning-based re-ranker that fine-tunes Llama 2 [47] with LoRA [16] to project the representation of the end-of-sequence token to relevance score. Rank1-7B [53] is reasoning-based re-ranker that fine-tunes Qwen 2.5 [58] to generate reasoning trace before producing true/false decision (<think> ... </think> true/false). The ground truth training reasoning traces are generated by DeepSeek-R1 [13] on MS MARCO [4]. As in monoT5, the probability assigned to the true token is used as the relevance score. All re-rankers are trained on the MS MARCO V1 passage ranking dataset [4] and trained to operate on passage-level inputs. Note that evaluating larger re-ranker variants (e.g., RankLLaMA-13B and Rank1-14B/32B) is beyond the scope of this work. 3.2.3 Dataset. We use BrowseComp-Plus [6], deep research dataset built on BrowseComp [50], which comprises 1,266 fact-seeking, reasoning-intensive, long-form queries together with their answers; the answers are generally short and objective, making answer evaluation easier. BrowseComp-Plus adds document corpus and humanverified relevance judgments via three-stage process. (i) each questionanswer pair is sent to OpenAI o3 to identify clues (a clue is part of the question) useful for deriving the answer and to return supporting web documents; (ii) human annotators verify whether each clue is well supported by its supporting documents and whether the combination of clues and documents enables answering the question; annotators revise the clues and supporting documents when necessary; (iii) the final document corpus is constructed by including all human-verified supporting documents, together with mined hard-negative documents. Some queries are removed in (i) and (ii) when OpenAI o3 fails to return valid clues/supporting documents, or it is too hard for human annotators to correct them, resulting in final set of 830 queries. The dataset provides two types of relevance judgments: gold and evidence. Gold documents contain the final answers (not necessarily exact matches) to the queries, while evidence documents include 3.2.4 Passage corpus construction. To segment the documents into passages, we follow [9, 36] to split each document in the original document corpus of BrowseComp-Plus into canonical passages of at most 250 words using the spaCy toolkit with the en_core_web_sm model; we use the publicly available code.2 The statistics of the passage corpus are shown in Table 1. Each passage is assigned new passage ID, and we record the mapping from each passage to its original document. Following [8], when document title is available, we extract it and prepend it to the beginning of each corresponding passage to provide additional context. 3.2.5 Query-to-question (Q2Q) reformulation. To translate web search query ğ‘ğ‘¡ issued by an agent at iteration ğ‘¡ into naturallanguage question, we define query-to-question (Q2Q) reformulator ğ‘” that takes ğ‘ğ‘¡ as input and outputs natural-language question ğ‘ğ‘¡ , i.e., ğ‘ğ‘¡ = ğ‘”(ğ‘ğ‘¡ ); then, ğ‘ğ‘¡ is sent to text ranking method ğ‘“ to return ranked list ğ·ğ‘¡ = ğ‘“ ( ğ‘ğ‘¡ ). However, web search query ğ‘ğ‘¡ may be ambiguous and may not clearly reflect the agents search intent; relying solely on the query may therefore cause the reformulation to deviate from the agents search intent. To provide additional context about the agents search intent, we introduce another variant of Q2Q that includes the recent reasoning trace ğ‘ ğ‘¡ generated by the agent, namely ğ‘ğ‘¡ = ğ‘”(ğ‘ğ‘¡, ğ‘ ğ‘¡ ). 3.2.6 Evaluation. We follow the original BrowseComp-Plus evaluation protocol [6] and report the number of search calls, recall, and accuracy; we use the evaluation code released by the dataset authors.3 Search calls denote the average number of search invocations per query. Recall measures the proportion of evidence documents returned across all search calls for query. Accuracy is computed using an LLM-as-judge that compares an agents final answer with the ground-truth answer. To analyse token-budget efficiency, we additionally report completion rate, i.e., the percentage of queries for which the agent outputs final answer before reaching the maximum context-window or output-token limits; queries reaching these limits receive accuracy 0. Evaluating on passage corpus. Because BrowseComp-Plus provides relevance judgments at the document level, they cannot be directly applied to the passage corpus. When evaluating passage retrieval, we therefore adopt the Max-P strategy [8], mapping retrieved passages to documents by assigning each document the maximum score amongst its retrieved passages. Implementation details. Regarding the agent setup, for both 3.2.7 gpt-oss-20b4 and GLM-4.7-Flash (30B)5, following [6], we set the maximum output length to 40,000 tokens and the maximum number of iterations to 100. We run gpt-oss-20b in the high reasoningeffort mode (the default setting); the reasoning effort of GLM-4.7Flash (30B) is not configurable. Both agents are deployed locally using vLLM version 0.15, which satisfies the minimum version requirement of GLM-4.7-Flash. 2 https://github.com/grill-lab/trec-cast-tools/tree/master/corpus_processing 3 https://github.com/texttron/BrowseComp-Plus 4 https://huggingface.co/openai/gpt-oss-20b 5 https://huggingface.co/zai-org/GLM-4.7-Flash Revisiting Text Ranking in Deep Research Table 2: Sanity-check comparison of agent performance for gpt-oss-20b using Qwen3-Embed-8B as search tool on the original BrowseComp-Plus document corpus. Results from two recent studies [6, 41] are shown alongside our replicated results. We also report results for GPT-5.2 (high reasoning mode), which tends to generate longer reasoning traces and search much more aggressively, causing 354 of 830 queries to reach the maximum iteration limit (100); these queries are assigned an accuracy of 0, resulting in lower overall accuracy. Agent Search calls Recall Acc. vLLM gpt-oss-20b [6] gpt-oss-20b [41] gpt-oss-20b (ours) GPT-5.2 (ours) 23.87 29.63 30.14 73.83 0.493 0.557 0.570 0. 0.346 0.422 0.421 0.451 v0.9.0.1 v0.13 v0.15 - Regarding the text ranking setup, following [6, 41], we return the top-5 ranked documents (or passages) to the agent after each search call and truncate each document to its first 512 tokens before feeding it to the agent to prevent long documents from exhausting the context window. We use Pyserinis BM25 with its default parameters (ğ‘˜1 = 0.9, ğ‘ = 0.4), following [6]. We implement SPLADE-v3 (naver/splade-v3), RepLLaMA (castorini/repllama-v17b-lora-passage)6, RankLLaMA-7B (castorini/rankllama-v1-7b-lorapassage), and Qwen3-Embed-8B (Qwen/Qwen3-Embedding-8B) using Tevatron.7 We implement ColBERTv2 (colbert-ir/colbertv2.0) using PyLate [5].8 We implement monoT5-3B (monot5-3b-msmarco) following PyTerrier_t5.9 Rank1-7B (jhu-clsp/rank1-7b) is implemented following the original authors implementation.10 For document indexing, we follow [6] to set the maximum input length of Qwen3-Embed-8B to 4,096 tokens; the remaining neural retrievers use maximum length of 512 tokens, as they are trained on passage-level inputs. For passage indexing, all retrievers use maximum length of 512 tokens. We implement the query-to-question (Q2Q) reformulator (see Section 3.2.5) using gpt-oss-20b in the low reasoning-effort mode; we randomly sample natural language questions from TREC-DL 2019 [7] and include them in the prompt as examples to specify the desired question-style output. Experiments are conducted using NVIDIA RTX 6000 Ada (48 GB), H100 (80 GB), and H200 (141 GB) GPUs, subject to hardware availability."
        },
        {
            "title": "4 Results and Discussions\n4.1 Sanity check for reproduction\nAs a sanity check, we attempt to replicate the results reported in re-\ncent studies [6, 41]. We evaluate gpt-oss-20b (high-reasoning mode)\nwith Qwen3-Embed-8B as the retriever on the original BrowseComp-\nPlus document corpus. The results are shown in Table 2. Our results\nobtained with the latest vLLM version (v0.15) are very similar to\nthose produced using v0.13 in [41]. We observe that newer vLLM\nversions (v0.15 and v0.13) lead to higher performance compared\nto the old one (v0.9.0.1). Besides replicating gpt-oss-20b, we also\ntest GPT-5.2, a current state-of-the-art commercial LLM. We find\nthat GPT-5.2 requires substantially more iterations; 354 queries (830",
            "content": "6 We also evaluated the RepLLaMA checkpoint trained on the MS MARCO document corpus (castorini/repllama-v1-7b-lora-doc); however, it performed worse on BrowseComp-Plus than the passage-trained checkpoint (repllama-v1-7b-lora-passage). 8 https://github.com/lightonai/pylate 7 https://github.com/texttron/tevatron 10 https://github.com/orionw/rank1 9 https://github.com/terrierteam/pyterrier_t5 queries in total) require over 100 iterations. Such large number of iterations would incur significant API costs, making it less feasible for broader researchers and practitioners (all queries cost roughly $1,000 to $2,000); therefore, we do not use it in this work."
        },
        {
            "title": "4.2 Retrievers on passage and document corpora\nTo answer RQ1, we compare two agents, gpt-oss-20b and GLM-\n4.7-Flash (30B), across different retrievers on both the passage and\ndocument corpora. For experiments on the document corpus, fol-\nlowing [6, 41], we truncate each retrieved document to the first 512\ntokens, to avoid exhausting the input context window. To mitigate\ntruncation-induced information loss, we follow [6] to evaluate a\nsetting where agents can access a full-document reader tool on the\ndocument corpus. We present the results in Tables 3 (gpt-oss-20b)\nand 4 (GLM-4.7-Flash). We have four main observations.",
            "content": "First, both agents achieve higher answer accuracy on the passage corpus than on the document corpus (without the full-document reader) across all retrievers, except Qwen3-Embed-8B, likely because it is trained on substantially longer documents (up to 32K tokens) and is less effective on passages due to distribution shift. Notably, the relative improvement on passages is more pronounced for the shorter-context gpt-oss-20b (131K context window) than for GLM-4.7-Flash (200K). E.g., with SPLADE-v3, gpt-oss-20b achieves 0.516 accuracy on passages, an 8.4% relative improvement over documents (0.476), whereas GLM-4.7-Flash gains 4.02%. We further observe that both agents issue more search calls on passages than on documents (e.g., 32.75 vs. 28.95 with gpt-oss-20b using SPLADE-v3). Moreover, for gpt-oss-20b, the completion rate is markedly higher on passages (0.980 vs. 0.824), while GLM-4.7-Flash shows similar completion rates across corpora; note that both models share the same output-token limit but differ in context-window limit. Taken together, these results suggest that passage-level units enable more search and reasoning iterations before reaching context-window limits, which in turn improves answer accuracy, particularly for agents with smaller context windows. Second, when paired with the lexical retriever BM25, both agents achieve highly competitive performance on the passage corpus compared with neural rankers. For instance, gpt-oss-20b with BM25 attains the highest recall (0.616) and answer accuracy (0.572) on the passage corpus. The 0.572 accuracy is also the highest across all retrieval settings in Tables 3 and 4. Table 5 presents examples of agent-issued queries; they follow web-search style, characterised by keywords, phrases, and quotation marks for exact matching, making them well suited to lexical retrievers such as BM25. In contrast, BM25 performs worst on the document corpus; we analyse this issue in more detail later in this section. Third, single-vector dense retrievers (RepLLaMA, Qwen3-Embed8B), despite their substantially larger model sizes, consistently underperform smaller BERT-based [10] learned-sparse (SPLADE-v3) and multi-vector dense (ColBERT-v2) retrievers. This finding is consistent with prior work showing that SPLADE and ColBERT perform well across diverse query formats on the BEIR dataset [11, 46] that include keyword-rich queries or those requiring exact matching. ColBERT, in particular, has been shown to exhibit strong preferences for exact matching [33]. In contrast, single-vector approaches often struggle to adapt to new tasks and domains [46] and are theoretically constrained in representational capacity [52]. Chuan Meng et al. Table 3: Agent performance across retrievers on BrowseComp-Plus. The agent is based on gpt-oss-20b; it supports maximum context window of 131,072 tokens. #Search and #GetDoc denote the number of search and full-document reader calls, respectively. Compl. denotes the completion rate, i.e., the percentage of queries for which the agent outputs final answer before reaching the maximum context-window or output-token limits; queries reaching these limits receive accuracy 0. The best values in recall and accuracy are boldfaced, and the second-best ones are underlined. Retriever Passage corpus Document corpus Document corpus+GetDoc #Search Recall Acc. Comp. #Search Recall Acc. Comp. #Search #GetDoc Recall Acc. Comp. BM25 SPLADE-v3 RepLLaMA Qwen3-Embed-8B ColBERTv2 30.97 32.75 36.13 37.83 32.88 0.616 0.545 0.449 0.470 0.552 0.572 0.516 0.406 0.417 0.521 0.968 0.980 0.961 0.963 0.978 32.22 28.95 30.69 30.14 27. 0.366 0.628 0.514 0.570 0.633 0.259 0.476 0.363 0.421 0.481 0.805 0.824 0.786 0.821 0.855 28.14 24.64 26.86 26.56 24.31 1.31 1.65 1.46 1.63 1.75 0.343 0.602 0.476 0.559 0. 0.301 0.529 0.399 0.455 0.538 0.746 0.829 0.816 0.823 0.835 Table 4: Agent performance across different retrievers on BrowseComp-Plus. The agent is based on GLM-4.7-Flash (30B); it supports maximum context window of 200,000 tokens. Metric definitions and formatting follow Table 3. Retriever Passage corpus Document corpus Document corpus+GetDoc #Search Recall Acc. Comp. #Search Recall Acc. Comp. #Search #GetDoc Recall Acc. Comp. BM25 SPLADE-v3 RepLLaMA Qwen3-Embed-8B ColBERTv2 69.41 72.69 75.19 74.90 72.73 0.581 0.578 0.456 0.482 0.571 0.445 0.466 0.331 0.357 0. 0.863 0.883 0.905 0.886 0.882 53.63 51.22 51.51 52.49 52.11 0.309 0.639 0.493 0.580 0.640 0.196 0.448 0.330 0.374 0.430 0.923 0.881 0.907 0.882 0.878 43.72 36.83 41.86 40.23 37. 3.00 5.33 4.13 4.34 5.30 0.282 0.597 0.471 0.482 0.595 0.263 0.525 0.407 0.456 0.535 0.947 0.939 0.955 0.956 0.955 Table 5: Examples of three search queries issued by the gpt-oss-20b and GLM-4.7-Flash agents for Query 781 on BrowseComp-Plus. Agent Search query gpt-oss-20b 90+7 attendance 61700 Man United 4-1 90+4 assist 41 Premier League 2020 GLM-4.7-Flash 90+4 football match attendance 61,888 football match Stockholm Vienna Prague goal in the 6th minute football Table 6: Performance of the gpt-oss-20b agent across different retrievers on BrowseComp-Plus. Metric definitions and formatting follow Table 3. Retriever Passage corpus+GetDoc #Search #GetDoc Recall Acc. Comp. BM25 SPLADE-v3 RepLLaMA Qwen3-Embed-8B 26.87 27.54 31.82 32.11 2.24 2.11 1.77 1. 0.556 0.542 0.510 0.532 0.369 0.399 0.404 0.438 0.877 0.895 0.868 0.853 Fourth, enabling the full-document reader on the document corpus reduces search calls and recall but improves answer accuracy. E.g., with gpt-oss-20b using SPLADE-v3, accuracy increases from 0.476 to 0.529. This suggests that the reader compensates for information loss caused by document truncation. With the reader enabled on documents, gpt-oss-20b achieves performance comparable to the passage setting, while GLM-4.7-Flash generally surpasses the passage setting and maintains high completion rates, likely due to its longer context window, which allows processing of more complete documents. We also evaluate enabling the reader on the passage corpus using gpt-oss-20b with several retrievers; see results in Table 6. We find the reader slightly degrades performance (e.g., with BM25, accuracy decreases from 0.572 to 0.542), likely because passage-level units already provide direct access to relevant segments within document, rendering the reader redundant. Why does BM25 perform poorly on the document corpus? Prior work [19] shows that lexical document retrieval is sensitive to document-length normalisation. BM25 has two parameters ğ‘˜1 and ğ‘ [38]: ğ‘˜1 controls term-frequency saturation (larger ğ‘˜1 values lead to slower term frequency saturation), while ğ‘ controls document length normalisation (larger ğ‘ values increase the penalty on longer documents). To investigate this issue, we evaluate the gpt-oss-20b agent using BM25 under two settings: (i) truncating each document to its first 512 tokens before indexing, thereby reducing the impact of document-length variation; and (ii) instead of using the default parameter (ğ‘˜1 = 0.9, ğ‘ = 0.4) following [6], we use document-retrieval-oriented BM25 parameters (ğ‘˜1 = 3.8, ğ‘ = 0.87), which increase length normalisation and have been shown to be effective for the MS MARCO document retrieval task11. We present the results in Table 7. Indexing only the first 512 tokens of each document substantially improves performance, yielding 64.2% relative gain in recall and 98.1% gain in answer accuracy, suggesting that reducing the effect of document-length normalisation benefits BM25 on documents. Using the document-oriented BM25 parameters also improves performance, with 76.8% gain in recall and 71.0% gain in accuracy. To better understand the optimal BM25 parameter settings, we perform grid search of retrieval performance on BrowseCompPlus using the original full queries across different parameter values. The results are shown in Figure 1. We find that larger values of ğ‘ generally improve performance, i.e., penalising long documents 11 https://github.com/castorini/anserini/blob/master/docs/experiments-msmarco-doc.md Revisiting Text Ranking in Deep Research (a) Document corpus (Recall@5) (b) Document corpus (nDCG@10) (c) Passage corpus (Recall@5) (d) Passage corpus (nDCG@10) Figure 1: Heatmap from grid search on BrowseComp-Plus using the original full queries (not end-to-end), showing the effectiveness (evaluated by evidence judgments) of BM25 under different hyperparameter settings. The red denotes the default parameter setting following [6], while the green + denotes the best parameter setting found by the grid search. The lighter the colour, the higher the retrieval performance. Table 7: Performance of the gpt-oss-20b agent using BM25 under different hyperparameter settings on BrowseCompPlus. Index len. indicates which portion of each document is indexed, i.e., the full document or the first 512 tokens. ğ‘˜1 = 0.9 and ğ‘ = 0.4 are the default BM25 parameters, following [6]. The best values in recall and accuracy are boldfaced, and the second-best ones are underlined. BM25 setting Document corpus Index len. #Search Recall Acc. Comp. ğ‘˜1 = 0.9, ğ‘ = 0.4 ğ‘˜1 = 0.9, ğ‘ = 0.4 ğ‘˜1 = 3.8, ğ‘ = 0.87 full doc ğ‘˜1 = 10, ğ‘ = 1 full doc full doc 512 token 32.22 28.00 28.66 28.73 0.366 0.805 0.259 0.642 0.513 0.812 0.839 0.601 0.443 0.647 0.506 0.848 with higher term frequencies is beneficial. Moreover, the performance gap between the default setting (ğ‘˜1 = 0.9, ğ‘ = 0.4) and the optimal parameter settings is substantially larger on the document corpus than on the passage corpus. E.g., on the document corpus, ğ‘˜1 = 10 and ğ‘ = 1 appear to be sweet spot (see Figures 1a and 1b), and performance using it is substantially different from the default setting. We further evaluate gpt-oss-20b using BM25 with ğ‘˜1 = 10 and ğ‘ = 1. Table 7 shows that this configuration achieves the highest recall (0.647) across all settings in this section. We note that ğ‘˜1 and ğ‘ are generally regarded as corpus-dependent parameters rather than query-specific ones [14]. Indeed, Figure 1 shows that strong length normalisation (ğ‘ = 1) and large ğ‘˜1 values are important for effective BM25 retrieval over BrowseComp-Plus documents. Precise tuning of these parameters is not required for strong effectiveness; in particular, wide range of ğ‘˜1 values performs well. Nonetheless, we urge readers to exercise some caution when comparing the tuned BM25 results with other retrievers, since the parameter selection was performed directly on all BrowseCompPlus queries (due to the absence of validation set)."
        },
        {
            "title": "4.3 Re-ranking in deep research\nTo answer RQ2, we evaluate two agents (gpt-oss-20b and GLM-\n4.7-Flash) that access ranking pipelines with varying retrievers,\nre-rankers, and re-ranking depths. Given the advantages of the\npassage corpus shown in Section 4.2, we perform all experiments\non the passage corpus. We use BM25, SPLADE-v3, and Qwen3-\nEmbed-8B as initial retrievers; monoT5-3B, RankLLaMA-7B, and\nRank1-7B as re-rankers; and, following [41], we use three re-ranking",
            "content": "depths (10, 20, and 50). Due to limited GPU resources, for GLM-4.7Flash, we exclude Qwen3-Embed-8B, Rank1 and depth 20. We show the results for gpt-oss-20b and GLM-4.7-Flash in Tables 8 and 9, respectively. We make three key observations. First, re-ranking consistently improves recall and accuracy while typically reducing search calls compared to no re-ranking. Notably, gpt-oss-20b with the BM25monoT5 pipeline (depth 50) achieves the best performance (recall and accuracy) in our study, reaching 0.716 recall and 0.689 accuracy with 27.57 search calls. Relative to no re-ranking, this represents gains of 16.23% in recall and 20.45% in accuracy, alongside 10.98% reduction in search calls. Despite using only 20B agent with BM25 retriever and 3B re-ranker, this configuration achieves accuracy comparable to GPT-5based agent using Qwen3-Embed-8B (0.701; Table 1 in [6]). Second, no single re-ranker consistently performs best. Interestingly, the reasoning-based re-ranker Rank1 [53] does not show clear advantage over non-reasoning ones. Table 10 presents failure case in which Rank1 misinterprets the search intent by incorrectly treating the independent keywords radiation and protein as semantic unit, finally leading to wrong relevance prediction. This suggests that keyword-rich, phrase-driven web-search queries may reduce the effectiveness of Rank1s explicit reasoning. Rank1 is trained on MS MARCO [4] that consists of natural-language questions, resulting in traininginference query mismatch with agent-issued queries. We examine this issue in detail in Section 4.4. Third, deeper re-ranking and stronger initial retrievers tend to improve effectiveness while reducing search calls. For gpt-oss-20b with the BM25monoT5 pipeline, increasing the depth from 10 to 20 improves recall and accuracy by 6.21% and 6.81%, respectively, while reducing search calls by 3.11%. Further increasing the depth from 20 to 50 yields gains of 2.14% in recall and 2.23% in accuracy, alongside 4.80% reduction in search calls. These trends align with [41], which reports improvements with deeper depths for listwise re-rankers in deep research. Regarding initial retrievers, with gpt-oss-20b and monoT5 (depth 50), BM25 as the retriever yields relative improvements of 16.61% in recall and 23.26% in accuracy over Qwen3-Embed-8B, while reducing search calls by 16.30%. 4.4 Traininginference query mismatch To address RQ3, we evaluate the gpt-oss-20b agent using BM25, SPLADE-v3, and Qwen3-Embed-8B as retrievers under three searchquery conditions: agent-issued queries, and questions generated by two variants of our query-to-question (Q2Q) method (see Section 3.2.5). The two variants generate question using only the Chuan Meng et al. Table 8: Performance of the gpt-oss-20b agent across ranking pipelines with different retrievers, re-rankers, and re-ranking depths on the passage corpus of BrowseComp-Plus. ğ‘‘ denotes the re-ranking depth. The best values in recall and accuracy are boldfaced, and the second-best ones are underlined. Re-ranker no re-ranking monoT5 (ğ‘‘ = 10) monoT5 (ğ‘‘ = 20) monoT5 (ğ‘‘ = 50) RankLLaMA (ğ‘‘ = 10) RankLLaMA (ğ‘‘ = 20) RankLLaMA (ğ‘‘ = 50) Rank1 (ğ‘‘ = 10) Rank1 (ğ‘‘ = 20) Rank1 (ğ‘‘ = 50) BM25 SPLADE-v3 Qwen3-Embed-8B #Search Recall Acc. Comp. #Search Recall Acc. Comp. #Search Recall Acc. Comp. 30.97 29.89 28.96 27.57 29.85 29.03 27.10 29.62 28.28 26. 0.616 0.572 0.660 0.701 0.716 0.657 0.681 0.710 0.662 0.694 0.712 0.631 0.674 0.689 0.617 0.655 0.678 0.628 0.669 0.687 0.980 0.971 0.960 0.974 0.980 0.971 0.977 0.966 0.971 0. 32.75 31.58 30.07 29.72 31.30 30.82 28.96 31.18 30.73 29.12 0.545 0.516 0.630 0.647 0.689 0.632 0.646 0.691 0.617 0.672 0.702 0.599 0.598 0.646 0.595 0.605 0.663 0.580 0.617 0. 0.980 0.965 0.974 0.971 0.964 0.981 0.959 0.978 0.953 0.959 37.83 35.64 34.16 32.94 36.07 34.25 32.85 35.50 34.04 32.72 0.470 0. 0.524 0.570 0.614 0.508 0.553 0.613 0.530 0.579 0.630 0.471 0.541 0.559 0.465 0.494 0.568 0.454 0.528 0.564 0.963 0.969 0.959 0.952 0.964 0.961 0.964 0.951 0.969 0.949 Table 9: Performance of the GLM-4.7-Flash (30B) agent on the passage corpus of BrowseComp-Plus. Metrics and formatting follow Table 8. Retriever Re-ranker #Search Recall Acc. Comp. BM25 SPLADE -v3 no re-ranking monoT5 (ğ‘‘ = 10) monoT5 (ğ‘‘ = 50) RankLLaMA (ğ‘‘ = 10) RankLLaMA (ğ‘‘ = 50) no re-ranking monoT5 (ğ‘‘ = 10) monoT5 (ğ‘‘ = 50) RankLLaMA (ğ‘‘ = 10) RankLLaMA (ğ‘‘ = 50) 69.41 66.79 62.91 77.00 65. 72.69 64.43 62.09 67.91 69.21 0.581 0.665 0.696 0.660 0.707 0.578 0.632 0.693 0.612 0.695 0.445 0.549 0.586 0.503 0.576 0.466 0.528 0.575 0.490 0.543 0.863 0.892 0.901 0.836 0. 0.883 0.865 0.900 0.880 0.842 raw query issued by the agent (denoted as Q) or using both the raw query and the agents recent reasoning trace (denoted as Q+R). Following Sections 4.2 and 4.3, experiments are performed on the passage corpus. Due to limited GPU resources, we exclude GLM4.7-Flash. We present results in Table 11. We make three observations. First, for both retrievers (SPLADEv3 and Qwen3-Embed-8B), Q2Q (Q+R) consistently achieves significant improvements over raw agent-issued queries. E.g., with SPLADE-v3, Q2Q (Q+R) yields relative gains of 7.34% in recall and 7.95% in accuracy. These results indicate that mismatch between agent-issued queries and the natural-language questions used to train neural rankers severely limits their effectiveness in deep research. Q2Q (Q+R) effectively mitigates this traininginference query mismatch. Second, Q2Q (Q) provides little to no improvement over raw agent-issued queries. To illustrate why, Table 12 illustrates examples of raw query generated by the gpt-oss-20b agent and its reformulations by Q2Q (Q) and Q2Q (Q+R). The agents search intent is to retrieve football matches with an attendance of 61,880. The raw query (61,880 football attendance) is keyword-driven and under-specified. Q2Q (Q) reformulates it as What is the football attendance number 61,880?, shifting the focus from identifying matches to explaining the number itself. In contrast, Q2Q (Q+R), which incorporates the agents reasoning trace, better captures the search intent. Third, for BM25, Q2Q-generated questions even hurt performance, indicating that web-search-style queries are better suited to BM25 than natural-language questions. Given that Q2Q (Q+R) reduces query mismatch for neural retrievers, we further investigate whether it also alleviates mismatch in re-ranking. Section 4.3 shows that keywordand phrase-based web-search queries can weaken the reasoning effectiveness of the re-ranker Rank1 [53]. We therefore evaluate gpt-oss-20b using SPLADE-v3Rank1 pipeline (re-ranking depth ğ‘‘ = 10). SPLADE-v3 uses raw agent-issued queries, while Rank1 uses the raw queries or the Q2Q (Q+R) reformulations. As shown in Table 13, Q2Q (Q+R) significantly mitigates query mismatch for Rank1, yielding relative gains of 3.40% in recall and 5.69% in accuracy over raw queries. 5 Related Work Text ranking. Unsupervised lexical retrievers (e.g., BM25 [38]) have long dominated text ranking. With the rise of pre-trained language models (e.g., BERT [10] and T5 [37]) and large-scale humanlabelled training data [4], neural rankers have rapidly advanced. wide range of neural retrievers has been developed, including single-vector dense [54], multi-vector dense [20, 39], and learned sparse retrievers [12, 22]. In addition, cross-encoder re-rankers [35] based on BERT [10] or T5 [37] have achieved strong effectiveness in re-ranking. More recently, LLMs have further advanced neural ranking [30], e.g., through stronger representation capabilities [28] and large-scale synthetic training data [63]. LLM reasoning has also been explored to enhance re-ranking [53, 59]. Text ranking has been studied across diverse settings, including single-hop retrieval [4, 21], multi-hop retrieval [15, 60], reasoning-intensive search [40, 44], and RAG systems [3, 32, 45]. However, little work has systematically examined the performance of text ranking methods in deep research. Deep research. The origins of deep research can be traced back to multi-hop question answering (QA) [15, 48, 60]. Most multi-hop QA benchmarks rely on Wikipedia, which has been extensively used in LLM pre-training, making such questions less challenging today [6]. In contrast, deep research requires iterative web-scale search and evidence synthesis across the open web [50, 64]. E.g., the BrowseComp benchmark [50] includes queries that humans typically cannot solve within ten minutes using search engines. Solving these queries typically requires LLM-based agents to perform multiple rounds of search and reasoning to gather, synthesise, Revisiting Text Ranking in Deep Research Table 10: Example of reasoning trace generated by Rank1 [53], given search query issued by the gpt-oss-20b agent and one retrieved passage for Query 37 on BrowseComp-Plus. The passage is irrelevant to Query 37. The agents most recent reasoning trace is: We will search for radiation, section 4.1, protein, two species, and 2010. Rank1 incorrectly treats the independent keywords radiation and protein as coherent semantic unit, ultimately leading to an incorrect prediction. Search query Passage Reasoning text \"section 4.1\" radiation protein subset panel of significantly altered proteins was selected to build predictive models of radiation exposure and received radiation dose useful for population screening in future radiological or nuclear event. The query mentions \"section 4.1\" which might refer to specific section in document or paper. The term \"radiation protein\" could be typo or shorthand for specific protein related to radiation exposure. The query might be referencing specific section in paper titled \"radiation protein 2010.\" Since the passage is from 2010 study and discusses radiation-related proteins, its likely relevant. Score 0.999 Table 11: Performance of the gpt-oss-20b agent under different retrievers and query conditions on the BrowseComp-Plus passage corpus. Q2Q denotes our query-to-question method (Section 3.2.5); uses only the raw agent-issued query, while Q+R additionally incorporates the agents recent reasoning trace. Within each retriever, the best values in recall and accuracy are boldfaced, and the second-best ones are underlined. denotes statistically significant improvement of Q2Q (Q+R) over raw agent-issued queries (paired ğ‘¡-test, ğ‘ < 0.05). Retriever Query #Search Recall Acc. Comp. BM25 SPLADE-v Qwen3 -Embed-8B Raw Q2Q (Q) Q2Q (Q+R) Raw Q2Q (Q) Q2Q (Q+R) Raw Q2Q (Q) Q2Q (Q+R) 30.97 31.88 32.15 32.75 32.88 31. 37.83 37.42 35.82 0.616 0.593 0.583 0.545 0.550 0.585 0.470 0.457 0.507 0.572 0.578 0.557 0.516 0.510 0.557 0.417 0.404 0.459 0.968 0.977 0.974 0.980 0.976 0.983 0.963 0.969 0.965 Table 12: Examples of search queries issued by the gpt-oss20b agent, and queries reformulated by the Q2Q method, for Query 781 on BrowseComp-Plus. The agents recent reasoning trace is: Lets recall some matches that had an attendance of exactly 61,728, etc. Perhaps it may be easier to search for attendance 61,880 football. Method Search query Raw query Q2Q (Q) Q2Q (Q+R) What football match had an attendance of 61,880? 61,880 football attendance What is the football attendance number 61,880? Table 13: Performance of the gpt-oss-20b agent using ranking pipeline with the SPLADE-v3 retriever and the Rank1 reranker on the BrowseComp-Plus passage corpus. SPLADE-v3 always uses raw agent-issued queries, while Rank1 operates under different query conditions. Metric definitions and formatting follow Table 11. Re-ranker Query Raw Q2Q (Q+R) #Search Recall 0.617 0.638 31.18 31.15 Acc. 0.580 0.613 Comp. 0.978 0.970 Rank1 (ğ‘‘ = 10) and verify evidence [23, 27]. Most existing approaches equip LLMbased agents with live web search APIs [49, 56], but such black-box systems lack transparency. To address this issue, Chen et al. [6] curate BrowseComp-Plus, which extends the original BrowseComp dataset with fixed document corpus and human-verified relevance judgments, allowing white-box retrievers to be used. On this resource, Sharifymoghaddam and Lin [41] study listwise LLM-based re-rankers to analyse effectivenessefficiency trade-offs. We differ from prior studies [6, 41] by systematically reproducing broad spectrum of text ranking methods in deep research. Unlike contemporaneous work [17] that tests BM25 and LLM-based singlevector retrievers in scientific literature search (domain-specific deep research), we use broader range of retrievers (e.g., learned sparse and multi-vector) and re-rankers in general open-domain setting."
        },
        {
            "title": "6 Conclusions & Future Work\nWe have reproduced an extensive set of text ranking methods in\ndeep research, conducting experiments on BrowseComp-Plus [6]\nwith 2 open-source agents, 5 retrievers, and 3 re-rankers.",
            "content": "Overall, our results show that several well-established findings in text ranking continue to hold in deep research: (i) the lexical retriever BM25 with appropriate setup outperforms neural rankers in most cases; notably, gpt-oss-20b with BM25 on the passage corpus achieves the highest answer accuracy across all retrieval settings in our study; (ii) BERT-based learned sparse [22] and multi-vector dense [39] retrievers generalise better than LLM-based single-vector dense retrievers; and (iii) re-ranking remains highly effective, improving recall and answer accuracy while reducing search calls, with deeper re-ranking depths further amplifying these gains. However, the web-search-style syntax of agent-issued queries (e.g., quoted exact matches) induces distribution drift for neural rankers, limiting their effectiveness in deep research and preventing prior finding from generalising to this setting: the reasoning-based re-ranker Rank1 [53] often misinterprets such queries, diminishing the benefits of reasoning and showing no clear advantage over non-reasoning methods. Our proposed Q2Q method significantly mitigates this drift, improving neural ranking performance. Beyond validating these findings, we find that passage-level units benefit agents with limited context windows, and reduce sensitivity to document-length normalisation in lexical retrieval. We identify future directions: (i) we only use one deep research dataset in our work; it is worthwhile to augment other deep research benchmarks with fixed corpus and manual relevance judgements; (ii) we only use two LLMs as agents; evaluating additional model families and larger model sizes would help assess the generalisability of our findings; and (iii) exploring additional rankers [2, 25, 40] and configurations, such as scaling laws, is an important direction. Acknowledgments We thank Zijian Chen and Xueguang Ma for their guidance on using BrowseComp-Plus. This research was supported by Turing AI Acceleration Fellowship funded by the Engineering and Physical Sciences Research Council (EPSRC), grant number EP/V025708/1. References [1] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. 2025. gpt-oss-120b & gpt-oss-20b Model Card. arXiv preprint arXiv:2508.10925 (2025). [2] Mohammad Kalim Akram, Saba Sturua, Nastia Havriushenko, Quentin Herreros, Michael GÃ¼nther, Maximilian Werk, and Han Xiao. 2026. jina-embeddings-v5text: Task-Targeted Embedding Distillation. arXiv preprint arXiv:2602.15547 (2026). [3] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In ICLR. [4] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: Human Generated MAchine Reading COmprehension Dataset. arXiv preprint arXiv:1611.09268 (2018). [5] Antoine Chaffin and RaphaÃ«l Sourty. 2025. PyLate: Flexible Training and Retrieval for Late Interaction Models. In CIKM. 63346339. [6] Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, et al. 2025. BrowseCompPlus: More Fair and Transparent Evaluation Benchmark of Deep-Research Agent. arXiv preprint arXiv:2508.06600 (2025). [7] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2019. Overview of the TREC 2019 Deep Learning Track. In REC 2019. [8] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with [9] [10] Contextual Neural Language Modeling. In SIGIR. 985988. Jeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2021. TREC CAsT 2021: The Conversational Assistance Track Overview. In TREC. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL. 41714186. [11] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and StÃ©phane Clinchant. 2021. SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval. arXiv preprint arXiv:2109.10086 (2021). [12] Thibault Formal, Benjamin Piwowarski, and StÃ©phane Clinchant. 2021. SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. In SIGIR. 22882292. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948 (2025). [14] Ben He and Iadh Ounis. 2005. Term Frequency Normalisation Tuning for BM and DFR Models. In ECIR. 200214. [15] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In COLING. 66096625. [16] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR. [17] Tiansheng Hu, Yilun Zhao, Canyu Zhang, Arman Cohan, and Chen Zhao. 2026. SAGE: Benchmarking and Improving Retrieval for Deep Research Agents. arXiv preprint arXiv:2602.05975 (2026). [18] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning. In COLM. [19] Marcin Kaszkiel and Justin Zobel. 1997. Passage Retrieval Revisited. In ACM SIGIR Forum, Vol. 31. ACM New York, NY, USA, 178185. [20] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In SIGIR. 3948. [21] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural Questions: Benchmark for Question Answering Research. TACL 7 (2019), 453466. [22] Carlos Lassance, HervÃ© DÃ©jean, Thibault Formal, and StÃ©phane Clinchant. 2024. SPLADE-v3: New baselines for SPLADE. arXiv preprint arXiv:2403.06789 (2024). [23] Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. 2025. WebSailor: Navigating Super-human Reasoning for Web Agent. arXiv preprint arXiv:2507.02592 (2025). Chuan Meng et al. [24] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic Search-Enhanced Large Reasoning Models. arXiv preprint arXiv:2501.05366 (2025). [26] [25] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards General Text Embeddings with Multi-stage Contrastive Learning. ArXiv (2023). Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2022. Pretrained transformers for text ranking: Bert and beyond. Springer Nature. Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, et al. 2025. WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents. arXiv preprint arXiv:2509.06501 (2025). [28] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine- [27] Tuning LLaMA for Multi-Stage Text Retrieval. In SIGIR. 24212425. [29] Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, and Maarten de Rijke. 2024. Ranked List Truncation for Large Language Model-based Re-Ranking. In SIGIR. 141151. [30] Chuan Meng, Jiqun Liu, Mohammad Aliannejadi, Fengran Mo, Jeff Dalton, and Maarten de Rijke. 2026. Re-Rankers as Relevance Judges. arXiv preprint arXiv:2601.04455 (2026). [31] Chuan Meng, Francesco Tonolini, Fengran Mo, Nikolaos Aletras, Emine Yilmaz, and Gabriella Kazai. 2025. Bridging the Gap: From Ad-hoc to Proactive Search in Conversations. In SIGIR. 6474. [32] Fengran Mo, Yifan Gao, Chuan Meng, Xin Liu, Zhuofeng Wu, Kelong Mao, Zhengyang Wang, Pei Chen, Zheng Li, Xian Li, et al. 2025. UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations. In ACL. 69366949. [33] Ariane Mueller and Craig Macdonald. 2025. Semantically Proportioned nDCG for Explaining ColBERTs Learning Process. In ECIR. 341356. [34] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019). [35] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document Ranking with Pretrained Sequence-to-Sequence Model. In EMNLP. 708 718. [36] Paul Owoicho, Jeffrey Dalton, Mohammad Aliannejadi, Leif Azzopardi, Johanne R. Trippas, and Svitlana Vakulenko. 2022. TREC CAsT 2022: Going Beyond User Ask and System Retrieve with Initiative and Response Generation. In TREC. [37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. Journal of Machine Learning Research 21, 140 (2020), 167. [38] Stephen Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at TREC-3. Nist Special Publication Sp 109 (1995), 109. [39] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. In NAACL. 37153734. [40] Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, et al. 2025. ReasonIR: Training Retrievers for Reasoning Tasks. arXiv preprint arXiv:2504.20595 (2025). [41] Sahel Sharifymoghaddam and Jimmy Lin. 2026. Rerank Before You Reason: Analyzing Reranking Tradeoffs through Effective Token Cost in Deep Search Agents. arXiv preprint arXiv:2601.14224 (2026). [42] Zhengliang Shi, Yiqun Chen, Haitao Li, Weiwei Sun, Shiyu Ni, Yougang Lyu, Run-Ze Fan, Bowen Jin, Yixuan Weng, Minjun Zhu, et al. 2025. Deep Research: Systematic Survey. arXiv preprint arXiv:2512.02038 (2025). [43] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2503.05592 (2025). [44] Hongjin SU, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Liu Haisu, Quan Shi, Zachary Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Arik, Danqi Chen, and Tao Yu. 2025. BRIGHT: Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. In ICLR. [45] Weihang Su, Yichen Tang, Qingyao Ai, Junxi Yan, Changyue Wang, Hongning Wang, Ziyi Ye, Yujia Zhou, and Yiqun Liu. 2025. Parametric Retrieval Augmented Generation. In SIGIR. 12401250. [46] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). [47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288 (2023). [48] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop Questions via Single-hop Question Composition. TACL 10 (2022), 539554. Revisiting Text Ranking in Deep Research [50] [49] Yibo Wang, Lei Wang, Yue Deng, Keming Wu, Yao Xiao, Huanjin Yao, Liwei Kang, Hai Ye, Yongcheng Jing, and Lidong Bing. 2026. DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation. arXiv preprint arXiv:2601.09688 (2026). Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. BrowseComp: Simple Yet Challenging Benchmark for Browsing Agents. arXiv preprint arXiv:2504.12516 (2025). Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent Abilities of Large Language Models. Transactions on Machine Learning Research (2022). [51] [52] Orion Weller, Michael Boratko, Iftekhar Naim, and Jinhyuk Lee. 2025. On the Theoretical Limitations of Embedding-Based Retrieval. arXiv preprint arXiv:2508.21038 (2025). [53] Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn Lawrie, and Benjamin Van Durme. 2025. Rank1: Test-Time Compute for Reranking in Information Retrieval. In COLM. [54] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. In ICLR. [55] Fangyuan Xu, Rujun Han, Yanfei Chen, Zifeng Wang, Hsu, Jun Yan, Vishy Tirumalashetty, Eunsol Choi, Tomas Pfister, Chen-Yu Lee, et al. 2026. SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback. arXiv preprint arXiv:2601.18202 (2026). [56] Yilong Xu, Zhi Zheng, Xiang Long, Yujun Cai, and Yiwei Wang. 2026. SelfManager: Parallel Agent Loop for Long-form Deep Research. arXiv preprint arXiv:2601.17879 (2026). [57] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388 (2025). [58] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 Technical Report. arXiv preprint arXiv:2412.15115 (2024). [59] Eugene Yang, Andrew Yates, Kathryn Ricci, Orion Weller, Vivek Chari, Benjamin Van Durme, and Dawn Lawrie. 2025. Rank-K: Test-Time Reasoning for Listwise Reranking. arXiv preprint arXiv:2505.14432 (2025). [60] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In EMNLP. 23692380. [61] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. ReAct: Synergizing Reasoning and Acting in Language Models. In The eleventh international conference on learning representations. [62] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. 2025. GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models. arXiv preprint arXiv:2508.06471 (2025). [63] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. 2025. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. arXiv preprint arXiv:2506.05176 (2025). [64] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. 2025. BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese. arXiv preprint arXiv:2504.19314 (2025). [65] Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation. arXiv preprint arXiv:2206.10128 (2022)."
        }
    ],
    "affiliations": [
        "The University of Edinburgh",
        "University of Glasgow"
    ]
}