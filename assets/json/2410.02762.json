{
    "paper_title": "Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations",
    "authors": [
        "Nick Jiang",
        "Anish Kachinthaya",
        "Suzie Petryk",
        "Yossi Gandelsman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 2 6 7 2 0 . 0 1 4 2 : r INTERPRETING AND EDITING VISION-LANGUAGE REPRESENTATIONS TO MITIGATE HALLUCINATIONS Nick Jiang, Anish Kachinthaya, Suzie Petyrk ,Yossi Gandelsman University of California, Berkeley {nickj,anishk,spetryk,yossi gandelsman}@berkeley.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "We investigate the internal representations of vision-language models (VLMs) to address hallucinations, persistent challenge despite advances in model size and training. We project VLMs internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to models latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how deeper understanding of VLMs latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-Language Models (VLMs) have recently emerged as powerful tools for understanding images via text (Dai et al., 2023; Liu et al., 2024). They have demonstrated remarkable capabilities across multimodal tasks such as image captioning (Li et al., 2023a), visual question answering (Ye et al., 2023), and complex multimodal reasoning (Bai et al., 2023). Despite their capabilities, VLMs tend to hallucinate content that does not appear in the images (Ji et al., 2023), which poses serious concerns for the reliability of these models in real-world applications (Hu et al., 2023; Luo et al., 2024). Widespread belief has been that scaling to larger models and more training data will naturally mitigate hallucinations. However, recent studies have shown that hallucinations persist even in larger and more advanced models (Rohrbach et al., 2019; Li et al., 2023b), suggesting that this issue cannot be solved by scale alone. Current methods reduce hallucinations by applying external interventions (e.g. object detectors; Yin et al. (2023)) or additional model fine-tuning (e.g. on hallucination examples; Zhou et al. (2024); Zhang et al. (2024a)). Nevertheless, these methods often struggle to distinguish between subtle hallucinations and existing details, requiring new models or updated model parameters. In this paper, we aim to introduce fine-grained edits directly to the image latent representations of VLMs to reduce hallucinations without hindering their performance, an approach that has had some success in large language models (Zhang et al., 2024b; von Rutte et al., 2024). To edit the latent representations of VLMs, we first explain their role via text. We employ the logit lens technique (nostalgebraist, 2020) to directly interpret the spatial VLM image representations with VLM text vocabulary. Surprisingly, the characteristics of these image representations are different for real objects that appear in the image and objects that are hallucinated. Moreover, the logit lens enables spatially localizing objects within the input image. Relying on the ability to detect hallucinated objects, we edit them out by intervening in their internal representations. We introduce knowledge erasure algorithm, PROJECTAWAY, to target and remove objects by linearly orthogonalizing image features with respect to the text features of target objects. We find that PROJECTAWAY can erase both real and hallucinated objects with high rates of removal. Equal contribution as first authors. Equal contribution as last authors. 1Code: https://github.com/nickjiang2378/vl-interp Figure 1: Interpreting VLM internal image representations. (a) Given VLM, (b) we unembed the latent representations from image embeddings to the vocabulary and classify hallucinations. We remove hallucinations by (c) linearly editing them out of the latent representations. We use our interpretation and editing approach for three tasks. First, we utilize the logit lens on image features to detect hallucinations in the image. We find that it improves mAP by 22.45% and 47.17% in two VLMs. Then, we combine our editing and detection method to erase hallucinations from the VLMs internal representations, reducing hallucinations up to 25.7% on standard benchmarks, while preserving accuracy in image captioning. Finally, we use the logit lens to localize objects in the image features. We find that our spatial mapping provides comparable performance to state-of-the-art zero-shot segmentation methods. Our results indicate that understanding the internal representations of VLMs can be achieved and used to repair model hallucinations and introduce new capabilities."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 INTERPRETING LATENT REPRESENTATIONS IN LANGUAGE MODELS Interpreting the inner workings of large language models enables fine-grained improvement of the language model behavior. Recent work involves utilizing the models attention maps (Kobayashi et al., 2020; Chefer et al., 2021), activation patterns (Conmy et al., 2023; Meng et al., 2023; Bronzini et al., 2024), and latent representations (Ghandeharioun et al., 2024; Cunningham et al., 2023; Bricken et al., 2023) to understand their behavior with applications such as early exiting (Halawi et al., 2024) and editing or erasing the models knowledge (Dai et al., 2022; Ravfogel et al., 2024). One class of methods probe the VLMs knowledge with linear classifiers (Hewitt & Manning, 2019; Tucker et al., 2021; Li et al., 2024; Belrose et al., 2023). The logit lens method (nostalgebraist, 2020), which we will use in our analysis, finds the output distribution over the vocabulary of the language model at intermediate layers with the models own unembedding matrix. We apply this approach to VLMs to interpret the models understanding of visual information in the models textual vocabulary. 2.2 INTERPRETING LATENT REPRESENTATIONS IN VISION MODELS Understanding the internal dynamics of vision models is critical for ensuring safety and reliability in multimodal systems. Early works in this area focused on producing saliency maps (Petsiuk et al., 2018), analyzing individual neurons (Bau et al., 2020; 2019; Dravid et al., 2023), and training networks to map latent representations to concepts (Esser et al., 2020). With the emergence of transformerbased vision models like CLIP (Radford et al., 2021), recent methods explain latent tokens (Chen et al., 2023) and the roles of attention heads and neurons with natural language (Gandelsman et al., 2024b;a; Shaham et al., 2024). Few works currently interpret the internal computation of VLMs: Palit et al. (2023) develop neuron causal tracing tool; Schwettmann et al. (2023) identifies multi-modal neurons; and Huo et al. (2024) ablates domain-specific neurons to improve vision question-answering. 2 Whereas past papers have primarily studied the mechanisms (e.g. neuron analysis) that drive VLMs, we focus on interpreting and editing their latent representations for real-world applicability."
        },
        {
            "title": "2.3 DETECTING AND REDUCING VLM HALLUCINATIONS",
            "content": "While VLM performances on image caption and visual question answering are continually improving, they continue to hallucinate facts that are not supported by the visual input. Existing methods for detecting hallucinations in language models during inference utilize latent representations (He et al., 2024; Su et al., 2024), activations (Chen et al., 2024), and output logit values (Varshney et al., 2023). SAPLMA (Azaria & Mitchell, 2023) trains hallucination classifier on the internal latent representations. LUNA (Song et al., 2024) learns transition function on latent representations and identifies abnormal transitions. Varshney et al. (2023) uses the final layer logits to score the models confidence in an entity or keyword and intervenes by instructing the model to either repair or remove the hallucinated information. Among VLMs, LURE (Zhou et al., 2024) is fine-tuned revisor model to detect and reduce hallucinations. OPERA (Huang et al., 2024) uses the models internal attention weights to detect and suppress patterns that align with the beginning of hallucinated phrases. In contrast to these methods, we leverage the internal image representations in the VLMs for hallucination reduction and for zero-shot segmentation."
        },
        {
            "title": "3 EXTRACTING KNOWLEDGE FROM VLMS",
            "content": "We start by introducing VLMs and the general framework of their architectures in most recent work. We then describe our approach for decoding the features in intermediate image representations in VLMs into text, and apply it to two types of VLMs. Surprisingly, this approach effectively probes the knowledge about objects present in images and can localize objects within the image. 3.1 PRELIMINARIES Vision-Language Models. The architecture of recent state-of-the-art VLMs for text generation typically involves three main components: vision encoder to process image inputs, mapping network to map image features to image embeddings, and an autoregressive language model to process the image embeddings and prompt embeddings to generate text. We focus on two recent state-of-the-art VLMs: LLaVA 1.5 (Liu et al., 2024) and InstructBLIP (Dai et al., 2023). We use 7B versions of both these models. LLaVA utilizes frozen CLIP vision encoder and an MLP as mapping network to project the vision encoder outputs into image embeddings for the language model. The MLP is pre-trained on large vision-language dataset and both the MLP and the language model are fine-tuned on an instruction-focused dataset. In contrast, InstructBLIP freezes both the vision encoder and the language model and only trains the mapping network. Notations. For the purposes of our work, we define the VLM architecture as follows. The vision encoder processes an input image to produce image features. These image features are projected to embedding space via the mapping network, resulting in d-dimensional image embeddings {ki : ki Rd, = 1, ..., n}. For the language model, the entire set of text tokens constitutes the vocabulary with vocabulary size . The image embeddings, followed by text embeddings {ti : ti Rd, = 1, ..., m} of the prompt tokens, are input to the language model through decoder layers. For an input embedding Rd, we define hl(x) Rd to be the latent representation for embedding at layer {1, ..., L}, the output of the decoder layer, which is conditioned on previous tokens of the input sequence. An unembedding matrix WU RV maps the last latent representation hL(tm) to probability distribution over the vocabulary for the next token tm+1. Logit Lens. Logit Lens is an interpretability method for intermediate language model representations introduced in Section 2.1. The logit lens technique applies the unembedding matrix WU to latent representations hl(x) in the intermediate layers in the language model to retrieve the logit distributions over the vocabulary. fl(tm) = WU hl(tm) = [logit1, logit2, logit3, . . . , logitV ] (1) This is the logit distribution representing the predictions of the model after layers, where logitj corresponds to the token in the vocabulary. 3 Figure 2: Comparison of internal confidence in objects present and not present in the image. We examine the internal confidence of COCO objects that exist and do not exist in the image within intermediate VLM image representations. We observe that objects that do not exist in the image have lower internal confidence."
        },
        {
            "title": "3.2 APPLYING LOGIT LENS ON VLMS",
            "content": "We apply the logit lens to probe the language model as it processes the image representations. This enables us to interpret the image features output distributions as they are transformed by the layers of the language model and localize objects spatially within the image. Extracting probability distributions from intermediate image representations. We apply logit lens on the image representations in the VLM. For given image embedding ki, we find the latent representation of the image embedding at layer l, hl(ki), taking the logit lens to get the probability distribution over the vocabulary, softmax(fl(ki)). We define an object o, an object word composed of tokens from the vocabulary. We inspect the probability of specific object o, softmax(fl(ki))o. For multi-token objects, we take the maximum probability value over the object tokens. This provides generalizable framework for analyzing specific latent image representations via text, with respect to specific objects. Next, we find the maximum probability over all image representations over all layers. For object o, we compute: co = max 1lL 1in {softmax(fl(ki))o} (2) We define co as the VLMs internal confidence of an object existing in the image: the highest probability of object presence across image representations through layers of the language model. Comparing the internal confidence of present and not present objects. To determine if internal confidence provides meaningful information about objects in the image, we examine co for objects present and not present in an image. We use InstructBLIP and LLaVA to caption 5000 random COCO2014 images in the Karpathy validation split (Lin et al., 2015) and determine co for all 80 COCO objects, only few of which are present in each image. Since there are many more objects not present than present, we randomly sample subset of the internal confidences for objects not present. Figure 2 exhibits the internal confidences for objects present and not present in the image. We empirically find that the VLMs internal confidences are higher for present objects than not present ones. We use this claim later to classify objects as hallucinations in Section 5.1. Object localization. Given that the language model can distinguish between objects present and not present in an image, we examine whether it can attribute high object internal confidence to specific patches in an image. For each image embedding ki in image embeddings, we find the maximum softmax probability of an object within the layers of the model, max1lL{softmax(fl(ki))o}. Using these internal confidence values, we localize the objects in the image patches, each of which maps to an image embedding. We focus on LLaVA for this task, since its image encoder preserves the spatial mapping of image patches to image features. We observe that image representations that exhibit higher internal confidence for specific objects correspond to the image patches in which those objects are visually present (examples in Figure 3). Building on our previous observation, we see that the intermediate image representations semantically align with latent token representations of objects present in them while maintaining their spatial locality. We use this unique finding for zero-shot segmentation in Section 5.3. 4 Figure 3: Localizing objects using internal confidence values. We find the probabilities of objects through layers of the language model for every image embedding in LLaVA. We use the highest layer probability per image embedding to localize an object within the image. While the model is not directly trained to map the image representations closer to the text representations of objects within them, we can unembed the image representations in the text vocabulary for localization and find differences in internal confidence for present and hallucinated objects. In Section 5.1, we will use this observation for various applications including hallucination detection and zero-short segmentation."
        },
        {
            "title": "4 ERASING KNOWLEDGE FROM VLMS",
            "content": "Recognizing that image embeddings are directly interpretable (Section 3.2), we edit these embeddings to erase the presence of objects from image captions. We propose linear editing algorithm that subtracts the text embedding of target object from all image embeddings. When applied on singular and multiple object removals, we find that it erases hallucinated objects more effectively than correctly detected (CD) objects (i.e. real objects that the model correctly detects). 4.1 ERASING OBJECTS FROM IMAGE REPRESENTATIONS We present an algorithm, PROJECTAWAY (Figure 4), that orthogonalizes image representations with respect to text representations in order to erase objects in image captions, applying it to remove objects one at time and all at once. Given an image and an object to remove, we edit the latent representations hlI (ki) at hidden layer lI across all image embeddings ki. We do not modify any latent representations outside of those belonging to image features. We compute the dot product, p, of hlI (ki) and the objects text embedding t, subtracting weighted from hlI (ki) only if the dot product is positive. At α = 1, PROJECTAWAY is equivalent to orthogonalizing the image representations with respect to the text representation. To compute text representation t, we pass the object (e.g. hot dog) into the VLMs text model and extract hlT (t-1) at hidden layer lT , where t-1 is the last token of the object. We use the last token of the object to capture the whole of the objects meaning. Algorithm 1: PROJECTAWAY Input: set of image embeddings K, text embedding t, and weight factor α Output: set of modified image embeddings projected away from the text embedding Initialization: for do t if > 0 then {k α t2 2 t} else {k} end if end for Figure 4: Our editing algorithm erases the presence of an object from image embeddings by orthogonalizing them with respect to the objects text embedding. 5 Edit Scope Model Individual RR (%) Mass RR (%) CD change (%) Ci Cs No edits Hallucinations CD InstructBLIP LLaVA - - InstructBLIP LLaVA InstructBLIP LLaVA 83.3 86.0 16.2 6.9 - - 74.3 72.8 15.0 8. - - +0.07 +0.01 -2.2 -1.6 15.0 14.6 8.94 11.2 17.3 15. 54.1 51.1 33.2 35.5 58.3 52.4 Table 1: Removing mentioned objects individually & in-mass. Using PROJECTAWAY, we remove hallucinated objects and observe high hallucination reduction with CHAIR, mass-removal rate (Mass RR), and individual removal rate (Individual RR). We also remove correctly detected (CD) objects but find that they are more resistant to linear editing. Denote CHAIRS as CS and CHAIRI as CI ."
        },
        {
            "title": "4.1.1 REMOVING OBJECTS ONE BY ONE",
            "content": "We evaluate the PROJECTAWAY algorithms effectiveness at erasing individual objects from captions across multiple images and objects. Experimental setting. We apply PROJECTAWAY on 5000 random images from the COCO2014 training set on all mentioned COCO objects (i.e. hallucination and CD) individually and measure the removal rate at which objects no longer appear in the caption. For InstructBLIP, we set (lI , lT , α) = (1, 2, 1.5). For LLaVA, we set (lI , lT , α) = (19, 21, 3.5). These parameters are fixed irrespective of image and are chosen for their maximal effect (see ablations in Section 4.2). To differentiate hallucinations from CD, we compute CHAIR (Rohrbach et al., 2019), an evaluation criteria that compares model-generated captions to ground-truth human annotations. CHAIR provides two main scores, CHAIRI and CHAIRS, that quantify hallucinations for instances and sentences, respectively: CHAIRS = {captions with hallucinated objects} {all captions} , CHAIRI = {hallucinated objects} {all objects mentioned} (3) Results. Table 1 shows that PROJECTAWAY is significantly more effective in erasing individual hallucinated objects at an individual level than CD objects for both InstructBLIP and LLaVA. Along with the insight that hallucinated objects have lower softmax scores (Figure 2), these results suggest that hallucinated objects manifest more weakly in image embeddings and are hence easier to remove than CD objects. 4.1.2 MASS-REMOVING OBJECTS We iteratively apply PROJECTAWAY to set of objects, following the same experimental setup and observing similarly different removal rates for hallucinated objects and CD objects. Mass-removing hallucinations. We mass-remove hallucinations identified with ground truth annotations using PROJECTAWAY. Table 1 shows that editing out all the hallucinations of an image yields similar removal rate as individually editing out and, importantly, that erasing hallucinated objects together does not interfere with each other. We achieve hallucination reduction rate of 41.3% for InstructBLIP and 23.3% for LLaVA (see Table 4). Recall count slightly increases for both models, indicating that caption accuracy is preserved. This may be because removed hallucinations are replaced with objects the model is more confident in. Qualitative results are in Figure 5. Mass removing CD. We similarly find that applying PROJECTAWAY can successfully remove CD objects when edited all together in Table 1. Furthermore, CHAIR scores minimally change, which indicates that this mass-removal merely erases object presence without eroding caption accuracy. While the removal rate is lower than for hallucinated objects, this insight proves useful when we apply PROJECTAWAY for hallucination reduction in Section 5.2. 4.2 ABLATION STUDY: MASS-REMOVING HALLUCINATIONS We perform ablations on parameters of PROJECTAWAY to improve object removal rate for erasing hallucinations in-mass. 6 Figure 5: Qualitative results for mass object removal. We present example images and their captions after mass-removing hallucinations (red) with PROJECTAWAY., which can effectively remove hallucinations while preserving, even increasing, correctly detected objects (green). . Experimental setting. We ablate the three parameters of PROJECTAWAY: layer lI to edit at, layer lT to retrieve the text representation, and weight factor α. At lT = 1, we average together the objects constituent token embeddings. At lI = 1, we edit the image embeddings directly inputted to the text model. We evaluate across 500 training samples from COCO 2014 that have at least one hallucination. Hidden layers. Figure 6 shows hallucination reduction rate on LLaVA from mass-removing hallucinations on every combination of lI and lT (each from -1 to 31). As core concern is that editing erodes caption accuracy, we gray out any combination that reduces CD objects. For InstructBLIP (see Figure 10), the best parameters (lI = 1, lT = 2) reduces hallucinations by 38.5%. For LLaVA, our best parameters (lI = 19, lT = 21) reduce hallucinations by 25.7%, and the middle layers are the best to edit and extract latent text embeddings from. Our results also provide wide range of reasonable parameter alternatives to use if this reduction rate does not generalize beyond our samples. Weight factor. Using the best-reduced hidden layers, we ablate the weight factor α for PROJECTAWAY across the same 500 randomly selected COCO images. Figure 7 shows that as α increases, hallucinations are removed at higher rate, and the overall hallucination count drops significantly. At high α, we observe through anecdotal examples that captions become nonsensical, as quantitatively shown by the complete loss of both correctly detected and hallucinated objects from the caption. Therefore, as pre-caution, we only select weight factors that do not reduce CD objects when we apply PROJECTAWAY to erase hallucinated objects."
        },
        {
            "title": "5 APPLICATIONS",
            "content": "5.1 HALLUCINATION DETECTION When extracting knowledge from VLMs in Section 3.2, we found that applying logit lens on incontext image representations exhibit useful information about visual objects present in the image. Using these observations, we present an approach for object presence classification that only relies on the VLMs own parameters. We utilize the internal confidence co value to classify object presence, 7 Figure 6: Hidden layer ablations for LLaVA. We track hallucination reduction (%) across different layers to edit at and extract latent embeddings for the text embedding, crossing out (red) parameters from consideration where there is decrease in correctly detected objects. Figure 7: Weight ablations for LLaVA. We vary the weight factor α and measure changes in correctly detected objects, removal rate, and hallucination reduction. We observe decline in hallucinations as weight grows and mark weight where there is no loss in correctly detected objects. Figure 8: Object Presence Classification Curves for InstructBLIP and LLaVA. We show the Precision-Recall and ROC curves of our confidence measure for present object-hallucination classification on the COCO training subset. Classifying object presence with the internal confidence outperforms the baseline, indicating that the language models image representations know which objects are hallucinations and which are truly present. since the internal confidence for objects that are not present in the image, or hallucinated, are lower within the image representations. Experimental setting. We evaluate the strength of the internal confidence co as an indicator of object presence. We sample 5000 images from the MSCOCO training set, using the image captioning objective to caption methods with both InstructBLIP and LLaVA. We use the co for present objects and hallucinations within the captions generated by each VLM. We assess how well the internal confidence aligns with the ground truth labels of object presence, where negative sample is hallucination and positive sample is present object. Baseline. As baseline, we use the maximum output probability of the objects tokens. This is the confidence of the model prediction. Previous works such as Zhou et al. (2024) have found that hallucinations occur more frequently on objects characterized by high uncertainty during generation. Results. We present quantitative results in Figure 8 and Appendix A.5. We show qualitative results for LLaVA (Figure 12) and InstructBLIP (Figure 13) in the Appendix. We find that utilizing internal confidence to classify object hallucinations provides 47.17% improvement in mAP in InstructBLIP and 22.45% in LLaVA. Furthermore, the ROC AUC improves over the baseline by 50.10% in InstructBLIP and 44.68% in LLaVA, indicating stronger object presence classification. 8 Model Method CHAIRi CHAIRs Hallucinated Objects InstructBLIP LLaVA Greedy Nucleus Beam Search OPERA Ours Greedy Nucleus Beam Search OPERA Ours 57.0 58.0 53.4 45.6 43.8 49.2 55.8 52.4 44.8 42.0 23.3 24.0 14.6 13.9 12.5 14.2 17.1 15.0 12.8 12.2 512 508 564 472 419 532 618 583 462 Table 2: Hallucination intervention performance. We mass-remove hallucinations detected by the method in Section 5.1 and outperform other baselines. We observe considerable drop in the raw count of hallucinated objects. 5.2 HALLUCINATION REMOVAL We use the mass editing technique to remove hallucinations detected by the prior method. Section 4.1.2 successfully removes significant portion of hallucinations but presupposes knowledge of what the hallucinations are. We threshold on the internal confidence of each object to identify hallucinations and mass-remove them using PROJECTAWAY. Our chosen threshold prioritizes precision over recall (i.e. we allow classification of some CD objects as hallucinations) because CD objects are less affected by the removal method, as shown in Section 4.1.2. Experimental setting. We threshold hallucinations as co < 0.2 for InstructBLIP and co < 0.1 for LLaVA. Based on prior ablations (Section 4.2), we select (lI = 1, lT = 2, α = 1.5) for InstructBLIP and (lI = 19, lT = 21, α = 3.5) for LLaVA. Our prompt is Please describe this image in detail. Baselines. Since our method intervenes during the decoder step, we compare our method with 3 standard decoding algorithms. Greedy decoding predicts the next token based on the highest logit probability. Beam search maintains tree of beams and selects the best beam at generation end. Nucleus sampling selects the next token from set of high probability tokens whose cumulative probability reaches threshold p. We also evaluate against OPERA (Huang et al., 2024), which mitigates hallucinations by adding an overtrust penalty during decoder generation. We set = 0.9 for nucleus sampling. We use beam search in our method and unify Nbeam = 5 for the baseline. Results. We apply these parameters to 500 COCO images from the Karpathy validation set. We provide qualitative results in Figure 15 and Figure 14. Quantitative results in Table 2 show that we outperform our baselines and reduce hallucinations by 25.7% on InstructBLIP and 23.8% on LLaVA compared to beam search. Our approach achieves similar hallucination reduction rate as Section 4.1.2, despite not precisely differentiating hallucinations and some CD objects being incorrectly edited out. Notably, our method relies on no training or external models, effectively offering free lunch. 5.3 ZERO-SHOT SEGMENTATION Building upon our findings in Section 3.2, we utilize the internal confidence per image feature for zero-shot image segmentation. This application leverages the spatial information encoded in the image representations and demonstrates how VLMs internally represent and localize objects within images. Method. Our approach leverages the spatial correspondence between image patches and their associated image embeddings. We use LLaVA to generate the name of the class in the image and we focus on the internal confidence of that class per image patch. We take the mean internal confidence for tokens comprising class word. We resize the set of 24 24 internal confidence values per image patch back into fixed image size of 336 366 pixels. We then apply threshold to these confidence values to binarize them into foreground/background segmentation for the object in the image. 9 Model Method Pixel Acc. mIoU mAP raw attention (CLIP) TextSpan (Gandelsman et al., 2024b) raw attention (VLM) Ours Image Encoder Image Encoder VLM VLM 69.81 75.57 67.28 76.16 45.19 53.60 39.27 54.26 77.30 80.22 73.96 79. Table 3: Segmentation Performance on ImageNet-segmentation. Localizing objects using their probabilities within the image representations results in more accurate zero-shot segmentation than previous vision-encoders-based and VLM-based methods. Baseline. As baseline, we extract the attention values of generated tokens with the image embeddings from LLaVA. We also compare to the segmentation method introduced by Gandelsman et al. (2024b), which utilizes the attention heads of the image encoder without the additional VLM processing, using the same image encoder (CLIP-ViT-L/14 at 336px). Results. We evaluate our method on the Imagenet validation set. Qualitative results are shown in Figure 9 and quantitative comparisons with the baselines in Section 5.3. We improve mAP by 8.03% over using the VLMs raw attention values and provide better and/or comparable performance to other state-of-the-art methods that utilize just the image encoder. While the VLM is not directly trained for segmentation, our technique reveals that it still encodes significant spatial information about objects within its intermediate image representations."
        },
        {
            "title": "6 DISCUSSION AND LIMITATIONS",
            "content": "Figure 9: Zero-shot segmentation. Warmer areas indicate higher internal confidence for the class at that image patch. We binarize these values with threshold to generate segmentations. We interpreted VLMs image representations through the language model layers and discovered that linear editing of these representations can selectively remove object information via simple orthogonalization. Our findings enabled hallucination reduction and improved zero-shot segmentation. We present two limitations of our work and conclude with future directions. Multi-token objects. Our method simplifies the use of object words that may be composed of multiple tokens, such as by taking the max internal confidence over object tokens or utilizing the average token embedding for editing. This can introduce noise to the internal confidence if certain tokens are common in multiple different words and lead to an approximation of the objects latent representations when editing. Fine-grained edits. The editing approach may struggle with highly abstract or longer sentences that involve attributes or interactions of objects. Removing full sentence, for example, is not something we assessed in this paper, since our focus is on the removal of individual objects. Future work. While our focus was on interpreting objects and object hallucinations in VLMs, we believe that our approach can be extended to other key elements of visual scenes, such as people, attributes, and actions. We also focused on object removal, but we believe that editing can also be extended to inject objects into caption (by adding instead of subtracting the text embedding). We hope to explore the applications of our approach in other multimodal architectures. Our insights may help design better VLMs that are more robust to hallucinations and have improved spatial understanding. We plan to explore these directions in our future work."
        },
        {
            "title": "6.1 ACKNOWLEDGMENTS",
            "content": "We thank Kayo Yin for her comments and feedback on our paper. YG is supported by the Google Fellowship. Authors, as part of their affiliation with UC Berkeley, were supported in part by the the Berkeley Artificial Intelligence Research (BAIR) commons program."
        },
        {
            "title": "REFERENCES",
            "content": "Amos Azaria and Tom Mitchell. The internal state of an LLM knows when its lying. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 967976, Singapore, December 2023. Association doi: 10.18653/v1/2023.findings-emnlp.68. URL https: for Computational Linguistics. //aclanthology.org/2023.findings-emnlp.68. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966. David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B. Tenenbaum, William T. Freeman, and Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2019. David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba. Understanding the role of individual units in deep neural network. Proceedings of the National Academy of Sciences, 2020. ISSN 0027-8424. doi: 10.1073/pnas.1907375117. URL https: //www.pnas.org/content/early/2020/08/31/1907375117. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens, 2023. URL https://arxiv.org/abs/2303.08112. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. URL https://transformer-circuits. pub/2023/monosemantic-features/index.html. Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, and Andrea Passerini. Unveiling llms: The evolution of latent representations in dynamic knowledge graph, 2024. URL https: //arxiv.org/abs/2404.03623. Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers, 2021. URL https://arxiv.org/abs/2103.15679. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms internal states retain the power of hallucination detection, 2024. URL https: //arxiv.org/abs/2402.03744. Haozhe Chen, Junfeng Yang, Carl Vondrick, and Chengzhi Mao. Interpreting and controlling vision foundation models via text explanations, 2023. URL https://arxiv.org/pdf/2310. 10591. Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri`a GarrigaAlonso. Towards automated circuit discovery for mechanistic interpretability, 2023. URL https: //arxiv.org/abs/2304.14997. Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models, 2023. URL https://arxiv. org/abs/2309.08600. 11 Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers, 2022. URL https://arxiv.org/abs/2104.08696. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. URL https://arxiv.org/abs/2305.06500. Amil Dravid, Yossi Gandelsman, Alexei A. Efros, and Assaf Shocher. Rosetta neurons: Mining the common units in model zoo. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 19341943, October 2023. Patrick Esser, Robin Rombach, and Bjorn Ommer. disentangling invertible interpretation network for explaining latent representations, 2020. URL https://arxiv.org/pdf/2004.13166. Yossi Gandelsman, Alexei A. Efros, and Jacob Steinhardt. Interpreting the second-order effects of neurons in clip, 2024a. URL https://arxiv.org/abs/2406.04341. Yossi Gandelsman, Alexei A. Efros, and Jacob Steinhardt. Interpreting clips image representation via text-based decomposition, 2024b. URL https://arxiv.org/pdf/2310.05916. Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. Patchscopes: unifying framework for inspecting hidden representations of language models, 2024. URL https://arxiv.org/abs/2401.06102. Danny Halawi, Jean-Stanislas Denain, and Jacob Steinhardt. Overthinking the truth: Understanding how language models process false demonstrations, 2024. URL https://arxiv.org/abs/ 2307.09476. Jinwen He, Yujia Gong, Kai Chen, Zijin Lin, Chengan Wei, and Yue Zhao. Llm factoscope: Uncovering llms factual discernment through inner states analysis, 2024. URL https:// arxiv.org/abs/2312.16374. John Hewitt and Christopher D. Manning. structural probe for finding syntax in word representations. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41294138, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1419. URL https://aclanthology.org/N19-1419. Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and Xiang Li. Rsgpt: remote sensing vision language model and benchmark, 2023. URL https://arxiv.org/abs/2307.15266. Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation, 2024. URL https://arxiv.org/abs/ 2311.17911. Jiahao Huo, Yibo Yan, Boren Hu, Yutao Yue, and Xuming Hu. Mmneuron: Discovering neuronlevel domain-specific interpretation in multimodal large language model, 2024. URL https: //arxiv.org/pdf/2406.11193v1. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138, March 2023. ISSN 1557-7341. doi: 10.1145/3571730. URL http://dx.doi.org/10.1145/3571730. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. Attention is not only weight: In Bonnie Webber, Trevor Cohn, Yulan He, and Analyzing transformers with vector norms. Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 70577075, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.574. URL https://aclanthology.org/ 2020.emnlp-main.574. 12 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023a. URL https:// arxiv.org/abs/2301.12597. Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring sequence model trained on synthetic task, 2024. URL https://arxiv.org/abs/2210.13382. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023b. URL https://arxiv.org/ abs/2305.10355. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405.0312. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024. URL https://arxiv.org/abs/2310.03744. Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong Sun, and Yang Liu. Codis: Benchmarking context-dependent visual comprehension for multimodal large language models, 2024. URL https://arxiv.org/abs/2402.13607. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt, 2023. URL https://arxiv.org/abs/2202.05262. nostalgebraist. Interpreting GPT: The 2020. lens. https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/ LessWrong, Aug logit URL interpreting-gpt-the-logit-lens. Vedant Palit, Rohan Pandey, Aryaman Arora, and Paul Pu Liang. Towards vision-language mechanistic interpretability: causal tracing tool for blip, 2023. URL https://arxiv.org/pdf/ 2308.14179. Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of black-box models, 2018. URL https://arxiv.org/pdf/1806.07421. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https: //arxiv.org/pdf/2103.00020. Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan Cotterell. Linear adversarial concept erasure, 2024. URL https://arxiv.org/abs/2201.12091. Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning, 2019. URL https://arxiv.org/abs/1809.02156. Sarah Schwettmann, Neil Chowdhury, Samuel Klein, and Antonio Torralba. Multimodal neurons in pretrained text-only transformers, 2023. URL https://arxiv.org/pdf/2308.01544. Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob Andreas, and Antonio Torralba. multimodal automated interpretability agent, 2024. URL https://arxiv.org/pdf/2404.14394. Da Song, Xuan Xie, Jiayang Song, Derui Zhu, Yuheng Huang, Felix Juefei-Xu, and Lei Ma. Luna: model-based universal analysis framework for large language models, 2024. URL https: //arxiv.org/abs/2310.14211. Weihang Su, Changyue Wang, Qingyao Ai, Yiran HU, Zhijing Wu, Yujia Zhou, and Yiqun Liu. Unsupervised real-time hallucination detection based on the internal states of large language models, 2024. URL https://arxiv.org/abs/2403.06448. 13 Mycal Tucker, Peng Qian, and Roger Levy. What if this modified that? syntactic interventions via counterfactual embeddings, 2021. URL https://arxiv.org/abs/2105.14002. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation, 2023. URL https://arxiv.org/abs/2307.03987. Dimitri von Rutte, Sotiris Anagnostidis, Gregor Bachmann, and Thomas Hofmann. language models guide through latent space, 2024. URL https://arxiv.org/pdf/2402.14433. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023. URL https://arxiv.org/abs/2311.04257. Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models, 2023. URL https://arxiv.org/abs/2310.16045. Jinrui Zhang, Teng Wang, Haigang Zhang, Ping Lu, and Feng Zheng. Reflective instruction tuning: Mitigating hallucinations in large vision-language models, 2024a. URL https://arxiv.org/ abs/2407.11422. Shaolei Zhang, Tian Yu, and Yang Feng. Truthx: Alleviating hallucinations by editing large language models in truthful space, 2024b. URL https://arxiv.org/pdf/2402.17811. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models, 2024. URL https://arxiv.org/abs/2310.00754."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 MASS-REMOVING OBJECTS We mass-remove mentioned objects (hallucinations and correctly detected) with PROJECTAWAY and tally up the total number of unique hallucinated and CD objects in Table 4. A.2 ABLATIONS FOR INSTRUCTBLIP We show hidden layer and weight ablations for mass-removing hallucinations in InstructBLIP referenced in Section 4.2. The hidden layer ablations indicate that most of the parameter space is too sensitive to edit and leads to losses in correctly detected objects. We find that smaller lT and lI parameters are the most effective for reducing hallucinations. Our best parameters (lI = 1, lT = 2) It is not fully understood why the majority of the parameter reduce hallucinations by 38.5%. search space is invalid in comparison with LLaVA in Figure 6. It is possible that the fine-tuning step in LLaVA semantically aligns hidden image representations with text embeddings more than InstructBLIP, allowing linear edits to have the precise, intended effect. A.3 HALLUCINATION DETECTION We show quantitative comparisons from our hallucination detection approach using internal confidence (Section 5.1) to the baseline in Appendix A.5. We also show qualitative examples for LLaVA in Figure 12 and for InstructBLIP in Figure 13. These samples exhibit model-generated captions, parsed objects, and whether they are classified as hallucinated or correctly detected based on their internal confidence score. A.4 HALLUCINATION REDUCTION We exhibit sample results from our hallucination reduction approach (Section 5.2), which linearly removes text representations of hallucinations from image representations, in Figure 15 for InstructBLIP and Figure 14 for LLaVA. We show the image caption before and after our linear editing method, removing objects detected as hallucinations. A.5 OBJECT LOCALIZATION We show qualitative examples for localization with internal confidence for specific image representations, specifically for the LLaVA model, in Figure 16. Edit Scope Model Hallucinations CD No edits Hallucinations CD InstructBLIP LLaVA InstructBLIP LLaVA InstructBLIP LLaVA 4545 4372 2672 3348 5078 4583 14178 15053 14189 13864 14826 Table 4: Supplemental metrics for Table 1. We measure unique hallucinated and correctly detected (CD) objects. 15 InstructBLIP LLaVA Method mAP ROC AUC mAP ROC AUC"
        },
        {
            "title": "Baseline\nOurs",
            "content": "0.53 0.78 0.55 0.83 0.49 0.60 0.47 0.68 Table 5: Object Presence Classification performance. We use internal confidence co as confidence score to classify whether the object is present in the image. We evaluate the mAP and ROC AUC of our classification method against the baseline for both the InstructBLIP and LLaVA models. Figure 10: Hidden layer ablations for InstructBLIP. We track hallucination reduction (%) across different layers to edit at and extract latent embeddings for the text embedding, crossing out (red) parameters from consideration where there is decrease in correctly detected objects. Figure 11: Weight ablations for InstructBLIP. We vary the weight factor α and measure changes in correctly detected objects, object removal rate, and hallucination reduction. We observe decline in hallucinations as weight increases and mark weight where there is no loss in correctly detected objects. 16 Figure 12: LLaVA Object Presence Classification. Sample image captions from LLaVA and the internal confidence scores for objects in the caption used for classification as correctly detected objects or hallucinations. 17 Figure 13: InstructBLIP Object Presence Classification. Figure 14: Qualitative results for LLaVA hallucination intervention. Our algorithm removes hallucinations and, at times, adds correctly detected objects. 19 Figure 15: Qualitative results for InstructBLIP hallucination intervention. 20 Figure 16: Object Localization Samples."
        }
    ],
    "affiliations": [
        "University of California, Berkeley"
    ]
}