{
    "paper_title": "Forest Before Trees: Latent Superposition for Efficient Visual Reasoning",
    "authors": [
        "Yubo Wang",
        "Juntian Zhang",
        "Yichen Wu",
        "Yankai Lin",
        "Nils Lukas",
        "Yuhan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a \"Forest-before-Trees\" cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 1 ] . [ 1 3 0 8 6 0 . 1 0 6 2 : r Forest Before Trees: Latent Superposition for Efficient Visual Reasoning Yubo Wang*, Juntian Zhang*, Yichen Wu, Yankai Lin, Nils Lukas, Yuhan Liu MBZUAI Fudan University Gaoling School of Artificial Intelligence, Renmin University of China Harvard University yuhan.liu@mbzuai.ac.ae yubowang25@m.fudan.edu.cn zhangjuntian@ruc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning(DWAL). Instead of forcing point-wise prediction, Laser aligns the latent state with dynamic validity window of future semantics. This mechanism enforces \"Forest-before-Trees\" cognitive hierarchy, enabling the model to maintain probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains. We hope this work encourages paradigm shift from explicit next-token prediction to latent visual reasoning: (cid:135)Laser"
        },
        {
            "title": "Introduction",
            "content": "Vision-Language Models (VLMs) have revolutionized visual understanding by integrating Large Language Models (LLMs) with robust visual encoders [1, 2]. While adapting Chain-of-Thought (CoT) [3] has enabled multi-step reasoning, as shown in Fig 1 (a), explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are lost in discrete tokenization [4]. Emerging Latent Space Reasoning approaches [5, 6] attempt to bypass this by reasoning within high-dimensional hidden states. However, these methods typically retain standard autoregressive objectives, forcing the latent state to strictly minimize prediction error for specific token at every step. We argue that this strict point-wise mapping is fundamentally misaligned with visual perception. Unlike text generation, visual reasoning is hierarchical, evolving from global semantic apprehension to local feature extraction [7]. Forcing the latent state to prematurely \"collapse\" into precise object token before grasping the holistic context induces premature semantic collapse, creating \"tunnel vision\" effect that hinders the capture of complex relationships. *Equal contribution. The order was decided by coin flip. Work done during an internship at MBZUAI. Corresponding author. Preprint. To address this gap between sequential objectives and hierarchical perception, we propose Laser (Latent Superposition for Effective Visual Reasoning). Laser is grounded in the insight that an effective reasoning state should not be pointer to single future word, but container for validity domain of future possibilities. We redefine the optimization goal via Dynamic Windowed Alignment Learning (DWAL). As shown in Fig 1 (b), instead of predicting the immediate next token, Laser trains the latent state to align with dynamic semantic window encompassing the entire remaining reasoning path. This formulation encourages the latent representation to maintain probabilistic superposition, simultaneously encoding high-level global semantics while keeping specific details in potential state. As the reasoning process unfolds, this window naturally shrinks, enforcing progressive transition from global exploration to local precision, mimicking the generalto-specific nature of human visual processing. Figure 1: Laser replaces verbose textual rationales (a) with efficient latent superpositions (b). Achieving this \"uncollapsed\" state without external supervision presents significant optimization challenge: an unconstrained latent space risks diverging into meaningless high-entropy distributions. First, we introduce Self-Refined Superposition, which leverages the models own estimation of the future window to construct stable soft target.Second, to prevent loss of focus, we design an EntropyRegularized Intervention. This mechanism acts as an implicit curriculum: it dynamically injects rigid ground-truth guidance when the models uncertainty is high, and reverts to soft superposition when the model demonstrates grasp of the global context. In summary, Laser transforms visual reasoning from rigid sequential matching task into flexible, windowed manifold alignment problem. Our extensive experiments on complex visual reasoning benchmarks demonstrate that this approach significantly outperforms both standard CoT and baseline latent reasoning methods. By enabling VLMs to \"think\" in superpositions before collapsing to answers, Laser bridges the gap between the continuous nature of vision and the discrete nature of language. Our contributions are summarized as follows: We propose Laser, latent reasoning paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). This approach prevents premature semantic collapse by enforcing forest-before-trees cognitive process within the latent space. We design supervision framework combining Self-Refined Superposition and EntropyRegularized Intervention. This establishes an implicit curriculum that stabilizes latent learning without external annotations, dynamically balancing exploration and grounding. We achieve superior efficiency-performance balance: Laser obtains state-of-the-art results across 6 benchmarks while reducing inference tokens by over 97%. Furthermore, it demonstrates robust generalization on out-of-distribution tasks, validating the efficacy of the learned visual logic."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Vision-Language Models The evolution of VLMs has rapidly advanced from static cross-modal alignment to dynamic perception.Foundational architectures like Flamingo and BLIP-2 pioneered efficient alignment strategies using Q-Former bottlenecks to bridge frozen vision encoders with Large Language Models (LLMs) [8, 9].Subsequently, Open-source models such as LLaVA and MiniGPT-4 demonstrated that simple linear projection layers, coupled with high-quality visual instruction tuning, could achieve strong multimodal following [10, 11]. To overcome perceptual limitations in fine-grained and temporal 2 tasks, recent architectures have focused on scaling visual resolution and context length. The latest InternVL3.5, scaled vision encoders to massive parameters using dynamic tiling strategies to handle detailed imagery [12, 13]. Concurrently, Qwen2.5-VL and the advanced Qwen3-VL refined the Naive Dynamic Resolution mechanism, enabling the processing of images at arbitrary aspect ratios and extending capabilities to long-context video understanding [14, 15]. Recent research has focused extensively on enhancing VLMs reasoning capabilities. Vision-R1 [16] and VL-Rethinker [17] utilize Group Relative Policy Optimization (GRPO) with forced caption-reason-answer formats and rethinking tokens , while VISC focuses on the advancement of multi-image reasoning capabilities [18]. In parallel, architectural innovations focus on perceptual self-improvement: ViPER [19] establishes self-evolutionary framework via self-critiquing cycles, and DeepEyes [20] integrates active perception through dynamic tool invocation. Differently from these approaches, our Laser proposes efficient latent-space reasoning. 2.2 Latent Space Reasoning Explicit Chain-of-Thought enhances model capabilities but suffers from information loss due to discrete tokenization; consequently, LLMs like Quiet-STaR [21], Coconut [22], and SoftCoT [23] perform intermediate computations entirely within latent states. In VLMs, the focus shifts to anchoring latent thinking in visual evidence: CoCoVa [24] and MCOUT [25] refine representations via latent attention, whereas Mirage [26], IVT-LR [27], and ILVR [28] employ interleaved decoding to stabilize reasoning. While LVR [4] strengthens alignment through autoregressive reconstruction, it risks representation collapse; alternatively, visual scratchpads like Latent Sketchpad [29] and SkiLa [30] preserve interpretability via reconstructable latent sketches. Beyond supervised anchoring, other methods directly optimize latent reasoning trajectories (Monet [31], LaCoT [32], DMLR [33]). Mull-Tokens [34] offers modality-agnostic workspace, while Titans [35] targets long-range dependencies. Distinct from these, our Laser method introduces Dynamic Windowed Alignment mechanism. By encoding global visual semantics into compact superposition state, it achieves superior balance between latent reasoning and efficiency."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we propose Laser, an efficient visual reasoning method operating in the latent space. Section 3.1 provides formal definition of the problem. The key to realizing Laser lies in data acquisition and the design of the training methodology, which are elaborated in Section 3.2 and Section 3.3 respectively. 3.1 Problem Formulation We formulate the visual reasoning task as two-stage conditional generation process: Latent Visual Reasoning followed by Explicit Answer Generation. Given an input image and textual query Q, the model Mθ aims to synthesize chain of visual concepts = {c1, c2, . . . , cT } that acts as an intermediate reasoning path, before producing the final response = {a1, a2, . . . , aM }. The core of our formulation focuses on the Latent Reasoning Trajectory. The model processes the multimodal context to generate sequence of high-dimensional hidden states = {h1, h2, . . . , hT }, where ht = Mθ(I, Q, c<t). This latent state is projected onto the vocabulary space via linear head to obtain the logits zt = Wuht. These logits define the probability distribution over the next token via the Softmax function: Pθ(ct+1 I, Q, c<t) = Softmax(zt). (1) In standard autoregressive frameworks, the optimization objective strictly forces each latent state ht to minimize the negative log-likelihood of the immediate next token ct+1. However, this rigid, local constraint compels the latent representation to collapse prematurely into single semantic point. In this work, we redefine the optimization goal: rather than solely predicting the next token, the latent state ht is tasked with aligning with dynamic validity domain of future visual semantics. Thus, the problem transforms from minimizing point-wise prediction error to maximizing the Windowed Semantic Alignment between the latent trajectory and the reasoning chain C, laying the theoretical foundation for the Dynamic Windowed Alignment Learning (DWAL) detailed in Sec. 3.3. 3 Figure 2: Overview of the Laser. Laser employs DWAL. At each step t, dynamic validity window Wt is defined over future semantic tokens to construct Reference Superposition Distribution. The latent state is then optimized to align with this distribution via LDW AL. The final answer is generated explicitly after the reasoning using LCE. 3.2 Synthesizing Cognitive Scanpaths For the Laser method, we require dataset that captures the intermediate visual reasoning process without relying on expensive human annotations. Unlike previous approaches such as Visual CoT [36], which anchor reasoning chains with explicit bounding boxes, our aim is to bridge perception and language through implicit latent alignment. Consequently, we construct scalable annotation pipeline that operates under weakly supervised setting: relying solely on synthesized semantic sequences while deliberately excluding explicit Region-of-Interest (ROI) supervision. We leverage GPT-4o as Visual Cognitive Engine to sequentially synthesize reasoning paths composed of discrete semantic tokens. Crucially, our prompt design is grounded in the Global Precedence Hypothesis [7], which posits that human perception inherently prioritizes holistic structures (forest) before processing detailed components (trees). Guided by this principle, we enforce strict Global-to-Local Scanning Logic: the synthesized sequences must initiate with global anchor, progressively narrow focus to relevant objects, and culminate in the critical visual evidence required to answer the query. This structure ensures that the data represent valid deductive trajectories rather than static descriptions. After applying rigorous filtering protocol, we obtain final dataset named ScanPath, consisting of 270k high-quality samples. More details on prompt engineering and filtering statistics are provided in the Appendix and G. 3.3 Dynamic Windowed Alignment Learning At the core of Laser is the Dynamic Windowed Alignment Learning (DWAL). Standard autoregressive objectives typically enforce rigid, point-wise collapse to single ground-truth token at each step, which we argue is suboptimal for early-stage visual reasoning where global context is paramount. In contrast, DWAL formulates reasoning as Windowed Probabilistic Alignment problem. By defining dynamic validity domain of future semantics, this approach allows the latent state to encode superposition of potential visual concepts, mimicking the general-to-specific nature of human visual processing. 4 3.3.1 Dynamic Semantic Windows Let = {h1, h2, . . . , hT } denote the sequence of latent hidden states generated by the model during the reasoning phase, and let = {c1, c2, . . . , cT } denote the corresponding sequence of text tokens representing visual concepts, which are annotated during data construction. Previous approaches [4] typically enforce Strict Point-wise Mapping, minimizing the divergence D(ht, ct) between the latent state ht and the specific ground-truth visual concept ct at every timestep t. To overcome the limitations of this premature semantic collapse, we introduce Dynamic Semantic Window Wt for each reasoning step t. This window defines the valid semantic field that the current state should encompass: Wt = {ck }. (2) The objective is to encourage the latent representation ht to cover the full spectrum of the valid window Wt, rather than peaking solely at the immediate next token ct+1. As increases, the window Wt naturally shrinks (Wt 1), enforcing progressive transition from global semantic superposition to local precision. Note that to ensure the reasoning process terminates effectively, the dedicated special token <laser_end> is explicitly excluded from the validity window Wt for all steps [1, ]. Instead, <laser_end> serves as deterministic target only after the completion of the final reasoning step , signaling the phase transition from implicit reasoning to explicit answer generation. 3.3.2 Learning via Latent Superposition To supervise the model within the dynamic window Wt without relying on external soft labels, we employ Self-Refined Superposition mechanism. This approach leverages the models own estimation of the valid semantic manifold to construct stable soft target. Specifically, we extract the logits corresponding to the tokens in Wt and apply stop-gradient operation to prevent unstable self-reinforcement loops. Let ˆz(k) ) denote the detached logit for token Wt. We define reference superposition distribution Qt via temperature-scaled Softmax: = StopGrad(z(k) Qt(k) = exp(ˆz(k) /τ ) exp(ˆz(j) (cid:80) jWt /τ ) , (3) where τ is hyperparameter controlling the sharpness of the distribution. This formulation encourages the hidden state ht to maintain probabilistic superposition of future visual concepts. However, relying solely on soft targets can lead to optimization divergence, where the model converges to high-entropy uniform distribution lacking semantic focus. To mitigate this, we introduce an Entropy-Regularized Intervention. We first compute the normalized entropy of the reference distribution to gauge the models uncertainty: H(Qt) = 1 log Wt (cid:88) kWt Qt(k) log Qt(k). (4) that dynamically switches between the soft superposition We then construct hybrid target target and rigid next-token alignment based on this uncertainty: (cid:26)α yhard + (1 α) Qt, target = Qt, if H(Qt) > η otherwise (5) where yhard is the one-hot vector for the immediate next token ct+1, η is predefined entropy threshold, and α [0, 1] controls the intensity of the hard intervention. This mechanism creates an implicit curriculum: it enforces precise grounding when the model exhibits high uncertainty (high entropy), while enabling superposition-based reasoning when the model has grasped the global context. 3.3.3 Optimization Objective The total optimization objective unifies the latent reasoning process and the explicit answer generation. For the reasoning chain, the DWAL Loss minimizes the cross-entropy between the hybrid target and the models prediction, effectively aligning the latent trajectory with the dynamic semantic windows: LDWAL ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) (cid:88) t=1 kWt"
        },
        {
            "title": "P target\nt",
            "content": "(k) log Pθ(k I, Q, c<t). (6) Subsequently, for the answer generation phase, the model produces the final response tokens = {aj}M j=1 based on the evolved visual understanding. We adopt the standard Cross-Entropy (CE) loss, conditioning on the image I, the original query Q, and the completed visual chain C: LCE ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) j=1 log Pθ(aj I, Q, C, a<j). The final training objective is weighted summation of these two components: LTotal = LDWAL + LCE. (7) (8) By minimizing LTotal, Laser effectively balances the exploration of global visual semantics during the reasoning phase with the exploitation of precise local semantics for answer generation."
        },
        {
            "title": "4 Experiments",
            "content": "We first demonstrate the superiority of Laser through extensive evaluations on 6 diverse benchmarks against state-of-the-art models. We then conduct further investigations into our method through multi-faceted studies and in-depth analysis. 4.1 Experimental Setup We instantiate Laser using Qwen2.5-VL-7B-Instruct as the backbone. To preserve the pre-trained visual representations and ensure training efficiency, we freeze the vision tower and the modality merger, exclusively optimizing the LLM parameters. Regarding our specific Laser configuration, we set the temperature τ = 1.0 to modulate the softness of the reference distribution, and the entropy threshold η = 0.6 to control the intervention mechanism. Comprehensive hyperparameters are detailed in Appendix A. 4.2 Baselines We evaluate Laser against comprehensive set of state-of-the-art baselines across three paradigms: (1) Zero-shot VLMs (GPT-4o [1], LLaVA-OneVision [2], InternVL3.5-8B [13], Qwen2.5-VL-7B [14]), (2) Explicit Visual Interaction methods, including tool-augmented reasoning (DeepEyes [20]) and RL-enhanced VLM reasoning (Vision-R1 [16], PAPO [37], VL-Rethinker [17], and (3) Latent VLM Reasoning approaches (LVR [4], Monet [31]). Please refer to Appendix for more details. 4.3 Benchmarks We evaluate Laser on six comprehensive benchmarks covering diverse scenarios.For visual perception, we use BLINK [38] to stress image-dependent perception such as depth and spatial cues, and MMVP [39] to probe CLIP-blind visual patterns. For visual reasoning, we adopt MMStar [40], which evaluates fine-grained reasoning axes while minimizing shortcut leakage. To probe highresolution understanding, we include HRBench [41] for ultra-high-resolution (4K only) visual perception. Moreover, we assess trustworthiness and text-rich understanding using HallusionBench [42] to diagnose visual illusion and language hallucinations (measured by Question Accuracy, Q-Acc), and SEED-Bench-2-Plus [43] to evaluate comprehensive understanding of text-intensive visuals such as charts, maps, and web pages. Each benchmark are detailed in Appendix C. 4.4 Results The comparative results across six benchmarks are presented in Table 1. Laser establishes new state-of-the-art among latent reasoning methods and demonstrates superior capabilities even against computationally intensive explicit reasoning paradigms. 6 Model MMVP BLINK SEEDBENCH2PLUS MMStar Hallusion Bench HRBENCH Overall GPT-4o [1] Qwen2.5-VL-7B [44] LLaVA-OneVision [2] InternVL3.5-8B [45] PAPO [37] Vision-R1 [16] VL-Rethinker [17] DeepEyes [20] Monet [31] LVR [46] Laser (Ours) 68.70 65.67 74.00 57.67 68.67 72.67 72.67 70. 68.00 64.00 72.00 4.00 68.00 53.60 49.34 54.81 52.66 52.71 55.55 51.08 50.71 53.60 56.92 6. Zero-Shot VLMs 72.00 65.31 61.22 69.78 64.70 59.70 59.13 53.33 Tool-use & RL Enhanced Reasoning 54.11 68.95 70.27 69.08 Latent Reasoning 65.88 47.39 70. 4.17 45.80 62.67 63.20 58.73 60.33 57.93 60.27 -0.33 56.57 51.10 56.15 57.52 63.83 71.08 62. 56.36 65.19 67.72 11.36 68.25 63.00 59.38 68.12 75.12 63.50 69.12 68.00 53.62 72.50 4. 61.52 59.63 58.52 57.81 65.99 66.05 63.43 61.55 56.96 66.58 5.03 Table 1: Main results comparing Laser with baselines across three paradigms: Zero-Shot VLMs, Tooluse & RL, and Latent Reasoning. The best results among latent reasoning methods are highlighted in bold, and the second best are underlined. denotes the absolute performance gain over the strongest latent baseline, Monet [31]. As our primary focus, Laser significantly outperforms existing latent reasoning baselines. Compared to the previous best method, Monet, Laser achieves remarkable +5.03% gain in the overall score. Notably, we observe the most substantial improvements on HallusionBench (+11.36%) and BLINK (+6.21%). We attribute these gains to the proposed Dynamic Windowed Alignment. By maintaining semantic superposition rather than collapsing to rigid token prematurely, Laser effectively mitigates the hallucination issues common in point-wise latent methods and captures fine-grained visual details. In contrast, LVR, which enforces strict next-token reconstruction, lags significantly behind (-9.62%), highlighting the necessity of our flexible windowed strategy. Beyond the latent domain, Laser compares favorably against heavyweight paradigms. Despite operating purely in the compact latent space without external tools or iterative reinforcement learning search, Laser surpasses both the leading RL-based method, Vision-R1, and the tool-augmented VL-Rethinker. This suggests that optimizing the internal cognitive trajectory via superposition is more efficient path to enhanced reasoning than externalizing the process into lengthy textual chains. Laser consistently outperforms its backbone model, Qwen2.5-VL-7B, across all evaluated benchmarks. Specifically, on the MMVP benchmark, which tests CLIP-blind patterns, Laser improves upon the baseline by +6.33%. This demonstrates that our method is effectively unlocks latent visual discrimination capabilities that are otherwise dormant in standard supervision settings."
        },
        {
            "title": "5 Discussions",
            "content": "In this section, we address five key research questions through systematic investigation, providing deeper insights into Lasers performance and behavior. RQ1: How efficient is Laser compared to baselines? Beyond raw accuracy, the practical deployment of VLMs is often constrained by inference latency and computational cost. We analyze the inference efficiency of Laser compared to both standard baselines and recent latent reasoning methods on the BLINK and HRBench. The results are summarized in Table 2. Existing explicit reasoning approaches, such as VL-Rethinker, typically rely on generating lengthy textual rationales to bridge the gap between perception and reasoning. As shown in Table 2, this imposes severe computational burden. For instance, on HRBench, VL-Rethinker increases token consumption by 157.2% compared to the base model, directly leading to high inference latency. Interestingly, while Monet is designed as latent reasoning framework, it still incurs substantial computational overhead. On BLINK, Monet uses 118.3 tokens on average. Although this is reduction compared to the base model, it remains orders of magnitude heavier than other latent approaches, suggesting that its visual thoughts still occupy dense latent sequence. 7 Model Blink (N =1901) HrBench (N =800) Avg Tokens Avg Tokens Qwen2.5-VL-7B VL-Rethinker Monet-7B LVR Laser 223.5 207.0 118.3 8.0 6.0 -7.4% -47.1% -96.4% -97.3% 55.9 143.8 86.8 8.0 5.7 +157.2% +55.3% -85.7% -89.7% Table 2: Efficiency comparison on Blink and HrBench. Our Laser achieves significant reduction in token usage. The light yellow background indicates Explicit Reasoning methods, while the light blue background represents Latent Reasoning methods. denotes the total number of samples, represents the relative change compared to the Qwen2.5-VL-7B, and indicates efficiency improvement. In contrast, Laser achieves exceptional efficiency by shifting the reasoning process from the discrete token space to the continuous latent space, significantly outperforming both explicit and latent baselines. On the BLINK benchmark, Laser reduces the average token count to merely 6.0 tokens, reduction of 97.3%. This makes Laser substantially more efficient than Monet (118.3 tokens) and even surpasses the comparable latent method LVR (8.0 tokens). Crucially, as noted in Table 1, this efficiency does not compromise performance. While LVR suffers from semantic degradation due to its strict reconstruction objective, Lasers superposition mechanism allows it to encode richer, non-collapsed semantic information within the same compact latent budget. This confirms that Laser achieves superior trade-off between efficiency and accuracy: it delivers the reasoning depth of explicit Chain-of-Thought models while maintaining the inference speed of directanswer models. By condensing reasoning into compact superposition state, Laser eliminates the need for generating hundreds of intermediate tokens, offering near-instantaneous inference suitable for real-time applications. RQ2: What is Lasers impact across variant tasks? Figure 3: Fine-grained comparison across 14 distinct categories. Laser outperforms Qwen2.5-VL-7B and Monet in 11 tasks, highlighting superior high-level semantic and spatial reasoning. We further conducted an in-depth experimental analysis of Laser across range of task domains covering perception, understanding, and reasoning. As shown in Figure 7, Laser exhibits distinct performance profile: it achieves dominant superiority in 11 out of 14 tasks, with the only exceptions being Object Localization, Jigsaw, and Functional Correspondence. This polarization reveals fundamental insight into the \"Forest Before Trees\" cognitive mechanism. Laser excels in tasks demanding high-level discrimination, such as Visual Similarity and Spatial Relation, by avoiding the premature information loss inherent in rigid tokenization. By preserving \"probabilistic haze\" of visual nuances, the model effectively soft-matches ambiguous patterns and grasps relative 3D geometry. This confirms that our Dynamic Semantic Window successfully captures the holistic \"Forest\" structure, prioritizing scene-level contextual coherence over isolated object identification. Conversely, Laser slightly underperforms in Object Localization and Jigsaw. This creates an intriguing contrast: while Laser is superior at understanding spatial relations, it is less precise at absolute pixel-level grounding. We attribute this to our weakly-supervised design choice. Unlike methods that rely on explicit bounding box regression (Region-of-Interest supervision), Laser learns grounding implicitly via latent alignment. It prioritizes the semantic flow of reasoning over rigid pixel reconstruction. Tasks like Jigsaw require exact low-level feature matching rather than abstract semantic reasoning. Lasers Forest-first strategy naturally favors holistic scene understanding, leading the model to abstract away fine-grained pixel details. While this results in minor trade-off in absolute localization precision, it yields the significant robustness observed in complex reasoning scenarios, mimicking human cognition, which is often semantically precise but metrically approximate. RQ3: Does Laser influence general vision-language abilities? Model Multi-View Reasoning Relative Depth Geometry Math Web Chart Qwen2.5-VL-7B Laser (Ours) 51.88 55. 70.16 70.97 53.24 53.24 66.00 67.20 75.45 83.48 61.98 67.16 Table 3: We compare Laser against Qwen2.5-VL-7B across three dimensions: Spatial Perception (Multi-View, Depth, Geometry), Visual Logic (Math), and Structural Understanding (Web, Charts). The results confirm that Laser successfully transfers reasoning patterns to unseen domains while preserving general spatial capabilities Addressing the pervasive risk of catastrophic forgetting, we evaluate whether Laser maintains its general-purpose visual foundation. The results on out-of-distribution domains1confirm that our method avoids performance degradation and effectively preserves general skills. Specifically, Laser achieves substantial gains of 8.03% on Web and 5.18% on Chart tasks while maintaining steady score of 53.24 on Geometry, suggesting that our \"Forest-before-Trees\" hierarchy reinforces the grasp of global structures. Furthermore, the learned deductive patterns successfully transfer to unseen logic domains, evidenced by improvements of 1.20% in Math and 0.81% in Relative Depth. As shown in Table 3, Laser boosts specialized reasoning without compromising the models robust general capabilities. RQ4: Is Lasers reasoning interpretable? Figure 4: Visualization of the latent cognitive trajectory. The decoded tokens reveal structured multi-hop reasoning path, evolving from entity localization (Step 0: Seats) to spatial analysis (Step 1: Fence) and final deduction. 1Task sources: Multi-View and Relative Depth (Blink), Geometry (Geo), Math (MMStar), and Web/Chart (SEED-Bench-2+). pivotal advantage of Laser is its inherent interpretability, derived from the rigorous alignment between the visual projector and the LLMs semantic space. Unlike the opaque continuous vectors found in standard latent reasoning models, Lasers hidden states can be directly projected onto the vocabulary via the frozen LM head. This allows us to inspect the top-k tokens at each intermediate step, effectively visualizing the models cognitive trajectory. We demonstrate this capability using representative case from MMStar, where the model must locate the seated person in baseball scene dominated by salient pitcher in the foreground. Initially (Step 0), the decoded tokens, led by Se- (suggesting Seats), Spect- (Spectators), and Crowd, reveal that the model successfully overcomes visual saliency bias, shifting its attention to the background audience. Subsequently (Step 1), the latent state evolves to encode spatial constraints, with tokens such as Fence, Behind, and Out emerging to define the boundary between the spectators and the field. Finally (Step 2), this spatial reasoning converges into semantic decision, as the probability mass shifts to Outside and the correct option label C. This trajectory confirms that Laser performs explicit-like multi-hop reasoning, progressing from entity localization to spatial analysis and final deduction, entirely within the compact latent space. RQ5: How critical is each component? Figure 5: Ablation study. We contrast the full Laser model with variants lacking the DWAL (w/o DWAL) and the dynamic windows (w/o Windows). The consistent performance gap across six benchmarks highlights the necessity of the proposed Dynamic Windowed Alignment Learning for effective visual reasoning. To investigate the mechanisms behind the performance gains of Laser, we conduct an ablation study isolating the superposition objective and the windowing strategy. As illustrated in Figure 5, the results reveal that these components serve distinct roles across different task types. First, removing the DWAL objective (reverting to standard next-token prediction) leads to significant drop in fine-grained perception benchmarks. This empirically validates that probabilistic superposition is crucial for preventing the premature semantic collapse often observed in standard autoregressive models. Second, using fixed validity window (w/o Windows) primarily impairs performance on complex reasoning tasks, with less impact on pure perception. This confirms that the Dynamic Window strategy is essential for enforcing the Forest-before-Trees hierarchy. By progressively shrinking the semantic scope, it ensures the model captures global context before focusing on local details."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce Laser, method that transcends discrete Chain-of-Thought via continuous latent superposition. By reformulating visual deduction with Dynamic Windowed Alignment Learning, we enforce \"Forest-before-Trees\" hierarchy that prevents premature semantic collapse. Laser achieves state-of-the-art performance among latent reasoning methods with superior robustness and 97% reduction in inference overhead. Notably, it stands as the first interpretable latent reasoning approach, suggesting that emancipating reasoning from rigid tokenization fosters native multimodal intelligence."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2024. [3] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [4] Bangzheng Li et al. Latent visual reasoning. arXiv preprint arXiv:2509.24251, 2025. [5] Georges Zelikman, Eric andovor et al. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024. [6] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, et al. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [7] David Navon. Forest before trees: The precedence of global features in visual perception. Cognitive psychology, 9(3):353383, 1977. [8] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [9] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [10] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [11] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In ICLR, 2024. [12] Zhe Chen, Jiannan Wu, Pipu Wang, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [13] Weiyun Wang, Zhangwei Gao, Zhe Chen, et al. Internvl 3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [14] Shuai Bai, Keqin Chen, Xuejing Liu, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [15] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025. [16] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 11 [17] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vlrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [18] Juntian Zhang, Chuanqi Cheng, Yuhan Liu, Wei Liu, Jian Luan, and Rui Yan. Weaving context across images: Improving vision-language models through focus-centric visual chains. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2778227798, 2025. [19] Juntian Zhang, Song Jin, Chuanqi Cheng, Yuhan Liu, Yankai Lin, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei Lin, et al. Viper: Empowering the self-evolution of visual perception abilities in vision-language model. arXiv preprint arXiv:2510.24285, 2025. [20] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [21] Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-star: Language models can teach themselves to think before speaking. In First Conference on Language Modeling. [22] Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-star: Language models can teach themselves to think before speaking. In First Conference on Language Modeling. [23] Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot: Soft chain-of-thought for efficient reasoning with llms. arXiv preprint arXiv:2502.12134, 2025. [24] Jizheng Ma, Xiaofei Zhou, Yanlong Song, and Han Yan. Cocova: Chain of continuous visionlanguage thought for latent space reasoning. arXiv preprint arXiv:2511.02360, 2025. [25] Tan-Hanh Pham and Chris Ngo. Multimodal chain of continuous thought for latent-space reasoning in vision-language models. arXiv preprint arXiv:2508.12587, 2025. [26] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025. [27] Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, and Liqiang Nie. Reasoning in the dark: Interleaved vision-text reasoning in latent space. arXiv preprint arXiv:2510.12603, 2025. [28] Shuai Dong, Siyuan Wang, Xingyu Liu, and Zhongyu Wei. Interleaved latent visual reasoning with selective perceptual modeling. arXiv preprint arXiv:2512.05665, 2025. [29] Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, et al. Latent sketchpad: Sketching visual thoughts to elicit multimodal reasoning in mllms. arXiv preprint arXiv:2510.24514, 2025. [30] Jintao Tong, Jiaqi Gu, Yujing Lou, Lubin Fan, Yixiong Zou, Yue Wu, Jieping Ye, and Ruixuan Li. Sketch-in-latents: Eliciting unified reasoning in mllms. arXiv preprint arXiv:2512.16584, 2025. [31] Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, and Yisen Wang. Monet: Reasoning in latent visual space beyond images and language. arXiv preprint arXiv:2511.21395, 2025. [32] Guohao Sun, Hang Hua, Jian Wang, Jiebo Luo, Sohail Dianat, MAJID RABBANI, Raghuveer Rao, and Zhiqiang Tao. Latent chain-of-thought for visual reasoning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [33] Chengzhi Liu, Yuzhe Yang, Yue Fan, Qingyue Wei, Sheng Liu, and Xin Eric Wang. Reasoning within the mind: Dynamic multimodal interleaving in latent space. arXiv preprint arXiv:2512.12623, 2025. 12 [34] Arijit Ray, Ahmed Abdelkader, Chengzhi Mao, Bryan Plummer, Kate Saenko, Ranjay Krishna, Leonidas Guibas, and Wen-Sheng Chu. Mull-tokens: Modality-agnostic latent thinking. arXiv preprint arXiv:2512.10941, 2025. [35] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2025. [36] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. [37] Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, et al. Perception-aware policy optimization for multimodal reasoning. arXiv preprint arXiv:2507.06448, 2025. [38] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. [39] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. [40] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large visionlanguage models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. [41] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 79077915, 2025. [42] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [43] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. SEED-Bench-2-Plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. [44] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [45] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [46] Bangzheng Li, Ximeng Sun, Jiang Liu, Ze Wang, Jialian Wu, Xiaodong Yu, Hao Chen, Emad Barsoum, Muhao Chen, and Zicheng Liu. Latent visual reasoning. arXiv preprint arXiv:2509.24251, 2025."
        },
        {
            "title": "Table of Contents",
            "content": "A Implementation Details Baseline Details Benchmark Details RL Analysis . D.1 Optimization Objective . D.2 Exploration Strategy . . . D.3 Composite Reward Engineering . . D.4 Experimental Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Threshold Analysis for an Entropy-Adaptive Mechanism . . E.1 Analysis of Entropy Threshold (η) . E.2 Analysis of Intervention Intensity (α) . . . . . . . . . . . . . . . . Time-Aware Semantic Decay Prompt Engineering Dataset Details Details of Human Annotations Case Study"
        },
        {
            "title": "A Implementation Details",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 15 15 16 16 16 17 18 18 18 18 19 19 20 The model is fine-tuned for 320 steps on 8 MI210 GPUs using 8 gradient accumulation steps. We utilize the AdamW optimizer with ϵ = 1e6 and weight decay of 0.1. The learning rate is initialized at 1e5 with cosine decay scheduler and warmup ratio of 0.03. To optimize memory efficiency, we employ DeepSpeed ZeRO-3 with CPU offloading and enable Flash Attention 2. We adopt dynamic batching strategy with token cap of 8,192 and maximum per-device batch size of 16. For the proposed Laser objective, when the entropy intervention is triggered, the mixing coefficient for the hard target is set to α = 0.8. The input image resolution is dynamic, ranging from 128 to 8,192 tokens (approx. 100K to 6.4M pixels)."
        },
        {
            "title": "B Baseline Details",
            "content": "Vision-R1 [16]. Vision-R1 explores R1-style post-training for MLLMs by constructing large multimodal chain-of-thought cold start dataset and then applying RL with strategies such as progressive thinking suppression to improve multimodal reasoning. PAPO [37]. PAPO (Perception-Aware Policy Optimization) targets the perception bottleneck in multimodal RLVR by introducing an implicit perception loss (KL term) that can be plugged into GRPO/DAPO, improving vision-dependent tasks without extra reward models or teacher models. DeepEyes [20]. DeepEyes studies interleaved multimodal reasoning (thinking with images) and incentivizes tool-assisted visual reasoning behaviors via end-to-end RL, using tailored data selection and reward design to improve grounding and reduce hallucinations. 14 Monet [31]. Monet enables latent visual reasoning by generating continuous visual embeddings as intermediate visual thoughts and proposes multi-stage distillation SFT pipeline plus visual-latent policy optimization to better train reasoning in latent visual space. VL-Rethinker [17]. VL-Rethinker enhances slow-thinking for VLMs via RL by adapting GRPO with selective sample replay and enforcing explicit self-reflection steps, improving multi-step multimodal reasoning performance. LLaVA-OneVision [2]. LLaVA-OneVision is unified open multimodal model designed to perform well across single-image, multi-image, and video scenarios, with strong cross-scenario transfer from images to videos. InternVL3.5-8B [45]. InternVL3.5 is family of open-source multimodal models that improves reasoning and efficiency using cascade RL training and dynamic visual resolution routing; we adopt the 8B variant as representative strong open baseline. Qwen2.5-VL-7B [44]. Qwen2.5-VL is flagship open VLM series with improved recognition, localization, and document/video understanding; it introduces dynamic-resolution processing and absolute time encoding for long-video comprehension. We use the 7B model as the base open VLM baseline. LVR [46]. Latent Visual Reasoning (LVR) enables autoregressive reasoning in the visual embedding space by training the model to generate latent states that reconstruct key visual tokens, interleaved with text generation; it can be further combined with RL to balance latent reasoning and textual outputs."
        },
        {
            "title": "C Benchmark Details",
            "content": "MMStar [40] is vision-indispensable multimodal benchmark with carefully curated and purified samples to reduce language-only shortcuts and data contamination. It contains 1.5K questions and evaluates LVLMs over multiple core capabilities with fine-grained axes, providing balanced diagnosis of both perception and reasoning. MMVP [39] is constructed from 150 CLIP-blind image pairs and probes whether models can truly discriminate basic visual patterns that are obvious to humans but challenging for CLIP-like embeddings. It emphasizes failures where models produce high-confidence yet incorrect answers, often accompanied by plausible-sounding but ungrounded rationales. BLINK [38] is perception-centric benchmark with 3.8K multiple-choice questions, designed to test visual skills that humans can solve within blink, such as correspondence, depth/geometry cues, forensics, and multi-view reasoning. Its tasks are intentionally difficult to solve from language priors alone, thereby stressing image-dependent perception. SEEDBench2Plus [43] targets text-rich visual comprehension in real-world formats (Charts, Maps, Web pages). It uses human-annotated multiple-choice questions to assess whether MLLMs can robustly read and reason over dense visual-text content. Hallusionbench [42] is diagnostic benchmark for entangled language hallucination and visual illusion in LVLMs. It uses human-crafted question structures with control-group design to quantify logical consistency and common hallucination/illusion failure modes. HRBench [41] evaluates fine-grained perception on high-resolution (4K/8K) images, where downsampling typically removes crucial details. It systematically tests HR visual understanding (including fine-grained single-/cross-instance perception) to measure whether models can handle true HR content."
        },
        {
            "title": "D RL Analysis",
            "content": "While our proposed method, Laser, demonstrates robust performance in the supervised fine-tuning stage, we further explore reinforcement learning framework to align training behavior with infer15 ence dynamics and enable an autonomous early exit mechanism. To this end, we introduce the EPG-GRPO (Expected Policy Gradient - Group Relative Policy Optimization) algorithm. This framework integrates variance-reduced gradient estimator with length-invariant policy optimization scheme, successfully guiding the model to significantly reduce token consumption while maintaining performance parity. D.1 Optimization Objective Our objective combines Expected Policy Gradients (EPG) to stabilize learning within the highvariance latent space and modified GRPO formulation to eliminate length bias. Within LVR regions, standard single-token sampling often leads to gradient instability due to semantic ambiguity. To mitigate this, we calculate the gradient over the expectation of the Top-P subspace (STopP) rather than single sampled token. We define the importance ratio ρw for each token STopP and compute the expected surrogate loss L(t) πθ(wst) πold(wst) + ϵ epg as follows: ρw = (9) , epg = EwSTopP [min (ρwAt, clip(ρw, 1 ϵ, 1 + ϵ)At)] , L(t) (10) where At is the advantage derived from group sampling. For global optimization, standard GRPO normalizes by sequence length, which inadvertently incentivizes verbosity. We address this by standardizing the global loss using fixed maximum completion length Lmax, independent of the actual trajectory length Ti: Lpolicy = 1 Lmax (cid:88) Ti(cid:88) L(i,t) token + β KL(πrefπθ). (11) t=1 Here, Ltoken utilizes the derived Lepg for LVR steps and the standard clipped surrogate loss for explicit tokens. i=1 D.2 Exploration Strategy We employ composite exploration strategy to prevent convergence to local optima and robustly learn termination conditions. First, we implement Relative Norm Perturbation to enhance the diversity of deterministic LVR hidden states. We inject Gaussian noise scaled by the signals norm during the forward pass. For hidden state h, the perturbed state is given by: = + λ ϵ ϵ, ϵ (0, I), (12) where we set the noise ratio λ = 0.05. This ensures the perturbation is significant yet non-destructive across varying signal magnitudes. Second, to encourage efficiency, we apply Stochastic Horizon Truncation. For subset of the generated samples, the maximum allowed steps Tmax are randomly sampled from range [Tmin, Tupper]. This forces the model to attempt convergence within limited horizons, thereby learning to optimize the efficiency of its reasoning path without relying on fixedlength priors. D.3 Composite Reward Engineering The reward function Rtotal acts as multi-objective optimization signal, aggregating components to balance accuracy, structural validity, efficiency, and diversity: Rtotal = Racc + Rfmt + Reff + Rdiv. (13) Accuracy Reward (Racc). Since our evaluation relies principally on multiple-choice questions or exact answer matching, we assign binary reward based on the correctness of the final output: Racc = (cid:26)1 0 if answer is correct, otherwise. (14) 16 Format Reward (Rfmt). This component enforces the structural integrity of the dynamic protocol. reward rfmt is granted solely if the sequence includes the requisite start token, successfully triggers the autonomous termination token <laser_end>, and correctly encloses the final result in answer tags. Efficiency Bonus (Ref ). To incentivize the model to voluntarily \"exit early\" when confident, we introduce dynamic efficiency bonus. This reward is conditional on three strict constraints: the answer must be correct, the trajectory must not be forcibly truncated by the system limit (Tmax), and the output must be free of format anomalies. Ref = (cid:26)βbase λstep Tactual 0 if Correct Tactual < Tmax, otherwise. (15) Here, βbase represents the maximum potential bonus, and λstep is penalty coefficient that reduces the reward linearly with each reasoning step used. Under this formulation, correct answer obtained via system truncation yields zero bonus, explicitly encouraging the model to learn autonomous termination logic. Diversity Penalty (Rdiv). To prevent \"state stagnation\"where adjacent reasoning steps exhibit excessive semantic redundancywe apply penalty based on the squared cosine similarity between consecutive hidden states ht and ht1. Rdiv = λdiv 1 1 1 (cid:88) t= (sim(ht, ht1))2 . (16) We utilize the squared similarity to impose non-linear penalty that aggressively punishes highsimilarity states while remaining tolerant of the minor correlations necessary for coherent reasoning flow. The term is weighted by λdiv and normalized by trajectory length to ensure consistent scaling across varying reasoning depths. D.4 Experimental Analysis To validate the efficacy of the EPG-GRPO framework, we compare the supervised baseline (Laser) with its RL-enhanced variant (Laser + EPG). As shown in Table 4, the results demonstrate that our strategy successfully balances computational efficiency with reasoning capability. The most significant impact is observed in inference efficiency. The RL-enhanced model reduces the average number of generated tokens by approximately 50% across dynamic benchmarks such as BLINK and HRBench. This substantial decrease confirms that the model effectively internalized the autonomous early exit mechanism incentivized by the efficiency bonus Ref and stochastic horizon truncation, learning that concise reasoning paths are often sufficient. Crucially, this efficiency gain is achieved without compromising general performance. The overall accuracy remains stable, with the method showing particular robustness on hallucinations (HallusionBench) and multi-modal reasoning (MMStar). While minor regressions were observed in select sensitivity-heavy tasks, the Subspace-EPG objective successfully preserved the semantic richness of the latent space, preventing the catastrophic forgetting or mode collapse often associated with RL fine-tuning. Model Avg. Tokens Accuracy (%) BLINK HRBench MMStar MMVP BLINK SEED Hallusion HRBench Overall 6.03 3. Laser (Main) Laser + EPG Table 4: Comparison between the main model (Laser) and the RL-enhanced model (Laser + EPG). Avg. Tokens denotes the average number of generated tokens on BLINK and HRBench. 72.00 72.00 66.58 66.73 70.05 70.79 67.72 68.98 60.27 60. 72.50 72.00 56.92 55.76 5.74 2.87 17 Threshold (η) Trigger MMStar MMVP BLINK SEED Hallusion HRBench Overall η = 1.0 η = 0.8 η = 0.6 (Ours) η = 0. 0.0% 2.5% 10.0% 18.0% 59.93 60.33 60.27 57.40 69.00 69.67 72.00 71.67 56.86 56.08 56.92 54.97 69.52 69.96 70.05 68.95 67.93 68.24 67.72 64. 71.88 71.75 72.50 72.75 65.85 66.01 66.58 65.05 Table 5: Ablation study on the entropy threshold η. Trigger denotes the intervention activation ratio. η = 0.6 yields the best balance and is used as our default setting. Threshold Analysis for an Entropy-Adaptive Mechanism E.1 Analysis of Entropy Threshold (η) We first analyze the impact of the entropy threshold η by correlating it with the Trigger Ratio, defined as the percentage of tokens where the models high uncertainty necessitates hard teacher intervention. As shown in Table 5, optimal performance is achieved at η = 0.6, corresponding to trigger ratio of approximately 10%. This suggests that intervening on roughly one in ten tokens provides sufficient grounding signals to correct the reasoning trajectory without disrupting the semantic flow. When the threshold is lowered to η = 0.5, the trigger ratio rises to 18%, imposing stricter constraint akin to standard supervision. While this rigid guidance benefits tasks requiring precise alignment (yielding the highest score on HRBench), it stifles the latent exploration necessary for complex logic, resulting in performance degradation on reasoning-heavy benchmarks like MMStar (57.40) and HallusionBench (64.56). Conversely, higher thresholds (η = 0.8, 1.0) lead to negligible intervention (< 2.5%). Although this preserves flexibility, it lacks the necessary corrective mechanism to fix visual grounding errors, leading to suboptimal results in fine-grained perception tasks such as MMVP. Thus, η = 0.6 strikes an effective balance, enforcing visual validity while maintaining cognitive flexibility. Threshold (α) MMStar MMVP BLINK SEED Hallusion HRBench Overall α = 0.2 α = 0.5 α = 0.8 (Ours) 60.00 59.87 60.27 70.00 73.00 72.00 55.50 55.65 56. 69.65 69.83 70.05 68.66 68.24 67.72 72.62 72.38 72.50 66.07 66.50 66.58 Table 6: Ablation study on the parameter α. α [0, 1] controls the intensity of the hard intervention. We observe that α = 0.8 achieves the optimal balance. E.2 Analysis of Intervention Intensity (α) We further examine the impact of the parameter α, which modulates the intensity of the hard intervention once high uncertainty is detected. As indicated in Table 6, increasing α leads to consistent, albeit marginal, improvement in overall performance. We observe that lower α (e.g., 0.2) applies softer correction, which appears insufficient to fully resolve ambiguity when the model is highly uncertain. Conversely, higher setting (α = 0.8) provides more decisive guidance signal, effectively acting as hard reset to realign the latent trajectory with the ground truth. This suggests that during critical states of high uncertainty, prioritizing deterministic constraints over soft superposition yields slightly more robust reasoning. Time-Aware Semantic Decay We further explore Time-Aware Semantic Decay component to regulate the semantic distribution within the validity window. This strategy applies temporal bias to the target logits based on their relative distance from the current step: = ˆz(k) z(k) + ln(γkt), (17) 18 where Wt denotes the token index and γ (0, 1] acts as decay factor. This formulation allows the framework to flexibly modulate the attention density assigned to distant future tokens relative to immediate deductive steps. Model MMStar MMVP BLINK SEED Hallusion HRBench Overall Larse (Default) w/ Time Decay 60.27 59. 72.00 73.00 56.92 55.34 70.05 70.18 67.72 67.40 72.50 72.38 66.58 66. Table 7: Ablation study comparing the standard Laser model with time-aware variant."
        },
        {
            "title": "G Prompt Engineering",
            "content": "To ensure the synthesis of high-quality cognitive scanpaths, we employed the following structured system prompt. It enforces strict Global-to-Local scanning logic and output format constraints. ### Role Definition You are Visual Cognitive Engine designed to deconstruct the visual reasoning process into strictly ordered \"Visual Scanpath\". ### Core Objective Generate sequential stream of Atomic Visual Concepts. This stream must represent logical flow of discovery: scanning from the global environment, zooming into specific objects, and accumulating visual cues, culminating in the most critical information needed to answer the users query. ### Precision & Format Principles (CRITICAL) 1. Atomic Specificity: Be as specific as the image clarity allows immediately (e.g., \"Ferrari\" not \"Car\"), but strictly use single words or 1-3 word phrases. 2. De-Grammatized Output: Output dense information only. REMOVE all stop words (is, the, a, of, in). 3. Visual Certainty: Only output concepts that are visually observable. Use broader terms if blurry. 4. Contextual Anchoring: Repeat previous entity ONLY if necessary to attach the final resolution. ### The 4-Stage Scanning Logic (Strict Order) 1. Global Anchor (Step 1-2): Start with the broadest visible context (e.g., \"Kitchen\", \"Blue Sky\"). 2. Subject Localization (Step 3-X): Locate the main subject relevant to the question. 3. Visual Evidence (Step X-Y): List visible attributes or actions supporting the answer. 4. Critical Resolution (Final Step): The specific concept answering the query must appear at the very end. ### Negative Constraints - NO Premature Reveals: Do not output the answer early. - NO Artificial Hierarchy: No \"Fruit\" \"Apple\", just \"Apple\". - NO Sentences: Raw concepts only. ### Output Format Output strictly valid JSON: { \"reasoning_chain\": [ \"String1\", \"String2\", ... ] }"
        },
        {
            "title": "H Dataset Details",
            "content": "Table 8 outlines the detailed statistics of the ScanPath dataset, comprising 269,773 samples across six visual domains. We measure chain complexity using Reasoning Nodesdefined as discrete semantic anchors (e.g., region identification or attribute verification) rather than linguistic token length. As shown in the table, the node distribution naturally aligns with task difficulty: fine-grained tasks like CUB-200 require deeper reasoning paths (Mean: 8.55 nodes) compared to basic detection tasks like OpenImages (Mean: 6.38 nodes), resulting in balanced overall average of 7.14 nodes."
        },
        {
            "title": "I Details of Human Annotations",
            "content": "To assess the quality of our dataset, we recruited three expert annotators (Ph.D. candidates in Computer Science) to manually evaluate 200 randomly sampled instances from ScanPath. The evaluation focused on two key dimensions: (1) the validity of the visual reasoning chain, and (2) adherence to the global-to-local logic. The results demonstrated validity rate of 91.5% with 19 Source Task Domain Data Scale Reasoning Nodes Count Ratio Min Max Mean Captioning VQA/Reasoning Flickr30k GQA OpenImages Detection Visual7W CUB VSR VQA Fine-grained Cls. Spatial Reasoning 103,790 86,218 42,639 30,271 3,521 3, 38.5% 32.0% 15.8% 11.2% 1.3% 1.2% 2 2 3 1 4 2 Total / Avg. - 269,773 100% 20 18 17 17 18 16 20 7.50 7.13 6.38 6.85 8.55 7.08 7.14 Table 8: Detailed statistics of the ScanPath dataset. The dataset integrates six diverse visual tasks. Nodes refers to the number of discrete semantic anchors in the reasoning chain (distinct from token length). substantial inter-annotator agreement (Fleiss κ = 0.677), confirming the reliability of our automated pipeline and filtering protocol."
        },
        {
            "title": "J Case Study",
            "content": "We present test cases of our method during inference, as illustrated in Figure xx. These cases clearly demonstrate the efficiency and accuracy of our method during the reasoning process. Figure 6: test case from MMStar showcases the efficacy and efficiency of our Laser. 20 Figure 7: This multi-image reasoning test case from MMStar illustrates the effectiveness and efficiency of our Laser."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "Harvard University",
        "MBZUAI"
    ]
}