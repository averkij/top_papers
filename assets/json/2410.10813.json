{
    "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
    "authors": [
        "Di Wu",
        "Hongwei Wang",
        "Wenhao Yu",
        "Yuwei Zhang",
        "Kai-Wei Chang",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introduces LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into four design choices across the indexing, retrieval, and reading stages. Built upon key experimental insights, we propose several memory designs including session decomposition for optimizing value granularity, fact-augmented key expansion for enhancing the index structure, and time-aware query expansion for refining the search scope. Experiment results show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 1 ] . [ 1 3 1 8 0 1 . 0 1 4 2 : r LONGMEMEVAL: BENCHMARKING CHAT ASSISTANTS ON LONG-TERM INTERACTIVE MEMORY Di Wu1, Hongwei Wang2, Wenhao Yu2, Yuwei Zhang3*, Kai-Wei Chang1, Dong Yu2 1UCLA, 2Tencent AI Lab Seattle, 3UC San Diego {diwu,kwchang}@cs.ucla.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introduces LONGMEMEVAL, comprehensive benchmark designed to evaluate five core longinformation extraction, multi-session term memory abilities of chat assistants: reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LONGMEMEVAL presents significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing 30% accuracy drop on memorizing information across sustained interactions. We then present unified framework that breaks down the long-term memory design into four design choices across the indexing, retrieval, and reading stages. Built upon key experimental insights, we propose several memory designs including session decomposition for optimizing value granularity, fact-augmented key expansion for enhancing the index structure, and time-aware query expansion for refining the search scope. Experiment results show that these optimizations greatly improve both memory recall and downstream question answering on LONGMEMEVAL. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI. To facilitate future research, we publicly release the benchmark and our experiment code at https://github.com/xiaowu0162/LongMemEval."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have exhibited impressive capabilities in solving diverse tasks through natural language, leading to numerous successful chat assistant applications (OpenAI, 2022; Microsoft, 2023). Nevertheless, LLMs face limitations on tasks relying heavily on personal knowledge accumulated through long-horizon user-AI interactions, such as psychological counseling or secretarial duties (Zhong et al., 2024). Failing to incorporate user background and preferences into responses can diminish the responses accuracy as well as user satisfaction. To personalize LLM-based assistants, long-term memory, the ability to memorize, recall, and reason with the longterm interaction history, is indispensable. Recently, several commercial (OpenAI, 2024; Coze, 2024) and open-sourced memory-augmented assistant systems (Zhong et al., 2024; Zhang et al., 2024) have been introduced. These systems leverage techniques like compressing, indexing, and retrieving from chat histories to generate more accurate and personalized responses. Despite these advances in memory-augmented chat systems, there has been limited progress in holistically evaluating their memory capability in long-term interactions. While several benchmarks evaluate LLMs on understanding long chat histories (Xu et al., 2022a;b; Zhong et al., 2024; Maharana et al., 2024; Du et al., 2024; Kim et al., 2024), they have two major shortcomings. First, they do not accurately reflect the nature of long-term user-AI interactions. Many datasets focus solely on human-human conversations (Xu et al., 2022a; Maharana et al., 2024; Kim et al., 2024), work done during internship at Tencent AI Lab, mentored by Hongwei and Wenhao. 1 while others omit task-oriented dialogues, which represent significant portion of chat assistant usage. Their interactive histories also typically have short and non-configurable length, spanning only few thousand tokens, limiting the challenge as current systems continue to improve. Second, current benchmarks questions only offer limited coverage of the memory abilities required to leverage the dynamic, ever-changing, and accumulative user information in long-term interactions. For instance, MemoryBank (Zhong et al., 2024) and PerLTQA (Du et al., 2024) under-evaluate the ability to synthesize information across numerous sessions or reasoning with temporal metadata or time references. All benchmarks including recent ones such as LoCoMo (Maharana et al., 2024) also fail to evaluate recall of assistant-side information or reasoning with updated user information. We introduce LONGMEMEVAL, comprehensive, challenging, and scalable benchmark designed to assess the long-term memory capabilities of chat assistants. LONGMEMEVAL consists of 500 human-curated, high-quality questions to test five core memory abilities: information extraction, cross-session reasoning, temporal reasoning, knowledge updates, and abstention. Each question requires recalling information hidden within one or more multi-turn task-oriented user-AI dialogues that are LLM-simulated and human-edited. Inspired by the needle-in-a-haystack test (Kamradt, 2023), we design an attribute-controlled pipeline to compile coherent, extensible, and timestamped chat history for each question. chat system, then, is required to parse the dynamic interactions online for memorization, and answer the question after all the interaction sessions. While the length of the history is freely configurable, we provide two standard settings for consistent comparison: LONGMEMEVALS with approximately 115k tokens per problems history and LONGMEMEVALM with 500 sessions (around 1.5 million tokens). Preliminary evaluations highlight the difficulty of LONGMEMEVAL, as long-context LLMs show 30%60% performance drop on LONGMEMEVALS, and manual evaluations reveal that state-of-the-art commercial systems (such as GPT4o) only achieve 30%70% accuracy in setting much simpler than LONGMEMEVALS (3.5). Finally, we present unified view for memory-augmented chat assistants. Leveraging LONGMEMEVAL, we comprehensively analyze memory design choices across three key execution stagesindexing, retrieval, and readingand four control points: value, key, query, and reading strategy. Experimental insights identify several effective memory designs: (5.2) Instead of sessions, round is the best granularity for storing and utilizing the interactive history. While further compression into individual user facts harms overall performance due to information loss, it improves the multi-session reasoning performance. (5.3) While using flat index with the memory values themselves as the keys is strong baseline, expanding the keys with extracted user facts greatly facilitates both memory recall (4% higher recall@k) and downstream question answering (5% higher accuracy). (5.4) Simplistic memory designs perform poorly on temporal reasoning questions. We propose simple time-aware indexing and query expansion strategy to narrow down the search range, which improves the memory recall for the temporal reasoning by 7%11%. (5.5) Even with perfect memory recall, accurately reading the retrieved items is still nontrivial. Applying Chain-of-Note (Yu et al., 2023) and structured prompt format (Yin et al., 2023) improves the reading accuracy by as much as 10 absolute points across three LLMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Long-Term Dialogue Benchmarks Early benchmarks focused on language modeling evaluation, requiring models to generate personalized responses by referencing human-human (Xu et al., 2022a) or human-AI (Xu et al., 2022b) conversation histories. To more precisely evaluate how accurately models recall past information, subsequent benchmarks shifted toward question answering (QA). For example, MemoryBank (Zhong et al., 2024) features multi-day chat histories from 15 users with 194 human-written probing questions. LoCoMo (Maharana et al., 2024) includes 50 long-term chat histories and questions testing single-hop, multi-hop, temporal, commonsense, world knowledge, and adversarial reasoning. LoCoMo also introduces two new tasks event graph summarization and multimodal dialogue generation. PerLTQA (Du et al., 2024) scales the evaluation to 3,409 dialogues and 8,593 questions, covering world knowledge, personal profiles, social relationships, events, and dialogue history. DialSim (Kim et al., 2024) evaluates models memory ability by role-playing TV show characters and introduces time constraint that penalizes slow system responses. Despite 2 Benchmark Domain #Sess #Q Context Depth MSC (Xu et al., 2022a) DuLeMon (Xu et al., 2022b) MemoryBank (Zhong et al., 2024) PerLTQA (Du et al., 2024) LoCoMo (Maharana et al., 2024) DialSim (Kim et al., 2024) LONGMEMEVAL (this work) Open-Domain Open-Domain Personal Personal Personal TV Shows Personal 5k 30k 300 4k 1k 1k2k 50k - - 194 8593 7512 1M 500 1k 1k 5k 1M 10k 350k 115k, 1.5M Core Memory Abilities IE MR KU TR ABS Table 1: comparison between LONGMEMEVAL and existing long-term memory benchmarks. We use different colors to denote human-human dialogue and human-AI dialogue. #Sess and #Q denote the total number of sessions and questions. Context depth is calculated as the number of tokens in the history. Finally, we the coverage of five core abilities: information extraction (IE), multi-session reasoning (MR), knowledge update (KU), temporal reasoning (TR), and abstaining on unanswerable questions (ABS). Not reported in the paper, based on our approximation. at most 2 sessions. these advancements, existing QA-based benchmarks overlook several memory capabilities critical to long-term user-assistant interactions: synthesizing information across numerous sessions, recalling assistant-side information, and reasoning about updated user details or complex temporal references. Additionally, the chat histories are often too brief and do not reflect the nature of task-oriented userassistant interactions. Table 1 compares between LONGMEMEVAL and previous works, highlighting its advantages in both (1) featuring long and freely extensible iterative history and (2) holistically covering critical memory abilities in uniquely challenging way (further examples in Figure 1). Long-Term Memory Methods To equip chat assistants with long-term memory capabilities, three major techniques are commonly explored. The first approach involves directly adapting LLMs to process extensive history information as long-context inputs (Beltagy et al., 2020; Kitaev et al., 2020; Fu et al., 2024; An et al., 2024). While this method avoids the need for complex architectures, it is inefficient and susceptible to the lost-in-the-middle phenomenon, where the ability of LLMs to utilize contextual information weakens as the input length grows (Shi et al., 2023; Liu et al., 2024). second line of research integrates differentiable memory modules into language models, proposing specialized architectural designs and training strategies to enhance memory capabilities (Weston et al., 2014; Wu et al., 2022; Zhong et al., 2022; Wang et al., 2024). Lastly, several studies approach long-term memory from the perspective of context compression, developing techniques to condense lengthy histories into compact representations, whether in the form of LLM internal representations (Mu et al., 2023; Chevalier et al., 2023), discrete tokens (Jiang et al., 2023; Xu et al., 2024), or retrievable text segments via retrieval-augmented generation (Shi et al., 2024a; Wang et al., 2023; Sarthi et al., 2024; Chen et al., 2023a; Gutierrez et al., 2024). Although LONGMEMEVAL can evaluate all types of memory systems, we will take an online context compression perspective, where each history interaction session is sequentially processed, stored, and accessed on-demand through indexing and retrieval mechanisms (4). This formulation aligns with current literature (Zhong et al., 2024; Gutierrez et al., 2024) and commercial memory systems (OpenAI, 2024; Coze, 2024). Its plug-and-play nature also facilitates the integration into other existing chat assistant systems."
        },
        {
            "title": "3 LONGMEMEVAL",
            "content": "3.1 PROBLEM FORMULATION The evaluation of LONGMEMEVAL requires an instance of 4-tuple (S, q, tq, a). is sequence of history chat sessions ordered from the [(t1, S1), (t2, S2), ..., (tN , SN )] earliest to the latest, where Si is multi-turn interaction between the user and chat assistant and ti is the date and time of the session. During test time, the tested system observes sequentially. and tq > tN represent the question from the user and its date. is short phrase indicating the answer, or natural language rubric describing the preferred answer in the case where is open-ended. 3 Figure 1: Examples of the seven diverse question types in LONGMEMEVAL. For each example, we show the associated evidence statements on the left and the question with the answer on the right. 3.2 LONGMEMEVAL: BENCHMARK CURATION We argue that the main challenge in building reliable personalized assistant is performing online recording, recalling, updating, and reasoning on the dynamically evolving user information. To faithfully reflect the challenge, LONGMEMEVAL formulates five core long-term memory abilities: Information Extraction (IE): Ability to recall specific information from extensive interactive histories, including the details mentioned by either the user or the assistant. Multi-Session Reasoning (MR): Ability to synthesize the information across multiple history sessions to answer complex questions that involve aggregation and comparison. Knowledge Updates (KU): Ability to recognize the changes in the users personal information and update the knowledge of the user dynamically over time. Temporal Reasoning (TR): Awareness of the temporal aspects of user information, including both explicit time mentions and timestamp metadata in the interactions. Abstention (ABS): Ability to refrain from answering questions that involve unknown information, i.e., information not mentioned in the interaction history. As shown in Table 1, LONGMEMEVAL represents more comprehensive ability coverage compared to prior long-term memory benchmarks like MemoryBank and PerLTQA. To thoroughly assess these abilities, LONGMEMEVAL features seven question types. Single-session-user and single-sessionassistant test memorizing the information mentioned by user or assistant within single session. Single-session-preference tests whether the model can utilize the user information to generate personalized response. The other types of questions are multi-session (MR), knowledge-update (KU), and temporal-reasoning (TR). Finally, we draw 30 questions from the previous question types and modify them false premise questions, testing whether the model can correctly abstain from answering (ABS). Figure 1 presents an example for each question type. Question Curation Pipeline We design an attribute-controlled pipeline for question curation, as depicted in Figure 2. We first define an ontology of 164 user attributes in five categories: lifestyle, 4 Figure 2: Data creation pipeline of LONGMEMEVAL. The question construction process is performed by human experts (a), and the evidence sessions are LLM-simulated and human-edited (b). The history construction process (c) is performed at test time and is freely configurable. belongings, life events, situations context, and demographic information. For each attribute, we leverage an LLM1 to generate attribute-focused user background paragraphs, each of which includes detailed discussion of the users life experience. To create question, we randomly sample paragraph and use an LLM to propose several seed (question, answer) pairs. However, we observe that LLM-proposed seed questions often lack depth and diversity. We thus manually rewrite all the questions to achieve the desired difficulty and diversity. Then, based the background, we manually decompose the answer into one or more evidence statements with optional timestamps. Each evidence statement is then separately embedded into task-oriented evidence session created by self-chatting (Xu et al., 2023). The user LLM is instructed to convey the evidence statement indirectly. For instance, to mention the user bought new car last month, the user LLM may begin with questions regarding car insurance, and reveal the fact incidentally within the dialogue. This approach enhances the benchmarks difficulty by requiring systems to recognize and memorize key details not directly emphasized in conversations. We present the full details in Appendix A.1. To ensure high-quality test data, all the evidence sessions are manually adjusted to (1) verify evidence inclusion, (2) distribute evidence across different conversation positions, and (3) rephrase statements into more natural, colloquial language, especially for time mentions, which LLMs often express too formally. We also meticulously annotate the position of the evidence statement within each evidence session. For questions involving temporal information, we manually add timestamps to both the sessions and the questions. Figure 3 presents the basic statistics of LONGMEMEVAL, revealing that most questions require evidence from multiple sessions (up to six) and that evidence statements are positioned diversely within sessions, increasing the challenge to the memory design. 3.3 HISTORY COMPILATION For each question, LONGMEMEVAL compiles coherent user-AI chat history that contains the associated evidence sessions (Figure 2c). Specifically, we sample number of unrelated user-AI chat sessions, randomly insert the evidence sessions in the middle, and assign plausible timestamp to each session. We draw the irrelevant sessions from two sources: (1) self-chat sessions simulated using facts extracted from the user backgrounds with non-conflicting attributes and (2) publicly released user-AI style chat data including ShareGPT (Zheng et al., 2023) and UltraChat (Ding et al., 2023). This design allows us to simulate realistic and extensible user-AI chat histories with minimal conflicts. We document the full details of history session sampling and timestamp specification in Appendix A.2. Although our pipeline is able to output histories of arbitrary length, we provide 1Unless otherwise mentioned, Llama 3 70B Instruct (Dubey et al., 2024) is used as the LLM in the pipeline. (a) Distribution of question types in LONGMEMEVAL. (b) Distribution of the number of evidence sessions. Most questions emphasize multi-session reasoning, requiring reading up to six sessions to answer. (c) Distribution of the location of the evidence statement within the evidence sessions. Most evidence statements are located at the beginning of the chat. Figure 3: LONGMEMEVAL challenges chat assistants through its diverse question types (a), emphases on multi-session reasoning (b), and diverse evidence locations within sessions (c). two standard benchmark settings for consistent comparison: LONGMEMEVALS with approximately 115k tokens per question and LONGMEMEVALM with 500 sessions (around 1.5 million tokens). It is worth noting that LONGMEMEVALs testing strategy is inspired by the needle-in-a-haystack test for long-context understanding (Kamradt, 2023), which asks model to retrieve certain short information (the needle) embedded in long document (the haystack). However, LONGMEMEVAL presents more challenging and realistic scenario, as it involves identifying and synthesizing information from multiple extended evidence sessions. Additionally, the needles in LONGMEMEVAL are contextually integrated within the chat history, creating coherent haystack. 3.4 EVALUATION METRIC Question Answering As the correct answers in LONGMEMEVAL can take flexible forms, using an exact matching strategy as in previous works can result in high error rates. To address this, we employ prompt-engineered large language model to assess response quality (Liu et al., 2023). Specifically, we use the gpt-4o-2024-08-06 model via the OpenAI API. Our meta-evaluation study demonstrates that the evaluator achieves more than 97% agreement with human experts. We include the human meta-evaluation details and the evaluation prompt in Appendix A.3. Memory Recall As LONGMEMEVAL contains human-annotated answer location labels, intermediate retrieval metrics can be reported if the chat system exposes its retrieval results. In this paper, we report Recall@k and NDCG@k, where is the number of top items retrieved by the system. 3.5 LONGMEMEVAL REPRESENTS SIGNIFICANT CHALLENGE Using LONGMEMEVAL, we conduct pilot study on commercial systems and long-context LLMs. Commercial systems We evaluate two commercial systems that maintain set of memorized user facts as the user chats with the assistant: ChatGPT (OpenAI, 2024) and Coze (Coze, 2024). Since these systems only support memory features via their web interfaces, we randomly selected 97 questions and created short chat history of 3-6 sessions (approximately 10x shorter than LONGMEMEVALS). Human annotators interacted with the chat assistants session-by-session, ending with new session where the question was posed2. In Figure 4a, we compare this online memory evaluation with offline reading, where GPT-4o is prompted to answer with the complete history provided as context. Both ChatGPT and Coze exhibited significant performance drops compared to offline reading, underscoring the challenging nature of LONGMEMEVAL and highlighting the substantial gap between recalling isolated user facts and demonstrating comprehensive memory ability. We find ChatGPT tends to overwrite crucial information as the number of interaction sessions increases, while Coze often fails to record user information presented indirectly. We provide further human study details and analyses in Appendix B. 2All evaluations were conducted in the first two weeks of August 2024. 6 System LLM Accuracy Offline Reading GPT-4o ChatGPT Coze GPT-4o GPT-4o-mini GPT-4o GPT-3.5-turbo 0. 0.5773 0.7113 0.3299 0.2474 (a) Commercial memory-augmented LLMs exhibit weak performance on LONGMEMEVAL. The accuracy on 97 questions degrades by large amount compared to directly reading the context (Offline Reading). Specifically, ChatGPT and Coze instantiated with GPT-4o exhibits 37% and 64% performance drop, respectively. Model Oracle % Drop No Chain-of-Note GPT-4o Llama 3.1 8B Instruct Phi-3 Medium 128k Instruct Llama 3.1 8B Instruct 0.870 0.744 0.702 0.710 With Chain-of-Note GPT-4o Llama 3.1 8B Instruct Phi-3 Medium 128k Instruct Llama 3.1 8B Instruct 0.924 0.848 0.722 0.710 0.606 0.334 0.380 0.454 0.640 0.286 0.344 0.420 30.3% 55.1% 45.9% 36.1% 30.7% 66.3% 52.4% 40.8% (b) Long-context LLMs exhibit large performance drops on LONGMEMEVALS (column S), compared to answering the questions based on only the evidence sessions (column Oracle). Figure 4: Pilot study of commercial systems (a) and long-context LLMs (b) on LONGMEMEVAL. Long-Context LLMs While LONGMEMEVAL poses significant challenge to online memory systems, is the benchmark easily tackled with offline reading over the entire history? In Figure 4b, we evaluated four advanced long-context LLMs on LONGMEMEVALS (with history length of approximately 115k tokens): GPT-4o, Llama 3.1 Instruct (Dubey et al., 2024), and Phi-3-Medium (Abdin et al., 2024). Compared to the oracle retrieval setting (answering with only the evidence sessions as the context), these LLMs showed 30% to 60% performance decline when tasked with reading the entire LONGMEMEVALS history, regardless of whether the chain-of-note technique (Yu et al., 2023) was applied (discussed further in 4.2). Its important to note that the histories in LONGMEMEVALS is still short (50 sessions), and the performance is likely to further degrade as the interaction history expands. In summary, even the most capable long-context LLMs currently require an effective memory mechanism to manage an ever-growing interaction history."
        },
        {
            "title": "4 A UNIFIED VIEW OF LONG-TERM MEMORY ASSISTANTS",
            "content": "In this section, we formulate three-stage long-term memory model for chat assistants. Despite its simplicity, this model provides unified view of existing long-term memory assistant works and enables us to investigate crucial control points for each stages design. 4.1 LONG-TERM MEMORY SYSTEM: FORMULATION We formulate long-term memory as massive key-value datastore [(k1, v1), (k2, v2), ...], where the keys ki can be heterogeneous and the values vi might repeat. As shown in Figure 5, we formulate three stages for memory-augmented assistant: (1) indexing, converting each history session (ti, Si) into one or more key-value items, (2) retrieval, formulating retrieval query and collecting most relevant items, and (3) reading, an LLM reads the retrieval result and generates response. In Appendix C, we further revisit nine memory-augmented chat assistant systems with this framework. 4.2 LONG-TERM MEMORY SYSTEM: DESIGN CHOICES Based on this formulation, we identify four crucial control points for long-term memory of chat assistants (illustrated in Figure 5). We analyze design choices from existing works and their limitations, and propose our optimizations. Due to space constraints, we present high-level descriptions here, with detailed designs in 5 and Appendix D. CP 1: Value The value represents the format and granularity of each session stored in memory. Given that user-AI chat sessions are often lengthy and cover multiple topics, storing each session as single item can hinder effective retrieval and reading. Conversely, compressing sessions into summaries or user-specific facts (as seen in prior work such as Zhong et al. (2024) and Du et al. (2024)) can lead to information loss, harming the performance of the system to answer detailed 7 Figure 5: unified view of chat assistant with long-term memory in operation. We formulate three stages and four control points (CP). We provide further examples in Appendix C. questions. decomposing sessions into individual rounds, and further applying summary/fact extraction. In 5.2, we compare three value representation strategies: storing entire sessions, CP 2: Key Even when sessions are decomposed and compressed, each item still contains substantial information, with only fraction relevant to the users query. Therefore, using the value itself as the key, common practice in prior works (Zhong et al., 2024; Maharana et al., 2024), may be suboptimal. We introduce key expansion approach in 5.3, where summaries, keyphrases, user facts, and timestamped events are extracted from the values to augment the index. This optimization highlights the key information and enables effective retrieval with multiple pathways. CP 3: Query For straightforward user queries, the aforementioned key-value optimizations may yield high retrieval accuracy. However, when queries involve temporal references (e.g., Which restaurant did you recommend last weekend?), naive similarity search proves insufficient. We address this with time-aware indexing and query expansion strategy, where values are indexed with timestamped events, and retrieval is restricted to items within the relevant time range (5.4). CP 4: Reading Strategy Answering complex queries may require recalling numerous memory items. Although the retrieval accuracy can be enhanced through the preceding designs, it does not guarantee that the LLM can effectively reason over the extensive context (Shi et al., 2023; Liu et al., 2024). In 5.5, we explore reading strategies and demonstrate that optimizations such as extracting key information before answering (Chain-of-Note, (Yu et al., 2023)) and using structured format prompting (Yin et al., 2023) are crucial for achieving high reading performance."
        },
        {
            "title": "5 EXPERIMENT RESULTS",
            "content": "5.1 EXPERIMENTAL SETUP We mainly study three LLMs: GPT-4o, Llama 3.1 70B Instruct, and Llama 3.1 8B Instruct. For the retriever, we choose the 1.5B Stella V5 embedding (Zhang, 2023), given its high performance on MTEB (Muennighoff et al., 2023). Extensive comparisons between Stella V5 and alternative retrievers are provided in Appendix E.1. For the indexing stage, we employ Llama 3.1 8B Instruct to extract summaries, keyphrases, user facts, and timestamped events. All tasks use in-context learning, except for summaries and keyphrases, as detailed in Appendix D. When sessions or rounds are used as the key, we only keep the user-side utterances. In the reading stage, the retrieved items are always sorted by their timestamp to help the reader model maintain temporal consistency. Throughout 5.2 to 5.4, we apply the two optimizations discussed in 5.5 by default. All experiments were conducted on local server with 8 NVIDIA A6000 GPUs. 5.2 VALUE: DECOMPOSITION IMPROVES RAG PERFORMANCE Using LONGMEMEVALM, we compare different value choices in budget-aware manner. As shown in Figure 6, decomposing sessions into rounds significantly enhances reading performance with GPT-4o as the reader, while performing similarly to non-decomposed sessions when using Llama 3.1 8B Instruct as the reader. However, despite their efficiency in token usage, replacing sessions 8 Figure 6: QA performance on LONGMEMEVALM with different value designs. Decomposing sessions into rounds improves the QA performance. For the multi-session reasoning questions, further representing the values with the extracted facts improves the QA accuracy. Retrieval End-to-End QA Key Design Metrics@5 GPT-4o Recall NDCG Recall NDCG Top-5 Top-10 Top-5 Top-10 Top-5 Top-10 Metrics@ L3.1 70B L3.1 8B 0.582 = 0.530 = fact 0.282 = keyphrase 0.644 = + fact = + keyphrase 0.478 0.706 = 0.572 = summary 0.642 = fact 0.482 = keyphrase 0.689 = + summary 0.732 = + fact = + keyphrase 0.710 0.481 0.411 0.159 0.498 0.359 0.617 0.448 0.524 0.375 0.608 0.620 0. 0.512 0.449 0.303 0.536 0.410 Value = Round 0.615 0.588 0.425 0.657 0.541 Value = Session 0.670 0.554 0.644 0.618 0.658 0.714 0.665 0.638 0.468 0.571 0.401 0.624 0.652 0.602 0.692 0.654 0.392 0.784 0.636 0.783 0.648 0.814 0.576 0.749 0.862 0.768 0.670 0.664 0.489 0.720 0. 0.676 0.252 0.512 0.498 0.666 0.700 0.672 0.600 0.564 0.404 0.638 0.538 0.592 0.498 0.544 0.440 0.568 0.588 0.590 0.624 0.610 0.450 0.682 0.620 0.570 0.512 0.550 0.450 0.560 0.584 0.566 0.518 0.510 0.378 0.566 0. 0.524 0.444 0.470 0.388 0.518 0.530 0.526 0.534 0.534 0.432 0.572 0.524 0.464 0.216 0.404 0.414 0.494 0.490 0.508 Table 2: Retrieval and end-to-end QA performance on LONGMEMEVALM with different key designs for indexing. L3.1 = Llama 3.1 Instruct. We find applying document expansion with the extracted user facts (row = + fact) greatly improves both retrieval and the downstream QA. or rounds with extracted summaries or facts negatively impacts QA performance due to information loss. The only exception is with multi-session reasoning questions, where fact decomposition consistently improves performance. We hypothesize this is because fact decomposition extracts the same type of information across all sessions in more uniform and simplified format, aiding retrieval and reading (Chen et al., 2023b). Finally, we observe that the optimal token budget varies by the readers capability: while Llama 3.1 8B Instructs performance drops sharply beyond 3k retrieved tokens, GPT-4o continues to improve even with over 20k retrieved tokens. 5.3 KEY: MULTI-KEY INDEXING IMPROVES RETRIEVAL AND RAG In Table 2, we first explore whether summaries, keyphrases, or user facts condensed from the value can serve as better keys than the value itself. Interestingly, despite their more focused semantics, using these condensed forms alone does not enhance the memory recall performance. We hypothesize that this is due to the retrievers ability to already effectively handle long-text semantics. To leverage both the highlighted information from compression and the completeness of the original value, we applied simple document expansion technique (Tao et al., 2006; Efron et al., 2012), where the compressed information is concatenated with the original value to form the key during indexing3. This approach, particularly when using user facts, yielded an average improvement of 4% 3We also experimented with merging at the retrieval stage by combining the ranks from different pathways, but it underperformed compared to indexing-stage merging. See Appendix E.2 for details. 9 Key Setting Value = Session Value = Round Metric@5 Metric@10 Metric@5 Metric@ Recall NDCG Recall NDCG Recall NDCG Recall NDCG (1) = 0.639 (1) w/ Query Expansion (MT = GPT-4o) 0.654 (1) w/ Query Expansion (MT = Llama 3.1 8B Instruct) 0.624 0.684 (2) = + fact (2) w/ Query Expansion (MT = GPT-4o) 0.722 (2) w/ Query Expansion (MT = Llama 3.1 8B Instruct) 0.677 0.630 0.660 0.627 0.688 0.732 0.688 0.651 0.707 0.647 0.721 0.797 0.711 0.707 0.679 0.692 0.782 0.758 0.744 0.421 0.451 0.384 0.489 0.526 0. 0.462 0.565 0.448 0.500 0.548 0.532 0.499 0.495 0.489 0.550 0.722 0.570 0.511 0.538 0.488 0.598 0.669 0.647 Table 3: Retrieval performance on the temporal reasoning subset of LONGMEMEVALM. Timeaware query expansion significantly facilitates retrieval by narrowing down the retrieval scope. Figure 7: Question answering performance under the oracle retrieval setting. CoN with JSON format outperforms the other three parameter combinations by large margin. in retrieval metrics and 5% in final accuracy across all models. In Appendix E.1, we further analyze different different retrievers and find with alternative retrievers, key expansion with summary and keyphrases could improve Recall@5 when session is used as the value granularity. These results suggest that multi-pathway retrieval can significantly enhance memory retrieval performance. In the following section, we will investigate the time constraint as another pathway to optimize. 5.4 QUERY: TIME-AWARE QUERY EXPANSION IMPROVES TEMPORAL REASONING key challenge highlighted by LONGMEMEVAL in building real-world assistant systems is the need to utilize temporal information present in both metadata and user utterances to correctly answer time-sensitive queries. To address this need, we introduce simple yet effective time-aware indexing and query expansion scheme. Specifically, values are additionally indexed by the dates of the events they contain. During retrieval, an LLM MT extracts time range for time-sensitive queries, which is used to filter out large number of irrelevant values. As shown in Table 3, this simple design improves recall by an average of 11.4% when using rounds as the value and by 6.7% when using sessions as the value. This improvement remains consistent when key expansion is applied during indexing. We also find that the effectiveness of this method depends on using strong LLM for MT to accurately infer time ranges from queries. Llama 8B, on the other hand, struggles to generate accurate time ranges, often hallucinating or missing temporal cues even with numerous in-context examples. Further analysis is provided in Appendix E.3. 5.5 IMPROVING READING WITH CHAIN-OF-NOTE AND STRUCTURED FORMAT Since LONGMEMEVAL requires synthesizing information from multiple sessions in the interaction history, even an optimal memory retrieval mechanism might need to return several items to capture all relevant details. This makes correctly extracting and reasoning with retrieved information challenging. To enhance the models ability to handle long contexts, we applied two key optimizations. First, we present retrieved items in structured JSON format (Yin et al., 2023), which helps the model clearly recognize memory items as structured data for processing. Additionally, we adopt the Chain-of-Note (CoN) reading approach (Yu et al., 2023), instructing the LLM to first extract information from each memory item and then reason based on these notes. This effectively decomposes the long-context reading task into two simpler subtasks: extracting important details through copying, and reasoning with the concise notes. As shown in Figure 7, even under the oracle 10 retrieval setting, suboptimal reading strategy can result in up to 10-point absolute performance drop compared to the recommended approach. Notably, when CoN is not applied, the JSON format does not consistently outperform the natural language format. However, with CoN, the JSON format consistently benefits reader LLMs of various sizes and capabilities. In Appendix E.4, we provide an analysis of the error patterns of different LLMs with the enhanced reading strategy."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this study, we introduced LONGMEMEVAL, comprehensive and challenging benchmark designed to evaluate the long-term memory abilities of chat assistants across five core memory tasks: information extraction, cross-session reasoning, temporal reasoning, knowledge updates, and abstention. Through extensive experiments with both commercial systems and long-context LLMs, we demonstrated the significant challenges posed by LONGMEMEVAL, with current systems exhibiting substantial performance drops. By analyzing key design choices across indexing, retrieval, and reading stages, we proposed effective strategies such as session decomposition, factaugmented key expansion, and time-aware query expansion, which collectively improved both memory recall and the question answering performance. Our findings highlight the need for more sophisticated memory mechanisms to achieve personalized and reliable conversational AI, and LONGMEMEVAL offers valuable benchmark to drive future advancements in long-term memory capabilities for chat assistants."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "In the paper writing and the subsequent code release, we are committed to enabling other researchers to easily leverage our resources, replicate our results, and build upon our findings. We have documented the benchmark construction process in detail in 3.2 and Appendix A.1, including all the attributes and instructions used to prompt LLMs. We have created and will publicly release the two fixed evaluation datasets, LONGMEMEVALS and LONGMEMEVALM. In addition, we will also release the algorithm and source mixture used to create these two datasets, so that future studies could build upon them to create chat histories of any length. Finally, we have meticulously documented all the implementation details of our memory optimization in Appendix D, and our code will be released along with the benchmark as well. We believe that these efforts of transparency can help advance the field and foster future research endeavors."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Caio Cesar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024. doi: 10.48550/ARXIV.2404.14219. URL https://doi.org/10.48550/arXiv.2404.14219. Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou. Make your LLM fully utilize the context. CoRR, abs/2404.16811, 2024. doi: 10.48550/ARXIV.2404.16811. URL https://doi.org/10.48550/arXiv.2404.16811. 11 Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150, 2020. Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading. CoRR, abs/2310.05029, 2023a. doi: 10.48550/ARXIV.2310.05029. URL https://doi.org/10.48550/arXiv.2310. 05029. Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. Dense retrieval: What retrieval granularity should we use? CoRR, abs/2312.06648, 2023b. doi: 10.48550/ARXIV.2312.06648. URL https://doi.org/10. 48550/arXiv.2312.06648. Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 38293846. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.232. URL https://doi.org/10.18653/v1/2023. emnlp-main.232. Coze. Memory overview guide. https://www.coze.com/docs/guides/memory_ overview?_lang=en, 2024. Accessed: September 15, 2024. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, and Kam-Fai Wong. Perltqa: personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering. CoRR, abs/2402.16288, 2024. doi: 10.48550/ ARXIV.2402.16288. URL https://doi.org/10.48550/arXiv.2402.16288. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Miles Efron, Peter Organisciak, and Katrina Fenlon. Improving retrieval of short texts through document expansion. In William R. Hersh, Jamie Callan, Yoelle Maarek, and Mark Sanderson (eds.), The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 12, Portland, OR, USA, August 12-16, 2012, pp. 911920. ACM, 2012. doi: 10.1145/2348283.2348405. URL https://doi.org/10.1145/2348283. 2348405. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and In Forty-first Hao Peng. Data engineering for scaling language models to 128k context. International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=TaAqeo7lUh. Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. CoRR, abs/2405.14831, 2024. doi: 10.48550/ARXIV.2405.14831. URL https://doi.org/10.48550/arXiv. 2405.14831. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn. Res., 2022, 2022. URL https://openreview.net/forum?id= jKN1pXi7b0. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing In Houda Bouamor, Juan Pino, prompts for accelerated inference of large language models. and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 1335813376. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.825. URL https://doi.org/10.18653/v1/2023.emnlp-main.825. 12 Gregory Kamradt. Needle in haystack - pressure testing llms. GitHub, 2023. URL https: //github.com/gkamradt/LLMTest_NeedleInAHaystack. Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, and Edward Choi. Dialsim: real-time simulator for evaluating long-term dialogue understanding of conversational agents, 2024. URL https://arxiv.org/abs/2406. 13144. Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again! llm-powered personalized agent for long-term dialogue. CoRR, abs/2406.05925, 2024. doi: 10. 48550/ARXIV.2406.05925. URL https://doi.org/10.48550/arXiv.2406.05925. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Trans. Assoc. Comput. Linguistics, 12:157173, 2024. doi: 10.1162/TACL 00638. URL https://doi.org/ 10.1162/tacl_a_00638. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Geval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 25112522, Singapore, December 2023. Association doi: 10.18653/v1/2023.emnlp-main.153. URL https:// for Computational Linguistics. aclanthology.org/2023.emnlp-main.153. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of LLM agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1385113870, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.747. URL https://aclanthology.org/2024.acl-long.747. Announcing microsoft companion, Microsoft. https://blogs.microsoft.com/blog/2023/09/21/ 2023. announcing-microsoft-copilot-your-everyday-ai-companion/. Accessed: September 15, 2024. everyday copilot, URL your ai Jesse Mu, Xiang Li, and Noah D. Goodman. Learning to compress prompts with gist tokens. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 3d77c6dcc7f143aa2154e7f4d5e22d68-Abstract-Conference.html. the 17th Conference of Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text In Andreas Vlachos and Isabelle Augenstein (eds.), Proceedings embedding benchmark. of the Association for Computational Linguistics, pp. 20142037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.148. URL https://aclanthology.org/ 2023.eacl-main.148. the European Chapter of OpenAI. Chatgpt, 2022. URL https://chat.openai.com/chat. Accessed: September 15, 2024. OpenAI. Memory and new controls for chatgpt. https://openai.com/index/ memory-and-new-controls-for-chatgpt/, 2024. Accessed: September 15, 2024. Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333389, 2009. doi: 10.1561/1500000019. URL https: //doi.org/10.1561/1500000019. 13 Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. In The Manning. RAPTOR: recursive abstractive processing for tree-organized retrieval. Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= GN921JHCRw. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 3121031227. PMLR, 2023. URL https://proceedings.mlr.press/ v202/shi23a.html. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models. In Kevin Duh, Helena Gomez-Adorno, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 83718384. Association for Computational Linguistics, 2024a. doi: 10.18653/V1/2024.NAACL-LONG.463. URL https://doi.org/10.18653/v1/2024. naacl-long.463. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: Retrieval-augmented black-box language models. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 83718384, Mexico City, Mexico, June 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.463. URL https://aclanthology.org/2024.naacl-long.463. Tao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. Language model information In Robert C. Moore, Jeff Bilmes, Jennifer Chu-Carroll, retrieval with document expansion. and Mark Sanderson (eds.), Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pp. 407414, New York City, USA, June 2006. Association for Computational Linguistics. URL https://aclanthology.org/N06-1052. Augmenting language models with long-term memory. Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ ebd82705f44793b6f9ade5a669d0f0bf-Abstract-Conference.html. Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory. Advances in Neural Information Processing Systems, 36, 2024. Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014. Yuhuai Wu, Markus Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model In Houda Bouamor, Juan Pino, and Kalika with parameter-efficient tuning on self-chat data. Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 62686278, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.385. URL https://aclanthology.org/ 2023.emnlp-main.385. 14 Fangyuan Xu, Weijia Shi, and Eunsol Choi. RECOMP: improving retrieval-augmented lms with In The Twelfth International Conference on context compression and selective augmentation. Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=mlJLVigNHp. Jing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term open-domain conversation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 51805197, Dublin, Ireland, May 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.356. URL https://aclanthology.org/ 2022.acl-long.356. Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. Long time no see! open-domain conversation with long-term persona memory. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 26392650, Dublin, Ireland, May 2022b. Association doi: 10.18653/v1/2022.findings-acl.207. URL https:// for Computational Linguistics. aclanthology.org/2022.findings-acl.207. Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 30633079. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.172. URL https://doi.org/10.18653/v1/ 2023.acl-long.172. Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chain-of-note: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210, 2023. Dun Zhang. STELLA EN 1.5B v5. https://huggingface.co/dunzhang/stella_en_ 1.5B_v5, 2023. Accessed: September 15, 2024. Hongming Zhang, Xiaoman Pan, Hongwei Wang, Kaixin Ma, Wenhao Yu, and Dong Yu. Cognitive kernel: An open-source agent system towards generalist autopilots, 2024. URL https:// arxiv.org/abs/2409.10277. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, ThirtySixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 1972419731. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29946. URL https://doi.org/10.1609/aaai.v38i17.29946. Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 56575673, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. emnlp-main.382. URL https://aclanthology.org/2022.emnlp-main.382."
        },
        {
            "title": "A SUPPLEMENTAL DETAILS FOR LONGMEMEVAL",
            "content": "A.1 DATASET CONSTRUCTION In this section, we discuss the details of our benchmark construction process. Attribute Ontology In Table 4, we provide the full attribute ontology used in LONGMEMEVAL. This attribute ontology is constructed manually to reflect commonly mentioned topics in userassistant chats. Five major categories are included: demographic information, lifestyle, situational context, life events, and belongings. Attribute 1: Demographic Information age, gender, ethnicity, nationality, language, education level, occupation Attribute 2: Lifestyle 2.1: Shopping: online shopping frequency, favorite stores, loyalty program, sales events, coupons, gift purchasing habits, eco-friendly product preferences, luxury vs budget shopping, technology gadget purchasing, fashion and apparel, grocery shopping, shopping for others 2.2: Media Consumption: book, movie, tv show, music, podcast, video game, streaming service, theater, magazine and newspaper, youtube, educational content, audiobook and e-book 2.3: Social Media Engagement: posting, commenting, followers, groups, hashtags, campaigns, messaging, live streaming, social media breaks 2.4: Daily Routines: wake-up time, bedtime, work or school start time, meal time, exercise routines, coffee or tea break, commuting, evening activities, weekend routines, cleaning schedules, time spent with family or friends 2.5: Travel: frequency, destination, road trips, travel agencies, outdoor adventures, airlines, hotel, travel with family vs solo travel, packing habits 2.6: Recreation: reading, painting, musical instruments, dancing, watching sports, participating in sports, gardening, bird watching, fishing or hunting, board games, video games, fitness classes, yoga, sculpting, photography, stand-up comedy, writing, collecting, model building, aquarium keeping 2.7: Eating and Cooking: home cooking, food delivery, vegetarian or vegan, favorite cuisines, snacking habits, barbecue, baking, cocktail mixing, cooking classes 2.8: Event Participation: concerts, theater, galleries and museums, sports games, film festivals, religious services, book readings, charity events, trade shows, lectures or workshops, theme parks, local markets, networking events, sports, auto racing, workshops, museum tours Attribute 3: Situational Context 3.1: Home: living room, kitchen, bathroom, room style, room lighting, furniture, technology, plants 3.2: Social Context: alone, family, friends, interactions with strangers 3.3: Time Context: time of day, day of week, seasonal Attribute 4: Life Events graduations, academic achievements, study abroad, significant academic projects, job promotions, starting business, births and adoptions, marriages, family reunions, illness or surgeries, mental health journeys, purchasing home, trips, movement, living abroad, refugee or immigration, loss of loved ones, name change, belief, milestone Attribute 5: Belongings cars, bikes, vehicles, computer, phone, pet, farm animal, animal care items, home, land, art, antiques, collectible, rare items, clothing, jewelry, shoes, bag, sports gear, musical instruments, health related devices, crafting, photography Table 4: Human-constructed user attribute ontology. Each attribute represents unique dimension of human experience along which user biography could be constructed. For LONGMEMEVAL, we sample user backgrounds based on each of the attributes and construct questions on top of them. Background Sampling Based on each of the attributes, we prompt Llama 3 70B Instruct to generate background paragraph outlining the user memory and experience. In our preliminary studies, we find that the following zero-shot prompt in Figure 8 can already guide the model to generate long and focused user background with sufficient details, which suffices for the next step of question creation. We thus use the same prompt for the final version of LONGMEMEVAL. 16 will give you topic. Please imagine you are user that wants to recall and record recent personal facts along the topic. Generate long text describing these personal facts. Use your imagination and generate the personal facts. Make it long and involve several recent facts or recent events spanning many days, weeks, or monthes. You may state the facts in plain language and no need to make it story-like. Topic: {attribute} Recent Personal Facts related to {attribute}: Figure 8: The prompt for constructing user backgrounds based on an attribute. will give you past memory. Use the memory to act as normal user to chat with chat assistant. In the chat, you may ask it to assist you various tasks or ask it about various information. However, make sure that your convey the following fact about you: {evidence statement}. In addition, make sure your message is concise (1-2 simple sentences), since the real users often do not bother write long message. will provide you with the chat history and the response from the assistant. Directly generate the next response from the users perspective. You must simulate the tone of neutral user and do not be overly enthusiastic, verbose, formal, or polite. For conciseness, DO NOT react to the assistants message with e.g., thanks or will do that. Instead, directly state the follow-up questions or new questions. Memory: {background} Chat History: assistant: Hi! How can assist you today? ... (more rounds as the conversation continues) ... Figure 9: The prompt for instruction an LLM to act as user and initiates task-oriented dialogue with another LLM. Both the background and the evidence statement is provided. This prompt is used for the question type single-session. For the other question types, the prompt components are slightly different but the prompt overall follows the same style. Question Construction As discussed in 3.2, based on the generated user backgrounds, Llama 3 70B Instruct is used to propose question and answers for each question type. Additionally, for the question type temporal-reasoning, multi-session reasoning, and single-session-preference, we use GPT-4o to propose several questions. Nevertheless, we find most of the questions to be In total, approximately 1000 unsatisfactory and manually filter and edit most of the questions. questions were generated for each question type, and the final yield rate is about 5%. For each question, we then manually decompose the answer into the evidence statements. If the question or the evidence statements involve time mentions, we assign timestamp to the question and the evidence statements at this stage. Note that if timestamps are specified for the evidence statements at this stage, these timestamps will always be used for corresponding evidence sessions. Otherwise, the timestamps will be randomly assigned at the history construction stage with all the other sessions. Evidence Session Construction Using the question and the decomposed evidence statements, we use Llama 3 70B Instruct to simulate one user-AI chat history per evidence statement via self-chat. In Figure 9, we present an example of the chat simulation prompt, where we ask the user LLM to indirectly mention the evidence statement while avoiding to talk about other evidence statements for the question, if there are any. We include two crucial instructions in the prompt to make sure (1) the evidence statement is provided in an indirect way and (2) the generated messages are concise and thus mimic the style of user messages. On the assistant side, we directly provide the input generated by the user LLM without any prompt engineering. We simulate the chat for 10 round at maximum and stopped prematurely when either side of the LLMs generates an empty sequence indicating the end of the conversation. Subsequently, expert annotators manually inspect and edit each of the generated sessions to ensure that (1) the required evidence statements are present in the conversation, (2) no other evidence statements are leaked into the conversation, (3) the evidence statements are provided in colloquial style, especially for the data and time mentions, and (4) the conversation ends gracefully. In total, roughly 70% of the sessions are human edited. We note that in few rare instances, the user LLM fails by assuming the assistant role instead. When these failures are identified, we discard the instance if the conversation cannot be fixed. A.2 HISTORY CONSTRUCTION In order to construct coherent and freely extensible chat history, we design three-staged pipeline that include session pool construction, session sampling, and timestamp resolution. Session pool construction For each question, we draw the history sessions from three sources: ShareGPT (Zheng et al., 2023), UltraChat (Ding et al., 2023), and the simulated sessions corresponding to other attributes using the same pipeline mentioned in the previous section. This pool ensures that the non-evidence history sessions have similar topic or format as the evidence sessions, while avoiding providing conflicting information that would invalidate the question. Session sampling To sample history containing sessions, we randomly sample from the aforementioned three sources and shuffle the sessions together with the questions evidence sessions. For LONGMEMEVAL, we always use the following mixture: 25% ShareGPT, 25% UltraChat, and 50% simulated sessions. If the evidence sessions need to follow specific order, we swap their orders accordingly after shuffling. Timestamp resolution Finally, we randomly assign timestamps to the session following their order of the history. If the evidence sessions are associated with pre-defined timestamps, we use them as anchors to determine the range of timestamp of the non-evidence sessions preceding or following them. Other wise, we randomly assign tiemstamps in May 2023. A.3 EVALUATION METRIC BUILDING To accurately evaluate the diverse responses of LLMs, we use an expert-written prompt to instruct GPT-4o as the correctness judge. We present the full prompt in Figure 10. To enable the model to handle detailed edge cases as how expert evaluators would do, we design separate prompts for number of tasks. To ensure the prompt has high agreement with expert judge, we sample 30 questions per problem type, collect the long-context generation results from GPT-4o and Llama 3.1 8B Instruct, and report the judgment correctness by category. As shown in Table 5, the prompt-engineered GPT-4o judge achieves reliable performance in evaluating both GPT-4o and Llama-3.1-8B-Instruct as the generation model. The evaluators judgment slightly deviates from human experts for the single-session-preference and abstention problems due to the open-ended nature of the response. Nevertheless, our evaluation prompt still achieves 90% or higher accuracy under all settings. We will include this prompt in the benchmark package that we will release to enable consistent comparisons for future work. 18 temp-reasoning will give you question, correct answer, and response from model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains subset of the information required by the answer, answer no. In addition, do not penalize off-by-one errors for the number of days. If the question asks for the number of days/weeks/months, etc., and the model makes off-by-one errors (e.g., predicting 19 days when the answer is 18), the models response is still correct. knowledge-update will give you question, correct answer, and response from model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response contains some previous information along with an updated answer, the response should be considered as correct as long as the updated answer is the required answer. single-session-preference will give you question, rubric for desired personalized response, and response from model. Please answer yes if the response satisfies the desired response. Otherwise, answer no. The model does not need to reflect all the points in the rubric. The response is correct as long as it recalls and utilizes the users personal information correctly. Other question types will give you question, correct answer, and response from model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains subset of the information required by the answer, answer no. Figure 10: Evaluation instructions for the GPT-4o judge. We provide the question, answer, and the models hypothesis after the instruction and ask GPT-4o to directly generate yes or no. Question Type Answer Model GPT-4o Llama-3.1-8B-instruct single-session-user 1.00 (30/30) single-session-assistant 1.00 (30/30) single-session-preference 0.90 (27/30) 1.00 (30/30) 1.00 (30/30) 1.00 (30/30) 0.97 (29/30) multi-session knowledge-update temporal-reasoning abstention Average 0.98 0.97 (29/30) 1.00 (30/30) 0.97 (29/30) 1.00 (30/30) 1.00 (30/30) 0.97 (29/30) 0.90 (27/30) 0. Table 5: Meta-evaluation results of prompt-engineered GPT-4o judge. We observe high evaluation accuracy across all the problem types in LONGMEMEVAL."
        },
        {
            "title": "B A HUMAN STUDY ON COMMERCIAL MEMORY CHATBOTS",
            "content": "We evaluate two commercial memory-augmented chatbots: ChatGPT (OpenAI, 2024) and Coze (Coze, 2024). We randomly selected 97 questions and created short chat history of 3-6 sessions by sampling according to mixture of 50% ShareGPT and 50% simulated sessions. We skip two type of questions that the assistants cannot answer: (1) subset of temporal-reasoning, since our manual analysis cannot afford interacting with the (potentially evolving) systems across multiple months, and (2) single-session-assistant, since the systems do not remember any information given by the assistant. We also did not evaluate the abstention ability of these two systems because an early version of the dataset without any abstention questions was used for this analysis. Since these systems only support memory features via their web interfaces, human annotators manually interacted with the chat assistants session-by-session, ending with new session where the question was posed. After collecting the models response, the annotator manually evaluates the answers correctness. Finally, to start evaluating the next instance from fresh state, the annotator manually clears the models memory through the web interface. We distribute the questions across five annotators. discussion is performed among the annotators whenever there is concern with the models response or with the evidence sessions. All evaluations were conducted in the first two weeks of August 2024. In Table 6, we present the detailed human evaluation results by problem types. We observe that when the task is memorizing the information from single session (column IE), both systems can answer considerable number of problems correctly. However, for the other question types where aggregation across multiple sessions is generally required, both systems exhibit significant performance drops. Compared to ChatGPT, we find most of Cozes errors are due to failing to record information from some session. On the other hand, ChatGPT generally records the evidence statements immediately after it has been presented in the evidence session. However, as the interaction proceeds, ChatGPT often modify this information when it compresses the history, resulting in information loss. This highlights the potential trade-off between reliable personalization and efficiency."
        },
        {
            "title": "IE MR KU",
            "content": "TR"
        },
        {
            "title": "ChatGPT",
            "content": "GPT-4o-mini GPT-4o 1.000 0.647 0.667 0.652 0.688 0.441 0.833 0."
        },
        {
            "title": "Coze",
            "content": "GPT-3.5-turbo 0.625 0.118 0.375 0.043 0.813 0.147 0.208 0.391 GPT-4o Table 6: Human evaluation results of two systems categorized by evaluated ability types. We use the questions from single-session-user for the IE column. For the temporal reasoning column (TR), we use the questions that do not require reasoning with metadata."
        },
        {
            "title": "C EXISTING MEMORY SYSTEMS FROM THE UNIFIED VIEW",
            "content": "In 4, we have presented unified view of the memory systems with three stages and four control points. In this section, we provide closer look at nine memory-augmented chat systems and view them as specific instances of the unified framework. Specifically, we consider in-context RAG (Shi et al., 2024b), Memorybank (Zhong et al., 2024), LD-Agent (Li et al., 2024), CoN (Yu et al., 2023), ChatGPT, Coze, RAPTOR (Sarthi et al., 2024), MemWalker (Chen et al., 2023a), and HippoRAG (Gutierrez et al., 2024). Table 7 provides an overall comparison between the systems, among which we also situate the recommended design identified through the papers empirical study. Indexing For the value representation, most of the surveyed systems either directly use the sessions themselves or use mixture of them with the extracted facts or summaries. Only three systems (LD-Agent, ChatGPT, and Coze) use only the compressed values to replace the original ones, which may incur an information loss. When it comes to the indexing key, we observe three major types of decisions. First, number of systems (in-context RAG, Memorybank, LD-Agent, and CoN) simply use the values as the keys, which wins in terms of simplicity. By comparsion, 20 Method Value Key Query Retrieval Time-aware Reading In-context RAG MemoryBank LD-Agent CoN ChatGPT Coze RAPTOR MemWalker HippoRAG round/session summary + round summary + fact round/session fact fact round/session round/session round/session = = = = - - node summary node summary entity question question keyphrase question - - question question entity flat flat flat flat - - flat/interactive interactive PPR Our Design round = + fact question + time flat No Yes Yes No No No No No No Yes direct direct direct CoN - - - interactive direct CoN Table 7: comparison of nine memory-augmented frameworks through the lens of the proposed unified framework. For ChatGPT and Coze, we skip several designs as they are unknown. We also remark that they may have other value representations that are not directly exposed to the user. HippoRAG builds an entity-centric index, and RAPTOR/Memwalker build hierarchical index It is noteworthy that while more complex memory indexing using recursive summarization. structure can potentially benefit certain types of queries, they also increase the cost of creating and maintaining the index in online interactions. Specifically, for HippoRAG, RAPTOR, and Memwalker, some level of re-indexing is required when new session is added to the memory, increasing the computational overhead of these sytems. Retrieval For most of the systems, we find that the question is generally used as the query, which is intuitive since the questions are generally short. LD-Agent and HippoRAG extract additional information and leverage them to better pinpoint entity-centric knowledge within the index. In terms of the query-key matching process, most systems use flat retrieval setup, where similarity search is directly conducted between the query and the keys. HippoRAG uses Personalized PageRank (PPR) to leverage its entity graph structure to match passages that feature entities close to the seed entities extracted from the query. Different from these approaches, MemWalker bridges the retrieval and inference through an interactive reading mechanism where the reader LLM self-navigates the indexing structure to find the answer. This method brings the additional robustness in case the retriever fails, but also incurs additional latency costs due to more LLM inference. Generation Most of the systems apply direct reading mechanism where the reader model is provided with the retrieved memory items and directly asked to generate the response. However, as we show in the main results, adapting an extract-before-read strategy is important to high performance. On the other hand, the interactive reading mechanism such as that used in MemWalker allows the reader to backtrack and search again in case the recalled memory is inadequate. MEMORY OPTIMIZATIONS: IMPLEMENTATION DETAILS In this section, we describe our implementations of the memory optimizations in detail. Value Decomposition We design an LLM-based method for compressing the value, which are either sessions or rounds, into three different formats: summaries, keyphrases, or user facts. In Figure 11, we present the corresponding zero-shot and few-shot prompts. Note that few-shot learning is only applied for user fact extraction, and we find it to be unnecessary for the rest two tasks. For in-context examples, we randomly sample ten examples, five from the simulated user sessions and five from ShareGPT. The expected responses are generated by GPT-4o and modified by human annotators. We manually tune the prompts by observing several examples. Llama 3.1 8B Instruct is used for all the extraction experiments. Note that for all the experiments, we only provide the user-side messages for the extraction, skipping the assistants responses. Key Expansion To expand the value documents with summaries, keyphrases, or user facts, we follow the same pipeline in the previous section to first extract these pieces of information from each value. Then, we directly prepend the extracted information to the corresponding key. 21 Time-Aware Indexing and Query Expansion At the indexing stage, we instruct Llama 3.1 8B Instruct to extract the event mentions in the text whose timestamp could be inferred. At the retrieval stage, we instruct an LLM (we explore GPT-4o and Llama 3.1 8B Instruct) to extract the time range for retrieval for the problems that specifically focus on range of time. We present the corresponding prompts in Figure 12. Ten human-written in-context examples are additionally provided. Reading Strategy We present the prompt for reading in Figure 13, which is small variation of Chain-of-Note (CoN). Overall, the prompt shares the same idea with CoN, that the model is asked to traverse the documents and extract the evidence before generating the answer to the question. We use greedy search for all the experiments, and set the maximum generation length to 800 tokens. Summaries Below is transcript of conversation between human user and an AI assistant. Please summarize the following dialogue as concisely as possible in short paragraph, extracting the main themes and key information. In your summary, focus more on what the user mentioned or asked for. Dialogue content: {session or round} Keyphrases Below is transcript of conversation between human user and an AI assistant. Generate list of keyphrases for the session. Separate each keyphrase with semicolon. Dialogue content: {session or round} User Facts You will be given list of messages from human user to an AI assistant. Extract all the personal information, life events, experience, and preferences related to the user. Make sure you include all details such as life events, personal experience, preferences, specific numbers, locations, or dates. State each piece of information in simple sentence. Put these sentences in json list, each element being standalone personal fact about the user. Minimize the coreference across the facts, e.g., replace pronouns with actual entities. If there is no specific events, personal information, or preference mentioned, just generate an empty list. Human user messages: {session} Personal facts about the user (a list of strings in json format; do not generate anything else): Figure 11: Zero-shot prompts for extracting information from the value items for indexing. For user fact extraction, we additionally provide ten in-context examples. 22 Extracting Timestamped Events You will be given list of messages from human user to an AI assistant, as well as the time the conversation took place. Extract all events related to the user as long as its date is specified or could be inferred. If the time some event took place cannot be inferred, do not extract that event. Return the events in json list where each item contains two fields: date and event. Write date in the form YYYY/MM/DD. If there is no specific event, just write an empty list. Query Expansion You will be given question from human user asking about some prvious events, as well as the time the question is asked. Infer potential time range such that the events happening in this range is likely to help to answer the question (a start date and an end date). Write json dict two fields: start and end. Write date in the form YYYY/MM/DD. If the question does not have any temporal referencea, do not attempt to guess time range. Instead, just say N/A. Figure 12: Zero-shot prompt for extracting timestamped events from the value items for indexing and the prompt for extracting time range from the user question. For both settings, we additionally provide ten in-context examples. CoN Prompt will give you several history chats between you and user. Please answer the question based on the relevant chat history. Answer the question step by step: first extract all the relevant information, and then reason over the information to get the answer. History Chats: {chat history} Current Date: {question date} Question: {question} Answer (step by step): Non-CoN Prompt will give you several history chats between you and user. Please answer the question based on the relevant chat history. History Chats: {chat history} Current Date: {question date} Question: {question} Answer: Figure 13: The prompt for reading with recalled memory with and without Chain-of-Note. 23 Key Design Retriever Value = session Value = round Metric@ Metric@10 Metric@5 Metric@10 Recall NDCG Recall NDCG Recall NDCG Recall NDCG = = + summary = + fact = + keyphrases BM25 Contriever 0.634 0.723 Stella V5 1.5B 0.720 BM25 Contriever 0.626 0.732 Stella V5 1.5B 0. BM25 Contriever 0.683 0.762 Stella V5 1.5B 0.732 BM25 Contriever 0.632 0.740 Stella V5 1.5B 0.710 0.516 0.634 0.594 0.544 0.632 0. 0.560 0.632 0.520 0.546 0.638 0.587 0.710 0.823 0.794 0.694 0.823 0.749 0.757 0.862 0.862 0.711 0.849 0. 0.540 0.663 0.615 0.565 0.657 0.624 0.582 0.658 0.552 0.569 0.668 0.602 0.472 0.589 0.660 0.352 0.454 0. 0.538 0.747 0.784 0.372 0.495 0.528 - - - 0.554 0.612 0.644 0.453 0.546 0.478 - - - 0.454 0.489 0.498 0.391 0.450 0.359 - - - 0.608 0.756 0.784 0.538 0.672 0.636 - - - 0.473 0.530 0.536 0.416 0.489 0.410 Table 8: comparison between three retrievers under different key-value indexing settings."
        },
        {
            "title": "E EXTENDED ANALYSES",
            "content": "We provide additional analyses on LONGMEMEVAL, including retriever selection, memory optimization, and error analyses of end-to-end retrieval-augmented generation. E.1 ABLATIONS ON RETRIEVER SELECTION In Table 8, we compare the performance of Stella V5 with two alternative popular retrievers: BM25 (Robertson & Zaragoza, 2009) and Contriever (Izacard et al., 2022). We do not report the results of retrievers that use larger embedding models, as their latency is generally unrealistic for real applications. The retrievers are compared under two settings: (1) vanilla key design where the values themselves are used as the key and (2) the key expansion strategies introduced in the main text. We list the key observations below: Both dense retrieval embeddings have significantly higher performance than the BM25 sparse retrieval method. The trend is the same across most settings. The recommended technique of performing key expansion with user fact consistently improves the performance over directly using the value as the key. Key expansion with summary and keyphrases could improve the performance in some settings. For example, both summary and keyphrase improve the Recall@5 of Contriever when session is used as the value granularity. However, expanding the key with facts still gives the greatest performance gain. E.2 POST-RETRIEVAL RANK MERGING FOR INDEX EXPANSION In 5.3, we have introduced and evaluated the key expansion strategy for multi-path retrieval, where important information is extracted from the value items and then used to augment the key. However, an alternative strategy is to directly create separate key with the retrieved information and place it in parallel as the original keys. At the retrieval stage, the query could be used to retrieve from pools defined multiple types of keys and the values from different sequences could be merged subsequently according to their rank. Effectively, this implements the true multi-path retrieval. We call this strategy rank merging. In Table 9, we thoroughly evaluate rank merging verse key merging, the strategy we recommended in the main text. We find that rank merging has much lower performance than key merging. One potential reason is that rank merging increases the index size by + 1 times, where is the number of information pieces extracted from each (key, value) pair. By comparison, key merging highlights the important information while avoiding explording the size of the index. 24 Key Design Metrics@5 GPT-4o Recall NDCG Recall NDCG Top-5 Top-10 Top-5 Top-10 Top-5 Top-10 Metrics@10 L3.1 70B L3.1 8B Retrieval End-to-End QA 0.582 = 0.478 = + fact (RM) 0.386 = + keyphrase (RM) 0.644 = + fact (KM) = + keyphrase (KM) 0.478 0.706 = 0.608 = + summary (RM) 0.618 = + fact (RM) 0.550 = + keyphrase (RM) 0.689 = + summary (KM) 0.732 = + fact (KM) = + keyphrase (KM) 0.710 0.481 0.372 0.376 0.498 0.359 0.617 0.488 0.511 0.435 0.608 0.620 0.587 0.692 0.568 0.536 0.784 0. Value = Round 0.512 0.403 0.329 0.536 0.410 Value = Session 0.638 0.511 0.548 0.466 0.624 0.652 0.602 0.783 0.684 0.754 0.650 0.749 0.862 0.768 0.615 0.558 0.485 0.657 0.541 0.670 0.600 0.622 0.560 0.658 0.714 0.665 0.670 0.596 0.590 0.720 0.652 0.676 0.620 0.688 0.620 0.666 0.700 0. 0.600 0.536 0.460 0.638 0.538 0.592 0.510 0.574 0.454 0.568 0.588 0.590 0.624 0.586 0.558 0.682 0.620 0.570 0.544 0.574 0.506 0.560 0.584 0.566 0.518 0.458 0.426 0.566 0.472 0.524 0.468 0.468 0.492 0.518 0.530 0. 0.534 0.482 0.466 0.572 0.524 0.464 0.466 0.466 0.482 0.494 0.490 0.508 Table 9: Retrieval and end-to-end QA performance on LONGMEMEVALM with different key designs for indexing. L3.1 = Llama 3.1 Instruct. RM = Rank Merging and KM = Key Merging. E.3 STRONG AND WEAK LLMS FOR EXTRACTING TIME RANGES FROM QUERIES In 5.4, we concluded that strong model is required to accurately extract the time ranges from the questions, even if in-context examples (with balanced labels) are provided. In this section, we further illustrate this finding with examples in Table 10. We find that the Llama 3.1 8B Instruct model can generate correct temporal range when the time references are clearly defined (example 2). However, in many of the cases where the question does not have any temporal reference (example 1, 2, 4), the model often mistakenly extracts time range, which erroneously prunes out the search space, leading to low memory recall. By contrast, GPT-4o is able to refuse to generate time range when the question does not have time reference. Example 1 Question Date: 2023/05/28 Question: How long had been taking guitar lessons when bought the new guitar amp? Predicted time range (GPT-4o): No date extracted [correct] Predicted time range (Llama 3.1 8B Instruct): 2023/05/012023/05/28 [false positive] Example 2 Question Date: 2023/04/27 Question: Which airline did fly with the most in March and April? Predicted time range (GPT-4o): 2023/03/012023/04/30 [correct] Predicted time range (Llama 3.1 8B Instruct): 2023/03/012023/04/30 [correct] Example 3 Question Date: 2023/06/28 Question: How many days before the Rack Fest did participate in the Turbocharged Tuesdays event? Predicted time range (GPT-4o): No date extracted [correct] Predicted time range (Llama 3.1 8B Instruct): 2023/06/212023/06/28 [false positive] Example 4 Question Date: 2023/03/10 Question: Which seeds were started first, the tomatoes or the marigolds? Predicted time range (GPT-4o): No date extracted [correct] Predicted time range (Llama 3.1 8B Instruct): 2023/03/012023/03/10 [false positive] Table 10: Example of time span extraction outputs from GPT-4o and Llama 3.1 8B Instruct. 25 E. ERROR ANALYSIS In this section, we analyze the source of error of various LLMs with our best memory design. Specifically, we use round as the value granularity, expand the key with extracted user facts, and apply CoN as the reading strategy. Stella v5 1.5B is used as the retriever, and top-10 items are provided to the model with JSON structured prompt. In Figure 14, we analyze the distribution of the correct and error cases for three different reader LLMs. First, we observe that substantial proportion of errors corresponds to correct retrieval yet wrong generation (15%19% of all instances, and 40%50% among the error instances). The proportion is higher when weaker reader LLM is used. This indicates that the reading strategy still has large room of improvement. In addition, we observe that for the reader LLM to generate correct answer, performing correct retrieval is necessary in 90% of the time. We observe that the rest 10% instances are mostly of the question type knowledge-update and the the retriever was only able to identify the updated knowledge but failed to retrieved the previous information before update. For these cases, our strict retrieval evaluation criteria will deem that the retrieval has failed. Overall, this result also highlights the high quality of LONGMEMEVAL, as the reader LLM cannot take any shortcut to answer the question correctly without correct memory recall. Figure 14: An analysis of the error distribution of different reader models. We use and to indicate correct Recall@10 and correct answer based on the top-10 retrieved results."
        }
    ],
    "affiliations": [
        "Tencent AI Lab Seattle",
        "UC San Diego",
        "UCLA"
    ]
}