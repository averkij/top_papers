{
    "paper_title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models",
    "authors": [
        "Jinsong Li",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Yuhang Cao",
        "Jiaqi Wang",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 9 1 8 0 0 . 8 0 5 2 : r BEYOND FIXED: VARIABLE-LENGTH DENOISING"
        },
        {
            "title": "FOR DIFFUSION LARGE LANGUAGE MODELS",
            "content": "Jinsong Li1,2 Xiaoyi Dong1,2 Yuhang Zang2 Yuhang Cao2 Jiaqi Wang2 Dahua Lin1,2 1 The Chinese University of Hong Kong 2 Shanghai AI Laboratory https://github.com/Li-Jinsong/DAEDAL"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion Large Language Models (DLLMs) are emerging as powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by critical architectural constraint: the need for statically predefined generation length. This static length allocation leads to problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from short initial length and iteratively expands it to coarse task-appropriate length, guided by sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion Large Language Models (DLLMs) (Nie et al., 2025; DeepMind, 2025; Inception Labs et al., 2025) have recently emerged as promising paradigm for Large Language Models (LLMs)(Yang et al., 2025; Achiam et al., 2023), garnering significant attention from both academia and industry. Unlike the conventional autoregressive (AR) framework, which generates text sequentially via nexttoken prediction, DLLMs operate through multi-step iterative denoising process. By leveraging bidirectional attention(Vaswani et al., 2017) to refine an initially masked sequence into coherent output, this approach offers several distinct advantages(Nie et al., 2024), including the ability to utilize global context for tasks that require holistic planning, flexible trade-off between the number of inference steps and the quality of the generated sample. Given these unique properties, DLLMs have become compelling research direction that offers an alternative to the autoregressive paradigm. Despite their promising potential, the inference of DLLMs suffers from fundamental limitation rooted in their denoising paradigm: the denoise starts from fully-masked sequence of fixed length, hence the final output length is also statically predefined. This static setup leads to flawed inference This work is done during an internship in Shanghai AI Laboratory. Corresponding author 1 Figure 1: Overview of DAEDALs effectiveness on LLaDA-Instruct-8B. (a) DAEDAL uses unified and short initial length, consistently surpassing the baseline, which needs its length meticulously tuned for each benchmark to achieve peak performance. (b) DAEDAL dynamically adjusts length and adaptively expands on per-problem basis, resulting in varied distribution of response lengths. In contrast, the baseline is constrained to fixed length for all problems. compared to their autoregressive counterpart: AR LLMs flexibly adjust the output length based on the given task, while DLLMs must adapt the task output based on the given length. The requirement of manually pre-defined length leads to severe dilemma. An overly short length hinders the model from solving complex problems due to an insufficient token length. Conversely, universally adopting long generation length introduces new set of issues. First, it incurs large computational overhead due to the quadratic complexity of bidirectional attention. Second, as shown in Figure 1 (a), we observe that excessively long initial lengths may degrade model performance. This rigid, pre-defined length constraint not only creates the dilemma at the outset of inference, but more fundamentally, it cripples the models ability to adapt dynamically. For instance, DLLMs lack the critical test-time scaling capability(Muennighoff et al., 2025) of AR models, which can extend their output to self-correct (e.g., Wait, let me rethink...)(Shah et al., 2025; Wei et al., 2022). This problem is exacerbated by the non-sequential generation nature of DLLMs. model might generate the beginning and end of sequence first, only to find the allocated space for intermediate reasoning insufficient, leading directly to incomplete logic and degraded performance. Fortunately, we find the solution lies within the DLLMs intrinsic capabilities its planning ability. In each denoise step, the model plans the final output by predicting all the mask tokens with different confidence. We discovered that the models prediction confidence acts as powerful, general-purpose signal indicating whether the generation space is sufficient. For example, as shown in Figure 2, in the first denoising step (t=1), the model confidently predicts more End-of-Sequence tokens (EOS) from the fully masked sequence when the length is sufficient for the given task, while less confident in predicting EOS when the length is insufficient. This core insight paves the way for variable-length denoising strategies, and we present DAEDAL, novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Initial Length Adjustment. Before the denoising process, DAEDAL starts from short initial length and iteratively expands it to coarse task-appropriate length. The expansion is guided by sequence completion metric, which is calculated by the predicted confidence of EOS tokens within fixed window. 2) Iterative Mask Insertion. During the denoising process, DAEDAL dynamically expands the sequence to develop better output. The expansion is realized by inserting mask tokens into insufficient regions, where the model struggles to plan and the corresponding prediction confidence is quite low. With DAEDAL, DLLMs no longer require manually tuned, task-specific generation lengths. Instead, they can start from short, unified initial length and dynamically expand as needed. Experiments Figure 2: Visualization of the DLLMs awareness of length sufficiency. The heatmaps show the difference in average EOS token confidence at the sequence terminus, measured after the first prediction on fully masked 128-token input. This difference is the result of subtracting the average confidence on length-insufficient problems (those answered correctly only with much longer sequence) from that on length-sufficient problems (those answered correctly under 128 tokens). The experiment is conducted with LLaDA-Instruct-8B. The predominantly green color (difference > 0) indicates that EOS confidence is higher for length-sufficient problems, validating our core insight. demonstrate that DAEDAL not only allocates appropriate computational resources for diverse tasks, as shown in Figure 1(b), but also achieves performance that is comparable, and in some cases superior, to the peak performance of meticulously tuned fixed-length baselines. Our method thus achieves strong performance while significantly improving computational efficiency."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Diffusion Large Language Models. In recent years, Diffusion Language Models (DLLMs) have emerged as prominent area of research. Among them, LLaDA(Nie et al., 2025) stands out as the first large-scale diffusion model trained from scratch to reach the billion-parameter scale. Trained on 2.3 trillion tokens, LLaDA-8B has demonstrated performance competitive with state-of-the-art autoregressive models like LLaMA-3-8B(Grattafiori et al., 2024) on multiple tasks(Hendrycks et al., 2020; Suzgun et al., 2022; Cobbe et al., 2021), proving the remarkable scalability and potential of the native diffusion architecture. Subsequently, LLaDA-1.5(Zhu et al., 2025) advanced this paradigm by successfully applying reinforcement learning for preference alignment, achieving significant further improvements on benchmarks for mathematics, code, and alignment. In contrast to LLaDA, another line of research has explored adapting existing AR LLMs into DLLMs. For instance, models like DiffuLLaMA(Gong et al., 2024) and Dream(Ye et al., 2025) were developed by fine-tuning pre-trained AR LLMs such as GPT2(Radford et al., 2019), LLaMA2(Touvron et al., 2023) and Qwen(Yang et al., 2024). While these adapted models have also achieved strong results, our work focuses on native, from-scratch DLLMs like LLaDA, seeking to explore their generation mechanisms and address the specific challenge of fixed-length inference. Inference Strategies for DLLMs. Existing research on inference strategies for DLLMs has predominantly focused on enhancing generation speed through computational optimizations. For example, Fast-dLLM(Wu et al., 2025) introduces novel block-wise approximate Key-Value (KV) Cache tailored for bidirectional attention models, combined with confidence-aware parallel decoding strategy, to achieve up to an improvement in throughput. Similarly, dLLM-Cache(Liu et al., 2025) observes the static nature of prompts and the dynamic sparsity of responses during DLLM inference, proposing an adaptive caching framework that combines long-interval prompt caching with partial response updates to achieve lossless speedup. Dimple(Yu et al., 2025) proposes Confident Decoding strategy that dynamically adjusts the number of tokens generated at each step based on model confidence, thereby significantly reducing the total number of iterations. In summary, while all these methods(Ma et al., 2025; Israel et al., 2025; Ben-Hamu et al., 2025) have made significant strides in improving the inference speed of DLLMs via computational caching and parallel decoding. They do not address the more fundamental issue that the generation length itself needs to adapt dynamically to different task requirements. To our knowledge, the problem of dynamically adjusting and expanding the total generation length of DLLMs at inference time remains unexplored. Our work, therefore, aims to fill this critical gap by proposing novel dynamic adaptive expansion strategy. 3 Figure 3: Inference process of Fixed-Length Denoising (Baseline) and DAEDAL. (a) The standard inference process for current DLLMs, which performs iterative denoising on sequence of predefined, static length. (b) Our proposed two-stage inference process, which first employs Initial Length Adjustment to determine an appropriate generation length before denoising, followed by Iterative Mask Insertion to expand the sequence on-demand during the denoising process."
        },
        {
            "title": "3.1 OVERVIEW OF DIFFUSION LARGE LANGUAGE MODELS",
            "content": "Training. The training of Diffusion Language Model aims to define and learn model distribution pθ(x0) that approximates the true data distribution. This is achieved through forward and reverse probabilistic process(Austin et al., 2021a; Ou et al., 2024). The forward process defines fixed data noising mechanism indexed by continuous time variable [0, 1]. It progressively masks an original sequence x0 until it is fully masked at = 1. For any given t, noised version xt is generated by independently replacing each token in x0 with [MASK] token with probability t, while keeping it unchanged with probability 1 t. The reverse process is where learning occurs. Transformer model, parameterized by θ, is trained to reverse the forward process by learning to predict the original sequence x0 from its noised version xt. This is achieved by optimizing the model to minimize cross-entropy loss computed only on the masked token positions. This objective is principled as it corresponds to maximizing the Evidence Lower Bound (ELBO) on the datas log-likelihood, thereby pushing the model distribution pθ to approximate the true data distribution. Inference. During inference, LLaDA employs multi-step iterative denoising process to generate text. As illustrated in Figure 3(a), this process begins with sequence of length composed entirely of [MASK] tokens, where is predefined hyperparameter. In each denoising step t, the model 4 takes the current sequence xt as input and predicts the tokens for all masked positions, yielding an estimate ˆx0 of the complete, clean sequence. Subsequently, remasking strategy is applied to determine which tokens are finalized and which are re-masked for the next denoising step, 1. LLaDA demonstrates the effectiveness of low-confidence remasking strategy(Nie et al., 2025; Chang et al., 2022), where tokens predicted with the highest confidence are kept, while those with low confidence are re-masked for further refinement in subsequent steps. This iterative process continues for fixed number of steps until the final text sequence is generated. This standard inference pipeline exposes core problem: the generation length must be statically specified before inference begins, preventing it from adapting to the actual requirements of the task."
        },
        {
            "title": "3.2 DAEDAL",
            "content": "To address the static length limitation of standard DLLM inference, we introduce DAEDAL, training-free, two-stage strategy. This approach allows the model to allocate an appropriate sequence length for each task and to insert additional space for reasoning where needed. detailed algorithmic description of this process is provided in Algorithm 1. 3.2."
        },
        {
            "title": "INITIAL LENGTH ADJUSTMENT",
            "content": "To overcome the limitation of static generation length, we introduce the first stage of DAEDAL, Initial Length Adjustment. The core insight of this stage is that the models confidence in generating an End-of-Sequence (EOS) token at the end of the sequence can be interpreted as an internal signal of whether the current token length is sufficient. To validate this insight, we visualize the models behavior in Figure 2. Specifically, we measure the average EOS confidence at the sequence terminus after the first prediction, when the predefined generation length is 128. We then compare this confidence between two empirically defined groups of problems: those answered correctly in under 128 tokens (length-sufficient) and those answered correctly only with much longer sequence (lengthinsufficient). The visualization clearly shows that the model predicts EOS tokens with significantly higher confidence for the length-sufficient problems, indicating an awareness that the current length is adequate. Conversely, for problems where the length is insufficient, the model utilizes the available space more thoroughly, resulting in lower EOS confidence at the sequences terminus. If the model deems the current length inadequate to fully articulate its response, it will tend to utilize all available space, making it less likely to generate EOS tokens with high confidence. Based on this insight, we introduce preliminary length estimation loop that precedes the main denoising process. As depicted in Figure 3(b), this loop begins with short initial generation length. In each estimation iteration, the model performs forward pass on the current sequence (prompt plus [MASK] tokens) to evaluate its predictions. We specifically focus on window at the end of the sequence and calculate the models average confidence in predicting the EOS token in these positions. If this confidence falls below predefined threshold, we interpret this as length insufficient signal. This indicates that the model, being forced to conclude prematurely, has not yet formed complete response and is therefore unwilling to commit to an EOS token. In response, we expand the generation length by appending additional [MASK] tokens to the end of the sequence. This length adjustment loop repeats until the EOS confidence surpasses the threshold or maximum length limit is reached. Through this mechanism, DAEDAL dynamically allocates task-appropriate generation length for the model before commencing the fine-grained denoising process. 3.2."
        },
        {
            "title": "ITERATIVE MASK INSERTION",
            "content": "After the first stage allocates an appropriate length for the task, the second stage of DAEDAL, Iterative Mask Insertion, further enhances generation flexibility during the denoising process. We propose that predictions with exceptionally low confidence are not merely signals of uncertainty; on deeper level, they indicate that the local context is too constrained to articulate complex thought or logical step. In other words, this is signal that the model requires more discursive space to refine its reasoning. Therefore, during each denoising step, in addition to identifying and filling high-confidence tokens, our method also flags the masked position with the lowest prediction confidence, provided it falls below very low threshold. As shown in Figure 3(b), this position is not treated as merely difficult token but is marked as an expansion point. When position is marked for expansion, instead 5 Algorithm 1 The DAEDAL Inference Process 1: Input: Prompt c, model fθ, initial/max length Linit/Lmax, thresholds τeos, τhigh, τlow, τexpand, expansion factor Ef actor, EOS confidence window size Weos 2: Output: Generated sequence 3: [c, [MASK], . . . , [MASK] ] (cid:125) (cid:124) (cid:123)(cid:122) Linit Stage 1: Initial Length Adjustment Initialize sequence 4: while length(x) < Lmax do Llogits fθ(x) 5: confeos ComputeEOSConfidence(Llogits, x, Weos) 6: if confeos < τeos then 7: 8: [x, [MASK], . . . , [MASK] ] (cid:125) (cid:124) (cid:123)(cid:122) Ef actor Expand length if EOS confidence is low else 9: 10: end if 11: 12: end while break length is sufficient, exit loop Stage 2: Iterative Denoising and Mask Insertion 13: while ContainsMask(x) do Llogits fθ(x) 14: Pconf, ˆx GetConfidenceAndPredictions(Llogits) 15: 16: Mmasked {i xi = [MASK]} 17: 18: 19: 20: 21: 22: 23: 24: 25: end for confeos ComputeEOSConfidence(Llogits, x, Weos) if confeos < τexpand length(x) < Lmax Icandidates > 0 then Ifill {i Mmasked Pconf,i > τhigh} Identify high-confidence set for filling Icandidates {i Mmasked Pconf,i < τlow} Find low-confidence candidate positions for Ifill do xj ˆxj Fill in high-confidence tokens Select the position with the lowest confidence iexpand arg miniIcandidates Pconf,i Replace xiexpand with [[MASK], . . . , [MASK] ] Expand sequence at the selected position (cid:125) (cid:124) (cid:123)(cid:122) Ef actor end if 26: 27: end while 28: return of simply remasking it, we dynamically replace the single [MASK] token with block of multiple [MASK] tokens. This operation effectively inserts additional space into the sequence. This mechanism provides the model with breathing room precisely where complex reasoning or detailed description is needed, allowing it to better structure its language and logic in subsequent denoising iterations. Unlike the first stage, which performs holistic length adjustment, Iterative Mask Insertion is localized, on-demand refinement that occurs in real-time during generation. This enables DAEDAL to handle complex scenarios where the required length exceeds the initial length estimate, significantly enhancing the models expressive ability."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "We utilize LLaDA-Instruct-8B and LLaDA-1.5-8B as our baseline models. To ensure fairness and reproducibility, all experiments are conducted using the official generation code released with LLaDA, without any acceleration or caching optimizations proposed in subsequent works. All experiments were conducted on server equipped with 8 NVIDIA A800 80G GPUs, with the batch size set to 8. 6 Table 1: Main Results of DAEDAL on LLaDA-Instruct-8B. We compare the baseline performance at various generation lengths (64 to 2048) against DAEDAL. Acc denotes accuracy, Etoken is the average effective tokens (the response length excluding trailing padding), Ntoken is the average total tokens, and Eratio is the effective token ratio. The best configuration for the baseline is highlighted in orange. The best results are bold and underlined, and the second-best results are underlined. Benchmark Metric Fixed-Length Denoising (Baseline) DAEDAL 64 128 256 512 2048 64 GSM8K MATH500 MBPP HUMANEVAL Acc Etoken Ntoken Eratio Acc Etoken Ntoken Eratio Acc Etoken Ntoken Eratio Acc Etoken Ntoken Eratio 48.0 62 64 82.6 294 2048 97.1% 97.0% 91.2% 56.0% 27.7% 14.4% 83.8 284 1024 67.9 124 128 77.6 234 256 83.3 287 512 24.0 62 64 39.6 718 2048 96.4% 96.4% 95.8% 82.8% 56.9% 35.1% 39.4 583 1024 29.0 123 128 35.6 245 256 38.8 424 512 20.8 61 64 38.8 336 2048 95.1% 95.7% 90.6% 64.7% 32.7% 16.4% 37.4 335 1024 38.2 331 512 28.0 122 128 37.4 232 256 19.5 59 64 39.6 695 2048 92.8% 97.0% 96.0% 92.0% 63.3% 33.9% 46.3 649 1024 37.8 246 256 46.3 471 512 22.6 124 128 Average Acc 28. 36.88 47.10 51.65 51.73 50.15 85.8 267 363 73.5% 44.2 541 704 76.8% 40.8 324 618 52.5% 48.2 523 813 64.3% 54."
        },
        {
            "title": "4.2 BENCHMARKS AND METRICS",
            "content": "To comprehensively evaluate the effectiveness of DAEDAL, we conducted experiments on four benchmarks spanning the domains of mathematical reasoning and code generation. For mathematical reasoning, we utilize GSM8K(Cobbe et al., 2021), which consists of grade-school math word problems to assess multi-step reasoning, and the more challenging MATH500(Lightman et al., 2023) benchmark, composed of competition-level mathematics problems; performance on both is measured by accuracy. To evaluate code generation, we employ MBPP(Austin et al., 2021b) benchmark for entry-level Python tasks and the more complex, handwritten HumanEval(Chen et al., 2021) benchmark to test program synthesis capabilities. For these code generation tasks, we report the pass@1 metric to assess the correctness of the generated code in single attempt."
        },
        {
            "title": "4.3 MAIN RESULTS",
            "content": "We conducted comprehensive evaluation on four benchmarks. The results for LLaDA-Instruct-8B and LLaDA-1.5-8B, comparing the Fixed-Length Denoising baseline against our DAEDAL method, are presented in Table 1 and Table 2, respectively. For the baseline models, performance is highly dependent on manually tuning the generation length for each specific task. We therefore report baseline performance across six fixed-length configurations, from 64 to 2048 tokens. In addition to accuracy (Acc), we introduce three key metrics: the total number of tokens generated (Ntoken), which for the baseline is its preset fixed length; the number of effective tokens used to answer the question (Etoken), representing the net response length after removing trailing EOS padding; and the effective token ratio (Eratio). DAEDAL achieves strong performance with unified initial length. The results clearly demonstrate the superior performance of DAEDAL. Despite starting with short initial length by default, DAEDALs two-stage length adjustment and expansion mechanism allows it to not only significantly 7 Figure 4: Distribution of individual Response Lengths (Ntoken) on LLaDA-Instruct-8B. The figure compares the distribution of total tokens used per problem by DAEDAL (orange histogram) and the baseline (blue histogram) across four benchmarks. DAEDALs dynamic, per-problem adaptation results in varied distribution of lengths. In contrast, the baseline is constrained to single fixed length for all problems within benchmark, represented by single bar in its histogram. outperform baselines with the same initial length, but also achieve performance that is comparable, and in some cases superior, to the peak performance of the meticulously tuned fixed-length baseline. This finding highlights the effectiveness of DAEDAL and exposes the inherent impracticality of the fixed-length paradigm, as the optimal length for the baseline varies across different benchmarks, underscoring the necessity of dynamic length adaptation. To visually illustrate this dynamic adaptability, Figure 4 contrasts the distribution of total generation lengths (Ntoken) used by DAEDAL against the single fixed length of the best-performing baseline for each benchmark. Unlike the baseline, which is constrained to single, pre-defined length for all problems, DAEDAL automatically adapts its generation length on per-problem basis, resulting in diverse length distribution that reflects varying task complexity. DAEDAL adaptively finds the optimal generation length. Further analysis reveals that DAEDAL intelligently estimates and generates responses of an appropriate length. In most cases, the number of effective tokens (Etoken) produced by DAEDAL is comparable to that of the baselines bestperforming configuration. This suggests that DAEDAL adaptively finds the models inherent sweet spot for the token length required by given task. The baselines behavior corroborates this: when the fixed length is set beyond its optimal point, performance degrades even though the number of effective tokens may continue to increase. DAEDALs adaptive nature effectively avoids this performance decay from over-expansion. DAEDAL significantly improves computational efficiency. Furthermore, DAEDAL offers significant efficiency advantages. While achieving superior accuracy, the total number of tokens (Ntoken) generated by DAEDAL is generally lower than that of the baseline at its peak-performance setting. similar effective token count with lower total token count results in much higher effective token ratio (Eratio). This dramatically improves the utilization of the computational resource by reducing the overhead of bidirectional attention on unnecessarily long sequences and minimizing wasted resources on generating meaningless padding tokens. In summary, our results demonstrate that DAEDAL, through its Initial Length Adjustment and Iterative Mask Insertion, not only achieves performance comparable, and at times superior, to meticulously tuned fixed-length baselines across multiple benchmarks but also adaptively allocates an appropriate length for each task. This leads to substantial gains in both model performance and computational efficiency."
        },
        {
            "title": "4.4 ANALYSIS ON DAEDAL",
            "content": "DAEDALs two stages are individually effective and synergistic when combined. As shown in Table 3, we conducted an ablation study to analyze the individual contributions of DAEDALs two 8 Table 2: Main Results of DAEDAL on LLaDA-1.5-8B. We compare the baseline performance at various generation lengths (64 to 2048) against DAEDAL. Acc denotes accuracy, Etoken is the average effective tokens (the response length excluding trailing padding), Ntoken is the average total tokens, and Eratio is the effective token ratio. The best configuration for the baseline is highlighted in orange. The best results are bold and underlined, and the second-best results are underlined. Benchmark Metric Fixed-Length Denoising (Baseline) DAEDAL 64 128 256 512 2048 64 GSM8K MATH500 MBPP HUMANEVAL Acc Etoken Ntoken Eratio Acc Etoken Ntoken Eratio Acc Etoken Ntoken Eratio Acc Etoken Ntoken Eratio 49.4 62 64 84.5 292.5 2048 97.4% 97.5% 92.6% 57.2% 28.1% 4.3% 83.8 287 1024 71.0 125 128 80.4 237 256 83.7 293 512 23.2 62 64 40.2 717 2048 96.9% 97.5% 96.2% 83.7% 57.1% 35.0% 43.6 584 1024 29.6 125 128 35.4 246 256 40.2 429 512 20.6 61 64 39.6 356 2048 95.1% 96.6% 92.7% 68.7% 33.6% 17.4% 39.8 344 1024 30.2 124 128 39.2 237 256 38.6 352 512 18.3 60 64 50.0 754 2048 94.0% 97.7% 98.2% 92.9% 66.1% 36.8% 49.4 677 1024 37.8 251 256 45.1 475 512 22.0 125 128 Average Acc 27. 38.20 48.20 51.90 54.15 53.58 85.5 275 377 73.0% 42.4 588 743 75.2% 40.2 342 645 53.0% 48.8 561 848 66.2% 54.23 core components: Initial Length Adjustment (Stage 1 only) and Iterative Mask Insertion (Stage 2 only). The results indicate that applying either stage individually yields significant performance gains over the baseline. The full DAEDAL method, which combines both stages, ultimately achieves the best performance, surpassing the results of using either stage alone. This demonstrates that the two stages are complementary and indispensable for achieving optimal results. DAEDALs stages highlights the importance of the initial length for global planning. When using the Iterative Mask Insertion (Stage 2) stage in isolation, we observe that its performance is sensitive to the initial length. When starting from very short initial length (e.g., 64), its performance, while substantially better than the baseline at that same length, still falls short of the baselines peak performance achieved at an optimal, longer length. Yet, when initiated with more reasonable length (e.g., 256), this stage alone can surpass the baselines overall best result. This behavior is expected and highlights the nature of the second stage as local, on-demand expansion mechanism. If the initial length is severely constrained, the DLLMs ability to form sound global plan for the solution is compromised from the outset. While subsequent local expansions can provide remedies, the overall performance ceiling is still limited by the flawed initial plan. This not only demonstrates the effectiveness of Iterative Mask Insertion but also underscores the necessity of Initial Length Adjustment (Stage 1) for establishing solid foundation for global planning. DAEDAL exhibits strong robustness to the initial length. core advantage of DAEDAL is its ability to achieve strong performance from short, unified initial length. We conduct an ablation study to verify its robustness to this hyperparameter, Linit. As shown in Table 4, DAEDAL delivers remarkably stable performance across wide range of initial lengths, from 32 to 512 tokens. On HumanEval, the accuracy remains identical across all settings, while the variation on GSM8K is negligible. This result demonstrates that DAEDAL is largely insensitive to the initial length setting. It confirms that users do not need to meticulously tune this hyperparameter; unified and short initial length (e.g., 64) is sufficient to achieve optimal performance, fulfilling its original design objective. 9 Table 3: Ablation Results of DAEDALs Two Stages. Experiments are conducted on GSM8K with LLaDA-Instruct-8B. We compare the performance of the full DAEDAL method, as well as its individual stages (Stage 1 and Stage 2), against the baseline. The baseline is evaluated at multiple fixed lengths, with its best configuration highlighted in orange. Stage 1 and DAEDAL evaluated with an initial length of 64, while Stage 2 is evaluated with varying initial lengths (64, 128, 256). Metric Acc Etoken Ntoken Eratio Fixed-Length Denoising (Baseline) w/ Stage1 w/ Stage2 DAEDAL 64 128 512 1024 2048 64 64 256 64 48.0 62 64 82.6 294 2048 97.1% 97.0% 91.2% 56.0% 27.7% 14.4% 83.8 284 1024 83.3 287 67.9 124 128 77.6 234 256 84.1 253 311 81.3% 72.3 127 152 81.1 167 209 83.1% 79.7% 75.3% 84.7 256 85.8 267 363 73.5% Table 4: Ablation Results on DAEDALs Initial Length. Experiments are conducted on GSM8K and HUMANEVAL using LLaDA-Instruct-8B. DAEDAL is evaluated with initial lengths ranging from 32 to 512. We highlight our default setting (Linit = 64) in blue. Metric Acc Etoken Ntoken Eratio GSM8K HUMANEVAL 32 85.8 267 32 64 85.8 267 64 128 85.8 267 256 85.8 268 256 512 85.1 277 512 32 48.2 523 64 48.2 523 64 128 48.2 523 128 256 48.2 523 512 48.2 523 512 73.5% 73.5% 73.5% 72.7% 52.0% 64.4% 64.4% 64.4% 64.4% 64.1% Table 5: Ablation Results on DAEDALs Expansion Factor and EOS Confidence Window Size. Both ablation studies are conducted on GSM8K using LLaDA-Instruct-8B. The left panel shows results for varying Ef actor ranging from 8 to 32, and the right panel for varying Weos ranging from 8 to 32. We highlight our default setting (Ef actor = 8 and Weos=32) in blue. Metric Acc Etoken Ntoken Eratio Expansion Factor (Ef actor) 8 85.8 267 363 16 85.8 272 386 86.4 272 392 32 86.3 274 405 73.5% 70.5% 69.3% 67.6% Metric Acc Etoken Ntoken Eratio EOS Confidence Window Size (Weos) 8 82.9 232 289 80.2% 16 83.5 252 327 77.1% 84.4 260 347 75.0% 32 85.8 267 363 73.5% DAEDALs performance is insensitive to the expansion factor granularity. We conduct an ablation study on the expansion factor, which controls the number of [MASK] tokens added during single expansion event. As shown in the left panel of Table 5, DAEDALs performance remains remarkably stable across range of expansion factors (8 to 32). This result suggests that the specific granularity of each expansion step is not critical. smaller factor leads to more frequent, fine-grained expansions, while larger factor results in fewer, coarser expansions. Regardless of the step size, the model robustly converges to similar, task-appropriate total length. This finding further corroborates our core hypothesis that the model possesses strong internal estimate of the length required for given problem, and DAEDALs mechanism effectively enables the model to reach that target. DAEDAL is robust to the EOS confidence window size, achieving optimal performance with large window. We also analyze the sensitivity to the EOS confidence window size, used to determine length sufficiency. As shown in the right panel of Table 5, performance is stable for larger window sizes but degrades for very small values (e.g., 8). Notably, even this sub-optimal configuration still yields significant gains over the baseline at comparable short initial length (77.3 vs. 48.0 Acc on GSM8K when the baseline starts at Linit=64). We attribute the performance drop at small window sizes to higher probability of misjudging the models intent. larger window provides Figure 5: Ablation Results on DAEDALs Thresholds. The two 4x4 heatmaps present grid search over two interdependent threshold pairs: (τhigh, τlow) and (τeos, τexpand). All 32 configurations were evaluated on GSM8K using LLaDA-Instruct-8B. Higher accuracy is indicated by darker green. The color bar also provides reference color for performance of baseline. Our default settings are in blue boxes. The results demonstrate remarkable stability, with all configurations comparable to the best-performing baseline, and some even outperforming it. more robust signal by averaging confidence over wider context. In contrast, small window is susceptible to localized fluctuations and may prematurely terminate length expansion based on few high-confidence EOS tokens at the sequences immediate end, leading to length under-allocation and drop in final performance. DAEDAL demonstrates broad robustness across its threshold settings. We conduct comprehensive ablation study on all the four key threshold hyperparameters in DAEDAL: τeos, τexpand, τhigh, τlow. We analyze these in interdependent pairs via grid search: (τhigh, τlow), which govern token-level filling and expansion decisions. Specifically, the high-confidence threshold τhighcis analogous to the confident decoding strategy proposed in Dimple, which aims to accelerate inference by simultaneously filling all tokens that exceed certain confidence level. The second pair, (τeos, τexpand), controls sequence-level length adjustments. The results on GSM8K, presented in Figure 5, demonstrate DAEDALs exceptional robustness. Across the 32 tested configurations, all configurations are comparable to the best-performing baseline (83.8 Acc), with some even outperforming it, and the overall performance variation across all settings is minimal. This indicates that DAEDAL is largely insensitive to the precise choice of these thresholds, confirming that it can deliver strong and stable performance without requiring extensive hyperparameter tuning."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we addressed fundamental architectural constraint of Diffusion Large Language Models: the reliance on statically predefined generation length. This limitation hinders their practical application by creating difficult dilemma: insufficient lengths cripple performance on complex tasks, while excessive lengths not only incur significant computational overhead but can sometimes lead to performance degradation. We introduced DAEDAL, novel training-free, two-stage denoising strategy that resolves this issue by leveraging the models own internal signals. DAEDAL first performs an Initial Length Adjustment to set coarse, task-appropriate budget, and then uses Iterative Mask Insertion to dynamically expand the sequence at regions requiring more detailed reasoning during the denoising process. Our extensive experiments and analyses demonstrate that DAEDAL successfully endows DLLMs with the ability for dynamic, per-problem length adaptation. This allows model starting from short, unified initial length to achieve performance that is comparable, and at times superior, to that of meticulously tuned fixed-length baselines. By removing the need for manual length tuning and enabling the model to find its own optimal response length, DAEDAL not only enhances performance but also improves computational efficiency. Ultimately, this work bridges critical capability gap between diffusion and autoregressive models, paving the way for more flexible, efficient, and capable non-autoregressive language generation."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021a. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021b. Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, and Brian Karrer. Accelerated sampling from masked diffusion models via entropy bounded unmasking. arXiv preprint arXiv:2505.24857, 2025. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168, 9, 2021. DeepMind. Gemini diffusion, 2025. URL https://deepmind.google/models/ gemini-diffusion/. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, Aditya Grover, and Volodymyr Kuleshov. Mercury: Ultra-fast language models based on diffusion, 2025. URL https://inceptionlabs.ai. Daniel Israel, Guy Van den Broeck, and Aditya Grover. Accelerating diffusion llms via adaptive parallel decoding. arXiv preprint arXiv:2506.00413, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. 12 Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, JiRong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Darsh Shah, Peter Rushton, Somanshu Singla, Mohit Parmar, Kurt Smith, Yash Vanjani, Ashish Vaswani, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, et al. Rethinking reflection in pretraining. arXiv preprint arXiv:2504.04022, 2025. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023. URL https://arxiv. org/abs/2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. URL https://hkunlp.github.io/blog/2025/dream. Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong"
    ]
}