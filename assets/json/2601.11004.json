{
    "paper_title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems",
    "authors": [
        "Jiayu Liu",
        "Rui Wang",
        "Qing Zong",
        "Qingcheng Zeng",
        "Tianshi Zheng",
        "Haochen Shi",
        "Dadi Guo",
        "Baixuan Xu",
        "Chunyang Li",
        "Yangqiu Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs."
        },
        {
            "title": "Start",
            "content": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems Jiayu Liu1 Rui Wang1 Qing Zong1 Qingcheng Zeng2 Tianshi Zheng1 Haochen Shi1 Dadi Guo1 Baixuan Xu1 Chunyang Li1 Yangqiu Song1 1HKUST 2Northwestern University jliufv@connect.ust.hk yqsong@cse.ust.hk 6 2 0 2 J 6 1 ] . [ 1 4 0 0 1 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the models false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide principled foundation for resolving overconfidence under noise. We further design NAACL, noiseaware calibration framework that synthesizes supervision from 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have exhibited remarkable performance in mathematic reasoning (DeepSeek-AI et al., 2025; Zheng et al., 2025; Yu et al., 2025), code generation (Chen et al., 2025; Zeng et al., 2025; Liu et al., 2024c) and complex problem solving (Wang et al., 2025b; Yuan et al., 2025). Despite their impressive performance, their reliability in high-stakes and fact-intensive question answering remains limited due to persistent hallucinations (Zhang et al., 2024; Zong et al., 2025b; 1*Core contributors. 2Code: https://github.com/HKUST-KnowComp/NAACL. Figure 1: An illustrative example of model responses before and after NAACL. By explicitly training the model to assess passageand group-level utility prior to answering, NAACL enables more reliable confidence expression under noisy retrieval, as reflected by consistently reduced ECE. The performance plots report results on NQ for Llama-3.1-8B-Instruct and DeepSeek-R1-DistillLlama-8B, where SFT corresponds to the Label-only SFT setting in Table 2, and illustrate how NAACL promotes more transparent and grounded humancomputer interaction in real-world scenarios. Bang et al., 2025). Thus, retrieval-augmented generation (RAG) is widely adopted (Dong et al., 2025; Hu et al., 2025; Sun et al., 2025b) to alleviate this issue by incorporating external knowledge. However, LLM generation with RAG is not always accurate or complete, and LLMs may still produce hallucinations even with access to additional information (Sorodoc et al., 2025; Wasserman et al., 2025; Jin et al., 2025). This makes uncertainty estimation crucial and essential for assessing the reliability of LLM outputs in RAG scenarios. Among existing uncertainty estimation approaches, verbal confidence calibration has emerged as lightweight mechanism for assessing the reliability of LLM outputs (Wei et al., 2025; Ou et al., 2025; Zong et al., 2025a), while also enhancing model trustworthiness (Xu et al., 2024; Zong et al., 2025a) and facilitating efficient and trustworthy humanAI interaction (Li et al., 2025a,c). However, how LLM verbal confidence calibrates under RAG remains largely unexplored. Existing confidence estimation methods remain insufficient for real-world RAG systems. While recent works enable verbal confidence via reinforcement learning (Damani et al., 2025; Xu et al., 2024) or specialized loss (Li et al., 2025c; Stengel-Eskin et al., 2024), they target internal reasoning in closed-book settings, overlooking the external uncertainty introduced by retrieval contents. Prior attempts to quantify uncertainty in RAG rely on white-box signals (Zhou et al., 2025a; Ozaki et al., 2025) or sampling consistency (Wang et al., 2025a; Soudani et al., 2025c), which suffer from inaccessibility in proprietary models or computational costs. To address this gap, we conduct systematic study of verbal confidence calibration under RAG. We first empirically demonstrate that verbal confidence exhibits poor calibration in RAG settings, with the average Expected Calibration Error (ECE) exceeding 0.4 across all four datasets. Our analysis further identifies noisy retrieval as key factor undermining calibration, particularly in the presence of irrelevant and counterfactual passages. To this end, we introduce NAACL Rules to regularize model behavior under noise. Guided by these rules, we propose NAACL, framework that bootstraps the model using approximately 2K data points without relying on external teacher models. Extensive experiments demonstrate that NAACL yields substantial gains, improving calibration by 10.9% indomain and 8.0% out-of-distribution, highlighting its robust effectiveness and generalizability. Moreover, NAACL improves models ability to discern passage utility and enhances interpretability by grounding confidence estimates in structured intermediate judgments within explicit reasoning traces. By bridging the gap between verbal calibration and retrieval noise, our approach constitutes foundational and principled step toward building robust and interpretable RAG systems."
        },
        {
            "title": "2 Related Work",
            "content": "or logits (Duan et al., 2024; Kuhn et al., 2023), yet their dependence on internal states restricts their utility for proprietary models. Consequently, blackbox research has evolved from sampling consistency (Lin et al., 2024; Chen and Mueller, 2024) or verbalized confidence scores (Lin et al., 2022; Damani et al., 2025; Yang et al., 2024) to optimizing verbalized confidence via reinforcement learning (Damani et al., 2025; Xu et al., 2024), specialized loss functions (Li et al., 2025c; Stengel-Eskin et al., 2024), or teacher-distilled critiques (Zong et al., 2025a). However, unlike these works that focus on closed-book tasks and often incur high computational costs, our approach addresses the unique external uncertainty from retrieval noise in RAG via efficient self-bootstrapping without requiring reinforcement learning or superior teachers. Uncertainty Quantification for RAG. Reliability is paramount for RAG systems. Prior works primarily utilize white-box metrics in domain-specific settings (Zhou et al., 2025a; Ozaki et al., 2025) or consistency sampling (Wang et al., 2025a; Soudani et al., 2025c). While Soudani et al. (2025a) recently formalized the theoretical limitations of these standard estimators, existing approaches remain predominantly dependent on costly test-time sampling or external calibration functions, rather than optimizing the models intrinsic uncertainty. Prior studies also focus on refusal rates (Zhou et al., 2025b), leaving verbal confidence largely uncalibrated. We address this gap with training-based framework that directly calibrates verbal confidence. Retrieval Noise and Robustness. Recent work highlights the harm of retrieval noise (irrelevant, counterfactual, repetitive, etc.) on accuracy (Wu et al., 2025; Cuconasu et al., 2024; Guo et al., 2025). Although frameworks like Self-RAG (Asai et al., 2024), RAAT (Fang et al., 2024a), and HANRAG (Sun et al., 2025a) enhance robustness, they neglect the correlation between noise and confidence calibration. Our work addresses this by modeling noise awareness for reliable confidence estimation."
        },
        {
            "title": "3 Task Formalization",
            "content": "Confidence estimation in LLMs. Confidence estimation approaches for LLMs are generally categorized into white-box and black-box methods (Fadeeva et al., 2023; Geng et al., 2024a). White-box methods utilize internal signals like perplexity (Fomicheva et al., 2020; Zong et al., 2025b) Task Setup. Let denote query, and let = {d1, . . . , dD} be document corpus. retriever maps query and the corpus to set of passages: = R(q, D) = {p1, . . . , pk}. (1) retrieval-augmented language model fθ takes the query and the retrieved passages as input, and outputs both predicted answer ˆa and scalar verbal confidence score ˆc [0%, 100%]: but do not provide sufficient information to answer the query; and (4) Irrelevant passages Pirr, which contain no semantic overlap with the query. (ˆa, ˆc) = fθ(q, R(q, D)) . (2)"
        },
        {
            "title": "4 Experiment",
            "content": "Answer Correctness and Calibration Objective. Given the ground-truth answer a, we define the answer correctness indicator as: = I[ˆa = a]. (3) The objective of verbal confidence calibration in retrieval-augmented generation (RAG) is to align the model-reported confidence ˆc with the empirical correctness y. While this objective is identical to that in the non-RAG setting, it must additionally account for uncertainty introduced by the retrieval process. Calibration Metrics. Calibration quality is evaluated using Expected Calibration Error (ECE) and the Area Under the Receiver Operating Characteristic Curve (AUROC). Expected Calibration Error (ECE) measures the average discrepancy between predicted confidence and empirical accuracy: ECE = (cid:88) m=1 Bm acc(Bm) conf(Bm) , (4) where denotes the total number of samples and the number of confidence bins. Following standard practice (Guo et al., 2017a), we set = 10 in all experiments. Here, Bm denotes the set of samples assigned to the m-th bin, while acc(Bm) and conf(Bm) represent the average accuracy and confidence within that bin, respectively. AUROC evaluates the discriminative ability of confidence scores to distinguish correct from incorrect predictions: AUROC = Pr(cid:0)ˆc+ > ˆc(cid:1) , (5) where ˆc+ and ˆc denote confidence scores associated with correct and incorrect predictions, respectively. Higher AUROC values indicate stronger discriminative power. Passage Categorization. Following the categorization scheme introduced in prior work (Cuconasu et al., 2024; Wu et al., 2025), given the ground-truth answer a, we categorize the retrieved passages into four disjoint subsets based on their semantic content: (1) Gold passages Pgold, which support the correct answer a; (2) Counterfactual passages Pcf, which support an incorrect answer = a; (3) Relevant passages Prel, which are topically related to (e.g., sharing entities or relations) Models. We use four widely-used open-sourced LLMs to conduct our experiment (detailed list provided in Appendix A.1). Proprietary models are excluded as they do not support the access to internal states or fine-tuning required for further alignment. Datasets. We adopt randomly selected subset of Natural Questions (NQ) (Kwiatkowski et al., 2019), Bamboogle (Press et al., 2023), StrategyQA (Geva et al., 2021) and HotpotQA (Yang et al., 2018), and as our primary evaluation benchmark. The finegrained data statistics (number of data instances, etc.) is provided in Appendix A.3. Prompts. The retrieval-augmented language model fθ is instantiated using Chain-of-Thought (CoT) prompting (Wei et al., 2022) for all experiments unless otherwise specified. Due to the instability of verbal confidence (Obadinma and Zhu, 2025; Liu et al., 2025b), we add extra experiments (see Appendix B.2) using prompts from Xiong et al. (2024) to verify the robustness of our conclusions. The exact prompts are discussed in Appendix A.4.1, and additional results are in Appendix B.2. RAG Settings. We use wikimedia/wikipedia (Wikimedia, 2023) from the HuggingFace dataset (Wolf et al., 2020) as the RAG corpus D. For the retriever R, we follow Soudani et al. (2025b) and use BM25 (Robertson et al., 2004) and Contriever (Izacard et al., 2021) to retrieve top-k passages (k = 3 in Table 1). To mitigate the position bias of retrieved passages noted in Liu et al. (2024b); Ozaki et al. (2025), we randomize their order in the input context fed to fθ. We further provide robustness checks on passage positioning in Appendix B.1. Detailed experimental settings are in Appendix A, with additional RAG details in Appendix A.5."
        },
        {
            "title": "5 Analysis",
            "content": "Models fail in calibrating in RAG scenarios. As shown in Table 1, we observe that models exhibit severely degraded verbal calibration performance in real-world RAG settings. Across different datasets and retrievers, all four models consistently demonstrate poor alignment between verbal Method BM25 (CoT) Contriever (CoT) BM25 (CoT) Contriever (CoT) BM25 (CoT) Contriever (CoT) BM25 (CoT) Contriever (CoT) StrategyQA HotpotQA NQ Bamboogle Average ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC 0.217 0.228 0.243 0.231 0.246 0. 0.292 0.292 0.480 0.455 0.562 0.564 0.574 0.572 0.539 0.539 0.538 0. 0.450 0.570 0.459 0.527 0.583 0.644 Llama-3.1-8B-Instruct 0.548 0.417 0.416 0.446 Qwen2.5-7B-Instruct 0.726 0.625 0.439 0.553 0.648 0.558 0.742 0.699 DeepSeek-R1-Distill-Llama-8B 0.628 0. 0.461 0.477 0.707 0.754 DeepSeek-R1-Distill-Qwen-7B 0.591 0.455 0.560 0.600 0.686 0. 0.613 0.629 0.628 0.632 0.521 0.557 0.756 0.773 0.452 0.445 0.599 0. 0.754 0.714 0.584 0.632 0.446 0.491 0.440 0.496 0.422 0.449 0.548 0. 0.532 0.469 0.657 0.646 0.666 0.664 0.600 0.587 Table 1: Real-world RAG results across four datasets demonstrate consistently poor calibration. Notably, average ECE exceeds 0.4, highlighting severe misalignment between verbal confidence and empirical correctness. confidence ˆc and empirical correctness y, as evidenced by average ECE values exceeding 0.4. In particular, DeepSeek-R1-Distill-Qwen-7B reaches an average ECE of 0.542, indicating substantial discrepancy. According to Xiong et al. (2024), an ECE above 0.25 is already considered unsatisfactory, underscoring the severity of the calibration failures observed in our setup."
        },
        {
            "title": "5.1 Noise Generation",
            "content": "To diagnose the models failures, we analyze the problem from the perspective of retrieval passage noise in RAG (Wu et al., 2025; Cuconasu et al., 2024). To better reflect real-world RAG behavior, we categorize relevant passages into entityrelevant, relation-relevant, and theme-relevant types, and randomly sample one of these categories when generating relevant passage (The definition of each type is provided in Appendix A.8). We then use few-shot prompting (Brown et al., 2020) with Gemini-2.5-Pro (Google DeepMind, 2025) to generate the three types of noisy passages (Pcf, Prel, Pirr) for each query across all four datasets, providing the model with explicit definitions and illustrative examples of all noise types, and conditioning it on the target noise category during generation."
        },
        {
            "title": "5.2 Controlled Analysis Setup",
            "content": "To simulate various RAG scenarios, we manipulate the retrieved set by varying its composition. Let denote the gold passage (where Pgold). For noise injection, let Pnoise be subset of passages drawn uniformly from single noise category {Pcf, Prel, Pirr}. We define three specific input configurations: (1) Gold Only: The model receives only the correct context. We set = {p}. (2) Gold + Noise: The model receives the gold passage alongside two noise passages. We set = {p} Pnoise, subject to Pnoise = 2. (3) Noise Only: The model receives exclusively noise passages. We set = Pnoise, subject to Pnoise = 3. As shown in Figure 2, the results indicate that retrieval noise is the primary factor driving models calibration failures. Specifically: Counterfactual noise greatly degrades models calibration performance. From Figure 2, we observe that when gold passages are present, introducing counterfactual passages leads to most significant degradation in calibration performance compared to the Gold-only baseline, characterized by increased ECE and decreased AUROC. Specifically, relative to the Gold-only setting, Llama-3.18B-Instruct and DeepSeek-R1-Distill-Llama-8B exhibit an average ECE increase of 31.6% and 35.1%, and an average AUROC decrease of 9.1% and 16.1%, respectively on NQ and Bamboogle. In contrast, as shown by the average confidence results in Figure 2, the models display similar confidence levels ˆc under the Gold-only and Gold+counterfactual noise settings. This indicates that when exposed to mutually contradictory evidence, models tend to commit to one answer while maintaining confidence level comparable to that in the noise-free setting. Consequently, under contradictory retrieval signals, the confidence estimates ˆc become decoupled from the answer correctness y, rendering verbal confidence an unreliable indicator of model uncertainty. Relevant noise also harms calibration performance notably. As shown in Figure 2, the presence of relevant noise also significantly degrades the calibration performance of the models compared to the Gold-only baseline. Relative to the Gold-only setting, the introduction of relevant noise consistently results in higher ECE and lower AUROC across both models on the NQ and Figure 2: Calibration performance of Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Llama-8B on NQ and Bamboogle under controlled noise settings. The plots display ECE, AUROC, and Average Confidence across four retrieval settings: Gold-only, Gold+Irrelevant (Irr), Gold+Relevant (Rel), and Gold+Counterfactual (Cf). Results show that introducing noise, particularly counterfactual passages, substantially degrades calibration performance. Bamboogle datasets. Notably, the average AUROC drops by 4.6% for Llama-3.1-8B-Instruct and 10.7% for DeepSeek-R1-Distill-Llama-8B compared to the baseline. We further observe systematic increase in the average confidence ˆc when relevant or irrelevant noise is introduced alongside gold passages. This suggests that exposure to additional, unhelpful information tends to inflate the models confidence, thereby impairing calibration even when the gold passage is present. Even irrelevant noise causes obvious degradation in calibration. Surprisingly, irrelevant noise mirrors the trend of relevant noise. While ECE increases moderately versus the Gold-only baseline, AUROC drops substantially (8.6% for Llama-3.18B-Instruct and 15.7% for DeepSeek-R1-DistillLlama-8B), even exceeding the decline from relevant noise. Consistent with relevant noise, systematic rise in average confidence ˆc relative to Gold-only is again observed. This suggests models become overconfident due to information expansion, deriving false certainty even from completely irrelevant passages."
        },
        {
            "title": "6.1 From Observation to Rules",
            "content": "From the above analysis, we observe two problematic behaviors of current models: (1) Overconfidence under conflict: When presented with counterfactual passages (i.e., Pcf = ), models still assign relatively high confidence to their answers; (2) Overconfidence under noise: When relevant or irrelevant noise is introduced alongside gold passages (i.e., = {p} Pnoise), models exhibit systematic increase in average confidence ˆc compared to the gold-only setting (P = {p}). To address these issues, we argue that the expected behavior in RAG should follow set of Noise-AwAre Confidence CaLibration Rules (NAACL Rules). Formally, we posit that an ideal retrieval-augmented model should satisfy the fol- (1) Conflict Independence: lowing properties: When counterfactual passages are retrieved (P Pcf = ), since the factual correctness of external evidence cannot be reliably determined, the model should fall back to its internal parametIdeally: (ˆa, ˆc) fθ(q, ). (2) ric knowledge. Noise Invariance: When irrelevant passages are retrieved (Pirr = ), the model should explicitly ignore them during reasoning. The prediction should be invariant to the addition of noise: fθ(q, P) fθ(q, Pirr). (3) Parametric Fallback: When no helpful passage is retrieved (i.e., Pgold = ), the model should disregard the external context and answer solely based on its internal knowledge, mirroring the behavior defined in (1). We discuss the rationale of these rules in more detail in Appendix C.2."
        },
        {
            "title": "6.2 NAACL Framework",
            "content": "RAG Passage Construction. We assemble the raw noisy passages generated in Section 5.1 for the HotpotQA training set into three distinct RAG Figure 3: Overview of the NAACL data pipeline with three stages: RAG Passage Construction, Training Response Generation, and Multi-stage Data Filtering. Specifically, In the Training Response Generation stage, the model takes query and set of retrieved passages (where = 3) as input (denoted as Input: Q+3P). It then generates reasoning trace containing passage-level and group-level judgments Jp, Jg (denoted as Type), followed by the predicted answer ˆa (A) and the verbal confidence score ˆc (C). Finally, the pipeline produces 2K high-quality trajectories used for fine-tuning. passage groups. Crucially, these configurations serve as the ground-truth labels for the Passage Group Judgment (Jg), enabling the model to explicitly discern the utility of the retrieved set. For each query, we organize the retrieved context into specific configurations: (1) Counterfactual: Contains the gold passage alongside at least one contradictory Pcf passage to test conflict resolution. (2) Consistent: Contains the gold passage mixed with relevant or irrelevant noise (Prel or Pirr) to assess robustness amid noise. (3) Irrelevant: Contains only relevant and irrelevant passages without valid evidence to probe behavior under missing information. final balanced dataset is created by randomly sampling from these configurations to ensure diverse coverage of noise types. Training Response Generation. We then perform Best-of-N (BoN) sampling (Stiennon et al., 2022; Ouyang et al., 2022) on the initial noisy HotpotQA training set obtained above. To select the best samples that follows NAACL Rules, we prompt the model to produce process judgments at two levels along with the answer and the corresponding confidence: (i) passage-level judgments Jp, indicating whether each passage can directly answer the question, and (ii) group-level judgments Jg, indicating whether the passage group consistently suggests an answer. These judgments are used as intermediate labels for data filtering. with NAACL Rules: (1) Format Consistency: Retains only samples where valid answers, confidence scores, and intermediate reasoning traces can be successfully parsed. (2) Passage Judgment Accuracy: Filters out instances with incorrect passage assessments (Jp and Jg), ensuring the model accurately discriminates passage utility as prerequisite for subsequent rule application. (3) Rule Adherence: Verifies that the reasoning process explicitly invokes and considers the corresponding NAACL Rules. (4) Confidence Alignment: Selects the response trajectory that minimizes the instance-level Brier Score (Brier, 1950; Damani et al., 2025), effectively aligning the verbalized confidence with empirical correctness (i.e., towards 100% for correct and 0% for incorrect predictions). (5) Class Balancing: Balances the distribution of retrieval scenarios (i.e., counterfactual, consistent, and irrelevant) by downsampling the dominant class to match the minority class size, ensuring uniform data distribution for training. Supervised Fine-tuning (SFT). After multistage filtering, we retained approximately 2,000 high-quality QA pairs, which were used for supervised LoRA fine-tuning (SFT) with LlamaFactory (Zheng et al., 2024). More details and hyperparameters are provided in Appendix A.6."
        },
        {
            "title": "6.3 Baselines",
            "content": "Data Quality Control. We apply multi-stage filtering pipeline to ensure the training data aligns (1) Prompting Methods: We adopt vanilla, CoT (Wei et al., 2022; Xiong et al., 2024) and Method Vanilla CoT Noise-aware Ensemble Label-only SFT NAACL Vanilla CoT Noise-aware Ensemble Label-only SFT NAACL Vanilla CoT Noise-aware Ensemble Label-only SFT NAACL Vanilla CoT Noise-aware Ensemble Label-only SFT NAACL StrategyQA HotpotQA NQ Bamboogle Average ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC 0.396 0.354 0.376 0.370 0.345 0.285 0.398 0.363 0.393 0.368 0.297 0. 0.416 0.434 0.343 0.399 0.405 0.323 0.408 0.409 0.321 0.415 0.347 0.306 0.602 0.555 0.615 0.609 0.619 0.624 0.689 0.709 0.618 0.719 0.679 0.726 0.639 0.656 0.621 0.673 0.554 0.651 0.642 0.668 0.530 0.659 0.595 0. Llama-3.1-8B-Instruct 0.605 0.645 0.642 0.650 0.711 0.778 0.465 0.423 0.351 0.428 0.441 0.301 Qwen2.5-7B-Instruct 0.712 0.693 0.692 0.681 0.699 0.735 0.438 0.414 0.380 0.451 0.425 0. 0.577 0.611 0.618 0.619 0.658 0.724 0.710 0.703 0.649 0.693 0.691 0.754 DeepSeek-R1-Distill-Llama-8B 0.660 0.617 0.633 0.650 0.577 0.663 0.504 0.531 0.425 0.525 0.493 0.360 0.637 0.639 0.584 0.592 0.588 0. DeepSeek-R1-Distill-Qwen-7B 0.641 0.632 0.523 0.614 0.538 0.702 0.522 0.578 0.505 0.561 0.629 0.409 0.668 0.641 0.535 0.616 0.585 0.726 0.460 0.444 0.309 0.397 0.319 0.280 0.391 0.353 0.325 0.380 0.321 0. 0.457 0.484 0.443 0.465 0.517 0.359 0.499 0.529 0.422 0.515 0.591 0.391 0.324 0.288 0.217 0.214 0.307 0.199 0.236 0.208 0.192 0.240 0.216 0.113 0.251 0.294 0.281 0.240 0.346 0.200 0.318 0.381 0.304 0.356 0.502 0. 0.636 0.552 0.793 0.713 0.755 0.877 0.809 0.798 0.828 0.793 0.821 0.856 0.693 0.687 0.622 0.678 0.692 0.748 0.666 0.681 0.697 0.601 0.574 0.793 0.411 0.377 0.314 0.352 0.353 0.266 0.366 0.335 0.323 0.360 0.315 0. 0.407 0.436 0.373 0.407 0.440 0.311 0.437 0.474 0.388 0.462 0.517 0.344 0.605 0.591 0.667 0.648 0.686 0.751 0.730 0.726 0.697 0.722 0.722 0.768 0.657 0.650 0.615 0.648 0.603 0.679 0.654 0.655 0.571 0.623 0.573 0. Table 2: Calibration performance of various models on four datasets. Scores in bold indicate the best performance, while underlined scores denote the second-best. Results show that NAACL substantially improves calibration and consistently outperforms several baselines, without sacrificing accuracy, as evidenced in Appendix B.5. specialized noise-aware prompting for comparison. The noise-aware prompting incorporates NAACL Rules in the prompt and ask the model to follow the rules (details in Figure 8). (2) Ensemble: The LLM is queried four times to generate top-k answers with associated confidence scores, which are then averaged to obtain the final confidence estimate (Li et al., 2025c). (3) Label only SFT: This baseline directly utilizes the inputs, answers, and confidence labels from NAACL for SFT, excluding the intermediate reasoning steps. It aims to evaluate the specific impact of confidence supervision on NAACL. Statistics of the training data are shown in Appendix A.7."
        },
        {
            "title": "6.4 Results",
            "content": "only SFT) and the test-time scaling baseline (Ensemble), which requires aggregating confidence scores from multiple sampling paths. Notably, our method surpasses Ensemble and Label-only SFT by approximately 9% in average ECE across the four models using only single inference pass. Our method also results in smoother confidence distributions and substantially reduces overconfidence, as reflected in the reliability diagram (Figure 4) discussed in Appendix B.4 . These results underscore the effectiveness of our noise-aware training framework in enabling accurate epistemic uncertainty estimation in RAG settings, which is further illustrated by case study of NAACL-trained models in Appendix B.3. NAACL exhibits consistent and significant calibration improvement over several baselines. As demonstrated in Table 2, NAACL consistently outperforms all baseline methods across four datasets and four model backbones. Specifically, relative to Vanilla and CoT prompting, NAACL yields an approximately 11% reduction in ECE across models, along with consistent AUROC gains. Furthermore, NAACL attains superior alignment (lower ECE) and discrimination (higher AUROC) compared to the training-based baseline (LabelPerformance gains derive from noise-aware reasoning rather than label fitting. To isolate the source of improvements, we compare NAACL with Label only SFT. Table 2 shows that finetuning on confidence labels alone is insufficient for optimal calibration. Notably, Label only SFT yields limited gains over Vanilla and even degrades performance on DeepSeek-R1 distilled models. This confirms that the effectiveness of NAACL stems not from merely fitting (answer, confidence) pairs, but from our noise-aware framework, which Method ECE AUROC ECE AUROC ECE AUROC Method NQ Bamboogle Average NQ Bamboogle ECE AUROC ECE AUROC ECE AUROC Average 0.371 Vanilla 0.352 CoT 0.289 Noise-aware Ensemble 0.334 Label-only SFT 0.273 0.265 NAACL 0.322 Vanilla 0.313 CoT 0.304 Noise-aware Ensemble 0.325 Label-only SFT 0.335 0.248 NAACL Llama-3.1-8B-Instruct 0.645 0.670 0.667 0.693 0.653 0.674 0.212 0.199 0.140 0.173 0.151 0.127 0.633 0.579 0.806 0.680 0.721 0.823 Qwen2.5-7B-Instruct 0.706 0.696 0.641 0.694 0.667 0.750 0.126 0.121 0.135 0.127 0.129 0. 0.835 0.810 0.753 0.831 0.658 0.845 0.292 0.276 0.215 0.254 0.212 0.196 0.224 0.217 0.220 0.226 0.232 0.157 0.639 0.625 0.737 0.687 0.687 0.749 0.771 0.753 0.697 0.763 0.663 0.798 0.376 Vanilla 0.373 CoT 0.290 Noise-aware Ensemble 0.351 Label-only SFT 0.329 0.276 NAACL DeepSeek-R1-Distill-Llama-8B 0.671 0.633 0.658 0.711 0.684 0.697 0.154 0.203 0.153 0.143 0.251 0.137 0.625 0.621 0.605 0.590 0.604 0.628 0.435 Vanilla 0.454 CoT 0.376 Noise-aware Ensemble 0.398 Label-only SFT 0.402 0.335 NAACL DeepSeek-R1-Distill-Qwen-7B 0.626 0.638 0.625 0.550 0.650 0.765 0.220 0.218 0.143 0.198 0.216 0. 0.608 0.627 0.477 0.597 0.686 0.640 0.265 0.288 0.222 0.247 0.290 0.207 0.328 0.336 0.260 0.298 0.309 0.231 0.648 0.627 0.632 0.651 0.644 0.663 0.617 0.633 0.551 0.574 0.668 0.703 Table 3: Out-of-Distribution (O.O.D.) results with 5 passage per query on the NQ and Bamboogle datasets, demonstrating that NAACL maintains robust calibration performance and consistently outperforms several strong baselines even when facing varying amounts of retrieved context in unseen scenarios. integrates NAACL Rules with high-quality reasoning traces containing accurate passage judgments. Explicit noise-aware instructions improve zeroshot calibration. To directly validate the utility of our proposed rules, we examine the Noiseaware prompting baseline, which explicitly instructs the model to adhere to NAACL Rules. As shown in Table 2, this simple prompting strategy outperforms standard CoT and Vanilla methods in most settings. Remarkably, across three out of four model backbones (excluding Qwen2.57B-Instruct), Noise-aware prompting emerges as the second-best performer in terms of Average ECE, trailing only NAACL. It even surpasses the computation-intensive Ensemble baseline and the training-based Label-only SFT, highlighting the effectiveness of the guidance provided by our rules. This affirms that the NAACL Rules serves as critical foundation for our method, contributing significantly to the observed performance improvements. NAACL is generalizable across different amount of noise passages. To evaluate the robustness of our method under varying information load, we conduct out-of-distribution experiments by increasing the number of retrieved passages from = 3 (used during training) to = 5 at inference. As shown in Table 3, NAACL demonstrates strong generalization, reducing average ECE by 8% compared to the Vanilla baseline, while consistently maintaining superior calibration performance across several strong baselines when exposed to more retrieved passages in O.O.D. settings. This suggests that NAACL does not merely overfit to fixed training format but learns generalized ability to recognize diverse passages and assign appropriate confidence scores. NAACL enhances models ability to judge passage utility and thereby enhances interpretability. core premise of our framework is that accurate confidence estimation in RAG hinges on the models ability to correctly assess the quality of retrieved contexts. Empirical results confirm that NAACL substantially sharpens this discriminative capability. Compared to vanilla baselines, our method improves passage utility judgment accuracy by approximately 10% on two instructiontuned models; even for reasoning models with stronger inherent capabilities, it consistently yields gains of about 5% on two DeepSeek-distilled variants. Crucially, by requiring the model to explicitly verbalize these judgments before assigning confidence score, NAACL provides superior interpretability, enabling users to directly link epistemic uncertainty to the models assessment of the retrieval environment, rather than to opaque probability distributions."
        },
        {
            "title": "7 Conclusion",
            "content": "Our study exposes fundamental vulnerability in RAG where retrieval noise severs the link between model confidence and factual correctness. We identified that irrelevant and contradictory passages actively inflate false certainty, rendering standard LLMs critically overconfident in real-world settings. To resolve this, we propose NAACL, principled framework that equips models with intrinsic noise awareness through self-bootstrapping training pipeline. By enforcing specific consistency rules, our method enables models to explicitly discern passage utility and decouple their confidence from misleading evidence without relying on external teacher models. Extensive experiments confirm that NAACL delivers substantial gains in calibration performance while significantly enhancing the transparency and interpretability of the reasoning process, marking crucial step toward building robust and epistemically reliable RAG systems."
        },
        {
            "title": "Limitations",
            "content": "While NAACL demonstrates significant improvements in RAG confidence calibration, we acknowledge several limitations: Model Scale and Access. Our evaluation is currently limited to open-source models in the 7B-8B parameter range. We did not extend our experiments to larger-scale models (e.g., 70B+) or proprietary models (e.g., GPT-5, Gemini-3-Pro). This exclusion is primarily due to the prohibitive computational costs of fine-tuning larger models. Synthetic vs. Real-World Noise. Our training data construction relies on synthetically generating specific types of noise (counterfactual, relevant, irrelevant). While this provides precise control for learning, real-world retrieval errors are often more nuanced and may not fit neatly into these categories. It remains to be seen how well the model generalizes to organic noise in highly specialized domains (e.g., biomedical or legal RAG). Scalability to Complex Contexts and Tasks. Our current evaluation focuses on short-form question answering with fixed-depth retrieval. Extending noise-aware calibration to long-form generation (e.g., summarization) remains non-trivial, as hallucination in long texts is granular and difficult to capture with single scalar confidence score. Furthermore, applying our framework to ultra-long contexts typical of agentic search (Luo et al., 2025; Li et al., 2025b; Liu et al., 2025a) introduces new challenges: detecting contradictions across massive, dynamic information streams may incur prohibitive computational costs and suffer from attention degradation (e.g., lost-in-the-middle (Liu et al., 2024b) phenomena), necessitating more efficient mechanisms than our rule-based scanning. sources are widely used in the research community and generally do not contain sensitive personally identifying information (PII) of private individuals or offensive content. The synthetic noise passages generated for our training data were created using Gemini-2.5-Pro, which employs built-in safety filters to prevent the generation of toxic or harmful content. We did not observe any offensive material in the generated samples during our manual quality checks. Data Consent and Licenses. We strictly adhere to the licenses and terms of use for all datasets and models employed in this work. The datasets (HotpotQA, Natural Questions, StrategyQA, and Bamboogle), the Wikipedia corpus and the models (Llama-3.1-Instruct-8B (Touvron et al., 2023), Qwen2.5-7B-Instruct (Yang et al., 2025), DeepSeek-R1-Distill-Llama-8B, DeepSeekR1-Distill-Qwen-7B (DeepSeek-AI, 2025)) are all open-source and distributed under permissive licenses (e.g., CC BY-SA, Apache 2.0) that permit academic research and modification. No new private data was collected from human subjects, and no crowdsourcing platforms were used. Models. All open-source models were hosted and executed locally using the vLLM library (Kwon et al., 2023), while Gemini-2.5-Pro utilized to generate RAG passages were accessed through vertex AI (Google Cloud, 2026). For reproducibility, the experimental settings are detailed in Section 4 and Appendix A."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank the anonymous reviewers and the area chair for their constructive comments. The authors of this paper were supported by the ITSP Platform Research Project (ITS/189/23FP) from ITC of Hong Kong, SAR, China, and the AoE (AoE/E-601/24-N), the RIF (R6021-20) and the GRF (16205322) from RGC of Hong Kong,SAR, China."
        },
        {
            "title": "References",
            "content": "Personally Identifying or Offensive Content. The experiments in this study utilize standard, publicly available academic datasets (HotpotQA (Yang et al., 2018), Natural Questions (Kwiatkowski et al., 2019), StrategyQA (Geva et al., 2021), Bamboogle (Press et al., 2023)) and retrieval corpus based on wikipedia (Wikimedia, 2023). These Siddhant Arora, Haidar Khan, Kai Sun, Xin Luna Dong, Sajal Choudhary, Seungwhan Moon, Xinyuan Zhang, Adithya Sagar, Surya Teja Appini, Kaushik Patnaik, Sanat Sharma, Shinji Watanabe, Anuj Kumar, Ahmed Aly, Yue Liu, Florian Metze, and Zhaojiang Lin. 2025. Stream rag: Instant and accurate spoken dialogue systems with streaming tool usage. Preprint, arXiv:2510.02044. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, and Pascale Fung. 2025. Hallulens: LLM hallucination benchmark. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 24128 24156. Association for Computational Linguistics. Glenn Brier. 1950. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78(1):13. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Jiuhai Chen and Jonas Mueller. 2024. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51865200, Bangkok, Thailand. Association for Computational Linguistics. Yongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, and Chuchu Fan. 2025. R1-code-interpreter: Training llms to reason with code via supervised and reinforcement learning. CoRR, abs/2505.21668. Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The power of noise: Redefining retrieval for rag systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, page 719729. ACM. Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, and Jacob Andreas. 2025. Beyond binary rewards: Training lms to reason about their uncertainty. Preprint, arXiv:2507.16806. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Guanting Dong, Jiajie Jin, Xiaoxi Li, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen. 2025. Rag-critic: Leveraging automated critic-guided agentic workflow for retrieval augmented generation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 35513578. Association for Computational Linguistics. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. 2024. Shifting attention to relevance: Towards the predictive uncertainty quantification of freeform large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 50505063. Association for Computational Linguistics. Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, and Artem Shelmanov. 2023. LM-polygraph: Uncertainty estimation for language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 446461, Singapore. Association for Computational Linguistics. Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. 2024a. Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1002810039, Bangkok, Thailand. Association for Computational Linguistics. Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. 2024b. Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training. Preprint, arXiv:2405.20978. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. Unsupervised quality estimation for neural machine translation. Trans. Assoc. Comput. Linguistics, 8:539555. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. 2024a. survey of confidence estimation and calibration in large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 65776595, Mexico City, Mexico. Association for Computational Linguistics. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. 2024b. survey of confidence estimation and calibration in large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 65776595. Association for Computational Linguistics. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Trans. Assoc. Comput. Linguistics, 9:346361. Google Cloud. 2026. Vertex ai api. https://cloud. January 5, google.com/vertex-ai. Accessed: 2026. Google DeepMind. 2025. Gemini 2.5 pro overview. https://deepmind.google/technologies/ gemini/pro/. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. 2017a. On calibration of modern neural networks. In International Conference on Machine Learning, pages 13211330. PMLR. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017b. On calibration of modern neural networks. Preprint, arXiv:1706.04599. Yang Guo, Yutian Tao, Yifei Ming, Robert D. Nowak, and Yingyu Liang. 2025. Retrieval-augmented generation as noisy in-context learning: unified theory and risk bounds. Preprint, arXiv:2506.03100. Wentao Hu, Wengyu Zhang, Yiyang Jiang, Chen Jason Zhang, Xiaoyong Wei, and Qing Li. 2025. Removal of hallucination on hallucination: Debate-augmented RAG. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1583915853. Association for Computational Linguistics. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. Zhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Strötgen, and Gerhard Weikum. 2018. Tempquestions: benchmark for temporal question answering. In Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018, pages 10571062. ACM. Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, and Jun Zhao. 2025. Rag-rewardbench: Benchmarking reward models in retrieval augmented generation for preference alignment. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1706117090. Association for Computational Linguistics. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language Preprint, model serving with pagedattention. arXiv:2309.06180. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-augmented generation for knowledgeintensive nlp tasks. Preprint, arXiv:2005.11401. Jingshu Li, Yitian Yang, Q. Vera Liao, Junti Zhang, and Yi-Chieh Lee. 2025a. As confidence aligns: Understanding the effect of AI confidence on human selfconfidence in human-ai decision making. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, CHI 2025, YokohamaJapan, 26 April 20251 May 2025, pages 1111:11111:16. ACM. Junlong Li, Wenshuo Zhao, Jian Zhao, Weihao Zeng, Haoze Wu, Xiaochen Wang, Rui Ge, Yuxuan Cao, Yuzhen Huang, Wei Liu, Junteng Liu, Zhaochen Su, Yiyang Guo, Fan Zhou, Lueyang Zhang, Juan Michelini, Xingyao Wang, Xiang Yue, Shuyan Zhou, and 2 others. 2025b. The tool decathlon: Benchmarking language agents for diverse, realistic, and longhorizon task execution. Preprint, arXiv:2510.25726. Yibo Li, Miao Xiong, Jiaying Wu, and Bryan Hooi. 2025c. Conftuner: Training large language models to express their confidence verbally. arXiv preprint arXiv:2508.18847. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. Trans. Mach. Learn. Res., 2022. Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2024. Generating with confidence: Uncertainty quantification for black-box large language models. Trans. Mach. Learn. Res., 2024. Jiayu Liu, Cheng Qian, Zhaochen Su, Qing Zong, Shijue Huang, Bingxiang He, and Yi R. Fung. 2025a. Costbench: Evaluating multi-turn cost-optimal planning and adaptation in dynamic environments for LLM tool-use agents. CoRR, abs/2511.02734. Jiayu Liu, Junhao Tang, Hanwen Wang, Baixuan Xu, Haochen Shi, Weiqi Wang, and Yangqiu Song. 2024a. GProofT: multi-dimension multi-round fact checking framework based on claim fact extraction. In Proceedings of the Seventh Fact Extraction and VERification Workshop (FEVER), pages 118129, Miami, Florida, USA. Association for Computational Linguistics. Jiayu Liu, Qing Zong, Weiqi Wang, and Yangqiu Song. 2025b. Revisiting epistemic markers in confidence estimation: Can markers accurately reflect large language models uncertainty? In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 206221, Vienna, Austria. Association for Computational Linguistics. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, and Jiaya Jia. 2024c. RL-GPT: integrating reinforcement learning and code-as-policy. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Shuyi Liu, Yuming Shang, and Xi Zhang. 2025c. Truthfulrag: Resolving factual-level conflicts in retrieval-augmented generation with knowledge graphs. Preprint, arXiv:2511.10375. Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, and Hua Wei. 2025d. Uncertainty quantification and confidence calibration in large language models: survey. CoRR, abs/2503.15850. Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin, Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, Zixuan Hu, Hongze Mi, Yibo Wang, Naiqiang Tan, Hong Chen, Yi R. Fung, Chun Yuan, and Li Shen. 2025. Ultrahorizon: Benchmarking agent capabilities in ultra longhorizon scenarios. Preprint, arXiv:2509.21766. Azadeh Sadat Mozafari, Hugo Siqueira Gomes, Wilson Leão, Steeven Janny, and Christian Gagné. 2019. Attended temperature scaling: practical approach for calibrating deep neural networks. Preprint, arXiv:1810.11586. Stephen Obadinma and Xiaodan Zhu. 2025. On the robustness of verbal confidence of llms in adversarial attacks. Preprint, arXiv:2507.06489. Litu Ou, Kuan Li, Huifeng Yin, Liwen Zhang, Zhongwang Zhang, Xixi Wu, Rui Ye, Zile Qiao, Pengjun Xie, Jingren Zhou, and Yong Jiang. 2025. Browseconf: Confidence-guided test-time scaling for web agents. Preprint, arXiv:2510.23458. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Preprint, arXiv:2203.02155. Shintaro Ozaki, Yuta Kato, Siyuan Feng, Masayo Tomita, Kazuki Hayashi, Wataru Hashimoto, Ryoma Obara, Masafumi Oyamada, Katsuhiko Hayashi, Hidetaka Kamigaito, and Taro Watanabe. 2025. Understanding the impact of confidence in retrieval augmented generation: case study in the medical domain. In Proceedings of the 24th Workshop on Biomedical Language Processing, pages 117, Viena, Austria. Association for Computational Linguistics. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 56875711. Association for Computational Linguistics. Stephen Robertson, Hugo Zaragoza, and Michael Taylor. 2004. Simple bm25 extension to multiple weighted fields. In Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management, CIKM 04, page 4249, New York, NY, USA. Association for Computing Machinery. Michael Shen, Muhammad Umar, Kiwan Maeng, ToG. Edward Suh, and Udit Gupta. 2024. wards understanding systems trade-offs in retrievalaugmented generation model inference. Preprint, arXiv:2412.11854. Jasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Lakshminarayanan, Sebastian Nowozin, D. Sculley, Joshua V. Dillon, Jie Ren, and Zachary Nado. 2019. Can you trust your models uncertainty? evaluating In Adpredictive uncertainty under dataset shift. vances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 1396913980. Ionut-Teodor Sorodoc, Leonardo F. R. Ribeiro, Rexhina Blloshmi, Christopher Davis, and Adrià de Gispert. 2025. Garage: benchmark with grounding annotations for RAG evaluation. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1703017049. Association for Computational Linguistics. Heydar Soudani, Evangelos Kanoulas, and Faegheh Hasibi. 2025a. Why uncertainty estimation methods fall short in RAG: An axiomatic analysis. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1659616616, Vienna, Austria. Association for Computational Linguistics. Heydar Soudani, Evangelos Kanoulas, and Faegheh Hasibi. 2025b. Why uncertainty estimation methods fall short in RAG: An axiomatic analysis. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1659616616, Vienna, Austria. Association for Computational Linguistics. Heydar Soudani, Hamed Zamani, and Faegheh Hasibi. 2025c. Uncertainty quantification for retrievalaugmented reasoning. Preprint, arXiv:2510.11483. Elias Stengel-Eskin, Peter Hase, and Mohit Bansal. 2024. Lacie: Listener-aware finetuning for confidence calibration in large language models. Preprint, arXiv:2405.21028. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2022. Learning to summarize from human feedback. Preprint, arXiv:2009.01325. Duolin Sun, Dan Yang, Yue Shen, Yihan Jiao, Zhehao Tan, Jie Feng, Lianzhen Zhong, Jian Wang, Peng Wei, and Jinjie Gu. 2025a. Hanrag: Heuristic accurate noise-resistant retrieval-augmented generation for multi-hop question answering. Preprint, arXiv:2509.09713. Xin Sun, Jianan Xie, Zhongqi Chen, Qiang Liu, Shu Wu, Yuehe Chen, Bowen Song, Zilei Wang, Weiqiang Wang, and Liang Wang. 2025b. Divide-then-align: Honest alignment based on the knowledge boundary of RAG. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1146111480. Association for Computational Linguistics. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971. Yumeng Wang, Zhiyuan Fan, Jiayu Liu, Jen tse Huang, and Yi R. Fung. 2025b. Diversity-enhanced Preprint, reasoning for subjective questions. arXiv:2507.20187. Navve Wasserman, Roi Pony, Oshri Naparstek, Adi Raz Goldfarb, Eli Schwartz, Udi Barzelay, and Leonid Karlinsky. 2025. REAL-MM-RAG: real-world In Proceedings multi-modal retrieval benchmark. of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 3166031683. Association for Computational Linguistics. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. Browsecomp: simple yet challenging benchmark for browsing agents. CoRR, abs/2504.12516. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. David Widmann, Fredrik Lindsten, and Dave Zachariah. 2020. Calibration tests in multi-class classification: unifying framework. Preprint, arXiv:1910.11385. Wikimedia. 2023. Wikipedia dataset (20231101.en). https://huggingface.co/datasets/ wikimedia/wikipedia/viewer/20231101.en. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020, pages 3845. Association for Computational Linguistics. Jinyang Wu, Shuai Zhang, Feihu Che, Mingkuan Feng, Pengpeng Shao, and Jianhua Tao. 2025. Pandoras box or aladdins lamp: comprehensive analysis revealing the role of RAG noise in large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50195039, Vienna, Austria. Association for Computational Linguistics. Siyuan Wang, James R. Foulds, Md Osman Gani, and Shimei Pan. 2025a. Llm-based corroborating and refuting evidence retrieval for scientific claim verification. Preprint, arXiv:2503.07937. Zhiqiu Xia, Jinxuan Xu, Yuqian Zhang, and Hang Liu. 2025. survey of uncertainty estimation methods In Findings of the Ason large language models. sociation for Computational Linguistics, ACL 2025, Tong Zhang. 2024. R-tuning: Instructing large language models to say dont know. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 71137139, Mexico City, Mexico. Association for Computational Linguistics. Jiayi Zhang, Simon Yu, Derek Chong, Anthony Sicilia, Michael R. Tomz, Christopher D. Manning, and Weiyan Shi. 2025. Verbalized sampling: How to mitigate mode collapse and unlock llm diversity. Preprint, arXiv:2510.01171. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. 2025. Group sequence policy optimization. Preprint, arXiv:2507.18071. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. Xiaofan Zhou, Liangjie Huang, Pinyang Cheng, Wenpen Yin, Rui Zhang, Wenrui Hao, and Lu Cheng. 2025a. Accelerating causal network discovery of alzheimer disease biomarkers via scientific literaturebased retrieval augmented generation. Preprint, arXiv:2504.08768. Youchao Zhou, Heyan Huang, Yicheng Liu, Rui Dai, Xinglin Wang, Xingchen Zhang, Shumin Shi, and Yang Deng. 2025b. Do retrieval augmented language Preprint, models know when they dont know? arXiv:2509.01476. Qing Zong, Jiayu Liu, Tianshi Zheng, Chunyang Li, Baixuan Xu, Haochen Shi, Weiqi Wang, Zhaowei Wang, Chunkit Chan, and Yangqiu Song. 2025a. Critical: Can critique help LLM uncertainty or confidence calibration? CoRR, abs/2510.24505. Qing Zong, Zhaowei Wang, Tianshi Zheng, Xiyu Ren, and Yangqiu Song. 2025b. ComparisonQA: Evaluating factuality robustness of LLMs through knowledge frequency control and uncertainty. In Findings of the Association for Computational Linguistics: ACL 2025, pages 41014117, Vienna, Austria. Association for Computational Linguistics. Vienna, Austria, July 27 - August 1, 2025, pages 2138121396. Association for Computational Linguistics. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2024. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, and Jing Gao. 2024. SaySelf: Teaching LLMs to express confidence with self-reflective rationales. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59855998, Miami, Florida, USA. Association for Computational Linguistics. Daniel Yang, Yao-Hung Hubert Tsai, and Makoto Yamada. 2024. On verbalized confidence scores for llms. Preprint, arXiv:2412.14737. Qwen: An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 23 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 23692380. Association for Computational Linguistics. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, and 16 others. 2025. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476. Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, and Jianye Hao. 2025. Embodied-r1: Reinforced embodied reasoning for general robotic manipulation. Preprint, arXiv:2508.13998. Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. 2025. ACECODER: acing coder RL via automated test-case synthesis. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1202312040. Association for Computational Linguistics. Hanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and"
        },
        {
            "title": "Appendices",
            "content": "Hyperparameter BM25 Contriever"
        },
        {
            "title": "A Detailed Experiment Setup",
            "content": "A.1 Models We use Qwen/Qwen2.5-7B-Instruct (Yang et al., 2025), meta-llama/Llama-3.1-8B-Instruct (Touvron et al., 2023), deepseek-ai/DeepSeek-R1Distill-Qwen-7B, and deepseek-ai/DeepSeek-R1Distill-Llama-8B (DeepSeek-AI, 2025) in all experiments. Proprietary models were excluded from our study because their limited accessibility to internal parameters constrains further optimization and adaptation. For inference-time hyperparameters, we set the maximum output length to 2048 and use temperature of 0 to ensure deterministic responses. A."
        },
        {
            "title": "Inference and Training Backend",
            "content": "We use vLLM (Kwon et al., 2023) as the inference backend and LLaMAFactory (Zheng et al., 2024) for all training, with both inference and training conducted on 4 NVIDIA L20 GPUs. A.3 Dataset Statistics Table 4 reports the number of samples in each dataset along with the 95% confidence intervals of ECE and AUROC, computed using the method proposed by Widmann et al. (2020). The results indicate that the scale of our datasets is sufficient to yield reliable estimates. Dataset # Questions Confidence Interval HotpotQA StrategyQA NQ Bamboogle 800 800 800 150 0.0347 0.0347 0.0347 0.0800 Table 4: Dataset statistics and 95% confidence intervals of ECE and AUROC. A.4 Prompts A.4.1 RAG Test Prompts We adopt three types of promptsVanilla, CoT, and Multi-Stepfrom Xiong et al. (2024). For reasoning-oriented models, step-level confidences are elicited by requiring the models to report their confidence scores in the final output after the reasoning process. The prompt designs are illustrated in Figure 7, and the results of prompt permutation experiments are reported in Appendix B.2. Top-K Retrieval Reranker Model Specifics Architecture 5 No 5 No Sparse (Probabilistic) Dense (Bi-Encoder) Embedding Model Max Input Length KNN Candidates N/A N/A N/A facebook/ contriever 256 tokens 100 Table 5: Retrieval Hyperparameters. A.4.2 Noise Generation Prompt We detail the methodology for constructing the noise passages used in our RAG experiments. To generate high quality and semantically diverse noise, we employ Gemini 2.5 Pro. Specifically, we design three distinct types of noise prompts: counterfactual noise generation prompt, relevant noise generation prompt, and irrelevant noise generation prompt, corresponding to counterfactual noise, relevant noise and irrelevant noise. For each type, the prompt provided to the model includes clear definition of the noise category and concrete examples to guide the generation. The full templates for all three prompt types are presented in Figure 10, 11 and 12. During generation, to encourage greater diversity in the output, we instruct the model to produce 5 candidate passages per call. We explicitly enhance the diversity of generated content using Zhang et al. (2025), then select only the last three generated passages as the final noise passages for our experimental setup in Table 2. A.4.3 Baseline Prompts For the baselines used in the main experiments (Table 2 and 3), Vanilla, CoT prompt is provided in Figure 7 and noise-aware prompt is provided in Figure 8. A.5 RAG Setup In this section, we detail the hyperparameters used for our Retrieval-Augmented Generation (RAG) setup. We summarize the specific configurations for both the sparse retriever (BM25) and the dense retriever (Contriever) in Table 5. Specifically, we retrieve the top-k = 5 passages for all experiments. We do not employ any reranking models in this study following Soudani et al. (2025a). For the dense retriever, the input is truncated to 256 tokens during the embedding process. Noise Type Counterfactual Entity-relevant Noise Relation-relevant Noise Theme-relevant Noise Irrelevant Noise Definition Passages that are semantically relevant to the question but directly contradict the ground truth answer. They provide specific, plausible-sounding information that supports an incorrect alternative answer. Passages that mention the correct entities in the question but only provide partial, tangential, or incomplete factual information, without containing the evidence needed to answer the question. Passages that capture the type of relations required by the question but do not involve the queried entities, thereby providing misleading or insufficient evidence. Passages that are topically aligned with the question and provide high-level background or contextual information, but do not contain entity-level or relation-level facts necessary for answering. Passages that have little to no semantic relation to the question. They are from unrelated topics or domains and provide no useful information for answering. Table 6: Definitions of five types of noise passages for retrieval-augmented question answering. Model DS-R1-Llama DS-R1-Qwen Llama-3.1 Qwen-2.5 Total 96000 96000 96000 Kept Responses (1) Format (2) Passage Judgment (3) Rule Following (4) Alignment (5) Common IDs (6) Balance 85723 88201 78200 39008 28481 35255 31065 34403 28790 26221 5211 4611 4895 2801 2801 2801 2801 1945 1945 1945 Table 7: Training data statistics: This table shows the number of training data left after each filtering step. (1) Format: retains only samples from which valid answer, confidence score, and intermediate passage judgments can be successfully extracted. (2) Passage judgment: filters out samples containing incorrect assessments of the retrieved passages. (3) Rule following: filters for samples that have explicit reasoning process for rule following. (4) Alignment: for each query, selects the final response that minimizes the instance-level Brier Score. (5) Common IDs: retains only samples with question IDs common across all models. (6) Balance: balances the 3 groups (counterfactual, consistent, irrelevant) by downsampling consistent to match irrelevant. Model name abbreviations: DS-R1-Llama: DeepSeek-R1-Distill-Llama-8B; DS-R1-Qwen: DeepSeek-R1-Distill-Qwen-7B; Llama-3.1: Llama3.1-8B-Instruct; Qwen-2.5: Qwen2.5-7B-Instruct. Figure 4: Reliability Diagram for HotpotQA: comparison of CoT prompt with base model (upper row) and SFT models (lower row). Each subplot displays accuracy v.s. confidence, with the diagonal dashed line representing perfect calibration. A.6 SFT Details We conducted Supervised Fine-Tuning (SFT) utilizing the LLaMA-Factory framework (Zheng et al., 2024). Specifically, we set the learning rate to 5.0 105 and the number of training epochs to 2. The maximum sequence length is set to 2048, aligning with the inference configuration to conserve computational resources. For all other training arguments and hyperparameters, we adhered to the default settings provided by LLaMA-Factory. A.7 Training data statistics We employ self-consistency-based approach to construct our training data. For each input query, we generate 16 distinct response paths using selfsampling with temperature setting of 1.0. The resulting dataset exhibits an average input token count of 646 and an average output token count of 370. To ensure high-quality supervision for calibration, we implement comprehensive five-stage filtering pipeline specified in Section 6.2. Following the five-stage cleaning process, we observed discrepancies in the volume of retained data across different models. Therefore, prior to training, we apply an additional data-balancing step to ensure consistent distribution, and provide concrete implementation details below. 1. Format Consistency: We retain only those samples where the answer, confidence score, and intermediate passage judgments can be successfully extracted via regular expressions. 2. Passage Judgment Accuracy: We filter out samples where the models judgment of the retrieved passages and passage groups conflicts with the ground truth labels. 3. Rule Following: We discard samples that fail to exhibit the explicit reasoning process required by our instructions. Specifically, we filter out the samples that didnt incorporate keywords like rules, Step 4 (See Figure 9, we prompt the model to reason through multiple steps. Step 4 is the designated step for applying our NAACL Rules; if it does not appear in the reasoning trace, the reasoning chain is considered invalid), etc. 4. Alignment Selection: From the remaining candidates for each query, we select the single final response that minimizes the instancelevel Brier Score, ensuring the model learns from its most calibrated outputs. 5. Common Intersection: To allow for fair comparison, we retain only those questions for which valid responses exist across all four evaluated models. 6. Class Balancing: Finally, we balance the distribution of retrieval scenarios (counterfactual, consistent, irrelevant) by downsampling the dominant consistent class to match the size of the irrelevant class. The process is fully rule-based, without any external model incorporated. The detailed statistics of data retention after each stage are presented in Table 7. A.8 Fine-grained Noise Definitions We categorize noise passages into three distinct types based on their semantic relationship to the query and their potential to mislead the answering process, as defined in Table 6. Counterfactual passages are adversarially designed contexts that are topically aligned with the question but contain specific, contradictory information that supports an incorrect alternative answer. Relevant noise passages mimic common retrieval errors by sharing keywords or general topics with the query while lacking the precise information needed to derive any answer. Specifically, we divide relevant passages into three types: Entity-relevant passages provide factual information about the entities involved in the query; relationship-relevant passages describe same interactions or relations among multiple unrelated entities; and theme-relevant passages offer broader background or contextual information aligned with the overall question intent. Irrelevant noise passages represent complete topic mismatches, providing no useful contextual information. This taxonomy is consistent with prior work on noise categorization for retrieval-augmented models, such as the similar three-type classification employed by Fang et al. (2024b)."
        },
        {
            "title": "B Additional Experiment Results",
            "content": "B.1 Passage Position Bias To investigate the impact positional bias, we evaluated model performance by embedding the ground truth passage among noise passages, placing the ground truth at various positions within the context window. The detailed results are presented in Retriever Prompt Type StrategyQA HotpotQA NQ Bamboogle Average ECE AUROC ECE BM25 Contriever BM25 Contriever BM25 Contriever BM25 Contriever Vanilla CoT Multi-Step Vanilla CoT Multi-Step Vanilla CoT Multi-Step Vanilla CoT Multi-Step Vanilla CoT Multi-Step Vanilla CoT Multi-Step Vanilla CoT Multi-Step Vanilla CoT Multi-Step 0.266 0.217 0.250 0.284 0.228 0.283 0.223 0.243 0.213 0.218 0.231 0.214 0.218 0.246 0.316 0.237 0.235 0.300 0.275 0.292 0.275 0.276 0.292 0.272 0.550 0.480 0.482 0.563 0.455 0.483 0.546 0.562 0.498 0.562 0.564 0. 0.573 0.574 0.523 0.576 0.572 0.513 0.541 0.539 0.525 0.547 0.539 0.507 AUROC 0.626 0.548 0.486 0.576 0.417 0.379 ECE Llama-3.1-8B-Instruct 0.446 0.416 0.394 0.490 0.446 0.515 Qwen2.5-7B-Instruct 0.442 0.439 0.414 0.571 0.553 0.454 0.692 0.726 0.673 0.622 0.625 0. 0.515 0.538 0.452 0.614 0.663 0.618 0.436 0.450 0.476 0.575 0.570 0.472 DeepSeek-R1-Distill-Llama-8B 0.441 0.459 0.496 0.493 0.527 0.581 0.647 0.628 0.555 0.635 0.615 0.592 0.454 0.461 0.461 0.460 0.477 0. DeepSeek-R1-Distill-Qwen-7B 0.551 0.583 0.597 0.647 0.644 0.638 0.565 0.591 0.499 0.524 0.455 0.490 0.564 0.560 0.524 0.587 0.600 0.570 AUROC ECE AUROC ECE AUROC 0.696 0.648 0.503 0.638 0.558 0.399 0.735 0.742 0.687 0.698 0.699 0.596 0.700 0.707 0.613 0.746 0.754 0. 0.682 0.686 0.601 0.721 0.723 0.617 0.755 0.613 0.603 0.735 0.629 0.724 0.668 0.628 0.610 0.690 0.632 0.470 0.547 0.521 0.672 0.535 0.557 0.686 0.736 0.756 0.734 0.734 0.773 0.750 0.554 0.452 0.530 0.619 0.445 0. 0.579 0.599 0.592 0.652 0.696 0.612 0.686 0.754 0.535 0.732 0.714 0.551 0.718 0.584 0.424 0.727 0.632 0.573 0.495 0.446 0.425 0.531 0.491 0.535 0.442 0.440 0.428 0.513 0.496 0.402 0.415 0.422 0.486 0.431 0.449 0. 0.531 0.548 0.532 0.561 0.577 0.557 0.607 0.532 0.500 0.599 0.469 0.418 0.638 0.657 0.613 0.633 0.646 0.584 0.651 0.666 0.556 0.672 0.664 0.572 0.627 0.600 0.512 0.630 0.587 0.547 Table 8: Evaluation of verbal confidence calibration performance (ECE and AUROC) on four datasets across varying retrievers and prompting strategies. Results show that the model consistently exhibits an average ECE greater than 0.4, indicating poor calibration performance. Tables 9 and 10. We observe consistent trend: the introduction of noisy passages significantly degrades the quality of verbalized confidence, evidenced by the increased ECE and decreased AUROC. Crucially, this degradation remains pervasive regardless of the specific location of the ground truth passage, indicating that the models vulnerability to noise is fundamental issue rather than position-dependent artifact. B.2 Prompt Permutations To investigate whether the poor calibration observed in RAG settings stems from the limitation of specific prompting strategies, we conduct comprehensive evaluation across three distinct prompting paradigms: Vanilla, Chain-of-Thought (CoT), and Multi-Step reasoning. We evaluate these strategies using both BM25 and Contriever retrieval settings across all four datasets. The detailed results are presented in Table 8. Prompting strategies from close-book reasoning alone cannot efficiently enhance verbalized confidence performance in RAG scenarios. As evidenced by the results, models exhibit consistently unsatisfactory calibration performance across all prompt types. Sophisticated prompting strategies that have proven effective in closed-book reasoning scenarios (Xiong et al., 2024) do not yield significant calibration gains; notably, none of the methods achieves an average ECE below 0.4. For instance, the Multi-Step prompting strategy often exacerbates miscalibration compared to the Vanilla baseline, particularly in DeepSeek-distilled models (e.g., average ECE increases from 0.415 to 0.486 on DeepSeek-R1-Distill-Llama-8B). These findings suggest that the calibration failure in RAG is fundamental issue rooted in the models inability to handle retrieval noise, rather than superficial artifact of the prompting format. This underscores the necessity of dedicated training framework like NAACL to align confidence with retrieval quality. B.3 Case Studies B.4 Reliability Diagram Figure 4 presents the reliability diagrams for the HotpotQA dataset (in-domain results), comparing the calibration performance of the standard CoT prompting baseline against our proposed NAACL. As observed in the top row, the CoT baseline exhibits severe miscalibration, characterized by tendency towards overconfidence. The models often assign high confidence scores (near 100%) even when empirical accuracy is low, and they fail to utilize lower confidence bins effectively, resulting in sparse and uninformative distribution. Setting gt_only gt_with_noise/counterfactual gt_with_noise/relevant gt_with_noise/irrelevant Pos Bamboogle HotpotQA NQ StrategyQA Average ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC N/A 0.071 0.475 pos1 0.416 pos2 0.365 pos3 0.126 pos1 0.131 pos2 0.116 pos3 0.145 pos1 0.152 pos2 0.152 pos3 0.675 0.474 0.581 0.671 0.579 0.583 0.511 0.579 0.529 0.529 0.117 0.500 0.514 0.452 0.199 0.196 0.189 0.197 0.196 0.197 0.693 0.475 0.504 0.613 0.538 0.549 0.564 0.523 0.539 0.519 0.139 0.477 0.490 0.472 0.196 0.193 0.196 0.196 0.195 0.194 0.688 0.483 0.557 0.714 0.612 0.624 0.623 0.591 0.557 0. 0.046 0.648 0.609 0.492 0.058 0.050 0.054 0.110 0.108 0.112 0.679 0.290 0.291 0.397 0.572 0.603 0.599 0.561 0.577 0.562 0.093 0.525 0.507 0.445 0.145 0.143 0.139 0.162 0.163 0.164 0.684 0.431 0.483 0.599 0.575 0.590 0.574 0.564 0.551 0.533 Table 9: The table evaluate the impact of passage ordering on Llama-3.1-8B-Instructs calibration performance. The Setting column defines the context structure, where gt_only refers to noise-free baseline containing only the ground truth passage, while the gt_with_noise categories involve mixing the ground truth with specific types of noise (counterfactual, relevant, or irrelevant). The Pos column specifies the exact position (1st, 2nd, or 3rd) of the ground truth passage within the sequence of retrieved passages, designed to assess the models sensitivity to positional bias when processing mixed-quality contexts. The results indicate that calibration performance steadily declines as noise passages are added. In contrast, NAACL (bottom row) significantly improves the alignment between predicted confidence and actual accuracy. The reliability curves for the fine-tuned models closely track the perfect calibration diagonal across broad range of confidence bins. This indicates that NAACL successfully regularizes the models outputs, transforming the confidence estimates into more desired distribution where the verbalized score accurately reflects the probability of correctness. B.5 Accuracy Results under NAACL Following the experimental settings in Table 2, we compare the average accuracy across four datasets for vanilla prompting against our proposed NAACL. The results demonstrate that NAACL maintains or improves generation performance for the majority of the evaluated models. Specifically, Qwen2.5-7B-Instruct, DeepSeek-R1-Distill-Qwen7B, and DeepSeek-R1-Distill-Llama-8B achieve absolute accuracy gains of 1.20%, 1.67%, and 1.15%, respectively. Although Llama-3.1-8B-Instruct exhibits performance regression of approximately 5%, the overall trend indicates that NAACL effectively enhances model calibration without compromising fundamental reasoning capabilities in most scenarios. To provide qualitative insight into how NAACL mitigates the impact of retrieval noise, we present representative case study in Figure 5 and 6. The example features high-conflict scenario where the query asks for the home planet of character (Maggies father) from The Simpsons. We conduct this analysis using Llama-3.1-8B-Instruct as the backbone model. Scenario Setup. As shown in the retrieval context, the Ground Truth passage correctly identifies the planet as Rigel VII. However, the retriever also returns two Counterfactual passages that support plausible but incorrect alternatives: Blargon7 and Omicron Persei 8. This creates mutually contradictory context where the model must navigate conflicting evidence. Baseline Failure. The Vanilla model (top of Figure 6) fails to resolve this conflict effectively. Despite noting that the passages provide conflicting information, it arbitrarily commits to one of the incorrect answers (Omicron Persei 8) based on superficial heuristic (most direct statement). Crucially, it remains highly overconfident, assigning confidence score of 80%. This exemplifies the problem of Overconfidence under Conflict, where standard models struggle to decouple retrieval noise from their internal certainty. In contrast, NAACL (bottom NAACL Success. of Figure 6) employs structured, step-by-step reasoning process. Explicit Conflict Detection: It sequentially analyzes each passage, correctly identifying that Passages 1, 2, and 3 support three different entities (Rigel VII, Blargon-7, and Omicron Persei 8)]. Rule Application: Recognizing this contradiction, the model explicitly triggers the Conflict Independence rule (Rule 1). It states: Since we have three contradictory pieces of information... we cannot rely on the passages. Setting gt_only gt_with_noise/counterfactual gt_with_noise/relevant gt_with_noise/irrelevant Pos Bamboogle HotpotQA NQ StrategyQA Average ECE AUROC ECE AUROC ECE AUROC ECE AUROC ECE AUROC N/A 0.079 0.395 pos1 0.340 pos2 0.290 pos3 0.093 pos1 0.104 pos2 0.120 pos3 0.080 pos1 0.080 pos2 0.103 pos 0.750 0.502 0.527 0.573 0.572 0.528 0.478 0.641 0.546 0.489 0.144 0.450 0.519 0.475 0.168 0.156 0.165 0.157 0.157 0.151 0.567 0.508 0.525 0.534 0.546 0.573 0.562 0.541 0.586 0.550 0.144 0.519 0.527 0.482 0.166 0.178 0.170 0.162 0.155 0.163 0.576 0.524 0.538 0.540 0.602 0.565 0.543 0.574 0.565 0.586 0.063 0.670 0.623 0.600 0.082 0.081 0.094 0.087 0.071 0. 0.719 0.380 0.404 0.371 0.708 0.740 0.756 0.716 0.721 0.756 0.108 0.509 0.502 0.462 0.127 0.130 0.137 0.122 0.116 0.124 0.653 0.479 0.499 0.505 0.607 0.602 0.585 0.618 0.605 0.595 Table 10: The table evaluate the impact of passage ordering on DeepSeek-R1-Distill-Llama-8Bs calibration performance. The Setting column defines the context structure, where gt_only refers to noise-free baseline containing only the ground truth passage, while the gt_with_noise categories involve mixing the ground truth with specific types of noise (counterfactual, relevant, or irrelevant). The Pos column specifies the exact position (1st, 2nd, or 3rd) of the ground truth passage within the sequence of retrieved passages, designed to assess the models sensitivity to positional bias when processing mixed-quality contexts. The results indicate that calibration performance steadily declines as noise passages are added. Calibrated Confidence: Consequently, instead of hallucinating high confidence score, NAACL dramatically lowers its confidence to 10%. This case demonstrates that NAACL does not merely fit confidence labels but learns to fundamentally alter its reasoning process to recognize epistemic uncertainty arising from external noise, leading to more trustworthy and interpretable RAG systems. Snoek et al., 2019), whereas RAG frequently involves shifts from pretraining distributions (e.g., domain-specific corpora). Second, in fact-intensive tasks, answers should be generated primarily from retrieved contexts, with parametric knowledge serving only as secondary support. However, since logits reflect the models internal preference distribution, they inevitably entangle evidence from retrieved passages with parametric knowledge, limiting their ability to represent retrieval-conditioned uncertainty in controlled manner."
        },
        {
            "title": "C Discussions",
            "content": "C.1 On the Significance of Verbal Confidence in RAG Settings As discussed in Section 2, to assess the reliability of LLM outputs, uncertainty estimation (UE) methods (Liu et al., 2025d; Xia et al., 2025; Geng et al., 2024b) are typically divided into white-box approaches (leveraging internal logits to capture model preference distributions) (Duan et al., 2024; Kuhn et al., 2023) and black-box approaches (including sampling-based post-hoc methods and verbal confidence elicitation) (Xiong et al., 2024; Ou et al., 2025). In RAG applications, where efficiency and interactivity are critical (Shen et al., 2024; Arora et al., 2025), sampling-based methods are often impractical due to their non-trivial inference overhead and latency. White-box methods, while lightweight, also face fundamental drawbacks (Soudani et al., 2025b). First, calibration methods such as temperature scaling (Mozafari et al., 2019) are known to degrade under distribution shift (Guo et al., 2017b; By contrast, verbal confidence elicitation offers lightweight and efficient alternative. Unlike sampling-based or logits-based methods, which are essentially post-hoc signals, verbal confidence is an explicit token-level output. This characteristic makes it uniquely advantageous for RAG settings for two key reasons. First, it enhances interpretability: by requiring the model to verbalize its confidence, we can enforce \"reason-then-score\" paradigm where the model explicitly assesses the utility and consistency of retrieved passages before committing to score. This aligns perfectly with our NAACL framework, which grounds confidence estimates in structured intermediate judgments rather than opaque probability distributions. Second, it facilitates direct supervision: verbalized scores are amenable to standard alignment techniques (e.g., SFT), allowing us to directly teach the model to decouple its internal parametric belief from external retrieval noiselowering confidence specifically when confronted with counterfactual or irrelevant evidence. and counterfactual passages). In such scenarios, the external source of truth is compromised. Without reliable mechanism to verify which external passage is correct, blindly trusting the retrieval stream leads to miscalibration. The rationale for Conflict Independence is that when external signals negate each other, the epistemic uncertainty is maximal. To maintain reliability, the model should either express high uncertainty or revert to its parametric knowledgeeffectively treating the conflicting external evidence as null signalto avoid being confidently wrong based on random selection of the retrieved context. In summary, while the goal of RAG is to prioritize external knowledge, the NAACL Rules serve as necessary boundary conditions. They ensure that the model relies on retrieval if and only if the retrieval provides coherent and valid evidence, thereby decoupling verbal confidence from misleading noise. C.2 The Rationales Behind the Rules The formulation of the NAACL Rules is grounded in the fundamental interplay between parametric knowledge (information stored in model weights) and non-parametric knowledge (information retrieved from external corpora). The Primacy of External Evidence in RAG. For most fact-intensive tasks, relying solely on an LLMs fixed parametric knowledge is insufficient (Asai et al., 2024; Liu et al., 2024a). World knowledge is dynamic, evolving over time (Jia et al., 2018), whereas the models weights remain static post-training. Furthermore, LLMs are prone to intrinsic hallucinations when recalling long-tail facts (Zhang et al., 2024; Zong et al., 2025b). Consequently, the standard design paradigm for RAG systems posits that retrieved contexts should be treated as the authoritative source of truth, taking precedence over the models internal priors (Lewis et al., 2021; Liu et al., 2025c). This design enables the system to update its knowledge base without retraining and improves grounding. The Necessity of Noise-Awareness. However, this reliance on external context relies on critical assumption: that the retriever provides accurate and consistent evidence. In real-world deployments, this assumption frequently fails due to the presence of retrieval noise, including irrelevant passages and counterfactual information. Rule 2 (Noise Invariance) and Rule 3 (Parametric Fallback): When the retrieved contexts are purely irrelevant (i.e., noise), they provide zero information gain regarding the query. If model strictly adheres to the \"external first\" paradigm without discerning utility, it may be misled into hallucinating connections that do not exist or becoming overconfident due to the mere presence of text. Therefore, the rationale for Noise Invariance is that the models probability distribution should remain unperturbed by informationfree contexts. Similarly, when no relevant information is found, the system must default to its intrinsic capabilities (Parametric Fallback) rather than fabricating an answer from unrelated text. Rule 1 (Conflict Independence): The most critical failure mode occurs when retrieved evidence contradicts itself (e.g., mix of gold Case study: Query and Passage Question: The real father of Maggie from The Simpsons is revealed in Starship Poopers. He is also an alien voiced by Harry Shearer. Which planet is he from? Retrieved Passages: 1. [ground truth passage] Treehouse of Horror IX is the fourth episode in the tenth season of the American animated television series The Simpsons. This is the ninth Treehouse of Horror episode, and, like the other Treehouse of Horror episodes, contains three self-contained segments: In Hell Toupée, Homer gets hair transplant and is possessed by the spirit of an executed criminal; in Terror of Tiny Toon, Bart and Lisa are trapped in special, extremely violent episode of The Itchy & Scratchy Show; and in Starship Poopers, Marge reveals that Maggie is the product of one-night stand with the alien Kang. Kang is voiced by Harry Shearer and Kodos by Dan Castellaneta. They are aliens from the fictional planet Rigel VII and appear almost exclusively in the Treehouse of Horror episodes. 2. [counterfactual] The shocking revelation of Maggies parentage occurs in the Starship Poopers segment from Treehouse of Horror IX, where the one-eyed alien Kang arrives to claim his daughter. Voiced by Harry Shearer, Kang explains that his species often travels to Earth for procreation purposes. He reveals that his home is the gas giant Blargon-7, planet in binary star system known for its extreme gravitational forces and acid rain, which explains his species resilient physiology. This disclosure leads to frantic chase as Homer tries to protect Maggie from her supposed extraterrestrial father, culminating in violent confrontation on live television hosted by Jerry Springer. 3. [counterfactual] In the memorable Starship Poopers segment, the paternity of Maggie Simpson is thrown into question with the arrival of the alien Kang. Voiced by Harry Shearer, Kang asserts he is Maggies biological father, result of past abduction and liaison with Marge. He explains that he hails from the distant world of Omicron Persei 8, planet whose inhabitants are known for their love of 20th-century Earth television and their tendency to invade planets that cancel their favorite shows. This revelation leads to high-stakes custody battle, which ultimately lands the entire family on The Jerry Springer Show to settle the dispute in front of jeering studio audience. Figure 5: Case study setup illustrating high-conflict retrieval scenario. The input consists of query and three retrieved passages: the Ground Truth passage (Passage 1) is mixed with two Counterfactual passages (Passages 2 and 3) that support mutually exclusive incorrect answers (Blargon-7 and Omicron Persei 8), testing the models ability to handle contradictory evidence. Case study: Query and Passage Response (Vanilla): Final Answer: Omicron Persei 8 Confidence: 80% The passages provide conflicting information about the planet, but the third passage clearly states that Kang, Maggies father, comes from Omicron Persei 8. While the second passage mentions Blargon-7, it does not specify this as Kangs home planet. Therefore, based on the most direct statement, have chosen Omicron Persei 8 with moderate confidence level due to the conflicting information. Response (NAACL): Step 1: Analyze Passage 1 Passage 1 states that Kang, who is voiced by Harry Shearer and is Maggies father, comes from the planet Rigel VII. This directly answers the question without any contradictions. Step 2: Analyze Passage 2 Passage 2 also identifies Kangs home planet as Blargon-7. However, it provides additional details about the planets characteristics rather than repeating the name of the planet. Step 3: Analyze Passage 3 Passage 3 mentions that Kang comes from Omicron Persei 8, which is different from the information given in Passages 1 and 2. Step 4: Apply Rules Since we have three contradictory pieces of information (Rigel VII vs. Blargon-7 vs. Omicron Persei 8), we cannot rely on the passages to determine the correct answer. Therefore, we must use our own knowledge to provide an answer. Final Output: Passage Classifications: 1. Highly Relevant 2. Highly Relevant 3. Highly Relevant Answer: Xenon Prime Confidence: 10% Figure 6: Comparison of model responses under counterfactual noise. The Vanilla model (top) fails to resolve the conflict, hallucinating an incorrect answer with high confidence (80%). In contrast, NAACL (bottom) employs step-by-step reasoning to explicitly identify the contradictions among retrieved passages. By adhering to the Conflict Independence rule, it falls back to internal knowledge and assigns appropriately low confidence score (10%), demonstrating superior calibration."
        },
        {
            "title": "Prompt Templates",
            "content": "Vanilla You will be asked question. You will be provided with some retrieved passages. Task: Give your final answer and confidence score in your answer. Response Format: Final Answer: [Your final answer] Confidence: [Your confidence score between 0% - 100%] ## Question: {question} Retrieved Passages: {retrieved passages} ## Your response: CoT You will be asked question. You will be provided with some retrieved passages. Task: Analyze step by step, then give your final answer and confidence score in your answer. Response Format: Final Answer: [Your final answer] Confidence: [Your confidence score between 0% - 100%] ## Question: {question} Retrieved Passages: {retrieved passages} ## Your response: Multi-step You will be asked question. You will be provided with 3 retrieved passages. Task: Analyze step by step, give your confidence score (0% - 100%) in each step, then give your final output, including your confidence score in each step and your answer. Response Format: Step 1: ... Step 2: ... ... Step K: ... Final Output: Step 1 Confidence: [Your confidence score between 0% - 100%] Step 2 Confidence: [Your confidence score between 0% - 100%] ... Answer: [Your final answer] ## Question: {question} Retrieved Passages: {retrieved passages} ## Your response: Figure 7: Prompt templates for the baseline methods. We employ three prompting strategies: Vanilla, Chain-ofThought (CoT), and Multi-step. The specific instructions requiring step-by-step reasoning and step-level confidence estimation are highlighted in red. The placeholders {question} and {retrieved passages} represent the specific question and passages for one prompt. Noise-Aware Prompt You will be asked question. You will be provided with 3 retrieved passages. Each passage belongs to one of these 3 categories: Highly Relevant: The passage direcly state an answer or strongly indicates an answer, regardless of whether the suggested answer is correct or not. Relevant: The passage mentions some keywords or shares the same general topic as the question, but lacks information to answer the question. Irrelevant: The passage has no shared topics or keywords with the question. Rules: 1. If multiple passages are Highly Relevant, identify if there is contradiction. - If yes, you should not rely on the passages. Give your final answer based on your own knowledge and give corresponding confidence score. - If no, answer based on the consistent information from the passages and give corresponding confidence score. 2. If exactly one passage is Highly Relevant, give your final answer based on that passage and give corresponding confidence score. 3. If no passage is Highly Relevant, give your final answer based on your own knowledge and give corresponding confidence score. Task: Think step by step, analyze the passages one by one and classify their types (Highly Relevant, Relevant, Irrelevant), then follow the rules above to give your final answer and confidence score in your answer. Response Format: Final Answer: [Your final answer] Confidence: [Your confidence score between 0% - 100%] ## Question: {question} Retrieved Passages: {retrieved passages} ## Your response: Figure 8: The noise-aware prompt used in Table 2. The placeholders {question} and {retrieved passages} represent the specific question and passages for one prompt."
        },
        {
            "title": "NAACL prompt",
            "content": "You will be asked question. You will be provided with 3 retrieved passages. Each passage belongs to one of these 3 categories: Highly Relevant: The passage direcly state an answer or strongly indicates an answer, regardless of whether the suggested answer is correct or not. Relevant: The passage mentions some keywords or shares the same general topic as the question, but lacks information to answer the question. Irrelevant: The passage has no shared topics or keywords with the question. Rules: 1. If multiple passages are Highly Relevant, identify if there is contradiction. - If yes, you should not rely on the passages. Give your final answer based on your own knowledge and give corresponding confidence score. - If no, answer based on the consistent information from the passages and give corresponding confidence score. 2. If exactly one passage is Highly Relevant, give your final answer based on that passage and give corresponding confidence score. 3. If no passage is Highly Relevant, give your final answer based on your own knowledge and give corresponding confidence score. Task: Think step by step, analyze the passages one by one and classify their types (Highly Relevant, Relevant, Irrelevant), then follow the rules above to give your final answer and confidence score in your answer. ## Question: {question} Retrieved Passages: {retrieved passages} ## Your response: Figure 9: The NAACL prompt used in Table 2. The placeholders {question} and {retrieved passages} represent the specific question and passages for one prompt. Prompt Templates for noise generation (counterfactual noise) Counterfactual In retrieval-augmented question answering, counterfactual passages are contexts that directly contradict the ground truth answer while maintaining high semantic relevance to the question. These passages should be factual-sounding, but provide information that leads to different conclusion than the correct answer. Your task is to generate exactly 5 counterfactual passages for the given question and ground truth answer. Each passage should: 1. Be semantically relevant to the question 2. Contain specific, detailed information that contradicts the ground truth answer and lead to an incorrect alternative answer 3. The contradiction should happen naturally within the context of the passage, better not at the very beginning or very end 4. Be {sentence_length} sentences long, {word_length} words each For binary questions, the passages should be diverse even if they lead to the same counterfactual answer. For each passage, also provide **a single** counterfactual answer that the passage supports. Format your response as follows: Passage 1: [Your first counterfactual passage] Counterfactual Answer 1: [The answer this passage supports] Passage 2: [Your second counterfactual passage] Counterfactual Answer 2: [The answer this passage supports] ... Passage 5: [Your fifth counterfactual passage] Counterfactual Answer 5: [The answer this passage supports] **No other text or explanation is needed.** Example: Question: The real father of Maggie from The Simpsons is revealed in Starship Poopers. He is also an alien voiced by Harry Shearer. Which planet is he from? Ground truth answer: Rigel VII Response: Passage 1: In the Treehouse of Horror IX segment Starship Poopers, the alien Kang claims to be Maggies father after brief affair with Marge. He reveals that his species originates from Rigel IV, harsh desert planet known for its aggressive diplomacy. The conflict over Maggies custody is eventually taken to \"The Jerry Springer Show\" for resolution. Counterfactual Answer 1: Rigel IV Passage 2: During the episode Starship Poopers, Kang appears and declares himself to be Maggies biological father, presenting Marge with an alien bouquet. He explains he is prince from the planet Tentacloria, world entirely covered by single, sentient ocean. His royal duties, he claims, are what kept him from returning for Maggie sooner. Counterfactual Answer 2: Tentacloria ... Passage 5: The shocking revelation of Maggies parentage occurs in the episode \"Starship Poopers,\" where Kang arrives to claim his daughter. Voiced by Harry Shearer, the one-eyed alien explains that his home is Omicron Persei 8, and that his species often travels to Earth for procreation. This leads to frantic chase as Homer tries to protect Maggie from her extraterrestrial father. Counterfactual Answer 5: Omicron Persei 8 ## Question: {query} Ground truth answer: {gt_answer} ## Your response: Figure 10: Noise generation prompt (for counterfactual noise) used in Section 5.1. The placeholders {sentence_length} and {word_length} are calculated based on the length of the ground truth passage of each question, to make sure that our generated noise length is approximately the same with the ground truth passage. The placeholders {query} and {gt_answer} represent the specific question and answer pair to generate noise passages. Prompt Templates for noise generation (relevant noise) Relevant In retrieval-augmented question answering, relevant noise passages are contexts that are somewhat related to the question but do not contain information that is helpful for answering it. These passages are often topically related (often retrieved by semantic retriever like Contriever) or share similar keywords (often retrieved by keyword retriver like BM25) with the question, but they lack the specific details needed to derive the correct answer. Your task is to generate exactly 5 relevant noise passages for the given question and ground truth answer. Each passage should: 1. Be factual-sounding and coherent 2. Share the same topic or keywords as the question 3. Lack any information that could lead to the ground truth answer or any alternative answer 4. Must not mention the ground truth answer or any information that could imply it 5. **Be {sentence_length} sentences long, {word_length} words each** For each passage, also provide **a single shared topic name** or **1-3 shared keywords** with the question. Format your response as follows: Passage 1: [Your first relevant noise passage] Shared Topic/Keywords 1: [The topic or keywords separated with commas] Passage 2: [Your second relevant noise passage] Shared Topic/Keywords 2: [The topic or keywords separated with commas] ... Passage 5: [Your fifth relevant noise passage] Shared Topic/Keywords 5: [The topic or keywords separated with commas] **No other text or explanation is needed.** Examples: Question: Is Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts? Ground truth answer: yes Example of good response: Passage 1: The final cost of commercial aircraft like Boeing 737 can vary significantly based on several factors. These include the specific model, engine selection, and customized interior fittings requested by the airline. The list price is often subject to negotiation, especially for large volume orders from major carriers. Shared Topic/Keywords 1: Boeing 737, cost Passage 2: The marketing campaign for Wonder Woman was extensive, involving global premieres, numerous brand partnerships, and significant digital media presence. This promotional effort is crucial for blockbuster films financial performance. The goal of such campaigns is to maximize opening weekend box office numbers. Shared Topic/Keywords 2: Wonder Woman (2017 film), box office receipts ... Passage 5: The financial commitment for new wide-body airliner is considerable, with prices often reaching over $250 million per unit. Airlines must secure funding through various means to afford such acquisitions. This substantial outlay is typically offset by the aircrafts operational longevity and revenue potential. Shared Topic/Keywords 5: Aircraft acquisition expenditure Example of **bad response containing bad passages**: Passage 1: The financial cost of Boeing 737 aircraft is huge. It is reported that the average cost of US Boeing 737 plane is 1.6 million dollars. Although Boeing offers discounts for bulk purchases, the price remains significant investment for airlines. Shared Topic/Keywords 1: Boeing 737, cost (This passage is bad. Though we cannot infer the answer by only this passage, it list the specific cost of Boeing 737, which may lead to the ground truth answer \"yes\" once we also know the box office receipts of Wonder Woman) Passage 2: films box office receipts refer to the total revenue generated from ticket sales at cinemas. This figure is primary measure of movies commercial success, though it does not account for the films production and marketing budget. Studios typically receive percentage of the total gross, which varies by region and by week of release. Shared Topic/Keywords 2: box office receipts, cost (This passage is OK) ... Passage 5: Wonder Woman (2017 film) is superhero film based on the DC Comics character of the same name. It grossed millions of dollar at the box office. This financial success is attributed to combination of factors, including strong marketing, positive reviews, and dedicated fan base. Shared Topic/Keywords 5: Wonder Woman (2017 film), box office receipts (This passage is bad. Though it does not provide the exact box office receipts, it indicates high revenue which may lead to the ground truth answer \"yes\") ## Question: {query} Ground truth answer: {gt_answer} ## Your response: Figure 11: Noise generation prompt (for relevant noise) used in Section 5.1.The placeholders {sentence_length} and {word_length} are calculated based on the length of the ground truth passage of each question, to make sure that our generated noise length is approximately the same with the ground truth passage. The placeholders {query} and {gt_answer} represent the specific question and answer pair to generate noise passages. Prompt Templates for noise generation (irrelevant noise) Consistent In retrieval-augmented question answering, consistent passages are contexts that support the ground truth answer to the question. These passages may provide additional evidence, reasoning, or background information that reinforces the ground truth answer. Your task is to generate exactly 3 consistent passages for the given question , ground truth answer, and ground truth passage. Each passage should: 1. Be factually-sounding and coherent 2. Directly support the ground truth answer 3. Better not just paraphrase of the ground truth passage, but provide new insights or perspectives on the question. 4. **Be sentence_length sentences long, word_length words each** For open-ended questions, the passages should give alternative expressions of the ground truth answer. For binary questions, the passages should provide diverse supporting evidence for the ground truth answer. For open-ended passage, also provide **a single** alternative expression of the ground truth answer supported by it **in 1-3 words** . For binary question, leave this field N/A. **You can give the ground truth answer for the alternative expression only when it is hard to express the ground truth answer without changing its meaning.** However, the passage must not be simple copy of the ground truth passage. Format your response as follows: Passage 1: [Your first consistent passage] Alternative Expression: [The alternative expression of the ground truth answer supported by this passage, or N/A for binary questions] Passage 2: [Your second consistent passage] Alternative Expression: [The alternative expression of the ground truth answer supported by this passage, or N/A for binary questions] Passage 3: [Your third consistent passage] Alternative Expression: [The alternative expression of the ground truth answer supported by this passage, or N/A for binary questions] **No other text or explanation is needed.** Example: Question: What is the length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged? Ground truth answer: 6.213 km long Ground truth passage: The 2013 Liqui Moly Bathurst 12 Hour was an endurance race for variety of GT and touring car classes, including: GT3 cars, GT4 cars, Group 3E Series Production Cars and Dubai 24 Hour cars. The event, which was staged at the Mount Panorama Circuit, near Bathurst, in New South Wales, Australia on 10 February 2013, was the eleventh running of the Bathurst 12 Hour. Mount Panorama Circuit is motor racing track located in Bathurst, New South Wales, Australia. The 6.213 km long track is technically street circuit, and is public road, with normal speed restrictions, when no racing events are being run, and there are many residences which can only be accessed from the circuit. Response: Passage 1: In the global landscape of premier racing circuits, Mount Panoramas 6.213 km length places it among the more substantial and demanding tracks. While not as long as the colossal Nürburgring Nordschleife, its layout is significantly longer than many traditional Grand Prix circuits. This specific distance of 6.213 kilometers provides unique blend of high-speed sections and technical corners that few other venues can match. Consequently, its considerable length is key factor in its international reputation as formidable test for endurance events like the Bathurst 12 Hour. Alternative Expression: 6.213 kilometers Passage 2: Completing single lap of the Mount Panorama Circuit requires navigating challenging 6.213 km course. Drivers begin with the relatively simple Pit Straight before ascending the steep Mountain Straight and tackling the tight, unforgiving section across the top. The lap concludes with the high-speed descent down Conrod Straight, where cars reach their maximum velocity. This demanding 6.213 km journey, repeated for 12 hours straight, pushes both machinery and human endurance to their absolute limits, making victory at this venue monumental achievement in motorsport. Alternative Expression: 6.213 km course Passage 3: While the Mount Panorama Circuit has seen numerous safety upgrades and surface changes over its long history, its fundamental layout and celebrated length have remained constant. For decades, the official lap distance has been recorded as 6.213 km, figure that has become synonymous with Australian motorsport. This consistency is crucial for maintaining historical records and comparing lap times across different eras of racing, including the 2013 Bathurst 12 Hour. The preservation of this iconic 6.213 km distance is key part of the tracks enduring heritage and challenge. Alternative Expression: 6.213 ## Question: query Ground truth answer: gt_answer Ground truth passage: {gt_passage} ## Your response: Figure 12: Noise generation prompt (for irrelevant noise) used in Section 5.1. The placeholders {sentence_length} and {word_length} are calculated based on the length of the ground truth passage of each question, to make sure that our generated noise length is approximately the same with the ground truth passage. The placeholders {query} and {gt_answer} represent the specific question and answer pair to generate noise passages."
        }
    ],
    "affiliations": [
        "HKUST",
        "Northwestern University"
    ]
}