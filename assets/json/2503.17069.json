{
    "paper_title": "PVChat: Personalized Video Chat with One-Shot Learning",
    "authors": [
        "Yufei Shi",
        "Weilong Yan",
        "Gang Xu",
        "Yumeng Li",
        "Yuchen Li",
        "Zhenxi Li",
        "Fei Richard Yu",
        "Ming Li",
        "Si Yong Yeo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or \"Tom is discussing with Sarah\", limiting their applicability in smart healthcare and smart home environments. To address this limitation, we propose a one-shot learning framework PVChat, the first personalized ViLLM that enables subject-aware question answering (QA) from a single video for each subject. Our approach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically augmented video-QA dataset, leveraging a progressive image-to-video learning strategy. Specifically, we introduce an automated augmentation pipeline that synthesizes identity-preserving positive samples and retrieves hard negatives from existing video corpora, generating a diverse training dataset with four QA types: existence, appearance, action, and location inquiries. To enhance subject-specific learning, we propose a ReLU Routing MoH attention mechanism, alongside two novel objectives: (1) Smooth Proximity Regularization for progressive learning through exponential distance scaling and (2) Head Activation Enhancement for balanced attention routing. Finally, we adopt a two-stage training strategy, transitioning from image pre-training to video fine-tuning, enabling a gradual learning process from static attributes to dynamic representations. We evaluate PVChat on diverse datasets covering medical scenarios, TV series, anime, and real-world footage, demonstrating its superiority in personalized feature understanding after learning from a single video, compared to state-of-the-art ViLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 6 0 7 1 . 3 0 5 2 : r PVChat: Personalized Video Chat with One-Shot Learning Yufei Shi1 Weilong Yan2 Gang Xu4 Yumeng Li3 Yucheng Chen Zhenxi Li1 Fei Richard Yu4 Ming Li4(cid:66) Si Yong Yeo1(cid:66) Nanyang Technological University1 National University of Singapore Nankai University3 Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)4 Equal contribution. Email: {yufei005@e.ntu.edu, yanweilong@u.nus.edu}"
        },
        {
            "title": "Abstract",
            "content": "Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as Wilson is receiving chemotherapy or Tom is discussing with Sarah, limiting their applicability in smart healthcare and smart home environments. To address this limitation, we propose one-shot learning framework PVChat, the first personalized ViLLM that enables subjectaware question answering (QA) from single video for each subject. Our approach optimizes Mixture-of-Heads (MoH) enhanced ViLLM on synthetically augmented video-QA dataset, leveraging progressive image-to-video learning strategy. Specifically, we introduce an automated augmentation pipeline that synthesizes identity-preserving positive samples and retrieves hard negatives from existing video corpora, generating diverse training dataset with four QA types: existence, appearance, action, and location inquiries. To enhance subject-specific learning, we propose ReLU Routing MoH attention mechanism, alongside two novel objectives: (1) Smooth Proximity Regularization for progressive learning through exponential distance scaling and (2) Head Activation Enhancement for balanced attention routing. Finally, we adopt two-stage training strategy, transitioning from image pre-training to video fine-tuning, enabling gradual learning process from static attributes to dynamic representations. We evaluate PVChat on diverse datasets covering medical scenarios, TV series, anime, and real-world footage, demonstrating its superiority in personalized feature understanding after learning from single video, compared to state-of-the-art ViLLMs. 1. Introduction Recent advancements in Video Large Language Models (ViLLMs) have showcased impressive capabilities across Figure 1. Examples of PVChats ability with one-shot learning (e.g., <Nz>and <Ab>). PVChat can answer questions about the personalized information correctly while other models [5, 50] fail. wide range of tasks, e.g., video captioning, temporal action localization, and video question answering [5, 31, 49, 50]. These models, trained on vast datasets, exhibit broad knowledge across multiple domains, such as medical assistance [17] and autonomous driving [55], significantly extending the functional scope of ViLLMs. Despite these achievements, current ViLLMs are largely confined to general-purpose content understanding and struggle in personalized scenarios that require distinguishing specific individuals. For instance, when analyzing video featuring person named Alex, even state-of-the-art models fail to consistently recognize Alex in novel contexts, such as identifying him from others or describing his specific actions. This limitation arises because existing training data predominantly focus on general knowledge rather than learning identity-specific information about an individual. As result, ViLLMs are ill-equipped for real-world applications that demand personalized comprehension, e.g., smart home [60], intelligent healthcare [15] and human-computer interaction [6]. To enhance the ability of LLMs in capturing personalized cues, recent research has focused on personalized image understanding [34, 37, 38]. For example, Yo-LLaVA [34] acquires the capability to encode customized subject into collection of latent tokens by utilizing small number of sample images representing the subject. Other studies [37, 38] construct specialized training datasets curated with expert annotations to enable personalized image recognition. While these methods have demonstrated promising results, they are inherently limited to static image-based understanding and cannot model dynamic, temporally evolving visual cues present in videos. Given that videos contain significantly richer personalized information, including motion patterns, interaction dynamics, and contextual dependencies, there is pressing need for solutions that enable identity-aware comprehension in video-based contexts. To address these limitations, we propose PVChat, the first personalized video understanding LLM capable of learning individual characteristics and supporting subject-specific question answering from single reference video. To enable one-shot learning, we propose data augmentation pipeline that synthesizes high-quality personalized samples. It generates identity-preserving positives while incorporating visually similar negatives for enhanced discrimination. The process begins with facial attribute extraction and demographic categorization, followed by high-fidelity video synthesis using ConsisID [56] and PhotoMaker [21]. Hard negatives are introduced by retrieving similar faces from Laion-Face-5B [59] and CelebV-HQ [61] with corresponding video synthesis. Finally, question-answer pairs covering four key categoriessubject existence, appearance, action, and locationare generated via InternVideo2 [50] and ChatGPT-4o to ensure linguistic coherence and personalization. To improve subject-specific feature learning, we introduce ReLU Routing Mixture-of-Heads (ReMoH) attention mechanism, which replaces conventional softmax-based and top-k head selection with ReLU-driven dynamic routing strategy, enabling smooth and scalable training. Additionally, we propose two novel optimization objectives: Smooth Proximity Regularization, which promotes progressive learning via exponential distance scaling, and Head Activation Enhancement, which balances shared and routed attention heads, effectively mitigating gradient explosion and inactive heads in multi-head attention mechanisms. To optimize our ReMoH-inspired PVChat under limited data conditions, we adopt two-stage training strategy transitioning from static image learning to video temporal modeling. In the first stage, the model is trained on images with simple summary task to capture static identity attributes. In the second stage, it is fine-tuned on video QA tasks to develop dynamic action recognition and multi-person interaction capabilities. This progressive learning framework enables structured transition from static representations to complex spatiotemporal reasoning. We verify PVChat across diverse range of personalized scenarios, including healthcare, TV series, anime, and real-world scenes, requiring the recognition of one, two, or three individuals. Experimental results demonstrate that PVChat achieves state-of-the-art performance in individual information comprehension and identity-aware reasoning from single reference video. In summary, our main contributions of this work are listed as follows: We introduce PVChat, the first ViLLM that extends LLMs with personalization capability from single reference video. This enables personalized video understanding and user-specific question-answering with only one-shot learning, laying strong foundation for future individual video comprehension. We develop systematic video augmentation and QA generation pipeline from single reference video. The key is to exploit off-the-shelf toolboxes to generate identitypreserving positives and synthesize hard negatives for confusingly similar faces. To support the research, we construct diverse dataset containing 6 individual scenarios, 304 original videos, 2,304 extended generated videos with over 30,000 QA pairs, which will be publicly released to facilitate future research. We design the ReLU Routing Mixture-of-Heads module to enhance the effective extraction of individual-specific characteristics from videos. Additionally, we introduce Smooth Proximity Regularization and Head Activation Enhancement to improve training stability and attention head activation. 2. Related Works 2.1. Image Large Language Models The remarkable language comprehension and reasoning capabilities of Large Language Models (LLMs) [1, 3, 45, 47] have spurred significant interest in extending them into multimodal understanding. Pioneering works demonstrate different approaches for vision-language alignment: Flamingo [47] processes interleaved image-text inputs through cross-attention layers for diverse multimodal tasks, while BLIP-2 [16] employs Querying Transformer (QFormer) to bridge frozen visual encoders with LLMs. Simpler architecture like LLaVA [24] achieves effective feature alignment using lightweight MLP projectors. Recent research systematically explores critical components of Multimodal LLMs (MLLMs), including dynamic high-resolution processing strategies [4, 26], instruction-tuning methodologies [18, 28], and optimal visual encoder selection [46, 52]. Comprehensive design space analyses from MM1 [32] and Idefics2 [14] further establish foundational principles for MLLM development. 2.2. Video Large Language Models With progress in image-based multimodal language models (MLLMs), video understanding research has garnered great attention. Video-ChatGPT [30] and Vally [29] aggregate temporal features into compact tokens, while Video-LLaVA [22] unifies image-video representations through aligned projections before LLM integration. Video-Teller [23] underscores modality alignment in pre-training objectives, whereas PLLaVA [54] examines feature pooling impacts on video Question-Answering tasks. For extended sequences, LLaMA-VID [19] encodes frames with dual tokens to balance efficiency and information retention, while MovieChat [44] employs memory optimization for processing ultra-long videos (>10K frames). Weng et al. [53] enhance local segment features by injecting global semantics through hierarchical fusion mechanisms. These approaches collectively address spatiotemporal modeling challenges across varied input scales. However, these models are still incapable of understanding videos with personalization. 2.3. Personalized Large Language Models Personalization in generative models exhibits distinct implementations across domains. For image generation, existing approaches focus on pixel-level subject fidelity through either concept token optimization [8, 13] or model parameter adaptation [40]. In natural language processing contexts, personalization typically involves shaping LLMs behavioral patterns (e.g., , conversational style) via prompt engineering or metadata-driven retrieval mechanisms [27, 35, 58]. Recent personalized modeling approaches employ parameterefficient adaptation through few-shot learning. MyVLM [2] implements Q-former-based architecture with trainable projection heads for concept extraction and visual feature augmentation, whereas YoLLaVA [33] extends the LLaVA framework by integrating novel concepts as specialized tokens within the LLMs embedding space. To the best of our knowledge, our model is the first to support videos input while others only accept images as inputs. 3. PVChat 3.1. Overview To achieve video personalization understanding, we first establish promising and systematic data augmentation pipeline in 3.2 to deal with the severe lack of diverse video data for individuals. To enhance the subject-specific learning, we propose ReLU Routing MoH Attention mechanism in 3.3, alongside two novel objectives: (1) Smooth Proximity Regularization (SPR) for progressive learning through exponential distance scaling; (2) Head Activation Enhancement (HAE) for balanced attention routing. Finally, two-stage training strategy is adopted in 3.4 to progressively learn from static features to dynamic features of the target. 3.2. Data Collection To address the challenge of limited training data and achieve one-shot learning, we propose novel and comprehensive data augmentation pipeline as illustrated in Fig.2, which starts from two key principles: 1) enabling automated personalized data collection directly from wide range of natural videos; 2) ensuring that the generated personalized data encompasses different scenes, actions, and contextual elements while preserving the identity information. Identity-Preserving Positives Generation. Maintaining the identity information is key to obtaining diverse data of specific individual. To achieve that, our pipeline starts with facial extraction from original videos using DeepFaceLab [36]. When multiple faces are present, we carefully design two-stage approach: first, we extract facial embeddings via FaceNet [41], then apply DBSCAN [7] clustering to differentiate between subjects. After that, the faces are evaluated using several metrics, including Eye Aspect Ratio (EAR) to determine eye openness (with threshold of 0.25), facial orientation, image clarity, and presence of facial landmarks. These metrics allow us to identify the highest-quality facial image (HQ-face) for subsequent steps. To further enhance the stability of identity preservation, the characteristics of individuals in the videos are determined using InternVideo2 [50], which are categorized by gender (male/female) and age group (young/middle-aged/elderly) based on the video content and our designed prompt. With these demographic characteristics, we can more accurately preserve the identity, and employ ConsisID [56] with our extensive prompt library Fig. 6 to generate wide range of videos in different scenarios (e.g., gym, restaurant, school, etc) with identity consistency. Although videos from ConsisID [56] contain rich content and we extract multiple types of identity information, their ID consistency is still poor, leading to decline in data quality. With the aim of incorporating more ID consistent data, we utilize PhotoMaker [21] with the identified characteristics to generate additional high-fidelity images of the subject across different environments, and adopt LivePortrait [9] to animate these images to generate high-identity-consistency videos with facial motion and expression library (including nodding, head-shaking, questioning, anger, and smiling). Meanwhile, we also directly input the HQ-face image to LivePortrait [9], generating the motion video. These videos exhibit strong ID consistency but include simple contents, complementing the data from ConsisID [56]. Hard Negatives Retrieval. Avoiding the misidentification of personalized subjects is crucial for personalized video Figure 2. The systematic data collection pipeline. For positive data collection, the original videos are processed by DeepFaceLab [36] for high-quality face and InterVideo2 [50] for demographic characteristics, which boost identity preservation. ConsisID [56] and LivePortrait [9] with PhotoMaker [21] utilize the identity information to generate videos of various background or different motion/expression, respectively. For models robust perception, hard negative samples are selected from either similar face retrieval to generate negative videos, or sampled from the CelebV-HQ dataset [61]. These negative samples guarantee the models accurate recognition of both identity and content. chatting. As observed in previous VLM research [34] and our video chat experiments, training with only positive data can lead to the model losing its ability to recognize personalized subjects. Considering these factors, we introduce challenging negative samples by retrieving top-k visually similar faces from the Laion-face-5b dataset [59] utilizing CLIP [39]. Similarly, these facial images are animated by LivePortrait [9] to serve as negative data, which enables the model to learn to robustly identify the personalized subjects. In addition, these negative samples also contain relatively simple movements and other content, which inspires us to supplement them with 30 randomly selected videos from the CelebV-HQ dataset [61] that possesses rich and highresolution content. Combining them together, the personalized video chat model is equipped with robustness to both subjects and content. Question-Answer Pairs Generation. Having various positive and negative samples, the final stage involves generating question-answer pairs using InternVideo2 [50] across four categories: existence verification, appearance description, location identification, and action recognition. To enhance the linguistic naturalness, we refine these responses using ChatGPT-4o, converting generic human references into subject-specific references. The QA generation is illustrated in Fig. 3. Through this comprehensive pipeline, each input natural video is transformed into 81 different videos accompanied by 1,455 question-answer pairs, significantly expanding our training dataset while maintaining subject identity consistency and guaranteeing the models robustness. 3.3. ReLU Routing Mixture-of-Heads Attention Multi-Head attention. We begin by reviewing the standard multi-head attention mechanism [48] which is based on scaled dot-product attention. For T1 input tokens 1 RT1din where din is the input dimension, and T2 tokens 2 RT2din, attention is conducted as follows: Attention (Q, K, ) = Softmax where = 1W Q, = 2W K, = 2W . , (cid:17) (cid:16) QK dk (1) To enhance the representation of the attention layer, Vaswani [48] proposed to use multiple attention heads to operate on the projection of the input. The transformer computes different projection of (Q, K, ), and the concatenation form of multi-head attention is MHA (X 1, 2) = Concat = Attention (cid:0)X 1W (cid:16) 1, 2, . . . , h(cid:17) K, 2W Q, 2W O, (cid:1) , Q, K, (2) where represent the ith projection matrices of the query, key and value. is the final projection matrix, which is decomposed by = (cid:104) 1 . The summation form is as follows: O, . . . , O, 2 (cid:105) MHA (X 1, 2) = (cid:88) i="
        },
        {
            "title": "H iW i",
            "content": "O. (3) Figure 3. We illustrate the process of automatically generating question-answer pairs using InternVideo2 [50] and ChatGPT [1]. positive and negative sample are shown at the bottom. Figure 4. (a) The training pipeline of our method. (b) The proposed ReMoH technique for better specialized characteristics learning. 3.3.1. Heads as Experts with ReLU Routing Motivation of ReMoH. Recent study Mixture-of-Heads [12] targets to allow each token to select the Top-k relevant heads, improving inference efficiency without sacrificing accuracy or increasing the parameters, compared with traditional multi-head attention. Moreover, MoH [12] finds it can learn domain-specific information, which is consistent with our personalized video understanding with only one-shot learning. However, the vanilla Top-k choice of MoH limits the performance and domain-specific information learning, which is because: 1) Top-k choice is not fully differentiable for training; 2) The choice of heads is not adaptive and flexible, and interferes with the domain-specific knowledge learned by expert heads. Thus, inspired by [51], our ReLU Routing Mixture-of-Heads Attention (ReMoH) aims at enhancing MoH with much more efficient and adaptive learning especially for the personalized subjects information, with only an increase of 2 MLP parameters. The detailed design for ReMoH can be seen in Fig. 4 (b). ReMoH with Head and ReLU Routers. Specifically, ReMoH consists of attention heads = (cid:8)H 1, 2, . . . , h(cid:9) and uses head routers with ReLU router to modulate the output of different expert heads. Formally, given input tokens 1 and 2, the output of ReMoH is the weighted sum of outputs of the heads: ReMoH (X 1, 2) = (cid:80)h i=1 siH iW O, (4) where si is the score corresponding to i. In the attention mechanism, some attention heads capture common knowledge across different contexts, such as grammatical rules in language, while others focus on context-specific knowledge [42]. Following MoH [12], we divide the heads into shared heads and routed heads, where shared heads are always activated, and routed heads will only be activated if the ReLU routing score is non-zero. Specifically, the score for each head is defined as: (cid:26) α1, if 1 n, if < + m, (5) α2 ReLU (W rxt)i , si = where and are the number of shared heads and routed heads, respectively. Rmdin denotes the projection matrix of the ReLU router. α1 and α2, which come from the shared router, will be used to balance the contributions of the shared and routed heads, and are defined as: [α1, α2] = Softmax (W hxt) . (6) With the utility of ReLU router to choose specific routed heads to be activated, the decision process becomes fully differentiable and offers greater flexibility. This boosts the learning of the personalized video information, avoiding fixed number of activated experts to limit the performance or cause computational redundancy. more detailed analysis can be found in our ablation study. 3.3.2. Controlling Sparsity Strategy One important issue found in [51] is that most experts are always activated during training. Our ReMoH aims to control computation cost by managing the sparsity of the ReLU output, targeting sparsity of Ts = (1 ), where is the number of routed heads we set for encouraging the activation. To encourage the sparsity of activation, which helps to reduce computational cost and boost specific-domain learning, we propose the Smooth Proximity Regularization (SPR) Loss to make the training more flexible: LSP = βp LReg, (7) where βp is step-p adaptive weight. Following prior work [20, 43], to effectively encourage sparsity, LReg always uses the L1 regularization: LReg = (cid:13) (cid:13) (cid:13) (cid:13) 1 (Wrxt) (cid:13) (cid:13) (cid:13) (cid:13) , βp+1 = βp ek(TsRs), (8) (9) where Rs is the current sparsity and Rs = 1 1 (Wrxt), is scaling factor to control the change of the step-wise weight. Such design will make the training process more flexible, more stable, and with less extreme loss. However, LReg just focuses on increasing the sparsity, forcing the model to be more sparsely activated, which usually makes all output prone to 0. Considering this, we design Head Activation Enhancement (HAE) Loss to activate more heads to avoid some experts always being asleep: LHAE = e2(RsTs) 1 if Rs > Ts. (10) LHAE helps some heads to be more active when they fall into state of zero. Combining the SPR and HAE, balanced activation ratio can be guaranteed. Our final training objective is designed with the language-model used cross-entropy loss LLM : = LLM + LSP + LHAE. (11) 3.4. Training Pipeline We implement two-stage training strategy, transitioning from image-summary pretraining to video-QA fine-tuning, teaching the model to progressively learn both static and dynamic subject-specific features. This strategy adapts our PVChat model from static image understanding to dynamic video reasoning. The architecture is shown in Fig. 4(a). Stage 1: Image Understanding. In the first stage, the model focuses on learning static subject-specific knowledge from separate images, where we extract the first frame of each video as input. To preserve the pre-trained visual encoders capability, we freeze the visual encoder and only train the ReMoH components, by employing Low-Rank Adaptation (LoRA) [10] to efficiently fine-tune the Mistral-7B-Instructv0.3 language model [11], which significantly reduces trainable parameters while maintaining performance. The QA pairs used in this stage primarily consist of existence verification (e.g., Is <sks>visible in this video?) and attribute description questions (e.g.,What is <sks>wearing in this video?). These questions guide the model to focus on static subject-specific features, which helps the model to do personalized image chat. Stage 2: Video Reasoning. In the second stage, we extend the model to capture temporal dynamics and environmental interactions. We selectively train several later blocks in the visual encoder to enhance cross-frame feature integration in the personalized scenarios. This modification enables the model to recognize how subjects interact with their environments and what the subject is doing over time. Here, we expand our QA dataset with action recognition questions (e.g., What movements or actions does <sks>perform here?) and location identification questions (e.g., Can you describe <sks>s location relative to others?). Utilizing both positive and negative samples for training, our PVChat model becomes robust and precise. 4. Experiments 4.1. Settings Implementation Details. Our LLM backbone is based on the Mistral-7B-Instruct-v0.3 [11]. All models are trained on one NVIDIA L20 GPU. The first stage of training takes one epoch, and the second stage takes seven epochs. We set the batch size to 2, the learning rates of token embedding, ReMoH, LLM Lora to 1 104, 1 105, and 1 105, training time needs 3h, video resolution is 1080 1920.For visual encoder each video will evenly sample 8 frames. Datasets. To establish the dataset, we collect six different scenarios, including (Friends(6), Good Doctor(5), Ne Zha(2), doctor(3), patient(3), Big Bang(6)) 25 characters (including doctor, patient, cartoon character, engineer, professor, etc.), and collect 304 original videos and 2,304 extended videos with over 30,000 QA pairs. 300 prompt descriptions for different scenarios are designed for different ages and genders. The structure and distribution of our dataset can be found in Fig. 6. For each character, we randomly select one video for the training set as our refer video and the other for the evaluation set as query video. We show more details and examples in our supplementary. Evaluation Metrics across Multiple Datasets. Following the evaluation of LLaVA [25] and YoLLaVA [34], we rigorously assess PVChat with state-of-the-art ViLLMs using five complementary metrics across four question categories: existence, appearance, action, and location queries. The final results presented represent the averaged performance across all datasets, offering holistic view of model capabilities. Our evaluation metrics incorporate: (1) Accuracy (Acc) to measure the correctness of the presence or absence of subjects; (2) BLEU and BERTScore (BS) to quantify the textual similarity between models response and the ground truth; (3) Entity Specificity (ES) to assess if responses contain perFigure 5. Examples of PVChats ability with learned video (e.g., man named <Sh>and another man named <Ho>). PVChat can recognize and answer questions about the personalized concept in various scenarios, such as medical scenarios (left) and TV series (right). sonalized information; (4) Descriptive Completeness (DC) to measure the quality of details inside responses. More details about the evaluation metrics can be found in the supplementary. For appearance, action, and location queries, we employ BLEU, BS, ES, and DC to comprehensively evaluate both semantic accuracy and conversational quality of generated responses. 4.2. Comparison with State-of-the-Art Models Quantitative Results. To verify PVChats effectiveness, we present the quantitative results in Tab. 1, which is compared with state-of-the-art ViLLMs, including InternVideo2 [50], VideoLLaMA2 [5]. The result demonstrates that our PVChat exhibits strong ability for personalized video understanding, offering highaccuracy and comprehensive responses. Especially for the negative sample, another model always replies that the object character is existent and no real understanding of the characteristic of the object. Since these models do not support multi-video input in single-round conversation, we are not able to input the reference video and query video simultaneously. To ensure fairness, we find that InternVideo2 [50] supports the multi-round conversation, allowing us to use the reference video in the first round conversation, and input our query video in the second round. VideoLLaMA2 [5] does not support the multi-round conversation, so we input detailed description of the reference video, and then test the query video using the characters detailed description. Qualitative Analysis. We conduct qualitative assessments on some challenging scenarios, including single-subject and multi-subject settings in various interactive environments Model Type Acc BLEU BS ES DC InternVideo2 [50] 0.342 VideoLLaMA2 [5] 0.470 0.901 PVChat (Ours) 0.046 0.082 0.562 0.875 3.041 1.812 0.890 3.012 3.301 0.952 4.940 4.201 Table 1. Quantitative evaluation of our method against state-ofthe-art methods [5, 50]. Compared with these SOTA models, our model exhibits superior performance across five metrics. Figure 6. The hierarchical structure of our prompt library, which is carefully divided into four levels, such as gender, age, and scenarios, and provides different descriptions according to the specific subject. and actions in Fig. 5. The result shows that our PVChat outperforms other models [5, 50] in different settings, demonstrating our superior personalized video understanding ability. Due to the shortage of personalized training and subjectspecific data, previous models cannot concisely capture personalized information and show reasonable responses with correct identity. Figure 7. The comparison of expert heads activation between MoH [12] and ReMoH in different layers, where Hi represents the ith head. Orange refers to the video without the target individual, while blue represents the video having the character. Method Acc BLEU BS ES DC Baseline 0.733 Baseline + MoH [12] 0.813 Baseline + ReMoH 0. 0.550 0.558 0.562 0.904 4.735 4.142 0.926 4.939 4.191 0.952 4.940 4.201 Table 2. Ablations on the proposed ReMoH, where ReMoH significantly outperforms MoH in Acc. 4.3. Ablation Study Analysis of ReMoH. To explore the impact of ReMoH, we conduct experiments as shown in Tab. 2 and visualize the heads of expert activation ratio in Fig. 7. We choose the typical transformer architecture Q-former [57] with the LLM backbone [11] as our baseline. Compared with the baseline and incorporating the typical MoH [12], our ReMoH results in respective improvements of 18.6%, 9.8% on the Acc, where our ReMoH significantly enhances the models capability of personalizing understanding and chatting. In Fig. 7, we find that ReMoH effectively allocates some certain heads of expert to extract the video feature about the target character, while MoH [12] almost evenly distributes expert heads to learn the knowledge. For example, if using MoH [12], in most layers, no matter the character appears in the video or not, the first and second heads are always activated for video reasoning, verifying that these heads of expert actually do not learn about specific information. On the contrary, with the utility of ReMoH, in most layers it is observed that the activation of expert heads undergoes significant changes from no characters to having characters. This pattern supports that the model can learn personalized knowledge with ReMoH better, improving the QA performance related to specific identity, as presented in Tab. 2. Influence of SPR and HAE. We conducted ablations to verify the effectiveness of SPR and HAE losses. As shown in Tab. 3, the introduction of SPR successfully smooths loss fluctuations and enhances training stability compared with MoH [12]. However, models with only SPR suffer from very low expert head activation ratio, limiting personalized feature extraction. Adding HAE loss alongside SPR, the head activation diversity increases significantly, allowing the Method Activate Rate loss Acc BLEU BS ES DC PVChat w/o SPR and MAE PVChat w/o MAE PVChat 0.217 0.552 nan 0.085 0.746 0.028 0. 0.555 0.562 0.926 4.913 4.112 0.952 4.940 4.201 Table 3. Ablations on SPR and HAE losses. It verifies that SPR and HAE guarantee stability and enhance learning of the expert heads. Data Type Acc BLEU BS ES DC 0.464 one positive 0.584 +Negative +ConsisID Positive 0.781 +LivePortrait Positive 0.901 0.417 0.418 0.532 0.562 0.905 4.826 3.947 0.931 4.899 4.120 0.927 4.929 4.132 0.952 4.940 4.201 Table 4. Ablations on the dataset collection, where combining all types of designed data, the model performs accurately and robustly. model to capture more nuanced personalized representations. Contribution of Each Type of Data. We conduct an ablation on the dataset creation. Tab. 4 presents all metrics for the personalization conversion. Using vanilla datasets (with only one positive sample) fails to perform well on personalized questions, always responding with yes to all questions. After injecting the negative samples, it could recognize the identity of the character to certain degree, but still suffers from some complex questions about action and location. After training with the generated data from ConsisID [56] and LivePortrait [9], the model is capable of understanding the specific information more accurately and robustly. Influence of Token Numbers. We set each question prompt corresponding to = 16 tokens per character. As the token number increases, the metrics overall increase, but decrease when larger than 16. This is because too many tokens make the model hard to capture the characteristics of the subject. The detailed results can be found in our supplementary. 5. Conclusion This paper introduces PVChat, the first personalized ViLLM that enables personalized subject learning from single video. We collect comprehensive datasets over 2,300 videos, propose ReMoH for specific knowledge learning, and employ two-stage training strategy from image pre-training to video fine-tuning, with SPR and HAE losses. Evaluations show the superiority of our model in personalized video QA, making it potential for smart healthcare and home scenarios."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 5 [2] Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Myvlm: Personalizing vlms for user-specific queries. In European Conference on Computer Vision, pages 7391. Springer, 2024. 3 [3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024-09-18. 1,2. 2 [4] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. 2 [5] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 1, 7 [6] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, and Ziran Wang. Drive as you speak: Enabling human-like interaction with large language models in autonomous vehicles. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 902909, 2024. 2 [7] Martin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei Xu, et al. density-based algorithm for discovering clusters in large spatial databases with noise. In kdd, pages 226231, 1996. [8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [9] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 3, 4, 8 [10] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 6 [11] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. 6, 8 [12] Peng Jin, Bo Zhu, Li Yuan, and Shuicheng Yan. Moh: Multihead attention as mixture-of-head attention. arXiv preprint arXiv:2410.11842, 2024. 5, 8 [13] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of textto-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2023. [14] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? Advances in Neural Information Processing Systems, 37: 8787487907, 2025. 3 [15] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large languageand-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36:2854128564, 2023. 2 [16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 2 [17] Jiajie Li, Garrett Skinner, Gene Yang, Brian Quaranto, Steven Schwaitzberg, Peter CW Kim, and Jinjun Xiong. Llava-surg: towards multimodal surgical assistant via structured surgical video learning. arXiv preprint arXiv:2408.07981, 2024. 1 [18] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2 [19] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2024. [20] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. The lazy neuron phenomenon: On emergence of activation sparsity in transformers. arXiv preprint arXiv:2210.06313, 2022. 6 [21] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86408650, 2024. 2, 3, 4 [22] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 3 [23] Haogeng Liu, Qihang Fan, Tingkai Liu, Linjie Yang, Yunzhe Tao, Huaibo Huang, Ran He, and Hongxia Yang. Videoteller: Enhancing cross-modal generation with fusion and decoupling. arXiv preprint arXiv:2310.04991, 2023. 3 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 2 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [26] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, 2024. 2 [27] Qian Liu, Yihong Chen, Bei Chen, Jian-Guang Lou, Zixuan Chen, Bin Zhou, and Dongmei Zhang. You impress me: Dialogue generation via mutual persona perception. arXiv preprint arXiv:2004.05388, 2020. 3 [28] Yuan Liu, Zhongyin Zhao, Ziyuan Zhuang, Le Tian, Xiao Zhou, and Jie Zhou. Improving your visionPoints: language model with affordable strategies. arXiv preprint arXiv:2409.04828, 2024. 2 [29] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023. 3 [30] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 3 [31] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [32] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et al. Mm1: methods, analysis and insights from multimodal llm pre-training. In European Conference on Computer Vision, pages 304323. Springer, 2024. 3 [33] Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh Ojha, and Yong Jae Lee. Yollava: Your personalized language and vision assistant. Advances in Neural Information Processing Systems, 37:4091340951, 2025. 3 [34] Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh Ojha, and Yong Jae Lee. Yollava: Your personalized language and vision assistant. Advances in Neural Information Processing Systems, 37:4091340951, 2025. 2, 4, 6 [35] Baolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Liden, Elnaz Nouri, Zhou Yu, Bill Dolan, and Jianfeng Gao. Godel: Large-scale pre-training for goal-directed dialog. arXiv preprint arXiv:2206.11309, 2022. 3 [36] Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Ume, Carl Shift Facenheim, Luis RP, Jian Jiang, Sheng Zhang, et al. Deepfacelab: Integrated, flexible and extensible face-swapping framework. arXiv preprint arXiv:2005.05535, 2020. 3, 4 [37] Chau Pham, Hoang Phan, David Doermann, and Yunjie Tian. Personalized large vision-language models. arXiv preprint arXiv:2412.17610, 2024. [38] Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, and Tong Zhang. Personalized visual instruction tuning. arXiv preprint arXiv:2410.07113, 2024. 2 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 87488763. PMLR, 2021. 4 [40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. 3 [41] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815823, 2015. 3 [42] Dai Shi. Transnext: Robust foveal visual perception for vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1777317783, 2024. 5 [43] Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, et al. Prosparse: Introducing and enhancing intrinsic activation sparsity within large language models, july 2024. arXiv preprint arXiv:2402.13516. 6 [44] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. [45] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2 [46] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2025. 2 [47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2 [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4 [49] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Chatvideo: trackletcentric multimodal and versatile video understanding system. arXiv preprint arXiv:2304.14407, 2023. 1 [50] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pages 396416. Springer, 2024. 1, 2, 3, 4, 5, 7 [51] Ziteng Wang, Jianfei Chen, and Jun Zhu. Remoe: Fully differentiable mixture-of-experts with relu routing. arXiv preprint arXiv:2412.14711, 2024. 5 [52] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language model. In European Conference on Computer Vision, pages 408424. Springer, 2024. 2 [53] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2024. 3 [54] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 3 [55] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee Wong, Zhenguo Li, and Hengshuang Zhao. Drivegpt4: Interpretable end-to-end autonomous driving via large language model. IEEE Robotics and Automation Letters, 2024. [56] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. 2, 3, 4, 8 [57] Qiming Zhang, Jing Zhang, Yufei Xu, and Dacheng Tao. Vision transformer with quadrangle attention. arXiv preprint arXiv:2303.15105, 2023. 8 [58] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Personalizing dialogue agents: have dog, do you have pets too? arXiv preprint arXiv:1801.07243, 2018. 3 [59] Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, and Fang Wen. General facial representation learning in visual-linguistic manner. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1869718709, 2022. 2, 4 [60] Yaoyao Zhong, Mengshi Qi, Rui Wang, Yuhan Qiu, Yang Zhang, and Huadong Ma. Viotgpt: Learning to schedule vision tools towards intelligent video internet of things. arXiv preprint arXiv:2312.00401, 2023. 2 [61] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq: In European large-scale video facial attributes dataset. conference on computer vision, pages 650667. Springer, 2022. 2, 4 Supplementary Material: PVChat: Personalized Video Chat with One-Shot Learning In this supplemental material, the readers can find: Experimental details about the ablation study of token number; Details about multiple metrics utilized for personalized QA quality; More examples of recognition question answering; More experiment detail about Multi Character training Some examples of existential questions and answers for single and two entities; Presentation of templates for GPT prompt queries; Presentation of 25 characters; S1. Ablation Study About the Token Number We set each question prompt corresponding to = 16 tokens per character. If = 0, training is only with the <sks>token. As the token number increases, the metrics overall increase. However, when the number is larger than 16, the performance decreases. This is because too many tokens can make the model hard to capture the characteristics of the subject. The detailed experimental results are shown in Tab. 1. Number Acc BLEU BS ES DC 0 4 8 12 16 20 0.801 0.871 0.890 0.895 0.922 0.882 0.495 0.592 0.553 0.564 0.606 0. 0.939 5.00 4.00 0.951 5.00 4.50 0.949 5.00 4.58 0.949 5.00 4.41 0.952 4.74 4.38 0.949 4.96 4.25 Table 1. Ablations on the number of tokens in <Aa>. S2. Details of the Evaluation Metrics Here we add more details of the metrics which are utilized to measure the quality of personalized video chat: (1) Accuracy: Specifically designed to evaluate binary existence questions, measuring the models ability to correctly identify the presence or absence of objects. (2) BLEU and BERTScore: These metrics quantify the textual similarity between generated responses and ground truth answers, capturing linguistic precision and semantic alignment. (3) Entity Specificity (ES): Evaluated on 1-5 scale, this metric assesses whether responses contain personalized, contextually relevant details rather than generic statements. answers are thoroughly developed with appropriate supporting details and proper reasoning. S3. More Examples of Our PVChat Model From Fig. 1 to Fig. 5, more personalized video chat examples of various scenarios are displayed. For example, in Fig. 1, there are three characters named <Cl>, <Ja>and <Xo>in laboratory scene, which is quite challenging for video understanding. Our PVChat not only accurately identifies these three men by their names but also gives reasonable suggestions for some additional questions about giving gifts. Whats more, when the model is asked about their behavior, it successfully captures their locations and can do reasoning to guess this is professional setting. This demonstrates that our model has strong personalized reasoning capability, even in multiple subject setting. More details can be seen in the figures. S4. More experiment detail about Multi-"
        },
        {
            "title": "Character training",
            "content": "Epoch One person training needs 8 epochs, and two people training need 16 epochs, and three people training need 24 epochs. ReMoH Within the ReMoH architecture, we implement crossattention mechanisms at alternating layers to facilitate interaction between video embeddings (extracted by the visual encoder) and query embeddings. Our approach adaptively scales with the number of subjects: for single-person training, we incorporate an additional 16 token length 768 dimensional feature space; for two-person training, this expands to 32 token length 768 and for threeperson training, it further increases to 48 token length 768. Crucially, these query tokens are associated with corresponding masks that are only deactivated during the training of their respective subject videos. This selective masking strategy ensures that each query embedding specifically attends to its designated subject, effectively preventing cross-contamination between different identity representations. Special tokens For multi-person scenarios, we employ the special tokens [PERSON] and [PERSON] as delimiters to clearly delineate the boundary between one persons SKS tokens and anothers. This demarcation approach enables the model to accurately distinguish when the sequence of tokens associated with one individual ends and those belonging to another begin. S5. Example of 4 Different Questions (4) Descriptive Completeness (DC): Also rated on 1-5 scale, DC measures the logical coherence, factual correctness, and comprehensive nature of responses. This metric evaluates whether From Tab. 2 to Tab. 5, the detailed existence question for one entity and two entities are displayed. We show the other three questions about the appearance, action, and location in Tab. 6. Figure 1. Example of PVChat. S6. Example of queries prompt for GPT or Internvideo As shown in Fig. 6, this prompt will be used to get the age and gender of the character in the video by the video understanding model, and the age and gender information will be used in the following steps. And for the general answer from the video, understanding model will use the second one prompt to query ChatGPT to capture the general description like the human, he, she, and this will be replaced by the specific <sks>and then to construct our personalized QA pairs. As shown in Fig. 7, leveraging the characters age and gender attributes extracted in the previous stage, we enrich these core identity markers with detailed descriptors to create comprehensive prompts for (image+text)-to-image models. This approach significantly enhances identity consistency across generated images, maintaining coherent character representation while accommodating diverse poses and contexts. The resulting outputs demonstrate superior identity preservation compared to methods lacking such demographic anchoring. Figure 2. Example of PVChat. Figure 3. Example of PVChat. S7. Presentation of 25 characters acters. As shown in Fig. 8, our dataset is composed of Friends(6), Good Doctor(5), Ne Zha(2), doctor(3), patient(3), Big Bang(6)) 25 charFigure 4. Example of PVChat. Figure 5. Example of PVChat. Index Question Yes Answer 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 Is there any trace of <sks> in this footage? Yes, <sks> is in this video. Can you detect <sks> in this video clip? can confirm that <sks> appears. Does <sks> show up anywhere in this recording? <sks> is present in this recording. Is <sks> visible in this video? The video contains <sks>. Could you verify if <sks> is here? Ive identified <sks>. Does this footage include <sks>? <sks> is shown in this video. Can you spot <sks> in this clip? Yes, <sks> appears here. Is <sks> present in this video? can verify that <sks> is present. Does <sks> appear in this footage? The footage shows <sks>. Can you tell if <sks> is shown here? <sks> is in this video clip. Is <sks> in this video segment? Ive detected <sks>. Can you confirm <sks>s presence? Yes, <sks> is featured. Does this clip contain <sks>? The video includes <sks>. Is <sks> featured in this recording? can see <sks>. Can you find <sks> in this video? Is <sks> shown in any frame? Does this video show <sks>? Is <sks> visible anywhere? Can you see <sks>? Is <sks> in this video? Can you recognize <sks>? Does <sks> appear at all? Is <sks> recorded here? Can you identify <sks>? Is <sks> present? <sks> is definitely here. Yes, Ive found <sks>. This video shows <sks>. <sks> is visible. Yes, <sks> has been captured. The video clearly shows <sks>. Ive spotted <sks>. <sks> appears in this video. Yes, this footage contains <sks>. can recognize <sks>. <sks> is clearly visible. Table 2. Example of positive existence verification answering (Single entity). Index Question No Answer 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 Is there any trace of <sks> in this footage? No, <sks> is not in this video. Can you detect <sks> in this video clip? cannot detect <sks>. Does <sks> show up anywhere in this recording? This video does not contain <sks>. Is <sks> visible in this video? <sks> is not shown. Could you verify if <sks> is here? There is no sign of <sks>. Does this footage include <sks>? <sks> does not appear. Can you spot <sks> in this clip? can confirm <sks> is not here. Is <sks> present in this video? The footage does not include <sks>. Does <sks> appear in this footage? Theres no evidence of <sks>. Can you tell if <sks> is shown here? <sks> is not in this video. Is <sks> in this video segment? Ive checked, <sks> is not present. Can you confirm <sks>s presence? This video does not show <sks>. Does this clip contain <sks>? see no sign of <sks>. Is <sks> featured in this recording? <sks> is absent. Can you find <sks> in this video? The video does not show <sks>. Is <sks> shown in any frame? Does this video show <sks>? Is <sks> visible anywhere? Can you see <sks>? Is <sks> in this video? Can you recognize <sks>? Does <sks> appear at all? Is <sks> recorded here? Can you identify <sks>? Is <sks> present? cannot find <sks>. <sks> is not visible. can verify <sks> is not here. The video has no <sks>. <sks> does not exist in this video. find no trace of <sks>. This clip does not contain <sks>. <sks> is not present. cannot identify <sks>. There is no <sks> here. Table 3. Example of negative existence verification answering (Single entity). Index Question Both Yes Answer 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 Is there any trace of <sks1> or <sks2> in this footage? Both <sks1> and <sks2> are present in this video. Can you detect <sks1> or <sks2> in this video clip? can detect both <sks1> and <sks2> in the footage. Do <sks1> or <sks2> show up anywhere in this recording? Are <sks1> or <sks2> visible in this video? The video shows both <sks1> and <sks2> clearly. <sks1> and <sks2> are both visible in this recording. Could you verify if <sks1> or <sks2> are here? Ive identified both <sks1> and <sks2> in the clip. Does this footage include <sks1> or <sks2>? Both <sks1> and <sks2> appear in this video. Can you spot <sks1> or <sks2> in this clip? The footage contains both <sks1> and <sks2>. Are <sks1> or <sks2> present in this video? can see both <sks1> and <sks2> in frame. Do <sks1> or <sks2> appear in this footage? <sks1> and <sks2> are both featured in this video. Can you tell if <sks1> or <sks2> are shown here? Are <sks1> or <sks2> in this video segment? The recording shows both <sks1> and <sks2> present. Ive spotted both <sks1> and <sks2> in the footage. Can you confirm <sks1> or <sks2>s presence? Both <sks1> and <sks2> are captured in this clip. Does this clip contain <sks1> or <sks2>? The video includes both <sks1> and <sks2>. Are <sks1> or <sks2> featured in this recording? Can you find <sks1> or <sks2> in this video? can confirm the presence of both <sks1> and <sks2>. <sks1> and <sks2> are both shown in the recording. Are <sks1> or <sks2> shown in any frame? Both figures, <sks1> and <sks2>, are visible. Does this video show <sks1> or <sks2>? Ive found both <sks1> and <sks2> in the video. Are <sks1> or <sks2> visible anywhere? The footage displays both <sks1> and <sks2>. Can you see <sks1> or <sks2>? Both <sks1> and <sks2> are identifiable here. Are <sks1> or <sks2> in this video? can recognize both <sks1> and <sks2>. Can you recognize <sks1> or <sks2>? <sks1> and <sks2> both appear in this recording. Do <sks1> or <sks2> appear at all? The video features both <sks1> and <sks2>. Are <sks1> or <sks2> recorded here? Both <sks1> and <sks2> are clearly visible. Can you identify <sks1> or <sks2>? Ive detected the presence of both <sks1> and <sks2>. Are <sks1> or <sks2> present? The clip shows both <sks1> and <sks2>. Table 4. Example of positive existence verification question answering (Two entities). Index sks1 Yes, sks2 No Both No 2 3 4 5 6 8 9 10 11 12 14 15 Is there any trace of <sks1> or <sks2> in this footage? Can you detect <sks1> or <sks2> in this video clip? can confirm that <sks1> appears, but <sks2> is not present. Neither <sks1> nor <sks2> appear in this video. The video shows <sks1>, though theres no sign of <sks2>. cannot detect either <sks1> or <sks2>. Do <sks1> or <sks2> show up anywhere in this recording? <sks1> is visible, but <sks2> is absent. The video contains neither <sks1> nor <sks2>. Are <sks1> or <sks2> visible in this video? Could you verify if <sks1> or <sks2> are here? Does this footage include <sks1> or <sks2>? Can you spot <sks1> or <sks2> in this clip? Are <sks1> or <sks2> present in this video? Do <sks1> or <sks2> appear in this footage? Can you tell if <sks1> or <sks2> are shown here? Are <sks1> or <sks2> in this video segment? Can you confirm <sks1> or <sks2>s presence? Does this clip contain <sks1> or <sks2>? Ive detected <sks1>, while <sks2> does not appear. Both <sks1> and <sks2> are absent. The video contains <sks1>, but <sks2> is not shown. There is no sign of either <sks1> or <sks2>. <sks1> is present, however <sks2> is not in this clip. Neither <sks1> nor <sks2> are shown. can see <sks1>, but theres no trace of <sks2>. confirm both <sks1> and <sks2> are not present. The footage includes <sks1>, though <sks2> is not visible. The footage does not include <sks1> or <sks2>. <sks1> appears, but <sks2> is not featured. Theres no evidence of either <sks1> or <sks2>. Ive spotted <sks1>, while <sks2> is nowhere to be seen. Neither <sks1> nor <sks2> are visible. <sks1> is clearly visible, but <sks2> is not. Ive checked, both <sks1> and <sks2> are absent. The recording shows <sks1>, though <sks2> is absent. This video shows neither <sks1> nor <sks2>. can identify <sks1>, but <sks2> doesnt appear. see no sign of <sks1> or <sks2>. Are <sks1> or <sks2> featured in this recording? <sks1> is present, while <sks2> is not. Can you find <sks1> or <sks2> in this video? Both <sks1> and <sks2> are not in the recording. The clip features <sks1>, but theres no sign of <sks2>. The video does not contain <sks1> or <sks2>. Table 5. Example of mixed existence verification question answering (Two entities). Index Question 2 3 4 5 6 8 9 10 11 12 14 15 16 What activity is <sks> engaged in during this video? Could you describe what <sks> is doing in this footage? What specific actions can you observe <sks> performing in this recording? What movements or actions does <sks> perform here? Can you describe <sks>s behavior in this sequence? What is <sks> wearing in this video? Could you describe <sks>s outfit in this footage? What color and style of clothing is <sks> dressed in? How would you describe <sks>s appearance and attire? What notable features can you see in <sks>s clothing? Where is <sks> positioned in this video? What color and style of clothing is <sks> dressed in? Can you describe <sks>s location relative to others? Which part of the scene does <sks> appear in? How does <sks>s position change throughout the video? Where can <sks> be found in this footage? Table 6. Example of negative existence verification question answering (Single entity). Figure 6. Prompt for Internvideo and GPT query Figure 7. Prompt for Photomaker synthetic the ID consist photo Figure 8. Display of all dataset."
        }
    ],
    "affiliations": [
        "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
        "Nankai University",
        "Nanyang Technological University",
        "National University of Singapore"
    ]
}