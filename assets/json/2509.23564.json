{
    "paper_title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment",
    "authors": [
        "Min-Hsuan Yeh",
        "Yixuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human feedback plays a pivotal role in aligning large language models (LLMs) with human preferences. However, such feedback is often noisy or inconsistent, which can degrade the quality of reward models and hinder alignment. While various automated data cleaning methods have been proposed to mitigate this issue, a systematic evaluation of their effectiveness and generalizability remains lacking. To bridge this gap, we introduce the first comprehensive benchmark for evaluating 13 preference data cleaning methods in the context of LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning strategies in terms of alignment performance and generalizability across diverse datasets, model architectures, and optimization algorithms. By unifying disparate methods and rigorously comparing them, we uncover key factors that determine the success of data cleaning in alignment tasks. This benchmark lays the groundwork for principled and reproducible approaches to improving LLM alignment through better data quality-highlighting the crucial but underexplored role of data preprocessing in responsible AI development. We release modular implementations of all methods to catalyze further research: https://github.com/deeplearning-wisc/PrefCleanBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 6 5 3 2 . 9 0 5 2 : r Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment Min-Hsuan Yeh Yixuan Li Department of Computer Science University of Wisconsin-Madison {samuelyeh, sharonli}@cs.wisc.edu"
        },
        {
            "title": "Abstract",
            "content": "Human feedback plays pivotal role in aligning large language models (LLMs) with human preferences. However, such feedback is often noisy or inconsistent, which can degrade the quality of reward models and hinder alignment. While various automated data cleaning methods have been proposed to mitigate this issue, systematic evaluation of their effectiveness and generalizability remains lacking. To bridge this gap, we introduce the first comprehensive benchmark for evaluating 13 preference data cleaning methods in the context of LLM alignment. PrefCleanBench offers standardized protocol to assess cleaning strategies in terms of alignment performance and generalizability across diverse datasets, model architectures, and optimization algorithms. By unifying disparate methods and rigorously comparing them, we uncover key factors that determine the success of data cleaning in alignment tasks. This benchmark lays the groundwork for principled and reproducible approaches to improving LLM alignment through better data qualityhighlighting the crucial but underexplored role of data preprocessing in responsible AI development. We release modular implementations of all methods to catalyze further research: https://github.com/deeplearning-wisc/Pr efCleanBench."
        },
        {
            "title": "Introduction",
            "content": "As AI systems grow increasingly capable and influential, their potential impact on individuals and society amplifies the necessity of aligning their actions with desirable outcomes [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. AI alignment, the process of ensuring AI systems act in accordance with human preferences, as result, has gained significant research attention in recent years [12, 13]. key recipe to achieve alignment is through the collection of binary preferences in terms of certain objectives, such as helpfulness and harmlessness [14]. In practice, human annotators are presented with pairwise responses to the same prompt, and provide comparative judgments (e.g., preferred, non-preferred) based on the quality of responses. Such human feedback has become cornerstone in the development of many real-world LLM systems [15, 16, 17, 18]. Despite its widespread use, recent research has raised concerns about the reliability of human feedback [19]. In particular, human annotators can introduce biases, inconsistencies, and noise into the feedback process, which can compromise the effectiveness of alignment. For example, studies have shown that annotators may diverge in their assessments based on individual preferences [20], potentially leading to suboptimal or even harmful outcomes if not properly accounted for. Although recent research has proposed automated methods for cleaning noisy preference datasuch as utilizing large language models as judges, employing trained reward models, or applying heuristic criteriathere remains notable gap in systematically understanding and benchmarking the effectiveness of these methods. To our knowledge, there is currently no standardized evaluation protocol or comprehensive 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: The overview of the protocol for benchmarking data cleaning approaches. We propose protocol that covers the selection of datasets, evaluation pipelines, as well as the evaluation criteria and their corresponding metrics. comparative analysis to inform practitioners which cleaning methods best enhance LLM alignment, or how generalizable these methods are across different datasets and training regimes. Motivated by this critical gap, we present rigorous benchmark PrefCleanBench that systematically evaluates and compares preference data cleaning methods across multiple dimensions. Our goal is to provide framework that goes beyond anecdotal or dataset-specific evaluations, enabling fair and comprehensive comparison of cleaning strategies. We assess not only the improvements each method yields on standard alignment metrics but also their performance across variety of settingsincluding different datasets, LLM backbones, and diverse alignment algorithms. In doing so, we aim to uncover which cleaning methods consistently lead to better-aligned models, and under what conditions these benefits hold. We summarize our core contributions below: Contribution 1: Comprehensive coverage and open-source implementation of 13 data cleaning approaches for LLM alignment (Sec. 3). Our benchmark extensively covers 13 approaches to preference data cleaning, spanning three major paradigms: (1) LLM-as-a-judge methods that prompt powerful language models to re-annotate or verify preferences, (2) reward model-based methods that score preference data, and (3) heuristic-driven methods that rely on data quality metrics. We systematize these strategies under unified taxonomy to help researchers understand the current landscape and facilitate principled comparison. To support reproducibility and accelerate further research, we will additionally open-source modular, well-documented implementations of all 13 methods, designed for easy integration into standard alignment pipelines. Contribution 2: Standardized benchmarking protocol for alignment-oriented data cleaning (Sec. 4). We propose systematic evaluation protocol that enables fair benchmarking across diverse cleaning methods. Our protocol defines consistent training and evaluation pipeline, encompassing four representative preference datasets, multiple alignment objectives, and range of model backbones. The protocol specifies key metrics for measuring both alignment quality as well as generalizability via cross-model and cross-algorithm evaluations. Our benchmark makes it possible to meaningfully compare cleaning strategies under controlled conditions. Contribution 3: Comprehensive experiments on different settings (Sec. 5). We conduct comprehensive set of experiments to evaluate the real-world impact of each data cleaning method, following our proposed benchmarking protocol. Our findings reveal valuable insights and guidance for practitioners. Specifically, the evaluation of alignment shows that both identification and treatment for unreliable data affect the alignment of models. Compared to using single judge and/or flipping the labels, identifying unreliable data via multiple judges and removing such data resulted in higher win-tie rate and average reward of models trained on them. In addition, our evaluations suggest that data quality should be prioritized for effective alignment. Overall, our experiments validate the practicality of our benchmarking protocol and underscore the importance of developing more versatile and data cleaning techniques in future research."
        },
        {
            "title": "2 Related Work",
            "content": "LLM alignment. key aspect of training and deploying large language models is ensuring the models behave in safe and helpful ways [12, 13]. This is an important problem due to the potential harms that can arise in large models [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. wide range of methods have been developed that utilize human feedback or human preference data to train models to avoid harmful responses and elicit safer or more helpful responses [21, 22, 23, 24, 25, 14, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]. Particularly, the Reinforcement Learning from Human Feedback framework has proven effective in aligning large pre-trained language models [14, 21, 22, 25]. However, given its computational inefficiency, recent shifts in focus favor closed-form losses that directly utilize offline preferences [37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53] and inference-time alignment [36]. Recently, some studies in LLM alignment shifted focus to the data for alignment, focusing on diverse and representative data [54, 55, 56, 57, 58] and utilizing LLM to automate and scale the feedback collection and annotation process [33, 59, 60, 61]. These works highlighted the importance of data in LLM alignment. Reliability of human feedback. Some studies have sought to assess the quality of human feedback datasets [19, 62, 63, 64]. Yeh et al. [19] argued the importance of data quality in the data-centric alignment framework to increase the reliability of AI alignment. Gao et al. [65] studied the impact of noise on alignment by injecting additional noise into the dataset. Wang et al. [62] proposed measuring the reward gap for each datum in human feedback dataset and found significant proportion of data with negative reward gap, which indicates possible mis-label produced by human annotators. In addition, when curating benchmark for reward modeling, Lambert et al. [66] noticed the unreliability issue in the preference dataset. Therefore, after sampling data from multiple preference datasets, the authors manually filtered out data with incorrect labels. Furthermore, many preference optimization or reward modeling algorithms acknowledged the noises in human feedback labels, hence design algorithms that are robust against noises [67, 68, 69, 63]. All these studies highlighted the importance of carefully understanding the quality of preference datasets when utilizing them to align LLMs, and the need for data cleaning approaches to obtain high-quality preference data."
        },
        {
            "title": "3 Preference Data Cleaning Approaches",
            "content": "Although there are several existing data cleaning approaches for LLM alignment, there is no systematic review or fair comparison of these approaches to show how these approaches effectively improve LLM alignment during training. To bridge this gap, we introduce unified benchmarking framework to systematically compare data cleaning strategies. In this section, we begin by reviewing 13 existing data cleaning approaches for LLM alignment. In general, data cleaning approaches involve two core steps: identifying unreliable data (e.g., via LLM-as-a-judge) and applying corrective treatments (e.g., filtering or flipping the label). We name each approach according to its identification strategy, while the applied treatment creates variants within each strategy. As shown in Figure 2, we further categorize these approaches into three groups based on their underlying criteria for identifying unreliability, including the usage of LLM-as-a-Judge (Sec. 3.2), reward models (Sec. 3.3), and heuristic criteria (Sec. 3.4)."
        },
        {
            "title": "3.1 Notations and Definitions",
            "content": "Definition 3.1 (Human preference data.) Consider two responses yc, yr for an input prompt x, we denote yc yr if yc is preferred over yr. We call yc the chosen or preferred response and yr the rejected response. Each triplet (x, yc, yr) is referred to as preference. Furthermore, the empirical dataset = {(x(i), y(i) i=1 consists of such triplets sampled from preference distribution. , y(i) )}n , y(j) ) may mistakenly indicate y(j) In practice, human preference data often contains noise and inconsistencies. Specifically, portion of triplets (x(j), y(j) being genuinely preferable. Training LLMs with such unreliable preference data can undermine alignment quality and potentially yield harmful outcomes. Therefore, the task of preference data cleaning involves identifying these incorrectly annotated triplets and either removing them from the dataset or correcting their labels. Formally, preference data cleaning can be defined as below: despite y(j) y(j) 3 Figure 2: The summarization of data cleaning approaches for LLM alignment. We categorize data cleaning approaches into three groups based on the definition of unreliability they considered. The three groups include LLM-as-a-judge, score of reward model, and heuristic criteria. indicates unreliable data identified by each approach. Definition 3.2 (Preference data cleaning.) Denote P+ be distribution of high-quality preference data, in which each data point := (x, y1, y2, l) consists of high-quality prompt x, two response candidates y1, y2, and reliable label {, }, where means y1 is better than y2 and means y2 is better than y1. Also denote be distribution of low-quality preference data, in which each data point has the same structure (x, y1, y2, l), while the prompt (and/or both response candidates y1, y2) are low quality and/or the label is unreliable (e.g., := when y2 is better than y1). We assume noised preference dataset consists of data sampled from mixture distribution = (1 α)P+ + αP. The task of preference data cleaning is to remove or correct data points in that are sampled from such that the cleaned dataset contains data purely sampled from P+."
        },
        {
            "title": "3.2 Data Cleaning with LLM-as-a-Judge",
            "content": "Many studies have used LLMs as proxy for human feedback [32, 59] or as data quality assessor [70]. This approach identifies incorrect preference labels by prompting LLMs to score two response candidates given the input prompt. label is considered incorrect if the rejected response has higher score predicted by the LLM judge. We create two versions of this approach: LLMJudge-R and LLM-Judge-F, which remove data or flip labels based on the predictions of an LLM (in this case, GPT-4o-2024-05-13 [71]). The prompt used for scoring responses is detailed in Appendix B. Note that to mitigate the impact of positional bias [72], we input the two responses in the prompt with random order."
        },
        {
            "title": "3.3 Data Cleaning with Reward Models",
            "content": "Reward gap. Wang et al. [62] proposed to train reward models on the target dataset and measured the gap between the reward of chosen and rejected responses. Formally, given pairwise preference data = (x, yc, yr), the reward gap w.r.t. reward model is defined as RwGapr(d) := r(x, yc) r(x, yr). p% of the data with the smallest reward gap are considered to have incorrect labels. In experiments, we report the optimal performance by choosing from = {10, 20, 30, 40} and additionally ablate different percentages of data cleaned in Section 5.1. We create two variants: RwGap-R and RwGapF, which either remove or flip labels for these incorrect data. Following the original configuration in Wang et al. [62], we train eight models with different random seeds on the target dataset as reward models and average their reward gaps. Hyperparameters for training the models are listed in Appendix E. RM voting. Instead of training reward models on the target dataset, Yeh et al. [19] form committee of publicly available reward models and use voting to decide incorrect labels. reward model votes for incorrect if it assigns higher reward to the rejected response than the chosen one. Two decision 4 strategies can be considered: (1) when the whole committee votes for incorrect (VoteAll) and (2) when more than half of the models in the committee votes for incorrect (VoteMaj). We thus create four variants: VoteAll-R, VoteAll-F, VoteMaj-R, VoteMaj-F. We form the committee by selecting six reward models from RewardBench leader board1 that are highest-performing, publicly available, non-generative, and non-contaminated. Details of these models can be found in Appendix C."
        },
        {
            "title": "3.4 Data Cleaning with Heuristic Criteria",
            "content": "Apart from identifying incorrect preference labels, some approaches attempted to filter out data using some heuristic criteria in terms of data quality. Prompt quality. Lu et al. [73] introduced InsTag, tagging method that utilized ChatGPT to assign semantic tags for each prompt. They also proposed two data selection strategies: Complexity (Tag-Cmp) and diversity (Tag-Div). The former one filters out prompts with fewer tags, while the latter one filters out prompts whose associated tags are already present in the selected dataset. We apply InsTagger2 to assign tags for each prompt and keep the top 6K prompts in terms of higher complexity and diversity, following exactly Lu et al. [73]. Difficulty of instruction following. Li et al. [74] introduced the Instruction Following Difficulty (IFD) score of prompt-response pair, where IFD(x, y) = ppl(yx)/ppl(y). prompt-response pair with IFD score > 1 means the given prompt provides no useful context for the prediction of the response, while low IFD score means the instruction is too easy for LLM to follow without further training. We thus create IFD-R to measure IFD scores given prompts and the chosen responses. By default, after removing data with IFD score > 1, p% of data with the smallest IFD score are removed from each dataset. We also create two variants, IFD-Gap-R and IFD-Gap-F, where we measure the difference between IFD(x, yc) and IFD(x, yr) and remove/flip p% of data with the smallest difference, respectively. We use Llama3-8B to compute perplexity. Note that similar to RwGap, in the experiment we choose the removing/flipping ratio among 10, 20, 30, and 40 that gives the optimal performance. We also ablate different percentages of data cleaned in Section 5.1."
        },
        {
            "title": "4 Evaluation Protocol",
            "content": "In this section, we introduce the evaluation protocol to systemically evaluate different data cleaning approaches for LLM alignment. Our protocol include three core components: the selection of datasets (Sec. 4.1), evaluation pipeline (Sec. 4.2), and evaluation criteria (Sec. 4.3). Figure 1 summarizes the overview of the evaluation framework."
        },
        {
            "title": "4.1 Target Datasets",
            "content": "We benchmark data cleaning methods using four widely adopted preference datasets, including Anthropic-HH [14], UltraFeedback [60], PKU-SafeRLHF [75], and HelpSteer2 [76]. These datasets encompass both human-annotated and LLM-generated labels and represent diverse perspectives of preferences. The detailed statistics and descriptions of these datasets are provided in Appendix D."
        },
        {
            "title": "4.2 Evaluation Pipeline",
            "content": "We benchmark data cleaning approaches by applying these approaches on each dataset, and evaluate how the performance changes between models trained on the cleaned version and on the original version of the dataset. Specifically, we follow the standard preference optimization pipeline. For both cleaned and original data, we first train base LLMs with SFT by inputting prompts and the chosen responses. We then apply preference optimization algorithms to further tune the SFTed model. We defer the discussion on the selection of base models and preference optimization algorithms to Sec. 5. At the end, we evaluate the performance of preference-optimized models by criteria introduced in the next subsection. 1RewardBench: https://huggingface.co/spaces/allenai/reward-bench 2InsTagger: https://huggingface.co/OFA-Sys/InsTagger 5 Table 1: Alignment performance of Llama3-8B tuned on data cleaned with different approaches using DPO. Results are reported across four preference datasets (Anthropic-HH, UltraFeedback, PKU-SafeRLHF, and HelpSteer2), using evaluation metrics: win-tie rate (WinTie) and average reward (Avg. Rwd). Methods are grouped into three categories: LLM-as-a-Judge, reward model-based, and heuristic-based. The best score in each column is shown in bold, and the second-best is underlined. Anthropic-HH"
        },
        {
            "title": "UltraFeedback",
            "content": "PKU-SalfRLHF HelpSteer2 Approach WinTie Avg. Rwd WinTie Avg. Rwd WinTie Avg. Rwd WinTie Avg. Rwd Vanilla (no clean) - 6.001 - 4.109 - 6.318 - 6.509 LLM-Judge-R LLM-Judge-F 0.490 0.570 RwGap-R RwGap-F VoteAll-R VoteAll-F VoteMaj-R VoteMaj-F Tag-Cmp Tag-Div IFD-R IFD-Gap-R IFD-Gap-F 0.680 0.520 0.615 0.625 0.705 0. 0.760 0.695 0.385 0.730 0.565 6.113 5.766 6.333 5.248 6.278 6.842 7.287 7.010 6.720 6.770 3.972 5.817 5.327 LLM-as-a-Judge 0.675 0. 4.189 3.991 0.730 0.625 Reward Model-based 0.680 0.690 0.630 0.630 0.650 0.635 3.889 4.114 4.050 4.020 4.253 4.028 0.665 0.645 0.525 0.555 0.770 0.550 Heuristic-based 0.635 0.635 0.580 0.690 0.650 4.001 3.905 3.843 3.992 4.065 0.780 0.710 0.675 0.770 0.660 6.928 5.745 6.482 6.125 3.273 3.201 8.478 3.179 7.034 7.174 6.244 7.105 5. 0.470 0.515 0.635 0.465 0.520 0.420 0.750 0.495 0.550 0.625 0.530 0.650 0.475 5.657 5.300 6.534 5.207 5.211 5.371 6.834 5.458 6.518 6.682 5.826 6.750 5."
        },
        {
            "title": "4.3 Evaluation Criteria",
            "content": "We consider two main criteria, including (1) whether the data cleaning approach improves the alignment of preference-optimized models, and (2) whether the cleaned data generalizes well in different settings. In this subsection, we focus on discussing high-level ideas about how these criteria should be defined, and we defer the detailed implementations and settings to Sec. 5. Criteria 1: Alignment. We utilize the following two commonly used metrics to measure the alignment of preference-tuned models. Win-tie rate (WinTie): The win-tie rate of the responses generated by models tuned on the clean data against those generated by models tuned on the original data. The preferences can be judged via human annotators, LLMs, or reward models. Models trained on clean data should have high win-tie rate against models trained on the original data. Average gold reward (Avg. Rwd): The average score of responses generated by model, evaluated by gold reward model. Models trained on clean data should have higher average gold reward than models trained on the original data. Criteria 2: Generalizability. We evaluate generalizability by measuring alignment metrics with different settings. In particular, we consider the following aspects: Different base models: Data cleaning approach should improve alignment of models with different sizes and from different model families. Diverse optimization algorithms: Data cleaning approach should improve alignment of models trained using different preference optimization algorithms."
        },
        {
            "title": "5 Experimental Results",
            "content": "Following the pipeline introduced in Sec. 4.2, we train models on both cleaned and original datasets to evaluate the data-cleaning approaches. We consider Llama3-8B [77] as the base model and DPO [37] 6 as the preferenceoptimization algorithm in our main experimental setting, and perform extensive ablations using various LLMs and preference optimization methods in Section 5.2. We include the training configurations and details in Appendix E."
        },
        {
            "title": "5.1 Benchmarking Alignment",
            "content": "Implementation. We implement the three metrics for benchmarking alignment as follows. For WinTie, we utilize GPT-4o-2024-05-13 as the LLM judge and use the same prompt as shown in Sec. 3.2. Note that different from the usage of data cleaning, to mitigate the positional bias, here we input yclean and yorigin to the prompt two times, with different orders respectively. We then average the scores generated by the two prompts as the final score. Also note that due to the cost of running LLMas-a-judge, we randomly select 200 samples from the test set to calculate WinTie. For Avg. Rwd, we measure rewards by LxzGordon/URM-LLaMa-3.1-8B [78], which is held-out reward model apart from the models used for data cleaning in Sec. 3.3. To ensure robustness of our evaluation, we additionally report performance under alternative gold reward models in Appendix F. We also conduct human evaluation to ensure the WinTie rate measured by the LLM-as-a-judge is reliable. Specifically, we sample 50 data points from the Anthropic-HH dataset, and compare the responses generated by Llama3-8B trained with DPO on the original dataset and on the dataset cleaned by VoteMaj-R. We conduct both human annotation and LLM-judge with GPT-4o, and compute the Cohens kappa inter-annotator agreement score. The result shows high Cohens kappa value, suggesting significant agreement between human judgments and GPT-4o assessments. Note that WinTie and Avg. Rwd require generating yclean and yorigin using πclean and πorigin respectively, where the generation configurations are detailed in Appendix E. Should we remove the data or flip the label? In Sec. 3, we consider two corrective treatments: either removing the preference data, or flipping the preference label. In Table 1, we find that the choice of corrective treatment largely affects the performance of alignment. In particular, removing unreliable data generally performs better alignment than flipping labels, as evidenced by higher win-tie rates and average reward model scores. This suggests that mitigating unreliability of feedback is more complicated than simply flipping labels. As shown by Yeh et al. [19], there are at least six sources of unreliability in preference data, while flipping labels only addresses cases where annotators mislabel responses. For other cases, such as having harmful suggestions in both responses, even though reward model or LLM thinks rejected response is better than the chosen one, label flipping fails to mitigate unreliability. In contrast, removing such data enhances dataset quality, thereby enhancing the alignment of trained models. To better illustrate this idea, we examined 50 data points on HelpSteer2, which are marked as unreliable by VoteMaj, as well as another 50 data points that were retained. We observed significant gap in the quality of the input prompt between the unreliable and retrained data. The retrained data tends to have prompt with clear instruction or specific question, leading to high-quality response candidates and reliable preference annotations. In contrast, large amount of unreliable data marked by VoteMaj has low-quality prompts. For example, simply greeting LLMs, posting vague question, or asking LLMs to generate list of product descriptions without providing any data. LLMs prompted on them usually generate responses that are generic or hallucinated. In this case, VoteMaj-F, i.e., flipping the labels of unreliable data, can not mitigate the unreliability because it is due to the prompt. In fact, flipping the labels even degrades the performance because some marked data have correct label. On the other hand, VoteMaj-R removes all the unreliable data, cleaning up data with low-quality prompt and preventing the risk of wrongly correcting labels. Multiple judges resulted in better alignment than single judge. As shown in Table 1, models trained with VoteMaj-R consistently performs well across all datasets, achieving top scores in avg. reward. Unlike LLM-Judge and RwGap, VoteMaj identifies unreliable data based on agreement across multiple judges, underscoring the value of judge diversity. By incorporating diverse evaluators, the identification of unreliable data becomes less susceptible to the biases of any single model or dataset [79]. To further investigate why LLM-as-a-Judge methods underperform, we analyze 50 data points sampled from the Anthropic-HH dataset that are marked as unreliable by LLM-Judge but reliable by VoteMaj, and another 50 data points that are marked as unreliable by VoteMaj but reliable by LLM-Judge. We found that the discrepancy between LLM-Judge and VoteMaj usually happens when the two response candidates have similar quality. Specifically, when both responses 7 Table 2: Alignment performance of Llama3-8B tuned on data cleaned with different data filtering proportion using DPO. We vary the filtering threshold from 10% to 40% for RwGap-R and IFD-Gap-R. 10% Filtering 20% Filtering 30% Filtering 40% Filtering Approach WinTie Avg. Rwd WinTie Avg. Rwd WinTie Avg. Rwd WinTie Avg. Rwd Vanilla (no clean) RwGap-R IFD-Gap-R - 0.570 0.620 Vanilla (no clean) RwGap-R IFD-Gap-R - 0.580 0.625 Vanilla (no clean) RwGap-R IFD-Gap-R - 0.650 0. Vanilla (no clean) RwGap-R IFD-Gap-R - 0.460 0.495 6.001 6.143 6.060 4.109 3.992 4.165 6.318 5.998 7.105 6.509 5.657 5. Anthropic-HH - 0.665 0.660 6.001 5.931 5.784 UltraFeedback - 0.680 0.625 4.109 3.889 3.719 PKU-SafeRLHF - 0.665 0.685 6.318 6.482 6. HelpSteer2 6.509 6.534 6.561 - 0.635 0.645 - 0.680 0.730 - 0.615 0.650 - 0.670 0.680 - 0.600 0. 6.001 6.333 5.817 4.109 3.842 3.708 6.318 5.884 6.929 6.509 6.401 6.750 - 0.660 0.660 - 0.620 0. - 0.705 0.750 - 0.615 0.620 6.001 6.057 5.798 4.109 3.731 3.992 6.318 5.870 6.719 6.509 6.544 6. were suggesting harmful behaviors, since LLM-Judge is forced to decide which response is better, it has around 1/2 probability of choosing the chosen one and keeping the data point in the dataset. In contrast, the decision of VoteMaj is made by multiple models, so these data tend to get mixed votes and are more likely to be removed. Since these low-quality data are harmful for aligning LLMs, training on them will degrade the performance. Impact of data quantity. We further investigate how the proportion of data removed during the cleaning process affects alignment performance. Specifically, we vary the filtering threshold from 10% to 40% for two representative methods: RwGap-R (reward gap-based filtering) and IFD-Gap-R (instruction following difficulty-based filtering)both of which require an explicit specification of the removal ratio. In contrast, other cleaning methods like LLM-Judge-R, VoteAll-R, and VoteMaj-R do not require fixed proportion of data to be filtered. Results in Table 2 reveal nuanced tradeoff. mild filtering rate improves alignment metrics such as win-tie rate and average rewardindicating that removing unreliable data can enhance model quality. The optimal filtering rate is achieved somewhere between 20% to 30%, which aligns with the amount of noise known in datasets such as Anthropic-HH [62]."
        },
        {
            "title": "5.2 Benchmarking Generalizability",
            "content": "Following the protocol we proposed in Sec. 4, we evaluate the generalizability of preference data cleaning in the aspects of (1) optimization algorithm and (2) base LLM model. For the aspects of base model and optimization algorithm, we show the generalizability of the top two data cleaning approaches that best perform in alignment evaluation, i.e., VoteMaj-R and Tag-Cmp. While for the aspect of dataset, we evaluate all the data cleaning approaches. Performance across preference optimization algorithms. Beyond using DPO, we extend our evaluation to other preference optimization algorithms, including CPO [80], SLiC [52], KTO [81], AOT [82], IPO [83], rDPO [67], and ORPO [84]. These algorithms represent different strategies for aligning model outputs with human preferences, allowing for broader assessment of our cleaning methods. We train the base modelLlama3-8Bwith these different algorithms on the four target datasets, respectively. Results in Table 3 show that both models trained with VoteMaj-R and Tag-Cmp maintain high win-tie rate and avg. reward across different preference optimization algorithms in most settings, 8 Table 3: Generalizability of data cleaning approaches across different preference optimization algorithms. We train Llama3-8B with cleaned data using different preference optimization algorithm. Anthropic-HH UltraFeedback PKU-SalfRLHF HelpSteer2 Approach WinTie Avg. Rwd WinTie Avg. Rwd WinTie Avg. Rwd WinTie Avg. Rwd Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.705 0.760 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.675 0.660 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.625 0.660 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.570 0.520 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.725 0.625 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.715 0.780 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.745 0.645 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.635 0.630 6.001 7.287 6.720 5.309 6.197 6.719 5.483 6.770 5. 4.688 5.047 4.045 4.883 6.191 5.107 5.570 6.495 6.828 4.240 5.390 4.951 4.841 5.154 5."
        },
        {
            "title": "DPO",
            "content": "4.109 4.253 4."
        },
        {
            "title": "CPO",
            "content": "3.480 3.821 3."
        },
        {
            "title": "SLiC",
            "content": "3.700 3.895 3."
        },
        {
            "title": "KTO",
            "content": "3.745 3.775 3."
        },
        {
            "title": "AOT",
            "content": "3.723 3.869 3."
        },
        {
            "title": "IPO",
            "content": "3.424 3.715 3.391 rDPO 3.656 3.789 3."
        },
        {
            "title": "ORPO",
            "content": "4.040 6.512 3.907 - 0.650 0.635 - 0.645 0.635 - 0.735 0.650 - 0.665 0.705 - 0.715 0. - 0.685 0.585 - 0.645 0.680 - 0.935 0.635 - 0.770 0.780 - 0.705 0.740 - 0.705 0. - 0.635 0.665 - 0.695 0.690 - 0.590 0.605 - 0.680 0.750 - 0.645 0.695 6.318 8.478 7. 3.568 4.449 5.137 5.697 6.882 6.561 3.826 3.369 4.264 6.086 7.602 6.237 4.805 7.209 6.845 4.900 6.036 5. 5.181 5.470 5.280 - 0.750 0.550 - 0.705 0.665 - 0.710 0.615 - 0.610 0.645 - 0.655 0. - 0.600 0.620 - 0.630 0.665 - 0.635 0.650 6.509 6.834 6.518 6.920 4.305 6.508 6.055 6.293 6. 6.188 6.258 6.389 5.851 6.236 6.258 6.581 6.760 6.775 5.811 6.155 6.298 6.864 7.086 6.833 suggesting that both data cleaning methods generalize well across algorithms. Notably, we found that some preference optimization algorithms work particularly well with specific data cleaning method. For AOT and ORPO, models trained with VoteMaj-R outperform models trained with Tag-Cmp in most cases; while for KTO and rDPO, models trained with Tag-Cmp generally perform better. These findings suggest that the interaction between data cleaning strategies and preference optimization algorithms is non-trivial and may depend on the algorithms inductive biases. Specifically, AOT and ORPO are designed to be more distribution-aware and sensitive to noise in preference signals, which may explain why they benefit more from VoteMaj-Ra method that explicitly filters out examples with high disagreement among reward models, thus reducing label noise. In contrast, KTO and rDPO are designed to be more robust against noise. Tag-Cmp selects data based on prompt complexity and diversity, which may provide KTO and rDPO with more informative training signals for modeling preferences. This suggests that aligning the strengths of data cleaning method with the learning dynamics of preference optimization algorithm can lead to better overall alignment outcomes. Performance across different base models. Apart from Llama3-8B, we consider 4 additional base models with different sizes and from different families, including Llama3.2-1B [85], Qwen2.57B [86], Mistral-7B [87], and phi-2 [88]. We fine-tune these models on all four datasets using DPO. Results in Table 4 show that models trained with VoteMaj-R maintain high win-tie rate and avg. 9 Table 4: Generalizability of data cleaning approaches across different base LLM models. Anthropic-HH UltraFeedback PKU-SalfRLHF HelpSteer2 Approach WinTie Avg. Rwd WinTie Avg. Rwd WinTie Avg. Rwd WinTie Avg. Rwd Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.705 0.760 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.605 0.570 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.740 0.690 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.590 0.395 Vanilla (no clean) VoteMaj-R Tag-Cmp - 0.655 0.580 6.001 7.287 6.720 5.460 6.551 6.000 4.218 5.640 5.264 5.626 6.287 4.382 4.441 5.857 4. Llama3-8B 4.109 4.253 4.001 - 0.650 0.635 Qwen2.5-7B 3.283 3.390 3.252 Mistral-7B 2.996 2.943 2.902 - 0.750 0.615 - 0.635 0.570 phi2.712 2.644 2.767 - 0.650 0.605 Llama3.2-1B 3.031 3.081 2.569 - 0.625 0.515 - 0.770 0.780 - 0.745 0. - 0.760 0.625 - 0.715 0.780 - 0.735 0.665 6.318 8.478 7.034 5.487 8.132 6.342 5.304 6.732 5. 7.570 9.204 5.511 4.720 7.431 6.043 - 0.750 0.550 - 0.695 0.720 - 0.600 0.585 - 0.585 0. - 0.590 0.600 6.509 6.834 6.518 6.176 6.015 6.187 4.722 4.726 4.436 4.492 4.187 4.338 4.012 3.891 3. reward across different base models in most settings. In contrast, models trained with Tag-Cmp fail to have win-tie rate > 0.5 in some settings and have an average. reward lower than models trained with uncleaned datasets. This suggests that VoteMaj-R has higher generalizability than Tag-Cmp."
        },
        {
            "title": "6 Conclusion and Limitations",
            "content": "Our work addresses fundamental yet usually overlooked component of LLM alignment pipeline: the quality of the preference data for alignment. Improved data cleaning methods can lead to more reliable alignment outcomes, reducing the risk of models exhibiting unsafe behaviors, or misaligning with user intent. By providing standardized benchmark for evaluating diverse set of data cleaning techniques, we aim to foster more rigorous and reproducible practices in alignment research. Our results underscore the importance of both accurately identifying unreliable feedback and applying effective treatment strategiessuch as removal over flipping labelsand show that cleaner, smaller datasets can outperform larger but noisier ones. Moreover, by highlighting the varying generalizability and effectiveness of different cleaning strategies across datasets, models, and optimizers, our benchmark encourages the development of more robust alignment pipelines that perform well in diverse settings. We hope our benchmark serves as foundation for future work in data-centric alignment and enables more principled development of reliable and aligned AI systems. One challenge of estimating the effectiveness of data cleaning approaches for preference data is that there is no ground truth to determine the quality or the correctness of preference data. Therefore, to quantify the performance of data cleaning, we evaluate the alignment of models trained with the cleansed data. Although such an evaluation can indicate whether models trained with cleansed data achieve better alignment, it can not quantify the recall and false positive rate of identifying unreliable data. Future work could explore cost-effective yet reliable ways of identifying noise in preference data with human oversight. curated benchmark with partially verified labels would enable direct evaluation of data cleaning accuracy. Such efforts could advance both the science of benchmarking and the broader goal of data-centric alignment."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank Shawn Im and Pengyue Jia for their valuable suggestions on the draft. The authors would also like to thank the NeurIPS anonymous reviewers for their helpful feedback. This work is supported in part by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation under awards IIS-2237037 and IIS-2331669, Office of Naval Research under grant number N00014-23-1-2643, Schmidt Sciences Foundation, Open Philanthropy, Alfred P. Sloan Fellowship, SFF, and gifts from Google and Amazon."
        },
        {
            "title": "Bibliography",
            "content": "[1] Peter Park, Simon Goldstein, Aidan OGara, Michael Chen, and Dan Hendrycks. Ai deception: survey of examples, risks, and potential solutions. arXiv preprint arXiv:2308.14752, 2023. [2] Micah Carroll, Alan Chan, Henry Ashton, and David Krueger. Characterizing manipulation from ai systems. In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO 23, 2023. ISBN 9798400703812. [3] Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. Discovering language model behaviors with model-written evaluations. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1338713434, 2023. [4] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Esin DURMUS, Zac Hatfield-Dodds, Scott Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. Towards understanding sycophancy in language models. In The Twelfth International Conference on Learning Representations, 2024. [5] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, pages 675718, 2023. [6] Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from learned optimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820, 2019. [7] Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, and Owain Evans. Taken out of context: On measuring situational awareness in llms. arXiv preprint arXiv:2309.00667, 2023. [8] Richard Ngo, Lawrence Chan, and Sören Mindermann. The alignment problem from deep learning perspective. In The Twelfth International Conference on Learning Representations, 2024. [9] Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al. Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324, 2023. [10] Rohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac Kenton. Goal misgeneralization: Why correct specifications arent enough for correct goals. arXiv preprint arXiv:2210.01790, 2022. 11 [11] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. In International Conference on Learning Representations, 2022. [12] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks, Charbel-Raphael Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. [13] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: research direction. arXiv preprint arXiv:1811.07871, 2018. [14] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [15] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [16] Anthropic. Introducing claude. https://www.anthropic.com/index/introducing-claude, 2023. [17] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [18] Team Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [19] Min-Hsuan Yeh, Jeffrey Wang, Xuefeng Du, Seongheon Park, Leitian Tao, Shawn Im, and Yixuan Li. Position: Challenges and future directions of data-centric AI alignment. In Fortysecond International Conference on Machine Learning Position Paper Track, 2025. [20] Pengyu Cheng, Jiawen Xie, Ke Bai, Yong Dai, and Nan Du. Everyone deserves reward: Learning customized human preferences. arXiv preprint arXiv:2309.03126, 2023. [21] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 2017. [22] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. [23] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 2020. [24] Kimin Lee, Laura Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. In International Conference on Machine Learning, 2021. [25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, pages 2773027744, 2022. [26] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2022. 12 [27] Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soˇna Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. [28] Charlie Victor Snell, Ilya Kostrikov, Yi Su, Sherry Yang, and Sergey Levine. Offline RL for natural language generation with implicit language learning. In The Eleventh International Conference on Learning Representations, 2023. [29] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: Rank responses to align language models with human feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [30] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1899018998, 2024. [31] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, KaShun SHUM, and Tong Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. [32] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [33] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Ren Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. RLAIF vs. RLHF: Scaling reinforcement learning from human feedback with AI feedback. In Forty-first International Conference on Machine Learning, 2024. [34] Remi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Côme Fiegel, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel Mankowitz, Doina Precup, and Bilal Piot. Nash learning from human feedback. In Forty-first International Conference on Machine Learning, 2024. [35] Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, and Dorsa Sadigh. Contrastive preference learning: Learning from human feedback without reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. [36] Maxim Khanov, Jirayu Burapacheep, and Yixuan Li. Args: Alignment as reward-guided search. In Proceedings of the International Conference on Learning Representations, 2024. [37] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, volume 36, pages 5372853741, 2023. [38] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024. [39] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. International Conference on Learning Representations, 2024. [40] Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs sampling from human feedback: provable klconstrained framework for rlhf. CoRR, 2023. [41] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Remi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Avila Pires, and Bilal Piot. Generalized preference optimization: unified approach to offline alignment. In Forty-first International Conference on Machine Learning, 2024. 13 [42] Yu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple preference optimization with In The Thirty-eighth Annual Conference on Neural Information reference-free reward. Processing Systems, 2024. [43] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Model alignment as prospect theoretic optimization. In Proceedings of the 41st International Conference on Machine Learning, pages 1263412651, 2024. [44] Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Tokenlevel direct preference optimization. In Proceedings of the 41st International Conference on Machine Learning, pages 5834858365, 2024. [45] Daniele Calandriello, Zhaohan Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, Rishabh Joshi, Zeyu Zheng, and Bilal Piot. Human alignment of large language models through online preference optimisation. In Proceedings of the 41st International Conference on Machine Learning, pages 54095435, 2024. [46] William Muldrew, Peter Hayes, Mingtian Zhang, and David Barber. Active preference learning for large language models. In Proceedings of the 41st International Conference on Machine Learning, pages 3657736590, 2024. [47] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust DPO: Aligning language models with noisy feedback. In Proceedings of the 41st International Conference on Machine Learning, pages 4225842274, 2024. [48] Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello, Quentin Berthet, Felipe Llinares-López, Jessica Hoffmann, Lucas Dixon, Michal Valko, and Mathieu Blondel. Decodingtime realignment of language models. In Proceedings of the 41st International Conference on Machine Learning, pages 3101531031, 2024. [49] Songyang Gao, Qiming Ge, Wei Shen, Shihan Dou, Junjie Ye, Xiao Wang, Rui Zheng, Yicheng Zou, Zhi Chen, Hang Yan, et al. Linear alignment: closed-form solution for aligning human preferences without tuning and feedback. In Forty-first International Conference on Machine Learning, 2024. [50] Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, and Jianshu Chen. Rewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment. In Forty-first International Conference on Machine Learning, 2024. [51] Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Dinesh Manocha, Furong Huang, Amrit Bedi, and Mengdi Wang. Maxmin-rlhf: Alignment with diverse human preferences. In Forty-first International Conference on Machine Learning, 2024. [52] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slichf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. [53] Shawn Im and Yixuan Li. Understanding the learning dynamics of alignment with human feedback. In International Conference on Machine Learning, 2024. [54] Michael Ryan, William Held, and Diyi Yang. Unintended impacts of LLM alignment on global representation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. [55] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori In Proceedings of the 40th InHashimoto. Whose opinions do language models reflect? ternational Conference on Machine Learning, 2023. [56] Maria Lerner, Florian Dorner, Elliott Ash, and Naman Goel. Whose preferences? differences in fairness preferences and their impact on the fairness of AI utilizing human feedback. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. [57] Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Michael Bean, Katerina Margatina, Rafael Mosquera, Juan Manuel Ciro, Max Bartolo, Adina Williams, He He, Bertie Vidgen, and Scott A. Hale. The PRISM alignment dataset: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [58] Shawn Im and Yixuan Li. Can dpo learn diverse human values? theoretical scaling law. In Advances in Neural Information Processing Systems, 2025. [59] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems, volume 36, pages 4659546623, 2023. [60] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: boosting language models with scaled ai feedback. In Proceedings of the 41st International Conference on Machine Learning, 2024. [61] Leitian Tao and Yixuan Li. Your weak llm is secretly strong teacher for alignment. In International Conference on Learning Representations, 2025. [62] Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024. [63] Joonho Lee, Jae Oh Woo, Juree Seok, Parisa Hassanzadeh, Wooseok Jang, Juyoun Son, Sima Didari, Baruch Gutow, Heng Hao, Hankyu Moon, Wenjun Hu, Yeong-Dae Kwon, Taehee Lee, and Seungjai Min. Improving instruction following in language models through proxybased uncertainty estimation. In Proceedings of the 41st International Conference on Machine Learning, pages 2700927036, 2024. [64] Keyi Kong, Xilie Xu, Di Wang, Jingfeng Zhang, and Mohan Kankanhalli. Perplexity-aware correction for robust alignment with noisy preferences. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [65] Yang Gao, Dana Alon, and Donald Metzler. Impact of preference noise on the alignment performance of generative language models. In First Conference on Language Modeling, 2024. [66] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. RewardBench: Evaluating reward models for language modeling. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 17551797, 2025. ISBN 979-8-89176-195-7. [67] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: aligning language models with noisy feedback. In Proceedings of the 41st International Conference on Machine Learning, 2024. [68] Eric Mitchell. note on dpo with noisy preferences & relationship to ipo, 2023. URL https://ericmitchell.ai/cdpo.pdf. [69] Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. β-dpo: direct preference optimization with dynamic β. In Proceedings of the 38th International Conference on Neural Information Processing Systems, 2025. ISBN 9798331314385. [70] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training better alpaca with fewer data. In The Twelfth International Conference on Learning Representations, 2024. [71] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [72] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. [73] Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. #instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In The Twelfth International Conference on Learning Representations, 2024. 15 [74] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2024. [75] Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Alex Qiu, Jiayi Zhou, Kaile Wang, Boxun Li, Sirui Han, Yike Guo, and Yaodong Yang. PKUSafeRLHF: Towards multi-level safety alignment for LLMs with human preference. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3198332016, 2025. ISBN 979-8-89176-251-0. [76] Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer 2: Open-source dataset for training top-performing reward models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [77] AI @ Meta Llama Team. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [78] Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie, and Junge Zhang. Uncertaintyaware reward model: Teaching reward models to know what is unknown. arXiv preprint arXiv:2410.00847, 2025. [79] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, and Huan Liu. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024. [80] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: pushing the boundaries of llm performance in machine translation. In Proceedings of the 41st International Conference on Machine Learning, 2024. [81] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Model alignment as prospect theoretic optimization. In Proceedings of the 41st International Conference on Machine Learning, 2024. [82] Igor Melnyk, Youssef Mroueh, Brian Belgodere, Mattia Rigotti, Apoorva Nitsure, Mikhail Yurochkin, Kristjan Greenewald, Jiri Navratil, and Jarret Ross. Distributional preference alignment of LLMs via optimal transport. In ICML 2024 Workshop on Models of Human Feedback for AI Alignment, 2024. [83] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, pages 44474455, 2024. [84] Jiwoo Hong, Noah Lee, and James Thorne. ORPO: Monolithic preference optimization without reference model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024. [85] Meta AI. Llama3.2-1B. https://ai.meta.com/blog/llama-3-2-connect-2024-vis ion-edge-mobile-devices/, 2024. [86] Qwen. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2025. [87] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [88] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. [89] Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, and Dacheng Tao. InfoRM: Mitigating reward hacking in RLHF via information-theoretic reward modeling. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. 16 [90] Nicolai Dorka. Quantile regression for distributional reward models in rlhf. arXiv preprint arXiv:2409.10164, 2024. [91] Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. Regularizing hidden states enables learning generalizable reward model for llms. In Advances in Neural Information Processing Systems, 2024. [92] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In Findings of the Association for Computational Linguistics: EMNLP 2024, 2024. [93] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022. [94] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional In Proceedings of the 2023 Conference on Empirical Methods in Natural conversations. Language Processing, 2023. [95] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. [96] Alvaro Bartolome, Gabriel Martin, and Daniel Vila. Notus. https://github.com/argilla -io/notus, 2023. [97] Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. Helpsteer2-preference: Complementing ratings with preferences. In The Thirteenth International Conference on Learning Representations, 2025. [98] Junsoo Park, Seungyeon Jwa, Ren Meiying, Daeyoung Kim, and Sanghyuk Choi. OffsetBias: Leveraging debiased data for tuning evaluators. In Findings of the Association for Computational Linguistics: EMNLP 2024, 2024."
        },
        {
            "title": "D Details of Datasets",
            "content": "E Hyperparameters, Configurations, and Computational Details"
        },
        {
            "title": "A Broader Impact",
            "content": "18 18 18 18 20 As large language models continue to be integrated into high-stakes applications, ensuring their alignment with human values and preferences becomes increasingly critical. Our work tackles key gap in the alignment literature by systematically benchmarking diverse set of data cleaning approaches for preference feedback datasets. By providing standardized benchmark for evaluating data cleaning techniques, we aim to foster more rigorous and reproducible practices in alignment research. We acknowledge that automated data cleaning methods may themselves introduce biases or remove minority viewpoints, especially if not carefully designed. Thus, we hope our benchmark encourages the community to develop data cleaning strategies that are not only effective but also equitable and inclusive. Ultimately, we believe that improving the effectiveness and robustness of data cleaning approaches is key step toward responsible AI development. Our contributions aim to support both academic research and practical deployment efforts by providing tools to critically evaluate and improve the data foundations of human-aligned AI systems."
        },
        {
            "title": "B Prompts of LLM",
            "content": "Table 5 presents the prompts we provided to GPT-4o for data cleaning, adopted from [36]. Each GPT-4 request comprises both system and user prompt. The system prompt delineates the proxys attributes and its specific task, while the user prompt poses question and provides responses from the two methods. These prompts are also used in measuring the win-tie rate for benchmarking alignment."
        },
        {
            "title": "C Reward Models for RM Voting",
            "content": "Table 6 shows the list of our selected gold RMs for RM voting, as well as their performance. The six RMs are selected based on their performance on RewardBench [66]. Specifically, these RMs cover wide range of reward model architectures, including InfoRM [89], QRM [90], GRM [91], and ArmoRM [92]."
        },
        {
            "title": "D Details of Datasets",
            "content": "We consider the following four preference datasets as targets to benchmark data cleaning approaches. Table 7 shows the statistics of each dataset. Anthropic-HH [14]. The authors recruited crowdworkers to have conversations with their three models, and select preferred response from two candidates. The dataset contains two splits: 18 Table 5: Sample prompt for the GPT-4 evaluation. Text highlighted in orange represents the question, while text in blue represents the responses under comparison. System prompt System prompt [System] [System] You are helpful, harmless, and precise assistant for checking the quality of the answer. We would like to request your You are helpful, harmless, and precise assistant for checking the quality of the answer. We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above. feedback on the performance of two AI assistants in response to the user question displayed above. Please rate the harmlessness, helpfulness, and level of detail of their responses. Your evaluation should consider Please rate the harmlessness, helpfulness, and level of detail of their responses. Your evaluation should consider factors such as the helpfulness, harmlessness, relevance, accuracy, depth, creativity, and level of detail of the response. factors such as the helpfulness, harmlessness, relevance, accuracy, depth, creativity, and level of detail of the response. Each assistant receives an overall score on scale of 1 to 10, where higher score indicates better overall Each assistant receives an overall score on scale of 1 to 10, where higher score indicates better overall performance. Please first output single line containing only two values indicating the scores for Assistant 1 and 2, performance. Please first output single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by space. In the subsequent line, please provide comprehensive respectively. The two scores are separated by space. In the subsequent line, please provide comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. presented does not affect your judgment. User prompt User prompt [Question] [Question] ###Human: What are some symptoms of caffeine withdrawel? ###Assistant: ###Human: What are some symptoms of caffeine withdrawel? ###Assistant: [The Start of Assistant 1s Answer] [The Start of Assistant 1s Answer] When caffeine is removed from someone who has habitually consumed it, the body has natural reaction to produce When caffeine is removed from someone who has habitually consumed it, the body has natural reaction to produce more adrenaline, in order to reduce the need for caffeine. This increased adrenaline production causes many different more adrenaline, in order to reduce the need for caffeine. This increased adrenaline production causes many different symptoms, ranging from jitteriness and restlessness, to lethargy and drowsiness. symptoms, ranging from jitteriness and restlessness, to lethargy and drowsiness. [The End of Assistant 1s Answer] [The End of Assistant 1s Answer] [The Start of Assistant 2s Answer] [The Start of Assistant 2s Answer] You might feel an overall sense of mental tiredness, decreased ability to concentrate, and problems sleeping. You You might feel an overall sense of mental tiredness, decreased ability to concentrate, and problems sleeping. You may also experience headaches and sense of jitteriness or edginess. There are also some physical symptoms that may also experience headaches and sense of jitteriness or edginess. There are also some physical symptoms that can appear, such as muscle pain and vomiting. can appear, such as muscle pain and vomiting. [The End of Assistant 2s Answer] [The End of Assistant 2s Answer] Table 6: Selected RMs for RM voting and their scores on RewardBench."
        },
        {
            "title": "Score Chat Hard Safety Reason",
            "content": "infly/INF-ORM-Llama3.1-70B ShikaiChen/LDL-Reward-Gemma-2-27B-v0.1 nicolinho/QRM-Gemma-2-27B Skywork/Skywork-Reward-Gemma-2-27B-v0.2 Ray2333/GRM-Llama3.2-3B-rewardmodel-ft RLHFlow/ArmoRM-Llama3-8B-v0.1 95.1 95.0 94.4 94.3 90.9 90.4 96.6 96.4 96.6 96.1 91.6 96.9 91.0 90.8 90.1 89.9 84.9 76.8 93.6 93.8 92.7 93.0 92.7 90.5 99.1 99.0 98.3 98.1 94.5 97. helpfulness and harmlessness. For helpfulness, crowdworkers were instructed to ask models for help, advice, or to accomplish tasks. Workers then chose response that was more helpful. For harmlessness, workers were asked to attempt to elicit harmful responses from models, and to choose the less harmful one. We combine the two splits in both training and evaluation phases. UltraFeedback [60]. The prompts in this dataset were sampled from several QA and instructionfollowing datasets, including TruthfulQA [93], UltraChat [94], and ShareGPT [95]. The authors generated candidate responses using 17 models, and prompt GPT-4 to score each response in four aspects: instruction-following, truthfulness, honesty, and helpfulness. Each aspect is assessed on Likert-5 scale. Note that in order to fit the definition of preference data in Sec. 3, we use its binarized version processed by Bartolome et al. [96]. In addition, since UltraFeedback does not provide test set, we randomly split it into train (90%) and test (10%) set. PKU-SafeRLHF [75]. The authors utilized LLMs to generate harmful prompts with 19 harm categories, and adopted other LLMs to generate responses for each prompt. The authors then conducted human+AI annotation process to label harm category, severity, as well as preferences in terms of helpfulness and harmlessness. They released the dataset in both single-preference and dual-preference versions, where we utilize the single-preference version in our experiment. 19 Table 7: Statistics of the four target datasets. Split Anthrpoic-HH UltraFeedback PKU-SafeRLHF HelpSteer2 Train Test Total 160,800 8,552 169, 54,825 6,092 60,917 72,996 8,109 81,105 8,677 448 9, Table 8: Training hyperparameters for SFT and PEFT models. Table 9: Configurations of generating responses. Parameter Max new token Do sample Temperature Top Value 256 True 1.0 SFT"
        },
        {
            "title": "PEFT",
            "content": "Parameter Number of epochs Learning rate Batch size Gradient accumulation steps Maximum sequence length DeepSpeed Zero stage Weight decay LoRA rank Number of epochs Learning rate β Batch size Gradient accumulation steps Maximum sequence length DeepSpeed Zero stage Weight decay LoRA rank Value 1 1 105 96 1 512 2 0 0 1 5 105 0.1 64 1 512 2 1 104 HelpSteer2 [76]. The prompts in this dataset were mainly sampled from ShareGPT. For each prompt, two responses were generated from diverse sources, including different LLMs and human annotators. Three to five annotators were hired to annotate one response in five aspects (helpfulness, correctness, coherence, complexity, and verbosity) on Likert-5 scale. In this paper, we utilize HelpSteer2-Preference [97], where each response pair was further labeled by crowdworkers with 7 preference options. Hyperparameters, Configurations, and Computational Details Models training. Table 8 shows the summary of hyperparameters we used for training SFT and PEFT models. All models are trained on 4 Nvidia H200 GPUs. For SFT, each model takes less than 2 hours for training; for PEFT, it takes less than 1.5 hours to train model. Note that for ORPO, we skip the SFT stage as it already includes the SFT term in the loss. Response generation. Table 9 shows the summary of configurations we used for generating responses. Computational cost. We summarize all computational resources/API costs for each data cleaning approach, using the Anthropic-HH dataset (N=160k) as reference. LLM-Judge-R/LLM-Judge-R: Given the GPT-4o API pricing ($2/1M input tokens and $8/1M output tokens), the total API cost on Anthropic-HH is approximately 350USD (<1000 input tokens and <20 output tokens for each data point). RwGap-R/RwGap-F: Training 8 DPO models takes under 12 hours on 4xH200 GPUs. Computing rewards of the 8 DPO models for the entire dataset takes additional <4 hours on 4xH200 GPUs. In total, it takes less than 16 hours on 4xH200 GPUs to clean the dataset. 20 Table 10: Avg. Rwd measured by different reward models. We report the Avg. Rwd of each data cleaning approach measured by QRM and OffsetBias respectively. Anthropic-HH"
        },
        {
            "title": "UltraFeedback",
            "content": "PKU-SalfRLHF HelpSteer2 Approach QRM OffsetBias QRM OffsetBias QRM OffsetBias QRM OffsetBias Vanilla (no clean) 0.656 -4. 0.563 -4.714 0.670 -6.424 0.730 -3. LLM-Judge-R LLM-Judge-F 0.670 0.649 -4.934 -5.021 0.558 0.552 -4.712 -4.783 0.688 0. -6.202 -6.743 0.702 0.689 -4.321 -4.466 LLM-as-a-Judge RwGap-R RwGap-F VoteAll-R VoteAll-F VoteMaj-R VoteMaj-F Tag-Cmp Tag-Div IFD-R IFD-Gap-R IFD-Gap-F 0.662 0.624 0.672 0.685 0.707 0.693 0.694 0.695 0.556 0.666 0.635 -4.815 -5.165 -4.861 -4.721 -4.652 -4.737 -4.901 -4.884 -5.688 -4.801 -5.219 Reward Model-based 0.552 0.557 0.554 0.553 0.560 0.554 -4.792 -4.751 -4.792 -4.791 -4.658 -4. 0.666 0.674 0.580 0.574 0.748 0.563 Heuristic-based 0.551 0.547 0.546 0.556 0.555 -4.756 -4.791 -4.846 -4.687 -4.765 0.705 0.704 0.769 0.697 0.619 -6.531 -6.604 -7.484 -7.433 -5.541 -7. -6.161 -6.036 -6.373 -6.087 -6.773 0.684 0.686 0.685 0.691 0.746 0.694 0.736 0.742 0.708 0.707 0.694 -4.511 -4.525 -4.537 -4.511 -3.653 -4.444 -3.844 -3.845 -4.184 -4.251 -4.435 VoteAll-R/VoteAll-F/VoteMaj-R/VoteMaj-F: Each reward model takes <1 hour on 4xH200 GPUs to compute reward for the entire dataset. In total, it takes less than 6 hours on 4xH200 GPUs to clean the dataset. Tag-Cmp/Tag-Div: Generate tags and clean the full dataset takes >24 hours using HuggingFaces AutoModelForCausalLM. The process could be significantly faster with optimized backends like vLLM3. IFD-R/IFD-Gap-R/IFD-Gap-F: It takes less than 6 hours to compute IFD score with Llama3-8B on 4xH200 GPUs for the entire dataset. Overall, VoteMaj-R and IFD-Gap-R offer strong trade-offs between cleaning effectiveness and computational efficiency."
        },
        {
            "title": "F Additional Experimental Results",
            "content": "In Sec. 5.1, we measure average gold rewards by LxzGordon/URM-LLaMa-3.1-8B [78]. To ensure robustness of our evaluation, we additionally measure rewards using nicolinho/QRM-Llama3.18B-v2 [90] and NCSOFT/Llama-3-OffsetBias-RM-8B [98]. Table 10 shows that although different reward models compute rewards with different scale, they follow consistent trend that VoteMaj-R achieves the highest rewards in most cases. 3vLLM: https://github.com/vllm-project/vllm"
        }
    ],
    "affiliations": [
        "Department of Computer Science University of Wisconsin-Madison"
    ]
}