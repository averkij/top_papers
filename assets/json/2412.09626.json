{
    "paper_title": "FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion",
    "authors": [
        "Haonan Qiu",
        "Shiwei Zhang",
        "Yujie Wei",
        "Ruihang Chu",
        "Hangjie Yuan",
        "Xiang Wang",
        "Yingya Zhang",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. To tackle this challenge, we propose FreeScale, a tuning-free inference paradigm to enable higher-resolution visual generation via scale fusion. Specifically, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Notably, compared with the previous best-performing method, FreeScale unlocks the generation of 8k-resolution images for the first time."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 6 2 6 9 0 . 2 1 4 2 : r FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion Haonan Qiu1, Shiwei Zhang*2, Yujie Wei3, Ruihang Chu2, Hangjie Yuan2, Xiang Wang2, Yingya Zhang2, Ziwei Liu 1Nanyang Technological University 2Alibaba Group 3Fudan University Project Page: http://haonanqiu.com/projects/FreeScale.html Figure 1. Gallery of FreeScale. Original SDXL citesdxl can only generate images with resolution of up to 10242 without losing quality, while FreeScale successfully extends SDXL to generate 81922 images without any fine-tuning. All generated images are produced using single A800 GPU. Best viewed ZOOMED-IN."
        },
        {
            "title": "Abstract",
            "content": "Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency *Project Leader Corresponding Authors information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. To tackle this challenge, we propose FreeScale, tuning-free inference paradigm to enable higher-resolution visual generation via scale fusion. Specifically, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Notably, compared with previous best-performing methods, FreeScale unlocks the generation of 8k-resolution images for the first time. 1 1. Introduction Diffusion models have revolutionized visual generation [10, 11, 36, 45, 48, 50], empowering individuals without any artistic expertise to effortlessly create distinctive and personalized designs, graphics, and short films using specific textual descriptions. Nonetheless, current visual diffusion models are generally trained on data with limited resolution, such as 5122 for SD 1.5 [39], 10242 for SDXL [36], and 320 512 for VideoCrafter2 [10], hampering their ability to generate high-fidelity images or videos at higher resolutions. Given the scarcity of high-resolution visual data and the substantially greater model capacity required for modeling such data, recent efforts have focused on employing tuning-free strategies for high-resolution visual generation to inherit the strong generation capacities of existing pretrained diffusion models. Despite the advances achieved by existing methods, they are still prone to producing low-quality images or videos, particularly manifesting as repetitive object occurrences and unreasonable object structures. ScaleCrafter [18] puts forward that the primary cause of the object repetition issue is the limited convolutional receptive field and uses dilated convolutional layers to achieve tuning-free higherresolution sampling. But the generated results of ScaleCrafter still suffer from the problem of local repetition. Inspired by MultiDiffusion [2] fusing the local patches of the whole images, DemoFusion [13] designed mechanism by fusing the local patches and global patches, almost eliminating the local repetition. Essentially, this solution just transfers the extra signal of the object to the background, leading to small object repetition generation. FouriScale [23] reduces those extra signals by removing the high-frequency signals of the latent before the convolution operation. Although FouriScale completely eliminates all types of repetition, the generated results always have weird colors and textures due to its violent editing on the frequency domain. To generate satisfactory visual contents without any unexpected repetition, we propose FreeScale, tuning-free inference paradigm that enables pre-trained image and video diffusion models to generate vivid higher-resolution results. Building on past effective modules [14, 18], we first propose tailored self-cascade upscaling and restrained dilated convolution to gain the basic visual structure and maintain the quality in higher-resolution generation. To further eliminate all kinds of unexpected object repetitions, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components, ensuring both the structures overall rationality and the objects local quality. This fusion is smoothly integrated into the original self-attention layers, thereby bringing only minimal additional time overhead. Finally, we demonstrate the effectiveness of our model on both the text-to-image model and the text-to-video model, pushing the boundaries of image generation even up to an 8k resolution. Our contributions are summarized as follows: We propose FreeScale, tuning-free inference paradigm to enable pre-trained diffusion models to generate vivid higher-resolution results via fusing the information from different scales. We empirically evaluate our approach on both the text-toimage model and the text-to-video model, demonstrating the effectiveness of our model. We unlock the generation of 8k-resolution images for the first time. Compared to other state-of-the-art tuning-free methods, FreeScale gains superior visual quality with less inference time. 2. Related Work Diffusion Models for Visual Generation. The advent of diffusion models has transformed the landscape of image and video generation by enabling the production of exceptionally high-quality outputs [10, 11, 36, 41, 45, 4750]. Initial breakthroughs like DDPM [20] and Guided Diffusion [12] demonstrated that diffusion processes could yield remarkable image quality. To enhance computational efficiency, LDM [39] introduced latent space diffusion, which operates in compressed space, significantly lowering the computational burden and training demands; this method laid the groundwork for Stable Diffusion. Building on this, SDXL [36] further advanced high-resolution image synthesis. Inspired by Dit[35], Pixart-alpha [11] adopted transformer-based architecture, achieving both high fidelity and cost-effective image generation. For video generation, VDM [21] pioneered the application of diffusion in this domain, followed by LVDM [17], which extended the method to propose hierarchical latent video diffusion framework capable of generating extended video sequences. To bridge text-to-image and text-to-video (T2V) capabilities, Align-Your-Latents [6] and AnimateDiff [15] introduced temporal transformers into existing T2I models. VideoComposer [46] then offered controllable T2V generation approach, allowing precise management of spatial and temporal cues. VideoCrafter [9, 10] and SVD [5] scaled these latent video diffusion models to handle extensive datasets. Lumiere [3] proposed temporal downsampling within space-time U-Net for greater efficiency. Finally, CogVideoX [48] and Pyramid Flow [26] two recent highly regarded open-source models, showcase impressive video generation capabilities, demonstrating the superior performance of DiT structure in video generation. Since the DiT structure models often take up more memory, achieving high-resolution generation on single GPU is difficult even in the inference phase. Therefore, we still use the U-Net structure models in this work. We chose SDXL [36] as our pre-trained image model, and VideoCrafter2 [10] as our pre-trained video model. 2 Higher-Resolution Visual Generation. High-resolution visual synthesis is classic challenging in the generative field due to the difficulty of collecting plenty of highresolution data and the requirement of substantial computational resources. Recent methods for higher-resolution generation can mainly be divided into two categories: 1) training/tuning methods with high-resolution data and large models [14, 22, 33, 38, 43, 51], or 2) tuning-free methods without any additional data requirement [7, 13, 16, 18, 25, 27, 30, 31]. Training with high-resolution data on larger models should be more fundamental solution. However, high-resolution visual data, especially high-resolution videos, only accounts for small proportion. Meanwhile, targeting for modeling higher-resolution data demands notably increased requirement in model capacity. Based on current data and calculation resources, tuning-free approaches are more achievable for high-resolution generation. One straightforward approach is to generate video patches of the same resolution as the training video and then stitch them together. Although eliminating the traininginference gap, this method results in disconnected and incoherent patches. MultiDiffusion [2] addresses this issue by smoothly fusing patches during the denoising process. DemoFusion [13] utilizes this mechanism and adds global perception to ensure the rationality of the overall layout. However, this solution easily leads to small object repetition generation. ScaleCrafter [18] argues that the object repetition issue is mainly caused by the limited convolutional receptive field and uses dilated convolutional layers to enlarge the convolutional receptive field. Although successfully removing small object repetition, ScaleCrafter suffers from new problem of local repetition. FouriScale [23] concludes that all kinds of repetitions are from the nonalignment of frequency domain in different scales. FouriScale removes the high-frequency signals of the latent before the convolution operation and achieves no repetition at all. But this violent editing operation on the frequency domain leads to weird results with unnatural colors and textures. Another solution is directly removing the text semantics from unexpected areas in the input level [32, 34]. However, it only works for small object repetition and will suffer information leakage through the temporal layers in the video generation. 3. Methodology 3.1. Preliminaries Latent Diffusion Models (LDM) first encodes an given image to the latent space via the encoder of the pretrained auto-encoder E: = E(x). Then forward diffusion process is used to gradually add noise to the latent data z0 p(z0) and learn denoising model to reverse this process. The forward process contains timesteps, 3 which gradually add noise to the latent sample z0 to yield zt through parameterization trick: q(ztzt1) = (zt; (cid:112)1 βtzt1, βtI), q(ztz0) = (zt; αtz0, (1 αt)I), (1) where βt is predefined variance schedule, is the timestep, αt = (cid:81)t i=1 αi, and αt = 1 βt. The reverse denoising process obtains less noisy latent zt1 from the noisy input zt at each timestep: pθ (xt1 xt) = (xt1; µθ (zt, t) , Σθ (zt, t)) , (2) where µθ and Σθ are determined through noise prediction network ϵθ (zt, t) with learnable parameters θ. 3.2. Tailored Self-Cascade Upscaling Directly generating higher-resolution results will easily produce several repetitive objects, losing the reasonable visual structure that was originally good. To address this issue, we utilize self-cascade upscaling framework from previous works [13, 14], which progressively increases the resolution of generated results: (cid:0) z2r 1 αKI(cid:1) , αKϕ (zr 0) , (3) where means the noised intermediate latent, is the resolution level (1 represents original resolution, 2 represents the twice height and width), and ϕ is an upsampling operation. In this way, the framework will generate reasonable visual structure in low resolution and maintain the structure when generating higher-resolution results. There are two options for ϕ: directly upsampling in latent (ϕ (z) = UP(z)) or upsampling in RGB space (ϕ (z) = E(UP(D(z))), where and are the encoder and decoder of pre-trained VAE, respectively. Upsampling in RGB space is closer to human expectations but will add some blurs. We empirically observe that these blurs will hurt the video generation but help to suppress redundant overfrequency information in the image generation. Therefore, we adopt upsampling in RGB space for higher-solution image generation and latent space upsampling in highersolution video generation. Flexible Control for Detail Level. Different from superresolution tasks, FreeScale will endlessly add more details as the resolution grows. This behavior will hurt the generation when all reasonable details are generated. To control the level of newly generated details, we modify pθ (zt1 zt) to pθ (zt1 ˆzt) with: = zr ˆzr where = (cid:0)(cid:0)1 + cos (cid:0) decay factor with scaling factor α. + (1 c) zr , π(cid:1)(cid:1) /2(cid:1)α (4) is scaled cosine Figure 2. Overall framework of FreeScale. (a) Tailored Self-Cascade Upscaling. FreeScale starts with pure Gaussian noise and progressively denoises it using the training resolution. An image is then generated via the VAE decoder, followed by upscaling to obtain higher-resolution one. We gradually add noise to the latent of this higher-resolution image and incorporate this forward noise into the denoising process of the higher-resolution latent with the use of restrained dilated convolution. Additionally, for intermediate latent steps, we enhance high-frequency details by applying region-aware detail control using masks derived from the image. (b) Scale Fusion. During denoising, we adapt the self-attention layer to global and local attention structure. By utilizing Gaussian blur, we fuse high-frequency details from global attention and low-frequency semantics from local attention, serving as the final output of the self-attention layer. Even in the same image, the detail level varies in different areas. To achieve more flexible control, α can be 2D-tensor and varies spatially. In this case, users can assign different values for different semantic areas according to (zr 0) calculated in the previous process already. 3.3. Restrained Dilated Convolution ScaleCrafter [18] observes that the primary cause of the object repetition issue is the limited convolutional receptive field and proposes dilated convolution to solve it. Given hidden feature map h, convolutional kernel k, and the the dilation operation Φd() with factor d, the dilated convolution can be represented as: k(h) = hΦd(k), (h Φd(k)) (o) = (cid:88) s+dt=p h(p)k(q), (5) where o, p, and are spatial locations used to index the feature or kernel. denotes convolution operation. To avoid catastrophic quality decline, ScaleCrafter [18] only applies dilated convolution to some layers of UNet while still consisting of several up-blocks. However, we find that dilated convolution in the layers of up-blocks will bring many messy textures. Therefore, unlike previous works, we only apply dilated convolution in the layers of 4 down-blocks and mid-blocks. In addition, the last few timesteps only render the details of results and the visual structure is almost fixed. Therefore, we use the original convolution in the last few timesteps. 3.4. Scale Fusion Although tailored self-cascade upscaling and restrained dilated convolution can maintain the rough visual structures and effectively generate 4 resolution images, generating 16 resolution images still leads to artifacts such as local repetition, e.g., additional eyes or noses. This issue arises because dilated convolution weakens the focus on local features. DemoFusion [13] addresses this by using local patches to enhance local focus. However, although the local patch operation mitigates local repetition, it brings small object repetition globally. To combine the advantages of both strategies, we design Scale Fusion, which fuses information from different receptive scales to achieve balanced enhancement of local and global details. Regarding global information extraction, we utilize global self-attention features. the self-attention layer enhances the patch information based on similarity, making it easier for the subsequent crossattention layer to aggregate semantics into complete obThe reason is that Table 1. Image quantitative comparisons with other baselines. FreeScale achieves the best or second-best scores for all quality-related metrics with negligible additional time costs. The best results are marked in bold, and the second best results are marked by underline. Method 20482 40962 FID KID FIDc KIDc Time (min) FID KID FIDc KIDc Time (min) SDXL-DI [36] ScaleCrafter [18] DemoFusion [13] FouriScale [23] Ours 64.313 67.545 65.864 68.965 44. 0.008 0.013 0.016 0.016 0.001 31.042 60.151 63.001 69.655 36.276 0.004 0.020 0.024 0.026 0.006 0.648 0.653 1.441 1.224 0.853 134.075 100.419 72.378 93.079 49.796 0.044 0.033 0.020 0.029 0. 42.383 116.179 94.975 128.862 71.369 0.009 0.053 0.045 0.068 0.029 5.456 9.255 11.382 8.446 6.240 ject. This can be formulated as: hglobal out = SelfAttention (hin) = softmax (cid:18) QK (cid:19) V, where = LQ(hin), = LK(hin), = LV (hin). In this formulation, query Q, key K, and value are calculated from hin through the linear layer L, and is scaling coefficient for the self-attention. is After the to that, applied self-attention layer local these indepenrepresentalatent dently tions via hout, = SelfAttention (hin, n). And then Hlocal out = [hout, 0 , hout, , hout, N] is reconstructed to the original size with the overlapped parts averaged (cid:1), where Rlocal denotes the as hlocal out reconstruction process. = Rlocal (cid:0)Hlocal out Regarding local information extraction, we follow previous works [2, 13, 37] by calculating self-attention locally to enhance the local focus. Specifically, we first apply shifted crop sampling, Slocal(), to obtain series of local latent representations before each self-attention layer, i.e., Hlocal in = Slocal (hin) = [hin, 0 , hin, , hin, N] , hin, (cid:16) (Hh) (cid:17) Rchw, where = , dh with dh and dw representing the vertical and horizontal stride, respectively. After that, the self-attention layer is independently applied to these local latent representations via hout, = SelfAttention (hin, n). The resulting outputs Hlocal out = [hout, 0 , hout, , hout, N] are then mapped back to the original positions, with the overlapped parts av- (cid:1), where Rlocal denotes eraged to form hlocal the reconstruction process. (cid:16) (W w) dw out = Rlocal (cid:0)Hlocal + 1 + 1 out (cid:17) While hlocal out tends to produce better local results, it can bring unexpected small object repetition globally. These artifacts mainly arise from dispersed high-frequency signals, which will originally be gathered to the right area through global sampling. Therefore, we replace the highfrequency signals in the local representations with those from the global level hglobal : out (cid:16) out = hglobal hfusion (cid:124) out (cid:123)(cid:122) high frequency hglobal out (cid:17) (cid:125) + (cid:0)hlocal (cid:1) , (7) out (cid:124) (cid:125) (cid:123)(cid:122) low frequency where is low-pass filter implemented as Gaussian blur, and hglobal acts as high pass of hfusion out hglobal out (cid:16) (cid:17) . out (6) 4. Experiments Experimental Settings. We conduct experiments based on an open-source T2I diffusion model SDXL [36] and an open-source T2V diffusion model VideoCrafter2 [10]. Considering the computing resources that can be afforded, we evaluate the image generation at resolutions of 20482 and 40962, and video generation at resolutions of 640. All experiments are produced using single A800 GPU. Datasets. We evaluate image generation on the LAION5B dataset [40] with 1024 randomly sampled captions. Specifically, to better align with human preference, we randomly selected prompts from the LAION-Aesthetics-V26.5plus dataset to evaluate image generation. The LAIONAesthetics-V2-6.5plus is subset of the LAION 5B dataset, characterized by its high visual quality, where images have scored 6.5 or higher according to aesthetic prediction models. Regarding the evaluation of video generation, we use randomly sampled 512 captions from the WebVid-10M dataset [1]. Evaluation Metrics. Since higher-resolution inference methods are intended to maintain the quality of the original resolution outputs, we calculate all metrics between the originally generated low-resolution images/videos and the corresponding high-resolution outputs. To evaluate the quality of generated images, we report Frechet Image Distance (FID) [19], and Kernel Image Distance (KID) [4]. FID and KID need to resize the images to 299 before the comparison and this operation may cause quality loss for high-resolution images. Inspired by previous work [8], we also use cropped local patches to calculate these metrics without resizing, termed FIDc and KIDc. We use Frechet Video Distance (FVD) [44] to evaluate the quality of video generation. In addition, we test dynamic degree and aesthetic quality from the VBench [24] to evaluate the dynamics and aesthetics. 4.1. Higher-Resolution Image Generation We compare FreeScale with other higher-solution image generation methods: (i) SDXL [36] direct inference 5 Figure 3. Image qualitative comparisons with other baselines. Our method generates both 20482 and 40962 vivid images with better content coherence and local details. Best viewed ZOOMED-IN. 6 Figure 4. Qualitative results of flexible control for detail level. better result will be generated by adding the coefficient weight in the area of Griffons and reducing the coefficient weight in the other regions. Best viewed ZOOMED-IN. Figure 5. Video qualitative comparisons with other baselines. While other baselines fail in video generation, FreeScale effectively generates higher-resolution videos with high fidelity. Best viewed ZOOMED-IN. (SDXL-DI) (ii) ScaleCrafter [18] (iii) DemoFusion [13], and (iv) FouriScale [23]. FreeU [41] is used if compatible. Qualitative comparison results are shown in Figure 3. We observe that direct generation often results in multiple duplicated objects and loss of the original visual structure. ScaleCrafter tends to produce localized repetitions, while DemoFusion generates isolated small objects nearby. FouriScale can drastically alter the style for certain prompts. In contrast, the proposed FreeScale is capable of generating high-quality images without any unexpected repetition. The quantitative results also confirm the superiority of FreeScale. As shown in Table 1, SDXL-DI achieves the best FIDc and KIDc. The reason is that SDXL-DI tends to generate multiple duplicated objects and its crop may be closer to the reference images. However, this behavior will sacrifice the visual structure thus SDXL gains the worst FID and KID in the resolution of 40962. Overall, our approach achieves the best or second-best scores for all quality-related metrics with negligible additional time costs. In addition, FreeScale provides flexible control for detail level in generated results. Figure 4 shows demo of changing the detail level of different semantic areas. After we get the upsampled 1 results, it is easy to calculate semantic masks [28] and assign different α for each region in Equa7 Table 2. Image quantitative comparisons with other ablations. Our final FreeScale achieves better quality-related metric scores in all experiment settings. The best results are marked in bold. Method 40962 FID KID FIDc KIDc Time (min) FID KID FIDc KIDc Time (min) w/o Scale Fusion Dilated Up-Blocks Latent Space Upsampling Ours 75.717 75.372 72.454 44.723 0.017 0.017 0.015 0.001 76.536 76.673 71.793 36.276 0.026 0.025 0.023 0.006 0.614 0.861 0.840 0.853 68.115 67.447 65.081 49. 0.012 0.011 0.009 0.004 100.065 98.558 88.632 71.369 0.037 0.035 0.029 0.029 4.566 6.245 6.113 6.240 Table 3. Video quantitative comparisons with baselines. FreeScale achieves the best scores for all metrics. Method FVD Dynamic Degree Aesthetic Quality Time (min) VC2-DI [10] ScaleCrafter [18] DemoFusion [13] Ours 611.087 723.756 537.613 484.711 0.191 0.104 0.342 0.383 0.580 0.584 0.614 0. 4.077 4.098 9.302 3.787 tion 4. As shown in Figure 4, we will obtain better result when we add the coefficient weight in the area of Griffons and reduce the coefficient weight in other regions. Figure 6. Qualitative image comparisons with ablations. Our full method performs the best. The resolution of results is 40962 for better visualizing the difference between the various strategies. Best viewed ZOOMED-IN. 4.2. Higher-Resolution Video Generation We compare FreeScale with other tuning-free highersolution video generation methods: (i) VideoCrafter2 [10] direct inference (VC2-DI) (ii) ScaleCrafter [18], and (iii) DemoFusion [13]. FouriScale [23] is not evaluated since its bundled FreeU [41] does not work well in video generation. As shown in Figure 5, the behavior of VC2-DI and ScaleCrafter are similar to the corresponding version in image generation, tending to generate duplicated whole objects and local parts, respectively. However, DemoFusion has completely unexpected behavior in the video generation. Its Dilated Sampling mechanism brings strange patterns all over the frames and Skip Residual operation makes the whole video blur. In contrast, our FreeScale effectively generates higher-resolution videos with high fidelity. Table 3 exhibits that our method achieves the best FVD with shorter inference time. More quantitative evaluations can be found in the supplementary file. 4.3. Ablation Study The proposed FreeScale mainly consists of three components: (i) Tailored Self-Cascade Upscaling, (ii) Restrained Dilated Convolution, and (iii) Scale Fusion. To visually demonstrate the effectiveness of these three components, we conducted ablations on the SDXL generating 20482 and 40962 images. First, we show the advantage of upsampling in RGB space. As shown in Figure 6, upsampling in latent space brings certain artifacts in the lions eyes. Then dilating the convolution in up-blocks or removing Scale Fusion will cause some cluttered textures that appear in the generated results due to small repetition problems. Table 2 shows that our final FreeScale achieves better quality-related metric scores in all experimental settings. 5. Conclusion This study introduces FreeScale, tuning-free inference paradigm designed to enhance high-resolution generation capabilities in pre-trained diffusion models. By leveraging multi-scale fusion and selective frequency extraction, FreeScale effectively addresses common issues in highresolution generation, such as repetitive patterns and quality degradation. Experimental results demonstrate the superiority of FreeScale in both image and video generation, surpassing existing methods in visual quality while also having significant advantages in inference time. Compared to prior approaches, FreeScale not only eliminates various forms of visual repetition but also ensures detail clarity and structural coherence in generated visuals. Eventually, FreeScale achieves unprecedented 8k-resolution image generation. 6. Acknowledgements This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2022-01-035T), the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-T2EP202210012), NTU NAP, and Alibaba Group. Special thanks to Yingqing He and Lanqing Guo for sharing their invaluable experience and advice in getting us started quickly in the higher-resolution visual generation task."
        },
        {
            "title": "References",
            "content": "[1] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for In IEEE International Conference on end-to-end retrieval. Computer Vision, 2021. 5 [2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113, 2023. 2, 3, 5 [3] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: spacetime diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. 2 [4] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. 5 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 2 [7] Boyuan Cao, Jiaxin Ye, Yujie Wei, and Hongming Shan. Ap-ldm: Attentive and progressive latent diffusion model for training-free high-resolution image generation. arXiv preprint arXiv:2410.06055, 2024. [8] Lucy Chai, Michael Gharbi, Eli Shechtman, Phillip Isola, and Richard Zhang. Any-resolution training for highIn European Conference on resolution image synthesis. Computer Vision, pages 170188. Springer, 2022. 5 [9] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 2 [10] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 2, 5, 8 [11] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 2 [12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2 [13] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion: Democratising highresolution image generation with no $$$. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61596168, 2024. 2, 3, 4, 5, 7, 8, 1 [14] Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xintao Wang, Qifeng Chen, et al. Make cheap scaling: self-cascade diffusion model for higher-resolution adaptation. arXiv preprint arXiv:2402.10491, 2024. 2, [15] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2 [16] Moayed Haji-Ali, Guha Balakrishnan, and Vicente Ordonez. Elasticdiffusion: Training-free arbitrary size image generation through global-local content separation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66036612, 2024. 3 [17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity arXiv preprint video generation with arbitrary lengths. arXiv:2211.13221, 2022. 2 [18] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higherresolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2024. 2, 3, 4, 5, 7, 8, 1 [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 5 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2 [22] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution imIn International Conference on Machine Learning, ages. pages 1321313232. PMLR, 2023. 3 [23] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. Fouriscale: frequency perspective on training-free high-resolution image synthesis. arXiv preprint arXiv:2403.12963, 2024. 2, 3, 5, 7, 8, 1 9 [24] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. arXiv preprint arXiv:2311.17982, 2023. 5 [25] Juno Hwang, Yong-Hyun Park, and Junghyo Jo. Upsample guidance: Scale up diffusion models without training. arXiv preprint arXiv:2404.01709, 2024. [26] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. 2024. 2 [27] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Trainingfree diffusion model adaptation for variable-sized text-toimage synthesis. Advances in Neural Information Processing Systems, 36:7084770860, 2023. 3 [28] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 7 [29] Black Forest Labs. Flux.1 : An advanced state-of-the-art generative deep learning model. Technical report, Black Forest Labs, 2024. 1, 2 [30] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. Syncdiffusion: Coherent montage via synchronized joint diffusions. Advances in Neural Information Processing Systems, 36:5064850660, 2023. 3 [31] Mingbao Lin, Zhihang Lin, Wengyi Zhan, Liujuan Cao, and Rongrong Ji. Cutdiffusion: simple, fast, cheap, and strong diffusion extrapolation method. arXiv preprint arXiv:2404.15141, 2024. 3 [32] Zhihang Lin, Mingbao Lin, Meng Zhao, and Rongrong Ji. Accdiffusion: An accurate method for higher-resolution image generation. arXiv preprint arXiv:2407.10738, 2024. [33] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. 2024. 3 [34] Xinyu Liu, Yingqing He, Lanqing Guo, Xiang Li, Bu Jin, Peng Li, Yan Li, Chi-Min Chan, Qifeng Chen, Wei Xue, et al. Hiprompt: Tuning-free higher-resolution genarXiv preprint eration with hierarchical mllm prompts. arXiv:2409.02919, 2024. 3 [35] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 2 [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 5, 1 [37] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. 5 [38] Jingjing Ren, Wenbo Li, Haoyu Chen, Renjing Pei, Bin Shao, Yong Guo, Long Peng, Fenglong Song, and Lei Zhu. Ultrapixel: Advancing ultra-high-resolution image synthesis to new peaks. arXiv preprint arXiv:2407.02158, 2024. 3 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. 2 [40] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 5 [41] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. In CVPR, 2024. 2, Freeu: Free lunch in diffusion u-net. 7, 8 [42] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 1 and Stefano Ermon. arXiv preprint [43] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. 3 [44] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 5 [45] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report, 2023. 2 [46] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. NeurIPS, 2023. 2 [47] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos In Proceedings of with customized subject and motion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65376549, 2024. 2 [48] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [49] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel AlInstructvideo: Instructing video difbanie, and Dong Ni. fusion models with human feedback. In CVPR, 2024. [50] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. [51] Qingping Zheng, Yuanfan Guo, Jiankang Deng, Jianhua Han, Ying Li, Songcen Xu, and Hang Xu. Any-sizediffusion: Toward efficient text-driven synthesis for any-size hd images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 75717578, 2024. 3 FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion"
        },
        {
            "title": "Supplementary Material",
            "content": "Overview. In the supplementary material, we introduce implementation details in Section A, show more evaluation results in Section B, exhibit gallery of 8k images in Section C, and finally, discuss limitation and future work in Section D. A. Implementation Details During sampling, we perform DDIM sampling [42] with 50 denoising steps, setting DDIM eta to 0. For image generation, the base inference resolution of SDXL is 1024 1024 pixels, and the scale of the classifier-free guidance is set to 7.5. For video generation, the base inference resolution of VideoCrafter2 is 320 512, the video length is 16 frames, and The scale of the classifier-free guidance is set to 12.0. For tailored self-cascade upscaling, we set = 700 in Equation 3 for all experiments. And in Equation 4, α is set as scaler, 2, by default. To avoid excessive and messy textures in generating 8k images, α is reduced to 1. In Figure 4, α is 3 and 0.5 in the targeted and other areas, respectively. For restrained dilated convolution, the dilation factor in Equation 5 is equal to the resolution level (1 represents original resolution, 2 represents the twice height and width). B. More Evaluation B.1. Ablation Study for Video Generation We also conduct an ablation study for higher-solution video generation. As discussed in the method part, we adopt latent space upsampling in video generation. Table 4 shows that our final setting achieves the best or second-best scores for all metrics. B.2. User Study In addition, we conducted user study to evaluate our results on human subjective perception. Users are asked to watch the generated images of all the methods, where each example is displayed in random order to avoid bias, and then pick up the best one in three evaluation aspects. total of 23 users were asked to pick the best one according to the image-text alignment, image quality, and visual structure, respectively. As shown in Table 5, our approach gains the most votes for all aspects, outperforming baseline methods by large margin. C. Gallery of 8k Images Figure 8 illustrates the effectiveness of FreeScale on generating ultra-high-resolution images (i.e., 8k-resolution imTable 4. Video quantitative comparisons with other ablations. Our final setting achieves the best or second-best scores for all metrics. The best results are marked in bold, and the second best results are marked by underline. Method FVD Dynamic Degree Aesthetic Quality Time (min) Dilated Up-Blocks RGB Upsampling Ours 523.323 422.245 484.711 0.363 0.381 0. 0.611 0.604 0.621 3.788 3.799 3.787 Table 5. User study. Users are required to pick the best one among our proposed FreeScale with the other baseline methods in terms of image-text alignment, image quality, and visual structure. Method Text Alignment Image Quality Visual Structure SDXL-DI [36] ScaleCrafter [18] DemoFusion [13] FouriScale [23] Ours 0.87% 7.83% 17.39% 2.17% 71.74% 0.00% 5.22% 14.35% 2.61% 77.83% 0.00% 7.83% 18.26% 1.74% 72.17% ages). As shown in Figure 7, FreeScale effectively enhances local details without compromising the original visual structure or introducing object repetitions. Different from simple super-resolution, FreeScale may regenerate the original blurred areas at low resolution based on the prior knowledge that the model has learned. In Figure 7, two originally chaotic and blurry faces are clearly outlined at 8k resolution. Visual Enhancement. FreeScale also supports using existing images to replace the intermediate 1 result. Compared to SDXL [36], FLUX [29] is better in visual text generation. In the center of Figure 8, we first use FLUX to generate the intermediate 1 result, dragon with FreeScale. Then we utilize the remaining pipeline of FreeScale to generate the final 8k-resolution result. In this sense, FreeScale is also tool to upscale resolution and enhance detail. D. Limitation and Future Work Inference Cost. We employ the scale fusion only in the self-attention layers thus bringing negligible time cost. And the omitted time steps almost offset the additional cost of tailored self-cascade upscaling. As result, the inference cost of FreeScale is close to the direct inference by the base model. However, the inference cost is still huge for ultrahigh-resolution generation. In future work, when users require image generation at resolutions exceeding 8k, memory constraints may be mitigated through multi-GPU inference strategies, while computational efficiency can be enhanced by employing inference acceleration techniques. Knowledge Limitation. Even ignoring the limitations of 1 Figure 7. Zoomed in details for the 8k image. FreeScale may regenerate the original blurred areas at low resolution based on the prior knowledge that the model has learned. As shown in the bottom row, two originally chaotic and blurry faces are clearly outlined at 8k resolution. Best viewed ZOOMED-IN. fully obtain the dragon with FreeScale in Figure 8, leveraging FLUXs superior visual text rendering capabilities. However, this approach results in slight loss of quality due to the gap in understanding of the same text by different models. The final solution still needs to be customized specifically for DiT-based LDMs. the computation, there is limit to the upscaling capability of FreeScale. When the desired resolution is beyond the prior knowledge that the model has learned, no more details can be reasonably added. In other words, the endless higher-resolution result will have either the same level of detail or unnatural messy detail. In addition, as tuningfree framework, FreeScales performance relies heavily on base models. During the tailored self-cascade process, the intermediate 1 result is equivalent to direct inference with base models. Some artifacts caused by inherently flawed (e.g., extra legs), will be inherited in further upscaling. Generalization. DiT-based LDMs (e.g., FLUX [29] and CogVideoX [48]), have showcased impressive visual generation capabilities recently. When DiT-based LDMs are required to directly generate higher-resolution results, obvious quality degradation also occurs, but in significantly different way than in the UNet-based LDMs. Therefore, FreeScale currently only supports the UNet-based LDMs without further modification. One temporary solution is using DiT-based LDMs to generate the intermediate 1 result and then utilizing the remaining pipeline of FreeScale to generate higher-resolution results. In this way, we successFigure 8. Gallery of generated 8k images. We place the original-resolution result in the lower right corner for reference. FreeScale effectively enhances local details without compromising the visual structure or introducing object repetitions. Best viewed ZOOMED-IN."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Fudan University",
        "Nanyang Technological University"
    ]
}