{
    "paper_title": "Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning",
    "authors": [
        "Yunhao Gou",
        "Kai Chen",
        "Zhili Liu",
        "Lanqing Hong",
        "Xin Jin",
        "Zhenguo Li",
        "James T. Kwok",
        "Yu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in slow-thinking language models (e.g., OpenAI-o1 and DeepSeek-R1) have demonstrated remarkable abilities in complex reasoning tasks by emulating human-like reflective cognition. However, extending such capabilities to multi-modal large language models (MLLMs) remains challenging due to the high cost of retraining vision-language alignments when upgrading the underlying reasoner LLMs. A straightforward solution is to decouple perception from reasoning, i.e., converting visual inputs into language representations (e.g., captions) that are then passed to a powerful text-only reasoner. However, this decoupling introduces a critical challenge: the visual extractor must generate descriptions that are both faithful to the image and informative enough to support accurate downstream reasoning. To address this, we propose Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) - a reasoning-guided reinforcement learning strategy that aligns the extractor's captioning behavior with the reasoning objective. By closing the perception-reasoning loop via reward-based optimization, RACRO significantly enhances visual grounding and extracts reasoning-optimized representations. Experiments on multi-modal math and science benchmarks show that the proposed RACRO method achieves state-of-the-art average performance while enabling superior scalability and plug-and-play adaptation to more advanced reasoning LLMs without the necessity for costly multi-modal re-alignment."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 9 5 5 4 0 . 6 0 5 2 : r Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning Yunhao Gou1,2 Kai Chen2 Zhili Liu2,3 Lanqing Hong3 Xin Jin4 Zhenguo Li3 James T. Kwok2 Yu Zhang1 1Southern University of Science and Technology 2The Hong Kong University of Science and Technology 3 Huawei Noahs Ark Lab 4 Huawei Cloud Code: https://github.com/gyhdog99/RACRO2/"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in slow-thinking language models (e.g., OpenAI-o1 and DeepSeekR1) have demonstrated remarkable abilities in complex reasoning tasks by emulating human-like reflective cognition. However, extending such capabilities to the multi-modal large language models (MLLMs) remains challenging due to the high cost of retraining vision-language alignments when upgrading the underlying reasoner LLMs. straightforward solution is to decouple perception from reasoning, i.e., converting visual inputs into language representations (e.g., captions) that are then passed to powerful text-only reasoner. However, this decoupling introduces critical challenge: the visual extractor must generate descriptions that are both faithful to the image and informative enough to support accurate downstream reasoning. To address this, we propose Reasoning-Aligned Perceptual Decoupling via Caption Reward Optimization (RACRO) reasoning-guided reinforcement learning strategy that aligns the extractors captioning behavior with the reasoning objective. By closing the perception-reasoning loop via reward-based optimization, RACRO significantly enhances visual grounding and extracts reasoning-optimized representations. Experiments on multi-modal math and science benchmarks show that the proposed RACRO method achieves state-of-the-art average performance while enabling superior scalability and plug-and-play adaptation to more advanced reasoning LLMs without the necessity for costly multi-modal re-alignment."
        },
        {
            "title": "Introduction",
            "content": "Recent developments in slow-thinking language models, such as OpenAI-o1 [26], DeepSeek-R1 [22], Gemini-Thinking [11], and Qwen3 [45], have led to significant advancements in the performance of large language models (LLMs) for complex math and science tasks. These models replicate humanlike reflective thinking, intentionally modeling reasoning as step-by-step process rather than relying on superficial shortcuts. As result, they surpass fast-thinking models like GPT-4o [25] and Claude 3.5 [2], showing improvements of over 30% on mathematical benchmarks such as AIME24 [1] and AMC23 [15], along with approximately 10% gains on science benchmarks like GPQA [41]. Motivated by these successes, there is growing interest in equipping multi-modal large language models (MLLMs) with similar slow-thinking capabilities to solve vision-based reasoning tasks that require both spatial perception and logical inference. common strategyadopted by methods like VL-Rethinker [47] and MM-EUREKA [38]is to fine-tune pre-trained MLLM (e.g., Qwen2.5-VL [5]) on multi-modal reasoning datasets using reinforcement learning with verifiable reward signals. However, this approach faces two critical limitations: (1) it is inherently restricted by the capabilities of underlying LLMs used for initialization; (2) adapting to more advanced LLMs necessitates redoing Equal contribution Correspondence to yu.zhang.ust@gmail.com Preprint. Under review. the vision-language alignment process, which involves burdensome multi-modal training that requires computing trillions of tokens. This raises challenging research question: Can we efficiently integrate slow-thinking strategies into multi-modal reasoning without retraining the entire multi-modal LLM? seemingly intuitive solution is to utilize perceptual decoupling approach: separate the vision and reasoning components by converting multi-modal inputs into natural language captions, which are then passed to an external, more powerful text-only reasoner. While this idea promises flexibility and modularity, it introduces fundamental challengethe success of the reasoning process now critically depends on the quality of intermediate captions. If the visual extractor generates captions that are incomplete, overly generic, or hallucinatory, even the strongest reasoning LLMs will fail to give correct reasoning processes. As shown in Figure 3, existing MLLMs might struggle to produce reasoning-relevant descriptions, especially for complex mathematical and scientific visual inputs [51]. To overcome this limitation, we propose ReasoningAligned Perceptual Decoupling via Caption Reward Optimization (RACRO) unified framework that not only decouples visual perception from reasoning, but also ensures their alignment through reasoningaware reinforcement learning procedure. In the proposed RACRO method, an MLLM-based extractor is responsible for encoding task-relevant visual information to natural language, while an external text-only LLM reasoner handles the slow-thinking reasoning process. Crucially, we introduce Caption Reward Optimization (CRO), reinforcement learning algorithm that fine-tunes the extractor using reasoning outcomes as feedback. Given an input image and question, the extractor generates multiple caption candidates; each is passed to the reasoner, and rewards are assigned based on whether the reasoner generates correct answers or not. This feedback loop aligns the extractor with the reasoning objective, guiding it to generate faithful and problem-relevant descriptions. Figure 1: Scalable multi-modal reasoning of RACRO. RACRO reveals better scalability compared to the traditional visual alignment, while remaining computationally efficient. By integrating perceptual decoupling with CRO into unified framework, we enable effective and scalable slow-thinking in multi-modal reasoning systems. Experiments on challenging benchmarkssuch as MathVerse [56], MathVision [48] and LogicVista [52]show that the proposed RACRO method achieves state-of-the-art results. Furthermore, the proposed RACRO method supports plug-and-play adaptation to novel reasoning LLMs without extra multi-modal retraining, making it practical and scalable solution for the rapid evolution of foundation models. It is illustrated in Figure 1, comparing the average performance of various MLLMs and their RACRO-enhanced variants across model sizes. For each RACRO-enhanced model group (e.g., Qwen2.5-VL-3B (RACRO)), the original variant (leftmost, solid) is only trained with base LLM reasoner with small amount of data (e.g., 39K) using CRO, which can then achieve scalable performance with stronger LLMs (see detailed configurations on the choices of LLMs in Appendix A). Connecting these points yields novel Pareto front that captures superior trade-off between model size and performance than other base MLLMs. Our contributions can be summarized as the following three perspectives: We propose RACRO, unified framework that decouples visual perception from reasoning and aligns them through reinforcement learning with verifiable rewards (RLVR). We introduce Caption Reward Optimization, novel reinforcement learning strategy that uses reasoning success as reward signal to optimize the extractors captioning behavior. We obtain state-of-the-art performance on multi-modal reasoning benchmarks, while maintaining scalability and eliminating the necessity for costly vision-language re-alignment."
        },
        {
            "title": "2 Related Work",
            "content": "Multi-modal Large Language Models (MLLM). With the development of LLMs, integrating visual perception capability into LLMs has received increasing attention [19, 27, 30]. To address the gap between different modalities, existing paradigms [5, 20, 21, 59] start with text-only LLM and conduct pre-training on large-scale image captioning data to obtain vision-language connector. 2 (a) Existing MLLM alignment methods (b) RACRO (ours) Figure 2: Comparisons between existing alignment methods for reasoning MLLMs and RACRO. For novel LLMs, existing methods (a) repeatedly conduct the intensive alignment process, while (b) RACRO decouples visual perception from reasoning (Sec. 3.1) by learning to extract reasoningaligned visual contexts with the proposed CRO paradigm (Sec. 3.2). Then, supervised fine-tuning (SFT) is applied with visual instruction tuning datasets. It is important to note that this procedure is computationally intensive. For example, Qwen2.5-VL [5] consumes around 4.1T tokens, and Kimi-VL [44], more recent MLLM with MoE architecture, requires 2.3T tokens for the alignment. Under this paradigm, it remains formidable to upgrade MLLMs to newer LLMs. Instead, this paper proposes computationally efficient solution for multi-modal reasoning. Empowering MLLMs with Slow-Thinking Capabilities. To empower MLLM with similar slowthinking abilities, widely adopted approach is to start with an existing MLLM (e.g., Qwen2.5-VL [5]) and perform reinforcement learning or knowledge distillation. For example, VL-Rethinker [47], MMEUREKA [38] and NoisyRollout [31] apply GRPO [42] (or its variants [13, 31, 32, 38]) to MLLMs to learn deliberate reasoning patterns. Distillation-based methods, such as the OpenVLThinker [13], R1-OneVision [54], and Vision-R1 [24], perform supervised fine-tuning (SFT) on the vision-language reasoning data generated via caption-and-reasoning pipeline [19], where visual inputs are converted to textual descriptions first, and strong text-only LLMs are then utilized to generate reasoning traces. However, their performances are limited by the base LLM (e.g., Qwen2.5 [53]), which often lags behind state-of-the-art reasoning models (e.g., DeepSeek-R1-Distilled [22]) in tasks like math. While replacing it with stronger LLM is an intuitive solution, re-aligning vision and language through full retraining on trillions of tokens is prohibitively costly. In this work, we introduce the RACRO, reasoning-aligned perceptual decoupling framework that bridges perception and reasoning via reward optimization, allowing seamless integration of stronger LLMs without retraining the vision-language interface, and meanwhile, demonstrating superior scalability w.r.t model sizes (c.f., Figure 1)."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we propose Reasoning-Aligned Caption Reward Optimization (RACRO), unified framework that enables multi-modal slow-thinking reasoning by aligning the visual perception module with the downstream reasoning objectives. RACRO builds on the intuition of perceptual decoupling, where visual inputs are first translated into natural language representations that can be processed by powerful text-only reasoners. However, rather than treating the decoupling as static and assuming fixed captions suffice, RACRO closes the perception-reasoning loop through reinforcement learning, optimizing the captioning behavior of the visual extractor for downstream reasoning success. 3.1 Reasoning-Aligned Perceptual Decoupling The perceptual decoupling paradigm is based on the assumption that the process of solving visual reasoning tasks can be separated into two distinct stages: visual perception and text-only reasoning. Specifically, given an image and related question, if all relevant visual details can be losslessly converted into natural language, the problem can be solved using text-only LLM. This decoupling allows for seamless upgrades to more powerful LLMs without the need to retrain or realign visionlanguage models. However, extracting all necessary visual details remains severe challenge, as visual information can be dense, complex, and ambiguous [19, 51]. To address that, we propose the Reasoning-Aligned Perceptual Decoupling framework, focusing on providing reasoning-aware visual 3 Figure 3: Inaccurate visual captions result in reasoning failure. MLLMs might produce hallucinated (left), vague, or incomplete captions (middle), particularly for complex visual reasoning, while captions containing necessary query-relevant contexts are essential for successful reasoning (right). details to the LLM instead of attempting to capture the entire visual input. Figure 2 gives an overview of the framework. Specifically, It consists of two steps: Extraction. Guided by the goal of reasoning-aligned perceptual decoupling, we design an extraction process that generates visual details tailored to the reasoning needs for given question. Instead of producing generic descriptions, we leverage an MLLM, such as Qwen2.5-VL [5] or InternVL3 [59], denoted as MV , to produce two complementary outputs from an image and query q: queryconditioned caption = MV (I, Pcap(q)), and tentative solution ˆy = MV (I, Pcot(q)), with each generated using distinct prompts (see Appendix B). The caption captures visual content which is selectively relevant to the question, functioning as reasoning-aligned abstraction. Although ˆy may not be fully accurate due to the limited reasoning capabilities of MLLMs, it often contains useful inductive signals that can inform and guide further downstream reasoning inference. Reasoning. The reasoning component is separately handled by text-only large language model (LLM) (e.g., R1-Distilled-7B/32B [22], Qwen3-8B [45], or QwQ-32B [46]), denoted as MR. It takes as input the question q, the reasoning-aligned caption c, and the tentative solution ˆy, and produces the final answer: = MR(Preason(q, c, ˆy)), where Preason is prompt formatting function that integrates the inputs into coherent textual reasoning context (see Appendix B). core advantage of this decoupled framework is its flexibility: by separating reasoning from visual perception, we can boost performance on multi-modal tasks simply by upgrading to more capable LLMs for MR, without modifying or retraining the MLLMs. While concurrent works have adopted similar decoupling paradigmseither as data engines for generating reasoning traces [24, 54] or as the inference framework [37]they fail to investigate how to generate 1) query-conditioned and 2) reasoning-aligned visual representations, which is the focus of this paper in the following. 3.2 Caption Reward Optimization While this design enables flexible upgrade of the reasoner without vision-language re-alignment, it exposes the system to critical bottleneck: the quality of the caption directly affects the reasoning outcome. In practice, existing MLLMs may generate hallucinated [51], vague, or incomplete captions, particularly for complex visual reasoning tasks such as the geometric math questions (see Figure 3). To address this bottleneck, RACRO introduces Caption Reward Optimizationa reasoning-guided reinforcement learning algorithm that fine-tunes the captioning extractor using the feedback from text-only reasoner, aiming at reinforcing captions that maximize likelihood of successful reasoning. Caption Reward Optimization is inspired by GRPO (Group Relative Policy Optimization) [42], which is policy optimization algorithm originally developed to enhance the reasoning capability of text-only LLMs. In our setting, the policy πθ to optimize becomes the MLLM MV performing visual captioning. For given input pair (I, q) of image and text question from the training set pD, the old policy generates caption rollouts, i.e., πθold (I, Pcap(q)). Denoting Ri as the reward for the i-th rollout, the normalized advantage is ˆAi = Ri σ(R) , where σ(R) denotes the standard deviation of 4 Figure 4: Caption Reward Optimization reinforces captions that induce correct reasoning results via reinforcement learning with verifiable rewards (RLVR). rewards within the group and the baseline reward is = 1 i=1 Ri. The objective incorporates surrogate loss clipped within [1 ϵ, 1 + ϵ](ϵ > 0) and KL-penalty DKL[πθπθref ] weighted by β (not shown here) to stabilize optimization: L(θ) = E(I,q)pD,cπθold (I,Pcap(q)) (cid:80)G (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:32) min πθ(ci I, Pcap(q)) πθold (ci I, Pcap(q)) ˆAi, clip (cid:16) πθ(ci I, Pcap(q)) πθold (ci I, Pcap(q)) , 1 ϵ, 1 + ϵ (cid:33)(cid:35) . (cid:17) ˆAi (1) However, different from the math reasoning problem traditionally studied by GRPO, there are no extractable answers in the generated captions. Therefore, the widely adopted rule-based reward is no longer applicable to our problem. To tackle this, we propose to combine the rule-based reward function r(, ) and reasoning LLM MR for reward computation. Specifically, the reward Ri of caption rollout ci is determined by whether reasoner LLM MR could arrive at the ground-truth answer using this caption ci as contexts, which can be expressed as: Ri = r(ygt, yi) = 1(ygt = parse(yi)), where yi = MR(P (2) where ygt denotes the ground-truth answer of reasoning question and 1() is an indicator function that outputs 1 if the final parsed prediction matches the ground-truth and 0 otherwise. Note that the reasoning process during reward computation is different from that in Sec. 3.1 as the tentative response is not provided here (check the ablations on this configuration in Table 3). In summary, the proposed Caption Reward Optimization offers the following benefits: reason(q, ci)), More accurate and task-relevant captions: As defined in Eq. (2), the reward encourages captions that facilitates successful downstream reasoning. Consequently, the MLLM learns to generate captions with less visual hallucination and more task-relevant visual details. Alignment scalability to any reasoners: Once the MLLM undergoes the Caption Reward Optimization process, even LLMs that are not involved during training can be incorporated into the reasoning framework because the captions are natural languages that generalize between different LLMs. This eliminates repeated alignments as done in the traditional MLLM instruction tuning approaches. As demonstrated in Figure. 1, our RACRO reveals superior scalability during inference time, even though not been specifically trained for."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Main Results Evaluation. Following VisualPRM [49], we evaluate our model on diverse set of multi-modal reasoning benchmarks, including the MathVista (testmini) [36], MathVision (test) [48], MathVerse (vision-only) [56], MMMU (val) [55], WeMath [39], DynaMath [60], and the LogicVista [52]. We report the worst-case accuracy for DynaMath and the strict accuracy for WeMath, while for the other benchmarks, we report the overall accuracy. All evaluations are conducted using VLMEvalKit [14]. Baselines. We compare our method with the following baselines: Proprietary models: GPT-4o [25], Claude-3.7-Sonnet [3], and Gemini-2.0-Flash [12] are generalpurpose multi-modal models without dedicated reasoning optimization. In contrast, OpenAI o1 [26] is explicitly designed to enhance reasoning capabilities. These models represent the current proprietary state-of-the-art. 5 Table 1: Comparison on multi-modal reasoning benchmarks. Model Math Vista Math Vision Math Verse MMMU We Math Dyna Math Logic Vista AVG Claude-3.7-Sonnet Gemini-2.0-Flash GPT-4o-20241120 o1 InternVL3-2B-MPO OpenVLThinker-7B R1-Onevision-7B Adora-7B MM-Eureka-7B InternVL3-8B-MPO InternVL3-14B-MPO MM-Eureka-32B InternVL3-38B-MPO LLaVA-OV-72B EMOVA-72B VL-Rethinker-72B Qwen2.5-VL-72B QVQ-72B-Preview InternVL3-78B-MPO 66.8 70.4 60.0 73. 57.0 70.2 64.1 73.5 73.0 71.6 75.1 74.8 75.1 67.5 69.9 80.3 74.2 70.3 79.0 Proprietary Models 46.7 47.7 40.6 - 75.0 72.6 70.7 78.2 Open-Source Models 25.3 - 40.0 - - 39.8 44.4 - 48.2 39.1 50.0 - 47.3 48.2 51. 48.6 - - - - 62.7 67.1 - 70.1 56.8 59.7 68.8 68.2 70.3 72.2 49.3 47.4 45.8 - 22.4 - - - - 37.1 43.0 - 48.6 - - - 49.1 39.0 46.0 41.9 43.6 31.2 42.2 21.7 25.3 29.9 23.0 26.9 29.3 37.2 34.4 34.2 30.1 - 43.9 39.3 34.9 43.1 39.7 42.1 34.5 - 14.6 - - - - 25.5 31.3 - 35.3 - - - 35.9 30.7 35.1 58.2 52.3 52.8 - 36.9 - - - - 44.1 51.2 - 58.4 - - - 55.7 58.2 55.9 53.9 53.7 47.9 - 32.4 - - - - 44.3 49.9 - 52.8 - - - 52.8 50.2 54.6 Qwen2.5-VL-3B w/ VisualPRM-8B-v1.1 64.5 (+0.0) 36.3 (+14.4) 39.3 (+10.5) 51.7 (+1.6) 31.5 (+7.3) 21.4 (+8.0) 46.5 (+6.9) 41.6 (+7.0) w/ RACRO (R1-7B) w/ RACRO (Qwen3-8B) 68.6 (+4.1) 39.6 (+17.7) 43.7 (+14.9) 58.6 (+8.5) 32.3 (+8.1) 29.3 (+15.9) 51.9 (+12.3) 46.3 (+11.7) 70.2 (+5.7) 41.5 (+19.6) 43.8 (+15.0) 58.4 (+8.3) 32.5 (+8.3) 30.3 (+16.9) 55.7 (+16.1) 47.5 (+12.9) w/ RACRO (R1-32B) 13.4 10.6 21.9 35.1 50.1 56.0 64.5 69.5 24.2 39.2 34.6 41. 28.8 36.3 39.6 43.4 Qwen2.5-VL-7B w/ VisualPRM-8B-v1.1 71.9 (+1.6) 38.9 (+13.1) 45.3 (+4.3) 58.7 (+1.4) 37.1 (+2.7) 27.0 (+7.6) 49.7 (+3.6) 46.9 (+4.9) w/ RACRO (R1-7B) w/ RACRO (Qwen3-8B) 70.0 (-0.3) 41.5 (+15.7) 44.8 (+3.8) 60.6 (+3.3) 38.6 (+4.2) 26.5 (+7.1) 50.1 (+4.0) 47.4 (+5.4) 74.4 (+4.1) 42.5 (+16.7) 46.6 (+5.6) 61.1 (+3.8) 31.0 (-3.4) 30.3 (+10.9) 55.0 (+8.9) 48.7 (+6.7) w/ RACRO (R1-32B) 19.4 17.8 25.8 27.1 57.3 56. 70.3 74.6 42.0 45.1 41.0 45.0 34.4 46.3 46.1 48.5 Qwen2.5-VL-32B w/ VisualPRM-8B-v1.1 75.4 (-1.4) 44.5 (+6.7) 52.9 (+2.8) 68.8 (-0.2) 48.3 (+5.2) 35.1 (+1.8) 54.6 (-0.4) 54.2 (+2.0) w/ RACRO (R1-7B) w/ RACRO (Qwen3-8B) 75.5 (-1.3) 47.1 (+9.3) 54.9 (+4.8) 67.9 (-1.1) 50.0 (+6.9) 35.7 (+2.4) 58.2 (+3.2) 55.6 (+3.4) 75.9 (-0.9) 46.7 (+8.9) 55.6 (+5.5) 69.0 (+0.0) 48.9 (+5.8) 37.1 (+3.8) 58.6 (+3.6) 56.0 (+3.8) w/ RACRO (R1-32B) 33.3 33.1 55.0 53.9 37.8 39.9 69.0 69.6 76.8 77.7 52.2 53. 50.1 52.2 43.1 50.4 Open-source general-purpose models: LLaVA-OneVision-72B (LLaVA-OV-72B) [27], EMOVA72B [6], and the Qwen2.5-VL series [5] are primarily pretrained or instruction-tuned on large-scale image-text datasets, without explicit reasoning alignment. Open-source reasoning models: The InternVL3-MPO variants [59], OpenVLThinker-7B [13], R1-Onevision-7B [54], Adora-7B, MM-Eureka (7B/32B) [38], VL-Rethinker (7B/72B) [47], and QVQ-72B-Preview [40]. These models are explicitly fine-tuned for reasoning tasks using methods such as SFT, DPO, and rule-based reinforcement learning. Multi-modal test-time scaling methods: We also compare our proposed method with multimodal test-time scaling methods such as Best-of-N search (BoN) [49], which has been shown to effectively improve the reasoning accuracy of MLLMs. In particular, we consider the latest version, VisualPRM-8B-v1.1 [49]. Note that in Table 1, BoN is adopted independently with RACRO. Implementation. To validate the effectiveness of the proposed method, we perform RACRO on the following MLLMs: Qwen2.5-VL series (3B, 7B, and 32B). During training, we use R1-Distilled-7B (referred to interchangeably as R1-7B) as the reasoner to compute reward signals for all MLLMs. During evaluation, in addition to R1-Distilled-7B, we also employ its 32B counterpart and Qwen3-8B to assess the scalability of our method. For training data, we adopt ViRL39K [47], curated dataset of 38,870 verifiable multi-modal question-answer pairs tailored for reinforcement learning. We implement the caption reward optimization algorithm using the verl [43] framework with global batch size of 256, rollout temperature of 1.0, and learning rate of 1e6. For each query, four captions are sampled. The clipping constant ϵ and KL-penalty coefficient β are set to 0.2 and 1e3. 6 Table 2: Ablation study of different components of our RACRO. Sol. Cap. LLM CRO Math Vista Math Vision Math Verse MMMU We Math Dyna Math Logic Vista AVG 31.5 36.0 10.0 39. 11.4 3.8 22.8 22.1 Qwen2.5-VL-3B 21.9 36.3 36.2 34.5 36. 28.8 30.2 12.7 30.3 39.3 Qwen2.5-VL-7B 25.8 38.0 35.8 37.6 38.9 41.0 41.0 27.9 42.3 45.3 50.1 47.6 47.8 51.7 51.7 57.3 53.9 50.6 56.4 58. 64.5 60.1 59.5 63.8 64.5 70.3 68.3 54.4 70.9 71.9 24.2 24.4 25.0 27.4 31.5 34.4 31.2 23.7 37.3 37. 13.4 14.6 13.4 17.4 21.4 19.4 22.4 7.6 24.6 27.0 39.6 44.5 35.1 43.9 46.5 46.1 47.4 34.9 48.3 49.7 34.6 36.8 32.8 38.4 41.6 42.0 43.2 33.6 45.3 46. Results. Table 1 presents the performance of the proposed method. As shown, the average scores across seven datasets are significantly improved compared to the original MLLMs. For example, applying RACRO to Qwen2.5-VL-7B with same-sized reasoner (R1-Distilled-7B) yields +4.9% gain. Further improvements are observed when using larger reasoner, such as +6.7% increase with R1-Distilled-32B. Notably, when applying RACRO to Qwen2.5-VL-32B and evaluating with R1-Distilled-32B, we achieve state-of-the-art results on MathVision, MathVerse, and LogicVista, outperforming even larger models such as InternVL3-78B-MPO. Importantly, the Qwen3-8B and R1-32B reasoners are only used during inference time via plug-and-play manner, since all trainings are conducted solely with the R1-7B reasoner. This demonstrates the scalability and generalization ability of our RACRO across both model sizes and (heterogeneous) reasoning LLM variants. Compared to Best-of-8 (Bo8) search, RACRO also achieves superior results. Notably, for stronger MLLMs such as Qwen2.5-VL-32B, VisualPRM-8B-v1.1 provides only limited improvement (e.g., +1.6%). RACRO, on the other hand, demonstrates greater scalability to stronger reasoners: when paired with same-sized reasoner as VisualPRM-8B-v1.1, it improves the performance of Qwen2.5VL-32B by +3.4%. This improvement is achieved without any re-training, highlighting RACROs plug-and-play compatibility with more capable reasoners. 4.2 Ablation Study In this section, we first investigate the effectiveness of the proposed components, i.e., ReasoningAligned Perceptual Decoupling and Caption Reward Optimization. Then, we ablate the choices on reward computation during training. We use the same training configurations as in Sec. 4.1. Qwen2.5-VL-3B/7B are adopted for ablations due to resource constraints. Unless otherwise specified, we use R1-Distilled-7B as the default LLM reasoner throughout the training and inference stages. Reasoning-Aligned Perceptual Decoupling & Caption Reward Optimization. For simplicity, we use the following shorthand markers to indicate the components: LLM indicates the usage of the text-only LLM reasoner MR. Cap. denotes the use of the query-conditioned captions generated by the MLLM MV . Sol. denotes the (tentative) solution provided by the MLLM MV . Note that if the reasoner MR (i.e., LLM above) is not used, it is considered as the final output solution. CRO denotes the use of Caption Reward Optimization that optimizes the quality of the generated captions. We assess the RACRO framework by progressively enabling its components, as in Table 2. Starting from the baseline MLLM, we first introduce the tentative solution as the input to the LLM reasoner. This alone leads to notable performance improvementse.g., from 34.6% to 36.8% for Qwen2.5VL-3B and from 42.0% to 43.2% for the 7B modeldemonstrating the reasoners ability to refine intermediate outputs from the MLLM. However, on tasks such as MathVista, MMMU, and WeMath, performance declines after introducing the LLM with tentative solutions only. To compensate for 7 the lack of visual details, we incorporate query-conditioned captions alongside the tentative solution, which effectively restores the performance. Surprisingly, when captions are adopted alone without tentative solutions, benchmark performance drops significantly, suggesting that while captions can provide valuable visual context, they are often noisysuffering from hallucinations and missing critical details. In contrast, the tentative solution acts as reliable fallback for the reasoner when the visual signal is weak or ambiguous. To improve the quality of the query-conditioned captions, we apply the proposed Caption Reward Optimization strategy. As result, we observe consistent performance gains across all tasks, confirming the quality issue of the captions and the effectiveness of the Caption Reward Optimization strategy. Choices on Reward Computation. Conducting CRO requires computing the reward of caption rollout, which involves running the reasoner LLM. Here we discuss (i) what type of reasoner and (ii) what kind of inputs to the reasoner lead to more accurate reward computation. Specifically, for reasoners, we consider R1-Distilled-1.5B/7B and Qwen-2.5-7B-Instruct. For the reasoner input, we consider Cap., which only includes the generated caption and Cap. & Sol. that further includes the MLLMs tentative solution (pre-generated before training). Note that the later is exactly the configuration adopted in the proposed Perceptual Decouple framework. Table 3: Ablation studies on (a) reasoner types and (b) inputs for reward computation. Q-VL-3B Q-VL-7B Configuration w/o CRO (baseline) 38.4 45.3 (a) Reasoner Types Qwen-2.5-7B-Instruct R1-Distilled-1.5B R1-Distilled-7B (b) Reasoner Inputs Cap. & Sol. Cap. 40.4 40.9 41.6 40.8 41.6 45.1 45.2 46.9 45.8 46.9 Table 3 shows the average performance with Qwen2.5-VL series as MLLMs (i.e., Q-VL-3B/7B) under different reasoner types and inputs. We observe that using smaller reasoners (i.e., R1-Distilled-1.5B) or models less specialized in reasoning (i.e., Qwen-2.5-Instruct) leads to worse overall performance. Intuitively, models performing poorly in reasoning are prune to generate incorrect solutions even though captions are accurate and comprehensive, providing noisy reward signals for CRO. However, larger reasoners significantly slow down the reward computing. Thus, we adopt R1-Distilled-7B as the reasoner as it strikes balance between computational efficiency and reward accuracy by default. Regarding the input to the reasoner, we found passing only captions yields the best performance. Although Cap. & Sol. perfectly aligns with the Perceptual Decoupling framework during inference time, including tentative response, allows the reasoner to take shortcut that ignore the captions during reasoning, thereby producing noisy reward signal for the captions. 4.3 Analysis Scalability of CRO. In practical scenario, after conducting CRO on an MLLM with certain LLM (e.g., R1-Distilled-7B), it is desirable to switch to new LLMs (e.g., QwQ-32B) without additional training. We refer to this as the scalability of CRO and reveal that CRO indeed possesses this property. Figure 5 shows the overall task performance under the Perceptual Decoupling framework using different reasoners across varying model sizes and generations. The MLLMs adopted are from the Qwen2.5-VL series, both with and without CRO. As shown, although all MLLMs were optimized with CRO using R1-Distilled-7B, switching to different LLMswhether in size (R1-Distilled1.5B/32B) or generation (QwQ-32B)still results in notable performance boost over their non-CRO counterparts. This confirms the scalability of CRO. Furthermore, we observe that this performance gain becomes more pronounced with larger or stronger LLMs. This is because, without CRO, these models are significantly bottlenecked by inaccurate or low-quality visual captions, preventing them from leveraging their full reasoning potential. In contrast, smaller or weaker LLMs are limited not only by caption quality but also by their own reasoning capacity, making the relative improvement from CRO less substantial. This highlights the effectiveness of CRO in improving the quality of image captions. Further, with stronger LLMs, the performance gap between 3B and 7B gradually diminishes. This reveals that reasoning is better decoupled from perception. CRO and GRPO are Complementary. The proposed CRO objective effectively enhances the quality of query-conditioned captions, thereby improving overall performance. Another direction for improving the reasoning capability of the LLM is to produce more accurate tentative responses, which can be achieved through reinforcement learning methods such as GRPO. To evaluate their contributions, we apply CRO and GRPO separately to the original MLLMs (e.g., Qwen2.5-VL) Figure 5: Ablation on the choice of reasoner. Figure 6: Ablation on CRO and GRPO. and compare their average task performance under the perceptual decoupling framework (details on GRPO are in Appendix C.1). To investigate whether the two objectives are complementary, we also assess combined settingdenoted as CRO+GRPOwhere tentative responses and captions are generated by models fine-tuned separately with GRPO and CRO. For comparison, results from models without additional training are labeled as No Training. Additionally, we further include VLRethinker-7B [47], which is initialized from Qwen2.5-VL-7B and thoroughly fine-tuned using GRPO. We further fine-tune this model with CRO and report the resulting performance as GRPO+CRO. All results are visualized in Figure 6. As shown, both CRO and GRPO improve the performance under the perceptual decoupling framework. More importantly, for all MLLMs, combining both objectives brings notable improvement over the individuals. This suggests that the proposed CRO is complementary to GRPO. Note that we did not report results with GRPO training in Table 1 to highlight the impact of perceptual decoupling. Evaluating the Quality of Captions. To see whether the quality of the generated captions are improved, we conduct pairwise comparisons [9, 34] among captions generated by Qwen2.5VL-3B trained with and without CRO. We randomly sample 1000 captions for each evaluation dataset and use GPT-4o 3 as the judge to determine the preferred caption. The judge receives the image, the question alongside two captions, and decides the winner or tie game. We instruct the judge to choose captions that include more comprehensive and accurate details required to answer the question and exclude any solving process in the captions (the detailed prompt is in Appendix C.2). Furthermore, we alternate the order of the two captions to avoid position bias following [57]. Results are reported in Figure 7. It can be observed that across all datasets, models trained with CRO offer clear advantage over the original ones. This confirms the quality issues of captions discovered by this paper and highlights the effectiveness of CRO. To ensure robustness of our automated evaluation, we further conduct human assessment through the same pairwise comparison framework with details in Appendix C.2. The results reinforces the quality improvement of the captions. Figure 7: Pairwise comparison on caption quality."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose the RACRO framework to construct multi-modal reasoning models efficiently. By decoupling multi-modal reasoning with the visual perception and text-only reasoning, RACRO embraces the extraordinary reasoning capabilities of frontier LLMs, while getting rid of the burdensome visual re-alignment. Enhanced with Caption Reward Optimization, precise captions are reinforced to provide abundant visual contexts for better downstream reasoning, effectively scaling towards more advanced LLM reasoners during inference time. We obtain state-of-the-art performance on multiple multi-modal reasoning benchmarks, while remaining computationally efficient. 3https://genai.ust.hk/"
        },
        {
            "title": "References",
            "content": "[1] AIME. Aime problems and solutions, 2024. https://artofproblemsolving.com/wiki/index.php/AIME_Problem s_and_Solutions. 2024. [2] Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet. 2024. [3] Anthropic. Claude 3. sonnet system card. https://www.anthropic.com/ claude-3-7-sonnet-system-card, 2025. Accessed: 2025-05-16. [4] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. [5] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] K. Chen, Y. Gou, R. Huang, Z. Liu, D. Tan, J. Xu, C. Wang, Y. Zhu, Y. Zeng, K. Yang, et al. Emova: Empowering language models to see, hear and speak with vivid emotions. arXiv preprint arXiv:2409.18042, 2024. [7] K. Chen, L. Hong, H. Xu, Z. Li, and D.-Y. Yeung. Multisiam: Self-supervised multi-instance siamese representation learning for autonomous driving. In ICCV, 2021. [8] K. Chen, Z. Liu, L. Hong, H. Xu, Z. Li, and D.-Y. Yeung. Mixed autoencoder for self-supervised visual representation learning. In CVPR, 2023. [9] K. Chen, C. Wang, K. Yang, J. Han, L. Hong, F. Mi, H. Xu, Z. Liu, W. Huang, Z. Li, D.-Y. Yeung, L. Shang, X. Jiang, and Q. Liu. Gaining wisdom from setbacks: Aligning large language models via mistake analysis. arXiv preprint arXiv:2310.10477, 2023. [10] K. Chen, E. Xie, Z. Chen, L. Hong, Z. Li, and D.-Y. Yeung. Integrating geometric control into textto-image diffusion models for high-quality detection data generation via text prompt. arXiv preprint arXiv:2306.04607, 2023. [11] G. DeepMind. Gemini 2.5: Our most intelligent ai model. https://blog.google/technology/googledeepmind/gemini-model-thinking-updates-march-2025/. 2025. [12] G. DeepMind. Gemini 2.0 flash. https://cloud.google.com/vertex-ai/generative-ai/docs/ models/gemini/2-0-flash, 2025. [13] Y. Deng, H. Bansal, F. Yin, N. Peng, W. Wang, and K.-W. Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. [14] H. Duan, J. Yang, Y. Qiao, X. Fang, L. Chen, Y. Liu, X. Dong, Y. Zang, P. Zhang, J. Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In ACM MM, 2024. [15] K. Engineering. Amc-23 dataset. https://huggingface.co/datasets/knoveleng/AMC-23. 2025. [16] R. Gao, K. Chen, Z. Li, L. Hong, Z. Li, and Q. Xu. Magicdrive3d: Controllable 3d generation for any-view rendering in street scenes. arXiv preprint arXiv:2405.14475, 2024. [17] R. Gao, K. Chen, B. Xiao, L. Hong, Z. Li, and Q. Xu. Magicdrivedit: High-resolution long video generation for autonomous driving with adaptive control. arXiv preprint arXiv:2411.13807, 2024. [18] R. Gao, K. Chen, E. Xie, L. Hong, Z. Li, D.-Y. Yeung, and Q. Xu. Magicdrive: Street view generation with diverse 3d geometry control. arXiv preprint arXiv:2310.02601, 2023. [19] Y. Gou, K. Chen, Z. Liu, L. Hong, H. Xu, Z. Li, D.-Y. Yeung, J. T. Kwok, and Y. Zhang. Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation. arXiv preprint arXiv:2403.09572, 2024. [20] Y. Gou, Z. Liu, K. Chen, L. Hong, H. Xu, A. Li, D.-Y. Yeung, J. T. Kwok, and Y. Zhang. Mixture of cluster-conditional lora experts for vision-language instruction tuning. arXiv preprint arXiv:2312.12379, 2023. [21] Y. Gou, H. Yang, Z. Liu, K. Chen, Y. Zeng, L. Hong, Z. Li, Q. Liu, J. T. Kwok, and Y. Zhang. Corrupted but not broken: Rethinking the impact of corrupted data in visual instruction tuning. arXiv preprint arXiv:2502.12635, 2025. 10 [22] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [23] J. Han, X. Liang, H. Xu, K. Chen, L. Hong, C. Ye, W. Zhang, Z. Li, X. Liang, and C. Xu. Soda10m: Towards large-scale object detection benchmark for autonomous driving. arXiv preprint arXiv:2106.11118, 2021. [24] W. Huang, B. Jia, Z. Zhai, S. Cao, Z. Ye, F. Zhao, Z. Xu, Y. Hu, and S. Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [25] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [26] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [27] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li. LLaVA-OneVision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [28] K. Li, K. Chen, H. Wang, L. Hong, C. Ye, J. Han, Y. Chen, W. Zhang, C. Xu, D.-Y. Yeung, et al. Coda: real-world road corner case dataset for object detection in autonomous driving. arXiv preprint arXiv:2203.07724, 2022. [29] P. Li, Z. Liu, K. Chen, L. Hong, Y. Zhuge, D.-Y. Yeung, H. Lu, and X. Jia. Trackdiffusion: Multi-object tracking data generation via diffusion models. arXiv preprint arXiv:2312.00651, 2023. [30] Y. Li, W. Zhang, K. Chen, Y. Liu, P. Li, R. Gao, L. Hong, M. Tian, X. Zhao, Z. Li, et al. Automated evaluation of large vision-language models on self-driving corner cases. arXiv preprint arXiv:2404.10595, 2024. [31] X. Liu, J. Ni, Z. Wu, C. Du, L. Dou, H. Wang, T. Pang, and M. Q. Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. [32] Z. Liu, C. Chen, W. Li, P. Qi, T. Pang, C. Du, W. S. Lee, and M. Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [33] Z. Liu, K. Chen, Y. Zhang, J. Han, L. Hong, H. Xu, Z. Li, D.-Y. Yeung, and J. Kwok. Geom-erasing: Geometry-driven removal of implicit concept in diffusion models. arXiv preprint arXiv:2310.05873, 2023. [34] Z. Liu, Y. Gou, K. Chen, L. Hong, J. Gao, F. Mi, Y. Zhang, Z. Li, X. Jiang, Q. Liu, et al. Mixture of insightful experts (mote): The synergy of thought chains and expert mixtures in self-alignment. arXiv preprint arXiv:2405.00557, 2024. [35] Z. Liu, J. Han, K. Chen, L. Hong, H. Xu, C. Xu, and Z. Li. Task-customized self-supervised pre-training with scalable dynamic routing. In AAAI, 2022. [36] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. [37] Y. Lu, J. Yuan, Z. Li, S. Zhao, Q. Qin, X. Li, L. Zhuo, L. Wen, D. Liu, Y. Cao, et al. Omnicaptioner: One captioner to rule them all. arXiv preprint arXiv:2504.07089, 2025. [38] F. Meng, L. Du, Z. Liu, Z. Zhou, Q. Lu, D. Fu, T. Han, B. Shi, W. Wang, J. He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [39] R. Qiao, Q. Tan, G. Dong, M. Wu, C. Sun, X. Song, Z. GongQue, S. Lei, Z. Wei, M. Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [40] Qwen. Qvq: To see the world with wisdom. https://qwenlm.github.io/blog/qvq-72b-preview/, 2024. [41] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. In COLM, 2024. [42] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 11 [43] G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: flexible and efficient rlhf framework. In EuroSys, 2025. [44] K. Team, A. Du, B. Yin, B. Xing, B. Qu, B. Wang, C. Chen, C. Zhang, C. Du, C. Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. [45] Q. Team. Qwen3 technical report. https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Re port.pdf. 2025. [46] Q. Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [47] H. Wang, C. Qu, Z. Huang, W. Chu, F. Lin, and W. Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [48] K. Wang, J. Pan, W. Shi, Z. Lu, H. Ren, A. Zhou, M. Zhan, and H. Li. Measuring multimodal mathematical reasoning with math-vision dataset. In NeurIPS, 2024. [49] W. Wang, Z. Gao, L. Chen, Z. Chen, J. Zhu, X. Zhao, Y. Liu, Y. Cao, S. Ye, X. Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. [50] Y. Wang, R. Gao, K. Chen, K. Zhou, Y. Cai, L. Hong, Z. Li, L. Jiang, D.-Y. Yeung, Q. Xu, and K. Zhang. Detdiffusion: Synergizing generative and perceptive models for enhanced data generation and perception. arXiv preprint arXiv:2403.13304, 2024. [51] J. Wu, T. T. Chung, K. Chen, and D.-Y. Yeung. Unified triplet-level hallucination evaluation for large vision-language models. arXiv preprint arXiv:2410.23114, 2024. [52] Y. Xiao, E. Sun, T. Liu, and W. Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. [53] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [54] Y. Yang, X. He, H. Pan, X. Jiang, Y. Deng, X. Yang, H. Lu, D. Yin, F. Rao, M. Zhu, et al. R1onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [55] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. [56] R. Zhang, D. Jiang, Y. Zhang, H. Lin, Z. Guo, P. Qiu, A. Zhou, P. Lu, K.-W. Chang, Y. Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In ECCV, 2024. [57] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023. [58] L. Zhili, K. Chen, J. Han, H. Lanqing, H. Xu, Z. Li, and J. Kwok. Task-customized masked autoencoder via mixture of cluster-conditional experts. In ICLR, 2023. [59] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, Y. Duan, H. Tian, W. Su, J. Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [60] C. Zou, X. Guo, R. Yang, J. Zhang, B. Hu, and H. Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024. 12 Table 4: Model Configurations for each RACRO-enhanced model group in Figure 1. - denotes the corresponding item (e.g., LLM reasoner, GRPO/CRO training) is not applied. Size (B) LLM GRPO CRO Avg. Performance Qwen2.5-VL-3B 3 7 11 35 7 11 15 32 39 40 64 - - Qwen3-4B ViRL39K ViRL39K Qwen3-8B ViRL39K ViRL39K ViRL39K ViRL39K R1-32B - Qwen2.5-VL-7B - - Qwen3-4B ViRL39K ViRL39K Qwen3-8B ViRL39K ViRL39K ViRL39K ViRL39K R1-32B - Qwen2.5-VL-32B - R1-7B Qwen3-8B R1-32B - - - - - ViRL39K ViRL39K ViRL39K 34.6 45.4 46.3 47.8 44.5 48.9 50.2 50.8 52.2 54.2 55.6 56.0 Table 5: Index of Prompt templates for RACRO. Component Purpose Notation Prompt Template MLLM MLLM LLM Reasoner LLM Reasoner Reward computation Pcot Tentative response Query-conditioned captions Pcap Inference Preason reason Figure 10 Figure 11 Figure 8 Figure"
        },
        {
            "title": "Appendix",
            "content": "A Model Configuration for Figure 1 For each RACRO-enhanced model group (e.g., Qwen2.5-VL-3B (RACRO)) in Figure 1, we train the original model (e.g., Qwen2.5-VL-3B, they are shown as hollow squares with different colors in the figure) using both the proposed CRO and the existing GRPO objectives, where the former encourages the MLLM to generate query-conditioned captions with higher quality while the later optimizes it to give better reasoning traces. The details for CRO and GRPO can be found in Sec. 4.1 and Appendix C.1. We then pair the trained MLLM with different LLM reasoners under our RACRO framework. Table 4 shows the configuration for each RACRO-enhanced model group. Specifically, for each model in the group, we report the total model size (B), paired LLMs, data used to conduct GRPO and CRO and the average performance across 7 tasks. Note that the results for Qwen2.5-VL-32B group does not involve GRPO training because we obtain worse results than the original model after GRPO. Results for other MLLMs without RACRO are directly taken from Table 1."
        },
        {
            "title": "B Prompt Templates Used in RACRO",
            "content": "In Table 5, we provide an index of the prompt templates used in RACRO."
        },
        {
            "title": "C Experimental Details",
            "content": "C.1 Details for GRPO Training GRPO (Group Relative Policy Optimization) [42] is policy optimization algorithm originally developed to enhance the reasoning capability of text-only LLMs. In our setting, the policy πθ to 13 Figure 8: Prompt templates used by the reasoner LLM for inference. Figure 9: Prompt templates used by the reasoner LLM for training. Figure 10: Prompt templates used by the MLLM to obtain the tentative response. The placeholder is for the question. Figure 11: Prompt templates used by the MLLM to obtain the query-conditioned captions. 14 optimize becomes the MLLM. For given input pair (I, q) of image and text question from the training set pD, the old policy generates rollouts, i.e., πθold (I, Pcot(q)). Denoting Ri as the reward for the i-th rollout, the normalized advantage is ˆAi = Ri σ(R) , where σ(R) denotes the standard deviation of rewards within the group and the baseline reward is = 1 i=1 Ri. The objective incorporates surrogate loss clipped within [1 ϵ, 1 + ϵ](ϵ > 0) and KL-penalty DKL[πθπθref ] weighted by β (not shown here) to stabilize optimization: L(θ) = E(I,q)pD,oπθold (I,Pcot(q)) (cid:80)G (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:32) min πθ(oi I, Pcot(q)) πθold (oi I, Pcot(q)) ˆAi, clip (cid:16) πθ(oi I, Pcot(q)) πθold (oi I, Pcot(q)) , 1 ϵ, 1 + ϵ (cid:33)(cid:35) . (cid:17) ˆAi (3) The reward Ri for the i-th rollout is expressed as: Ri = r(ygt, oi) = 1(ygt = parse(oi)), where ygt denotes the ground-truth answer of reasoning question and 1() is an indicator function that outputs 1 if the final parsed prediction matches the ground-truth and 0 otherwise. Implementation Details. Similar to CRO, for training data, we adopt ViRL39K [47] and implement GRPO using the verl [43] framework with global batch size of 256, rollout temperature of 1.0, and learning rate of 1e6. For each query, eight rollouts are sampled. The clipping constant ϵ and KL-penalty coefficient β are set to 0.2 and 1e3, respectively. C.2 Details for Pairwise Comparisons Prompt for GPT evaluations. We provide the prompt for GPT evaluations on the quality of the caption in Figure 12. Consistent with the details in Sec. 4.3, this instructs the GPT to (1) choose captions that include more comprehensive and accurate details required to answer the question and (2) exclude any solving process in the captions. Human evaluations. We present the details of the human evaluation conducted for the pairwise comparison experiment. For this, 100 questions are randomly sampled from the testmini set of MathVista, and captions are generated using Qwen2.5-VL-3B, trained with and without CRO. total of 4 trained human annotators are recruited, with each annotator responsible for evaluating 25 questions to determine winner or tie. The annotators primary task is to evaluate question and its two anonymous model-generated answers, then select the one they deem of higher quality, following the scoring principle adopted from [4]. If neither answer stood out, they are instructed to choose \"tie.\" In Table 6, we report the win/tie/lose ratio (i.e., win means captions generated by MLLMs with CRO is better) and an additional measure of consistency, calculated by the agreement rate between GPT-4o and human judgments. As can be seen, Qwen2.5-VL-3B trained with CRO demonstrates superior caption quality under human evaluation. This aligns with the result in Figure 7, which is further supported by the high consistency of 85%. This supports the rationale of using GPT-4o as judge for evaluating caption quality. Table 6: Human evaluation on pairwise comparison of the caption quality. Win Tie Lose Consistency 60% 29% 11% 85% C.3 Training Dynamics of CRO We show the average reward scores and caption lengths over training steps in Figure 13. We observe the following: Rewards increased as training progresses. This confirms the effectiveness of CRO as it allows the MLLM to generate captions that lead to higher reasoning accuracy. 15 Figure 12: Prompt templates used for GPT evaluations on caption qualities. Caption lengths grow as training progresses. An explanation for this phenomenon is that the MLLM learns to generate more comprehensive captions during training, which is reflected by longer lengths. This is also confirmed in the Appendix D, where we visualize the captions."
        },
        {
            "title": "D Case Study",
            "content": "Caption qualities. We conduct case study on the generated query-conditioned captions. Specifically, for multi-modal reasoning question and image, we investigate the quality of the generated captions. For MLLMs, we consider Qwen2.5-VL series (3B/32B) both with and without CRO. We visualize the question, image and captions in Tables 713 for Qwen2.5-VL-3B and Tables 1418 for Qwen2.5-VL-32B. Comparing the captions generated by MLLMs with and without CRO, we discover the following: CRO leads to more visual details. We highlight these visual details in red in the table. Notably, these details are important clues required to correctly solve the question. This shows that CRO is effective in improving the quality (especially comprehensiveness) of the query-conditioned captions. CRO leads to captions with more organized and hierarchical structures. For example, in Table 12, the MLLM with CRO describes the images at three levels, i.e., Tropic level, Terrestrial food chain and aquatic food chain. This allows the reasoner to quickly locate important information in the captions. However, the original MLLM uses sequence of sentences that are less clear. Large-sized MLLMs generate more comprehensive captions. We found the captions generated by Qwen2.5-VL-32B are significantly longer than those generated by the 3B (a) Qwen2.5-VL-3B (b) Qwen2.5-VL-7B Figure 13: Training dynamics of the CRO process: Reward and Length Over Steps for Qwen2.5VL-3B, Qwen2.5-VL-7B and Qwen2.5-VL-32B (c) Qwen2.5-VL-32B model. This is because larger MLLMs have better reasoning abilities that allow it to describe the image from multiple perspectives and in more logically coherent way. This leads to longer captions. Reasoning accuracies. In Table 19, we provide complete comparison of captions (generated by Qwen2.5-VL-3B with and without CRO) and the resulting reasoning results (produced by R1-7B). Similarly to the case study on caption quality, MLLMs trained with CRO generate captions that capture more details. This is critical to the correctness of the reasoning process. As can be seen, the reasoner that receives captions with CRO arrives at the correct answer after several rounds of thinking and reflection. However, the reasoner that accepts the under-optimized caption experiences multiple contradictions and confusion (highlighted in brown), which leads to responses that exceeds the maximum context length and finally fails this problem. 17 Table 7: Case Study with Qwen2.5-VL-3B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: Question: If driver travels an average of 4,250 miles per month driving only along motorways in an Xtam car, what is the predicted annual consumption of fuel (in gallons)? Select from A, B, C, and E. (A)Cannot say (B) 1,500 (C) 125 (D) 150 (E) 1,250 Query-conditioned Captions (Qwen2.5-VL-3B w/ CRO): The image is bar graph titled Fuel Consumption (miles to the gallon) that compares the fuel efficiency of five different cars, labeled as Taber, Ursa, Velvo, Tink, and Xtam. The graph is divided into two categories: City driving and Motorway driving. For each car, there are two bars: - The light blue bar represents fuel consumption for city driving. - The dark blue bar represents fuel consumption for motorway driving. The data for each car is as follows: - Taber: City driving: 48 miles per gallon Motorway driving: 43 miles per gallon - Ursa: City driving: 39 miles per gallon Motorway driving: 43 miles per gallon - Velvo: City driving: 35 miles per gallon Motorway driving: 33 miles per gallon - Tink: City driving: 33 miles per gallon Motorway driving: 28 miles per gallon - Xtam: City driving: 33 miles per gallon Motorway driving: 33 miles per gallon Below the bar graph, there is table that provides additional information about each car: - Max. Speed (mph): Taber (65), Ursa (60), Velvo (125), Tink (95), Xtam (110) - Cost to Purchase (): Taber (12,500), Ursa (15,250), Velvo (37,500), Tink (55,250), Xtam (62,500) The image does not contain any information related to the question about fuel consumption for specific car model. Query-conditioned Captions (Qwen2.5-VL-3B): 18 The image is bar graph titled Fuel Consumption (miles to the gallon) that compares the fuel efficiency of five different cars, Taber, Ursa, Velvo, Tink, and Xtam, under two driving conditions: city driving and motorway driving. The graph uses two colors to differentiate between these conditions: light blue for city driving and dark blue for motorway driving. Below the graph, there is table that provides additional information about each car, including: - Max. Speed (mph): The maximum speed each car can achieve. - Cost to Purchase (): The price of each car in pounds. The table entries are as follows: - Taber: Max. Speed: 65 mph Cost to Purchase: 12,500 - Ursa: Max. Speed: 60 mph Cost to Purchase: 15,250 - Velvo: Max. Speed: 125 mph Cost to Purchase: 37,500 - Tink: Max. Speed: 95 mph Cost to Purchase: 55,250 - Xtam: Max. Speed: 110 mph Cost to Purchase: 62,500 The graph shows that the Taber and Ursa cars have higher fuel efficiency in city driving compared to motorway driving, while the Velvo, Tink, and Xtam cars have higher fuel efficiency in motorway driving compared to city driving. Table 8: Case Study with Qwen2.5-VL-3B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: Question: According to the question shown in the image, please directly answer the question and provide the correct option letter, e.g., A, B, C, D. Query-conditioned Captions (Qwen2.5-VL-3B w/ CRO): The image is geometric diagram with the following elements: - Two parallel lines labeled AB and CD. 19 - line segment EF . - point where line segment EG intersects line segment CD. - An angle labeled 50 at point . - An angle labeled 2 at point G. - question asking about the measure of angle 2, with choices: A: 50 B: 60 C: 65 D: 90 The diagram shows triangle EF with angle 1 at point and angle 2 at point G. The angle at point is given as 50. The question asks for the measure of angle 2, given that AB CD and EG bisects angle BEF . Query-conditioned Captions (Qwen2.5-VL-3B): The image shows geometric figure with lines AB and CD parallel to each other. Line EG bisects angle BEF. Angle 1 is marked as 50 degrees. Table 9: Case Study with Qwen2.5-VL-3B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: Question: 20 Input binary number = 11000, find the corresponding decimal value by the algorithm shown in image. Query-conditioned Captions (Qwen2.5-VL-3B w/ CRO): The image is flowchart titled \"Start Binary To Decimal.\" It outlines process to convert binary number to its decimal equivalent. The flowchart starts with the instruction \"Get Binary Number Call it B,\" which is followed by initializing variables and Power to 0. The flowchart then checks if is greater than 0. If is greater than 0, it checks if modulo 10 raised to the power of (Power + 1) is equal to 0. If this condition is false, it updates by adding 2 raised to the power of Power, and then updates by subtracting 10 raised to the power of Power. If the condition is true, it increments Power. This process continues until is no longer greater than 0. Finally, it states \"Say The Answer is D\" and \"Stop.\" The image does not contain any visual elements or additional information beyond the flowchart itself. Query-conditioned Captions (Qwen2.5-VL-3B): Start Binary To Decimal Get Binary Number Call it = 0 Power = 0 > 0 B%10(Power + 1) = 0 = + 2Power = 10Power Yes Power ++ Table 10: Case Study with Qwen2.5-VL-3B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: Question: The pie chart below represents the number of each grade at United States elementary school. What is the percentage of Grade 2? The answer should be percentage between 0 and 100. 21 Query-conditioned Captions (Qwen2.5-VL-3B w/ CRO): The image is pie chart titled \"Number of Students in Each Grade.\" The chart is divided into five segments, each representing different grade level and the number of students in that grade. The grades and their corresponding numbers are as follows: - Grade 1: 79 students - Grade 2: 96 students - Grade 3: 73 students - Grade 4: 58 students - Grade 5: 91 students The pie chart visually represents the distribution of students across these grades. The segments are color-coded for each grade, with Grade 1 in red, Grade 2 in blue, Grade 3 in green, Grade 4 in orange, and Grade 5 in purple. The chart provides clear visual representation of the number of students in each grade at the elementary school. Query-conditioned Captions (Qwen2.5-VL-3B): The pie chart below represents the number of each grade at United States elementary school. What is the percentage of Grade 2? The answer should be percentage between 0 and 100. Table 11: Case Study with Qwen2.5-VL-3B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: Question: Is the left graph isomorphic with the right graph? choice: (A) Yes (B) No Query-conditioned Captions (Qwen2.5-VL-3B w/ CRO): The image consists of two graphs. The left graph is connected graph with the following vertices and edges: A, B, C, D, E, F, G, and H. The edges are as follows: is connected to H, is connected to F, is connected to E, is connected to F, is connected to E, is connected to G, is connected to G, and is connected to D. The right graph is simple cycle graph with 8 vertices, labeled as 1, 2, 3, 4, 5, 6, 7 and 8 and edges connecting each vertex to the next in circular manner. Based on the description, the left graph is not isomorphic to the right graph. The left graph has more complex structure with multiple connections and central node (E), while the right graph is simple cycle with no such central node and different number of vertices. Query-conditioned Captions (Qwen2.5-VL-3B): The left graph is graph with 8 vertices and 10 edges. The right graph is graph with 8 vertices and 8 edges. The left graph is not isomorphic to the right graph. 22 Table 12: Case Study with Qwen2.5-VL-3B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: Question: Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end. Question: What happens to fish if pelicans increase?Choices: (A) decrease (B) nothing (C) increase (D) none of the above Query-conditioned Captions (Qwen2.5-VL-3B w/ CRO): The image depicts food chain diagram showing the trophic levels and different types of consumers in both terrestrial and aquatic environments. Here is detailed description of the image: 1. Trophic Level: - Primary producer (autotroph): This level includes plants and phytoplankton, which are the base of the food chain. - Primary consumer (herbivore): This level includes caterpillars and zooplankton, which feed on the primary producers. - Secondary consumer (carnivore): This level includes lizards and fish, which feed on the primary consumers. - Tertiary consumer (secondary carnivore): This level includes snakes and pelicans, which feed on the secondary consumers. 2. Terrestrial food chain: - Primary producer (autotroph): Plant - Primary consumer (herbivore): Caterpillar - Secondary consumer (carnivore): Lizard - Tertiary consumer (secondary carnivore): Snake 3. Aquatic food chain: - Primary producer (autotroph): Phytoplankton - Primary consumer (herbivore): Zooplankton - Secondary consumer (carnivore): Fish - Tertiary consumer (secondary carnivore): Pelican The image also shows arrows indicating the direction of energy flow from one trophic level to the next, emphasizing the flow of energy through different ecosystems. 23 Query-conditioned Captions (Qwen2.5-VL-3B): The image shows food chain diagram with two types of food chains: terrestrial and aquatic. The terrestrial food chain includes caterpillar, lizard, fish, and snake, with the caterpillar being the primary producer and the snake being the tertiary consumer. The aquatic food chain includes zooplankton, phytoplankton, fish, and pelican, with the zooplankton being the primary producer and the pelican being the tertiary consumer. Table 13: Case Study with Qwen2.5-VL-3B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: Question: Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end. Question: If frogs were removed from this environment what animal would potentially see an increase in its population? Choices:(A) crickets (B) deer (C) snakes (D) hawks Query-conditioned Captions (Qwen2.5-VL-3B w/ CRO): The image is food web diagram depicting various organisms and their interactions within an ecosystem. The diagram includes the following elements: Deer (at the bottom left) consuming Trees. Mice (in the center) consuming Grasses. Crickets (in the center right) consuming Grasses. Snakes (at the top left) consuming Mice. Hawks (at the top center) consuming Snakes. Owls (at the top right) consuming Mice. Frogs (in the center right) consuming Crickets. Decomposers (at the bottom center) breaking down organic matter. The arrows indicate the direction of energy flow from one organism to another, showing the relationships between the different species in the ecosystem. Query-conditioned Captions (Qwen2.5-VL-3B): The image is food web diagram depicting various organisms and their interactions within an ecosystem. It includes deer, trees, grasses, mice, crickets, frogs, hawks, and owls. The arrows indicate the direction of energy flow from producers (trees and grasses) to consumers (deer, mice, crickets, frogs, hawks, and owls), and decomposers (decomposers). The diagram shows how energy moves through the ecosystem, with each organism feeding on others, and how the removal of frogs could potentially lead to an increase in the population of crickets, as they are primary food source for frogs. 24 Table 14: Case Study with Qwen2.5-VL-32B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: Question: Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end. Question: Which option is correct? Choices: (A) (B) (C) Query-conditioned Captions (Qwen2.5-VL-32B w/ CRO): 25 The image presents logic puzzle involving patterns and shapes. Here is detailed description of the image: Left Side: Grid with Shapes The grid is 3x3 matrix. Each cell contains shape with specific pattern or design. The shapes include circles, diamonds, and triangles, each with different internal designs (e.g., stars, squares, etc.). Row 1: 1. Cell 1: circle with star inside. 2. Cell 2: diamond with star inside. 3. Cell 3: triangle with star inside. Row 2: 1. Cell 1: Empty. 2. Cell 2: circle with square inside. 3. Cell 3: diamond with square inside. Row 3: 1. Cell 1: Empty. 2. Cell 2: triangle with downward arrow inside. 3. Cell 3: Empty. Right Side: Multiple Choice Options There are three options labeled A, B, and C. Each option shows sequence of three shapes. The task is to determine the correct pattern that fits the logic of the grid. Option A: 1. diamond with downward arrow inside. 2. triangle with square inside. 3. circle with star inside. Option B: 1. diamond with downward arrow inside. 2. triangle with square inside. 3. circle with downward arrow inside. Option C: 1. diamond with downward arrow inside. 2. triangle with square inside. 3. diamond with downward arrow inside. Analysis of the Grid Patterns Column-wise Observation: Column 1: Row 1: Circle with star. Row 2: Empty. Row 3: Empty. Column 2: Row 1: Diamond with star. Row 2: Circle with square. Row 3: Triangle with downward arrow. Column 3: Row 1: Triangle with star. Row 2: Diamond with square. Row 3: Empty (to be determined). Row-wise Observation: Row 1: Circle (star), Diamond (star), Triangle (star) Row 2: Empty, Circle (square), Diamond (square) Row 3: Empty, Triangle (downward arrow), ? (to be determined) Shape Progression: The shapes in each row and column follow specific order: Circle Diamond Triangle. The internal designs also follow pattern: Star Square Downward Arrow. Query-conditioned Captions (Qwen2.5-VL-32B): The image consists of two main sections: Left Section: 3x3 grid with various shapes and patterns. The grid contains: circle with star inside in the top-left cell. diamond with star inside in the top-middle cell. triangle with star inside in the top-right cell. circle with square inside in the middle-left cell. diamond with square inside in the middle-middle cell. triangle with square inside in the middle-right cell. triangle with downward-pointing arrow inside in the bottom-left cell. The bottom-middle and bottom-right cells are empty. 27 Right Section: Three options labeled A, B, and C. Each option contains three shapes: Option A: diamond with downward-pointing arrow inside, triangle with square inside, and circle with star inside. Option B: circle with downward-pointing arrow inside, triangle with square inside, and another circle with downward-pointing arrow inside. Option C: circle with downward-pointing arrow inside, triangle with square inside, and diamond with downward-pointing arrow inside. The task appears to involve identifying pattern or sequence based on the shapes and their internal symbols in the grid and selecting the correct option from the choices provided. Table 15: Case Study with Qwen2.5-VL-32B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: Question: Hint: Please answer the question requiring an integer answer and provide the final value, e.g., 1, 2, 3, at the end. Question: How many models in the figure achieve an Acc score greater than 60? Query-conditioned Captions (Qwen2.5-VL-32B w/ CRO): The image is scatter plot titled \"Model Performance,\" which compares the performance of various large language models on specific task, likely related to the GSM8K dataset. The plot uses different colors and line styles to distinguish between models and their training settings. Here is detailed description of the image: Title and Labels: Title: \"Model Performance\" X-axis: Labeled \"Pretrain Loss,\" ranging from approximately 1.2 to 2.0. Y-axis: Labeled \"ACC\" (presumably short for Accuracy), ranging from 0 to 80. Legend: The legend on the left side lists various models and their corresponding colors and markers: GPT3-3B (pink), GPT3-6B (orange), GPT3-12B (yellow), GPT3-175B (dark green) LLaMA-7B (light green), LLaMA2-7B (teal), LLaMA-13B (dark teal), LLaMA2-13B (light blue) LLaMA-33B (blue), LLaMA-65B (purple), LLaMA2-70B (pink), GPT-4 (dark pink) Line Styles: Blue Lines: Represent the SFT (Supervised Fine-Tuning) setting. Red Lines: Represent the ICL (In-Context Learning) setting. Dotted lines connect points for each model, showing trends in performance. Data Points: Each model is represented by series of points on the plot, with the position determined by: X-axis (Pretrain Loss): Lower values indicate better pretraining. Y-axis (ACC): Higher values indicate better accuracy. Key Observations: 1. GPT-4 Performance: GPT-4 (dark pink) achieves the highest accuracy (ACC) score, well above 60, and has low pretraining loss. It is plotted as single point at the top-right of the graph. 2. LLaMA and LLaMA2 Models: Various LLaMA and LLaMA2 models (e.g., LLaMA-7B, LLaMA2-7B, LLaMA-13B, etc.) are scattered across the plot. Larger models (e.g., LLaMA2-70B) tend to have higher accuracy scores. 3. GPT3 Models: GPT3 models (GPT3-3B, GPT3-6B, GPT3-12B, GPT3-175B) are plotted with lower accuracy scores compared to LLaMA and LLaMA2 models. GPT3-175B performs better than smaller GPT3 models but still lags behind the larger LLaMA models. 4. Accuracy Threshold (ACC > 60): Models achieving an ACC score greater than 60 are primarily from the LLaMA2 series and GPT-4. Notable models include: LLaMA2-70B LLaMA2-65B LLaMA2-33B GPT-4 Text Caption: The caption below the figure provides additional context: Figure 2: Describes the performance of SFT (blue lines) and ICL (red lines) settings on GSM8K. Notes that GPT-4 uses some part of the GSM8K data in pretraining and suggests its performance lies between SFT and ICL. 29 Query-conditioned Captions (Qwen2.5-VL-32B): The image is scatter plot titled \"Model Performance.\" It compares the performance of various large language models on task, likely related to the GSM8K dataset, based on two metrics: \"ACC\" (accuracy) on the y-axis and \"Pretrain Loss\" on the x-axis. The models are represented by different colored dots, and the legend on the left identifies each model by name and color. The models include various versions of GPT-3 (3B, 6B, 12B, 175B), LLaMA (7B, 13B, 33B, 65B, 70B), LLaMA2 (7B, 13B, 33B, 65B, 70B), and GPT-4. Key Features: 1. Axes: Y-axis (ACC): Ranges from 0 to 80, representing accuracy scores. X-axis (Pretrain Loss): Ranges from 1.2 to 2.0, representing pretraining loss values. 2. Models: Each model is represented by colored dot, with the model name labeled near the dot. Models include the GPT-3 series, LLaMA series, LLaMA2 series, and GPT-4. 3. Trends: There are two sets of lines: Blue lines (SFT settings): Represent the performance of models under supervised fine-tuning (SFT). Red lines (ICL settings): Represent the performance of models under in-context learning (ICL). GPT-4 is shown as single point, suggesting its unique position in the performance landscape. 4. Legend: The legend on the left provides color-coded list of models, including: GPT-3 variants: 3B, 6B, 12B, 175B LLaMA variants: 7B, 13B, 33B, 65B, 70B LLaMA2 variants: 7B, 13B, 33B, 65B, 70B GPT-4 5. Caption: The caption explains that the figure shows the performance of SFT (blue lines) and ICL (red lines) settings on GSM8K. It notes that GPT-4 uses some part of the GSM8K data in pretraining and suggests its performance lies between SFT and ICL. Question: The question asks how many models in the figure achieve an ACC score greater than 60. The task is to count the number of models that meet this criterion based on the scatter plot. However, the instruction is to describe the image without answering the question. Table 16: Case Study with Qwen2.5-VL-32B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: 30 Question: Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end. Question: Does South Carolina have the highest value in the South ? Choices: (A) Yes (B) No Query-conditioned Captions (Qwen2.5-VL-32B w/ CRO): The image is choropleth map of the United States, depicting the distribution of health insurance coverage for nonelderly individuals (ages 064) through employer-sponsored plans in 2013. Here is detailed description of the image: Title and Subtitle: Title: Health Insurance Coverage of Nonelderly 064: Employer, 2013 This indicates that the map shows the number of nonelderly individuals (aged 064) who had employer-sponsored health insurance in 2013. Color Coding: The map uses gradient of blue shades to represent different ranges of employer-sponsored health insurance coverage. The legend at the bottom explains the color coding: Lightest Blue: 281,900 2,024,500 Light Blue: 2,098,300 4,739,300 Medium Blue: 4,757,100 6,467,400 Darkest Blue: 7,133,900 16,665,500 State Colors: Darkest Blue (Highest Coverage): States in this category include California (CA), New York (NY), Texas (TX), and Florida (FL). These states have the highest number of nonelderly individuals with employer-sponsored health insurance. Medium Blue: States like Illinois (IL), Ohio (OH), Pennsylvania (PA), and Michigan (MI) fall into this category. Light Blue: States such as Colorado (CO), Arizona (AZ), and Tennessee (TN) are in this range. Lightest Blue (Lowest Coverage): States with the lowest coverage include Alaska (AK), Hawaii (HI), Wyoming (WY), and several others in the Midwest and Northeast. 31 South Region Analysis: Highest Value in the South: The darkest blue states in the South are Texas (TX) and Florida (FL), indicating they have the highest employer-sponsored health insurance coverage in the region. South Carolina (SC): South Carolina is colored in light blue shade, indicating it falls into the range of 2,098,300 4,739,300. This is not the highest value in the South. Other Observations: Northeast: New York (NY) and Pennsylvania (PA) have high coverage, with NY in the darkest blue category. Midwest: States like Illinois (IL) and Ohio (OH) have medium to high coverage. West: California (CA) stands out with the darkest blue, indicating the highest coverage in the West. Source: The map is sourced from the Kaiser Family Foundations State Health Facts, as noted at the bottom of the image. Question Analysis: The question asks whether South Carolina has the highest value in the South. Based on the map: South Carolina is in light blue category, not the darkest blue. The highest values in the South are represented by Texas and Florida, which are in the darkest blue category. Query-conditioned Captions (Qwen2.5-VL-32B): The image is choropleth map of the United States, illustrating the distribution of health insurance coverage for nonelderly individuals aged 064 through employer-sponsored plans in 2013. The map uses varying shades of blue to represent different ranges of coverage numbers across the states. The legend at the bottom indicates four categories of coverage: Lightest Blue: 281,900 2,024,500 Light Blue: 2,098,300 4,739,300 Medium Blue: 4,757,100 6,467,400 Darkest Blue: 7,133,900 16,665,500 Key observations from the map: States like California (CA), Texas (TX), and Florida (FL) are shaded in the darkest blue, indicating the highest range of employer-sponsored health insurance coverage. Many states in the Midwest and West are shaded in lighter blues, indicating lower coverage numbers. The source of the data is cited as the Kaiser Family Foundations State Health Facts. The question asks whether South Carolina (SC) has the highest value in the South. The map shows South Carolina in light blue shade, indicating it falls into the second-lowest category of coverage. Table 17: Case Study with Qwen2.5-VL-32B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: 32 Question: <image 1>A battery, an ammeter, three resistors, and switch are connected to form the simple circuit shown above. When the switch is closed what would happen to the potential difference across the 15 ohm resistor? Query-conditioned Captions (Qwen2.5-VL-32B w/ CRO): The image depicts an electrical circuit involving battery, an ammeter, three resistors, and switch. Here is detailed description of the components and their arrangement: Components in the Circuit: 1. Battery: Located on the left side of the circuit. Serves as the power source, providing the electromotive force (EMF) to the circuit. 2. Ammeter (A): Placed immediately after the battery. Used to measure the current flowing through the circuit. It is connected in series with the rest of the circuit. 3. Resistors: There are three resistors with the following resistances: 20 Ω resistor 15 Ω resistor 30 Ω resistor The resistors are connected in combination of series and parallel configurations. 4. Switch (S): Located on the right side of the circuit. Controls the flow of current in the circuit. When closed, the circuit is complete, and current can flow. Circuit Configuration: The 20 Ω resistor is connected in series with the rest of the circuit. The 15 Ω and 30 Ω resistors are connected in parallel. The parallel combination is then connected in series with the 20 Ω resistor. The ammeter is in series with all the resistors, measuring the total current. The switch is placed after the parallel combination, completing the circuit when closed. Path of Current: 1. Current flows from the positive terminal of the battery. 2. It passes through the ammeter. 3. It then encounters the 20 Ω resistor. 4. After the 20 Ω resistor, the current splits: One part flows through the 15 Ω resistor. The other part flows through the 30 Ω resistor. 5. The currents recombine after the parallel branch. 6. The combined current continues through the rest of the circuit and returns to the battery. Analysis of Potential Difference: When the switch is closed, current flows through the circuit and potential differences develop across each resistor. The 15 Ω and 30 Ω resistors are in parallel. In parallel configuration, the potential difference across each branch is the same. Therefore, the potential difference across the 15 Ω resistor is equal to that across the 30 Ω resistor. Query-conditioned Captions (Qwen2.5-VL-32B): The image depicts simple electrical circuit consisting of the following components: Battery: power source labeled as \"Battery\" is shown on the left side of the circuit. Ammeter (A): device used to measure the current in the circuit is connected in series with the battery. Resistors: Three resistors are present in the circuit: 20 Ω resistor is connected in series with the ammeter. 15 Ω resistor is connected in parallel with 30 Ω resistor. Switch (S): switch is placed in the circuit, which can be opened or closed to control the flow of current. Wires: The components are connected by wires to form complete circuit. Circuit Configuration: The 20 Ω resistor is in series with the ammeter and the battery. The 15 Ω resistor and the 30 Ω resistor are connected in parallel with each other. The switch is placed in the branch that includes the parallel combination of the 15 Ω and 30 Ω resistors. Key Observations: When the switch is closed, the parallel combination of the 15 Ω and 30 Ω resistors will be active, allowing current to flow through both resistors. The ammeter measures the total current flowing through the circuit. This description provides detailed overview of the circuits components and their arrangement. Table 18: Case Study with Qwen2.5-VL-32B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Image: 34 Question: Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.Question: Misty the cat has five kittens: two of them are striped, one spotty, the rest of them are absolutely white. In which picture can we see the kittens of Misty, knowing that the ears of one of them are of different colour? Choices: (A) (B) (C) (D) (E) Query-conditioned Captions (Qwen2.5-VL-32B w/ CRO): The image presents multiple-choice question involving illustrations of cats. Here is detailed description of the image: Overall Structure: The image contains five labeled sections, each representing different option: A, B, C, D, and E. Each section features group of cartoon-style cat illustrations. The question describes specific scenario about cat named Misty who has five kittens with particular characteristics. The task is to identify which section correctly represents Mistys kittens based on the given clues. Clues from the Question: 1. Misty has five kittens. 2. Two of the kittens are striped. 3. One kitten is spotty. 4. The rest (two kittens) are absolutely white. 5. One of the kittens has ears of different color. Analysis of Each Section: Section A: 1 striped, 1 spotty, 2 white kittens. No kitten has ears of different color. Section B: 2 striped, 1 spotty, 2 white kittens. No kitten with differently colored ears. Section C: 2 striped, 1 spotty, 2 white, 1 gray kittens. One kitten has ears of different color. 35 Section D: 2 striped, 1 spotty, 2 white kittens. One kitten has ears of different color. Section E: 1 striped, 3 spotty, 1 white kittens. One kitten has ears of different color. Query-conditioned Captions (Qwen2.5-VL-32B): The image shows five groups of cartoon-style cats, each labeled with letter (A, B, C, D, E). Each group contains multiple cats with various patterns and colors, including striped, spotty, and solid-colored cats. The cats are depicted in different poses, such as sitting, lying down, or standing. The question asks to identify the group that matches the description of Mistys kittens, which includes two striped, one spotty, and two white kittens, with one having differently colored ears. Detailed Description of Each Group: Group A: Contains cats with striped and spotty patterns. Some cats have differently colored ears. Group B: Features cats with striped and solid-colored patterns. No cats have differently colored ears. Group C: Includes cats with striped, spotty, and solid-colored patterns. No cats have differently colored ears. Group D: Shows cats with striped and solid-colored patterns. No cats have differently colored ears. Group E: Contains cats with striped, spotty, and solid-colored patterns. No cats have differently colored ears. The task is to identify the group that matches the specific description of Mistys kittens. Mistys Kittens: Two striped kittens One spotty kitten Two white (solid-colored) kittens One kitten has differently colored ears Conclusion: Only Group meets all the criteria, including the presence of differently colored ears. 36 Table 19: Reasoning Case Study with Qwen2.5-VL-3B. Additional visual details that are relevant to the question but are only generated by MLLMs with CRO are highlighted in red. Reasoning process that show confusing and contradiction are highlighted in brown. Image: Question: is the circumcircle of ABC, with AB = BC = 16. The arc AB is folded down along the chord AB to intersect BC at point D. If point is the midpoint of BC, what is the length of AC? Query-conditioned Captions (Qwen2.5-VL-3B w/ CRO): The image is geometric diagram of triangle ABC inscribed in circle with center O. The diagram shows the following elements: - Points A, B, and are the vertices of the triangle. - Point is the center of the circumcircle of ABC. - Line segment AB is chord of the circle. - Line segment BC is another chord of the circle. - Point is point on the circle such that the arc AB is folded down along the chord AB to intersect BC at D. - The diagram includes dashed line segment AD and dashed line segment BD. The image does not provide any numerical values or specific measurements, and it does not contain any text or additional information. The focus is purely on the geometric relationships and the diagram itself. Query-conditioned Captions (Qwen2.5-VL-3B): The image shows circle with center and points A, B, and on its circumference. The line segment AB is diameter of the circle, and the arc AB is folded down along the chord AB to intersect BC at point D. Point is the midpoint of BC. Reasoning Result of R1-7B (captions from Qwen2.5-VL-3B w/ CRO): Alright, let me try to figure out this geometry problem step by step. So, we have triangle ABC inscribed in circle with center O. AB and BC are both 16 units long, making it an isosceles triangle with AB = BC. The diagram shows that when arc AB is folded along chord AB, it intersects BC at point D, which is the midpoint of BC. So, BD = DC = 8 units. First, need to visualize this. Since ABC is isosceles with AB = BC, the base is AC, and the two equal sides are AB and BC. The circle is the circumcircle, so all three vertices lie on the circumference. When the arc AB is folded along AB, point is where the fold intersects BC. Since is the midpoint, BD = DC = 8. Folding along AB suggests that point is the reflection of point over the line AB. But since lies on BC, maybe theres some symmetry here. 37 think should consider the properties of the circumcircle. In triangle inscribed in circle, the perpendicular bisectors of the sides meet at the center O. Since AB = BC, the triangle is isosceles, so the median from to AC should also be the altitude and the angle bisector. Wait, but were dealing with fold along AB. When you fold along AB, point would map to some point on BC. Since is the midpoint, maybe theres reflection involved here. Let me try to see if triangle ABD is congruent or similar to another triangle. Alternatively, maybe using the Law of Cosines would help. Lets denote angle at as angle ABC. Since AB = BC = 16, the triangle is isosceles, so angles at and are equal. But when we fold arc AB over AB, point is created on BC. Since is the midpoint, perhaps theres relationship between the angles or the lengths. Wait, another approach: when you fold along AB, point maps to D. So, AD should equal AC because reflection preserves distances. So, AD = AC. Also, since is the midpoint of BC, BD = DC = 8. So, in triangle ABD, we have AB = 16, BD = 8, and AD = AC. So, triangle ABD has sides AB = 16, BD = 8, and AD = AC = x. Maybe can apply the Law of Cosines here. Lets denote AC as x. Then, AD = as well. In triangle ABD: Which is: AB2 + BD2 2 AB BD cos(ABD) = AD2 162 + 82 2 16 8 cos(ABD) = x2 256 + 64 256 cos(ABD) = x2 320 256 cos(ABD) = x2 But dont know angle ABD. Maybe can relate it to the original triangle ABC. Since ABC is isosceles with AB = BC = 16, angle ABC is equal to angle BAC and angle BCA? Wait, no, in an isosceles triangle, the angles opposite the equal sides are equal. So, since AB = BC, the angles opposite them, which would be angle ACB and angle BAC, are equal. Wait, no, in triangle ABC, AB = BC = 16, so the sides opposite angles at and respectively. So, angle at (angle ACB) equals angle at (angle BAC). Lets denote angle BAC as θ, so angle ACB is also θ, and angle ABC is 180 2θ degrees. When we fold along AB, point maps to on BC. So, the reflection over AB takes point to D. Therefore, AD = AC and angle BAD = angle BAC = θ. Wait, but AD is equal to AC because reflection preserves length. So, triangle ABD has sides AB = 16, BD = 8, and AD = AC = x. Maybe can use the Law of Cosines here as well. 38 Alternatively, maybe using the Law of Sines in triangle ABC. Since ABC is isosceles with AB = BC = 16, the circumradius can be found using the formula: Where is the length of side, and is the opposite angle. = 2 sin(A) Wait, but Im not sure if thats the right approach. Let me think again. Since ABC is isosceles with AB = BC = 16, then angle at equals angle at C. Let me denote angle at as ϕ. So, in triangle ABC, angles are ϕ at B, and (180 ϕ)/2 at and each. Now, when we fold along AB, point is reflected over AB to point on BC. So, AD = AC, and BD = 8, DC = 8. So, in triangle ABD, AB = 16, BD = 8, AD = x. So, using the Law of Cosines on triangle ABD: AB2 + BD2 2 AB BD cos(ABD) = AD2 162 + 82 2 16 8 cos(ABD) = x2 256 + 64 256 cos(ABD) = x2 320 256 cos(ABD) = x2 But angle ABD is the same as angle ABC, which is ϕ. So, cos(ABD) = cos(ϕ). From triangle ABC, using the Law of Cosines: AC 2 = AB2 + BC 2 2 AB BC cos(ϕ) x2 = 162 + 162 2 16 16 cos(ϕ) x2 = 512 512 cos(ϕ) But from triangle ABD, we have x2 = 320 256 cos(ϕ) So, setting them equal: 512 512 cos(ϕ) = 320 256 cos(ϕ) 512 320 = 512 cos(ϕ) 256 cos(ϕ) 192 = 256 cos(ϕ) 3 4 cos(ϕ) = 192 = 39 So, angle ϕ has cosine 3 4 . Now, substituting back into the equation for x2 from triangle ABC: (cid:18) 3 4 x2 = 512 512 (cid:19) x2 = 512 384 = 128 = 128 = 8 Wait, but this contradicts the initial thought that AC is 16 reflection part. Let me check. 2. Hmm, maybe made mistake in the Wait, when we fold along AB, point maps to on BC. So, AD should equal AC because reflection preserves distance. So, AD = AC = x. But in triangle ABD, we have AB = 16, BD = 8, AD = x. Using Law of Cosines: 162 + 82 2 16 8 cos(ABD) = x2 256 + 64 256 cos(ABD) = x2 320 256 cos(ABD) = x2 But angle ABD is the same as angle ABC, which is ϕ. So, cos(ABD) = cos(ϕ) = 3 4 . Thus, x2 = 320 256 (cid:0) 3 (cid:1) = 320 192 = 128 = 128 = 8 2 4 Wait, so that would make AC = 8 maybe the model response was incorrect. 2, not 16 2 as the initial model response suggested. Hmm, so Alternatively, perhaps made mistake in assuming angle ABD is equal to angle ABC. Let me think again. In triangle ABC, angle ABC is at point B, between AB and BC. When we fold along AB, the angle at remains the same, but the direction changes. So, the angle ABD in triangle ABD is actually angle between AB and BD, which is different from angle ABC. Wait, no, BD is part of BC, so angle ABD is actually the same as angle ABC because BD is along BC. So, maybe my initial assumption was correct. Wait, but then according to this calculation, AC = 8 and BC are 16, that seems plausible. 2, which is about 11.31, but given that AB Alternatively, maybe should consider that when folding, the angle at is right angle. So, maybe triangle ABD is right triangle. Wait, in the model response, they concluded that angle BAC is 90 degrees, which might not be correct. Let me see. 40 If angle BAC is 90 degrees, then triangle ABC would be right-angled isosceles triangle, making AC = AB 2. So, maybe the model response was wrong. 2. But according to my calculation, its 2 = 16 Wait, perhaps the mistake is in assuming that AD = AC. When you fold along AB, point maps to D, so the distance from to should equal the distance from to C, right? So, AD = AC. So, that part seems correct. Wait, but in the model response, they used Law of Cosines with angle BAC being 90 degrees, which led to AC = 16 2. But according to my calculation, angle BAC is not necessarily 90 degrees, and its actually determined by the folding. Wait, perhaps should use the fact that in the folded diagram, AD = AC because of reflection. So, maybe triangle ABD is congruent to triangle ABC? But no, because BD is only 8, not 16. Alternatively, perhaps using coordinate geometry would help. Let me place point at (0,0), point at (16,0), since AB = 16. Since ABC is isosceles with AB = BC = 16, point would be somewhere such that BC = 16. ... (rest of the reasoning) Therefore, the correct answer should be AC = 8 2 as in the model response. 2, not 16 But wait, maybe made mistake in assuming angle ABD is equal to angle ABC. Let me think again. In triangle ABC, angle at is ϕ, which we found cos(ϕ) = 3 at remains the same, so angle ABD is indeed ϕ. Therefore, my calculation should be correct. 4 . When folding along AB, the angle Therefore, the model response was incorrect, and the correct answer is AC = 8 2 . Reasoning Result of R1-7B (captions from Qwen2.5-VL-3B): Alright, so Im trying to solve this geometry problem. Let me read it again carefully. We have triangle ABC with AB = BC = 16. So, its an isosceles triangle with AB and BC equal. That means angles at and are equal, right? Wait, no, actually, in triangle ABC, if AB = BC, then the base is AC, so the base angles at and are equal. Hmm, but Im not sure if thats important right now. The problem mentions that the arc AB is folded down along the chord AB to intersect BC at point D. And is the midpoint of BC. So, BC is 16, so BD and DC are each 8. 41 Got it. We need to find the length of AC. First, should probably visualize the problem. Let me draw circle with center O, points A, B, and on the circumference. AB is diameter, but wait, hold on. The caption says the image shows circle with center and points A, B, on the circumference. The line segment AB is diameter, and the arc AB is folded down along AB to intersect BC at D, which is the midpoint. Wait, so AB is diameter, which would mean that angle ACB is right angle because any triangle inscribed in semicircle is right triangle. But hold on, in the problem statement, it says AB = BC = 16. If AB is diameter, then AB would be the longest side, but BC is also 16, so thats interesting because in right triangle, the hypotenuse is the longest side. So if AB is the diameter and side of the triangle, then AB would be the hypotenuse. But in the problem, AB = BC, so both would be legs, which cant be because in right triangle, the hypotenuse is longer than the legs. Hmm, maybe AB isnt the diameter? Wait, the initial caption says AB is diameter, but the problem statement says AB = BC = 16. That seems conflicting because if AB is diameter, it should be longer than BC. Maybe misread the problem. Let me check. Wait, no, the problem says is the circumcircle of triangle ABC. So is the circumradius. If AB is diameter, then the circumradius would be half of AB, which would make the circumradius 8. But if AB is 16, then the circumradius is 8. However, in that case, triangle ABC would have AB as the diameter, making angle ACB right angle. But in the problem, AB = BC = 16, so triangle ABC would have two sides equal to 16 and one side (AC) which we need to find. But if angle is 90 degrees, then AC would be the other leg, so AC = BC = 16, making it an 2. isosceles right triangle, so AC would be 16 But wait, that seems too straightforward, and maybe thats the answer, but Im not sure because the folding part might complicate things. Wait, the problem mentions that the arc AB is folded down along AB to intersect BC at D, which is the midpoint. 42 So maybe the initial assumption that AB is diameter isnt correct because when you fold the arc AB down along AB, it intersects BC at D, which is the midpoint, so maybe AB isnt diameter. Hmm, now Im confused. Alternatively, let me try to think differently. Maybe AB isnt the diameter, but just chord. So the circle has center O, and AB is chord, not necessarily the diameter. Then, the arc AB is folded along AB, meaning reflecting the arc AB over the chord AB, which would create reflection of point over AB, landing at point on BC. Since is the midpoint of BC, which is 16, so BD = DC = 8. So, when you fold arc AB over AB, the reflection of over AB is D, which lies on BC. So, perhaps triangle ABD is congruent to triangle ABC, but Im not sure. Alternatively, maybe the reflection of over AB is D, which is on BC. So, the distance from to AB is the same as from to AB, but since is on BC, maybe we can use some properties here. Wait, perhaps using coordinates would help. Let me try to place the points on coordinate system. Lets place point at the origin (0,0). Since AB = 16, let me place point at (16, 0). Then, since AB is chord of the circle, the center is somewhere in the plane. Let me denote point as (x, y). Since AB = BC = 16, the distance from to is 16, so (cid:112)x2 + y2 = 16. Also, since ABC is triangle, point is somewhere not on AB. Now, the arc AB is folded over AB, and it intersects BC at D, which is the midpoint of BC. So, since is at (0,0) and is at (x,y), the midpoint is at ( When we fold the arc AB over AB, the reflection of point over AB is point D. 2 ). 2 , So, the reflection of point over AB is D, which lies on BC. So, the reflection of point over AB is point D. So, to find the reflection of point over AB, lets find the equation of AB first. Since is at (16,0) and is at (0,0), AB is the x-axis from (0,0) to (16,0). So, the line AB is y=0. Reflecting point over the x-axis changes the sign of the y-coordinate. So, the reflection of point C=(x,y) over AB (y=0) would be (x, -y). But the reflection point is D, which is ( 2 , 2 ). So, according to the reflection, (x, -y) should equal ( 2 , 2 ). Wait, that cant be unless = 0 and = 0, which would place at the origin, but then ABC wouldnt be triangle. Hmm, that doesnt make sense. Maybe my assumption that AB is the x-axis is causing confusion because in reality, AB is just chord, not necessarily the x-axis. Alternatively, perhaps should place AB as horizontal chord but not necessarily aligned with the x-axis. Let me try that. Let me place point at (a, b) and point at (c, d), but this might complicate things. Maybe better approach is to use coordinate geometry with AB as chord, not necessarily aligned with any axis. Alternatively, maybe using vectors or geometric properties would be better. Lets think about the reflection. When you fold the arc AB over AB, point is reflected over AB to point on BC. ... omitted When we fold arc AB over AB, the reflection of point over AB is point D. So, the reflection of over AB is D. Since AB is the x-axis from (0,0) to (a,0), the reflection over AB would invert the y-coordinate. So, if is at (x,y), then would be at (x, -y). But is also the midpoint of BC, which is ( xB +xC 2 , yB +yC 2 ) = ( 0+x 2 , 0+y 2 ) = ( 2 , 2 ). So, setting (x, -y) equal to ( = x/2 => = 0 2 , 2 ): -y = y/2 => -y = y/2 => -2y = => 3y = 0 => = 0 Again, this leads to y=0, which would place on AB, making ABC degenerate. Hmm, clearly, placing AB on the x-axis isnt working because it forces to be on AB. Let me try different approach without coordinate geometry. Let me consider the properties of the reflection. When we fold arc AB over AB, point is reflected over AB to point on BC. So, AB is the perpendicular bisector of segment CD. Therefore, AB is perpendicular to CD, and the midpoint of CD lies on AB. Since is the midpoint of BC, the midpoint of CD is also the midpoint of BC, which is itself. So, lies on AB and is the midpoint of CD. 44 Therefore, AB is the perpendicular bisector of CD, so CD is perpendicular to AB, and is the midpoint of CD and lies on AB. Wait, but is also the midpoint of BC. So, if lies on AB, then BC must intersect AB at D, which is the midpoint of BC. So, in triangle ABC, is the midpoint of BC and lies on AB. Therefore, AB is median of the triangle, and since AB is also side, it must be that triangle ABC is such that AB is both side and median, which is only possible if ABC is an isosceles triangle with AB = AC. But the problem states AB = BC = 16, so if AB = AC, then its an equilateral triangle, but in that case, all sides would be 16, making AC = 16, but the answer seems to be 16 case. Im really confused now. Maybe the correct answer is 16 Im not sure why. 2, as in the initial tentative solution, but 2, so maybe thats not the Wait, another idea: since is the midpoint of BC and lies on AB, and AB is side, then ABC must be such that AB is median and an altitude, making it an isosceles triangle with AB = AC. So, AB = AC = 16. ... exceeding maximum context length (8192)"
        },
        {
            "title": "E Broader Impacts",
            "content": "RACRO improves multi-modal reasoning by aligning perception with reasoning through reinforcement learning. While beneficial for tasks requiring precise vision-language understanding, this alignment could be misused for generating misleading content or surveillance. Caution is advised when applying RACRO in sensitive domains. Mitigation strategies include human oversight, responsible deployment, and transparency in model behavior."
        },
        {
            "title": "F Limitation",
            "content": "We evaluate the effectiveness of the proposed RACRO framework exclusively on multi-modal reasoning tasks. However, its potential applicability could extend to other important domains, such as medical visual question answering (VQA) and geospatial reasoning. Exploring these areas represents promising directions for future work."
        },
        {
            "title": "G More Discussions",
            "content": "Future works. This work aims at decoupling multi-modal reasoning with visual perception and LLM reasoning, and the successful visual perception is essential for downstream reasoning. How to further enhance the visual perception capability is prevailing research direction (e.g., utilizing AIGCgenerated data [10, 16, 17, 18, 23, 28, 29, 33, 50] and enhanced visual representations [7, 8, 35, 58])"
        }
    ],
    "affiliations": [
        "Huawei Cloud",
        "Huawei Noahs Ark Lab",
        "Southern University of Science and Technology",
        "The Hong Kong University of Science and Technology"
    ]
}