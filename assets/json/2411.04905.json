{
    "paper_title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
    "authors": [
        "Siming Huang",
        "Tianhao Cheng",
        "J. K. Liu",
        "Jiaran Hao",
        "Liuyihan Song",
        "Yang Xu",
        "J. Yang",
        "J. H. Liu",
        "Chenchen Zhang",
        "Linzheng Chai",
        "Ruifeng Yuan",
        "Zhaoxiang Zhang",
        "Jie Fu",
        "Qian Liu",
        "Ge Zhang",
        "Zili Wang",
        "Yuan Qi",
        "Yinghui Xu",
        "Wei Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an \"open cookbook\" for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI."
        },
        {
            "title": "Start",
            "content": "OPENCODER: THE OPEN COOKBOOK FOR TOP-TIER CODE LARGE LANGUAGE MODELS Siming Huang Liuyihan Song Linzheng Chai Ge Zhang Tianhao Cheng Yang Xu J. Yang Ruifeng Yuan Zhaoxiang Zhang J.H. Liu J.K. Liu Jiaran Hao Chenchen Zhang Jie Fu Qian Liu Zili Wang Yuan Qi Yinghui Xu Wei Chu INF M-A-P Home Page: https://opencoder-llm.github.io"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, top-tier code LLM that not only achieves performance comparable to leading models but also serves as an open cookbook for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised finetuning stages. By offering this level of openness, we aim to broaden access to all aspects of top-tier code LLM, with OpenCoder serving as both powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI. 4 2 0 2 9 ] . [ 2 5 0 9 4 0 . 1 1 4 2 : r Figure 1: OpenCoder surpasses all previous fully open models (i.e., with open model weights and reproducible datasets) and other open-access models (i.e., with open model weights only) at the 6B+ parameter scale, pushing the frontier of fully open models to new heights. The first two authors contributed equally to this work. Work done during the internships of Siming Huang and Tianhao Cheng at INF. Correspondence to Wei Chu (chuwei@inftech.ai) and Zili Wang (ziliwang.do@gmail.com)."
        },
        {
            "title": "CONTENTS",
            "content": "1 Introduction 2 Pretraining Data"
        },
        {
            "title": "2.1 RefineCode .",
            "content": ". . ."
        },
        {
            "title": "2.1.1 Raw Code .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.1.2 Code-Related Web Data . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "2.1.3 Summary ."
        },
        {
            "title": "2.2 Annealing Data .",
            "content": ". . . 3 Pretraining"
        },
        {
            "title": "3.1 Model Architecture .",
            "content": "3.2 Training Details . . . . . . . . . . . 4 Post Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 Data Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Two-Stage Instruction-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Training Details . . 4.4 Decontamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Experimental Results 5.1 Evaluation on Base Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Evaluation on Instruct Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Analysis 6.1 Analysis of the Deduplication Level . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Analysis on the Importance of High-quality Data In the Annealing Phase . . . . . . 6.3 Analysis on the Effect of GitHub Stars . . . . . . . . . . . . . . . . . . . . . . . . 6.4 Analysis on the two-stage instruction tuning strategy . . . . . . . . . . . . . . . . 7 Related Work 8 Conclusion & Future Work Filtering Rules A.1 Design of Filtering Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Examples of Filtering Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Analysis on Chunk-level Deduplication Extra Data Processing C.1 Chinese Code-Like Domains Annotation . . . . . . . . . . . . . . . . . . . . . . . 4 4 5 5 7 8 10 10 10 11 12 13 13 13 13 16 16 17 18 19 20 27 27 27"
        },
        {
            "title": "Preprint Version",
            "content": "C.2 Code-Related Data from Github Text Files . . . . . . . . . . . . . . . . . . . . . . C.3 Jupyter Notebooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Comparison of RefineCode with The Stack Series Programming Languages Categories E.1 Included Programming Languages . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Excluded Programming Languages . . . . . . . . . . . . . . . . . . . . . . . . . . Raw Code Data Composition Prompts For SFT Synthetic Data 30 31 31 31 31 33"
        },
        {
            "title": "Preprint Version",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have achieved significant success in various domains (Wang et al., 2023; Que et al., 2024; Liu et al., 2024a;c; Wu et al., 2024), particularly in code-related tasks, revolutionizing the current paradigm of software development (Qian et al., 2024; Wang et al., 2024). Code-specific LLMs have emerged as critical area within LLM research, with tools such as ChatGPT, Copilot, and Cursor reshaping the workflows of developers. Despite this, the performance of open-source LLMs focused on code (Li et al., 2023; Tao et al.; Lozhkov et al., 2024a; Zhang et al., 2024a) still falls short compared to state-of-the-art LLMs (Hui et al., 2024; Zhu et al., 2024), largely because these leading models keep their training datasetsan essential factor in LLM developmentproprietary. This lack of transparency limits the broader research communitys ability to establish strong baselines and gain deeper insights into the workings of top-tier code LLMs. To remedy the gap, we set forth three primary goals by releasing OpenCoder and its development material: (1) Firstly, we aim to provide scholars with meticulously curated and fully transparent strong baseline code LLM for research on mechanical interpretability and the data distribution of code LLMs. (2) Secondly, we intend to conduct in-depth investigations into the pretrain and instruction data curation pipeline for the development of stronger code LLMs. (3) Thirdly, by enabling detailed review of the development of the models, we hope to unlock more diverse customized solutions based on transparent code LLM. Through OpenCoder, we strive to stimulate and accelerate the growth of the open-source code LLM community. Our comprehensive set of controlled experiments highlights key design choices for data curation for top-tier code LLMs in different training stages: (1) During the pretraining phase, the importance of data cleaning is highlighted (Zhou et al., 2024), emphasizing the removal of non-informative data such as pure hexadecimal code and excessively short code snippets that do not contribute to the learning process. (2) The impact of deduplication is significant, with file-level deduplication proving to be more effective than repository-level deduplication by maintaining data diversity and enhancing model performance on downstream tasks (Li et al., 2023). (3) The influence of GitHub stars is also examined, revealing that filtering data based on Github star count can possibly reduce data diversity and affect the overall data distribution, contributing to suboptimal result (Allal et al., 2023). (4) In the annealing phase, the use of high-quality data is crucial for further enhancing the models capabilities, indicating that data quality is more important than quantity in the later stages of model training. (5) Finally, during the instruction tuning phase, two-stage instruction tuning strategy is shown to be effective, allowing the model to acquire broad capabilities initially and then refine them with code-specific tasks, resulting in improved performance on both theoretical and practical coding tasks. These five key points underscore the importance of data quality, diversity, and targeted enhancement strategies in developing high-performing code generation model like OpenCoder. This work introduces the OpenCoder, completely open-source Code LLM, built on the transparent data process pipeline and reproducible dataset. As shown in Table 1, We provide the open cookbook to build code LLM from scratch by providing the data cleaning pipeline, reproducible pretraining dataset, large-scale SFT Corpus, and intermediate checkpoints. OpenCoder, through its meticulous data processing and advanced training methods, has surpassed expectations by achieving top-tier results on multiple code LLM evaluation benchmarks. The introduction of the open cookbook of code LLM is designed to push forward the field of code intelligence studies and to encourage its broad use in the community of code intelligence."
        },
        {
            "title": "2 PRETRAINING DATA",
            "content": "Pretraining data plays crucial role in the development of LLMs, where the scale, quality, and diversity of the data greatly affect the models overall performance. Therefore, we introduce an efficient and effective methodology for producing data tailored for our code LLM pretraining. In this section, we will comprehensively illustrate the data processing strategies used in both the general pretraining stage and the annealing stage."
        },
        {
            "title": "Preprint Version",
            "content": "Table 1: The comparison of released resources between our OpenCoder with other popular opensourced code LLMs. HumanEval scores are reported for the corresponding chat models. Models Data Processing Pipeline Reproducible Pretraining Dataset Largescale SFT Dataset (>1M) Intermediate Checkpoints Training Tokens HumanEval Pass@1 Open Model Weights & Reproducible Datasets Open Model Weights 2.5T 4.1T 1.3T 2.5T 6.5T 10.2T 6.0T 23.5T 83.5 72.6 34.1 34.8 56.1 81.1 85.4 88. OpenCoder-8B StarCoder2-15B Crystal-7B CodeLlama-7B CodeGemma-7B DS-Coder-V2-Lite Yi-Coder-9B Qwen2.5-Coder-7B 2.1 REFINECODE Pretraining data forms the foundation for the capabilities of large language models. In the LLM open-source community, The Stack v2 (Lozhkov et al., 2024a) has provided valuable code dataset, which significantly facilitates the training of code LLMs. However, the quality of the training part in The Stack v2 is insufficient to train LLMs with top-rated performance. To address this, we present RefineCode, high-quality, reproducible dataset of 960 billion tokens across 607 programming languages, incorporating over 130 language-specific rules with customized weight assignments. This dataset is composed of two main parts: raw code and code-related web data. Specifically, we collect the raw code primarily from GitHub repositories up to November 2023 with non-GitHub data from The Stack v2. Additionally, the code-related web data is primarily sourced from web corpora. detailed comparison with previous versions of The Stack is provided in the Appendix D. Besides, to ensure both quality and diversity, as shown in Figure 2, we have designed sophisticated data processing pipeline to produce code pretraining corpus. In the following sections, we have provided detailed description of our processing pipeline and the details of our RefineCode dataset. 2.1.1 RAW CODE To ensure the curation of high-quality raw code data, we have developed the code-specific data processing pipeline including modules of preprocessing, deduplication, transformation, filtering, data sampling. The following sections provide the details of these processes. Preprocessing Initially, we exclude files exceeding 8 MB in size, as these are predominantly nontext files, which require considerable resource overhead. Furthermore, given the miscellaneous file types present on GitHub, we restrict our selection to those file types related to programming languages by their file extension referring to linguist1, and filter those types with low capacity or low quality. Finally, we preserve 607 different types of programming language files. comprehensive list of the included and excluded programming languages is provided in Appendix E. Deduplication The purpose of deduplication is to construct an unbiased and diverse training set while significantly reducing the data volume. Owing to the extremely high repetition of the source code in Github, we prioritize the deduplication process early in the pipeline and adopt an aggressive file-level deduplication strategy (see elaborate analysis in Section 6.1). More specifically, we leverage both exact deduplication and fuzzy deduplication methods to eliminate documents containing identical or near-identical code content shown as follows: Exact Deduplication: Due to the prevalence of forking and copy-pasting within the codebase, nearly 75% of files are completely duplicated. On account of this, differing from general deduplication 1https://github.com/github-linguist/linguist/blob/main/lib/linguist/ languages.yml"
        },
        {
            "title": "Preprint Version",
            "content": "Figure 2: The illustration of our pretraining data processing workflow. process, Identity removal is applied towards code data at the first step in this module. We compute the SHA256 hash value for each document, where files with identical hash values are compared, and only the code files with the highest star count as well as the latest commit time are retained. Fuzzy Deduplication: Following the fuzzy deduplication setting in the general data pipeline, we split the raw text into 5-gram pieces, and then calculate the 2048 MinHash functions (Broder, 1997). Additionally, we utilize LSH (Leskovec et al., 2014) by setting bands to 16 and rows to 128, to retain only those distinct files with the highest stars and latest commit time. This process removes 6% file volume. Transformation Filtering is generally adequate for removing files that fail to meet specific criteria. However, certain issues, though small in text size, are pervasive across numerous files. In such cases, it is unacceptable to exclude all those issued files. Instead, we opt to transform these files to rectify the identified issues before the filtering module. Concretely, we implement two types of transformation rules as follows: Copyright Removal: There are over 15% code files including the copyright notices at the beginning of the content like Copyright Intel Corporation (C) 2014-2016, which are highly repetitive and irrelevant to the coding tasks, possibly affecting the performance of the LLM. Consequently, we specifically identified and removed these copyright notices from the initial code comments. PII Reduction: Personally Identifiable Information (PII) encompasses content such as passwords, emails, IP addresses. Training on those data containing PII implies significant privacy risks. Therefore, we employ complex regular expressions to detect such information and replace them with placeholders such as <name> and <password>. Filtering The quality of the original code files on GitHub exhibits significant variability, where lower-quality code potentially hinders the LLM pretraining process. Given the distinct nature of code compared to natural language, the criteria for high-quality code differ significantly from those for natural language. Furthermore, different programming languages also exhibit distinct properties. Based on this, we believe that designing set of detailed heuristic filtering rules tailored specifically to the characteristics of pretraining data is important to enhance the models capabilities. Drawing inspiration from the principles of high-quality code data proposed in Gunasekar et al. (2023), we consider the following guidelines when designing our filters: 1) Filter out files with poor selfcontainment; 2) Filter out files with poor or minimal logical structure; 3) Remove files that deviate significantly from standard formatting. Based on these guidelines and the characteristics of our dataset, our work presents the first heuristic filtering framework by considering the unique characteristics of different programming languages. Based on RedPajama (Computer, 2023), this framework extends and refines the existing rules from StarCoder (Li et al., 2023) to better align with the unique properties of code datasets, resulting in more precise and higher-quality data cleansing. We developed the following three categories of filtering rules:"
        },
        {
            "title": "Preprint Version",
            "content": "Figure 3: Visualization on the PCA data distributions of RefineCode and The Stack v2. 1. Natural Language Filtering Rules: These rules filter data based on common properties for all text files, such as file size, number of lines, and other general metrics. Both text and code files share these filtering rules. 2. General Code Filtering Rules: These rules apply to all code files by filtering data based on general code characteristics, such as the number of variables, average function length, and other common features. 3. Language-Specific Filtering Rules: These rules are designed according to the unique characteristics of specific programming languages, such as the frequency of pass statements in Python or the use of goto statements in C. We have developed these rules for the following eight commonly used programming languages: Python, C, C++, C#, Java, JavaScript, Go, and HTML. Heuristic rules involve extensive threshold setting. When defining these rules and determining thresholds, we consistently follow guiding principle: to remove harmful data as much as possible, while ensuring the overall distribution of the dataset is not significantly affected. We outline our motivations for rule design in Appendix A.1, along with detailed explanation of the tuning process for the corresponding thresholds. Besides, we show the details of several representative rules in Appendix A.2. Data Sampling We try to preserve the original data distribution as much as possible to maximize the utilization of our cleaned high-quality dataset. However, we downsample certain high-resource programming languages before using our dataset in pretraining. Specifically, we downsample Java data from 409GB to 200GB, due to its excessive volume compared to other common languages. Additionally, we downsample HTML data from 213GB to 64GB, as HTML files often contain significant amount of non-informative structured content and lack substantial coding logic. Finally, we produce about 730B tokens in the pretraining stage. Notably, as illustrated in Figure 3, we use PCA to visualize the embeddings extracted from CodeBERT (Feng et al., 2020) for The Stack V2 and RefineCode, and observe clear distinction between these datasets. Specifically, in Figure 3, The Stack V2 data shows greater number of outliers, while the embeddings of RefineCode appear more tightly clustered. Besides, after analyzing the outlier data, we observe the outliers usually show many low-quality patterns, such as pure text comments, hexadecimal-only data, and excessively short code lacking computational logic, which can distort the distribution of the pretraining dataset and ultimately hinder the efficiency of pretraining. 2.1.2 CODE-RELATED WEB DATA Inspired by the DeepSeekMath (Shao et al., 2024), we collect high-quality code-related data corpus from the Common Crawl dataset. Unlike the previous practice in the math domain, due to the lack of open-source fine-gained code corpus, we first annotate 500,000 high-quality code-like data from CommonCrawl using the Autonomous Data Selection (Zhang et al., 2024b) method as seed data for training fasttext(Joulin et al., 2016). These data serve as the initial code seed corpus."
        },
        {
            "title": "Preprint Version",
            "content": "Table 2: The Composition of RefineCode. Category Data Source # Tokens Percentage Raw Code Data Code-related Web Data Github Code Jupyter Notebooks The Stack v2 Processed CC Processed SkyPile Processed FineWeb OpenSource Data Processed AutoMathText 755 11 120 13 3 55 3 78.4% 1.1% 12.5% 1.4% 0.3% 5.7% 0.3% As shown in Figure 2, the processing pipeline of code-related web data comprises four main components: 1) FastText Model Training: To maintain controllable vocabulary size in fastText and enable tokenization of Chinese texts using spaces, we first apply the BPE (Byte Pair Encoding) tokenizer to segment the corpus. Subsequently, the open-source FastText framework is utilized for model training. 2) Recall From Common Crawl: We perform recall on Common Crawl to generate the code-related web corpus. 3) Code-related Domain Discovery, we conduct statistical analysis of the recalled data by domain URLs, and define domain as web pages with the same base URL(e.g. stackoverflow.com), where domains with over 10% of web pages are classified as coderelated. Note that given the scarcity of Chinese data, we provide detailed annotations of domain names related to code and mathematics within the CommonCrawl dataset in the Appendix C. 4) Url Annotation: We manually annotate the URLs associated with code content within these identified domains. For instance, we have identified all content under stackoverflow.com/questions as computer technology questions. Then, we include samples with URLs matching stackoverflow.com/questions, which are not correctly classified by fastText, into our code seed corpus. After three iterations, we obtain about 220GB code-related web data. Note that as the iteration progresses, the quantity and diversity of the seed corpus will be better. We also apply the same recall pipeline to FineWeb (Penedo et al., 2024a), Skypile (Wei et al., 2023a) and web part of AutoMathText (Zhang et al., 2024b) and produce 330GB code-related web data in total. Furthermore, we observe that only very small portion of the textual data in GitHub is also related to natural language text. Therefore, we also train classifier to determine whether the text is code-related and obtain an additional 178GB code-related web data. 2.1.3 SUMMARY Ultimately, we curated high-quality code pretraining dataset, RefineCode, consisting of about 960 billion tokens. The composition of the data sources is illustrated in Table 2, while the distribution of different program languages is displayed in Figure 4. For more details regarding the data composition of different program languages, please refer to Appendix F. To demonstrate the efficacy of RefineCode, we train 1.5B code LLM up to 600B using data from RefineCode and the training subset of The Stack v2 respectively. The results in Figure 1, indicate that RefineCode significantly improves training efficiency compared to The Stack v2, highlighting the superiority of our dataset. 2.2 ANNEALING DATA The annealing stage can be seen as bridge between the general pretraining stage and the supervised fine-tuning (SFT) stage. Following the training strategy in MiniCPM (Hu et al., 2024), our model also undergoes rapid learning rate annealing phase after the general pretraining stage, where very high-quality training data is used to further enhance the models capabilities. In addition to the RefineCode from the original distribution, we further incorporated the Algorithmic Corpus and synthetic data during the annealing phase. The detailed data mixture can be found in Table 3. Original Distribution Data In the annealing stage, its necessary to ensure that the overall data distribution remains similar to the pretraining phase. significant distribution shift can lead to catastrophic forgetting in the models knowledge, and we ensure that 84% of the annealing data"
        },
        {
            "title": "Preprint Version",
            "content": "Figure 4: The distribution of top program languages in RefineCode (before data sampling). comes from the original distribution of RefineCode. Note that given the limited computing budget available, this mixture ratio might not be ideal. Algorithmic Corpus Algorithmic code files exhibit strong code logic and minimal dependency on external files, demonstrating excellent self-containment. Additionally, these files are more aligned with the distribution of smaller, independent tasks commonly encountered in real-world interactive scenarios. Therefore, we sample certain proportion of the original pretraining data that contains keywords such as leetcode,, def solution, or class solution to create this corpus. Synthetic Data High-quality pretraining data rewriting is also extremely important during the pretraining stage, which helps the model memorize and embed knowledge for efficient retrieval (AllenZhu & Li, 2023). We select Algorithmic Corpus as the seed because it encompasses wide range of algorithmic logic. We employed two forms of rewriting enhancement: Verified Code Snippets and Code Textbooks. 1. High Quality Code Snippet: Inspired by the synthetic CodeExercises dataset in Gunasekar et al. (2023), we utilized the algorithmic corpus as seeds and employ strong LLM to synthesize batch of self-contained independent functions along with their corresponding test cases. We retained the data that successfully passed the test cases and included them in the annealing stage dataset. This approach was similarly extended to support multiple program languages. 2. Code Textbooks: To enable the model to understand code from multiple perspectives, we constructed educational text snippets based on the hqcode 2 dataset using Qwen2-72BInstruct (Yang et al., 2024). Hqcode is multilingual code dataset synthesized with GPT4o-Mini, where each entry describes an independent task and provides corresponding function as solution. We engaged LLMs to perform interactive analysis on the code within this dataset, extracting and elaborating on abstract code knowledge. This approach aims to enable the model to learn code from diverse perspectives. 2https://huggingface.co/datasets/yuxiang630/hqcode"
        },
        {
            "title": "Preprint Version",
            "content": "Table 3: Detailed data mixture for annealing data. Category Dataset"
        },
        {
            "title": "High Quality Code Snippet\nCode Textbooks",
            "content": "# Token"
        },
        {
            "title": "2.71 B\n0.91 B",
            "content": "Table 4: Overview of the key hyperparameters of OpenCoder, including 1.5B and 8B. OpenCoder-1.5B OpenCoder-8B 24 2240 14 14 Layers Model Dimension Attention Heads Key / Value Heads Activation Function Vocab Size Positional Embedding RoPE(θ = 10000) RoPE(θ = 500000) Context Window Size 32 4096 32 SwiGLU 96640"
        },
        {
            "title": "3 PRETRAINING",
            "content": "3.1 MODEL ARCHITECTURE In this section, we provide detailed overview of our model architecture. As shown in Table 4, the models are available in two sizes: 1.5 billion and 8 billion parameters. The 1.5 billion model consists of 24 layers with hidden size of 2240, 14 attention heads, and 14 key/value heads, supporting context window size of 4096. The 8 billion model architecture closely follows the Llama-3.1-8B architecture, with 32 layers, hidden size of 4096, and 8 attention heads. Both models use the SwiGLU activation function and have vocabulary size of 96,640, using the tokenizer proposed in ?. 3.2 TRAINING DETAILS The training process, based on the aforementioned model architecture, involved several critical details. The dataset encompassed both Chinese and English languages, alongside 607 programming languages, the complete list of which is provided in Appendix E. For the 1.5B model, due to the incomplete data curation, training was performed on 2 trillion tokens over four epochs. Following the pretraining phase, we conducted annealing training on an additional 100 billion tokens. The WSD learning schedule, referenced in MiniCPM (Hu et al., 2024), was employed, featuring warm-up phase of 2,000 steps across 8 billion tokens. The peak learning rate was 3e-4, which remained constant after the warm-up and subsequently decayed exponentially to 1e-5 during the annealing phase. micro-batch size of 4 and global batch size of 1024 were used. Training was conducted using Megatron-LM (Shoeybi et al., 2020) with distributed optimization and DDP gradient overlap on cluster of 256 H800 GPUs over total of 109.5 hours, equating to 28,034 GPU hours. For the 8B model, the WSD learning schedule was again employed with warm-up phase covering 8 billion tokens over 2,000 steps. This model was trained for 3.5 epochs on 2.5 trillion tokens, followed by decay phase with an additional 100 billion tokens. Unlike the 1.5 billion model, which lacked code-related recall data due to incomplete data processing, the 8 billion model incorporated this data during training. The learning rate schedule mirrored that of the 1.5B model. The microbatch size was set to 1, with TP of 2 and sequence length of 8192. The global batch size was 1024. Training was conducted on cluster of 512 H100 GPUs over 187.5 hours, totaling 96,"
        },
        {
            "title": "Preprint Version",
            "content": "Figure 5: The illustration of our instruction data synthesis workflow. GPU hours. It is noteworthy that the first 130,000 steps were trained with sequence length of 4096 and global batch size of 2048."
        },
        {
            "title": "4 POST TRAINING",
            "content": "4.1 DATA COMPOSITION Open-source Training Data To enhance the model training, we collect the open-source instruction corpora from the websites, including Evol-Instruct3 (Luo et al., 2024), Infinity-Instruct4, McEval5 (Chai et al., 2024; Yang et al., 2021), where the instruction data is created from the multilingual raw code snippet by language sampling with the fixed ratio. We employ an LLM to perform binary classification on the content of Infinity-Instruct, aiming to extract the segments specifically related to the code. Additionally, we sample real user queries from WildChat (Zhao et al., 2024) and Code-290k-ShareGPT6, extracting code-related dialogue histories using LLM and subsequently performing data cleaning. For low-quality responses, we employ robust LLM to regenerate the content, enhancing the overall data quality. This RealUser-Instruct dataset not only exhibits high diversity but also aligns more closely with real-world problem complexity, focusing on addressing practical issues in authentic scenarios. Educational Instruction Synthesis To ensure the diversity and richness of instruction-tuning datasets, prior work explores using code snippets sampled from real-world sources as seed data (Wei et al., 2023b), subsequently used to synthesize question-answer pairs. This approach is widely 3https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1 4https://huggingface.co/datasets/BAAI/Infinity-Instruct 5https://huggingface.co/datasets/Multilingual-Multimodal-NLP/ McEval-Instruct 6https://huggingface.co/datasets/cognitivecomputations/ Code-290k-ShareGPT-Vicuna"
        },
        {
            "title": "Preprint Version",
            "content": "adopted in the development of large language models. In synthesizing instruction-tuning datasets for Python code, we enhance the effectiveness of this method. Specifically, we observe that the educational value of the synthesized data largely depends on the quality of the seed data. Thus, during the seed data selection phase, we use scorer model where the input is code snippet to identify high-quality seed data. By using only high-quality seed data, we ensure that the resulting instruction-tuning dataset includes more educational example responses. Subsequently, we use teacher model to generate multiple test cases for the code sections in each problem. These test cases are appended to the code snippets and executed using Python interpreter. Only the data samples that successfully pass the tests are retained. By using this strategy, we maximize the likelihood that the generated data is both syntactically and semantically sound, thereby enhancing the reliability of the dataset. Package-related Instruction Synthesis Due to significant amount of outdated package usage in the pre-training data, LLM may sometimes employ methods from older versions of libraries when generating code, leading to suboptimal performance in tasks involving package invocation. For example, Pythons extensive ecosystem of librariessuch as NumPy, pandas, and TensorFloware frequently updated, with new functions, methods, and best practices emerging over time. As result, when users query NumPy, the model may give incorrect answers based on outdated information. Furthermore, if the model is significantly affected by outdated library syntax, it may fail to generate correct code, leading to errors when the code is executed in Python interpreter. This problem undermines the models ability to use tool calls to improve performance. To mitigate the impact of outdated programming syntax and obsolete external library interfaces in the pre-training dataset, we synthesized tool usage instruction tuning dataset using up-to-date external library documentation. Specifically, we analyzed commonly used external Python libraries and retrieved API signatures and usage examples for widely used syntax and tools via PyDoc. This information was sent to prompt teacher model that generated accurate and up-to-date question-answer pairs reflecting current usage. By fine-tuning the model on curated set of code that includes up-to-date usage of these libraries, we ensured that it could provide accurate, contemporary answers to questions about using them effectively. This is particularly important given the rapid pace of change in software development, where outdated code and obsolete practices can lead to incorrect answers and inefficient solutions. Large-scale Diverse Instruction Synthesis Following the previous work (Yue et al., 2024), to increase the diversity of the instruction dataset, we create large-scale instruction data synthesis framework. The framework for synthesizing code instruction data using LLMs incorporates the following key components: (1) An LLM is used first to clean the irrelevant context (e.g. advertisements on the web) in the websites and select useful sentences as the seed for further question generation. (2) task specification module defines programming languages, difficulty levels, and coding task types, utilizing configuration file for easy customization. The prompt engineering component employs template-based system to generate diverse, contextually rich prompts, incorporating realworld scenarios and best practices in software development. We set temperature = 1.0 for diverse questions. (3) An advanced LLM with more parameters first generates the created questions and then generates the corresponding answers. The validation module combines automated code execution and unit testing to check the correctness. (4) Then an LLM is adopted to refine the response by adding code comments and more explanation. 4.2 TWO-STAGE INSTRUCTION-TUNING In developing code LLM, particularly in computer science and software development, it is essential to ensure that the model excels in both theoretical knowledge and practical coding tasks. To address both needs, we implemented two-stage instruction fine-tuning process. The detailed composition of instruction tuning is presented in Table 5. The first stage of this fine-tuning process focused on synthesizing question-answer (QA) pairs related to theoretical computer science. Building on general-purpose pre-training data, we created specialized dataset that enabled the model to develop deeper understanding of theoretical computer science, such as algorithms, data structures, and networking principles. By fine-tuning the model with domain-specific QA pairs, we ensured that it could respond with greater precision to questions about concepts such as binary search trees, dynamic programming, and the intricacies of object-oriented design patterns."
        },
        {
            "title": "Preprint Version",
            "content": "Table 5: Detailed data composition of our two-stage instruction-tuning. Stage Data Source # Examples Stage1 Stage RealUser-Instruct Large-scale Diverse-Instruct Filtered Infinity-Instruct McEval-Instruct Evol-Instruct Educational-Instruct Package-Instruct"
        },
        {
            "title": "36 K\n111 K\n110 K\n110 K",
            "content": "In the second stage of the fine-tuning process, we shifted focus from theoretical knowledge to practical coding tasks. In this stage, we used high-quality code from GitHub to create dataset aimed at improving the models ability to generate and work with code. By fine-tuning the model on highquality code from GitHub, we ensured it was exposed to real-world examples of well-maintained and formatted code. One key advantage of using high-quality code in the fine-tuning process is that it enhances the models ability to generate code that is both syntactically and semantically correct. The two-stage fine-tuning approach allows the model to excel in theoretical knowledge and practical coding tasks, thereby avoiding the limitations of focusing on only one area. Models that only prioritize theory may struggle with coding, while those focused solely on code generation may lack depth in explaining complex concepts. By refining both areas, the model becomes technically proficient and versatile, able to meet the needs of developers, beginners, and professionals alike. 4.3 TRAINING DETAILS In the first stage of SFT, we trained for one epoch with batch size of 4096, learning rate (LR) of 2e-5, warmup steps set to 100, and cosine learning rate scheduler.In the second stage of SFT, we trained for three epochs using batch size of 512, learning rate of 5e-5, with 100 warmup steps, and the same cosine learning rate scheduler. 4.4 DECONTAMINATION We applied strict data deduplication for all SFT data. Specifically, we removed any data containing the entry points corresponding to test sets such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). Additionally, we performed 10-gram deduplication, removing any data with 10-gram overlap with the test sets."
        },
        {
            "title": "5 EXPERIMENTAL RESULTS",
            "content": "In this section, we conduct comprehensive and fair evaluation to demonstrate that the model we constructed using cleaned and synthesized data performs comparably to other closed large language models. We also compared the most widely used and powerful open-source language models, including the Crystal and StarCoder series. To further highlight the practicality and effectiveness of our models, we focus on tasks such as code generation, code completion, and code understanding. 5.1 EVALUATION ON BASE MODELS For base models, we focus on evaluating their code completion ability. Code completion is fundamental capability that enables code models to tackle complex tasks. This evaluation goal aligns with our optimization objective in the annealing stage, as code completion can be regarded as special case of the code generation task. To ensure the reproducibility of all results, we used publicly available LLM evaluation framework OpenCodeEval7. For comparing models, we compare open-coder-1.5B with state-of-the-art small language models. 7https://github.com/richardodliu/OpenCodeEval"
        },
        {
            "title": "Preprint Version",
            "content": "Table 6: Performance of various base models on HumanEval, MBPP, and the complete task of BigCodeBench. Models trained on reproducible datasets are marked with green. Model Size HumanEval HE HE+ MBPP MBPP+ 3-shot MBPP BigCodeBench Full Hard 1B+ Models DeepSeek-Coder-1.3B-Base Yi-Coder-1.5B CodeGemma-2B Qwen2.5-Coder-1.5B StarCoder2-3B OpenCoder-1.5B-Base 1.3B 34.8 1.5B 41.5 2B 31.1 1.5B 43.9 3B 31.7 1.5B 54.3 26.8 32.9 16.5 36.6 27.4 49. 6B+ Models CodeLlama-7B CodeGemma-7B DS-Coder-6.7B-Base DS-Coder-V2-Lite-Base (MoE) CodeQwen1.5-7B Yi-Coder-9B Qwen2.5-Coder-7B Crystal-7B StarCoder2-7B StarCoder2-15B OpenCoder-8B-Base 7B 33.5 7B 39.0 6.7B 47.6 16B 40.9 7B 51.8 9B 53.7 7B 61.6 7B 22.6 7B 35.4 15B 46.3 8B 66.5 26.2 32.3 39.6 34.1 45.7 46.3 53.0 20.7 29.9 37.8 63.4 55.6 27.0 51.1 69.2 60.2 70.6 55.3 50.5 70.2 71.9 72.2 48.4 76.9 38.6 54.4 66.2 79. 46.9 22.2 43.1 58.6 49.1 58.7 46.8 40.7 56.6 59.4 60.2 40.7 62.9 31.7 45.6 53.1 70.4 46.2 51.6 45.4 59.2 46.4 51.8 41.4 55.0 60.6 62.6 61.8 69.4 68.8 31.0 55.2 15.2 60.6 26.1 23.5 23.9 34.6 21.4 24.5 28.7 38.3 41.1 30.6 45.6 42.9 45.8 10.8 27.7 38.4 40. 3.4 3.4 7.4 9.5 4.7 5.4 5.4 10.1 11.5 8.1 15.6 14.2 16.2 4.1 8.8 12.2 9.5 HumanEval & MBPP We selected two widely used code completion benchmarks to evaluate OpenCoder, HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021). To further enhance the accuracy of the evaluation, EvalPlus (Liu et al., 2024d) extends HumanEval and MBPP into HumanEval+ and MBPP+ by adding unique and challenging test cases and correcting inaccurate ground-truth solutions. These results can be used to indicate the models ability to understand and apply basic Python data structures and knowledge of algorithms. For HumanEval, we report the 0-shot results. For MBPP, we report 3-shots on 500 questions in the test split from original dataset, while the others following EValPlus report results on 378 questions in the sanitized part. BigCodeBench BigCodeBench (Zhuo et al., 2024) is challenging benchmark for code completion, designed to assess models on their ability to handle complex instructions and make accurate function calls across diverse external libraries. In the Completion setup, models are provided with function signature and related documentation to generate appropriate code, along with unit test for the completed function. Covering range of practical programming tasks, it evaluates models ability to handle real-world scenarios involving complex, task-specific libraries. 5.2 EVALUATION ON INSTRUCT MODEL LiveCodeBench LiveCodeBench is comprehensive, contamination-free benchmark that assesses the reasoning and problem-solving abilities of highly complex algorithmic tasks. The benchmark is continuously updated with new problems from platforms such as LeetCode, AtCoder, and CodeForces, ensuring the challenges remain current and diverse. LiveCodeBench provides robust measure of models ability to handle sophisticated logical processes, which are essential in competitive programming contexts. The instruct models are evaluated on the 2305-2409 data split. MultiPL-E MultiPL-E extends the HumanEval benchmark to evaluate the code generation capabilities of large language models across multiple languages. MultiPL-E translates tasks into languages such as C++, Java, PHP, TypeScript, C#, Bash, and JavaScript, providing consistent basis for assessing how models apply their programming skills across different syntaxes and paradigms. We follow the evaluation code of Qwencoder8 to systematically measure performance in each lan8https://github.com/QwenLM/Qwen2.5-Coder"
        },
        {
            "title": "Preprint Version",
            "content": "Table 7: Performance of various chat models on HumanEval, MBPP, the instruct task of BigCodeBench and LiveCodeBench. Models trained on reproducible datasets are marked with green. Model Size HumanEval HE MBPP HE+ MBPP MBPP+ Full BigCodeBench LiveCodeBench 1B+ Models DS-coder-1.3B-Instruct Qwen2.5-Coder-1.5B-Instruct Yi-Coder-1.5B-Chat OpenCoder-1.5B-Instruct 1.3B 65.2 1.5B 70.7 1.5B 67.7 1.5B 72.5 61.6 66.5 63.4 67.7 61.6 69.2 68.0 72.7 6B+ Models DS-Coder-V2-Lite-Instruct CodeLlama-7B-Instruct CodeGemma-7B-It DS-Coder-6.7B-Instruct Yi-Coder-9B-Chat CodeQwen1.5-7B-Chat Qwen2.5-Coder-7B-Instruct CrystalChat-7B StarCoder2-15B-Instruct-v0.1 OpenCoder-8B-Instruct 16B 81.1 7B 45.7 7B 59.8 6.7B 78.6 9B 82.3 7B 86.0 7B 88.4 7B 34.1 15B 72.6 8B 83.5 75.0 39.6 47.0 70.7 72.6 79.3 84.1 31.7 63.4 78.7 82.3 39.9 69.8 75.1 81.5 83.3 83.5 39.1 75.2 79.1 52.6 59.4 59.0 61.9 68.8 33.6 59.0 66.1 69.3 71.4 71.7 32.7 61.2 69. 22.8 32.5 24.0 33.3 36.8 21.9 32.3 35.5 38.1 39.6 41.0 26.7 37.6 40.3 Hard 3.4 6.8 6.8 11.5 16.2 3.4 7.4 10.1 11.5 18.9 18.2 2.3 12.2 16.9 Avg 9.3 15.7 11.6 12.8 24.3 2.8 14.7 20.5 23.4 20.1 37.6 6.1 20.4 23.2 Table 8: Performance of various chat models on the MultiPL-E benchmark across different programming languages. Model Size Python Java C++ C# TS JS PHP Bash Average DS-Coder-1.3B-Instruct Yi-Coder-1.5B-Chat Qwen2.5-Coder-1.5B-Instruct OpenCoder-1.5B-Instruct DS-Coder-6.7B-Instruct DS-Coder-V2-Lite-Instruct CodeLlama-7B-Instruct CodeGemma-7B-It CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat Qwen2.5-Coder-7B-Instruct OpenCoder-8B-Instruct 1B+ Models 51.9 51.9 55.7 64.6 45.3 49.1 50.9 50.9 6B+ Models 68.4 76.6 32.2 48.1 70.9 76.0 76.5 72. 63.4 75.8 28.6 46.6 72.0 67.7 75.6 61.5 65.2 67.7 71.2 72.5 78.6 81.1 45.7 59.8 83.5 85.4 87.8 83.5 1.3B 1.5B 1.5B 1.5B 6.7B 16B 7B 7B 7B 9B 7B 8B 55.1 57.6 64.6 61. 72.8 76.6 32.9 51.9 75.9 76.6 80.3 75.9 59.7 57.9 61.0 63.5 67.2 80.5 39.0 54.7 76.7 72.3 81.8 78.0 52.2 59.6 62.1 62.1 72.7 77.6 43.5 54.0 77.6 78.9 83.2 79.5 45.3 52.2 59.0 55. 68.9 74.5 31.7 46.6 73.9 72.1 78.3 73.3 12.7 19.0 29.1 29.7 36.7 43.0 10.1 10.1 41.8 45.6 48.7 44.3 48.4 51.9 56.7 57.5 66.1 73.2 33.0 46.5 71.6 71.8 76.5 71.0 guage, providing insights into the adaptability and code generation accuracy of LLMs in multilingual context. McEval The comprehensive multilingual code evaluation benchmark McEval (Chai et al., 2024) employed detailed assessment of OpenCoders programming capabilities across 40 languages. In contrast to MultiPL-E, this benchmark is not derived from HumanEval or MBPP. Figure 6 depicts the results of the multilingual generation task for OpenCoder-8B-Instruct, which comprises nearly 2,000 samples. The figure illustrates that the model exhibits superior multilingual performance compared to other open-source models of comparable size. MdEval OpenCoder is also evaluated on the comprehensive multilingual code debugging benchmark MdEval (Liu et al., 2024e) across 18 languages. In contrast to McEval, this benchmark focuses on the assessment of code debugging, especially for language-specific bugs. Figure 7 shows the results of the multilingual automated program repair task for OpenCoder-8B-Instruct, which comprises nearly 1.2K samples, which demonstrates that OpenCoder can effectively find the bugs and fix them compared to other open-source models of comparable size."
        },
        {
            "title": "Preprint Version",
            "content": "Figure 6: The McEval performance of OpenCoder-8B-Instruct in comparison to other open-source code models of comparable size. Figure 7: The MdEval performance of OpenCoder-8B-Instruct in comparison to other open-source code models of comparable size."
        },
        {
            "title": "6 ANALYSIS",
            "content": "6.1 ANALYSIS OF THE DEDUPLICATION LEVEL Recent studies (Lee et al., 2021) have demonstrated the significant performance improvements that can be achieved by deduplicating training datasets for LLM, where MinHash combined with LSH has emerged as the predominant method for deduplication in code training datasets (Li et al., 2023;"
        },
        {
            "title": "Preprint Version",
            "content": "Lozhkov et al., 2024a; Guo et al., 2024; Mishra et al., 2024). Recently, DeepSeekCoder (Guo et al., 2024) claims that deduplication is performed at the repository level. However, we conduct extensive experiments on the Python corpus of RefineCode by performing deduplication at both the file and repository levels, respectively.Specifically, the deduplication is conducted at both the file level and repository level across the 485 million Python files available on GitHub, respectively, and then we train two 1.5B LLMs, where the findings are as follows: First, in Table 9, the number of retained tokens at the repository level deduplication is almost three times that of the file level deduplication. Second, in Figure 8, we compare the downstream performance of the two datasets (i.e., HumanEval and MBPP) during pretraining and observe that the performance of file level deduplication is better than the performance of repository level deduplication lot. Third, for repository level deduplication, we observe that substantial portion of 52 billion tokens exhibits complete character-level equivalence with another file. Fourth, when conducting file-level deduplication as post-processing step on the results of repository-level deduplication, we find that approximately 68 billion tokens (about 68.4% of the data) could be further deduplicated. Our further investigation into chunk-level deduplication revealed no observable benefits, as detailed in the Appendix B.In summary, for largescale code datasets, performing exact deduplication followed by file-level fuzzy deduplication is an efficient and CPU-saving approach. Table 9: The statistics for file level deduplication and repository level deduplication on Python code. Rows for file level and repository level represent the number of files and repositories, respectively. Deduplication Level # Total Rows # Retained Rows # Retained Tokens File level Repository level 485,817,123 11,037,352 30,488,834 7,480,488 32.74 99.47 Figure 8: Impact of using different deduplication strategies. 6.2 ANALYSIS ON THE IMPORTANCE OF HIGH-QUALITY DATA IN THE ANNEALING PHASE During the annealing phase of training, we conduct experiments by using different annealing data with different data distributions as shown in Figure 9. Similarly, we still train two 1.5B LLMs, where the first is trained by our original annealing data previously introduced and the second is trained by the data without using the high-quality data (i.e., Algorithmic Corpus and the Synthetic Data). From Figure 9, we observe that the performance drops lot when the high-quality training data is removed, which demonstrates the effectiveness of our constructed high-quality data in the annealing phase."
        },
        {
            "title": "Preprint Version",
            "content": "Figure 9: Impact of using high-quality data in the annealing stage. Figure 10: Impact of star-based data filtering on model performance. 6.3 ANALYSIS ON THE EFFECT OF GITHUB STARS Following SantaCoder (Allal et al., 2023), we also conduct experiments by comparing the performance trained by original code data and the filtered code data based on GitHub Stars, respectively. Specifically, as shown in Figure 10, we train two 1.5B LLMs, where one is trained original data and another is trained by data filtered by GitHub stars (stars>=5), and we have the following findings. First, in Figure 10, we observe that the LLM trained by original data is better than the LLM trained by filter data, which is similar to the results of SantaCoder. Second, in Figure 11, we also provide the training losses of these two LLMs and observe that the loss of the LLM trained by filtered data is fewer than the LLM trained by original data. For this phenomenon, we assume that the data quality is better when using stars as the filter signal, but the diversity is relatively limited compared to the original data. Besides, we find that this effect can be predicted from single data distribution through visualization alone, without the need for training. As dedicated in Figure 11, star filter significantly impacts the overall data distribution, compromising data diversity. Upon closer examination of the filtered data, we find that it still contains considerable amount of well-structured, algorithmically rich code. Therefore, we argue that using stars as filtering criterion is not an optimal choice."
        },
        {
            "title": "Preprint Version",
            "content": "Figure 11: Left figure: Losses of using different training data with different distributions. Right figure: Visualization of the embeddings for original data and filtered data. Note that filtering based on the number of stars can reduce data diversity and result in lower overall loss for pretraining. 6.4 ANALYSIS ON THE TWO-STAGE INSTRUCTION TUNING STRATEGY We compared three tuning strategies for OpenCoder-1.5B-Instruct: Stage1, Stage1+Stage2, and Mix Training. Table 10 indicates that the two-stage SFT training can bring consistent improvement in both public benchmarks and real-world scenarios. We observe that the data in Stage 1 exhibits significant diversity, though with relatively lower average quality. In contrast, the data in Stage 2 consists of high-quality, code-specific SFT data. This two-stage SFT strategy allows for the acquisition of broad capabilities in Stage 1, followed by targeted enhancement of code-related tasks in Stage 2. Besides, similar to Chatbot Arena, we adopt the CodeArena test set covering nearly 400 humancreated samples to emulate user code-related prompts in realistic environments. We use GPT-4 as the baseline and use GPT-4 to judge which LLM has better response, where the reported results are win rate compared to the GPT-4. Table 10 demonstrates the importance of the two-stage SFT training strategy in the algorithmic benchmarks Evalplus and the realistic benchmarks CodeArena. Table 10: Performance of different training strategies across benchmarks. Mix Training refers to the process of combining and shuffling the data from Stage 1 and Stage 2 for joint training. HE HE+ MBPP MBPP+ BigCodeBench Code Arena Stage1 Stage1 + Stage2 Mix Training 52.4 70.1 55.5 48.1 64.0 51.2 68.7 74.6 52.0 57.4 64.8 58.7 22.1 31.5 23.9 5.3 6.9 3."
        },
        {
            "title": "7 RELATED WORK",
            "content": "Code Large Language Models. The remarkable progress in generative language modeling has sparked numerous studies on AI applications for software engineering (Black et al., 2022; Brown et al., 2020; Radford et al., 2019; Touvron et al., 2023; Sun et al., 2024; Chai et al., 2024; Liu et al., 2024e). While proprietary models (Achiam et al., 2023; Chen et al., 2021; Chowdhery et al., 2023) achieve significant performance improvements in many code-related benchmark datasets (Chen et al., 2021; Hendrycks et al., 2020), the inaccessible model checkpoints hinder further innovation. In contrast, the research community has introduced several open-source models (e.g., CodeGen (Nijkamp et al., 2023a;b), StarCoder (Li et al., 2023; Lozhkov et al., 2024b), CodeLlama (Roziere et al., 2023) and DeepSeekCoder (Guo et al., 2024)), which greatly foster continued innovation in the field. Code Benchmarks. Code generation models can be leveraged to address programming challenges by interpreting and acting upon input specifications, which involves the automatic creation of programming solutions based on given problem descriptions (Athiwaratkun et al., 2023; Austin et al.,"
        },
        {
            "title": "Preprint Version",
            "content": "2021; Chen et al., 2021; Gu et al., 2024; Lai et al., 2023; Chai et al., 2024; Muennighoff et al., 2024a; Sun et al., 2024). Moreover, many benchmark datasets have been proposed to comprehensively assess code large language models, such as code retrieval (Husain et al., 2019; Lu et al., 2021), code translation (Yan et al., 2023), code efficiency (Du et al., 2024) and the challenging repository-level code completion tasks (Allal et al., 2023; Liu et al., 2023a; Shrivastava et al., 2023; Zhang et al., 2023; Deng et al., 2024; Liu et al., 2024b; Deng et al., 2024). Open Large Language Models. Recently, many open-sourced LLMs have been proposed to empower the open research community and inspire new wave of innovation. Specifically, many LLMs (e.g., LLaMA (Touvron et al., 2023), Mistral (Jiang et al., 2023), Qwen (Bai et al., 2023), ChatGLM (GLM, 2024)), pretraining datasets (e.g., RedPajama (Computer, 2023), SlimPajama (Soboleva et al., 2023), FineWeb (Penedo et al., 2024b)), and chat-related datasets (e.g., WildChat (Zhao et al., 2024), LMSYS-Chat-1M (Zheng et al., 2023)) are open-sourced, which greatly inspire more research innovations and accelerate the improvements of LLMs. Notably, several fully open LLMs have been introduced, which provide as many details as possible to reproduce high-performance LLMs. For example, in general LLMs, OLMo (Groeneveld et al., 2024), OLMoE (Muennighoff et al., 2024b), LLM360 (Liu et al., 2023b) and MAP-Neo (Zhang et al., 2024a) are proposed. These models release not only the final model checkpoint but also many training details (e.g., the data processing pipeline, the pretraining data, and the intermediate checkpoints). In code LLMs, StarCoder (Allal et al., 2023) and StarCoderV2 (Lozhkov et al., 2024a) also release high-quality code pretraining corpus."
        },
        {
            "title": "8 CONCLUSION & FUTURE WORK",
            "content": "In this paper, we present OpenCoder, an open LLM specialized in code intelligence that achieves top-tier performance. To advance research transparency and reproducibility, we release our complete training materials, including: the complete data processing pipeline, the reproducible pretraining dataset, the open code SFT dataset, rigorous experimental ablation results, detailed training protocols and intermediate checkpoints. The performance of OpenCoder is on par with leading proprietary models, and it surpasses most previous open-source models at the both 1B+ and 6B+ parameter scale. Furthermore, we conducted series of ablation analyses on each phase of the code LLM training process, providing valuable insights and recommendations for future code LLM training. We hope the release of OpenCoder can democratize access to all aspects of top-tier code LLM, serving as both powerful model and an open foundation to accelerate research and enable reproducible advancements in code AI. In the future, we will continue to update our model and data consistently, aiming to improve OpenCoders performance and expand its influence within the community. Our commitment is to ensure that OpenCoder remains at the forefront of technological advancements, providing users with the most efficient and accurate coding assistance possible. By regularly incorporating user feedback and the latest research findings, we strive to build more robust and versatile platform that can cater to the diverse needs of developers around the world."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. 2023. Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: dont reach for the stars! arXiv preprint arXiv:2301.03988, 2023. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316, 2023. Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta"
        },
        {
            "title": "Preprint Version",
            "content": "Sengupta, Dan Roth, and Bing Xiang. Multi-lingual evaluation of code generation models. In The Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=Bo7eeXm6An8. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. ArXiv preprint, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108. 07732. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoXIn Proceedings of BigScience Episode 20B: An open-source autoregressive language model. #5 Workshop on Challenges & Perspectives in Creating Large Language Models, pp. 95 136, virtual+Dublin, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9. Andrei Z. Broder. On the resemblance and containment of documents. In Bruno Carpentieri, Alfredo De Santis, Ugo Vaccaro, and James A. Storer (eds.), Compression and Complexity of SEQUENCES 1997, Positano, Amalfitan Coast, Salerno, Italy, June 11-13, 1997, Proceedings, pp. 2129. IEEE, 1997. doi: 10.1109/SEQUEN.1997.666900. URL https://doi.org/10. 1109/SEQUEN.1997.666900. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCanLanguage models are few-shot dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke Jin, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, et al. Mceval: Massively multilingual code evaluation. arXiv preprint arXiv:2406.07436, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Together Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Ken Deng, Jiaheng Liu, He Zhu, Congnan Liu, Jingxin Li, Jiakai Wang, Peng Zhao, Chenchen Zhang, Yanan Wu, Xueqiao Yin, Yuanxing Zhang, Wenbo Su, Bangyu Xiang, Tiezheng Ge, and"
        },
        {
            "title": "Preprint Version",
            "content": "Bo Zheng. R2c2-coder: Enhancing and benchmarking real-world repository-level code completion abilities of code large language models. ArXiv, abs/2406.01359, 2024. Mingzhe Du, Anh Tuan Luu, Bin Ji, Qian Liu, and See-Kiong Ng. Mercury: code efficiency benchmark for code large language models. arXiv preprint arXiv:2402.07844, 2024. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155, 2020. Team GLM. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah Smith, and Hannaneh Hajishirzi. OLMo: Accelerating the science of language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1578915809, Bangkok, Thailand, August 2024. Association for Computational Linguistics. Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. Cruxeval: benchmark for code reasoning, understanding and execution. 2024. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. 2020. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. arXiv preprint Codesearchnet challenge: Evaluating the state of semantic code search. arXiv:1909.09436, 2019. Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv, 2023. Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou, and Tomas Mikolov. Fasttext.zip: Compressing text classification models. arXiv: Computation and Language,arXiv: Computation and Language, Nov 2016. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, WenTau Yih, Daniel Fried, Sida I. Wang, and Tao Yu. DS-1000: natural and reliable benchIn Andreas Krause, Emma Brunskill, Kyunghyun mark for data science code generation."
        },
        {
            "title": "Preprint Version",
            "content": "Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 1831918345. PMLR, 2023. URL https://proceedings.mlr.press/v202/lai23b.html. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris CallisonBurch, and Nicholas Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021. Jure Leskovec, Anand Rajaraman, and Jeffrey D. Ullman. Mining of Massive Datasets, 2nd Ed. Cambridge University Press, 2014. ISBN 978-1107077232. URL http://www.mmds.org/. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang, Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, et al. E2-llm: Efficient and extreme length extension of large language models. arXiv preprint arXiv:2401.06951, 2024a. Jiaheng Liu, Ken Deng, Congnan Liu, Jian Yang, Shukai Liu, He Zhu, Peng Zhao, Linzheng Chai, Yanan Wu, Ke Jin, Ge Zhang, Zekun Moore Wang, Guoan Zhang, Bangyu Xiang, Wenbo Su, and Bo Zheng. M2rc-eval: Massively multilingual repository-level code completion evaluation. 2024b. Jiaheng Liu, Chenchen Zhang, Jinyang Guo, Yuanxing Zhang, Haoran Que, Ken Deng, Zhiqi Bai, Jie Liu, Ge Zhang, Jiakai Wang, et al. Ddk: Distilling domain knowledge for efficient large language models. arXiv preprint arXiv:2407.16154, 2024c. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024d. Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, Tao Sun, Jiaheng Liu, Yunlong Duan, Yu Hao, Liqun Yang, Guanglin Niu, Ge Zhang, and Zhoujun Li. Mdeval: Massively multilingual code debugging. arXiv preprint arXiv:2411.02310, 2024e. Tianyang Liu, Canwen Xu, and Julian J. McAuley. Repobench: Benchmarking repository-level code auto-completion systems. abs/2306.03091, 2023a. doi: 10.48550/ARXIV.2306.03091. URL https://doi.org/10.48550/arXiv.2306.03091. Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023b. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024a. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. 2024b. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664, 2021. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language In The Twelfth International Conference on Learning Representamodels with evol-instruct. tions, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=UnUwSIgK5W."
        },
        {
            "title": "Preprint Version",
            "content": "Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran, Shivdeep Singh, et al. Granite code models: family of open foundation models for code intelligence. arXiv preprint arXiv:2405.04324, 2024. Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024a. URL https://openreview.net/forum?id=mw1PWNSWZP. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. Olmoe: Open mixture-of-experts language models, 2024b. URL https://arxiv.org/abs/2409. 02060. Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. CodearXiv preprint gen2: Lessons for training llms on programming and natural languages. arXiv:2305.02309, 2023a. URL https://arxiv.org/abs/2305.02309. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program In International Conference on Learning Representations, 2023b. URL https: synthesis. //openreview.net/forum?id=iaYcJKpY2B_. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024a. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale, 2024b. Jim Plotts and Megan Risdal. Meta kaggle code, 2023. URL https://www.kaggle.com/ds/ 3240808. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1517415186, 2024. Haoran Que, Jiaheng Liu, Ge Zhang, Chenchen Zhang, Xingwei Qu, Yi Ma, Feiyu Duan, Zhiqi Bai, Jiakai Wang, Yuanxing Zhang, Xu Tan, Jie Fu, Wenbo Su, Jiamang Wang, Lin Qu, and Bo Zheng. D-cpt law: Domain-specific continual pre-training scaling law for large language models. ArXiv, abs/2406.01375, 2024. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. URL Language models are unsupervised multitask learners. https://cdn.openai.com/better-language-models/language_models_ are_unsupervised_multitask_learners.pdf. OpenAI preprint, 2019. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. Wu Y.K. Li, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020. URL https://arxiv.org/abs/1909.08053."
        },
        {
            "title": "Preprint Version",
            "content": "Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-level prompt generation for large language models of code. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3169331715. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/shrivastava23a.html. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, 2023. Tao Sun, Linzheng Chai, Jian Yang, Yuwei Yin, Hongcheng Guo, Jiaheng Liu, Bing Wang, Liqun Yang, and Zhoujun Li. Unicoder: Scaling code large language model via universal code. arXiv preprint arXiv:2406.16441, 2024. Tianhua Tao, Junbo Li, Bowen Tan, Hongyi Wang, William Marshall, Bhargav Kanakiya, Joel Hestness, Natalia Vassilieva, Zhiqiang Shen, Eric Xing, et al. Crystal: Illuminating llm abilities on language and code. In First Conference on Language Modeling. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, Jie Fu, and Junran Peng. Rolellm: Benchmarking, eliciting, and enhancing roleplaying abilities of large language models. arXiv preprint arXiv: 2310.00746, 2023. Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, et al. Skywork: more open bilingual foundation model. arXiv preprint arXiv:2310.19341, 2023a. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120, 2023b. Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. Conceptmath: bilingual concept-wise benchmark for measuring mathematical reasoning of large language models. arXiv preprint arXiv:2402.14660, 2024. Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, and Wen Wang. Codetransocean: comprehensive multilingual benchmark for code translation. 2023. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Jian Yang, Shuming Ma, Haoyang Huang, Dongdong Zhang, Li Dong, Shaohan Huang, Alexandre Muzio, Saksham Singhal, Hany Hassan, Xia Song, and Furu Wei. Multilingual machine transIn Loïc Barrault, Ondrej Bojar, Fethi lation systems from microsoft for WMT21 shared task. Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno-Yepes, Philipp Koehn, Tom Kocmi, André Martins, Makoto Morishita, and Christof Monz (eds.), Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021, Online Event, November 10-11, 2021, pp. 446455. Association for Computational Linguistics, 2021. URL https://aclanthology.org/2021.wmt-1.54. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. arXiv preprint arXiv:2405.03548, 2024."
        },
        {
            "title": "Preprint Version",
            "content": "Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023. URL https://arxiv.org/abs/2303.12570. Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et al. Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024a. Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Chi-Chih Yao. Automathtext: Autonomous data selection with language models for mathematical texts. arXiv preprint arXiv:2402.07625, 2024b. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning Representations, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: large-scale real-world llm conversation dataset, 2023. Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, and Pengfei Liu. Programming every example: Lifting pre-training data quality like experts at scale. arXiv preprint arXiv:2409.17115, 2024. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A FILTERING RULES",
            "content": "A.1 DESIGN OF FILTERING RULES Designing heuristic filtering rules is inherently challenging, often requiring iterative refinement and experimentation to ultimately develop an effective set of rules. Given this complexity, in addition to providing detailed explanations of our designed rules, we will also share the general insights and methodologies we have accumulated throughout the designing process. We believe that this section will offer valuable guidance for designing heuristic filtering rules applicable to any dataset, thereby significantly enhancing the efficiency of constructing an effective data cleaning pipeline. Heuristic rules filter data based on specific characteristics of file, which, for each file, are ultimately expressed as score representing the files attribute and corresponding threshold set by the rule. During the rule design process, we found that understanding the distribution of scores and the impact of different threshold settings on data filtering is critical to creating effective rules. Therefore, based on the approach used in RedPajama (Computer, 2023), we decompose the heuristic filtering process into two steps: quality signal computation and filtering execution. The quality signal computation calculates the scores for all rules for each file, while the filtering execution module decides whether file is retained based on its quality signal scores and the corresponding thresholds. Additionally, we recommend placing the heuristic filtering process as late as possible in the overall data pipeline. Unlike other, more fixed stages of the data processing pipeline, this stage requires frequent adjustments based on the final quality of the data. Placing it later in the process allows for more precise control over the data and minimizes the need to repeat subsequent steps after this filtering module. The specific steps for designing our heuristic filtering rules are as follows: 1. Quality Signals Designing: Based on the definition of low-quality data and the attributes of the dataset, we firstly design series of quality signals that describe the attributes contributing to file quality. 2. Coarse Threshold Tuning: Referring to the definition of low-quality data and the distribution of quality signal scores, we roughly set filtering thresholds for all rules at once. We then apply the filters to obtain an initial version of the filtered dataset. 3. Fine-grained Threshold Tuning: For each rule, we focus on the data that was exclusively affected by that specific rule, meaning it did not trigger other filters. This part of the data is directly influenced by the current rule, so we can examine whether the retention or removal of this data under different threshold settings aligns with the intended purpose of the rule. If rule is effective in improving data quality based on its target attribute, we select the optimal threshold; otherwise, the rule is discarded. After evaluating each rule, we apply the filters again to obtain more refined filtered dataset. 4. Data Quality Inspection: We then assess whether the filtered dataset meets our expectations for the quality of pretraining data. In addition to traditional manual inspection, we introduce perplexity (PPL)-based method for data quality evaluation. Specifically, we randomly sample set of data from the filtered dataset and use high-performing LLM to compute the PPL on these samples. We then examine the top-N and bottom-N samples based on PPL. Generally, extremely low PPL suggests that the data is overly simplistic, containing limited valuable knowledge, while extremely high PPL indicates that the data may lack learnable patterns. Both of them are advisable to be filtered out. We closely inspect both sets of samples and, based on their characteristics, decide whether to add new rules or adjust existing thresholds. This process can be repeated until the dataset reaches the desired quality. A.2 EXAMPLES OF FILTERING RULES We elaborate several representative examples about general code filtering rules in Table 11 and language-specific filtering rules in Table 12 and explain their rationale. It is essential to note that for general code filtering rules, the threshold values may be slightly adjusted depending on the"
        },
        {
            "title": "Preprint Version",
            "content": "programming language of the file. For specific threshold values, please refer to our implementation details of the data processing pipeline. Table 11: Examples of general code filtering rules."
        },
        {
            "title": "Explanation",
            "content": "The proportion of lines in strings with word count exceeding. The proportion of characters in words from strings with character count exceeding 20. The proportion of hexadecimal characters. The proportion of lines like \"you code here\", \"TODO\" or \"FIXME\". The proportion of lines containing an \"assert\" statement. Files with too many long strings indicate lack of code logic. String variables containing long sequences of characters are often indicative of meaningless content such as base64 data, Hash encoding, url, etc. Files with two many hexadecimal characters indicate lack of code logic. We found that these elements tend to be excessively repeated in the dataset, which increases the likelihood that the model, during code completion, will output placeholders like the ones mentioned above instead of generating actual code. Files containing large number of assert statements are often test files, which tend to have relatively simple and repetitive code patterns."
        },
        {
            "title": "Filtering Quota",
            "content": "score > 0.2 score > 0.4 score > 0.4 score > 0.01 score > 0.4 Table 12: Examples of python-specific filtering rules. Description Explanation The proportion of the number of python functions to the total number of lines. Whether the file can be parsed into an python abstract syntax tree (AST). The proportion of lines that are \"import\" statements. higher number of Python functions in file may indicate that the functions are overly simple, with limited code logic, or have bad code format. Files that cannot be parsed into an AST contain syntax errors and should be filtered out. file with exceeding prportion of \"import\" statements indicates to have sparse code logic. Filtering Quota score > 0.2 score == False score > 0."
        },
        {
            "title": "Preprint Version",
            "content": "B ANALYSIS ON CHUNK-LEVEL DEDUPLICATION During pretraining, data is first randomly concatenated and segmented into chunks of context length, followed by full-attention computation within each chunk. We further explored chunk-level deduplication. Specifically, the pretraining data was randomly concatenated and segmented into chunks of 4096 tokens, followed by MinHash and LSH deduplication on these chunks. Additionally, we applied chunk-level deduplication after file-level and repo-level deduplication. Table 13: Comparison of deduplication strategies on Python data. At the File level, \"Lines\" refers to the number of lines in individual files; at the Repo level, it indicates the line count of aggregated strings; Note that for all deduplication strategies involving the Chunk level, \"Lines\" specifically refers to 4096-token chunks. # Total Lines # Retained Lines # Retained Tokens Chunk-level File-level File-level + Chunk-level Repo-level Repo-level + Chunk-level 333,007,812 485,817,123 333,007,812 11,037,352 333,007,812 79,272,460 30,488,834 7,993,164 7,480,488 17,675,781 324.70 32.74 32.70 99.47 72.40 Figure 12: Comparison of Pass@1 performance on HumanEval & MBPP for different dedup strategies (File-Level, Repo-Level, and Repo-level + Chunk-Level) across RefineCode Python corpus. From the results in table 13, We observe that chunk-level deduplication alone was even less effective than repo-level deduplication, and applying chunk-level deduplication after file-level removed only an additional 0.04B of data. This indicates that chunk-level deduplication is not an effective approach. We pre-trained three 1.5B models on the data retained under file-level, repo-level, and repo-level + chunk-level deduplication strategies. The benchmark results are shown in Figure 12. It is evident that file-level deduplication achieves the highest training efficiency, while repo-level + chunk-level deduplication outperforms repo-level alone. We attribute the superior performance of file-level deduplication to its higher degree of data removal. Overall, we conclude that file-level deduplication is the most suitable method for GitHub data."
        },
        {
            "title": "C EXTRA DATA PROCESSING",
            "content": "C.1 CHINESE CODE-LIKE DOMAINS ANNOTATION The manual annotation of the URLs of the website is presented as shown in the table 14. For future new CC datasets, we can sample pages in these domains as initial seed corpus. Table 14: We manually annotate code-like and math-like Chinese domains, utilizing the % symbol as wildcard in our pattern matching. For example, the URL https://my.oschina.net/u/4/blog/11 is matched by the pattern %my.oschina.net%blog%."
        },
        {
            "title": "Domain",
            "content": "cloud.tencent.com cloud.tencent.com cloud.tencent.com cloud.tencent.com my.oschina.net ask.csdn.net www.cnblogs.com forum.ubuntu.org.cn q.cnblogs.com segmentfault.com segmentfault.com woshipm.com zgserver.com zgserver.com zgserver.com juejin.cn jiqizhixin.com help.aliyun.com jyeoo.com www.haihongyuan.com www.03964.com www.nbhkdz.com 9512.net lanxicy.com bbs.emath.ac.cn math.pro mathschina.com shuxue.chazidian.com shuxue.ht88.com"
        },
        {
            "title": "Tag",
            "content": "%cloud.tencent.com/developer/article% %cloud.tencent.com/ask% Code Code %cloud.tencent.com/developer/information% Code Code Code Code Code Code Code Code Code Code Code Code Code Code Code Code Math Math Math Math Math Math Math Math Math Math Math %cloud.tencent.com/document% %my.oschina.net%blog% %ask.csdn.net/questions% %www.cnblogs.com% %forum.ubuntu.org.cn% %q.cnblogs.com/q% %segmentfault.com/q% %segmentfault.com/a% %woshipm.com/data-analysis% %zgserver.com/server% %zgserver.com/linux% %zgserver.com/ubuntu% %juejin.cn/post% %jiqizhixin.com/articles% %help.aliyun.com/zh% %jyeoo.com% %haihongyuan.com%shuxue% %www.03964.com% %www.nbhkdz.com% %9512.net% %lanxicy.com% %bbs.emath.ac.cn% %math.pro% %mathschina.com% %shuxue.chazidian.com% %shuxue.ht88.com% C.2 CODE-RELATED DATA FROM GITHUB TEXT FILES Github Text files primarily consist of content written in natural languages, which includes abundant code-related knowledge. However, we observed that substantial portion of the dataset is unrelated to code, which is detrimental to the models ability to learn code-related knowledge. Therefore, we employed the following strategies to extract and retain the code-relevant portions before our filtering module. Firstly, following the strategy used in starcoder (Li et al., 2023), we retained the files with \"requirement\" in the lowercased filename, or if the filename without the extension is one of \"readme\", \"notes\", \"todo\", \"description\", \"cmakelists\", in order to ensure that only text files pertinent to coding contexts are preserved. This strategy recalled 3% volume of the whole text part. Additionally, we trained fasttext model to recall code-related text files and recalled extra 7% file volume from the original text data."
        },
        {
            "title": "Preprint Version",
            "content": "C."
        },
        {
            "title": "JUPYTER NOTEBOOKS",
            "content": "Our Jupyter notebook data is sourced from GitHub and Meta Kaggle code (Plotts & Risdal, 2023). We converted this type of data into the Jupyter-structured format used in StarCoder (Li et al., 2023), which consists of triplet of consecutive markdown, code, and code execution results. However, we discarded the Jupyter-script format mentioned in StarCoder. Because the code files generated from Jupyter notebook conversions tend to have poor overall code writing standards, and the content in Jupyter-script and Jupyter-structured formats is highly redundant, making it sufficient to retain only one format."
        },
        {
            "title": "D COMPARISON OF REFINECODE WITH THE STACK SERIES",
            "content": "Table 15 compares RefineCode with two versions of The Stack. RefineCode not only includes more tokens (960 billion) but also incorporates over 130 rules, significantly more than the 15 rules used in previous versions. Additionally, RefineCode leverages 75 billion web data tokens and introduces language-specific (LS) rules, providing more precise and fine-tuned handling across wide range of programming languages. Table 15: The Comparison of training data between RefineCode and series of The Stack. LS denotes Language Specific. # Tokens # Languages # Web Data Tokens # Rules LS Rules The Stack v1 The Stack v2 RefineCode 200 900 960 88 619 607 30 75 15 15"
        },
        {
            "title": "E PROGRAMMING LANGUAGES CATEGORIES",
            "content": "E.1 INCLUDED PROGRAMMING LANGUAGES Included programming languages can be categoried into three classes: code, data and text. Among them, the \"code\" category represents files rich in code logic, while the \"data\" category primarily consists of files with structured data, and the \"text\" category refers to files dominated by natural language content. The threshold settings for the filtering rules vary slightly depending on the data type. Code(470 types): 1C Enterprise, 4D, ABAP, ABAP CDS, AIDL, AL, AMPL, ANTLR, API Blueprint, APL, ASL, ASP.NET, ATS, ActionScript, Ada, Agda, Alloy, Alpine Abuild, AngelScript, Apex, Apollo Guidance Computer, AppleScript, Arc, AspectJ, Assembly, Astro, Asymptote, Augeas, AutoHotkey, AutoIt, Awk, BASIC, BQN, Ballerina, Batchfile, Beef, Befunge, Berry, Bikeshed, Bison, BitBake, Blade, BlitzBasic, BlitzMax, Bluespec, Boo, Boogie, Brainfuck, Brightscript, C, C#, C++, C2hs Haskell, CAP CDS, CLIPS, CMake, COBOL, CUE, Cadence, Cairo, CameLIGO, Capn Proto, Ceylon, Chapel, Charity, ChucK, Circom, Cirru, Clarion, Clarity, Classic ASP, Clean, Click, Clojure, Closure Templates, CodeQL, CoffeeScript, ColdFusion, ColdFusion CFC, Common Lisp, Common Workflow Language, Component Pascal, Coq, Crystal, Csound, Csound Document, Csound Score, Cuda, Curry, Cycript, Cypher, Cython, D, D2, DIGITAL Command Language, DM, Dafny, Dart, DataWeave, Dhall, Diff, Dockerfile, Dogescript, Dylan, E, ECL, EJS, EQ, Earthly, Edge, EdgeQL, Elixir, Elm, Elvish, Emacs Lisp, EmberScript, Erlang, F#, F*, FIRRTL, FLUX, Factor, Fancy, Fantom, Faust, Fennel, Filebench WML, Fluent, Forth, Fortran, Fortran Free Form, FreeBasic, Futhark, GAML, GAMS, GAP, GDB, GLSL, GSC, Game Maker Language, Genero 4gl, Genero per, Genshi, Gentoo Ebuild, Gentoo Eclass, Gherkin, Gleam, Glimmer JS, Glyph, Go, Golo, Gosu, Grace, Grammatical Framework, Groovy, Groovy Server Pages, HCL, HLSL, HTML, HTML+ECR, HTML+EEX, HTML+ERB, HTML+PHP, HTML+Razor, Hack, Haml, Handlebars, Harbour, Haskell, Haxe, HiveQL, HolyC, Hy, IDL, IGOR Pro, Idris, ImageJ Macro, Imba, Inform 7, Ink, Inno Setup, Io, Ioke, Isabelle, Isabelle ROOT, J, JCL, JFlex, JSONiq, Janet, Jasmin, Java, Java Server Pages, JavaScript, JetBrains"
        },
        {
            "title": "Preprint Version",
            "content": "MPS, Jinja, Jison, Jison Lex, Jolie, Jsonnet, Julia, Just, KRL, Kaitai Struct, KakouneScript, KerboScript, Kit, Kotlin, LFE, LLVM, LOLCODE, LSL, LabVIEW, Latte, Lean, Less, Lex, LigoLANG, LilyPond, Limbo, Liquid, Literate Agda, Literate CoffeeScript, Literate Haskell, LiveScript, Logos, Logtalk, LookML, Lua, Luau, M, M4, M4Sugar, MATLAB, MAXScript, MLIR, MQL4, MQL5, MTML, MUF, Macaulay2, Makefile, Mako, Marko, Mask, Mathematica, Mercury, Mermaid, Meson, Metal, MiniD, Mint, Mirah, Modelica, Modula-3, Module Management System, Mojo, Monkey, MoonScript, Motorola 68K Assembly, Move, Mustache, Myghty, NASL, NSIS, NWScript, Nearley, Nemerle, NetLinx, NetLogo, Nextflow, Nim, Nit, Nix, Nu, NumPy, Nunjucks, OCaml, Oberon, Objective-C++, Objective-J, Omgrofl, Opa, Opal, Open Policy Agent, OpenCL, OpenQASM, OpenSCAD, Ox, Oxygene, Oz, P4, PDDL, PEG.js, PHP, PLSQL, PLpgSQL, Pact, Pan, Papyrus, Parrot, Parrot Assembly, Parrot Internal Representation, Pascal, Pawn, Pep8, Perl, PigLatin, Pike, PogoScript, Polar, Pony, Portugol, PowerBuilder, PowerShell, Praat, Processing, Procfile, Prolog, Promela, Propeller Spin, Pug, Puppet, PureScript, Prover9, Pyret, Python, Q#, QML, QMake, Qt Script, Quake, R, RAML, REALbasic, REXX, RPGLE, RUNOFF, Racket, Ragel, Raku, Rascal, ReScript, Reason, ReasonLIGO, Rebol, Red, Redcode, RenderScript, Ring, Riot, RobotFramework, Roc, Rouge, Ruby, Rust, SAS, SMT, SQF, SQL, Sage, SaltStack, Sass, Scala, Scaml, Scenic, Scheme, Scilab, Self, Shell, ShellSession, Shen, Sieve, Singularity, Slash, Slim, Slint, SmPL, Smali, Smalltalk, Smarty, Smithy, Snakemake, SourcePawn, Squirrel, Stan, Standard ML, Starlark, Stata, Stylus, SugarSS, Svelte, Sway, Swift, SystemVerilog, TI Program, TL-Verilog, TLA, TSX, TXL, Talon, Tcl, Tcsh, Tea, Terraform Template, Thrift, Toit, Turing, Twig, TypeScript, Typst, Unified Parallel C, Uno, UnrealScript, UrWeb, V, VBA, VBScript, VCL, VHDL, Vala, Velocity Template Language, Verilog, Vim Script, Vim Snippet, Visual Basic .NET, Visual Basic 6.0, Volt, Vue, Vyper, WDL, WGSL, WebAssembly, WebIDL, Whiley, Witcher Script, Wollok, Wren, X10, XC, XProc, XQuery, XS, XSLT, Xojo, Xonsh, Xtend, YARA, YASnippet, Yacc, Yul, ZAP, ZIL, Zeek, ZenScript, Zephir, Zig, Zimpl, eC, fish, hoon, kvlang, mIRC Script, mcfunction, mupad, nesC, ooc, templ, wisp, xBase Data(115 types): ABNF, ASN.1, Adobe Font Metrics, Altium Designer, Ant Build System, ApacheConf, Avro IDL, BibTeX, Browserslist, CIL, CODEOWNERS, CSON, CSS, Cabal Config, Caddyfile, CartoCSS, Cloud Firestore Security Rules, CoNLL-U, DNS Zone, Darcs Patch, Debian Package Control File, Dotenv, EBNF, Eagle, Easybuild, Ecere Projects, EditorConfig, Edje Data Collection, FIGlet Font, Formatted, GEDCOM, GN, Gemfile.lock, Gerber Image, Git Attributes, Git Config, Glyph Bitmap Distribution Format, Go Checksums, Go Module, Go Workspace, Godot Resource, Gradle, Gradle Kotlin DSL, GraphQL, Graphviz (DOT), HAProxy, HOCON, HTTP, HXML, INI, Ignore List, JAR Manifest, JSON, JSON with Comments, Jest Snapshot, Kusto, Lark, Linker Script, Maven POM, NEON, NL, NPM Config, Nginx, Ninja, ObjDump, Object Data Instance Notation, OpenStep Property List, OpenType Feature File, Option List, PlantUML, PostCSS, Prisma, Protocol Buffer, Protocol Buffer Text Format, Python traceback, RBS, RON, Readline Config, Record Jar, Redirect Rules, Regular Expression, SCSS, SELinux Policy, SPARQL, SSH Config, STAR, STON, ShellCheck Config, Simple File Verification, Soong, Spline Font Database, TOML, TextMate Properties, Turtle, Type Language, Valve Data Format, Wavefront Material, Web Ontology Language, WebAssembly Interface Type, Wget Config, Windows Registry Entries, BitMap, Font Directory Index, XCompose, XML, XML Property List, XPages, YAML, YANG, cURL Config, crontab, desktop, dircolors, edn, nanorc Text(22 types): AsciiDoc, Creole, Gemini, Gettext Catalog, MDX, Markdown, Muse, Org, Pod, Pod 6, RDoc, RMarkdown, Rich Text Format, Roff, SRecode Template, Sweave, TeX, Texinfo, Text, Textile, Wikitext, reStructuredText E.2 EXCLUDED PROGRAMMING LANGUAGES 2-Dimensional Array, AGS Script, Adblock Filter List, Bicep, COLLADA, CSV, Checksums, DirectX 3D File, E-mail, G-code, Git Revision List, Gnuplot, IRC log, KiCad Layout, KiCad Legacy Layout, KiCad Schematic, Lasso, Linux Kernel Module, Max, Microsoft Developer Studio Project, Microsoft Visual Studio Solution, POV-Ray SDL, Pic, Pickle, PostScript, Public Key, Pure Data, PureBasic, Raw token data, Roff Manpage, STL, SVG, SubRip Text, TSV, Unity3D Asset, Wavefront Object, WebVTT, PixMap, robots.txt"
        },
        {
            "title": "F RAW CODE DATA COMPOSITION",
            "content": "Figure 16 shows the composition of raw code data for top 85 programming languages in the RefineCode dataset, both after deduplication and filtering process. It can be observed that, after filtering, the proportion of data for different programming languages has shifted significantly, with notable increase in the representation of commonly used programming languages. Table 16: Overview of the data composition of in RefineCode. The items in the table are sorted in descending order according to the file volume after filtering. Language html java python csharp javascript php cpp go typescript ruby perl rust swift kotlin dart java-pages css lua xml scala shell pascal fortran perl6 rmarkdown html+erb smali scss gettext catalog haskell tcl gradle scheme qml mdx classic asp xbase ini objective-c++ motorola68k gap After deduplication After filtering # Files Vol(GB) Ratio(%) # Files Vol(GB) Ratio(%) 141,081,897 215,177,833 109,725,362 88,825,202 190,670,421 84,378,361 51,362,503 35,649,865 40,211,985 15,735,042 16,354,543 10,605,421 6,132,978 4,238,754 4,493,548 4,087,329 6,174,654 39,822,744 4,027,221 61,171,289 5,897,567 12,054,632 1,306,130 2,274,663 1,943,430 1,317,760 7,618,377 3,457,531 18,061,278 1,100,044 1,746,444 253,345 2,431,985 357,909 354,756 795,525 220,344 192,780 7,232,136 197,416 1,066,095 752,261 3,175.4 706.8 493.3 364.2 1,925.0 374.4 375.2 301.1 287.4 244.5 121.7 63.6 92.5 47.9 56.4 33.0 31.0 241.5 116.0 1,934.2 19.7 23.0 27.8 39.7 16.4 14.0 11.4 37.9 35.6 51.3 24.0 4.2 2.9 4.7 1.8 6.4 2.8 2.5 19.1 2.4 26.5 2.6 8.56 1.90 1.33 0.98 5.19 1.01 1.01 0.81 0.77 0.66 0.33 0.17 0.25 0.13 0.15 0.09 0.08 0.65 0.31 5.21 0.05 0.06 0.07 0.10 0.04 0.04 0.03 0.10 0.10 0.14 0.06 0.01 0.01 0.01 0.01 0.17 0.08 0.07 0.05 0.01 0.07 0.01 45,100,466 124,751,295 58,640,346 57,910,485 69,579,517 60,089,397 38,037,406 26,723,829 20,621,755 8,285,561 9,532,620 6,086,150 4,803,109 2,938,498 3,123,156 2,161,462 4,145,336 15,771,061 2,538,234 3,173,128 4,204,979 6,043,070 960,497 1,218,491 1,034,748 827,951 4,452,355 1,408,274 7,705,822 442,385 1,218,491 136,171 724,609 201,170 252,621 222,013 141,236 80,396 1,517,099 149,223 220,218 510, 582.4 474.3 271.1 232.4 226.9 222.7 176.9 153.7 140.4 122.7 65.6 39.9 34.7 31.8 29.8 18.5 15.4 15.3 14.4 12.8 11.7 11.2 9.5 8.6 8.6 7.9 7.8 7.4 7.4 6.3 6.8 1.0 1.0 1.0 1.0 1.0 0.9 0.9 1.3 1.3 1.2 1.2 18.08 14.72 8.41 7.21 7.04 6.91 5.49 4.77 4.35 3.81 2.04 1.24 1.08 0.99 0.94 0.57 0.48 0.47 0.45 0.40 0.36 0.35 0.29 0.27 0.27 0.25 0.24 0.23 0.23 0.19 0.27 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.04 0.04 0.04 0."
        },
        {
            "title": "Prompt for Educational Instruction Synthesis",
            "content": "You are teaching assistant helping to create Python programming task from given code snippet. You must provide the best response to the Python programming task, including reasoning thought, reference solutions, explanation of test cases, and test code. [Code Snippet] {Code} Your response must have these parts: [Task] {Create an independent and detailed Python programming task} [Analysis] {Analyze the task and reason about the given task step by step} [Solution] {Write high-quality reference solution in self-contained script that solves the task} [Test] {Provide ten assert statements to check the correctness of your solution} Prompt for Package-related Instruction Synthesis You are exceptionally skilled at crafting high-educational level problems and offering precise solutions. Please gain inspiration from the following code snippet to create highquality programming problem, which is beneficial for learning the use of corresponding libraries. Present your output in two distinct sections: [Problem Description] and [Solution]. [Code Snippet] {Code} [Library Api Requirements] {Api Requirements} [Library Api Doc] {Api Doc} Guidelines for each section: [Problem Description]: This should be **completely self-contained**, providing all 1. the contextual information one needs to understand and solve the problem. Assume common programming knowledge, but ensure that any specific context, variables, or code snippets pertinent to this problem are explicitly included. This problem should be **educational for learning the provided Library api, and please explicitly request the use of the relevant package in the question. This question should only concern the writing of **one function**, and you need to be clear about the function name and role of this function. 2. [Solution]: Offer comprehensive, **correct** solution that addresses the [Problem Description] you provided. This solution should follow the standard of corresponding Library Api doc. Please ensure that the Solution only involves answering the Problem, **without addressing the requirements provided!** Please provide essential explanation abouth this solution, especially the use of requiremed Library Api."
        },
        {
            "title": "Preprint Version",
            "content": "Prompt for Large-scale Diverse Instruction Synthesis You are an expert in designing high-quality programming questions based on the given text. [Guidelines] - You can draw inspiration from the given text to create the programming questions. - The created question should be self-contained question, which does not depend on any external context. - The created response must contain the complete code snippet. [Given Text] {Given Text} [Created Question] {Created Question}"
        }
    ],
    "affiliations": []
}