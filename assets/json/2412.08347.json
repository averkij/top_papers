{
    "paper_title": "SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs",
    "authors": [
        "Sultan Alrashed"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SmolTulu-1.7b-Instruct, referenced in this report as SmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAI's Tulu 3 post-training pipeline to enhance Huggingface's SmolLM2-1.7B base model. Through comprehensive empirical analysis using a 135M parameter model, we demonstrate that the relationship between learning rate and batch size significantly impacts model performance in a task-dependent manner. Our findings reveal a clear split: reasoning tasks like ARC and GSM8K benefit from higher learning rate to batch size ratios, while pattern recognition tasks such as HellaSwag and IFEval show optimal performance with lower ratios. These insights informed the development of SmolTulu, which achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval ($\\Delta$11%), and mathematical reasoning with 51.6% on GSM8K ($\\Delta$3.4%), with an alternate version achieving scoring 57.1% on ARC ($\\Delta5.4%$). We release our model, training recipes, and ablation studies to facilitate further research in efficient model alignment, demonstrating that careful adaptation of optimization dynamics can help bridge the capability gap between small and large language models."
        },
        {
            "title": "Start",
            "content": "SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs Sultan Alrashed Saudi Data & Artificial Intelligence Authority srashed@sdaia.gov.sa 4 2 0 2 1 1 ] . [ 1 7 4 3 8 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present SmolTulu-1.7b-Instruct1, referenced in this report as SmolTulu-DPO-1130, an instruction-tuned language model that adapts AllenAIs Tulu 3 post-training pipeline (Lambert et al., 2024a) to enhance Huggingfaces SmolLM2-1.7B base model. Through comprehensive empirical analysis using 135M parameter model, we demonstrate that the relationship between learning rate and batch size significantly impacts model performance in task-dependent manner. Our findings reveal clear split: reasoning tasks like ARC and GSM8K benefit from higher learning rate to batch size ratios, while pattern recognition tasks such as HellaSwag and IFEval show optimal performance with lower ratios. These insights informed the development of SmolTulu, which achieves state-of-the-art performance among sub-2B parameter models on instruction following, scoring 67.7% on IFEval (11%), and mathematical reasoning with 51.6% on GSM8K (3.4%), with an alternate version achieving scoring 57.1% on ARC (5.4%). We release our model, training recipes, and ablation studies to facilitate further research in efficient model alignment, demonstrating that careful adaptation of optimization dynamics can help bridge the capability gap between small and large language models."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in language model post-training have demonstrated remarkable improvements in model capabilities through careful application of supervised finetuning (SFT), preference optimization, and reinforcement learning (Ouyang et al., 2022; Touvron et al., 2023). However, these techniques have primarily been developed and validated on large language models with tens or hundreds of billions of parameters with smaller models being underexplored. The recently released Tulu 3 1Huggingface link:https://huggingface.co/SultanR/ SmolTulu-1.7b-Instruct pipeline provides comprehensive, open-source approach to post-training, but its effectiveness on significantly smaller models remains unexplored. Understanding how post-training dynamics scale to smaller models is crucial for democratizing access to high-quality language models and enabling deployment in resource-constrained environments. While (Shallue et al., 2019) demonstrated that the relationship between batch size and training steps follows characteristic patterns across model families, they also found that the maximum useful batch size varies between workloads and depends heavily on model properties. This suggests that smaller models may require fundamentally different optimization strategies than their larger counterparts, especially across different downstream tasks. Through comprehensive ablations using Huggingfaces SmolLM2-135M (Allal et al., 2024) as base, we demonstrate that the relationship between learning rate and batch size significantly impacts model performance in task-dependent manner during supervised finetuning. Our findings reveal clear difference between reasoning and pattern recognition tasks: benchmarks requiring complex reasoning like ARC and GSM8K show optimal performance with higher learning rate to batch size ratios, while pattern recognition tasks such as HellaSwag and IFEval benefit from lower ratios. This aligns with theoretical work by (Masters and Luschi, 2018), who argue that smaller batch sizes, despite their higher gradient variance, enable more frequent parameter updates that can better track the underlying objectives local structure. Furthermore, (Keskar et al., 2017) demonstrate that optimization trajectories can lead to qualitatively different solutions with varying generalization properties. Our empirical results suggest that higher learning rate to batch size ratios may help smaller models find flatter minima that generalize better to complex reasoning tasks, while lower ratios appear optimal for tasks dominated by pattern recognition. This provides new insight into how careful tuning of optimization parameters might help compensate for limited model capacity in task-dependent manner. In this work, we investigate the adaptation of the Tulu 3 post-training pipeline (Lambert et al., 2024a) to enhance SmolLM2-1.7B (Allal et al., 2024), compact language model with just 1.7 billion parameters. Through set of ablations, we demonstrate that the relationship between learning rate and batch size plays crucial role in determining model performance across different types of tasks. Our key contributions include: Empirical evidence demonstrating how learning rate to batch size ratios differentially affect reasoning and pattern recognition capabilities in small language models Release of SmolTulu, achieving state-of-theart performance among sub-2B parameter models on instruction following and mathematical reasoning with verifiably no contamination. Theoretical analysis connecting optimization dynamics to task-specific performance in small language models Our results indicate that while the core techniques from Tulu 3 translate effectively to smaller scales, achieving optimal performance requires careful consideration of how optimization dynamics change with model size and task type. Through these insights, we hope to contribute to the development of more efficient and accessible language models and establish new best practices for training smaller models that diverge from conventional wisdom derived from large-scale training with the learning rate linearly adjusted to the batch size."
        },
        {
            "title": "Tuning",
            "content": "The development of modern post-training pipelines began with InstructGPT (Ouyang et al., 2022), which established the core supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF) workflow. This approach has been widely adopted and adapted in open-source efforts, notably through works like Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023), which demonstrated the viability of instruction tuning with synthetic and user-generated data respectively. More recent work has focused on improving these pipelines, with projects like Tulu (Ivison et al., 2023) and Zephyr-β (Tunstall et al., 2023), culminating in comprehensive open pipelines like Tulu 3 (Lambert et al., 2024a). 2.2 Direct Preference Optimization Direct Preference Optimization (DPO) (Rafailov et al., 2024) introduced simplified approach to preference learning that eliminates the need for separate reward model and complex RL training loops. This has been followed by various refinements and alternatives, including SLiC-HF (Zhao et al., 2023) and SimPO (Meng et al., 2024), which further simplified the training process while maintaining or improving performance. These developments have made preference learning more accessible and computationally efficient, particularly important for resource-constrained settings."
        },
        {
            "title": "Learning",
            "content": "Recent work has explored using verifiable outcomes to improve model capabilities, particularly in domains like mathematics and coding. STaR (Zelikman et al., 2022) pioneered the use of selftaught reasoning to improve mathematical capabilities, while TRICE (Phan et al., 2023) developed alternative approaches to learning from verifiable solutions. VinePPO (Kazemnejad et al., 2024) specifically targeted mathematical reasoning through reinforcement learning, demonstrating the effectiveness of RL approaches when ground truth answers are available. This line of work has shown particular promise in improving specific capabilities while maintaining model generality. Reinforcement Learning with Verifiable Rewards (RLVR) extends traditional RL approaches by using deterministic binary rewards based on ground truth outcomes rather than learned reward models(Lambert et al., 2024a). This approach has shown particular promise in domains with verifiable solutions like mathematics and programming, where correct outputs can be automatically validated."
        },
        {
            "title": "2.4 Learning Rate and Batch Size Studies",
            "content": "The relationship between learning rate and batch size has been subject of extensive study in deep learning. Foundational work by (Goyal et al., 2018) established key principles for large-batch training, demonstrating that learning rate should be scaled proportionally with batch size to maintain training stability. (Smith et al., 2018) later proposed the counterintuitive strategy of increasing batch size instead of decaying learning rate, showing that this approach could achieve similar convergence while simplifying hyperparameter tuning. More recent work has focused on understanding these relationships in the context of transformer architectures. (Kaplan et al., 2020) provided empirical evidence that optimal training dynamics vary with model scale, while (McCandlish et al., 2018) developed theoretical frameworks for understanding how batch size affects training efficiency. These insights have been particularly relevant for language models, where the interaction between learning rate, batch size, and model capacity can significantly impact the models ability to learn different types of capabilities (Hoffmann et al., 2022). However, most of this work has focused on large models, leaving open questions about how these relationships manifest in smaller architectures."
        },
        {
            "title": "2.5 Efficient and Small Language Models",
            "content": "Research on efficient language models has seen renewed interest with the success of smaller but capable architectures. Notable examples include Microsofts Phi models (Gunasekar et al., 2023), which achieved strong reasoning capabilities at 1.3B parameters (50.6% on HumanEval), and TinyLlama (Zhang et al., 2024), which adapted the Llama architecture to 1.1B parameters. The SmolLM family (Allal et al., 2024) represents significant advance in this direction, demonstrating that carefully trained compact models can achieve competitive performance on wide range of tasks while remaining deployable on consumer hardware."
        },
        {
            "title": "3.1 Dataset",
            "content": "To ensure fair evaluation, we analyzed the contamination levels of various benchmarks in the SFT dataset (allenai/tulu-3-sft-mixture). As shown in Table 1, most benchmarks exhibit minimal contamination rates below 1.5%, with many showing zero contamination. The highest contamination rate was observed in PopQA at 7.21%, while critical evaluation benchmarks like GSM8K, IFEval, and AGI Eval showed negligible to zero contamination, ensuring reliable performance measurements. Benchmark cais/mmlu openai/openai_humaneval openai/gsm8k ucinlp/drop lighteval/MATH google/IFEval akariasai/PopQA tatsu-lab/alpaca_eval lukaemon/bbh truthfulqa/truthful_qa allenai/wildguardmix allenai/wildjailbreak TIGER-Lab/MMLU-Pro Idavidrein/gpqa lighteval/agi_eval_en bigcode/bigcodebench deepmind/math_dataset Contamination 1.34% 0.00% 0.08% 0.20% 0.06% 0.00% 7.21% 1.37% 0.02% 1.47% 0.06% 0.00% 0.93% 0.00% 0.00% 0.00% 0.00% Table 1: Contamination of benchmarks in the SFT dataset used allenai/tulu-3-sft-mixture"
        },
        {
            "title": "3.2 Training",
            "content": "Foundation models have demonstrated remarkable capabilities across various tasks, yet their performance can be significantly influenced by the choice of hyperparameters during SFT. While work by (Smith et al., 2018) established foundational principles for batch size scaling in neural networks, and (McCandlish et al., 2018) provided theoretical frameworks for understanding large-batch training, recent work by (Masters and Luschi, 2018) suggests that the conventional wisdom about batch size scaling may need reconsideration, particularly for smaller models. Our experiments with SmolTulu variants build upon hyperparameter configurations established by AllenAIs extensive ablation studies for Tulu 3. As shown in Table 2, there is notable pattern in the learning rate to batch size ratio (LR/BS) across model scales: as models grow larger, this ratio tends to decrease. This aligns with the scaling laws observed by (Kaplan et al., 2020) and the compute-optimal training strategies described in (Hoffmann et al., 2022). The Tulu 3 70B model uses the smallest ratio of 0.016 106, while our SmolTulu SFT-1130 variant employs significantly larger ratio of 11.25 106. Hyperparameter SmolTulu SFT-1130 Tulu 3 SFT 70b Learning Rate (LR) 9.0 105 3.1 106 5.0 106 2.0 106 Batch Size (BS) LR 106 BS SmolTulu SFTTulu 3 SFT 8b 0.016 11.25 0.097 0.039 128 32 8 Table 2: SFT hyperparameter selection (a) Effect of learning rate and batch size on ARC score. (b) Effect of learning rate and batch size on GSM8K score. (c) Effect of learning rate and batch size on HellaSwag score. (d) Effect of learning rate and batch size on IFEval score. Figure 1: Contour analysis of learning rate and batch size effects on different evaluation metrics during supervised finetuning of SmolLM2-135M. The color scales represent scores for each metric, with black indicating higher performance. The patterns reveal task-dependent optimal ratios between learning rate and batch size. Metric ARC (Average) BBH (3-shot) GSM8K (5-shot) HellaSwag IFEval (Average) MMLU-Pro (MCF) PIQA SmolTulu SmolTulu SFT-1130 SFT-1207 1.7B-Instruct SmolLM2 51.0 34.7 49.0 61.5 61.0 17.6 72.7 55.6 34.0 42.8 67.5 47.8 17.9 76.9 51.7 32.2 48.2 66.1 56.7 19.3 74.4 Table 3: Performance comparison of SFT models"
        },
        {
            "title": "3.3 Results & Discussion",
            "content": "model demonstrates clear task-dependent patterns in the relationship between learning rate and batch size ratios. As shown in Figure 1a and Figure 1b, reasoning tasks like ARC and GSM8K consistently benefit from higher learning rate to batch size ratios. This aligns with theoretical work by (Keskar et al., 2017). The improvement is particularly pronounced for GSM8K, where performance increases monotonically with the learning rate to batch size ratio. The supervised finetuning experiments reveal complex relationships between optimization dynamics and model capabilities that evolve with model scale. Our initial ablation study using 135M parameter Conversely, at the 135M scale, pattern recognition tasks show markedly different behavior. Figure 1c and Figure 1d reveal that HellaSwag and IFEval achieve optimal performance with lower learning rate to batch size ratios. This contrast suggests that at smaller scales, different types of learning may benefit from fundamentally different optimization dynamics, supporting (Shallue et al., 2019)s observation that optimal batch sizes vary significantly between workloads. However, these relationships become more nuanced at larger model scales. In our 1.7B parameter model, while GSM8K continues to benefit from higher ratios (achieving 51.6% with SmolTulu1130s high ratio versus 44.7% with SmolTulu1207s lower ratio), the pattern for other tasks shifts. Notably, IFEval performance improves with higher ratios (67.7% vs 56.6%), while ARC and MMLUPro show optimal performance with lower ratios (57.1% and 19.1% respectively). This scale-dependent shift in optimization dynamics aligns with (Masters and Luschi, 2018)s argument that the relationship between batch size and learning rate fundamentally changes with model capacity. We hypothesize that at 135M parameters, the limited model capacity forces strict trade-off between different types of learning, requiring distinct optimization strategies for reasoning versus pattern recognition. At 1.7B parameters, the increased capacity appears to enable more flexible learning dynamics, allowing both GSM8K and IFEval to benefit from aggressive optimization while tasks like ARC and MMLU-Pro benefit from more conservative approaches. These findings suggest that the conventional wisdom about learning rate and batch size relationships derived from large-scale training may need significant adaptation for smaller models. The interaction between model capacity and optimal optimization strategy appears more complex previously thought."
        },
        {
            "title": "4.1 Dataset",
            "content": "We conducted contamination analysis on the DPO dataset (allenai/llama-3.1-tulu-3-8b-preferencemixture) to verify evaluation fairness. Table 4 shows consistently low contamination rates across benchmarks, with most falling below 1%. The highest contamination was found in PopQA at 2.72%, while crucial benchmarks like GSM8K, IFEval, and BBH maintained zero contamination, enabling trustworthy assessment of model improvements through preference optimization. Benchmark cais/mmlu openai/openai_humaneval openai/gsm8k ucinlp/drop lighteval/MATH google/IFEval akariasai/PopQA tatsu-lab/alpaca_eval lukaemon/bbh truthfulqa/truthful_qa allenai/wildguardmix allenai/wildjailbreak TIGER-Lab/MMLU-Pro Idavidrein/gpqa lighteval/agi_eval_en bigcode/bigcodebench deepmind/math_dataset Contamination 0.69% 0.00% 0.00% 0.07% 0.02% 0.00% 2.72% 1.24% 0.00% 0.61% 0.06% 0.00% 0.36% 0.00% 0.00% 0.00% 0.00% Table 4: Contamination of benchmarks in the DPO dataset used allenai/llama-3.1-tulu-3-8b-preferencemixture"
        },
        {
            "title": "4.2 Training",
            "content": "We employed Direct Preference Optimization (DPO) for preference learning while adapting the hyperparameters for our smaller model scale, using the same pipeline as described in Tulu 3 (Lambert et al., 2024a). DPO allows direct optimization of the policy without requiring separate reward model (Rafailov et al., 2024). The core objective function is shown in equation 1. We used the length-normalized variant of DPO, which showed superior performance in Tulu 3s ablations, where the log probabilities are normalized by sequence length. Pre-computing and caching log probabilities from the reference model instead of keeping it in memory (Hu et al., 2024) Performing separate forward passes for chosen and rejected sequences rather than concatenating them However, our experiments revealed that smaller models benefit from different hyperparameter configurations than their larger counterparts. As shown in Table 5, we maintained higher learning rate to batch size ratios compared to Tulu 3s models, with SmolTulu DPO-1130 using ratio approximately 17 times larger than Tulu 3 8Bs. Eyc,yrD max πθ (cid:20) (cid:18) log σ β log πθ(ycx) πref(ycx) β log (cid:19)(cid:21) πθ(yrx) πref(yrx) (1) Figure 2: Direct Preference Optimization objective function, where πref is the reference model (our SFT model) and β controls KL divergence from the reference model. Hyperparameter Learning Rate (LR) 8.0 107 Batch Size (BS) LR 107 BS 0.667 SmolTulu SmolTulu DPO-1130 DPO-1207 5 107 32 12 0.156 Tulu 3 DPO 8b Tulu 3 DPO 70b 5.0 107 2.0 107 128 0.039 128 0.016 Table 5: DPO hyperparameter selection For DPO-1130, we used learning rate of 8.0 107 and batch size of 12, while DPO-1207 used more conservative learning rate of 5.0 107 with batch size 32. Both variants used: Maximum sequence length of 2,048 tokens KL penalty coefficient β = 5 Linear learning rate schedule with 0.1 warmup ratio Single training epoch Our experimentation with higher learning rate to batch size ratios was motivated by the hypothesis that smaller models may require larger per-example updates to effectively learn from preference data, particularly for complex reasoning tasks where they lack the inherent capacity of larger models."
        },
        {
            "title": "4.3 Results & Discussion",
            "content": "Metric ARC (Average) BBH (3-shot) GSM8K (5-shot) HellaSwag IFEval (Average) MMLU-Pro (MCF) PIQA SmolTulu DPO-1130 DPO-1207 1.7B-Instruct SmolLM2 SmolTulu 51.5 33.8 51.6 61.1 67.7 17.4 72. 57.1 34.2 44.7 64.2 56.6 19.1 76.4 51.7 32.2 48.2 66.1 56.7 19.3 74.4 Table 6: Performance comparison of DPO models The DPO experiments yielded notable improvements over the SFT baseline across several key metrics. As shown in Table 6, SmolTulu DPO1130, using the higher learning rate to batch size ratio, achieved substantial gains on instruction following and mathematical reasoning tasks, reaching 67.7% on IFEval and 51.6% on GSM8K. Conversely, SmolTulu DPO-1207, employing lower learning rate to batch size ratio more similar to larger models, showed stronger performance on pattern recognition tasks like ARC (57.1%) and PIQA (76.4%). However, our exploration of hyperparameter configurations was significantly constrained by computational resources. Given that DPO builds directly upon the SFT model, the initial policys convergence properties may significantly influence subsequent preference learning. comprehensive understanding would require extensive ablation studies across different SFT checkpoints."
        },
        {
            "title": "5.1 Dataset",
            "content": "Our reward models (RM) were trained on the same preference dataset used in the DPO stage, combining UltraFeedback with additional synthetic preference data generated through the Tulu 3 pipeline. We maintained rigorous contamination controls across all evaluation benchmarks, as detailed in earlier sections."
        },
        {
            "title": "5.2 Training",
            "content": "Following the Tulu 3 methodology (Lambert et al., 2024a), we trained our reward models using the standard pairwise preference learning objective. Given preference dataset consisting of prompts and two responses per prompt (y, y), where one response is chosen yc and one is rejected yr, the reward model rϕ is trained to maximize: max rϕ E(x,yc,yr)D[log σ(rϕ(x, yc) rϕ(x, yr))] (2) where σ is the logistic function. This objective maximizes the difference between rewards, with this difference representing the log-likelihood that Hyperparameter SmolTulu RM-1130 Tulu 3 DPO 8b Learning Rate (LR) 4.0 105 7.5 107 5.0 107 Batch Size (BS) LR 107 BS SmolTulu RM-1207 0.938 0.039 128 100 4 Table 7: Reward model hyperparameter selection yc will be preferred over yr. We used the same several key implementation details from Tulu 3, except for ones that involved batch size and learning rate, where our changes can be seen in Table 7. 5.3 Results & Discussion benefit from larger per-example updates and more frequent gradient computations. However, establishing the precise nature of this relationship would require more extensive ablation studies, which we leave to future work with greater computational resources. Metric RB Chat RB Chat Hard RB Safety RB Reasoning RB Average UFB SmolTulu SmolTulu Tulu 3 RM-1130 RM-1207 8b RM 96.27 55.92 84.05 76.50 81.34 77.34 94.13 43.64 75.54 68.01 72.43 73.17 83.52 44.74 64.59 54.71 58.59 61.66 Table 8: Performance comparison of reward modsplit of alels, where UFB is lenai/ultrafeedback_binarized_cleaned and RB is RewardBench. the test_prefs The reward modeling experiments revealed interesting patterns consistent with our findings from SFT and DPO stages. SmolTulu RM-1130, employing much larger learning rate to batch size ratio, demonstrated strong performance across various metrics on RewardBench (RB) (Lambert et al., 2024b), achieving 94.13% on standard chat evaluation and 75.54% on safety assessments, as seen in Table 8. This pattern of strong relative performance extends across other metrics, with SmolTulu RM1130 achieving 73.17% accuracy on UltraFeedback benchmark test preferences, falling only 4.17 percentage points short of Tulu 3s 77.34% despite using approximately 21% of the parameters. Following (Shallue et al., 2019)s framework, these results suggest that reward modeling may scale more gracefully to smaller architectures than previously assumed, particularly when using appropriately adapted optimization strategies. The substantial performance gap between RM1130 and RM-1207 (72.43% vs 58.59% on RB reinforces our earlier findings about the importance of learning rate to batch size ratios in smaller models. The higher ratio used in RM-1130 appears to be particularly crucial for reward modeling, where the task of learning preference relationships may"
        },
        {
            "title": "Rewards",
            "content": "While our initial experiments with RLVR showed promise, the computational requirements for thorough hyperparameter exploration proved prohibitive. Our preliminary investigations suggest that the relationship between learning rate and batch size may be particularly complex in the RLVR setting, where the sparse binary reward signal introduces additional optimization challenges for smaller models. However, establishing concrete findings would require extensive ablation studies beyond our current computational resources. We leave comprehensive exploration of these dynamics to future work with greater computational capacity."
        },
        {
            "title": "7 Limitations",
            "content": "While our work demonstrates promising results in adapting the Tulu 3 pipeline to smaller models, several important limitations should be noted: Base Model Dependencies Our findings are specific to SmolLM2 and may not generalize to other small models, particularly those with different pretraining approaches or architectural choices. Multi-Stage Optimization Understanding The relationship between optimization choices in different training stages (SFT, DPO, RM, RLVR) remains poorly understood, especially in the context of smaller models. While we observed consistent benefits from higher learning rate to batch size ratios, the theoretical foundations for this interaction across training stages requires further investigation. Metric ARC (Average) BBH (3-shot) GSM8K (5-shot) HellaSwag IFEval (Average) MMLU-Pro (MCF) PIQA SmolTulu DPO-1130 DPO-1207 SFT-1130 SFT-1207 1.7B-Instruct 1B-Instruct 1.5B-Instruct SmolTulu SmolTulu SmolTulu Llama-3.2 SmolLM2 Qwen2.5 51.5 33.8 51.6 61.1 67.7 17.4 72.2 57.1 34.2 44.7 64.2 56.6 19.1 76. 51.0 34.7 49.0 61.5 61.0 17.6 72.7 55.6 34.0 42.8 67.5 47.8 17.9 76.9 51.7 32.2 48.2 66.1 56.7 19.3 74.4 41.6 27.6 26.8 56.1 53.5 12.7 72.3 46.2 35.3 42.8 60.9 47.4 24.2 73.2 Table 9: comparison against wider selection of models"
        },
        {
            "title": "References",
            "content": "We have demonstrated that careful adaptation of modern post-training techniques can yield strong results even at significantly smaller model scales. We found that smaller models may require substantially different optimization dynamics than their larger counterparts to achieve optimal performance. Our empirical results align with theoretical frameworks from optimization literature, suggesting that higher ratios can help compensate for limited model capacity, particularly on complex reasoning tasks. The resulting model, SmolTulu, achieves stateof-the-art performance among sub-2B parameter models on instruction following while maintaining strong mathematical reasoning capabilities as seen in Table 9. These results suggest that the effectiveness of post-training pipelines like Tulu 3 can extend to much smaller scales when properly adapted. Our findings indicate that optimal training strategies may need to vary significantly based on both model scale and target capabilities. Looking forward, we believe this work opens up promising directions for making high-quality language models more accessible and deployable in resource-constrained environments. Future work investigating adaptive optimization strategies that account for both model scale and task requirements could further advance this goal. Additionally, developing theoretical frameworks that specifically address the interaction between model capacity and optimization dynamics could help establish more principled approaches to training smaller models."
        },
        {
            "title": "9 Acknowledgements",
            "content": "Thank you Faisal Alhejary, Abdulmajeed Alrowaithy, Tariq Aljaber, Abdulaziz Albuainain, and Salman Alsubaihi for providing me with their support. Thank you AllenAI and Huggingface for your continual contributions to open-source. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Lewis Tunstall, Agustín Piqueres, Andres Marafioti, Cyril Zakka, Leandro von Werra, and Thomas Wolf. 2024. Smollm2 - with great data, comes great performance. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2018. Accurate, large minibatch sgd: Training imagenet in 1 hour. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks are all you need. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. 2024. Openrlhf: An easyto-use, scalable and high-performance rlhf framework. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. Camels in changing climate: Enhancing lm adaptation with tulu 2. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. 2024. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. 2017. On large-batch training for deep learning: Generalization gap and sharp minima. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2024a. Tülu 3: Pushing frontiers in open language model post-training. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. 2024b. Rewardbench: Evaluating reward models for language modeling. Dominic Masters and Carlo Luschi. 2018. Revisiting small batch training for deep neural networks. Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. 2018. An empirical model of large-batch training. Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with In Advances in Neural Inreference-free reward. formation Processing Systems (NeurIPS). Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Du Phan, Matthew D. Hoffman, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, and Rif A. Saurous. 2023. Training chain-of-thought via latent-variable inference. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl. 2019. Measuring the effects of data parallelism on neural network training. Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. 2018. Dont decay the learning rate, increase the batch size. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. 2023. Zephyr: Direct distillation of lm alignment. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. Tinyllama: An open-source small language model. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. 2023. Slic-hf: Sequence likelihood calibration with human feedback."
        },
        {
            "title": "A Language Distribution in Datasets",
            "content": "Using XLM-RoBERTa for language detection, we can get an estimate for the presence of each language in given dataset."
        },
        {
            "title": "B RLVR Dataset Contamination",
            "content": "We included the contamination results of our intended RLVR dataset for reference. Language English Hindi Swahili Russian Spanish Arabic Chinese Turkish Urdu Portuguese Vietnamese Japanese French Bulgarian Italian Dutch Polish German Thai Greek Presence (%) 83.13 3.79 2.02 2.00 1.15 0.98 0.94 0.87 0.78 0.77 0.64 0.63 0.66 0.33 0.32 0.31 0.25 0.23 0.10 0.09 Table 10: Language distribution in SFT dataset."
        },
        {
            "title": "Language\nEnglish\nHindi\nRussian\nFrench\nSpanish\nChinese\nUrdu\nSwahili\nGerman\nJapanese\nPortuguese\nArabic\nTurkish\nVietnamese\nItalian\nPolish\nDutch\nBulgarian\nThai\nGreek",
            "content": "Presence (%) 86.24 2.23 2.03 1.42 1.40 1.37 0.68 0.65 0.58 0.57 0.54 0.51 0.42 0.33 0.32 0.22 0.18 0.18 0.10 0.04 Table 11: Language distribution in DPO / RM dataset. Language English French Spanish Chinese German Russian Japanese Hindi Polish Portuguese Dutch Urdu Bulgarian Italian Turkish Arabic Vietnamese Swahili Presence (%) 94.80 1.29 1.04 0.66 0.55 0.48 0.40 0.23 0.10 0.10 0.08 0.07 0.07 0.05 0.03 0.03 0.02 0.00 Table 12: Language distribution in RLVR dataset. Benchmark cais/mmlu openai/openai_humaneval openai/gsm8k ucinlp/drop lighteval/MATH google/IFEval akariasai/PopQA tatsu-lab/alpaca_eval lukaemon/bbh truthfulqa/truthful_qa allenai/wildguardmix allenai/wildjailbreak TIGER-Lab/MMLU-Pro Idavidrein/gpqa lighteval/agi_eval_en bigcode/bigcodebench deepmind/math_dataset Contamination 0.65% 0.00% 0.00% 0.00% 0.24% 0.00% 0.45% 0.12% 0.00% 0.12% 0.00% 0.00% 0.66% 0.00% 0.00% 0.00% 0.00% Table 13: Contamination of benchmarks in the RLVR dataset allenai/RLVR-GSM-MATH-IF-MixedConstraints"
        }
    ],
    "affiliations": [
        "Saudi Data & Artificial Intelligence Authority"
    ]
}