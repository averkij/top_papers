{
    "paper_title": "IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization",
    "authors": [
        "Xinghua Zhang",
        "Haiyang Yu",
        "Cheng Fu",
        "Fei Huang",
        "Yongbin Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively."
        },
        {
            "title": "Start",
            "content": "IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization Xinghua Zhang, Haiyang Yu, Cheng Fu, Fei Huang, Yongbin Li* Tongyi Lab (cid:66): {zhangxinghua.zxh, yifei.yhy, fucheng.fuc, f.huang, shuide.lyb}@alibaba-inc.com 4 2 0 2 9 ] . [ 1 8 0 2 6 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively."
        },
        {
            "title": "Introduction",
            "content": "The rapid development of large language models (LLMs) has facilitated human-machine interaction, with instructions serving as the medium (Gao et al., 2024; Kim et al., 2024; Zhang et al., 2024b). As human needs evolve, there is an increasing expectation for models to handle more intricate tasks through complex instructions (Ge et al., 2023; Yang et al., 2024b; Wang et al., 2024). Consequently, the instruction-following ability, especially complex instructions, is garnering significant attention (Zhou et al., 2023; Xu et al., 2024; Li et al., 2024; Zhang et al., 2024a). *Corresponding author. 1 Figure 1: Alignment Paradigms (a) Existing DPO Series vs. (b) Proposed IOPO. The green arrow indicates that matches while the red one indicates mismatch. To evaluate the instruction-following abilities of LLMs, several benchmarks (Zhou et al., 2023; Qin et al., 2024; Li et al., 2024) have been proposed, which are designed to systematically assess how well these models can understand and execute instructions. IFEval (Zhou et al., 2023) focuses on verifiable instructions which are amenable to objective verification of compliance. Qin et al. (2024) introduces INFOBENCH which contains 500 distinct instructions and 2,250 decomposed questions to assess the ability of instruction following. Recently, the ability to follow complex instructions with multiple constraints is gaining increasing attention (He et al., 2024b; Jiang et al., 2024; Wen et al., 2024; He et al., 2024a) as LLMs are deployed in sophisticated real-world applications. Zhang et al. (2024a) proposes constraint-following benchmark CFBench with 1,000 multi-constraint samples. However, most of benchmarks lay emphasis on evaluating LLMs ability to follow complex instructions, lack of algorithms tailored for enhancing the corresponding ability. From RLHF (Reinforcement Learning from Human Feedback) (Ouyang et al., 2022; Bai et al., 2022a) to the following-up researches such as DPO (Direct Preference Optimization) (Rafailov et al., 2023), alignment algorithms which align LLMs with human preferences, have demonstrated their effectiveness in improving the LLMs capabilities to follow instructions. Nevertheless, these methods directly explore different responses (ywin, yloose) based on the same instruction x, as shown in Figure 3 (a). In the complex instruction scenario which contains multiple constraints, it is challenging to efficiently perceive the fine-grained constraints in solely by modeling different y. To bridge this gap, this paper first introduces TRACE benchmark to improve the ability of LLMs to track complex fine-grained constraint instructions and make them more obedient. TRACE is automatically constructed based on the manually sorted taxonomy of complex instructions with 26 constraint dimensions within 5 constraint types. We develop an automated data construction workflow that extends from open-source simple instructions to multi-constrained complex ones. In the end, we accumulate 120K complex instructions for model training and 1K human-verified data for evaluation. To enhance the ability of LLMs to follow complex instructions, this paper further proposes Input-Output Preference Optimization (IOPO) method. IOPO not only takes the instruction as input to directly learn the response preference, but also gradually delves deeper into instructions based on the same response y, to promote effective perception of fine-grained constraints, as shown in Figure 3 (b). The major contributions of this paper are summarized as follows: We introduce benchmark TRACE for complex instruction following, which includes both an evaluation set and training set, and an automated data construction workflow, further enriching the research community. Different from previous alignment paradigm, we propose IOPO alignment method which deeply explores the complex instructions (Input), not just directly learning response preference (Output). Extensive experiments on both in-domain and out-of-domain evaluations have confirmed the consistent improvements, with an average increase of 7.22% and 2.66%, compared to SFT and DPO, respectively."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Instruction Following Instruction following is the most fundamental and crucial ability for large language models (LLMs), which enables them to understand and execute user instructions accurately, making them more effective in wide range of applications. In fact, earlier studies have explored the extent to which models follow language instructions (Ye and Ren, 2021; Mishra et al., 2022; Hase and Bansal, 2022). It is effective to fine-tune LLMs on these annotated instruction data for improving the ability to follow natural language instructions. The instructionfollowing ability enhances adaptability to unseen tasks, which has become an efficient learning paradigm for novel task demands (Lou et al., 2023). As human demands grow higher, the instructions given to LLMs are also becoming increasingly complex. Recent studies are beginning to focus on the complex instruction-following ability of LLMs, where more complex or constrained instructions have been proven effective in enhancing LLMs abilities to follow instructions (Mukherjee et al., 2023; Xu et al., 2024; Luo et al., 2024). Constrained instructions, as type of complex instruction, are also gradually receiving attention from the research community. Increasing the complexity of constraints within the instruction (e.g., raising the number of constraints) can further improve the ability to follow complex instructions (Sun et al., 2024; Dong et al., 2024; He et al., 2024a). Sun et al. (2024) introduces instruction tuning dataset Conifer and proposes progressive learning scheme to enhance the ability of LLMs to follow multi-level instructions with complex constraints. He et al. (2024a) first finds that multi-constraint instructions can enhance LLMs understanding of complex instructions, and then introduces discrimination-based method for data acquisition, and finally proposes contrastive method with reinforcement learning fine-tuning (RLFT) for data utilization. In addition, some work focuses on evaluating the multi-constraint instruction-following capabilities of LLMs (Zhou et al., 2023; Qin et al., 2024; Jiang et al., 2024). Zhang et al. (2024a) introduces CFBench, benchmark which encompasses 2 instructions with multiple constraints, and proposes multi-dimensional evaluation framework to comprehensively assess model capabilities. 2.2 LLM Alignment LLM alignment aims to enhance LLMs by aligning them with human preference. Recent research has conducted extensive explorations for LLM alignment. From RLHF/PPO (Ouyang et al., 2022; Bai et al., 2022a) to DPO (Rafailov et al., 2023) and beyond (Bai et al., 2022b; Song et al., 2024; Meng et al., 2024), the evolution of xPO-series alignment algorithms has seen significant advancements. These methodologies have been pivotal in improving the alignment between LLMs and human values, ensuring that the outputs of these models are not only effective but also follow human preference. RLHF involves training models using humanprovided rewards or reward models to improve decision-making, optimizing policies through iterative feedback loops for more aligned outcomes. DPO directly optimizes the models output to match preferred response as indicated by human feedback, which simplifies the alignment process by focusing on direct comparisons between preferred and dispreferred outputs, allowing the model to learn human preference without needing an explicitly defined reward model. SimPO (Meng et al., 2024) proposes method for preference optimization that eliminates the need for the reference model πref , which is memory efficient and simple. Instead of using pairwise data, PRO (Song et al., 2024) utilizes the preference ranking of any length listwise preference dataset. To further enrich the community of multiconstraint instruction following ability, we construct TRACE benchmark which contains instructions with multiple constraints, more fine-grained constraint types and wider range of constraint In addition, we propose the tailorquantities. designed alignment algorithm IOPO for finegrained multi-constraint alignment, which is different from previous methods (e.g., DPO) only focusing on the output preference."
        },
        {
            "title": "3 Preliminary",
            "content": "Existing alignment methods have evolved from RLHF (Ouyang et al., 2022; Bai et al., 2022a), this section provides brief introduction to RLHF which mainly consists of three stages: 1) SFT: The generic pre-trained LM is finetuned with maximum likelihood supervised loss on downstream task data, and then we can get the SFT model πSFT. 2) Reward Model: The model πSFT is utilized with prompt to generate two different responses y1, y2. The pair of responses is labeled as preferred and dispreferred by human labelers, i.e., y1 y2 x. The reward model rϕ is trained with the following negative log-likelihood loss: LR = E(x,y1,y2)D[log σ(rϕ(x, y1) rϕ(x, y2))] (1) 3) RL: This stage uses the learned reward model rϕ to provide feedback to the language model policy, the optimization objective is as follows: max πθ ExD,yπθ(yx)[rϕ(x, y)] βDKL[πθ(yx)πref(yx)] (2) where LM policy πθ, base reference policy πref are both initialized with πSFT, β controls the deviation of πθ from the base policy πref ."
        },
        {
            "title": "4 TRACE Benchmark",
            "content": "This section describes the construction pipeline of TRACE, its statistics, and evaluation protocol."
        },
        {
            "title": "4.1 Construction Pipeline",
            "content": "The overall construction process includes several key stages: 1) Taxonomy of Constraint, 2) Constraint Expansion, 3) Instruction Structuring, 4) Quality Control, and 5) Response Generation & Evaluation. Taxonomy of Constraint. comprehensive constraint type system is developed through inference by LLM from large volume of open-source simple instructions, and further refined by human experts, into 5 constraint types (Jiang et al., 2024) and 26 constraint dimensions. The detailed description of constraints is shown in Appendix A. Constraint Expansion. This step aims to expand simple instructions into more complex ones that incorporate multiple constraints based on the taxonomy of constraint by prompting LLM. Instruction Structuring. To better distinguish different segments of the instruction, this step structures the flat instruction text expanded from the last step into Task Description, Constraints, and Input part by prompting LLM. 3 Figure 2: Construction Pipeline of TRACE. #N Min. Max. Avg. #Training #Evaluation 119,345 1,042 1 1 15 4.36 4.89 Table 1: The statistics of TRACE benchmark. #N is the number of instructions; Min., Max., and Avg. mean the minimum, maximum, and average number of constraints per instruction. Quality Control. To ensure the validity of the instructions, this step conducts quality control of the expanded instructions by prompting LLM, addressing some forms of invalidity such as redundancy between the description and constraints, incompleteness between the description and input. Response Generation & Evaluation. First, we prompt LLM with the instruction to generate the corresponding response y. To confirm its quality, we then prompt LLM to rate how well the response comply with constraints in the instruction x. Finally, data that fully follows all the constraints outlined in the instruction would receive perfect score of 10, and is selected to form the supervised fine-tuning (SFT) instruction dataset. The corresponding prompts for constraint expansion, instruction structuring, quality control, response generation and evaluation are shown in Appendix B. Figure 3: Constraint type distribution over evaluation set in TRACE."
        },
        {
            "title": "4.2 Dataset Statistics",
            "content": "As shown in Table 1, TRACE consists of 119,345 instructions for model training, and 1,042 instructions for evaluation, where the minimum and maximum number of constraints per instruction are 1 and 15, with average numbers of 4.36 and 4.89, respectively. Table 2 gives the constraint number distributions over training and evaluation set in TRACE. For example, when = 6, the corre4 = 1 2 4 5 6 7 8 10 11 12 13 14 15 #Training 991 8,003 26,421 34,155 26,327 13,858 5,882 2,185 999 464 8 20 20 8 4 #Evaluation 200 100 100 100 10 10 10 4 100 100 100 100 4 4 Table 2: Constraint number (C) distributions over training and evaluation set in TRACE. = represents the number of instructions with constraints. sponding column indicates that there are 13,858 instructions with 6 constraints in the training set, and 100 instructions with 6 constraints in the evaluation set. In Figure 3, we depict the constraint type distribution over the evaluation set. The inner circle represents the distribution of five major types (Content Constraint, Situation Constraint, Style Constraint, Format Constraint, and Example Constraint), and the corresponding outer one represents the distribution of concrete constraint dimensions. make corrections. Second, we make the manual annotation process involving multiple steps such as annotator training, small-scale trial annotation, selection of official annotators, and formal annotation. Finally, we randomly select 100 instructions for quality evaluation, which are then inspected by three labeling specialists based on the above check items and the overall validity. The agreement rate among three annotators on the sampled evaluation set is 95%. 4.3 Evaluation Protocol We use GPT-4o as the evaluator to assess the generated response based on the complex instruction. Concretely, inspired by Qin et al. (2024); Zhang et al. (2024a), we prompt the LLM evaluator to evaluate each constraint mentioned in the complex instruction on scale of 0 to 10 score, assessing the degree to which the response follows each constraint. higher score indicates stronger adherence to the specified constraint. The overall instruction following score IF on the evaluation set with complex instructions can be calculated as follows: IF = 1 (cid:88) I=10 mi(cid:88) j=1 Si,j mi (3) where Si,j [0, 10] is the score indicating the degree to which the j-th constraint in the i-th instruction is adhered to. mi is the number of conI=10 is 1 when straints in the i-th instruction. (cid:80)mi = 10, otherwise is 0. That is, response j=1 is considered correct only when all constraints in the complex instruction are fully followed. Si,j mi"
        },
        {
            "title": "4.4 Evaluation Set Quality",
            "content": "To generate the high-quality evaluation set, we further introduce rigorous post-inspection process after construction pipeline (Sec. 4.1). First, we use the powerful LLM GPT-4o to check the following items for each instruction in the evaluation set: 1) Is the description empty? 2) Is there redundancy between the constraints and description? 3) Does the input match the description? If any of the aforementioned issues arise, we prompt the GPT-4o to 5 Input-Output Preference Optimization Both RLHF (Ouyang et al., 2022; Bai et al., 2022a) and its variants, such as DPO (Rafailov et al., 2023), directly learn the response preference (Output) given the same instruction (Input). However, complex instructions consist of multiple finegrained constraints, direct preference learning for the output struggles to perceive fine-grained constraints in the input x. To enhance the models perception of fine-grained instruction, we further introduce the input preference learning which reflects on the constraints in the instruction based on the response y. By performing preference learning of both input and output, input-output preference optimization (IOPO) not only rapidly fits the better output but also meticulously considers the finegrained information of the input. Concretely, we construct pair of instructions <x1, x2> whose responses are respectively <y1, y2>, where x2 has subtle differences from x1 in some constraints, and these differences would result in substantially divergent responses. And then, we can get four input-output pairs <x1, y1>, <x1, y2>, <x2, y1>, and <x2, y2>, which can form preference group pair G1 G2 (G1 = {<x1, y1>, <x2, y2>}, G2 = {<x1, y2>, <x2, y1>}). The detailed data construction process is described in Appendix D. The first group is the matched inputoutput pair while the second one is mismatched. As derived from Eq.2 in Rafailov et al. (2023), the reward function r(x, y) can be represented by the policy model πr as follows:"
        },
        {
            "title": "6 Experiments",
            "content": "r(x, y) = β log πr(yx) πref(yx) + β log Z(x) (4) 6.1 Experimental Settings . (cid:16) 1 (cid:17) β r(x, y) where Z(x) = (cid:80) πref(yx) exp The BradleyTerry model (Bradley and Terry, 1952) is probability model for the outcome of pairwise comparisons between items, groups, or objects. Given pair of items and j, it estimates the probability that the pairwise comparison turns out true (Hunter, 2004), as p(i j) = pi pi + pj (5) where pi is positive real-valued score assigned to individual i, and the comparison can mean is preferred to j. Similarly, given pair of groups G1 and G2, we can define p1 = er(x1,y1)+r(x2,y2) for G1, and p2 = er(x1,y2)+r(x2,y1) for G2, as p(G1 G2) = erG1 erG1 + erG2 rG1 = r(x1, y1) + r(x2, y2) rG2 = r(x1, y2) + r(x2, y1) (6) Next, combining Eq. 6 and Eq. 4, we can further derive as follows (details in Appendix E): p(G1 G2) = σ (cid:19) (Π1 + Π2) (7) (cid:18) 1 2 Π1 = 2β log β log Π2 = 2β log β log πr(y1x1) πref (y1x1) πr(y1x2) πref (y1x2) πr(y2x2) πref (y2x2) πr(y2x1) πref (y2x1) β log πr(y2x1) πref (y2x1) β log πr(y1x2) πref (y1x2) (8) where σ is the sigmoid function. Therefore, the optimization objective of IOPO is to maximize p(G1 G2). Motivated by Rafailov et al. (2023), we can formulate maximum likelihood loss for parametrized policy model πθ as follows: LIOPO(πθ) = Ex1,y1,x2,y2D (cid:26) (cid:20) log σ (cid:18) 1 2 (2βlog βlog βlog πθ(y2x1) πref (y2x1) πθ(y1x2) πref (y1x2) βlog βlog πθ(y1x2) πref (y1x2) πθ(y2x1) πref (y2x1) + 2βlog (cid:19)(cid:21)(cid:27) ) πθ(y1x1) πref (y1x1) πθ(y2x2) πref (y2x2) Evaluation Datasets. We conduct experiments on three instruction-following datasets: TRACE, IFEval (Zhou et al., 2023), and CFBench (Zhang et al., 2024a). TRACE evaluation set is introduced in this paper, which has 1,042 instructions, and an average of 4.89 constraints per instruction, with maximum of 15 constraints. IFEval consists of 541 prompts, with each prompt containing one or multiple verifiable instructions. CFBench contains 1,000 samples that cover more than 200 real-life scenarios and over 50 NLP tasks, with each sample including multiple constraints. It is worth noting that TRACE is the in-domain evaluation set, IFEval and CFBench are the out-of-domain ones. Implementation Details. (1) TRACE Benchmark: we choose Qwen2-72B-Instruct (Yang et al., 2024a)1 for benchmark construction. (2) IOPO Alignment: we choose Qwen2-7B-Instruct2, and LLaMA3.1-8B-Instruct3 as the LLM backbone. All models, except for the base models (i.e. Qwen27B-Instruct and Llama-3.1-8B-Instruct), are trained on TRACEs training set or its variants (Train Once, Test Anywhere). The learning rate is 1e-4 for supervised fine-tuning (SFT), and 5e-6 for DPO and IOPO. The maximum length and epoch are set to 6,000 and 3 respectively. β is set to 0.1. We implement our code based on LLaMA-Factory (Zheng et al., 2024), perform parallel training on 4 8GPU machines, with batch size of 1 per GPU. The DPO training data construction is shown in Appendix C. Evaluation Metrics. For TRACE, we use GPT4o to evaluate if all constraints in the instruction have been followed (IF-S for single-constraint instructions, and IF-M for multi-constraint instructions), as described in Sec. 4.3. For IFEval, we use prompt-level strict and loose accuracy defined in Zhou et al. (2023), abbr. S-Acc and L-Acc respectively. CFBench (Zhang et al., 2024a) introduces three evaluation metrics with GPT-4o as the evaluation model: constraint satisfaction rate (CSR), instruction satisfaction rate (ISR), and priority satisfaction rate (PSR). 1https://www.modelscope.cn/models/Qwen/Qwen2-72BInstruct 2https://modelscope.cn/models/Qwen/Qwen2-7BInstruct 3https://huggingface.co/meta-llama/Llama-3.1-8B- (9) Instruct 6 Model Method TRACE IFEval CFBench IF-S 72.5 76.0 77.0 79.0 IF-M S-Acc L-Acc CSR 54.5 56.1 57.7 67.2 51. 52.3 51.4 52.7 56.4 54.2 53. 58.2 75.8 77.8 76.2 80.0 ISR 39.1 40.4 38.8 45.1 PSR 50. 52.9 50.6 57.9 Instruct SFT PPO DPO IOPO (Ours)Improv. 82.03.0 68.91.7 59.97.2 63.65.4 80.70.7 47.01.9 58.70.8 Instruct SFT PPO DPO 67.5 75.5 75.0 79.0 52.9 62. 57.3 69.2 74.3 71.0 69.9 71. 78.6 74.1 72.3 76.5 71.4 78. 75.9 80.8 35.7 43.2 40.9 48. 46.9 54.7 50.7 59.8 IOPO (Ours)Improv. 81.52.5 70.71.5 78.26.7 81.04.5 81.81.0 49.91.8 61.11.3 Qwen2-7B Llama3.1-8B Table 3: Main results on in-domain TRACE, and out-of-domain IFEval, and CFBench."
        },
        {
            "title": "CFBench",
            "content": "IF-S IF-M S-Acc L-Acc CSR ISR PSR"
        },
        {
            "title": "IOPO",
            "content": "82.0 68.9 Qwen2-7B w/o Output Pref 81.0 66. w/o Input Pref 80.9 67."
        },
        {
            "title": "IOPO",
            "content": "81.5 70.7 Llama3.1-8B w/o Output Pref 81.5 69. w/o Input Pref 79.0 69.0 59.9 55.1 56. 78.2 77.3 77.9 63.6 60.5 61. 81.0 80.6 80.2 80.7 47.0 58. 79.4 46.6 56.3 79.7 46.8 57. 81.8 49.9 61.1 80.6 48.6 58. 80.9 48.3 59.4 Table 4: Ablation studies on TRACE, IFEval, and CFBench."
        },
        {
            "title": "6.2 Experimental Results",
            "content": "Main Results. As shown in Table 3, we give the main results under different benchmarks, including in-domain TRACE, out-of-domain IFEval and CFBench. The experiments are conducted under two different base models, Qwen2-7B, and Llama3.18B, where Instruct means directly using Qwen27B-Instruct or Llama3.1-8B-Instruct for inference, SFT represents the model is trained on TRACE training set, and PPO, DPO, IOPO are respectively trained on preference data derived from TRACE training set. For in-domain evaluation on TRACE set, we can see 3.0%, 1.7% improvements of IOPO on singleand multi-constraint instructions with Qwen2-7B as the base model compared to DPO, and 2.5%, 1.5% improvements with Llama3.1-8B as the base model. For out-of-domain evaluation on IFEval and CFBench, IOPO achieves an average increase of 3.2%, and 3.06% in comparison with DPO based on Qwen2-7B and Llama3.1-8B respectively. The significant advantages of both in-domain and outof-domain evaluations confirm the effectiveness of input-output preference optimization, which intensively considers the constraint differences between instructions, enhancing the models perception of constraints. It is worth noting that IOPO has larger performance gap with SFT especially on IFEval, compared to DPO and SFT, which confirms the generalization of IOPO and the necessity of further modeling input preferences. Ablation Studies. To further confirm the effectiveness of input and output preference, we conduct the ablation studies on TRACE, IFEval, and CFBench as shown in Table 4, where w/o Out7 Method SFT DPO IOPO #Memory #Training Time #Inference Speed 1 14.54 1 2 26.30 1 4 34.27 1 Table 5: Analysis on the consumed GPU memory, training time, and inference speed under the same batch size. put Pref means we only consider the modeling of input preference with the same training data, w/o Input Pref means we only consider the modeling of output preference. We can see that output preference contributes to 2%, and 0.93% increases with Qwen2-7B and Llama3.1-8B respectively, input preference separately brings 1.51% and 1.58% performance gains, which confirms the effectiveness of both input and output preference modeling. Besides the paradigm for modeling output preference in existing alignment methods, its established that modeling input preference is crucial for deeply considering constraints within the instruction. Complexity Analysis. We conduct the analyses of complexity in Table 5, where all methods are conducted under the same experimental settings, such as the batch size and GPU. (1) For #Memory, DPO and IOPO are approximately twice and four times that of SFT respectively, because DPO needs pair of responses to calculate the corresponding loss (<x, y1>, <x, y2>), and IOPO needs to compute four groups of input-output pairs (<x1, y1>, <x2, y2>, <x1, y2>, <x2, y1>) in its loss. (2) For #Training Time, DPO and IOPO require the computation of more tokens compared to SFT under the same batch size, leading to longer training time. (3) For #Inference Speed, SFT, DPO, and IOPO are all the same base model optimized for inference, resulting the same inference speed. The training efficiency and GPU memory usage of IOPO are not the best among compared baselines, but their efficiencies are still of the same order of magnitude, which are reasonable and acceptable comprehensively considering the significant performance advantage. The Impact of Token Quantity. To address concerns regarding the IOPO training tokens, we conduct the analyses on the impact of token quantity and report the results in Figure 4, and Figure 5. For IOPO, there exist two instructions along with their corresponding responses ({<x1, y1>, <x2, y2>}). To ensure that DPO and IOPO consume Figure 4: Performance comparisons under the same quantity of tokens with Qwen2-7B as the base model. Figure 5: Performance comparisons under the same quantity of tokens with Llama3.1-8B as the base model. the same number of tokens, we construct two pairs of output preferences based on IOPOs instructions x1 (ywin = y1, yloose = y2), and x2 (ywin = y2, yloose = y1) for training DPO model, denoted by DPO. Similarly, we train SFT model with instruction data {<x1, y1>, <x2, y2>}, denoted by SFT. We can observe that increasing the token quantity does indeed yield better performance on some datasets. For example, compared to DPO, DPO has achieved performance improvement on IFEval (S-Acc, L-Acc) with Qwen2-7B as the base model. However, there are also cases of decline, such as comparing DPO with DPO on CFBench (CSR, PSR), which indicates that it is not the case that more tokens always lead to better performance. At the same time, although consuming the same number of tokens, SFT and DPO still have certain gaps compared to the proposed IOPO, which confirms that the performance improvement of our IOPO does not primarily come from using more tokens, but rather from better constraint-aware modeling of input-output preferences."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper focuses on the ability of LLMs to follow complex instructions, and introduces TRACE, multi-constraint complex instruction benchmark which consists of 120K training samples and 1K test cases. Furthermore, we propose IOPO align8 ment method by taking both input and output preferences into account, enabling LLMs to directly learn response preferences and subtly perceive constraints in instructions. The empirical results from extensive testing across in-domain and out-ofdomain datasets demonstrate the efficacy of IOPO, with notable improvements of 2.18% and 3.13% compared to DPO, respectively. For future work, we expect to introduce more in-depth reasoning process to improve constraint-aware abilities."
        },
        {
            "title": "Limitations",
            "content": "In TRACE, the evaluation set has undergone strict manual verification but the training set has not performed this process considering the cost. Although the models trained on the training set have achieved the significant improvements, we believe that if we can further improve the quality of the training set, it will lead to better model performance."
        },
        {
            "title": "References",
            "content": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024. Self-play with execution feedback: Improving instruction-following capabilities of large language models. arXiv preprint arXiv:2406.13542. Jie Gao, Simret Araya Gebreegziabher, Kenny Tsu Wei Choo, Toby Jia-Jun Li, Simon Tangi Perrault, and Thomas Malone. 2024. taxonomy for humanllm interaction modes: An initial exploration. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pages 111. Yingqiang Ge, Wenyue Hua, Kai Mei, jianchao ji, Juntao Tan, Shuyuan Xu, Zelong Li, and Yongfeng Zhang. 2023. Openagi: When llm meets domain experts. In Advances in Neural Information Processing Systems, volume 36, pages 55395568. Curran Associates, Inc. Peter Hase and Mohit Bansal. 2022. When can models learn from explanations? formal framework for understanding the roles of explanation data. In Proceedings of the First Workshop on Learning with Natural Language Supervision, pages 2939. Association for Computational Linguistics. Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, and Yanghua Xiao. 2024a. From complex to simple: Enhancing multi-constraint complex instruction following ability of large language models. arXiv preprint arXiv:2404.15846. Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. 2024b. Can large language models understand real-world complex instructions? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1818818196. David R. Hunter. 2004. MM algorithms for generalized Bradley-Terry models. The Annals of Statistics, 32(1):384 406. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2024. FollowBench: multi-level fine-grained constraints following benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46674688. Association for Computational Linguistics. Callie Kim, Christine Lee, and Bilge Mutlu. 2024. Understanding large-language model (llm)-powered human-robot interaction. In Proceedings of the 2024 ACM/IEEE International Conference on HumanRobot Interaction, pages 371380. Yizhi Li, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Noah Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, Wangchunshu Zhou, Yiming Liang, Lei Zhang, Lei Ma, Jiajun Zhang, Zuowen Li, Wenhao Huang, Chenghua Lin, and Jie Fu. 2024. CIF-bench: Chinese instruction-following benchmark for evaluating the generalizability of large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 1243112446, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Renze Lou, Kai Zhang, and Wenpeng Yin. 2023. comprehensive survey on instruction following. arXiv preprint arXiv:2303.10475. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2024. Wizardcoder: Empowering code large language models with evolinstruct. In The Twelfth International Conference on Learning Representations. Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with reference-free reward. In Annual Conference on Neural Information Processing Systems (NeurIPS). 9 Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024a. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. 2024b. Harnessing the power of llms in practice: survey on chatgpt and beyond. ACM Transactions on Knowledge Discovery from Data, 18(6):132. Qinyuan Ye and Xiang Ren. 2021. Learning to generate task-specific adapters from task description. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 646653. Association for Computational Linguistics. Tao Zhang, Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Fan Yang, Mingan Lin, Yujing Qiao, Weipeng Chen, Bin Cui, et al. 2024a. Cfbench: comprehensive constraints-following benchmark for llms. arXiv preprint arXiv:2408.01122. Xinghua Zhang, Haiyang Yu, Yongbin Li, Minzheng Wang, Longze Chen, and Fei Huang. 2024b. The imperative of conversation analysis in the era of llms: survey of tasks, techniques, and trends. arXiv preprint arXiv:2409.14195. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 34703487. Association for Computational Linguistics. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. InFoBench: Evaluating instruction following ability in large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 13025 13048. Association for Computational Linguistics. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, volume 36, pages 5372853741. Curran Associates, Inc. Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2024. Preference ranking optimization for human alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1899018998. Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang. 2024. Conifer: Improving complex constrained instructionfollowing ability of large language models. arXiv preprint arXiv:2404.02823. Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, et al. 2024. Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. In EMNLP. Association for Computational Linguistics. Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxin Xu, et al. 2024. Benchmarking complex instruction-following with mularXiv preprint tiple constraints composition. arXiv:2407.03978."
        },
        {
            "title": "A Taxonomy of Constraint",
            "content": "Constraint Type Content Constraint Constraint Dimension Theme Constraint Exclusion Constraint Inclusion Constraint Value Constraint Privacy Constraint Numerical Constraint"
        },
        {
            "title": "Situation Constraint",
            "content": "Role-Playing Constraint"
        },
        {
            "title": "Tone and Style Constraint",
            "content": "11 Description The generated content should focus on specific topic or field. Clearly specify the information or content that should not be included in the generated content. Clearly specify the particular information or content that must be included in the generated content. The generated content should not contain information that violates values, such as safety, false information, discrimination, or bias. The generated content should not include details that may infringe on privacy, such as personal data or sensitive information. Limit the length and number of words, sentences, and paragraphs in the generated content, or use numerical precision constraints to ensure accuracy. The generated content should be based on specific role or situational background. The generated content should target specific audience, which affects the terminology used, the level of detail provided, and the complexity of the content. When specific intention is met, particular process should be followed to perform an operation or output specific content. Add natural language form process information, such as procedures or business processes, to assist in generating answers. Add markdown-formatted process information, such as procedures or business processes, to assist in generating answers. Background information is presented in table form, providing series of markdownformatted tables to assist in generating answers. Background information is presented in text form, providing series of textual background information to assist in generating answers. The generated content should adopt specific tone and style, such as formal, polite, academic, concise, literary, romantic, or sci-fi. Emotion Constraint The generated content should express specific emotion or mood, such as ensuring the content is positive, inspiring, or empathetic."
        },
        {
            "title": "Citation Constraint",
            "content": "Text Pattern Constraint Multilingual Constraint Output Format Constraint Grammar Structure Constraint Linguistic Characteristics Constraint Use specific linguistic features, such as metaphors, personification, and other rhetorical devices. The content should be generated in specific language or switch between languages according to complex patterns. The generated content should be in specific data format, such as tables, JSON, HTML, LaTeX, or Markdown. Use specified fonts and font sizes, or special emoji, to ensure readability across different devices and platforms. The generated content should strictly follow specific grammatical structures, such as subject-predicate-object, subject-verb, etc. The generated content should include citations to sources, providing reliable sources and literature support; follow specific citation formats or reference styles. The generated content should use numbered lists or bullet points to organize information. The generated content should be organized according to specific hierarchical structure, such as using headings and subheadings. The generated content should follow specific layout or format, such as text alignment, paragraph indentation, and structural templates like introduction-bodyconclusion. Provide examples that meet the requirements, and require the model to generate content based on these examples. Provide examples that do not meet the requirements, and require the model to avoid generating content similar to these examples."
        },
        {
            "title": "Template Constraint",
            "content": "Format Constraint"
        },
        {
            "title": "Example Constraint",
            "content": "Table 6: Five constraint types and 26 constraint dimensions with their corresponding descriptions."
        },
        {
            "title": "B Prompt",
            "content": "Instruction Structuring Prompt: Constraint Expansion Prompt: /* Task Prompt */ You are an instruction enhancer. Given an instruction, you need to modify it by adding constraints to make it more complex. You can choose several appropriate types of constraints from those given below, but you must maintain the thematic consistency of the original instruction. {Constraints} /* Example */ INPUT <instruction>: Introduce Beijing to me. OUTPUT <explain>: Modify the original instruction by adding constraints from two types: text style and numerical. <result>: Introduce Beijing to me, requiring concise summary with no more than 1000 words, and bold any numbers and place names mentioned in the content. INPUT <instruction>: Provide short article about species extinction that includes at least 3 causes and 3 solutions. OUTPUT <explain>: Modify the original instruction by adding constraints from three types: numerical, citation and reference, and inclusion. <result>: Provide short article on species extinction. The length of the article should be between 500 and 800 words, detailing at least 3 reasons for species extinction and proposing at least 1 specific solution for each reason. The article should include an introduction and conclusion, and cite at least 2 scientific papers as support in the discussion. /* Input */ INPUT <instruction>: {} OUTPUT /* Task Prompt */ You are provided with an instruction. As prompt engineer, your task is to extract the task description, constraints, and the input contained in the given instruction. /* Requirements */ If there is no constraints information that can be extracted from the instruction, only output NULL in the constraints field. If there is no input information that can be extracted from the instruction, only output NULL in the input field. Information in the input field and constraints field cannot be duplicated. Information in the input field and task description field cannot be duplicated. Information in the task description field and constraints field cannot be duplicated. The content extracted for the task description, constraints, and input elements should be consistent with the semantics of the instruction to be extracted. Evaluate the quality of the instruction; if the instruction is poor, incomplete, or contradictory, do not perform constraints extraction. /* Example */ INPUT <instruction>: Help me answer by introducing Beijing, where the format of the generated answer needs to be JSON, the character length of the data field in the JSON object must not exceed 1000 characters to ensure the answer is concise and direct; the generated answer must be objective facts and cannot include any subjective opinions or conjectures; all characters must be UTF-8 encoded to ensure support for multilingual responses; the answer must not contain any personal privacy information, copyrightrestricted content, or information that violates relevant laws and policies; numbers, place names mentioned in the answer need to be specially marked in bold. OUTPUT #Task Description: Introduce Beijing #Constraints: The format of the generated answer needs to be JSON; 13 The answer style is concise, with character length not exceeding 1000 characters; All characters must be UTF-8 encoded to ensure support for multilingual responses; The generated answer must be objective facts and cannot include any subjective opinions or conjectures; The answer must not contain any personal privacy information, copyright-restricted content, or information that violates relevant laws and policies; Numbers, place names mentioned in the answer need to be specially marked in bold; #Input: NULL /* Input */ INPUT <instruction>: {} OUTPUT Judge Completeness Prompt: /* Task Prompt */ You are an instruction integrity discriminator, capable of determining whether given instruction is complete. /* Requirements */ The given instruction consists of three parts: <Task Description, Constraints, Input>, where Input can be NULL; You can refer to the examples given in Example, but you should not directly copy the examples; /* Example */ INPUT #Task Description: Select the correct word from multiple options to fill in the blank that fits the context and sentence flow, and briefly explain the reason for the choice. #Constraints: The chosen word must fit the overall logic of the sentence; The selected word must not only be grammatically correct but also fit the context of the sentence;"
        },
        {
            "title": "The reason for the choice must be briefly",
            "content": "explained in parentheses; #Input: My car is _____ and blue. OUTPUT <explain>: The Task Description mention selecting the correct word from multiple choices to fit the context and sentence flow, but the instruction does not provide any options, so this instruction is incomplete. <tag>: Incomplete INPUT #Task Description: Calculate the area of US dollar bill. #Constraints: The calculation result should be rounded to two decimal places; The answer should detail each step of the calculation process. #Input: The length of US dollar bill is 6.14 inches, and the width is 2.61 inches. OUTPUT <explain>: The Task Description mention calculating the area of US dollar bill, and the Inputs provide the length of the US dollar bill as 6.14 inches and the width as 2.61 inches. Additionally, the Constraints section provides several conditions, so this instruction is complete. <tag>: Complete /* Input */ INPUT {} OUTPUT Judge Redundancy Prompt: /* Task Prompt */ You are the redundancy detector for instructions, capable of determining whether given instructions are redundant; /* Requirements */ The given instructions consist of <Task Description, Constraints, Input>, where Input can be NULL; You can refer to the Examples provided, but you should not directly copy the examples; /* Example */ INPUT #Task Description: 14 Provide detailed explanation of the phrase on an even keel, including its origin and modern usage in English, presented in professional academic tone and formatted in Markdown. #Constraints: The explanation must be given in professional academic tone; The output format must be Markdown; The explanation must include the origin of the phrase and its usage in modern English; #Input: NULL OUTPUT <explain>: The Task Description mention including its origin and modern usage in English, presented in professional academic tone and formatted in Markdown, which overlaps with Constraints 1, 2, and 3. Therefore, this instruction is redundant. <tag>: Redundant INPUT #Task Description: Introduce new product line to consumers who have high interest in environmental protection, emphasizing the environmental characteristics of the products and how they enhance quality of life. #Constraints: The introduction should be enthusiastic and inspiring; The article length should be controlled between 200 and 300 words to maintain reader attention; Clearly state the core selling points: causing no harm to the environment and influencing your life; Elaborate on the specific meanings of the revolution of sustainable living and being the change you wish to see. #Input: NULL OUTPUT <explain>: There is no repetition between the information mentioned in the Task Description and the four constraints listed under Constraints, so this instruction is not redundant. <tag>: Not Redundant /* Input */ INPUT {} OUTPUT Response Generation Prompt: {Instruction} Response Evaluation Prompt: [System] You are fair judge, and please evaluate the quality of an AI assistants responses to user query. You need to assess the response based on the following constraints. We will provide you with the users query, some constraints, and the AI assistants response that needs your evaluation. When you commence your evaluation, you should follow the following process: 1. Evaluate the AI assistants response on different constraints, and after each constraint evaluation, assign score from 0 to 10. 2. Aggregate the assessments from each constraint to give an overall score for the AI assistants response, ranging from 0 to 10. 3. Your scoring should be as strict as possible, overall, the higher the quality of the models response, the higher the score. 4. When the models response is irrelevant to the question, or contains significant factual errors, or generates harmful content, the Constraints Overall Score must be 0 points. 5. It is necessary to strictly follow the format in the /* Example */ for generation, the Fine Grained Score format is Json, and Constraints Overall Score format is List. Please remember to provide evaluations and explanations before your scoring. After your explanation of each constraint, include score for that constraint. /* Example */ INPUT #Task Description: Create password for this account #Constraints: The password must be at least 8 characters long; 15 It must contain 1 uppercase letter; It must contain 1 lowercase letter; It must include 2 numbers; #Input: NULL #Response: Ax7y4gTf OUTPUT Explanation: Password Length: The password Ax7y4gTf is 8 characters long, meeting the first constraint, scoring 10 points. Contains 1 uppercase letter: The password Ax7y4gTf contains two uppercase letters, and T, which means it meets the second constraint, but the explanation incorrectly states it does not meet the constraint, scoring 0 points. Contains 1 lowercase letter: The password Ax7y4gTf contains three lowercase letters, x, y, and g, which means it meets the third constraint, but the explanation incorrectly states it does not meet the constraint, scoring 0 points. Includes 2 numbers: The password Ax7y4gTf includes two numbers, 7 and 4, meeting the fourth constraint, scoring 10 points. Fine Grained Score: [ { \"The password must be at least 8 characters long\": 10, \"It must contain 1 uppercase letter\": 0, \"It must contain 1 lowercase letter\": 0, \"It must include 2 numbers\": 10 } ] Constraints Overall Score: [[5]] /* Input */ INPUT #Task Description: {task_description} #Constraints: {constraint} #Input: {input} <response>: {ans} OUTPUT DPO-Series Data Construction We construct DPO training data based on TRACE training set by prompting Qwen2-72B-Instruct to generate worse response yloose compared to original response ywin. The construction process is Figure 6: DPO-series Data Construction. depicted in Figure 6, the prompt is shown as follows: #Task Description: {task_description} #Constraints: {constraint} #Input: {input} #Ref: The provided answer is: {response} According to #Task Description, #Constraints and #Input, please generate Worse answer in terms of complying with the #Constraint than the provided one. Please ONLY output the answer."
        },
        {
            "title": "D IOPO Data Construction",
            "content": "We construct IOPO training data based on TRACE training set by the following steps (the detailed process is shown in Figure 7): Step 1: prompting Qwen2-72B-Instruct to generate new constraints by add, remove, and revise operations, making the response not comply with the new constraints, and then the task description, new constraints, and input are combined to form x2. The corresponding prompt is as follows: x2 Generation Prompt: #Task Description: {task_description} Figure 7: IOPO Data Construction. #Constraints: {constraint} #Input: {input} # Ref: The provided answer is: {response} According to #Task Description, #Constraints and #Input, please {OP} items of original CONSTRAINTS to generate the new CONSTRAINTS, making the provided answer NOT comply with the new CONSTRAINTS."
        },
        {
            "title": "Please ONLY output",
            "content": "the new CONSTRAINTS. OP can be randomly selected from {ADD new items into the, DELETE partial, REVISE specific} according to uniform distribution. Step 2: For instruction x2, we prompt Qwen272B-Instruct to generate the corresponding response y2. The prompt is Response Generation Prompt. Step 3: We finally prompt Qwen2-72B-Instruct to evaluate the response y2, and only keep the fullscore ones. The prompt is Response Evaluation Prompt. Derivation for p(G1 G2) As described in Eq. 6 as follows: p(G1 G2) = er(x1,y1)+r(x2,y2) er(x1,y1)+r(x2,y2) + er(x1,y2)+r(x2,y1) (10) As described in Eq. 4, the reward function r(x, y) can be represented by the policy model πr as follows: r(x, y) = β log πr(yx) πref(yx) + β log Z(x) (11) Combining above equations, we can derive that: = exp (cid:16) β log π (y1 x1) πref (y1x1) + β log Z(x1) + β log π (y2x2) (cid:17) πref (y2x2) + β log Z(x2) + exp (cid:16) β log π (y2x1) πref (y2x1) + β log Z(x1) + β log π (y1x2) πref (y1x2) + β log Z(x2) (cid:17) exp (cid:16) β log π (y1x1) πref (y1x1) + β log Z(x1) + β log π (y2x2) πref (y2x2) + β log Z(x2) (cid:17) = (cid:16) 1 + exp β log π (y2x1) πref (y2x1) β log π (y1 x1 ) πref (y1x1) + β log π (y1 x2) πref (y1x2) β log π (y2x2) πref (y2x2) (cid:17) 1 1 2 = σ (2β log (cid:124) π(y1x1) πref (y1x1) β log π(y2x1) πref (y2x1) β log (cid:123)(cid:122) <x1,y1> for different x,y π(y1x2) πref (y1x2) (cid:125) + 2β log (cid:124) (12) π(y2x2) πref (y2x2) β log π(y1x2) πref (y1x2) β log (cid:123)(cid:122) <x2 ,y2> for different x,y π(y2x1) πref (y2x1) (cid:125) )"
        }
    ],
    "affiliations": [
        "Alibaba Inc."
    ]
}