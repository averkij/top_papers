{
    "paper_title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic",
    "authors": [
        "Mohammad Zbeeb",
        "Hasan Abed Al Kader Hammoud",
        "Bernard Ghanem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as a compact task vector. We source two publicly available, identically initialized Qwen2.5 models, one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract a reasoning vector: $v_{\\text{reason}} = \\theta_{\\text{GRPO}} - \\theta_{\\text{SFT}}$. We hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation (-11.8% on GSM8K), demonstrating the vector's strong contribution to the model's reasoning abilities. This work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering a practical way to enhance models by recycling prior computational investments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 3 6 3 1 0 . 9 0 5 2 : r Preprint - Under Review REASONING VECTORS: TRANSFERRING CHAIN-OFTHOUGHT CAPABILITIES VIA TASK ARITHMETIC Mohammad Zbeeb1,2 Hasan Abed Al Kader Hammoud1 Bernard Ghanem1 1King Abdullah University of Science and Technology (KAUST) 2American University of Beirut (AUB)"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as compact task vector. We source two publicly available, identically initialized QWEN2.5 models, one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract reasoning vector: vreason = θGRPO θSFT. We hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation (-11.8% on GSM8K), demonstrating the vectors strong contribution to the models reasoning abilities. This work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering practical way to enhance models by recycling prior computational investments. Figure 1: Merging the Fine-Tuning and Reasoning Vectors. Let = θf θ0 denote the finetuning vector (f ) and vreason = θr θf denote the reasoning vector (r). By adding vreason to base model, we obtain an enhanced model with improved reasoning capabilities, effectively transferring the outcome of the reinforcement learning phase. This work was completed during the authors research internship at KAUST. 1 Preprint - Under Review"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) excel at knowledge retrieval, but often falter on multistep reasoning tasks. While training-time methods like reinforcement learning from human feedback (RLHF) [23; 36] robustly improve reasoning, they demand substantial computational resources and intricate tuning. This high cost limits their broad application, creating the need for more accessible methods to enhance reasoning. Task arithmetic [13] presents paradigm in which the capabilities learned during fine-tuning can be represented as vectors and transferred between models by arithmetic. This raises compelling question: Can complex reasoning capabilities, acquired through resource-intensive reinforcement learning, be extracted from existing models and transferred as reusable task vectors? This paper answers affirmatively by introducing the concept of reasoning vector. Our approach takes advantage of the growing availability of open-source models. We source two models with identical initialization and training data history, differing only in their final optimization stage: one with supervised fine-tuning (θSFT) and the other with Group Relative Policy Optimization (θGRPO). We define the reasoning vector as their difference: vreason = θGRPO θSFT. This controlled subtraction aims to isolate the parameter changes associated with the RL-induced reasoning enhancements, while minimizing the influence of dataset-specific knowledge shared by both models. Adding this vector to compatible instruction-tuned model allows for the transfer of reasoning abilities without requiring any new training. This is illustrated in Figure 1. This method allows for the reuse of the significant computational effort already invested in training advanced models. By sourcing checkpoints from public model hubs, one can enhance base models reasoning capabilities through few simple tensor operations. Our experiments on QWEN2.5 models (1.5B and 7B parameters) show consistent performance improvements across diverse reasoning benchmarks. For the 1.5B model, adding the reasoning vector improves accuracy on GSM8K by 4.9%, HumanEval by 4.3%, and BigBenchHard by 12.3%. These gains hold under adversarial perturbations. An ablation study confirms the vectors impact: its removal degrades performance on GSM8K by 11.8%, falling below the SFT baseline. Our contributions are: 1. We demonstrate that reasoning capability associated with reinforcement learning can be extracted as modular vector component from existing, publicly available models. 2. We show that reasoning vector derived from mathematical training data generalizes to improve performance on other domains, including code generation, scientific QA, and logical deduction. 3. We provide reproducible method that leverages open-source checkpoints and simple tensor arithmetic, increasing the accessibility of reasoning-enhanced models by promoting the reuse of existing resources."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Reasoning in Large Language Models. Enhancing reasoning in LLMs follows two primary approaches. Prompting strategies generate reasoning from existing parameters: chain-of-thought prompting [29] encourages step-by-step verbalization, self-consistency [28] samples multiple reasoning paths, tree-of-thoughts [33] explores branching logic, and zero-shot reasoning triggers like think step by step [15] activate latent capabilities. Program-aided approaches, such as Program-ofThought (PoT) [3] and Program-Aided Language Models (PAL) [8], offload computation to external interpreters. Training-based methods directly encode reasoning through supervised fine-tuning on annotated datasets [5; 10] or reinforcement learning, including RLHF [23; 4], PPO [26], and GRPO [24]. Some methods combine RL with verifier models [5; 17] to evaluate reasoning steps. Despite progress, benchmarks reveal that even advanced models can struggle with complex multistep reasoning. Our work bridges these approaches: we extract the capabilities learned through RL and transfer them without requiring additional training cycles. Task Arithmetic and Model Merging. Task arithmetic [13] demonstrates that fine-tuning capabilities can be represented as vectors in parameter space and composed through arithmetic operations. 2 Preprint - Under Review Extensions include TIES-Merging [31], which reduces interference by resolving sign conflicts; Fisher merging [19], which weights parameters by their importance; and RegMean [14], which formulates optimal parameter combinations as regression problem. Model soups [32] average model weights for single-task improvement, while methods like Ratatouille [25] target out-of-domain generalization. Linear Mode Connectivity [7; 1] provides theoretical foundation, showing that models fine-tuned from an identical initialization lie in connected low-loss regions, enabling safe linear interpolation. Recent work has scaled these techniques to billion-parameter models [12; 34], with practical tools like MergeKit [9] operationalizing parameter space composition. While prior work focuses on domain knowledge or task-specific skills, we investigate whether complex cognitive capability like multi-step reasoning, acquired via RL, can similarly be isolated and transferred. Modular Capability Enhancement. Parameter-efficient methods add small, trainable components to frozen models: LoRA [11] introduces low-rank adaptation matrices, prefix tuning [18] prepends learnable tokens, and prompt tuning [16] optimizes continuous prompts. Knowledge editing techniques [20] modify specific facts by targeting individual neurons, while tangent space methods [22] aim to improve weight disentanglement for better merging. Instruction tuning has become particularly amenable to merging, with studies showing that averaging instruction-tuned experts can outperform standard multitask training [37; 2]. Unlike these approaches, which often require task-specific training or narrow edits, our method explores training-free, global enhancement. We examine if reasoning vector derived from mathematical training (GSM8K) can generalize to improve code generation, scientific QA, and logical deduction, suggesting that reasoning can be treated as transferable capability distinct from task-specific patterns [35; 30]."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 PROBLEM FORMULATION Consider two models sourced from public repository that share an identical architecture, initialization, and pre-training history. Let θSFT denote the parameters of model that has undergone supervised fine-tuning on specific dataset (e.g., the GSM8K training set) using standard crossentropy loss. Let θGRPO denote the parameters of counterpart model that was optimized using Group Relative Policy Optimization (GRPO), reinforcement learning algorithm, on the same dataset with reasoning-focused reward function. This controlled comparison allows us to isolate the impact of the reasoning vector itself indepedent of the pretraining dataset. 3.2 REASONING VECTOR EXTRACTION AND TRANSFER We define the reasoning vector, vreason, as the difference in parameters between these two models: vreason = θGRPO θSFT (1) We hypothesize that this vector, vreason, captures the essential parameter updates introduced by the reinforcement learning process that enhance multi-step reasoning. Because both donor models share the same data and base knowledge, the subtraction is intended to factor out this shared, dataset-specific information, leaving behind more general representation of the reasoning capability. Given target instruction-tuned model, θtarget, that is compatible with the donor models, we can enhance its reasoning ability through simple arithmetic operation: θenhanced = θtarget + α vreason (2) where α [0, 1] is scalar coefficient that controls the magnitude of the transferred vector. For more fine-grained control, this operation can be applied to specific layers or modules by introducing binary mask {0, 1}θ: θenhanced = θtarget + α (m vreason) (3) where denotes element-wise multiplication. In our experiments, we found that applying the full vector (m = 1) with scaling factor of α = 1 was consistently effective, suggesting the extracted vector is well-calibrated for transfer without needing further adjustment. 3 Preprint - Under Review Figure 2: Reasoning vector operations in weight space. Each panel illustrates different transformation: (1) Vector injection shifts base model toward improved reasoning. (2) Negation removes the reasoning component, suppressing chain-of-thought behavior. (3) Addition combines multiple skill vectors, enabling multi-skill reasoning. (4) Analogy-style composition transfers capabilities across domains, supporting generalization. 3.3 THEORETICAL FOUNDATION The safety and effectiveness of this transfer relies on the principle of Linear Mode Connectivity (LMC) [7]. LMC states that when two models are fine-tuned from the same initialization, they typically lie in the same connected low-loss basin of the optimization landscape. Formally, for parameters θA and θB obtained from the same starting point, their convex interpolation satisfies: L(λθA + (1 λ)θB) max (cid:0)L(θA), L(θB)(cid:1) + ϵ, λ [0, 1], (4) where is the loss function and ϵ is small value. Intuitively, this inequality guarantees that the straight line between θA and θB in weight space does not leave the low-loss region; moving continuously from one model to the other does not increase the loss beyond that of the worse endpoint. In geometric terms, both models occupy the same flat \"valley\" of the loss surface, and the connecting path avoids high-loss barriers. Because θSFT and θGRPO share the same initialization and were trained on the same data, they are expected to satisfy the conditions for LMC. Their difference vector, vreason = θGRPO θSFT therefore points in direction within this shared low-loss basin. Adding this vector to another compatible model corresponds to moving it along trajectory that has been implicitly validated to remain within stable, low-loss region. This explains why the transfer is effective and can enhance reasoning ability without catastrophically destabilizing the base models existing capabilities. 3.4 IMPLEMENTATION DETAILS The complete procedure for enhancing model requires only two tensor operations, as summarized below. Extract: vreason θGRPO θSFT Transfer: θenhanced θtarget + vreason (5) (6) These operations are element-wise and computationally inexpensive. The vector vreason can be pre-computed and stored, then applied to any number of compatible target models on demand. Compatibility Requirements. For successful transfer, the target model must satisfy: 1. Architecture Match: Identical layer structures, hidden dimensions, and parameter tensor shapes. 2. Tokenizer Compatibility: The same vocabulary and token-to-ID mapping to ensure semantic alignment, especially in the embedding layer. 4 Preprint - Under Review 3. Initialization Similarity: The models should ideally originate from the same pre-trained checkpoint family to ensure their parameter spaces are sufficiently aligned. Activation Strategy. While the enhanced models demonstrate improved performance intrinsically, we find that prefixing input prompts with simple instruction like Think step by step reliably activates and enhances the transferred reasoning capability. This minimal cue appears to prime the model to leverage the problem-solving pathways encoded in the reasoning vector, analogous to how RL-trained models are conditioned to engage in systematic reasoning."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate the effectiveness of reasoning vector transfer across multiple dimensions: performance on core reasoning benchmarks, robustness under adversarial conditions, and ablations to understand the vectors impact. Our experiments utilize QWEN2.5 models at 1.5B and 7B parameter scales."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Model Configuration. We use publicly available QWEN2.5 models at 1.5B and 7B scales. For each size, our donor models consist of checkpoint fine-tuned on the GSM8K training split via SFT (θSFT) and counterpart further trained on the same data with GRPO (θGRPO). The reasoning vector vreason = θGRPO θSFT is extracted and then added to the corresponding official QWEN2.5-Instruct base model (θtarget) using the MergeKit library [9]. Evaluation Configurations. To isolate the effects of the vector and prompting, we compare four configurations across all benchmarks: Baseline: The original instruction-tuned QWEN2.5-Instruct model without modification. G+T: The GRPO-tuned donor model prompted with \"Think step by step.\" This column serves as reference for the performance of the RL-tuned source model. +Vector: The baseline model enhanced with the reasoning vector via addition (α = 1). +Vector+Think: The vector-enhanced model evaluated with the prefix Think step by step. Benchmarks. We evaluate performance on five diverse, reasoning-oriented benchmarks: GSM8K [5]: Multi-step arithmetic reasoning on grade-school math problems. HumanEval & HumanEval+ [6]: Python code generation from natural language docstrings, with HumanEval+ offering more rigorous tests. SciQ [21]: Multiple-choice science questions requiring domain knowledge and reasoning. BigBenchHard [27]: suite of tasks designed to be difficult for LLMs, focusing on multi-hop reasoning, logic, and symbolic manipulation. Evaluation Protocol. Accuracy is the primary metric, with pass@1 used for code generation tasks. For reproducibility, we use greedy decoding (T = 0) for deterministic tasks (GSM8K, SciQ, BigBenchHard) and set temperature to = 0.5 for creative generation (HumanEval, HumanEval+). We use the standardized prompt templates shown in Figure 3 to ensure consistency. We report results from single run for each experiment and acknowledge that multi-run evaluations would be needed to establish statistical significance. 4.2 MAIN RESULTS Table 1 and Figure 4 present the performance of our method across all benchmarks. The results show consistent positive trend: adding the reasoning vector generally improves model performance, with further gains often achieved by adding simple reasoning prompt. For the 1.5B model, vector injection alone boosts GSM8K accuracy from 45.1% to 47.7% (+2.6%). When combined with reasoning prompt, the accuracy reaches 50.0%, total improvement of 5 Preprint - Under Review Figure 3: Evaluation prompt templates for HumanEval and GSM8K. This design allows us to distinguish between improvements from parameter modification alone versus those elicited by explicit reasoning prompts. Figure 4: Accuracy improvements from reasoning vector transfer. Performance of QWEN2.5 models (1.5B left, 7B right) on five benchmarks. Bars compare baseline (blue), vector-enhanced (+Vector, green), and vector with prompt (+Vector+Think, orange). Green annotations show absolute accuracy gains. The vector transfer consistently improves performance, especially on complex benchmarks like BigBenchHard (BBH). +4.9%. This positive trend holds across other domains. On HumanEval, the vector provides +2.2% gain, increasing to +4.3% with prompting. Most strikingly, on the complex BigBenchHard benchmark, the vector enhances the models performance from near-random 6.7% to 19.0% (+12.3%), demonstrating substantial improvement in challenging reasoning scenarios. The 7B model exhibits similar improvements at higher performance baseline. On GSM8K, the vector and prompt together increase accuracy from 55.3% to 60.3% (+5.0%). On HumanEval, performance reaches 80.5% (+3.7%). While the vector addition alone led to minor performance drop on SciQ for the 7B model (-1.5%), this was recovered and surpassed when reasoning prompt was included, resulting in net gain of +1.7%. Overall, the addition of the reasoning vector provides consistent and positive impact, indicating the method is robust across different model scales. 4.3 ROBUSTNESS ANALYSIS To test whether the performance gains stem from genuine reasoning enhancement rather than superficial pattern matching, we evaluated the 1.5B model on three custom, adversarially modified versions of the GSM8K dataset. 6 Preprint - Under Review Table 1: Accuracy (%) of Qwen2.5 models on reasoning benchmarks. Results show consistent improvements from reasoning vector injection. Green text indicates improvements over baseline, red indicates degradation. Absolute change is in parentheses. All results are from single run. Qwen2.5 1.5B Qwen2.5 7B Benchmark Base G+T +Vec +Vec+T Base G+T +Vec +Vec+T GSM8K HumanEval HumanEval+ SciQ BigBenchHard 45.1 54.9 51.2 25.6 6.7 46.8 51.8 49.3 25.8 16.7 47.7 (+2.6) 57.1 (+2.2) 51.7 (+0.5) 27.1 (+1.5) 16.9 (+10.2) 50.0 (+4.9) 59.2 (+4.3) 54.3 (+3.1) 27.2 (+1.6) 19.0 (+12.3) 55.3 76.8 71.3 39.6 27. 57.3 76.2 72.0 37.0 28.2 58.9 (+3.6) 79.3 (+2.5) 74.5 (+3.2) 38.1 (-1.5) 28.2 (+0.9) 60.3 (+5.0) 80.5 (+3.7) 75.0 (+3.7) 41.3 (+1.7) 28.7 (+1.4) Perturbation Design. We created three challenging variants: GSM Hard Lite: Problems with extended numerical ranges and more reasoning steps, increasing computational and logical complexity. Noise+Digit: The injection of irrelevant numerical tokens, typos, and distracting punctuation to test the models focus. Sentence Shuffle: The order of sentences within problem is altered while preserving logical dependencies, requiring the model to reconstruct the reasoning path from content rather than position. As shown in Table 2 and Figure 5, the benefits of the reasoning vector persist robustly under all adversarial conditions. The enhanced model maintains consistent 2-6% performance advantage over the baseline. Even when faced with noise and structural shuffling designed to break simple heuristics, the vector-enhanced model consistently outperforms the baseline. This provides strong evidence that the transferred capability is fundamental improvement in systematic problem-solving, rather than brittle, memorized pattern. 4.4 ABLATION STUDIES To investigate the properties and direct impact of the reasoning vector, we conducted series of systematic ablations on the 1.5B model using the GSM8K benchmark. Vector Removal Analysis. In critical test, we subtracted the reasoning vector from the baseline model (θdegraded = θbase vreason). As shown in Table 3, this operation resulted in catastrophic performance collapse. Accuracy on GSM8K plummeted to 33.4%, an 11.8% drop relative to the baseline. This strong, symmetric effectwhere addition improves performance and subtraction severely degrades itunderscores the vectors significant contribution to the models reasoning abilities. Scaling Analysis. We investigated the effect of the scaling factor α from Equation 2, testing values in {0.5, 1.0, 1.5, 2.0}. We found that α = 1.0 achieved the optimal performance (50.0%), while α = 0.5 yielded smaller gain (47.2%), and values greater than 1.0 began to degrade performance (48.1% for α = 1.5). This suggests the reasoning vector as extracted is naturally well-calibrated for direct transfer. Table 2: Robustness evaluation on Qwen2.5 1.5B. The reasoning vector maintains its advantage even under challenging, custom-designed perturbations of the GSM8K dataset. Configuration GSM8K Hard Lite Noise+Digit Shuffle Baseline + Vector + Vector + Think 45.7 47.7 (+2.0) 49.4 (+3.7) 38.2 41.9 (+3.7) 43.8 (+5.6) 41.0 44.3 (+3.3) 46.8 (+5.8) 40.6 43.2 (+2.6) 45.5 (+4.9) 7 Preprint - Under Review Figure 5: Robustness of Qwen2.5 1.5B under four perturbation conditions. Blue bars show baseline, orange is vector-enhanced, and green adds reasoning prompt. The vector provides consistent gains across all conditions. Cross-Domain Transfer. To assess generalization, we tested the transferability of reasoning vectors between domains. vector derived from code-based RL training (on HumanEval) improved GSM8K performance by 2.1%. Conversely, the math-derived vector from GSM8K improved HumanEval performance by 1.8%. While these cross-domain gains are smaller than in-domain gains, their existence indicates that the extracted vectors capture some domain-general components of reasoning. The ablation studies provide strong evidence for the vectors direct impact on reasoning, its wellcalibrated nature, and its potential for modest generalization. Table 3: Ablation study on GSM8K using Qwen2.5 1.5B. Subtracting the vector causes severe performance drop (red), while adding it consistently improves results (green), highlighting the vectors strong impact. Model Configuration Think Prefix Accuracy (%) SFT Baseline GRPO Donor + Reasoning Vector + Vector + Think - Vector (Subtracted) No Yes No Yes Yes 45.1 46.8 47.7 (+2.6) 50.0 (+4.9) 33.4 (-11.8)"
        },
        {
            "title": "5 LIMITATIONS",
            "content": "While our findings demonstrate the promise of reasoning vectors, it is crucial to acknowledge the limitations of this work. Architectural and Initialization Constraints. The success of our method hinges on strict compatibility between the donor and target models. Effective transfer requires identical architectures, tokenizers, and shared pre-training initialization family to ensure the parameter spaces are sufficiently aligned. Transferring reasoning vectors across different model families (e.g., from Llama model to Qwen model) is not guaranteed to work and remains an important open question. Dependence on Existing Donor Models. Our approach is framed around reusing prior computational effort, not eliminating it. The method is contingent on the public availability of suitable SFT and RL-tuned donor models that meet the compatibility criteria. The significant cost of creating these donor models in the first place is externalized, and the methods applicability is therefore tied to the richness of the open-source model ecosystem. 8 Preprint - Under Review"
        },
        {
            "title": "6 CONCLUSION",
            "content": "This work establishes that reasoning ability can be extracted and transferred as compact task vector between compatible models. By isolating parameter differences between SFT and GRPO checkpoints, we demonstrate that vreason = θGRPO θSFT captures transferable cognitive capabilities that generalize across diverse domains. Our key finding is that the reasoning behaves as modular and transferable component in parameter space. Consistent improvements in mathematical reasoning (GSM8K, +4.9%), code generation (HumanEval, +4.3%), and logical deduction (BigBenchHard, + 12.3%) indicate that the vector encodes domain-general problem solving strategies. Symmetric effects in ablation studies provide compelling evidence: Adding vreason improves performance, while subtracting it causes severe degradation (-11.8% in GSM8K), demonstrating that reasoning capabilities exist as manipulable directions in parameter space. Rather than requiring costly RL training for each target model, practitioners can now enhance reasoning through two tensor operations completing in seconds. This transforms reasoning enhancement from compute-intensive training to lightweight model editing, democratizing access by leveraging existing open-source checkpoints. The success of cross-domain transfer, where math-derived vectors improve code generation, reveals deeper connections between reasoning modalities. Our work demonstrates that reasoning is task vector that can be moved, combined, and reused, opening new avenues for efficient model enhancement in the open-source AI era."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Samuel K. Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries, 2023. URL https://arxiv.org/abs/2209.04836. [2] Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving model selection and boosting performance in domain generalization, 2022. URL https://arxiv.org/abs/2110.10832. [3] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2023. URL https://arxiv.org/abs/2211.12588. [4] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. In I. Guyon, U. Von Luxburg, Deep reinforcement learning from human preferences. S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf. [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. [6] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. Evaluating large language models in class-level code generation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE 24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400702174. doi: 10.1145/3597503.3639219. URL https://doi.org/10. 1145/3597503.3639219. [7] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In Proceedings of the 37th International Conference on Machine Learning, ICML20. JMLR.org, 2020. [8] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, In Proceedings of the 40th and Graham Neubig. Pal: program-aided language models. International Conference on Machine Learning, ICML23. JMLR.org, 2023. 9 Preprint - Under Review [9] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vladimir Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcees MergeKit: toolkit for merging large language models. In Franck Dernoncourt, Daniel Preotiuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp. 477485, Miami, Florida, US, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-industry.36. URL https://aclanthology.org/2024.emnlp-industry.36/. [10] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. [11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1 (2):3, 2022. [12] Chenyu Huang, Peng Ye, Tao Chen, Tong He, Xiangyu Yue, and Wanli Ouyang. Emrmerging: Tuning-free high-performance model merging. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 122741122769. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ dda5cac5272a9bcd4bc73d90bc725ef1-Paper-Conference.pdf. [13] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic, 2023. URL https://arxiv.org/abs/2212.04089. [14] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion by merging weights of language models, 2025. URL https://arxiv.org/abs/2212. 09849. [15] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. [16] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 30453059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243/. [17] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Imanol Schlag, Theo Gutman-Solo, Vinay Ramasesh, Ambrose Slone, Cem Anil, Solving quanYuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. titative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 38433857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 18abbeef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf. [18] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021. URL https://arxiv.org/abs/2101.00190. [19] Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. [20] Kevin Meng, David Bau, Alex Andonian, and editing factual associations in gpt. wal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Locating In S. Koyejo, S. Mohamed, A. AgarInformation and Yonatan Belinkov. 10 Preprint - Under Review Processing Systems, volume 35, pp. 1735917372. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf. [21] Steven Moore, Ellen Fang, Huy A. Nguyen, and John Stamper. Crowdsourcing the evaluation of multiple-choice questions using item-writing flaws and blooms taxonomy. In Proceedings of the Tenth ACM Conference on Learning @ Scale, L@S 23, pp. 2534, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400700255. doi: 10.1145/3573051.3593396. URL https://doi.org/10.1145/3573051.3593396. [22] Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the tangent space: Improved editing of pre-trained models, 2023. URL https://arxiv.org/ abs/2305.12827. [23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language In S. Koyejo, S. Mohamed, models to follow instructions with human feedback. A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2773027744. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ b1efde53be364a73914f58805a001731-Paper-Conference.pdf. [24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 5372853741. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf. [25] Alexandre Ramé, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, Léon Bottou, and David LopezPaz. Model ratatouille: recycling diverse models for out-of-distribution generalization. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. [26] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. [27] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology. org/2023.findings-acl.824/. [28] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. [29] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. [30] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael GontijoLopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, 2022. URL https://arxiv.org/abs/2203. 05482. 11 Preprint - Under Review [31] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: resolving interference when merging models. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. [32] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities, 2024. URL https://arxiv.org/abs/2408.07666. [33] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: deliberate problem solving with large language models. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. [34] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: absorbing abilities from homologous models as free lunch. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. [35] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465. [36] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models, 2022. URL https://arxiv.org/abs/2210.03493. [37] Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, and Nanyun Peng. Model extrapolation expedites alignment. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10251041, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.51. URL https://aclanthology.org/2025.acl-long.51/. 12 Preprint - Under Review"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 BENCHMARK DETAILS The BigBenchHard benchmark is collection of challenging tasks designed to probe the limits of LLM reasoning. The tasks used in our evaluation span several domains. Table 4: Breakdown of tasks within the BigBenchHard benchmark. Task Domain Problems Complexity logical_deduction_seven_objects temporal_sequences multistep_arithmetic_two tracking_shuffled_objects_seven_objects Object-tracking/working-memory dyck_languages Logical deduction/constraint reasoning Temporal/event-order reasoning Multi-step word arithmetic Formal languages/bracket-matching 250 250 250 250 250 High Medium Medium High Medium"
        },
        {
            "title": "B SAMPLE MODEL OUTPUTS SHOWING ABLATIONS",
            "content": "B.1 PROBLEM STATEMENT fellowship program evaluates applicants through sequence of stages: 1. Stage 1 (Aptitude): Out of 960 applicants, 5 2. Stage 2 (Technical): Of those who passed Stage 1, 3 3. Retake: Of the Stage 2 failures, 1 4. Portfolio Review: From the Stage 2 failures, subset is invited to submit portfolios, of 4 are invited to retake the test, and 2 5 pass while 2 3 of them pass. 8 pass. 5 fail. which half succeed. 5. Final Interview: 70% of all applicants reaching this stage are awarded the fellowship. Question: How many applicants ultimately receive the fellowship? B.2 MODEL θbase vreason Stage 1 (Aptitude) Applicants = 960, Pass = 960 5 Pass rate = 5 8 8 = 600 Stage 2 (Technical) Pass rate = 3 5 , Fail rate = 2 5 5 = 360, Pass = 600 3 Fail = 600 2 5 = 240 Retake (from Stage 2 failures) Portfolio (Side Door) Incorrectly applied to Stage 1 passers Invited = 240 1 Pass = 60 4 = 60 3 = 40 Invited = 600 1 Pass = (cid:0) 200 (cid:1) 1 3 9 = 200 3 66. 2 33.33 Final Interview 13 Preprint - Under Review Final pass rate = 7 10 Final passes 33.33 10 23.33 23 B.3 MODEL θbase + vreason Stage 1 (Aptitude) Applicants = 960, Pass = 960 5 Pass rate = 5 8 Fail = 960 600 = 360 8 = 600, Stage 2 (Technical) Pass = 600 3 5 = 360, Fail = 600 360 = 240 Retake (from Stage 2 failures) Portfolio (from Stage 2 failures) Final Interview Invited = 240 1 Pass = 60 2 4 = 60 3 = 40 Invited = 360 1 Pass = 40 1 9 = 40 2 = 20 Candidates = 360 (Stage 2 pass) + 40 (Retake pass) + 20 (Portfolio pass) = Final pass rate = 7 10 Final passes = 420 7 10 = 294 At-a-glance comparison Model Computation Path Final Passes θbase vreason Portfolio misapplied to Stage 1 passers θbase + vreason Correct aggregation at Final Interview 23 294 These examples qualitatively illustrate the effect of the reasoning vector. Removing it causes the model to lose the correct sequence of steps and misapply rules (e.g. applying the portfolio stage to Stage-1 passers), yielding an implausibly small final count. Adding it restores coherent solution path: The model tracks subsets correctly, aggregates at the final stage, and recovers the expected result. The purpose of these examples is illustrative rather than evaluativethey are not used to compute accuracybut they show that the vector reliably strengthens multistep reasoning. We also observe secondary benefit: outputs become more structured and consistent in formatting after vector injection."
        }
    ],
    "affiliations": [
        "American University of Beirut (AUB)",
        "King Abdullah University of Science and Technology (KAUST)"
    ]
}