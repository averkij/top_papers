{
    "paper_title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
    "authors": [
        "Chenxing Wei",
        "Jiarui Yu",
        "Ying Tiffany He",
        "Hande Dong",
        "Yao Shu",
        "Fei Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages."
        },
        {
            "title": "Start",
            "content": "ReDit: Reward Dithering for Improved LLM Policy Optimization Chenxing Wei * 1 2 Jiarui Yu 3 Ying Tiffany He 1 Hande Dong 3 Yao Shu 4 Fei Yu 1 2 5 2 0 2 3 2 ] . [ 1 1 3 6 8 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While its perfect reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks and different LLMs demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits 4% performance improvement over vanilla GRPO when trained for similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages. 1. Introduction Reinforcement learning (RL) is pivotal in Large Language Model (LLM) development(AI@Meta, 2025; Anthropic, *Work done during an internship at Tencent. 1College of Computer Science and Software Engineering, Shenzhen University, China 2Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China 3Tencent, Shenzhen, China 4Hong Kong University of Science and Technology (Guangzhou), China. Correspondence to: Yao Shu <yaoshu@hkust-gz.edu.cn>. Proceedings of the 2 nd Workshop on Models of Human Feedback for AI Alignment at the International Conference on Machine Learning (ICML), Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 Initially, RL from human 2024; OpenAI et al., 2024). feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019) was employed to align pre-trained LLMs with human preferences (Lang et al., 2024; Ouyang et al., 2022). This typically involves training separate reward model (RM) on human preference data (Kaufmann et al., 2024), which then guides the LLM policy optimization (Lambert, 2025). While effective, this approach introduces considerable training overhead (Cao et al., 2024b). Subsequently, methods like Direct Preference Optimization (DPO) (Rafailov et al., 2023) were developed, enabling LLMs to learn directly from preference data and thus bypassing explicit RM training. However, these methods still require extensive collection of high-quality preference data. For reasoning tasks such as mathematics and coding, DeepSeek-R1 (DeepSeek-AI et al., 2025) with Group Relative Policy Optimization (Shao et al., 2024)(GRPO) proposes an alternative: optimizing the LLM policy directly using rule-based reward system (Kong & Yang, 2022; Wang et al., 2025), thereby avoiding the need for external RMs or large preference datasets. For instance, such system might assign reward of 1 for outputs meeting predefined criteria (e.g., correctness, format compliance) and 0 otherwise (DeepSeek-AI et al., 2025). The simplicity and unbiased nature of these rule-based rewards prevent LLMs from hacking them, potentially fostering enhanced reasoning capabilities (Chan et al., 2023). However, such reward functions are often discrete, posing significant optimization challenges (Rengarajan et al., 2022; Vasan et al., 2024; Goyal et al., 2019). Consider an RL scenario with binary reward (Chatterji et al., 2021): policy model receives 1 for correct answer and 0 otherwise. During early training phases, policy LLM rarely generates completely correct answers, resulting in predominantly zero rewards across mini-batches (Cao et al., 2024a). Although the model may engage in exploratory behavior on difficult examples, the corresponding gradients remain minimal due to small advantage magnitudes (Chan et al., 2024). Thus, these hard examples and potentially beneficial explorations (Chan et al., 2024) are largely unexploited during the early stages. Conversely, the model may repeatedly reinforce easy examples (Xie et al., 2024), thus reducing incentives to explore alternative strategies for more difficult problems (Weaver & Tao, 2001). This phenomenon can lead to training stagnation in intermediate and advanced ReDit: Reward Dithering for Improved LLM Policy Optimization Figure 1. Training Dynamics of Gradient Norm and Reward for Qwen2.5-7B (Qwen et al., 2025) on GSM8K Dataset. Fig.s (a) and (b) compare gradient distributions and reward trends across training steps. The original GRPO method (Fig. (a)) suffers from significant gradient instabilityboth vanishing (red dots, norms 0.01) and exploding (purple asterisks, norms 5). In contrast, ReDit with Gaussian reward smoothing (Fig. (b)) effectively stabilizes optimization throughout training. stages. Consistent with this, as shown in Fig. 1(a), we observe that the policy model frequently suffers from gradient vanishing (Razin et al., 2024; Abdul Hameed et al., 2023) or explosion (Zhang et al., 2025) during these phases. This combination of insufficient exploration and gradient instability substantially impedes model convergence, representing critical obstacle to efficient RL in LLM. Figure 2. The figure illustrates how ReDit of different variances gradually smooth the reward distribution, showing the smoothing effect of perturbations of different variances. This observed phenomenon highlights that even perfectly accurate discrete reward functions face significant limitations within gradient-based optimization frameworks. Lending theoretical support to this, recent studies (Ivison et al., 2024; Chen et al., 2024; Wen et al., 2025) have established that singular focus on increasing reward model accuracy does not necessarily translate to enhanced language model performance. In particular, Wen et al. (2025) theoretically substantiates the necessity for effective reward models to 2 integrate adequate variance and uncertainty to enable efficient optimization. The theoretical details are given in Sec. 3.2. Consequently, an optimal reward system must balance accuracy with appropriate levels of variance. Inspired by these observations and theoretical insights, we propose ReDit, simple yet effective technique that applies zero-mean random perturbations to discrete reward signals during training. By introducing controlled noise to the reward function  (Fig. 2)  , ReDit transforms hard reward boundaries into smoother gradients. This softened approach generates greater reward variance within minibatches which, as established in previous research, enhances model performance and accelerates convergence. Fig.1 demonstrates the impact of ReDit on LLM policy optimization for GSM8K. The orange lines reveal that GRPO with ReDit achieves substantially higher early-phase rewards than the baseline, indicating the effectiveness of ReDit . We hypothesize that ReDit encourages broader exploration by assigning varied rewards to outputs that only partially meet strict evaluation criteria, thereby accelerating convergence. Although both approaches eventually attain high rewards after 1000 training steps, our method exhibits superior test set performance, suggesting better generalization. As evidenced by Fig.1(a), ReDit maintains stable gradients throughout training while the baseline suffers from gradient vanishing (red point) and explosion (purple star). These results confirm the advantages of ReDit: more stable optimization, faster convergence, and enhanced overall performance. Moreover, theoretical analysis indicates that greater reward variance can enhance performance and accelerate conReDit: Reward Dithering for Improved LLM Policy Optimization vergence in RL (Wen et al., 2025). We increase reward variance within mini-batches while preserving the expected gradient through reward dithering. By carefully injecting noise into the reward function, ReDit achieves balance between reward signal fidelity and reward variance, leading to enhanced policy optimization. In summary, our main contributions are: We observe that policy optimization under discrete reward functions suffer from unstable gradients and slow convergence (Section 3.1). We propose Reward Dithering (ReDit), simple yet effective technique that introduces perturbations to discrete rewards. This method is shown to accelerate convergence speed and enhance final model performance (Algorithm 1 and Section 4). Extensive experiments across diverse downstream tasks, RL algorithms, perturbation distributions and different LLMs demonstrate that ReDit achieves superior performance and enhanced convergence (Section 5). Theoretical analysis proves that ReDit produces an unbiased estimate of the original gradient (Proposition 6.1), introduces beneficial gradient variance that mitigates vanishing and exploding gradients (Proposition 6.2), and significantly improves convergence speed (Proposition 6.3). 2. Preliminaries We frame LLM generation as sequential decision-making problem solvable via RL. The process is modeled as Markov Decision Process (MDP) (Hallak et al., 2015) where the state st = q; o<t includes the prompt and generated tokens o<t, the action ot is the next token selected from the vocabulary, and the policy πθ(otst) is parameterized by θ. The goal is to optimize the policy to maximize the expected sequence-level reward R(q, o) = (cid:80)o t=1 r(st, ot) over the prompt distribution pQ: J(πθ) = EqpQ (cid:2)Eoπθ(q)[R(q, o)](cid:3) . (1) for stability, is given by: JGRPO(θ) = EqpQ (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) oi (cid:88) (cid:16) min i=1 t= ri,t(θ) ˆAGRPO i,t , clip (ri,t(θ), 1 ϵ, 1 + ϵ) ˆAGRPO i,t (cid:35) (cid:17) (2) βEqpQ [DKL(πθ(q)πref(q))] , where ri,t(θ) = πθ(oi,tsi,t) πθold (oi,tsi,t) . Subsequent methods such as DAPO (Yu et al., 2025), Dr.GRPO (Liu et al., 2025), and REINFORCE++ (Hu et al., 2025) generally adopt this discrete reward paradigm (see Appendix for more related work). While simplifying the overall RL process by avoiding complex RMs, this shift to discrete, sequence-level rewards introduces significant optimization challenges. The inherent sparsity and abrupt value changes (e.g., 0 to 1) hinder policy gradient estimation and lead to training instability (Section 3.1). Figure 3. GRPO has unstable performance on the MATH test set. The figure plots the test accuracy achieved for the checkpoints saved during the training run shown in Fig. 4(b). 3. Motivation This section articulates the fundamental motivations driving our research and establishes the critical challenges that our work aims to address. In Section 3.1, we examine the optimization challenges inherent in discrete reward structures, followed by an exposition of the theoretical principles informing our methodological framework in Section 3.2. Recently, GRPO (Shao et al., 2024) was proposed as PPO alternative that eliminates the need for independent RMs and value functions. GRPO typically processes sparse, discrete rewards directly, rather than continuous RM scores. For tasks like mathematical reasoning, this discrete reward R(q, o) {0, 1} is often determined by simple function checking correctness or format. GRPO estimates the advantage ˆAGRPO by sampling responses {oi}G i=1 and normalizing their discrete rewards within the set. Its objective function, which includes KL divergence term DKL(πθπref) i,t 3.1. Difficulties in Optimization Caused by Discrete"
        },
        {
            "title": "Rewards",
            "content": "Optimizing LLM policies using algorithms like GRPO in conjunction with discrete sequence-level rewards (e.g., binary correctness metrics) presents significant optimization challenges. Fig. 4 plots the policy gradient norm (blue line) and average reward (orange line) during standard GRPO training on the GSM8K and MATH datasets, respectively. Two main issues are immediately apparent: 3 ReDit: Reward Dithering for Improved LLM Policy Optimization Figure 4. Qwen2.5-7B Gradient norm and reward training dynamics of standard GRPO on GSM8k and MATH datasets. During the whole optimization process, the gradient of standard GRPO is unstable, and there are lot of gradient vanishing or gradient exploding cases. Gradient Vanishing. The figure illustrates instances where the gradient norm approaches zero (red dot), occurring when most examples in GRPO batch yield identical binary rewards. Consequently, the population relative advantage estimate ˆAGRPO becomes negligible across examples, providing insufficient learning signals and causing training stagnation. This phenomenon is evident in Fig. 4(b) post-step 2000. i,t Gradient Eplosion. Conversely, training dynamics exhibit sporadic sharp spikes in gradient norm (purple asterisks) when small policy changes cause sequences to transition from incorrect (reward 0) to correct (reward 1). These transitions create disproportionately large advantage estimates for newly successful sequences, triggering sudden, destabilizing gradient updates as shown in Fig. 4(a). Such spikes induce reward fluctuations in subsequent steps, hindering smooth convergence and learning efficiency. The discrete, sparse rewards induce unstable oscillations between vanishing and exploding gradients. Fig. 3 demonstrates that model performance fluctuates correspondingly with these oscillations. This inherent instability not only compromises optimization efficiency but also serves as key motivation for our research. 3.2. Theoretical Principles to Address the Limitations of"
        },
        {
            "title": "Discrete Rewards",
            "content": "To overcome the critical challenges with discrete rewards outlined in Section 3.1, we propose approach to improve the quality of the reward signal. Our solution derives from Theorem 3.3 and Theorem 3.4, which reveal fundamental relationships between reward variance, accuracy, and learning efficiency. From Definition 1, 2 in Razin et al. (2025), The accuracy and variance of the reward function is as follows: Definition 3.1. Given prompt , the accuracy of reward model rRM : [1, 1] with respect to distribution over unordered output pairs is defined by: accx,D(rRM ) := E{y,y}D (cid:34) (cid:20) sign(cid:0)rRM (x, y) rRM (x, y)(cid:1) = sign(cid:0)rG(x, y) rG(x, y)(cid:1) (cid:21)(cid:35) , (3) where rG is the ground truth reward, 1[] is an indicator function, and sign : {1, 0, 1} is the sign function.1 Definition 3.2. Given policy πθ, prompt , and reward model rRM : [1, 1], the reward variance induced by rRM for πθ and is defined by: Varyπθ(x)[rRM (x, y)] := Eyπθ(x) (cid:34)(cid:18) rRM (x, y) Eyπθ(x) (cid:2)rRM (x, y)(cid:3) (cid:19)2(cid:35) . (4) Theorem 3.3 establishes that the time tγrequired for policy improvement is inversely proportional to reward variance. When rewards exhibit insufficient variancefailing to adequately differentiate between high-quality and low-quality outputs under policy πθ, convergence slows significantly. This finding suggests that strategically increasing reward variance can accelerate policy convergence. 1For set of prompts, accuracy refers to the mean accuracy over the set. 4 ReDit: Reward Dithering for Improved LLM Policy Optimization Theorem 3.3 (Policy network optimization time lower bound). From Theorem 1 in Razin et al. (2025). Suppose that we maximize the objective (Eq. (1)), using general autoregressive policy πθ(yx) = (cid:81)y ). For any γ > 0, prompt , and reward function r, the time it takes until Eyπθ(t)(x)[r(x, y)] Eyπθ(0)(x)[r(x, y)] + γ is: l=1 softmax(fθ(x, y<l)yl (cid:18) Ω ExS (cid:104) varyπθ(0)(x)(r(x, y)) (cid:105) 3 (cid:19) Complementarily, Theorem 3.4 demonstrates that effective reward models must incorporate calibrated degree of uncertainty. This controlled uncertainty creates essential exploration space during early training stages, preventing premature convergence and facilitating more efficient optimization. Theorem 3.4 (Policy network optimization time upper bound). From Theorem 2 in Razin et al. (2025). the form πθ(yx) = Assume πθ is policy of let γ > softmax[θ:,x]y. Given hint S, 0 and denote by tγ > 0 the initial time of Eyπθ(t)(x)[rG(x, y)] Eyπθ(0)(x)[rG(x, y)] + γ. For any initial policy πθ(0), perfect RM converges to tγ that can be arbitrarily large, while relatively inaccurate RM has an upper bound of O(πθ(0)(yγx)1). While perfectly accurate reward functions resist reward hacking, they paradoxically impede optimization by producing discrete rewards with minimal variance and insufficient randomness. This limitation severely constrains the growth rates of both training reward rRM and true reward rG during policy gradient updates. To address this fundamental tension, we introduce ReDita method that injects zeromean perturbations into discrete rewards. This approach preserves the expected reward value while introducing beneficial variance and controlled uncertainty in each update step, dramatically improving both model performance and convergence speed. 4. Reward Dithering (ReDit) As discussed previously, the discrete nature of rewards commonly used in GRPO can lead to unstable gradient dynamics. To address this, we propose ReDit . The core idea, detailed in Algorithm 1, is to inject calibrated, zero-mean perturbations into the discrete rewards obtained from sampled outputs before using them to compute the GRPO objective for policy updates. Importantly, our ReDit method preserves the overall optimization structure of the GRPO objective function as defined in Eq. (2), the optimization still aims to maximize this objective. Algorithm 1 ReDit within one optimization step 1: Input: Base policy πθold; Discrete reward function : {0, 1, 2, 3, ...}; Prompt q; Number of samples G. Noise parameters: Gaussian std dev σ > 0 or Uniform radius > 0. 2: Output: Updated policy πθ. 3: Sample outputs {oi}G ri r(oi) for = 1, . . . , G. i=1 πθold( q) and compute 4: Sample ϵi (0, σ2) or U[a, a] and compute ri ri + ϵi for = 1, . . . , G.// Generate noise and smooth rewards. 5: Compute JGRPO and Optimize(θold, JGRPO, ri).// Optimization using i=1 {ri}G θ 6: return Updated policy πθ. The crucial modification introduced by ReDit lies in how the advantage term ˆAGRPO i,t within Eq. (2) is computed. Instead of directly using the raw discrete rewards ri = r(oi) obtained for each sampled output oi in the batch {oi}G i=1 (line 3 in Algorithm 1), we first compute smoothed rewards ri. This is done by adding independently sampled zero-mean perturbation ϵi (e.g., from (0, σ2) or U[a, a]) to each discrete reward (line 6 in Algorithm 1): ri = ri + ϵi (5) These smoothed rewards {rk}G k=1 are then used as the basis for calculating the advantage. GRPO often computes advantage based on the relative performance within the batch, typically involving normalization. With ReDit, the core component of the advantage calculation, which relies on these rewards, is effectively modified as follows: ˆAGRPO i,t ri mean({rk}G std({rk}G k=1) k=1) ReDit ˆADithering i,t ri mean({rk}G std({rk}G k=1) k=1) (6) i,t Thus, the relative standing of each output oi within the batch, which informs its advantage ˆAGRPO used in Eq. (2), is determined by the continuous smoothed reward ri rather than the discrete ri. This substitution transforms the optimization landscape. By introducing continuous variations via ri, the added noise provides informative, non-zero gradients even when discrete rewards ri are sparse or identical within batch, mitigating gradient vanishing. It also dampens the sharp changes in expected advantage resulting from small policy shifts affecting discrete outcomes, thus reducing the likelihood of gradient explosion. This overall smoothing effect facilitates more stable gradient flow, enabling more robust and efficient optimization of the policy πθ using the GRPO objective (line 8 in Algorithm 1). 5 ReDit: Reward Dithering for Improved LLM Policy Optimization Figure 5. Test accuracy across datasets on Qwen2.5-7B-Instruct and Qwen2.5-VL-7B-Instruct. The horizontal dashed line marks ReDits performance at 1000 steps, which GRPO fails to match even after 9000 steps. 5. Empirical Results This section presents thorough evaluation of our ReDit framework, assessing its effectiveness and efficiency. We begin by detailing the datasets and experimental configurations in Section 5.1. Subsequently, Section 5.2 provides comprehensive analysis of the primary findings. To isolate the contributions of key components, we also conduct ablation studies, the results of which are presented in Section 5.3. 5.1. Datasets and Setup To rigorously evaluate the effectiveness of our proposed ReDit framework, we conducted extensive experiments. The specific experimental settings are detailed below. Datasets. Our dataset selection and setup largely follow the methodology of (Shao et al., 2024), primarily to assess the mathematical reasoning capabilities of the models. This encompasses mathematical problem-solving datasets such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), as well as the multimodal geometric reasoning dataset Geometry3K (Lu et al., 2021). Each dataset provides distinct training and test splits, which we utilize accordingly for model training and subsequent evaluation. See the Appendix D.1 for details of the dataset. Reward Functions. We designed dataset-specific reward functions. For the GSM8K dataset, which involves simpler problem structures, we implemented several reward types: accuracy-based, strict format adherence, sort format adherence, integer value correctness, and inference step adherence. For the more complex MATH and Geometry3K datasets, our supervision relied solely on accuracy-based and inference-based reward functions. Detailed implementations of these reward functions are provided in the Appendix D.2. Initial Policy. To rigorously assess the effectiveness of ReDit without confounding factors introduced by supervised fine-tuning (SFT), we initialized our experiments directly with instruct models without any additional SFT training. Previous research by Shao et al. (2025) demonstrated that even random rewards can enhance performance for Qwen models. Therefore, we conducted comprehensive evaluations across diverse set of instruction-tuned models, including Qwen2.5-7B-Instruct, Qwen2.5-VL-7B-Instruct, Llama-3.2-3B-Instruct, Llama-3.1-8B-Instruct, Ministral8B-Instruct-2410, and Mistral-7B-Instruct-v0.3, to establish the generalizability of ReDit. Other Training Settings. For parameter-efficient finetuning, we employed Low-Rank Adaptation (LoRA) (Hu et al., 2022). Our implementation leverages the official GRPO implementation within the TRL library (von Werra et al., 2020). Specific configurations for LoRA and GRPO parameters are detailed in the Appendix D.3. Model evaluation was conducted using the OpenCompass (Contributors, 2023). All experiments were executed on single NVIDIA H20 GPU. 5.2. Main Results In our main experiments, we validate the effectiveness of our proposed ReDit. For these experiments, we primarily use either uniform smoothing kernel with radius = 0.05 or Gaussian smoothing kernel with standard deviation 3. More experimental results can be found in the σ = a/ Appendix E. Accelerated Convergence Across Datasets and LLMs. We demonstrate that integrating our proposed method, ReDit, with GRPO substantially accelerates convergence and improves final performance across wide range of datasets  (Fig. 5)  and LLMs, including Llama-3.2-3B, Llama3.1-8B, Ministral-8B and Mistral-7B  (Fig. 6)  . On all tested models, both Gaussian and uniform variants of ReDit enable GRPO to reach competitive performance level within merely 1000 training steps. Notably, this performance already surpasses that of the baseline GRPO trained for the full 9000 steps. Consequently, ReDit not only enhances training efficiency but also leads to superior final accuracy. The Gaussian variant, in particular, consistently yields the strongest results and promotes more stable training trajecto6 ReDit: Reward Dithering for Improved LLM Policy Optimization Figure 6. Accuracy of different LLMs on GSM8K. ReDit improves training efficiency and final performance in various LLMs. ries with lower volatility compared to the baseline. provided in the Appendix E.7. Table 1. Test accuracy comparison across datasets for original Backbone, GRPO, and ReDit represents percentage point improvement of the superior ReDit variant over the GRPO. Name GSM8K MATH Geometry3K Backbone GRPO(Baseline) w/ ours(Gauss) w/ ours(Uniform) 84.91 89.07 90.76 90.46 +1.69 39 48.01 52.55 51.96 +4.54 40.43 43.10 44.67 44.36 +1. Generalization to Diverse Baselines. Fig. 8 presents results from applying ReDit to additional reinforcement learning baselines (DAPO, Dr.GRPO, and REINFORCE++) on the GSM8K dataset. Across all algorithms, ReDit (both Gaussian and uniform variants) consistently enhances performance and accelerates learning. Beyond these early-stage improvements, ReDit also substantially boosts the final accuracy of these baselines, as quantitatively demonstrated in Table 2. These accuracy gains  (Table 1)  complement the qualitative evidence in Fig. 8, confirming that ReDit enables faster and more stable learning across diverse algorithms. Table 2. Comparison of the accuracy for different baselines under 9000 steps on GSM8K. Name DAPO DR.GRPO REINFORCE++ Baseline w/ ours(Gauss) w/ ours(Uniform) 87.52 89.34 88. +1.82 86.13 87.69 87.34 +1.56 86.25 87.96 87.59 +1.71 Optimal Performance with Scheduled Perturbation. We further investigate convergence behavior under various scheduled perturbation schemes: SquareRoot, Cosine, and CosineReverse perturbations. These schedules dynamically adjust perturbation variance throughout training, potentially benefiting model learning. Fig. in the Appendix E.7 illustrates the different perturbation schedules, while Fig. 7 presents their performance. Compared to standard GRPO, ReDit achieves both faster convergence and superior final performance, with the CosineReverse perturbation schedule yielding particularly strong results. Additional details are Figure 7. CosineReverse achieves the best performance. 5.3. Ablation Studies Perturbation variance affects performance. To study the sensitivity of ReDit to the perturbation amplitude, we performed an ablation study by varying the parameter in the Gaussian smoothing kernel with standard deviation σ = a/ 3. This effectively changes the variance of the applied perturbation. As shown in Fig. 9, applying reward smoothing (i.e., for any > 0.00) consistently leads to faster convergence compared to the baseline without smoothing (a = 0.00). Moreover, in most cases, increasing the perturbation amplitude (larger a) tends to improve the final performance of the model. Notably, the configuration with = 0.05 shows superior performance, achieving not only the fastest convergence but also the best peak model performance, see the Fig. 9 annotation. However, these results highlight key trade-off. While moderate perturbations are beneficial, excessive perturbations (e.g., = 0.5) may over-smooth the reward landscape. This may mask the original reward signal and lead to performance degradation. Conversely, if the perturbation variance is too small (e.g., = 0.01), the smoothing effect is small and the improvement over the baseline is limited. This suggests that there is an optimal perturbation variance. We recommend conducting preliminary experiments on smaller dataset to effectively determine this optimal variance before applying it to larger-scale training scenarios. For detailed theoretical introduction to σ, please refer to Section 6. 7 ReDit: Reward Dithering for Improved LLM Policy Optimization Figure 8. Accuracy of different GRPO variants tested on the GSM8K dataset. The horizontal dashed line highlights the performance of using ReDit at about 1000 training steps, and even after 9000 steps, its accuracy is comparable to the baseline. Figure 9. Appropriate perturbation achieves the best performance. Isolating the Effect on Discrete Rewards. To verify that the performance gains of ReDit stem specifically from smoothing discrete rewards, we conducted crucial ablation study. In this experiment, we replaced the discrete reward signal with continuous one generated by reward model pre-trained on human preference data. This model provides continuous quality score within the range [0,1]. We then applied the ReDit perturbation mechanism directly to these continuous rewards. The results, presented in Fig. 10, show that applying ReDit in this setting yields no discernible impact on either the convergence speed of model or its final performance. This outcome strongly indicates that the benefits of ReDit are nullified when the reward landscape is already smooth. We therefore conclude that the efficacy of ReDit lies specifically in addressing the optimization challenges inherent to sparse and discrete reward signals. Comparison with Direct Gradient Manipulation Baselines. We benchmark ReDit against established techniques that directly address gradient instability: Gradient Clipping (Zhang et al., 2020), which mitigates exploding gradients, and Dynamic Sampling (Yu et al., 2025), which alleviates vanishing gradients. The objective is to compare our ReDit approach with methods that operate directly on the gradient signal. As illustrated in Figure 11, ReDit subFigure 10. ReDit has little effect on improving the performance of GRPO based RM. stantially outperforms both baseline methods. We attribute this performance gap to the inherent limitations of these heuristics. Gradient Clipping, for instance, crudely truncates gradient magnitudes, non-principled operation that can introduce significant estimation bias. Conversely, while Dynamic Sampling can be effective for vanishing gradients, it offers no mechanism to prevent gradients from exploding. In contrast, ReDit stabilizes the training process by smoothing the reward, which provides more principled solution to prevent both gradient vanishing and explosion, thereby leading to more efficient and effective training. Figure 11. Appropriate perturbation achieves the best performance. 8 ReDit: Reward Dithering for Improved LLM Policy Optimization enables the optimization trajectory to probabilistically bypass unstable regions of high curvature. Furthermore, the noise variance σ can be adjusted to control the magnitude of the gradient noise for optimal results. This mechanism enhances the robustness of policy optimization and explains the empirical improvements observed in training stability and convergence speed from reward perturbations. For detailed derivation, see Appendix B.2. Proposition 6.3 (Optimization time upper and lower bounds). When we introduce Gaussian noise ϵ (0, σ2) to the reward, it not only increases the variance of the reward, but also reduces the accuracy of the originally perfect reward function. According to (Razin et al., 2025), for the target J(πθ) (Eq. (7)), the time tγ required for the real reward rG to increase γ satisfies: (cid:16)(cid:0)EqpQ 3 (cid:17) (cid:2)varoπθ(q)R(q, o)(cid:3) + σ2(cid:1) 1 Ω tγ. tγ (cid:0)πθ(0)(yγx)1(cid:1) . Remark. Proposition 6.3 analyzes the time required for policy to reach or exceed certain performance threshold starting from πθ(0). Compared with optimization using the unperturbed reward, the perturbed reward introduces larger reward variance, which can reduce the lower bound of the convergence time. Meanwhile, the perturbation introduces inaccuracies relative to the perfect reward. This inaccuracy limits the upper bound of the convergence time. This advantage arises because the random noise in the perturbation can effectively improve the perturbed rewards of non-zero probability outputs under the initial policy πθ(0), thereby effectively encouraging broader exploration, see Fig. 1. This makes these outputs easier to perform gradient ascent, which helps to discover outputs with higher true rewards. For detailed proof, see Appendix B.3. 7. Limitations and Conclusions ReDit enhances RL by introducing zero-mean noise to discrete rewards, effectively smoothing gradients, preventing gradient pathologies, and accelerating convergence through increased reward variance. Our empirical evaluation across multiple benchmarks confirms these benefits, demonstrating improvements in both speed and performance. Though effective, the approach requires perturbation variance tuningcurrently done through experimentation. Future work will focus on automating this parameter selection. 6. Theoretical Insights We provides theoretical analyzing how perturbing discrete reward signals with, e.g., Gaussian noise, accelerates RL convergence, offering principled explanation for observed empirical benefits. Problem Setup. Our analysis uses simplified RL framework (from Eq. (1)) focusing on binary rewards R(q, o) {0, 1} for complete outputs (e.g., GRPO (Shao et al., 2024)), not token-level rewards. We investigate how Gaussian noise ϵ (0, σ2) injection improves convergence. The perturbed objective is: J(πθ) = EqpQ (cid:104) Eoπθ(q) R(q, o) (cid:105) , (7) where the perturbed reward is R(q, o) = R(q, o) + ϵ. Proposition 6.1 (Unbiased estimate of gradient). Introducing noise will still ensure the unbiased estimate of the gradient of the original optimization target Eq. (1), that is: (cid:104) θ J(πθ) (cid:105) = [θJ(πθ)] . Remark. Proposition 6.1 provides theoretical proof that introducing Gaussian noise perturbations into the discrete reward signal preserves the unbiased nature of the policy gradient estimate. This means that, under the perturbed reward, the expected direction of the policy update is consistent with the original objective being optimized. Maintaining this unbiased nature ensures that the injected noise does not introduce systematic biases into the learning dynamics, thus providing theoretical basis for the empirical observation that our approach helps to consistently improve performance. See Appendix B.1 for detailed proof. Proposition 6.2 (Introducing the variance of gradient estimation). Suppose we are optimizing non-degenerate strategy, that is, its gradient θ log πθ is not completely zero. Introducing noise will introduce gradient noise on the originally calculated gradient, and its variance is: Var(Gradient Noise) = σ2E (cid:2)θ log πθ(oq)2(cid:3) > 0. Remark. In Proposition 6.2, we analyze how Gaussian reward perturbations affect the variance of policy gradient estimates. Adding Gaussian noise ϵ (0, σ2) to the reward introduces gradient noise component proportional to ϵ θ log πθ(oq) in the gradient estimate. The increased variance has significant optimization benefits: Mitigate vanishing gradients: Gradient noise provides consistent stochastic updates even when the original gradient terms are small or vanishing, thus helping to avoid flat regions. Avoid exploding gradients: The randomness induced by the noise 9 ReDit: Reward Dithering for Improved LLM Policy Optimization"
        },
        {
            "title": "References",
            "content": "Abdul Hameed, M. S., Chadha, G. S., Schwung, A., and Ding, S. X. Gradient monitored reinforcement learning. IEEE Transactions on Neural Networks and Learning Systems, 34(8):41064119, 2023. doi: 10.1109/TNNLS. 2021.3119853. AI@Meta. The llama 4 herd. https://ai.meta.com/ blog/llama-4-multimodal-intelligence/, 2025. Accessed: 2025. Anthropic. Claude 3.5 sonnet. https://www. anthropic.com/news/claude-3-5-sonnet, 2024. Accessed: 2024. Cao, M., Shu, L., Yu, L., Zhu, Y., Wichers, N., Liu, Y., and Meng, L. Enhancing reinforcement learning with dense rewards from language model critic. In AlOnaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 91199138, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 515. URL https://aclanthology.org/2024. emnlp-main.515/. Cao, Y., Zhao, H., Cheng, Y., Shu, T., Chen, Y., Liu, G., Liang, G., Zhao, J., Yan, J., and Li, Y. Survey on large language model-enhanced reinforcement learnIEEE Transing: Concept, taxonomy, and methods. actions on Neural Networks and Learning Systems, pp. 121, 2024b. ISSN 2162-2388. doi: 10.1109/tnnls.2024. 3497992. URL http://dx.doi.org/10.1109/ TNNLS.2024.3497992. Chan, A. J., Sun, H., Holt, S., and van der Schaar, M. Dense reward for free in reinforcement learning from human feedback. In International Conference on Machine Learning, 2024. URL https://openreview. net/forum?id=eyxVRMrZ4m. Chan, H., Mnih, V., Behbahani, F., Laskin, M., Wang, L., Pardo, F., Gazeau, M., Sahni, H., Horgan, D., Baumli, K., Schroecker, Y., Spencer, S., Steigerwald, R., Quan, J., Comanici, G., Flennerhag, S., Neitz, A., Zhang, L. M., Schaul, T., Singh, S., Lyle, C., Rocktaschel, T., ParkerHolder, J., and Holsheimer, K. Vision-language modIn Second Agent Learnels as source of rewards. ing in Open-Endedness Workshop, 2023. URL https: //openreview.net/forum?id=Xw1hVTWxxQ. Chatterji, N. S., Pacchiano, A., Bartlett, P. L., and Jordan, M. I. On the theory of reinforcement learning with onceper-episode feedback. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS 21, Red Hook, NY, USA, 2021. Curran Associates Inc. ISBN 9781713845393. Chen, Y., Zhu, D., Sun, Y., Chen, X., Zhang, W., and Shen, X. The accuracy paradox in RLHF: When better reward models dont yield better language models. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 29802989, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 174. URL https://aclanthology.org/2024. emnlp-main.174/. Christiano, P. F., Leike, J., Brown, T. B., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, pp. 43024310, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. Chu, X., Huang, H., Zhang, X., Wei, F., and Wang, Y. Gpg: simple and strong reinforcement learning baseline for model reasoning, 2025. URL https://arxiv.org/ abs/2504.02546. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Contributors, O. Opencompass: universal evaluation platform for foundation models. https://github. com/open-compass/opencompass, 2023. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, ReDit: Reward Dithering for Improved LLM Policy Optimization X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Goyal, P., Niekum, S., and Mooney, R. J. Using natural language for reward shaping in reinforcement learning. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI19, pp. 23852391. AAAI Press, 2019. ISBN 9780999241141. Hallak, A., Castro, D. D., and Mannor, S. Contextual markov decision processes, 2015. URL https:// arxiv.org/abs/1502.02259. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https:// openreview.net/forum?id=nZeVKeeFYf9. Hu, J., Liu, J. K., and Shen, W. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv.org/ abs/2501.03262. Ivison, H., Wang, Y., Liu, J., Wu, Z., Pyatkin, V., Lambert, N., Smith, N. A., Choi, Y., and Hajishirzi, H. Unpacking DPO and PPO: Disentangling best practices for learning from preference feedback. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=JMBWTlazjW. Kaufmann, T., Weng, P., Bengs, V., and Hullermeier, E. survey of reinforcement learning from human feedback, 2024. URL https://arxiv.org/abs/ 2312.14925. Kong, D. and Yang, L. F. Provably feedback-efficient reinforcement learning via active reward learning. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. Lambert, N. Reinforcement learning from human feedURL https://arxiv.org/abs/ back, 2025. 2504.12501. Lang, H., Huang, F., and Li, Y. Fine-tuning language models with reward learning on policy. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 13821392, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.naacl-long.75. URL https://aclanthology. org/2024.naacl-long.75/. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective, 2025. URL https://arxiv. org/abs/2503.20783. Lu, P., Gong, R., Jiang, S., Qiu, L., Huang, S., Liang, X., and Zhu, S.-C. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning, 2021. URL https://arxiv.org/abs/2105. 04165. Ma, H., Fu, G., Luo, Z., Wu, J., and Leong, T.-Y. Exploration by random reward perturbation, 2025. URL https://arxiv.org/abs/2506.08737. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. nature, 518(7540): 529533, 2015. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han, 11 ReDit: Reward Dithering for Improved LLM Policy Optimization J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, J. H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mely, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., OKeefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M., Pong, V. H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M. B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Gray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=TG8KACxEON. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, 12 B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=HPuSIXJaa9. Razin, N., Zhou, H., Saremi, O., Thilak, V., Bradley, A., Nakkiran, P., Susskind, J. M., and Littwin, E. Vanishing gradients in reinforcement finetuning of language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=IcVNBR7qZi. Razin, N., Wang, Z., Strauss, H., Wei, S., Lee, J. D., and Arora, S. What makes reward model good teacher? an optimization perspective, 2025. URL https://arxiv.org/abs/2503.15477. Rengarajan, D., Vaidya, G., Sarvesh, A., Kalathil, D., and Shakkottai, S. Reinforcement learning with sparse rewards using guidance from offline demonstration. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=YJ1WzgMVsMt. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/ 1707.06347. Shao, R., Li, S. S., Xin, R., Geng, S., Wang, Y., Oh, S., Du, S. S., Lambert, N., Min, S., Krishna, R., Tsvetkov, Y., Hajishirzi, H., Koh, P. W., and Zettlemoyer, L. Spurious rewards: Rethinking training signals in rlvr, 2025. URL https://arxiv.org/abs/2506.10947. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Tariq, S., Chhetri, M. B., Nepal, S., and Paris, C. A2c: modular multi-stage collaborative decision framework for human-ai teams, 2024. URL https://arxiv.org/ abs/2401.14432. ReDit: Reward Dithering for Improved LLM Policy Optimization for adaptivity. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=BJgnXpVYwS. Zhang, K., Hong, Y., Bao, J., Jiang, H., Song, Y., Hong, D., and Xiong, H. Gvpo: Group variance policy optimization for large language model post-training, 2025. URL https://arxiv.org/abs/2504.19599. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. URL https: //arxiv.org/abs/1909.08593. Vasan, G., Wang, Y., Shahriar, F., Bergstra, J., Jagersand, M., and Mahmood, A. R. Revisiting sparse rewards for goal-reaching reinforcement learning, 2024. URL https://arxiv.org/abs/2407.00324. von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., Huang, S., Rasul, K., and Gallouedec, Q. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. Wang, Z., Feng, P., Lin, Y., Cai, S., Bian, Z., Yan, J., and Zhu, X. Crowdvlm-r1: Expanding r1 ability to vision language model for crowd counting using fuzzy group relative policy reward, 2025. URL https://arxiv. org/abs/2504.03724. Weaver, L. and Tao, N. The optimal reward baseline for gradient-based reinforcement learning. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI01, pp. 538545, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1558608001. Wen, X., Lou, J., Lu, Y., Lin, H., XingYu, Lu, X., He, B., Han, X., Zhang, D., and Sun, L. Rethinking reward model evaluation: Are we barking up the wrong tree? In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=Cnwz9jONi5. Xie, T., Zhao, S., Wu, C. H., Liu, Y., Luo, Q., Zhong, V., Yang, Y., and Yu, T. Text2reward: Reward shaping with language models for reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=tUM39YTRxH. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.- Y., Zhang, Y.-Q., Yan, L., Qiao, M., Wu, Y., and Wang, M. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/ abs/2503.14476. Zhang, H., Lei, Y., Gui, L., Yang, M., He, Y., Wang, H., and Xu, R. CPPO: Continual learning for reinforceIn The Twelfth ment learning with human feedback. International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=86zAUE80pP. Zhang, J., He, T., Sra, S., and Jadbabaie, A. Why gradient clipping accelerates training: theoretical justification ReDit: Reward Dithering for Improved LLM Policy Optimization A. Related Work Reinforcement Learning with Discrete Rewards. Group Relative Policy Optimization (GRPO) (Shao et al., 2024) utilizes discrete rewards generated by rule-based reward function to guide the policy model update. This reward function, known for its simplicity and unbiasedness, effectively mitigates reward hacking and has demonstrated strong performance. However, GRPO faces challenges related to slow training speed and unstable gradients during training. To address these issues, various methods have been proposed. DAPO (Yu et al., 2025) introduced dynamic sampling strategy to improve gradient effectiveness by dynamically filtering invalid samples, thereby increasing sample efficiency, although this reduced training speed. CPPO (Zhang et al., 2024) prunes completions with low absolute advantages, significantly reducing the number of gradient calculations and updates required, which enhances training efficiency but can lead to gradient estimation errors. GPG (Chu et al., 2025) directly optimizes the original reinforcement learning objective, eliminating the need for proxy loss function and improving training efficiency. However, this simplification may result in significant divergence between the actor and policy models. Dr.GRPO (Liu et al., 2025) improves token efficiency while maintaining inference performance. Despite these efforts, critical challenge remains: these algorithms largely neglect the inherent difficulties introduced by discrete rewards during the optimization process. The oscillations caused by gradient vanishing and exploding are major contributors to the slow optimization speed. Our work specifically aims to overcome the challenges in gradient optimization that arise from using discrete rewards. The Reward Design Challenge in LLM Reinforcement Learning. The predominant approach to aligning Large Language Models (LLMs), Reinforcement Learning from Human Preferences (RLHF), relies on learned reward model to score model outputs (Christiano et al., 2017). However, this paradigm introduces fundamental trade-off between reward accuracy and variance. On one hand, low-fidelity reward models are prone to reward hacking, where the policy model exploits inaccuracies in the reward signal rather than achieving the intended goal (Ivison et al., 2024; Chen et al., 2024; Wen et al., 2025). On the other hand, increasing the reward models accuracy often reduces reward variance, which can lead to vanishing gradients and subsequently slow down policy optimization (Razin et al., 2024). This accuracy-variance dilemma is formalized in recent theoretical work, which posits that an effective reward function must strike balance between its bias and variance (Razin et al., 2025). Several lines of work attempt to navigate this challenge. For instance, GRPO (Shao et al., 2024; Liu et al., 2025) sidesteps reward model inaccuracies by using deterministic, high-accuracy reward function. However, by providing sparse, low-variance rewards, it can exacerbate gradient instability and hinder optimization. Other approaches rely on heuristics. For instance, the addition of spurious rewards has shown empirical gains but lacks theoretical foundation and has only been validated on narrow range of models (Shao et al., 2025). In stark contrast, our method, ReDit, is not only underpinned by rigorous theoretical framework but has also demonstrated broad effectiveness and applicability across various LLMs. Similarly, while methods like Random Reward Perturbation (RRP) (Ma et al., 2025) also use perturbations, their focus is on improving sample efficiency in classic RL algorithms like PPO (Schulman et al., 2017), DQN (Mnih et al., 2015) and A2C (Tariq et al., 2024), not on accelerating the convergence of modern LLM policy optimization frameworks like GRPO. Our work, therefore, is specifically designed to address the aforementioned accuracy-variance trade-off by introducing structured perturbations to high-accuracy reward signal, preserving low bias while injecting sufficient variance for stable and efficient optimization. B. Theorems and proofs B.1. Proof of Proposition 6.1 The proof of Proposition 6.1 is expressed as follows: Proof. By the policy gradient theorem, the gradient of the original objective (1) expands to: θJ(πθ) = EqpQ Eoπθ(q) [R(q, o)θ log πθ(oq)] . For the noise-injected objective, its gradient becomes: θ J(πθ) = EqpQ Eoπθ(q) (cid:104) R(q, o)θ log πθ(oq) (cid:105) . 14 (8) (9) ReDit: Reward Dithering for Improved LLM Policy Optimization Substituting R(q, o) = R(q, o) + ϵ and leveraging linearity of expectation: (cid:105) (cid:104) θ = Eq,o,ϵ [(R(q, o) + ϵ)θ log πθ(oq)] = Eq,o [R(q, o)θ log πθ(oq)] (cid:125) (cid:123)(cid:122) E[θJ] (cid:124) +Eϵ[ϵ] Eq,o [θ log πθ(oq)] . Zero-mean noise: Eϵ[ϵ] = 0 by definition of (0, σ2). Thus, the cross-term vanishes: E[θ J] = E[θJ] + 0 = E[θJ]. (10) (11) (12) B.2. Proof of Proposition 6.2 Proof. Consider the perturbed objective function with noise-augmented reward R(q, o) + ϵ. The estimated value of the gradient of the noise enhancement objective function using samples is: ˆJ(θ) = 1 (cid:88) i=1 [ log πθ(oiqi) (R(qi, oi) + ϵi)] , where ϵi (0, σ2) is the Gaussian noise. The original reward gradient is: ˆJ(θ) = 1 (cid:88) i=1 [ log πθ(oiqi) (R(qi, oi))] . Under this condition, the Eq. (13) simplifies to: ˆJ(θ) = ˆJ(θ) (cid:124) (cid:123)(cid:122) (cid:125) origin gradient + (cid:88) i=1 1 (cid:124) [ log πθ(oiqi) ϵi] . (cid:123)(cid:122) noise gradient (cid:125) (13) (14) (15) While the expectation Eϵ[ϵ] = 0 implies the noise contributions mean is zero, the variance of the gradient term persists. To compute this variance, we use the definition: Var(X) = E[X 2] (E[X])2. Applying this to the noise-induced component ϵ θ log πθ(oq), we get: Var (ϵ θ log πθ(oq)) = (cid:2)ϵ2 θ log πθ(oq)2(cid:3) (E [ϵ θ log πθ(oq)])2 . Since E[ϵ] = 0, the second term vanishes. For the first term, note that: E[ϵ2] = Var(ϵ) + (E[ϵ])2 = σ2 + 0 = σ2. This allows us to simplify the variance expression to: Var(noise gradient) = Var (ϵ θ log πθ) = σ2 (cid:2)θ log πθ(oq)2(cid:3) > 0, provided θ log πθ is not identically zero (a reasonable assumption for non-degenerate policies). (16) (17) (18) B.3. Proof of Proposition 6.3 The proof of Proposition 6.3 is expressed as follows: Proof. Suppose the original reward model is rRM (x, y). We add zero-mean Gaussian random variable ϵ with variance σ2 to it, resulting in the new reward signal rRM (x, y): rRM (x, y) = rRM (x, y) + ϵ, (19) where ϵ (0, σ2), and we assume that ϵ is independent of (conditioned on x). 15 ReDit: Reward Dithering for Improved LLM Policy Optimization 1. Change in Reward Variance (Based on Definition 2). Definition 2 defines the reward variance induced by the reward model rRM under policy πθ and prompt as: Varyπθ(x)[rRM (x, y)] := Eyπθ(x) (cid:104)(cid:0)rRM (x, y) Eyπθ(x)[rRM (x, y)](cid:1)2(cid:105) . This is the standard definition of variance: the expected squared deviation from the mean. Now, we compute the variance of the perturbed reward signal rRM : Varyπθ(x)[rRM (x, y)] = Varyπθ(x)[rRM (x, y) + ϵ]. Using the property of variance for two random variables and B: Var(A + B) = Var(A) + Var(B) + 2Cov(A, B). (20) (21) (22) Here, = rRM (x, y) (a random variable depending on y), and = ϵ (independent of y). Since ϵ is independent of y, and are independent, so Cov(A, B) = 0. Therefore: Varyπθ(x)[rRM (x, y) + ϵ] = Varyπθ(x)[rRM (x, y)] + Var(ϵ). Since Var(ϵ) = σ2, the perturbed reward variance becomes: Varyπθ(x)[rRM (x, y)] = Varyπθ(x)[rRM (x, y)] + σ2. (23) (24) Conclusion: If σ2 > 0, adding zero-mean Gaussian noise to the reward model increases the reward variance by exactly σ2. This aligns with our intuitive understanding based on Theorem 3.3. 2. Change in Reward Model Accuracy (Based on Definition 1). Definition 1 defines the accuracy of reward model rRM at given prompt and distribution over unordered output pairs as: accx,D(rRM ) := E(y,y)D [1 [sign(rRM (x, y) rRM (x, y)) = sign(rG(x, y) rG(x, y))]] , (25) where rG is the ground truth reward function. This measures the probability that the reward model correctly ranks pair (y, y) relative to the ground truth. Now consider the accuracy of the perturbed reward model rRM : accx,D(rRM ) = E(y,y)D [1 [sign(rRM (x, y) rRM (x, y)) = sign(rG(x, y) rG(x, y))]] . (26) We analyze the difference: rRM (x, y) rRM (x, y) = (rRM (x, y) + ϵ1) (rRM (x, y) + ϵ2), where ϵ1, ϵ2 (0, σ2) are independently sampled. Let: rRM = rRM (x, y) rRM (x, y), rG = rG(x, y) rG(x, y), and define η = ϵ1 ϵ2 (0, 2σ2). Then: rRM (x, y) rRM (x, y) = rRM + η. The condition for accuracy becomes: compared to the original condition: sign(rRM + η) = sign(rG), sign(rRM ) = sign(rG). We now analyze how the addition of noise affects this condition: 16 (27) (28) (29) (30) (31) ReDit: Reward Dithering for Improved LLM Policy Optimization - If the original model is accurate, i.e., sign(rRM ) = sign(rG), then adding noise η may cause sign(rRM + η) = sign(rG), especially when η is large enough to flip the sign of rRM . - If the original model is inaccurate, i.e., sign(rRM ) = sign(rG), there is small chance that η flips the sign back to match sign(rG), but this is not systematic and depends on the values of rRM and rG. In general, since η is independent of both rRM and rG, it is more likely to disrupt the correct sign relationship rather than correct it. Thus, adding random noise tends to make the ranking decisions more random. Conclusion: Adding zero-mean Gaussian noise to the reward model makes its relative ranking of output pairs more random, thereby reducing the reward models accuracy (acc). 3. Upper and lower bounds of training time). According to Theorem 3.3, the lower bound on the training time tγ is influenced by the variance of the reward function. Specifically, using Equation (24), we derive that the lower bound on tγ satisfies: 3 (cid:17) (cid:2)Varoπθ(q)R(q, o)(cid:3) + σ2(cid:1) 1 According to Theorem 3.4, in some cases (particularly when perfectly accurate model would lead to arbitrarily slow training), an inaccurate model could instead lead to faster increases in the true reward. We derive that the lower bound on tγ satisfies: (cid:16)(cid:0)EqpQ tγ. (32) Ω tγ O(πθ(0)(yγx)1) (33) C. Training Dynamic In this section, we show more Training Dynamic information. Figure 12. Training Dynamics of Gradient Norm and Reward on Math Dataset. Figure 12 shows the training dynamics of using and not using ReDit on the Math dataset, indicating that using ReDit can solve the problems of gradient oscillation and gradient vanishing, and improve training stability Fig 13 and Fig 14 Training dynamics using uniform and Gaussian perturbations. For both uniform and Gaussian perturbations, ReDit shows amazing gradient stability and training stability. D. Experimental setting D.1. Dataset In this section, we introduce the statistics of the dataset and the additional processing performed on the dataset. The statistics of the dataset are shown in Table 3. 17 ReDit: Reward Dithering for Improved LLM Policy Optimization Figure 13. Training dynamics of gradient norm and reward on the GSM8K dataset, showing the impact of perturbations of different distributions. Figure 14. Training dynamics of gradient norm and reward on the Math dataset, showing the impact of perturbations of different distributions. In addition, We added new templates to the original dataset to ensure the model could complete the required tasks and output formats. It is important to note that the added templates did not alter the original dataset, and special processing was performed for different LLMs. The specific examples are as follows: Dataset Format of GSM8K dataset: GSM8K \"prompt\": [ {\"role\": \"system\", \"content\": \"Respond in the following format: <reasoning> ... </reasoning> <answer> ...</answer>\"}, {\"role\": \"user\", \"content\": \"What is the largest single-digit prime number?\"}, {\"role\": \"assistant\", \"content\": \"<reasoning> 9 is divisble by 3 and 8 is divisible by 2, but 7 is prime. </reasoning> <answer>7</answer>\", {\"role\": \"user\", \"content\": {question}} ], \"answer\": {answer} 18 ReDit: Reward Dithering for Improved LLM Policy Optimization Table 3. Number of samples in the train, validation, and test datasets for various dateset."
        },
        {
            "title": "Number of samples",
            "content": "train dataset validation dataset test dataset GSM8K MATH Geometry3K 7473 7506 2100 - - 1319 5003 601 Dataset Format of MATH dataset: MATH \"prompt\": [ {\"role\": \"system\", \"content\": \"Respond in the following format: <reasoning> ... </reasoning> <answer> ...</answer>\"}, {\"role\": \"user\", \"content\": \"{question} Let\"s think step by step and output the final answer within boxed{}.\" ], \"answer\": {answer} Dataset Format of Geometry3K dataset: Geometry3K \"prompt\": [ {\"role\": \"user\", \"content\": [{ \"type\": \"image\", \"image\": {image}, }, { } \"type\": \"text\", \"text\": {question} + \". You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in boxed{}.\" },], ] \"answer\": {answer} D.2. Reward function We design five reward functions for the GSM8K dataset and show how to implement ReDit: 19 ReDit: Reward Dithering for Improved LLM Policy Optimization GSM8K Accuracy Reward Function (cid:7) 1 def correctness_reward_func_with_noise(prompts, completions, answer, **kwargs) -> list[float]: def extract_number(s: str) -> str: match = re.search(rd+, s) return match.group(0) if match else responses = [completion[0][content] for completion in completions] = prompts[0][-1][content] extracted_responses = [extract_xml_answer(r) for in responses] original_rewards = [2.0 if extract_number(r) == extract_number(a) else 0.0 for , in zip(extracted_responses, answer)] # ReDit add noisy_rewards = [r + random.uniform(-m * 2.0, * 2.0) for in original_rewards ] #noisy_rewards = [r + random.gauss(0, 2.0 * / (3 ** 0.5)) for in original_rewards] return noisy_rewards GSM8K Int Reward Function (cid:7) 1 def int_reward_func_with_noise(completions, **kwargs) -> list[float]: responses = [completion[0][content] for completion in completions] extracted_responses = [extract_xml_answer(r) for in responses] original_rewards = [0.5 if r.isdigit() else 0.0 for in extracted_responses] # ReDit add noisy_rewards = [r + random.uniform(-m * 0.5, * 0.5) for in original_rewards ] #noisy_rewards = [r + random.gauss(0, 0.5 * / (3 ** 0.5)) for in original_rewards] return noisy_rewards GSM8K Strict Format Reward Function (cid:7) 1 def strict_format_reward_func_with_noise(completions, **kwargs) -> list[float]: 2 pattern = r\"ˆ<reasoning>n[sS]*?n</reasoning>n<answer>n[sS]*?</answer>$\" completion_contents = [completion[0][\"content\"].strip() for completion in completions] matches = [re.match(pattern, content, re.DOTALL re.MULTILINE) for content in completion_contents] original_rewards = [1.0 if match else 0.0 for match in matches] # ReDit add noisy_rewards = [r + random.uniform(-m * 1.0, * 1.0) for in original_rewards ] #noisy_rewards = [r + random.gauss(0, 1.0 * / (3 ** 0.5)) for in original_rewards] return noisy_rewards 20 3 4 5 6 7 9 10 11 12 13 (cid:6) 3 4 5 6 7 9 (cid:6) 3 4 5 7 8 9 10 (cid:6) (cid:4) (cid:5) (cid:13) (cid:4) (cid:5) (cid:13) (cid:4) (cid:5) (cid:13) ReDit: Reward Dithering for Improved LLM Policy Optimization GSM8K Sort Format Reward Function (cid:7) 1 def soft_format_reward_func_with_noise(completions, **kwargs) -> list[float]: 2 pattern = r\"ˆ<reasoning>[sS]*?</reasoning>[sS]*?<answer>[sS]*?</answer>$\" completion_contents = [completion[0][\"content\"].strip() for completion in completions] matches = [re.match(pattern, content, re.DOTALL re.MULTILINE) for content in completion_contents] original_rewards = [1.0 if match else 0.0 for match in matches] ReDit add # noisy_rewards = [r + random.uniform(-m * 1.0, * 1.0) for in original_rewards ] #noisy_rewards = [r + random.gauss(0, 1.0 * / (3 ** 0.5)) for in original_rewards] return noisy_rewards 3 4 5 6 7 9 10 (cid:6) GSM8K Reasoning Format Reward Function (cid:7) 1 def xmlcount_reward_func_with_noise(completions, **kwargs) -> list[float]: 2 def count_xml(text) -> float: 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 (cid:6) count = 0.0 if text.count(\"<reasoning>n\") == 1: count += 0.125 if text.count(\"n</reasoning>n\") == 1: count += 0. if text.count(\"n<answer>n\") == 1: count += 0.125 #count -= len(text.split(\"n</answer>n\")[-1])*0.001 if text.count(\"n</answer>\") == 1: count += 0.125 count -= (len(text.split(\"n</answer>\")[-1]) - 1)*0.001 return count contents = [completion[0][\"content\"] for completion in completions] original_rewards = [count_xml(c) for in contents] # ReDit add noisy_rewards = [r + random.uniform(-m * 0.5, * 0.5) for in original_rewards ] #noisy_rewards = [r + random.gauss(0, 0.5 * / (3 ** 0.5)) original_rewards] return noisy_rewards for in (cid:4) (cid:5) (cid:13) (cid:4) (cid:5) (cid:13) As shown in the above code block, ReDit does not need to be modified in complex way, only the reward function needs to be modified, and any method can be easily integrated. The reward functions of other datasets can be found in the code. D.3. Specific experimental parameters In this section, we present the experimental parameters, including LoRA parameters, GRPO and other baseline experimental parameters. Table 4. LoRA Parameters LoRA Target LoRA Rank LoRA Alpha LoRA Dropout & Proj 8 64 0.05 ReDit: Reward Dithering for Improved LLM Policy Optimization Table 5. GRPO Parameters Learning Rate Num Generations Epochs 5e-6 4 10 Table 6. DAPO Parameters Clip Ratio Low Clip Ratio Low Clip Ratio Num Generations Max 0.2 0.28 10.0 10 E. More result In this section, we present detailed numerical results for all experiments. E.1. Main Result In this section, we show the results in Figure 5, the performance of GRPO and GRPO+ReDit on different datasets. Table 7. Performance Comparison of Different Training Steps on the Math Dataset Method Step Instruct model GRPO Uniform ReDit Gauss ReDit 39 - - - 1000 2000 3000 4000 6000 7000 8000 9000 - 47.86 50.02 49.78 - 49.46 50.23 50. - 47.18 50.34 51.03 - 47.28 50.78 51.07 - 47.26 50.96 51.53 - 47.57 51.27 51.43 - 47.63 51.37 52.01 - 47.89 51.37 52. - 48.01 51.96 52.55 Tables 7, 8, 9 show the comparison of ReDit on different datasets. ReDit significantly improves the convergence speed of GRPO. At any same step, ReDit achieves better performance. E.2. Baseline Result In this section, we present all numerical results in Fig. 8. As shown in Table 10, we demonstrate the effect of using ReDit on GSM8K based on the GRPO improvement method. The experimental results show that ReDit can also improve the convergence speed and performance on these algorithms. E.3. Different LLMs Result In this section, we present all numerical results in Fig. 6. E.4. Reward Model Result In this section, we present all numerical results in Fig. 10. E.5. Direct Gradient Manipulation Result In this section, we present all numerical results in Fig. 11. E.6. Variance Result In this section, we show more results on the performance of ReDit as the perturbation changes. As shown in Figure 15, the variance of uniform perturbation is similar to the variance of Gaussian perturbation, and the appropriate variance can achieve the best performance. The specific numerical results are shown in Tables 17 and 18. 22 ReDit: Reward Dithering for Improved LLM Policy Optimization Table 8. Performance Comparison of Different Training Steps on the GSM8K Dataset Method Step 0 2000 3000 4000 5000 6000 8000 9000 Instruct model GRPO Uniform ReDit Gauss ReDit 84.91 - - - - 85.70 89.16 89.02 - 86.01 89.16 89. - 86.47 89.31 89.61 - 86.73 89.31 89.54 - 87.13 89.31 89.54 - 87.78 89.99 89.54 - 88.52 89.99 89.61 - 88.73 89.99 89. - 89.07 90.76 90.46 Table 9. Performance Comparison of Different Training Steps on the Geometry3K Dataset Method Step 0 1000 3000 4000 5000 6000 7000 9000 Instruct model GRPO Uniform ReDit Gauss ReDit 40.43 - - - - 40.60 43.37 43.67 - 42.93 43.89 43.98 - 38.77 44.01 44. - 39.77 44.23 44.25 - 38.94 44.23 44.25 - 39.10 44.23 44.25 - 40.10 44.12 44.25 - 41.36 44.36 44.67 - 43.10 44.36 44. E.7. Scheduled Perturbation Result In this section, we show the changing trends of different scheduled perturbation strategies, as shown in Figure 16. We took the perturbation of Gauss distribution as an example and conducted experiments. The experimental results are shown in Table 19. The CosineReverse strategy shows the best performance. 23 ReDit: Reward Dithering for Improved LLM Policy Optimization Table 10. Performance Comparison at Different Training Steps on Different Baseline Method Step DAPO Uniform ReDit Gauss ReDit DR.GRPO Uniform ReDit Gauss ReDit REINFORCE++ Uniform ReDit Gauss ReDit 1000 84.99 87.03 87.76 84.69 86.27 86. 84.91 86.21 86.17 2000 86.20 87.15 87.96 84.23 86.36 86.23 84.69 86.11 86.27 86.35 87.26 88.01 84.53 86.45 87.10 85.06 86.67 86.47 4000 86.35 87.54 88.01 84.91 86.54 87. 85.14 86.31 86.83 5000 86.75 87.54 88.10 85.67 86.75 87.56 85.14 86.75 86.83 87.04 87.69 88.37 85.67 87.03 87.67 85.14 87.01 87.06 7000 87.12 87.83 88.67 85.67 87.26 87. 86.10 87.26 87.63 8000 87.17 88.03 88.96 85.90 87.16 87.67 86.17 87.59 87.76 87.52 88.57 89.34 86.13 87.34 87.69 86.25 87.59 87.96 Table 11. Llama-3.2-3B-Instruct performance comparison of different training steps on the GSM8K dataset Method Step 7000 5000 4000 6000 8000 3000 2000 0 Instruct model GRPO Uniform ReDit Gauss ReDit 73.62 - - - - 73.60 74.81 74.30 - 73.34 74.81 74. - 74.74 74.89 75.13 - 74.27 75.05 75.13 - 73.90 75.05 75.13 - 73.90 75.66 76.48 - 73.90 76.13 76.40 - 74.01 76.27 76. - 74.04 76.27 76.25 Table 12. Llama-3.1-8B-Instruct performance Comparison of Different Training Steps on the GSM8K Dataset Method Step 0 1000 3000 4000 5000 6000 7000 9000 Instruct model GRPO Uniform ReDit Gauss ReDit 81.05 - - - - 81.02 83.81 83.80 - 81.30 83.24 83.44 - 81.92 83.45 83. - 81.79 84.32 84.47 - 82.01 84.01 84.86 - 82.36 84.32 84.62 - 82.36 84.12 84.12 - 81.63 84.73 84.35 - 82.12 84.92 84. Table 13. Mistral-7B-Instruct-v0.3 performance Comparison of Different Training Steps on the GSM8K Dataset Method Step 0 1000 2000 4000 5000 6000 7000 8000 Instruct model GRPO Uniform ReDit Gauss ReDit 53.68 - - - - 56.33 58.25 58.57 - 58.00 58.48 58.85 - 57.70 58.45 58.68 - 57.39 59.83 59. - 58.23 59.68 59.29 - 58.23 59.23 60.68 - 57.92 60.29 60.61 - 58.91 61.45 61.61 - 59.14 62.07 61.76 Table 14. Ministral-8B-Instruct-2410 performance Comparison of Different Training Steps on the Geometry3K Dataset Method Step 0 1000 2000 3000 5000 6000 7000 8000 9000 Instruct model GRPO Uniform ReDit Gauss ReDit 82.34 - - - - 82.94 85.39 85.24 - 83.85 85.78 85.46 - 84.53 85.69 85.44 - 84.84 85.69 85.44 - 84.84 85.91 86. - 85.22 86.90 86.44 - 84.61 86.90 86.69 - 84.15 87.01 86.76 - 83.70 87.23 86.12 Table 15. Performance Comparison at Different Training Steps on Reward Model Method Step Instruct model RM RM + Gauss ReDit RM + Uniform ReDit 0 84.91 - - - 1000 - 85.03 85.13 85.76 - 86.01 85.99 85.63 3000 - 86.73 85.79 87.03 24 4000 - 87.06 86.37 86. 5000 - 87.53 87.36 87.13 6000 - 87.56 87.36 87.53 7000 - 87.87 87.24 88. 8000 - 88.03 87.96 88.63 9000 - 88.21 88.12 88.63 ReDit: Reward Dithering for Improved LLM Policy Optimization Table 16. Performance Comparison at Different Training Steps on Different Gradient Manipulation Method Step Instruct model GRPO Gradient Clipping Dynamic Sampling Uniform ReDit Gauss ReDit 0 84.91 - - - - - 1000 - 85.03 85.65 85.15 89.16 89. 2000 - 86.01 86.32 86.12 89.16 89.37 3000 - 86.73 86.34 86.12 89.31 89.61 4000 - 87.06 86.37 86.43 89.31 89. 5000 - 87.53 87.12 87.01 89.31 89.54 6000 - 87.56 87.75 87.03 89.99 89.54 7000 - 87.87 88.31 87.34 89.99 89. 8000 - 88.03 88.57 87.92 89.99 89.61 9000 - 88.21 88.57 88.01 90.76 90.46 Figure 15. ReDit uniform perturbation performance changes with variance. Table 17. Performance Comparison of Different variance on the Gauss Perturbation Variance Step 0.01 0.02 0.05 0.1 0.3 0.5 1000 85.97 86.40 89.02 87.64 87.87 86.81 2000 87.01 87.70 89.37 89.08 88.48 87. 3000 87.40 88.16 89.61 89.69 88.78 87.41 4000 87.54 89.23 89.54 89.84 88.93 87.64 5000 87.92 89.39 89.54 90.07 89.39 87. 6000 88.76 90.22 89.54 89.84 89.39 87.95 7000 88.84 90.14 89.61 89.84 89.39 88.32 8000 89.54 90.14 89.61 89.84 89.46 88. 9000 89.54 90.14 90.46 90.07 89.46 88.95 Table 18. Performance Comparison of Different variance on the Uniform Perturbation Variance Step 0.01 0.02 0.05 0.1 0.3 0.5 85.67 85.44 89.16 88.17 87.49 86.73 2000 86.79 86.52 89.16 88.25 88.25 87.72 3000 87.43 87.20 89.31 89.01 88.25 87.64 87.96 88.32 89.31 89.84 88.17 87.79 6000 88.05 88.46 89.99 89.54 87.95 88.48 7000 88.89 88.99 89.99 89.54 88.93 87.87 89.01 89.31 89.99 89.61 88.70 88.02 9000 89.09 89.53 90.76 89.61 88.78 87.87 4000 87.68 88.03 89.31 89.01 88.02 87.64 ReDit: Reward Dithering for Improved LLM Policy Optimization Figure 16. ReDit scheduled perturbation Variance trend with training step (taking the original variance as 0.05 as an example) Table 19. Performance Comparison of Different Scheduled Perturbation Methods Method Step SquareRoot SquareRootReverse Factor FactorReverse MutilFactor MutilFactorReverse Cosine CosineReverse 88.10 88.55 88.25 88.48 87.87 88.17 88.32 89.08 2000 89.31 89.54 88.63 88.32 89.31 88.78 88.32 87.95 3000 88.93 89.46 89.69 89.39 89.01 88.86 89.39 89.54 89.69 90.07 89.46 88.78 89.01 89.01 89.84 89.08 5000 89.46 90.07 89.23 88.93 89.01 88.93 89.76 89.16 6000 89.46 89.31 89.54 89.54 89.61 88.93 89.61 90.37 89.46 89.61 89.46 89.61 89.16 89.39 90.14 90.07 8000 89.46 89.54 89.31 89.76 89.61 89.16 90.46 90.84 9000 90.22 89.69 89.69 89.46 89.46 89.54 90.23 91."
        }
    ],
    "affiliations": [
        "College of Computer Science and Software Engineering, Shenzhen University, China",
        "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China",
        "Hong Kong University of Science and Technology (Guangzhou), China",
        "Tencent, Shenzhen, China"
    ]
}