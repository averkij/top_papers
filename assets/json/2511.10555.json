{
    "paper_title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space",
    "authors": [
        "Huijie Liu",
        "Shuhao Cui",
        "Haoxiang Cao",
        "Shuai Ma",
        "Kai Wu",
        "Guoliang Kang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code."
        },
        {
            "title": "Start",
            "content": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space Huijie Liu1,2 Shuhao Cui2 Haoxiang Cao2,3 Shuai Ma1 Kai Wu2, Guoliang Kang1, 1Beihang University 2Kolors Team, Kuaishou Technology 3South China Normal University Co-Corresponding Author 5 2 0 2 8 2 ] . [ 5 5 5 5 0 1 . 1 1 5 2 : r Figure 1. Visual demonstration of CoTyle. The figure shows five groups of samples (top-left, top-right, bottom-left, bottom-right, center), each generated from distinct style code, with consistent style within each group and distinct styles across groups. Our homepage can be found in https://kwai-kolors.github.io/CoTyle/."
        },
        {
            "title": "Abstract",
            "content": "Innovative visual stylization is cornerstone of artistic creation, yet generating novel and consistent visual styles This work was conducted during the authors internship at Kolors Team, Kuaishou Technology. remains significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we consider the code-to-style image generation task, which aims to produce images with novel and consistent visual styles specified by only numerical code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train discrete style codebook from collection of images to extract style embeddings. These embeddings serve as conditions for text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, numerical style code is mapped to unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Extensive experiments validate that CoTyle effectively converts numerical code into style controller, demonstrating style is worth one code. Compared to existing methods, the stylized images generated by our method are more diverse and consistent, unlocking vast space of reproducible styles from minimal input. 1. Introduction Innovative visual stylization is cornerstone of artistic creation, enabling the expression of unique identities and emotions across digital art, design, and media. With generative models, e.g., diffusion models [1416, 24, 26, 29], users can generate high-fidelity images with arbitrary styles. Existing works on stylistic image generation can be categorized into several groups according to the style specification type, including methods specifying styles by textural prompt, visual image, Low-Rank Adaptation (LoRA) [13], as illustrated in Fig. 2. The radar chart in the figure compares representative methods across three key evaluation including consistency, creativity and reprodimensions, ducibility. Methods specified style by textural prompts usually exhibit poor stylistic consistency, as images generated from identical textual style descriptions often exhibit substantial variations in visual appearance. Other methods either leverage the style of reference image as the style definition [17, 39, 41, 42] or insert pre-trained style LoRA [13] into the generative model to enable specific style generation [7, 30]. Nevertheless, both methods rely on existing style images (either as reference images or LoRA training data), rendering them incapable of creating novel, unseen artistic styles, i.e. they own poor creativity. Furthermore, transferring style information for reproducible generation via these methods requires sharing pixel-level reference images or complex LoRA weights, resulting in poor reproducibility. Thus, existing methods cannot simultaneously achieve high consistency, creativity and reproducibility. To address these limitations, we propose code-to-style generation, novel paradigm that uses numerical style code to create novel styles. This code also functions as both unique identifier for style creation and consistent style reproduction. To date, this task has only been primarily explored by industrial settings (e.g., Midjourney [2]), with no open-source research from the academic community. In response, we propose CoTyle, the first framework designed to realize code-to-style generation and democratize this capability for open-source community and academic research. The core concept of CoTyle is to design style generator capable of producing novel style embedding that guides the T2I-DM in generating images with specific style. Specifically, we begin by training discrete style codebook [6, 21, 37] with contrastive loss on paired style images, with the objective of mapping images sharing the same style to similar distribution, while pushing apart the distributions of distinct styles. This process allows the discrete codebook to pool stylistic information from image features while suppressing irrelevant content. The trained codebook is integrated into pre-trained T2I-DM. We train the T2I-DM to condition its generation on the codebooks output. This results in the T2I-DM capable of imageconditioned stylization. To unlock code-to-style generation, we train an autoregressive transformer as style generator. It learns the distribution of style indices derived from the style codebook under next-token prediction objective, effectively modeling sequences of indices that represent coherent styles. Thus, the autoregressive model serves as an unconditional style generator, producing novel style index sequences. During inference, random token, which is fixed by input code, initializes the autoregressive generation process. The model predicts complete sequence of indices. These indices act as conditioning signals to guide the T2I-DM, enabling the generation of images with consistent, code-defined style. To evaluate our method, we employ the CSD [32] to assess both style consistency and diversity. Extensive experiments validate the efficiency of CoTyle, which generates images in target styles without requiring reference images, lengthy prompts, or model fine-tuning. Furthermore, we extend CoTyle to support image-driven stylized image generation and style interpolation. As illustrated in Fig. 1, CoTyle addresses limitations existing in previous works: 1) similar to image-conditioned methods, it guarantees high stylistic consistency across multiple images generated with the same style code; 2) it enables the creation of novel artistic styles based on arbitrary style codes, achieving strong creativity; 3) the numerical style code serves as concise and portable representation of the target style, ensuring excellent reproducibility for seamless sharing and reproducible generation. In nutshell, our contributions are as follows: We introduce code-to-style image generation, novel task Figure 2. Different to previous methods, CoTyle uses numerical style code to represent style, eliminating the need for complex prompts, images, or LoRAs, and allowing easy creation of unique styles just modifying the code. Creativity, Consistency, and Reproducibility refer to models ability to (1) generate novel styles, (2) produce multiple images in the same style consistently, and (3) reproduce styles using simple, user-friendly style definitions. which enables the creation of diverse, consistent visual styles conditioned solely on numerical style code. We propose CoTyle, the first open-source framework that achieves code-to-style generation by learning discrete style codebook and an autoregressive style generator. Further, we expand it to facilitate image-conditioned generation and supports style interpolation. We conducted extensive experiments that demonstrate the effectiveness of CoTyle. The results validate that single code can serve as powerful, compact style controller, unlocking vast space of reproducible novel styles. 2. Related Work Conditioned Image Generation. Conditioned image generation produces visual content conditioned on userprovided control signals. Early methods [10, 26, 28] established text-based conditioning for text-to-image generation, while subsequent work introduced richer signals. ControlNet [44] and T2I-Adapter [22] provide structural guidance via edge/depth maps or sketches, and DreamBooth [27] with Textual Inversion [8] enable identity-specific generation from few-shot examples. Common conditioning mechanisms include noise inversion [4, 33], adapter modules [11, 43, 45], parameter-efficient fine-tuning [7, 20, 30], and token concatenation [19, 34, 35, 46, 47]. Few studies leverage vision-language models to encode visual information as prompt embeddings; while Qwen-Image [40] does so, it is primarily designed for image editing. Stylistic Image Generation. Stylistic image generation aims to produce images with specific style based on userprovided style references and text descriptions. Certain approaches [16, 17, 25, 38, 39, 41, 42] employ style images as visual conditions, while others [7, 30] train style LoRA [13] modules to steer the generation toward target styles. However, these signals require complex pixel-level maps or additional model parameters for representation, and more importantly, they lack the capability to create novel styles. In industrial applications, Midjourney [2] addressed this limitation by introducing style code-based image generation, yet the absence of technical reports has hindered its adoption and development within the open-source community. 3. Method In this section, we propose CoTyle, the first open-source method for code-to-style generation, as shown in Fig. 3. CoTyle comprises three main components. We begin by training discrete style codebook with pairs of style images (Sec. 3.1). The codebook can extract discrete style embeddings from reference images. Using these style embeddings, we train T2I-DM capable of generating images that share the same style as the reference image. (Sec. 3.2). Finally, we train an autoregressive style generator to generate style indices, unlocking seed-to-style generation (Sec. 3.3). 3.1. Style Codebook Our core idea is to design an autoregressive style generator that produces style indices to guide the T2I-DM in generating images with specific style. To realize this generator, we first train discrete style codebook to act as style extractor. The use of discrete codebook offers two main advantages: (1) the discrete indices align naturally with the next-token prediction objective of autoregressive modeling, and (2) the quantization process inherently suppresses irrelevant content information, which facilitates more effective pooling of style features from images. However, unlike traditional codebooks [6, 21, 37] used Figure 3. Overview of CoTyle. (a) We first train style codebook and an image generation model conditioned on style images. (b) Then, we use the corresponding codebook indices of the style images to train an autoregressive style generator. (c) During inference, style code is used to randomly sample the first index and autoregressively predict the rest. for image reconstruction, the style codebook is not designed to reconstruct the original image with high fidelity using discrete embedding. Instead, its purpose is to encode images with identical styles but different content into the same distribution, while encoding images with divergent styles into different distributions. Thus, we employ contrastive loss to train the model. Without the contrastive loss, the model would simply learn to map diverse styles to an identical embedding. The codebook is trained using features extracted from vision transformer (ViT) [5] and the loss function can be formulated as follows, Lcontrast = 1 (cid:88) i=1 (cid:104) yi (1 si)2 + (1 yi) (ReLU(si m))2(cid:105) , (1) si = F(v1,i) v2,i F(v1,i)v2,i , (2) where is the batch size, yi {0, 1} is the label for the i-th sample pair (1 for identical style and 0 for different style), is the margin parameter that defines the minimum desired separation for negative pairs, v1,i and v2,i are the ViT features of the two samples in the i-th pair, and F() denotes the style codebook. Further, we find that adding reconstruction loss is essential to avoid codebook collapse during training (Sec. 4). This is because we need to leverage the capabilities of the pre-trained vision language model (Sec. 3.2), and our style embeddings should remain close to the image embeddings output by the VLMs image encoder. Lrecon ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:20) F(v1,i) v1,i (F(v1,i)v1,i (cid:21)2 , (3) vector quantization loss Lvq is defined as the sum of them. The final overall loss function is formulated as, Lstyle = Lcontrast + αLrecon + βLvq, (4) where α and β are weighting coefficients. 3.2. T2I-DM Conditioned on Style Codebook To utilize the embedding quantized by the style codebook as style condition, we integrate the codebook into T2I-DM as shown in Fig. 3 (a). Unlike traditional style transfer methods [9, 25, 41, 43], we argue that style information should not be narrowly defined as color, but encompasses rich semantic features. Thus, we treat style embeddings as form of textual input and injecting them into the Diffusion Transformer (DiT [23]) through textual branch. Specifically, we employ vision language model (VLM) [36] as our text encoder, while the style embedding replaces the original image features. This method can help the T2I-DM learn style information that better aligns with human perception. During training, for each pair of images x1 and x2 sharing the same style, we extract the ViT features v1 from x1 and quantize it into style embedding F(v1). We train the T2I-DM [23] conditioned on both the style embedding F(v1) and the text prompt y2 (which corresponds to x2) to generate the target image x2 by rectified flow matching [18]. After training, DiT can generate images with specific styles based on the style embeddings output by the codebook. Notably, although CoTyle is designed for code-to-style generation, it inherently supports image-conditioned generation and outperforms existing methods (Sec. 4). 3.3. Code-to-Style Image Generation Similar to traditional vector-quantized methods [6, 21, 37], we employ commit loss and codebook loss. The We now construct T2I-DM conditioned on embeddings from style codebook. However, these embeddings are Algorithm 1: Code-to-Style Inference Inputs: style code n, prompt Given: style codebook C, codebook size K, number of tokens , suppression coefficient s(i) xt {0, 1} SetSeed(n) I0 U{0, . . . , K} for = 1 . . . do Ii = AR(I0:i1, s(i)); = Lookup(C, I0...N ) x0 = RectifiedFlow(xt, [c, y]) Return: x0 derived from existing images, thus limiting the creation of novel styles. To enable code-to-style generation, we need to train an unconditional style generator to create novel styles. As shown in Fig. 3 (b), for given image, we extract its ViT feature and acquire the corresponding discrete indices from the style codebook. These indices are then used to train an autoregressive model via next token prediction [31], effectively learning the distribution of style features. Fig. 3 illustrates the inference process. user-provided numerical code initializes the random seed. Using this fixed seed, the model samples an initial token I0 and autoregressively generates the subsequent 1 tokens, where is hyperparameter defining the total token sequence length. Using these indices, we retrieve the corresponding vectors from the codebook and decode them into style embeddings. Subsequently, we generate images with specific styles in manner similar to the approach described in Sec. 3.2. The pseudocode for the inference process is presented in Alg. 1. To further enhance the intensity and diversity of the style, we propose sampling strategy. We analyzed the frequency of codebook index across collection of images, and find that certain indices exhibit significantly higher selection frequencies than others (see Appendix for details). Empirical results  (Fig. 7)  show that high-frequency indices represent form of arbitrary style information, just like meaningless placeholders. Sampling exclusively from high-frequency indices yields images with no specific style. Therefore, during inference, we down-weight the logits corresponding to these indices by multiplying suppression coefficient. The suppression coefficient can be formulated as, s(i) = (cid:40) 1, ek(f (i)τ ), if (i) < τ if (i) τ (5) where is the index and () denotes mapping index to frequency value. τ is threshold and is hyperparameter. 4. Experiment 4.1. Implementation Details During the codebook training phase, we set the codebook vocabulary size to 1,024 and the embedding dimension to 64. The model was trained for 20,000 steps with batch size of 128 and learning rate of 1e-5. In the DiT training stage, we initialized the model from pretrained QwenImage [40] and trained it for 60,000 steps with batch size of 64 and learning rate of 4e-6. For the style generator, we employed the Qwen2-0.5B [1] architecture but trained it from scratch without loading any pre-trained weights. This component was trained for 100,000 steps with batch size of 64 and learning rate of 1e-5. We resized style references to 392392 during training, as stylistic features are robust to geometric transformations. Each image was then encoded into 196 style tokens, represented by 196 discrete style indices (i.e. = 196). 4.2. Benchmark For code-to-style generation, we randomly sampled 500 codes, generating 4 images per code, resulting in 2,000 images for evaluation. For image-conditioned generation, we constructed an evaluation set of 500 prompt and reference image pairs. Following [41, 42], we adopted CSD [32] to evaluate image-conditioned methods. For style consistency (Consistency), we computed the CSD score among images generated from the same code for CoTyle and Midjourney [2]. For image-conditioned methods [17, 25, 39, 41, 42], we measured the CSD score between their outputs and reference images. To evaluate style diversity (Diversity), we randomly sampled multiple codes, generated corresponding images, and computed pairwise CSD scores among them. We define the Diversity score as 1 minus the CSD score, so that higher values indicate greater diversity. The Diversity score measures the degree of stylistic diversity across styles corresponding to different codes. Therefore, this metric is specifically designed for the code-to-style task. Additionally, we evaluate text-image alignment using CLIP textimage similarity (CLIP-T) [17, 41, 42] and assess the aesthetic quality of generated images via the recently popular QualityCLIP (Aesthetics) [3]. 4.3. Baselines For the code-to-style task, it is currently the only approach comparable to Midjourney [2]. Since Midjourney is not open-source, we manually collected 500 images from the Midjourney website for evaluation. Given that CoTyle also supports style generation conditioned on reference images, we have compared it against several popular style reference methods, including [15, 17, 39, 41, 42]. Figure 4. Qualitative comparison with Midjourney [2] on code-to-style generation. Each image set (23 grid) is generated from the same style code. Red boxes highlight cases with suboptimal style consistency. 4.4. Results Code-to-style image generation. We presented comparison between CoTyle and the closed-source Midjourney [2] on the code-to-style task in Fig. 4 and Tab. 1, demonstrating that our method achieves significantly superior style consistency. CoTyle achieved better style consistency, where different images generated from the same code exhibited highly similar styles. However, CoTyle showed slightly inferior diversity, which may be attributed to the insufficient breadth of the training dataset, constraining CoTyles ability to learn wider spectrum of stylistic variations. This represented potential direction for future improvement. Comparison with image-conditioned methods. We compared our method with existing image-conditioned approaches. CoTyle demonstrated significant advantages in Table 1. Quantitative comparison with other methods. * denotes CoTyle conditioned on reference images. Bold highlights the best score; underlines indicate the second-highest, omitted in code-to-style evaluation for brevity. CLIP-T measures text-image alignment. For image-conditioned methods, diversity is constrained by the reference image and is neither measurable nor meaningful to evaluate. Methods Open-Source Condition Diversity Aesthetics CLIP-T Consistency StyleStudio [17] CSGO [42] USO [41] Flux-Kontext [16] InstantStyleXL [39] CoTyle* (Ours) Midjourney [2] CoTyle (Ours) Image Image Image Image Image Image Code Code - - - - - - 0.8088 0.7764 0.5074 0.6283 0.7153 0.7636 0.7135 0. 0.5948 0.7173 0.3168 0.3017 0.3331 0.3056 0.3134 0.3230 0.3090 0.3119 0.4711 0.5540 0.4395 0.4222 0.5753 0.5791 0.4734 0.6007 Table 2. Comparison of injecting style condition to DiT through visual branch and textual branch. Methods Aesthetics CLIP-T Consistency Visual Branch Textual Branch 0.7175 0. 0.3255 0.3230 0.5306 0.5791 Table 3. Effect of style loss Lstyle. w/o. negative samples indicates that each data pair sharing an identical style. Loss Aesthetics CLIP-T Consistency Lstyle w/o. negtive samples w/o. Lrecon 0.7178 0.7174 0.7001 0.3230 0.3260 0.3237 0.5791 0.4890 0.4102 Table 4. Effect of high-frequency suppression s(i). Methods Diversity Aesthetics CLIP-T Consistency CoTyle w/o. s(i) 0.7764 0.7488 0.7173 0. 0.3119 0.3210 0.6007 0.5301 of crystal blocks. Effect of style loss. During style codebook training, the style loss Lstyle comprised contrastive loss for style learning and reconstruction loss. We validated the necessity of both the contrastive and reconstruction components, The contrastive loss helped the model better extract style information, enhancing style consistency, while the reconstruction loss aligns the style embedding with the original ViT feature distribution, improving overall performance. The results in Tab. 3 support this hypothesis. Effect of frequency suppression. We found that sampling exclusively from high-frequency indices resulted in images devoid of stylistic diversity, as shown in Fig. 7. We hypothesized that high-frequency indices might function as meaningless placeholders. Therefore, we propose high-frequency suppression strategy. Tab. 4 illustrates that without this strategy, the generated images exhibit low style diversity, as most style codes tend to produce photorealistic results. More analyses are provided in the Appendix. Figure 5. We compare injecting style through textual branch with the existing method through visual branch. Injecting style from the textual branch better preserves semantic information. style consistency. For instance, in Fig. 6, the images generated by our method exhibited more accurate color tones. Furthermore, in the example shown in Row 2, where the image edges featured milky-white borders, our method successfully learned this stylistic representation, while other methods showed no responsiveness to this style. 4.5. Ablation Effect of style injection through textual branch. CoTyle leverages VLM as the text encoder to inject style information through the textual branch. For comparison, we implement style injection through the visual branch using the OminiControl [34, 35], i.e., by concatenating noise features with style condition features along the token dimension. The quantitative results in Tab. 2 confirm this perspective. As shown in the first row of Fig. 5, visual branch injection captures the red tones of the paper-cut style but fails to recognize semantic-style elements (e.g., circular contours). For the second row, visual branch injection only captures warm-toned cues, whereas textual branch injection enables the model to correctly generate human figure composed Figure 6. Qualitative comparison. CoTyle is not only capable of generation conditioned on style codes but also supports style images. Our model can faithfully follow the input text while simultaneously generating the specified style. Figure 7. Sampling solely from high-frequency indices yields style-less images. Row 1 shows the results of vanilla T2I-DM without any style indices, and Row 2, guided by high-frequency indices, produces results nearly identical to T2I-DM. Figure 8. Style interpolation. The leftmost and rightmost images represent two distinct styles. CoTyle enables smooth style interpolation by linearly combining multiple style indices according to user-specified weights. See more results in Appendix. 4.6. Style Interpolation 5. Conclusion Within CoTyle, each style is composed of indices predicted by style generator. Thus, fusion of different styles can be achieved by combining subsets of indices from each. The intensity of the constituent styles is conditioned by adjusting their respective sampling ratios. This combinatorial approach supports the blending of both predefined style codes and styles extracted from user-provided pixel images. As illustrated in Fig. 8, the leftmost and rightmost images represented two distinct styles. By varying the proportion of their indices in the combination, different degrees of stylistic interpolation between them were achieved. This result reveals the composability of style features, suggesting that CoTyle may provide insights for controlling image styles. In this paper, we introduce code-to-style image generation, novel task that enables stylistic image generation through numerical style codes. We propose CoTyle, the first opensource framework to support this task. CoTyle employs dedicated style generator trained to produce novel style embedding from numerical style codes. Then, diffusion model generates images with specific style conditioned on the style embedding. Further, we extend CoTyle to support additional functionalities, including image-conditioned style generation and style interpolation. Extensive experiments demonstrate that CoTyle effectively maps discrete codes to diverse visual styles. Our work establishes opens up future research in discrete stylistic representations."
        },
        {
            "title": "References",
            "content": "[1] Qwen2 technical report. 2024. 5 [2] Midjourney, 2025. 2, 3, 5, 6, 7, 1 [3] Lorenzo Agnolucci, Leonardo Galteri, and Marco Bertini. Quality-aware image-text alignment for opinion-unaware image quality assessment. arXiv preprint arXiv:2403.11176, 2024. 5 [4] Haoxiang Cao, Chaoqun Wang, Yongwen Lai, Shaobo Min, and Xuejin Chen. Causalctrl: Causality-aware control frameIn Proceedings of the work for text-guided visual editing. 33rd ACM International Conference on Multimedia, pages 99209929, 2025. 3 [5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 4 [6] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2, 3, 4 [7] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Cohen-Or. Implicit style-content separation using b-lora. In European Conference on Computer Vision, pages 181198. Springer, 2024. 2, 3 [8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [9] Junyao Gao, Yanchen Liu, Yanan Sun, Yinhao Tang, Yanhong Zeng, Kai Chen, and Cairong Zhao. Styleshot: snapshot on any style. arXiv preprint arXiv:2407.01414, 2024. 4 [10] Gabriel Goh, James Betker, Li Jing, and Aditya Ramesh. Dall3, 2023. 3 [11] Zinan Guo, Yanze Wu, Chen Zhuowei, Peng Zhang, Qian He, et al. Pulid: Pure and lightning id customization via contrastive alignment. Advances in neural information processing systems, 37:3677736804, 2024. 3 [12] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1 [13] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 2, 3 [14] Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, and Wei Liu. Dialoggen: Multi-modal interactive dialogue system for multi-turn text-to-image generation. arXiv preprint arXiv:2403.08857, 2024. [15] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 5, 1 [16] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 2, 3, 7, 1 [17] Mingkun Lei, Xue Song, Beier Zhu, Hao Wang, and Chi Zhang. Stylestudio: Text-driven style transfer with selective control of style elements. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 23443 23452, 2025. 2, 3, 5, 7, 1 [18] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [19] Huijie Liu, Bingcan Wang, Jie Hu, Xiaoming Wei, and Guoliang Kang. Omni-dish: Photorealistic and faithful image generation and editing for arbitrary chinese dishes. arXiv preprint arXiv:2504.09948, 2025. 3 [20] Huijie Liu, Jingyun Wang, Shuai Ma, Jie Hu, Xiaoming Wei, and Guoliang Kang. Separate motion from appearance: Customizing motion via customizing text-to-video diffusion models. arXiv preprint arXiv:2501.16714, 2025. 3 [21] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. 2, 3, 4 [22] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 42964304, 2024. 3 [23] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 4 [24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. [25] Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, and Yongdong Zhang. Deadiff: An efficient stylization diffusion model with disentangled representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86938702, 2024. 3, 4, 5 [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 [42] Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, and Zechao Li. Csgo: Content-style composition in text-to-image generation. arXiv preprint arXiv:2408.16766, 2024. 2, 3, 5, 7, 1 [43] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3, 4 [44] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 3, [45] Xuanpu Zhang, Dan Song, Pengxin Zhan, Qingguo Chen, Kuilong Liu, and Anan Liu. Better fit: Accommodate CoRR, variations in clothing types for virtual abs/2403.08453, 2024. 3 try-on. [46] Xuanpu Zhang, Xuesong Niu, Ruidong Chen, Dan Song, Jianhao Zeng, Penghui Du, Haoxiang Cao, Kai Wu, and An an Liu. Group relative attention guidance for image editing, 2025. 3 [47] Xuanpu Zhang, Dan Song, Pengxin Zhan, Tianyu Chang, Jianhao Zeng, Qingguo Chen, Weihua Luo, and An-An Liu. Boow-vton: Boosting in-the-wild virtual try-on via In IEEE/CVF Conference mask-free pseudo data training. on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 2639926408. Computer Vision Foundation / IEEE, 2025. 3 [28] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [30] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: In Any subject in any style by effectively merging loras. European Conference on Computer Vision, pages 422438. Springer, 2024. 2, 3 [31] Claude E. Shannon and Warren Weaver. The Mathematical Theory of Communication. University of Illinois Press, Urbana, Illinois, 1949. 5 [32] Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. arXiv preprint arXiv:2404.01292, 2024. 2, 5 [33] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [34] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and uniarXiv preprint versal control for diffusion transformer. arXiv:2411.15098, 2024. 3, 7 [35] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. Ominicontrol2: Efficient conditioning for diffusion transformers. arXiv preprint arXiv:2503.08280, 2025. 3, 7 [36] Qwen Team. Qwen2.5-vl, 2025. 4 [37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 2, 3, 4 [38] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. 3, 1 [39] Haofan Wang, Peng Xing, Renyuan Huang, Hao Ai, Qixun Instantstyle-plus: Style transfer Wang, and Xu Bai. with content-preserving in text-to-image generation. arXiv preprint arXiv:2407.00788, 2024. 2, 3, 5, 7, 1 [40] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 3, [41] Shaojin Wu, Mengqi Huang, Yufeng Cheng, Wenxu Wu, Jiahe Tian, Yiming Luo, Fei Ding, and Qian He. Uso: Unified style and subject-driven generation via disentangled and reward learning. arXiv preprint arXiv:2508.18966, 2025. 2, 3, 4, 5, 7, 1 Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Experimental Details In this chapter, we introduce the settings of the baselines we compared against. When validating style-code-conditioned image generation, we compared our method with MidJourney [2]. Since MidJourneys code is not open-sourced, we manually collected all test data from the MidJourney website. When validating image generation conditioned on style images, we compared our method with state-of-the-art style image-based approaches, including StyleStudio [17], CSGO [42], USO [41], Flux-Kontext [15, 16], and InstantStyle [38]. For StyleStudio [17], we set the classifierfree guidance (CFG) [12] to 5.0 and the number of denoising steps to 50. For the style scale controlling style intensity, we used the default value of 1.0 provided by the official implementation. For CSGO [42], we set CFG to 10.0 and the denoising steps to 50. During generation, since no image condition is required, we followed the official recommendation and set the ControlNet [44] conditioning scale to 0.01 and the style scale to 1.0. For USO [41], we set CFG to 4.0 and the denoising steps to 25. For Flux-Kontext [15, 16], we set CFG to 2.5 and, following the procedure in its paper, used the style image as condition while prefixing the prompt with Using this style, .... For InstantStyle [39], we set CFG to 5.0, the denoising steps to 30, and the style scale to 1.0. For all the above methods, we loaded the official pre-trained model weights. 7. Analysis of High-frequency Index To further investigate the learned style codebook, we analyze the utilization frequency of its indices to uncover potential biases or emergent patterns in how the model leverages its discrete representation space. Specifically, we construct diverse dataset of stylistically varied images and encode each image using the trained codebook to extract the sequence of discrete indices that best reconstruct its style. We then aggregate the occurrence counts of each codebook index across the entire dataset, yielding distribution that reflects the relative usage of each latent token. As illustrated in Fig. A1, where the horizontal axis denotes the codebook index and the vertical axis represents the normalized selection frequency, we observe pronounced long-tail distribution: small subset of indices (approximately 510% of the total) is selected with significantly higher frequency than the vast majority of others. This imbalance suggests that the model disproportionately relies on Figure A1. Frequency distribution of style codebook indices. batch of images is encoded using the style codebook, and the selection frequency of all indices is calculated. limited number of tokens to encode stylistic information, rather than distributing semantic content evenly across the codebook. Motivated by this observation, we conduct detailed analysis in Sec. 3.3 of the main paper to determine whether these high-frequency indices correspond to interpretable stylistic attributessuch as brushstroke texture, color palette, or compositional layout. Through controlled ablations, we replace these dominant indices with random or shuffled values and observe minimal degradation in reconstruction quality or perceptual style fidelity. Furthermore, when we isolate the outputs generated using only the top-k high-frequency indices, the resulting images retain strong stylistic coherence despite the absence of low-frequency tokens. These findings strongly indicate that the high-frequency indices do not encode speInstead, cific, semantically meaningful stylistic features. they function as placeholder tokens, essentially serving as generic, high-probability surrogates that the model defaults to for efficiency, likely due to overfitting or insufficient codebook capacity during training. This behavior mirrors the phenomenon observed in language models, where certain filler tokens dominate usage in the absence of finegrained semantic differentiation. Our analysis thus reveals critical limitation in the codebooks expressiveness and motivates future work toward more balanced, semantically disentangled discrete representations. 8. Implications and Insights In this paper, we not only propose novel approach to codeto-style generation but also offer several insightful observa9. Limitations The CoTyle framework represents significant advancement in the domain of code-to-style image generation. However, several limitations should be acknowledged and explored in future research. Firstly, while the experimental results indicate that the stylistic diversity of images generated by CoTyle is commendable, it remains somewhat lower than that observed in Midjourney. This outcome is likely linked to the training regimen of the autoregressive style generator, which relies on discrete style codebook. This codebook, constructed from limited dataset of curated style-paired images, may not fully encapsulate the extensive variability inherent in human artistic expressions. Expanding the diversity of training data with broader range of abstract and diverse artistic sources, or implementing more advanced generative architectures like mixture-of-experts models, may enhance the representation of complex, multi-modal style distributions. Secondly, the resolution and creativity of the generated styles are largely dictated by the quality of the style codebook. Despite utilizing contrastive loss to differentiate styles, the necessity to quantize these into finite codebook inevitably entails some loss of detailed stylistic subtleties. As result, certain intricate or hybrid nuances found in reference images may be attenuated or omitted, potentially affecting the expressiveness of the generated outputs. Finally, although the numerical style code offers robust framework for reproducibility, the exploration of aesthetically or artistically engaging styles presently involves stochastic search across vast code space. Introducing more intuitive mechanisms for navigating or modifying this latent style space could significantly enhance both user agency and practical applicability. Although CoTyle still has some areas for improvement, as the first open-source work on the task of code-to-style generation, it introduces an overall framework for addressing this task, holding the potential to inspire subsequent research in the field of code-to-style generation. 10. More Results 10.1. Style Interpolation We provide additional style interpolation results in Fig. A3. Given two input images, we encode them into two sets of indices using the style codebook and achieve style interpolation by randomly blending the two sets of indices according to specified mixing ratio. In Fig. A3, the leftmost and rightmost images represent the source style references, respectively, while the intermediate images show the results of style interpolation generated with given prompt, using progressively varying blending ratios between the two styles. Figure A2. The impact of token selection strategy on the generation results during style interpolation. tions worth further discussion. First, we introduce discrete style codebook to extract and represent stylistic information from images in discrete latent space. Building upon this representation, we demonstrate that smooth style interpolation can be achieved by blending different style codes. As shown in Fig. A2, we observe that the specific token selection strategy during interpolation has minimal impact on the generated results, regardless of whether it is achieved through random sampling or splitting tokens evenly between the two styles (e.g., first half from style and second half from style B). This indirectly highlights key distinction between our learned style representation and traditional image representations: the stylistic information is invariant to the order of its tokens. This property aligns naturally with human intuition about style, which is typically perceived as holistic attribute rather than sequence-dependent structure. Style interpolation is just one emergent application enabled by our learned style representation; the structural properties of this discrete space may facilitate the development of additional, more sophisticated, and practically valuable functionalities. Furthermore, this discrete feature extraction paradigm holds promise for extension to other modalities, such as audio. Second, we advocate for injecting style information from the textual modality rather than the visual. We argue that human perception of style is inherently semantic rather than purely chromatic; thus, conditioning on text enables the model to capture stylistic attributes that better align with human intuition. Finally, code-to-style image generation is task of substantial practical potential, yet it has remained unexplored in the academic literature. We present the first comprehensive framework to address this problem and publicly release both the model weights and source code, which we hope will inspire further research in this emerging direction. 10.2. Code-to-Style Generation We present additional code-to-style generation results in Fig. A4, Fig. A5, and Fig. A6 to provide more comprehensive evaluation of our methods capability in translating structured code inputs into visually coherent stylistic outputs. In each figure, the generation process is carefully controlled: all images within given row are produced using the same random seed, ensuring that variations across columns arise solely from differences in the input text prompt, rather than stochastic sampling. Conversely, all images within column are generated under the same textual instruction, allowing direct comparison of stylistic consistency and fidelity across different code conditions. This design enables clear disentanglement of the influence of code structure versus textual guidance on the final output. The corresponding text prompts for each column are listed in Table A1. These results further demonstrate the robustness of our approach in mapping diverse code-based styles to rich, semantically aligned visual outputs. Table A1. Prompts used in visual figures. id prompt 1 2 3 4 6 7 8 woman in dress, with her hair tied in bun, faces away from the viewer. The background is composed of floral patterns. The image shows an artwork featuring silhouette of woman in long dress, standing next to bench, with an abstract painting in the background containing large circular object that appears to be the moon. The image shows portrait of woman, with her face obscured by leaves and few leaves decorating the background. The image shows spiral staircase with light streaming down from the top, illuminating the staircases surface. The staircase is located inside large building, with the walls and ceiling in the background. The image shows dog wearing sweater, standing by window, with the scenery outside the window blurred in the background. The image shows natural landscape with several uniquely shaped rocks and trees growing in the foreground. In the background is sky dotted with clouds and two flying birds. The rocks and trees are clearly reflected in the water. The image shows cake covered in buttercream and candies, with clouds in the background. The image shows corridor made of arches and spheres, with floor made of blocks and background. Figure A3. More visual results of style interpolation. Figure A4. More visual results of CoTyle. Figure A5. More visual results of CoTyle. Figure A6. More visual results of CoTyle."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Kolors Team, Kuaishou Technology",
        "South China Normal University"
    ]
}