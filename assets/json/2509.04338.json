{
    "paper_title": "From Editor to Dense Geometry Estimator",
    "authors": [
        "JiYuan Wang",
        "Chunyu Lin",
        "Lei Sun",
        "Rongying Liu",
        "Lang Nie",
        "Mingxing Li",
        "Kang Liao",
        "Xiangxiang Chu",
        "Yao Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be a more suitable foundation for fine-tuning. Motivated by this, we conduct a systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by ``refining\" their innate features, and ultimately achieve higher performance than their generative counterparts. Based on these findings, we introduce \\textbf{FE2E}, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editor's original flow matching loss into the ``consistent velocity\" training objective. And we use logarithmic quantization to resolve the precision conflict between the editor's native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiT's global attention for a cost-free joint estimation of depth and normals in a single forward pass, enabling their supervisory signals to mutually enhance each other. Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100$\\times$ data. The project page can be accessed \\href{https://amap-ml.github.io/FE2E/}{here}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 8 3 3 4 0 . 9 0 5 2 : r FROM EDITOR TO DENSE GEOMETRY ESTIMATOR Jiyuan Wang1,2 Chunyu Lin1 (cid:66) Lei Sun2 Rongying Liu1 Lang Nie3 Mingxing Li2 Kang Liao4 Xiangxiang Chu2 Yao Zhao1 1BJTU 2AMAP Alibaba Group 3CQUPT 4NTU Figure 1: We present FE2E, DiT-based foundation model for monocular dense geometry prediction. Trained with limited supervision, FE2E achieves promising performance improvements in zero-shot depth and normal estimation. Bar length indicates the average ranking across all metrics from multiple datasets, where lower values are better. represents the amount of training data used. ABSTRACT Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be more suitable foundation for fine-tuning. Motivated by this, we conduct systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by refining their innate features, and ultimately achieve higher performance than their generative counterparts. Based on these findings, we introduce FE2E, framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editors original flow matching loss into the consistent velocity training objective. And we use logarithmic quantization to resolve the precision conflict between the editors native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiTs global attention for cost-free joint estimation of depth and normals in single forward pass, enabling their supervisory signals to mutually enhance each other. Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100 data. The project page can be accessed here. Work done during the internship at AMAP, Alibaba Group. (cid:66) Corresponding author Project Leader 1 Figure 2: FE2E Adaptation Pipeline. The grey background shows the original editors workflow, while the other details FE2E: ① pre-trained VAE encodes the logarithmically quantized depth d, input image x, and normals into latent space. ② The DiT fθ learns constant velocity from fixed origin zy 1, independent of or instructions. ③ By repurposing the discarded output region, FE2E jointly predicts depth and normals without extra computation. Training loss is computed in the latent space, with final predictions decoded by VAE only at inference. 0 to the target latent zy"
        },
        {
            "title": "INTRODUCTION",
            "content": "Dense geometry prediction tasks, such as depth/normal estimation, are crucial for wide range of applications such as augmented reality (Minaee et al., 2022), and 3D reconstruction (Li et al., 2025). Estimating pixel-level geometric attributes from single image is an ill-posed problem and can only be solved with the help of prior knowledge, such as typical object shapes and sizes, occlusion patterns, etc. Based on this observation, recent works ingeniously leverage the priors from pretrained text-to-image (T2I) generators, typically Stable Diffusion (Rombach et al., 2022), for zeroshot dense prediction (Ke et al., 2024), yielding impressive results with limited training data. However, these generative models are initially designed for T2I generation and lack the ability to capture the geometric cues from the absent image inputs. In contrast, image editing models have recently risen to be universal framework to solve more diversified image-to-image (I2I) tasks, such as semantic segmentation and depth estimation (Wu et al., 2025). We argue that these editing models not only align with the dense estimation paradigm but also possess deep understanding of input images while maintaining the generative advantages, and offer more suitable foundation for dense geometry prediction. Motivated by this intuition, we systematically analyze the fine-tuning process of the image editing models versus their generative counterparts. Our analysis reveals that the features of editing models are inherently aligned with geometric structures, and the fine-tuning process only requires to refine and focus this perceptual ability for dense estimation tasks. In contrast, although generative models can gradually acquire this capability from scratch, this process leads to substantial feature reshaping and cannot fundamentally bridge this gap (Sec. 3.1). Therefore, in this paper, we explore this editing option and propose From Editor to Estimator (FE2E)  (Fig. 2)  , diffusion transformer (DiT) model built upon the current SoTA editor Step1X-Edit (Liu et al., 2025), along with fine-tuning protocol to adapt it for dense geometry prediction tasks. However, the direct adaptation of an image editing model for geometric dense prediction is often suboptimal, due to the inherent differences between the two tasks. First, compared to editing, the dense geometry prediction is more deterministic, as only one unique ground-truth (GT) exists. Our analysis of Step1X-Edit reveals that its training objectivespredicting instantaneous velocity1are only tailored for indeterminate tasks and will introduce errors in dense prediction. Based on this observation, we reformulate the training objective as consistent velocity1 and set fixed starting point for stable training. Second, image editing models like Step1X-Edit are typically trained with BF16 precision, which is sufficient for RGB outputs, whereas dense geometry prediction tasks like 1velocity denotes the direction and speed of transformation that change input to output, see Sec. 3.2 depth estimation demand much higher numerical precision. This discrepancy does not occur in previously adopted foundation models like Stable Diffusion v1.5/v2, which offer FP32 checkpoints. To address this limitation and improve computational efficiency, we analyze the GT quantization strategies and adopt logarithmic quantization to alleviate precision-related artifacts. After adapting the editor, we now focus on its role as an estimator. In this paper, we select two primary geometry prediction tasks: zero-shot depth estimation and normal estimation, to validate FE2Es effectiveness. Depth and normal both contribute to unified geometric representation, and joint estimation can leverage their potential connections. Unlike GeoWizard(Fu et al., 2024) that introduces additional cross-attention and switchers, we observe that the global attention mechanism of DiT can be repurposed to perform joint estimation of depth and surface normals in single forward pass, incurring virtually no additional computational cost. This design allows the supervisory signals from both tasks to interact, enhancing the overall performance. Extensive experiments demonstrate that our model achieves notable zero-shot performance improvements compared to previous state-of-the-art (SoTA) models. Our contribution can be summarized as follows: We systematically analyze the fine-tuning process of image editors and generators, revealing that editing models are more suitable for dense geometry prediction. Based on this, we introduce FE2E, novel framework that, for the first time, successfully adapts pre-trained image editing model for this task. We identify and address the challenges that arise from this paradigm shift: 1) Reformulate the training objective to align with the deterministic nature of dense prediction; 2) Adopt logarithmic quantization to resolve the precision conflict. 3) Design cost-free joint estimation to allow supervision from different tasks to mutually enhance predictions. Based on above enhancements, FE2E achieves impressive performance gains, including 35% AbsRel improvement on the ETH3D dataset. Even when using only 0.2% of the geometric GT data for training, FE2E outperforms the data-driven models like DepthAnything v1/v2."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Image Generative and Editing models In the field of image generation, Stable Diffusion series (Rombach et al., 2022) and FLUX series (Labs, 2024) models have basically become the community standard. They both trained with massive datasets and demonstrate extremely high generation quality. Concurrently, the field of image editing is still evolving rapidly. Recent advancements include Step1X-Edit (Liu et al., 2025), model fine-tuned from FLUX, which demonstrates superior instruction-following and image understanding capabilities; The multi-modal Qwen-Image (Wu et al., 2025) Editor combines with LLM, attempting to expand the editor into unified computer vision framework. We conduct more detailed review in the appendix Sec E. Dense Geometry Estimation, encompassing tasks like depth and normal estimation(Wang et al., 2024a;b), is cornerstone of 3D computer vision. Early research predominantly focused on supervised learning paradigms, where models were trained and evaluated on specific datasets (Eigen et al., 2014; Eigen & Fergus, 2015). significant shift occurred with MiDaS (Ranftl et al., 2020), which pioneered cross-dataset generalization for dense estimation. This line of work was extended by models like DPT (Ranftl et al., 2021) and Omnidata (Eftekhar et al., 2021), which further improved zero-shot performance. More recently, the field has witnessed the rise of data-driven models such as the Depth Anything series (Yang et al., 2024a;b) and the Metric3D series (Hu et al., 2024), which leverage massive datasets to train powerful, general-purpose geometric estimators. Generative Models for Dense Estimation. In parallel to the trend of scaling up data, an alternative approach emerged by leveraging the rich priors of pre-trained generative models. Works like Marigold (Ke et al., 2024) and GeoWizard (Fu et al., 2024) showed that fine-tuning diffusion models on limited data could yield remarkable performance, effectively harnessing the models learned world knowledge. This paradigm was further refined by GenPercept (Xu et al., 2024), StableNormal (Ye et al., 2024), Diffusion-E2E-FT (Martin Garcia et al., 2025), Lotus (He et al., 2024), and Jasmine (Wang et al., 2025). These studies identified and addressed the limitations of standard diffusion formulations, developing the single-step denoising architecture to boost performance. In this paper, we also build FE2E with limited data, posit that the I2I editing models are inherently better than the T2I models, like Stable Diffusion (Rombach et al., 2022) for dense estimation. 3 Figure 3: Comparison between the Generative and Editing foundation models. We analyze the feature evolution at both the initial (Epoch 1) and final (Epoch 30) stages of fine-tuning, resulting in 4 groups. Each group presents: the DiT features at the input end (Block1), middle layers (Block20), output end (Block35), and the depth predictions AbsRel (Absolute Relative error). Visual implementation detailed in Sec B."
        },
        {
            "title": "3 METHODS",
            "content": "We first conduct an in-depth investigation into the fine-tuning process between the editor and generator in Sec 3.1. Then, we adapt the editor into an estimator (Sec 3.2) by introducing three key contributions to the training objective (Sec 3.3), GT quantization(Sec 3.4), and joint estimation (Sec 3.5). 3.1 FINE-TUNING ANALYSIS OF EDITOR AND GENERATOR In this paper, we select Step1X-Edit as the editor and FLUX as the generator, owing to their shared DiT architecture and SoTA performance in their respective tasks. Note that FE2E can also generalize to other DiT-based editing models. To facilitate fair comparison, we adopt an improved experimental setup and additionally train FLUX-based estimator for this analysis. The training details are provided in Sec B. Through this systematic analysis, we identify three key advantages of editing-based models over generative predecessors for dense estimation tasks. First, the editing model possesses superior inductive bias for image-to-image dense estimation tasks, providing much stronger starting point for finetuning. This is evident in Fig. 3 (a1 v.s. a2), in the early stage and blocks, the editors internal features already align with the input images geometric structures, while the generative ones are abstract and unstructured. The loss difference in Fig. 4 shows the same conclusion. Figure 4: Quantitative comparison of the training loss between Generative and Editing foundation models. The main plot details the convergence loss from epoch 5 to 30, while the inset displays the steep initial loss reduction during the first 10 epochs, which occurs on different scale. Second, the above difference directly impacts the learning dynamics: as illustrated in Fig. 4, the editor achieves more stable convergence, in contrast to the oscillations seen in the generative ones. This difference can be further explained in Fig. 3, the fine-tuning process significantly reshapes the characteristics of generative models, while the editors features are more like refinement and focusing. After 30 epochs of fine-tuning, the generated model learned highly structured and semantic features (a4, b4, c4) from chaotic states (a2, b2, c2), achieving qualitative leap. Whereas the editing ones make the well-structured features (a1, b1, c1) clearer and task-oriented (a3, b3, c3), with their features being incrementally honed rather than fundamentally altered. Third, the structured learning and characteristics reshaping mentioned above are unable to address the shortcomings of the generative model. As shown in Fig. 4 (epochs 20-30, especially ), the generative models training loss meets bottleneck around 0.08, while the editing ones can reduce to 0.073. The Table 4 (ID6 vs. ID7) further verifies that this bottleneck persists at test time, resulting in significant performance gap. 4 In summary, the analysis of feature evolution, training dynamics, and performance consistently demonstrates that editing models provide more stable, effective, and promising foundation for dense geometry estimation."
        },
        {
            "title": "3.2 FROM EDITOR TO ESTIMATOR",
            "content": "As shown in Fig. 2, we pose monocular dense estimation as an image editing task and use the Flow Matching Loss for supervision. Initially, we take the input image RHW 3 as the editing source and the geometric annotation RHW 3 as the expected editing results. First, the VAE, which consists an encoder E() and decoder D(), is used to encode the input image into latent representation zx = E(x) Rhwc. Then, the editing process is modeled as flow path from noise vector zy 0 (0, I) to the target latent representation zy 1 = E(y). The trajectory is defined as: = tzy zy 1 + (1 t)zy 0, [0, 1]. (1) The DiT backbone, denoted as fθ, is trained to predict the velocity vector of this flow, which is simply = dzy 0. The model is optimized by minimizing the flow matching loss (Lipman et al., 2022): dt = zy 1 zy = Et,zy 1 ,zy 0 fθ(zx, zy , t)2. (2) In the inference stage, we predict the editing target ˆzy equation: 1 by solving the following ordinary differential 1 = zy ˆzy 0 + (cid:90) 1 0 fθ(zx, zy , t)dt, (3) and the final dense geometry predictions are given by ˆy = D(ˆzy process are provided in Sec D. 1). More details of the flow matching 3.3 CONSISTENT VELOCITY FLOW MATCHING WITH DETERMINISTIC DEPARTURE Flow Matching has been widely adopted in modern generative and editing modeling. As shown in Fig. 5, previous work MeanFlow (Geng et al., 2025) has identified that, since the model learns the velocity over all possible flow paths in Eq 1, the global instantaneous velocity field becomes inherently nonlinear and typically induces curved trajectory. During inference, as shown in Fig. 5 (b), the ideal integration path in Eq. 3 is approximated by discrete numerical solver, which introduces non-trivial approximation error for high-precision tasks such as dense geometric estimation. An intuitive idea is to find straight integration path, which means the velocity direction always remains the same. In this paper, we further require the velocity magnitude to be consistent so that the velocity is completely independent of t, and redefine the loss as: = Ezy 1 ,zy 0 fθ(zx, zy 0)2. (4) Figure 5: Left: GT velocity field for network training. The gray dots represent different Gaussian noise (top) or zero starting points (bottom), the red dots represent data samples. Right: Instantaneous velocity determines the tangent direction and creates errors in the cumulative path (top); The constant speed path is straight line. Additionally, one key characteristic of generative/editing models is their stochastic nature, which is essential for producing diverse outputs. For deterministic dense prediction tasks, however, this stochasticity is not only unnecessary but also introduces undesirable variance into the training objective, complicating the optimization of fθ. Therefore, we simplify the objective by reducing it from 5 (5) (6) stochastic expectation over all possible zy further refines the loss as: = Ezy 1 and the inference process can be simplified as: fθ(zx)2, 0 to deterministic formulation with fixed zy 0 = 0. This 1 = zy zy 0 + (cid:90) 1 0 fθ(zx)dt = 0 + (1 0)fθ(zx) = fθ(zx). Overall, as shown in Fig. 5 (c) and (d), our refined flow matching not only eliminates the errors introduced by discretized curved trajectories and random starting points, but also significantly reduces inference time, achieving simultaneous improvements in both performance and efficiency."
        },
        {
            "title": "3.4 LOGARITHMIC ANNOTATION QUANTIZATION",
            "content": "Table 1: Quantization errors at BF16 precision on Virtual KITTI dataset. Calculation details in Sec C. (a) Uniform Error Absrel 80m 16cm 0.002 0.1m 16cm 1.600 (b) Inverse Error Absrel 125m 1.563 0.2mm 0.002 (c) Logarithmic Error Absrel 1.04m 0.013 1.3mm 0.013 Figure 6: Illustration of BF16 quantization error. (b) and (d) show GT depth visualized with BF16 precision. For clarity, (a) and (c) use Maigolds VAE regularization, mapping max/min values to 1/-1, respectively. Modern generative/editing models are almost exclusively trained with BF16 precision. This is not only because BF16 precision saves the training cost, but also for this their typical outputs, RGB images; precision is entirely sufficient. Specifically, normalized BF16 value is represented as: = (1)S 2(E127) (1.F )2, where is the sign (1 bit), is the exponent (8 bits), and is the fraction (7 bits). Due to the [-1,1] data range of VAE encoded input (Liu et al., 2025), the worstcase precision occurs at [0.5, 1.0], which is 2126127 27 = 1/256 and perfectly satisfies the RGB range of 0-255. Uncritically finetuning these models with FP32, as done in Marigold or Lotus, not only increases training/inference costs, but also leads to suboptimal inheritance of the baseline models priors, and restricts the capabilities of BF16-only models like Step1X-Edit. Therefore, finetuning with BF16 is necessary. However, as shown in Fig. 6 (a,b), when meeting the depth annotations in the Virtual KITTI dataset, the valid depth range is 0-80m. Uniformly regularizing to [-1,1] requires reduction of 40 times, and the accuracy of 1/256 is reflected in the original depth with significant error of 40/2560.16m. These errors result in an AbsRel of 1.6 at 0.1m (Table 1 (a)) and make the finetune process unfeasible. Previous works have employed an inverse quantization scheme, which means converting the reciprocal of the depth, or disparity, to BF16 precision (Fig. 6 (c, d)). As shown in Table 1 (b), despite offering extremely high precision at close ranges, this scheme becomes entirely unusable at greater distances, and even makes 39m and 78m correspond to the same value. The principles and calculations are detailed in Sec C. After numerous attempts and explorations, we use the logarithmic depth quantization to achieve good precision at both near and far ranges (Table 1(c)) while reducing training and inference costs. Specifically, we first perform the logarithmic quantization with Dlog = ln(DGT + 1e 6), then follow and refine Marigolds depth normalization strategy, defining the supervision label yD as: yD = (cid:28)(cid:18) (Dlog Dlog,2) (Dlog,98 Dlog,2) (cid:19) (cid:29) 0.5 2 , (7) where Dlog,i corresponds to the i% percentiles of Dlog, and is the BF16 precision truncation. 6 Figure 7: Quantitative comparison on zero-shot depth and normal estimation. The 1st row shows the input, the 2nd, 3rd rows are previous SoTA methods results, and the 4th row is ours prediction. White arrows highlight the regions we significantly improved. Zoom in for better view. 3.5 COST-FREE JOINT ESTIMATION There are inherent connections between depth and normal, as both contribute to unified geometric representation of 3D shapes. Normals describe surface variations and undulations, while depth outlines the spatial arrangement that guides the orientation of the normals. Thus, previous work like GeoWizard (Fu et al., 2024) attempted to control the SD model with geometry switcher, but this approach doubled the training cost. In contrast, as shown in Fig. 2 grey part, Step1X-Edit and other DiT-based editing works have found that, the DiT architecture can effectively guided image generation by horizontally concatenating the noise and condition latents, that is, the input is formulated as zx+Θ = concat(zx, zΘ) Rh2wc (zΘ is the noise latents, shown in Fig. 2). However, after processing by the DiT, although the models output has the same shape as the input, fθ(zx+Θ) = [pl, pr] Rh2wc, supervision is only applied to the region corresponding to the original noise, i.e., = pr2, where pl, pr Rhwc. It is worth noting that DiT architecture possesses the global attention across the (h, 2w) dimension, which naturally allows for mutual information exchange and high consistency between pl and pr. Based on this observation, without introducing any additional training or inference costs, we additionally incorporate another tasks supervision on pl during finetuning, modifying the flow matching loss to: Lf = Ezy 1 (vD pl2 + vN pr2). (8)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 IMPLEMENTATION DETAILS We build FE2E upon the Step1X-Edit v1.0 framework (Liu et al., 2025). To further enhance the DiTs representational power, we also introduce an auxiliary dispersion loss that encourages features from different samples to spread out in the hidden space, which is detailed in Sec A.1. During finetuning, all parameters except for the DiT module are frozen, and the language control input is left blank. The process employs LoRA (Hu et al., 2021) with rank = 64 and scale factor α = 32. We trained for 30 epochs using the AdamW optimizer (Loshchilov & Hutter, 2019) with an initial learning rate of 1 104. With gradient checkpoint enabled, the model can be trained on single RTX 4090 GPU, but to accelerate experimentation, training was conducted on NVIDIA H20 GPUs, completing in approximately 1.5 days. 4.2 TRAINING DATASETS We train our model for joint depth and normal estimation on mixture of two synthetic datasets: Hypersim (Roberts et al., 2021) and Virtual KITTI (Cabon et al., 2020). For Hypersim, photorealistic indoor dataset, we use its official training split after filtering out samples with over 1% invalid pixels, resulting in approximately 51k images at 1024768 resolution. For Virtual KITTI, synthetic street view dataset, we utilize four driving scenarios, totaling around 20k samples at 1216352 resolution with maximum depth of 80m. Following Marigold, each training batch is constructed by sampling from Hypersim and Virtual KITTI with probabilities of 90% and 10%, respectively. The evaluation dataset and evaluation metrics also follow Marigold and are detailed in Sec A.2, A.3, respectively."
        },
        {
            "title": "4.3 QUANTITATIVE EVALUATION",
            "content": "Table 2: Quantitative comparison on zero-shot affine-invariant depth estimation between FE2E and SoTA methods. The best and second best performances are highlighted. denotes the method relies on pre-trained Stable Diffusion. Method Training NYUv2 (Indoor) KITTI (Outdoor) ETH3D (Various) ScanNet (Indoor) DIODE (Various) Avg Data AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 Rank MiDaS GeoWizard GenPercept Marigoldv1.1 Marigold DepthAnything V2 Lotus-G Diffusion-E2E-FT Lotus-D DepthAnything FE2E 2M 280K 74K 74K 74K 62.6M 59K 74K 59K 62.6M 71K 11.1 5.6 5.6 5.8 5.5 4.5 5.4 5.4 5.1 4.3 4.1 88.5 96.3 96.0 96.1 96.4 97.9 96.8 96.5 97.2 98.1 97.7 23.6 14.4 13.0 11.0 9.9 7.4 8.5 9.6 8.1 7.6 6.6 63.0 82.0 84.2 88.8 91.6 94.6 92.2 92.1 93.1 94.7 96. 18.4 6.6 7.0 7.0 6.5 13.1 5.9 6.4 6.1 12.7 3.8 75.2 95.8 95.6 95.5 95.9 86.5 97.0 95.9 97.0 88.2 98.7 12.1 6.4 6.2 6.6 6.4 - 5.9 5.8 5.5 4.3 4.4 84.6 95.0 96.1 95.3 95.2 - 95.7 96.5 96.5 98.1 97.5 33.2 33.5 35.7 30.4 30.8 26.5 22.9 30.3 22.8 26.0 22.8 71.5 72.3 75.6 77.3 77.3 73.4 72.9 77.6 73.8 75.9 81. 10.6 8.4 7.8 7.6 6.3 5.4 4.7 4.6 3.7 3.5 1.4 Zero-shot Depth Estimation Comparison As presented in Table 2, FE2E significantly outperforms recent SoTA methods across five challenging benchmarks. Notably, on the ETH3D and KITTI datasets, it reduces the AbsRel error by 35% and 10% respectively, compared to the 2nd-best method. Remarkably, despite being trained on only 0.071M images, FE2Es average rank surpasses that of the DepthAnything series, which was trained on massive 62.6M image dataset. This highlights the effectiveness of our strategy: inheriting the editing model priors rather than simply scaling up training data. Furthermore, qualitative comparisons in Fig. 1 and 7 demonstrate that FE2E produces superior results in challenging lighting conditions (extreme-light, low-light, etc.) and better preserves distant details, which reveal the core advantages that contribute to FE2Es superior performance. We provide further comparisons with concurrent unified works in Sec F. Table 3: Quantitative comparison on zero-shot surface normal estimation between FE2E and SoTA methods. refers to the Marigold normal model as detailed in this link. Method Marigold GeoWizard GenPercept StableNormal Lotus-G DSINE Lotus-D Diffusion-E2E-FT Marigoldv1.1 FE2E Training Avg. Data MeanErr 11.25 MeanErr 11.25 MeanErr 11.25 MeanErr 11.25 Rank ScanNet (Indoor) iBims-1 (Indoor) NYUv2 (Indoor) Sintel (Outdoor) 74K 280K 74K 250K 59K 160K 59K 74K 77K 71K 20.9 18.9 18.2 18.6 16.5 16.4 16.2 16.5 16.1 16.2 50.5 50.7 56.3 53.5 59.4 59.6 59.8 60.4 60.5 59.6 21.3 17.4 17.7 17.1 15.1 16.2 14.7 14.7 14.5 13.8 45.6 53.8 58.3 57.4 63.9 61.0 64.0 66.1 66.1 67.2 18.5 19.3 18.2 18.2 17.2 17.1 17.1 16.1 16.3 15.1 64.7 63.0 64.0 65.0 66.2 67.4 66.4 69.7 68.5 70. - 40.3 37.6 36.7 33.6 34.9 32.3 33.5 - 31.2 - 12.3 16.2 14.1 21.0 21.5 22.4 22.3 - 22.3 9.5 8.9 7.4 7.2 5.2 4.6 3.0 2.6 2.0 1.6 Zeroshot Normal Estimation Comparison As presented in Table 3, FE2E also achieves SoTA performance on the zero-shot normal estimation task, outperforming the methods in the recent 2 years across four benchmarks. This quantitative superiority stems from its ability to handle complex geometries. As illustrated in Fig. 1, 7, FE2E excels at reconstructing intricate details such as surface folds and small objects, which are often challenging for other models."
        },
        {
            "title": "4.4 ABLATION STUDY",
            "content": "Effect of Foundation Model. FE2E is based on the new foundation model Step1X-Edit, the direct adaptation protocol (ID2) establishes strong baseline, outperforming Marigold (based on SD v2) by 8% and 4% on ETH3D and KITTI datasets, respectively (AbsRel, the same below). Building on this, FE2E (ID8) further reduces the AbsRel by 32.1% on ETH3D and 30.5% on KITTI, which confirms the effectiveness of our proposed techniques. Effect of Editing Priors. We trained the FLUX-based model under both DirectAdapt and Improved settings, corresponding to ID1 and ID7 in Table 4. Compared with their counterparts (ID2 and ID6), the editing-based models consistently outperform the generative models, regardless of equipping our proposed improvements. These results, together with the findings in Sec 3.1, highlight the effectiveness of leveraging editing model priors for dense prediction tasks. Effect of Improved Flow Matching. Adopting the consistent velocity training objective effectively eliminates accumulated inference errors from the original paradigm, leading to notable performance gains of 7% on KITTI and 10% on ETH3D (ID2 v.s. ID3). Introducing fixed starting point further eases optimization and brings additional improvements (ID3 v.s. ID4). Effect of Data Quantization. Supervision leads to substantial performance gains, with ID6 outperforming ID4 by 19% and 13% on the KITTI and ETH3D datasets, respectively. Notably, inverse quantization (ID5) generally outperforms uniform quantization, typically because there are more valid pixels nearby. Effect of Joint Estimation. Results from ID6 and ID8 demonstrate that joint prediction further enhances model performance. This synergistic effect is more clearly illustrated in Fig. 8, where joint training yields notable improvements in challenging scenarios such as flat butterfly structures and distant buildings. Figure 8: Qualitative comparison on the Joint Estimation. The w/o Joint Estimation shows 2 models results. Table 4: Ablation studies on the step-by-step design of our adaptation protocol. Here we show the results in zero-shot depth estimation. CV: Consistent Velocity; FS: Fixed Start; JE: Joint Estimation; Quant: Quantization, sub-items same with Table 1). DirectAdapt refers to the formulation in Sec 3.2 and Improved are the generative/editing models with improvements in Sec 3.3 and 3.4. ID Note Foundation CV FS JE Quant KITTI ETH3D AbsRel δ1 AbsRel δ1 1 2 3 4 5 6 7 8 DirectAdapt DirectAdapt Improved Improved FE2E Flux Step1X-Edit Step1X-Edit Step1X-Edit Step1X-Edit Step1X-Edit Flux Step1X-Edit Direct Direct Direct Direct Inverse Logarithmic Logarithmic Logarithmic 9.7 9.5 8.8 8.6 6.9 6.8 7.1 6.6 91.2 91.4 93.2 94.0 95.1 95.6 94.9 96. 6.0 5.6 5.0 4.8 4.6 3.9 4.5 3.8 96.0 96.2 97.2 97.3 98.2 98.6 97.8 98."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, our systematic analysis shows that editors provide more stable and effective foundation than their generative counterparts. Based on this, we introduced FE2E, novel framework that successfully adapts pre-trained editing model for dense geometry prediction. To bridge the gap between these tasks, we proposed consistent velocity training objective for stable convergence and logarithmic quantization to resolve precision conflicts. We also designed cost-free joint estimation strategy, enabling mutual enhancement within single forward pass. FE2E achieves SoTA performance and validates the From Editor to Estimator paradigm, showcasing that harnessing the inherent ability of editing models is an effective and data-efficient approach for dense prediction."
        },
        {
            "title": "REFERENCES",
            "content": "Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. 18 Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael Black. naturalistic open source movie for optical flow evaluation. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VI 12, pp. 611625. Springer, 2012. 14 Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 18 Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: unified llama backbone for vision tasks. In European Conference on Computer Vision, pp. 118. Springer, 2024. 18 Xiangxiang Chu, Renda Li, and Yong Wang. Usp: Unified self-supervised pretraining for image generation and understanding. In ICCV, 2025. 18 Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. 14 Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1078610796, 2021. David Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with common multi-scale convolutional architecture. In Proceedings of the IEEE international conference on computer vision, pp. 26502658, 2015. 3 David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. Advances in neural information processing systems, 27, 2014. 3 Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. arXiv preprint arXiv:2403.12013, 2024. 3, 7 Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pp. 33543361. IEEE, 2012. 14 Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling, 2025. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. URL https: //arxiv.org/abs/1406.2661. 17 Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, and Ying-Cong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction. arXiv preprint arXiv:2409.18124, 2024. 3, 15 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 17 Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. 10 Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. 3 Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44014410, 2019. 17 Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 81108119, 2020. 17 Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. Advances in Neural Information Processing Systems, 34:852863, 2021. 17 Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 94929502, 2024. 2, 3, Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based single-image depth estimation methods. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pp. 00, 2018. 14 Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 3, 18 Rui Lan, Yancheng Bai, Xu Duan, Mingxing Li, Lei Sun, and Xiangxiang Chu. Flux-text: simple and advanced diffusion transformer baseline for scene text editing, 2025. 18 Changbai Li, Haodong Zhu, Hanlin Chen, Juan Zhang, Tongfei Chen, Shuo Yang, Shuwei Shao, Wenhao Dong, and Baochang Zhang. Hrgs: Hierarchical gaussian splatting for memory-efficient high-resolution 3d reconstruction, 2025. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 18 Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2022. 5, 16 Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 2, 3, 6, 7, 18 Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022. URL https://arxiv.org/abs/2209.03003. 17 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, and Bastian Leibe. Fine-tuning image-conditional diffusion models is easier than you think. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025. 3 Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations, 2022. URL https://arxiv.org/abs/2108.01073. 18 11 Shervin Minaee, Xiaodan Liang, and Shuicheng Yan. Modern augmented reality: Applications, trends, and future directions, 2022. 2 Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 Conference Proceedings, 2023. 18 William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. 18 Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks, 2016. URL https://arxiv.org/abs/ 1511.06434. 17 Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 88218831. PMLR, 2021. 18 Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. 3 Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1217912188, 2021. 3 Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1091210922, 2021. 7 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. 2, 3, 18 Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 18 Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with high-resolution images and In Proceedings of the IEEE conference on computer vision and pattern multi-camera videos. recognition, pp. 32603269, 2017. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 18 Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pp. 746760. Springer, 2012. 14 12 Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. 2025. 18 Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. Jiyuan Wang, Chunyu Lin, Lang Nie, Shujun Huang, Yao Zhao, Xing Pan, and Rui Ai. Weatherdepth: Curriculum contrastive learning for self-supervised depth estimation under adverse weather conditions. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 49764982. IEEE, 2024a. 3 Jiyuan Wang, Chunyu Lin, Lang Nie, Kang Liao, Shuwei Shao, and Yao Zhao. Digging into contrastive learning for robust depth estimation with diffusion models. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 41294137, 2024b. 3 Jiyuan Wang, Chunyu Lin, Cheng Guan, Lang Nie, Jing He, Haodong Li, Kang Liao, and Yao Zhao. Jasmine: Harnessing diffusion prior for self-supervised depth estimation, 2025. 3 Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. URL https://arxiv.org/abs/ 2508.02324. 2, 3, 18 Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv preprint arXiv:2403.06090, 2024. 3 Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 13161324, 2018. 17 Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1037110381, 2024a. 3 Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024b. Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. arXiv preprint arXiv:2406.16864, 2024. 3 Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 59075915, 2017. 17 Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan++: Realistic image synthesis with stacked generative adversarial networks. IEEE transactions on pattern analysis and machine intelligence, 41(8):19471962, 2018. 17 Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive learning for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 833842, 2021."
        },
        {
            "title": "APPENDIX",
            "content": "In this appendix, we provide more implementation details, experiments, analysis, and discussions for comprehensive evaluation and understanding of FE2E. Detailed contents are listed as follows: Experiment Settings A.1 Auxiliary Dispersion Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Evaluation Datasets A.3 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Details of Finetune Analysis B. B.2 Improved Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation Details of Generative-based Models . . . . . . . . . . . . . . . . . Quantization Error Calculation Details C.1 Uniform Quantization . C. Inverse Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Logarithmic Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Preliminaries of Flow Matching Reviews of Related Generative and Editing Models Addition Experiments Results Limitations and Future Work"
        },
        {
            "title": "A EXPERIMENT SETTINGS",
            "content": "A.1 AUXILIARY DISPERSION LOSS 14 14 14 15 15 15 15 16 16 16 17 18 18 Following Diffuse-and-Disperse (Wang & He, 2025), we apply this loss to the output of the 9th block: Table 5: Ablation study on Disperse Loss. The baseline is the ID4 model in main paper, Table 4. Ldisp = log Ei,j (cid:2)exp(ηi ηj2 2/τ )(cid:3) , (9) Method KITTI ETH3D AbsRel δ1 AbsRel δ1 where ηi,j are the output features for the i-th and j-th samples in batch, respectively, and temperature τ = 1. Finally, the training loss is defined as: Ltrain = Lf + λLdisp, λ = 0.5. The choices of λ, τ , and block all follow the optimal hyperparameters identified in the experiments from Diffuse-and-Disperse. Baseline (CV + FS) + Disperse Loss (DL) 94.0 94.4 97.3 97.6 4.8 4.5 8.6 8.4 For integrity, we also conducted ablation studies on this dispersed loss. The performance gains observed in Table 5 confirm that this loss is also effective for dense geometric estimation tasks. A.2 EVALUATION DATASETS We evaluate our model on two tasks: Zero-shot Affine-Invariant Depth Estimation. We evaluate on five standard benchmarks: NYUv2 (Silberman et al., 2012), ScanNet (Dai et al., 2017), KITTI (Geiger et al., 2012), ETH3D (Schops et al., 2017), and DIODE (Vasiljevic et al., 2019). Following standard practice, we report the Absolute Relative error (AbsRel) and δ1 accuracy. Surface Normal Prediction. We evaluate on NYUv2, ScanNet, iBims-1 (Koch et al., 2018), and Sintel (Butler et al., 2012) benchmarks. The evaluation metrics are the mean angular error (MeanErr) and the percentage of pixels with an angular error below 11.25. 14 A.3 EVALUATION METRICS For zero-shot depth estimation, similar to (Ke et al., 2024), we employ the following evaluation metrics: AbsRel: 1 Mvl (cid:80) dMvl dgt /dgt; a1: percentage of such that max( dgt , dgt ) < 1.25 ; where dgt and denote the GT and estimated pixel depth, Mvl is the valid mask (mask rules are consistent with (He et al., 2024)). For zero-shot normal estimation, we use the following evaluation metrics: MeanErr = 1 11.25: The percentage of where the angular error is less than 11.25; 180 π arccos(clamp(n ngt, 1, 1)) nMvl Mvl (cid:80) where ngt and denote the GT and estimated normal vector."
        },
        {
            "title": "B TRAINING DETAILS OF FINETUNE ANALYSIS",
            "content": "B.1 IMPROVED EXPERIMENT SETUP For clarity, we term the direct adaptation of the original editing/generative formulation as DirectAdapt (Sec 3.2), and Table 4 shows that DirectAdapt fails to achieve satisfactory performance. To address this, we introduce two key improvements on training objective (Sec 3.3) and GT quantization(Sec 3.4). They can benefit both editing and generative models, and these improved models are better for analyzed our core motivation (Sec 3.1), as they isolate the error from training data and the denoising process. We finally introduce joint training on the editing-based model to obtain FE2E. B.2 IMPLEMENTATION DETAILS OF GENERATIVE-BASED MODELS Step1X-Edit is fine-tuned from the generative model FLUX, and both share an almost identical DiT architecture. To further reduce confounding factors, we follow the Step1X-Edit protocol and replace the original FLUX input with horizontally concatenated noise and RGB image. All hyperparameters, including LoRA settings, optimizer, and training data, are kept exactly the same as those used for FE2E in-depth estimation. FLUX consists of 38 block layers, each producing outputs of consistent dimensions. After rearrangement, the feature map has the shape 192 H/8 W/8, where is the batch size, and are the height and width of the input image. Typically, the output from the final block is projected to 16 channels and passed to the VAE for reconstruction to 3 . For visualization, we chose 1, 20, and 35 blocks, operate on the 192 H/8 W/8 feature map, normalize it to 1 H/8 W/8 using the L2 norm, upsample it to 1 , and finally visualize it using the Rainbow colormap. The visualization of depth and normals follows the approach of Lotus. Since our experimental comparisons are conducted using the improved model, only one single denoising step is performed during inference. Consequently, the output from the VAE decoder directly represents the depth map (the 3 output mentioned before was averaged to obtain 1-channel depth map), which makes it easier to visualize meaningful features."
        },
        {
            "title": "C QUANTIZATION ERROR CALCULATION DETAILS",
            "content": "The following calculations are based on the effective depth range of 0-80m from the Virtual KITTI dataset. The normalization scheme consistently maps an input domain to the VAEs mandatory input range of [-1, 1] using the standard min-max scaling formula: = 2 XXmin 1. While other mapping schemes from [0m, 80m] to [-1, 1] may exist, they are not explored in this work. All calculations use the worst-case precision of BF16 over the [-1, 1] interval, which corresponds to single quantization step of 1/256. XmaxXmin 15 C.1 UNIFORM QUANTIZATION In this scheme, the depth value is linearly mapped to the [-1, 1] interval. The depth range is [Dmin, Dmax] = [0m, 80m]. The mapping function is = 2 D0 40 1. quantization step of = 1/256 in the normalized space corresponds to an error in the real-world depth space. This error is constant across the entire depth range: 1 256 = 40 = 40 800 1 = 0.15625 At 80m: Error 16cm. AbsRel = 0.16m At 0.1m: Error 16cm. AbsRel = 0.16m 80m = 0.002. 0.1m = 1.600. This method yields an unacceptably large relative error at close distances. C."
        },
        {
            "title": "INVERSE QUANTIZATION",
            "content": "This scheme quantizes the reciprocal of depth, i.e., disparity = 1/D. We consider an effective depth range of [0.1m, 80m] to avoid division by zero. The corresponding disparity range is [Pmin, Pmax] = [1/80, 1/0.1] = [0.0125, 10]. The disparity is linearly mapped to [-1, 1]. The quantization step in disparity, , is constant: = (Pmax Pmin) 2 = (10 0.0125) 1/256 2 0.0195. The relationship between depth error and disparity error is given by d(1/P ) 1 2 = D2P . At 80m: Error = (80m)2 0.0195 = 6400 0.0195 124.8m 125m. AbsRel = 125m dP = At 0.1m: Error = (0.1m)2 0.0195 = 0.01 0.0195 = 0.000195m 0.2mm. AbsRel = 0.0002m 0.002. As mentioned in the main text, the disparities for 39m and 78m are 1/39 0.0256 and 1/78 0.0128, respectively. Their difference is 0.0128, which is smaller than the disparity quantization step 0.0195, making them indistinguishable after quantization. This scheme fails completely at large distances. C.3 LOGARITHMIC QUANTIZATION This scheme quantizes the logarithmic depth, Dlog = ln(D). We again consider the depth range [0.1m, 80m]. The corresponding log-depth range is [ln(0.1), ln(80)] [2.30, 4.38]. The log-depth Dlog is linearly mapped to [-1, 1]. The quantization step in log-depth, Dlog, is constant: Dlog = (ln(80) ln(0.1)) = (4.38 (2.30)) 1/256 2 0.013. The relationship between depth error and log-depth error Dlog is given by d(eDlog ) Dlog = eDlog Dlog = Dlog. This implies that the absolute relative error, AbsRel dDlog = D/D, is approximately constant and equal to Dlog 0.013. At 80m: AbsRel 0.013. Error = 80m 0.013 = 1.04m. At 0.1m: AbsRel 0.013. Error = 0.1m 0.013 = 0.0013m = 1.3mm. This method maintains reasonable and nearly constant relative error across both near and far ranges, making it well-balanced and effective solution. The percentile-based normalization used in the main text is more robust implementation of this fundamental principle."
        },
        {
            "title": "D PRELIMINARIES OF FLOW MATCHING",
            "content": "Flow Matching (Lipman et al., 2022) is highly effective framework for training Continuous Normalizing Flows (CNFs). The core idea is to smoothly transform simple prior distribution p0 (e.g., 16 80m 1.563. 0.1m = the standard Gaussian distribution (0, I)) into complex target data distribution p1 over continuous time variable [0, 1]. This transformation process can be described by an Ordinary Differential Equation (ODE), where the velocity at any time and point is defined by vector field vt(z). However, estimating this marginal vector field vt(z) directly from data samples is challenging. The Flow Matching framework elegantly bypasses this issue by regressing much simpler and easier-to-compute conditional vector field ut(zz0, z1) instead. Specifically, we first sample pair of points, (z0, z1), from the prior distribution p0 and the target distribution p1, respectively. We then define simple path zt from z0 to z1 and its corresponding conditional vector field ut = dzt dt . It has been proven that if neural network fθ(z, t) is trained to approximate this simple conditional vector field ut, then in expectation over all sample pairs (z0, z1) and time t, the network fθ will converge to the complex marginal vector field vt that we truly wish to learn. Rectified Flow (Liu et al., 2022) presents particularly simple and powerful instance of Flow Matching. It defines the path between z0 and z1 as straight line: zt = tz1 + (1 t)z0, [0, 1]. The derivative of this path is trivial, yielding constant velocity vector that is independent of both time and space: = = z1 z0. dzt dt Consequently, the training objective (loss function) becomes exceedingly simple: aligning the neural networks prediction with this constant velocity vector v: = Et,z1,z0(z1 z0) fθ(zt, t)2. Application in DirectAdapt In this paper, we adapt this framework for conditional image editing task. Our goal is not to learn an unconditional generative model, but rather flow from noise zy 0 to the target geometry latent zy 1, guided by the input image (encoded as zx). Therefore, our velocity prediction model fθ must take zx as an additional condition. As shown in Eq. 2 in the main text, our loss function is: (zy During inference, we generate the target latent ˆzy the guiding condition: = Et,zy 1 ,zy 0 0) fθ(t, zx)2. 1 zy 1 by solving the following ODE, with zx serving as dˆzy dt = fθ(t, zx), with initial value ˆzy 0 (0, I). By integrating from = 0 to = 1 using numerical ODE solver (e.g., Euler method), we can obtain the final prediction ˆzy 1."
        },
        {
            "title": "E REVIEWS OF RELATED GENERATIVE AND EDITING MODELS",
            "content": "The fields of image generation and image editing have always been complementary, and they have undergone several paradigm shifts. The first major breakthrough was the Generative Adversarial Network (GAN) (Goodfellow et al., 2014), which introduced novel adversarial training process. Then, key advancements in this era include architectural refinements like DCGAN (Radford et al., 2016), the development of conditional and text-to-image GANs such as the StackGAN series (Zhang et al., 2017; 2018), AttnGAN (Xu et al., 2018), and Cross-Modal Contrastive Learning based models (Zhang et al., 2021). The StyleGAN series (Karras et al., 2019; 2020; 2021) marked high point for GANs, achieving unprecedented photorealistic high-resolution image synthesis and offering fine-grained control over visual attributes through disentangled latent space, which became cornerstone for many subsequent editing techniques. More recently, the field has transitioned to Denoising Diffusion Models (Ho et al., 2020), which have become the state-of-the-art for their superior image quality and textual coherence. series of influential diffusion-based methods were introduced, including GLIDE (Nichol et al., 2021), 17 DALLE (Ramesh et al., 2021) and its successor DALLE 2 (Ramesh et al., 2022), Imagen (Saharia et al., 2022), and PIXART-α (Chen et al., 2023). The open-source Stable Diffusion (SD) (Rombach et al., 2022) model, trained on the large-scale LAION-5B dataset (Schuhmann et al., 2022), further democratized high-quality image generation and quickly became community standard. growing body of evidence suggests that Diffusion Transformers (Peebles & Xie, 2023; Chu et al., 2024; 2025; Labs, 2024) outperform U-Nets, motivating the shift toward training modern diffusion models with Transformer architectures. Building on these powerful generative foundations, the domain of image editing (Lan et al., 2025) (generalized editing) has also advanced rapidly. Early diffusion-based methods like SDEdit (Meng et al., 2022) demonstrated that real images could be edited by adding noise and then denoising with new text prompt. significant leap was made with instruction-guided editing, pioneered by InstructPix2Pix (Brooks et al., 2022), which enabled edits based on natural language commands. The field has since diversified with numerous innovative approaches. For instance, DragGAN (Pan et al., 2023) introduced novel point-based interaction, allowing users to drag pixels to precisely deform object shapes. OmniControl (Tan et al., 2025) further enhances controllability by creating unified framework that accepts diverse spatial guidance signals for both synthesis and editing. This trend towards more powerful and versatile models is also reflected in large-scale systems like UniWorld (Lin et al., 2025), which uses unified transformer for multi-modal understanding and generation, Step1X-Edit (Liu et al., 2025), fine-tuned from the FLUX architecture for superior instruction following, and multi-modal editors like Qwen-Image (Wu et al., 2025), which leverage Large Language Models (LLMs) to build more comprehensive visual editing frameworks."
        },
        {
            "title": "F ADDITION EXPERIMENTS RESULTS",
            "content": "Table 6: Quantitative comparison on zero-shot affine-invariant depth estimation between FE2E and the concurrent unified model. Method NYUv2 (Indoor) KITTI (Outdoor) ETH3D (Various) ScanNet (Indoor) DIODE (Various) Avg AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 Rank Qwen-Image DINOv3 FE2E 5.5 4.3 4.1 96.7 98.0 97. 7.8 7.3 6.6 95.1 96.7 96.0 6.6 5.4 3.8 96.2 97.5 98.7 4.7 4.4 4.4 97.4 98.1 97. 19.7 25.6 22.8 83.2 82.2 81.2 2.6 1.8 1.6 Comparison with Concurrent Unified Model The field of dense geometry estimation is advancing rapidly, with the task of depth estimation particularly fast. Recently, several concurrent works have been explored to unify the visual tasks, which also include depth estimation benchmarks. As shown in Table 6, our method consistently achieves the top average ranking, even though they are trained with extremely huge data compared to FE2E (e.g., Qwen Image utilizes billions of samples, and DINO v3 is trained on 1.7 billion images). Additional Qualitative Comparison Fig. 9 presents qualitative comparison between FE2E and other methods. The results demonstrate that our approach produces more refined and accurate depth predictions, particularly in structurally complex regions that may not be fully captured by quantitative metrics. Furthermore, as illustrated in Fig. 10, FE2E consistently delivers precise surface normal predictions, effectively handling intricate geometries and diverse environments. These results highlight the robustness of our method in fine-grained prediction tasks."
        },
        {
            "title": "G LIMITATIONS AND FUTURE WORK",
            "content": "Large computational load We present the inference latency and computational complexity of the FE2E model in Table 7, alongside comparisons with previous SDbased and unified methods. Although incorporating DiT does lead to notable inTable 7: Performance comparison of different models. Methods Marigold Lotus-D Qwen Image DINO v3 FE2E 28.9T MACs 1.78s RunTime 3.8 AbsRel 14.5T 632ms 5.4 2.65T 212ms 6.1 2.13P 63.4s 6. 133T 9.67s 6.5 18 Figure 9: Additional qualitative comparison on zero-shot affine-invariant depth estimation. FE2E achieves more accurate depth predictions, particularly in structurally complex regions. White arrows highlight these improvements. crease in computational complexity relative to other self-supervised approaches, FE2E strikes trade-off between performance and computational efficiency. Diversifying foundation models The field of image editing is evolving rapidly, and our approach is designed to be model-agnostic. In future work, we plan to incorporate broader range of editing models to further substantiate the motivation and conclusions presented in this paper. Scaling up the training data While key contribution of this work is demonstrating strong generalization performance with limited amount of training data, we still anticipate that scaling up the training dataset could further improve the models capabilities. This direction is meaningful for domains that are not sensitive to computational complexity but require extremely high prediction accuracy. We leave the exploration for future research. 19 Figure 10: Additional qualitative comparison on zero-shot surface normal estimation. FE2E offers improved accuracy, particularly in detailed and complex regions."
        }
    ],
    "affiliations": [
        "AMAP Alibaba Group",
        "BJTU",
        "CQUPT",
        "NTU"
    ]
}