{
    "paper_title": "QwenLong-CPRS: Towards $\\infty$-LLMs with Dynamic Context Optimization",
    "authors": [
        "Weizhou Shen",
        "Chenliang Li",
        "Fanqi Wan",
        "Shengyi Liao",
        "Shaopeng Lai",
        "Bo Zhang",
        "Yingcheng Shi",
        "Yuning Wu",
        "Gang Fu",
        "Zhansheng Li",
        "Bin Yang",
        "Ji Zhang",
        "Fei Huang",
        "Jingren Zhou",
        "Ming Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This technical report presents QwenLong-CPRS, a context compression framework designed for explicit long-context optimization, addressing prohibitive computation overhead during the prefill stage and the \"lost in the middle\" performance degradation of large language models (LLMs) during long sequence processing. Implemented through a novel dynamic context optimization mechanism, QwenLong-CPRS enables multi-granularity context compression guided by natural language instructions, achieving both efficiency gains and improved performance. Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key innovations: (1) Natural language-guided dynamic optimization, (2) Bidirectional reasoning layers for enhanced boundary awareness, (3) Token critic mechanisms with language modeling heads, and (4) Window-parallel inference. Comprehensive evaluations across five benchmarks (4K-2M word contexts) demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority over other context management methods like RAG and sparse attention in both accuracy and efficiency. (2) Architecture-agnostic integration with all flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3, and Qwen2.5-max, achieves 21.59$\\times$ context compression alongside 19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct, QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on Ruler-128K and InfiniteBench, establishing new SOTA performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 2 9 0 8 1 . 5 0 5 2 : r QWENLONG-CPRS: Towards -LLMs with Dynamic Context Optimization Weizhou Shen, Chenliang Li, Fanqi Wan, Shengyi Liao, Shaopeng Lai, Bo Zhang, Yingcheng Shi, Yuning Wu, Gang Fu, Zhansheng Li, Bin Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan Qwen-Doc Team, Alibaba Group https://github.com/Tongyi-Zhiwen/QwenLong-CPRS https://huggingface.co/Tongyi-Zhiwen/QwenLong-CPRS-7B https://modelscope.cn/models/iic/QwenLong-CPRS-7B"
        },
        {
            "title": "Abstract",
            "content": "This technical report presents QWENLONG-CPRS, context compression framework designed for explicit long-context optimization, addressing prohibitive computation overhead during the prefill stage and the lost in the middle performance degradation of large language models (LLMs) during long sequence processing. Implemented through novel dynamic context optimization mechanism, QWENLONGCPRS enables multi-granularity context compression guided by natural language instructions, achieving both efficiency gains and improved performance. Evolved from the Qwen architecture series, QWENLONG-CPRS introduces four key innovations: (1) Natural language-guided dynamic optimization, (2) Bidirectional reasoning layers for enhanced boundary awareness, (3) Token critic mechanisms with language modeling heads, and (4) Window-parallel inference. Comprehensive evaluations across five benchmarks (4K-2M word contexts) demonstrate QWENLONG-CPRSs threefold effectiveness: (1) Consistent superiority over other context management methods like RAG and sparse attention in both accuracy and efficiency. (2) Architecture-agnostic integration with all flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3, and Qwen2.5max, achieves 21.59 context compression alongside 19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct, QWENLONG-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on Ruler-128K and InfiniteBench, establishing new SOTA performance."
        },
        {
            "title": "Introduction",
            "content": "Enhancing the long-context processing capabilities of large language models (LLMs) has emerged as critical research frontier in both academia and industry [31, 39, 34, 1]. Recent advancements in positional embedding techniques [38, 33] and the curation of synthetic long-context training data [3] have enabled significant progress in extending LLMs context windows, achieving expansions from 4K to over 1M tokens [46, 10]. Despite these advances, two critical challenges persist. First, the quadratic computational complexity of processing long sequences imposes prohibitive efficiency costs. Second, the unresolved lost in the middle phenomenon [25], where LLMs struggle to effectively prioritize critical information within lengthy inputs. fundamental strategy involves efficiently managing long context by focusing on key content within the models reliable context window [26]. Building upon this strategy, two primary approaches have Corresponding author Preprint. Work in progress. (a) The input compression rate and performance gain when different LLMs are cascaded with QWENLONG-CPRS. (b) Performance of Qwen2.5-32b-instruct with different context management methods. Figure 1: Illustration of the performance of QWENLONG-CPRS. Figure 1a compares the input token consumption and model performance of various LLMs on Ruler-128K before (marked with (cid:51)) and after (marked with ) cascading QWENLONG-CPRS. Figure 1b highlights the performance improvements of QWENLONG-CPRS over other context management methods, such as RAG [22] and sparse attention [16]. been proposed: Retrieval-augmented generation (RAG) frameworks [21, 7] enhance computational efficiency by dynamically retrieving query-relevant text chunks from input contexts, enabling selective processing of contextual information. Conversely, sparse attention (SA) mechanisms [16, 27, 49] redesign the self-attention mechanism within LLMs, either by restricting attention computations to structured patterns or by prioritizing critical token interactions during sequential generation. Despite their advantages, both approaches exhibit significant limitations. First, RAG systems, while efficient, rely on coarse-grained chunk-level embeddings, leading to imprecise outputs. This limitation becomes particularly problematic in scenarios requiring fine-grained localization of uniformly distributed knowledge [23]. On the other hand, SA methods, though flexible in token-level aggregation, necessitate substential data construction and computationally intensive model training to optimize attention patterns [27, 49], alongside specialized infrastructure investments. To address these challenges, we introduce an innovative dynamic context optimization paradigm, which aims to improve context-processing efficiency through maximizing information density. As depicted in Figure 2, this approach dynamically compresses input contexts into query-tailored segments across different granularities, enabling concise and accurate context optimization for various user queries. This paradigm advances existing methods in two key aspects. First, it replaces RAGs coarse chunk-level retrieval with precise token-level content selection, enhancing information identification accuracy. Second, it operates independently as plug-and-play component, eliminating SAs requirement for model retraining while maintaining compatibility with any downstream LLMs. Building upon the dynamic context optimization paradigm, we propose novel compression system QWENLONG-CPRS. Specifically, QWENLONG-CPRS takes the control prompt, task query, and long context as input, and then labels the token critic score to compress task-relevant content based on single forward pass. To endow QWENLONG-CPRS with both precise and controllable characteristics, we redesign the attention mechanism into hybrid architecture that combines bi-directional language modeling for comprehensive context location and causal language modeling for reliable language representation. Additionally, we develop language modeling as token critic framework that repurposes the existing LLMs language modeling head to label token-level importance scores, thus maintaining the pretrained knowledge for better context compression As illustrated in Figure 1a, QWENLONG-CPRS demonstrates remarkable context compression effect, achieving context compression rate ranging from 72.6 to 290.5 times. This indicates that QWENLONG-CPRS possesses efficient context optimization capabilities and can be seamlessly reused across various large models. More experimental results in Section 4 across four long-context benchmarks whose input context lengths ranging from 4K to 2M tokens demonstrate QWENLONGCPRSs superiority over direct prompting, RAG, and SA methods, with remarkable performance improvements and considerably higher inference efficiency. Notably, we show that smaller, short2 Figure 2: The concept of dynamic context optimization, which aims to enhance context processing efficiency by maximizing information density. Given long-context input, this paradigm dynamically compresses it into query-specific content at varying granularities, facilitating concise and accurate information extraction for different user queries. For instance, keywords for search queries, sentences for question answering, and paragraphs for summarization. context LLMs augmented with QWENLONG-CPRS can outperform larger long-context counterparts. These findings highlight the potential of context optimization paradigms, offering scalable and efficient pathway to augment LLMs long-context processing capabilities. Our key contributions are as follows: We introduce dynamic context optimization, novel paradigm for long-context management through dynamic, instruction-guided token-level compression. This paradigm optimizes information retention while adaptively prioritizing critical content. We propose QWENLONG-CPRS, an innovative model that advances long-context processing via token-level critical scoring, enabling granular context optimization without sacrificing precision. We transformed the Qwen series models into an dynamic context optimization model by integrating hybrid attention mechanism and leveraging the language modeling head token critic module. Furthermore, we systematically designed the framework for training data construction. Through extensive evaluation across four distinct benchmarks, we demonstrate that QWENLONGCPRS achieves substantial performance gains while reducing inference overhead. The framework shows consistent efficacy across LLMs of varying parameter scales and context lengths, establishing its versatility as context-optimization-augmentation solution for long-context processing."
        },
        {
            "title": "2 QWENLONG-CPRS",
            "content": "In this section, we initially present the formal definition of dynamic context optimization in Section 2.1. Subsequently, Section 2.2 details the model architecture of QWENLONG-CPRS and elaborates on the training methodology employed to align QWENLONG-CPRS with the objectives of dynamic context optimization. Our effective window-parallelism inference method is also introduced in Section 2.2. Lastly, we describe the data construction method we have devised in Section 2.3. 2.1 Dynamic Context Optimization The input structure for long-context tasks comprises two components: the user query q, and the long context Xl. When Xl exceeds the effective window size of LLMs, it typically causes either essential input truncation or the \"lost-in-the-middle\" phenomenon, ultimately degrading response quality. To address this, we propose identifying an information-condensed subset Xs such that: Xs Xl, where Xs Xl (1) This process, termed context optimization, aims to find the minimal-length Xs that preserves maximally informative content for generating high-quality responses . Formally, we define our 3 (a) The model architecture of QWENLONGCPRS. (b) The workflow of generative LLMs cascading QWENLONGCPRS in this paper. Figure 3: The model architecture and workflow of QWENLONG-CPRS. objective function as: = max ϕ EXsXl (cid:20) I(Y ; [Xs, q]) Xsβ (cid:21) , (2) where I(, ) is the mutual information, β controls the length penalty intensity, and ϕ parameterizes In this paper, we propose QWENLONG-CPRS, which achieves context the context optimizer. optimization by identifying and retaining the most semantically crucial tokens from Xl. In addition, we introduce natural language prompt that enables users to dynamically configure the granularity of the optimized context and how it will contribute the the response. Therefore, the resulting dynamically optimized context Xs is formalized as: Xs = Fϕ(P, q, Xl), (3) where F() is the token selection operation. 2.2 Model Architecture The model architecture of QWENLONG-CPRS is illustrated in Figure 3a. To ensure model generalization and achieve better awareness of the uses controlling prompt and query, we adopt the parameters and vocabulary of the Qwen-2-Base [45] series models2 as the starting checkpoint for initializing our QWENLONG-CPRS. Beside the parameter initialization and vocabulary inheritance, there are several key modifications for QWENLONG-CPRS to achieve better context optimization performance: In this paradigm, We define the input Dynamic Control via Natural Language: structure for QWENLONG-CPRS as concatenation of three text components: {system prompt, user formatted according to the Qwen message organization temquery, long context}, the system prompt specifies the desired properties of the opplate. timized context, instance: You are an expert in information extraction; your task is to extract sentences from the documents that support the users question. The user query contains the original instruction provided by the user, and the long context component is the source document requiring optimization. Through this paradigm design, QWENLONG-CPRS can adaptively extract informative context according to the requirements of the system prompt and user query. such as its granularity and its relationship to the query. For Bidirectional Location Reasoning: QWENLONG-CPRS implements context optimization through token scoring paradigm, preserving original token positions for prediction outputs. To improve 2Our experimental results demonstrate that the Qwen-2-Base series models outperform the Qwen-2.5-Base series models 4 boundary detection accuracy in optimized contexts, we introduce bidirectional location reasoning mechanism that enables global awareness of long-range contextual dependencies. As shown in Figure 3a, we retain the causal masking in lower Transformer layers to maintain the base models inherent language modeling capabilities. On the other hand, the upper layers employ bi-directional attention, allowing token-level decisions to incorporate both forward and backward contextual signals. This modification ensures the stable retention of linguistic knowledge and enhanced boundary detection via bi-directional reasoning. Language Modeling as Token Critic: Unlike conventional methods that predict token importance through scalar scores [15, 32] or restricted tagging schemes [24, 12], QWENLONG-CPRS unifies semantic categorization and positional reasoning within single framework. As illustrated in Figure 3a, we repurpose the base models language modeling head to predict token-level semantic categories from the LLM vocabulary V, while simultaneously employing secondary head to generate sequence labeling scores for boundary detection. The resulting search space constitutes the Cartesian product of the vocabulary the positional tag set. This combinatorial formulation expands the decision space by considerable scale, enabling dynamic adaptation to diverse optimization criteria through prompt conditioning while preserving the base models linguistic knowledge via parameter inheritance. With this language modeling as token critic setting, we maximize the log-probability of tokens label during the supervised fine-tuning. Window-Parallel Inference: We propose window-parallel inference strategy for efficient context optimization, as depicted in Figure 3b. Given window size w, the long context Xl is partitioned into Xl/w non-overlapping windows. Each window is concatenated with the system prompt and query to form independent inputs for parallel processing via QWENLONG-CPRS. Let ρ denote the parallelism factor, the computational complexity of the LLMs pre-filling stage reduces to: O( Xl ρw w2) + O(Xs2) = O( ρ Xl) + O(Xs2) (4) In the above equation, O( Xl ρw w2) represents the window-parallel inference overhead of QWENLONG-CPRS, while O(Xs2) denotes the prefill computation cost for the generative LLM processing optimized context Xs. Given constant parameters and ρ, and Xs Xl in practice, this formulation achieves strictly lower complexity than the O(Xl2) baseline of direct prompting. The windowing strategy further enables theoretically infinite long context optimization - the length Xl can scale with arbitrarily large while maintaining fixed memory overhead per parallel computation unit. 2.3 Training Data Construction The supervised fine-tuning process of QWENLONG-CPRS is conducted based on the token critic task described in Section 2.2. To enhance contextual reasoning capabilities, we construct two specialized training datasets: (1) Multi-granularity context optimization data for fundamental context grounding skills, and (2) Query-aware context optimization data that specifically improves querycontext interaction understanding. This dual training strategy enables simultaneous development of general contextual awareness and targeted query-response alignment capabilities. Multi-granularity context optimization: In the multi-granularity context optimization training data, we focus on how QWENLONG-CPRS can adaptively and controllably compress the long context into different granularities of optimized according to the system prompt: Keyword granularity: We collected training data from open source datasets such as named entity recognition (NER) [42, 6, 20] and machine reading comprehension (MRC) with phrase-level answer [13, 36, 18]. In addition to these open datasets, we also crawled publicly accessible documents, including arXiv papers, financial reports, contracts, bidding announcements, and court judgments. We hired annotators to highlight and annotate keywords and entities within these documents and trained the model to extract these keywords. Sentence granularity: We modified the word granularity compression training data to sentence granularity compression, such as altering extract the keyword in the context to extract sentences mentioning the keyword in the context. Additionally, we constructed set of training data inspired by the Needle in Haystack task [17], enabling the model to extract needle sentences from long documents. Furthermore, in sentence-level compression, we also aimed for 5 QWENLONG-CPRS to learn how to compress the long context into information-rich sentences. We assume that the summary of context represents the richest information in that context to some extent. Therefore, we collected open-source summarization datasets [37] and our own annotated summarization datasets, leveraging the greedy search algorithm from SummaRuNNer [29] to construct sentence-level extractive summarization training data. Paragraph granularity: In paragraph-granularity compression training, we intended for QWENLONG-CPRS to learn the ability to compress entire paragraphs. Using the document parsing toolkit DocMind 3, we segmented the crawled documents into paragraphs, tables, images, and other blocks. We employed annotators to label the meaning of randomly sampled paragraphs and tables, and trained the model to extract paragraphs or tables that correspond to these meanings from the context. Query-aware context optimization: This training set focuses on enabling QWENLONG-CPRS to perform context optimization aligned with user queries, thereby enhancing practical application performance. We leverage existing long-context QA datasets containing annotated supporting facts for reference answers [36, 47, 43], directly incorporating these into QWENLONG-CPRSs training to develop supporting fact extraction capabilities. To augment data diversity, we implement two complementary context compression synthesis approaches: Forward synthesis initiates with context segmentation into 256-token fragments, preserving semantic integrity through boundary sentence pruning. From these fragments, (1 3) are randomly selected to generate query-answer pairs via self-instruction prompting [44], with the LLM simultaneously producing supporting facts from the selected fragments. This process creates training instances for query-driven fact extraction from extended contexts. Backward synthesis operates on pre-annotated query-answer pairs, applying identical context segmentation followed by map-reduce processing. The LLM evaluates each fragments relevance to answering the target query, enabling construction of training data for context segment retrieval aligned with specific information needs. Both synthesis pipelines incorporate answer consistency verification: generated supporting facts are fed back to the LLM for answer reproduction, with subsequent comparison against original annotations to filter low-quality outputs. Through aformentioned methodologies, we curate multi-domain, multilingual context compression corpus containing 126K samples (1.2B tokens) spanning various granularities and task types."
        },
        {
            "title": "3 Experimental Setup",
            "content": "3.1 Implementation Details QWENLONG-CPRS was initialized using the Qwen2-7b-Base architecture [45], inheriting its parameters and vocabulary. The first 21 Transformer layers were retained as causal attention modules, while layers 2228 were reconfigured as bi-directional location reasoning layers following the design in Section 2.2. 3-epoch supervised fine-tuning regimen was implemented for the token critic task, employing the following configurations: window-parallel inference with 8192-token context windows, global batch size of 256, and constant learning rate of 1e-5. Training stability was ensured through Zero-3 partitioning with optimizer state offloading [35]. To address input token optimization sparsity in long-context processing, we applied random gradient masking to 50% of non-critical token positions during backpropagation. 3.2 Benchmarks Our evaluation protocol employs five long-context benchmarks with input lengths exceeding standard capabilities: Ruler-128K [14]: Generates synthetic long-context data through controlled noise injection (essays, sentence repetitions, UUID strings) into conventional text. We assess performance on three representative subsets: Needle-in-a-Haystack (NIAH), Variable Tracking (VT), and Question Answering (QA). 3https://www.aliyun.com/product/ai/docmind 6 Table 1: Evaluation results on Ruler-128K and InfiniteBench Model Qwen2.5-72b-instruct [34] Qwen2.5-max [34] Qwen2.5-turbo-1M [46] Qwen3-32b Qwen3-235b-a22b gpt-4o [30] claude-3.7-sonnet [2] deepseek-v3 [11] gemini-2.0-pro [9] Qwen-Long LLaMA3.1-8b-instruct [1] + RAG [22] + MOBA [27] + MInference [16] + QWENLONG-CPRS Qwen2.5-7b-instruct [34] + RAG [22] + MInference [16] + QWENLONG-CPRS Qwen2.5-32b-instruct [34] + RAG [22] + MInference [16] + QWENLONG-CPRS NIAH-sub NIAH Ruler-128k VT QA Avg QA.EN QA.ZH MC.EN RT.Passkey RT.NUM RT.KV Avg InfiniteBench 47.92 10.00 41.67 48.26 48.60 47.92 49.17 16.25 58.47 86.61 47.22 78.40 75.58 47.80 99.65 21.33 82.65 37.87 99.87 50.70 86.41 50.62 99.93 75.00 19.84 75.00 60.54 61.57 71.25 70.94 30.47 71.63 94.92 65.69 91.20 - 73.66 99. 59.98 93.30 69.34 99.85 75.76 93.38 76.14 99.95 91.60 23.60 90.00 70.08 70.04 77.60 76.00 34.40 75.48 99.84 28.63 93.32 - 29.08 100.00 24.48 98.92 22.92 98.68 93.56 97.92 95.84 99. 66.00 38.00 65.31 40.37 39.55 76.00 70.00 30.00 72.20 68.70 59.80 71.80 - 46.10 73.53 45.90 65.70 45.70 72.20 50.61 65.30 51.10 78.20 77.53 27.15 76.77 57.00 57.05 74.95 72.31 31.26 73.10 87.82 51.37 85.44 - 49.16 91. 43.45 85.97 45.99 90.24 73.31 85.53 74.36 92.67 27.36 18.11 17.46 15.83 9.62 31.72 35.95 20.69 38.06 17.46 26.74 23.22 - 27.60 26.20 16.69 15.56 16.80 26.43 20.88 14.97 21.65 26. 31.29 18.87 25.86 26.86 17.73 25.24 17.02 26.76 32.03 25.80 35.18 35.78 - 30.28 35.82 21.34 20.26 21.12 28.82 18.17 20.44 18.0 26.62 76.42 69.87 65.94 75.55 77.29 86.46 74.67 74.67 90.39 71.56 26.20 34.70 - 44.54 64. 59.82 79.04 62.54 79.91 79.04 76.86 80.79 90.39 79.66 22.03 100.00 71.19 71.19 79.66 79.62 41.19 79.66 100.00 94.58 96.70 - 87.80 95.93 94.91 100.00 94.92 100.00 91.53 99.66 94.92 100. 79.66 22.33 99.83 71.19 71.19 79.66 80.00 41.86 79.66 100.00 91.86 93.60 - 84.24 96.78 94.91 99.66 94.92 100.00 85.08 100.00 94.92 100.00 32.00 16.40 38.00 32.00 33.00 58.00 59.20 21.60 57.80 36.20 64.00 68.18 - 22.60 99. 14.60 73.60 13.20 98.80 35.20 73.00 36.4 99.20 54.40 27.93 57.85 48.77 46.67 60.12 57.74 37.18 62.93 58.50 56.43 58.70 - 49.51 69.73 50.38 64.69 50.38 72.33 54.98 64.16 57.78 73. For cross-system comparability with closed-source approaches [27], we also select the multi-key level 2, multi-key level 3 and multi-value tasks from Ruler-NIAH to derive the NIAH-sub benchmark. InfiniteBench [50]: Features multilingual (English/Chinese) and multi-task evaluations across six categories: QA (EN/ZH), multiple choice (MC.EN) and specialized retrieval tasks (Passkey, Numeric, Key-Value). Context lengths span 122K2M words, testing extreme-scale processing capabilities. Longbench V1 [4]: Comprehensive benchmark with 13K-word average context length. We evaluated the compared methods through established protocols [48, 49] across three dimensions: singledocument QA, multi-document QA, and summarization. Longbench V2 [5]: Advanced successor featuring complex reasoning tasks with 8K-word to 2Mword contexts, requiring deep linguistic understanding and logical inference. Needle-in-a-Haystack [17]: Precision evaluation for target statement retrieval from irrelevant contexts. We test the performance of QWENLONG-CPRS with the context lengths range from 32K to 1M tokens, and the inserted depths range from 0% to 100%. 3.3 Compared Baselines Our comparative analysis includes state-of-the-art LLMs and specialized long-context management approaches across three categories: Proprietary LLMs: Qwen2.5-Max [34], Qwen2.5-Turbo-1M [46], GPT-4o [30], Claude-3.7Sonnet [2], Gemini-2.0-Pro [9], and Qwen-Long4. Open-Source LLMs: Qwen2.5-7b/32b/72b-instruct [34], DeepSeek-V3 [11], LLaMA3.1-8binstruct [1], Qwen3-32b/235b-a22b [41]. RAG: The long context is partitioned into 600-token chunks with GTE embeddings [22] for top-k retrieval, accumulating results until reaching 16K tokens. Sparse Attention: The open-source implementation: Minference [16]. And the experimental results reported at the original papers of the established methods: InfiniteRetrieval [48], MOBA [27], and NSA [49]."
        },
        {
            "title": "4 Experimental Results",
            "content": "4.1 Overall Results The evaluation results across Tables 1, 2, and 3 demonstrate QWENLONG-CPRSs effectiveness against comparative baselines, revealing several key findings: 4https://help.aliyun.com/zh/model-studio/long-context-qwen-long 7 Table 2: Evaluation results on Longbench V1 Model Qwen2.5-72b-instruct [34] Qwen2.5-max [34] Qwen2.5-turbo-1M [46] Qwen3-32b Qwen3-235b-a22b gpt-4o [30] claude-3.7-sonnet [2] deepseek-v3 [11] gemini-2.0-pro [9] Qwen-Long LLaMA3.1-8b-instruct [1] + RAG [22] + MInference [16] + InfiniteRetrieval [48] + QWENLONG-CPRS Qwen2.5-7b-instruct [34] + RAG [22] + MInference [16] + InfiniteRetrieval [48] + QWENLONG-CPRS Qwen2.5-32b-instruct [34] + RAG [22] + MInference [16] + QWENLONG-CPRS SingleDoc QA MultiDoc QA MF (en) MF (zh) NQ Qasp HPQA 2Wiki. Musi. Du. Avg_qa 54.26 51.40 54.41 46.71 45.60 53.99 53.45 56.69 55.77 53.62 49.39 32.09 55.21 44.72 54.37 52.35 33.93 51.86 50.92 53.42 52.68 34.17 50.98 54. 66.20 64.75 60.66 62.90 61.45 62.56 63.82 66.72 65.47 64.49 60.64 25.91 60.69 - 60.87 62.94 26.97 62.95 - 62.48 66.22 32.17 66.14 65.82 33.78 27.80 24.59 27.79 30.16 35.13 37.57 34.80 33.10 30.92 26.81 12.83 28.61 18.88 28. 25.40 12.15 28.25 25.48 24.97 31.91 13.21 27.90 30.56 47.56 43.47 42.73 40.73 42.06 46.97 48.80 47.59 51.39 43.98 37.79 18.36 47.57 36.45 42.92 45.40 21.97 45.87 42.12 44.16 47.99 22.79 46.46 45. 65.88 65.77 63.03 66.56 68.40 68.79 66.15 69.53 70.69 64.80 53.07 30.91 53.60 50.10 59.03 56.92 33.15 56.29 57.52 63.95 64.81 35.86 62.37 67.44 64.46 62.51 52.72 74.00 75.42 68.51 67.22 69.65 80.43 62.53 40.42 21.35 40.43 29.98 52. 45.02 23.72 43.03 50.26 56.16 59.81 25.30 59.22 66.28 42.00 40.16 39.21 52.57 56.20 46.20 48.37 57.62 68.18 41.06 29.79 10.64 29.08 27.26 34.58 31.16 11.14 31.16 30.62 41.01 40.60 13.19 39.13 47. 19.29 20.09 23.88 18.48 17.59 17.71 19.53 21.23 12.10 21.83 21.06 15.21 27.76 - 23.93 28.57 20.64 23.15 - 19.04 22.00 21.13 24.90 19.81 45.89 46.99 45.15 48.72 49.61 49.98 50.16 52.98 54.64 47.90 39.87 20.91 42.87 - 44. 43.47 22.96 42.82 - 45.65 48.25 24.73 47.17 49.74 GR 19.56 16.90 17.33 27.73 27.35 15.82 18.08 17.85 16.41 33.04 19.09 12.09 19.88 21.94 20.02 32.64 20.48 18.57 19.26 17. 17.09 27.83 18.46 16.37 Summary QM MN 18.69 18.64 18.39 19.93 20.00 17.11 16.73 17.85 17.41 24.06 17.99 15.50 18.19 20.17 18. 23.35 20.15 18.64 20.47 17.63 17.84 20.12 17.62 18.01 13.52 13.57 14.32 21.69 21.45 14.40 15.42 15.06 13.35 22.44 14.80 13.77 15.40 24.14 15.06 23.20 21.41 14.50 20.60 15.30 14.88 21.29 14.32 14. VC 16.30 16.52 19.12 13.99 13.50 14.95 16.61 17.81 10.64 13.71 9.63 6.94 17.37 - 16.84 13.69 9.81 17.89 - 18.02 18.80 10.15 17.36 19.23 Avg 38.46 36.80 35.87 39.42 39.93 38.51 39.61 41.03 41.25 39.71 31.71 17.97 34.48 - 35.58 36.72 21.29 34.35 - 36.12 37.89 23.10 37.07 38.85 Table 3: Evaluation results on Longbench V2 Model Qwen2.5-72b-instruct [34] Qwen2.5-max [34] Qwen2.5-turbo-1M [46] Qwen3-32b [41] Qwen3-235b-a22b [41] gpt-4o [30] claude-3.7-sonnet [2] deepseek-v3 [11] gemini-2.0-pro [9] Qwen-Long Qwen2.5-7b-instruct [34] + RAG [22] + MInference [16] + QWENLONG-CPRS Qwen2.5-32b-instruct [34] + RAG [22] + MInference [16] + QWENLONG-CPRS Overall Difficulty Easy Hard Length (< 32K; 32K-128K; > 128K) Short Medium Long 42.1 46.5 40.8 48.7 51.9 46.0 50.9 45.3 60.6 43.0 27.4 31.4 27.8 33.1 41.7 32.2 35.8 42. 42.7 51.6 45.3 57.3 58.1 50.8 60.4 51.0 71.6 49.2 30.2 35.9 27.6 30.9 47.9 36.5 38.0 45.5 41.8 43.4 37.9 43.4 48.1 43.0 45.0 41.8 53.7 39.2 25.7 28.6 28.0 34.4 37.9 29.6 34.4 39. 45.6 56.7 46.1 57.2 60.3 47.5 55.0 51.7 69.1 48.3 35.0 33.3 31.7 36.7 46.1 35.6 32.8 42.2 38.1 38.6 37.7 42.3 46.7 47.9 47.9 40.0 54.0 37.7 24.2 28.4 27.0 29.8 38.1 27.0 38.1 38. 44.4 45.4 38.0 47.2 48.1 39.8 50.0 45.4 59.4 44.9 21.3 34.3 23.1 33.6 41.7 37.0 36.1 49.5 Performance Enhancement Through QWENLONG-CPRS: Model cascading with QWENLONGCPRS yields consistent improvements across all evaluated LLMs. LLaMA3.1-8b-Instruct, Qwen2.57b-Instruct, and Qwen2.5-32b-Instruct achieve respective performance gains of 39.72, 55.79, and 19.26 on Ruler-128K, with comparable improvements of 13.30, 21.95, and 18.83 on InfiniteBench. LongBench evaluations show sustained enhancements, averaging +2.8 on V1 and +3.0 on V2. Significantly, QWENLONG-CPRS-augmented open-source models surpass proprietary LLMs on Ruler-128K and InfiniteBench, establishing new state-of-the-art performances. This demonstrates that resource-constrained open-source LLMs can achieve parity with commercial counterparts in long-context tasks when integrated with QWENLONG-CPRS. Superior Performance with Extended Context Lengths: Experimental results demonstrate QWENLONG-CPRSs effectiveness correlates positively with input context length across evaluated tasks. It shows pronounced advantages when processing contexts exceeding standard LLM capacity limits, achieving average performance gains of 38.20 on Ruler-128K, 18.02 on InfiniteBench, and 10.5 on LongBench V2-Long. Conversely, minor improvements occur on LongBench V1s shorter contexts (2K-18K tokens). This dichotomy reveals two critical insights: (1) Current LLMs exhibit sufficient competence for conventional-length tasks, and (2) QWENLONG-CPRS provides essential performance augmentation specifically for extreme-length scenarios beyond standard model Table 4: The system prompt and performances of QWENLONG-CPRS in different Tasks, with Qwen2.5-7b-instruct as the generative LLM. The Avg. Len. represents the average token numbers of the optimized context. Task Granularity Avg. Len. Improvement Prompt Example InfiniteBench-RT.KV Words (UUID) 69.32 +84.20 Extract the {key}:{value} pair for the key in users question Ruler-128K-NIAH Ruler-128K-VT InfiniteBench-RT.Passkey InfiniteBench-RT.NUM Short Sentences 52.13 53.30 24.00 36.95 +39.87 +74.20 +5.19 +5.19 Extract the needles in the format of One of the special magic {type_needle_v} for {key} is: {value}. from the document Ruler-128K-QA Long Sentences 1173.17 +13.73 Extract some sentences from the documents as the supporting facts for answering the users question. InfiniteBench-EN.QA InfiniteBench-ZH.QA InfiniteBench-EN.MC Long Paragraphs 26287.13 63737.20 30466.04 +9.74 +7.48 +20.09 Retrieve chunks or paragraphs from the document that are related to the users query. Table 5: Performance comparison between QWENLONG-CPRS and NSA Model SingleDoc QA MultiDoc QA MF (en) MF (zh) Qasp HPQA 2Wiki. Musi. Du. Avg () DeepSeekMOE [8] + NSA [49] LLaMA3.1-8b-instruct [1] + QWENLONG-CPRS 51.20 50.30 49.39 54.37 62.30 62.40 60.64 60.87 40.90 43. 37.79 42.92 35.00 43.70 53.07 59.03 30.50 35.60 40.42 52.46 32.40 30. 29.79 34.58 29.40 34.10 21.06 23.93 40.24 42.86 (+2.62) 41.74 46.88 (+5.14) capacities. These findings strategically position QWENLONG-CPRS as solution for ultra-long context applications. Multi-Granularity Context Optimization: As detailed in Section 2, QWENLONG-CPRS enables dynamic context optimization through system-prompt-controlled granularity adaptation. Table 4 presents the implemented prompt-granularity configurations for Ruler-128K and InfiniteBench tasks, demonstrating consistent performance gains across all granularity levels. This multi-scale optimization capability permits customized prompt engineering while maintaining robust performance, which is particularly valuable for applications requiring flexible context optimization strategies. Comparative Analysis with RAG: The evaluation reveals distinct performance characteristics between QWENLONG-CPRS and RAG approaches. RAG demonstrates effectiveness in elementary retrieval tasks requiring single-fact extraction from noisy contexts (e.g., Ruler-NIAH, RT.Passkey, RT.NUM), achieving 82.4% average accuracy across these benchmarks. However, its performance degrades markedly in complex scenarios: (1) multi-hop reasoning tasks (QA.EN, QA.ZH, MultiDoc QA) and contexts with high query-context similarity (RT.KV). In contrast QWENLONG-CPRS effectively mitigates these limitations through dynamic context optimization, achieving 18.52% and 25.20% relative improvements respectively in these challenging scenarios. Comparative Analysis with Sparse Attention: Our analysis contrasts QWENLONG-CPRS against two sparse attention paradigms: training-free approaches (Minference [16], InfiniteRetrieval [48]) and trainable implementations (MOBA [27], NSA [49]). The training-free methods demonstrate limited improvements on Ruler-128K and Infinitebench, with Minference underperforming direct prompting on other benchmarks. The trainable MOBA achieves +28.36 on NIAH-sub versus direct prompting, though substantially lower than QWENLONG-CPRSs +52.43 gain. For NSA which is trained with DeepseekMOE [8]), relative improvements with comparable base models in Table 5 reveal QWENLONG-CPRSs superior scalability (+5.14 vs NSAs +2.62). This comparative analysis demonstrates that while parametric sparse attention requires model-specific training for marginal gains, while QWENLONG-CPRS delivers performance advantages across heterogeneous architectures without specialized optimization. Depth-Robust Needle Retrieval: Figure 4 demonstrates QWENLONG-CPRS-enhanced Qwen2.57b-Instructs performance in the Needle-in-a-Haystack paradigm across full-spectrum depth variations (0% to 100%) and context lengths (32K to 1M tokens). The system achieves perfect accuracy scores under all test configurations, matching the claimed capabilities of contemporary LLMs and agent systems advertising over 1M token capacities [27, 34, 46, 40, 28]. 9 Figure 4: Performance of QWENLONG-CPRS in NIAH test with input length is upto 1M. Figure 5: Comparative performance analysis of LLMs with and without QWENLONG-CPRS integration. Numerical values in parentheses indicate each models maximum input capacity. 4.2 QWENLONG-CPRS with Stronger LLMs To investigate QWENLONG-CPRSs capacity to push performance boundaries in long-context scenarios, we deploy it across high-parameter LLMs with superior general capabilities. Figure 5 reveals consistent performance enhancements: all QWENLONG-CPRS-augmented models exceed prior direct prompting baselines with the best performance (77.5 on Ruler-128K and 62.9 on InfiniteBench). Crucially, QWENLONG-CPRS effectively compensates for varying input constraints5, delivering average gains of 54.9, 49.0, 21.7 and 15.7 for the LLMs with 32K, 64K, 128K and 1M context windows, respectively. This demonstrates QWENLONG-CPRSs ability to elevate length-limited LLMs to parity with specialized long-context LLMs through optimized context, enabling resourceefficient deployment of short-input LLMs in extended-sequence applications. 5For Qwen2.5-max and Deepseek-v3, we utilize the version provided by the Aliyun Bailian platform, which supports 32K and 64K input contexts, respectively. Figure 6: Performance comparison between RAG and QWENLONG-CPRS across varying retrieved token quantities Figure 7: System latency of different context management methods with various input length. 4.3 Context Optimization Efficiency This section analyzes token efficiency by examining RAGs performance variation with increasing retrieved tokens from 1K to 64K across three Ruler-128K subsets. As shown in Figure 6, RAG exhibits suboptimal performance when the retrieved tokens are less than 8K across all tasks due to critical information loss in coarse-grained retrieval. RAGs performance peaks at 16K tokens, and then keeps fluctuation on NIAH-sub and Variable Tracking, and even go worse on QA when the number of retrieved tokens increase. This pattern emerges from RAGs incomplete critical data capture at low token quantities and noise accumulation at higher scales. QWENLONG-CPRS demonstrates superior performance through context optimization, achieving 99.59%, 92.66%, and 99.66% less tokens than RAGs peak-requirement volumes the three tasks. Furthermore, it surpasses RAGs maximum scores by 17.27% on NIAH-sub and 4.03% on QA, establishing both quantitative and qualitative advantages in context optimization efficiency. 4.4 Latency Analysis We evaluate QWENLONG-CPRSs impact on LLM prefill latency through four system configurations: (1) Direct prompting with Qwen2.5-7b-instruct, (2) RAG-enhanced implementation (16K retrieved tokens), (3) Minference sparse attention integration, and (4) QWENLONG-CPRS-cascaded architecture illustrated in Figure 3b. The QWENLONG-CPRS configuration employs window size = 8192 and parallelism factor ρ = 5. All systems utilize Qwen2.5-7b-instruct deployed 1 NVIDIA A100 GPU via vLLM with paged attention [19] for memory optimization. Figure 7 presents Time-to-First-Token (TTFT) measurements of the above four systems under increasing context lengths, revealing three critical patterns: 11 Table 6: Performance Comparison of QWENLONG-CPRS-Augmented LLMs with Original vs. Customized Prompts Model SingleDoc QA MultiDoc QA MF (en) MF (zh) NQ Qasp HPQA 2Wiki. Musi. Du. Avg_qa GR Summary QM MN VC Avg Qwen2.5-7b-instruct [34]+ QWENLONG-CPRS + Original Prompt + Customized Prompt Qwen2.5-32b-instruct [34]+QWENLONG-CPRS + Original Prompt + Customized Prompt 53.42 54.73 54.59 53.42 62.48 62. 65.82 65.73 24.97 24.2 44.16 45 63.95 62.72 56.16 56.33 41.01 41. 19.04 20.79 45.65 45.98 17.37 17.87 17.63 18.1 15.30 14.65 18.02 17. 36.12 36.32 30.56 29.52 45.54 45.31 67.44 68.3 66.28 65.44 47.89 47. 19.81 19.42 49.74 49.27 16.37 17.37 18.01 17.72 14.62 15.03 19.23 18. 38.85 38.56 The baseline direct prompting method exhibits quadratic latency growth, reaching 26.76s at 128K tokens. QWENLONG-CPRS demonstrates superior linear scaling with 3.47 acceleration over baseline at 128K, despite current implementation limitations that leave the optimization of computation kernel unexplored. Minferences sparse attention shows paradoxical behavior: below 96K tokens, dynamic sparse pattern computation overhead causes higher latency than direct prompting (10.42s vs 8.35s at 64K). At 128K tokens, it achieves 1.42 acceleration, which is substantially lower than QWENLONGCPRSs 3.47 improvement. RAG demonstrates constant-time latency characteristics, though this comes at accuracy costs shown in Section 4.1 and Section 4.3. Our future work will focus on bridging QWENLONGCPRSs current linear complexity toward constant-time performance while preserving its accuracy advantages. 4.5 Prompt-Agnostic Integration with Foundation Models This section analyzes the viability of direct QWENLONG-CPRS integration with foundation models without prompt engineering. We evaluate two configurations: (1) Standard Prompting using original instructions, and (2) Customized Prompting explicitly stating the optimized context nature. Table 6 reveals statistically comparable performance between prompt configurations for both Qwen2.57B-Instruct (=+0.20) and Qwen2.5-32B-Instruct (=-0.29). This consistency demonstrates QWENLONG-CPRSs output stability and seamless compatibility with existing LLM interfaces. The findings suggest practitioners can adopt QWENLONG-CPRS augmentation without workflow disruption, maintaining conventional prompting strategies while gaining long-context processing benefits. 4.6 Case Study Three practical implementations of QWENLONG-CPRS are presented in Figures 8, 9, and 10. Figures 8 and 9 demonstrate QWENLONG-CPRSs capability to compress query-relevant key sentences into minimal optimized contexts, effectively supporting downstream LLM inference through targeted information preservation. Figure 10 reveals QWENLONG-CPRSs standalone potential for critical contractual element extraction without LLM integration. This independent functionality suggests broader applicability as specialized service component in practical business intelligence applications requiring automated document analysis."
        },
        {
            "title": "5 Conclusion and Future Works",
            "content": "This work introduces the dynamic context optimization paradigm through the QWENLONG-CPRS framework, enabling controlled context compression via four technical innovations: (1) natural language-guided dynamic optimization, (2) boundary-aware bidirectional reasoning layers, (3) token critic mechanisms with language modeling heads, and (4) window-parallel inference architecture. Our comprehensive evaluations across five long-context benchmarks demonstrate QWENLONGCPRSs advantages in performance enhancement and inference efficiency. Experimental results reveal consistent improvements across 10 mainstream LLMs, particularly enabling smaller and shorter LLMs to achieve superior performance over stronger and longer counterparts. The framework 12 achieves 97.3% relative compression versus RAG baselines with 7.3% accuracy improvements, while linear latency scaling enables 3.47 acceleration over direct prompting at 128K-token inputs. Future research will pursue three primary objectives: First, improving computational efficiency through the implementation of KV-cache mechanisms and optimization of kernel operations. Second, integrating global context awareness to enhance semantic coherence. Third, expanding the frameworks applicability by adapting it as foundational component for diverse use cases, such as long-chain reasoning compression and agent systems."
        },
        {
            "title": "References",
            "content": "[1] Meta AI. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [2] Anthropic. Claude 3.7 sonnet. https://www.anthropic.com/claude/sonnet, 2025. [3] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. Longalign: recipe for long context alignment of large language models, 2024. URL https://arxiv.org/abs/2401.18058. [4] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, 2024. [5] Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks, 2025. URL https: //arxiv.org/abs/2412.15204. [6] Xavier Carreras and Lluís Màrquez. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004) at HLT-NAACL 2004, pages 8997, Boston, Massachusetts, USA, May 6 - May 7 2004. Association for Computational Linguistics. URL https://aclanthology. org/W04-2412/. [7] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory maze: Beyond context limit through interactive reading, 2023. URL https: //arxiv.org/abs/2310.05029. [8] Damai Dai, Chengqi Deng, Chenggang Zhao, R.x. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y.k. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12801297, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.70. URL https://aclanthology.org/2024.acl-long.70/. [9] Google Deepmind. era. agentic the google-gemini-ai-update-december-2024/#ceo-message, 2024. Introducing for https://blog.google/technology/google-deepmind/ new ai model gemini Our 2.0: [10] Google Deepmind. Gemini 2.5: Our most intelligent ai model. https://blog.google/ technology/google-deepmind/gemini-model-thinking-updates-march-2025/ #gemini-2-5-thinking, 2025. [11] DeepSeek-AI. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412. 19437. [12] David Dukic and Jan Snajder. Looking right is sometimes right: Investigating the capabilities of decoder-only LLMs for sequence labeling. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics ACL 2024, pages 14168 14181, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl.843. 13 [13] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. DuReader: Chinese machine reading comprehension dataset from real-world applications. In Eunsol Choi, Minjoon Seo, Danqi Chen, Robin Jia, and Jonathan Berant, editors, Proceedings of the Workshop on Machine Reading for Question Answering, pages 3746, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-2605. URL https://aclanthology. org/W18-2605/. [14] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models?, 2024. URL https://arxiv.org/abs/2404.06654. [15] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models, 2023. URL https: //arxiv.org/abs/2310.05736. [16] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. [17] Gregory Kamradt. Needle in haystack - pressure testing llms. https://github.com/ gkamradt/LLMTestNeedleInAHaystack/tree/main, 2023. [18] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026/. [19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. URL https://arxiv.org/abs/2309. 06180. [20] Gina-Anne Levow. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108117, Sydney, Australia, July 2006. Association for Computational Linguistics. URL https://aclanthology.org/W06-0115. [21] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. URL https://arxiv.org/abs/2005.11401. [22] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning, 2023. URL https: //arxiv.org/abs/2308.03281. [23] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Retrieval augmented generation or long-context llms? comprehensive study and hybrid approach, 2024. URL https://arxiv.org/abs/2407.16833. [24] Zongxi Li, Xianming Li, Yuzhang Liu, Haoran Xie, Jing Li, Fu lee Wang, Qing Li, and Xiaoqin Zhong. Label supervised llama finetuning, 2023. URL https://arxiv.org/abs/2310. 01208. [25] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023. URL https://arxiv.org/abs/2307.03172. 14 [26] Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xuanjing Huang, and Xipeng Qiu. Thus spake long-context large language model, 2025. URL https://arxiv.org/abs/2502.17129. [27] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, and Jiezhong Qiu. Moba: Mixture of block attention for long-context llms, 2025. URL https://arxiv.org/abs/2502.13189. [28] MiniMax. Minimax-01: Scaling foundation models with lightning attention, 2025. URL https://arxiv.org/abs/2501.08313. [29] Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: recurrent neural network based sequence model for extractive summarization of documents, 2016. URL https:// arxiv.org/abs/1611.04230. [30] OpenAI. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. [31] OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. [32] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression, 2024. URL https://arxiv.org/abs/2403.12968. [33] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023. URL https://arxiv.org/abs/2309. 00071. [34] Qwen. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020. URL https://arxiv.org/abs/1910. 02054. [36] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for SQuAD. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://aclanthology.org/P18-2124/. [37] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10731083, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1099. URL https://www.aclweb.org/anthology/P17-1099. [38] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/ abs/2104.09864. [39] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530. [40] Qwen Team. Generalizing an llm from 8k to 1m context using qwen-agent. https://qwenlm. github.io/blog/qwen-agent-2405/, 2024. [41] Qwen Team. Qwen3: Think deeper, act faster. https://qwenlm.github.io/blog/qwen3/, 2025. [42] Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142147, 2003. URL https: //www.aclweb.org/anthology/W03-0419. 15 [43] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. doi: 10.1162/tacl_a_00475. URL https: //aclanthology.org/2022.tacl-1.31/. [44] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions, 2023. URL https://arxiv.org/abs/2212.10560. [45] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. [46] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. Qwen2.5-1m technical report, 2025. URL https://arxiv.org/abs/2501.15383. [47] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv.org/abs/1809.09600. [48] Xiaoju Ye, Zhichun Wang, and Jingyuan Wang. Infinite retrieval: Attention enhanced llms in long-context processing, 2025. URL https://arxiv.org/abs/2502.12962. [49] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native sparse attention: Hardware-aligned and natively trainable sparse attention, 2025. URL https://arxiv.org/abs/2502.11089. [50] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. bench: Extending long context evaluation beyond 100k tokens, 2024. URL https://arxiv.org/abs/2402.13718."
        },
        {
            "title": "A Case Study",
            "content": "Figure 8: Example case #1: multi-value Needle-in-a-Haystack test. 17 Figure 9: Example case #2: English multi-hop QA. 18 Figure 10: Example case #2: Contract element extraction."
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}