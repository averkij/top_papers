{
    "paper_title": "Convolutional Set Transformer",
    "authors": [
        "Federico Chinello",
        "Giacomo Boracchi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce the Convolutional Set Transformer (CST), a novel neural architecture designed to process image sets of arbitrary cardinality that are visually heterogeneous yet share high-level semantics - such as a common category, scene, or concept. Existing set-input networks, e.g., Deep Sets and Set Transformer, are limited to vector inputs and cannot directly handle 3D image tensors. As a result, they must be cascaded with a feature extractor, typically a CNN, which encodes images into embeddings before the set-input network can model inter-image relationships. In contrast, CST operates directly on 3D image tensors, performing feature extraction and contextual modeling simultaneously, thereby enabling synergies between the two processes. This design yields superior performance in tasks such as Set Classification and Set Anomaly Detection and further provides native compatibility with CNN explainability methods such as Grad-CAM, unlike competing approaches that remain opaque. Finally, we show that CSTs can be pre-trained on large-scale datasets and subsequently adapted to new domains and tasks through standard Transfer Learning schemes. To support further research, we release CST-15, a CST backbone pre-trained on ImageNet (https://github.com/chinefed/convolutional-set-transformer)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 8 8 2 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Convolutional Set Transformer",
            "content": "Federico Chinelloa,, Giacomo Boracchib aDep. of Computing Sciences, Bocconi University, Italy bDep. of Electronics, Information and Bioengineering, Politecnico di Milano, Italy Abstract We introduce the Convolutional Set Transformer (CST), novel neural architecture designed to process image sets of arbitrary cardinality that are visually heterogeneous yet share high-level semanticssuch as common category, scene, or concept. Existing set-input networks, e.g., Deep Sets and Set Transformer, are limited to vector inputs and cannot directly handle 3D image tensors. As result, they must be cascaded with feature extractor, typically CNN, which encodes images into embeddings before the set-input In contrast, CST operates network can model inter-image relationships. directly on 3D image tensors, performing feature extraction and contextual modeling simultaneously, thereby enabling synergies between the two processes. This design yields superior performance in tasks such as Set Classification and Set Anomaly Detection and further provides native compatibility with CNN explainability methods such as Grad-CAM, unlike competing approaches that remain opaque. Finally, we show that CSTs can be pre-trained on largescale datasets and subsequently adapted to new domains and tasks through standard Transfer Learning schemes. To support further research, we release CST-15, CST backbone pre-trained on ImageNet (link). Keywords: set learning, deep learning, transfer learning, explainability 1. Introduction Images are ubiquitous and often occur in sets. For example, images on web page or in user-generated social media content are typically related Corresponding author. Email addresses: federico.chinello@studbocconi.it (Federico Chinello), giacomo.boracchi@polimi.it (Giacomo Boracchi) Figure 1: Deep Sets (DS) and Set Transformer (ST) pipelines compared to the CST architecture (ours). In DS/ST pipelines, shared CNN is applied independently to each image in the input set, yielding set of uncontextualized activation volumes whose spatial dimensions are subsequently collapsed via Global Average Pooling, resulting in set of uncontextualized latent vectors. These vectors are then fed to the DS/ST module, which performs contextualization and outputs set of contextualized latent vectors for downstream processing. In contrast, CST (ours) processes the set of images through stack of SetConv2D blocks, simultaneously performing feature extraction and context modeling, and yielding contextualized activation volumes that retain spatial dimensions (H ) and can be directly used in downstream tasks. to common concept and can therefore be processed together to enhance automatic visual recognition. In medicine, jointly analyzing multiple images from the same patient can improve diagnostic accuracy, such as in melanoma detection [1]. Similarly, [2] demonstrates how sets of room images can be used for fine-grained hotel classification, task that supports efforts to combat human trafficking. In disaster response, collections of images depicting the same building from different viewpoints provide valuable input for automated damage assessment models [3]. Overall, processing images as sets is advantageous because it enables visual recognition tasks to exploit both the complementary information carried by individual images and the semantic relationships that link them. Unlike matrices or tensors, sets have no fixed cardinality and lack an inherent ordering of their elements. Convolutional Neural Networks (CNNs) have achieved remarkable success across wide range of computer vision tasks, but they cannot natively process sets or exploit the contextual relationships among images within set. By contrast, set-input neural networks such as Deep Sets [4] and Set Transformer [5] are explicitly designed to handle 2 Figure 2: SetConv2D layer. SetConv2D takes set of volumes as input and processes each of these by shared convolutional layer. Global Average Pooling (GAP) is applied to each volume to yield set of latent vectors that is fed to Multi-Head Self-Attention (MHSA) unit. The output of MHSA is used as context-aware bias vectors that are summed to the volumes fed to the GAP, via residual connection. After applying these biases, the volumes are passed through non-linear activation function. set-structured data. However, these models operate only on sets of vectors and cannot directly process sets of 3D tensors such as images. As result, each image in set must first be converted into an embedding vector by feature extractortypically CNNand only then passed to the set-input network, as illustrated in Figure 1. In this cascaded pipeline, the feature extractor processes each image independently and identically, without leveraging any contextual information, while the set-input network subsequently applies set-specific operations to capture inter-image relationships. While end-to-end training of the CNN and the Deep Sets or Set Transformer is possible, feature extraction and context modeling remain strictly sequential. As result, the two processes cannot develop true synergies: convolutional layers enforce locality and translation-invariance priors during feature extraction, but these inductive biases cannot interact directly with the set-specific operations that capture semantic relationships among images, since the latter only occur once feature extraction has been completed. This cascaded design also limits explainability: saliency methods such as Grad-CAM [6] can only be applied to the convolutional layers, which precede any set-specific operation. To address these limitations, we introduce the Convolutional Set Transformer (CST), general framework for deep learning architectures specifically designed for sets of images. Its fundamental building block is SetConv2D (see Figure 2), permutation-equivariant set-to-set layer that takes as input set of 3D tensorssuch as images or activation volumesand returns Figure 3: Unlike Set Transformer (ST), CST is transparent and explainable. The Set Anomaly Detection task aims to identify anomalous images within set. The figure shows two image sets derived from the CelebA dataset [7]. In each set, majority of normal images share two attributes (wearing hat and smiling in the first, no beard and attractive in the second), while minority lack these attributes and are thus anomalous. After training CST and an ST on CelebA for Set Anomaly Detection, we evaluate the explainability of their predictions by overlaying Grad-CAMs on anomalous images. CST explanations correctly highlight the anomalous regions, whereas ST explanations fail to provide meaningful insights. For further details, see Section 6.2. corresponding set of activation volumes, each primarily associated with one input image. The key distinction between Conv2D and SetConv2D is that, in the latter, biases are dynamically adjusted at inference time to incorporate contextual information extracted from the entire image set. These dynamic biases are computed by Multi-Head Self-Attention (MHSA) [8] unit integrated into each SetConv2D layer. CST networks are obtained by stacking multiple SetConv2D layers and, unlike Deep Sets and Set Transformers, can exploit contextual information from the set during feature extraction (Figure 1). In particular, dynamic biases in the early SetConv2D layers model relationships between images by analyzing low-level patterns, while in deeper layers they capture increasingly complex semantic connections. Our experiments show that CST consistently outperforms Deep Sets and Set Transformer across multiple tasks, including Set Classification and Set Anomaly Detection. Moreover, unlike Deep Sets and Set Transformer, CST is natively compatible with explainability methods for CNNs, such as 4 Grad-CAM [6]. In particular, we demonstrate that CST enables meaningful Grad-CAM visualizations in Set Anomaly Detection, while Set Transformer fails to provide reliable explanations (Figure 3). Finally, whereas prior set learning literature, such as [4, 5], has mainly relied on training models from scratch on small, low-resolution datasets, we train CST on large-scale benchmark (ImageNet [9]) and demonstrate that, once pre-trained, it can be effectively adapted to new domains via Transfer Learning. As case study, we address the challenging task of event recognition from personal photo albums [10]. We summarize the key contributions of our paper: We introduce the Convolutional Set Transformer (CST), deep neural architecture designed to operate on sets of images, and the SetConv2D block, the foundational module of CST. We demonstrate that CST can be pre-trained on large-scale datasets such as ImageNet and then adapted to variety of downstream tasks through Transfer Learning. To this end, we introduce Contextualized Image Classification (CIC), natural and effective pretraining objective for CST, and Combinatorial Training (CT), training strategy designed to be used in conjunction with CIC. As concrete contribution, we publicly release CST-15, CST encoder pre-trained on ImageNet, for download. We show that CST is natively compatible with standard CNN explainability tools such as Grad-CAM, while existing set-input architecturessuch as Deep Sets and Set Transformerare inherently opaque due to their cascaded design. 2. Problem Formulation We now present the specific problems addressed in this paper. We address two broad categories of visual recognition tasks that operate on an input set of images {I1, . . . , IN Ii RHW C}: set-to-set tasks and set-to-global tasks. In set-to-set tasks, the goal is to produce an output for each individual image while exploiting the contextual relationships among the images in {I1, . . . , IN }. Within this category, we consider two tasks: Set Anomaly Detection and Contextualized Image Classification (CIC). Set Anomaly 5 Detection is binary classification task meant to identify images in set that are anomalous or inconsistent with the majority of the set. Here, the notion of anomaly is relative: the same image may be considered anomalous in one set but not in another, depending on the surrounding context. In Contextualized Image Classificationa task we introduce in this paperwe assume that all images in {I1, . . . , IN } share common (unknown) label Λ. Rather than inferring this label independently for each image, we generate contextualized image-level predictions, where the prediction for any given image is influenced by the other images in the same set. We propose CIC as an effective pre-training task for CSTs. In contrast, set-to-global tasks aim to generate shared, global output for the entire set, such as in Set-level Classification, where single label Λ is predicted for the entire set {I1, . . . , IN }. For Set Classification tasks, the training dataset contains sets of images, where each set has an associated class label. For Set Anomaly Detection, following [4, 5], the training dataset consists of image sets in which images that are anomalous with respect to the surrounding context are marked. At both training and inference time, sets may contain any number of images, and their order is irrelevant. Classification tasks can also be performed on singleton sets containing only single image. Set-input networks are either equivariant or invariant with respect to set permutations [4, 5]. This distinction aligns with nature of the tasks discussed: permutation equivariance is essential for set-to-set tasks, where permutation of the input set must result in corresponding permutation of the output set. In contrast, permutation invariance is required for set-to-global tasks, ensuring that the output remains unaffected by any permutation of the input set. 3. Related Work Here, we present the literature relevant to our contributions, specifically set-input neural networks and networks that process multiple images as input. Learning from sets. In the last decade, set-input deep neural networks have attracted significant interest from the research community [4, 5, 11, 12, 13, 14, 15, 16, 17] and found application to broad range of tasks, including inference from point clouds [18, 19]. 6 Deep Sets [4] and Set Transformer [5] are deep learning architectures designed to operate on set-structured data. However, they are restricted to processing sets of vectors and cannot manage sets of 3D tensors. Therefore, if the input consists of image sets, images must be encoded as embedding vectors by conventional feature extractor, such as CNN, before they can be fed into the Deep Sets/Set Transformer network (Figure 1). In contrast, CST models contextual information simultaneously with feature extraction, preserving spatial information, and leveraging the locality and translation invariance priors of images. Our experiments demonstrate that CST achieves superior performance and enables using explainability tools for CNNs on each individual image of the set. In contrast, Deep Sets and Set Transformer networks are opaque and their predictions are not always explainable. Deep Sets for Symmetric elements (DSS) [20] is another architecture designed to operate on sets, but it is tailored for fundamentally different class of problems than CST. In the case of image set processing, DSS layer applies shared convolutional layer L1 to each image in the input set, while simultaneously computing the spatial summation of all images and processing it through second convolutional layer L2. The final output is then obtained by adding together the contributions from L1 and L2. Importantly, the use of spatial summation assumes high degree of spatial alignment among the set elements, such as when the set contains shifted versions of the same image, noisy augmentations, or multi-channel variants of common signal. This strong inductive bias limits DSS to tasks where such spatial alignment holds, like image restoration and denoising. DSS cannot capture the abstract semantic relationships that arise when set contains visually diverse but conceptually related images. In contrast, CST is explicitly designed for these heterogeneous settings, where summing aligned features would be either ineffective or inappropriate. By combining Global Average Pooling with Multi-Head Self-Attention [8] within the SetConv2D framework, CST is able to exploit high-level semantic relationships across diverse set elements. As result, CST can address complex tasks such event recognition from personal photo albumsapplications that directly violate the core spatial alignment prior assumed by DSS. Learning from multiple images. Multi-image deep networks have demonstrated their effectiveness in several domains, including 3D shape recognition [21, 22, 23, 24, 25], biomedical image analysis [26, 27, 28, 29, 2], anomaly detection [30], and plant species identification [31, 32]. Typically, in these models, latent representations are extracted from each image and fused into single representation, which is then further processed by nonlinear layers [33]. As result, image-specific information is lost after fusion. In contrast, CST encoders are specifically designed to integrate contextual information from multiple input images while preserving, for each image, individual representations that capture image-level peculiarities. 4. Convolutional Set Transformer (CST) In this section, we introduce SetConv2D, the core building block of our architecture (Section 4.1), describe how to assemble CST networks (Section 4.2), and discuss CST explainability (Section 4.3). 4.1. SetConv2D SetConv2D is convolutional block designed for processing arbitrarily sized sets of 3D volumes, which can be either input images or convolutional activation maps. SetConv2D operates as an equivariant set-to-set layer, returning an output volume for each input volume. Thus, SetConv2D retains different representation for each image in the input set. More specifically, SetConv2D processes each volume within set by preserving and exploiting the locality and translation invariance priors of CNNs, while leveraging the context information from other volumes within the set to enhance the output representation. The five stages composing SetConv2D are illustrated in Figure 2 and detailed in what follows. 1) 2D convolutional layer. shared convolutional layer processes each volume in the input set. Its hyperparametersnumber of filters, kernel size, padding, and strideare defined at SetConv2D initialization. No activation is applied at this stage. 2) Global Average Pooling. Each volume returned by the convolutional layer at the previous step is reduced via GAP, yielding latent vector. 3) Multi-Head Self-Attention. Latent vectors corresponding to different volumes in the set interact within Multi-Head Self-Attention module, implemented as in [8]. Positional encoding is omitted to maintain permutation-equivariance. 4) Context-aware biases. Thanks to skip connections illustrated in Figure 2, each contextualized latent vector serves as bias vector to be added to the corresponding volume computed at step 1). 8 5) Activation. The volumes are finally passed through non-linear activation function. SetConv2D supports valid and same padding, stride 1, and dilation. In our preliminary tests, we evaluated alternative pooling and attention mechanisms within SetConv2D blocks, and we observed that the combination of GAP and MHSA consistently delivered the strongest performance. 4.2. CST Architecture CST consists of permutation-equivariant encoder E, followed by taskspecific downstream network H, which may be either equivariant or invariant depending on the task. The encoder simultaneously performs feature extraction for each image in the input set while modeling the contextual relationships that arise within the set. It is built by stacking layers of SetConv2D blocks, much like Conv2D layers are stacked in standard CNNs. Since SetConv2D operates on arbitrarily sized sets and is permutation equivariant, the encoder naturally inherits these properties. Given an input image set, produces set of contextualized latent representations: {R1, . . . , RN } = E({I1, . . . , IN }) (1) The downstream network further processes {R1, . . . , RN } to accomplish the specific task. For set-to-set tasks, operates on each individual image representation independently. In its simplest form, it can be classifier or regressor that performs inference on each images contextualized representation. On the other hand, for set-to-global tasks, typically employs an invariant pooling operation, such as summation or maximum, to aggregate {R1, . . . , RN } into global set-level representation, which is further processed to produce set-level prediction. Similar to CNNs, CSTs extract hierarchical features by applying series of convolution operations. The first SetConv2D layers react to fine-grained image patterns, such as edges and small details, while deep SetConv2D layers capture higher-level concepts. The key difference between shared CNN backbone applied in parallel to each image in the input set and CST encoder is that the former relies on static biases learned during training, whereas the latter, through the SetConv2D mechanism, employs dynamic biases that are adjusted at both training and inference time based on contextual information. 9 Note that CSTs can also process single images as sets of unit size. SetConv2D blocks are compatible with Conv2D layers and it is possible to interleave the two when designing CSTs, as we show in our experiments. This also implies that existing CNNs can be modified to include SetConv2D blocks, enabling training and inference on sets of images. 4.3. Explainability Explainability in set-input networks is largely unexplored research area. CST is the first architecture to provide demonstrated explainability support, as shown by our experiments, whereas competing approaches, such as Deep Sets and Set Transformer, remain inherently opaque due to their design choices. As discussed in the previous sections, CSTs are constructed by stacking SetConv2D layers, in the same way that CNNs are built by stacking standard Conv2D layers. This design makes CST natively compatible with standard CNN explainability tools such as Grad-CAM. Since Grad-CAM relies on gradients with respect to the spatial representations produced by convolutional layers, it can be applied to the outputs of SetConv2D blocks, which provide contextualized spatial representation for each image in the input set. The resulting explanation maps not only highlight which features of an image drive the prediction, but also reveal how these features are modulated by the presence of other images in the set. Notably, the same image can yield different explanation maps depending on the set it belongs to, making CSTs contextual reasoning directly observable. Unlike CST, which natively operates on sets of 3D tensors such as images, Deep Sets and Set Transformer can only process vector embeddings. Consequently, images must first be reduced by CNN to latent vectors, discarding spatial information before contextualization. Although Grad-CAMs can be computed with respect to the convolutional outputs of the CNN, this stage lies entirely before the Deep Sets or Set Transformer module. As result, the explanation maps highlight features of individual images but offer no insight into the contextual modeling performed by the set-input network, which remains unexplained. 5. CST Pre-training and Transfer Learning Deep Sets and Set Transformers are usually trained from scratch, as in [1]. Pre-trained set-learning models are not available, and Transfer Learning on these architectures has never been studied in the literature. In contrast, 10 CST supports Transfer Learning in natural way. Indeed, after pre-training on large scale dataset like ImageNet, CST encoder can be reused as generic backbone for set-learning tasks. To adapt pre-trained CST to new task, the downstream network is re-initialized or even replaced with an architecture tailored to the task, while the encoder is either frozen or fine-tuned. To facilitate future research on Transfer Learning with CSTs, we publicly release CST-15, the first set-input backbone pre-trained on ImageNet. In most real-world scenarios, training datasets provide only single image per instance. This makes it infeasible to train set-input architectures such as Deep Sets, Set Transformers, or CSTs from scratch, since these models cannot learn how to leverage contextual interactions among images in set, if only singleton sets are provided during training. CST addresses this challenge by enabling Set-free Transfer Learning: our experiments show that CST retains the ability to perform inference on arbitrarily large image sets with improved accuracy, even when the Transfer Learning phase is limited to training on individual images rather then image sets. This is possible because the CST encoder preserves the contextualization capabilities it acquired during large-scale pre-training, thereby eliminating the need to re-learn how to exploit inter-image relationships when adapting to new tasks. In Section 5.1, we propose Contextualized Image Classification as pretraining objective for CSTs. In Section 5.2, we introduce Combinatorial Training, training strategy that is highly beneficial when pre-training CSTs for the CIC task. 5.1. Pre-training Objective: Contextualized Image Classification (CIC) We propose Contextualized Image Classification (CIC): pre-training task for CST encoders, just like Image Classification is used to pre-train CNN encoders. In CIC, permutation-equivariant network takes as input set of images that all belong to the same (unknown) class and therefore share the same label. The model returns contextualized prediction for each image, based not only on the information specific to that image, but also on the contextual information provided by the other images in the set. By leveraging the shared context among the images, the network is able to improve its image-level classification accuracy. We design network for CIC pre-training by concatenating CST encoder and MLP classifier H. The encoder consists of stack of SetConv2D blocks followed by GAP to collapse the spatial dimensions of the convolutional volumes returned by the last SetConv2D block. Thus, receives set 11 of images as input and returns set of contextualized, image-level latent vectors. Each contextualized latent vector is then classified identically and independently by H. By training for CIC on large datasets, the encoder learns both to extract features from each image and to model the contextual relationships between different images in set. After training, can be removed, and adapted to different set-learning tasks, following standard Transfer Learning practice. At first glance, the CIC task may seem counter-intuitive. Since all images in set share the same label, it might appear more natural to aggregate their representationse.g., by averagingand produce single set-level prediction, as in Set-level Classification. However, this yields permutation-invariant network, which is unsuitable as general-purpose encoder: set-pooling irreversibly collapses image-level representations that are crucial for set-to-set tasks such as Set Anomaly Detection. Moreover, removing the pooling module after training to recover equivariance is not viable, as it might disrupt the functioning of the network. In contrast, CIC enables the pre-training of permutation-equivariant CST encoders that preserve contextualized, imagelevel representations and can be seamlessly adapted to both set-to-set tasks, which require equivariant outputs, and set-to-global tasks, where invariance is readily obtained by applying pooling operation to the outputs of the equivariant backbone E. In practice, CIC also yields more stable training than Set-level Classification. Importantly, the same dataset can be annotated at different levels of granularity, giving us an additional degree of freedom to exploit during CST pre-training with CIC. For instance, ImageNet [34] organizes an ontology of images based on the semantic hierarchy of WordNet [35]. Depending on the selected level of granularity, an image of an husky can be assigned to the class husky, working dog, dog, canine, carnivore, placental, or mammal. Higher granularity results in more classes and lower intra-class variability, while lower granularity leads to fewer, coarser classes with greater intra-class variability. By selecting the granularity of the labels, we can influence the type of features that the CST learns during pre-training. Finer granularity encourages the network to capture subtle distinctions between visually similar categories, leading to more specialized representations. Coarser granularity, instead, pushes the model to focus on higher-level semantic connections between images within set, resulting in more general representations. 12 5.2. Combinatorial Training (CT) In the CIC task, network predicts the class of an image conditionally on variable number of other images from the same (unknown) class. As result, when training model for CIC, the number of training samples scales combinatorially with the number of possible sets that can be formed from images of the same class. We leverage this insight to design Combinatorial Training (CT), novel training strategy for CST architectures and, in principle, for any set-input network. In CT, before each epoch starts, we draw set size uniformly at random in the range [nmin, nmax], where nmin and nmax are hyperparameters controlling the minimum and maximum number of images in set. Then, training images within each class are randomly assembled into sets of size n, and sets from all the classes are randomly assigned to batches of size b. The batch size defines the number of sets in batch, which is kept fixed across epochs. Consequently, since the number of images per set varies in [nmin, nmax], the total number of images per batch varies in [b nmin, nmax] during training. CT operates as an effective form of data augmentation, since the chances of the model encountering the same set more than once during training are extremely low. Furthremore, concurrent training on sets of varying sizes allows the model to learn how to handle input sets of different sizes effectively. Importantly, the benefits of CT are not due to permuting the elements of the setsince CSTs are either permutation-equivariant or invariantbut rather stem from building random image sets of varying size during training, which has data augmentation effect. Setting nmin = 1 can speed up convergence when training on large datasets, as training only with sets may act as an overly strong augmentation that slows optimization. CT enables memory-efficient training of Convolutional Set Transformers as small sets can be used for training, while preserving the models capacity to handle arbitrarily large sets during inference. This strategy ensures scalable performance without incurring the high memory costs typically associated with larger sets during training. For example, the CST-15 model is trained on ImageNet using minimum set size of nmin = 1 and maximum set size of nmax = 2, i.e., it is trained on individual images (as in standard CNN training) and pairs of images from the same class. Despite this, at inference time, it can process image sets of any size with increasing accuracy, as demonstrated in Section 6.3. 13 6. Experiments In Section 6.1, we show that CST outperforms Deep Sets and Set Transformer in Set Classification tasks across several datasets. In Section 6.2 we focus on Set Anomaly Detection and demonstrate that CST achieves superior performance while enabling explainability. Finally, in Section 6.3, we introduce CST-15, CST pre-trained on ImageNet, showing that it can be adapted to new tasks by Transfer Learning. 6.1. Classification Experiments We carried out extensive experiments to benchamrk CSTs against Deep Sets, Set Transformers, and CNNs in the Contextualized Image Classification (CIC) and Set-level Classification (SC) tasks. CIC is introduced in Section 5.1 as pre-training task, but we can also test on this as the ultimate network In both CIC and SC, network receives as input set of images task. belonging to the same unknown class, but in SC the network predicts single class probability distribution at the set level, while in CIC the output consists of multiple (contextualized) distributions at the image level. Notably, CIC models are by design equivariant with respect to permutations of the input set, while SC models are invariant. 6.1.1. Tested Architectures We tested many CIC and SC models across several datasets, including ImageNet64x641 [36], Tiny ImageNet [37], CIFAR-10, and CIFAR-100 [38], showing that CSTs consistently outperform baselines models, often by substantial margin. For each dataset, we trained the following equivariant networks for Contextualized Image Classification (see also Table 1): CST designed by stacking pairs of SetConv2D blocks interleaved with max-pooling layers. For models trained on ImageNet64x64 and Tiny ImageNet, the the architecture comprises 8 SetConv2D blocks, whereas for the CIFAR datasets it uses 6. Each SetConv2D has kernel size of 3 with same padding, and the number of attention heads is set to min(nf ilters, 64). The final SetConv2D block is followed by GAP and dense layer with softmax activation to produce the class probabilities. 1In this section, we use ImageNet64x64, downsampled version of ImageNet, to keep computations tractable when comparing architectures. In Section 6.3, we will present CST-15, CST pre-trained on the ImageNet dataset at standard 224 224 resolution. 14 CST 8M params. ImageNet64x64 CNN 5.2M ST-S 7.3M ST-L 8.4M DS 6M params. params. CST 1.9M params. Tiny ImageNet CNN 1.2M ST-S 1.8M ST-L 2M DS 1.6M params. params. params. params. Input: 64 64 RGB images Conv2D(64, 3) Conv2D(64, 3) SetConv2D(64, 3) SetConv2D(64, 3) params. params. Input: 64 64 RGB images Conv2D(32, 3) Conv2D(32, 3) SetConv2D(32, 3) SetConv2D(32, 3) CIFAR-10/100 CST 462K/474K params. CNN ST-S ST-L DS 288K/300K 421K/433K 487K/499K 387K/398K params. params. params. params. SetConv2D(32, 3) SetConv2D(32, 3) Input: 32 32 RGB images Conv2D(32, 3) Conv2D(32, 3) Maxpool Maxpool SetConv2D(128, 3) SetConv2D(128, 3) Conv2D(128, 3) Conv2D(128, 3) SetConv2D(64, 3) SetConv2D(64, 3) Conv2D(64, 3) Conv2D(64, 3) SetConv2D(64, 3) SetConv2D(64, 3) Maxpool Maxpool SetConv2D(256, 3) SetConv2D(256, 3) Conv2D(256, 3) Conv2D(256, 3) SetConv2D(128, 3) SetConv2D(128, 3) Conv2D(128, 3) Conv2D(128, 3) SetConv2D(128, 3) SetConv2D(128, 3) Maxpool Maxpool SetConv2D(512, 3) SetConv2D(512, 3) Conv2D(512, 3) Conv2D(512, 3) SetConv2D(256, 3) SetConv2D(256, 3) Conv2D(256, 3) Conv2D(256, 3) Maxpool GAP Maxpool GAP - - - - - - SAB(512) SAB(512) DS(512) SAB(512) SAB(512) DS(512) SAB(512) DS(512) - FC(1000) Softmax - - - - - - SAB(256) SAB(256) DS(256) SAB(256) SAB(256) DS(256) SAB(256) DS(256) - FC(200) Softmax - - - - - Maxpool Conv2D(64, 3) Conv2D(64, 3) Maxpool Conv2D(128, 3) Conv2D(128, 3) Maxpool - - - GAP - - - SAB(128) SAB(128) - SAB(128) SAB(128) SAB(128) DS(128) DS(128) DS(128) FC(10/100) Softmax Table 1: Overview of architectures trained for the Contextualized Image Classification (CIC) task. SetConv2D blocks and Conv2D layers specified with (nf ilters, kernel size), All convolutions are implemented with same padding. In SetConv2D blocks, attention heads have dimension min(nf ilters, 64). Maxpooling is implemented with pool size 2. For Set Attention Blocks (SAB) and Deep Sets blocks (DS), we report the dimension of the output space in parenthesis. In all architectures, we use ReLU activations. CNN obtained from the CST by replacing all SetConv2D blocks with standard Conv2D layers. We refer to this CNN as the equivalent CNN since it preserves the same convolutional structure and receptive field as the corresponding CST. Unlike the CST model, however, the equivalent CNN does not support set processing and can operate only on individual images. Table 1 reports higher parameter count for CST compared to the equivalent CNN, but this difference is not substantial, as the two architectures are directly comparable only when operating on individual images (set size = 1). In this setting, the MHSA component within each SetConv2D block collapses to single linear channel projection, so CST and its equivalent CNN present the same number of nonlinearities. Furthermore, CST still retains the full set of MHSA weight matrices within each SetConv2D blockincluding the now functionally redundant query and key matriceswhich inflates its reported parameter count. Two Set Transformers [5] with two (ST-S) and three (ST-L) Set Attention Blocks (SABs) on the top of the equivalent CNN. ST-S has fewer parameters than the competing CST, while ST-L has more parameters. GAP layer is applied to each convolutional volume returned by the equivalent CNN, since the subsequent Set Attention Blocks are designed 15 to operate on sets of vectors rather than spatial tensors. As in [5], Set Attention Blocks are defined as an adaptation of the encoder block of the Transformer [8], without positional encoding and dropout: SAB(SN ) = LayerN orm(H + rF (H)) (2) = LayerN orm(SN + HSA(SN )) Here, SN RN represents the input set, with rows corresponding to latent vectors. The notation HSA() denotes Multi-Head SelfAttention, while rF () refers to row-wise feedforward layer that processes all the latent vectors independently. (3) Deep Sets [4] network (DS), with three Deep Sets equivariant layers on the top of the equivalent CNN. As for ST models, GAP is applied to each convolutional volume computed by the equivalent CNN, yielding set of latent vectors, since Deep Sets layers cannot handle sets of spatial tensors. We investigated several implementations of the Deep Sets layers. In the best-performing variant, the mean latent vector of the set is added to each individual vector, and the result is then passed through dense layer with nonlinear activation: DeepSets(SN ) = rF (SN + 1 11T SN ) (4) Here, SN RN represents the input set, 1 RN , and rF (.) denotes row-wise feedforward layer that processes latent vectors independently and identically. In our experiments, increasing the number of Deep Sets layers beyond three leads to lower performance. Furthermore, for each dataset, we assessed the following invariant architectures for Set-level Classification: Score Fusion (SF) models [33]. By default, the CIC models reported in Table 1 output image-level predictions, but can be adapted to generate set-level predictions without any further training. Indeed, at inference time, we can simply average the estimated class probabilities from all the images, yielding set-level distribution. Late Fusion (LF) models [33]. An invariant set pooling module can be added to the CIC architectures of Table 1, placed just before the final 16 classifier, enabling set-level predictions. The pooling module collapses the image-level latent vectors into an aggregate set-level representation, before classification. In our implementation, we compute the mean latent vector of the set and feed it to dense layer with non-linear activation: LateF usion(SN ) = σ(β + 1T SN Γ) (5) 1 Here, SN RN represents the input set, Γ and β are learnable weights and biases, σ is non-linear activation function, while 1 RN . In all Late Fusion models, we introduce Layer Normalization [39] step following the fusion module, as it enhances training stability, particularly with complex datasets such as ImageNet64x64. We train Late Fusion networks from scratch, separately from the corresponding CIC models. 6.1.2. Training Hyperparameters We trained both CSTs and all the other modelsexcept for standard CNNs, which completely disregard set informationusing Combinatorial Training with nmin = 2 and nmax = 5. Note that CT, introduced in Section 5.2 as training strategy to be used in conjunction with CIC, can be applied to SC as well. Figure 4: Combinatorial Training ablation. Relative increase in Top-1 Test Accuracy for CIC models trained on the ImageNet64x64 dataset when employing CT compared to conventional training. Our analysis reveals that CT benefits both CSTs and baseline models. Figure 4 shows the relative increase in Top-1 test accuracy for CIC models trained on the ImageNet64x64 dataset when employing CT compared to conventional training2. CT is advantageous for CSTs, Set Transformers, and Deep Sets models, with relative improvements in Top-1 accuracy ranging from +24% to +32% for sets of size 1, and from +14% to +21% for size 2. We report an in-depth ablation study on Combinatorial Training in Appendix A, which confirms these findings. To ensure comparability, we trained all competing models using the same hyperparameters and fixed random seeds. Our results are robust across different hyperparameter configurations. detailed description of the hyperparameters used can be found in Appendix B. 6.1.3. Results and Analysis Table 2 shows the Top-1 test accuracy as function of the input set size for CIC and SC tasks across all models trained on ImageNet64x64, Tiny ImageNet, CIFAR-10, and CIFAR-100. CSTs outperform the baselines in 58 out of 60 cases, spanning various combinations of dataset, task, fusion strategy (for SC models), and input-set size. In relative terms, CSTs outperform the best alternative up to +20.3%, and by more than +10% in 17 cases. For the sake of convenience, we report results for sets of up to five images; however, all set-based models can process arbitrarily large image collections, with performance improvements as the set size grows. As expected, performance exhibits diminishing returns with increasing set size: for larger input sets, additional context images provide mostly redundant information, and performance gains saturate. Moreover, the accuracy of different architectures converges as the context size increases, suggesting that the model choice becomes less critical when the context is sufficiently large. To study how larger context sharpens class separation in the latent space of CST trained for CIC, we designed controlled experiment on CIFAR-10. We trained modified version of the CIFAR-10 CST model presented in Table 1, inserting bottlenecka linear layer with two neurons and no activationafter GAP and before the classifier. Once trained, we removed the classifier, leaving CST encoder that outputs contextualized 2D embedding for each image in set. Using this encoder, we mapped each CIFAR-10 test image into the latent space by placing it within sets of = 1, 2, 4, 6, 8, 2When CT is not applied, we create sets of three images from the same class before the training process begins and use these sets to train the model in every epoch. 18 Model ImageNet64x64 4 3 2 5 1 Tiny ImageNet 4 3 2 5 1 CIFAR 10 2 4 5 1 CIFAR 100 3 2 5 CST 39.30 63.71 76.46 83.14 87.02 39.04 59.87 74.16 81.07 87.15 80.59 94.06 97.48 98.89 99.16 49.49 70.81 82.41 88.56 92.33 -2.28 +3.91 +2.93 +1.72 +0.85 +5.51 +8.06 +8.57 +4.78 +6.37 +1.20 +1.19 +0.45 +0.55 +0.23 +5.87 +7.38 +6.99 +5.22 +4.33 Contextualized Image Classification (CIC) - 98.34 97.47 98.30 - 92.87 91.70 91. 79.39 79.22 78.48 78.04 - 73.53 72.99 71.67 - 59.80 59.78 58.37 - 51.81 51.78 49.25 - 97.03 95.66 96.36 - 79.60 80.78 78. - 73.91 76.29 71.99 33.53 31.24 30.96 30.42 - 81.42 80.95 79.27 - 98.93 98.11 98.77 - 86.17 85.57 83.89 CNN 41.58 ST-S 35.58 ST-L 35.72 DS 35. - 65.08 65.59 62.84 Set-level Classification (SC) CST + SF 39.30 63.82 76.83 83.53 87.47 39.04 60.46 74.93 82.20 87.50 80.59 94.12 97.69 98.87 99.39 49.49 71.55 82.91 89.28 92.75 -2.28 +3.78 +3.06 +1.29 +0.82 +5.51 +8.14 +8.78 +5.55 +6.25 +1.20 +1.23 +0.69 +0.30 +0.34 +5.87 +7.98 +7.75 +5.43 +4.25 76.79 88.50 86.33 85.99 CST + LF 41.17 65.65 78.93 85.60 89.68 37.37 59.33 72.79 81.55 87.00 80.92 93.73 97.87 99.31 99.72 47.57 70.15 82.26 88.59 92.35 +5.14 +5.47 +4.72 +3.49 +2.11 +6.30 +7.60 +7.33 +6.38 +4.86 +2.22 +1.28 +0.90 +0.53 +0.45 +6.15 +6.85 +5.89 +4.65 +3.29 89.06 88.62 86.44 88.78 CNN + SF 41.58 55.39 60.04 ST-S + SF 35.58 59.74 ST-L + SF 35.72 58.85 DS + SF 35.56 CNN + LF 35.00 ST-S + LF 36.03 ST-L + LF 35.10 DS + LF 35.28 75.16 86.65 85.79 85.32 97.38 99.00 97.99 99.05 99.11 99.27 99.00 99. 85.12 86.92 87.57 85.14 70.39 82.24 81.15 80.88 64.14 73.77 73.21 72.74 94.14 97.00 95.61 96.54 65.12 81.25 81.25 81.25 66.28 75.16 73.57 72. 65.36 65.46 63.93 62.83 81.25 82.14 79.24 79.35 74.91 75.17 73.22 72.14 33.53 31.24 30.96 30.42 53.35 65.69 66.15 64.23 60.89 74.87 76.65 73. 96.42 96.97 96.06 95.91 75.07 76.37 75.62 74.61 31.07 31.07 30.45 29.22 79.51 82.11 81.89 80.12 71.61 74.21 73.64 72.35 45.95 52.32 51.85 49. 58.03 60.18 59.14 58.39 50.88 51.73 49.73 49.14 - 75.42 73.44 72.15 89.80 92.89 91.67 91.74 79.39 79.22 78.48 78.04 96.31 98.44 97.40 98. 43.62 42.19 40.88 39.05 56.99 63.57 61.99 60.14 72.61 83.85 81.03 81.25 78.21 78.70 78.18 78.39 92.00 92.45 92.25 91.74 98.48 98.78 97.96 98. 40.76 40.70 41.42 40.16 63.30 61.84 62.85 61.39 83.12 83.94 81.86 82.34 43.62 42.19 40.88 39.05 - 63.43 62.02 59.41 - 83.34 80.78 80. - 88.00 86.23 84.64 Table 2: Top-1 Test Accuracy (%) as function of Set Size (up to 5 images). SF and LF are acronyms for Score Fusion and Late Fusion, respectively. Rows labeled as report the absolute difference in accuracy between CST models and the best-performing baseline. Figure 5: 2D latent representations of CIFAR-10 test images with varying context size. Each color corresponds to class. images sampled from the same class. Figure 5 illustrates the resulting embeddings across different set sizes. When no context is available (N = 1), embeddings are scattered throughout the latent space, with substantial overlap across classes. As context increases, the model progressively refines the embeddings, leading to clearer separation. With sufficient context (N = 10), the ten classes emerge as well-separated clusters with minimal overlap. 6.2. Explainable Set Anomaly Detection Set-input networks like Deep Sets and Set Transformer have been applied to Set Anomaly Detection [4, 5], task where model receives set of images and must identify any anomalous image. Anomalous images stand out within set by either lacking an element that is present in most of the other 19 images or including an element that is absent from the majority. While Deep Sets and Set Transformers can identify such anomalous images, they fail to explain why an image is considered anomalous, highlighting image regions that influenced classification. In this experiment, we demonstrate that CSTs outperform Deep Sets and Set Transformers in Set Anomaly Detection and also offer reliable explanations of model predictions. CelebA [7] is dataset containing 202,599 facial images across 10,177 subjects. Each image is annotated with 40 binary attributes, which describe physical characteristics (e.g., oval face), facial expressions (e.g., smiling), and the presence of accessories (e.g., eyeglasses). We utilize CelebA attributes to form image sets. Each image set is obtained by selecting majority of images that exhibit two randomly chosen attributes (normal images), along with minority of images in which both these attributes are absent (anomalous images). The total number of images in set, as well as the percentage of anomalous images, varies across sets. Sets without any anomalies are also allowed. We train CST network, two Set Transformers (ST-S and ST-L), and Deep Sets network (DS) to predict, for each image in set, whether it is anomalous or not with respect to the set. This corresponds to binary classification task. The experiment is conducted at the original CelebA resolution of 178 218 pixels, without any cropping. Details on the tested architectures and the training hyperparameters can be found in Appendix B. Anomaly % Set size ST-S CST 27.8M 27.3M 28.3M 19.1M params. params. params. params. ST-L DS 0.1 0.2 0.3 0. 10 20 40 10 20 40 10 20 40 10 20 40 0.8133 0.8300 0.8326 0.8886 0.8966 0. 0.9182 0.9261 0.9272 0.8991 0.9170 0.9224 0.7878 0.8105 0.8188 0.8791 0.8843 0.8896 0.9158 0.9215 0.9231 0.8891 0.9084 0. 0.8052 0.8252 0.8302 0.8880 0.8948 0.8970 0.9141 0.9215 0.9221 0.8918 0.9122 0.9185 0.7273 0.7456 0.7505 0.8160 0.8286 0. 0.8455 0.8662 0.8740 0.8535 0.8760 0.8866 Table 3: Set Anomaly Detection. AUPRC at various input set sizes and anomaly prevalence rates. DS has fewer parameters than other architectures as larger DS models (with more Deep Sets layers) exhibit significant convergence issues. 20 Results, presented in Table 3, demonstrate that CSTs consistently outperform competing models across various set sizes and anomaly percentages. Furthermore, CSTs can generate interpretable explanations for model predictions. Figure 3 illustrates some image sets with GradCAMs overlaid on anomalous images, providing visualizations for both CST and ST-L. CST explanations are intuitive and meaningful. For example, in the first set, attention is drawn to the head and mouth, as anomalous images represent individuals without hat and not smiling. On the other hand, ST-L GradCAMs fail to provide meaningful explanations. This is not unexpected, given that calculating Grad-CAMs for Set Transformer involves linearizing the entire stack of Set Attention Blocks, which is the network component responsible for modeling the contextual relationships between set images. In contrast, CSTs include only one linear layer after the last SetConv2D block as the contextual relationships between images in set are fully modeled in the stack of SetConv2D blocks, jointly with feature extraction. 6.3. Pre-training CSTs for Transfer Learning In the image processing domain, Deep Sets and Set Transformers are typically trained from scratch, as in [1]. Pre-trained models are not available, and Transfer Learning on these architectures has not been explored yet. In this section, we demonstrate that CSTs can be pre-trained on largescale datasets and adapted to new tasks in Transfer Learning scenarios. To support this claim, we introduce and publicly release for download CST-153, Convolutional Set Transformer pre-trained on the ImageNet dataset [9] for the Contextualized Image Classification task. Section 6.3.1 provides an overview of the CST-15 architecture and its performance on the ImageNet validation set, while Section 6.3.2 presents the results of challenging Transfer Learning task: event recognition from personal photo albums. 6.3.1. CST-15 Trained on ImageNet Table 6 summarizes the CST-15 architecture and compares it to VGG19 [40], which we selected as reference due to its similar convolutional structure. Unlike VGG-19, which relies exclusively on standard convolutions, CST-15 employs SetConv2D blocks, allowing for concurrent context modeling and feature extraction. Moreover, CST-15 employs Global Average Pooling followed by single classification layer, instead of the flattening and fully 3https://github.com/chinefed/convolutional-set-transformer 21 CST28M params. VGG-19 144M params. Input: 224 224 RGB images Conv2D(64, 3) 2 Maxpool Conv2D(128, 3) 2 Maxpool SetConv2D(256, 3) 2 Conv2D(256, 3) Maxpool SetConv2D(512, 3) 4 Conv2D(512, 3) 4 Maxpool SetConv2D(512, 3) 4 Conv2D(512, 3) 4 GAP - - Maxpool FC(1000) Softmax Flatten FC(4096) FC(4096) Figure 6: CST-15 and VGG-19 architectures. SetConv2D blocks and Conv2D layers specified with (nf ilters, kernel size). All convolutions are implemented with same padding. In SetConv2D blocks, attention heads have dimension 64. Maxpooling is implemented with pool size 2. SetConv2D blocks and Conv2D layers are followed by ReLU6 activation for CST-15 and ReLU for VGG-19. Model # Params. 2 3 4 5 CST-15 VGG-19 28M 144M 71.37 88.42 92.71 94.62 95.55 - 71.24 - - - Figure 7: CST-15 and VGG-19 performance on the ImageNet validation set. Single-crop Classification Accuracy (%) as function of the input set size. Figure 8: Event recognition from personal photos, PEC dataset. We report the macroaveraged F1 Score as function of the input set size. Both CST-15 and VGG-19 perform classification at the image level, but while VGG-19 can only processes each image in set independently of the others, CST-15 leverages the contextual information within the set to improve its image-level classification accuracy. connected layers used in VGG-19, resulting in significantly smaller model size (28M vs. 144M parameters). Finally, CST-15 uses ReLU6 activations [41], which improve robustness in low-precision environments by capping activation values at 6 [42, 43]. We train CST-15 from scratch for the CIC task, using Combinatorial Training at 224 224 resolution. The CT hyperparameters nmin and nmax are set to 1 and 2, respectively. This setup accelerates convergence on complex dataset like ImageNet and reduces the memory footprint, as the model is trained only on small setssingle images and pairs of imagesyet it still 22 generalizes to arbitrarily large sets at inference time. We refer to Appendix for detailed description of the training setup. Table 6 compares the CIC performance of CST-15 on the ImageNet validation set with the single-image classification performance of VGG-194. For set size of 1, CIC reduces to standard single-image classification, where CST-15 already outperforms VGG-19, despite the lower parameter count. Furthermore, while VGG-19 operates only on individual images, CST-15 can handle arbitrarily large input sets. By exploiting contextual relationships among images in set, CST-15 achieves consistent performance gains as the set size increases. As discussed in Section 6.2, unlike Deep Sets and Set Transformers, CSTs are readily compatible with standard explainability tools used for CNNs. Appendix presents qualitative evaluation of CST-15s explainability on the ImageNet dataset. 6.3.2. Transfer Learning on the PEC dataset In this section, we show that CSTs pre-trained on large-scale datasets can be adapted to new set-learning tasks via standard Transfer Learning, similarly to CNNs. As case study, we consider event recognition from personal photos, where images are grouped into albums documenting individual events. For this purpose, we use the PEC dataset [10] which includes 807 personal photo albums, totaling 61,364 images. Each album, captured by single photographer, documents specific event and is labeled with one of 14 categories such as birthday, concert, or road trip. All images are centercropped and resized to 224 224 pixels. Notably, PEC is challenging dataset, often containing cluttered or irrelevant images to event recognition, such as accidental shots, close-ups of individuals, and photos of random objects. Figure 9 shows some image sets drawn from the PEC dataset. We assess CIC task, where each individual photo must be assigned the event label of its album based both on its own content and on contextual information from the other photos in the album. This formulation is substantially more challenging than album-level classification which, like SC, collapses all image representations into single album-level representation, loosing photo-specific information prior to prediction. By contrast, CIC forces the 4We obtain the VGG-19 ImageNet weights from TensorFlow. 23 Figure 9: Four image sets drawn from the PEC dataset. Each row corresponds to an image set, with all images in set drawn from the same personal album. From top to bottom, the event categories are Christmas, Cruise, Childrens Birthday, and Wedding. Notably, some images are irrelevant to the respective event category, introducing noise. In the figure, faces are masked to comply with the datasets usage policies, although the experiment was conducted on the original, unmasked images. network to preserve distinct yet contextualized representations for each image, encoding the both the images unique content and the broader album context. We compare the performance of our CST-15 model, the first set-input encoder pre-trained on ImageNet, with that of VGG-19 applied in parallel to each image in set. For Transfer Learning, we adapt both CST-15 and VGG-19 by replacing their original 1000-way ImageNet classifier with new layer for the 14 PEC classes, freezing the backbones and training only the final classifier. To ensure fair comparison, CST-15 is trained using the Set-free Transfer Learning scheme introduced in Section 5. Consequently, during the Transfer Learning phase, both CST-15 and VGG-19 are exposed only to individual images (i.e., singleton sets) without any context. Nevertheless, at inference time, CST-15 can process arbitrarily large sets of images from the same personal album, classifying each image with increasing accuracy as the set size grows, since it has learned to exploit contextual relationships among images during its ImageNet pre-training (see Figure 9). In contrast, VGG-19s performance remains unaffected by the set size, since it does not support set-based inputs and thus can only process each image in set independently, without leveraging contextual information from the other images. Notably, CST-15 outperforms VGG-19 even when the set size is 1, i.e., when both models receive single image as input without any context. Additional details about the experiment are available in Appendix B. 24 7. Conclusion We introduce the Convolutional Set Transformer (CST), deep-learning architecture for inference tasks on image sets. Unlike Deep Sets and Set Transformer, CST models the contextual relationships between images in set directly within convolutional blocks, jointly with feature extraction. Thanks to this, CST delivers superior performance across diverse tasks and datasets and, different from competing architectures, allows for seamless use of explainability tools for CNNs. As concrete contribution, we present CST15, the first set-input network pre-trained on the ImageNet dataset, which we release publicly. We further show that CST-15 can be readily adapted to new tasks via Transfer Learning. Looking ahead, future research should explore the many potential applications of CSTs in domains where data is naturally organized as sets of images, including medical imaging, surveillance, e-commerce, and social media. References [1] J. Collenne, R. Iguernaissi, S. Dubuisson, D. Merad, Reset: residual set-transformer approach to tackle the ugly-duckling sign in melanoma detection, in: ICIP, IEEE, 2024, pp. 31863191. [2] S. Black, R. Souvenir, Multi-view classification using hybrid fusion and mutual distillation, in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024, pp. 270280. [3] A. B. Khajwal, C.-S. Cheng, A. Noshadravan, Post-disaster damage classification based on deep multi-view image fusion, Computer-Aided Civil and Infrastructure Engineering 38 (4) (2023) 528544. [4] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov, A. J. Smola, Deep sets, NeurIPS 30 (2017). [5] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, Y. W. Teh, Set transformer: framework for attention-based permutation-invariant neural networks, in: ICML, PMLR, 2019, pp. 37443753. [6] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-cam: Visual explanations from deep networks via gradient-based localization, in: ICCV, 2017. 25 [7] Z. Liu, P. Luo, X. Wang, X. Tang, Deep learning face attributes in the wild, in: ICCV, 2015. [8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin, Attention is all you need, NeurIPS (2017). [9] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual recognition challenge, International journal of computer vision 115 (2015) 211252. [10] L. Bossard, M. Guillaumin, L. Van Gool, Event recognition in photo collections with stopwatch hmm, in: ICCV, 2013. [11] Z. Chen, X. Zhu, D. Su, J. C. Chuang, Stacking deep set networks and pooling by quantiles, in: ICML, 2024. [12] E. Wagstaff, F. Fuchs, M. Engelcke, I. Posner, M. A. Osborne, On the limitations of representing functions on sets, in: ICML, PMLR, 2019, pp. 64876494. [13] C. Bueno, A. Hylton, On the representation power of set pooling networks, NeurIPS 34 (2021) 1717017182. [14] E. Wagstaff, F. B. Fuchs, M. Engelcke, M. A. Osborne, I. Posner, Universal approximation of functions on sets, Journal of Machine Learning Research 23 (151) (2022) 156. [15] N. Naderializadeh, J. F. Comer, R. Andrews, H. Hoffmann, S. Kolouri, Pooling by sliced-wasserstein embedding, NeurIPS 34 (2021) 33893400. [16] S. Bartunov, F. B. Fuchs, T. P. Lillicrap, Equilibrium aggregation: Encoding sets via optimization, in: Uncertainty in Artificial Intelligence, PMLR, 2022, pp. 139149. [17] Y. Zhang, J. Hare, A. Prügel-Bennett, Fspool: Learning set representations with featurewise sort pooling, arXiv preprint arXiv:1906.02795 (2019). [18] C. R. Qi, H. Su, K. Mo, L. J. Guibas, Pointnet: Deep learning on point sets for 3d classification and segmentation, in: CVPR, 2017, pp. 652660. [19] C. R. Qi, L. Yi, H. Su, L. J. Guibas, Pointnet++: Deep hierarchical feature learning on point sets in metric space, NeurIPS 30 (2017). [20] H. Maron, O. Litany, G. Chechik, E. Fetaya, On learning sets of symmetric elements, in: International conference on machine learning, PMLR, 2020, pp. 67346744. [21] H. Su, S. Maji, E. Kalogerakis, E. Learned-Miller, Multi-view convolutional neural networks for 3d shape recognition, in: ICCV, 2015, pp. 945953. [22] X. Wei, R. Yu, J. Sun, View-gcn: View-based graph convolutional network for 3d shape analysis, in: CVPR, 2020, pp. 18501859. [23] Q. Liang, Q. Li, L. Zhang, H. Mi, W. Nie, X. Li, Mhfp: Multi-view based hierarchical fusion pooling method for 3d shape recognition, Pattern Recognition Letters 150 (2021) 214220. [24] Z. Liu, Y. Zhang, J. Gao, S. Wang, Vfmvac: View-filtering-based multiview aggregating convolution for 3d shape recognition and retrieval, Pattern Recognition 129 (2022) 108774. [25] X. Yang, Q. Guo, W. Chen, M. Song, Webly supervised 3d shape recognition, Pattern Recognition 158 (2025) 110982. [26] Y. Lin, X. Dou, X. Luo, Z. Wu, C. Liu, T. Luo, J. Wen, B. W.-k. Ling, Y. Xu, W. Wang, Multi-view diabetic retinopathy grading via cross-view spatial alignment and adaptive vessel reinforcing, Pattern Recognition 164 (2025) 111487. [27] L. Sun, J. Wang, Z. Hu, Y. Xu, Z. Cui, Multi-view convolutional neural networks for mammographic image classification, IEEE Access 7 (2019) 126273126282. [28] X. Liu, F. Hou, H. Qin, A. Hao, Multi-view multi-scale cnns for lung nodule type classification from ct images, Pattern Recognition 77 (2018) 262275. [29] G. Van Tulder, Y. Tong, E. Marchiori, Multi-view analysis of unregistered medical images using cross-view transformers, in: Medical Image Computing and Computer Assisted InterventionMICCAI 2021: 24th 27 International Conference, Strasbourg, France, September 27October 1, 2021, Proceedings, Part III 24, Springer, 2021, pp. 104113. [30] Y. Zhang, Y. Cao, T. Zhang, W. Shen, Attention fusion reverse distillation for multi-lighting image anomaly detection, arXiv preprint arXiv:2406.04573 (2024). [31] T.-B. Do, H.-H. Nguyen, H. Vu, T.-L. Le, et al., Plant identification using score-based fusion of multi-organ images, in: 2017 9th International conference on knowledge and systems engineering (KSE), IEEE, 2017, pp. 191196. [32] S. H. Lee, C. S. Chan, P. Remagnino, Multi-organ plant classification based on convolutional and recurrent neural networks, IEEE TIP 27 (9) (2018) 42874301. [33] M. Seeland, P. Mäder, Multi-view classification with convolutional neural networks, Plos one (2021). [34] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: large-scale hierarchical image database, in: 2009 IEEE conference on computer vision and pattern recognition, Ieee, 2009, pp. 248255. [35] C. Fellbaum, WordNet: An electronic lexical database, MIT press, 1998. [36] P. Chrabaszcz, I. Loshchilov, F. Hutter, downsampled variant of imagenet as an alternative to the CIFAR datasets, arXiv preprint arXiv:1707.08819 (2017). [37] Y. Le, X. Yang, Tiny imagenet visual recognition challenge (2015). [38] A. Krizhevsky, Learning multiple layers of features from tiny images (2009). [39] J. L. Ba, Layer normalization, arXiv preprint arXiv:1607.06450 (2016). [40] K. Simonyan, A. Zisserman, Very deep convolutional networks for largescale image recognition, in: ICLR, 2015. [41] A. Krizhevsky, Convolutional deep belief networks on CIFAR-10 (2010). URL https://www.cs.toronto.edu/kriz/conv-cifar10-aug2010. pdf 28 [42] A. G. Howard, Mobilenets: Efficient convolutional neural networks for mobile vision applications, arXiv preprint arXiv:1704.04861 (2017). [43] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen, Mobilenetv2: Inverted residuals and linear bottlenecks, in: CVPR, 2018, pp. 45104520. [44] D. P. Kingma, J. Ba, Adam: method for stochastic optimization, in: ICLR, 2015. [45] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, S. Xie, convnet for the 2020s, in: CVPR, 2022, pp. 1197611986. [46] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: CVPR, 2016, pp. 770778. [47] K. He, X. Zhang, S. Ren, J. Sun, Identity mappings in deep residual networks, in: ECCV, 2016, pp. 630645. A. Ablation Study on Combinatorial Training Mirroring Table 2 in the main paper, which reports the performance of CSTs and baseline models trained with Combinatorial Training for the CIC and SC tasks across multiple benchmarks (ImageNet64x64, Tiny ImageNet, CIFAR-10, and CIFAR-100), Table A1 summarizes the results of the same models when trained without Combinatorial Training.5 Model ImageNet64x64 4 3 2 1 1 Tiny ImageNet 4 3 2 5 1 CIFAR 10 3 4 5 1 CIFAR 100 3 4 5 Contextualized Image Classification (CIC) 76.79 70.23 70.33 64.36 87.30 87.79 87.28 64.59 63.84 65. 49.77 49.28 51.21 47.44 47.11 41.84 73.61 73.66 74.20 27.98 28.92 25.19 79.53 79.40 79.87 ST-S 27.85 ST-L 27.22 DS 28. CST 29.72 52.83 67.54 76.03 80.97 29.56 49.61 61.94 71.40 +0.97 +1.62 +2.24 +1.83 +1.10 +0.64 +2.17 +0.06 -0.07 75.88 75.73 90.84 96.28 98.07 98.95 40.55 63.43 74.80 82.82 89.10 -1.81 +4.08 +3.05 +2.20 +1.69 +0.95 +3.52 +5.73 +4.21 +2.74 +4.25 84.85 97.83 83.67 98.00 83.57 97.88 61.88 71.47 77.69 71.65 71.28 60.36 71.29 54.35 Set-level Classification (SC) CST + SF 29.72 52.89 67.60 76.19 81.14 29.56 49.65 62.11 72.92 76.17 75.73 91.00 96.36 98.13 99.00 40.55 63.71 74.61 83.25 88.84 +0.97 +0.95 +0.92 +0.52 -0.34 +0.64 +2.06 -0.42 +1.22 -2.12 +4.08 +3.36 +1.95 +1.25 +0.73 +3.52 +5.40 +3.74 +2.61 +3.24 85.60 84.15 85.38 CST + LF 32.44 56.33 70.90 79.91 84.81 33.15 54.81 69.04 77.52 84.60 75.43 90.75 96.15 98.48 99.11 41.71 64.93 76.99 84.51 90.96 +3.47 +4.05 +4.30 +3.71 +2.86 +5.59 +7.96 +7.45 +5.34 +6.20 +3.71 +2.69 +1.98 +1.65 +0.67 +5.39 +7.04 +5.67 +4.74 +5.30 85.66 83.20 83.82 83.48 47.59 62.53 71.70 78.29 71.65 71.28 70.62 47.18 71.29 66.45 42.50 CNN + LF 28.10 ST-S + LF 28.97 ST-L + LF 28.53 DS + LF 27.86 27.98 79.75 74.14 73.85 28.92 79.74 75.67 81.48 25. ST-S + SF 27.85 ST-L + SF 27.22 DS + SF 28.75 80.73 81.75 81.95 80.89 98.44 98.05 98.10 97.88 27.56 27.54 26.62 26.23 74.20 75.56 76.20 74.30 94.17 93.72 93.96 93. 71.32 67.84 69.63 66.99 78.40 77.29 74.11 77.68 72.18 69.57 68.32 69.49 61.59 59.70 59.02 59.34 65.10 66.46 66.60 65.08 49.87 52.28 51.39 50. 46.85 45.33 44.88 45.58 71.72 71.50 71.47 71.06 88.06 87.60 87.79 87.66 96.83 96.53 96.22 96.57 36.32 34.50 35.10 32.84 57.89 55.45 57.11 54. 79.77 78.30 77.69 76.52 97.77 98.27 98.16 93.69 94.08 93.75 70.59 68.65 68.49 80.08 77.35 77.46 37.03 34.38 34. 57.70 56.14 55.57 96.22 96.33 96.38 93.66 94.17 94.41 70.87 69.17 69.60 49.95 49.26 51.94 64.74 64.03 66. 37.03 34.38 34.65 96.14 96.22 96.88 87.27 87.64 87.48 58.31 56.29 56.11 80.64 77.65 78.99 77.51 72. 60.45 55.40 Table A1: Top-1 Test Accuracy (%) as function of Set Size (up to 5 images). All the models are trained without CT. SF and LF are acronyms for Score Fusion and Late Fusion, respectively. Rows labeled as report the absolute difference in accuracy between CST models and the best-performing baseline. Model ImageNet64x64 4 3 2 1 1 Tiny ImageNet 4 3 2 5 1 CIFAR 10 3 4 5 1 CIFAR 100 3 2 5 Contextualized Image Classification (CIC) CST +32.23 +20.59 +13.21 +9.35 +7.47 +32.07 +20.68 +19.73 +13.54 +14.85 +6.42 +3.54 +1.25 +0.84 +0.21 +22.05 +11.63 +10.17 +6.93 +3.63 ST-S +27.76 +20.15 +13.84 +10.61 +8.35 +11.65 +9.21 +5.17 +3.41 +2.46 +10.57 +6.38 +3.56 +2.20 +1.12 +13.93 +9.93 +6.84 +4.07 +3.71 ST-L +31.23 +21.31 +14.33 +9.90 +7.77 +7.05 +9.91 +8.66 +8.47 +5.20 +10.10 +4.45 +1.68 +1.18 +0.11 +18.91 +10.47 +6.98 +4.43 +3.06 DS +23.69 +13.98 +9.75 +6.83 +5.03 +20.76 +17.71 +15.62 +11.86 +11.63 +9.47 +5.00 +2.78 +1.99 +0.91 +12.70 +6.91 +5.34 +3.30 +1.28 Set-level Classification (SC) CST + SF +32.23 +20.67 +13.65 +9.63 +7.80 +32.07 +21.77 +20.64 +12.73 +14.87 +6.42 +3.43 +1.38 +0.75 +0.39 +22.05 +12.31 +11.12 +7.24 +4.40 ST-S + SF +27.76 +20.20 +13.95 +10.93 +8.65 +11.65 +9.94 +5.05 +4.42 +3.78 +10.57 +6.44 +3.57 +2.39 +1.26 +13.93 +9.02 +6.05 +3.98 +3.39 ST-L + SF +31.23 +21.27 +14.34 +9.88 +7.59 +7.05 +9.90 +9.43 +8.54 +4.83 +10.10 +4.60 +1.53 +1.23 -0.28 +18.91 +10.13 +6.36 +4.35 +2.59 DS + SF +23.69 +13.30 +9.09 +6.89 +4.71 +20.76 +17.51 +15.94 +9.93 +12.25 +9.47 +4.87 +2.26 +1.74 +0.91 +12.70 +7.18 +4.30 +2.86 +0.71 CST + LF +26.91 +16.55 +11.33 +7.12 +5.74 +12.73 +8.25 +5.43 +5.20 +2.84 +7.28 +3.28 +1.79 +0.84 +0.62 +14.05 +8.04 +6.85 +4.83 +1.53 CNN + LF +24.56 +16.36 +10.00 +7.16 +5.44 +12.74 +8.60 +6.12 +3.78 +3.64 +9.05 +4.47 +2.39 +1.70 +0.68 +12.22 +9.35 +5.26 +4.20 +3.97 ST-S + LF +24.37 +15.11 +11.66 +8.67 +6.32 +12.82 +14.12 +9.65 +8.05 +6.28 +10.07 +5.54 +3.47 +2.33 +1.24 +17.97 +11.52 +12.57 +7.20 +6.51 ST-L + LF +23.03 +15.08 +10.57 +7.47 +6.86 +14.39 +10.81 +8.32 +7.17 +6.92 +9.39 +5.08 +2.23 +1.81 +0.92 +18.01 +10.05 +8.60 +5.37 +3.13 DS + LF +26.63 +16.64 +11.17 +7.83 +5.25 +11.40 +7.81 +5.88 +3.81 +2.15 +10.32 +4.65 +2.14 +1.80 +1.26 +22.29 +12.42 +11.37 +7.61 +6.35 Table A2: Relative increase (%) in Top-1 Test Accuracy as function of Set Size (up to 5 images) when CT is used compared to conventional training. 5When CT is not applied, we create sets of three images from the same class before the training process begins and use these sets to train the model in every epoch. 30 Even when CT is not applied, CSTs outperform the baselines in 55 of 60 test cases. Table A2 shows, for all models, the relative increase in test accuracy when CT is used as opposed to conventional training. CT is consistently beneficial, with relative improvements often reaching double digits. As result, CT proves to be an effective training strategy for CSTs and general set-input networks. B. Additional Details About the Experiments In this appendix, we provide additional details about the experiments described in the main paper, including the hyperparameters used for training. B.1 covers the classification experiments from Section 6.1 of the paper. B.2 focuses on Set Anomaly Detection, as discussed in Section 6.2. Finally, B.3 and B.4 provide further details on CST-15, presented in Section 6.3.1, and the Transfer Learning experiment from Section 6.3.2. B.1. Classification Experiments We train both CSTs and all the baseline modelsexcept for standard CNNs, which completely disregard set informationusing Combinatorial Training (with nmin = 2 and nmax = 5). The batch size is 256 for training on ImageNet64x64, 128 for Tiny ImageNet, and 64 for CIFAR-10 and CIFAR100. We utilize the Adam optimizer [44] (with β1 = 0.9 and β2 = 0.999) along with warm-up learning rate schedule. The learning rate starts at 1 104 and linearly increases over the first 5 epochs, reaching 3 104 when training on ImageNet64x64 and 5 104 when training on the other datasets. Notably, CT necessitates low learning rates as concurrent training on sets of different sizes makes the optimization process less stable. For all models, L2 regularization is applied with coefficient of 5 104. We implement gradient norm clipping for the Late Fusion models, at threshold of 5. When training CSTs, we impose 10% dropout probability on attention weights within SetConv2D blocks. For all models, training is terminated when the validation accuracy stops improving for 50 epochs, at which point we select the best-performing model so far. B.2. Explainable Set Anomaly Detection Table B1 presents the architectures tested in the Explainable Set Anomaly Detection experiment. Since it is not possible to have exactly the same number of parameters across different architectures, the CST is compared to 31 CST 27.8M params. ST-S 27.3M params. ST-L 28.3M params. DS 19.1M params. Input: 178 218 RGB images Conv2D(64, 3) Conv2D(64, 3) Maxpool Conv2D(128, 3) Conv2D(128, 3) Maxpool SetConv2D(256, 3) SetConv2D(256, 3) Conv2D(256, 3) Conv2D(256, 3) SetConv2D(512, 3) SetConv2D(512, 3) SetConv2D(512, 3) SetConv2D(512, 3) SetConv2D(512, 3) SetConv2D(512, 3) SetConv2D(512, 3) SetConv2D(512, 3) Maxpool Conv2D(512, 3) Conv2D(512, 3) Conv2D(512, 3) Conv2D(512, 3) Maxpool Conv2D(512, 3) Conv2D(512, 3) Conv2D(512, 3) Conv2D(512, 3) Maxpool GAP - SAB(512) 8 SAB(512) 9 DS(512) 1 FC(1) Sigmoid Table B1: Architectures tested in the Explainable Set Anomaly Detection experiment. SetConv2D blocks and Conv2D layers specified with (nf ilters, kernel size), All convolutions are implemented with same padding. In SetConv2D blocks, attention heads have dimension 64. Maxpool operations are implemented with pool size 2. For SAB and Deep Sets blocks, we report the dimension of the output space in parenthesis. In all architectures, we use ReLU activations. Set Transformer with more parameters (ST-L) and one with fewer parameters (ST-S). The Deep Sets model has fewer parameters than both CST and Set Transformers. We experimented with larger Deep Sets models (i.e., with more Deep Sets equivariant blocks), but they faced substantial convergence issues. We implement Set Attention Blocks and Deep Sets equivariant layers as described in Section 6.1.1 of the paper. The models are trained from scratch to detect anomalous images within sets derived from the CelebA dataset. This task is framed as binary classification, where the model predicts for each image whether it is normal or anomalous. 32 The construction of the sets follows the protocol detailed below: 1. randomly select two attributes from the 40 available in the CelebA annotations; 2. randomly select panomaly anomalous images that lack both attributes, where is the set size and panomaly is the anomaly prevalence rate; 3. randomly select panomaly normal images that exhibit both attributes. We use the default training, validation, and test splits of CelebA. The experiment is conducted at the original CelebA resolution of 178 218 pixels, without any cropping. During training, sets are continuously generated based on the protocol above, with set size of 10 and an anomaly prevalence rate that varies randomly between 0 and 0.4. We optimize with Adam (β1 = 0.9 and β2 = 0.999). The learning rate is set to 1104. We add L2 regularization with coefficient of 5 104. For all models, training is terminated when the validation AUPRC stops improving for 10 epochs, at which point we select the best-performing model so far. B.3. CST-15 trained on ImageNet We train CST-15 from scratch using Combinatorial Training at resolution of 224 224. Input pre-processing involves scaling pixel values to the [0, 1] range, and normalizing each channel with respect to the ImageNet dataset. For scale augmentation, we follow the multi-scale training approach outlined in [40]. We optimize with Adam [44] (with β1 = 0.9 and β2 = 0.999). The learning rate is initially set to 1 104 and reduced as needed in later training stages. We apply L2 regularization with coefficient of 0.1. The CT hyperparameters, nmin and nmax, are set to 1 and 2, respectively. The batch size is set to 320. The model is trained on single A100 GPU for 250 epochs. B.4. Transfer learning on the PEC dataset For each event class, we reserve six personal albums for validation. The official PEC test split is used for testing. Images are center-cropped and resized at 224 224. We adapt both CST-15 and VGG-19 by replacing their original 1000-way ImageNet classifier with new layer for the 14 PEC classes. The backbone networks are kept frozen and only the new classification layer is trained. Following the Set-free Transfer Learning scheme, during the training 33 of the CST-15 model, we input individual images without any set, just as we do for VGG-19. The learning rate is set to 1 103, the L2 regularization coefficient to 5 104, and the batch size to 64. We use Adam [44] for optimization (with β1 = 0.9 and β2 = 0.999). Training is terminated when the validation accuracy stops improving for 10 epochs, at which point we select the best-performing model so far. C. Explainability of CSTs CSTs are readily compatible with standard explainability tools designed for CNNs. Grad-CAMs [6] computed for CST-15 demonstrate remarkable accuracy, especially when considering the simplicity of the CST-15 architecture. Figure C1 presents qualitative comparison of Grad-CAMs generated for CST15 (28M params), ConvNeXt-XL (350M params) [45], ResNet50 (26M params) [46, 47], and VGG-19 (144M params) [40]. To ensure fair comparison, we feed isolated images without context to CST-15. Explanations are computed with respect to the ground truth class. CST-15 Grad-CAMs are more precise and focused compared to ResNet50 and VGG-19, and comparable or even better than ConvNeXt-XL explanations. Consider, for instance, the first image in Figure C1. It depicts space shuttle being transported by shuttle carrier aircraft. Even to the human eye, it is difficult to distinguish the shuttle from the carrier aircraft. However, the CST-15 explanation map accurately identifies the space shuttle, distinguishing it from the aircraft. In contrast, the Grad-CAMs generated for the other models are significantly less precise, highlighting coarse region that encompasses both the shuttle and the carrier aircraft. Importantly, when dealing with CSTs trained for CIC, Grad-CAMs should be computed at the penultimate SetConv2D block. As Figure C2 shows, explanations computed at the final SetConv2D block are negatively affected by the overwhelming importance of contextual information. Recall that CSTs evenly spread contextual information across the spatial dimensions. Figures C3, C4, and C5 present further Grad-CAMs computed for sets of 4 images fed to CST-15 (each row corresponds to an image set). 34 Figure C1: CST-15 Grad-CAMs as opposed to ConvNeXt-XL, ResNet50, and VGG-19 Grad-CAMs. For CST-15, we compute Grad-CAMs at the penultimate SetConv2D block. For the other models, we compute Grad-CAMs at the last layer before GAP or Flattening. 35 Figure C2: Grad-CAMs for an image set provided as input to CST-15, computed with respect to the ground truth class at various layers of the CST-15 architecture. 36 Figure C3: Grad-CAMs for image sets provided as input to CST-15 with respect to the ground truth class. Each row corresponds to set of images. Grad-CAMs are computed at the penultimate SetConv2D block. 37 Figure C4: Grad-CAMs for image sets provided as input to CST-15 with respect to the ground truth class. Each row corresponds to set of images. Grad-CAMs are computed at the penultimate SetConv2D block. 38 Figure C5: Grad-CAMs for image sets provided as input to CST-15 with respect to the ground truth class. Each row corresponds to set of images. Grad-CAMs are computed at the penultimate SetConv2D block."
        }
    ],
    "affiliations": [
        "Dep. of Computing Sciences, Bocconi University, Italy",
        "Dep. of Electronics, Information and Bioengineering, Politecnico di Milano, Italy"
    ]
}