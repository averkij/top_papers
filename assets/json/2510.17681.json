{
    "paper_title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
    "authors": [
        "Yuandong Pu",
        "Le Zhuo",
        "Songhao Han",
        "Jinbo Xing",
        "Kaiwen Zhu",
        "Shuo Cao",
        "Bin Fu",
        "Si Liu",
        "Hongsheng Li",
        "Yu Qiao",
        "Wenlong Zhang",
        "Xi Chen",
        "Yihao Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image editing has achieved remarkable progress recently. Modern editing models could already follow complex instructions to manipulate the original content. However, beyond completing the editing instructions, the accompanying physical effects are the key to the generation realism. For example, removing an object should also remove its shadow, reflections, and interactions with nearby objects. Unfortunately, existing models and benchmarks mainly focus on instruction completion but overlook these physical effects. So, at this moment, how far are we from physically realistic image editing? To answer this, we introduce PICABench, which systematically evaluates physical realism across eight sub-dimension (spanning optics, mechanics, and state transitions) for most of the common editing operations (add, remove, attribute change, etc). We further propose the PICAEval, a reliable evaluation protocol that uses VLM-as-a-judge with per-case, region-level human annotations and questions. Beyond benchmarking, we also explore effective solutions by learning physics from videos and construct a training dataset PICA-100K. After evaluating most of the mainstream models, we observe that physical realism remains a challenging problem with large rooms to explore. We hope that our benchmark and proposed solutions can serve as a foundation for future work moving from naive content editing toward physically consistent realism."
        },
        {
            "title": "Start",
            "content": "PICABENCH: HOW FAR ARE WE FROM PHYSICALLY REALISTIC IMAGE EDITING? Yuandong Pu1,2 Le Zhuo3,4 Songhao Han5 Jinbo Xing6 Kaiwen Zhu1,2 Shuo Cao2,7 Bin Fu2 Si Liu5 Hongsheng Li3 Yu Qiao2 Wenlong Zhang2 Xi Chen8 Yihao Liu2 1Shanghai Jiao Tong University 5Beihang University 6Tongyi Lab 2Shanghai AI Laboratory 3CUHK MMLab 7 USTC 8 The University of Hong Kong"
        },
        {
            "title": "4 Krea AI",
            "content": "5 2 0 2 0 2 ] . [ 1 1 8 6 7 1 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Image editing has achieved remarkable progress recently. Modern editing models could already follow complex instructions to manipulate the original content. However, beyond completing the editing instructions, the accompanying physical effects are the key to the generation realism. For example, removing an object should also remove its shadow, reflections, and interactions with nearby objects. Unfortunately, existing models and benchmarks mainly focus on instruction completion but overlook these physical effects. So, at this moment, how far are we from physically realistic image editing? To answer this, we introduce PICABench, which systematically evaluates physical realism across eight sub-dimension (spanning optics, mechanics, and state transitions) for most of the common editing operations (add, remove, attribute change, etc.). We further propose the PICAEval, reliable evaluation protocol that uses VLM-as-a-judge with per-case, region-level human annotations and questions. Beyond benchmarking, we also explore effective solutions by learning physics from videos and construct training dataset PICA100K. After evaluating most of the mainstream models, we observe that physical realism remains challenging problem with large rooms to explore. We hope that our benchmark and proposed solutions can serve as foundation for future work moving from naive content editing toward physically consistent realism. Project page: https://picabench.github.io"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in instruction-based image editing have brought remarkable progress (Wu et al., 2025a; Batifol et al., 2025; OpenAI, 2025; Google, 2025; Seedream et al., 2025; Liu et al., 2025; Cai et al., 2025). In particular, with the emergence of unified multi-modal models (Deng et al., 2025; Lin et al., 2025; Wu et al., 2025b), they can seamlessly follow natural language instructions and produce visually compelling, semantically coherent edits. These systems have demonstrated strong generalization capabilities across diverse domains, establishing new standard for controllable and high-quality image manipulation. However, the realism of image editing depends not only on semantic accuracy but also on the correct rendering of physical effects. Even simple operations like object addition or removal often trigger complex interactions with lighting, shadows, and object support in the scene. Existing benchmarks overlook this limitation by solely emphasizing semantic fidelity and visual consistency. Although some recent benchmarks (Wu et al., 2025c; Li et al., 2025) attempt to probe scientific-plausible editing capabilities, their test cases diverge from common user-edit scenarios but focus on scientific domains with specific physical or chemistry knowledge. Consequently, we lack clear understanding of how far we are from physically realistic image editing. To address this gap, we introduce PICA (PhysICs-Aware) Bencha diagnostic benchmark designed to evaluate physical realism in image editing beyond semantic fidelity. Drawing on common requireCorresponding Authors"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Challenging cases from PICABench. Despite providing instruction-aligned outputs, current SoTA models still struggle with generating physically realistic edits, resulting in unharmonized lighting, deformation, or state transitions with common editing operations. ments in real-world editing applications Taesiri et al. (2025), we categorize physical consistency into three intuitive dimensions that are often overlooked in typical editing tasks: Optics, Mechanics, and State Transition. These dimensions were selected to reflect common but under-penalized error types, such as unrealistic lighting effects, impossible object deformations, or implausible state changes. Together, they span eight sub-dimensions, each defined by concrete, checkable criteria: Optics includes light propagation, reflection, refraction, and light-source effects; Mechanics captures deformation and causality; and State Transition addresses both global and local state changes. This fine-grained taxonomy facilitates systematic assessment of whether edited images adhere to principles such as lighting consistency, structural plausibility, and realistic state transitions. Together, it enables comprehensive evaluation and targeted diagnosis of physics violations in image editing models. With the carefully curated test cases, evaluating the physical correctness remains challenging. We introduce PICAEval, reliable and interpretable protocol tailored for physics-aware assessment. While existing VLM-as-Judge setups (Wu et al., 2025c; Niu et al., 2025; Sun et al., 2025; Zhao et al., 2025) offer convenient way to automate evaluation, they typically rely on general prompts without grounding in physical principles. As result, these setups often lack sensitivity to nuanced physical violations and may produce hallucinated judgments when faced with subtle or localized cues. Facing this challenge, PICAEval adopts targeted, per-example Q&A aligned with specific physical sub-dimensions, substantially improving diagnostic accuracy. To further reduce hallucination, we incorporate grounded human-annotated key regions (e.g., reflection surfaces, contact interfaces), directing the models attention to physically relevant evidence. This protocol yields high agreement with human assessments, offering reliable measurement for physical correctness. Beyond evaluation, we provide strong baseline by learning physics from videos. Specifically, we present PICA-100K, synthetic dataset of 100k editing examples constructed from videos. Prior work (Yu et al., 2025; Chen et al., 2025; Chang et al., 2025; Cao et al., 2025a) has shown that editing pairs derived from videos can enhance the quality and robustness of editing models. Motivated by recent advances in video generation approaching world-simulator (Wan et al., 2025), we design an automatic pipeline that integrates text-to-image model as scene renderer and an image-to-video model as state-transition simulator. From the generated videos, we extract temporally coherent editing pairs and further recalibrate multi-level editing instructions using GPT-5. Our experiments shows that finetuning on PICA-100K significantly improves the baseline models capability to generate physically realistic editing results without sacrificing semantic quality."
        },
        {
            "title": "Preprint",
            "content": "We benchmark 11 openand closed-source image editing models across diverse architectures and scales. PICABench comprehensively distinguishes models based on their level of physical awareness, while PICA-100K effectively improves model performance. As shown in Fig. 1, modeling physical realistic transformations is still challenging for current SoTA models, which underlines the significance of advancing from semantic editing toward physically grounded image manipulation in the future. Our main contributions could be summarized as follows. We introduce PICABench, comprehensive and fine-grained benchmark for physics-aware image editing. It covers diversified physical effects (eight sub-dimensions) and includes the great majority of commonly required editing operations in practical applications. We propose PICAEval, region-aware, VQA-based evaluation protocol that incorporates human-annotated key regions to provide interpretable and reliable assessments for physical correctness, improving robustness to subtle errors compared to general scoring prompts. We construct PICA-100K, large-scale dataset derived from synthetic videos, and show that fine-tuning existing models (e.g. FLUX.1 Kontext) on this dataset effectively enhances their physical consistency while preserving semantic fidelity."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 INSTRUCTION-BASED IMAGE EDITING MODELS Recent advances in instruction-based image editing have led to substantial progress in controllable and diverse visual manipulation. Prior approaches implement image editing in training-free manner (Yang et al., 2023; Pan et al., 2023; Couairon et al., 2022). Recent training-based methods such as HiDream-E1.1 (Cai et al., 2025), Step1X-Edit (Liu et al., 2025), FLUX.1 Kontext (Batifol et al., 2025), and Qwen-Image-Edit (Wu et al., 2025a) improve edit quality, responsiveness, and instruction alignment, while unified frameworks (e.g., Bagel (Deng et al., 2025), OmniGen2 (Wu et al., 2025b), UniWorld-V1 (Lin et al., 2025)) integrate instruction-following, visual reasoning, and multitask learning to support diverse tasks like free-form manipulation, future-frame prediction, multiview synthesis, segmentation, and composition. Lumina-OmniLV (Pu et al., 2025) proposes unified multi-task framework that can handle diverse visual tasks with diffusion prior. Closed-source systems (e.g., GPT-Image-1 (OpenAI, 2025), Seedream 4.0 (Seedream et al., 2025), Nano-Banana (Google, 2025)) further demonstrate strong user-intent alignment and high visual fidelity across text-to-image and image-to-image workflows. However, despite these gains, most approaches prioritize semantic and perceptual quality and often neglect physical constraints, leading to artifacts such as unrealistic shadows, refractions, and deformations, underscoring the need for physics-aware editing. 2.2 INSTRUCTION-BASED IMAGE EDITING BENCHMARKS Instruction-based image editing benchmarks have evolved from early reliance on semantic (DINO, CLIP (Zhang et al., 2023; Wang et al., 2023; Ma et al., 2024)) and pixel-level metrics, which capture similarity but miss fine-grained semantic alignment, to modern VLM-as-a-Judge evaluations (Wu et al., 2025c; Niu et al., 2025; Zhao et al., 2025; Sun et al., 2025; Ye et al., 2025; Liu et al., 2025; Cao et al., 2025b; Zhuo et al., 2025) that use vision-language models to rate instruction adherence, perceptual quality, and realism across diverse, complex prompts. While these LLM-based approaches enable general multi-dimensional scoring, they are prone to overlooking physically implausible edits (e.g., unrealistic lighting, deformations, or object interactions) and can hallucinate, allowing visually appealing yet inconsistent outputs to score well. To close this gap, we introduce physicsaware benchmark and the PICAEvala region-grounded, QA-based metric that evaluates physical consistency through localized, interpretable assessments anchored to specific regions of interest."
        },
        {
            "title": "3 METHOD",
            "content": "In this section, we first give an overall introduction of PICABench, benchmark structured to evaluate physical realism in image editing. We then dive into the construction steps, begin with the data curation pipeline, which pairs diverse images with multi-level editing instructions. Next, we present PICAEval, region-grounded evaluation protocol for reliable assessment. Finally, we"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Statistics Analysis of PICABench. PICABench is comprehensive benchmark designed to evaluate physical realism of image editing models across eight sub-dimentinons. Fig. 2(a) shows distribution of QA pairs. Fig. 2(b) presents words length distribution of editing instruction across three levels of prompt. Fig. 2(c) provides perspective on overall composition of PICABench. propose PICA-100K, synthetic dataset built from videos, and show how fine-tuning on it provides strong baseline for improving physics-aware editing."
        },
        {
            "title": "3.1 PICABENCH",
            "content": "We introduce the task coverage and overall statistics of PICABench. Our benchmark focuses on three core dimensions of physical realism: Optics, Mechanics, and State Transition, which reflect common yet overlooked failure modes such as unrealistic lighting, implausible deformations, and invalid state changes. As shown in Fig. 2(c) and Fig. 3, the benchmark includes 900 editing samples spanning these three dimensions, further divided into eight sub-dimensions with concrete and checkable criteriaranging from optical effects, to mechanical plausibility, and to realistic state transitions. Optics. This category evaluates whether edited images follow the basic physical rules of light, including how it casts shadows, reflects from surfaces, bends through transparent materials, and interacts with light sources. Edits should produce shadows, reflections, refractions, and light-source effects that align with the scenes geometry and lightingmatching shadow direction and occlusion, enabling viewand shape-dependent reflections, ensuring smooth background distortion through transparent media, and maintaining consistent color, softness, and falloff for added light sources. These effects, while often subtle, are key to making edits appear natural and physically believable. Mechanics. This category evaluates whether edited objects remain mechanically and causally consistent with the scene. Deformation should follow material propertiesrigid objects must retain shape, while elastic ones deform smoothly with consistent texture and geometry. Causality covers broader range of physically plausible effects, including structural responses to force redistribution, agent reactions to added or removed stimuli, and environmental changes that alter object behavior, all of which must follow consistent physical or behavioral laws. State transition. This category evaluates whether environmental and material changes unfold in physically coherent manner, either across the entire scene or within localized regions. Global state transitions, such as changes in time of day, season, or weather, must update all relevant visual cues consistentlyranging from lighting and shadows to vegetation, surface conditions, and atmospheric effects. These changes require coordinated, scene-wide modifications that follow natural temporal or environmental progression. Local state transitions, on the other hand, involve targeted physical changes confined to specific objects or regions. These include phenomena such as wetting, drying, melting, burning, freezing, wrinkling, splashing, or fracturing. Edits must integrate smoothly with surrounding context, preserve material boundaries, and maintain plausible causal triggers."
        },
        {
            "title": "3.2 DATA CURATION",
            "content": "To enable reliable, fine-grained evaluation of physically realistic image editing, we curate benchmark entries that pair natural images with editing instructions explicitly designed to test physical consistency. Our data curation pipeline is aligned with the taxonomy in Sec. 3.1 and structured into two stages: Data Collection and Edit Instruction Construction. visual overview is shown in Fig. 4."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Overview of PICABench. We present illustrative examples from eight sub-dimensions. Key regions are annotated to help reduce hallucination for VLMs. Data collection. We begin by defining structured vocabulary mapped to the eight sub-dimensions. To broaden the coverage, we use GPT-5 to expand this vocabulary into rich keyword set encompassing materials, lighting contexts, and long-tail phenomena. We then use these keywords to retrieve candidate images from licensed and public sources. We prioritize visually diverse scenes that exhibit salient physical cues, such as directional lighting, transparent or reflective media, deformable objects, or phase-changeable substances. Human annotators filter duplicates and artifacts and tag applicable sub-dimensions for each image to support subsequent annotation. Instruction construction. Each retained image is paired with human-written natural language instruction that induces physics-relevant edit, grounded in the scenes physical affordances and designed to implicitly target specific sub-dimension. To assess not only whether models can follow surface-level commands but also whether they can internalize and apply physical knowledge under varying prompt conditions, we construct three levels of instruction complexity: superficial prompts that issue plain edit commands without explanation; intermediate prompts that include brief rationale grounded in physical rules; and explicit prompts that further describe the expected results of the edit. We use GPT-5 to expand each human-authored instruction into these three forms, followed by manual review to ensure clarity, factual correctness, and alignment with the visual context. For each sample, the benchmark retains canonical version of the instruction."
        },
        {
            "title": "3.3 PICAEVAL",
            "content": "Evaluating physically realistic image editing remains challenging. Unlike semantic fidelity or perceptual quality, physical realism is inherently contextual: it depends not only on the edited content but also on its alignment with the physical constraints implied by the original scene and instruction. Moreover, there is no reference image to serve as ground truth, and general prompting strategies such as Is this edit correct? often yield vague or hallucinated responses from VLMs. To address this, we introduce PICAEval, region-grounded, question-answering based metric designed to assess physical realism in modular, interpretable manner. Inspired by recent work (Chai et al., 2024; Han et al., 2025), PICAEval decomposes each evaluation instance into multiple region-"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Overall pipeline for benchmarks construction and evaluation. (ab) We enrich physics-specific keyword set and retrieve diverse candidate images. (cd) Human-written editing instructions are expanded into three levels of complexity using GPT-5. (e) Annotators mark physicscritical regions. (f) Spatially grounded yes/no questions are generated to evaluate physical plausibility. (g) During evaluation, VLMs answer each question with reference to the edited region. specific verification questions that can be reliably judged by VLM. Each benchmark entry is paired with curated set of spatially grounded yes/no questions designed to probe whether the edited image preserves physical plausibility within key regions. These questions are tied to observable physical phenomenasuch as shadows, reflections, object contact, or material deformationand are anchored to human-annotated regions of interest (ROIs). This design encourages localized, evidence-based reasoning and reduces the influence of irrelevant image content on the VLMs judgment. Evaluation pipeline. As illustrated in Fig. 4(ef), the evaluation proceeds as follows: (1) Annotators mark key regions in the input image where physics-critical evidence is expected to appear post-editing (e.g., reflective surfaces, deformation zones, cast shadows); (2) Using the edit instruction and region, GPT-5 generates set of 45 binary QA pairs per entry, which are then manually reviewed for clarity and coverage; (3) At test time, VLM (e.g., GPT-5) is prompted with the edited image, instruction, region, and question, and produces an answer constrained to the visible content within the region. PICAEval is computed as the proportion of questions for which the VLM answer exactly matches the reference label. Compared to direct prompting, this QA-based protocol offers three key advantages: (i) spatial grounding reduces hallucination, (ii) decomposition increases interpretability and robustness, and (iii) the format better mirrors how humans evaluate physical plausibilitythrough concrete, localized checks. We report quantitative comparisons and per-subdimension breakdowns to enable diagnostic analysis of physics-aware image editing capabilities in Sec. 4."
        },
        {
            "title": "3.4 STRONG BASELINE: LEARNING PHYSICAL REALISM FROM VIDEOS",
            "content": "To address the limitations identified in Sec. 3.1, we introduce PICA-100K, purely synthetic dataset designed to improve physics-aware image editing. Our decision to use fully generated data is driven by three primary motivations. First, prior work (Yu et al., 2025; Chen et al., 2025; Cao et al., 2025a; Chang et al., 2025) has demonstrated that constructing image-editing data from video is an"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: PICA-100K construction pipeline. We first curate structured prompts for scene and subject composition, refined by GPT-5 and rendered using FLUX.1-Krea-dev for text-to-image generation. Motion-based edit instructions are created via GPT-5 and applied using Wan2.2-14B to synthesize short videos depicting physical transformations. effective strategy for enhancing model performance, particularly for capturing real world dynamics. Second, building large-scale, real-world datasets tailored to physics-aware editing is prohibitively expensive and labor-intensive. Third, the rapid progress in generative modeling has unlocked new possibilities: state-of-the-art text-to-image models (Labs, 2024) can now generate highly realistic and diverse images, while powerful image-to-video (I2V) models such as Wan2.2-14B (Wan et al., 2025) simulate complex dynamic processes with remarkable physical fidelity. Together, these generative priors enable the creation of training data with precise and controllable supervision signals, which are essential for training models to perform fine-grained, physically realistic edits. We find that fine-tuning the baseline on PICA-100K enhances the models performance in real-world evaluation. PICA-100K dataset. As shown in Fig. 5, we begin by constructing two structured prompt dictionaries: Subject Dictionary and Scene Dictionary, which include wide array of subjects and environments (e.g., tea pot, black kitchen table). These entries are paired using handcrafted text-to-image (T2I) templates and further refined using GPT-5, resulting in high-quality natural language instructions. The refined instructions are passed to the FLUX.1-Krea-dev (Lee et al., 2025) to generate static source images that are both visually realistic and semantically diverse. Next, we generate motion-oriented instructions to simulate physical edits. This is accomplished by designing series of I2V instruction templates, describing plausible motion-based changes such as rotations, movements, or tilts. These templates are expanded using GPT-5 to improve clarity and behavioral precision. The motion instructions (e.g., remove the tea pot, tilt the vase until it tips over, or swing the lantern gently in the wind) are then applied to the corresponding images using Wan2.2-14B-I2V, which synthesizes short video clips depicting the intended physical transformations. For each video, we extract the first and last frames to construct (source, edited) image pair. These pairs, along with the corresponding instruction, are used to form supervision signals. GPT-5 is employed to annotate each pair automatically, labeling the final frame as the preferred output. This pipeline eliminates the need for manual labeling while maintaining high annotation consistency. Our final dataset contains 105,085 instruction-based editing samples distributed across eight physics categories. The experimental results in Sec. 4 demonstrate that this pipeline can effectively generate high-quality data, significantly enhancing model performance on physics-aware image editing tasks. Training paradigm. To demonstrate the effectiveness of PICA-100K, we fine-tune FLUX.1-Kontextdev (Batifol et al., 2025), 12B flow-based diffusion transformer for image editing. We employ LoRA (Hu et al., 2022) with rank of 256 for fine-tuning. The model is trained using batch size of 64 and optimized using the AdamW optimizer with learning rate of 105. The entire fine-tuning procedure is conducted over 10,000 optimization steps on 16 NVIDIA A100 GPUs."
        },
        {
            "title": "4.1 EVALUATION DETAILS",
            "content": "We evaluate 11 closedand open-source models, covering most recent image-editing and unified vision-language systems. Closed-source systems include GPT-Image-1 (OpenAI, 2025), Nano Banana (Google, 2025), and Seedream 4.0 (Seedream et al., 2025). Open-source baselines include FLUX.1 Kontext (Batifol et al., 2025), Step1X-Edit (Liu et al., 2025), Bagel (Deng et al., 2025),"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Quantitative comparison on PICABench evaluated by GPT-5 for instruction-based editing models, where Acc , Con denote Accuracy (%) and Consistency (dB). LP, LSE, GST, LST mean Light propagation, Light Source Effects, Global State Transition, Local State Transition respectively. and indicate the best and second best score in category, respectively. Model Gemini GPT-Image-1 Seedream 4.0 DiMOO Uniworld-V1 Bagel Bagel-Think OmniGen Hidream-E1.1 Step1X-Edit Qwen-Edit LP LSE Reflection Refraction Deformation Causality GST LST Overall Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con 60.29 27.37 59.30 28.14 65.94 25.52 53.95 25.36 59.90 24.81 55.27 25.96 60.60 13.55 59.88 24.70 59.87 23.47 61.26 16.82 66.04 15.71 62.39 17.20 59.21 17.00 59.66 17.62 52.88 17.21 70.75 10.62 59.04 15.01 61.08 15.48 62.71 25.28 65.50 27.47 65.77 24.86 53.51 26.55 59.17 24.60 53.45 26.66 65.12 11.17 66.11 28.54 61.91 23.26 46.00 24.08 29.38 26.68 43.68 22.12 35.53 20.76 39.36 25.53 36.71 23.19 22.52 22.56 40.54 25.44 35.66 23.70 42.37 18.50 34.50 19.96 46.04 18.59 46.05 17.48 40.10 18.82 39.52 18.11 22.85 17.62 39.50 19.41 37.68 18.48 46.97 34.12 39.35 35.53 49.41 33.11 42.54 28.36 44.25 33.12 39.24 33.51 46.80 10.48 49.27 30.53 45.07 28.42 49.88 32.44 50.40 29.10 47.05 33.37 43.42 28.87 49.88 27.59 38.68 32.88 45.70 11.66 50.94 27.28 46.48 26.88 49.64 25.69 48.79 28.34 56.49 27.78 39.04 24.84 44.74 29.28 39.80 26.93 51.10 12.18 39.09 25.89 46.79 24.12 49.15 22.38 48.25 22.87 49.07 20.44 46.49 22.68 44.50 21.16 40.51 21.36 56.40 9.20 40.33 19.66 47.90 18.91 45.04 30.38 47.44 27.53 53.46 29.32 34.21 32.37 45.72 29.71 42.90 30.92 55.85 8.75 46.57 20.92 48.23 24.68 62.95 19.87 61.19 23.07 62.90 21.56 55.26 23.72 48.66 21.49 48.95 22.65 67.33 10.19 54.89 20.26 58.29 19. Flux.1 Kontext 54.96 27.58 57.41 25.97 57.50 26.92 36.40 26.76 51.83 28.86 38.12 29.69 48.79 12.52 47.61 25.70 48.93 24.57 Flux.1 Kontext+SF 57.38 27.90 58.49 26.58 63.07 26.99 36.40 27.01 53.30 29.19 41.07 29.54 47.02 14.44 49.27 27.01 50.64 25.23 Improvement +2.42 +0.32 +1.08 +0.61 +5.57 +0.07 +0.00 +0.25 +1.47 +0.33 +2.95 -0.15 -1.77 +1.92 +1.66 +1.31 +1.71 +0.66 Table 2: Performance across different prompt specificity levels. Model performance improves with prompt specificity, indicating that more detailed prompts yield higher performance. Model LP LSE Reflection Refraction Deformation Causality GST LST Overall Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Bagel-superficial Bagel-intermediate Bagel-explicit 46.97 34.12 39.35 35.53 49.41 33.11 42.54 28.36 44.25 33.12 39.24 33.51 46.80 10.48 49.27 30.53 45.07 28.42 55.93 23.78 61.73 18.94 57.50 28.91 47.37 21.08 49.88 22.60 44.87 26.09 57.51 8.81 56.13 23.15 54.06 21.14 62.71 15.39 72.24 13.89 62.39 18.74 55.26 16.68 57.70 15.24 59.35 19.98 77.26 8.14 65.90 15.65 65.61 15.20 Flux.1 Kontext-superficial 54.96 27.58 57.41 25.97 57.50 26.92 36.40 26.76 51.83 28.86 38.12 29.69 48.79 12.52 47.61 25.70 48.93 24.57 Flux.1 Kontext-intermediate 58.60 25.61 63.61 23.91 61.21 26.70 33.33 26.81 54.03 26.58 49.23 26.70 53.42 12.81 49.69 25.61 53.77 23.42 61.74 25.61 68.19 21.20 63.24 25.89 42.98 24.89 58.44 24.75 62.59 23.54 70.75 10.70 61.75 23.68 63.30 21.54 Flux.1 Kontext-explicit Bagel-Think (Deng et al., 2025), HiDream-E1.1 (Cai et al., 2025), UniWorld-V1 (Lin et al., 2025), OmniGen2 (Wu et al., 2025b), Qwen-Image-Edit (Wu et al., 2025a), and DiMOO (Team, 2025). All input images are resized proportionally to maximum resolution of 1024 on the longer side prior to evaluation. To ensure fairness and reproducibility, we run all models using their default settings from official repositories or web APIs. We choose superficial prompts as our default setting. For PICAEval, we first use the provided annotation masks to crop the edited region from the image. The cropped region is then resized proportionally to 1024 on the longer side before being passed to the VQA-based evaluator. This ensures standardized input size while preserving relevant physical cues within the editing region. We report results using both the current state-of-the-art closed-source model (GPT-5) and the leading open-source alternative (Qwen2.5-VL72B) as VLM evaluator. For consistency evaluation, we compute PSNR over the non-edited regions by masking out the predicted edit area, thereby measuring how well models preserve the original content outside the editing scope."
        },
        {
            "title": "4.2 BENCHMARK RESULTS",
            "content": "We are still far from physically realistic image editing. Tab. 1 presents comprehensive evaluation of existing methods. All open-source models score below 60 on the benchmark, and only the closedsource modelsGPT-Image-1 and Seedream 4.0slightly exceed this threshold. These results underscore persistent gap in the ability of current image editing models to generate physics-aware and physically realistic outputs. The gap between understanding and physical realism. Among open-source models, unified architectures consistently underperform compared to dedicated image editing models. Although unified MLLMs attempt to integrate visual understanding and generation within single framework, the presumed advantage of enhanced world understanding does not translate into improved physical realism. This suggests that stronger understanding alone is insufficient, and effectively coupling understanding with generation remains an open challenge. Tab. 2 presents performance across different prompt specificity levels. As shown in Tab. 2, model performance improves as prompts become more detailed. However, the gain from intermediate prompts is smaller than that from explicit prompts. We speculate this is due to the lack of internalized physics principles, which prevents models"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Ablation Results. We construct real-video-based dataset (Mira400K). The model trained on Mira400K underperforms, highlighting the effectiveness of our targeted synthetic data pipeline."
        },
        {
            "title": "Model",
            "content": "LP"
        },
        {
            "title": "Overall",
            "content": "Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Flux.1 Kontext 54.96 27.58 57.41 25.97 57.50 26.92 36.40 26.76 51.83 28.86 38.12 29.69 48.79 12.52 47.61 25.70 48.93 24.57 +PICA100K 57.38 27.90 58.49 26.58 63.07 26.99 36.40 27.01 53.30 29.19 41.07 29.54 47.02 14.44 49.27 27.01 50.64 25.23 +MIRA400K 52.06 28.80 54.72 29.92 57.34 27.93 37.28 28.05 49.88 29.48 38.68 32.41 44.59 40.17 41.58 31.22 46.96 32.08 Figure 6: Alignment between evaluation results and human preference. We make Pearson correlation analysis between Elo scores from human study and different settings. PICAEval-GPT5, PICAEval-Qwen use GPT-5 and Qwen2.5-VL-72B as the evaluator respectively. Baseline-Qwen adopts Qwen2.5-VL-72B but without edit region annotations. Results show that incorporating stronger VLMs and region-level infomation yields higher alignment with human preference. from leveraging the additional information. Interestingly, the Bagel model outperforms Flux Kontext under explicit prompts, likely because its unified architecture enhances long-text comprehension. Video data helps physics learning. Fine-tuning FLUX.1-KONTEXT on our PICA-100K dataset yields consistent improvements across multiple dimensions of physical realism. As shown in Appendix A.4, our model consistently produces more physically plausible results, while other models often exhibit unrealistic lighting effects, implausible object deformations, or invalid state changes. Quantitative results in Tab. 3 further support this: our fine-tuned model achieves +1.71% improvement in overall accuracy over the base model. In addition, it demonstrates better overall physical consistency, improving from 24.57db to 25.23db. These findings suggest that synthetic supervision signals derived from videos can effectively enhance models capacity for physics-aware image editing. They also validate the effectiveness of our video-to-image data generation pipeline in capturing diverse and complex physical phenomena. However, we observe slight drop in global state transition accuracy and causality consistency, possibly due to limitations in directly using first and last frames of video to represent meaningful state changes. We plan to explore more fine-grained strategies to extract temporal context and leverage intermediate frames. We also experimented with using real video data to construct an image editing dataset. Following the data pipeline of UniReal (Chen et al., 2025), we employed Miradata (Ju et al., 2024) to generate 400K edited images (MIRA400K) and trained the model under the same settings. However, as shown in Tab. 3, the model trained on MIRA400K performed even worse in overall accuracy. This further demonstrates the efficiency and effectiveness of our proposed data generation pipeline."
        },
        {
            "title": "4.3 VALIDITY OF PICAEVAL",
            "content": "We conduct human study using Elo ranking to further validate the effectiveness of PICAEval. As shown in Fig. 6, PICAEval achieves higher correlation with human judgments than the baseline. This result demonstrates that our per-case, region-level human annotations and carefully designed questions effectively mitigate VLM hallucinations, leading to outcomes that better reflect human preferences. Additional details of the human study are provided in Appendix A.3."
        },
        {
            "title": "5 LIMITATIONS AND FUTURE DIRECTIONS",
            "content": "While our approach demonstrates clear benefits in physics-aware image editing, it has several limitations. First, the PICA-100K dataset, though effective, is built using relatively simple generation pipeline and remains limited in scale. Second, our model is trained purely via supervised finetuning (SFT), which brings modest gains but may underexploit the full potential of data. Third, the current"
        },
        {
            "title": "Preprint",
            "content": "framework only supports single-image inputs, lacking the ability to incorporate multi-image or multi-condition contexts. In future work, we aim to enhance the data pipeline, explore RL-based post-training, and extend the model to support more expressive conditioning formats. wo"
        },
        {
            "title": "6 CONCLUSION",
            "content": "We present PICABench, new benchmark for evaluating physical realism in image editing, along with PICAEval, region-grounded, QA-based metric for fine-grained assessment. Our results show that current models, still far from producing physically realistic edits. To improve this, we introduce PICA-100K, synthetic dataset derived from videos. Fine-tuning on this dataset significantly boosts physical consistency, demonstrating the promise of video-based supervision. We hope our benchmark, metric, and dataset can drive progress toward physics-aware image editing."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, 2025. ByteDance. Seedream 4.0. 2025. URL https://seed.bytedance.com/en/seedream4_ 0/. Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. Mingdeng Cao, Xuaner Zhang, Yinqiang Zheng, and Zhihao Xia. Instruction-based image manipulation by watching how things move. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 27042713, 2025a. Shuo Cao, Nan Ma, Jiayang Li, Xiaohui Li, Lihao Shao, Kaiwen Zhu, Yu Zhou, Yuandong Pu, Jiarui Wu, Jiaquan Wang, Bo Qu, Wenhai Wang, Yu Qiao, Dajuin Yao, and Yihao Liu. Artimuse: Fine-grained image aesthetics assessment with joint scoring and expert-level understanding, 2025b. URL https://arxiv.org/abs/2507.14533. Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. Di Chang, Mingdeng Cao, Yichun Shi, Bo Liu, Shengqu Cai, Shijie Zhou, Weilin Huang, Gordon Wetzstein, Mohammad Soleymani, and Peng Wang. Bytemorph: Benchmarking instruction-guided image editing with non-rigid motions. arXiv preprint arXiv:2506.03107, 2025. Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1250112511, 2025. Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Google. Nano banana. image-generation/. 2025. URL https://gemini.google/overview/ Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, and Si Liu. Videoespresso: large-scale chain-of-thought dataset for fine-grained video reasoning via core frame selection. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 2618126191, June 2025. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. Proceedings of the International Conference on Learning Representations (ICLR), 2022. Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. Advances in Neural Information Processing Systems, 37:4895548970, 2024."
        },
        {
            "title": "Preprint",
            "content": "Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Sangwu Lee, Titus Ebbecke, Erwann Millon, Will Beddow, Le Zhuo, Iker Garcıa-Ferrero, Liam Esparraguera, Mihai Petrescu, Gian Saß, Gabriel Menezes, and Victor Perez. Flux.1 krea [dev]. https://github.com/krea-ai/flux-krea, 2025. Jialuo Li, Wenhao Chai, Xingyu Fu, Haiyang Xu, and Saining Xie. Science-t2i: Addressing scientific illusions in image synthesis, 2025. URL https://arxiv.org/abs/2504.13129. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Yiwei Ma, Jiayi Ji, Ke Ye, Weihuang Lin, Zhibin Wang, Yonghan Zheng, Qiang Zhou, Xiaoshuai Sun, and Rongrong Ji. I2ebench: comprehensive benchmark for instruction-based image editing. Advances in Neural Information Processing Systems (NeurIPS), 37:4149441516, 2024. Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Chaoran Feng, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. OpenAI. Gpt-image-1. 2025. introducing-4o-image-generation/. URL https://openai.com/index/ Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen Huang. Effective real image editing with accelerated iterative diffusion inversion. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 1591215921, 2023. Yuandong Pu, Le Zhuo, Kaiwen Zhu, Liangbin Xie, Wenlong Zhang, Xiangyu Chen, Peng Gao, Yu Qiao, Chao Dong, and Yihao Liu. Lumina-omnilv: unified multimodal framework for general low-level vision. arXiv preprint arXiv:2504.04903, 2025. Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. Kaiyue Sun, Rongyao Fang, Chengqi Duan, Xian Liu, and Xihui Liu. T2i-reasonbench: Benchmarking reasoning-informed text-to-image generation. arXiv preprint arXiv:2508.17472, 2025. Mohammad Reza Taesiri, Brandon Collins, Logan Bolton, Viet Dac Lai, Franck Dernoncourt, Trung Bui, and Anh Totti Nguyen. Understanding generative ai capabilities in everyday image editing tasks. arXiv preprint arXiv:2505.16181, 2025. Alpha VLLM Team. Lumina-dimoo: unified masked diffusion model for multi-modal generation and understanding, 2025. URL https://github.com/Alpha-VLLM/Lumina-DiMOO. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David Fleet, Radu Soricut, et al. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1835918369, 2023. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a."
        },
        {
            "title": "Preprint",
            "content": "Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, Ming-Hsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025c. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1838118391, 2023. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. Xin Yu, Tianyu Wang, Soo Ye Kim, Paul Guerrero, Xi Chen, Qing Liu, Zhe Lin, and Xiaojuan Qi. Objectmover: Generative object movement with video prior. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1768217691, 2025. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated In Advances in Neural Information Processing dataset for instruction-guided image editing. Systems, 2023. Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, Zicheng Zhang, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, and Haodong Duan. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. Le Zhuo, Songhao Han, Yuandong Pu, Boxiang Qiu, Sayak Paul, Yue Liao, Yihao Liu, Jie Shao, Xi Chen, Si Liu, and Hongsheng Li. Factuality matters: When image generation and editing meet structured visuals. arXiv preprint arXiv:2510.05091, 2025."
        },
        {
            "title": "A MORE DETAILS OF PICABENCH",
            "content": "A.1 TASK DEFINITION A.1.1 OPTICS Light propagation requires shadows that are geometrically consistent with the dominant light source, including direction, length, softness, and occlusion. Typical failure modes include misaligned or missing cast shadows and flat shading that ignores occluders. Reflection consistency demands view-dependent behavior for specular highlights and mirror reflections. Mirror images must preserve pose and depth; highlight positions should vary with surface curvature and viewpoint. Failures include floating reflections or highlights that remain fixed despite evident shape or view changes. Refraction requires continuous, coherent background distortion through transparent or translucent media. When edited objects involve glass or water, background edges should bend and scale according to interface geometry, with preserved edge continuity. Discontinuous refractive boundaries or inverted distortions indicate violations. Light-source effects evaluate whether new light-introducing edits (like add lamp) are consistent with the global illumination contextcolor casts, shadow penumbra, and brightness falloff should integrate naturally with the scene. Common issues include mismatched color temperatures, overly hard shadows, or inconsistent falloff relative to distance. A.1.2 MECHANICS Deformation assesses whether shape changes respect expected material properties. Rigid objects should not bend plastically; elastic deformations should be smooth and bounded. Texture and patterning should warp consistently with geometry rather than tear or duplicate. For instance, changing chairs height should not collapse its frame or produce rubber-like bending. Causality requires physically plausible contacts and supports under gravity. Edited objects should not float, interpenetrate, or rest in unstable equilibria (e.g., heavy object balanced on non-supporting point). Support relations must imply load transfer and stability. Violations include hovering objects, impossible stacking, and intersecting geometries that break solidity. A.1.3 STATE TRANSITION Global transitions affect the entire scene (e.g., day-to-night, dry-to-wet, solid-to-molten). Changes must propagate consistently: illumination color and intensity should update across surfaces; wetness should alter reflectance and darkening on all relevant materials; phase changes should be coherent and, when implied, justified by scene-level cues (e.g., pervasive heat source). Inconsistencies include night skies with daylight shadows or partial melting without corresponding global evidence. Local transitions involve spatially confined edits (e.g., adding steam, charring an edge, or melting corner). These effects must integrate with nearby context and causal cues. Steam implies heat and moisture and may induce local condensation; flames produce light spill and secondary reflections; partial melting should respect material boundaries and continuity. When localized changes ignore surrounding context or violate material behavior, the edit becomes physically implausible. A.2 MORE BENCHMARK RESULTS Tab. 4 lists the performance of models on PICABench, evaluated by Qwen2.5-VL-72B (Bai et al., 2025). It can be seen that the general rule and conclusion are similar to those suggested by Tab. 1: Most models have very low scores (below 60), indicating fatal gap in the ability to generate physics-aware images. We present benchmark results on different prompt levels. The results of PICABench with intermediate prompts evaluated by GPT-5 are shown in Table 5, while the results with explicit prompts evaluated by GPT-5 are provided in Table 6. The results of PICABench using intermediate prompts evaluated by"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Quantitative comparison on PICABench evaluated by Qwen2.5-VL-72B for instructionbased editing models, where Acc , Con , LP, LSE, GST, LST denote Accuracy (%) and Consistency (%), Light propagation, Light Source Effects, Global State Transition, Local State Transition respecindicates the best and second best score in category, respectively. tively. and Model Gemini GPT-Image-1 Seedream 4.0 DiMOO Uniworld-V1 Bagel Bagel-Think OmniGen2 Step1X-Edit Hidream-E1.1 Qwen-Edit LP LSE Reflection Refraction Deformation Causality GST LST Overall Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con 48.91 27.37 47.98 28.14 51.43 25.52 52.63 25.36 44.25 24.81 44.30 25.96 52.21 13.55 50.73 24.70 49.08 23.47 58.35 16.82 56.06 15.71 50.76 17.20 63.60 17.00 40.83 17.62 46.27 17.21 63.36 10.62 52.81 15.01 53.96 15.48 55.45 25.28 57.41 27.47 55.82 24.86 54.39 26.55 45.72 24.60 47.82 26.66 59.16 11.17 56.13 28.54 54.23 23.26 38.26 24.08 24.80 26.68 30.19 22.12 46.05 20.76 24.94 25.53 31.50 23.19 18.21 22.56 31.19 25.44 28.57 23.70 33.41 18.50 29.11 19.96 32.88 18.59 46.49 17.48 26.16 18.82 32.91 18.11 18.54 17.62 29.94 19.41 29.18 18.48 38.50 34.12 36.39 35.53 37.27 33.11 51.75 28.36 30.32 33.12 36.29 33.51 40.18 10.48 40.12 30.53 38.23 28.42 39.71 32.44 43.67 29.10 36.26 33.37 48.68 28.87 33.74 27.59 36.71 32.88 39.62 11.66 39.09 27.28 38.86 26.88 36.32 25.69 41.78 28.34 45.36 27.78 48.25 24.84 32.52 29.28 36.29 26.93 42.72 12.18 33.89 25.89 39.52 24.12 37.29 30.38 43.94 27.53 40.64 29.32 43.42 32.37 32.03 29.71 35.72 30.92 48.79 8.75 38.46 20.92 40.59 24.68 40.44 22.38 41.24 22.87 41.15 20.44 48.25 22.68 30.32 21.16 37.13 21.36 48.23 9.20 38.46 19.66 40.95 18.91 52.54 19.87 52.02 23.07 49.07 21.56 57.46 23.72 38.14 21.49 42.62 22.65 57.73 10.19 47.82 20.26 49.71 19.43 Flux.1 Kontext 48.43 27.58 53.64 25.97 43.84 26.92 43.86 26.76 33.74 28.86 34.04 29.69 41.06 12.52 37.01 25.70 41.07 24.57 Flux.1 Kontext+SF 49.64 27.90 51.21 26.58 47.22 26.99 46.49 27.01 33.99 29.19 35.44 29.54 39.29 14.44 40.75 27.01 41.93 25.23 Improvement +1.21 +0.32 -2.43 +0.61 +3.38 +0.07 +2.63 +0.25 +0.25 +0.33 +1.40 -0.15 -1.77 +1.92 +3.74 +1.31 +0.86 +0. Table 5: Quantitative comparison on PICABench-Intermediate evaluated by GPT-5. Model Nano Banana Seedream 4.0 GPT-Image-1 DiMOO Uniworld-V1 OmniGen2 Bagel-Think Bagel Step1X-Edit Hidream-E1.1 Qwen-Edit LP LSE Reflection Refraction Deformation Causality GST LST Overall Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con 56.17 29.57 63.88 30.86 61.72 28.03 53.07 28.18 63.57 27.86 63.57 27.78 66.11 38.62 64.86 30.69 62.72 30.86 63.68 23.51 70.89 22.66 68.63 24.62 50.88 26.70 63.57 25.92 63.15 25.48 71.30 36.49 63.20 32.31 65.86 28.01 66.10 19.08 74.39 19.26 64.08 18.99 62.72 18.79 64.30 20.73 63.57 20.33 78.59 35.76 69.23 22.15 68.87 23.19 46.25 27.70 32.61 33.26 43.00 24.00 32.02 23.93 40.34 30.65 34.32 27.39 21.30 49.42 39.09 36.09 34.78 32.73 47.22 18.89 39.35 20.48 49.75 19.16 32.89 19.06 45.97 19.16 36.99 18.10 26.27 17.54 39.71 19.56 38.69 18.90 57.38 19.52 58.22 22.04 61.38 23.39 42.98 23.27 52.32 24.99 44.02 25.50 48.79 38.12 38.05 22.51 50.27 26.29 48.91 30.91 55.53 25.89 50.93 33.72 50.88 26.08 53.30 30.33 42.76 34.20 53.53 35.45 52.39 31.38 50.71 32.02 55.93 26.00 61.73 23.30 57.50 30.92 47.37 23.01 49.88 27.08 44.87 30.42 57.51 34.39 56.13 28.85 54.06 29.18 52.06 30.36 56.06 28.27 55.99 30.89 38.60 31.76 53.06 30.57 44.02 32.64 60.26 35.68 52.39 29.58 52.80 31.72 56.17 19.41 62.26 22.33 57.84 19.66 43.86 20.37 52.08 20.97 44.16 22.19 61.92 34.75 50.94 21.77 54.45 23.81 62.47 23.09 65.23 24.46 66.61 24.64 44.74 27.48 54.77 28.02 50.49 26.47 67.44 36.37 60.91 28.38 60.41 28.12 Flux.1 Kontext 58.60 27.94 63.61 28.18 61.21 28.63 33.33 28.74 54.03 30.18 49.23 30.56 53.42 38.16 49.69 31.34 53.77 31.27 Flux.1 Kontext+SF 61.26 29.54 64.15 30.16 65.43 28.68 39.47 28.48 58.92 31.04 48.10 31.55 52.76 39.65 53.22 32.78 55.59 32.34 Improvement +2.66 +1.60 +0.54 +1.98 +4.22 +0.05 +6.14 -0.26 +4.89 +0.86 -1.13 +0.99 -0.66 +1.48 +3.53 +1.44 +1.82 +1.07 Qwen2.5-VL-72B is shown in Table 7, and the results of PICABench with explicit prompts evaluated by GPT-5 are reported in Table 8. As demonstrated above, model performance improves with more informative prompts. Furthermore, the fine-tuned model consistently outperforms the baseline, indicating the effectiveness of the PICA100K dataset. A.3 DETAILED HUMAN EVALUATION PROTOCOL Study setup. We use the Rapidata1 platform to conduct pairwise human preference comparisons for evaluating image editing quality. Each trial presents reference image and two model outputs (A/B) under fixed unified instruction: Select the image that more closely matches the editing instruction. The A/B order is randomized per trial. Annotators are vetted beforehand and participate on voluntary basis. Datasets and models. We evaluate 9 models over the PICABench dataset at three difficulty levels (superficial, intermediate, explicit), forming 36 unordered model pairs per item. For each difficulty, we sample 50 items via stratified sampling over the physics law taxonomy. Each item yields 36 comparisons, each judged by 5 annotators, resulting in 27,000 votes per split. 1https://www.rapidata.ai/"
        },
        {
            "title": "Preprint",
            "content": "Table 6: Quantitative comparison on PICABench-Explicit evaluated by GPT-5. Model Gemini GPT-Image-1 Seedream 4.0 DiMOO Uniworld-V1 Bagel-Think OmniGen2 Bagel Step1X-Edit Hidream-E1.1 Qwen-Edit LP LSE Reflection Refraction Deformation Causality GST LST Overall Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con 63.92 26.98 65.23 25.89 68.30 25.12 54.39 25.79 66.99 24.41 68.21 24.87 69.32 13.37 64.66 25.50 66.46 23.02 63.92 16.34 72.51 14.97 66.44 16.46 60.53 16.73 63.08 16.87 63.85 16.40 78.48 10.46 64.66 16.44 68.07 15.16 65.62 19.87 73.32 16.76 69.14 20.42 60.53 24.77 69.93 20.65 76.23 20.58 82.23 9.69 76.72 24.95 73.76 18.73 44.55 24.72 29.92 28.06 40.47 23.17 26.32 23.56 47.43 25.25 33.19 24.35 20.86 23.74 41.58 25.66 34.39 24.65 47.70 17.95 42.86 19.03 51.77 18.34 31.58 19.06 49.39 18.76 38.82 18.17 35.76 15.78 46.15 18.93 42.78 17.96 52.54 27.05 60.38 17.96 57.84 29.35 43.86 24.63 55.99 24.64 48.10 28.01 62.91 9.34 57.80 23.38 56.01 22.34 57.14 12.81 57.14 14.62 64.59 17.54 46.93 18.02 56.72 15.91 55.98 18.88 59.82 10.39 51.98 15.38 57.39 15.19 62.71 15.39 72.24 13.89 62.39 18.74 55.26 16.68 57.70 15.24 59.35 19.98 77.26 8.14 65.90 15.65 65.61 15.20 55.45 22.26 62.26 18.86 61.55 25.58 40.79 26.99 58.92 25.12 56.82 26.01 67.44 9.42 58.63 23.52 59.73 21.25 57.87 14.76 68.73 15.46 61.21 15.52 46.49 15.91 64.06 15.98 55.70 16.00 71.30 9.55 60.71 15.52 62.23 14.40 65.62 17.78 71.43 18.81 64.59 20.45 51.75 24.29 58.44 21.14 66.39 19.90 78.81 9.76 69.65 19.33 68.02 17.96 Flux.1 Kontext 61.74 25.61 68.19 21.20 63.24 25.89 42.98 24.89 58.44 24.75 62.59 23.54 70.75 10.70 61.75 23.68 63.30 21.54 Flux.1 Kontext+SF 62.95 26.82 69.00 23.04 63.91 26.07 46.05 25.68 63.81 26.47 66.10 25.15 67.99 11.08 63.20 25.19 64.47 22.63 Improvement +1.21 +1.21 +0.81 +1.84 +0.67 +0.18 +3.07 +0.79 +5.37 +1.72 +3.51 +1.61 -2.76 +0.38 +1.45 +1.51 +1.17 +1.09 Table 7: Quantitative comparison on PICABench-Intermediate evaluated by Qwen2.5-VL-72B. Model Gemini GPT-Image-1 Seedream 4.0 DiMOO Uniworld-V1 OmniGen2 Bagel-Think Bagel Step1X-Edit Hidream-E1.1 Qwen-Edit LP LSE Reflection Refraction Deformation Causality GST LST Overall Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con 48.67 27.27 50.67 26.79 50.93 25.91 49.12 26.24 43.77 23.62 51.34 23.77 56.84 13.16 53.22 25.09 51.51 22.91 58.35 16.58 60.92 14.92 51.10 16.83 59.21 16.85 45.72 16.73 54.15 16.37 70.09 10.36 53.85 16.58 57.66 15.22 57.14 21.08 61.73 18.35 56.66 22.30 55.70 24.76 51.10 22.17 55.56 21.43 66.00 10.97 55.09 26.78 58.24 20.06 37.53 23.86 23.18 26.85 31.87 22.55 38.60 23.38 26.41 25.12 31.08 23.09 17.66 23.75 29.52 25.55 27.94 24.09 39.23 18.71 31.81 19.44 33.22 18.75 31.14 19.52 29.58 19.39 30.38 18.39 19.65 17.33 33.68 19.24 29.79 18.62 47.70 17.15 49.60 17.63 46.21 20.94 44.74 21.34 35.70 20.64 38.12 21.41 39.40 12.74 31.39 16.78 40.90 18.19 42.37 28.45 47.71 21.74 40.81 31.55 53.95 24.15 39.12 26.02 37.55 30.18 45.70 9.88 44.49 25.64 43.09 24.04 48.91 23.78 55.26 18.94 46.04 28.91 56.14 21.08 37.41 22.60 36.85 26.09 48.01 8.81 44.28 23.15 45.50 21.14 42.13 27.83 50.13 23.59 43.17 28.79 46.05 29.83 36.43 26.95 39.52 28.61 52.76 10.24 43.66 23.85 44.72 23.76 46.73 17.01 53.10 17.89 46.04 17.43 48.68 18.44 38.39 16.95 38.40 18.34 56.40 9.23 41.16 16.16 46.52 15.82 54.24 20.37 58.49 20.28 50.42 22.49 49.12 25.54 42.30 24.26 43.46 22.36 57.40 10.91 50.31 22.78 50.97 20.14 Flux.1 Kontext 48.91 25.61 55.53 23.91 45.87 26.70 43.86 26.81 38.14 26.58 44.30 26.70 44.15 12.81 43.87 25.61 45.28 23.42 Flux.1 Kontext+SF 47.70 27.23 58.22 25.73 48.74 26.73 46.49 26.54 41.56 27.41 42.33 27.81 42.27 14.38 44.49 27.16 45.62 24.53 Improvement -1.21 +1.62 +2.69 +1.82 +2.87 +0.03 +2.63 -0.27 +3.42 +0.83 -1.97 +1.11 -1.88 +1.57 +0.62 +1.55 +0.34 +1.11 Elo computation. To aggregate preferences, we use robust Elo rating system. For match between model and with current ratings (RA, RB), the expected win probability of is: EA ="
        },
        {
            "title": "1\nRB −RA\nS",
            "content": "1 + 10 , where = 400 is the scaling factor. Given the vote ratio sA [0, 1] for model A, with sB = 1 sA, the ratings are updated as: R = max(Rmin, RA + Keff (sA EA)), = max(Rmin, RB + Keff (sB EB)), (1) (2) where Keff = 5 adjusts for vote count = vA + vB, and = 24 is the base step size. Robust aggregation. To reduce order effects and improve stability, we shuffle the comparison stream and re-run Elo updates for = 50 rounds. The final Elo score for model is computed as: Rm ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 R(t) , σm = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) (cid:16) t=1 R(t) Rm (cid:17) . (3) Parameter setting. Table 9 summarizes the Elo configuration used in all human evaluations. A.4 MORE VISUALIZATION Fig. 7-14 presents generated images of various models prompted by samples in our PICABench. The prompts cover all eight physics laws and three complexity levels. They demonstrate that the"
        },
        {
            "title": "Preprint",
            "content": "Table 8: Quantitative comparison on PICABench-Explicit evaluated by Qwen2.5-VL-72B. Model Gemini GPT-Image-1 Seedream 4.0 DiMOO Uniworld-V1 Bagel-Think OmniGen2 Bagel Step1X-Edit Hidream-E1.1 Qwen-Edit LP LSE Reflection Refraction Deformation Causality GST LST Overall Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con Acc Con 53.27 26.98 54.45 25.89 55.99 25.12 56.58 25.79 47.68 24.41 58.93 24.87 58.17 13.37 52.81 25.50 55.40 23.02 59.56 16.34 61.99 14.97 52.61 16.46 61.84 16.73 44.99 16.87 53.16 16.40 70.53 10.46 51.56 16.44 57.83 15.16 58.84 19.87 66.04 16.76 58.85 20.42 62.72 24.77 50.12 20.65 67.09 20.58 77.37 9.69 63.62 24.95 64.91 18.73 32.93 24.72 23.99 28.06 27.82 23.17 31.14 23.56 30.32 25.25 27.29 24.35 17.99 23.74 33.06 25.66 26.78 24.65 37.77 17.95 34.50 19.03 37.44 18.34 30.70 19.06 30.32 18.76 34.18 18.17 28.81 15.78 38.67 18.93 33.80 17.96 42.86 27.05 52.29 17.96 43.17 29.35 48.25 24.63 40.10 24.64 38.40 28.01 53.86 9.34 46.99 23.38 45.91 22.34 51.09 12.81 47.98 14.62 48.74 17.54 45.18 18.02 42.79 15.91 48.24 18.88 52.76 10.39 42.41 15.38 48.18 15.19 54.48 15.39 63.34 13.89 52.28 18.74 55.70 16.68 42.05 15.24 52.32 19.98 68.43 8.14 54.05 15.65 56.44 15.20 43.10 22.26 52.29 18.86 47.05 25.58 47.37 26.99 40.34 25.12 46.69 26.01 57.95 9.42 47.19 23.52 48.83 21.25 49.39 14.76 54.99 15.46 44.52 15.52 45.18 15.91 40.83 15.98 45.15 16.00 58.72 9.55 47.61 15.52 49.22 14.40 54.00 17.78 60.38 18.81 52.28 20.45 53.07 24.29 44.74 21.14 56.68 19.90 65.78 9.76 59.04 19.33 57.00 17.96 Flux.1 Kontext 52.54 25.61 59.57 21.20 51.10 25.89 46.05 24.89 40.59 24.75 53.16 23.54 62.03 10.70 54.47 23.68 53.84 21.54 Flux.1 Kontext+SF 52.06 26.82 59.30 23.04 52.78 26.07 47.37 25.68 42.79 26.47 53.73 25.15 61.48 11.08 53.64 25.19 54.18 22.63 Improvement -0.48 +1.21 -0.27 +1.84 +1.68 +0.18 +1.32 +0.79 +2.20 +1.72 +0.57 +1.61 -0.55 +0.38 -0.83 +1.51 +0.34 +1.09 Table 9: Elo parameter setting."
        },
        {
            "title": "Parameter",
            "content": "Initial Elo rating Elo scaling factor Base K-factor Minimum Elo rating Rmin Number of rounds Votes per match Model pairs per item Items per difficulty Benchmark splits Total comparisons per split Total votes per split"
        },
        {
            "title": "Value",
            "content": "1,000 400 24 700 50 5 45 50 3 (superficial/intermediate/explicit) 5400 27,000 performance of these models varies considerably in complying with physical laws. Most models either just perform superficial edits and ignore the physics law, or completely fail to understand the instruction. Only few models, including ours, can yield physically plausible images in most cases. Therefore, the ability to follow physical laws is crucial but lacking in most models, and by PICABench we hope to draw the communitys attention to this critical problem. A.5 PROMPTS FOR PICABENCH We present the prompts used in constructing PICABench. Fig. 16 shows the prompt used to generate QAs to evaluate editing models performance. Fig. 15 shows the prompts used to generate edit instructions in PICA-100K. MORE DETAILS ABOUT PICA-100K B.1 EXAMPLE OF PICA-100K Fig. 17 shows some examples in PICA-100K dataset. For each pair, we focus on the manifestation of physical laws. B.2 PROMPTS FOR PICA-100K We show the prompts used in constructing PICA-100K. Fig. 18 shows the prompt used in generating video from generated images. Fig. 19 shows the prompts used to generate edit instructions in PICA-100K."
        },
        {
            "title": "Preprint",
            "content": "Superficial Prompt: Remove the yellow chair"
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext BAGEL-Think Step1X-Edit GPT-Image"
        },
        {
            "title": "Nano Banana",
            "content": "HiDream-E1.1 Seedream 4."
        },
        {
            "title": "DiMOO",
            "content": "Superficial Prompt: Move the potted plant to left side of the table."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext BAGEL-Think Step1X-Edit GPT-Image"
        },
        {
            "title": "Nano Banana",
            "content": "Qwen-Image HiDream-E1.1 OmniGen2 Figure 7: Examples of how models follow the law of light propagation in optics (superficial propmts)."
        },
        {
            "title": "Preprint",
            "content": "Superficial Prompt: Turn on the lamp on the bedside table."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "BAGEL-Think Step1X-Edit GPT-Image"
        },
        {
            "title": "Nano Banana",
            "content": "HiDream-E1.1 OmniGen2 Superficial Prompt: Turn off the lamp in the room."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "BAGEL-Think GPT-Image"
        },
        {
            "title": "Nano Banana",
            "content": "Qwen-Image HiDream-E1.1 OmniGen2 Figure 8: Examples of how models follow the law of light source effects in optics (superficial propmts)."
        },
        {
            "title": "Preprint",
            "content": "Superficial Prompt: Move the hot air balloon in the image to the left."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "BAGEL-Think GPT-Image 1 Qwen-Image HiDream-E1.1 Seedream 4.0 OmniGen Superficial Prompt: Move the man to the right."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "BAGEL-Think GPT-Image 1 Qwen-Image HiDream-E1.1 Seedream 4.0 OmniGen Figure 9: Examples of how models follow the law of reflection in optics (superficial propmts)."
        },
        {
            "title": "Preprint",
            "content": "Superficial Prompt: Remove the magnifying glass from the image."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext BAGEL-Think GPT-Image 1 Qwen-Image HiDream-E1.1 Seedream 4."
        },
        {
            "title": "DiMOO",
            "content": "OmniGen2 Superficial Prompt: Add blue straw to the glass of water."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "GPT-Image"
        },
        {
            "title": "Nano Banana",
            "content": "Qwen-Image HiDream-E1.1 Seedream 4.0 OmniGen2 Figure 10: Examples of how models follow the law of refraction in optics (superficial propmts)."
        },
        {
            "title": "Preprint",
            "content": "Superficial Prompt: Add dog on top of the pillow."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "GPT-Image"
        },
        {
            "title": "Nano Banana",
            "content": "Qwen-Image HiDream-E1.1 Seedream 4.0 OmniGen2 Superficial Prompt: Change the womans position from sitting to standing."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "BAGEL-Think Step1X-Edit"
        },
        {
            "title": "Nano Banana",
            "content": "HiDream-E1."
        },
        {
            "title": "DiMOO",
            "content": "OmniGen2 Figure 11: Examples of how models follow the law of deformation in mechanics (superficial propmts)."
        },
        {
            "title": "Preprint",
            "content": "Superficial Prompt: Remove the skateboard."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "BAGEL-Think Step1X-Edit GPT-Image 1 Qwen-Image"
        },
        {
            "title": "DiMOO",
            "content": "OmniGen2 Superficial Prompt: Remove the white table."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "BAGEL-Think GPT-Image"
        },
        {
            "title": "Nano Banana",
            "content": "Qwen-Image HiDream-E1.1 Seedream 4.0 Figure 12: Examples of how models follow the law of causality in mechanics (superficial propmts)."
        },
        {
            "title": "Preprint",
            "content": "Superficial Prompt: Change the foggy weather to sunny day."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext BAGEL-Think Step1X-Edit GPT-Image"
        },
        {
            "title": "Nano Banana",
            "content": "Qwen-Image HiDream-E1.1 OmniGen2 Superficial Prompt: Change the weather in the scene to snowy day."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "BAGEL-Think Step1X-Edit GPT-Image 1 Qwen-Image HiDream-E1.1 Seedream 4. Figure 13: Examples of how models follow the law of global in state transition (superficial propmts)."
        },
        {
            "title": "Preprint",
            "content": "Superficial Prompt: Make the bench wet."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext"
        },
        {
            "title": "BAGEL",
            "content": "BAGEL-Think Step1X-Edit GPT-Image"
        },
        {
            "title": "Nano Banana",
            "content": "HiDream-E1.1 OmniGen2 Superficial Prompt: Freeze the soda can."
        },
        {
            "title": "Ours",
            "content": "Flux.1 Kontext BAGEL-Think Step1X-Edit GPT-Image 1 Qwen-Image HiDream-E1. Seedream 4.0 OmniGen2 Figure 14: Examples of how models follow the law of local in state transition (superficial propmts)."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Prompt used to generate edit instruction for PICABench."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: Prompt used to generate QAs for PICABench"
        },
        {
            "title": "Preprint",
            "content": "Figure 17: Examples of PICA-100K dataset. Figure 18: Prompts used to generate i2v caption for Wan2.2 14B"
        },
        {
            "title": "Preprint",
            "content": "Figure 19: Prompts used to generate edit instruction for PICA100K"
        }
    ],
    "affiliations": [
        "Beihang University",
        "CUHK MMLab",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The University of Hong Kong",
        "Tongyi Lab",
        "USTC"
    ]
}