{
    "paper_title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos",
    "authors": [
        "Haobo Yuan",
        "Xiangtai Li",
        "Tao Zhang",
        "Zilong Huang",
        "Shilin Xu",
        "Shunping Ji",
        "Yunhai Tong",
        "Lu Qi",
        "Jiashi Feng",
        "Ming-Hsuan Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work presents Sa2VA, the first unified model for dense grounded understanding of both images and videos. Unlike existing multi-modal large language models, which are often limited to specific modalities and tasks, Sa2VA supports a wide range of image and video tasks, including referring segmentation and conversation, with minimal one-shot instruction tuning. Sa2VA combines SAM-2, a foundation video segmentation model, with LLaVA, an advanced vision-language model, and unifies text, image, and video into a shared LLM token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2 in producing precise masks, enabling a grounded, multi-modal understanding of both static and dynamic visual content. Additionally, we introduce Ref-SAV, an auto-labeled dataset containing over 72k object expressions in complex video scenes, designed to boost model performance. We also manually validate 2k video objects in the Ref-SAV datasets to benchmark referring video object segmentation in complex environments. Experiments show that Sa2VA achieves state-of-the-art across multiple tasks, particularly in referring video object segmentation, highlighting its potential for complex real-world applications."
        },
        {
            "title": "Start",
            "content": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos Haobo Yuan1, Xiangtai Li2 , Tao Zhang2,3, Zilong Huang2, Shilin Xu4, Shunping Ji3, Yunhai Tong4, Lu Qi2, Jiashi Feng2, Ming-Hsuan Yang1 1UC Merced 2Bytedance Seed 3Wuhan University 4Peking University : Project lead. *: equal technical contribution to the work. E-mail: xiangtai94@gmail.com Project Pages: https://lxtgh.github.io/project/sa2va Huggingface link: https://huggingface.co/ByteDance/Sa2VA-4B 5 2 0 2 7 ] . [ 1 1 0 0 4 0 . 1 0 5 2 : r Figure 1. Illustration of capabilities of our proposed Sa2VA. (a). Given video, Sa2VA is able to segment the referred object and understand the whole scene. (b).Sa2VA supports image conversation, video conversation, image referring segmentation, video referring segmentation, and grounded caption generation with single-shot instruction-tuning. (c).Sa2VA achieves strong results on multiple images, video referring segmentation, and chat benchmarks compared with existing MLLMs, such as GLaMM [66] and OMG-LLaVA [99]."
        },
        {
            "title": "Abstract",
            "content": "This work presents Sa2VA, the first unified model for dense grounded understanding of both images and videos. Unlike existing multi-modal large language models, which are often limited to specific modalities and tasks, Sa2VA supports wide range of image and video tasks, including referring segmentation and conversation, with minimal one1 shot instruction tuning. Sa2VA combines SAM-2, foundation video segmentation model, with LLaVA, an advanced vision-language model, and unifies text, image, and video into shared LLM token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2 in producing precise masks, enabling grounded, multi-modal understanding of both static and dynamic visual content. Additionally, we introduce Ref-SAV, an auto-labeled dataset containing over 72k object expressions in complex video scenes, designed to boost model performance. We also manually validate 2k video objects in the Ref-SAV datasets to benchmark referring video object segmentation in complex environments. Experiments show that Sa2VA achieves state-of-the-art across multiple tasks, particularly in referring video object segmentation, highlighting its potential for complex real-world applications. Code, dataset, and models are released for further research. 1. Introduction Multi-modal Large Language Models (MLLMs) have made significant progress with the rapid development of Large Language Models (LLMs) [33, 73, 86]. They have benefited from various image and video-level tasks such as visual question answering (VQA) [1, 70], narrative story generation [26, 87, 104], and interactive editing [30, 39, 76]. One important direction is to understand video content in fine-grained manner, including segmenting and tracking pixels with language descriptions, and performing VQA on visual prompts in the video. In particular, we want to achieve promptable fine-gained analysis of video since the user can be in the loop when playing the video in an interactive mode, as shown in Fig. 1 (a). This leads to various applications, such as short video editing [6, 58, 75], robot navigation [21, 23, 24], and surveillance analysis [63]. However, either state-of-the-art video perception models [11, 48, 67, 98] or video MLLMs [43, 50, 55, 95] cannot achieve this. The formers are limited by the constrained semantic concepts, lacking the open-ended ability (Video VQA, or named video chat tasks). For example, the latest foundation model, SAM-2 [67], can achieve promptable segmentation and tracking. However, it cannot perform text-aware tasks, such as understanding language expressions or video conversation. On the other hand, video MLLMs [42, 50, 51, 55] can understand long videos and perform VQA. For example, the latest LLaVA [42] can achieve strong results on video VQA. However, it cannot carry out perception tasks, and also it cannot understand visual prompts. Several works [18, 85, 99] explore the combination of perception models and MMLMs. However, most works explore image datasets or try to solve one specific video understanding task. To our knowledge, no work can leverage the benefits from both sides. Through the above analysis, in this work, we make the first step to unify two visual foundation models, SAM2 [67] and LLaVA-like MLLMs [53], into one framework. In this way, our model not only inherits the spatial-temporal perception capabilities of SAM-2 and the open-ended abilities of MLLMs, but also benefits from learning additional knowledge derived from their fine-grained training data. However, to achieve this goal, three key challenges must be addressed: (1) Task formulation: How to effectively formulate range of tasks in one-shot training setting, particularly for multimodal inputs. In particular for video perception tasks and video VQA tasks. Previous works [18, 85] only solve partial of them. (2) Performance balance: How to resolve conflicts between tasks, such as ensuring strong referring visual understanding abilities without sacrificing the language proficiency of MLLMs. Note that previous works [82, 99] find conversation tasks degraded when per- (3) Knowledge sharing: How forming grounding tasks. to leverage the pre-trained knowledge from SAM-2 and MLLMs and build robust, unified model. SAM-2 is trained with more than 1B masks, and most MLLMs are trained with massive instructing pairs. We present Sa2VA, the first model that integrates SAM-2 with LLaVA-like MLLMs, creating unified, grounded understanding of both images and videos. We formulate various tasks into one-shot visual instruction tuning format, including referring segmentation, visual question answering (VQA), and grounded conversation generation (GCG) for both image and video. Our key insight is to leverage the flexible token-length handling of LLMs, treating all input images, videos, and visual prompts as visual tokens without requiring additional specific design. Through joint cotraining, we demonstrate that conflicts between grounding and conversation tasks can be effectively resolved. Unlike existing approaches that use MLLMs as agents or connectors to call on visual experts, our model is trained endto-end, showcasing scalability in both the model and the datasets. Moreover, we adopt decoupled design in which SAM-2s decoder and memory module are frozen, allowing us to retain the perception and tracking capabilities of SAM-2. This design also makes our method plug-andplay module, enabling our model to be updated with the latest MLLMs. In addition, we introduce challenging referring video segmentation dataset, Ref-SAV, curated through an automatic pipeline based on the source SA-V datasets [67]. This is from our empirical observation that existing referring video segmentation datasets are limited to small-scale and short clips with fewer occlusions. We benchmark several existing models on this dataset and find ample room for improvement and further exploration. Sa2VA is co-trained with multi-dataset co-training, including image and video datasets. Without any bells and 2 Table 1. Comparison of capabilities of different representative models. Our method supports various tasks and modalities. Benefiting from these interactive features on video, Sa2VA can perform multiple promptable tasks in the video, as shown in Fig. 1 (a) and (b). Support Inputs Dense Grounding Conversation Image Video Visual Prompts RES Ref-VOS Inter-VOS GCG Image-Chat Video-Chat Video Caption Interactive Caption End to End Training Method LLAVA [53] LLaVA-OneVision [42] InternVL 2.0 [9] Osprey [92] LISA [40] GLAMM [66] VIP-LLaVA [5] VISA [85] OMG-LLaVA [99] PSALM [101] GSVA [83] LLaMA-VID [50] ST-LLM [56] F-LLM [82] Sa2VA (Ours) whistles, Sa2VA achieves state-of-the-art results on six referring image and video segmentation datasets while keeping strong image and video chat ability compared with previous MLLMs, as shown in Fig. 1 (c). On Ref-SAV, our method achieves over 15% previous methods under zeroshot test setting and further stronger results using our proposed Ref-SAV training set. As shown in Fig. 1 (c), our work builds new strong baseline for unified, dense, grounded understanding of images and video. Overall, our contributions are as follows: From the visual token perspective, we re-think several tasks, including image chat, image referring segmentation, video chat, referring video object segmentation, and grounded caption generation. We formulate all these tasks as single instruction-tuning process. We develop Sa2VA, the first simple framework that combines SAM-2 and LLaVA-like models into one model, where we achieve spatial-temporal grounded understanding. We introduce simple decouple framework with shared SEG token. We also present simple mask tracking method by re-utilizing SAM-2s knowledge. We introduce challenging referring video object segmentation benchmark, Ref-SAV, with manual check for each example. The benchmark introduces heavier occlusions, long text inputs, motion blurs. We also develop simple data annotation pipeline to build the Ref-SAV training dataset, where we find the training dataset improve model performance on Ref-SAV. 2. Related Work Multi-modal Large Language Models. Earlier works [31, 44, 45] explore better multi-modal fusion methods and feature extractors and design various fusion architectures, particularly for vision language tasks. With the advanced LLMs frameworks [4, 33, 73], multi-modal instruction tuning on LLMs [2, 8, 53, 71] becomes the main pipeline. Then, various benchmarks [19, 32, 49, 59, 72] come up and serve as better sources to achieve stronger performance, meaning that data plays central role in current MLLMs. One representative work, LLaVA [53], treats the visual features as visual tokens. The authors also provide unified data format to unify extensive VQA tasks. After that, several works [92] explore stronger visual cues to enhance the visual inputs of LLaVA. Meanwhile, several works [14, 15, 29, 40, 51, 65, 68, 9497] add extra components to adapt LLaVA for visual grounding, detection, segmentation, and video VQA. Recently, there has also been new trend [42, 55] to unify image, video, and multi-image analysis in one framework. LLaVA-OneVision [42] design single model to handle four different input sources. In visual perception, several works [48, 67, 84] also explore multi-dataset and multi-task co-training. SAM-2 [67] designs unified system for joint image and video interactive segmentation. Our model, Sa2VA, integrates SAM-2 into current VLM models into one end-to-end model and aims to unify image and video for dense, grounded understanding, including segmentation, chat, and caption. Referring Segmentation. This task aims to output specific masks (for image inputs) or tracked masks (for video input) driven by language description. Earlier works [13, 38, 52, 80, 91, 101] explore various fusion modules to achieve better performance. Then, several stronger models [84] adopt transformer-based methods to achieve unified segmentation and tracking in video. Equipped with LLMs, several recent works [40, 65, 83, 99] propose more complex referring tasks, including reasoning referring or joint mask and caption generation. For example, LISA [40] involves reasoning-based segmentation. Then, GLaMM [66] annotates new dataset and proposes region-level caption and segmentation tasks. Meanwhile, several recent works explore joint instruction tuning on referring segmentation and conversation. Our method extends these studies into the video domain with the help of SAM-2, while keeping stronger performance on both image/video referring tasks 3 and VQA tasks. Video Segmentation and Grounding. Current video segmentation methods [34, 47, 105] focus on segmenting and tracking pixels in closed sets. Several works [25, 103] explore open-vocabulary settings. However, the concepts are still limited compared with the knowledge space of LLMs. For video grounding, recent works [28] explore LLM into the joint understanding of video and audio. However, there is no fine-grained spatial-temporal modeling or enough image data for co-training, which makes the results unconvincing. Our model, Sa2VA, adds interactive chat and open-ended understanding to current dense video perception models and achieves strong VQA performance. 3. Method 3.1. Unifying Multi-task Representations Developing unified model for addressing diverse image and video understanding tasks is challenging due to the distinct formats of these tasks. To overcome this, we begin by reexamining the task formulations and propose unified representation approach, laying the groundwork for the development of Sa2VA. Referring Image/Video Object Segmentation. For image referring segmentation, given input text tokens Ti RN D, the model takes input images Ii RHW 3 and outputs corresponding binary masks Mo RHW that align with text description. and are the number and dimension of text tokens. For video object referring segmentation, the model takes input videos Vi RT HW 3 and outputs binary spatio-temporal masks (masklets) Mo RT HW . , H, and are video frame number, height, and width. Image/Video Chat and Grounded Caption Generation. For image and video chat tasks, given input text tokens Ti and corresponding images Ii or video Vi, the model outputs answer text To. For grounded caption generation tasks, the model jointly outputs the corresponding masks Mo and aligned text To. Visual Prompt Understanding Tasks. For visual prompt understanding tasks, in addition to text tokens Ti and image Ii, the model further takes extra visual prompt tokens Pi (boxes or points on the image) as inputs and output corresponding masks Mo and aligned text answers To. Unified Task Representation. Existing works solve the above tasks via specific design models or partial unified models (different tasks have different model weights). In this work, we point out that all the above tasks can be unified as one-shot instruction-tuning process, since we can leverage the flexibility of LLMs to handle various visual tokens. The overall process can be formulated as the following: To, Mo = LLM ({Ii, Vi, Pi}, Ti). (1) For chat-only tasks, the model only outputs text tokens To. For referring segmentation tasks, the model outputs masks (image) or masklets (video) Mo. For grounded caption generation tasks, the model outputs both texts and masks. For different tasks, the input visual tokens are changing according to specific tasks. Since Mo (whether image masks or masklets) can be controlled with SEG tokens, one SEG token represents one prompt input of SAM-2s decoder. Thus, it is natural to put all these tasks into one shared encoderand-decoder framework and co-train one LLM and finetune the SAM decoder. 3.2. Sa2VA Framework It The overall architecture of Sa2VA is shown in Fig. 2. contains two parts: the LLaVA-like model and SAM-2. Pre-trained MLLMs. We adopt pre-trained LLaVA-like It contains one visual encoder, models as the MLLMs. one visual projection layer, and one LLM. The visual encoder takes input images, video, and sub-images as inputs. The visual projection layer maps inputs into visual tokens. These tokens, combined with the input text tokens, are the input of LLMs and perform output text token prediction. Note that we adopt pre-trained MLLMs following previous works [40, 66, 85, 99]. For both image and video chat datasets, we follow the same pipeline [2, 9] without further modification. Decoupled Design. We append SAM-2 alongside the pretrained LLaVA model. We do not take the SAM-2s output tokens (visual features or decoder outputs) into LLM. There are three reasons. First, we want to make the combination as simple as possible without increasing extra computation costs. Secondly, adding extra tokens needs an extra alignment process. Thirdly, via this design, we can fully make our work as plug-in-play framework to utilize pre-trained MLLMs since the MLLM community goes fast. Thus, we adopt decoupled design without introducing further communication between LLaVA and SAM-2. Tuning SAM-2 Decoder via SEG Tokens. We connect SAM-2 and the MLLM via the special token \"[SEG]\". The hidden states of the \"[SEG]\" token are used as new type of prompt and fed into SAM-2s Decoder, where they are decoded into segmentation masks. The hidden states of \"[SEG]\" can be seen as novel spatial-temporal prompt for SAM-2. SAM-2 segments the corresponding object mask in image and video based on the spatial-temporal prompt. During training, the SAM-2 decoder can be tuned to understand the spatial-temporal prompt, and gradients can be backpropagated through the \"[SEG]\" token to the MLLM, allowing it to output the spatial-temporal prompt better. Utilizing SAM-2 knowledge for mask tracking. For RefVOS tasks, we design simple framework to achieve strong results on public benchmarks. In particular, for giving input video, we adopt \"[SEG]\" token to generate the masks of 4 Figure 2. Our proposed Sa2VA model. The model first encodes the input texts, visual prompts, images, and videos into token embeddings. These tokens are then processed through large language model (LLM). The output text tokens are used to generate the [SEG] token and associated language outputs. The SAM-2 decoder receives the image and video features from the SAM-2 encoder, along with the [SEG] token, to generate corresponding image and video masks. with the largest object area from the video and mask out the non-object pixels. The cropped and full images are then fed separately into InternVL2-76B [9] to generate detailed descriptions. The descriptions are processed by Qwen2-72B [86] for consistency checking, with conflicting descriptions discarded. 2. Scene-level annotation. Yellow contours are used to highlight the object in the image. Both the image and the object/part-level description from the previous stage are sent into InternVL2-76B [9] to generate detailed object description that includes relationships to the scene and surrounding objects. 3. Video-level annotation. We uniformly sample 8 frames from the video, applying yellow borders to highlight the object in each frame. These frames, along with the scene-level description, are processed by InternVL276B to generate video-level description, capturing the objects movement and action. Ref-SAV Training Dataset. Using the above pipeline, we have automatically annotated detailed object expressions for the SA-V dataset, resulting in the creation of Ref-SAV. This dataset contains 37,311 videos and 72,509 object expressions. Notably, we refrain from augmenting the object expressions, ensuring that each object has only one corresponding expression. Ref-SAV Evaluation Benchmark. We select subset of videos from the SA-V datasets training set to construct the Ref-SAV evaluation benchmark, as the validation and test sets contain only limited number of videos. These videos are strictly orthogonal to Ref-SAVs training dataset. The Figure 3. Data annotation pipeline. Our proposed automatic data annotation pipeline consists of three stages: object/part-level, scene-level, and video-level text expression annotation. We use different colors in the final expression to highlight the information derived from each stage. Best view on screen and zoom out. the key frames. Then, we use the memory encoded by the key frame features to generate the mask for the remaining frames. We detailed this process in the appendix. 3.3. Ref-SAV Dataset and Benchmark Data Annotation Pipeline. We have carefully designed an automatic annotation pipeline to generate referring object text expressions for the SA-V dataset [67]. As shown in Fig. 3, the pipeline consists of 3 stages: 1. Object/part-level annotation. We first select the frame Table 2. Experiment results on image/video referring segmentation benchmarks and image/video chat benchmarks. Method LLAVA-1.5-13B [54] Video-LLaVA-7B [51] LLaMA-VID-7B [50] mPLUG-Owl3-8B [89] InternVL2-8B [9] PixelLM-7B [68] LaSagnA [77] LISA-7B [40] GLaMM-7B [66] LLaVA-G-7B [96] GSVA-13B [83] OMG-LLaVA-7B [99] VISA-13B [85] Sa2VA-1B (Ours) Sa2VA-4B (Ours) Sa2VA-8B (Ours) Sa2VA-26B (Ours) Image Segmentation RefCOCO+ [36] RefCOCO [36] RefCOCOg [90] MeViS [13] Video Segmentation Ref-DAVIS17 [38] ReVOS [85] Image Chat MME [19] MMBench [59] SEED-Bench [41] Video Chat Video-MME [20] MMBench-Video [17] GCG GCG [66] - - - - - 73.0 76.8 74.1 79.5 77.1 79.2 78.0 72.4 77.4 78.9 81.6 82.5 - - - - - 66.3 66.4 62.4 72.6 68.8 70.3 69.1 59. 69.9 71.7 76.2 78.8 - - - - - 69.3 70.6 66.4 74.2 71.5 75.7 72.9 65.5 72.3 74.1 78.7 79.7 - - - - - - - - - - - - 44.5 50.8 52.1 57.0 57.3 - - - - - - - - - - - - 70. 72.3 73.8 75.2 77.0 - - - - - - - - - - - - 50.9 47.6 53.2 57.6 58.4 1531 - 1521 - - 309/135 0/0 1/1 14/9 - - 1177/235 - 1381/405 1536/530 1617/511 1691/538 68.8 60.9 65.1 77.6 81.7 17.4 0.0 0.4 36.8 - - 47.9 - 68.3 77.3 81.6 83.7 70.1 - 59.9 - 76.2 - - - - - - 56.5 - 64.8 73.3 75.1 76.8 - 39.9 - 53.5 54.0 - - - - - - - - 39.9 50.4 52.1 52.6 - 1.03 1.08 1.35 1.28 - - - - - - - - 1.07 1.23 1.34 1.45 - - - - - - - - 28.9 - - 29.9 - 23.8 28.2 31.0 33.5 evaluation benchmark consists of two parts: 1) The longexpression set, generated using the above automatic annotation pipeline and carefully filtered by human annotators. 2) The short expression set, which is entirely manually annotated. The evaluation benchmark includes 1,147 videos and 1,945 object expressions, of which 1,694 are long expressions and 251 are short expressions. For training and testing referring examples, please refer to the appendix. 3.4. Sa2VA Training and Testing. One For All Co-Training. Sa2VA is co-trained multidatasets. For VQA tasks, we adopt text regression loss Ltext as common MLLMs. For segmentation tasks Lmask, we apply pixel-wised cross-entropy loss LCE and dice loss LDICE. Not that there is no pre-trained stag as previous works [53], we perform supervised fine-tuning in one-shot training. Lintruction = Ltext + Lmask, Lmask = LCE + LDICE. (2) In Sec. 4, in addition to co-training for one model, we further report specific tuned models on single tasks following [66, 99] to enable fair comparison. One For All Testing. All tasks can be encompassed within the Eq. 1 paradigm. During the inference stage, we encode the necessary task requirements, such as text prompts, visual prompts, image, and video features, into tokens to input into the LLM. The output tokens of LLM are then decoded into text responses (LLM prediction head), segmentation mask (SAM-2 decoder), and SAM-2 mask tracking module responses according to the task definition. We strongly recommend that the readers check the detailed model inference table in the appendix. 4. Experiments Baseline. We construct the baseline by combining the SOTA MLLM models like InternVL2 [9] and SAM2 [67]. Table 3. Datasets used for experiments. Type Datasets LLaVA 1.5 (665K) Image QA Image Segmentation RefCOCO (17K), RefCOCO+ (17K), RefCOCOg (22K), Grand-f (214K) Video QA Video Segmentation ChatUniVi (100K) Ref-YTVOS (3.5K), MeVIS (0.6K), ReVOS (1.7K), Ref-SAV (37K) Similar to previous works [40, 66, 99], the segmentation mask is obtained by decoding the hidden state of the [SEG] token through SAM2s decoder. Inspired by Mask2FormerVIS [10], an object shares the same [SEG] token throughout the video frames, allowing our model to handle both image and video referring segmentation tasks in unified way. In addition, we further use more advanced MLLM models, InternVL2-5 to show the model scaling effect of Sa2VA. Datasets and Metrics. We use four types of datasets to train Sa2VA, including image QA, video QA, image segmentation, and video segmentation datasets. As shown in Tab. 3, Sa2VAs training data contains approximately 1.1M image-text or video-text pairs. Since InternVL2 has been trained with large amount of image QA and video QA data, we only used 665K LLaVA 1.5 [57] and 100K ChatUniVi [35] data to prevent the MLLM from forgetting its image and video QA capabilities. We used 56K referring expression data [36, 90] and 214K grounding conversation generation data [66] for image-level text-driven segmentation. For video-level referring expression segmentation, we used 5.8K existing referring VOS data from RefYouTubeVOS [69], MeVIS [13], and ReVOS [85]. Additionally, we used 37K long text referring VOS data generated by our proposed automatic annotation pipeline to enhance Sa2VAs understanding of long referring text and its object grounding capabilities for complex videos. For image referring segmentation, we adopt cIoU. For referring video object segmentation, we adopt J&F. For image and video chat tasks, we follow previous works [50, 53] and report performance accordingly. Table 4. Performance on image-level benchmarks. Our method achieves the best accuracy trade-off between image chat and referring segmentation datasets. Method MME [19] MMBench [59] SEED-Bench [41] AI2D [37] MMStar [7] MMMU [93] SQAtest [60] RefCOCO RefCOCO+ RefCOCOg LISA-7B [40] PixelLM-7B [68] LaSagnA-7B [77] GLaMM-7B [66] OMG-LLaVA-7B [99] Sa2VA-4B (ours) Sa2VA-8B (ours) 1/1 309/135 0/0 14/9 1177/235 1553/540 1651/578 0.4 17.4 0.0 36.8 47.9 76.8 82.4 - - - - 56. 72.6 75.5 0.0 0.0 0.0 28.2 42.9 79.9 82.1 - - - - - 53.7 60.3 - - - - - 46.2 44.7 - - - - - 95.8 96.8 74.1 73.0 76.8 79.5 78.0 80.4 81.9 62.4 66.3 66.4 72.6 69. 74.3 76.5 66.4 69.3 70.6 74.2 72.9 76.7 78.9 Table 5. Ref SAV validation sets. zs: zero-shot testing. ft: trained with our proposed Ref-SAV training dataset. Table 6. Comparison with Fine-tuned Models. J&F Overall Method UniRef++ [79] (zs) UNINEXT [84] (zs) MeVIS [13] (zs) VISA [85] (zs) Sa2VA-8b (zs) Sa2VA-8b (ft ) Long 10.8 8.3 7.1 12.2 50.9 60.4 14.1 11.7 12.1 16.1 47.7 57.0 J&F 12.5 10.0 11.3 14.1 49.3 58.7 9.0 5.8 6.2 12.3 31.5 39.5 Short 8.2 4.4 5.3 9.6 8.6 5.1 5.5 9.2 35.0 42.9 33.3 41.2 11.6 8.8 12.2 13.2 39.6 48. 9.5 6.4 9.8 11.3 43.0 51.7 J&F 10.5 7.6 10.3 11.8 41.3 50.0 Model Type RefCOCO RefCOCO+ RefCOCOg LAVT [88] GlaMM-7B [66] OMG-LLaVA-7B [99] F-LLM-7B [82] Sa2VA-4B (ours) Sa2VA-8B (ours) 72.7 79.5 78.0 76.1 80.4 82.3 62.1 72.6 69.1 65. 74.3 77.3 61.2 74.2 72.9 68.5 75.7 79.3 Implementation Details. We adopt XTuner [12] codebase for training and testing. During the instruction tuning stage, the initial learning rate is set to 4e-5, with only the perception model kept frozen, and the LLM is fine-tuned with LoRA [27]. The maximum sequence length in the LLM is set to 8,192. All training is conducted on eight NVIDIA A800 GPUs with 80GB of memory. The instruction tuning stage took 48 hours. We adopt VLMEvalKit [16] for chat evaluation. For our benchmark testing, we adopt original open-sourced codebase [79, 84, 85] and model weights to inference the video results. more detailed setup for each task can be found in the appendix. 4.1. Main Results Comparison With SOTA MLLMs. As shown in Tab. 2, Sa2V-8B achieves 81.6, 76.2, and 78.9 cIoU on RefCOCO, RefCOCO+, and RefCOCOg, respectively, surpassing GLaMM-7B by 2.1, 3.6, and 4.5 cIoU. Undoubtedly, Sa2VA sets new SOTA results on RefCOCO+ and RefCOCOg, significantly outperforming previous grounding MLLMs, including LISA, GLaMM, PixelLLM, PSALM, and OMG-LLaVA. Meanwhile, Sa2VA also demonstrates strong conversational capabilities, achieving 2128, 81.6, and 75.1 scores on MME, MMbench, and SEED-Bench, respectively, while existing grounding MLLMs perform poorly in conversation. Sa2VA achieves comparable performance to InternVL2 on image QA benchmarks, demonstrating that Sa2VA largely preserves the base MLLM InternVL2s image chat performance. Sa2VA also performs excellently on video benchmarks. Sa2VA achieves 57.0, 75.2, and 57.6 J&F on MeVIS, RefDAVIS17, and ReVOS, respectively, surpassing the previous SOTA VISA-13B by 12.5, 4.8, and 6.7 J&F. Furthermore, Sa2VA-8B achieves score of 1.34 on the video QA benchmark MMBench-Video, even surpassing InternVL28Bs 1.28. The main experimental results demonstrate that our Sa2VA is versatile and powerful MLLM. Due to page limitations, more results can be found in the appendix. Detailed Results on Image-Level Benchmarks. As indicated by [40, 82, 99], performing instruction tuning hurts the performance on chat tasks. We take co-training to handle this problem. In Tab. 4, we further report more image chat datasets. Our model can still achieve strong results on multiple image chat benchmarks while keeping SOTA referring segmentation results. Results on Ref-SAV validation set. In Tab. 5, we benchmark several state-of-the-art Ref-VOS models using our proposed Ref-SAV benchmark. We find that even the foundation model [84] and recent video MLLM model [85] cannot achieve strong results on our benchmarks. This is because our benchmark has more occlusion, longer text description, and part-whole annotations from SAM-2 [67]. Conversely, our method, Sa2VA, even with open-sourced datasets (Tab. 3), can achieve strong results. Our method can be boosted further with our training set, which means that our Ref-SAV training set is good supplementary to the video understanding community. Comparison with specific fine-tuned methods. Following previous works [66, 99], we also compare con-current MLLMs in specific fine-tuned datasets. As shown in Tab. 6, we find 1.5% improvements on three datasets over the cotraining model, and Sa2VA works best among recent works. It should be noted that our model with smaller size can still achieve stronger results than previous MLLMs. 4.2. Ablation Studies Effectiveness of Joint Co-training. Sa2VAs superior performance on image and video QA and segmentation bench7 Table 7. Ablation study on co-training effect on multiple datasets. Data All Data w/o Image QA w/o Image Segmentation w/o Video QA w/o Video Segmentation Image Segmentation RefCOCO+ RefCOCO RefCOCOg MeViS Video Segmentation Ref-DAVIS17 77.4 78.0 20.2 78.0 77.4 69.9 70.1 20.6 70.4 69.1 72. 72.2 23.2 72.6 72.4 50.8 48.3 38.0 50.7 44.4 72.3 73.0 48.8 74.3 69.0 Image Chat MMBench SEED-Bench Video Chat Video-MME MMBench-Video 68.3 63.4 70.1 69.1 67.8 64.8 63.8 65.7 65.0 64. 39.9 39.7 41.2 41.3 40.4 1.07 0.39 1.08 0.71 1.04 MME 1381/ 1298/359 1393/408 1370/402 1403/398 Table 8. Experiment results using stronger InternVL2.5 in our Sa2VA. Base MLLM Image Segmentation RefCOCO+ [36] RefCOCO [36] Video Segmentation Image Chat RefCOCOg [90] MeViS [13] Ref-DAVIS17 [38] MME [19] MMBench [59] SEED-Bench [41] AI2D [37] MMStar [7] SQAtest [60] InternVL2.0-4B InternVL2.0-8B InternVL2.5-4B InternVL2.5-8B 80.4 81.9 82.4 82.6 74.3 76.5 77.6 78.0 76.7 78.9 79.7 80. 52.1 57.0 55.9 58.9 73.8 75.2 73.7 75.9 1553/540 1651/578 1691/610 1690/610 76.8 82.4 81.8 84.4 72.6 75.5 74.9 76.5 79.9 82.1 81.4 82. 53.7 60.3 57.9 62.4 95.8 96.8 96.8 97.4 marks is attributed to unified instruction tuning and joint co-training. We conducted ablation studies on the training datasets, and the results are shown in Tab. 7. When training without image QA datasets, Sa2VAs performance on MME and MMBench drops by 129 and 4.9 scores, respectively. When training without image segmentation datasets, Sa2VA only achieves 20.2, 20.6, and 23.2 cIoU on RefCOCO, RefCOCO+, and RefCOCOg, while video segmentation benchmark performance declined significantly. Sa2VAs performance on MMBench-Video decreases by 34% when training without video QA datasets. When training without video segmentation datasets, Sa2VAs performance on MeVIS and Ref-DAVIS17 drops by 4.4 and 3.3 J&F, respectively. The ablation results demonstrate that joint cotraining with these four types of datasets is key to Sa2VAs exceptional performance across the various tasks. Ablation study on the Segmentation Token Design. We have explored several segmentation token designs, with the results shown in Tab. 9. When using the repeat strategy, which makes the LLM output [SEG] tokens for frames, this approach slightly decreases video segmentation performance due to being more prone to missing or repeating [SEG] tokens. We also explored using different segmentation tokens for different frames, such as setting [SEG 1] to [SEG n] to segment the object across frames 1 to n. However, this prevented knowledge sharing with image segmentation tasks, resulting in larger performance drop. Scale ability of Sa2VAs Data. In the main experiments, we have demonstrated that our Sa2VA can scale with model size. We conduct ablation experiments on additional data to verify that it can scale with data, as shown in Tab. 10. When we add 3M image QA data from Infinity-MM [22] (only stage-4), Sa2VA-1B shows 2.1 score improvement on MMBench, with minimal impact on the image and video segmentation benchmark performance. Using our annotated Ref-SAV data, Sa2VA-1B exhibits 1.7 J&F perforFigure 4. The samples of our Ref-SAV benchmark. Our proposed benchmark features multi-granularity, complex occlusion and reappearing, and both short and long-format text expressions. mance boost on the MeVIS benchmark. These results indicate that Sa2VAs performance can continue to improve as the scale of SFT data increases. Visualization examples. We provide some demos of Sa2VA, shown in Fig. 1. Additionally, in Fig. 4, we showcase the features of our Ref-SAV benchmark through samples that demonstrate multi-granularity, occlusion reappearance, and both short and long text expressions. These features make Ref-SAV challenging and comprehensive referring VOS benchmark. We provide more visualization results, ablation studies, and analysis in the appendix. We strongly recommend readers to check that. 5. Conclusion In this work, we present Sa2VA, versatile framework that integrates SAM-2, foundation video segmentation model, with LLaVA-like MLLMs, to achieve dense, grounded un8 Table 9. Ablation study on [SEG] token design. Table 12. Comparison with Vision Expert Models. Type RefCOCO RefCOCO+ RefCOCOg DAVIS MeVIS Single Repeat Multiple 77.4 77.3 77.6 69.9 70.2 70.3 72.3 72.5 72.4 72.3 71.1 68.6 50.8 49.6 46. Table 10. Ablation study on using more datasets. Dataset baseline Size 1.2M RefCOCO RefCOCOg MMBench MME 77.4 72.3 68.3 1381/405 MeVIS 50.8 Inifinity-MM [22] Ref-SAV 1.2M+3M 77.1(-0.3) 1.2M+37K 77.2(-0.2) 72.6(+0.3) 72.6(+0.3) 70.4(+2.1) 68.2(-0.1) 1396/346(-44) 1384/418(+16) 51.2(+0.4) 52.5(+1.7) Table 11. Comparison with Recent Video MLLMs. Model Type MeViS ReVOS Ref-DAVIS17 PG-Video-LLaVA [61] GLaMM + SAM2 [62] VideoGLaMM [62] ViLLa [102] VISA-13B [85] VideoLISA-3.8B [3] Sa2VA-4B (ours) 18.87 38.7 45.2 - 44.5 44.4 52.1 - - - - 50.9 - 53.2 - - - 64.4 70.4 68.8 73. derstanding of both images and video. Our method can handle various image and video understanding tasks, including referring image/video segmentation and image/video conversation, with just one-shot instruction tuning. By leveraging the knowledge from both LLaVA and SAM-2, our model has strong capabilities in both mask and language generation. To verify the effectiveness of our proposed method, we propose challenging referring video object segmentation benchmark, Ref-SAV. Based on our experimental results, Sa2VA achieves strong performance on various benchmarks from different tasks. 6. Appendix Overview. Our supplementary includes the following aspects: Appendix gives more experiment results on our Sa2VA models, datasets effectiveness and more tasks. Appendix presents more detailed descriptions of the model, comparison with existing MLLMs, and inference pipelines. Appendix shows more detailed process of training dataset generation process. Appendix gives visual examples of Sa2VA on various dense grounded tasks. Appendix discuss the shortcomings of Sa2VA, future directions of our Ref-SAV benchmarks, and board impacts. Model Type RefCOCO RefCOCO+ RefCOCOg MeVIS Ref-DAVIS17 LAVT [88] ReferFormer [78] UniRef++-L [79] EVF-SAM [100] LMPM [13] UniVS [46] Sa2VA-26B (ours) 72.7 - 81.4 82.4 - - 82.5 62.1 - 74.0 76.5 - - 78.8 61.2 - 76.0 78.2 - - 79.7 - 31.0 - - 37.2 - 57.3 - 61.1 67.2 - - 59. 77.0 Table 13. Sa2VA with Qwen2-VL model. MLLM Type Sa2VA: Qwen2-VL-2B [2] Sa2VA: Intern-VL2-1B Sa2VA: Intern-VL2-4B RefCOCO RefCOCO+ RefCOCOg Ref-DAVIS17 MeVIS ReVOS 76.9 77.4 78.9 68.7 69.9 71. 72.9 72.3 74.1 72.0 72.3 73.8 49.4 50.8 52.1 40.0 47.6 53.2 Table 14. Ref SAV validation sets. ft: trained with our proposed Ref-SAV training dataset. zs: zero-shot testing. Method Long J&F Short J&F Overall UniRef++ [79] (zs) 14.1 10.8 12. 9.0 8.2 8.6 11.6 9.5 J&F 10.5 UniRef++ (ft) 19.2 15.1 17.2 12. 11.7 12.0 15.8 13.4 14.6 Table 15. Region caption performance on RefCOCOg dataset. Method METEOR Sa2VA-4B OMG-LLaVA [99] Osprey [92] GLaMM [66] GRIT [81] Kosmos-2 [64] 17.3 15.3 16.6 16. 15.2 14.1 A. More Experiment Results Comparison with recent Video MLLM models. Recently, several works (including the not published work) also combine MLLMs into video referring segmentation tasks. As shown in Tab. 11, we list and compare these methods for fair comparison on three datasets. In particular, we also include recent work, VideoGLaMM, and combined strong baseline (SAM2 + GlaMM). The combined baseline requires more computation and parameter costs than our model. As shown in that table, our model still achieves the best results with 4B model size, along with the ability for image/video chat, GCG, and visual prompt understanding. To be noted our model achieves much stronger results than the combined baseline (SAM2 + GlaMM); this further shows the effectiveness of our joint-co-training and the necessity for task unification. Comparison with vision expert models. In Tab. 12, we further compare the recent vision expert models, which are specifically designed for referring segmentation. Note that we list the best model results. Our best model with 26B mode size still achieves stronger results among these vision expert models. Effectiveness of training dataset on more methods. We further show the effectiveness of our training dataset on more methods. In particular, we choose the previous strong vision expert, UniRef++. As shown in Tab. 13, with our training set, we find significant improvement. This in9 dicates that our automatic data engine narrows the domain gaps and improves the performance of existing Ref-VOS models. Sa2VA with more MLLMs. In Tab. 13, we further include our Sa2VA with other MLLMs. We choose QWen2-VL. Compared with InternVL2 (the default model in our paper), we find little performance drop in referring segmentation tasks. We will explore more MLLMs in future work. Results on visual prompts understanding tasks. We also report visual prompt understanding tasks, following previous works [92, 99]. As shown in Tab. 15, our method also achieves the best results among recent visual prompt understanding models. This indicates that Sa2VA can also generate strong region-aware captions. B. Model Details and Implementation Details Sa2VA model details of InternVL-2 baseline. In InternVL-2, for each video frame, we first resize it to 448 448, and then compress it into 256 tokens using the vision encoder of InternVL-2. The five key frames are arranged in temporal order and placed before the question. We modified the tokenizer and added the [SEG] token. The hidden state of the last layer in the LLM corresponding to the [SEG] token is used to generate the language prompt. During the inference process, if [SEG] is not present, it is assumed that the described object is not present in the video. If there is only single image input, we follow InternVL-2s dynamic resolution approach [9] to capture detailed information from the image. Sa2VA model details of Qwen2-VL baseline. In Qwen2VL [74], for each video frame, we first resize it to 448 448, and then compress it into 256 tokens following InternVL-2 for fair comparison. The five key frames are arranged in temporal order and placed before the question. We use M-RoPE as in Qwen2-VL [74]. We modified the tokenizer and added the [SEG] token. The hidden state of the last layer in the LLM corresponding to the [SEG] token is used to generate the language prompt. During the inference process, if [SEG] is not present, it is assumed that the described object is not present in the video. If there is only single image input, use the same dynamic resolution approach as in InternVL-2 for fair comparison. Sa2VA model details of Visual Prompt Understanding. We implement mask-based visual prompt understanding simply and effectively by cropping image features. Specifically, given an object mask, we resize it to match the image feature size and extract and flatten the image features from the overall view corresponding to the mask. The object will be represented by these vision tokens belonging to it. The format is: Object1: <vp><IMG_FEAT>, ..., <IMG_FEAT></vp>.\" <vp>\" and </vp>\" are two special text tokens that indicate the beginning and end of visual In subsequent dialogue, this object will prompt features. Algorithm 1: Ref-VOS Inference Pipeline 1 Input: Video length ; Number of key frames ; Video frames SN (X1, X2, X3,. . ., XN ); Language description ; 2 Output: Sequence of masks M1, M2, M3,. . ., MN ; 3 Run: Sa2VA Model for Ref-VOS; 4 Extract key frames: SM SN ; 5 Visual embeddings: Ev Encoder(SM ); 6 Language embeddings: El Encoder(T ); 7 Answers: LLM({Ev, El}); 8 Prompt embedding: Pl Linear(Find(A, [SEG])); 9 for = 1, 2, . . . , do 10 11 12 13 14 16 SAM-2 feature: Fi Encoder(X0); Mask: Mi Decoder({Pl, Fi}); Update Memory: em Cross-Attention({M em, Mi}); for = + 1, + 2, . . . , do SAM-2 feature: Fi Encoder(X0); Mask: Mi Decoder({M em, Fi}); Update Memory: em Cross-Attention({M em, Mi}); 17 emit M1, M2, M3,. . ., MN ; Table 16. Comparison with previous Ref-VOS benchmarks. Property Short Text Long Text Large Object Motion Large Camera Motion Heavy Occlusion DAVIS17-RVOS ReVOS Ref-YT-VOS MeVIS Ours be referred to as object1\". To obtain the ability to understand the visual prompt, we jointly co-train the model with Osprey-727K [92] and the datasets used in the main paper. Ref-VOS inference pipeline with SAM-2 knowledge. As described in Algorithm 1, the pipeline begins by extracting the first five frames of the input video as keyframes, ensuring that they capture the critical context for the following processing. These key frames are fed into CLIP and flattened to visual sequential tokens for LLM processing. The LLM takes the visual and language tokens as input and uses these tokens to extract information about the video to genIn SAM-2, the prompt encoder erate the [SEG] token. encodes boxes or clicks to prompt embeddings for object referring. Different from SAM-2, we use two linear layers to project the [SEG] token into the language prompt embedding, which serves as an extension of the SAM-2 prompt encoders. With the language prompt embedding, we use the SAM-2 decoder to generate the masks of the key frames. Then, we use the memory encoder of SAM-2 to generate memory based on the output key-frame masks. Finally, the memory attention in SAM-2 generates the remaining masks using the memory generated from the key-frame and previous non-key-frame masks. C. More Details on Ref-SAV Dataset Comparison with existing Ref-VOS benchmarks. In Tab. 16, we further compare the previous existing Ref-VOS benchmarks from five different aspects: short text, long text, large object motion, large camera motion, and heavy occlusion. 10 addition, understanding long text and improving long text grounding ability is also important to explore. Board impact. Our research explores unified solution of grounded understanding, VQA, and visual prompt understanding. Our work has application potential in the finegrained promptable video understanding the task, in particular, in zero-shot inference mode, as shown in our teaser in the main paper. In addition, with solid experiment results, our exploration will inspire the development of interactive video-dense understanding in the MLLM community. The previous benchmarks only contain partial aspects of these five challenging cases. On the other hand, our benchmark is built from SAM-2 [67] with long text, which is more challenging than previous benchmarks. This is why the previous Ref-VOS models cannot achieve good results on our benchmark. D. Visualization Results Results on image referring segmentation. As shown in Fig. 5, we visualize the image referring segmentation task. With different language descriptions, our Sa2VA can segment different masks with the clues from the description. Results on video referring segmentation. As shown in Fig. 6, our method performs well in handling diverse and challenging conditions, showcasing remarkable ability to adjust and perform effectively even in occlusion scenes or highly dynamic environments. Results on visual prompt understanding. As shown in Fig. 7, our method can generate descriptions based on the visual prompts. The descriptions generated by our method demonstrate high degree of contextual awareness, effectively capturing details within the visual cues. Results on GCG task. In Fig. 8, we further show several visual examples in grounded caption generation tasks. Compared with the previous SOTA model, OMG-LLaVA, our model achieves better results in both mask quality and text-mask alignments. The former indicates better-aligned segmentation outputs, while the latter shows the latter indicates good text-to-region alignments. E. Failure Cases and Future Work Failure cases of Sa2VA and Future work. Despite Sa2VA achieving stronger results on various ref-segmentation and video ref-segmentation datasets. However, there are still several cases of failure that need to be explored. We identify two shortcomings of Sa2VA in our future work. One is long video with hard, distinguished referring examples, as shown in Fig 9. This is because our Sa2VA works in online mode without knowing the entire video content to align the long, complex text. The other is how to improve the VQA tasks (in image and video) without hurting the referring segmentation tasks. As shown in the ablation table of the main paper, increasing the scale of VAQ data leads to performance drop in referring segmentation tasks. Thus, how to balance both types and keep the grounded prediction knowledge is still unsolved. Future directions on Ref-SAV benchmarks. As discussed in Tab. 16, long text, occlusion inputs, and longer video with camera motion are critical problems in our benchmarks. Thus, the new models need to design more robust RefVOS system to handle these challenges with several speIn cific designs, including more robust memory designs. Figure 5. Visualization results on image referring segmentation task."
        },
        {
            "title": "References",
            "content": "[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In ICCV, 2015. 2 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 3, 4, 9 [3] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. NeurIPS, 2024. 9 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. 3 [5] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Making large multimodal models understand arbitrary visual prompts. In CVPR, 2024. 3 [6] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In ICCV, 2023. 2 [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evalarXiv preprint uating large vision-language models? arXiv:2403.20330, 2024. 7, [8] Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, and Chunyuan Li. Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing. arXiv preprint arXiv:2311.00571, 2023. 3 [9] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. arXiv preprint arXiv:2404.16821, 2024. 3, 4, 5, 6, 10 [10] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention In mask transformer for universal image segmentation. CVPR, 2022. 6 [11] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Figure 6. Visualization results on video referring segmentation. Lee, and Alexander Schwing. Putting the object back into video object segmentation. In CVPR, 2024. 2 [12] XTuner Contributors. Xtuner: toolkit for efficiently fine-tuning llm. https://github.com/InternLM/ xtuner, 2023. 7 [13] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale benchmark for In ICCV, video segmentation with motion expressions. 2023. 3, 6, 7, 8, 9 [14] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. [15] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. In NeurIPS, 2024. 3 [16] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source In toolkit for evaluating large multi-modality models. 13 Figure 7. Visualization results on visual prompt understanding task. We use the masks predicted by our model under the GCG task as visual prompts, and generated region-level descriptions for these masks. The object masks and their captions for the corresponding region are highlighted in the same color. Figure 8. Visualization results on GCG tasks. Top: our method. Bottom: OMG-LLaVA [99]. Note that, our method has stronger and fined-grained grounding ability and text alignment than OMG-LLaVA [99], previous strong baseline. 14 Figure 9. Visualization failure cases. ACMMM, 2024. 7 [17] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. 6 [18] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: unified pixel-level vision llm for understanding, generating, segmenting, editing. In NeurIPS, 2024. 2 [19] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 3, 6, 7, 8 [20] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 6 [21] Brent Griffin, Victoria Florence, and Jason Corso. Video object segmentation-based visual servo control and object depth estimation on mobile robot. In WACV, 2020. 2 [22] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et al. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data. arXiv preprint arXiv:2410.18558, 2024. 8, 9 [23] Tianrui Guan, Divya Kothandaraman, Rohan Chandra, Adarsh Jagan Sathyamoorthy, Kasun Weerakoon, and Dinesh Manocha. Ga-nav: Efficient terrain segmentation for robot navigation in unstructured outdoor environments. RAL, 2022. 2 [24] Tianrui Guan, Ruitao Song, Zhixian Ye, and Liangjun Zhang. Vinet: Visual and inertial-based terrain classification and adaptive navigation over unknown terrain. In ICRA, 2023. 2 [25] Pinxue Guo, Tony Huang, Peiyang He, Xuefeng Liu, Tianjun Xiao, Zhaoyu Chen, and Wenqiang Zhang. Openvis: Open-vocabulary video instance segmentation. arXiv preprint arXiv:2305.16835, 2023. 4 [26] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940, 2023. [27] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. In Lora: Low-rank adaptation of large language models. ICLR, 2022. 7 [28] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In CVPR, 2024. 4 [29] Kuan-Chih Huang, Xiangtai Li, Lu Qi, Shuicheng Yan, and Ming-Hsuan Yang. Reason3d: Searching and reasoning 3d segmentation via large language model. In 3DV, 2025. 3 [30] Nisha Huang, Yuxin Zhang, and Weiming Dong. Style-avideo: Agile diffusion for arbitrary text-based video style transfer. SPL, 2024. 2 [31] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with arXiv preprint text by deep multi-modal transformers. arXiv:2004.00849, 2020. 3 [32] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. [33] Louis Martin Hugo Touvron, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and et al. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. 2, 3 [34] Sukjun Hwang, Miran Heo, Seoung Wug Oh, and Seon Joo Kim. Video instance segmentation using inter-frame communication transformers. In NeurIPS, 2021. 4 [35] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In CVPR, 2024. 6 [36] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. 6, 8 [37] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, 2016. 7, 8 [38] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In ACCV, 2018. 3, 6, [39] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. In ICML, 2024. 2 [40] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, 2024. 3, 4, 6, 7 [41] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: BenchIn CVPR, marking multimodal large language models. 2024. 6, 7, 8 [42] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 3 [43] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixtureof-experts model. arXiv preprint arXiv:2410.05993, 2024. 2 [44] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. 3 [45] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In frozen image encoders and large language models. ICML, 2023. [46] Minghan Li, Shuai Li, Xindong Zhang, and Lei Zhang. Univs: Unified and universal video segmentation with prompts as queries. In CVPR, 2024. 9 [47] Xiangtai Li, Haobo Yuan, Wenwei Zhang, Guangliang Cheng, Jiangmiao Pang, and Chen Change Loy. Tube-link: flexible cross tube baseline for universal video segmentation. In ICCV, 2023. 4 [48] Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, and Chen Change Loy. Omg-seg: Is one model good enough for all segmentation? In CVPR, 2024. 2, 3 [49] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, 2023. 3 16 [50] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In ECCV, 2024. 2, 3, [51] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In EMNLP, 2024. 2, 3, 6 [52] Chang Liu, Henghui Ding, and Xudong Jiang. GRES: Generalized referring expression segmentation. In CVPR, 2023. 3 [53] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2, 3, 6 [54] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 6 [55] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 3 [56] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In ECCV, 2024. [57] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In ECCV, 2024. 6 [58] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In CVPR, 2024. 2 [59] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multimodal model an all-around player? In ECCV, 2024. 3, 6, 7, 8 [60] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022. 7, 8 [61] Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, and Fahad Khan. Pg-video-llava: Pixel grounding large video-language models. arXiv preprint arXiv:2311.13435, 2023. 9 [62] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and Salman Khan. Videoglamm: large multimodal model for pixel-level visual grounding in videos. arXiv preprint arXiv:2411.04923, 2024. [63] Prashant Patil, Akshay Dudhane, Ashutosh Kulkarni, Subrahmanyam Murala, Anil Balaji Gonde, and Sunil Gupta. An unified recurrent video object segmentation IEEE framework for various surveillance environments. TIP, 2021. 2 [64] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 9 [65] Lu Qi, Yi-Wen Chen, Lehan Yang, Tiancheng Shen, Xiangtai Li, Weidong Guo, Yu Xu, and Ming-Hsuan Yang. Generalizable entity grounding via assistance of large language model. arXiv preprint arXiv:2402.02555, 2024. 3 [66] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. Glamm: Pixel grounding large multimodal model. In CVPR, 2024. 1, 3, 4, 6, 7, 9 [67] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 2, 3, 5, 6, 7, 11 [68] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In CVPR, 2024. 3, 6, [69] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with large-scale benchmark. In ECCV, 2020. 6 [70] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics for In CVPR, knowledge-based visual question answering. 2023. 2 [71] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 3 [72] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. 3 [73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023. 2, 3 [74] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [75] Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint, 2023. 2 [76] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. In arXiv, 2024. 2 [77] Cong Wei, Haoxian Tan, Yujie Zhong, Yujiu Yang, and Lin Ma. LaSagnA: Language-based segmentation assistant for 17 complex queries. arXiv preprint arXiv:2404.08506, 2024. 6, 7 [78] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video object segmentation. In CVPR, 2022. [79] Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, and Ping Luo. Uniref++: Segment every reference object in spatial and temporal spaces. In ICCV, 2023. 7, 9 [80] Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, et al. Towards language-driven video inpainting via multimodal large language models. In CVPR, 2024. 3 [81] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: generative region-to-text transformer for object understanding. In ECCV, pages 207224. Springer, 2025. 9 [82] Size Wu, Sheng Jin, Wenwei Zhang, Lumin Xu, Wentao Liu, Wei Li, and Chen Change Loy. F-lmm: GroundarXiv preprint ing frozen large multimodal models. arXiv:2406.05821, 2024. 2, 3, 7 [83] Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, and Gao Huang. Gsva: Generalized segmentation via multimodal large language models. In CVPR, 2024. 3, 6 [84] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Zehuan Yuan, Ping Luo, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In CVPR, 2023. 3, 7 [85] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. arXiv preprint arXiv:2407.11325, 2024. 2, 3, 4, 6, 7, 9 [86] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 2, [87] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683, 2024. 2 [88] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In CVPR, 2022. 7, 9 mPLUG-Owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint, 2024. 6 [90] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, 2016. 6, 8 [91] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara Berg. Mattnet: Modular attention network for referring expression comprehension. In CVPR, 2018. 3 [92] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel In CVPR, understanding with visual instruction tuning. 2024. 3, 9, [93] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 7 [94] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models. IJCV, 2024. 3 [95] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In EMNLP, 2023. 2 [96] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Chunyuan Li, Jainwei Yang, et al. Llava-grounding: Grounded visual chat with large multimodal models. In ECCV, 2024. 6 [97] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 3 [98] Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, and Pengfei Wan. DVIS: Decoupled video instance segmentation framework. In ICCV, 2023. 2 [99] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Change Loy Chen, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. In NeurIPS, 2024. 1, 2, 3, 4, 6, 7, 9, 10, 14 [100] Yuxuan Zhang, Tianheng Cheng, Rui Hu, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, and Xinggang Wang. Evf-sam: Early vision-language fusion for arXiv preprint text-prompted segment anything model. arXiv:2406.20076, 2024. [101] Zheng Zhang, Yeyao Ma, Enming Zhang, and Xiang Bai. Psalm: Pixelwise segmentation with large multi-modal model. In ECCV, 2024. 3 [89] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. [102] Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, and Hengshuang Zhao. Villa: Video reasoning 18 segmentation with large language model. arXiv preprint arXiv:2407.14500, 2024. 9 [103] Hao Zhou, Tiancheng Shen, Xu Yang, Hai Huang, Xiangtai Li, Lu Qi, and Ming-Hsuan Yang. Rethinking evaluation metrics of open-vocabulary segmentaion. arXiv preprint arXiv:2311.03352, 2023. [104] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. arXiv preprint arXiv:2405.01434, 2024. 2 [105] Feng Zhu, Zongxin Yang, Xin Yu, Yi Yang, and Yunchao Wei. Instance as identity: generic online paradigm for video instance segmentation. In ECCV, 2022."
        }
    ],
    "affiliations": [
        "Bytedance Seed",
        "Peking University",
        "UC Merced",
        "Wuhan University"
    ]
}