{
    "paper_title": "Rethinking Expert Trajectory Utilization in LLM Post-training",
    "authors": [
        "Bowen Ding",
        "Yuhan Chen",
        "Jiayang Lv",
        "Jiyao Yuan",
        "Qi Zhu",
        "Shuangshuang Tian",
        "Dantong Zhu",
        "Futing Wang",
        "Heyuan Deng",
        "Fei Mi",
        "Lifeng Shang",
        "Tao Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories."
        },
        {
            "title": "Start",
            "content": "RETHINKING EXPERT TRAJECTORY UTILIZATION IN LLM POST-TRAINING , Jiayang Lv2 , Jiyao Yuan4 , Qi Zhu4, Shuangshuang Tian2 , , Futing Wang1,2 , Heyuan Deng4 , Fei Mi4 , Lifeng Shang4 , Tao Lin2,3 , Bowen Ding1,2 , Yuhan Chen2 Dantong Zhu2 1 Zhejiang University 2 School of Engineering, Westlake University 3 Institute of Advanced Technology, Westlake Institute for Advanced Study 4 Huawei Noahs Ark Lab 2{dingbowen, wangfuting, lintao}@westlake.edu.cn 4{yuanjiyao1, zhuqi41, dengheyuan, mifei2, Shang.Lifeng}@huawei.com 5 2 0 D 2 1 ] . [ 1 0 7 4 1 1 . 2 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting Less is More in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories. Code: https://github.com/LINs-lab/RETU."
        },
        {
            "title": "INTRODUCTION",
            "content": "The transformation of pre-trained Large Language Models (LLMs) into powerful Large Reasoning Models (LRMs) hinges on effective post-training, which typically interleaves Supervised FineTuning (SFT) and Reinforcement Learning (RL) (DeepSeek-AI, 2025; GLM et al., 2025). SFT leverages expert trajectories (prompt-solution pairs) to instill reasoning priors via imitation, while RL methods such as GRPO (Shao et al., 2024) allow models to leverage prompt-answer pairs to selfexplore reasoning paths through reward incentives, without the expert trajectory utilization. Despite the consensus on the necessity of both, critical question remains unresolved: What is the optimal mechanism to utilize expert trajectories (i.e., SFT data) to maximize the posttraining performance ceiling? The methodology for effective expert trajectory utilization currently faces an unresolved paradigm dilemma. Recent works propose Synchronized SFT-RL (Syn-SFT-RL) algorithms, such as UPT (Lv et al., 2025), SRFT (Fu et al., 2025), and LUFFY (Yan et al., 2025), which integrate imitation loss directly into the RL optimization loop. While these methods often claim superior efficiency over sequential approaches, this advantage is critically constrained by their reliance on limited SFT data (only about 46K). This raises fundamental question: whether Syn-SFT-RL can maintain its claimed superiority and robustness when provided with the substantially large-scale data necessary for achieving state-of-the-art ceilings. Work done at Westlake University as an intern. Corresponding authors. 1 Figure 1: The conceptual overview of LLM post-training. Sequential SFT-then-RL (blueorange) achieves the highest performance ceiling Apost, outperforming Pure RL (orange) and Synchronized SFT-RL (striped blueorange) paths. Insets highlight that larger, harder data increases plasticity, and RL should start during the Stable SFT. Conversely, some LLM practitioners (Yang et al., 2025; GLM et al., 2025; DeepSeek-AI, 2025) typically favor the straightforward sequential SFT-then-RL pipeline. However, the principles governing this successful approach remain largely empirical and lack systematic definition in two critical areas. First, concerning the Optimal Timing for switching from SFT to RL, the criteria lack systematic definition. Second, regarding Data Properties, although the Less is More (Ye et al., 2025; Muennighoff et al., 2025) approach achieves comparable SFT accuracy with minimal data, it is unclear whether this compromises the subsequent RL scaling potential or leads to premature convergence. Similarly, while harder data push SFT boundaries (Tong et al., 2024; Zhang et al., 2025a), its precise influence on the overall post-training ceiling remains unclarified. Consequently, these tensions highlight the urgent need for unified framework to understand how the characteristics of SFT data dictate the entire post-training performance. To rigorously address these systemic gaps, we propose Plasticity-Ceiling analytical framework in 4. This framework provides unified view of all paradigms and enables the quantitative decomposition of the theoretical performance ceiling (Apost) into two measurable components: the SFT Performance (Psft) achieved under SFT compute xsft, and the remaining RL Plasticity (P Lrl), which represents the maximum potential for subsequent RL improvement. By conducting extensive experiments with the large-scale (i.e., 889K samples) SFT data on Qwen2.5-7B (Qwen et al., 2025) and validating on Llama3.2-3B (Meta AI, 2024) across six mathematical benchmarks, we demystify expert trajectory utilization and establish rigorous standard for post-training scaling: ➊ Sequential Paradigm Dominance ( 6.1). We empirically establish the superiority of the Sequential SFT-then-RL pipeline over the unstable, sensitive Synchronized approach, as well as pure SFT and RL. robust SFT phase is necessary to establish the foundational SFT performance (Psft) and unlock the maximum plasticity (P Lrl) of subsequent RL. ➋ Switch RL until SFT Saturation ( 6.2.1). We identify the Stable or Mild-Overfitting Sub-phase of validation loss saturation as the optimal SFT-to-RL transition window, where the Psft is maximized and Lrl is uncompromising. ➌ Scale and Difficulty Extend Ceiling ( 6.2.2). We refute the Less is More hypothesis in the context of SFT-then-RL scaling. While minimal data yields SFT efficiency, the SFT data scale remains the primary determinant of the final ceiling, while the trajectory difficulty acts as multiplier. Furthermore, the minimum SFT validation loss serves as robust predictor of the final post-training ceiling. Our contributions are summarized as follows: ➊ We propose the Plasticity-Ceiling Framework, theoretical mechanism that decomposes post-training performance into realized SFT performance and the subsequent RL plasticity to guide paradigm selection. ➋ We systematically benchmark di2 verse training strategies, identifying the Sequential SFT-then-RL pipeline as the rigorous standard for stability and performance over synchronized approaches. ➌ We formulate precise operational guidelines for scaling, linking data properties and training dynamics to the final reasoning ceiling to enable predictable post-training development."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Post-Training Paradigms. Post-training primarily relies on Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). While theoretical works attempt to unify them (Swamy et al., 2025; Wang et al., 2025), they exhibit distinct empirical behaviors regarding generalization and distribution shifts (Huan et al., 2025; Shenfeld et al., 2025). The sequential SFT-then-RL strategy is the industrial standard (Yoshihara et al., 2025; Vattikonda et al., 2025), though optimizing the transition is nontrivial; Kang et al. (2025) caution that high SFT scores can be misleading, as over-fitted models may fail to improve during RL. Alternatively, Synchronized SFT-RL methods like LUFFY (Yan et al., 2025), UPT (Lv et al., 2025) and SRFT (Fu et al., 2025) integrate imitation directly into RL to boost efficiency. Our work systematically compares these paradigms to identify the optimal mechanism for maximizing the performance ceiling. Expert Trajectories Utilization. The properties of SFT data critically influence post-training. Regarding scale, Less is More philosophy suggests that minimal, high-quality data suffices for SFT (Ye et al., 2025; Muennighoff et al., 2025). However, others argue that scale remains essential for complex reasoning (Sun et al., 2025). Regarding difficulty, methods like MetaMath (Yu et al., 2023) and D3 (Zhang et al., 2025a) demonstrate that harder, difficulty-aware data selection improves SFT outcomes. Crucially, prior works often evaluate SFT in isolation. We extend this inquiry to the RL phase, investigating how SFT data scale and difficulty dictate the models plasticity (its headroom for subsequent RL scaling) rather than just immediate imitation accuracy."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "We summarize the algorithmic foundations used in our study: (1) standard supervised fine-tuning (SFT), (2) reinforcement learning (GRPO and DAPO), and (3) synchronized SFTRL (Syn-SFTRL) fusion methods used as single-stage baselines. 3.1 SFT SFT tunes the policy πθ via imitation learning using the answer and expert trajectory pair (q, τ ) in the SFT dataset DSFT: JSFT(θ) = E(q,τ )DSFT t=1log πθ(τt q,τ<t) (cid:104)(cid:80)τ (cid:105) (1) Such paradigm reliably imparts instruction-following and basic reasoning priors (Abdulhai et al., 2023), but its performance is bounded by the training distribution (Ouyang et al., 2022) and lacks exploratory capability. 3.2 RL RL extends the model beyond imitation by optimizing reward-guided exploration. GRPO (Shao et al., 2024) and DAPO (Yu et al., 2025) are two widely-used RL algorithms. GRPO replaces critic with group-normalized advantage (Ai,t). For each query-answer pair (q, a) in dataset DRL, GRPO samples response trajectories {τi}G i=1 based on the old policy πθold. Each trajectory receives rule-derived reward score Ri. The group-normalized advantage is computed as: Ai,t = Ri mean({Rj}G std({Rj}G j=1) j=1) . (2) With the advantage, GRPO aims to maximize the expected advantage while regularizing the policy towards reference policy πref via the KL divergence term β DKL [πθπref]. The policy loss 3 JGRPO(θ) is: JGRPO(θ) = (cid:80)G 1 i=1 τi i=1 t=1 (q,a)DRL, {τi}G i=1πθold (cid:88) τi (cid:88) min (cid:0)rt Ai,t, Ai,t (cid:1) (3) β DKL [πθπref] , = πθ(τi,tq,τi,<t) where rt token τi,t. Its clipped counterpart, trust region, preventing excessively large and destabilizing policy updates. πθold (τi,tq,τi,<t) represents the importance ratio between the new and old policies for i, 1 ϵ, 1 + ϵ), confines the policy update within = clip(rt DAPO (Yu et al., 2025) further stabilizes training via asymmetric clipping (ϵlow, ϵhigh) and dynamically filter the prompts with all correct or wrong on-policy generations. We adopt DAPO as our primary RL algorithm due to its robustness on mathematical reasoning tasks."
        },
        {
            "title": "3.3 SYN-SFT-RL",
            "content": "The Syn-SFT-RL paradigm merges SFT and RL by injecting expert trajectories into the optimization loop. We introduce three typical algorithms: LUFFY (Yan et al., 2025), SRFT (Fu et al., 2025), and UPT (Lv et al., 2025). LUFFY modifies the JGRPO(θ) in Eq. 3 by jointly optimizing on-policy trajectories and off-policy ones. It removes both the KL regularization and importance-ratio clipping, and aggregates tokenlevel advantages over mixture of SFT and RL data. The mixture dataset DMIX contains triplets (q, {τj}N j=1 (N =1 as the official setup), and answer a. Hence, LUFFYs loss is formalized as: j=1, a) with the prompt q, expert trajectories {τj}N JLUFFY(θ) = (q,{τj }N {τi}G j=1,a)DMIX i=1πθold (cid:34) 1 + 1 (cid:88) τi (cid:88) i=1 t=1 (cid:88) τj (cid:88) ˆrt ˆAj,t j=1 t=1 (cid:35) , rt ˆAi,t (4) where = (cid:80)N j=1 τj + (cid:80)G computed without normalization: i=1 τi normalizes over all tokens, and the mixed advantages are ˆAi,t = Ri mean (cid:16) {Rj}N j=1 {Ri}G i=1 (cid:17) , (5) To avoid entropy collapse on off-policy data, LUFFY further applies regularized importance shaping, which transforms the importance ratio rt + γ) with small constant γ = 0.1. = rt to ˆrt j/(rt SRFT combines four components: (i) the standard SFT loss JSFT in Eq. 1, (ii) the off-policy loss Joff from LUFFY (the first term in Eq. 4), and (iii) on-policy objectives for positive and negative trajectories in Eq. 6. For on-policy positive rollouts {τ + i=1 and on-policy negative ones {τ j=1 , SRFT maximizes the likelihood of positive trajectories while suppressing that of negative ones: }GM }M Jpos(θ) = τ + (cid:88) (cid:104) log πθ (cid:104) Jneg(θ) = t= τ (cid:88) t=1 log πθ (cid:0)τ + i,t qi, τ + i,<t (cid:1)(cid:105) , (cid:0)τ j,t qj, τ j,<t (cid:1)(cid:105) . The final SRFT objective uses entropy-guided dynamic weights: JSRFT = w1 JSFT + Joff + w2 Jpos + Jneg, w1 = 0.5 stop_grad(cid:0)eH(πθ )(cid:1), w2 = 0.1 stop_grad(cid:0)eH(πθ )(cid:1). 4 (6) (7) where H(πθ) denotes the policy entropy and stop_grad prevents gradients from flowing through the weights. UPT employs hard gating mechanism to mix SFT and RL. Let denote the average reward over the trajectories sampled for the current prompt q, and γ be threshold. UPT defines mixed loss where (fp, gp) are determined by p: JUPT = fp JSFT + gp JGRPO, (fp, gp) = (cid:40) (1, 0), γ, (0, 1), > γ. (8) (9) When the model performs poorly on prompt (p γ), the gate prioritizes SFT-style imitation. Once the reward exceeds the threshold (p > γ), the gate switches to pure GRPO optimization to focus on exploration."
        },
        {
            "title": "4 THE PLASTICITY-CEILING FRAMEWORK",
            "content": "To systematically evaluate the trade-offs between different post-training paradigms, we propose the Plasticity-Ceiling analytical framework. Unlike prior works that study SFT or RL scaling in isolation (Chen et al., 2025; Khatri et al., 2025), our framework treats the SFT-then-RL pipeline as unified continuum. This allows us to quantify the respective contributions of the SFT and RL phases to the overall post-training performance ceiling (Apost), whose functional form is defined in 1. 4.1 DECOMPOSE THE POST-TRAINING PERFORMANCE Formally, we decompose the post-training performance Ppost of the typical SFT-then-RL pipeline into three distinct components based on the training stages: Ppost(xsft, xrl) = P0 + (Psft(xsft) P0) (cid:124) (cid:125) (cid:123)(cid:122) SFT gain, Psft(xsft) + (Prl(xrl) Psft (xsft)) (cid:125) (cid:123)(cid:122) RL gain, Prl(xrl) (cid:124) Psft (xsft = 0) = P0, Prl(xrl = 0) = Psft (xsft) , (10) where P0 denotes the base models initial performance, and xsft, xrl denote the compute cost (in FLOPs measured in Appx. C) during the SFT and RL phases, respectively. This decomposition explicitly isolates the performance contributors: Psft represents the gain realized from SFT given cost xsft, while Prl represents the gain from RL given cost xrl. Note that Eq. 10 reduces to Pure-SFT when xrl = 0, and to Pure-RL (including Syn-SFT-RL variants) when xsft = 0. 4.2 CEILING AND PLASTICITY To estimate the asymptotic ceiling, we model the SFT or RL performance (x) as function of compute using sigmoidal power laws (Khatri et al., 2025)1: (x) = Pstart + Pstart 1 + (x/Cmid)B , (11) where and Cmid dictate convergence dynamics. Based on this formulation, we define two fundamental properties that characterize the scaling process: Definition 1 (Asymptotic Ceiling). The ceiling, denoted by A, represents the maximum performance achievable as computation goes to infinity. 1The sigmoidal power law enables the characterization of the scaling of most SFT or RL runs, except the unstable training instances, such as SRFT in Figure 2 (Left). Definition 2 (Plasticity). The plasticity, denoted by = Pstart, measures the effective headroom available for improvement from the starting performance Pstart. Then, we can extend these concepts into the SFT-then-RL pipeline. Firstly, the SFT phase costs xsft compute to achieve foundation performance Psft(xsft). After that, the RL phase initiates from the realized SFT outcome, the RL performance extends towards the ultimate Post-training Ceiling (Apost) as the following scaling formulation: Ppost(xsft, xrl) = Psft(xsft) + Apost Psft(xsft) 1 + (xrl/Cmidrl )Brl . (12) Consequently, the RL plasticity becomes Lrl = Apost Psft(xsft). Crucially, unlike Lsft which is fixed for given dataset, Lrl is dynamic and depends on the quality of the SFT starting point. Theoretical Implication. The framework reveals fundamental insight: maximizing SFT efficiency depends solely on Psft. However, if the SFT data is suboptimal (e.g., limited in scale), it may shrink the Lrl and thereby constrain Apost. In this work, we focus on the upper bound Apost resulting from different expert utilization training configurations. This bound is derived by fitting sigmoidal power law (Eq. 11) to the training points (Compute, Performance). We adopt robust fitting strategy detailed in Appx. to estimate these curves, which yields all Apost values presented in 6.1 and 6.2. The detailed fitting results are presented in Table 4."
        },
        {
            "title": "5 EXPERIMENTAL SETUP",
            "content": "To determine the optimal mechanism for utilizing expert trajectories, we organize experiments progressively to address three core research questions: RQ1: Paradigm Selection. Among Pure-RL, Pure-SFT, Synchronized SFT-RL, and Sequential SFT-then-RL, which paradigm establishes the most effective post-training baseline, and what are their characterizations? RQ2: Optimal SFT-to-RL Transition. Building upon the optimal paradigm identified in RQ1, what is the optimal time to transit to RL from SFT for maximum final ceiling? RQ3: Data Properties (Scale & Difficulty). With the paradigm (RQ1) and optimal timing strategy (RQ2) established, what roles do data scale and difficulty play in maximizing the performance ceiling, and do they support or refute the Less is More hypothesis? 5.1 MODELS AND DATA Models. We primarily use Qwen2.5-7B (Qwen et al., 2025) in 6.1 and 6.2, and Llama3.23B (Meta AI, 2024) in 6.3 for cross-validation. In 6.1, we apply Syn-SFT-RL algorithms to Qwen2.5-Math-7B (Yang et al., 2024) to further examine the influence of model priors. Training Data. We construct SFT datasets of varying scales and difficulties by curating mathematical trajectories from distilled DeepSeek outputs (Zhao et al., 2025; Tian et al., 2025). The resulting datasets include the large-scale SFT889K with around 889K unique samples, three medium-scale variants controlled for difficulty (Uniform/Easy/Hard102K, refer to Table 3 for the difficulty classification), and held-out validation set Val-199 with 199 prompt and trajectory pairs. To test data efficiency, we also include S1K-1.1 (Muennighoff et al., 2025) (S1K for short), containing 1K highquality R1-style trajectories. For RL in the SFT-then-RL pipeline, we use RL62K, filtered prompt set from Skywork-OR1RL (He et al., 2025). For Syn-SFT-RL methods, we augment RL62K with expert trajectories in SFT889K to create MIX37K, which is the subset of SFT889K. Refer to Appx. E.1 for details. Benchmarks. To prevent data leakage, we filter out the benchmark prompts with over 0.8 cosine similarity against our training set using Qwen3-8B-Embedding (Zhang et al., 2025c). We evaluate on the resulting 2,157 unique problems from the following cleaned benchmarks (counts denote original 6 Figure 2: Computeperformance scaling of post-training paradigms under different initialization (Left) Initializing from Qwen2.5-7B. Early RL-like runs converge quickly (exconditions. cept unstable instances), while early SFT shows mild performance disruption due to policy shift (Zhang et al., 2025b). (Middle) Initializing from saturated SFT checkpoint (10,800 steps on Qwen2.5-7B). SFT-then-DAPOd outperforms other paradigms. DAPOd (74.3) and LUFFY (72.7) yield the highest ceilings among pure RL and Syn-SFT-RL paradigms, respectively. (Right) Initializing from Qwen2.5-Math-7B. UPT and LUFFY demonstrate notable efficiency advantages in this setting. to cleaned): GSM8K (Cobbe et al., 2021) (1319 to 1317), OlympiadBench (He et al., 2024) (675 to 291), Minerva (Lewkowycz et al., 2022) (272 to 262), MATH (Lightman et al., 2023) (500 to 237), and AIME24/25 (LI et al., 2024) (30 to 25). We report the average performance on these unique problems unless otherwise specified. 5.2 TRAINING AND EVALUATION Training. Our experiments include two primary paradigms: (1) Syn-SFT-RL: we implement LUFFY (Yan et al., 2025), SRFT (Fu et al., 2025), and UPT (Lv et al., 2025) using the official codebase and recommended configurations. (2) Sequential SFT-then-RL: we first fine-tune the base model on SFT data, then apply RL on the fine-tuned checkpoints. For comparison in 6.1, we adopt GRPO and DAPOd (GRPO with dynamic difficulty sampling (Yu et al., 2025)) as the Pure-RL baseline. In the SFT-then-RL pipeline, we use the enhanced DAPOdc method, which further incorporates asymmetric ratio clipping into DAPOd. See Appx. for full implementation details. Evaluation. We report pass@1 accuracy sampled with temperature of 0.7 and top-p 1.0 to ensure generation diversity. For the smaller AIME24/25 datasets, we use Avg@16 for robust estimation. All responses are generated with maximum length of 8,192 tokens."
        },
        {
            "title": "6 EXPERIMENTAL RESULTS",
            "content": "6.1 PARADIGMS COMPARISON To determine the optimal paradigm (RQ1), we systematically benchmark four approaches: PureSFT, Pure-RL (GRPO, DAPOd), Syn-SFT-RL (LUFFY, SRFT, UPT), and the SFT-then-RL pipeline. To ensure fairness, all RL (or Syn-SFT-RL) runs utilize MIX37K, distribution-consistent subset of SFT889K. Figure 2 demonstrates that MIX37K suffices to capture performance limits, as RL (or Syn-SFT-RL) methods typically saturate or destabilize within single epoch. Limitations of Syn-SFT-RL. Contrasting prior claims (Yan et al., 2025; Lv et al., 2025; Fu et al., 2025), our experiments reveal severe practical limitations in Syn-SFT-RL methods, which exhibit training instability. For instance, SRFT shows performance fluctuations with standard deviation 2.6 higher than the stable DAPOd baseline in Figure 2 (Left) and fails to converge stably from saturated SFT checkpoint (Figure 2 Middle). Furthermore, they are highly sensitive to model priors. UPTs superior efficiency is limited to Qwen2.5-Math-7B (Figure 2 Right) and vanishes on general-purpose models, quickly plateauing below GRPO and DAPOd (Figure 2 Left). RL Variants Trade Ceiling for Efficiency. Pure-RL, and stable Syn-SFT-RL methods show common trade-off: superior initial efficiency but restricted ceiling. While GRPO, DAPOd, and 7 Figure 3: SFT Compute Scaling Dynamics of the SFT-then-RL Pipeline across Diverse Data Properties. The charts illustrate the evolution of the post-training ceiling (Apost) against increasing SFT compute (xsft). Apost is decomposed into the SFT Performance (Psft) and RL Plasticity (P Lrl). Background colors highlight the SFT sub-phases (Adaptive, Stable, Mild, and Severe Overfitting) defined by validation loss in 6.2.1. More details refer to 4. LUFFY surge to around 71.5 points within 25 exaFLOPs (outperforming Pure-SFTs 69.8), they plateau prematurely, yielding negligible subsequent gains (Figure 2 Left). This suggests that without dedicated supervised phase for internalizing reasoning patterns, improvement headroom is structurally limited. SFT Foundation and Sequential RL Maximization. In contrast, Pure-SFT demonstrates Slow but High scaling, achieving continuous improvement through extensive imitation to reach peak of 76.9 points, significantly surpassing Pure-RL and Syn-SFT-RL ceilings, which are 74.3 and 72.7, respectively. Crucially, transitioning to RL after SFT saturation successfully unlocks further gains (Figure 2 Middle). The SFT-then-RL pipeline (SFT DAPOd) achieves the best performance with 78.1 points among all baselines, extending the post-training performance frontier by optimally synergizing the SFT performance with further RL improvement. Answer to RQ1: Sequential SFT-then-RL is the superior paradigm. Large-scale SFT provides the necessary robust foundation, which sequential RL then leverages to maximize the final performance frontier. 6.2 SFT-THEN-RL PIPELINE Building on the superiority of the SFT-then-RL paradigm ( 6.1), we now examine the two key factors governing its final ceiling (Apost): SFT compute allocation (RQ2) and data properties (RQ3). We first establish robust timing strategy, followed by an analysis of data scale and difficulty impacts. 6.2.1 THE IMPACT OF SFT COMPUTE ALLOCATION Balancing realized SFT performance (Psft) against preserving RL plasticity (P Lrl) is crucial for determining the optimal SFT-to-RL transition. SFT Sub-phases. To rigorously identify the optimal transition point, we temporally partition the SFT process based on the SFT validation loss L(xsft). The entire trajectory is segmented based on 8 the following mathematically defined regions: Tstable = { xsft L(xsft) (1 + δ)Lmin }, Tmild = {xsft (1 + δ)Lmin < L(xsft) < (1 + δ2) Lmin}, Tsevere = { xsft L(xsft) (1 + δ2) Lmin }. (13) where Lmin is the global minimum validation loss observed during training, (δ, δ2) are tolerance thresholds being set as (0.02, 0.1) empirically. Therefore, we have Adaptive Sub-phase (Padapt), where SFT is underfitting in the region. Padapt = {xsft xsft < min Tstable} (14) Stable Sub-phase (Pstable), where the validation loss saturates within small tolerance threshold of 2% (i.e., δ = 0.02). Pstable = Tstable (15) Mild Overfitting Sub-phase (Pmild), where the region where loss rises slightly but remains below the 10% tolerance, representing the risky sweet spot. Pmild = {xsft xsft > max Tstable and xsft Tmild} (16) Severe Overfitting Sub-phase (Psevere), where loss significantly diverges ( 10% rise when δ2 = 0.1), leading to rapid plasticity collapse (see Easy102K in Figure 3). Psevere = {xsft xsft > max Tstable and xsft Tsevere} (17) The Dynamics of Post-training Ceiling. The blue solid line in Figure 3 illustrates how Apost evolves across these phases. We observe that initiating RL prematurely during the Adaptive Subphase is consistently suboptimal because the model lacks foundational competence that subsequent RL cannot fully recover. For instance, on SFT889K, switching early at 69.8 exaFLOPs yields ceiling of only 81.1 points, whereas extending training to the Stable Sub-phase (1047.6 exaFLOPs) boosts the ceiling to its peak of 85.7 points. Ideally, for high-quality data (e.g., SFT889K, Hard102K), the Stable Sub-phase aligns perfectly with peak performance. However, on limited or simple datasets (e.g., S1K, Easy102K), the peak ceiling often shifts into the Mild Overfitting Sub-phase, indicating that slightly delayed transition is acceptable and can even be beneficial due to the improvement of Psft. Conversely, aggressively continuing SFT into the Severe Overfitting Sub-phase is detrimental. As demonstrated on Easy102K in Figure 3, training SFT to 335.9 exaFLOPs leads to rapid decline in the final ceiling due to collapse in RL plasticity. Answer to RQ2: Train SFT to Saturation. The optimal strategy is to surpass the Adaptive Phase and target the Stable Sub-phase, strictly avoiding Severe Overfitting to preserve the RL plasticity. While Mild Overfitting is permissible for small or easy datasets, the Stable Sub-phase remains the robust standard for scalable data to maximize the total ceiling. 6.2.2 THE IMPACT OF SFT DATA PROPERTIES Data scale and difficulty are critical determinants of the quality of the SFT prior. In this section, we focus on investigating how these two fundamental data properties influence the asymptotic posttraining performance ceiling. Larger Scale Begets Higher Ceiling. Comparing datasets of varying scales (S1K, Uniform102K, and SFT889K in Figure 3) reveals that while minimal data can achieve rapid initial SFT gains, extensive data scale is indispensable for reaching higher post-training ceiling. Initially, smallscale data exhibits deceptive efficiency: S1K achieves an SFT performance of approximately 73.8 points using only 2.3 exaFLOPs, matching the performance level that requires 69.3 exaFLOPs on Uniform102K and 174.6 exaFLOPs on SFT889K. However, this efficiency proves to be unsustainable. The realized SFT performance Psft of S1K saturates prematurely at this level. In contrast, Uniform102K and SFT889K continue to improve with additional compute, reaching peak SFT performances of 74.8 and 76.3, respectively, thereby establishing superior foundation for the subsequent RL phase. Crucially, large-scale SFT also preserves greater RL plasticity. SFT889K maintains an average Lrl of 9.4, exceeding both S1K and Uniform102K by 5.7 points. Consequently, by leveraging both higher realized SFT performance Psft and enhanced RL plasticity Lrl, large-scale SFT unlocks significantly higher post-training ceiling. Harder Data Elevates the Ceiling. Controlling for scale (102K samples), we examine the impact of trajectory difficulty using Easy102K, Uniform102K, and Hard102K. Figure 3 (Bottom Row) reveals that training on harder trajectories yields superior returns. Hard102K achieves higher average SFT performance (Psft) of 74.6 points, outperforming Easy102K and Uniform102K by 1.5 and 0.8 percentage points, respectively. More importantly, data difficulty positively correlates with subsequent RL potential. Hard102K maintains the highest average Lrl of 5.4, surpassing Easy102K and Uniform102K by 1.2 and 1.7 points, respectively. Consequently, the synergistic combination of higher SFT performance and enhanced RL plasticity makes harder data the superior choice for maximizing the post-training ceiling. Minimum Validation Loss as Predictive Indicator. compelling finding across diverse SFT data configurations is the strong negative correlation (Pearson = 0.90) between the minimum SFT validation loss and the maximal subsequent post-training ceiling (Apost), as shown in Figure 5a. This establishes minimum validation loss as valuable priori indicator requiring no expensive RL training: lower minimum loss reliably signals greater overall post-training capacity within the SFT-then-RL pipeline. Answer to RQ3: Scale Dominates, Difficulty Optimizes. Refuting Less is More, we establish Data Scale as the primary factor to improve the post-training ceiling, while Difficulty acts as multiplier. Harder trajectories are helpful when the data scale is limited. Thus, scaling must prioritize volume before difficulty, with the final potential reliably predicted by the minimum SFT validation loss. Table 1: Llama3.2-3B validation results. We report the maximum post-training performance (max Ppost) and minimum SFT validation loss (Min. Val Loss). The strong negative correlation (Pearson = 0.98) between SFT loss and peak post-training performance confirms that the SFT validation loss is reliable predictor of the performance ceiling. DAPOd and DAPOdc are DAPO variants for the fair comparison, whose difference is detailed in B.2.1. The highest performance and lowest loss are bolded."
        },
        {
            "title": "Methods",
            "content": "Llama3.2-3B DAPOd UPT LUFFY S1K Easy102K Uniform102K Hard102K SFT889K SFT889K DAPOd S1K DAPOdc Easy102K DAPOdc Uniform102K DAPOdc Hard102K DAPOdc SFT889K DAPOdc"
        },
        {
            "title": "Paradigm",
            "content": "max Ppost Min. Val Loss -"
        },
        {
            "title": "Pure RL",
            "content": "Syn-SFT-RL"
        },
        {
            "title": "SFT",
            "content": "SFT-then-RL SFT-then-RL 10 2.3 2.2 12.2 8.5 24.0 52.0 53.2 55.3 67.1 68.7 24.9 53.7 55.1 56.3 70. - - - - 0.7 0.59 0.54 0.50 0.40 - - - - - - Figure 4: The analysis of the max post-training performance max Ppost when performing SFT-thenRL on Llama3.2-3B. SFT is performed on SFT889K. Stable sub-phase begets higher max Ppost results. 6.3 THE VALIDATION ON LLAMA3.2-3B We validate our findings on Llama3.2-3B (Meta AI, 2024) to ensure generalization across model architectures and sizes. To prioritize practical relevance, we report the maximum achieved posttraining performance (max Ppost) instead of the theoretical ceiling (Apost), with all RL training capped at 200 steps. For RQ1, Table 1 confirms that the SFT-then-RL pipeline is vastly superior, boosting the Llama3.23B baseline to 68.7 points with 29 gain. In contrast, single-stage methods struggle significantly: Pure-RL (DAPOd) and Syn-SFT-RL (LUFFY) yield only minimal improvements, and even UPT trails the sequential approach by over 56 points. For RQ2, consistent with our earlier findings, Llama3.2-3B also achieves peak performance during the SFT Stable Sub-phase at 532.5 exaFLOPs, as shown in Figure 4. We observe that with Llama3.2-3B, light SFT (Adaptive Sub-phase) fails to unlock the potential of RL and can even lead to performance degradation (at 17.8 exaFLOPs). In contrast, as SFT is intensified, max Ppost rises steadily. Thus, for smaller models, training SFT to saturation becomes even more critical to approaching the models maximum post-training potential. For RQ3, to ensure fair comparison near the performance ceiling, we select the SFT checkpoint exhibiting the minimum validation loss from each data configuration for the subsequent RL phase. The impact of SFT data properties shows the same pattern as Qwen2.5-7B as follows: Data Scale Dominance: As shown in Table 1, the SFT-then-RL instance trained on the largest dataset (SFT889K) achieves the highest overall performance of 70.1, significantly exceeding the models trained on the 102K-scale (Uniform102K) and 1K-scale (S1K) datasets. 11 Difficulty Optimization: While increasing difficulty (Hard102K) yields consistent gains over easier subsets(Uniform102K, Easy102K), it cannot compensate for the performance gap caused by insufficient scale. Predictive Power of Validation Loss: the strong negative correlation between minimum SFT validation loss and the final performance ceiling persists (Pearson = 0.98), reinforcing validation loss as robust indicator of post-training potential, consistent with our observations in 6.2.1."
        },
        {
            "title": "7 CONCLUSIONS",
            "content": "This work presents the Plasticity-Ceiling Framework for optimizing expert trajectory utilization, formalizing the trade-off between supervised fine-tuning performance (Psft) and reinforcement learning plasticity (P LRL). We derive three core principles for effective scaling: (1) The sequential SFT-then-RL pipeline outperforms alternative paradigms in approaching the post-training performance ceiling. (2) Within this pipeline, RL should be initiated at SFT saturation, point reliably predicted by validation loss minimization. (3) SFT data scale primarily determines the performance ceiling, and trajectory difficulty further optimizes the ceiling when data is limited. Together, these findings transform expert trajectory optimization from empirical guesswork into systematic and predictable process, establishing rigorous standard for maximizing reasoning model performance."
        },
        {
            "title": "REFERENCES",
            "content": "Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: Large-scale machine learning on heterogeneous distributed systems, 2016. URL https://arxiv.org/abs/1603. 04467. Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models, 2023. URL https://arxiv.org/abs/2311.18232. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. URL https://arxiv.org/abs/2305.13245. Yangyi Chen, Binxuan Huang, Yifan Gao, Zhengyang Wang, Jingfeng Yang, and Heng Ji. Scaling laws for predicting downstream performance in llms, 2025. URL https://arxiv.org/ abs/2410.08527. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Volcano Engine. VERL utils: FLOPs counter (line 149). https://github.com/ volcengine/verl/blob/59049a66/verl/utils/flops_counter.py#L149, 2023. version 59049a6; Accessed: 2024-12-01. Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, and Dongbin Zhao. SRFT: single-stage method with supervised and reinforcement fine-tuning for reasoning. arXiv preprint arXiv:2506.19767, 2025. doi: 10.48550/ arXiv.2506.19767. URL https://arxiv.org/abs/2506.19767. Team GLM, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen 13 Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, and Jie Tang. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025. URL https: //arxiv.org/abs/2508.06471. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, and Xiang Yue. Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning, 2025. URL https://arxiv.org/abs/2507. 00432. P.J. Huber and E.M. Ronchetti. Robust Statistics. Wiley Series in Probability and Statistics. Wiley, 2011. ISBN 9781118210338. URL https://books.google.com.hk/books?id= j1OhquR_j88C. Hugging Face. Math-verify. https://github.com/huggingface/Math-Verify, 2024. Boris Iglewicz and David Hoaglin. How to detect and handle outliers, volume 16. Asqc Quality Press Milwaukee, WI, 1993. Feiyang Kang, Michael Kuchnik, Karthik Padthe, Marin Vlastelica, Ruoxi Jia, Carole-Jean Wu, and Newsha Ardalani. Quagmires in sft-rl post-training: When high sft scores mislead and what to use instead, 2025. URL https://arxiv.org/abs/2510.01624. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. Devvrit Khatri, Lovish Madaan, Rishabh Tiwari, Rachit Bansal, Sai Surya Duvvuri, Manzil Zaheer, Inderjit S. Dhillon, David Brandfonbrener, and Rishabh Agarwal. The art of scaling reinforcement learning compute for llms, 2025. URL https://arxiv.org/abs/2510.13786. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858. Christophe Leys, Christophe Ley, Olivier Klein, Philippe Bernard, and Laurent Licata. Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median. Journal of Experimental Social Psychology, 49(4):764766, 2013. ISSN 0022-1031. doi: https://doi.org/10.1016/j.jesp.2013.03.013. URL https://www.sciencedirect.com/ science/article/pii/S0022103113000668. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, Numinamath. [https://github.com/project-numina/aimo-progress-prize](https: //github.com/project-numina/aimo-progress-prize/blob/main/ report/numina_dataset.pdf), 2024. and Stanislas Polu. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Xingtai Lv, Yuxin Zuo, Youbang Sun, Hongyi Liu, Yuntian Wei, Zhekai Chen, Lixuan He, Xuekai Zhu, Kaiyan Zhang, Bingning Wang, Ning Ding, and Bowen Zhou. Towards unified view of large language model post-training, 2025. URL https://arxiv.org/abs/2509.04419."
        },
        {
            "title": "Meta",
            "content": "AI."
        },
        {
            "title": "Llama\ncustomizable",
            "content": "with and https://ai.meta.com/blog/ open, llama-3-2-connect-2024-vision-edge-mobile-devices/, September 2024. Meta AI blog; accessed 2025-04-13; 15 minute read."
        },
        {
            "title": "Revolutionizing",
            "content": "models. vision edge 3.2: ai Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm, 2021. URL https://arxiv.org/abs/2104.04473. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Peter Rousseeuw. Least median of squares regression. Journal of the American statistical association, 79(388):871880, 1984. Peter J. Rousseeuw and Katrien Driessen. Computing lts regression for large data sets. Data Min. Knowl. Discov., 12(1):2945, January 2006. ISSN 1384-5810. doi: 10.1007/s10618-005-0024-4. URL https://doi.org/10.1007/s10618-005-0024-4. Peter Rousseeuw and Annick Leroy. Robust regression and outlier detection. John Wiley & Sons, 1987. Yangjun Ruan, Chris J. Maddison, and Tatsunori Hashimoto. Observational scaling laws and the predictability of language model performance, 2024. URL https://arxiv.org/abs/2405. 10938. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less, 2025. URL https://arxiv.org/abs/2509.04259. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 15 Yiyou Sun, Georgia Zhou, Hao Wang, Dacheng Li, Nouha Dziri, and Dawn Song. Climbing the ladder of reasoning: What llms can-and still cant-solve after sft?, 2025. URL https: //arxiv.org/abs/2504.11741. Gokul Swamy, Sanjiban Choudhury, Wen Sun, Zhiwei Steven Wu, and J. Andrew Bagnell. All roads lead to likelihood: The value of reinforcement learning in fine-tuning, 2025. URL https: //arxiv.org/abs/2503.01067. Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, and Xiangang Li. Deepdistill: Enhancing llm reasoning capabilities via large-scale difficulty-graded data training, 2025. URL https://arxiv.org/abs/2504.17565. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. https://arxiv.org/abs/2407. 13690, 2024. URL https://arxiv.org/abs/2407.13690. arXiv:2407.13690, cs.CL. Dheeraj Vattikonda, Santhoshi Ravichandran, Emiliano Peñaloza, Hadi Nekoei, Megh Thakkar, Thibault Le Sellier de Chezelles, Nicolas Gontier, Miguel Muñoz-Mármol, Sahar Omidi Shayegan, Stefania Raimondo, Xue Liu, Alexandre Drouin, Laurent Charlin, Alexandre Piché, Alexandre Lacoste, and Massimo Caccia. How to train your LLM web agent: statistical diagnosis. arXiv preprint arXiv:2507.04103, 2025. doi: 10.48550/arXiv.2507.04103. URL https://arxiv.org/abs/2507.04103. Bo Wang, Qinyuan Cheng, Runyu Peng, Rong Bao, Peiji Li, Qipeng Guo, Linyang Li, Zhiyuan Zeng, Yunhua Zhou, and Xipeng Qiu. Implicit reward as the bridge: unified view of sft and dpo connections, 2025. URL https://arxiv.org/abs/2507.00018. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance, 2025. URL https://arxiv.org/abs/ 2504.14945. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. URL https://arxiv.org/abs/2502.03387. Hiroshi Yoshihara, Taiki Yamaguchi, and Yuichi Inoue. practical two-stage recipe for mathematical llms: Maximizing accuracy with sft and efficiency with reinforcement learning, 2025. URL https://arxiv.org/abs/2507.08267. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, 16 Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. doi: 10.48550/arXiv.2503.14476. URL https://arxiv.org/abs/2503.14476. Jia Zhang, Chen-Xi Zhang, Yao Liu, Yi-Xuan Jin, Xiao-Wen Yang, Bo Zheng, Yi Liu, and Lan-Zhe Guo. D3: Diversity, difficulty, and dependability-aware data selection for sample-efficient llm instruction tuning, 2025a. URL https://arxiv.org/abs/2503.11441. Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, and Jingren Zhou. On-policy rl meets off-policy experts: Harmonizing supervised fine-tuning and reinforcement learning via dynamic weighting, 2025b. URL https://arxiv.org/abs/ 2508.11408. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025c. Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. 1.4 million open-source distilled reasoning dataset to empower large language model training, 2025. URL https://arxiv.org/abs/2503.19633."
        },
        {
            "title": "A EXPERIMENTAL PLATFORMS",
            "content": "All SFT experiments in 6.1 run on 16 GPUs; RL and Syn-SFT-RL experiments in 6.1 are implemented on 8 GPUs, and RL experiments in 6.2 are conducted on 128 Ascend 910B NPUs."
        },
        {
            "title": "B TRAINING CONFIGURATION",
            "content": "B.1 SFT We train SFT889K and all 102K variants with batch size of 512 and learning rate 1e-5 for 8 and 9 epochs, respectively. To study severe overfitting, we continue training Easy102K up to 6,120 steps (335.9 exaFLOPs). Checkpoints are saved every 360 steps (0.2 epochs for SFT889K, 1.8 epochs for 102K variants). For S1K, we follow the official setup: batch size 16, learning rate 1e-5, weight decay 1e-4, and train for 5 epochs, with checkpoints saved every 62 steps (1 epoch). B.2 RL PRACTICE For all RL and Syn-SFT-RL runs, we employ binary correctness reward, where correct trajectory receives reward of 1 and an incorrect trajectory receives 0. This correctness is verified using script powered by Math-Verify (Hugging Face, 2024). Furthermore, token-level loss aggregation is uniformly applied across all runs. B.2.1 THE RL IN PARADIGMS COMPARISON We summarize the RL configuration for the training in 6.1, including the Pure-RL (GRPO and DAPOd), and Syn-SFT-RL (LUFFY, SRFT, UPT). The shared training hyperparameters for PureRL and Syn-SFT-RL methods are summarized in Table 2. Unless specified, all algorithms use this default configuration. Pure-RL. GRPO and DAPOd serve as Pure-RL baselines. DAPOd adds the dynamic difficulty sampling strategy (Yu et al., 2025) on GRPO. For DAPOd, the dynamic difficulty sampling strategy employs batch size of 128 responses per inference round, and the asymmetric clipping ratio strategy is not applied. Default value Hyperparameter 128 Batch size 64 Update batch size 8 Rollout number 1024 Max prompt length 8192 Max response length 1e6 Learning rate 0.001 Entropy coefficient KL loss term removed Std. in group advantage norm removed Table 2: Shared training hyperparameters for GRPO, DAPOd, LUFFY and SRFT in 6.1. Syn-SFT-RL. We adopt training configurations from the Unify-Post-Training codebase (Lv et al., 2025). UPT uses smaller learning rate of 5e6 instead of 1e6. For rollout generation, UPT adaptively allocates up to 8 trajectories between on-policy and off-policy samples, whereas LUFFY (Yan et al., 2025) and SRFT (Fu et al., 2025) maintain fixed 7:1 ratio of on-policy to expert trajectories per prompt. The maximum trajectory length for all Syn-SFT-RL algorithms is set to 8192, as suggested by Yan et al. (2025), Fu et al. (2025), and Lv et al. (2025). B.2.2 THE RL IN SFT-THEN-RL PIPELINE Recognizing the superiority of DAPOd when starting from an SFT checkpoint in 6.1, we further improve DAPOd to DAPOdc as the RL method in the following SFT-then-RL experiments in 6.2 and 6.3. DAPOdc adds the asymmetric clipping ratio strategy to DAPOd, setting (ϵhigh, ϵlow) = 18 Table 3: Statistical summary of the constructed SFT datasets. The table lists average prompt and response lengths, as well as Win Rate (WR) across different DeepSeek model sizes. These metrics confirm the intended difficulty stratification, distinguishing the complexity levels of Easy, Uniform, and Hard subsets. Dataset Avg. prompt length Avg. response length WR (1.5B) WR (7B) WR (671B) EASY102K UNIFORM102K HARD102K SFT889K S1K 64 74 101 78 2253 3673 8532 3693 9884 1 0.79 0.09 - - 0.98 0.84 0.30 - - 0.99 0.87 0.41 - - (0.28, 0.2). In DAPOdc, each inference round uses 128 responses for dynamic difficulty sampling. The batch size and update batch size are 64, the learning rate is 1e-6, and the rollout number is 8. Maximum lengths for prompt and response are 1024 and 8192, respectively. Furthermore, the entropy and KL term coefficients are 0, and the group advantage normalization is enabled."
        },
        {
            "title": "C COMPUTE ESTIMATION",
            "content": "We adopt FLoating-point OPerations (FLOPs) as our computational metric because it is hardwareagnostic and parallelization-agnostic, depending only on model architecture and sequence lengths during training. We employ the FlopsCounter code (Engine, 2023) of the Verl framework (Sheng et al., 2024) for estimation. For SFT, FLOPs are estimated based on the sequence length of the SFT dataset; For RL and Syn-SFT-RL, we dynamically compute FLOPs using real-time prompt and response lengths recorded in TensorBoard (Abadi et al., 2016) logs. During training, both forward and backward cost the computation. Forward FLOPs Per-Token Estimation. The the theoretical forward FLOPs per token is denoted as Fforward_token, based on the model configuration and average sequence length S. Let be the number of layers, the hidden size, Hf the intermediate size of the feed-forward network, and the vocabulary size. For the attention mechanism, we define DKV as the total dimension of the Key and Value heads, accounting for Grouped Query Attention (GQA) (Ainslie et al., 2023). First, we define the parameter counts for the constituent dense components. The MLP block, which utilizes SwiGLU activation function with three linear projections (gate, up, and down), has parameter count PMLP. The linear projections in the attention layer (comprising WQ, WK, WV , WO) contribute Pattn_linear. The embedding layer and the language model head share the vocabulary-dim parameters, denoted as Pvocab. These are formulated as: PMLP = 3HHf Pattn_linear = H(H + 2DKV + H) = 2H(H + DKV ) (18) Pvocab = 2V The total FLOPs consists of the dense computation part (Fdense) and the attention score computation part (Fattn_core). The dense part aggregates the parameters from all layers and the vocabulary projections, multiplied by factor of 2 (for multiply-accumulate operations). The attention core part depends linearly on the sequence length S. The final estimation is given by: Fdense =2 [L (PMLP + Pattn_linear) + Pvocab] Fattn_core = 4 Fforward_token = Fdense + Fattn_core (19) Backward FLOPs Per-Token Estimation. According to Narayanan et al. (2021) and Kaplan et al. (2020), the theoretical backward FLOPs per token is approximately two times that of forward. Let Fforward_token be the theoretical backward FLOPs per token: Fbackward_token = 2 Fforward_token (20) C.1 SFT PER-STEP ESTIMATION Per-step SFT accounts for one forward and one backward pass per step. Let denote the batch size (number of sequences), the average sequence length used for fine-tuning. 19 The total number of tokens processed during SFT per step is given by Ttotal = S. Since the backward pass requires approximately twice the FLOPs of the forward pass, the total FLOPs per token during training is 3 Fforward_token. Therefore, the total computational cost for SFT, denoted as FSFT, is calculated as: Ftrain_token = Fforward_token + Fbackward_token = 3 Fforward_token FSFT = Ftrain_token = 3 Fforward_token (21) C.2 RL PER-STEP ESTIMATION DAPO. For DAPO, the computational cost per step is divided into Generation Phase (dynamic sampling) and Training Phase (actor update). Let Bgen denote the generation batch size, the number of dynamic sampling iterations, and the number of responses per prompt (i.e., group size). In the generation phase, the model explores large solution space by generating Bgen sequences. Since this phase involves only inference, the cost is purely forward FLOPs. In the training phase, subset of data (removing all correct and wrong trajectories) is selected, denoted by the training batch size Btrain (where Btrain < Bgen). The update step involves one forward pass to compute new log-probs and one backward pass. Following standard estimation, the combined update cost (forward + backward) is approximately 3 times the forward cost per token (Kaplan et al., 2020). Given the total sequence length = Sprompt + Sresponse, the FLOPs for one DAPO step are estimated as: Fgen = (K Bgen G) Fforward_token Ftrain = (Btrain G) 3 Fforward_token FDAPO = Fgen + Ftrain =(K Bgen + 3 Btrain) Fforward_token (22) GRPO. The algorithm serves as the baseline where no dynamic difficulty sampling is performed. In this setting, the generation batch size equals the training batch size (Bgen = Btrain = B) and sampling is performed once (K = 1). The model generates responses for all prompts in the batch and updates on all of them. Thus, the FLOPs estimation simplifies to: Fgen = (1 G) Fforward_token Ftrain = (B G) 3 Fforward_token FGRPO = Fgen + Ftrain = 4 Fforward_token (23) C.3 SYN-SFT-RL PER-STEP ESTIMATION LUFFY and SRFT. Both LUFFY and SRFT integrate expert demonstrations into the RL optimization loop. Let denote the number of on-policy sampled trajectories (group size) and denote the number of expert trajectories per prompt. In the Generation Phase, the model generates responses for each prompt in the batch B. In the Training Phase, the model updates parameters using both the on-policy generated data and the off-policy expert data. Thus, the effective training batch size per prompt becomes + (G = 7, = 1 in B.2.1). Given the real-time recorded average on/off-policy sequence length Son and Soff, the FLOPs for LUFFY and SRFT are calculated as the sum of inference cost on samples and update cost on + samples: Fgen =[(B G) Son + (B ) Soff] Fforward_token Ftrain =3[(B G) Son + (B ) Soff] Fforward_token FHybrid = Fgen + Ftrain =4[(B G) Son + (B ) Soff] Fforward_token (24) Note that for SRFT, although it computes multiple loss terms (Eq. 7), the dominant computational overhead remains the forward and backward passes through the transformer backbone on the combined data tokens (G + ), making this estimation applicable to both algorithms. 20 UPT. The per-step FLOPs of UPT are estimated dynamically based on the actual composition of the training batch, which consists of Non on-policy samples processed via GRPO and Noff expert samples processed via SFT. Let Son be the average on-policy sequence length. The algorithm processes Son tokens during the Generation Phase. In this phase, the computational cost is given by: Fgen = Son Fforward_token (25) Subsequently, the algorithm filters samples based on difficulty, retaining on on-policy samples and Noff off-policy samples per batch. Consequently, the FLOPs consumption during the Training Phase is: Therefore, the total computational cost for single UPT step is formulated as: Ftrain = 3 (Non Son + Noff Soff) Fforward_token FUPT = Fgen + Ftrain = [G Son + 3 (Non Son + Noff Soff)] Fforward_token (26) (27)"
        },
        {
            "title": "D ROBUST CURVE FITTING",
            "content": "In 4, we model the SFT and RL scaling progress using sigmoidal curves (Ruan et al., 2024; Khatri et al., 2025). To accurately model the relationship between computational investment (FLOPs) and model performance, particularly in the presence of training noise and potential anomalies, we employ robust curve-fitting pipeline. This pipeline integrates an iterative outlier detection mechanism based on Modified Z-scores with Least Trimmed Squares (LTS) regression optimization. D.1 DATA FORMULATION Let = {(xi, yi)}N i=1 denote the dataset, where xi represents cumulative FLOPs and yi represents the evaluation metric. The data is partitioned into training set Dtrain (first Nfit points) and held-out validation set Dval. Due to variations in training convergence across runs, the train-validation split may differ slightly. For most runs, approximately 85% of the data is used for training, with the remaining 15% reserved for validation. D.2 ROBUST ESTIMATION ALGORITHM Standard least-squares estimation is highly sensitive to anomalies. To derive scaling law that reflects the consistent signal rather than transient noise, we employ hierarchical robust optimization framework that integrates iterative statistical filtering (Modified Z-score) with subset-based optimization (Least Trimmed Squares) to isolate the true performance signal, ensuring that the derived scaling laws are predictive and generalizable across different compute regimes. Stage-1: Coarse Outlier Rejection (Modified Z-Score). First, we filter gross statistical anomaIn each iteration, we compute residuals ri = yi (xi; θ) and the median residual lies. = median(r). To quantify deviation robustly, we calculate the Median Absolute Deviation (MAD) (Huber & Ronchetti, 2011; Leys et al., 2013): Subsequently, the Modified Z-score Mi is computed as (Iglewicz & Hoaglin, 1993): MAD = median(ri r) Mi = 0.6745 (ri median(r)) MAD (28) (29) Points where Mi > τ are removed from the active training set. The factor 0.6745 normalizes the score such that it is consistent with the standard deviation under normal distribution, while the use of MAD ensures resilience against extreme values that would skew standard variance calculation. Stage-2: Least Trimmed Squares Regression. To further refine the model against subsets of data that may distort the global trend, we employ Least Trimmed Squares (LTS) (Rousseeuw, 1984; 21 Rousseeuw & Leroy, 1987) regression. Instead of minimizing the sum of all residuals, LTS regression minimizes only the smallest squared residuals: ˆθLTS = arg min (cid:88) (r2)(j)(θ) θ j=1 (30) where (r2)(1) (r2)(Nfit) are the ordered squared residuals over the training set, and = Nfit α is determined by the parameter α. We define (k+1) as the set of indices corresponding to the smallest squared residuals, i.e., (k+1) = {i r2 (r2)(h)}. We optimize this objective using the Concentration Step (C-step) algorithm (Rousseeuw & Driessen, 2006), which proceeds iteratively as follows: Estimation: Compute squared residuals r2 = (yi (xi; θ(k)))2 for all Nfit training points using the current parameters θ(k). Selection: Identify the index set (k+1) corresponding to the smallest squared residuals. Update: Update parameters to θ(k+1) by fitting the model strictly to the data points indexed by (k+1). Convergence: Repeat the process until the parameter estimate θ stabilizes. Fitting Results. Across all fitting instances, the average Root Mean Square Error (RMSE) on the validation split is 0.5, and the average fitting goodness R2 on the training split is 0.88, indicating robust fits. We present the fitting results including Pstart, Cmid and of Eq. 11 in Table 4, and visualize the SFT-then-RL curves in Figure 5."
        },
        {
            "title": "E DATASET CURATION",
            "content": "We summarize the key characteristics of our SFT data in Table 3. E.1 EXPERT TRAJECTORY COLLECTION We curate high-quality reasoning trajectories from two large-scale datasets: AM-DeepSeek-R1Distilled-1.4M (Zhao et al., 2025) and AM-DeepSeek-Distilled-40M (Tian et al., 2025). To ensure data distribution consistency and quality, we retain only mathematics-domain data, select trajectories distilled from DeepSeek-R1-671B (DeepSeek-AI, 2025) to unify trajectory style, and perform deduplication based on prompt matching. The resulting filtered datasets are denoted as amthink-1.4m and amthink-40m. From these sources, we construct multiple datasets for our experiments: SFT889K, Uniform102K, Easy102K, and Hard102K for SFT training, and Val-199 for SFT validation. Difficulty Classification. To understand the influence of SFT data difficulty on post-training outcomes, we extract data of varying difficulty levels from amthink-40m. We use the Win Rate (WR) as proxy for problem difficulty, defined as the ratio of successful attempts to the total number of attempts , i.e., WR = S/N . This metric quantifies problems success probability: higher WR indicates easier problems, and lower WR indicates harder ones. We derive WRs using the DeepSeek-Distilled-40M model with = 4 attempts across three models: DeepSeek-R1-Distill1.5B, 7B, and DeepSeek-R1-671B (DeepSeek-AI, 2025). Based on WRs from the 1.5B model, problems are classified as Easy (WR = 1.0) or Hard (WR = 0 or 0.25). For comparison, we construct two datasets: Easy102K and Hard102K, each containing 102.4K samples from the respective difficulty pools. We also uniformly sample 102.4K data points from amthink-40m to generate Uniform102K as medium-scale, neutral-difficulty baseline. E.2 RL DATA We curate RL62K (62.3K prompts) from Skywork-OR1-RL by filtering out extreme difficulty levels and prompts containing Chinese characters. For Syn-SFT-RL, we construct MIX37K (36.7K samples) by augmenting these prompts with matched expert trajectories from SFT889K, excluding sequences exceeding 8,192 tokens as suggested by (Yan et al., 2025) to ensure the complete trajectory utilization in each update step. Crucially, this data scale is sufficient, as our experiments 22 Table 4: The fitting results of different SFT-then-RL configurations, including the Pure-RL (1st row). Use-LTS denotes whether the Least Trimmed Squares (LTS) regression technique (Stage-2 in D.2) was applied during curve fitting. For RL plasticity (P Lrl), SFT performance (Psft), posttraining ceiling (Apost), and the steepness (B), higher values are better; the maximum within each SFT configuration is bolded. Conversely, Cmid (RL compute cost for 0.5 Lrl, indicating efficiency) should be lower, and its minimum is also bolded. Across all SFT data configurations, increasing SFT compute generally diminishes RL training efficiency (indicated by Cmid) but improves Apost. Notably, neither Cmid nor Apost is strictly monotonic with respect to SFT compute xsft. SFT data SFT Step SFT Compute xsft (exaFLOPs) Use-LTS Lrl = Apost Psft Cmid - S1K Easy102K Uniform102K Hard102K SFT889K 0 62 124 186 248 310 360 720 1080 1440 1800 360 720 1080 1440 1800 360 720 1080 1440 360 720 1080 1440 1800 3600 5400 7200 9000 10800 12600 14080 0 0.6 1.1 1.7 2.3 2.9 19.8 39.5 59.3 79.0 98.8 34.7 69.3 104.0 138.7 173.4 89.3 178.5 267.8 357.0 446. 34.9 69.8 104.8 139.7 174.6 349.2 523.8 698.4 873.0 1047.6 1222.2 1365.8 FALSE FALSE FALSE FALSE FALSE FALSE True (α =0.85) True(α =0.85) True (α =0.85) FALSE FALSE FALSE True (α =0.85) FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE 25.2 4.5 4.1 3.3 3.7 3.1 4.7 4.2 4.6 3.6 4.1 5.7 3.1 3.6 3.6 2.5 5.1 4.4 9.8 4.4 3. 14.7 10.1 8.8 9.1 8.3 9.3 8.8 9.0 8.2 9.4 9.3 8.3 1 11 16 46 79 5 16 50 27 40 40 45 13 34 43 17 25 30 76 48 10 6 8 9 8 13 12 15 15 13 13 17 1.3 2.1 1.6 2.5 3.1 0.6 1.4 0.8 0.5 0.9 0.4 1.2 2.1 1.1 1.1 1. 0.7 1.1 2.1 3.6 0.6 0.9 1.8 2.1 1.6 1.9 1.6 1.6 1.6 2.5 1.5 1.7 1.8 Psft Apost = Lrl + Psft 46.1 68.2 70.6 72.7 73.8 72.8 70.9 73.4 73.5 73.9 73. 72.2 73.7 74.0 74.8 74.6 72.5 73.8 75.4 76.2 75.3 70.1 71.0 72.3 72.8 73.7 75.3 76.0 76.4 76.5 76.3 76.0 76.8 71.3 72.6 74.6 76.0 77.5 75.9 75.5 77.5 78.1 77.5 77. 77.9 76.8 77.6 78.3 77.1 77.6 78.1 85.2 80.6 78.7 84.8 81.1 81.1 82.0 82.0 84.6 84.8 85.4 84.8 85.7 85.2 85.1 show that RL variants typically reach saturation or instability before exhausting single epoch over MIX37K. 23 (a) Correlation Analysis (b) SFT889K (c) S1K (d) Easy102K (e) Uniform102K (f) Hard102K Figure 5: Visualization of SFT-then-RL fitting across different SFT data configurations. (a) Correlation analysis between Apost and Minimum Validation Loss. (b)-(f) The SFT-then-RL scaling dynamics under various data configurations. The SFT trajectory is depicted by black dashed line. RL scaling curves initiated from different SFT steps are distinguished by color gradient, where lighter shades indicate higher number of SFT steps. The specific starting SFT step count for each RL curve is annotated in bold black text. Data points from the training split used for fitting the RL scaling curves are marked with solid circles, while those from the validation split used for assessing curve goodness-of-fit are marked with crosses. Magnified views are provided for the low-compute regions of SFT889K and S1K."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab",
        "Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "School of Engineering, Westlake University",
        "Zhejiang University"
    ]
}