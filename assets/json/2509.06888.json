{
    "paper_title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning",
    "authors": [
        "Marc Marone",
        "Orion Weller",
        "William Fleshman",
        "Eugene Yang",
        "Dawn Lawrie",
        "Benjamin Van Durme"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 8 8 8 6 0 . 9 0 5 2 : r MMBERT: Modern Multilingual Encoder with Annealed Language Learning Marc Marone Orion Weller William Fleshman Eugene Yang Dawn Lawrie Benjamin Van Durme"
        },
        {
            "title": "Johns Hopkins University",
            "content": "{mmarone1,oweller2}@jhu.edu"
        },
        {
            "title": "Abstract",
            "content": "Encoder-only languages models are frequently used for variety of standard machine learning tasks, including classification and retrieval. However, there has been lack of recent research for encoder models, especially with respect to multilingual models. We introduce MMBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build MMBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 lowresource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAIs o3 and Googles Gemini 2.5 Pro. Overall, we show that MMBERT significantly outperforms the previous generation of models on classification and retrieval tasks on both high and low-resource languages."
        },
        {
            "title": "Introduction",
            "content": "Encoder-only language models (LMs) were developed during the the early years of scaling language model pre-training, including models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). These models were followed by multilingual variants such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (also known as XLM-R) (Conneau et al., 2019). However, these models cannot generate text, and thus have fallen out of popularity in favor of larger decoder-only language models (Brown et al., 2020; Achiam et al., 2023; Team et al., 2023; Dubey et al., 2024). Despite this, encoder-only models still are frequently used for natural language understanding (NLU) tasks, including classification, clustering, and retrieval. For many years, these older models were still the best encoder-only models available. However, there has been recent revival of encoder-only model pretraining (Nussbaum et al., 2024; Portes et al., 2023; Warner et al., 2024; Boizard et al., 2025), bringing modern pre-training techniques for decoder-only language models to encoder-only models. There have also been new analyses showing that encoder-only models are significantly better for classification/retrieval than decoder-only models for given size, even beating decoders an order-of-magnitude larger (Weller et al., 2025; Gisserot-Boukhlef et al., 2025). In this encoder-only model revival there has been conspicuous lack of large-scale multilinguality. Although it is over six years old, XLM-R is still SOTA especially surprising when you consider the fast-paced nature of the LM field. Thus, we aim to provide more recent improved version. * Authors contributed equally Models, data, and code are available at https://github.com/jhu-clsp/mmBERT We do this by pre-training our new model suite, MMBERT, on 3T tokens of multilingual text using an architecture inspired from ModernBERT (Warner et al., 2024). We also propose novel contributions to the pre-training recipe: (1) an inverse mask schedule learning rate (high low), (2) an annealing language schedule (more biased more uniform), and (3) increasing the number of languages at each training phase (601101833), allowing for maximal impact of the smaller amount of data. MMBERT improves over XLM-R across the board, and even beats models like OpenAIs o3 (OpenAI, 2025) and Googles Gemini 2.5 Pro (Comanici et al., 2025) on low-resource languages. We show that including the low-resource languages in the decay phase enables rapid learning, boosting performance on these languages roughly 2x despite only using 100B tokens. Overall, MMBERT is the first model to show significant improvements over XLM-R for massively multilingual data while also introducing new techniques for multilingual LM pre-training that apply to both encoders and decoders."
        },
        {
            "title": "2 Related Work",
            "content": "Encoder-only Models Encoder-only models were the predominant language model in the early LM days, with ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019) being early examples of scaling language models up to the trillion token data range. Encoder-only models are still the predominant models used for classification and retrieval tasks when inference speed is important. However, decoder-only models have been scaled to the trillion parameter scale whereas encoder models typically remain less than 1 billion parameters. The revival of encoder-only LM development was spurred by works such as MosiacBERT showing BERT-equivalent model could be trained in under 24 hours (Portes et al., 2023; Nussbaum et al., 2024). More recently, ModernBERT (Warner et al., 2024) further scaled these recipes and showed that you could greatly improve performance. Since then there have been several more: EuroBERT focusing on 15 languages (Boizard et al., 2025), NeoBERT (Le Breton et al., 2025) on English, and Ettin showing paired open-data encoder and decoder recipes (Weller et al., 2025). However, none of these more recent models have scaled to more than 15 languages. In the massively multilingual setting, there are very few available models: mBERT with 104 languages (Devlin et al., 2019), XLM-R with 100 languages (Conneau et al., 2019), and mGTE with 74 languages (Zhang et al., 2024). Works such as multilingual DistilBERT (Sanh et al., 2019) and multilingual MiniLM (Wang et al., 2020) have distilled from mBERT and XLM-R respectively to derive smaller variants. The more recent mGTE showed slightly improved performance over XLM-R while allowing for longer contexts, while mBERT is generally not used due to the improvements of XLM-R. Despite XLM-Rs release date, it has aged very well: its design was well ahead of its era through its use of 6T training tokens, more than any other encoder-only model has ever been trained on, even to this day (including our MMBERT). However, data quality has significantly increased since 2019, allowing us to achieve higher scores with only half of the tokens (Penedo et al., 2024). Multilingual LMs Multilingual models have been developed early on through the use of machine translation systems (Artetxe et al., 2017; Team et al., 2022; Fan et al., 2021), and recently with decoder-only LMs (Gemma et al., 2024; Dubey et al., 2024). Most new LM releases are multilingual in order to be broadly accessible (Comanici et al., 2025; Achiam et al., 2023). However, much of the details of these models are not available, including pre-training data and training recipes. However, from the models which do release information, we see that improvements are generally due to improved data quality and the use of parallel data (Martins et al., 2025). Our work uses the higher quality multilingual data but not parallel data, as current parallel texts are relatively short and noisy."
        },
        {
            "title": "3 Training Details",
            "content": "3.1 Architecture We use an identical architecture to ModernBERT, but employ the Gemma 2 tokenizer1 (Gemma Team et al., 2024) to handle multilingual input. We use 22 layers and an 1152 intermediate dim for both base 1During training of MMBERT the Gemma 3 tokenizer was released. We would encourage future work to pre-train using this tokenizer after modifying the pre-tokenizer to include prefix spaces, which we did not use. This likely would help for NER and POS tasks. 2 Category Dataset Tokens (B) % Tokens (B) % Tokens (B) % Pre-training Mid-training Decay Phase Code (ProLong) Code Starcoder Code DCLM Crawl DCLM (Dolmino) Crawl Crawl FineWeb2 Instruction Tulu Flan Math Reference Books Reference Textbooks (ProLong) Reference Wikipedia (MegaWika) Scientific Arxiv Scientific PeS2o Social Social Dolmino Math StackExchange StackExchange (Dolmino) 100.6 5.1 600.0 30.2 1196.6 60.2 0.8 0.6 0.2 0.2 1.4 0.4 0.9 0.1 15.3 11.2 4.3 4.7 27.8 8.4 18.6 1.4 2.9 17.2 1.7 10.0 40.0 6.7 506.7 84.3 0.5 0.7 0.7 0.2 0.9 0.5 0.5 0.5 3.1 4.3 3.9 1.2 5.4 3.2 3.0 2.8 2.7 2.8 0.5 0.5 2.0 2.0 78.5 76.0 1.0 1.0 0.5 0.5 2.1 2.2 3.0 3.1 9.2 9.5 3.2 3.3 Total 1989.0 100.0 600.8 100.0 103.3 100.0 Table 1: Training data mixture across the various training stages (pre-training, mid-training, decay). Later stages use higher quality data, including the recent Dolmino (OLMo et al., 2025) and FineWeb2HQ (Messmer et al., 2025) datasets. Dashes indicate that no data from that source was used. We trained for 2.3T tokens for pre-training, 600B for mid-training (e.g. context-extension to 8192 sequence length), and 100B for the decay phase. We sample from the dataset and repeat (or undersample) as needed to hit the token counts used for training. Note that the decay phase included three different mixtures and the resulting weights were merged together, this includes only one version (Decay-Cont). See Appendix for more details. If not specified, the source is Dolma v1.7. and small versions (same as ModernBERT-base), but use hidden dimension of 768 for base and 384 for small. For the rest of the training configurations that are shared, see Table in the Appendix. Our base version has the same number of non-embedding parameters as ModernBERT-base (110M) but total of 307M parameters due to the larger vocabulary. MMBERT small has 140M total parameters, with 42M non-embedding parameters. As we show in (4.5) these architectures are also significantly faster than any previous multilingual encoder model. 3.2 Training Data We follow the Ettin recipe (Weller et al., 2025) as the only open-data ModernBERT equivalent. Crucially though, we change the source of web crawl to account for more multilingual data. Previous work such as XLM-R and mT5 (Xue et al., 2020) used very low percentage of Engish content (5.7% for mT5). However, the highest quality data (filtered DCLM (Li et al., 2024a; OLMo et al., 2025)) only exists in English. Thus we choose to use significantly higher percentage of English comparative to previous work (from 10% to 34% depending on the stage, see Table 9). Nonetheless, significant portion of our training data was non-English: for this we gathered data from FineWeb2 (Penedo et al., 2025) and filtered version of 20 langauges from FineWeb2 called FineWeb2-HQ (Messmer et al., 2025). We also use and filter MegaWika v2 (Barham et al., 2023, 2025) for Multilingual Wikipedia, which covers 60 languages (those in our first stage). We include several other curated corpora in English. From Dolma (Soldaini et al., 2024) we use StarCoder, Stackexchange, Arxiv, and PeS2o. From Dolmino (OLMo et al., 2025) we use math, filtered Stackexchange (which is mainly code), Tulu Flan instruction data, and books (OLMo et al., 2025). From ProLong (Gao et al., 2024) we use their code repositories and their textbooks. All our data is publicly available on HuggingFace.2 Thus, our data mix is higher quality than previous work (through the use of filtered DCLM and FineWeb2), more diverse in content (code, instructions, web data, papers), and includes greater variety of languages and scripts. 2We did not create any new datasets for training, however, we did gather them and create the final data mix. We thank the open-source efforts cited above for making this possible. 3 Figure 1: Our inverse temperature sampling ratio throughout training from Fineweb2 data, from τ of 0.7 to 0.5 to 0.3. Note that other sources are excluded from this chart and are not temperature sampled. For full language percent details, see Appendix D. Our training starts out more high-resource biased but becomes increasingly uniform. We also include another 50 languages in mid-training and 1723 more for the decay phase not visualized. Cascading Annealed Language Learning (ALL) Unlike previous work which uses fixed set of languages and set temperature to sample multilingual data, we use novel approach that changes the temperature during training and iteratively adds new languages (i.e. annealing over languages). We observe that lower-resource languages have smaller amounts of pre-training data and relatively lower quality (since there is less to filter). Thus, we want to learn from these languages in the most impactful way (e.g. avoiding more than 5x epochs of them) while also keeping our data quality high. To do this, we start with higher resource languages and slowly add languages throughout training. At each change point, we also re-sample the ratio, taking the distribution from more high-resource biased to more uniform. This allows us to avoid doing many epochs on lower-resource data and allows for quicker learning of new languages as they already have strong base in the existing language set (e.g. starting with Icelandic and then quickly learning Faroese). We start with set of 60 languages (as well as code) that cover broad range of language families and scripts. We then increase this to 110 languages, covering more mid-resource\" languages (greater than 200 million tokens of data). Finally, we include all languages/scripts included in FineWeb2 (1833 languages, 1895 language/script variants). Our temperature for sampling languages goes from 0.7 to 0.5 to 0.3 (see Figure 1). Including all the languages at the very end allows us to take advantage of the decay phase learning to rapidly increase performance. We see this validated in Section 4.4. 3.3 Training Recipe We use the same three phase approach as ModernBERT and Ettin but employ novel inverse masking rate learning schedule. Rather than simply using lower masking ratio at the end of training as shown by Boizard et al. (2025); Weller et al. (2025), we progressively lower the mask rate at each stage.3 For MMBERT small, we also initialize the weights from base using strided sampling (Sanh et al., 2019). Base Pre-training This stage encompasses the warmup and stable phase of the trapezoidal learning rate, training for 2.3T tokens. We use both learning rate and batch size warmup. The data in this stage does not include the filtered FineWeb2 data or the higher quality DCLM. We use just 60 languages (plus code languages) in this stage of training and 30% masking rate4 to start. Context Extension / Mid-Training Here we increase the quality of the data by switching to the filtered higher-quality versions of the datasets  (Table 1)  . We also change the RoPE values (Su et al., 3Due to the scale of pre-training, we were not able to ablate this except in the decay phase. However, we found that the smaller the masking rate the better for the decay phase of training. 4 MMBERT small lowered the mask rate to 20% / LR to 4e-4 after 1.2T tokens when it had stopped learning. 4 Single Sentence Paraphrase and Similarity Natural Language Inference Model Name CoLA SST-2 MRPC STS-B QQP MNLI QNLI mDistilBERT S 34.7 Multilingual MiniLM 25.4 61.8 MMBERT Small B EuroBERT 210m XLM-R Base mGTE Base MMBERT Base ModernBERT Base 36.8 54.2 54.7 61.9 65.3 89.4 91.6 93.1 90.6 93.1 93.3 94.0 95.3 90.5 91.3 91.6 92.4 92.2 92.7 91.9 90. 87.5 88.6 90.3 89.8 89.4 89.9 91.0 91.5 86.6 87.7 88.6 88.3 88.5 89.0 89.4 89.4 79.0 82.1 85.8 85.3 85.0 85.3 87.7 88. 87.5 89.7 91.9 91.3 90.6 91.1 93.3 93.7 RTE 73.3 75.8 81.9 78.3 78.7 82.3 85.6 87.7 Avg 77.5 78.3 84.7 81.2 83.3 84.0 86.3 87.4 Table 2: GLUE (English) benchmark results. We see that MMBERT outperforms all other models for their size, with the base version coming close to even ModernBERTs performance. Cross-lingual Understanding Question Answering Structured Prediction Model Name XNLI PAWS-X XCOPA XQuAD MLQA TyDiQA WikiANN UDPOS Avg mDistilBERT S 60.8 Multilingual MiniLM 71.2 73.6 MMBERT small XLM-R base mGTE Base MMBERT Base B 74.6 73.9 77.1 80.2 84.6 86.7 85.9 86.4 87.7 52.7 59.2 61.8 61.2 63.6 67. 49.4 68.2 73.0 73.4 75.7 77.6 43.5 56.8 62.5 62.1 64.3 66.0 44.2 63.0 66.7 70.5 69.9 74. 54.5 59.2 54.3 61.4 60.7 58.2 67.1 74.2 70.6 74.3 74.3 74.0 56.5 67.1 68.6 70.4 71.1 72. Table 3: XTREME benchmark results. Note that we exclude EuroBERT as it excludes many of the languages tested in this benchmark. For fair comparison with EuroBERT see Section 4.2. 2024) to handle up to 8192 tokens (i.e. theta of 160k) for global and local layers. We further increase the number of languages to 110 languages (plus code). We train for 600B tokens and continue the stable phase, lowering the mask rate to 15%. Decay Phase Finally, we use an inverse square root learning rate schedule to decay for 100B tokens to 0.02 of the peak LR. Unlike ModernBERT and Ettin, we choose to decay with 5% mask rate and use three different datasets to produce three different variants: English-focused (Decay-Eng), 110 languages (same as the mid-training phase, Decay-Cont), and 1833 language variant (all FineWeb2 languages, Decay-All). For specific details into each mixture, see Appendix D. Model Merging We then use model merging to combine the best qualities from each decay mixture. For the base version, we select the best checkpoint from each mixture and use TIES-merging (Yadav et al., 2023) to mitigate parameter interference. Merging across mixtures was ineffective for small, likely due to less parameter agreement in the smaller weight space. Therefore, we merged an exponential weighting of the Decay-All checkpoints, as that merged mixture performed best."
        },
        {
            "title": "4 Results",
            "content": "4.1 Benchmark Scores Datasets We benchmark MMBERT on existing encoder benchmarks for NLU and retrieval: GLUE (Wang et al., 2018), XTREME (Hu et al., 2020), and MTEB (Enevoldsen et al., 2025). We differ from early encoder work by dropping the retrieval section of XTREME as there exist improved retrieval benchmarks from MTEB. We also include the code retrieval benchmark CoIR (Li et al., 2024b), although code is not the primary focus of MMBERT. Baselines We use variety of baselines: the older but still ubiquitous XLM-R (Conneau et al., 2019), mGTE (Zhang et al., 2024), and EuroBERT-210m (Boizard et al., 2025) for the base size and mDistilBERT (Sanh et al., 2019) and Multilingual MiniLM (Wang et al., 2020) for the small size.5 For English, we also show results with ModernBERT (Warner et al., 2024) as an upper bound. We perform sweep over various hyperparameters for each model, see Appendix for details. 5We do not compare with RTD models. These models have consistently been shown to be significantly worse for embeddings. We validate this for mDeBERTa (He et al., 2023) in App C, where it is more than 11 pts worse. 5 Model Name Pair Class. Class. STS Retrieve Cluster Rerank Summ. Avg Multilingual MiniLM S mDistilBERT MMBERT small B EuroBERT 210m XLM-R base mGTE base ModernBERT base MMBERT base 77.3 75.9 78.9 79.5 79.3 79.9 80.5 80.2 59.3 59.0 62.1 62.3 63.9 64.3 65.6 64.8 71.9 72.4 74. 73.6 73.4 74.4 75.5 74.8 35.0 38.4 40.7 43.0 39.8 43.7 44.8 44.9 36.9 38.3 40.8 40.3 39.3 40.2 43.3 41.7 42.4 42.7 44. 43.3 44.2 43.7 44.1 44.9 19.3 19.4 23.6 21.5 24.2 23.0 22.6 26.0 48.9 49.4 52.1 51.9 52.0 52.7 53.8 53.9 Table 4: MTEB v2 English results. Average is done over categories. Best model in the section is bolded. See Appendix for sweep details. We see that both MMBERT models significantly outperform all multilingual models and MMBERT base even ties ModernBERT. Model Name Bitext Mining Pair Class. Class. STS Retrieve Multilabel Class. Cluster Rerank Avg mDistilBERT S Multilingual MiniLM MMBERT small XLM-R base mGTE base MMBERT base B 37.2 46.6 50.5 56.6 52.6 59.2 76.6 76.7 77.4 78.2 78.5 79.2 50.3 51.0 50. 54.5 53.1 53.6 62.1 64.6 64.8 66.0 66.2 67.1 36.7 35.6 41.9 43.0 46.4 45.8 14.6 14.0 15. 15.5 17.2 17.5 37.5 34.9 38.7 38.1 38.8 40.2 62.2 64.1 66.1 67.6 69.1 69.9 47.1 48.4 50. 52.4 52.7 54.1 Table 5: MTEB v2 multilingual results. Average is done over categories. See Appendix for sweep details. We see that both MMBERT models significantly outperform all other models for their size. Task () Text-to-Code Code-to-Text Code-to-Code Hybrid Code Model (param.) Apps CosQA Synthetic Text2sql Code SearchNet SN-CCR CodeTrans StackOver CodeFeedBack -Contest -DL Flow QA -ST -MT mDistilBERT S 2.7 Multilingual MiniLM 2.2 4.3 MMBERT small B XLM-R base mGTE base MMBERT base EuroBERT 210m 3.1 3.8 6.1 5.6 8.4 7.6 19.5 13.2 13.2 24.6 20.3 35.2 22.7 41.5 30.5 33.8 48.1 43. 69.4 59.8 64.1 75.6 74.8 63.8 77.0 31.0 21.8 38.4 30.7 39.1 41.7 43.8 27.5 24.9 56.3 34.4 45.2 54.9 62. 21.0 22.4 32.9 22.1 30.6 33.0 35.0 45.8 40.4 60.3 51.2 58.0 62.1 64.4 37.4 35.6 52.4 47.3 50.3 55.5 57. 22.8 22.6 40.7 28.1 39.9 32.4 44.6 Avg 30.1 26.0 41.0 33.6 38.9 42.2 45.3 Table 6: Retrieval scores on the CoIR Benchmark. MMBERT models outperform all others except EuroBERT, which used the higher quality but not publicly accessible Stack v2 training data. NLU Results We start with English GLUE in Table 2. MMBERT small performs significantly better than other small variants (84.7 average vs MiniLMs 78.3) and even outperforms all other previous base-sized models, including XLM-R. MMBERT base outperforms all other multilingual models, while even approaching ModernBERTs English performance (86.3 MMBERT vs 87.4 ModernBERT), despite using majority of non-English data. For multilingual performance in XTREME  (Table 3)  , MMBERT base also outperforms all other models on average (72.8 average vs XLM-Rs 70.4), except for structured prediction where it lags behind on POS and ties on NER. This is likely due to the same problem that ModernBERT has, i.e. the lack of consistent prefix whitespace token during pre-training. We would recommend future work resolve this issue. However, MMBERT base show significant improvements in classification (e.g. 77.1 XNLI accuracy vs XLM-Rs 74.6) and in question-answering (74.5 F1 on TyDiQA vs 70.5 XLM-R). MMBERT outperforms the smaller variants: 73.6 XNLI vs Multilingual MiniLMs 71.2, even coming close to XLM-Rs 74.6 despite having 1/3 of the amount of non-embed parameters. Retrieval Results We train all models on MS MARCO (English, B) and evaluate on both English and multilingual benchmarks from MMTEB v2. We also evaluate on code using the CoIR benchmark. We again see large gains in English MTEB v2  (Table 4)  with even MMBERT small outperforming mGTE and XLM-R. MMBERT outperforms mGTE (the next closest) with an average of 53.9 vs 52.7, even performs similarly to ModernBERTs 53.8 average. For multilingual MTEB v2  (Table 5)  , we find that both MMBERTs score about 1.5 points better on average than their similarly sized counterparts (i.e. 54.1 avg for MMBERT base vs XLM-Rs 52.4). Model ar EuroBERT 210m 66.8 71.4 MMBERT small 74.5 MMBERT base de 72.5 77.0 79.6 en 84.7 85.8 85.9 es 76.7 79.9 81.2 XNLI fr 75.9 79.0 80. hi 60.6 67.9 71.1 PAWS-X ja ru 72.6 75.6 77.0 tr 66.4 72.6 73.8 vi 70.1 75.3 76.2 EuroBERT 210m mmBERT small mmBERT base 87.8 89.8 90.7 94.9 95.3 95.8 89.6 90.5 90.2 90.0 90.6 92.0 77.6 80.7 81.8 zh 72.7 73.2 77. 80.2 82.9 83.6 Avg 71.9 75.8 77.7 86.7 88.3 89.0 Table 7: Comparing EuroBERT and MMBERT on EuroBERTs in-distribution languages (excluding others) on XNLI and PAWS-X. We see the MMBERT improves even on these languages. Dashes indicate language not tested by the benchmark. Averages are for these languages only. On code retrieval tasks (CoIR, Table 6) we see that MMBERT performs significantly better than any other massively multilingual model (42.2 MMBERT base average vs XLM-Rs 33.6), but underperforms compared to EuroBERT-210m (45.3 average). This is likely due to EuroBERTs use of the higher quality Stack v2 corpus, which we did not have access to. Overall, we see that MMBERT is an improved drop-in replacement for XLM-R, and that even MMBERT small can come close to XLM-Rs performance. 4.2 Comparison to EuroBERT As EuroBERT was only trained on 15 languages, it has disadvantage on these massively multilingual evaluations and was excluded due to its low performance. Instead, we directly compare EuroBERT only on languages it was trained on. We show scores on these selected languages for XNLI and PAWS-X.6 Table 4.1 shows that MMBERT base and small outperform EuroBERT-210m on these languages as well (i.e. 74.5 F1 on Arabic XNLI for MMBERT base vs 66.8 F1 for EuroBERT, etc.). 4.3 Comparison to Similar-Sized SOTA Decoder Models Although numerous previous works (Weller et al., 2025; Gisserot-Boukhlef et al., 2025) have shown that encoder-only models significantly outperform decoder models within the same size range (or even one order-of-magnitude bigger), we also test this by running the recent Gemma 3 270M (Team et al., 2025) model on the same classification tasks. Using the same hyperparameter sweep detailed previously, it scores 69.0 on XNLI and averages 82.9 on GLUE. Notably, this is much worse than even MMBERT small, again showing the benefits of encoder-only models for these tasks.7 4.4 Annealing Language Learning We test whether our decision to include more languages at the decay phase significantly improves model performance. We do this by selecting evaluation datasets that test the model on language learned only during the decay phase. However, since these languages are low-resource there are not many high-quality evaluation datasets available. Thus, we test only two languages that have high quality evaluation data and are commonly8 used: TiQuaD for Tigray (Gaim et al., 2023) and FoQA for Faroese (Simonsen et al., 2025). We evaluate in the same way as XQuAD (e.g. zero-shot from English SQuAD) and compare scores for the the different decay mixtures and the final merged model. 6Since the official EuroBERT model does not have ForQuestionAnswering\" or ForMultipleChoice\" class available we select two tasks that work for classification. 7We note that an embedding version of Gemma 3 270M, EmbeddingGemma, outperforms other models on MTEB, but crucially they fine-tune the model using proprietary 320 billion token training set. They would likely see increased performance over GemmaEmbeddings if they simply fine-tuned MMBERT with the same data. However, as their data is not publicly available we cannot run this comparison. 8Are included in other benchmarks, i.e. ScandEval (Nielsen, 2023) or won best paper award (TiQuAD). Figure 2: Performance of models using different decay phases on two languages (Tigray and Faroese) only added during the decay phase. We see that MMBERT with the 1833 language decay phase shows rapid performance improvements despite only having the models in the last 100B tokens of training. The final MMBERT models shows improvements by merging together checkpoints. Figure 2 shows that there is significant jump in performance for the variants that included the language: 68% increase for base (12.1 absolute F1) on Tigray (110 to 1833 langs) and 26% increase (15.4 absolute F1) for Faroese. On FoQA, which benchmarked larger LMs, it even outperforms Googles Gemini 2.5 Pro by 6 pts (69.8) and OpenAIs o3 by 8.3 points (67.7). Even MMBERT small outperforms these giant LMs. Thus we find that annealing language learning significantly boosts low-resource language performance. We also see that the model merging allowed the model to retain most of its performance despite merging with English and higher-resource focused versions. 4.5 Efficiency MMBERT models are significantly faster than any previous multilingual encoder-only model. Figure 3 benchmarks this using the best settings available for each model using the transformers (Wolf et al., 2020) implementation.9 We find that MMBERT base is more than 2x faster on variable sequences 9e.g. by installing xformers or flash-attention when helpful. Figure 3: Throughput efficiency for various sequence lengths and variable input length (top row is small models, bottom row is base models). MMBERT is more efficient because of the use of Flash Attention 2 and unpadding techniques it inherits from ModernBERT. Empty bars indicate that the model cannot use sequence length greater than 512. Error bars show standard error over five seeds. 8 and significantly faster on long context lengths ( 4x). For MMBERT small results are roughly 2x faster than MMBERT base and again roughly 2x faster than similarly-sized other multilingual models. We note that previous multilingual models such as MiniLM and XLM-R cannot go past 512 tokens whereas MMBERT can perform at up to 8192 tokens and can do so as fast as other models at 512."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce MMBERT, modern multilingual encoder trained on 3T tokens and 1833 languages. We introduce several novel elements in training: an inverse masking schedule and cascading annealed language learning schedule for multilingual data. MMBERT improves over the previous generation of multilingual encoders such as XLM-R while being drop-in replacement. We further show that leaving out low-resource languages until the decay phase allows us to rapidly learn the languages on minute amounts of data, improving performance to levels past even SoTA large LMs like OpenAIs o3 and Googles Gemini 2.5 Pro. We open-source our models, data, and checkpoints."
        },
        {
            "title": "6 Limitations",
            "content": "Although MMBERT shows significant improvements on low-resource languages, there are still many languages that have very small amounts of data, or even none at all. This is especially notable for high-quality data, such as edu-style filtering (Lozhkov et al., 2024). We leave improvements in this area to future work, which would further improve scores on low-resource languages."
        },
        {
            "title": "Acknowledgments",
            "content": "This work has been supported by both DARPA SciFy and the U.S. National Science Foundation under grant 2204926. Any opinions, findings, and conclusions or recommendations expressed in this article are those of the authors and do not necessarily reflect the views of the National Science Foundation or DARPA. OW is supported by an NSF GRFP fellowship. We thank Neha Verma for helpful discussions in model development. We thank Databricks and the Johns Hopkins Data Science and AI Institute for compute that supported this research."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine translation. arXiv preprint arXiv:1710.11041, 2017. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. Samuel Barham, Orion Weller, Michelle Yuan, Kenton Murray, Mahsa Yarmohammadi, Zhengping Jiang, Siddharth Vashishtha, Alexander Martin, Anqi Liu, Aaron Steven White, et al. Megawika: Millions of reports and their sources across 50 diverse languages. arXiv preprint arXiv:2307.07049, 2023. Samuel Barham, Chandler May, and Benjamin Van Durme. Megawika 2: more comprehensive multilingual collection of articles and their sources. arXiv preprint arXiv:2508.03828, 2025. Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Duarte Alves, André Martins, Ayoub Hammal, Caio Corro, Céline Hudelot, Emmanuel Malherbe, Etienne Malaboeuf, Fanny Jourdan, et al. Eurobert: scaling multilingual encoders for european languages. arXiv preprint arXiv:2503.05500, 2025. 9 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171 4186. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1423. URL https://doi.org/10.18653/v1/n19-1423. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzeminski, Genta Indra Winata, et al. Mmteb: Massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025. Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond englishcentric multilingual machine translation. Journal of Machine Learning Research, 22(107):148, 2021. Fitsum Gaim, Wonsuk Yang, Hancheol Park, and Jong Park. Question-answering in low-resourced language: Benchmark dataset and models for tigrinya. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1185711870, 2023. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively), 2024. URL https://arxiv.org/abs/2410.02660. Team Gemma, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris 10 Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozinska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucinska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open language models at practical size, 2024. URL https://arxiv.org/abs/2408.00118. Hippolyte Gisserot-Boukhlef, Nicolas Boizard, Manuel Faysse, Duarte M. Alves, Emmanuel Malherbe, André F. T. Martins, Céline Hudelot, and Pierre Colombo. Should we still pretrain encoders with masked language modeling?, 2025. URL https://arxiv.org/abs/2507.00994. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=sE7-XhLxHA. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. Xtreme: massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International conference on machine learning, pp. 44114421. PMLR, 2020. Lola Le Breton, Quentin Fournier, Mariam El Mezouar, and Sarath Chandar. Neobert: next generation bert. Transactions on Machine Learning Research, 2025. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024a. Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Hao Zhang, Xinyi Dai, Yasheng Wang, and Ruiming Tang. Coir: comprehensive benchmark for code information retrieval models. arXiv preprint arXiv:2407.02883, 2024b. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. URL https://huggingface.co/datasets/ HuggingFaceFW/fineweb-edu. 11 Pedro Henrique Martins, Patrick Fernandes, João Alves, Nuno Guerreiro, Ricardo Rei, Duarte Alves, José Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, et al. Eurollm: Multilingual language models for europe. Procedia Computer Science, 255:5362, 2025. Bettina Messmer, Vinko Sabolˇcec, and Martin Jaggi. Enhancing multilingual llm pretraining with model-based data selection. arXiv, 2025. URL https://arxiv.org/abs/2502.10361. Dan Saattrup Nielsen. Scandeval: benchmark for scandinavian natural language processing. arXiv preprint arXiv:2304.00906, 2023. Zach Nussbaum, John X. Morris, Brandon Duderstadt, and Andriy Mulyar. Nomic embed: Training reproducible long context text embedder. CoRR, abs/2402.01613, 2024. doi: 10.48550/ARXIV. 2402.01613. URL https://doi.org/10.48550/arXiv.2402.01613. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656. OpenAI. Openai o3 and o4-mini system card, 2025. URL https://openai.com/index/ o3-o4-mini-system-card/. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. Guilherme Penedo, Hynek Kydlíˇcek, Vinko Sabolˇcec, Bettina Messmer, Negar Foroutan, Amir Hossein Kargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, and Thomas Wolf. Fineweb2: One pipeline to scale them all adapting pre-training data processing to every language, 2025. URL https://arxiv.org/abs/2506.20920. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 22272237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https://aclanthology.org/N18-1202/. Jacob Portes, Alexander Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, and Jonathan Frankle. Mosaicbert: bidirectional encoder optimized In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz for fast pretraining. Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/ 2023/hash/095a6917768712b7ccc61acbeecad1d8-Abstract-Conference.html. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. Annika Simonsen, Dan Saattrup Nielsen, and Hafsteinn Einarsson. Foqa: faroese questionanswering dataset. arXiv preprint arXiv:2502.07642, 2025. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. 12 Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. NLLB Team, Marta Costa-Jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems, 33:57765788, 2020. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, et al. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. arXiv preprint arXiv:2412.13663, 2024. Orion Weller, Kathryn Ricci, Marc Marone, Antoine Chaffin, Dawn Lawrie, and Benjamin Van Durme. Seq vs seq: An open suite of paired encoders and decoders. arXiv preprint arXiv:2507.11412, 2025. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6/. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36:70937115, 2023. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mgte: Generalized long-context text representation and reranking models for multilingual text retrieval. In Franck Dernoncourt, Daniel Preotiuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: EMNLP 2024 - Industry Track, Miami, Florida, USA, November 12-16, 2024, pp. 13931412. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.emnlp-industry.103."
        },
        {
            "title": "A Architecture Details",
            "content": "Common architecture details are in Table A. One other area of difference between the two models is that MMBERT small used learning rate warmup of 4B tokens and batch size warmup of 100B tokens. MMBERT base had learning rate warmup of 3B tokens and batch size warmup of 60B tokens. 13 Parameter"
        },
        {
            "title": "Learning Rate\nWeight Decay\nTraining Batch Size\nVocabulary Size\nMax Sequence Length\nTokenizer\nAttention Layer\nAttention Dropout\nAttention Output Bias\nAttention Output Dropout\nAttention QKV Bias\nTransformer Layer\nEmbedding Dropout\nEmbedding Norm\nFinal Norm\nSkip First PreNorm\nMLP Dropout\nMLP Input Bias\nMLP Layer Type\nMLP Output Bias\nNormalization\nNorm Epsilon\nNorm Bias\nHidden Activation\nHead Pred Activation\nActivation Function\nPadding\nRotary Embedding Base\nRotary Embedding Interleaved\nAllow Embedding Resizing\nSliding Window\nGlobal Attention Every N Layers\nUnpad Embeddings",
            "content": "Value 8e-4 8e-5 4.7M tokens 256,000 1024->8192 Gemma 2 RoPE 0.0 false 0.1 false prenorm 0.0 true true true 0.0 false GLU false LayerNorm 1e-12 false GELU GELU GELU unpadded 10k -> 160k false true 128 3 true Table 8: Common Configuration Parameters. Note that MMBERT small had to lower the LR/WD to half the initial value after 1.2T tokens, due it plateauing early."
        },
        {
            "title": "B Hyperparameter and Compute Details",
            "content": "Compute We use mix of H100 and L40s for training and inference, using the L40s mainly for the smaller model inference. We use 8xH100s for roughly 10 days to train the small version and 8xH100s for roughly 40 days for the base version. For evaluation, each experiment takes roughly 1-2 hours to run for given setting. NLU Sweep We sweep over seven LRs ({2e-5, 3e-5, 4e-5, 5e-5, 6e-5, 7e-5, 8e-5 } and four epoch options ({1, 2, 3, 5, 10}) using batch size of 32 and warmup ratio of 0.06 (following mGTE). We select the best result for each model on each task in an oracle fashion. Most models had the best score in the 2e-5 or 3e-5 range with varying amounts of epochs according to the task. Embedding Sweep We sweep four LRs, following Weller et al. (2025) {1e-4, 3e-4, 5e4, 7e-4} and use the best one. We train with SentenceTransformers (Reimers & Gurevych, 2019) using 1.25M hard triplets for one epoch on MS MARCO data (Bajaj et al., 2016) from sentence-transformers/msmarco-co-condenser-margin-mse-sym-mnrl-mean-v1. We then evaluate using MTEB (Enevoldsen et al., 2025) and select the LR that performed the best. In all cases the best was using the 1e-4 LR (except for MiniLM which was 5e-4). 14 FoQA and TiQuAD We use partial sweep from the NLU section, using just the two LRs that performed best from the original sweep (2e-5, 3e-5) while also varying epochs."
        },
        {
            "title": "C Comparison with DeBERTa",
            "content": "Both Ettin and ModernBERT found that comparable RTD-trained models show significantly worse performance on embedding tasks. We thus exclude them from our main analysis. We validate this by running mDeBERTa on MTEB tasks. On the multilingual benchmark, we find it has an average of 42.5, more than 11 points worse than MMBERTs 54.1. mDeBERTa scores 48.6 on the English MTEB v2, again much worse than any comparable model (including our MMBERT small). Thus, RTD-trained models may do well at classification but they do so at the expense of embedding tasks which are main use case of encoder-only models."
        },
        {
            "title": "D Language Distributions",
            "content": "The data per language is found in Table 9. We use an inverse temperature sampling with values 0.7, 0.5, and 0.3 respectively. As only 90 rows would fit on the page, the full data is available on the Github in CSV form. 15 Pretrain Mid-Training Decay-Eng Decay-Cont Decay-All Lang Value % Value % Value % Value % Value % eng code rus deu spa fra cmn ita por jpn nld pol swe ind kor arb ces tur ukr fas vie ron nob hun fin dan tha ell bul slk hrv srp heb hin lit ekk slv uzn bos ben cat zsm lvs azj als kaz tam urd kat isl mkd afr tel mya ary bel mar fil glg mal npi lat bod khk pan gmh guj anp hye rmy eus kan cym khm swh sin ars nno bew ory kir arz gle tgk som amh pbt gsw tat hif 686.4 102.0 115.4 87.3 89.8 79.8 103.7 57.3 46.9 58.6 37.2 36.5 18.6 32.5 28.4 21.2 22.7 24.9 17.5 23.3 22.2 22.4 20.9 20.2 14.9 19.2 18.2 16.4 12.9 12.2 11.3 0.0 8.3 10.0 8.8 7.0 7.9 0.0 0.0 6.5 0.0 6.3 6.2 4.8 0.0 3.0 3.1 3.6 2.5 0.0 2.6 2.7 2.1 1.8 0.0 0.0 2.6 2.6 2.2 2.0 2.5 0.0 0.0 1.7 0.0 0.0 1.7 0.0 0.0 0.0 0.0 0.0 0.0 1.4 1.3 1.2 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.9 0.0 0.0 0.0 34.51% 5.13% 5.80% 4.39% 4.51% 4.01% 5.21% 2.88% 2.36% 2.95% 1.87% 1.84% 0.94% 1.63% 1.43% 1.07% 1.14% 1.25% 0.88% 1.17% 1.12% 1.12% 1.05% 1.02% 0.75% 0.96% 0.92% 0.83% 0.65% 0.61% 0.57% 0.00% 0.42% 0.50% 0.44% 0.35% 0.40% 0.00% 0.00% 0.33% 0.00% 0.32% 0.31% 0.24% 0.00% 0.15% 0.15% 0.18% 0.13% 0.00% 0.13% 0.13% 0.10% 0.09% 0.00% 0.00% 0.13% 0.13% 0.11% 0.10% 0.13% 0.00% 0.00% 0.08% 0.00% 0.00% 0.09% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.07% 0.06% 0.06% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.05% 0.00% 0.00% 0.00% 0.04% 0.00% 0.00% 0.00% 73.0 20.0 30.2 24.7 25.2 23.1 27.9 18.3 16.5 18.6 13.4 13.2 8.1 12.2 11.1 9.0 9.4 10.1 7.8 9.6 9.3 9.3 8.9 8.7 7.0 8.4 8.1 7.5 6.3 6.1 5.8 3.8 4.6 5.3 4.8 4.1 4.5 2.4 4.7 3.9 4.4 3.8 3.7 3.1 2.9 2.2 2.3 2.6 1.9 2.1 2.0 2.1 1.7 1.6 2.1 1.8 2.0 2.0 1.8 1.7 2.0 1.4 0.8 1.5 1.2 1.6 1.5 1.6 1.2 1.0 1.4 1.4 1.1 1.3 1.2 1.1 1.2 1.1 1.1 0.9 1.0 1.1 1.0 1.0 1.0 0.7 0.9 0.9 0.9 0.8 12.14% 3.33% 5.02% 4.11% 4.19% 3.85% 4.65% 3.05% 2.75% 3.10% 2.23% 2.20% 1.36% 2.03% 1.84% 1.50% 1.57% 1.68% 1.30% 1.60% 1.54% 1.55% 1.48% 1.45% 1.16% 1.40% 1.34% 1.25% 1.05% 1.01% 0.96% 0.63% 0.77% 0.88% 0.80% 0.68% 0.74% 0.40% 0.78% 0.64% 0.74% 0.63% 0.62% 0.52% 0.49% 0.37% 0.38% 0.43% 0.32% 0.35% 0.33% 0.34% 0.29% 0.26% 0.35% 0.29% 0.34% 0.34% 0.30% 0.28% 0.33% 0.24% 0.13% 0.24% 0.20% 0.26% 0.26% 0.27% 0.19% 0.17% 0.23% 0.23% 0.18% 0.21% 0.20% 0.19% 0.20% 0.19% 0.19% 0.15% 0.17% 0.18% 0.17% 0.17% 0.17% 0.11% 0.16% 0.15% 0.15% 0.14% 18.7 3.3 2.8 2.5 2.5 2.4 2.5 2.0 1.9 2.0 1.8 1.7 1.4 1.5 1.5 1.4 1.4 1.4 1.3 1.4 1.3 1.3 1.3 1.3 1.2 1.2 1.2 1.2 1.0 1.0 1.0 1.0 0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.8 0.7 0.7 0.6 0.6 0.6 0.5 0.6 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0. 17.89% 3.12% 2.72% 2.44% 2.38% 2.28% 2.38% 1.96% 1.83% 1.87% 1.68% 1.65% 1.37% 1.47% 1.39% 1.30% 1.32% 1.32% 1.22% 1.30% 1.27% 1.26% 1.22% 1.20% 1.12% 1.15% 1.14% 1.12% 0.99% 0.96% 0.92% 0.92% 0.85% 0.87% 0.83% 0.79% 0.80% 0.80% 0.79% 0.74% 0.77% 0.72% 0.72% 0.66% 0.65% 0.56% 0.56% 0.58% 0.51% 0.53% 0.50% 0.50% 0.48% 0.48% 0.51% 0.50% 0.48% 0.48% 0.47% 0.45% 0.47% 0.45% 0.45% 0.43% 0.43% 0.43% 0.41% 0.42% 0.41% 0.40% 0.40% 0.39% 0.39% 0.38% 0.37% 0.36% 0.37% 0.35% 0.35% 0.34% 0.34% 0.33% 0.32% 0.32% 0.32% 0.32% 0.31% 0.31% 0.31% 0.30% 13.6 3.3 3.1 2.8 2.7 2.6 2.6 2.2 2.0 2.0 2.0 1.9 1.7 1.6 1.5 1.5 1.5 1.4 1.4 1.4 1.4 1.4 1.3 1.3 1.3 1.2 1.2 1.2 1.1 1.0 1.0 1.0 0.9 0.9 0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 13.19% 3.15% 2.98% 2.75% 2.62% 2.50% 2.47% 2.12% 1.98% 1.96% 1.90% 1.86% 1.62% 1.56% 1.47% 1.43% 1.41% 1.40% 1.39% 1.39% 1.37% 1.32% 1.30% 1.27% 1.24% 1.20% 1.19% 1.19% 1.04% 0.99% 0.96% 0.93% 0.92% 0.90% 0.86% 0.84% 0.83% 0.81% 0.80% 0.79% 0.77% 0.75% 0.75% 0.69% 0.65% 0.61% 0.60% 0.60% 0.56% 0.53% 0.53% 0.52% 0.52% 0.51% 0.51% 0.51% 0.50% 0.50% 0.49% 0.49% 0.49% 0.45% 0.45% 0.44% 0.44% 0.43% 0.43% 0.42% 0.41% 0.41% 0.40% 0.39% 0.39% 0.39% 0.38% 0.38% 0.37% 0.35% 0.35% 0.35% 0.34% 0.33% 0.33% 0.33% 0.32% 0.32% 0.32% 0.32% 0.31% 0.31% 11.6 6.3 3.0 2.8 2.7 2.6 2.5 2.1 2.0 2.0 1.9 1.9 1.6 1.6 1.5 1.7 1.4 1.4 1.4 1.4 1.4 1.4 1.3 1.3 1.3 1.2 1.2 1.2 1.1 1.0 1.0 0.9 0.9 1.2 0.9 0.8 0.8 0.7 0.8 0.9 0.8 0.8 0.8 0.7 0.6 0.6 0.7 0.9 0.6 0.5 0.5 0.5 0.6 0.5 0.5 0.4 0.5 0.5 0.5 0.6 0.5 0.4 0.2 0.4 0.4 0.4 0.5 0.4 0.3 0.3 0.4 0.4 0.3 0.4 0.4 0.8 0.4 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.3 0.2 0.3 0.3 0.3 0.3 10.15% 5.51% 2.61% 2.46% 2.37% 2.23% 2.21% 1.88% 1.76% 1.75% 1.70% 1.68% 1.42% 1.41% 1.31% 1.48% 1.24% 1.25% 1.25% 1.24% 1.23% 1.25% 1.17% 1.14% 1.10% 1.08% 1.06% 1.05% 0.93% 0.89% 0.87% 0.82% 0.81% 1.04% 0.78% 0.74% 0.74% 0.63% 0.72% 0.78% 0.70% 0.73% 0.66% 0.65% 0.54% 0.54% 0.61% 0.74% 0.50% 0.44% 0.47% 0.46% 0.49% 0.42% 0.45% 0.39% 0.48% 0.44% 0.44% 0.49% 0.44% 0.34% 0.21% 0.37% 0.33% 0.37% 0.39% 0.38% 0.30% 0.28% 0.34% 0.35% 0.30% 0.34% 0.33% 0.66% 0.32% 0.30% 0.30% 0.26% 0.28% 0.30% 0.30% 0.28% 0.28% 0.21% 0.29% 0.25% 0.29% 0.26% Total 803.3 134.8 154.5 120.2 122.8 110.4 139.2 82.0 69.4 83.2 56.2 55.3 31.5 49.4 43.9 34.7 36.4 39.3 29.4 37.1 35.6 35.8 33.8 32.8 25.5 31.2 29.9 27.5 22.4 21.3 20.0 6.7 15.7 18.3 16.2 13.7 14.9 4.8 7.2 12.8 6.8 12.4 12.2 10.0 4.9 7.0 7.2 8.3 6.1 3.7 6.2 6.3 5.4 4.9 3.7 3.3 6.2 6.1 5.5 5.2 6.0 2.7 1.9 4.4 2.5 2.9 4.6 2.9 2.4 2.2 2.6 2.6 2.2 3.9 3.7 3.9 2.3 2.2 2.2 1.9 2.1 2.1 3.0 2.0 2.0 1.6 2.8 1.8 1.9 1.8 Total 1989.0 100.00% 600.8 100.00% 104.4 100.00% 103.3 100.00% 114.2 100.00% 2911.8 Table 9: Language data (in billions, rounded to nearest 100M) with stage percentages for the first 90 language included. More languages would not fit on the page, see the Github for the full CSV details."
        }
    ],
    "affiliations": [
        "Johns Hopkins University Center for Language and Speech Processing (CLSP)"
    ]
}