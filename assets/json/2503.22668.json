{
    "paper_title": "Understanding Co-speech Gestures in-the-wild",
    "authors": [
        "Sindhu B Hegde",
        "K R Prajwal",
        "Taein Kwon",
        "Andrew Zisserman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-text-speech associations: (i) gesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal speech-text-video-gesture representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs), across all three tasks. Further analysis reveals that speech and text modalities capture distinct gesture-related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal"
        },
        {
            "title": "Start",
            "content": "Understanding Co-speech Gestures in-the-wild Sindhu Hegde*, Prajwal*, Taein Kwon, Andrew Zisserman Visual Geometry Group, University of Oxford {sindhu, prajwal, taein, az}@robots.ox.ac.uk https://www.robots.ox.ac.uk/vgg/research/jegal 5 2 0 2 8 2 ] . [ 1 8 6 6 2 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Co-speech gestures play vital role in non-verbal communication. In this paper, we introduce new framework for cospeech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate models capability to comprehend gesture-text-speech associations: (i) gesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present new approach that learns tri-modal speech-text-video-gesture representation to solve these tasks. By leveraging combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that strong gesture representation can be learned in weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs), across all three tasks. Further analysis reveals that speech and text modalities capture distinct gesture-related signals, underscoring the advantages of learning shared tri-modal embedding space. The dataset, model, and code are available at: https: //www.robots.ox.ac.uk/vgg/research/jegal. 1. Introduction Humans gesture when they talk gesturing is an integral part of human communication, together with speech and facial expressions. Gestures can vary from beats two phase hand movements (up/down, left/right etc) that emphasize particular words or phrases and match the rhythm of the speech, but do not carry semantic content to iconic and deictic gestures that are representational and illustrate the content of the speech [9, 34]. For example, hands and arms moving apart can accompany speech segment indicating that something is huge, or as illustrated in Fig 1, an inward pointing gesture to depict the uttered word my. Non-verbal communication accounts for 55% of overall communication1, highlighting the need for machines to understand non-verbal gestural elements in order to have holistic understanding of human communication. clear application is enriching human-computer interaction (HCI) through gestures, and this requires machines to comprehend the semantics of the users hand gestures. Another application is to detect if person is speaking based on their gestures, or spot specific words or phrases in video based on gestures alone. More generally, being able to recognize gestures and determine their semantic and temporal alignment with speech enables human communication to be studied at scale [26]. In this paper, our objective is to learn and evaluate co-speech gesture representations. To this end, we propose three tasks and evaluation benchmarks that act as proxy for assessing real world applications: (1) gesture-based cross-modal retrieval, (2) gesture word spotting, and (3) active speaker detection via gestures. We perform large-scale training featuring 7000 speakers and evaluate on in-the-wild videos from the AVSpeech dataset [20]. Figure 1. Co-speech gestures supplement the spoken language we show examples for three phrases here, with common words. Learning to associate gestures with the uttered phrases is essential for holistic understanding of human communication. All three tasks require models to learn to associate gesture clips with speech segments or their corresponding textual transcripts. For this, we propose model termed Joint Embedding space for Gestures, Audio, and Language (JEGAL)2 that facilitates matching gestures to words and phrases in the accompanying speech. The matches can be based on the style of the speech communication-is-nonverbal/ *equal contribution 1https://online.utpb.edu/about-us/articles/communication/how-much-of2JEGAL, known as Zhuge Liang in Chinese, was prominent historical figure from Chinas Three Kingdoms period and is regarded as symbol of wisdom. (intonation, stress, prosody) or the semantic content of the phrase or particular words. However, learning rich joint gesture-audiolanguage embedding space is very challenging task. The associations between gestures and speech are typically sparse and ambiguous, with high degree of variability across speakers. Usually, only few of the spoken words are clearly gestured. Additionally, the same sentence can be gestured very differently in different contexts and by different people. Gestures also depend on the speakers emotion, culture, and social scenario (formality, private vs. public, with friends or strangers, etc.). Furthermore, some types of gestures, such as beat gestures, carry no semantic information, resulting in no direct mapping from the gesture to words. The sparse and weak cross-modal correlations makes gesture representation learning very unique research problem. We make three key design choices in our approach that result in strong tri-modal gesture representation. To start with, we learn gesture video representations from large-scale weak cross-modal supervision. The supervision is weak because we only use phrase-level speech audio and transcripts since we do not have any information on which words in the speech are gestured for videos collected in-the-wild. Second, we obtain cross-modal supervision in the form of both audio and the corresponding text transcript (in Section 6.1, we demonstrate that speech and text modalities capture complementary gesture-related signals). Third, we introduce new gesture-word alignment and spotting loss that strongly encourages learning of word-level correspondences. To summarize, we make the following contributions: (i) we propose new framework for co-speech gesture understanding with three new tasks and evaluation benchmarks; (ii) we learn joint tri-modal embedding space in weakly-supervised manner with combination of global phrase-level objective, and local word-level gesture coupling loss; (iii) we demonstrate that the learned JEGAL representation outperforms previous vision-language foundation models on the three gesture-centric tasks and is useful for practical applications. 2. Related Work Spectrum of Human Gestures. Gestures can be classified broadly into four major classes [34]. Emblematics convey clear symbolic meaning, e.g. thumbs-up. Iconic gestures are used to convey meaning co-occurring with the articulated speech, e.g. revolving door is accompanied with the hands moving in circular motion. Deictic gestures are pointing gestures using the index finger. The most common are the beat gestures which are co-speech gestures that are temporally aligned with the prosodic characters of human speech. They are prominent on lexically stressed syllables. Past works [26, 55] have studied how these gesture classes relate to the speech. Several papers have been attempted to recognize hand, head, and facial emblematic gestures [18, 22]. Recognizing deictic gestures can help in identifying the referring object being pointed to in conversation, an essential part of human-robot interaction [33]. The other two gesture classes, i.e., beat and iconic, are quite diverse and are difficult to associate with well-defined set of words or hand movements. This work takes data-driven approach to learn gesture representations with the help of speech and natural language. Co-speech Gesture Understanding. Spoken discourse consists of multiple streams of information: language, lip movements, facial expressions, and hand gestures. As opposed to the advancements in the other streams [12, 40, 44], co-speech gesture understanding is relatively under-explored area. One possible reason for this could be that gestures are only sparsely correlated with the speech. As result, some of the works have resorted to developing models for specific cases where the gestures are clear, e.g. weather narration [27, 49]. Some works [23, 36, 37] have tried to detect and recognize gestures in laboratory settings while using multiple stereo and IR cameras. Recently, GestSync [25] learns gesture representations by solving for cross-modal synchronization with speech. This objective can lead to the model capturing low-level associations rather than high-level semantics, leading to poor performance on tasks like retrieval, and word spotting. Our work is the first one to learn gesture representations which capture the semantics, style and also learn word-level associations. Understanding Gestures in Sign Language. Sign language understanding and recognition is another body of work where models need to understand and associate gestures to words and phrases to solve tasks like sign recognition [8, 14, 35, 41, 46, 64, 66], sign language retrieval [15, 19], sign language translation [13, 14, 53, 60] and sign language production [47, 48, 51]. Sign language gesture understanding is quite different compared to co-speech gesture understanding. In sign language, text transcription is translation of what is being signed/gestured. Thus, the words in the text can be summary or paraphrasing of what is being gestured, with even mismatch in the temporal ordering. In co-speech gestures, the speaker is the gesturer, and the gestures are being made by the speaker to directly accompany each word he (she) utters. Hence, these two tasks require very different approaches. Co-speech Gesture Generation. Several works have focused on generating natural gestures that match given speech segment. This task has the advantage of being freely supervised large-scale datasets can be curated for this task with almost no manual effort as it only requires unlabeled videos of people talking. Speech2Gesture [24] trains speaker-specific models to generate hand skeleton motion for given speech segment. Recent papers [10, 61, 63] have moved towards more speakerindependent approaches while also using text to obtain strong semantic supervision. In particular, GestureDiffuCLIP [10] learns joint gesture-text embedding to improve gesture generation. As will be seen in the results, one clear distinction from the work presented in this paper is that [10] does not learn word-level correspondences, which makes significant difference to the gesture understanding tasks that we evaluate. Gesture Recognition Datasets. ChaLearn ConGD and Figure 2. The JEGAL architecture. The three input modalities (video, text, speech) are each encoded with modality-specific encoder, followed by fusion block to merge speech and text representations. The encoder outputs are average-pooled to obtain global (phrase-level) gesture and speech-text embeddings. During training, these provide the inputs for the global phrase contrastive loss. The gesture alignment module aggregates the relevant video frames to obtain local word-level gesture embedding for each speech-text word. During training, these provide the inputs for the local gesture-word coupling loss. The two losses encourage the learning of global and local correspondences between the three modalities. IsoGD [56] are two gesture recognition datasets, providing benchmarks for the ChaLearn challenges. However, these datasets are of people using gestures for task, (e.g. playing game or controlling an appliance), and are not suitable for learning or evaluating co-speech gestures. Montalbano II [21] is another dataset covering gestures from vocabulary of 20 Italian sign gesture categories. Again, this is not suitable for our task. Learning Video Representations. Representation learning in videos [39, 42, 54, 57] has gained significant attention driven by the availability of large-scale video datasets. Learning video representations from text offers promising advantage by incorporating interpretability via language. Recent works on vision language representation learning [7, 29, 43, 52, 58] highlight this potential. On the other hand, since audio is naturally paired with video, other studies [28, 50] have explored learning video representations from audio. More recently, multimodal approaches have emerged for video representation learning. LanguageBind [65] utilizes depth, infrared, audio, and video to enhance video representations, while Video-LLaMA [62] learns video representations from free-form text and audio. Following these successes, our work aims to leverage multimodal data - video, audio and text - to advance the understanding of gestures. 3. Method Our goal is to learn co-speech gesture representations from speech and text supervision. Given dataset G, S, of gesture clips, the accompanying speech segments and their corresponding transcriptions, our goal is to learn gesture representations that capture the rich semantics (from text, usually) and utterance style (usually from speech) of what is being spoken. 3.1. Overview JEGAL learns gesture representations by solving two multimodal contrastive objectives between the gesture video and the two other modalities, i.e., speech and text. Each of the three modalities are first encoded using separate encoders G, S, to get modality-specific embeddings gT RT d, sw RW d/2, lw RW d/2 to obtain framelevel (T ) and word-level (W ) representations. The speech and text embeddings are fused into joint speech-text embeddings cw RW as depicted in Fig 2. We learn these representations using (1) global phrase-level contrastive objective, and (2) local gesture-word coupling loss. The first one encourages the model to learn global semantics to match gesture clip to speech/text segment. The second objective enforces the model to find the strongest word-level matches between the gesture clip and the other two modalities. We describe our architecture and loss functions in detail below. 3.2. Gesture Encoder Gesture backbone. Given gesture clip (T,h,w,3) of frames, we encode it using stack of 3D convolution layers, similar to previous audio-visual networks [6, 40, 41], where the first layer has temporal receptive field of 5 frames to capture the motion information. We obtain sequence of visual feature vectors of dimension d. These feature vectors are further encoded using stack of Transformer encoder layers to get fg RT d. We initialize the backbone weights from GestSync [25] and keep it frozen. Gesture head. We use Transformer encoder followed by projection layer to get the gesture embeddings, gT RT d. 3.3. Text Encoder Text backbone. Given text transcription corresponding to the gesture video clip, we use the final layer outputs from pre-trained bi-directional language model, multilingual Roberta XLM-Base [17] to obtain text representations. The output of the text backbone is sequence of sub-word feature vectors, fl. Text head. The text head is similar to the gesture head that uses stack of Transformer layers. The sub-word embeddings fl are encoded and projected to get the final sub-word embeddings lsw of feature dimension d/2 each. We aggregate these sub-word tokens to word-level tokens later in the fusion block. 3.4. Audio Encoder Given speech waveform S, we convert it into melspectrograms which is encoded using stack of 2D-CNN layers following previous works [6, 16]. The output of the audio encoder is sequence of speech feature vectors, sRT d/2. 3.5. Fusion Block Before we fuse the speech and text embeddings, we aggregate the text and audio embeddings to obtain word-level feature vectors. We average the sub-word embeddings for each word to get word-level text embeddings lw RW d/2. Using the start and end times of each word, we average the speech features for each word to get word-level features sw RW d/2. We fuse the speech and text features by concatenating along the feature dimension to get joint word-level representations, cw RW d. 3.6. Gesture-Word Alignment The word boundaries are aligned with the speech, but not necessarily with the gestures in the video. The gestures can be longer or shorter than the window in which the word is uttered, and can also be offset. To handle this discrepancy, we propose an attention-based pooling mechanism to obtain the gesture embedding corresponding to the word. We first pad the speechbased word start-end times with p=10 video frames on either side. Let S,E be the start and end frames for the padded window for the word cwi. We obtain the word-level gesture embedding, gwi by using the word embedding cwi for attention-pooling over the extended temporal interval of the word: gwi = (cid:32) (cid:88) j=S exp(γgTj cwi) j=Sexp(γgTj cwi) (cid:80)E (cid:33) gTj (1) 3.7. Training Objective We only have reliable supervision at the phrase level meaning we know that specific text or speech segment corresponds to gesture clip. However, we do not know which individual words are gestured. Keeping this in mind, we employ two loss functions. Global Phrase Contrastive Loss. To obtain the global phrase embeddings, we average pool the speech-text embeddings and the video frame embeddings to give us and respectively. Given batch of samples, we employ the contrastive InfoNCE loss [38] to encourage similarity between the positive triplets, and dissimilarity between the 2N negative triplets: Lseq = 1 (cid:32) (cid:88) log i=1 exp(γcos(gi,ci)) j=1exp(γcos(gi,cj)) (cid:80)n (cid:33) (2) to weak gesture-word associations (Tab 6, row 1). However, directly training to match gestures to words is not possible very few words are gestured in given phrase, and we do not know which of them are. Thus, we devise new strategy to learn word-level correspondences using phrase-level supervision. Given pair of word-level gesture and speech-text (gw,cw) embeddings, we first find the closest gesture gwj for each speech-text word cwi. Our hypothesis is that matching (gw,cw) will have higher number of strong word couplings than non-matching pair. With this idea, we define the following scoring function and the gesture-word coupling loss: λ(gw ,cw )="
        },
        {
            "title": "1\nW",
            "content": "W (cid:88) i=1 max j=1,2..W cos(gwi ,cwj ) Lcouple ="
        },
        {
            "title": "1\nN",
            "content": "(cid:32) (cid:88) log i=1 exp(γλ(gw ,cw j=1exp(γλ(gw )) ,cw )) (cid:80)N (3) (4) (cid:33) The gesture-word coupling loss simply maximizes λ for matching gesture-speech-text samples while minimizing λ for the negative ones. In other words, the model is encouraged to find more strong word-level couplings for positive gesture-speech-text phrases in the batch. Our final loss function is weighted sum of the two losses: L=β Lseq +(1β) Lcouple (5) 3.8. Implementation Details We now describe the essential implementation details, more details can be found in the supplementary material. Training data. We train our model, JEGAL, on triplets of gesture clips, speech segments, and text transcriptions. For the gesture frame inputs, we resize the frames to 270480 pixels. We extract melspectrograms with hop length of 10ms. The word-aligned text transcriptions are tokenized into wordpiece tokens. Using the start-end time of the word boundaries, we randomly sample video clip between 210 seconds in length. Modality drop. In order to encourage the model to learn both speech and text representations equally well, we randomly set one of these modality inputs to zero 50% of the time. This is commonly done in audio-visual speech recognition models [3, 50]. This also allows us to use only one modality (speech or text) during inference, if necessary. Model hyper-parameters. For the text and gesture heads, we set the number of Transformer layers to 3 and 6 respectively. The Transformer uses hidden dimension of 512 and feed-forward dimension of 2048 with 8 attention heads. Training hyper-parameters. We use the AdamW optimizer [30] with learning rate of 5e5, weight decay of 1e4 and betas (0.9,0.98). We reduce the learning rate by factor of 5 when the validation performance does not improve for 2 epochs. 3.9. Training Datasets Local Gesture-word Coupling Loss. Pooling the word-level representations to compute the global phrase loss can lead We train our model using the following datasets: (i) PATS [24], (ii) LRS3 [4], and (iii) LRS3-Lang [5, 25]. By merging these Table 1. We train and evaluate on multiple datasets consisting of 720 hours of gesture clips spanning 5 languages and 7000+ speakers. For evaluation, we curate task-specific benchmarks from the publicly available AVSpeech [20] dataset. Dataset split # hours # spk. avg. clip duration (s) # videos PATS [24] LRS3 [4] LRS3-lang [5] Combined AVS-Ret AVS-Spot AVS-Asd train train train train test test test 162.3 189.6 366.5 718.4 0.26 0.38 0.53 24 4292 2642 6958 420 391 11.37 11.83 18.03 14.2 1.85 2.73 3.80 51390 57337 73173 181900 500 500 three datasets, we compile comprehensive multilingual gesture dataset suitable for both training and evaluation. The specifics of each dataset are outlined in Table 1. PATS [24] is publicly available dataset comprising 250 hours of video content from 25 speakers sourced from diverse platforms such as lectures, talk-shows, YouTube, and televangelists. The LRS3 [4] dataset is composed of 400 hours of TED talks spanning broad spectrum of speakers and rich vocabulary. We also use subset of the LRS3-lang [5] dataset, containing 366 hours across 4 distinct languages. We acquired this dataset directly from the authors, as it has not been made publicly available yet. Pre-processing: We resample all videos to 25 FPS, and the speech to 16kHz. We leverage WhisperX [11] in cases where datasets lack word-aligned text transcripts. Additionally, using the L2 distance between consecutive frame body keypoints, we filter out samples with minimal gesture activity. We also make sure to mask out the face region to avoid leakage from lip movements. Table 1 presents the final statistics of all the datasets. 4. Downstream Tasks and Evaluation We describe our newly curated evaluation benchmarks and the different downstream tasks to evaluate the quality of our learned gesture representations. The first is cross-modal retrieval, the second is spotting gestured words, and the third is active speaker detection. Note that in all the tasks, while we use the joint speech-text embedding, we can obtain uni-modal scores by inputting zeros to omit modality during inference. 4.1. Cross-modal Retrieval Given gallery of gesture-speech-text samples, the task is to retrieve gesture clip given speech segment and/or text and vice-versa. Concretely, given speech or text as query, we obtain speech-text embedding, Rd and rank the gesture embeddings Rd in the gallery by cosine similarity, highest being at the top. We do the same process for the gesture to speech-text retrieval as well. Retrieving relevant gestures for text or speech segment enables several practical applications. For digital avatars, we can retrieve most plausible hand gesture clips to accompany what the avatar is speaking, leading to more immersive and engaging experience. In gaming applications, given database of gesture sequences, the developer can automatically select the most relevant gestures to go with the in-game dialogues. Gestures can assist in language learning [32] by improving word-level memory retention (e.g. eat, kick, clap). Language teaching apps will be able to retrieve gesture clips for sentences to improve the speed of foreign language learning. 4.2. Spotting Gestured Words Given gesture clip with the accompanying speech/text segment and word of choice from this segment, the goal is to localize the word in the gesture clip. Concretely, we obtain word-level speech-text (cw) embeddings and frame-level gesture embeddings, gT . To localize the i-th word, cwi, we compute the cosine similarity of the word embedding with all the gesture frame embeddings. The localization of the word in the video is simply obtained by keeping only the locations with similarity scores δ =0.5. Spotting can be useful to enhance transcriptions by supplementing the plain words with stress and emotion labels. Another application would be to create word-level gesture databases, e.g. thousand different ways the word big is gestured by people all over the world, which will be useful for language and communication analysis. 4.3. Active Speaker Detection Given gesture clips of different speakers, and speech (S) and/or text segment (T ), the goal is to predict the active speaker who is uttering the queried speech/text. To do this, we extract the sequence-aggregated gesture features gi Rd,i 1,2,...,P for each of the clips. Given the query speech or text, we obtain the speech-text feature, c. The active speaker is the one whose gesture and speech-text cosine similarity is maximum: A= argmax i1,2,...,S cos(c, gi) (6) The majority of audio-visual models, encompassing tasks like speech recognition, generation, and translation, primarily operate on inputs containing single speaker. Thus, there arises necessity to identify the speaker within video segment. To determine the active speaker in multi-speaker scenario, previous works [2, 45] have shown the benefits of resorting to the face for lip-sync with the audio, and text subtitles when the audio is corrupted. We extend this thread even further. What happens if the lip region is occluded or unclear? Another important use-case is privacy preserving [59] active speaker detection: what if the active speaker detection needs to be done without leaking the face identity of the speaker? We show that we can successfully do this with very little identity information, i.e. by only using the hand gestures, we can determine who is speaking. 4.4. AVSpeech Test Benchmarks Using the AVSpeech official test set [20], we manually curate three separate evaluation benchmarks for the three downstream Table 2. Cross-modal retrieval performance on the AVS-Ret benchmark (Sec 4.4). JEGAL outperforms the baselines by large margin. Method Random Zero-shot Clip4Clip [31] Language-Bind [65] GestSync [25] Fine-tuned Clip4Clip [31] Language-Bind [65] GestSync [25] GestureDiffuClip [10] Ours JEGAL JEGAL JEGAL Speech-text to Gesture retrieval Mod. R@5 R@10 R@25 R@50 MR R@5 R@10 R@25 R@50 MR Gesture to Speech-text retrieval 5.00 10.00 250 10.00 2.00 1.00 1.00 2.00 5.00 1.90 1.70 1. 2.40 2.40 3.60 8.90 2.70 2.30 2.60 4.60 4.40 7.20 11.67 30.00 26.00 28.40 5.90 6.00 6.60 8.90 9.00 18.00 22. 50.20 49.40 53.00 15.10 14.00 14.40 14.00 15.60 30.20 33.20 58.80 58.80 69.80 194 207 200 214 187 113 25 26 22 2.00 2.00 1.80 2.40 2.80 6.20 9.70 10.80 11.60 16.40 3.10 3.20 3.60 5.00 4.60 9.20 14. 19.20 20.20 26.40 7.20 6.60 11.00 9.20 9.00 21.00 24.10 34.60 33.20 43.60 12.80 12.20 20.20 15.60 14.40 31.20 36. 47.80 45.00 55.20 205 212 159 219 199 109 94 55 62 38 15.80 14.20 17.40 gesture tasks. The statistics for the evaluation test sets are summarized in Table 1. AVS-Ret. We create new cross-modal retrieval benchmark containing diverse gesture clips of hundreds of unique speakers. We choose gallery of 500 clips, which also contain isolated clean speech and accurate text transcriptions. We verify that the clips contain reasonable gesture activity and transcripts with at least two nouns or verbs or adjectives. For evaluation, we use the standard metrics used in other video-text retrieval works [58, 65], i.e. Recall@K and Median Rank. We evaluate both gesture (g) to content (speech-text c) retrieval and vice-versa and show both unimodal and multimodal retrieval performance. AVS-Spot. To quantitatively evaluate the gesture spotting task, we manually curate new test dataset where we search and annotate clips that clearly contain word that is gestured. We obtain 500 such clips, each containing target word that is clearly gestured. The manual annotation process removes all kinds of label noise in the test set, allowing for faithful evaluation of our newly defined gesture spotting task. Additionally, we also manually annotate these target words with binary stress/emphasis labels, which can have important cues about the gesture  (Table 5)  . AVS-Asd. To build the evaluation dataset for active speaker detection, we first choose 500 target clips. For these target clips, we create three evaluation subsets, where we choose 1 clips from different speakers, where =2,4,6. We report the accuracy of detecting the correct target speaker out of the different speakers. 5. Results 5.1. Baselines For baselines, we report performance of zero-shot pre-trained video-language models [58, 65] and pre-trained GestSync [25], which learns gesture-audio correspondences by solving for audio-visual synchronization. We also report scores after fine-tuning all these models further on our training data for fair comparison. In addition, we compare with the semantic encoder of GestureDiffuCLIP [10], by training it on our dataset. 5.2. Cross-modal Retrieval In Table 2, we compare the performance of JEGAL against other baselines on the cross-modal retrieval task. Zero-shot evaluation of foundational vision-language models like LanguageBind [65] and Clip4Clip [31] leads to slightly higher than chance performance. These models are designed to capture different kinds of features: they cannot handle large number of frames, and learn non-gesture attributes like identity and scene. Fine-tuning these models nearly doubles their performance on the task, but it is still far from the performance of JEGAL . GestSync [25] is trained with gesture-related task, and hence performs nearly twice as well as the foundational vision-language models. However, since this network is trained to detect synchronization offsets in speech and video, its representations perform poorly for global semantic tasks like retrieval. This is also partly true for our model when we turn off the global phrase loss (Table 6 row 2 vs row 3). GestureDiffuCLIPs semantic encoder [10] performs the best among the baselines, however, the lack of local word-level semantic supervision leads to an inferior performance compared to JEGAL . Furthermore, none of the baseline approaches ingest and fuse multi-modal speech-text inputs. Our JEGAL model outperforms previous methods by large margin. JEGAL can retrieve gestures from speech or text queries with similar performance. The opposite direction is also true, i.e. retrieving speech or text for query gesture clip. Finally, retrieving with the fused speech-text representation, is clearly better than the unimodal variants, showing that the speech and text embeddings each encode information that is not present in the other modality. Table 3. Gesture-based word spotting performance on the AVS-Spot benchmark (Sec 4.4). Method Zero-shot Clip4Clip [31] Language-Bind [65] GestSync [25] Fine-tuned Clip4Clip [31] Language-Bind [65] GestSync [25] GestureDiffuClip [10] Ours JEGAL JEGAL JEGAL Mod. Accuracy 7.62 9.11 5.12 14.79 15.68 10.41 21.50 60.40 40.40 63.00 5.3. Spotting Gestured Words Table 3 compares the spotting accuracy of different methods on the AVS-Spot benchmark. Unlike JEGAL that uses the word-level Lcouple loss, all the baseline methods, including the semantic encoder of GestureDiffuCLIP [10], use only phraselevel loss and hence, struggle to learn fine-grained word-level associations. Through this task, we also see the big advantage of using text modality in addition to speech text-based gesture spotting is more accurate than using audio. This is expected, since word-level semantic correspondences are easier to learn in text space. Having said that, using speech alongside text still gives clear improvement even in the gesture spotting task. In Fig 3, we show two examples of spotting gestured words. The heatmaps show the similarity of the word (red is higher) along the video frames. In the first example, the lady gestures beautiful with her fingers, and in the second example, the speaker clenches the fist to show energy. We can see that not all words get high similarity score, only ones with distinctive gestures are spotted. Furthermore, the spotting does not exactly align with the speech-based word boundary (indicated by yellow vertical lines). Our alignment layer (Sec 3.6) allows the model to look beyond the speech-based boundary and find the exact frames where the word is gestured. 5.4. Active Speaker Detection In Table 4, we show the accuracy of identifying the target speaker for given text and/or speech segment. This task is different from our other tasks it does not need strong holistic understanding of the gesture sequence like retrieval, nor does it need semantic word-level understanding. It can simply be solved by checking for frame-level synchronization, which is exactly why GestSync [25], LanguageBind [65] baselines perform well in this task. Also, compared to the previous tasks, speech information is more useful here compared to text. While the text-only variant of JEGAL lags behind GestSync [25], the speech and speech-text variants actually emerges as the best performing approaches. This task shows that JEGAL learns gesture features that are useful for wide variety of tasks, across both speech and text modalities. Table 4. Performance of active speaker detection on AVS-Asd benchmark. We report the mean class accuracy of predicting the active speaker among set of speakers, where =2,4,6. Method 2 spk. Mod. 50.0 56.4 60.1 57.3 Random Zero-shot Clip4Clip [31] Language-Bind [65] GestSync [25] Fine-tuned Clip4Clip [31] Language-Bind [65] GestSync [25] GestureDiffuClip [10] Ours JEGAL JEGAL JEGAL 60.0 62.5 72.8 66.8 69.4 77.2 80.8 4 spk. 6 spk. 25.0 16.7 31.2 38.9 35.1 40.1 41.4 57.2 43.9 48.6 60.0 66. 23.0 27.2 26.2 28.8 28.2 48.0 34.8 37.4 52.2 59.0 6. Insights and Ablations In this section, we provide additional insights into the gesture signals learned from the speech and text modalities, the impact of the two loss functions on the downstream tasks, and the choice of speech-text fusion. 6.1. Speech v/s Text Modalities We have already seen how our model can flexibly leverage speech or text modality to solve three different kinds of tasks. In Fig 4, we show an example where audio-based gesture word spotting is successful, but text-based spotting is not. And we see that the speech clearly has the word action being emphasized. This example leads us to do deeper analysis of gesture word spotting based on stress cues. We divide the binary stress labels to split the AVS-Spot test set (500 samples) into two subsets, one containing only samples with stressed/emphasized words (100 samples) and the other subset containing the remaining words (400 samples). In Table 5, we report the same spotting accuracy metric on these subsets separately. While we saw previously in Table 3 that speech-based gesture spotting is clearly inferior compared to text-based gesture spotting, it is not always the case. We see that the difference in spotting accuracy Figure 3. JEGAL can spot the gestured words in video clip. Here, we show similarity heatmap of words vs video frames. The vertical yellow lines indicate the speech-based word boundaries of beautiful and energy. The red triangles zoom into the corresponding frames where JEGAL detects the words, clearly aligning with the gestures. For the word beautiful the gestured segment is smaller than the spoken word boundary and for the word energy it extends well beyond to the right of the word boundary. The alignment layer (Sec 3.6) allows the model to look beyond just the speech-based boundaries. Note that our model learns to perform gestured-word spotting without using any training labels on which words are gestured. between the stressed and non-stressed words is higher for the speech modality. This indicates that speech modality pays more attention to emphasis of the word compared to the text modality. Table 5. Stressed words are more likely to be spotted with speech-based spotting than non-stressed words. As seen in the column , the difference between stressed vs. non-stressed word spotting is higher for the speech modality. Table 6. Each loss encourages feature learning at different temporal granularity, and combination of the two loss functions performs the best. Loss Retrieval Spotting ASD R@5 R@10 MR Acc. Acc. 11.60 Seq. contrastive Word coupling 7.90 Seq. + Word coupling 17.40 21.40 13.40 28.40 43 78 22 18.83 51.98 63. 42.8 19.6 59.0 Modality With stress W/o stress All words 6.3. Fusion techniques Text Speech 68.00 51. 58.50 37.75 60.40 44.00 16.2% 35.1% 6.2. Impact of loss functions In Table 6, we study the impact of our two loss functions. In the first row, we only train with the phrase contrastive loss, which captures the global semantics. The second row is the variant trained only with gesture-word coupling loss, which captures local word-level semantics. Compared to the dual loss model, the results from these individual loss models are significantly worse. Specifically, the variant without the word coupling loss performs poorly on the spotting task and the one without the sequence contrastive loss performs poorly on retrieval and active speaker detection. The combination of the two losses performs the best across all the tasks, thus demonstrating the complementary nature of the two training objectives. Figure 4. Audio and text heatmap examples when words are stressed. In Table 7, we ablate different ways to fuse the speech and text features. The first case is to not fuse at all and have two separate pairwise contrastive losses: gesture-audio and gesture-text. We can see in the first two rows that this is an inferior design. In fact, it is better to train with single contrastive head after fusing the speech and text embeddings (rows 3, 4). The fusion strategy of choice would be to concatenate, rather than average. Table 7. Ablation study on fusing speech and text modalities. Training without fusing is far worse, as the model cannot perform the tasks by using multiple information streams at the same time. Retrieval Spotting ASD R@5 R@10 MR Acc. Acc. Pairwise (with text) 9.39 Pairwise (with audio) 9.80 16.00 Late fusion (avg.) 17.40 Late fusion (concat.) 15.58 16.60 24.60 28. 70 72 30 22 34.31 23.67 57.27 63.00 29.6 31.4 54.0 59.0 7. Conclusion In this work, we learn joint embedding space that captures cross-modal relationships with gestures, speech, and language. We show that we can learn such an embedding space with weak supervision using careful design of two loss functions. We evaluate these new representations on three new downstream tasks and manually curated test sets. We observe that the two modalities, i.e., speech and text, learn complementary features that can be useful for different kinds of gesture-related tasks. One promising future direction would be to explore 2D and 3D keypoint based inputs to make the network computationally lighter and less susceptible to distracting features. Acknowledgements. The authors would like to thank Piyush Bagad, Ragav Sachdeva, and Jaesung Hugh for their valuable discussions. They also extend their thanks to David Pinto for setting up the data annotation tool and to Ashish Thandavan for his support with the infrastructure. This research is funded by EPSRC Programme Grant VisualAI EP/T028572/1, and Royal Society Research Professorship RPR1191132."
        },
        {
            "title": "References",
            "content": "[1] 55% rule. https : / / online . utpb . edu / about - us / articles / communication / how - much - of - communication-is-nonverbal/. Accessed: 2024-1121. 12 [2] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. The conversation: Deep audio-visual speech enhancement. In INTERSPEECH, 2018. 5 [3] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. Deep lip reading: comparison of models and an online application. In INTERSPEECH, 2018. 4 [4] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. Lrs3-ted: large-scale dataset for visual speech recognition. arXiv preprint arXiv:1809.00496, 2018. 4, 5 [5] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. Now youre speaking my language: Visual language identification. In INTERSPEECH, 2020. 4, [6] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. In Proc. ECCV, 2020. 3, 4 [7] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 3 [8] Samuel Albanie, Gul Varol, Liliane Momeni, Hannah Bull, Triantafyllos Afouras, Himel Chowdhury, Neil Fox, Bencie Woll, Rob Cooper, Andrew McParland, and Andrew Zisserman. BOBSL: BBC-Oxford British Sign Language Dataset. 2021. 2 [9] Michael Andric and Steven Small. Gestures neural language. Frontiers in psychology, 3:99, 2012. 1 [10] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip: Gesture diffusion model with clip latents. ACM Transactions on Graphics (TOG), 42(4):118, 2023. 2, 6, 7 [11] Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. Whisperx: Time-accurate speech transcription of long-form audio. In INTERSPEECH, 2023. 5 [12] Loıc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al. Seamless: Multilingual expressive and streaming speech translation. arXiv preprint arXiv:2312.05187, 2023. [13] Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard Bowden. Neural sign language translation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 77847793, 2018. 2 [14] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. Sign language transformers: Joint end-to-end sign language recognition and translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1002310033, 2020. 2 [15] Yiting Cheng, Fangyun Wei, Jianmin Bao, Dong Chen, and Wenqiang Zhang. Cico: Domain-aware sign language retrieval In Proceedings of via cross-lingual contrastive learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1901619026, 2023. 2 [16] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Workshop on Multi-view Lip-reading, ACCV, 2016. 4 [17] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019. [18] Trevor Darrell, Irfan Essa, and Alex Pentland. Task-specific IEEE gesture analysis in real-time using interpolated views. Transactions on Pattern Analysis and Machine Intelligence, 18 (12):12361242, 1996. 2 [19] Amanda Duarte, Samuel Albanie, Xavier Giro-i Nieto, and Gul Varol. Sign language video retrieval with free-form textual queries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1409414104, 2022. 2 [20] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: speakerindependent audio-visual model for speech separation. ACM Trans. Graph., 37, 2018. 1, 5, 12 [21] Sergio Escalera, Jordi Gonz`alez, Xavier Baro, Miguel Reyes, Oscar Lopes, Isabelle Guyon, Vassilis Athitsos, and Hugo Escalante. Multi-modal gesture recognition challenge 2013: Dataset and In Proceedings of the 15th ACM on International results. conference on multimodal interaction, pages 445452, 2013. 3 [22] William Freeman and Michal Roth. Orientation histograms for hand gesture recognition. In International workshop on automatic face and gesture recognition, pages 296301. Citeseer, 1995. 2 [23] Esam Ghaleb, Ilya Burenko, Marlou Rasenberg, Wim Pouw, Ozyurek, and Peter Uhrig, Judith Holler, Ivan Toni, Aslı Raquel Fernandez. Co-speech gesture detection through multi-phase sequence labeling. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 40074015, 2024. 2 [24] Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and Jitendra Malik. Learning individual styles of In Proceedings of the IEEE/CVF conversational gesture. Conference on Computer Vision and Pattern Recognition, pages 34973506, 2019. 2, 4, 5 [25] Sindhu Hegde and Andrew Zisserman. Gestsync: Determining who is speaking without talking head. In Proc. BMVC, 2023. 2, 3, 4, 6, [26] Adam Kendon. Gesture: Visible action as utterance. Cambridge University Press, 2004. 1, 2 [27] Sanshzar Kettebekov, Mohammed Yeasin, and Rajeev Sharma. Improving continuous gesture recognition with spoken prosody. In 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings., pages II. IEEE, 2003. 2 [28] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, and Yale Song. Acav100m: Automatic curation of large-scale datasets for audio-visual video representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1027410284, 2021. 3 [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 3 [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 4 [31] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomput., 508:293304, 2022. 6, 7 [32] Manuela Macedonia, Karsten Muller, and Angela Friederici. The impact of iconic gestures on foreign language word learning and its neural substrate. Human brain mapping, 32(6):982998, 2011. [33] Cynthia Matuszek, Liefeng Bo, Luke Zettlemoyer, and Dieter Fox. Learning from unscripted deictic gesture and language In Proceedings of the AAAI for human-robot interactions. Conference on Artificial Intelligence, 2014. 2 [34] David McNeill. Hand and mind: What gestures reveal about thought. University of Chicago press, 1992. 1, 2 [35] Yuecong Min, Aiming Hao, Xiujuan Chai, and Xilin Chen. Visual alignment constraint for continuous sign language In Proceedings of the IEEE/CVF international recognition. conference on computer vision, pages 1154211551, 2021. 2 [36] Pavlo Molchanov, Xiaodong Yang, Shalini Gupta, Kihwan Kim, Stephen Tyree, and Jan Kautz. Online detection and classification of dynamic hand gestures with recurrent 3d convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 42074215, 2016. 2 [37] Louis-Philippe Morency, Ariadna Quattoni, and Trevor Darrell. Latent-dynamic discriminative models for continuous gesture recognition. In 2007 IEEE conference on computer vision and pattern recognition, pages 18. IEEE, 2007. 2 [38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 4 [39] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [40] Prajwal, Triantafyllos Afouras, and Andrew Zisserman. Sub-word level lip reading with visual attention. In Proc. CVPR, 2022. 2, 3 [41] Prajwal, Hannah Bull, Liliane Momeni, Samuel Albanie, Gul Varol, and Andrew Zisserman. Weakly-supervised fingerspelling recognition in british sign language videos. In Proc. BMVC, 2022. 2, 3 [42] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal In Proceedings of contrastive video representation learning. the IEEE/CVF conference on computer vision and pattern recognition, pages 69646974, 2021. 3 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3 [44] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 2849228518. PMLR, 2023. 2 [45] Akam Rahimi, Triantafyllos Afouras, and Andrew Zisserman. Reading to listen at the cocktail party: Multi-modal speech separation. In Proc. CVPR, 2022. 5 [46] Charles Raude, Prajwal, Liliane Momeni, Hannah Bull, Samuel Albanie, Andrew Zisserman, and Gul Varol. tale of two languages: Large-vocabulary continuous sign language recognition from spoken language supervision. arXiv, 2024. 2 [47] Ben Saunders, Necati Cihan Camgoz, and Richard Bowden. Progressive transformers for end-to-end sign language production. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16, pages 687705. Springer, 2020. [48] Ben Saunders, Necati Cihan Camgoz, and Richard Bowden. Signing at scale: Learning to co-articulate signs for large-scale In Proceedings of photo-realistic sign language production. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51415151, 2022. 2 [49] Rajeev Sharma, Jiongyu Cai, Srivat Chakravarthy, Indrajit Poddar, and Yogesh Sethi. Exploiting speech/gesture co-occurrence for improving continuous gesture recognition in weather narration. In Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), pages 422427. IEEE, 2000. 2 [50] Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Learning audio-visual speech representation arXiv preprint Mohamed. by masked multimodal cluster prediction. arXiv:2201.02184, 2022. 3, 4 [51] Stephanie Stoll, Necati Cihan Camgoz, Simon Hadfield, and Richard Bowden. Text2sign: towards sign language production using neural machine translation and generative adversarial International Journal of Computer Vision, 128(4): networks. 891908, 2020. 2 [52] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: joint model for video In Proceedings of the and language representation learning. IEEE/CVF international conference on computer vision, pages 74647473, 2019. [53] Laia Tarres, Gerard Gallego, Amanda Duarte, Jordi Torres, and Xavier Giro-i Nieto. Sign language translation from instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56255635, 2023. 2 [54] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 3 [55] Petra Wagner, Zofia Malisz, and Stefan Kopp. Gesture and speech in interaction: An overview, 2014. 2 [56] J. Wan, S. Z. Li, Y. Zhao, S. Zhou, I. Guyon, and S. Escalera. ChaLearn looking at people RGB-D isolated and continuous datasets for gesture recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 761769, 2016. 3 [57] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1454914560, 2023. 3 [58] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 3, [59] Runhua Xu, Nathalie Baracaldo, and James Joshi. Privacypreserving machine learning: Methods, challenges and directions. arXiv preprint arXiv:2108.04417, 2021. 5 [60] Aoxiong Yin, Tianyun Zhong, Li Tang, Weike Jin, Tao Jin, and Zhou Zhao. Gloss attention for gloss-free sign language translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 25512562, 2023. 2 [61] Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee. Speech gesture generation from the trimodal context of text, audio, and speaker identity. ACM Transactions on Graphics (TOG), 39(6):116, 2020. 2 [62] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An for video instruction-tuned audio-visual understanding. arXiv preprint arXiv:2306.02858, 2023. 3 [63] Yihao Zhi, Xiaodong Cun, Xuelin Chen, Xi Shen, Wen Guo, Shaoli Huang, and Shenghua Gao. Livelyspeaker: Towards semantic-aware co-speech gesture generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2080720817, 2023. 2 language model [64] Hao Zhou, Wengang Zhou, Yun Zhou, and Houqiang Li. Spatial-temporal multi-cue network for continuous sign language recognition. In Proceedings of the AAAI conference on artificial intelligence, pages 1300913016, 2020. 2 [65] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852, 2023. 3, 6, [66] Ronglai Zuo and Brian Mak. C2slr: Consistency-enhanced In Proceedings of continuous sign language recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51315140, 2022. 2 In Figure 8, our model detects the stressed keywords every and respond, whereas the text-only model misses these words. Evidently, the audio-only model looks for word emphasis cues (indicated by high pitch) as such words are more likely to be gestured. This would be difficult to infer from text modality alone. These examples illustrate the advantages of leveraging audio cues for gesture spotting. E. Limitations and Areas of Improvement Our work is the first to tackle large-scale co-speech gesture understanding. We highlight some of the limitations of our approach here. Having trained predominantly on TED/TEDx talks, our models can struggle with co-speech conversational gestures. Further, the TED/TEDx talk setting inherently encourages continuous gesturing and moving the arms, which might not always be the case for other settings (like conversational interactions). Another aspect the model struggles with is when there are limited gesture actions or hand movements that are unrelated to speech. Finally, given that we learn with only weak sequence-level supervision, the model can find shortcuts by focusing on simple rhythmic hand movements that occur in certain gestures classes like the beat gestures. This can affect the representation quality of iconic and deitic gestures that contain clear semantic meaning. While we still show that our models can spot such gestures, future works can focus on improving this imbalance in gesture classes. F. Potential Negative Societal Impacts While our research significantly contributes to advancing gesture understanding, there are some potential risks of surveillance, as the system could infer conversations from distance by identifying words/phrases. Nonetheless, we believe the benefits outweigh these risks, as the technology enhances humanmachine interaction by integrating non-verbal cues. According to the 55% rule [1], non-verbal communication constitutes 55% of overall communication. This highlights the importance of enabling machines to engage in holistic, natural interactions with humans by understanding non-verbal elements like gestures. A. Most Gestured Words In Figure 5, we show the most commonly spotted gestured words that are spotted by JEGAL on the AVS-Spot test set: pointing gestures (you, my, we), adjectives/adverbs (little, open, whole, gigantic, broad), direction words (forward, here, below) and numbers (one, two, first). Figure 5. Word cloud for the most commonly gestured words. B. Model Details In Table 8, we provide detailed description of the model architecture. We will release the code and models to support future research. C. Dataset Visualization In Figure 6, we present examples from our manually annotated AVS-Spot test set (curated from the publicly available AVSpeech test dataset [20]), designed to evaluate downstream gesture spotting performance. As shown, the dataset includes diverse collection of unique words, carefully curated to ensure clear and contextually appropriate gestures. For instance, in row-1, the word little is accompanied by gesture where two fingers move close together to indicate small size; in row-2, the speaker points backward to represent the word back; and in row-6, an inward motion is used to indicate my. D. Qualitative Results In Figure 7, we show additional qualitative examples for gesture spotting. In the left text panel, the red-highlighted word represents the keyword to be spotted, as curated in the AVS-Spot test set. The word-labeled vertical columns, separated by yellow lines, indicate the word boundaries derived from speech-text alignment. JEGAL successfully spots most of these keywords, as shown by the red heatmaps. Notably, the boundaries may vary slightly since speakers often gesture and speak at slightly different times, highlighting the inherent challenges of our weakly-supervised gesture representation learning task. In Figure 8, we present additional examples demonstrating that audio-based gesture spotting tends to focus on stressed regions in speech, unlike text-based spotting. This difference is evident from the audio and text heatmaps for each sample. Table 8. Overview of the model architecture, detailing the input modalities, network components, and key parameters used in each stage of our framework. Branch Visual Branch Layer/Module Vision backbone Projection MLP - Linear - LayerNorm - ReLU - Linear Positional Encoding Transformer (N=6 layers) - Self-Attention (h=8) - Feed Forward Output Projection Text Branch mRoberta Text backbone Transformer (N=3 layers) - Self-Attention (h=8) - Feed Forward Output Projection Audio Branch Input Shape Output Shape 3 270 480 1024 1024 512 512 512 512 512 512 512 W 768 768 768 512 512 512 512 512 512 512 512 768 768 768 256 Melspectrogram Input Conv2D + BN + ReLU (k=5, s=1, p=2) Conv2D + BN + ReLU (k=3, s=2, p=1) Conv2D + BN + ReLU (k=3, s=2, p=1) Conv2D + BN + ReLU (k=3, s=(3,1), p=1) Conv2D + BN + ReLU (k=3, s=(3,1), p=1) Conv2D (k=1, s=(3,1), p=0) Output Projection + reshape 1 80 4T - 1 80 4T 32 80 4T 32 40 2T 64 40 2T 64 40 2T 128 20 128 7 256 7 256 3 256 3 256 3 256 1 256 1 256 Late Fusion Encoded Features - Visual - Text + sub-word pooling - Audio + sub-word pooling 512 256 256 - 256 Figure 6. Visualization of the AVS-Spot dataset, showcasing video frames from different samples. Each row corresponds to single video, with the highlighted keyword indicating the annotated gestured word for spotting. The figure illustrates the datasets diversity, featuring wide range of unique keywords, various speakers, and distinct gestures. Figure 7. Additional gestured word spotting results on AVS-Spot dataset. Keywords are highlighted in red on the left panel and the speech-based force alignment word boundaries are marked by yellow lines. JEGAL successfully spots the gestured keywords, demonstrating its robustness across diverse gestures and speakers. The red triangles zoom into the corresponding frames where JEGAL detects the keywords, clearly aligning with the gestures. Note that in some cases (e.g., rows 2 and 3), ground-truth boundaries may slightly differ, as the speaker can gesture and utter the same word at slightly different times. JEGAL effectively estimates the approximate intervals where the target word is gestured. Figure 8. Examples highlighting the role of stressed speech regions in audio-based gesture spotting. The audio-only model successfully detects the stressed keywords every and respond, whereas the text-only model misses these words. Evidently, the audio-only model looks for word emphasis cues (indicated by high pitch) as such words are more likely to be gestured. This would be difficult to infer from text modality alone. These examples illustrate the advantages of leveraging audio cues for gesture spotting."
        }
    ],
    "affiliations": [
        "Visual Geometry Group, University of Oxford"
    ]
}