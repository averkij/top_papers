{
    "paper_title": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings",
    "authors": [
        "Safal Shrestha",
        "Minwu Kim",
        "Aadim Nepal",
        "Anubhav Shrestha",
        "Keith Ross"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we \"warm up\" the model by distilling Long CoTs from a toy domain, namely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: $(i)$ the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both the base model and the warmed-up model are RLVR trained on the same small dataset ($\\leq100$ examples), the warmed-up model consistently outperforms the base model; $(iii)$ Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; $(iv)$ Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments."
        },
        {
            "title": "Start",
            "content": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings Safal Shrestha* Minwu Kim Aadim Nepal Anubhav Shrestha Keith Ross New York University Abu Dhabi 5 2 0 2 9 1 ] . [ 1 8 1 7 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates major challenge when the amount of quality training data is scarce. We propose sampleefficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we \"warm up\" the model by distilling Long CoTs from toy domain, namely, Knights & Knaves (K&K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: (i) the warmup phase alone facilitates generalized reasoning, leading to performance improvements across range of tasks, including MATH, HumanEval+, and MMLU-Pro. (ii) When both the base model and the warmed-up model are RLVR trained on the same small dataset ( 100 examples), the warmed-up model consistently outperforms the base model; (iii) Warming up before RLVR training allows model to maintain cross-domain generalizability even after training on specific domain. (iv) Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments."
        },
        {
            "title": "Introduction",
            "content": "Reasoning-capable large language models (LLMs) have driven major shift in artificial intelligence, particularly in tasks requiring multi-step, cognitively complex problem solving (Guo et al., 2025; *Corresponding author: safal.shrestha@nyu.edu 1Our code is available here. 2Our model weights and dataset are available here. 1 Team et al., 2025; OpenAI, 2024). These models generate long CoTs, capturing reasoning behaviors such as self-reflection, self-correction, and hypothesis testing (Guo et al., 2025; Team et al., 2025; Qin et al., 2024; Huang et al., 2024; Gao et al., 2024). The CoTs mirror human-like problem-solving and have shown notable improvement in accuracy on various benchmarks (Qin et al., 2024; Huang et al., 2024; Gao et al., 2024; Xiang et al., 2025). Most prior approaches to training reasoningcapable models rely on either Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with curated long-CoT demonstrations in specific domains such as math and coding (Guo et al., 2025; Team et al., 2025; Team, 2025; Xu et al., 2025b; Labs, 2025). While effective, these methods require substantial effort to collect and curate domainspecific data, raising the need for more effective and efficient training strategies. Thus, in the LLM reasoning space, there is need for meta-learning: learning generalizable reasoning strategies that model can rapidly adapt to multiple downstream tasks (Thrun and Pratt, 1998; Finn et al., 2017; Naik and Mammone, 1992). Following this motivation, we ask: Can we train models that acquire general reasoning strategies and rapidly adapt them to new domains with minimal supervision? In this work, we present new framework for training reasoning models in resource-constrained settings, that is, when only small amount of domain-specific training data is available. As shown in Figure 1, the framework has two training phases. In the first phase, we warm up the model on simplified logic game environmentKnights & Knaves (Smullyan, 1986; Xie et al., 2024)to induce generalizable reasoning behaviors. To accomplish this, we generate long CoTs for Knights & Knaves questions, using QwQ-32B, reasoning model (Qwen Team, 2025). Then, without folFigure 1: Our two stage methodology involving (1) warmup phase and (2) target-task adaptation phase for training reasoning models in resource-constrained settings lowing any specialized data filtering scheme, we distill the responses into the base model to create \"warmed-up model\". In the second phase, we adapt the warmed-up model to downstream tasks using only small number of task-specific examples. This setup mimics the meta-learning paradigm: first learn abstract reasoning strategies that can generalize and quickly specialize to new tasks. The main contributions of the paper are: Warmup alone creates strong generalizable reasoning model. In Section 2, we show that the initial warmup phase alone yields strong reasoning model. We warm up various models and consistently observe that it leads to significant improvement across benchmarks from different domains. For example, Qwen2.5-3B model with warmup gains +10.2% on MATH, +15.3% on HumanEval+, and +9.0% on MMLU-Pro. Strikingly, warming up the Qwen2.5-14B model increases its MATH accuracy to 77.4%, closely approaching the 80.2% achieved by full RLVR training with domain-specific data in previous work (Zeng et al., 2025). This suggests that the warmup phase activates generalizable reasoning behaviors in the model. Additionally, we find that our K&K distillation often outperforms distillation with s1K dataset, well-curated dataset of Long CoTs from diverse domains. Warmup turns the model into an effective metalearner In Section 3.1, we show that the warmedup model adapts more effectively when trained with RLVR using limited data. When both the base model and the warmed-up model are RLVR-trained on the same small dataset ( 100 examples), the warmed-up model consistently outperforms the base model. For example, with the Qwen2.53B model, the warmed-up model achieves absolute gains of +6.7% on MATH and +5.0% on HumanEval+ compared to the base model RLVRtrained under the same conditions. Remarkably, on MATH, our two-stage approach on just 100 random examples matches the performance of model trained directly with RLVR on the full training set of 7,500 examples. In addition to stronger final performance, the warmed-up model also converges faster, requiring fewer training steps to reach its peak. These findings demonstrate that the warmup phase turns the model into an effective meta-learner, enabling both efficient and effective RLVR training. Warming up allows model to maintain crossdomain generalizability even after subsequent RLVR training on specific domain. RLVR training can sometimes lead models to adopt concise, domain-specific reasoning patterns that optimize for the target task (Fatemi et al., 2025). While such specialization can yield strong in-domain performance, we show in Section 3.3 that it often comes at cost of degrading cross-domain reasoning abilities. However, by introducing the warmup phase into the pipeline, we can retain broader generalization capabilities even after task-specific RLVR training. To further assess generalization, we extend our evaluation beyond the commonly studied areas of math and coding to include subsets of the MMLU-Pro benchmark (in particular, physics & history), offering more comprehensive view of 2 how RLVR affects performance in varied domains (Guo et al., 2025; Team et al., 2025; Wei et al., 2025; Hu et al., 2025; Zeng et al., 2025). With regards to performance, sample efficiency, and generalization, our two-stage approach consistently outperforms baseline models, underscoring the effectiveness of simple warmup with domainagnostic reasoning traces. These results highlight the promise of our method as practical strategy for training robust reasoning models in resourceconstrained settings."
        },
        {
            "title": "2 Warmup phase",
            "content": "The warmup stage is based on the idea that reasoning behaviorssuch as self-reflection and selfcorrectionare broadly applicable across domains. Therefore, this stage is designed to expose the model to such reasoning behaviors such that it can later apply them across various domains. While in past work such warmup phase has mainly relied on domain-specific data (Luong et al., 2024; Guo et al., 2025), we instead train the LLM on reasoning traces from the simple Knights & Knaves (K&K) logic game (Smullyan, 1986; Xie et al., 2024, 2025). In this section, we ask: Can warming up base model with domain-agnostic reasoning patterns lead to substantial improvements on domain-specific tasks?"
        },
        {
            "title": "2.1 Knights & Knaves logic game",
            "content": "In K&K, the goal is to determine which characters are knights (who always tell the truth) and which are knaves (who always lie), based on their statements (Example shown in Table 2). We choose K&K for two reasons. First, solving these problems requires extensive reasoning, which highlights generalizable reasoning strategies in the teacher models responses. Second, K&K relies solely on basic boolean logic and is independent of domainspecific knowledgesuch as mathematical theorems or specialized software librariesallowing the model to focus purely on reasoning behaviors rather than memorization of domain-specific content."
        },
        {
            "title": "2.2 Experiment Details",
            "content": "We mainly conduct our experiments on the Qwen2.5-3B base model, chosen for its compact size and strong baseline abilities (Yang et al., 2024a). For warmup, we use QwQ-32B (Qwen Team, 2025), strong reasoning model, to generate long CoTs on K&K questions. One example of such long CoT is shown in Table 3. Since our goal is to collect responses reflecting extensive reasoning behaviors rather than improving accuracy on K&K, we do not apply rejection sampling that filters out wrong responses, similar to the approach taken in some prior work (Gandhi et al., 2025; Li et al., 2025a). After generating the K&K reasoning traces, we perform Supervised Fine-Tuning (SFT) on the base model with the traces to warmup our model. All details on SFT training with K&K is available in Appendix A.1. Because evaluating LLMs can be noisy (Hochlehnert et al., 2025), we sample multiple times for each question with careful answer extraction mechanisms, as discussed in Appendix A.3. We deal with three diverse datasets to assess the performance of our two-stage methodology across tasks with fundamentally different requirements: 1. MATH: benchmark comprising 12,500 competition-level math problems (7,500 in train and 5,000 in test) designed to assess mathematical reasoning in LLMs (Hendrycks et al., 2021). For evaluation, we use the MATH500 subset of the test set (Lightman et al., 2023). 2. HumanEval+: An extended benchmark of 164 hand-crafted programming problems, each with function signatures, docstrings, and unit tests, aimed at evaluating the code generation capabilities of LLMs (Chen et al., 2021; Liu et al., 2023). In this section, we use all 164 questions for evaluation. When training with RLVR (Section 3), we randomly sample 50 questions and use the remaining examples for evaluation. 3. MMLU-Pro: An enhanced version of the Massive Multitask Language Understanding (MMLU) benchmark, featuring challenging questions across 14 categories like physics, law, history, and so on to test real-world text understanding and complex problem solving (Hendrycks et al., 2020; Wang et al., 2024). For MMLU-Pro evaluation, we use the entire dataset except the examples we set aside for RLVR training (Section 3)."
        },
        {
            "title": "2.3 Results for Warmup Only",
            "content": "Table 1 shows the performance of warmup for the three domains for four base models: the Qwen2.53 Table 1: Performance of the warmed-up model distilled with the K&K game data/s1K data. Best performance highlighted in bold. Model Qwen2.5-3B K&K s1K Qwen2.5-Math-1.5B K&K s1K DeepSeek-Math-7B-Base K&K s1K Qwen2.5-14B K&K s1K MATH (%) HumanEval+ (%) MMLU-Pro (%) 43.8 0.8 54.0 1.4 53.9 1.3 32.5 1.2 47.8 3.7 39.5 4.0 41.2 1.6 65.2 1.0 53.7 1.1 20.6 2.3 28.7 2.1 32.4 2.5 55.6 3.23 77.4 1.9 51.5 0.7 8.8 2.0 5.5 0.8 7.0 0.7 5.3 1.2 13.8 4.9 5.9 3.3 64.3 3.0 67.3 2.3 24.3 1.9 29.2 0.3 38.2 0.4 28.1 0.1 18.3 0.2 27.5 0.2 22.4 0.5 21.1 0.5 23.2 0.3 26.3 0.6 52.7 0.4 62.7 0.2 34.9 0.7 3B base model as discussed previously; the Qwen2.5-1.5B-Math, math-specialized smaller model (Yang et al., 2024b); DeepSeek-Math-7BBase, relatively weaker but larger model (Shao et al., 2024), and Qwen2.5-14B, an even larger model from the Qwen family (Yang et al., 2024a). For comparison against our non-curated K&K game dataset, we also distilled the base model with s1K, dataset comprising of methodically collected, high-quality reasoning samples from various domains, including mathematics, physics, geography, logic, law, and so on (Muennighoff et al., 2025). From Table 1, we observe: 1. For the the Qwen2.5-3B base model, although the warmup process does not convey any mathematics or domain-specific knowledge, warmup leads to marked improvements over the base model across the three domainspecific benchmarks. This suggests that the warmup phase, using reasoning traces from the K&K domain alone, can effectively activate general reasoning capabilities in the model. 2. In similar vain to the 3B model, we see strong improvements for the other base models as well. In case of the 1.5B model, we see significant improvements on the MATH dataset, possibly because of the math-specialized nature of this model. Similarly, for the 14B Qwen model, we find significant improvements across all three benchmarks. Notably, the model achieves 77.4% on MATH after warmup alone, result that approaches the 80.2% reported in prior work using full-scale RL training with domain-specific data (Zeng et al., 2025). In case of the DeepSeek family model as well, we see notable improvements across the benchmarks, showcasing the effectiveness of domain-agnostic warmup. 3. As shown in the \"s1K\" rows of Table 1 for the models, distilling with CoTs for simple logic games (i.e., warming up) generally matches or even performs better than distilling with domain-knowledge-rich s1K data. Some recent studies have showcased that distillation with reasoning-intensive Long-CoT (like s1K data) on smaller models might struggle to learn effectively, showcasing minimal gains or even degradation (Xu et al., 2025a; Li et al., 2025b). Our findings show that distillation in simplified, domainagnostic setting with non-curated data like K&K may actually benefit smaller LLMs by avoiding the complexity of domain-specific reasoning and leading to general performance gains."
        },
        {
            "title": "Behaviors",
            "content": "The reasoning traces generated by the reasoning teacher model (QwQ) using the K&K games data have general reasoning behaviors, like selfreflection and verification, along with task-specific logic like performing boolean operations and han4 dling states. To test whether the performance gains stem from the reasoning behaviors rather than from the domain-specific logic of Knights & Knaves (K&K), we run controlled experiment. In addition to the warmup experiment just described in Section 2.3, we perform another distillation experiment. We again choose the Qwen2.5-3B model as our base model but now perform distillation with short CoTs which dont have explicit reasoning behaviors. To collect the short CoTs, we use the non-reasoning Qwen2.5-32B as the teacher In this setting, the model quickly overmodel. fits to the K&K task, exhibiting significantly reduced generalization abilities. For instance, accuracy on MATH drops to just 11%. In contrast, our \"warmed-up\" model improves to 54% as shown in Table 1. This shows that the model trained on the K&K games with reasoning supervision successfully abstracts transferable reasoning patterns, leading to substantial performance gains across diverse tasks."
        },
        {
            "title": "3 Target-Domain Adaptation",
            "content": "While simple reasoning behaviors may generalize across domains, as demonstrated in the previous section, many downstream tasks require domainspecific reasoning, e.g., decomposing complex numerical problems in math or generating unit tests in coding (Team, 2025). Such skills need to be polished and are unlikely to emerge from training on toy domain such as Knights & Knaves alone. Therefore, we introduce target adaptation phase, where the warmed-up model is RLVR-trained using few samples ( 100) from the resource-constrained target domain. We first ask can an initial warmup phase with domain-agnostic reasoning traces give significant boost to the performance of RLVR training? If the answer is yes, then in the spirit of meta-learning, we can simply warm up the base model once and then adapt it to many different low-resource domains with RLVR."
        },
        {
            "title": "3.1 Adaptation for math and coding",
            "content": "We first conduct experiments across the MATH and HumanEval+ datasets discussed in Section 2.2. To create the resource-constrained dataset, we randomly sample 100 questions from the MATH train set and 50 samples (of 164 available) from the HumanEval+. We use the remainder of the dataset for evaluation in case of HumanEval+, and we use the MATH500 dataset for evaluation (Lightman et al., 2023) for MATH. We perform two RLVR runs per dataset, one on the base model and one on our warmed-up (that is, K&K distilled) model. For our experiments, we employ the unbiased GRPO algorithm (Dr. GRPO)(Liu et al., 2025). Due to the high cost of RL training, we perform experiments only on the Qwen2.5-3B base model. MATH Using the training set of only 100 examples, we perform RLVR fine-tuning on both the base model and the warmed-up model, keeping all hyperparameters constant. As shown in first column of Figure 2, the base model after RLVR training achieves an absolute improvement of +14.0% over the base model after 250 training steps. In comparison, the warmed-up model achieves significantly larger absolute gain of +20.7% after just 100 steps. Since we have large training set (7,500 examples) for MATH, in order to assess performance at scale, we also conduct RLVR training on the base model with the full MATH training set until the models performance plateaus. Interestingly, the final performance of the base model trained on the full dataset is comparable to that of the warmed-up model trained on just 100 examples: the former reaches score of 63.2% on MATH500, while the latter achieves 64.5%. While large-scale training with well-curated datasets remains valuable and may ultimately lead to higher ceilings, these results highlight that warmed-up model can achieve strong performance with fewer examplesdemonstrating greater performance and sample efficiency in low-resource settings. Detailed results are shown in Table 8 and 9 in the Appendix. HumanEval+ For this experiment, we use the RLVR training set of 50 examples. As shown in Figure 2, the base model achieves an absolute improvement of +24.3% after 100 RLVR training steps. In comparison, the warmed-up model alone has good performance, showing an absolute gain of +15.3% without any domain-specific finetuning, and reaching an absolute improvement of +29.3% after just 50 RLVR training steps. These results highlight the effectiveness of our two-stage approach in the coding domain as well. Detailed results are shown in Table 10 and 11 in the Appendix. Figure 2: Absolute percentage increase relative to the base model for MATH, HumanEval+, and MMLU-Pro subsets (Physics & History) for various models: RLVR on base model, warmup only, and RLVR on warmed-up model 3.2 Adaptation for MMLU-Pro the Appendix. MATH and HumanEval+ primarily consist of problems that demand intensive reasoning (Hendrycks et al., 2021; Liu et al., 2023). To assess broader applicability, we expand our study to the MMLU-Pro benchmark. MMLU-Pro presents broader mix, combining both challenging reasoning tasks and factual recall questions. For instance, MMLU-Pro includes questions from the physics domain, which require mix of analytical problem-solving skills and familiarity with certain domain-specific concepts. It also includes the history domain, which has questions that rely more heavily on factual world knowledge. We perform RLVR runs on these two specific subsets (physics and history) to assess how the model performs in diverse reasoning and knowledge demands. Physics From the 1,299 available examples in the physics subset, we randomly sample 100 instances for training and use the remaining for evaluation. As shown in Figure 2, RLVR training improves the base models absolute performance by +10.0%, reaching peak performance after 200 steps. In comparison, the warmed-up model already exhibits strong performance out of the box with an absolute +9.8% gain, and further improves to +15.9% after just 150 training steps, demonstrating both faster convergence and higher final performance. We find in this case where we have mix of both reasoning-intensive and knowledge-intensive questions, RLVR training still leads to significant gains. Detailed results are shown in Table 12 and 13 in History The history subset of MMLU-Pro contains 381 examples, from which we select 100 at random for training and reserve the rest for evaluation. For this domain, RLVR training applied directly to the base model achieves notable absolute improvement of +14.8%, outperforming the warmed-up model, which reaches lower gain of +10.8% after RLVR training. Past work has shown that for knowledge-intensive tasks, even direct prompting without step by step reasoning outperforms Chain of Thought prompting (Wei et al., 2022; Wang et al., 2024). Thus, for knowledge-intensive tasks, having short CoT might be the optimal policy. As shown in Table 16, the base model after RLVR training learns to produce very short responses on history In questions, hence giving better performance. comparison, since the warmed-up model has been primed for reasoning through the warmup phase, it struggles to perform on par with the base model on non-reasoning task like history. However, as we discuss in Section 3.3, this domain-specific improvement with RLVR directly applied to the base model may come at the cost of broader generalization. Detailed results for the history runs are shown in Table 14 and 15 in the Appendix. While previous work has found that SFT using short CoT can limit the benefits of subsequent RL (Zeng et al., 2025), in this section, we showed that RL training model warmed up with long 6 Figure 3: Generalization results for base+RLVR, warmup-only, and warmup+RLVR models. Top labels indicate RLVR training domain; bottom labels indicate evaluation domain. Warmup uses no domain-specific data. CoT (reasoning traces), although from simplified domain (K&K), can significantly improve generalization and sample efficiency. This is especially beneficial for training in resource-constrained settings with limited samples. Details on all hyperparameters related to our experiments, training curves, and results along with standard deviations are available in Appendix A.6."
        },
        {
            "title": "3.3 Cross-Domain Generalization",
            "content": "While recent research has shown that large scale RLVR-trained models can generalize to out-ofdomain tasks (Zeng et al., 2025; Hu et al., 2025; Xie et al., 2024), we study generalization across multiple distinctly different domains when the sample size is low. We test for generalization across domains with fundamentally different structures, such as math, code generation, and language understanding as discussed in Section 2.2. In this section, we compare generalizability across the two RLVR runs discussed in Section 3; one with the base model and the other with the warmed-up model. For model RLVR-trained on one domain (such as physics), we test generalization by evaluating on other domains (such as mathematics and coding). We ask the question, how does the generalization performance of the two runs compare? Does warming up the base model before RLVR training help to maintain or even improve generalization to other domains? Loss in Generalizability on Base Model As shown in Figure 3, we observe that although RLVR with domain-specific questions typically increases the performance of the model when asked to solve problems in the same domain, it tends to lose its initial base-model reasoning capability when asked to solve problems in different domains. This effect is especially pronounced in case of training on reasoning-intensive domains like MATH and HumanEval+. For instance, RLVR training the base model on HumanEval+ leads to 13.8% drop on MATH, while RLVR training on the History subset results in 8.5% drop on MMLUPro. This degradation suggests that RLVR training may push the model to internalize narrow, domainspecific heuristics, thereby weakening its broader reasoning abilities. This interpretation is further supported by our observation (Appendix A.6) that RLVR fine-tuning often leads to more concise completionsconsistent with prior findings (Fatemi et al., 2025)potentially reflecting contraction in reasoning depth. Improved Generalizability with Warmup Figure 3 also shows the generalization results when we begin the pipeline with the warmed-up model. While the forgetting effect is occasionally observed in the warmed-up model (e.g., 1.4% decrease on HumanEval+ relative to the base model after RL training on MATH), beginning with the warmed-up model rather than with the base model consistently yields stronger cross-domain generalization. Figure 4 shows the change in average completion length relative to the base model after RLVR training on different datasets and evaluating on all others. Although both models have short completion lengths after training(A.6), the warmed-up model has strictly higher completion length relative to the base models original completions. The consistently longer completions may help preserve the models general reasoning abilities across multiple 7 and Pratt, 1998; Naik and Mammone, 1992; Finn et al., 2017). While some studies have shown impressive improvements in math domains with RLVR training on tiny curated datasets (Wang et al., 2025; Fatemi et al., 2025), our approach differs in both scope and methodology. We prioritize sample efficiency and cross-domain generalization across diverse domains, with fundamentally different requirements (math, coding, and language understanding), without relying on selective filtering during either the warmup or downstream adaptation phases. 4.2 SFT Warmup for Reasoning Previous work has shown that reasoning behaviors such as self-verification, backtracking, and self-reflection play critical role in effective reasoning for large language models (LLMs) (Gandhi et al., 2025; OpenAI, 2024; Guo et al., 2025). Although base model might exhibit some reasoning behaviors already, performing distillation with long CoTs from tasks requiring extensive reasoning, like Olympiad level math problems or complex coding tasks, can enable these models to become effective reasoners (Muennighoff et al., 2025; Zhao et al., 2025; Liu et al., 2025; Team, 2025; Labs, 2025). However, rather than relying on meticulously curated domain-specific data, we explore the effectiveness of distilling reasoning patterns in the absence of such meticulously curated, domainspecific Long CoTs. We find that warmup with simple domain-agnostic reasoning may also serve as strong activation mechanism that helps the model to learn to piece together reasoning behaviors for application in multiple downstream domains."
        },
        {
            "title": "5 Conclusion",
            "content": "We present two-stage training framework that combines lightweight warmup phase with reinforcement learning using verifiable rewards (RLVR) to develop reasoning-capable LLMs under limited supervision. By distilling general reasoning from toy domain and fine-tuning on small set of target examples, our approach consistently outperforms direct RLVR on base models across math, code, and language understanding tasks. Warmup enhances sample efficiency, boosts performance, and preserves cross-domain generalizationhighlighting its promise as practical strategy for training robust LLMs in low-resource settings. Figure 4: Relative change in completion length (vs. base model) after training on each dataset (y-axis) and evaluating on others (x-axis). Top: base model; Bottom: warmed-up model. domains. Interestingly, we find that training on domain like Physics, which blends both multi-step reasoning and factual recall, better maintains crossdomain performance for both the base and warmedup models. This observation suggests that task diversity during RLVR training may play critical role in preserving generalization."
        },
        {
            "title": "4.1 Meta-Learning & Generalization",
            "content": "Previous studies have adopted two main techniques for developing reasoning LLMs: distillation with well-curated long CoTs, and large scale RLVR training. Both techniques demand large amounts of high-quality data (Guo et al., 2025; Hu et al., 2025; Muennighoff et al., 2025). These methods typically focus on task-specific optimization and do not directly address how to equip models with the ability to adapt rapidly to new domains under low-resource scenarios. Our work, inspired by the meta-learning paradigm, aims to fill this gap (Thrun"
        },
        {
            "title": "Limitations",
            "content": "Our findings demonstrate that reasoning distilled from simple, abstract domain like Knights & Knaves (K&K) can generalize effectively to tasks in math, code generation, and general language understandingunderscoring the promise of domainagnostic reasoning as transferable prior. However, it remains an open question how well such reasoning transfers to more complex tasks involving richer dynamics, interaction, or domain-specific constraints, like multi-agent environments. While we selected K&K for its simplicity and interpretability, we do not claim that it is the only source of transferable reasoning. Other synthetic environments may offer equally or more effective reasoning priors tailored to downstream needs. We encourage future work to explore the design of such environments as playgrounds for models to learn generalizable reasoning behaviors and apply them to specific domains. Additionally, limited by our compute, we were able to perform distillation only till 14B models and RLVR train on 3B model; evaluating this methodology on larger models could offer further insights into the scalability and generalization potential of warmup-based reasoning."
        },
        {
            "title": "References",
            "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, and Concise reasonarXiv preprint Kartik Talamadupula. 2025. ing via reinforcement learning. arXiv:2504.05185. Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 11261135. PMLR. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. 2025. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307. Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, and Furu Wei. 2024. Meta reasoning for large language models. arXiv preprint arXiv:2406.11698. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. 2025. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. 2025. Openreasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. 2024. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489. Bespoke Labs. 2025. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation. Accessed: 2025-01-22. Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, and 1 others. 2025a. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374. Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. 2025b. Small models struggle to learn from strong reasoners. arXiv preprint arXiv:2502.12143. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. 2024. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 3. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. Devang Naik and Richard Mammone. 1992. Metaneural networks that learn by learning. In [Proceedings 1992] IJCNN International Joint Conference on Neural Networks, volume 1, pages 437442. IEEE. OpenAI. 2024. Openai o1 hub. Accessed: 2025-05-11. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and 1 others. 2024. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. Qwen Team. 2025. QwQ-32B: Embracing the Power of Reinforcement Learning. Accessed: 2025-05-11. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 16. IEEE. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasonarXiv preprint ing in open language models. arXiv:2402.03300. Raymond Smullyan. 1986. What is the name of this book? Touchstone Books Guildford, UK. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. NovaSky Team. 2025. Sky-t1: Train your own o1 preview model within $450. https://novasky-ai. github.io/posts/sky-t1. Accessed: 2025-01-09. models with one training example. arXiv preprint arXiv:2504.20571. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, and 1 others. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. 2025. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449. Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, and 1 others. 2025. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though. arXiv preprint arXiv:2501.04682. Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, and Ravi Kumar. 2024. On memorization of large language models in logical reasoning. arXiv preprint arXiv:2410.23123. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768. Haoran Xu, Baolin Peng, Hany Awadalla, Dongdong Chen, Yen-Chun Chen, Mei Gao, Young Jin Kim, Yunsheng Li, Liliang Ren, Yelong Shen, and 1 others. 2025a. Phi-4-mini-reasoning: Exploring the limits of small reasoning language models in math. arXiv preprint arXiv:2504.21233. Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, and 1 others. 2025b. Redstar: Does scaling long-cot data unlock betarXiv preprint ter slow-reasoning systems? arXiv:2501.11284. Sebastian Thrun and Lorien Pratt. 1998. Learning to In Learning to learn: Introduction and overview. learn, pages 317. Springer. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, and 1 others. 2025. Reinforcement learning for reasoning in large language An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, and 1 others. 2024b. 10 Qwen2.5-math technical report: Toward mathematarXiv ical expert model via self-improvement. preprint arXiv:2409.12122. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. 2025. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892. Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. 2025. Echo chamber: Rl post-training amplifies behaviors learned in pretraining. arXiv preprint arXiv:2504.07912."
        },
        {
            "title": "A Appendix",
            "content": "A.1 The Knights & Knaves (K&K) Game We use Knights & Knaves questions from previous work (Xie et al., 2024). The questions deal with 3 to 7 characters. Here, the number of characters determines the complexity of the problem. The dataset has total of 5, 000 examples. We retain 90% of the dataset (4, 500) for training and keep the rest for evaluation (500). After distillation on the Qwen 3B model, performance on K&K improves from 19.4% 1.6 to 81% 1.5 on the test set. In Table 2, we show an example of the K&K game. In Table 3, we show an example of reasoning trace, generated by the reasoning model QwQ-32B, on one of such puzzles. 11 Table 2: An example Knights & Knaves logic puzzle. Knights & Knaves Puzzle Question very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. You meet four inhabitants: Luke, Liam, Matthew, and Ella. Luke says: Ella is knave. Liam says: Liam is knight if and only if Luke is knave. Matthew says: Liam is knave if and only if Ella is knight. Ella says: Matthew is not knight. Who is knight and who is knave? Answer Luke is knave; Liam is knight; Matthew is knave; Ella is knight Table 3: Example of reasoning trace for the Knights & Knaves logic puzzle generated with QwQ-32B teacher model. As seen here, the response is quite verbose with complex reasoning behaviors like self-reflection, hypothesis testing, and so on. Prompt conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively. i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: Please solve this logical reasoning problem. After thinking, when you finally reach conclusion, clearly state the identity of each character within <answer> </answer> tags i.e., <answer> (1) Zoey is knight (2) ... </answer>. Here is my question, very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. You meet 3 inhabitants: Riley, Emily, and Lucas. In Rileys words: \"Lucas is not knave\". Emily said that Lucas is knight and Riley is knight. In statement by Lucas: \"Emily is knight or Emily is knave\". So who is knight and who is knave? Assistant: <think> Reasoning from QwQ-32B Okay, so need to figure out who among Riley, Emily, and Lucas are knights or knaves. Let me start by recalling that knights always tell the truth and knaves always lie. First, lets note down each persons statements: - Riley says: \"Lucas is not knave.\" - Emily says: \"Lucas is knight and Riley is knight.\" - Lucas says: \"Emily is knight or Emily is knave.\" Hmm, Lucass statement is bit tricky. Lets break it down. The statement \"Emily is knight or Emily is knave\" is actually tautology because every inhabitant is either knight or knave. So regardless of whether Emily is knight or knave, that statement is always true. Wait, but if Lucas is knave, he must lie. But his statement is true statement, so if hes knave, he cant tell the truth. That means Lucas cannot be knave, because his statement is always true. Therefore, Lucas must be knight? Because if he were knave, he would have to lie, but his statement is inherently true, so he cant lie about it. So Lucas must be knight. Wait, let me think again. Lets analyze Lucass statement more carefully. The statement \"Emily is knight or Emily is knave\" is logical OR. Since every inhabitant is either knight or knave, this OR is always true. So the statement is tautology. Therefore, regardless of Emilys actual status, the statement is true. So if Lucas is knight, he tells the truth, so thats fine. If he were knave, he would have to lie. But since his statement is true, knave cannot say it. Therefore, Lucas must be knight. Thats solid. 12 So Lucas is knight. Therefore, Rileys statement that \"Lucas is not knave\" is true. Since Rileys statement is true, Riley must be knight as well. Wait, but lets check Emilys statement. Emily says that Lucas is knight and Riley is knight. Since we just concluded both are knights, then Emilys statement is true. Therefore Emily must also be knight. But that cant be right because then all three would be knights, but let me check again. Wait, hold on. Let me go step by step. First conclusion: Lucas is knight (from his own statement being tautology). Therefore, Rileys statement \"Lucas is not knave\" is true, so Riley is knight. Then Emilys statement is \"Lucas is knight and Riley is knight.\" Since both parts are true (Lucas and Riley are knights), Emilys statement is true, so Emily must be knight. That would mean all three are knights. But is there any contradiction here? Wait, but let me see. If all three are knights, then all their statements are true, which they are. So that seems okay. But maybe missed something? Let me try another approach. Alternative approach: Suppose Lucas is knave. Then his statement \"Emily is knight or knave\" would be lie. But since the statement is tautology (always true), lying about it would require the statement to be false. But since its always true, Lucas cant be knave. Therefore, Lucas must be knight. So that part holds. Therefore, Lucas is knight. Then Rileys statement is true, so Riley is knight. Then Emilys statement is \"Lucas is knight and Riley is knight.\" Since both are true, Emilys statement is true, so Emily is knight. So all three are knights. That seems possible, but maybe the problem expects some of them to be knaves. Let me check again for errors. Wait, perhaps made an error in analyzing Emilys statement. Let me parse Emilys statement again. She says \"Lucas is knight and Riley is knight.\" If both are true, then Emily is knight. If either is false, then her statement is false, so she would be knave. But according to our previous conclusion, both are true, so she must be knight. So thats consistent. Therefore all three are knights. But the problem says \"inhabited only by knights and knaves,\" so having all knights is allowed. Hmm, but maybe missed something else? Alternatively, perhaps the problem has trick. Let me consider if theres another possibility. Suppose Lucas is knight (as before). Then Riley is knight. Emilys statement is true, so shes knight. So all three are knights. That works. Alternatively, is there scenario where someone is knave? Suppose, for contradiction, that Emily is knave. Then her statement \"Lucas is knight and Riley is knight\" would be lie. The negation of \"A and B\" is \"not or not B\". So either Lucas is knave, or Riley is knave, or both. But we already established that Lucas must be knight (since his own statement is tautology). Therefore, if Emily is knave, then Riley must be knave. But Rileys statement \"Lucas is not knave\" would then be lie, meaning Lucas is knave. But that contradicts our earlier conclusion that Lucas must be knight. Therefore, Emily cant be knave. So Emily must be knight. Therefore, all three are knights. Wait, but that seems bit too straightforward. Let me check again. Alternatively, perhaps the problem is designed so that one is knave. Let me see: Wait, Lucass statement: \"Emily is knight or knave\" since every inhabitant is one or the other, thats always true, so Lucas must be knight. So that part is solid. Then Rileys statement is true, so Riley is knight. Emilys statement is true, so shes knight. So all three are knights. So the answer would be all three are knights. Hmm, but maybe the problem is expecting different answer? Let me see the original problem again: Problem states: Riley says Lucas is not knave. Emily says \"Lucas is knight and Riley is knight.\" Lucas says \"Emily is knight or Emily is knave.\" Wait, the problem might have trick in Emilys statement. Let me parse Emilys exact words again: \"Lucas is knight and Riley is knight.\" So if Emily is knave, then her statement must be false, so either Lucas is not knight (so knave) or Riley is not knight (so knave). But since we know Lucas is knight, that would require Riley to be knave. But Rileys statement is \"Lucas is not knave,\" which would be true (since Lucas is knight), so Riley would be knight, contradicting the assumption that Riley is knave. Therefore Emily cant be knave. Therefore, all three must be knights. 13 Alternatively, is there any other possibility? Let me try assuming that Emily is knave again. If Emily is knave, then her statement is false. Since the statement is \"Lucas is knight AND Riley is knight,\" the negation is \"Lucas is knave OR Riley is knave.\" But since Lucas must be knight (from his own statement), then the only way for Emilys statement to be false is if Riley is knave. But then Rileys statement \"Lucas is not knave\" would be true, so Riley would be knight, which contradicts the assumption that Riley is knave. Therefore Emily cant be knave. Therefore the only possible solution is that all three are knights. Wait, but the problem says \"a very special island inhabited only by knights and knaves.\" Theres no rule that there must be at least one of each, so all three being knights is acceptable. Hmm, so the conclusion is that all three are knights. But maybe made mistake in the analysis. Let me try another angle. Suppose Lucas is knight (as established). Therefore, his statement is true, so the rest follows. Then Rileys statement is true, so Riley is knight. Emilys statement is true, so Emily is knight. So yes, thats consistent. Alternatively, lets suppose that Lucas is knave. Then his statement \"Emily is knight or knave\" is lie. But since the statement is tautology (always true), lying about it is impossible. So Lucas cannot be knave. Therefore, Lucas must be knight. Thus, the only possible conclusion is all three are knights. But maybe the problem is designed so that someone is knave? Let me think again. Wait, perhaps theres mistake in considering that \"Emily is knight or knave\" is tautology. Wait, actually, in logic, \"A or not A\" is tautology, but here Emily is either knight or knave, so \"Emily is knight or Emily is knave\" is indeed tautology. Therefore, Lucass statement is always true, so he must be knight. Therefore, think the correct answer is that all three are knights. So Riley: knight, Emily: knight, Lucas: knight. Wait, but let me check once more. Let me tabulate possible scenarios: Case 1: All knights. Then all statements are true. That works. Case 2: Suppose Lucas is knight, Riley knight, Emily knave. Then Emilys statement would be false, but since both parts are true (Lucas and Riley are knights), her statement is true, so she cant be knave. So this case invalid. Case 3: Suppose Emily is knave, then her statement is false, so either Lucas is knave or Riley is knave. But Lucas must be knight, so Riley must be knave. But then Rileys statement \"Lucas is not knave\" is true, making Riley knight, contradiction. So Emily cant be knave. Case 4: Suppose Riley is knave. Then his statement \"Lucas is not knave\" is false, so Lucas is knave. But Lucass statement would then be lie, but his statement is tautology (true), so Lucas cant be knave. Contradiction. So Riley must be knight. Thus, all three must be knights. Therefore the answer is all three are knights. </think> <answer> (1) Riley is knight (2) Emily is knight (3) Lucas is knight </answer> A.2 Codebase We use the TRL package3 with the DeepSpeed ZeRO Stage 3 (Rajbhandari et al., 2020) configuration for Supervised Fine-tuning and Reinforcement Learning with GRPO (Shao et al., 2024). The complete codebase for this work is available at https://anonymous.4open.science/r/ warmup-before-you-train-0EEF/. A.3 Evaluation We keep the temperature 0.7 and top-p 0.95 for both training and evaluation. Given the sensitivity during evaluation, we sample 4 times for each question and report the average accuracy along with the standard deviation. We use the math_verify package for math evaluation. For HumanEval+, we base our evaluation on OpenAIs codebase4. Finally, for MMLU-Pro, we adapt our evaluation from MMLU-Pros official repository5. We use the vllm package for running inference for evaluation on H100 GPUs.6 All evaluation code is made available in the shared repository. A.4 Distillation Experiment Details For all distillation experiments, we used the hyperparameters as shown in Table 4. Table 4: Key hyperparameters used in supervised finetuning"
        },
        {
            "title": "Hyperparameter",
            "content": "Optimizer Weight Decay Warmup Steps Max Sequence Length Batch Size (per device) Gradient Accumulation Steps Number of Epochs Learning Rate Scheduler bf16 GPUs"
        },
        {
            "title": "Value",
            "content": "AdamW 1e 4 25 32,768 1 2 3 constant True 6 H100s For learning rate, we choose 1e 6 for distillation with K&K, and choose 2e 5 for distillation with s1K data as they showed the best results in empirical observations. 3https://huggingface.co/docs/trl/en/index 4https://github.com/openai/human-eval 5https://github.com/TIGER-AI-Lab/MMLU-Pro 6https://docs.vllm.ai/en/latest/ 15 A.4.1 Overfitting Figure 5: Results of Qwen2.5-3B K&K distillation. Loss curve shown on top & performance on MATH500 shown on bottom. Choosing higher learning rate, 2e 5 has chance of overfitting to the K&K domain rather than learning generalizable reasoning behaviors. We found that choosing low learning rate was the best choice for distillation with K&K data. As shown in Figure 5, lower learning rate leads to sharp improvement in the performance of the base model in the MATH500 dataset. Since the data is specific to the K&K domain and might have lower token diversity, it might be possible that higher learning rate causes the model to overfit quickly to the K&K task. In comparison, running distillation with s1K data with the learning rate of 2e 5, keeping the rest of the hyperparameters constant  (Table 4)  , did not lead to drastic overfitting. To explore another alternative for hyperparameters, we followed the hyperparameter setting from the s1 paper (Muennighoff et al., 2025), choosing learning rate of 1e 5 followed by cosine decay schedule in contrast to our constant schedule, and training for 5 epochs instead of 3  (Table 4)  . Following these hyperparameters, we distilled on the Qwen2.5-14B model with s1K data. We found that the model still underperformed our K&K distillation. This new setting led to performance of 48.3 1.6 on MATH500, 31.4 6.0 on HumanEval+, and 44.2 0.5 on MMLU-Pro. With both old and new hyperparameter settings, distillation with s1K data on the 14B model still leads to significant performance degradation relative to the base model. We stick to our hyperparameter settings as we found that to give the best output overall for both distillations, with K&K and s1K, empirically. A.5 Distillation with R1 reasoning traces To ensure that the observed performance gains were not merely due to potential synergies between the teacher and student models from the Qwen family, we evaluated the effect of using reasoning traces from teacher model of different origin. In particular, we considered the possibility that the specific language patterns or token distributions in QwQs responses might be especially well-suited to eliciting reasoning behavior in Qwen models. To test this, we employed DeepSeek-R1-DistillQwen-32Ba model distilled on reasoning traces generated by DeepSeek R1 (Guo et al., 2025)to produce K&K traces for warmup, following the procedure described in Section 2. These traces were then distilled into the Qwen2.5-3B model. This setup still resulted in consistent improvements across all benchmarks: accuracy on MATH rose to 51.2 1.6, HumanEval+ to 53.5 4.3, and MMLU-Pro to 34.6 0.5. These results suggest that the warmup benefits are not limited to teacherstudent alignment within the same model family. A.6 RLVR Experiment Details We use the R1 prompt  (Table 5)  across our training and evaluations (Guo et al., 2025). We modify the prompt slightly based on the task for better instruction following. For RLVR, we use the hyperparameters shared in Table 6. Training curves of reward and completion length for MATH (Figure 6), HumanEval+ (Figure 7), Physics (Figure 8), and History (Figure 9) are shown below. Accuracy and completion lengths (in tokens) for MATH (Table 8 and Table 9), HumanEval+ (Table 10 and Table 11), Physics (Table 12 and Table 13), Table 5: Prompt used across training and evaluations R1 prompt conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> final answer inside boxed{} tag </answer>. User: You must put your answer inside <answer> </answer> tags, i.e., <answer> answer here </answer>. And your final answer will be extracted automatically by the boxed{} tag. {prompt} Assistant: <think> </answer> Table 6: RLVR training hyperparameters"
        },
        {
            "title": "Hyperparameter",
            "content": "optimizer learning_rate beta adam_beta1 adam_beta2 weight_decay warmup_steps Learning Rate Scheduler bf16 bf16_full_eval Batch Size (per device) Gradient Accumulation Steps Gradient Checkpoint num_generations max_prompt_length max_completion_length max_grad_norm use_vllm vllm_max_model_len temperature top-p GPUs"
        },
        {
            "title": "Value",
            "content": "AdamW 1e-6 0 0.9 0.99 0.1 25 constant True True 4 5 True 10 256 8192 0.1 True 8192 0.7 0.95 6 H100s and History (Table 14 and Table 15) are shown below. 16 Figure 6: Training curves for Reward (left) and Completion Length (right) for MATH training, smoothed with moving average (window size 10) Figure 7: Training curves for Reward (left) and Completion Length (right) for HumanEval+ training, smoothed with moving average (window size 10) cost of $2.99 per hour per GPU. We use the same setup of 6 H100 GPUs for RLVR training. For specific subject, we train both the base and warmed-up model for the same amount of training steps. Duration of training is shown in Table 7. A.8 History Samples After RLVR training the base model on history questions, it learns direct answering as it seems to be the most efficient strategy (Wei et al., 2022; Wang et al., 2024). However, since the warmed-up model is trained on reasoning, it struggles to learn the most efficient strategy as it is biased towards generating more tokens. sample of both of these cases is shown in Table 16. Table 7: RLVR Training Times (hh:mm) for Base vs. Warmed-up Models Across Tasks Task Base Model Warmed-up Model MATH History Physics HumanEval+ 2:00 3:30 6:30 3:30 4:00 3:30 11:00 4:00 A.7 Compute Costs The computational requirements varies across models. The smallest model 1.5B takes around 20 minutes for K&K distillation with the largest 14B model taking around 1 hour 10 minutes. Since the s1K data only has 1K examples, it takes around 5 minutes for the 1.5B model and around 25 minutes for the 14B model. We use 6 NVIDIA H100 SXM GPUs (80GB VRAM) from runpod7 coming at 7https://www.runpod.io/pricing 17 Figure 8: Training curves for Reward (left) and Completion Length (right) for Physics training, smoothed with moving average (window size 10) Figure 9: Training curves for Reward (left) and Completion Length (right) for History training, smoothed with moving average (window size 10) Table 8: Performance comparison of training with MATH on Base vs Warmup Model"
        },
        {
            "title": "Model",
            "content": "MATH (%) HumanEval+ (%) MMLU-Pro (%) Base Model + RL Warmup + RL 43.8 0.8 57.8 1.9 54.0 1.4 64.5 0.3 32.5 1.2 25.7 4. 47.8 3.7 31.1 4.8 29.2 0.3 28.8 0.2 38.2 0.4 39.2 0.3 Table 9: Completion Length after training with MATH on Base vs Warmup Model"
        },
        {
            "title": "MATH",
            "content": "HumanEval+ MMLU-Pro Base Model + RL 696.9 84.8 503.9 19.8 312.6 67.1 307.5 38.5 309.6 13.6 272.2 5.8 Warmup + RL 1623.5 19.7 1205.8 33.4 1205.9 94.0 1235.3 31.6 2203.7 24.5 1629.6 7.1 18 Table 10: Performance comparison of training with HumanEval+ on Base vs Warmup Model Model HumanEval+ (%) MATH (%) MMLU-Pro (%) Base Model + RL Warmup + RL 32.5 1.2 56.8 3.8 47.8 3.7 61.8 3.9 43.8 0.8 30.0 0. 54.0 1.4 52.6 1.5 29.2 0.3 27.4 0.9 38.2 0.4 36.2 0.3 Table 11: Completion Length after training with HumanEval+ on Base vs Warmup Model Model HumanEval+ MATH MMLU-Pro Base Model + RL 312.6 67.1 172.4 2.0 696.9 84.8 328.0 21.3 309.6 13.6 81.3 0. Warmup + RL 1205.9 94.0 351.0 11.6 1623.5 19.7 1269.2 29.6 2203.7 24.5 1336.4 12.8 Table 12: Performance comparison of training with Physics on Base vs Warmup Model"
        },
        {
            "title": "Model",
            "content": "Physics (%) MMLU-Pro (%) MATH (%) HumanEval+ (%) Base Model + RL Warmup + RL 24.4 2.6 33.6 1.1 34.4 1.2 41.6 1.6 29.2 0.3 36.0 0. 38.2 0.4 40.8 0.3 43.8 0.8 46.8 0.9 54.0 1.4 54.3 2.1 32.5 1.2 38.4 3.0 47.8 3.7 48.5 6.1 Table 13: Completion Length after training with Physics on Base vs Warmup Model"
        },
        {
            "title": "Physics",
            "content": "MMLU-Pro"
        },
        {
            "title": "MATH",
            "content": "HumanEval+ Base Model + RL 448.1 37.8 459.9 30.5 309.6 13.6 274.5 7.7 696.9 84.8 601.1 46.4 312.6 67.1 257.2 42. Warmup + RL 3607.8 30.5 1821.3 30.5 2203.7 24.5 1348.1 7.6 1623.5 19.7 1365.5 45.3 1205.9 94.0 1371.2 38.3 Table 14: Performance comparison of training with History on Base vs Warmup Model"
        },
        {
            "title": "Model",
            "content": "History (%) MMLU-Pro (%) MATH (%) HumanEval+ (%) Base Model + RL Warmup + RL 24.9 4.8 39.7 1.9 30.0 3.3 35.7 1.6 29.2 0.3 35.3 0. 38.2 0.4 40.5 0.5 43.8 0.8 35.3 1.0 54.0 1.4 50.5 1.8 32.5 1.2 37.1 3.2 47.8 3.7 49.8 3.9 Table 15: Completion Length after training with History on Base vs Warmup Model"
        },
        {
            "title": "History",
            "content": "MMLU-Pro"
        },
        {
            "title": "MATH",
            "content": "HumanEval+ Base Model + RL 207.2 44.1 360.6 59.3 309.6 13.6 277.9 10.5 696.9 84.8 498.0 55.8 312.6 67.1 199.5 39. Warmup + RL 1083.1 129.0 818.2 5.7 2203.7 24.5 1112.1 7.9 1623.5 19.7 1117.1 48.3 1205.9 94.0 862.8 42.6 Table 16: Base model and warmed-up model responses on History question after RLVR training on History. While the base model gives very short response without using any Chain of Thought behavior at all, the warmed-up model outputs long, complex reasoning chain that overcomplicates things. This shows that direct answering without any Chain of Thought seems to be the more favorable strategy for History questions. Question The ancient city of Cahokia contained more than and population of around . Options: A. 40 residential complexes; 40,000 B. 50 religious monuments; 5,000 C. 120 earthen mounds; 10,000 D. 70 public squares; 7,000 E. 80 ceremonial mounds; 50,000 F. 200 pyramidal mounds; 200,000 G. 60 burial mounds; 2,000 H. 30 agricultural terraces; 30,000 I. 150 defensive walls; 15,000 J. 100 trading posts; 1,000 Correct Answer (C) Sample Response after RLVR Training with History questions on base model The ancient city of Cahokia was home to more than 120 earthen mounds. The population of Cahokia was around 10,000. </think><answer> </answer> Correct. Sample Response after RLVR Training with History questions on warmed-up model The question is about the ancient city of Cahokia, which was significant Mississippian culture settlement in what is now the United States. The goal is to determine the correct answer from the given options regarding the number of residential complexes, religious monuments, earthen mounds, public squares, ceremonial mounds, pyramidal mounds, burial mounds, agricultural terraces, defensive walls, and trading posts. However, the options provided might be mix of different types of features, and need to find the pair that corresponds to the most accurate count for the citys size. First, let me recall that Cahokia was major urban center in the 11th to 13th centuries AD. The city was notable for its large population and impressive architecture. The question asks for the numbers related to the citys residential complexes, religious monuments, earthen mounds, etc., but the options dont list the actual counts of these features. Therefore, need to look up the historical data related to Cahokia. From historical records, the estimated population of Cahokia is around 15,000-20,000 people. However, the questions options are about specific counts (like the number of residential complexes, religious monuments, etc.), not the population. 20 So, perhaps the options are meant to be about the architectural or ceremonial features that are associated with the city. Let me check each option one by one: A. 40 residential complexes; 40,000 - The population of Cahokia is much smaller, around 15,000, so this seems too high. B. 50 religious monuments; 5,000 - This also seems too high for city with population of 15,000. A. 40 residential complexes; 40,000 - The population of Cahokia is much smaller, around 15,000, so this seems too high. B. 50 religious monuments; 5,000 - This also seems too high for city with population of 15,000. C. 120 earthen mounds; 10,000 - Again, the population is around 15,000, so this is too high. D. 70 public squares; 7,000 - This is also too high. E. 80 ceremonial mounds; 50,000 - Again, the population is much smaller. F. 200 pyramidal mounds; 200,000 - This is very high number for city with around 15,000 inhabitants. G. 60 burial mounds; 2,000 - The population is around 15,000, so this could be possible but lets see other options. H. 30 agricultural terraces; 30,000 - The population is around 15,000, so this is also high. I. 150 defensive walls; 15,000 - The population is around 15,000, so this is possible. J. 100 trading posts; 1,000 - The population is around 15,000, so this is possible. Now, looking at the options, the ones that could be closer to the actual counts of architectural features in Cahokia would be and J. However, the options dont list the actual numbers of each feature but rather their counts. Since and are about defensive walls and trading posts, which are both architectural features, but not necessarily the most accurate count, perhaps is more reasonable because defensive walls could be significant part of the citys infrastructure. However, the question might expect combination of residential complexes, religious monuments, etc., which are not provided. Alternatively, perhaps the question is testing knowledge of the approximate number of ceremonial mounds or public squares in Cahokia. Since the options are about different features, and the population is the only real constraint, need to find the option that aligns with the citys size. Cahokia had large number of earthen mounds, which were used for ceremonial purposes. The number of earthen mounds in Cahokia is estimated to be around 120. Lets see if this can be paired with possible residential complex count. The users question says \"\"The ancient city of Cahokia contained more than residential complexes and population of around .\"\" So, the first part is the number of residential complexes, and the second part is the population. Given that the population is around 15,000, the number of residential complexes would likely be significant number but not too high. Let me consider option G: 60 burial mounds. This is lower number but not necessarily impossible, but the question suggests that the number of residential complexes might be higher. Since the options are about different features, perhaps is the best choice here because its about tangible architectural feature that could be part of the citys layout, such as defensive walls. Therefore, the best choice among the given options is I: 150 defensive walls; 15,000. This is because the question asks for combination of \"\"more than residential complexes\"\" and ,\"\" and the defensive walls could be significant defensive structure in \"\"a population of around the city, contributing to the citys layout and thus the \"\"more than\"\" part. The population is around 15,000, which aligns with the actual historical estimates for Cahokia. </think> <answer> boxed{H} </answer> Incorrect."
        }
    ],
    "affiliations": [
        "New York University Abu Dhabi"
    ]
}