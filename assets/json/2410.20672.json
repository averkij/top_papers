{
    "paper_title": "Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA",
    "authors": [
        "Sangmin Bae",
        "Adam Fisch",
        "Hrayr Harutyunyan",
        "Ziwei Ji",
        "Seungyeon Kim",
        "Tal Schuster"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit \"layer tying\" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller \"Recursive Transformers\" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput."
        },
        {
            "title": "Start",
            "content": "2024-10-29 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Sangmin Bae1,*, Adam Fisch2, Hrayr Harutyunyan3, Ziwei Ji3, Seungyeon Kim2 and Tal Schuster2, 1KAIST AI, 2Google DeepMind, 3Google Research, *Work done during an internship at Google DeepMind, Corresponding Author Large language models (LLMs) are expensive to deploy. Parameter sharing offers possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit layer tying as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller Recursive Transformers that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use single block of unique layers that is then repeated multiple times in loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselinesand can even recover most of the performance of the original full-size model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In theoretical analysis, we show that this has the potential to lead to significant (2-3) gains in inference throughput. 1. Introduction Efficient deployment of large language models (LLMs) demands balance between performance and resources (Leviathan et al., 2023; Raposo et al., 2024; Rivi√®re et al., 2024; Wan et al., 2024; Zhou et al., 2024). While larger models with more parameters consistently demonstrate superior performance (Hoffmann et al., 2022; Rae et al., 2021; Rosenfeld et al., 2020), their substantial memory and computational demands are expensive (Pope et al., 2023). Parameter sharing approaches (e.g. Dehghani et al., 2019; Lan et al., 2020; Takase and Kiyono, 2023; Xia et al., 2019), wherein weights are reused across model layers, can lower these costs by reducing memory footprint, and thereby allow for the use of fewer (or lower-grade) accelerators, or larger batch sizes for better throughput. While parameter sharing has shown encouraging capabilities in previous work (Giannou et al., 2023; Lan et al., 2020), its application to modern LLMs has yielded limited reported success. In this work, we revisit parameter sharing for LLMs, and propose novel methodologies to convert existing, unshared models into smaller, and more efficient, Recursive Transformers. These models use single block of unique layers that are recursively reused across multiple loops, yet still achieve impressive performance relative to their reduced size. To mitigate the potential performance degradation associated with parameter sharing, we first initialize the shared block of layers based on the original models pre-trained parameters, and then finetune the resulting recursive model for limited number of uptraining steps. Importantly, we show that our initialization strategies allow us to achieve strong performance with minimal training time. This is aligned with observations that model compression techniques such as layer skipping (Elhoushi et al., 2024; Fan et al., 2020; Zeng et al., 2023; Zhang et al., 2024a), pruning (Frankle and Carbin, 2019; Ramanujan et al., 2020) or nesting (Devvrit et al., 2023) can preserve surprisingly high performancefurther motivating our approach of compressing models to more compact yet performant architectures (here, repeated layers with low-rank adapters). Correspondence to: bsmn0223@kaist.ac.kr, talschuster@google.com 2024 Google DeepMind. All rights reserved 4 2 0 2 8 2 ] . [ 1 2 7 6 0 2 . 0 1 4 2 : r Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Figure 1 Overview of the conversion from vanilla N-layer Transformer to Recursive Transformer with ùëÅ/ùêæ blocks of shared layers. The Recursive Transformer is obtained by repeating single block of layers multiple times, resulting in looped architecture. The Recursive Transformer can also be converted into Relaxed Recursive Transformer by adding layer-specific LoRA modules. This preserves many of the advantages of weight sharing, but also allows for better performance. As depicted in Figure 1, we further propose the Relaxed Recursive Transformer, an extension of the Recursive Transformer in which the weight tying across repeated layer blocks is slightly relaxed through the incorporation of multiple layer-specific, low-rank adaptation (LoRA) modules (Hu et al., 2022). Despite its simplicity, this strategy offers several non-trivial advantages. First, it allows for lowrank deltas between shared layers, while only adding minimal overhead. Second, the rank of the LoRA matrices can be adjusted to control the degree of relaxation, which directly influences model capacity. Furthermore, since the relaxed model has the same overall shape as the original Transformer, we can efficiently initialize LoRA modules via truncated Singular Value Decomposition (Hansen, 1987) on the residual matrices between the original layer weights and the shared layer weights. Hence, the rank values serve as pivotal hyperparameter, enabling the Relaxed Recursive Transformer to seamlessly transition between the two extremes of the vanilla and Recursive Transformer architectures. While the primary focus of this paper lies in how to formulate and train Recursive Transformers, we also highlight their potential to achieve significant throughput gains via new batched inference paradigm, Continuous Depth-wise Batching, that their recursive nature enables. Prior work introduced continuous sequence-wise batching (Kwon et al., 2023; Yu et al., 2022), which leverages the fact that the computation performed to compute new token is functionally the same (and uses the same model parameters) regardless of the token position within the sequence. This allows new requests to be continuously scheduled when slots within batch become available. For example, when one response is completed, the start of the next response to be formed can immediately take the finished responses place in the batch, without waiting for the rest of the batch responses that might be longer. In our Recursive Transformer, parameter sharing occurs not only across different timesteps, but also across different depths (loop iterations). This enables an extra dimension of dynamic grouping: jointly computing different iterations of the looped layer blocks per individual responses within the same batch. Our key contributions are as follows: We introduce framework for initializing and training Relaxed Recursive Transformers and demonstrate strong performance compared to non-recursive models of comparable size. For example, when we uptrained recursive Gemma 1B model converted from pretrained Gemma 2B (Team et al., 2024), we observed up to 13.5 absolute accuracy improvement (22% error reduction) on few-shot tasks compared to non-recursive Gemma 1B model (pretrained from scratch). Furthermore, we show that by incorporating knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016), our recursive Gemma model, uptrained on 60 billion tokens, achieves performance on par with the full-size Gemma model trained on massive 3 trillion token corpus (see 3.3 for details). 2 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Based on our Relaxed Recursive Transformer, we also evaluate key use case for continuous depth-wise batching with early-exiting (Bae et al., 2023; Elbayad et al., 2020; Graves, 2016a; Schuster et al., 2022), which opportunistically makes predictions for samples with high confidence at earlier stages. From our simulation, Early Exits reveal substantial throughput improvement of up to 2-3 compared to vanilla Transformer with the same architecture. Notably, the recursive Gemma model, which outperforms the vanilla Pythia model, can theoretically achieve nearly 4 increase in throughput (see 3.8 for details). 2. Effective Model Compression with Recursive Patterns In this section, we present the main details of our method for converting vanilla Transformer model into parameter-shared model that outperforms models of equivalent size. We first provide short overview of the Transformer architecture (2.1). Then, we introduce the Recursive Transformer and present effective techniques to initialize its looped layers by leveraging the weights of the original pretrained model (2.2). In 2.3, we relax the parameter-sharing constraint in the model design, and add limited set of layer-specific parameters to further improve the models accuracy while maintaining compact representations. Finally, we show how, beyond reduced memory, Recursive Transformers readily support further throughput optimizations via novel inference paradigm (2.4). 2.1. Basic Transformer Architecture Large language models (Dubey et al., 2024; OpenAI, 2023; Reid et al., 2024; Rivi√®re et al., 2024) typically leverage the Transformer architecture (Vaswani et al., 2017). Transformer consists of layers, where the hidden states at each time step are computed by running through the series of layers: ùë° = ùëì (h‚Ñì1 h‚Ñì ùë° ; Œ¶‚Ñì), ‚Ñì [1, ùêø], (1) ùë° representing the embedding of the token ùë¶ùë°1 from the previous time step, and Œ¶‚Ñì denoting with h0 the trainable parameters of the ‚Ñì-th layer. Each layer has two core components: multi-head attention (MHA) mechanism and feedforward network (FFN). MHA employs multiple attention heads to capture diverse relationships within the input sequence via linear attention weights and scaled dot-product attention mechanisms. The FFN structure typically consists of two linear transformations, but different models exhibits distinct structural variations. See Appendix for further details. 2.2. Recursive Transformer: Looped Layer Tying In this work, we revisit parameter sharing in the context of LLMs and propose the Recursive Transformer architecture. Among various looping strategies (refer to Appendix B), we specifically adopt the CYCLE strategy (Takase and Kiyono, 2023) for Recursive Transformers, wherein single block of unique layers is recursively reused. This inherent design aligns seamlessly with early-exiting mechanisms, potentially offering substantial speedup. The models hidden states are computed as: ‚Ñì1 ‚Ñì ùë° = ùëì (h ùë° ; Œ¶ ( (‚Ñì1) mod ùêø/ùêµ)+1), ‚Ñì [1, ùêø], (2) where the parameter-shared model is parameterized by Œ¶, and denotes the number of looping blocks (we restrict to be factor of L). For example, Gemma 2B (Team et al., 2024) with 18 layers can be converted to recursive variant with 2 blocks by storing weights for only the first 9 layers. The forward pass will loop twice through these 9 layers. We tie all trainable parameters, including the weights of the linear layers in the Transformer blocks and the weights of the RMSNorm (Zhang and Sennrich, 2019). 3 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Figure 2 Left: An example of unshared, full-size model with 6 layers. Middle: Three proposed methodologies for initializing looped layers in Recursive Transformer. Each layer number indicates the source layer in the full-size model used for initialization. Right: Example of Relaxed Recursive Transformer initialized by SVD method. Here, looped layers are initialized using the Average method. Initialization techniques for looped layers To mitigate the potential performance drop associated with reduced capacity in parameter-shared models, we propose several novel initialization methodologies to facilitate effective knowledge transfer from unshared, pretrained models to Recursive Transformers. Figure 2 illustrates three such techniques. The Stepwise method selects intermediate layers at specific intervals while keeping the first and last layer fixed. This is motivated by prior work (Fan et al., 2020; Liu et al., 2023; Zeng et al., 2023; Zhang et al., 2024a) showing minimal impact on generation quality when skipping few layers in LLMs. The Average method initializes the shared weights among tied layers by averaging their weight matrices, whereas the Lower method directly uses weights from the first layers of the unshared model. We conducted brief uptraining on 15 billion tokens to investigate the extent of performance recovery in these initialized models (3.4) and found the Stepwise approach to perform best for Recursive Transformers. However, we found the Average method to perform best for Relaxed Recursive Transformers, discussed next. 2.3. Relaxed Recursive Transformer: Multi-LoRA Layers While full layer-tying is effective for compressing the models size while maintaining strong capabilities, it has two noticeable limitations: (1) the set of possible model sizes is limited to scaling the number of layers, and (2) each model layer ends up having to serve multiple roles associated with different depths of the model. To address this, we introduce Relaxed Recursive Transformers in which we incorporate independent adapter modules (Houlsby et al., 2019; Hu et al., 2022) for each layer, relaxing the strict parameter sharing. While we experiment with various approaches like layer-specific prefixes (Liu et al., 2021) (see Appendix H), we find low-rank adaptation (LoRA) modules (Hu et al., 2022) to efficiently capture the subtle variations between tied layers. Specifically, we modify Eq. 2 to: ‚Ñì ùë° = ùëì (h ‚Ñì1 ùë° ; Œ¶ ( (‚Ñì1) mod ùêø/ùêµ)+1 , ŒîŒ¶ ‚Ñì), ‚Ñì [1, ùêø], (3) where ŒîŒ¶ is the (small) set of parameters for the LoRA modules. In this relaxed model, each looped layer is augmented with multiple LoRA modules. For example, recursive model with two loop iterations has single block of shared layers, and two different LoRA modules are attached to each layer within this block. The first and second LoRA modules per layer are used during the first and second loop iterations, respectively. Functionally, these LoRA modules introduce low-rank deltas to all of the shared, linear weight matrices. More concretely, for base transformation = Wx, our modified forward pass yields = Wx + ŒîWx = Wx + BAx, where ‚Ñù(ùëüùëò) and ‚Ñù(ùëëùëü) denote the weight matrices of LoRA with rank r. 4 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA (a) Vanilla Batching (b) Depth-wise Batching (c) With Early-exiting Figure 3 An illustrative example of continuous depth-wise batching strategy together with earlyexiting. We assume maximum batch size of 32, three model stages (e.g., layer blocks), and stream of batched inputs that arrive sequentially in time. In (a), all three model stages must complete for the first (non-maximal) batch of 16 before the second batch of 32 examples that arrives next can be started. In (b), however, half of second batch of 32 examples can share computation with the first batch of 16 that is still finishing. Finally, (c) demonstrates situation where some examples within each batch can early-exit after stage 2; their vacant slots in the batch are then immediately filled. LoRA initialization via truncated SVD Unlike typical LoRA finetuning setups that train only the LoRA parameters, here we train all model parameters to let the shared parameters learn an optimal centroid for all of the layer depths that they support. Therefore, instead of following standard zero initialization for adaptation to the frozen base model, we propose novel initialization methods, especially designed for Relaxed Recursive Transformers. To effectively match the performance of the original full-size model after initializing the tied weights as described in 2.2, we aim for the sum of the tied weights (Œ¶) and LoRA weights (ŒîŒ¶) to approximately recover the full-size models weights (Œ¶). We exploit truncated Singular Value Decomposition (SVD) (Hansen, 1987) on residual matrices between original weights and tied weights: U‚Ñì ùëü , Œ£‚Ñì ùëü , V‚Ñì ùëü = Truncated SVD(W‚Ñì ( (‚Ñì1) mod ùêø/ùêµ)+1; ùëü), ‚Ñì [1, ùêø], (4) where outputs retain the first columns corresponding to the largest singular values. denotes the weight matrices of the full-size model, and denotes those of the Recursive Transformer. We initialize the LoRAs weights with principal components in Eq. 4: as the product of Uùëü and Œ£ùëü, and as the transpose of the right singular vectors Vùëü (see Figure 2). Remark. By initializing LoRA weights through the proposed truncated SVD methodology, the rank of the LoRA modules serves as pivotal hyperparameter, enabling the Relaxed Recursive Transformer to seamlessly transition between the two extremes of the vanilla and Recursive Transformer architectures. With sufficiently large ranks, our Relaxed Recursive Transformer (Eq. 3) approximates the full-size vanilla model (Eq. 1): Wx Wx + (UùëüŒ£ùëü)(V ùëü )x = Wx + BAx = Wx + ŒîWx, (5) Meanwhile, setting the rank to zero reduces the model to Recursive Transformer, as the LoRA modules contribute no additional parameters, highlighting the flexibility of this relaxation approach. 2.4. Continuous Depth-wise Batching and Early-Exiting In real-world deployments, user requests arrive sequentially and asynchronously. Recent research has introduced continuous sequence-wise batching (Kwon et al., 2023; Yu et al., 2022), serving strategy that allows new requests to immediately replace completed (terminated) sequence within batch. 5 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA"
        },
        {
            "title": "Models",
            "content": "N-emb"
        },
        {
            "title": "Emb",
            "content": "Gemma 2B 1.98B 0.52B TinyLlama 1.1B 0.97B 0.13B Pythia 1B 0.81B 0.21B ùëÅùêø 18 16 ùëëùëöùëúùëëùëíùëô ùëÅ‚Ñéùëíùëéùëë ùëÅùêæùëâ ùëë‚Ñéùëíùëéùëë"
        },
        {
            "title": "Dataset",
            "content": "ùëÅùë°ùëúùëò 2048 2048 2048 8 8 1 4 8 256 256K 64 32K 256 50K"
        },
        {
            "title": "Unreleased",
            "content": "3T SlimPajama + 73B 32B Starcoderdata 300B Pile ùêøùëêùë°ùë• 8K 2K 2K Table 1 Key parameters and pretraining details of three models. The sizes of each model refer to the number of embedding parameters (embedding matrices and classifier heads), and all other non-embedding parameters. Gemma and TinyLlama utilize Multi-Query (Shazeer, 2019) and GroupedQuery (Ainslie et al., 2023) attention mechanisms, which leads to reduced number of key-value heads. We take an early TinyLlama checkpoint to study recursive conversions on top of an under-trained model on SlimPajama. The vanilla performance with longer pretraining is reported in Table D.1. This approach exploits the fact that the computation performed for new token is functionally the same and utilize the same model parameters. By continuously scheduling requests in this manner, models can operate at their maximum batch capacity, thereby enhancing serving efficiency. The repetitive structure of Recursive Transformers allows for the same function to be applied not just across sequences, but also across depths (loop iterations). This introduces new dimension for continuous batching, which we call Continuous Depth-wise Batching. This technique enables the simultaneous computation of different iterations of the looped layer block for different samples (See Figure 3 for an example with single forward pass; this easily extends to multiple decode iterations per request.) With maximum batch size of 32, standard Transformer must wait for all model stages to complete before processing new requests. In contrast, our Recursive Transformer, because it shares layer functions across all stages, can immediately schedule new incoming requests at timestep 2, maximizing batch size utilization. This strategy can yield substantial speedup in generation and reduce the time to first token (Fu et al., 2024; Miao et al., 2023) through faster scheduling. Throughput improvements from depth-wise batching are further amplified when combined with early-exiting (Bae et al., 2023; Elbayad et al., 2020; Schuster et al., 2022). As depicted in Figure 3c, once some samples exit after certain looping iterations, queued requests can then be immediately scheduled. While Recursive Transformers leverage the speedup from early-exiting, they also inherently address key challenge of batched inference in early-exiting approaches: the synchronization issue when serving large batches, as early-exited tokens might wait for others to complete processing through the entire model. We demonstrate that Recursive Transformers, equipped with this dynamic sample scheduling at various depths, can theoretically allow up to 2-3 speedup on evaluated LLMs. 3. Experiments 3.1. Experimental Setup We evaluate our method on three popular pretrained LLMs: Gemma 2B (Team et al., 2024), TinyLlama 1.1B (Zhang et al., 2024b), and Pythia 1B (Biderman et al., 2023). Table 1 summarizes each models architecture and pretraining recipes, and their few-shot performance is summarized in Appendix D. Unless stated otherwise, the number of looping blocks (B) is set to 2 for all experiments. The results for Gemma with = 3 is provided in the appendix. After converting to Recursive Transformers, we uptrained models on the SlimPajama dataset (Soboleva et al., 2023). We used the Language Model Evaluation Harness framework (Gao et al., 2023) to evaluate accuracy on seven few-shot tasks, and averaged them for performance comparison. Detailed experimental setup can be found in Appendix E. 6 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA"
        },
        {
            "title": "Uptrain",
            "content": "Perplexity Few-shot Accuracy PQ WG ARC-e ARC-c OB"
        },
        {
            "title": "Models",
            "content": "Gemma 2B TinyLlama 1.1B Pythia 1B - LD HS 11.46 8.18 13.52 63.1 71.4 78.1 65.0 72.3 67.6 64.9 N-emb PT ùëÅùë°ùëúùëò SlimP RedP PG19 1.99B 1.99B 15B 10.76 8.47 13.08 63.5 68.5 77.0 63.5 1.99B 60B 10.58 8.44 12.71 60.3 67.9 76.9 63.5 0.97B 0.97B 15B 0.97B 60B 0.81B 51.9 15.68 9.90 12.05 57.5 49.1 70.4 52.8 0.81B 15B 13.46 9.95 13.38 55.0 49.0 71.0 53.6 51.8 0.81B 60B 12.83 9.76 13.57 53.0 50.2 71.1 54.8 51.9 44.7 12.26 9.37 11.94 43.3 42.2 66.8 53.4 9.87 8.24 10.73 49.2 46.3 68.8 54.0 48.2 9.59 8.12 10.42 51.6 48.8 68.6 54.1 49.9 - - 41.9 38.1 37. 40.2 61.7 42.6 60.1 39.6 58.6 29.2 43.3 23.2 26.0 32.2 46.4 26.2 32.8 47.4 26.7 28.2 27.7 33.4 48.8 32.8 48.8 31.6 48.6 Table 2 Uptraining the pretrained models on datasets that differ significantly in quality or distribution from their pretraining datasets can lead to decreased performance. We evaluated models after uptraining on the SlimPajama dataset. We measured perplexity on test sets of the SlimPajama, RedPajama, and PG19, and few-shot accuracy on LAMBADA, HellaSwag, PIQA, WinoGrande, ARCeasy, ARC-challenge, and OpenBookQA benchmarks. 3.2. Non-Recursive Model Baselines Given that we leveraged pretrained model weights for initialization and subsequently uptrained the models, it becomes crucial to define clear performance targets for our parameter-shared models. Full-size model Our ultimate goal is for the Recursive Transformer to achieve performance comparable to the original, full-size pretrained model, without much uptraining. However, we observed that the distribution divergence between the pretraining and uptraining datasets can hinder achieving the desired performance. In particular, uptraining on new datasets, particularly those of comparatively lower quality, sometimes led to performance degradation on certain benchmarks. Table 2 summarizes the evaluation results of full-size models based on the number of uptraining tokens. For instance, in the case of Gemma, where the pretraining dataset is unreleased but potentially well-curated (Team et al., 2024), all few-shot performance metrics gradually decreased after uptraining on the SlimPajama dataset. This suggests that the achievable upper bound performance with the SlimPajama dataset might be considerably lower than the original model performance. Therefore, we set the target performance for Gemma and Pythia models as the performance achieved by uptraining full-size pretrained model with an equivalent number of tokens. Since TinyLlama was already pretrained on SlimPajamawhich is the same dataset we use for uptraining (eliminating any distribution shift)for slightly longer than our runs, we use the performance of the original checkpoint as reference. Reduced-size model To demonstrate the performance advantages of Recursive Transformers compared to models with an equivalent number of parameters, we introduce another baseline: reduced-size models. These models have either half or one-third the parameters of their full-sized counterparts, matching the parameter count of our recursive models. However, these reduced models are pretrained from scratch on the same training recipe (number of training tokens and distillation from full-size model), but without the benefits of the pretrained weights and the looping mechanism. This comparison serves to highlight the efficacy of our initialization techniques and the recursive function itself in attaining strong performance, even with constrained model size. 3.3. Main Results Figure 4 presents the few-shot performance of Recursive Transformers with two blocks and their relaxed variants. Recursive Transformers, even without relaxation, demonstrate remarkably high Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA (a) Gemma (b) TinyLlama (c) Pythia Figure 4 Recursive and Relaxed Recursive Transformers achieve comparable performance to full-size models, and significantly outperform reduced-size models. Recursive models were initialized using the Stepwise method, while relaxed models utilized Average and SVD methods for looped layers and LoRA modules. We show the performance of four different rank values: 64, 128, 256, and 512. Recursive and reduced-size models were either uptrained (recursive model) and pretrained from scratch (reduced-size model) on 60 billion tokens using knowledge distillation objective. performance despite having only half the parameters of the full-size model. The Gemma model achieved 10%p performance gain compared to the reduced-size model, which was also trained on 60 billion tokens using distillation loss. Remarkably, the recursive TinyLlama model even surpassed the vanilla models performance, even though the latter was pretrained on larger corpus of 105 billion tokens. Our initialization techniques proved highly effective in achieving this superior result, along with the benefit of the uptraining dataset (SlimPajama) being the same as its pretraining dataset. The relaxed models effectively interpolate between the full-size model and the Recursive Transformer, depending on the LoRA rank. As the model size increases with larger LoRA modules, SVD initialization methods allow for more precise approximation of full-rank matrices, resulting in improved performance. Notably, the relaxed Gemma model with rank of 512 achieves performance on par with the original model pretrained on 3 trillion tokens (58.4% vs. 58.6%), despite using fewer parameters and uptraining on only 60 billion tokens. This trade-off provides flexibility in selecting the best configuration for various deployment scenarios. We believe that additional uptraining and higher-quality datasets could yield better performance with even more streamlined models. In the subsequent sections, we provide comprehensive overview of extensive ablation studies conducted prior to achieving this final performance. In 3.4, we delve into the analysis of various Insights into the relaxation model are initialization methodologies for Recursive Transformers. detailed in 3.5. Finally, we explore enhanced training strategies like knowledge distillation (3.6). 3.4. Initialization Techniques for Looped Layers Stepwise initialization serves as the best initial point for Recursive Transformers We present the training loss of Gemma models initialized using three different methods in Figure 5a, and their few-shot performance in Figure 5b. Our proposed methods significantly outperformed random initialization, which simply adds recursion to reduced-size model, suggesting that leveraging pretrained weights in any manner is beneficial for performance boost. Moreover, the Stepwise methodology consistently demonstrated best performance, aligning with insights that LLMs can preserve performance even with few layers skipped (Elhoushi et al., 2024; Raposo et al., 2024; Zhang et al., 2024a). Interestingly, as summarized in Table F.1, the recursive TinyLlama model, uptrained on only 15 billion tokens, 8 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA (a) Loss curves for Gemma (b) Average few-shot performance (c) Recursive model performance Figure 5 (a) Among the proposed methods, the Stepwise method obtains the lowest training loss on the SlimPajama dataset. (b) The Stepwise method consistently demonstrate the highest average few-shot accuracy across three architectures. (c) Recursive Transformers initialized with the Stepwise method demonstrated significant performance gains compared to non-recursive model baselines. yields few-shot performance comparable to the original model pretrained on 105 billion tokens. This suggests that with sufficient training, even recursive architecture can match the performance of full-size pretrained model (Dehghani et al., 2019; Takase and Kiyono, 2023). Recursive Gemma 1B outperforms both pretrained TinyLlama 1.1B and Pythia 1B The looped Gemma 1B model, utilizing our proposed Stepwise method, outperformed reduced-size baselines with equivalent parameter counts by up to 13.5 percentage points (51.7% vs. 38.2%). Furthermore, it even outperformed the full-size TinyLlama 1.1B and Pythia 1B models (see Figure 5c). This is noteworthy achievement given that Pythia was pretrained on 300 billion tokens, whereas the recursive Gemma was uptrained on only 15 billion tokens. Consequently, high-performing LLMs serve as promising starting point, as their recursive counterparts readily outperform other ordinary vanilla models of similar size. Further details can be found in Appendix F."
        },
        {
            "title": "Takeaways for Recursive Transformer",
            "content": "We find that converting well-pretrained models into Recursive Transformers leads to highperforming models with minimal uptraining. Notably, initializing looped layers via the Stepwise method yields the best results. With just 15 billion tokens of uptraining, recursive Gemma 1B model outperforms even the full-size pretrained TinyLlama and Pythia models. 3.5. Relaxation of Strict Parameter Sharing via LoRA Modules Average initialization for looped layers is most compatible with Relaxed Recursive Transformer Figures 6a and 6b illustrate the effect of relaxing parameter sharing via layer-wise LoRA modules. Notably, initializing tied layers in relaxed models with Average method yielded substantial performance improvements, even outperforming the non-relaxed model initialized with Stepwise. Approximating residual matrices between averaged weights and their individual weights appears readily achievable using truncated SVD with low ranks. In contrast, we observed an intriguing phenomenon where our models initialized with Stepwise occasionally showed performance degradation after relaxation. This is likely because capturing the nuances between entirely distinct layer weights is challenging with an insufficient rank, leading to suboptimal solution. Further details are provided in Appendix G. 9 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA (a) Loss changes in Gemma (b) Accuracy gains from relaxation (c) Effects of SVD initialization Figure 6 The Relaxed Recursive Transformer, with its looped layer initialized using Average method, achieved the best performance in terms of both (a) training loss and (b) few-shot accuracy. The models utilize two blocks, with the LoRA modules initialized using the SVD method at rank of 512. (c) SVD initialization method significantly enhanced performance compared to zero initialization. SVD initialization to approximate pretrained weights outperforms zero initialization LoRA modules initialized with zero values guarantee that the model begins training from the same point as the non-relaxed model. Conversely, SVD initialization positions the model closer to either the full-size model (with full-rank) or the non-relaxed model (with small rank). To emphasize the effectiveness of initializing near full-size model weights, we compared these two methods at moderately large rank of 512, as shown in Figure 6c. Our proposed SVD strategy demonstrated an impressive performance boost of up to 6.5 points, facilitating faster convergence by updating the principal low-rank matrices (aligned with findings in Meng et al. (2024)). For results across other architectures, refer to Figure G.2. Higher rank enhances recovery of original pretrained weights At full rank, relaxed models can perfectly match full-size pretrained models. Consequently, as illustrated in Figure 7a, performance generally improves with increasing rank, resulting in clear Pareto frontier between model size and performance. However, only Stepwise initialization showed U-shaped performance trend: middlerange rank resulted in poor approximation, whereas very low ranks (akin to random initialization for LoRA modules) yielded better performance. The overall results are summarized in Table G.2."
        },
        {
            "title": "Takeaways for Relaxed Recursive Transformer",
            "content": "Adjusting the LoRA rank in the Relaxed Recursive Transformer, together with our SVD-based initialization technique, allows for smoother trade-off between fully weight-tied recursive model and vanilla model. Furthermore, we find that initializing the shared weights in the looped layers with the Average method leads to the best performance in this setting. 3.6. Extended Uptraining and Knowledge Distillation We further enhanced the performance of our low-rank models by introducing two techniques: uptraining on an extended corpus and knowledge distillation from the full-sized model. Specifically, we increased the number of uptraining tokens from 0.5% to 2% of the total 3 trillion tokens used for pretraining Gemma models, resulting in total of 60 billion tokens. Additionally, we regularized the losses using forward Kullback-Leibler divergence (Hinton et al., 2015; Kim and Rush, 2016), which exhibited the best performance gains among the examined distillation losses. Table I.1 summarizes the results of various ablation studies conducted to investigate the impact of these two techniques. 10 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA (a) Performance by LoRA rank (b) Gains from longer KD training (c) Overall performance Figure 7 (a) Increasing the LoRA rank typically leads to improved performance in relaxed Gemma models, attributed to the use of SVD initialization. (b) Extended uptraining and knowledge distillation yielded substantial accuracy gains for Gemma models. Note that the full-size model is pretrained model that is further uptrained on 60 billion tokens. (c) Recursive and Relaxed Recursive Transformers achieve compelling Pareto frontier with respect to model size and performance. Recursive and relaxed models used Stepwise and Average method to initialize looped layers, respectively."
        },
        {
            "title": "Looping",
            "content": "Early-Exit Train Few-shot Accuracy N-emb PT ùëÅùë°ùëúùëò Block Init ùëÅùë°ùëúùëò 0.99B 15B"
        },
        {
            "title": "Step",
            "content": "2 - CE -"
        },
        {
            "title": "KD LD",
            "content": "HS PQ WG ARC-e ARC-c OB"
        },
        {
            "title": "Avg",
            "content": "- 53.0 57.3 73.2 56.2 56.1 29.2 36.6 51.7 Œî - 0.99B 15B 0.99B 15B 0.99B 15B 0.99B 15B 2 2 2 Step 15B Agg (0.1) Step 15B Weighted 48.9 55.5 72.7 55.3 49.5 54.8 72.0 53.4 53.0 59.1 73.9 55.4 45.9 51.2 71.4 54.5 Step 15B Weighted 47.7 55.1 73.2 55.6 48.3 54.9 72.1 55.9 52.9 58.9 73.7 55.7 46.3 52.1 71.6 55.3 Step 15B Agg (0.1) 54.9 54.1 57.4 48.1 54.5 54.3 57.5 49.2 30.1 29.1 30.6 26. 29.1 28.4 31.1 28.5 36.0 50.5 1.2 35.6 49.8 37.8 52.5 +0.8 32.0 47.1 - - 37.2 50.4 1.3 35.4 49.9 38.2 52.6 +0.9 32.6 48.0 - - Table 3 small loss coefficient to the first loop output (intermediate output) can significantly improve intermediate performance without compromising the final performance. Performance was evaluated under static-exiting scenario (Schuster et al., 2022), where all tokens exit at either first or second loop. We further trained the previously uptrained Gemma models on 15 billion tokens (post-training). Delta (Œî) denotes the performance changes in the final outputs after early-exit training. The combined effect of these techniques is presented in Figure 7b, demonstrating an improvement of up to 4.1 percentage points in few-shot accuracy compared to the previous 15 billion token uptraining results. Notably, the relaxed Gemma model with rank of 512 nearly matched the performance of the full-size model. We also expect that further performance gains can be achieved with much lighter recursive model by utilizing superior teacher model or conducting more extensive training on high-quality data. Figure 7c illustrates the Pareto frontier achieved by the final models. All models exhibit competitive performance compared to the full-size model. Moreover, the superior performance of the recursive Gemma model strongly highlights the advantages of converting high-performing LLMs to recursive architecture. Additional details can be found in Appendix I. 3.7. Early-Exit Training Strategy for Recursive Transformer The throughput of Recursive Transformers can be amplified by an early-exiting framework. Hence, we further train intermediate representations from fewer looping iterations to enable token prediction. Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Œîùëâ N-emb Loop LoRA Batch Exit Acc. Thr. 1.99B 1.99B 0.99B 1.07B 1.15B 1.30B 1.60B ŒîùëÜùëíùëû 57.3 1080 1.00 0.71 - - 57.3 1528 1.41 1.00 CSB - CDB 54.0 2877 2.66 1.88 - CDB 54.0 2157 2.00 1.41 64 128 CDB 54.6 2149 1.99 1.41 256 CDB 55.2 1921 1.78 1.26 512 CDB 56.2 1719 1.59 1.13 - - 2 2 2 2 Figure 8 Continuous depth-wise batching (CDB) with early exiting enables Recursive Transformers to theoretically achieve significant throughput improvements. Throughput (tokens/sec) was averaged across SlimPajama, RedPajama, and PG19, and then normalized to the throughput of the vanilla Pythia model. The accompanying table gives detailed throughout and performance measurements for Gemma. Œîùëâ measures throughput relative to the vanilla Gemma model, while ŒîùëÜùëíùëû measures throughput relative to the vanilla Gemma model with continuous sequence-wise batching (CSB). We conducted an ablation study on various strategies, as summarized in Table 3 (more detailed results are presented in Table J.1). Directly applying the weighted CE loss (L = (cid:205)ùêµ ùõºùëñLùëñ where ùõºùëñ = ùëñ/(cid:205)ùëñ ùëñ) ùëñ=1 commonly used in prior works (Bae et al., 2023; Schuster et al., 2022) led to an overemphasis on the training of intermediate representations. To address this, we employ an aggressive coefficient strategy that aggressively reduces the loss coefficient for intermediate outputs while maintaining coefficient of 1 for the final output. Our experiments demonstrated that an aggressive coefficient of 0.1, utilizing knowledge distillation from the detached final outputs (Bae et al., 2023), effectively preserves final performance while enhancing intermediate performance. Notably, the first loop output yielded only difference of 4.6 percentage points in accuracy compared to the final output. This underscores the potential to maximize the benefits of early-exiting in parameter-shared LLMs. We applied this post-training strategy for early-exiting to our final uptrained models (shown in 3.3), with all experimental results detailed in Appendix J. The aggressive coefficient strategy, combined with self-distillation, consistently achieved the best performance for intermediate outputs while maintaining strong performance for the final loop output across all models. However, as the optimal strategy derived from the non-relaxed models was directly applied to the relaxed models, more tailored training approach might further enhance the performance of intermediate loop outputs in Relaxed Recursive Transformers. 3.8. Hypothetical Generation Speedup via Continuous Depth-wise Batching How we theoretically approximate actual throughput As developing practical early-exiting algorithms is beyond the scope of this work, we present hypothetical throughput improvements based on an oracle-exiting approach (Bae et al., 2023; Schuster et al., 2022). This assumes that tokens exit at the earliest looping block where their prediction aligns with the final loops prediction. We simulated the generation of language modeling datasets as if they were generated by our models, to obtain the exit trajectory for each token. Then, we measured the average per-token generation time under specific constraints, such as different memory limit or context lengths. Using these measurements and the exit trajectory data, we conducted simulations to estimate theoretical throughput. Detailed explanations and limitations are discussed in Appendix K. Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Continuous depth-wise batching paired with early-exiting can substantially boost throughput Figure 8 illustrates the throughput of our proposed models and the vanilla Transformer across three architectures. We consistently achieve higher speeds than the vanilla models by combining continuous depth-wise batching with early-exiting, even surpassing those with continuous sequencewise batching (Kwon et al., 2023; Yu et al., 2022). In particular, Recursive models demonstrate up to 2.66 speedup in generation compared to vanilla counterparts. Additionally, the recursive Gemma model significantly outperforms the vanilla pretrained Pythia model, with nearly 4 improvement in throughput. Relaxed recursive models show clear trade-off between achievable few-shot performance and throughput, modulated by the degree of relaxation through the LoRA ranks. This characteristic enables flexible model selection tailored to specific deployment scenarios. Comprehensive results are presented in Tables K.2 and K.4. Takeaways for Continuous Depth-wise Batching We analyze the potential for throughput improvement in the Recursive Transformer via continuous depth-wise batching, novel inference paradigm. In theory, we find that we can achieve up to 2-3 speedup compared to vanilla Transformer. This even outperforms the throughput gain achieved by existing continuous sequence-wise batching methods in vanilla models. 4. Related Work Cross-layer parameter sharing has proven to be an effective method for achieving parameter efficiency in deep learning models such as RNNs (Graves, 2016b; Sherstinsky, 2018), CNNs (Eigen et al., 2014; Guo et al., 2019; Savarese and Maire, 2019), and the popular Transformer architecture. The Universal Transformer (Dehghani et al., 2019), recurrent self-attentive model, demonstrated superior performance to non-recursive counterparts with significantly fewer parameters. This crosslayer parameter sharing approach has subsequently been explored in various tasks, including language understanding (Lan et al., 2020), language modeling (Bai et al., 2019), and machine translation, through stacking single layer (Dabre and Fujita, 2019), tying encoder (Milbauer et al., 2023) and decoder components (Xia et al., 2019), or partially tying layers (Takase and Kiyono, 2023). These methods often claim to achieve comparable performance with more compact models and increased computational speed, while also setting the ground for effective adaptive compute solutions (Dehghani et al., 2019; Graves, 2016b; Schuster et al., 2021). Concurrently, there has been growing interest in exploiting recurrent architectures for algorithmic or logical reasoning tasks (Saunshi et al., 2024). Prior research (McLeish and Tran-Thanh, 2022; Schwarzschild et al., 2021) has shown that recurrent networks can extrapolate reasoning strategies learned on simple problems to harder, larger problems through additional recurrences during inference. The looped Transformer structure has also been employed to emulate basic computing blocks for program simulation (Giannou et al., 2023), to learn iterative algorithms for data-fitting problems (Yang et al., 2024), to achieve length generalization in algorithmic tasks (Fan et al., 2024), and promising theoretical potential for few-shot learning (Gatmiry et al., 2024). However, previous work has predominantly focused on relatively small Transformer models, trained from scratch without leveraging pretrained model weights. Our work distinguishes itself by investigating parameter sharing in the context of LLMs and proposing effective initialization strategies that leverage the knowledge embedded within existing LLMs. To the best of our knowledge, we are the first to propose generalized framework for parameter-shared models, enabling relaxation in weight tying constraints through layer-specific modules. 13 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA In this paper, we also discuss how Recursive Transformers can be well suited for early-exiting techniques to accelerate decoding in LLMs. The inherent recursive structure readily enables earlyexiting for individual responses within large serving batch, which is often practical limitation of such techniques. Vanilla Transformers encounter synchronization issue with early-exiting, where the model must forward all layers if even single token in batch requires full processing (exited tokens must wait for them). Several approaches attempt to exploit this idle time by computing missing KV caches for exited tokens in later layers, which are essential for subsequent sequence generation. These techniques include state propagation (Elbayad et al., 2020; Schuster et al., 2022), SkipDecode (Del Corro et al., 2023), and parallel decoding (which can be combined with Speculative Decoding) (Bae et al., 2023; Chen et al., 2024b; Elhoushi et al., 2024; Liu et al., 2024; Tang et al., 2024). Nevertheless, the heterogeneous parameters across varying model depths still hinder the efficient progression of exited tokens to subsequent sequences. In contrast, our Recursive Transformers enable parallel computation for tokens at different depths and sequences (in continuous depth-wise batching paradigm)also allow for parallel computation of missing KV caches with minimal overhead during the memory-bounded decoding phase. 5. Discussion and Future Work In this work, we introduced Recursive Transformers, in which we compress LLMs via parameter sharing across recursively looped blocks of layers. Additionally, we presented novel relaxation strategy that allows for low-rank deltas between shared layers by integrating layer-specific LoRA modules into the fully-tied structure. Through novel initialization techniques for looped layers and LoRA modules, we achieved significant performance improvements that closely approximate the original pretrained model. Finally, by exploiting the recursive patterns and an early-exiting approach, we propose continuous depth-wise batching paradigm tailored for efficient serving systems of Recursive Transformers. We theoretically demonstrated that an oracle-exiting strategy can yield substantial throughput gains, reaching up to 2-3 speedup. This work motivates further research on recursive patterns in modern LLMs such as: Beyond hypothetical generation speedup Our oracle-exiting approach assumes any intermediate prediction matching the final output can be exited. However, accurate throughput measurement requires confidence-based early-exiting algorithms (Bae et al., 2023; Schuster et al., 2022). Moreover, practical deployment needs to address decoding bottlenecks like key-value cache computation for exited tokens in remaining loops. We leave the potential solutions, such as efficient confidence estimation or key-value cache sharing (Brandon et al., 2024; Sun et al., 2024), to future work. Efficient serving of multi-LoRA layers Relaxed models require the computation of distinct LoRA modules during batched inference, akin to multi-task learning (Feng et al., 2024; Wang et al., 2023), hindering parallel computation. We concatenated LoRA weights into single weight to improve efficiency over sequential computation, yet it introduces redundancy. To mitigate this, we can explore optimized CUDA kernels for LoRA serving (Chen et al., 2024a; Sheng et al., 2023) and parallelization across accelerators, inspired by distributed training for Mixture of Experts (Fedus et al., 2022; Gale et al., 2023). Scaling up Recursive Transformers Scaling our approach to larger LLMs (7B and beyond) is promising avenue for future research. While our methodology is expected to remain effective, achieving comparable performance may require significantly higher uptraining costs. Increased model size offers the potential for reduced memory footprint from recursive patterns; however, it is unclear whether this translates to larger batch sizes, given the corresponding increase in hidden dimensions. Nevertheless, our continuous depth-wise batching will yield considerable gains in serving efficiency. 14 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA"
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Jacob Eisenstein for valuable feedback on an earlier version of the paper. We thank Jiyoun Ha, Alfred Piccioni, Dayeong Lee for the support with setting up the experimental environment. We also thank Donald Metzler, Ivan Korotkov, Jai Gupta, Sanket V. Mehta, Vinh Q. Tran, Brennan Saeta, Jean-Fran√ßois Kagy, Zhen Qin, and Jing Lu for helpful conversations. Finally, we thank the Google Cloud Platform for awarding Google Cloud credits for this project."
        },
        {
            "title": "References",
            "content": "R. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R. Garea, M. Geist, and O. Bachem. On-policy In The Twelfth Interdistillation of language models: Learning from self-generated mistakes. national Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=3zKtaqxLhW. J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr√≥n, and S. Sanghai. GQA: training generalized multi-query transformer models from multi-head checkpoints. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 48954901. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.298. URL https:// doi.org/10.18653/v1/2023.emnlp-main.298. S. Bae, J. Ko, H. Song, and S. Yun. Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 59105924. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.362. URL https://doi.org/10.18653/v1/ 2023.emnlp-main.362. S. Bai, J. Z. Kolter, and V. Koltun. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. dAlch√©-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 688699, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 01386bd6d8e091c2ab4c7c7de644d37b-Abstract.html. Deep equilibrium models. S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. OBrien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal. Pythia: suite for analyzing large language models across training and scaling. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 23972430. PMLR, 2023. URL https://proceedings.mlr.press/v202/biderman23a. html. Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. W. Brandon, M. Mishra, A. Nrusimha, R. Panda, and J. Ragan-Kelley. Reducing transformer key-value cache size with cross-layer attention. CoRR, abs/2405.12981, 2024. doi: 10.48550/ARXIV.2405. 12981. URL https://doi.org/10.48550/arXiv.2405.12981. 15 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA L. Chen, Z. Ye, Y. Wu, D. Zhuo, L. Ceze, and A. Krishnamurthy. Punica: Multi-tenant lora serving. Proceedings of Machine Learning and Systems, 6:113, 2024a. Y. Chen, X. Pan, Y. Li, B. Ding, and J. Zhou. EE-LLM: large-scale training and inference of early-exit large language models with 3d parallelism. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024b. URL https: //openreview.net/forum?id=xFk0w9zoV3. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data. R. Dabre and A. Fujita. Recurrent stacking of layers for compact neural machine translation models. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 62926299. AAAI Press, 2019. doi: 10.1609/AAAI.V33I01.33016292. URL https: //doi.org/10.1609/aaai.v33i01.33016292. T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R√©. Flashattention: Fast and memory-efficient exact attention with io-awareness. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html. M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser. Universal transformers. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=HyzdRiR9Y7. L. Del Corro, A. Del Giorno, S. Agarwal, B. Yu, A. Awadallah, and S. Mukherjee. Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference. arXiv preprint arXiv:2307.02628, 2023. Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hajishirzi, S. Kakade, A. Farhadi, and P. Jain. Matformer: Nested transformer for elastic inference, 2023. URL https://arxiv.org/abs/2310.07707. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, A. Goyal, A. Hartshorn, A. Yang, A. Mitra, A. Sravankumar, A. Korenev, A. Hinsvark, A. Rao, A. Zhang, A. Rodriguez, A. Gregerson, A. Spataru, B. Rozi√®re, B. Biron, B. Tang, B. Chern, C. Caucheteux, C. Nayak, C. Bi, C. Marra, C. McConnell, C. Keller, C. Touret, C. Wu, C. Wong, C. C. Ferrer, C. Nikolaidis, D. Allonsius, D. Song, D. Pintz, D. Livshits, D. Esiobu, D. Choudhary, D. Mahajan, D. Garcia-Olano, D. Perino, D. Hupkes, E. Lakomkin, E. AlBadawy, E. Lobanova, E. Dinan, E. M. Smith, F. Radenovic, F. Zhang, G. Synnaeve, G. Lee, G. L. Anderson, G. Nail, G. Mialon, G. Pang, G. Cucurell, H. Nguyen, H. Korevaar, H. Xu, H. Touvron, I. Zarov, I. A. Ibarra, I. M. Kloumann, I. Misra, I. Evtimov, J. Copet, J. Lee, J. Geffert, J. Vranes, J. Park, J. Mahadeokar, J. Shah, J. van der Linde, J. Billock, J. Hong, J. Lee, J. Fu, J. Chi, J. Huang, J. Liu, J. Wang, J. Yu, J. Bitton, J. Spisak, J. Park, J. Rocca, J. Johnstun, J. Saxe, J. Jia, K. V. Alwala, K. Upasani, K. Plawiak, 16 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA K. Li, K. Heafield, K. Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783. D. Eigen, J. T. Rolfe, R. Fergus, and Y. LeCun. Understanding deep architectures using recursive convolutional network. In Y. Bengio and Y. LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings, 2014. URL http://arxiv.org/abs/1312.1847. M. Elbayad, J. Gu, E. Grave, and M. Auli. Depth-adaptive transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SJg7KhVKPH. M. Elhoushi, A. Shrivastava, D. Liskovich, B. Hosmer, B. Wasti, L. Lai, A. Mahmoud, B. Acun, S. Agarwal, A. Roman, A. A. Aly, B. Chen, and C. Wu. Layerskip: Enabling early exit inference and self-speculative decoding. In L. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1262212642. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.acl-long.681. A. Fan, E. Grave, and A. Joulin. Reducing transformer depth on demand with structured dropout. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SylO2yStDr. Y. Fan, Y. Du, K. Ramchandran, and K. Lee. Looped transformers for length generalization. arXiv preprint arXiv:2409.15647, 2024. W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23:120:1120:39, 2022. URL https: //jmlr.org/papers/v23/21-0998.html. W. Feng, C. Hao, Y. Zhang, Y. Han, and H. Wang. Mixture-of-loras: An efficient multitask tuning method for large language models. In N. Calzolari, M. Kan, V. Hoste, A. Lenci, S. Sakti, and N. Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pages 1137111380. ELRA and ICCL, 2024. URL https://aclanthology.org/2024.lrec-main.994. J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJl-b3RcF7. Q. Fu, M. Cho, T. Merth, S. Mehta, M. Rastegari, and M. Najibi. Lazyllm: Dynamic token pruning for efficient long context LLM inference. CoRR, abs/2407.14057, 2024. doi: 10.48550/ARXIV.2407. 14057. URL https://doi.org/10.48550/arXiv.2407.14057. T. Gale, D. Narayanan, C. Young, and M. Zaharia. Megablocks: Efficient sparse training with mixtureof-experts. Proceedings of Machine Learning and Systems, 5:288304, 2023. L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noach, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836. K. Gatmiry, N. Saunshi, S. J. Reddi, S. Jegelka, and S. Kumar. Can looped transformers learn to implement multi-step gradient descent for in-context learning?, 2024. 17 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA A. Giannou, S. Rajput, J. Sohn, K. Lee, J. D. Lee, and D. Papailiopoulos. Looped transformers as programmable computers. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1139811442. PMLR, 2023. URL https://proceedings.mlr.press/v202/giannou23a.html. A. Graves. Adaptive computation time for recurrent neural networks. CoRR, abs/1603.08983, 2016a. URL http://arxiv.org/abs/1603.08983. A. Graves. Adaptive computation time for recurrent neural networks, 2016b. Y. Gu, L. Dong, F. Wei, and M. Huang. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=5h0qf7IBZZ. Q. Guo, Z. Yu, Y. Wu, D. Liang, H. Qin, and J. Yan. Dynamic recursive neural network. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 51475156. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR. 2019.00529. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Guo_ Dynamic_Recursive_Neural_Network_CVPR_2019_paper.html. P. C. Hansen. The truncated svd as method for regularization. BIT Numerical Mathematics, 27: 534553, 1987. G. E. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in neural network. CoRR, abs/1503.02531, 2015. URL http://arxiv.org/abs/1503.02531. J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training computeoptimal large language models, 2022. URL https://arxiv.org/abs/2203.15556. N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for NLP. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 27902799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html. E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: LowIn The Tenth International Conference on Learning rank adaptation of large language models. Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https: //openreview.net/forum?id=nZeVKeeFYf9. S. Kim, D. Kim, C. Park, W. Lee, W. Song, Y. Kim, H. Kim, Y. Kim, H. Lee, J. Kim, C. Ahn, S. Yang, S. Lee, H. Park, G. Gim, M. Cha, H. Lee, and S. Kim. SOLAR 10.7b: Scaling large language models with simple yet effective depth up-scaling. In Y. Yang, A. Davani, A. Sil, and A. Kumar, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 2335. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.NAACL-INDUSTRY.3. URL https://doi.org/10.18653/v1/2024. naacl-industry.3. 18 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Y. Kim and A. M. Rush. Sequence-level knowledge distillation. In J. Su, X. Carreras, and K. Duh, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 13171327. The Association for Computational Linguistics, 2016. doi: 10.18653/V1/D16-1139. URL https://doi.org/10. 18653/v1/d16-1139. W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In J. Flinn, M. I. Seltzer, P. Druschel, A. Kaufmann, and J. Mace, editors, Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 611626. ACM, 2023. doi: 10.1145/3600006.3613165. URL https://doi.org/10.1145/3600006.3613165. Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. ALBERT: lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=H1eA7AEtvS. Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1927419286. PMLR, 2023. URL https: //proceedings.mlr.press/v202/leviathan23a.html. F. Liu, Y. Tang, Z. Liu, Y. Ni, K. Han, and Y. Wang. Kangaroo: Lossless self-speculative decoding via double early exiting. arXiv preprint arXiv:2404.18911, 2024. X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. CoRR, abs/2110.07602, 2021. URL https: //arxiv.org/abs/2110.07602. Z. Liu, J. Wang, T. Dao, T. Zhou, B. Yuan, Z. Song, A. Shrivastava, C. Zhang, Y. Tian, C. R√©, and B. Chen. Deja vu: Contextual sparsity for efficient llms at inference time. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2213722176. PMLR, 2023. URL https://proceedings.mlr.press/v202/ liu23am.html. I. Loshchilov and F. Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum? id=Skq89Scxx. I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. S. M. McLeish and L. Tran-Thanh. [re] end-to-end algorithm synthesis with recurrent networks: Logical extrapolation without overthinking. In ML Reproducibility Challenge 2022, 2022. F. Meng, Z. Wang, and M. Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. CoRR, abs/2404.02948, 2024. doi: 10.48550/ARXIV.2404.02948. URL https://doi.org/10.48550/arXiv.2404.02948. 19 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA X. Miao, G. Oliaro, Z. Zhang, X. Cheng, H. Jin, T. Chen, and Z. Jia. Towards efficient generative large language model serving: survey from algorithms to systems. CoRR, abs/2312.15234, 2023. doi: 10.48550/ARXIV.2312.15234. URL https://doi.org/10.48550/arXiv.2312.15234. T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 23812391. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1260. URL https://doi.org/10.18653/v1/ d18-1260. J. Milbauer, A. Louis, M. J. Hosseini, A. Fabrikant, D. Metzler, and T. Schuster. LAIT: Efficient multi-segment encoding in transformers with layer-adjustable interaction. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1025110269, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.571. URL https://aclanthology.org/2023.acl-long.571. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774. D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fern√°ndez. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean. In D. Song, M. Carbin, and T. Chen, editors, Proceedings of Machine Learning and Systems, volume 5, pages 606624. Curan, 2023. URL https://proceedings.mlsys.org/paper_files/paper/2023/file/ c4be71ab8d24cdfb45e3d06dbfca2780-Paper-mlsys2023.pdf. Efficiently scaling transformer inference. J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson dAutume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. Johnson, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving. Scaling language models: Methods, analysis & insights from training gopher, 2021. S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. V. Ramanujan, M. Wortsman, A. Kembhavi, A. Farhadi, and M. Rastegari. Whats hidden in In 2020 IEEE/CVF Conference on Computer Vision and randomly weighted neural network? Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 1189011899. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01191. URL 20 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA https://openaccess.thecvf.com/content_CVPR_2020/html/Ramanujan_Whats_ Hidden_in_a_Randomly_Weighted_Neural_Network_CVPR_2020_paper.html. D. Raposo, S. Ritter, B. A. Richards, T. P. Lillicrap, P. C. Humphreys, and A. Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. CoRR, abs/2404.02258, 2024. doi: 10.48550/ARXIV.2404.02258. URL https://doi.org/10.48550/arXiv.2404. 02258. J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 35053506, 2020. M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. P. Lillicrap, J. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, I. Antonoglou, R. Anil, S. Borgeaud, A. M. Dai, K. Millican, E. Dyer, M. Glaese, T. Sottiaux, B. Lee, F. Viola, M. Reynolds, Y. Xu, J. Molloy, J. Chen, M. Isard, P. Barham, T. Hennigan, R. McIlroy, M. Johnson, J. Schalkwyk, E. Collins, E. Rutherford, E. Moreira, K. Ayoub, M. Goel, C. Meyer, G. Thornton, Z. Yang, H. Michalewski, Z. Abbas, N. Schucher, A. Anand, R. Ives, J. Keeling, K. Lenc, S. Haykal, S. Shakeri, P. Shyam, A. Chowdhery, R. Ring, S. Spencer, E. Sezener, and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530, 2024. doi: 10.48550/ARXIV.2403.05530. URL https: //doi.org/10.48550/arXiv.2403.05530. M. Rivi√®re, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ram√©, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Casbon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome, A. Tsitsulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Momchev, M. Hoffman, S. Thakoor, J. Grill, B. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ahmad, A. Hutchison, A. Abdagic, A. Carl, A. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bastian, B. Piot, B. Wu, B. Royal, C. Chen, C. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopalnikov, D. Weinberger, D. Vijaykumar, D. Rogozinska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter, E. Eltyshev, F. Visin, G. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. KlimczakPlucinska, H. Batra, H. Dhand, I. Nardini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P. Zhou, J. Carrasqueira, J. Iljazi, J. Becker, J. Fernandez, J. van Amersfoort, J. Gordon, J. Lipschultz, J. Newlan, J. Ji, K. Mohamed, K. Badola, K. Black, K. Millican, K. McDonell, K. Nguyen, K. Sodhia, K. Greene, L. L. Sj√∂sund, L. Usui, L. Sifre, L. Heuermann, L. Lago, and L. McNealus. Gemma 2: Improving open language models at practical size. CoRR, abs/2408.00118, 2024. doi: 10.48550/ARXIV.2408.00118. URL https://doi.org/10.48550/arXiv.2408.00118. J. S. Rosenfeld, A. Rosenfeld, Y. Belinkov, and N. Shavit. constructive prediction of the generalization error across scales. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=ryenvpEKDr. K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 87328740. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6399. URL https://doi.org/10.1609/aaai.v34i05.6399. N. Saunshi, S. Karp, S. Krishnan, S. Miryoosefi, S. J. Reddi, and S. Kumar. On the inductive bias of stacking towards improving reasoning, 2024. 21 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA P. Savarese and M. Maire. Learning implicitly recurrent cnns through parameter sharing. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJgYxn09Fm. T. Schuster, A. Fisch, T. Jaakkola, and R. Barzilay. Consistent accelerated inference via confident adaptive transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 49624979, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.406. URL https: //aclanthology.org/2021.emnlp-main.406. T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Tran, Y. Tay, and D. Metzler. Confident adaptive language modeling. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 6fac9e316a4ae75ea244ddcef1982c71-Abstract-Conference.html. Can you learn an algorithm? A. Schwarzschild, E. Borgnia, A. Gupta, F. Huang, U. Vishkin, M. Goldblum, and T. Goldgeneralizing from easy to hard problems with restein. current networks. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 66956706, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 3501672ebc68a5524629080e3ef60aef-Abstract.html. N. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019. URL http://arxiv.org/abs/1911.02150. N. Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv. org/abs/2002.05202. Y. Sheng, S. Cao, D. Li, C. Hooper, N. Lee, S. Yang, C. Chou, B. Zhu, L. Zheng, K. Keutzer, J. E. Gonzalez, and I. Stoica. S-lora: Serving thousands of concurrent lora adapters. CoRR, abs/2311.03285, 2023. doi: 10.48550/ARXIV.2311.03285. URL https://doi.org/10.48550/arXiv.2311.03285. A. Sherstinsky. Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. CoRR, abs/1808.03314, 2018. URL http://arxiv.org/abs/1808.03314. D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, and N. Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. Y. Sun, L. Dong, Y. Zhu, S. Huang, W. Wang, S. Ma, Q. Zhang, J. Wang, and F. Wei. You only cache once: Decoder-decoder architectures for language models. CoRR, abs/2405.05254, 2024. doi: 10.48550/ARXIV.2405.05254. URL https://doi.org/10.48550/arXiv.2405.05254. S. Takase and S. Kiyono. Lessons on parameter sharing across layers in transformers. In N. S. Moosavi, I. Gurevych, Y. Hou, G. Kim, Y. J. Kim, T. Schuster, and A. Agrawal, editors, Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing, SustaiNLP 2023, Toronto, Canada (Hybrid), July 13, 2023, pages 7890. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.SUSTAINLP-1.5. URL https://doi.org/10.18653/v1/2023. sustainlp-1.5. 22 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA P. Tang, P. Zhu, T. Li, S. Appalaraju, V. Mahadevan, and R. Manmatha. DEED: dynamic early exit on decoder for accelerating encoder-decoder transformer models. In K. Duh, H. G√≥mez-Adorno, and S. Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 116131. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-NAACL.9. URL https://doi.org/10.18653/v1/2024. findings-naacl.9. G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivi√®re, M. S. Kale, J. Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Z. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, J. Liu, Z. Qu, S. Yan, Y. Zhu, Q. Zhang, M. Chowdhury, and M. Zhang. Efficient large language models: survey. Trans. Mach. Learn. Res., 2024, 2024. URL https://openreview.net/forum?id=bsCCJHbO8A. H. Wang, T. Sun, C. Jin, Y. Wang, Y. Fan, Y. Xu, Y. Du, and C. Fan. Customizable combination of parameter-efficient modules for multi-task learning. In The Twelfth International Conference on Learning Representations, 2023. Y. Wen, Z. Li, W. Du, and L. Mou. f-divergence minimization for sequence-level knowledge distillation. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1081710834, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.605. URL https://aclanthology.org/2023.acl-long.605. T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, In Proceedings of the 2020 et al. Transformers: State-of-the-art natural language processing. conference on empirical methods in natural language processing: system demonstrations, pages 3845, 2020. Y. Xia, T. He, X. Tan, F. Tian, D. He, and T. Qin. Tied transformers: Neural machine translation with shared encoder and decoder. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 54665473. AAAI Press, 2019. doi: 10.1609/ AAAI.V33I01.33015466. URL https://doi.org/10.1609/aaai.v33i01.33015466. L. Yang, K. Lee, R. D. Nowak, and D. Papailiopoulos. Looped transformers are better at learning learning algorithms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview. net/forum?id=HHbRxoDTxE. G. Yu, J. S. Jeong, G. Kim, S. Kim, and B. Chun. Orca: distributed serving system for transformerbased generative models. In M. K. Aguilera and H. Weatherspoon, editors, 16th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022, pages 521538. USENIX Association, 2022. URL https://www.usenix.org/conference/ osdi22/presentation/yu. R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA D. Zeng, N. Du, T. Wang, Y. Xu, T. Lei, Z. Chen, and C. Cui. Learning to skip for language modeling. CoRR, abs/2311.15436, 2023. doi: 10.48550/ARXIV.2311.15436. URL https://doi.org/10. 48550/arXiv.2311.15436. B. Zhang and R. Sennrich. Root mean square layer normalization. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. dAlch√©-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 1236012371, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 1e8a19426224ca89e83cef47f1e7f53b-Abstract.html. J. Zhang, J. Wang, H. Li, L. Shou, K. Chen, G. Chen, and S. Mehrotra. Draft& verify: Lossless large language model acceleration via self-speculative decoding. In L. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1126311282. Association for Computational Linguistics, 2024a. URL https://aclanthology.org/2024. acl-long.607. P. Zhang, G. Zeng, T. Wang, and W. Lu. Tinyllama: An open-source small language model, 2024b. Z. Zhou, X. Ning, K. Hong, T. Fu, J. Xu, S. Li, Y. Lou, L. Wang, Z. Yuan, X. Li, S. Yan, G. Dai, X. Zhang, Y. Dong, and Y. Wang. survey on efficient inference for large language models. CoRR, abs/2404.14294, 2024. doi: 10.48550/ARXIV.2404.14294. URL https://doi.org/10. 48550/arXiv.2404.14294. 24 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA A. Components in Transformer Architecture The Transformer block consists of two core components: multi-head attention (MHA) mechanism and feed-forward network (FFN). MHA utilizes multiple attention heads to capture diverse relationships within the input sequence. The computation within each attention head is formulated as: Attention(Q, K, V) = softmax (cid:19) (cid:18) QKùëá ùëëùëò V, where Q, K, and are linear projections of the input, parameterized by learned weight matrices Wùêæ and then projected back to the original hidden size using learned weight matrix Wùëúùë¢ùë° ‚Ñì ùëÑ ‚Ñì , ‚Ñì , respectively. The outputs from each head of the multi-head attention are concatenated ‚Ñì , and Wùëâ . While the FFN structure typically consists of two linear transformations, in the Gemma model, it deviates from this standard architecture as follows: FFN(x) = Wùëëùëúùë§ùëõ ‚Ñì (GELU(xW ùëîùëéùë°ùëí ‚Ñì ) xW ùë¢ùëù ‚Ñì ) with three learned linear weight matrices and GeGLU activation (Shazeer, 2020). B. Parameter Sharing Strategy Takase and Kiyono (2023) discuss three strategies for partial layer tying in Transformer models, as depicted in Figure B.1. The SEQUENE strategy is the simplest, assigning the same parameters to consecutive layers. The CYCLE strategy repeatedly stacks single block of unique layers to achieve the desired depth. Meanwhile, the CYCLE (REV) strategy stacks the lower layers in reverse order for the remaining layers. We specifically utilized the CYCLE strategy due to its superior compatibility with early-exiting, which can amplify the throughput of Recursive Transformers through continuous depth-wise batching. (a) SEQUENCE (b) CYCLE (c) CYCLE (REV) Figure B.1 Three strategies for parameter sharing (Takase and Kiyono, 2023). The examples utilize models with six layers, where identical colors represent shared weights. Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA C. Illustrative Examples of SVD Initialization in Relaxed Recursive Transformer We propose an SVD initialization approach for LoRA modules within Relaxed Recursive Transformer, effectively steering the summation of base and LoRA weights towards the pretrained weights of their corresponding depth. Figure C.1 illustrates an overview of how the LoRA module is initialized under three different initialization techniques (Stepwise, Average, and Lower) for looped layers. One crucial point is that if the initialized looped layers weights match those of the original pretrained model, its corresponding LoRA module undergoes standard zero initialization: random Gaussian for matrix and zero for B. For example, with the Stepwise method, the first loops LoRA module receives standard zero initialization, while the second loops LoRA is initialized using our proposed initialization. Figure C.1 We visualize LoRA modules to show which residual matrices they target for initialization under three different looping initialization methods, assuming full-size model with six layers and two looping blocks. For ease of understanding, matrices are colored according to the full-size model weights at the corresponding depth, while matrices are colored based on the looped layer weights. White matrices indicate cases where the full-size model and looped model weights are identical, resulting in standard zero initialization. D. Overview of Three Pretrained LLMs We utilized three pretrained modelsGemma 2B (Team et al., 2024), TinyLlama 1.1B (Zhang et al., 2024b), and Pythia 1B (Biderman et al., 2023)and converted them into Recursive Transformers. Their corresponding few-shot performance results are presented in Table D.1. Few-shot Accuracy"
        },
        {
            "title": "Models",
            "content": "N-emb"
        },
        {
            "title": "Dataset",
            "content": "ùëÅùë°ùëúùëòùëíùëõ LD HS PQ WG ARC-e ARC-c OB"
        },
        {
            "title": "Avg",
            "content": "Gemma 2B 1.99B"
        },
        {
            "title": "Unreleased",
            "content": "3T 63.13 71.38 78.13 65.04 72. 41.89 40.20 61.72 TinyLlama 1.1B 0.97B SlimPajama + Starcoderdata 105B 503B 1T 2T 3T 43.26 48.92 53.00 53.33 58.82 42.23 49.56 52.52 54.63 59.20 66.81 69.42 69.91 70.67 73.29 53.35 55.80 55.96 56.83 59.12 44.74 48.32 52.36 54.67 55. 23.21 26.54 27.82 28.07 30.12 29.20 31.40 33.40 33.40 36.00 43.26 47.14 49.28 50.23 53.13 Pythia 1B 0.81B"
        },
        {
            "title": "Pile",
            "content": "300B 57.52 49.10 70.40 52.80 51. 26.71 33.40 48.83 Table D.1 Few-shot performance of three pretrained models. Few-shot accuracy is measured on the LAMBADA, HellaSwag, PIQA, WinoGrande, ARC-easy, ARC-challenge, and OpenBookQA benchmarks. We evaluated intermediate checkpoints up to the fully trained checkpoint for TinyLlama 1.1B. Among these, we utilized the 105B intermediate checkpoint to study an under-trained model. 26 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA This diversity offers several benefits. First, with three versions of recursive models, we can compare their performance based on the number of trainable parameters. Notably, the comparison between the recursive Gemma and the pretrained TinyLlama and Pythia models highlights that leveraging well-trained model weights can lead to superior Recursive Transformer of equivalent size, even with substantially lower uptraining costs. Second, by utilizing models ranging from under-trained (e.g., TinyLlama) to significantly over-trained (e.g., Gemma), we can gain insights into the uptraining costs required for Recursive Transformers to closely match the performance of pretrained models. Finally, the diversity in pretraining datasets allows us to observe how Recursive Transformers perform when faced with distribution shifts in the uptraining dataset. Table 2 presents the evaluation results obtained after uptraining each of the pretrained models. While TinyLlama readily improves its performance due to uptraining on the same dataset, Gemma and Pythia show decline in few-shot performance with SlimPajama uptraining, which can be attributed to the differences in data distribution and the lower quality of the uptraining dataset. E. Experimental Setup Uptraining setting To convert vanilla Transformers into Recursive Transformers, we conducted further uptraining on either 15 billion or 60 billion tokens from the SlimPajama dataset (Soboleva et al., 2023). SlimPajama is an open-source dataset designed for training large language models, which is created by cleaning and deduplicating the RedPajama dataset (Computer, 2023). The source data primarily consists of web-crawled data, along with data from Github, books, Arxiv, Wikipedia, and StackExchange. We employed the HuggingFace training framework (Wolf et al., 2020) and enhanced memory efficiency through the Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) from the DeepSpeed library (Rasley et al., 2020), along with mixed precision training. The context length was set to 2048, and the batch size was approximately 2 million tokens. We used the AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate of 2e-4, utilizing cosine annealing learning rate scheduler (Loshchilov and Hutter, 2017). Additionally, we set warmup steps to 200 for 15 billion token training and 800 for 60 billion token training. Eight H100 GPUs were used for the training. Early-exit training setting Similar to the uptraining process, we used the SlimPajama dataset to enable models to predict next tokens at intermediate loops. Models with two looping blocks underwent additional training on total of two exit points, whereas models with three blocks were trained on three exit points. We explored various strategies, but by default, we continued training on an additional 15 billion tokens (SlimPajama dataset), starting from the uptrained Recursive Transformers. We also utilized eight H100 GPUs and maintained consistent configurations with the uptraining settings, including batch size, context length, and learning rates. Evaluation setting We evaluated perplexity on test sets from three language modeling datasets: SlimPajama, RedPajama, and PG19 (Rae et al., 2019). Additionally, we used the Language Model Evaluation Harness framework (Gao et al., 2023) to evaluate accuracy on seven few-shot tasks: LAMBADA (Paperno et al., 2016), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2020), ARC-easy and ARC-challenge (Clark et al., 2018), and OpenBookQA (Mihaylov et al., 2018). We adhered to the standard number of shots specified by the evaluation framework for each dataset. For few-shot datasets, excluding LAMBADA and WinoGrande, we normalized accuracy by the byte length of the target string. All evaluation performance measurements were conducted using single H100 GPU. Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Throughput measurement settings To present the hypothetical generation speeds of our Recursive Transformers, we prepared two key elements: per-token generation time and exit trajectory datasets. Firstly, we measured the generation time under various model configurations using dummy weights and inputs. We measured the time for each component, such as embedding matrices, Transformer blocks, and the classifier head (final throughput comparisons were based solely on the time spent within Transformer blocks.) We tested two settings of prefix and decoding lengths (512 / 2048 and 64 / 256), calculating the per-token time by dividing the total elapsed time by the decoding length. Using single A100 40GB GPU, we recorded these decoding times across different batch sizes, until an out-of-memory error occurred or under specific memory constraint was reached. To obtain exit trajectory data, we assumed an oracle-exiting approach, where all tokens could exit at intermediate loops if intermediate predictions matched the final loops prediction. Since our models are not finetuned on any specific downstream tasks, we did simulation with three language modeling datasets (SlimPajama, RedPajama, and PG19) as if they were generated by our models. For simplicity, we assumed queue of 20K samples, rather than considering their arrival in static or dynamic time intervals. We then recorded the exit loop of each token in these samples using the oracle-exiting algorithm. With these two measurement (per-token generation time and exit trajectories), we present the hypothetical throughput of Recursive Transformers under various simulation scenarios. F. Expanded Results of Initialization Methods for Looped Layers Ablation study of Stepwise method We initially hypothesized that the Stepwise methods performance could be significantly influenced by the specific rule used for layer selection from the pretrained model. To investigate this, we conducted controlled experiment (illustrated in Figure F.1a), where layers were selected at certain intervals starting from the first layer. We then varied whether the final layer of the pretrained model was included in the initialization or not. While Pythia model showed no significant differences in training loss or few-shot performance, other models like Gemma exhibited superior results when both the first and last layers were preserved. This observation aligns well with prior work suggesting that maintaining the weights of the first and last layers during depth up-scaling for LLMs can yield performance benefits (Kim et al., 2024). Ablation study of Average method The Average initialization method exhibited notably poor performance, particularly when applied to the Gemma model. We hypothesized that this could be attributed to instability in the models learned distribution, potentially arising from averaging of normalization layer weights. To investigate this further, we experimented with three different methods for initializing normalization weights, as outlined in Figure F.1b: averaging weights (Normavg), selecting weights from single layer (Norm-choice), and zero initialization (Norm-zero). The performance trend observed among these methods varied across different model architectures. However, zero initialization of normalization layers resulted in huge performance drop in certain architectures like TinyLlama and Pythia. Conversely, we observed no big difference between averaging and single-layer selection, suggesting that any form of distillation of the normalization weights appears to be sufficient for maintaining performance. 28 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA (a) Stepwise ablations (b) Average ablations Figure F.1 Training loss curves of Stepwise and Average initialization variants across three models with two blocks. (a) Fixed-start indicates that the first layer of the pretrained model is selected initially, and subsequent layers are repeatedly chosen at fixed interval. Fixed-ends means that the first and last layers are included, and intermediate layers are selected at specific step intervals. (b) When initializing the weights of normalization layer (RMSNorm in Gemma and TinyLlama, and LayerNorm in Pythia), we consider whether to average the weights (Norm-avg), select single layers weights (Norm-choice), or use zero initialization (Norm-zero). Overall comparison of training perplexity Figure F.2 presents comparative analysis of training loss across three model architectures and varying looping blocks, incorporating our proposed initialization methodologies. To set an upper bound on performance, we utilized full-size model further uptrained on SlimPajama, accounting for the distribution shift between uptraining and pretraining data. Additionally, we trained Recursive Transformer with random initialization, ensuring its exclusive reliance on the recursive architecture without leveraging any pretrained weights. While some variance was observed across architectures, all proposed methods utilizing pretrained model weights demonstrated significantly superior performance compared to random initialization. Notably, the Stepwise method consistently achieved the best performance across diverse settings. Although the full-size models performance was considerably higher, bridging this gap with only 15 billion tokens of uptraining represents remarkable achievement. (a) Gemma (2 blocks) (b) Gemma (3 blocks) (c) TinyLlama (2 blocks) (d) Pythia (2 blocks) Figure F.2 Training loss curves of Recursive Transformers using various initialization. We omitted separate curve for the full-size TinyLlama model, as we used the original pretrained model as the full-size baseline because both pretraining and uptraining datasets are same as SlimPajama. Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Overall comparison of few-shot performance Few-shot performance exhibited consistent trend with training perplexity. Table F.1 provides comparative summary of the proposed looping initialization methods against the full-size model, the reduced-size model, and Recursive Transformers utilizing random initialization. Moreover, Figure F.3 visually illustrates the performance differences across different few-shot datasets. Notably, the Stepwise method consistently demonstrated the best performance, showing performance improvement of up to 14.1%p compared to random initialization. Uptrain Looping Perplexity Few-shot Accuracy - - - 3 3 3 3 2 2 2 2 Models Gemma N-emb PT ùëÅùë°ùëúùëò Block 1.99B 15B 15B 0.99B 15B 0.66B 0.99B 15B 0.99B 15B 0.99B 15B 15B 0.99B 0.66B 15B 0.66B 15B 0.66B 15B 15B 0.66B 0.97B 15B 15B 0.48B TinyLlama 0.48B 15B 0.48B 15B 0.48B 15B 15B 0.48B 0.81B 15B 15B 0.40B 0.40B 15B 0.40B 15B 0.40B 15B 15B 0.40B Pythia 2 2 2 2 2 2 2 2 - - - - Init SlimP RedP PG19 LD HS PQ WG ARC-e ARC-c OB Avg - - - 10.76 8.47 13.08 63.5 22.63 20.03 32.60 28.9 24.44 21.69 36.03 27.2 68.5 77.0 63.5 31.6 63.1 52.3 30.6 63.8 50.5 67.6 41.2 40.6 38.1 22.5 22.0 42.6 60.1 27.8 38.2 27.0 37.4 Œî - - - Step Avg 12.85 10.29 16.21 53.0 57.3 73.2 56.2 56.1 50.5 15.15 12.57 19.86 43.6 52.2 Lower 15.03 12.46 19.63 42.5 39.7 22.66 20.06 32.86 27.4 Rand 47.4 70.4 52.6 48.0 71.0 54.6 31.6 63.4 50.5 Step Avg 14.75 12.10 19.32 45.0 49.9 69.8 55.8 52.7 46.5 17.45 14.65 23.63 39.4 49.5 Lower 15.96 13.24 20.90 41.9 41.0 22.67 20.09 32.77 28.1 Rand 39.0 66.6 48.7 43.2 70.0 52.6 31.4 63.8 51.1 29.2 36.6 51.7 +14.1 34.4 46.7 +9.1 27.8 33.8 47.1 +9.5 27.7 28.8 37.6 21.9 - 27.9 33.6 47.8 +9.9 31.8 42.4 +4.5 24.7 31.6 45.0 +7.1 26.6 26.6 37.9 23.0 - - - 12.26 9.37 11.94 43.3 16.61 15.66 20.27 22.3 42.2 66.8 53.4 30.0 60.9 50.6 44.7 37.0 23.2 23.0 29.2 43.3 28.0 36.0 - - Step Avg 11.61 11.86 10.29 13.42 38.6 Lower 14.67 12.67 16.68 31.9 16.14 15.11 19.55 24.7 Rand 9.89 13.00 39.6 39.8 66.5 52.9 44.3 42.7 39.1 36.4 39.4 66.1 52.8 32.3 62.6 52.0 30.7 61.2 50.6 - - 13.46 9.95 13.38 55.0 25.69 20.00 32.08 24. 49.0 71.0 53.6 30.0 61.9 50.7 Step Avg 16.38 12.37 17.74 43.4 40.5 67.4 50.8 39.1 68.2 51.9 16.76 12.76 18.63 43.6 Lower 17.04 12.62 18.44 43.9 39.2 66.3 53.4 30.2 62.1 51.1 Rand 24.45 18.93 29.63 25.2 51.8 38.3 46.3 45.4 45.4 39. 24.9 30.6 42.7 +6.2 30.6 42.2 +5.7 25.4 27.8 38.3 +1.8 22.1 29.2 36.5 22.6 - 28.2 22.3 32.8 48.8 26.0 36.2 - - 30.0 43.5 +7.3 25.7 29.8 43.3 +7.1 25.1 25.8 31.2 43.6 +7.4 23.6 36.2 22. - Table F.1 Evaluation results of various initialization methods for looped layers. We indicate whether pretrained weights are used and the number of uptraining tokens. Perplexity is evaluated on test sets of three language modeling datasets, and accuracy is evaluated on seven few-shot benchmarks. Delta values (Œî) show improvements over random initialization. 30 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA (a) Recursive Gemma with 2 blocks (b) Recursive Gemma with 3 blocks (c) Recursive TinyLlama with 2 blocks (d) Recursive Pythia with 2 blocks Figure F.3 Few-shot performance on seven benchmarks and their average accuracy based on four looping initialization methods. Full-size model performance is represented by gray dotted line. 31 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA G. Expanded Results of Relaxed Recursive Transformers Training perplexity changes with LoRA modules Figure G.1 illustrates the changes in training loss after incorporating the layer-wise LoRA modules. The Average and Lower initialization methods, when coupled with our proposed SVD-based initialization of LoRA modules, demonstrated significantly enhanced benefits. In particular, the Relaxed Recursive Transformer employing the Average method consistently outperformed the others. This suggests that it is considerably easier to learn the difference between the original pretrained weights and the averaged looped weights using low-rank matrices. (a) Gemma (b) TinyLlama (c) Pythia Figure G.1 Comparison of training loss for recursive and relaxed recursive models with two blocks. The LoRA rank is set to 512, and the SVD initialization method is used for LoRA modules. Comparison between SVD and zero initialization The utilization of layer-wise LoRA modules enhances model capacity by introducing additional parameters and relaxation, thereby potentially improving performance. As depicted in Figure G.2, SVD initialization significantly amplified these performance gains compared to standard zero initialization. However, an interesting exception was observed with the Stepwise method, where the SVD initialized LoRA module surprisingly led to performance degradation. This appears to be attributed to LoRA ranks being insufficient to adequately approximate the low-rank deltas across layers, resulting in initialization at sub-optimal point. (a) Gemma (b) TinyLlama (c) Pythia Figure G.2 Comparison of average few-shot accuracy between zero and SVD initialization methods across three models. Performance gains due to LoRA relaxation are indicated by hatched bars, while cases where performance is lower than the recursive counterpart (without LoRA modules) are represented by dotted lines. 32 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Ablation study on the LoRA rank values Our proposed SVD initialization ensures that the Relaxed Recursive Transformer can function as an interpolation between vanilla and Recursive Transformers. The approximation accuracy of SVD is directly influenced by the LoRA rank value; higher rank leads to improved restoration of the pretrained model weights. In Figure G.3, we present summary of the performance changes observed in the relaxed models by varying the LoRA ranks. As expected, for the Average and Lower looping initialization methods, larger rank value results in enhanced performance. The Stepwise method, consistent with previous experimental findings, exhibited U-shaped trend: with extremely low or high ranks, clear performance increase results. However, with mid-range values, the approximation becomes less accurate, leading to further performance decrease. (a) Gemma (b) TinyLlama (c) Pythia Figure G.3 Performance comparison with varying LoRA ranks under different initialization methods for looped layers. All LoRA weights are initialized using our proposed SVD initialization method. We further experimented with assigning different ranks to LoRA modules associated with each linear weights. Given the computational overhead inherent to LoRA modules, allocating different ranks to each module can offer better balance between performance and computational efficiency. The experimental results in Table G.1 reveal strong correlation between performance and overall model sizes. Due to the substantial hidden dimension of the linear weights within the FFN layer, reducing its rank led to the most significant performance drop. Conversely, the relatively smaller size of other attention weights resulted in less performance drop. It is intriguing that even minimal relaxation of key-value weights (achieved through small ranks) yielded comparable performance, despite the inherent strong sharing of key-value caches between attention heads in the Multi-Query attention structure (Ainslie et al., 2023). 33 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Uptrain Looping LoRA Perplexity Few-shot Accuracy PQ WG ARC-e ARC-c OB Avg - - - - - - - - - - - - - - 10.76 8.47 13.08 63.5 68.5 77.0 63.5 67.6 22.63 20.03 32.60 28.9 31.6 63.1 52.3 41.2 N-emb PT ùëÅùë°ùëúùëò Block Init KV Out FFN Init SlimP RedP PG19 LD HS 1.99B 15B 0.99B 15B 1.30B 15B 1.28B 15B 1.29B 15B 1.18B 15B 1.27B 15B 1.15B 15B 1.14B 15B 1.14B 15B 1.08B 15B 1.13B 15B 9.71 14.89 58.2 60.7 73.7 59.0 57.6 Avg 256 256 256 256 SVD 12.10 9.81 15.10 57.4 60.2 72.5 58.9 58.1 Avg 128 256 128 256 SVD 12.27 Avg 256 128 256 256 SVD 12.33 9.90 15.25 58.5 59.7 73.3 58.3 56.6 Avg 256 256 256 128 SVD 12.56 10.12 15.59 57.0 58.7 73.0 57.4 57.0 9.92 15.31 57.2 59.2 73.1 57.3 58.0 Avg 128 128 128 256 SVD 12.36 Avg 128 128 128 128 SVD 12.52 10.07 15.51 56.1 58.2 72.3 55.8 57.1 Avg 64 128 64 128 SVD 12.61 10.14 15.69 55.0 57.8 73.0 57.5 56.7 Avg 128 64 128 128 SVD 12.72 10.18 15.76 55.5 57.7 72.7 57.0 56.9 Avg 128 128 128 64 SVD 12.80 10.33 15.97 55.3 56.7 72.9 57.7 55.0 64 128 SVD 12.77 10.29 15.95 55.2 57.4 73.0 56.7 56.5 Avg 2 2 2 2 2 2 2 2 2 2 64 38.1 42.6 60.1 22.5 27.8 38.2 32.1 38.0 54.2 32.6 37.8 53.9 32.0 40.0 54.1 31.6 38.2 53.3 32.2 38.6 53.7 30.7 37.2 52.5 30.9 38.8 52.8 30.1 38.2 52.6 29.6 36.0 51.9 30.5 37.2 52. Table G.1 Evaluation results of relaxed recursive Gemma models with varying LoRA ranks for different Transformer components. We adjusted the LoRA ranks attached to query, key-value, out, and FFN linear weights. Non-embedding parameter sizes include both the base layer parameters and the attached LoRA weights. Overall performance comparison of Relaxed Recursive Transformers comprehensive performance comparison is presented in Table G.2. This encompasses an evaluation of the performance across three models and various looping initialization methods, considering the degree of relaxation induced by the layer-wise LoRA module. The configuration utilizing the Average method for looped layer initialization, paired with SVD initialization for the LoRA module, consistently outperformed all other baselines. Furthermore, clear trend of improved performance was observed with increasing rank. 34 - - - - 12.26 9.37 11.94 43.3 42.2 66.8 53.4 44.7 16.61 15.66 20.27 22.3 30.0 60.9 50.6 37. 23.2 29.2 43.3 23.0 28.0 36.0 - - Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Uptrain Looping LoRA Perplexity Few-shot Accuracy Init Rank Init SlimP RedP PG19 LD HS PQ WG ARC-e ARC-c OB Avg - - - - 10.76 8.47 13.08 63.5 68.5 77.0 63.5 67.6 22.63 20.03 32.60 28.9 31.6 63.1 52.3 41.2 38.1 42.6 60.1 22.5 27.8 38.2 Œî - - - - Step Step Step Step Step Step Avg Avg Avg Avg Avg Avg - - Step Step Step Step Step Step Avg Avg Avg Avg Avg Avg - - Step Step Step Step Step Step Avg Avg Avg Avg Avg Avg - - - - - Models Gemma 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 N-emb PT ùëÅùë°ùëúùëò Block 1.99B 15B 0.99B 15B 0.99B 15B 1.07B 15B 1.15B 15B 1.30B 15B 1.60B 15B 1.60B 15B 0.99B 15B 1.07B 15B 1.15B 15B 1.30B 15B 1.60B 15B 1.60B 15B 0.99B 15B 1.07B 15B 1.15B 15B 1.30B 15B 1.60B 15B 1.60B 15B 0.97B 0.48B 15B 0.48B 15B 0.53B 15B 0.58B 15B 0.68B 15B 0.86B 15B 0.86B 15B TinyLlama 0.48B 15B 0.53B 15B 0.58B 15B 0.68B 15B 0.86B 15B 0.86B 15B 0.48B 15B 0.53B 15B 0.58B 15B 0.68B 15B 0.86B 15B 0.86B 15B 0.81B 15B 0.40B 15B 0.40B 15B 0.44B 15B 0.48B 15B 0.55B 15B 0.70B 15B 0.70B 15B 0.40B 15B 0.44B 15B 0.48B 15B 0.55B 15B 0.70B 15B 0.70B 15B 0.40B 15B 0.44B 15B 0.48B 15B 0.55B 15B 0.70B 15B 0.70B 15B 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 Pythia - - - 12.85 10.29 16.21 53.0 57.3 73.2 56.2 56.1 - 64 SVD 12.76 10.21 15.99 52.1 57.2 73.0 57.8 56.9 128 SVD 13.44 10.80 16.98 50.5 53.0 71.5 54.4 55.9 256 SVD 14.02 11.44 18.09 46.1 49.1 71.8 53.2 52.8 512 SVD 13.13 10.66 16.63 53.0 54.3 72.1 54.9 54.8 9.97 15.58 54.9 58.8 74.0 58.1 58.8 512 Zero 12.46 - 15.15 12.57 19.86 43.6 47.4 70.4 52.6 50.5 - 64 SVD 12.83 10.35 16.02 55.9 56.8 72.5 56.8 55.7 128 SVD 12.52 10.07 15.51 56.1 58.2 72.3 55.8 57.1 9.71 14.89 58.2 60.7 73.7 59.0 57.6 256 SVD 12.10 512 SVD 11.83 9.46 14.57 59.3 62.8 74.0 61.6 60.1 512 Zero 13.78 11.31 17.71 49.8 52.4 71.7 53.3 51.2 - - 64 15.03 12.46 19.63 42.5 48.0 71.0 54.6 52.2 Lower Lower SVD 14.21 11.77 18.40 47.5 50.5 70.9 54.2 54.1 Lower 128 SVD 14.23 11.83 18.49 48.0 50.5 72.0 56.8 54.4 Lower 256 SVD 13.51 11.06 17.30 53.1 53.7 71.8 57.4 52.5 Lower 512 SVD 12.54 10.22 15.90 57.1 58.2 73.7 58.6 57.6 Lower 512 Zero 13.95 11.59 18.02 48.4 52.1 71.9 55.7 54.9 - 11. 9.89 13.00 39.6 39.8 66.5 52.9 44.3 - 64 SVD 12.10 10.40 13.75 38.9 38.3 65.2 51.5 42.0 128 SVD 12.41 10.72 14.10 36.8 37.4 64.7 53.4 42.2 256 SVD 11.96 10.35 13.48 38.9 38.3 65.8 51.9 43.1 9.79 12.61 42.2 40.9 67.7 51.1 45.0 512 SVD 11.33 9.60 12.56 42.0 41.0 67.4 52.2 44.5 512 Zero 11.24 - - 64 SVD 11.22 128 SVD 10.99 256 SVD 10.71 512 SVD 10.46 512 Zero 11.28 11.86 10.29 13.42 38.6 39.4 66.1 52.8 42.7 9.66 12.51 41.8 41.6 67.0 53.3 43.9 9.45 12.21 43.2 42.1 68.3 53.2 44.8 9.18 11.82 44.1 43.2 68.1 53.5 44.4 8.92 11.50 46.0 44.1 68.2 53.0 45.8 9.75 12.69 41.5 41.0 66.8 53.2 44.8 - - 14.67 12.67 16.68 31.9 32.3 62.6 52.0 39.1 Lower Lower SVD 13.68 11.77 15.48 35.5 34.0 63.8 51.0 40.0 Lower 128 SVD 13.00 11.14 14.61 37.6 35.4 65.3 51.5 40.4 Lower 256 SVD 12.14 10.39 13.59 40.0 37.7 66.1 52.6 42.5 Lower 512 SVD 11.31 9.61 12.49 43.2 40.5 66.0 50.8 43.9 Lower 512 Zero 14.56 12.69 16.57 21.2 32.9 63.9 52.6 39.5 - 16.38 12.37 17.74 43.4 40.5 67.4 50.8 46.3 - 64 SVD 16.44 12.44 17.89 43.7 40.4 66.5 52.9 46.5 128 SVD 16.63 12.61 18.35 42.4 39.3 68.0 51.5 46.3 256 SVD 16.54 12.61 18.39 42.8 39.1 67.2 53.7 46.4 512 SVD 15.68 11.88 17.25 45.4 41.3 68.5 52.6 46.7 512 Zero 15.88 12.01 17.16 45.5 41.8 68.0 52.6 46.6 - 16.76 12.76 18.63 43.6 39.1 68.2 51.9 45.4 - 64 SVD 16.03 12.19 17.59 45.8 40.9 67.3 50.0 45.8 128 SVD 15.67 11.93 17.10 46.9 41.9 67.4 51.2 45.4 256 SVD 15.22 11.54 16.47 48.5 43.3 67.2 51.4 46.7 512 SVD 14.70 11.07 15.71 50.2 44.7 68.2 51.6 47.6 512 Zero 15.97 12.14 17.65 45.7 41.5 68.1 51.7 46.5 - - 64 17.04 12.62 18.44 43.9 39.2 66.3 53.4 45.4 Lower Lower SVD 17.03 12.78 18.73 44.1 38.3 66.9 51.9 45.4 Lower 128 SVD 16.63 12.49 18.17 45.2 39.2 66.8 51.0 45.6 Lower 256 SVD 15.93 11.99 17.30 47.6 41.4 68.3 53.2 46.0 Lower 512 SVD 15.11 11.34 16.07 50.2 43.5 67.8 51.8 47.2 Lower 512 Zero 16.45 12.25 17.76 45.2 40.4 66.4 54.5 45.8 - 29.2 36.6 51.7 28.9 36.8 51.8 +0.1 29.3 34.8 49.9 1.8 27.8 33.4 47.8 3.9 28.8 35.4 50.5 1.2 30.6 36.6 53.1 +1.4 - 27.8 34.4 46.7 30.6 36.2 52.1 +5.4 30.7 37.2 52.5 +5.8 32.1 38.0 54.2 +7.5 32.9 37.6 55.5 +8.8 29.4 35.0 49.0 +2. - 27.7 33.8 47.1 29.2 36.0 48.9 +1.8 27.5 33.4 48.9 +1.8 28.7 35.2 50.3 +3.2 31.5 35.6 53.2 +6.1 28.8 34.6 49.5 +2.4 - 24.9 30.6 42.7 26.0 31.0 41.9 0.8 24.7 30.4 41.4 1.3 25.4 29.8 41.9 0.8 25.3 30.2 43.2 +0.5 25.9 31.2 43.4 +0.7 - 25.4 30.6 42.2 24.7 31.2 43.4 +1.2 25.9 30.4 44.0 +1.8 25.7 30.4 44.2 +2.0 25.1 31.2 44.8 +2.6 25.5 31.2 43.4 +1. - 22.1 27.8 38.3 24.6 28.0 39.5 +1.2 24.5 27.6 40.3 +2.0 24.8 30.0 42.0 +3.7 24.8 30.0 42.8 +4.5 22.9 27.8 37.3 1.0 - 25.7 30.0 43.5 26.2 28.8 43.6 +0.1 26.7 30.6 43.5 +0.0 25.9 27.8 43.3 0.2 25.4 31.2 44.4 +0.9 26.3 30.0 44.4 +0.9 - 25.1 29.8 43.3 25.5 31.8 43.9 +0.6 24.8 31.2 44.1 +0.8 25.5 32.0 44.9 +1.6 25.4 31.2 45.6 +2.3 25.7 30.0 44.2 +0. - 25.8 31.2 43.6 24.7 30.8 43.2 0.4 24.9 29.6 43.2 0.4 25.8 31.0 44.8 +1.2 25.2 32.0 45.4 +1.8 25.9 32.6 44.4 +0.8 - - - - 13.46 9.95 13.38 55.0 49.0 71.0 53.6 51.8 25.69 20.00 32.08 24.3 30.0 61.9 50.7 38.3 28.2 32.8 48.8 22.3 26.0 36. - - Table G.2 Overall evaluation results of Relaxed Recursive Transformers. Delta (Œî) represent the accuracy differences between relaxed and non-relaxed models using the same looping initialization. 35 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA H. Relaxation of Parameter Sharing with Prefix Tuning As discussed in 2.3, we employed LoRA modules to relax parameter sharing. However, efficiently computing multiple LoRA modules requires specialized kernels and sequential computations of the base layers and LoRA modules, incurring additional computational costs. Consequently, we explored replacing layer-specific prompts (Liu et al., 2021) as an alternative. In this approach, prompts specific to each layer are integrated as prefix tokens, being utilized to generate layer-wise key and value states for self-attention computation. This approach is significantly more amenable to parallel computation, leading to reduced computational overhead. Table H.1 summarizes performance of the prefix tuning method. Due to the reliance on small learnable prompts, performance gains were limited. Additionally, without mechanism to leverage the original pretrained weights, the performance of prefix tuning was significantly lower (52.1% vs. 47.6% with the Average method in 1.07B model size). While offering parallel computation advantages, further research is needed to enhance its effectiveness. Uptrain Looping Prefix Perplexity Few-shot Accuracy - - 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 Models Gemma N-emb PT ùëÅùë°ùëúùëò Block 1.99B 15B 0.99B 15B 0.99B 15B 1.00B 15B 1.01B 15B 1.03B 15B 1.07B 15B 0.99B 15B 1.00B 15B 1.01B 15B 1.03B 15B 1.07B 15B 0.99B 15B 1.00B 15B 1.01B 15B 1.03B 15B 1.07B 15B 0.97B 0.48B 15B 0.48B 15B 0.49B 15B 0.50B 15B 0.53B 15B 0.57B 15B TinyLlama 0.48B 15B 0.49B 15B 0.50B 15B 0.53B 15B 0.57B 15B 0.48B 15B 0.49B 15B 0.50B 15B 0.53B 15B 0.57B 15B 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 - - - Init Len Size SlimP RedP PG19 LD HS PQ WG ARC-e ARC-c OB Avg - - - - - - 10.76 8.47 13.08 63.5 68.5 77.0 63.5 67.6 22.63 20.03 32.60 28.9 31.6 63.1 52.3 41.2 38.1 42.6 60.1 22.5 27.8 38. Œî - - - 12.85 10.29 16.21 53.0 57.3 73.2 56.2 56.1 - Step 256 9.4M 12.62 10.06 15.80 53.5 58.3 73.9 57.6 57.5 Step Step 512 18.9M 12.67 10.10 15.85 54.1 57.8 73.8 58.4 57.2 Step 1024 37.7M 12.89 10.34 16.22 53.5 57.1 72.4 57.2 56.9 Step 2048 75.5M 12.75 10.21 16.09 55.0 57.3 73.3 58.2 56.8 Avg Avg Avg Avg Avg - 15.15 12.57 19.86 43.6 47.4 70.4 52.6 50.5 - 256 9.4M 14.85 12.31 19.41 46.9 48.3 70.4 52.7 51.4 512 18.9M 15.23 12.64 19.98 44.5 47.2 70.7 54.5 49.5 1024 37.7M 14.60 12.02 18.89 46.9 49.7 71.1 52.3 51.0 2048 75.5M 14.63 12.07 19.03 47.3 49.5 70.8 53.1 50.7 - 15.03 12.46 19.63 42.5 48.0 71.0 54.6 52.2 - Lower 256 9.4M 14.59 12.12 19.02 46.3 49.7 71.5 55.1 52.9 Lower Lower 512 18.9M 14.53 12.03 18.88 45.7 49.8 71.9 56.4 53.6 Lower 1024 37.7M 14.43 11.98 18.74 46.3 50.0 71.9 55.1 54.3 Lower 2048 75.5M 14.79 12.26 19.23 46.1 48.7 71.4 55.4 51.3 - 29.2 36.6 51.7 29.3 35.6 52.2 +0.5 28.7 35.8 52.3 +0.6 28.6 36.8 51.8 +0.1 29.2 37.8 52.5 +0.8 - 27.8 34.4 46.7 27.2 34.0 47.3 +0.6 28.0 33.2 46.8 +0.1 28.6 34.2 47.7 +1.0 28.2 33.4 47.6 +0.9 - 27.7 33.8 47.1 29.0 34.0 48.4 +1.3 29.4 33.2 48.6 +1.5 29.7 33.8 48.7 +1.6 28.2 34.0 47.9 +0.8 - - - - - - 12.26 9.37 11.94 43.3 42.2 66.8 53.4 44.7 16.61 15.66 20.27 22.3 30.0 60.9 50.6 37.0 23.2 29.2 43.3 23.0 28.0 36.0 - - - 11.61 - Step 256 11.5M 11.61 Step Step 512 23.1M 11.61 Step 1024 46.1M 11.60 Step 2048 92.3M 11.58 9.89 13.00 39.6 39.8 66.5 52.9 44.3 9.89 13.00 39.6 39.9 66.5 53.9 44.4 9.89 13.01 39.5 39.9 66.7 53.4 44.1 9.88 13.00 39.7 39.9 66.7 53.0 44.3 9.87 13.01 40.1 39.9 66.8 53.4 44.4 Avg Avg Avg Avg Avg - - 11.86 10.29 13.42 38.6 39.4 66.1 52.8 42.7 256 11.5M 11.86 10.28 13.41 38.5 39.4 66.2 52.5 42.8 512 23.1M 11.86 10.28 13.41 38.1 39.3 66.3 52.2 42.6 1024 46.1M 11.86 10.28 13.42 38.4 39.2 65.7 52.7 42.5 2048 92.3M 11.86 10.28 13.42 38.5 39.5 65.9 52.7 42.4 - - 14.67 12.67 16.68 31.9 32.3 62.6 52.0 39.1 Lower 256 11.5M 14.67 12.67 16.69 31.9 32.4 62.7 51.5 38.9 Lower Lower 512 23.1M 14.67 12.67 16.69 31.9 32.3 62.8 51.7 38.9 Lower 1024 46.1M 14.67 12.67 16.68 31.6 32.3 63.0 51.9 38.9 Lower 2048 92.3M 14.67 12.67 16.67 34.1 32.5 62.8 52.4 38.5 - 24.9 30.6 42.7 25.3 30.6 42.9 +0.2 25.3 29.8 42.7 +0.0 25.1 30.6 42.8 +0.1 24.9 30.0 42.8 +0. - 25.4 30.6 42.2 25.9 30.8 42.3 +0.1 25.6 30.8 42.1 0.1 25.5 31.0 42.1 0.1 25.7 31.0 42.2 +0.0 - 22.1 27.8 38.3 22.3 27.8 38.2 0.1 22.2 27.8 38.2 0.1 22.1 28.0 38.3 +0.0 23.0 27.6 38.7 +0.4 Table H.1 Evaluation results of relaxation through prefix tuning methods. Prefix length denotes the sequence length of trainable vectors used to generate key-value states in each self-attention layer. Non-embedding parameter sizes include the sizes of these trainable prefixes. Delta (Œî) represent the accuracy differences to non-relaxed models using the same looping initialization. Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA I. Expanded Results of Extended Uptraining and Knowledge Distillation Ablation study on individual techniques To further enhance performance through uptraining, we increased the number of uptraining tokens and employed knowledge distillation loss (Hinton et al., 2015; Kim and Rush, 2016). Specifically, we expanded the token number from 15 billion to 60 billion. Furthermore, we designated the teacher model as the full-size model for each architecture, uptrained on 15 billion tokens from the SlimPajama dataset. Given the huge number of uptraining tokens, we adopted an online approach to extract logits from the teacher model. Four loss functions were utilized: forward KL (FKL; Kim and Rush (2016)), reverse KL (RKL; Gu et al. (2024)), JensenShannon divergence (JSD; Agarwal et al. (2024)), and total variation distance (TVD; Wen et al. (2023)). Table I.1 summarizes the controlled experimental results for each method. We finally selected forward KL as the distillation loss function due to its simplicity and superior performance. Especially, we observed performance improvement of 1.7%p attributed to the extended uptraining and up to 1.7%p from the KD loss. This suggests that combining both techniques could yield even greater gains. Uptrain Looping LoRA Perplexity Few-shot Accuracy PQ WG ARC-e ARC-c OB Avg Œî - - - - - - - 2 2 Step Step N-emb PT ùëÅùë°ùëúùëò KD Func Block Init Rank Init SlimP RedP PG19 LD HS 0.99B 15B 0.99B 60B 0.99B 15B 0.99B 15B FKL 0.99B 15B RKL 0.99B 15B JSD 0.99B 15B TVD 1.30B 15B 1.30B 15B FKL 1.30B 15B RKL 1.30B 15B JSD 1.30B 15B TVD Avg 256 SVD 12.10 Avg 256 SVD 11.90 Avg 256 SVD 11.95 Avg 256 SVD 12.09 Avg 256 SVD 12. Step Step Step Step Step 2 2 2 2 2 2 2 2 2 2 - - - - - - - - - - - 12.85 10.29 16.21 53.0 57.3 73.2 56.2 56.1 9.70 14.84 52.5 59.9 74.7 58.5 58.0 12.00 12.85 10.29 16.21 53.0 57.3 73.2 56.2 56.1 12.36 9.85 15.45 56.8 58.6 74.8 58.6 59.1 12.56 10.09 15.80 55.6 58.3 74.3 58.6 58.3 12.60 10.06 15.77 56.1 58.4 73.4 57.0 58.4 9.92 15.52 55.1 58.5 74.0 58.2 58.9 12.47 9.71 14.89 58.2 60.7 73.7 59.0 57.6 9.52 14.63 59.9 62.0 74.1 60.0 58.6 9.62 14.79 60.0 61.6 74.5 60.0 58.1 9.65 14.81 58.1 61.1 73.1 60.8 59.0 9.62 14.78 59.3 61.5 73.9 60.5 59.0 29.2 36.6 51.7 30.3 40.2 53.4 +1.7 - - 29.2 36.6 51.7 29.2 36.6 53.4 +1.7 30.4 37.4 53.3 +1.6 29.8 37.2 52.9 +1.2 29.5 36.8 53.0 +1.3 - 32.1 38.0 54.2 33.2 38.0 55.1 +0.9 32.9 37.8 55.0 +0.8 33.2 38.6 54.8 +0.6 33.0 38.2 55.1 +0.9 Table I.1 Evaluation results of ablation studies related to longer uptraining and knowledge distillation loss. Performance improvements, represented by Delta, were measured for each technique. For the knowledge distillation loss function, we experimented with four options: FKL, RKL, JSD, and TVD. Forward KL was selected as the final configuration due to its simplicity and superior performance. (a) Gemma (b) TinyLlama (c) Pythia Figure I.1 Few-shot performance of three models with extended uptraining and knowledge distillation. Optimal configurations are used for each model size (Stepwise for recursive models and Average for relaxed models). Dotted lines represent the Pareto frontier, showing the optimal trade-offs between model size and performance for each setting. 37 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Overall performance after longer training with distillation loss Figure I.1 and Table I.2 summarize the performance gains achieved by incorporating these advanced training techniques. We consistently observed substantial improvements in few-shot performance across all architectures and with varying numbers of looping blocks. We anticipate that further performance enhancements can be achieved by utilizing superior teacher model and increasing the uptraining cost. Uptrain Looping LoRA Perplexity Few-shot Accuracy LD HS PQ WG ARC-e ARC-c OB Avg 10.58 8.44 12.71 60.3 67.9 76.9 63.5 64.9 15.33 13.04 20.37 42.3 43.0 68.8 53.4 49.4 22.63 20.03 32.60 28.9 31.6 63.1 52.3 41.2 16.79 14.39 22.85 37.5 38.4 68.7 50.4 46.5 24.44 21.69 36.03 27.2 30.6 63.8 50.5 40.6 12.85 10.29 16.21 53.0 57.3 73.2 56.2 56.1 Step 14.75 12.10 19.32 45.0 49.9 69.8 55.8 52.7 Step Avg SVD 12.83 10.35 16.02 55.9 56.8 72.5 56.8 55.7 Avg 128 SVD 12.52 10.07 15.51 56.1 58.2 72.3 55.8 57.1 9.71 14.89 58.2 60.7 73.7 59.0 57.6 Avg 256 SVD 12.10 9.46 14.57 59.3 62.8 74.0 61.6 60.1 Avg 512 SVD 11.83 - - - - - - - - - - - - - - - - - - - - - - - - - 64 - 3 3 3 3 3 2 2 2 2 2 Models Gemma - - 64 2 3 2 2 2 12.27 Step Avg SVD 12.13 Avg 128 SVD 11.83 Avg 256 SVD 11.43 Avg 512 SVD 11.01 11.44 Step Avg SVD 11.36 Avg 128 SVD 11.25 Avg 256 SVD 11.05 Avg 512 SVD 10.81 N-emb PT ùëÅùë°ùëúùëò KD Block Init Rank Init SlimP RedP PG19 1.99B 60B 0.99B 60B 0.99B 15B 0.66B 60B 0.66B 15B 0.99B 15B 0.66B 15B 1.07B 15B 1.15B 15B 1.30B 15B 1.60B 15B 0.99B 60B 1.07B 60B 1.15B 60B 1.30B 60B 1.60B 60B 0.66B 60B 0.74B 60B 0.82B 60B 0.97B 60B 1.27B 60B 0.97B - - 0.48B 60B 0.48B 15B 0.48B 15B 0.53B 15B 0.58B 15B TinyLlama 0.68B 15B 0.86B 15B 0.48B 60B 0.53B 60B 0.58B 60B 0.68B 60B 0.86B 60B 0.81B 60B 0.40B 60B 0.40B 15B 0.40B 15B 0.44B 15B 0.48B 15B 0.55B 15B 0.70B 15B 0.40B 60B 0.44B 60B 0.48B 60B 0.55B 60B 0.70B 60B 11.61 Step Avg SVD 11.22 Avg 128 SVD 10.99 Avg 256 SVD 10.71 Avg 512 SVD 10.46 10.51 Step Avg SVD 10.14 Avg 128 SVD 10.07 Avg 256 SVD 9.96 Avg 512 SVD 9.85 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 Pythia - 64 - - 64 - 64 - - - - - - - - - - - - - - - - - - - - - - - - - - - - 9.14 13.98 56.5 62.1 75.2 59.4 59.8 9.14 13.82 58.9 62.8 75.1 61.5 61.2 9.04 13.64 58.7 63.6 76.5 61.2 62.6 8.88 13.35 60.6 64.7 75.3 62.5 61.6 8.63 12.94 61.4 65.8 76.3 63.5 65.1 9.90 15.24 55.6 58.1 73.1 60.2 58.8 9.80 14.95 55.5 58.3 73.5 60.1 58.0 9.53 14.51 56.7 60.2 74.2 59.8 59.1 9.17 13.87 59.3 62.6 74.7 61.2 61.6 8.80 13.25 61.5 64.9 76.2 62.0 64.3 12.26 9.37 11.94 43.3 42.2 66.8 53.4 44.7 11.93 10.86 13.93 33.3 37.3 66.8 50.1 41.7 16.61 15.66 20.27 22.3 30.0 60.9 50.6 37.0 9.89 13.00 39.6 39.8 66.5 52.9 44.3 9.66 12.51 41.8 41.6 67.0 53.3 43.9 9.45 12.21 43.2 42.1 68.3 53.2 44.8 9.18 11.82 44.1 43.2 68.1 53.5 44.4 8.92 11.50 46.0 44.1 68.2 53.0 45. 9.01 11.60 44.2 43.1 68.2 52.4 44.7 8.77 11.19 44.3 44.9 69.5 52.5 46.5 8.68 11.07 45.9 45.1 69.4 50.5 46.8 8.56 10.93 46.2 45.7 69.0 53.2 47.9 8.44 10.76 47.4 46.3 69.7 52.8 47.5 9.76 13.57 53.0 50.2 71.1 54.8 51.9 12.83 18.27 14.39 21.93 32.1 35.0 66.1 49.6 42.9 25.69 20.00 32.08 24.3 30.0 61.9 50.7 38.3 16.38 12.37 17.74 43.4 40.5 67.4 50.8 46.3 Step Avg SVD 16.03 12.19 17.59 45.8 40.9 67.3 50.0 45.8 Avg 128 SVD 15.67 11.93 17.10 46.9 41.9 67.4 51.2 45.4 Avg 256 SVD 15.22 11.54 16.47 48.5 43.3 67.2 51.4 46.7 Avg 512 SVD 14.70 11.07 15.71 50.2 44.7 68.2 51.6 47.6 14.59 11.13 15.79 47.8 43.8 69.3 52.0 48.1 Step SVD 14.24 10.89 15.52 50.0 44.5 68.9 54.1 48.0 Avg Avg 128 SVD 14.10 10.79 15.27 50.1 45.5 69.0 52.6 48.3 Avg 256 SVD 13.91 10.61 14.91 50.5 45.6 68.7 51.2 48.4 Avg 512 SVD 13.59 10.38 14.43 52.0 47.0 69.6 53.4 48.9 37.2 39.6 58.6 26.3 31.0 44.9 22.5 27.8 38.2 24.6 31.6 42.5 22.0 27.0 37.4 29.2 36.6 51.7 27.9 33.6 47.8 30.6 36.2 52.1 30.7 37.2 52.5 32.1 38.0 54.2 32.9 37.6 55. Œî - - - - - - - - - - - 32.5 38.6 54.9 +3.2 33.7 37.6 55.8 +3.7 34.6 39.0 56.6 +4.1 35.3 38.8 57.0 +2.8 37.1 39.4 58.4 +2.9 30.2 37.2 53.3 +5.5 31.1 36.8 53.3 33.0 35.4 54.1 32.9 40.2 56.1 35.6 39.2 57.7 - - - - 23.2 29.2 43.3 23.9 30.2 40.5 23.0 28.0 36.0 24.9 30.6 42.7 24.7 31.2 43.4 25.9 30.4 44.0 25.7 30.4 44.2 25.1 31.2 44.8 - - - - - - - - 25.3 32.2 44.3 +1.6 26.1 31.6 45.1 +1.6 25.4 31.6 45.0 +1.0 25.9 31.6 45.6 +1.4 26.3 31.4 45.9 +1.1 27.7 31.6 48.6 24.2 27.0 39.5 22.3 26.0 36. 25.7 30.0 43.5 25.5 31.8 43.9 24.8 31.2 44.1 25.5 32.0 44.9 25.4 31.2 45.6 - - - - - - - - 25.4 30.4 45.2 +1.7 26.5 31.2 46.2 +2.3 25.8 32.0 46.2 +2.1 25.7 32.8 46.1 +1.2 26.9 31.2 47.0 +1.4 Table I.2 Evaluation results of our Recursive Transformers with 60 billion token uptraining and knowledge distillation loss. We utilized the forward KL loss as the knowledge distillation loss function. Full-size model baselines for Gemma and Pythia are the pretrained models further uptrained on 60 billion tokens, accounting for distribution shifts between Slimapajama and their pretraining datasets. Delta (Œî) represents the accuracy differences between the longer uptrained models with KD and their 15 billion uptrained counterparts. Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA J. Expanded Results of Early-Exit Training Ablation study on early-exit training strategy To enable early-exiting capabilities, all models require additional training to align intermediate representations with classifier heads. In this study, we conduct ablation studies on various strategies, demonstrating Recursive Transformers can be transformed into early-exiting models without compromising final loop outputs performance. Table J.1 presents comprehensive summary of experimental results across various categories, including training procedures, loss functions, and early-exit training data. Our key findings are as follows: Post-training after uptraining is essential for preserving final loop performance. Jointly training intermediate loop output during the uptraining phase (co-training) significantly degraded the final output performance, even with an aggressive loss coefficient strategy. While freezing all parameters, we attempted to train intermediate loop outputs by attaching trainable LoRA modules to frozen classifier head. However, we found that this proved ineffective. The aggressive coefficient strategy successfully maintained final loop output performance while enhancing intermediate output performance. Moreover, incorporating knowledge distillation from detached final outputs further enhanced intermediate performance. No significant performance differences were observed when using the overlapped uptraining dataset versus new SlimPajama tokens for post-training. Finally, we opted to perform post-training with new tokens sourced from the same SlimPajama dataset. Moreover, we incorporated distillation loss from the final loop output, while aggressively reducing the loss coefficients of intermediate outputs. Early-exit training results on final models We applied the aggressive coefficient strategy with distillation loss to our final models (uptrained on 60 billion tokens with knowledge distillation). Tables J.2 and J.3 summarize the performance of intermediate loops and the final loop across three models. For fair comparison, the full-size models (Gemma and Pythia) were also uptrained with 60 billion tokens and then post-trained with 15 billion tokens. Consistent with previous findings, the aggressive coefficient strategy yielded the best performance across both intermediate and final outputs. However, we find that intermediate loop outputs in LoRA-relaxed models underperformed their non-relaxed counterparts (recursive models). This could potentially reduce throughput gain, as early loop performance directly influences the number of tokens eligible for early-exit. In perfectly tied looping blocks, intermediate outputs seem to be distilled from the last, as all gradients are backpropagated to the same parameters. Conversely, since LoRA modules allow each layer to specialize based on its depth, intermediate representations appear to be optimized to facilitate performance of the final output, not for their own sake. Hence, relaxation introduces trade-off between final performance and early-exiting benefits. As the optimal strategy derived from the non-relaxed models was directly applied to the relaxed models, further exploration of optimal strategies specifically for relaxed models is left for future work. 39 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Uptrain Looping Early-Exit Train Perplexity Few-shot Accuracy N-emb PT ùëÅùë°ùëúùëò Block Init Train Freeze ùëÅùë°ùëúùëò 1.99B 15B 0.99B 15B 0.99B 15B - - Step - - 2 - - - - - - - - - CE KD Data SlimP RedP PG19 LD HS PQ WG ARC-e ARC-c OB Avg - - - - - - - - - 10.76 8.47 13.08 63.5 68.5 77.0 63.5 67.6 22.63 20.03 32.60 28.9 31.6 63.1 52.3 41.2 12.85 10.29 16.21 53.0 57.3 73.2 56.2 56.1 38.1 42.6 60.1 22.5 27.8 38.2 29.2 36.6 51. Œî - - - 0.99B 15B 0.99B 15B 0.99B 15B 0.99B 15B 0.99B 15B 0.99B 15B 0.99B 15B 0.99B 15B 0.99B 15B 0.99B 15B 0.99B 15B 2 2 2 2 2 2 2 2 2 Step PostStep PostStep PostStep PostStep PostStep PostStep PostStep PostStep PostStep CoStep Post15B Weighted Ovlp 12.97 10.51 16.55 48.9 55.5 72.7 55.3 54.9 13.11 10.59 16.71 49.5 54.8 72.0 53.4 54.1 30.1 36.0 50.5 1.2 29.1 35.6 49.8 - 15B Agg (0.3) Ovlp 12.60 10.21 15.75 51.8 58.2 73.7 56.8 57.0 13.63 11.02 17.55 47.5 53.0 71.2 54.9 50. 29.9 37.8 52.2 +0.5 28.2 34.8 48.5 - 15B Agg (0.1) Ovlp 12.37 9.94 15.37 53.0 59.1 73.9 55.4 57.4 14.55 11.87 19.00 45.9 51.2 71.4 54.5 48.1 30.6 37.8 52.5 +0.8 26.8 32.0 47. - 15B Agg (0.05) Ovlp 9.90 15.31 52.8 59.2 73.6 57.5 57.7 12.33 15.70 12.93 20.69 43.1 49.8 69.8 55.2 46.0 30.5 37.2 52.6 +0.9 26.9 31.2 46.0 - 15B Agg (0.01) Ovlp 12.28 9.80 15.23 52.9 59.5 73.3 56.5 57.2 22.76 20.37 30.39 32.2 45.2 67.5 53.9 40.3 30.1 37.2 52.4 +0.7 26.3 29.2 42.1 - 15B Weighted Ovlp 13.04 10.57 16.66 47.7 55.1 73.2 55.6 54.5 13.04 10.54 16.66 48.3 54.9 72.1 55.9 54.3 29.1 37.2 50.4 1.3 28.4 35.4 49. - 15B Agg (0.1) Ovlp 12.40 9.97 15.42 52.9 58.9 73.7 55.7 57.5 14.11 11.47 18.32 46.3 52.1 71.6 55.3 49.2 31.1 38.2 52.6 +0.9 28.5 32.6 48.0 - 15B Standard Ovlp 12.85 10.29 16.21 53.0 57.3 73.2 56.2 56.1 43.74 41.63 56.78 5.3 37.9 61.4 52.6 35.3 29.2 36.6 51.7 +0.0 24.0 29.0 35.0 - 15B Standard Ovlp 12.85 10.29 16.21 53.0 57.3 73.2 56.2 56.1 43.09 39.97 55.37 5.6 37.7 62.5 52.7 34. 29.2 36.6 51.7 +0.0 24.7 29.2 35.3 - 15B Agg (0.1) Ovlp 13.24 10.67 16.98 50.1 54.2 72.2 53.7 54.7 13.59 10.89 17.42 50.6 52.7 71.2 54.4 53.0 28.9 37.4 50.2 1.5 27.5 35.0 49. - 15B Agg (0.1) New 12.34 9.92 15.31 52.3 59.0 73.8 57.6 55.5 14.49 11.86 18.89 43.9 51.3 71.0 54.9 48.1 30.4 37.2 52.3 +0.6 27.5 31.4 46.9 - Table J.1 Ablation studies on early-exit training for recursive Gemma models. We evaluated performance in static-exiting scenario (Bae et al., 2023; Schuster et al., 2022), where all tokens exit at either first or second iteration loops (9th or 18th depths). We explored post-training (after uptraining) and co-training (during uptraining) approaches. Moreover, we explored freezing uptrained weights and adding LoRA with the rank of 128 to the classifier head. Different coefficient values were tested for the aggressive CE loss function. Early-exit training utilized 15 billion tokens, either overlapping with uptraining data or entirely new. Delta (Œî) indicates the performance changes of the final loop outputs. We highlight the final configuration: post-training with aggressive CE and KD loss on 15 billion new tokens. 40 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Uptrain Looping LoRA Early-exit Train Perplexity Few-shot Accuracy - - - - N-emb PT ùëÅùë°ùëúùëò KD Block Init Rank Init ùëÅùë°ùëúùëò 1.99B 60B 1.99B 75B 0.99B 60B 1.07B 60B 1.15B 60B 1.30B 60B 1.60B 60B - Step Avg SVD - Avg 128 SVD - Avg 256 SVD - Avg 512 SVD - 2 2 2 2 2 - 64 - - - - - - - CE KD SlimP RedP PG19 LD HS PQ WG ARC-e ARC-c OB Avg Œî - - - - - - - - - - - - - - 10.58 11.03 11.44 11.36 11.25 11.05 10.81 8.44 12.71 60.3 67.9 76.9 63.5 8.88 13.33 57.0 65.9 76.2 63. 64.9 63.0 37.2 39.6 58.6 35.9 38.8 57.3 1.3 - 59.8 9.14 13.98 56.5 62.1 75.2 59.4 61.2 9.14 13.82 58.9 62.8 75.1 61.5 62.6 9.04 13.64 58.7 63.6 76.5 61.2 8.88 13.35 60.6 64.7 75.3 62.5 61.6 8.63 12.94 61.4 65.8 76.3 63.5 65.1 32.5 38.6 54.9 33.7 37.6 55.8 34.6 39.0 56.6 35.3 38.8 57.0 37.1 39.4 58.4 - - - - - 0.99B 60B 1.07B 60B 1.15B 60B 1.30B 60B 1.60B 60B 1.07B 60B 1.15B 60B 1.30B 60B 1.60B 60B 0.66B 60B 0.74B 60B 0.82B 60B 0.97B 60B 1.27B 60B 0.66B 60B 0.74B 60B 0.82B 60B 0.97B 60B 1.27B 60B 0.74B 60B 0.82B 60B 0.97B 60B 1.27B 60B 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 Step - - 15B Agg (0.1) Avg 64 SVD 15B Agg (0.1) Avg 128 SVD 15B Agg (0.1) Avg 256 SVD 15B Agg (0.1) Avg 512 SVD 15B Agg (0.1) Avg 64 SVD 15B Agg (0.3) Avg 128 SVD 15B Agg (0.3) Avg 256 SVD 15B Agg (0.3) Avg 512 SVD 15B Agg (0.3) 11.71 9.56 14.46 54.0 61.7 75.1 58.9 13.68 11.39 17.60 45.0 54.1 71.9 58.5 11.79 9.70 14.52 53.7 60.8 73.6 61.1 19.45 16.46 26.10 30.7 37.9 66.5 55.3 9.59 14.32 53.3 62.1 74.9 60.0 11.66 19.65 16.77 26.44 29.7 37.7 66.8 52.6 11.47 9.39 14.03 54.9 63.0 74.5 61.7 19.67 16.82 26.40 29.7 38.3 66.4 53.1 11.20 9.14 13.58 57.2 64.1 75.2 61.7 19.29 16.47 25.73 32.0 39.6 67.6 53.3 12.11 9.98 14.97 52.6 59.8 74.4 59.4 16.09 13.54 21.19 35.4 42.8 69.8 52. 11.96 9.87 14.76 52.3 60.5 74.2 59.1 16.28 13.77 21.45 35.2 42.1 69.8 53.5 9.63 14.43 54.3 61.4 75.0 60.7 11.73 16.41 13.89 21.68 35.6 42.3 69.0 52.7 11.47 9.36 13.93 56.2 62.7 75.4 60.9 16.24 13.72 21.42 37.8 43.6 69.0 54.4 - - 64 - Step SVD - Avg Avg 128 SVD - Avg 256 SVD - Avg 512 SVD - - - - - - - - - - - 12.27 12.13 11.83 11.43 11.01 9.90 15.24 55.6 58.1 73.1 60.2 9.80 14.95 55.5 58.3 73.5 60.1 9.53 14.51 56.7 60.2 74.2 59.8 9.17 13.87 59.3 62.6 74.7 61.2 8.80 13.25 61.5 64.9 76.2 62.0 Step - - 15B Agg (0.1) Avg 64 SVD 15B Agg (0.1) Avg 128 SVD 15B Agg (0.1) Avg 256 SVD 15B Agg (0.1) Avg 512 SVD 15B Agg (0.1) Avg 64 SVD 15B Agg (0.3) Avg 128 SVD 15B Agg (0.3) Avg 256 SVD 15B Agg (0.3) Avg 512 SVD 15B Agg (0.3) 12.75 10.48 16.01 50.2 57.0 72.7 58.6 13.81 11.47 17.80 48.4 53.0 72.4 55.6 16.72 14.23 22.97 37.7 44.2 69.8 53.6 12.64 10.43 15.81 51.4 56.3 72.2 57.9 19.90 16.88 26.26 30.4 39.3 66.3 54.1 26.31 22.49 36.10 20.9 31.2 62.6 50.8 12.37 10.21 15.38 52.0 58.0 72.0 56.5 20.07 17.09 26.47 30.9 40.5 66.3 55.4 26.15 22.46 35.98 21.3 31.2 62.7 51.8 11.92 9.78 14.71 54.8 60.6 74.6 60.1 19.29 16.49 25.51 35.2 42.5 65.8 55.6 25.12 21.53 34.53 23.1 32.0 63.2 49. 9.38 14.00 56.1 62.7 74.4 60.5 11.49 18.52 15.79 24.34 36.7 44.9 67.2 55.3 24.19 20.70 33.20 24.4 32.4 63.9 50.8 13.07 10.84 16.49 47.7 54.4 71.7 56.1 16.68 14.08 21.86 35.4 42.4 68.2 53.8 21.43 18.26 29.12 24.4 34.1 64.3 50.5 12.71 10.54 15.92 50.4 55.9 73.1 57.5 16.90 14.32 22.18 37.6 43.5 67.6 54.5 21.23 18.13 28.88 25.3 34.0 64.6 51.7 12.26 10.15 15.23 53.5 58.5 73.5 58.8 16.56 14.09 21.68 42.6 45.1 68.2 57.7 20.78 17.72 28.29 27.9 34.3 66.3 52.2 11.80 9.68 14.45 54.1 61.2 74.0 59.0 16.02 13.53 20.86 43.5 47.5 68.3 56.2 20.20 17.21 27.50 28.9 35.2 65.6 52.9 58.6 49. 58.7 42.2 59.9 41.4 60.5 43.8 62.1 43.2 57.6 45.8 58.9 46. 58.8 46.8 60.4 45.5 58.8 58.0 59.1 61.6 64.3 56.7 51.6 44.2 56.7 41.2 37.2 58.4 40.8 36. 60.1 41.5 36.1 62.1 43.8 37.9 55.9 44.6 40.7 56.8 45.0 40.7 58.3 45.7 39.7 59.9 47.1 41. 31.9 37.6 54.0 0.9 28.8 33.4 48.8 - 32.9 37.2 54.0 1.8 25.3 27.6 40.8 - 33.4 38.8 54.6 2.0 25.3 28.0 40.2 - 33.1 38.4 55.2 1.8 24.7 27.6 40.5 - 34.6 38.2 56.2 2.2 25.8 30.2 41.7 - 31.1 37.0 53.1 2.7 25.8 31.0 43.3 - 33.0 37.2 53.6 3.0 25.8 31.2 43.4 - 33.1 38.6 54.6 2.4 26.4 29.8 43.2 - 34.0 37.0 55.2 3.2 26.4 31.2 44.0 - 30.2 37.2 53.3 31.1 36.8 53.3 33.0 35.4 54.1 32.9 40.2 56.1 35.6 39.2 57.7 - - - - - 30.0 38.2 51.9 1.4 27.2 35.2 49.0 24.6 30.2 43.5 - - 30.4 35.0 51.4 1.9 24.8 29.2 40.8 22.0 28.0 36.1 - - 30.0 35.2 51.7 2.4 24.4 29.6 41.1 22.9 26.2 36.1 - - 31.8 36.6 54.1 2.0 25.6 29.4 42.2 23.0 25.2 36.1 - - 34.9 38.8 55.7 3.0 26.0 30.4 43.5 21.9 27.4 37.0 - - 29.4 35.2 50.1 3.2 26.3 29.4 42.9 22.3 27.8 37.7 - - 30.1 34.8 51.2 2.9 25.3 29.0 43.2 23.0 26.4 38.0 - - 30.6 37.6 53.0 3.1 25.9 28.8 44.8 23.6 26.8 38.7 - - 32.9 38.0 54.1 3.6 27.0 30.4 45.7 23.2 26.6 39.2 - - Table J.2 Evaluation results of Gemma models after early-exit training. Delta (Œî) represent the accuracy changes in original last loop outputs after early-exit post-training. These changes should be compared in reference to the performance drops observed in 75B and 60B uptraining for the full-size model. The relaxed model with three blocks shows more significant performance drop because KD loss (from final loop output) could not be utilized due to out-of-memory issues. 41 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Uptrain Looping LoRA Early-exit Train Perplexity Few-shot Accuracy Models - - N-emb PT ùëÅùë°ùëúùëò KD Block Init Rank Init ùëÅùë°ùëúùëò 0.97B - 0.48B 60B 0.53B 60B 0.58B 60B 0.68B 60B 0.86B 60B - Step Avg SVD - Avg 128 SVD - Avg 256 SVD - Avg 512 SVD - 2 2 2 2 - 64 - - - - CE KD SlimP RedP PG19 LD HS PQ WG ARC-e ARC-c OB Avg - - - - - - - - - - - - 12.26 9.37 11.94 43.3 42.2 66.8 53.4 44.7 23.2 29.2 43.3 10.51 10.14 10.07 9.96 9.85 9.01 11.60 44.2 43.1 68.2 52.4 44.7 8.77 11.19 44.3 44.9 69.5 52.5 46.5 8.68 11.07 45.9 45.1 69.4 50.5 46.8 8.56 10.93 46.2 45.7 69.0 53.2 47.9 8.44 10.76 47.4 46.3 69.7 52.8 47.5 25.3 32.2 44.3 26.1 31.6 45.0 25.4 31.6 45.0 25.9 31.6 45.6 26.3 31.4 45. Œî - - - - - - 0.48B 60B 0.53B 60B 0.58B 60B TinyLlama 0.68B 60B 0.86B 60B 0.53B 60B 0.58B 60B 0.68B 60B 0.86B 60B 0.81B 60B 0.81B 75B 0.40B 60B 0.44B 60B 0.48B 60B 0.55B 60B 0.70B 60B 0.40B 60B 0.44B 60B 0.48B 60B Pythia 0.55B 60B 0.70B 60B 0.44B 60B 0.48B 60B 0.55B 60B 0.70B 60B 2 2 2 2 2 2 2 2 - - 2 2 2 2 2 2 2 2 2 2 2 2 Step - - 15B Agg (0.1) 10.55 9.16 11.68 45.0 43.7 68.9 53.4 44.8 12.28 10.62 13.83 38.2 39.4 65.8 52.3 41.5 25.3 32.2 44.8 + 0.5 24.7 30.6 41.8 - Avg 64 SVD 15B Agg (0.1) 10.34 9.08 11.50 43.4 44.8 69.5 53.4 46.9 21.23 18.63 24.85 16.8 29.0 57.6 48.9 33.2 25.6 32.0 45.1 + 0.1 23.1 27.0 33.7 - Avg 128 SVD 15B Agg (0.1) 8.97 11.36 45.2 45.5 68.8 54.0 46.5 10.25 21.30 18.56 24.75 18.5 28.9 58.4 48.0 34.1 25.0 31.6 45.2 + 0.2 21.8 27.4 33. - Avg 256 SVD 15B Agg (0.1) 10.13 8.84 11.23 45.2 45.9 69.6 53.6 46.9 20.95 18.16 24.22 20.1 28.8 57.8 48.9 33.8 25.9 32.0 45.6 + 0.0 22.5 25.0 33.9 - Avg 512 SVD 15B Agg (0.1) 10.02 8.74 11.04 46.6 46.5 68.6 54.5 47.9 20.38 17.70 23.57 19.9 28.8 58.2 49.0 34.7 26.3 32.2 46.1 + 0.2 22.8 25.8 34.2 - Avg 64 SVD 15B Agg (0.3) 10.61 9.36 11.87 42.1 43.7 68.6 54.1 46.1 16.83 14.88 19.77 22.0 30.3 60.7 50.7 36.9 26.0 31.2 44.6 0.4 24.1 27.8 36.1 - Avg 128 SVD 15B Agg (0.3) 10.50 9.22 11.71 44.2 44.2 69.2 53.0 46.0 17.10 15.03 19.99 23.5 30.1 60.8 51.3 36.5 25.5 31.2 44.8 0.2 23.8 26.4 36. - Avg 256 SVD 15B Agg (0.3) 10.34 9.07 11.51 44.0 45.0 68.4 53.0 45.8 17.06 14.92 19.82 24.2 30.4 59.9 51.7 36.2 26.0 31.2 44.8 0.8 23.9 27.2 36.2 - Avg 512 SVD 15B Agg (0.3) 10.21 8.94 11.28 45.1 45.8 69.3 54.5 46.7 16.76 14.68 19.43 24.4 30.0 61.1 51.9 37.1 25.9 33.4 45.8 0.1 22.9 28.2 36.5 - - - - - - - - - - - 64 - Step Avg SVD - Avg 128 SVD - Avg 256 SVD - Avg 512 SVD - - - - - - - - - - - - - - - 12.83 12.86 9.76 13.57 53.0 50.2 71.1 54.8 51.9 9.86 13.74 54.8 50.3 70.5 55.3 52.2 27.7 31.6 48.6 28.8 33.0 49.3 + 0.7 - 14.59 11.13 15.79 47.8 43.8 69.3 52.0 48.1 14.24 10.89 15.52 50.0 44.5 68.9 54.1 48.0 14.10 10.79 15.27 50.1 45.5 69.0 52.6 48.3 13.91 10.61 14.91 50.5 45.6 68.7 51.2 48.4 13.59 10.38 14.43 52.0 47.0 69.6 53.4 48.9 25.4 30.4 45.2 26.5 31.2 46.2 25.8 32.0 46.2 25.7 32.8 46.1 26.9 31.2 47.0 - - - - - Step - - 15B Agg (0.1) 14.72 11.38 16.31 47.0 44.2 69.2 53.4 48.6 18.61 14.11 20.96 38.4 38.1 67.0 53.7 43.3 24.7 30.4 45.4 + 0.2 24.4 29.0 42.0 - Avg SVD 15B Agg (0.1) 14.49 11.22 16.12 49.1 43.9 69.8 53.8 48.6 24.43 18.19 27.89 26.7 31.6 61.6 50.8 38.2 26.1 31.2 46.1 0.1 22.9 27.6 37.1 - Avg 128 SVD 15B Agg (0.1) 14.35 11.17 15.93 50.1 44.7 69.0 52.1 49.9 24.33 18.09 27.96 28.2 32.3 61.1 53.0 38. 25.3 32.6 46.2 + 0.0 23.7 27.4 37.8 - Avg 256 SVD 15B Agg (0.1) 14.14 10.96 15.54 50.8 45.5 68.2 53.9 48.8 24.18 17.87 27.48 28.1 32.3 61.9 54.1 38.1 25.3 32.8 46.5 + 0.4 22.9 28.6 38.0 - Avg 512 SVD 15B Agg (0.1) 13.81 10.72 15.11 52.4 47.0 69.3 52.7 50.1 23.50 17.49 26.72 29.5 32.8 63.2 52.3 38.8 26.9 32.0 47.2 + 0.2 22.8 27.8 38.2 - Avg SVD 15B Agg (0.3) 14.87 11.53 16.61 47.0 43.1 68.7 53.0 47.4 20.62 15.60 23.57 32.6 33.6 63.4 51.2 40.7 25.7 31.0 45.1 0.9 23.3 28.0 39.0 - Avg 128 SVD 15B Agg (0.3) 14.69 11.46 16.36 48.9 43.8 68.4 53.0 49.1 20.60 15.56 23.63 33.2 33.6 62.7 51.1 41. 26.2 31.6 45.9 0.3 23.6 27.8 39.0 - Avg 256 SVD 15B Agg (0.3) 14.44 11.20 15.94 50.0 44.7 69.2 52.3 48.1 20.61 15.48 23.45 33.3 34.2 63.4 52.2 40.8 25.4 32.2 46.0 0.1 23.0 28.8 39.4 - Avg 512 SVD 15B Agg (0.3) 14.08 10.94 15.44 51.1 46.4 68.7 52.2 50.0 20.20 15.25 22.98 34.6 34.1 63.5 53.0 41.5 26.9 31.6 46.7 0.3 23.6 27.6 39.7 - Table J.3 Evaluation results of TinyLlama and Pythia models after early-exit training. Delta (Œî) represents the accuracy change in the original last loop outputs after early-exit post-training. Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA K. Expanded Results of Hypothetical Generation Speedup Measuring the average per-token generation time First, we measured the generation time with various model configurations using dummy weights and inputs. We measured the elapsed time for each components, such as embedding matrices, Transformer blocks, and the classifier head. Especially, we calculated the time per token by dividing the total time by the decoding length. Default prefix and decoding lengths are set to 512 and 2048, but we also used shorter context lengths, like 64 and 256 to simulate scenarios where parameter memory sizes become limiting. We measured decoding speed using FlashDecoding (Dao et al., 2022), technique that has recently become standard in serving LLMs. Using single A100 40GB GPU, we measured generation times by increasing batch sizes until an out-of-memory error occurred or memory usuage reached the predefined limit. In Table K.1, generation time was measured up to the maximum batch size that single A100 GPU could accommodate before encountering out-of-memory errors, with prefix and decoding lengths set to 512 and 2048, respectively. Meanwhile, Table K.3 presents generation times measured in more memory-constrained deployment scenario, where the prefix and decoding lengths were reduced to 64 and 256, and the memory limit was set to 16GB. As anticipated, under severe memory constraints, the reduced parameter memory footprint of Recursive Transformers enabled substantially larger batch sizes. This observation indicates that Recursive Transformers, even without continuous batching techniques, can achieve higher throughput than vanilla Transformers due to their memory efficiency. When comparing the speed of the three models, Gemma 2B was the fastest, followed by TinyLlama 1.1B and then Pythia 1B. This order is the exact inverse of their non-embedding parameter sizes. This speed difference is attributed to the Grouped-Query (Shazeer, 2019) and Multi-Query attention (Ainslie et al., 2023). The main decoding bottleneck in Transformers is memory access of heavy key-value caches. Hence, Gemma that effectively reduces the key-value cache size through the MQA mechanism, achieves fastest speeds. Despite using GQA, TinyLlama 1.1B has similar speed to Gemma 2B due to its shallow and deep architecture (22 layers compared to Gemmas 18 layers). This deeper architecture likely offsets the speed gains from the attention mechanism. Comparison of hypothetical generation throughput We conducted early-exiting simulations using language modeling datasets (test sets of SlimPajama, RedPajama, and PG19), assuming our models generated those tokens. We used 20K samples to obtain their exit trajectories, and we employed an oracle-exiting algorithm to determine the earliest possible exit point for each token. Combining this trajectory data with previously measured per-token generation times across various batch sizes (considering only Transformer block computations), we estimated the hypothetical throughput for various settings and datasets. The results are detailed in Tables K.2 and K.4. Our analysis reveals that Recursive Transformers achieve 2-3 throughput gain over vanilla model counterparts. Relaxed models also demonstrated significant speedup despite their unoptimized LoRA computations. Currently, we merge multiple LoRAs into single, large LoRA module to enable parallel computation of samples at different looping iterations. However, this introduces extra overhead due to redundant computations, resulting in reduced throughput gains in memory-constrained scenarios (shorter context lengths and lower memory limits). This degradation stems from the increased proportion of LoRA computation time relative to overall processing time. Since attention computation has quadratic complexity with respect to sequence length, it becomes less expensive at shorter lengths, while the complexity of LoRA computation remains constant. This highlights the need for highly optimized LoRA computations to achieve substantial throughput gains in all scenarios. Nevertheless, these findings also suggest that relaxed models will yield even greater performance and throughput improvements with longer contexts where attention computation dominates. 43 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Approximation errors in our hypothetical throughput Since our throughput estimations are based on theoretical estimation, they may introduce certain approximation errors as follows: As our models are not fine-tuned for any downstream task, we simulated the exit trajectories of language modeling datasets by assuming they were generated by our models. While this approach is expected to closely approximate actual generation, empirical validation is necessary to confirm its accuracy. Throughput gains should be measured using realistic (confidence-based) early-exiting algorithms, rather than relying on the oracle-exiting algorithm. While early-exiting algorithms can introduce performance degradation due to inherent errors in confidence estimation, they also incur additional computational costs for estimating prediction confidence, necessitating further efficiency improvements. Our analysis solely focused on speed improvements within Transformer blocks. However, upon early exiting, the exited tokens require separate processing through the embedding layer or the classifier head for subsequent sequence generation. This necessitates non-exited tokens to wait for others, potentially reducing efficiency as the embedding layer computation may not fully utilize the maximum batch size. Early-exiting architectures require computing key-value caches in remaining layers for already exited tokens to prevent performance degradation (Bae et al., 2023). While this adds negligible overhead in memory-bound scenarios, it inevitably increases overhead in compute-bound scenarios where the maximum batch size is fully utilized. Our throughput estimation, however, excludes the computation time for these key-value caches in later loops (though we did account for their memory size). Incorporating these computations into more realistic analysis of early-exiting generation is direction for future work. 44 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Model Architecture Recursive Time (ms) per token Models ùëÅùêø ùëëùëöùëúùëëùëíùëô ùëÅ‚Ñéùëíùëéùëë ùëÅùêæùëâ Vocab N-emb Block Rank Batch Total Emb Transformer Head Gemma 18 2048 18 2048 2048 18 2048 18 2048 2048 18 2048 18 2048 2048 18 2048 18 2048 8 8 8 8 8 8 8 8 8 22 32 22 2048 32 TinyLlama 2048 32 22 2048 32 2048 32 22 2048 32 2048 16 2048 16 2048 2048 16 2048 16 2048 8 8 8 8 8 Pythia 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 8 8 8 8 8 256K 1.98B 256K 0.99B 256K 1.07B 256K 1.15B 256K 1.30B 256K 1.60B 256K 0.66B 256K 0.74B 256K 0.82B 256K 0.97B 256K 1.27B 32K 0.97B 32K 0.48B 32K 0.53B 32K 0.58B 32K 0.68B 32K 0.86B 50K 0.81B 50K 0.40B 50K 0.44B 50K 0.48B 50K 0.55B 50K 0.70B - 2 2 2 2 3 3 3 3 - 2 2 2 2 - 2 2 2 2 - - 64 256 512 - 64 128 512 - - 64 128 512 - - 64 128 512 1 43 1 43 1 41 1 41 1 1 39 1 43 1 43 1 43 1 41 1 1 329 1 233 1 211 1 209 1 209 1 1 53 1 61 1 63 1 59 1 59 1 22.994 0.657 13.918 0.336 15.858 0.398 15.708 0.398 15.456 0.450 15.489 0. 10.546 0.263 11.871 0.306 11.768 0.294 12.018 0.311 12.087 0.325 22.016 0. 12.657 0.446 15.243 0.454 15.456 0.454 15.223 0.457 15.383 0.461 13.280 1. 8.423 0.856 10.554 0.875 10.167 0.892 10.410 0.913 12.609 0.956 0.087 0. 0.088 0.002 0.080 0.002 0.080 0.002 0.083 0.002 0.078 0.002 0.081 0. 0.080 0.002 0.080 0.002 0.081 0.002 0.082 0.002 0.082 0.000 0.077 0. 0.079 0.000 0.082 0.000 0.081 0.000 0.080 0.000 0.080 0.002 0.081 0. 0.082 0.001 0.076 0.001 0.079 0.001 0.091 0.002 21.344 0.616 11.059 0. 13.096 0.323 12.969 0.324 12.721 0.372 12.775 0.422 7.394 0.182 8.724 0. 8.649 0.221 8.848 0.226 8.932 0.237 21.010 0.815 10.370 0.413 12.908 0. 13.118 0.421 12.908 0.423 13.062 0.428 12.286 1.206 6.378 0.606 8.519 0. 8.196 0.642 8.402 0.662 10.311 0.702 0.803 0.023 0.827 0.023 0.825 0. 0.822 0.024 0.818 0.025 0.817 0.025 0.827 0.023 0.827 0.023 0.825 0. 0.823 0.024 0.822 0.025 0.188 0.001 0.209 0.001 0.211 0.002 0.213 0. 0.208 0.002 0.211 0.002 0.235 0.005 0.262 0.005 0.260 0.005 0.256 0. 0.258 0.005 0.267 0.006 Table K.1 Measurements of generation time across three models using single A100 40GB GPU. We measured time per token for both batch size of 1 and the maximum batch size achievable by each model. The prefix length was set to 512 tokens, and the decoded output length to 2048 tokens. We then averaged the total elapsed time by the output length of 2048. Dummy input and dummy tensors were used for measurement. Both Gemma, employing multi-query attention, and TinyLlama, utilizing grouped-query attention, demonstrated fast generation speeds and large maximum batch sizes relative to their model sizes. TinyLlamas deep and narrow architecture allowed for significantly large maximum batch size, although its generation speed was slower due to the increased number of layers. 45 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Uptrain Looping LoRA Early-Exit Train Batching Few-shot Accuracy Throughput CE KD Type Exit Last Mid 1 Mid 2 Batch SlimP RedP PG19 Œîùëâ ŒîùëÜùëíùëû - - - - - CSB 57.3 57.3 - - - - - - - - - - - - - 64 64 - - 64 3 3 3 3 2 2 2 2 2 2 2 2 2 3 3 3 3 3 Models Gemma N-emb PT ùëÅùë°ùëúùëò KD Block Init Rank Init ùëÅùë°ùëúùëò 1.99B 75B 1.99B 75B 0.99B 60B 1.07B 60B 1.15B 60B 1.30B 60B 1.60B 60B 1.07B 60B 1.15B 60B 1.30B 60B 1.60B 60B 0.66B 60B 0.74B 60B 0.82B 60B 0.97B 60B 1.27B 60B 0.74B 60B 0.82B 60B 0.97B 60B 1.27B 60B 0.97B - - 0.97B - - 0.48B 60B 0.53B 60B 0.58B 60B TinyLlama 0.68B 60B 0.86B 60B 0.53B 60B 0.58B 60B 0.68B 60B 0.86B 60B 0.81B 75B 0.81B 75B 0.40B 60B 0.44B 60B 0.48B 60B 0.55B 60B 0.70B 60B 0.44B 60B 0.48B 60B 0.55B 60B 0.70B 60B 2 2 2 2 2 2 2 2 2 2 Pythia 2 2 2 2 2 2 2 - 64 - 64 64 64 - - - - - - - - - - - - - - - - - - - - - 15B Agg (0.1) CDB 54.0 48.8 Step SVD 15B Agg (0.1) CDB 54.0 40.8 Avg Avg 128 SVD 15B Agg (0.1) CDB 54.6 40.2 Avg 256 SVD 15B Agg (0.1) CDB 55.2 40.5 Avg 512 SVD 15B Agg (0.1) CDB 56.2 41.7 SVD 15B Agg (0.3) CDB 53.1 43.3 Avg Avg 128 SVD 15B Agg (0.3) CDB 53.6 43.4 Avg 256 SVD 15B Agg (0.3) CDB 54.6 43.2 Avg 512 SVD 15B Agg (0.3) CDB 55.2 44.0 15B Agg (0.1) CDB 51.9 49.0 Step SVD 15B Agg (0.1) CDB 51.4 40.8 Avg Avg 128 SVD 15B Agg (0.1) CDB 51.7 41.1 Avg 256 SVD 15B Agg (0.1) CDB 54.1 42.2 Avg 512 SVD 15B Agg (0.1) CDB 55.7 43.5 SVD 15B Agg (0.3) CDB 50.1 42.9 Avg Avg 128 SVD 15B Agg (0.3) CDB 51.2 43.2 Avg 256 SVD 15B Agg (0.3) CDB 53.0 44.8 Avg 512 SVD 15B Agg (0.3) CDB 54.1 45.7 - - - - - CSB 43.3 43.3 - - - 15B Agg (0.1) CDB 44.8 41.8 Step SVD 15B Agg (0.1) CDB 45.1 33.7 Avg Avg 128 SVD 15B Agg (0.1) CDB 45.2 33.9 Avg 256 SVD 15B Agg (0.1) CDB 45.6 33.9 Avg 512 SVD 15B Agg (0.1) CDB 46.1 34.2 SVD 15B Agg (0.3) CDB 44.6 36.1 Avg Avg 128 SVD 15B Agg (0.3) CDB 44.8 36.0 Avg 256 SVD 15B Agg (0.3) CDB 44.8 36.2 Avg 512 SVD 15B Agg (0.3) CDB 45.8 36.5 - - - - - CSB 49.3 49. - - - 15B Agg (0.1) CDB 45.4 42.0 Step SVD 15B Agg (0.1) CDB 46.1 37.1 Avg Avg 128 SVD 15B Agg (0.1) CDB 46.2 37.8 Avg 256 SVD 15B Agg (0.1) CDB 46.5 38.0 Avg 512 SVD 15B Agg (0.1) CDB 47.2 38.2 SVD 15B Agg (0.3) CDB 45.1 39.0 Avg Avg 128 SVD 15B Agg (0.3) CDB 45.9 39.0 Avg 256 SVD 15B Agg (0.3) CDB 46.0 39.4 Avg 512 SVD 15B Agg (0.3) CDB 46.7 39.7 - - - - - - - - - - - 43.5 36.1 36.1 36.1 37.0 37.7 38.0 38.7 39.2 - - - - - - - - - - - - - - - - - - - - - - 43 43 43 41 41 39 39 41 41 39 39 43 43 43 41 43 43 41 39 329 329 233 211 209 209 209 211 209 209 209 53 53 61 63 59 59 63 59 59 53 655 1228 1357 1.00 0.71 1622 1604 1357 1.41 1.00 3159 3050 2421 2.66 1.88 2357 2255 1858 2.00 1.41 2355 2250 1844 1.99 1.41 2047 1976 1740 1.78 1.26 1806 1754 1598 1.59 1.13 2454 2357 1929 2.08 1.47 2445 2346 1926 2.07 1.47 2123 2056 1804 1.85 1.31 1870 1819 1655 1.65 1.17 3120 3041 2729 2.74 1.94 2334 2274 2059 2.06 1.45 2290 2230 2007 2.02 1.42 2281 2219 1984 2.00 1.41 2181 2122 1900 1.91 1.35 2427 2372 2143 2.14 1.51 2376 2321 2084 2.09 1.48 2359 2300 2039 2.07 1.46 2251 2191 1975 1.98 1. 1205 1220 1194 1.00 0.99 1227 1225 1194 1.01 1.00 2038 2023 1933 1.66 1.64 1733 1719 1617 1.40 1.39 1733 1717 1609 1.40 1.39 1728 1714 1606 1.39 1.38 1716 1702 1581 1.38 1.37 1810 1796 1688 1.46 1.45 1802 1787 1668 1.45 1.44 1793 1779 1668 1.45 1.44 1778 1763 1637 1.43 1.42 702 829 785 827 822 1.00 0.93 822 1.07 1. 1339 1333 1281 1.71 1.60 1205 1203 1140 1.54 1.43 1156 1180 1108 1.49 1.39 1138 1139 1071 1.45 1.35 1051 1077 1021 1.36 1.27 1254 1252 1190 1.60 1.49 1200 1226 1153 1.55 1.45 1180 1180 1112 1.50 1.40 1088 1114 1058 1.41 1.32 Table K.2 Hypothetical generation speedup of Recursive Transformers across three models. We utilized the measurements of per-token generation time calculated in Table K.1, which were calculated using an A100 40GB GPU, prefix length of 512, and decoding length of 2048. We only considered the time spent within Transformer blocks, simulating generation on the SlimPajama, RedPajama, and PG19 test sets. We used vanilla Transformer models, both with and without continuous sequence-wise batching (CSB), as our baselines. Our recursive models further enhance throughput by applying continuous depth-wise batching (CDB), leveraging looping and early-exiting techniques. The throughput improvements over the vanilla Transformer without and with sequence-wise batching are denoted as Œîùëâ and ŒîùëÜùëíùëû, respectively. 46 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Model Architecture Recursive Time (ms) per token Models ùëÅùêø ùëëùëöùëúùëëùëíùëô ùëÅ‚Ñéùëíùëéùëë ùëÅùêæùëâ Vocab N-emb Block Rank Batch Total Emb Transformer Head Gemma 18 2048 18 18 2048 18 2048 18 18 2048 18 2048 18 18 2048 18 2048 18 8 8 8 8 8 8 8 8 8 8 2048 32 22 2048 32 TinyLlama 22 2048 32 22 2048 22 2048 32 22 2048 16 2048 16 2048 16 16 2048 16 2048 16 8 8 8 8 8 Pythia 1 1 1 1 1 1 1 1 1 4 4 4 4 4 8 8 8 8 8 256K 1.98B 256K 0.99B 256K 1.07B 256K 1.15B 256K 1.30B 256K 1.60B 256K 0.66B 256K 0.74B 256K 0.82B 256K 0.97B 256K 1.27B 32K 0.97B 32K 0.48B 32K 0.53B 32K 0.58B 32K 0.68B 32K 0.86B 50K 0.81B 50K 0.40B 50K 0.44B 50K 0.48B 50K 0.55B 50K 0.70B - 2 2 2 2 3 3 3 3 - 2 2 2 2 - 2 2 2 2 - - 128 256 512 - 64 256 512 - - 64 256 512 - - 64 256 512 1 111 1 123 1 117 1 1 111 1 103 1 131 1 123 1 121 1 1 107 1 1049 1 1121 1 1105 1 1089 1 1 1017 1 229 1 241 1 233 1 221 1 1 165 22.577 0.207 13.576 0.118 15.372 0.140 15.631 0.141 15.317 0. 15.379 0.158 10.528 0.087 11.957 0.105 11.898 0.103 11.734 0.106 11.986 0. 23.898 0.131 14.129 0.070 14.897 0.073 15.090 0.074 14.962 0.076 15.284 0. 13.341 0.176 8.336 0.121 10.408 0.133 10.426 0.137 10.509 0.151 11.254 0. 0.084 0.001 0.079 0.001 0.080 0.001 0.082 0.001 0.079 0.001 0.080 0. 0.080 0.001 0.081 0.001 0.080 0.001 0.079 0.001 0.080 0.001 0.080 0. 0.080 0.000 0.080 0.000 0.081 0.000 0.081 0.000 0.083 0.000 0.081 0. 0.079 0.000 0.081 0.000 0.082 0.000 0.080 0.000 0.080 0.001 20.937 0. 10.819 0.091 12.675 0.112 12.899 0.113 12.639 0.115 12.692 0.127 7.411 0. 8.855 0.075 8.787 0.074 8.654 0.076 8.856 0.090 22.909 0.129 11.846 0. 12.627 0.068 12.778 0.069 12.659 0.071 12.950 0.075 12.326 0.171 6.303 0. 8.353 0.097 8.378 0.101 8.471 0.115 9.241 0.139 0.801 0.010 0.815 0. 0.813 0.009 0.816 0.010 0.811 0.010 0.807 0.011 0.817 0.010 0.815 0. 0.816 0.009 0.813 0.009 0.809 0.010 0.189 0.001 0.202 0.001 0.202 0. 0.205 0.001 0.201 0.001 0.206 0.001 0.239 0.002 0.261 0.002 0.262 0. 0.259 0.002 0.256 0.002 0.257 0.002 Table K.3 Generation time measurements of Gemma models on single A100 40GB GPU with 16GB memory constraint. We measured generation time per token for both batch size of 1 and the maximum batch size achievable by each model. The prefix length was set to 64 tokens, and the decoded output length to 256 tokens. We then averaged the total elapsed time by the output length of 256. Dummy input and dummy tensors were used for measurement. 47 Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA Uptrain Looping LoRA Early-Exit Train Batching Few-shot Accuracy Throughput CE KD Type Exit Last Mid 1 Mid 2 Batch SlimP RedP PG19 Œîùëâ ŒîùëÜùëíùëû - - - - - CSB 57.3 57.3 - - - - - - - - - - - - - - 3 3 3 2 2 2 2 2 2 2 2 2 3 3 3 3 3 Models Gemma N-emb PT ùëÅùë°ùëúùëò KD Block Init Rank Init ùëÅùë°ùëúùëò 1.99B 75B 1.99B 75B 0.99B 60B 1.07B 60B 1.15B 60B 1.30B 60B 1.60B 60B 1.07B 60B 1.15B 60B 1.30B 60B 1.60B 60B 0.66B 60B 0.74B 60B 0.82B 60B 0.97B 60B 1.27B 60B 0.74B 60B 0.82B 60B 0.97B 60B 1.27B 60B 0.97B - - 0.97B - - 0.48B 60B 0.53B 60B 0.58B 60B TinyLlama 0.68B 60B 0.86B 60B 0.53B 60B 0.58B 60B 0.68B 60B 0.86B 60B 0.81B 75B 0.81B 75B 0.40B 60B 0.44B 60B 0.48B 60B 0.55B 60B 0.70B 60B 0.44B 60B 0.48B 60B 0.55B 60B 0.70B 60B 2 2 2 2 2 2 2 2 2 2 Pythia 2 2 2 2 2 2 2 2 - - - - - - - - - - - - - - - - - - - - - - - 64 15B Agg (0.1) CDB 54.0 48.8 Step SVD 15B Agg (0.1) CDB 54.0 40.8 Avg Avg 128 SVD 15B Agg (0.1) CDB 54.6 40.2 Avg 256 SVD 15B Agg (0.1) CDB 55.2 40.5 Avg 512 SVD 15B Agg (0.1) CDB 56.2 41.7 SVD 15B Agg (0.3) CDB 53.1 43.3 Avg Avg 128 SVD 15B Agg (0.3) CDB 53.6 43.4 Avg 256 SVD 15B Agg (0.3) CDB 54.6 43.2 Avg 512 SVD 15B Agg (0.3) CDB 55.2 44.0 15B Agg (0.1) CDB 51.9 49.0 Step SVD 15B Agg (0.1) CDB 51.4 40.8 Avg Avg 128 SVD 15B Agg (0.1) CDB 51.7 41.1 Avg 256 SVD 15B Agg (0.1) CDB 54.1 42.2 Avg 512 SVD 15B Agg (0.1) CDB 55.7 43.5 SVD 15B Agg (0.3) CDB 50.1 42.9 Avg Avg 128 SVD 15B Agg (0.3) CDB 51.2 43.2 Avg 256 SVD 15B Agg (0.3) CDB 53.0 44.8 Avg 512 SVD 15B Agg (0.3) CDB 54.1 45.7 - 64 64 - - - - - CSB 43.3 43.3 - - - 64 15B Agg (0.1) CDB 44.8 41.8 Step SVD 15B Agg (0.1) CDB 45.1 33.7 Avg Avg 128 SVD 15B Agg (0.1) CDB 45.2 33.9 Avg 256 SVD 15B Agg (0.1) CDB 45.6 33.9 Avg 512 SVD 15B Agg (0.1) CDB 46.1 34.2 SVD 15B Agg (0.3) CDB 44.6 36.1 Avg Avg 128 SVD 15B Agg (0.3) CDB 44.8 36.0 Avg 256 SVD 15B Agg (0.3) CDB 44.8 36.2 Avg 512 SVD 15B Agg (0.3) CDB 45.8 36. 64 - - - - - CSB 49.3 49. - - - 64 15B Agg (0.1) CDB 45.4 42.0 Step SVD 15B Agg (0.1) CDB 46.1 37.1 Avg Avg 128 SVD 15B Agg (0.1) CDB 46.2 37.8 Avg 256 SVD 15B Agg (0.1) CDB 46.5 38.0 Avg 512 SVD 15B Agg (0.1) CDB 47.2 38.2 SVD 15B Agg (0.3) CDB 45.1 39.0 Avg Avg 128 SVD 15B Agg (0.3) CDB 45.9 39.0 Avg 256 SVD 15B Agg (0.3) CDB 46.0 39.4 Avg 512 SVD 15B Agg (0.3) CDB 46.7 39.7 64 - - - - - - - - - - - 43.5 36.1 36.1 36.1 37.0 37.7 38.0 38.7 39.2 - - - - - - - - - - - - - - - - - - - - - - 111 111 1740 3059 4796 1.00 0.63 5287 5060 4796 1.58 1.00 43 41 41 39 41 41 39 39 43 43 43 41 39 43 43 41 39 3159 3050 2421 2.50 1.59 2357 2255 1858 1.87 1.19 2355 2250 1844 1.87 1.19 2047 1976 1740 1.86 1.18 1806 1754 1598 1.73 1.10 2454 2357 1929 1.95 1.24 2445 2346 1926 1.95 1.24 2123 2056 1804 1.93 1.22 1870 1819 1655 1.79 1.14 3120 3041 2729 2.62 1.66 2334 2274 2059 1.87 1.19 2290 2230 2007 1.90 1.20 2281 2219 1984 1.86 1.18 2181 2122 1900 1.62 1. 2427 2372 2143 1.94 1.23 2376 2321 2084 1.97 1.25 2359 2300 2039 1.92 1.22 2251 2191 1975 1.67 1.06 1049 6856 7481 4090 1.00 0.96 1049 7709 7481 4090 1.05 1.00 233 211 209 209 209 211 209 209 209 229 229 61 63 59 59 63 59 59 53 2038 2023 1933 1.70 1.62 1733 1719 1617 1.38 1.32 1733 1717 1609 1.36 1.30 1728 1714 1606 1.34 1.28 1716 1702 1581 1.28 1.23 1810 1796 1688 1.45 1.38 1802 1787 1668 1.41 1.35 1793 1779 1668 1.39 1.33 1778 1763 1637 1.33 1.27 4273 5346 5149 1.00 0.89 5813 5724 5149 1.13 1.00 1339 1333 1281 1.77 1.57 1205 1203 1140 1.44 1.28 1156 1180 1108 1.32 1.17 1138 1139 1071 1.22 1.08 1051 1077 1021 0.98 0.87 1254 1252 1190 1.50 1.33 1200 1226 1153 1.37 1.22 1180 1180 1112 1.27 1.12 1088 1114 1058 1.02 0. Table K.4 Hypothetical generation speedup of Recursive Transformers across three models. We utilized the measurements of per-token generation time calculated in Table K.3, which were calculated using an A100 GPU with 16GB memory constraint, prefix length of 64, and decoding length of 256. We only considered the time spent within Transformer blocks, simulating generation on the SlimPajama, RedPajama, and PG19 test sets. We used vanilla transformer models, both with and without continuous sequence-wise batching (CSB), as our baselines. Our recursive models further enhance throughput by applying continuous depth-wise batching (CDB), leveraging looping and early-exiting techniques. The throughput improvements over the vanilla Transformer without and with sequence-wise batching are denoted as Œîùëâ and ŒîùëÜùëíùëû, respectively."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Google Research",
        "KAIST AI"
    ]
}