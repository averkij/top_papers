{
    "paper_title": "LongCat-Image Technical Report",
    "authors": [
        "Meituan LongCat Team",
        "Hanghang Ma",
        "Haoxian Tan",
        "Jiale Huang",
        "Junqiang Wu",
        "Jun-Yan He",
        "Lishuai Gao",
        "Songlin Xiao",
        "Xiaoming Wei",
        "Xiaoqi Ma",
        "Xunliang Cai",
        "Yayong Guan",
        "Jie Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation."
        },
        {
            "title": "Start",
            "content": "LongCat-Image Technical Report Meituan LongCat Team longcat-team@meituan.com"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce LongCat-Image, pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the leading performance, high efficiency, and openness of LongCat-Image will provide robust support for developers and researchers, collectively pushing the frontiers of multilingual visual content creation. LongCat Chat: https://longcat.ai Hugging Face: https://huggingface.co/meituan-longcat/LongCat-Image GitHub: https://github.com/meituan-longcat/LongCat-Image 5 2 0 2 8 ] . [ 1 4 8 5 7 0 . 2 1 5 2 : r LongCat-Image Technical Report Figure 1: LongCat-Image exhibits strong performance in general text-to-image generation, text rendering and image editing. 2 LongCat-Image Technical Report"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Data 2.1 Data Curation . . 2.1.1 Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.2 Meta Infomation Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.3 Mutli-Granularity Captioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.4 Stratification . 2.2 Data Synthesis . . . . 3 Model Design 3.1 Diffusion Model . 3.2 Text Encoder . . . . . . . . 3.3 Positional Embedding . 3.4 Prompt Engineering . . 4 Model Training 4.1 Pre-training . 4.2 Mid-training . 4.3 Post-training . 4.3.1 SFT . . . . . 4.3.2 RLHF . . . . . . . . . . . . . . . . . . . . . 5 Model Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Benchmarks . . . . . . . . . . 5.1.1 Text-Image Alignment 5.1.2 Text Rendering . 5.2 Human Evaluation . 5.3 Qualitative Results . . . . . . . . . . . . . . 6 Image Editing 6.1 Data Curation . . . . . . . . . 6.1.1 Open-Source Datasets 6.1.2 Synthesized Data . 6.1.3 Video Frames . . . . . 6.1.4 Interleaved Corpus . . . . 6.1.5 Instruction Rewriting . 6.2 Model Design . . 6.3 Model Training . . . . . 6.3.1 Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 9 9 9 10 11 14 14 14 15 15 16 16 17 18 18 20 20 20 20 23 24 24 24 24 29 29 30 30 30 LongCat-Image Technical Report 6.3. SFT . . 6.3.3 DPO . 6.4 Discussion . . . . . . . . . . . . 6.5 Model Performance . 6.5.1 Benchmarks . . . . . . . . . . . . . . . . . . . . . 6.5.2 Human Evaluation . 6.5.3 Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Conclusion 8 Contributions and Acknowledgments 30 31 31 31 31 34 41 42 4 LongCat-Image Technical Report Figure 2: High-fidelity text-to-image generation results. 5 LongCat-Image Technical Report Figure 3: Showcase of versatile capabilities in general image editing. 6 LongCat-Image Technical Report Figure 4: Showcase on complex and comprehensive editing scenarios. Beyond basic edits, LongCat-Image-Edit exhibits robust handling of intricate modifications and composite instructions. 7 LongCat-Image Technical Report"
        },
        {
            "title": "Introduction",
            "content": "In recent years, significant advancements in Diffusion Models (DM) [Ho et al., 2020, Chen et al., 2024a, Labs, 2024, Wu et al., 2025a] have revolutionized the field of image generation, rapidly propelling the technology from academic research to widespread commercial applications, leading to the emergence of milestone products like Midjourney [Midjourney, 2025] and Seedream [Gong et al., 2025, Gao et al., 2025, Seedream et al., 2025]. As the technology has matured, the evaluation criteria for text-to-image (T2I) models have shifted from foundational metrics like instruction following and visual plausibility to more demanding benchmarks focusing on three core pillars: photorealism, aesthetics, and text rendering capabilities. Concurrently, image editing has emerged as another critical domain, gaining prominence with the release of various open-source and commercial products (i.e., Flux.1 Kontext [Batifol et al., 2025], Nano Banana (Gemini-2.5-flashimage)1). The primary challenges in image editing currently center on two key problems: executing editing instructions with high precision and maintaining strict visual consistency between the original and edited images [Wang et al., 2025a]. Although existing work [Batifol et al., 2025, Wang et al., 2025b, Wu et al., 2025a] has made important strides, significant gap remains in achieving seamless and reliable editing experience. To address these challenges in both generation and editing, prevailing trend has been the dramatic scaling of model parametersfrom PixArt-α [Chen et al., 2024a] at 0.6B, to Stable Diffusion3.0 [Esser et al., 2024] at 8B, and further to Qwen-Image [Wu et al., 2025a] at 20B and even larger Mixture-of-Experts (MoE) architectures like Hunyuan3.0 [Cao et al., 2025] with 80B full parameters. The expectation has been that, similar to Large Language Models (LLMs), diffusion models would experience breakthrough in performance through brute-force scaling. However, our observations reveal critical issue: unrestrained parameter growth has not delivered the anticipated qualitative leap. Instead, it has led to host of problems, including soaring computational costs, higher deployment barriers, and increased inference latency. This not only hinders the democratization of the technology but also poses challenges for open academic research. In this context, and guided by the LongCat teams consistent design philosophy of Building efficient and powerful model, we introduce LongCat-Imagea novel, lightweight diffusion model for image generation and editing. We contend that more optimal equilibrium must be struck between state-of-the-art performance and efficiency in training and inference. Through systematic experimentation, we determine that parameter scale of 6B serves as the ideal foundation for balancing capability and efficiency without compromising generative quality. Specifically, the models core diffusion architecture employs hybrid MM-DiT and Single-DiT structure, consistent with Flux1.dev [Labs, 2024], while leveraging the Qwen2.5VL-7B [Bai et al., 2025] as its text encoder to provide unified and powerful conditional space for both generation and editing tasks. To further enhance photorealism, we implement systematic overhaul of our data pipeline. We observe that even small proportion of AIGC-contaminated data can cause the model to prematurely converge to narrow local optimum during training. While this may accelerate initial convergence, it severely limits the models potential to achieve higher levels of realism during subsequent fine-tuning. Consequently, we rigorously exclude all AIGC data during the pre-training and intermediate training stages. In the Supervised Fine-Tuning (SFT) phase, any high-quality synthetic data introduced was meticulously hand-selected. Finally, during the Reinforcement Learning (RL) phase, we innovatively incorporate an AIGC detection model as one of the reward models, using its adversarial signal to guide the model toward generating images with the texture and fidelity of the real physical world. To overcome the industry-wide challenge of complex Chinese text rendering, we adopt comprehensive strategy spanning data, architecture, and training. On the data front, we utilized the SynthDoG tool [Kim et al., 2022] to generate large volume of text-in-image data, primarily with monotonous backgrounds, to minimize interference and improve the models focus on learning character glyphs. Architecturally, we modify the text encoder to apply character-level encoding to the text designated for rendering in the prompt (identified by ), which effectively reduces the memorization burden and enhances learning efficiency. During training, we use real-world text-in-image data in the SFT phase and introduce OCR and aesthetic reward models in the RL phase, significantly improving both the accuracy of text rendering and its natural integration with the background. To solve the core challenge of visual consistency in image editing, we develop meticulous training paradigm and stringent data filtering strategy. We deliberately choose to initialize the editing model with weights from the mid-training stage of the T2I model, rather than from highly optimized state after SFT or RL. The latter models exist in narrowed state space, which is less conducive to learning and generalizing across diverse editing tasks. During the pre-training and SFT phases of the editing model, we employ multi-task joint training, combining editing tasks with T2I tasks. This approach effectively mitigates catastrophic forgetting of generative knowledge and enhances the models comprehension 1https://aistudio.google.com/models/gemini-2-5-flash-image 8 LongCat-Image Technical Report of editing instructions. Data quality is paramount for ensuring visual consistency. We filter out samples with poor visual consistency during pre-training using task-specific strategy and exclusively use high-consistency, human-annotated data during the SFT phase. Experiments confirm that this series of measures enables our model to achieve an exceptional standard in both instruction-following accuracy and visual consistency. To empower the broader academic and industrial ecosystem, we are not only open-sourcing the final model but also releasing the mid-training checkpoint as development model. Furthermore, we are providing the complete training and fine-tuning codebase, covering the entire workflow from pre-training to RL. Our goal is to foster thoroughly open and accessible development ecosystem. Our main contributions are five-fold: Exceptional Efficiency and Performance: With only 6B parameters, LongCat-Image surpasses numerous opensource models that are several times larger across multiple benchmarks, demonstrating the immense potential of efficient model design. Remarkable Photorealism: Through an innovative data strategy and training framework, our model achieves remarkable photorealism in generated images. Powerful Chinese Text Rendering: The model demonstrates superior accuracy and stability in rendering common Chinese characters compared to existing SOTA open-source models and achieves industry-leading coverage of the Chinese dictionary. Superior Editing Performance: The LongCat-Image editing model achieves state-of-the-art performance among open-source models, delivering leading performance of instruction following and image quality, as well as superior visual consistency. Comprehensive Open-Source Ecosystem: We provide complete toolchain, from intermediate checkpoints to the full training code, significantly lowering the barrier for further research and development within the community."
        },
        {
            "title": "2 Data",
            "content": "The performance of generative models depends critically on the scale, diversity, and quality of the training corpus. Accordingly, we curate massive dataset comprising 1.2 billion samples. Fig. 5 provides detailed statistical overview of the data composition. 2.1 Data Curation As illustrated in Fig. 6, our data curation pipeline consists of four stages: filtering low-quality and duplicate samples, image metadata extraction, recaptioning, and data stratification for multi-stage training. 2.1.1 Filtering Deduplication. To address data redundancy across diverse sources, we employ two-tiered deduplication strategy: first, MD5 hashing is used to detect exact duplicates; second, SigLIP-based similarity assessment is applied to identify and eliminate near-duplicate entries. Resolution & Aspect Ratio. Low-resolution images and extreme aspect ratios often correlate with poor visual quality. We exclude images with shortest edge below 384 pixels. Furthermore, only images with aspect ratios between 0.25 and 4.0 are retained to ensure uniform and structurally coherent dataset. Watermark Detection. Watermarked images can introduce undesirable artifacts into the generated outputs. To mitigate this, we utilize specialized watermark detector to identify and remove samples exhibiting visible watermark patterns. Laion Aesthetics. To guarantee baseline of visual quality, each image is evaluated using the LAION-Aesthetics predictor [Schuhmann et al., 2022]. We discard images with scores below 4.5. This threshold is empirically selected to filter out low-quality samples while preserving sufficient diversity for model training. AIGC Detection. Our experiments indicate that small fraction of AI-generated content (AIGC) in the training data can disrupt optimization, resulting in plastic or greasy texture in generated images. Consequently, we develop an internal AIGC detector to purge synthetic data from the corpus. 9 LongCat-Image Technical Report Figure 5: Overview of training data. 2.1.2 Meta Infomation Extraction We delineate five key attributes essential for downstream processing: category, style, named entity, OCR, and aesthetics. In particular, category, style, and aesthetics are instrumental in achieving balanced data distribution and enabling hierarchical data structuring. Conversely, named entities and recognized text content play pivotal role in enhancing the accuracy and informativeness of image captions. Category. The category represents the semantic classification of an image according to its visual content. In our work, images are assigned to one of the following categories: portrait, sport, activity, plant, animal, food, object, landscape, cityscape, indoor, UI, cartoon, chart, rich-text, poster, and synthetic text. This categorization scheme provides structured framework for organizing the dataset and supports subsequent processes such as content analysis and distribution balancing. Style. Style serves as an indicator of the artistic characteristics inherent in an image. In our approach, we instruct the open-source VLM to produce set of plausible style descriptions in the form of phrases, rather than constraining the output to fixed set of predefined labels. Named Entity. Named entities serve as indicators of the world knowledge encapsulated within visual content. In our work, we employ the available VLMs to identify potential celebrities, fictional characters, biological species, commercial brands, and intellectual properties depicted in the images. Subsequently, this extracted semantic information is incorporated into the subsequent image recaptioning process to enhance descriptive accuracy and contextual richness. OCR Text. To enhance text rendering performance, an OCR model is employed to extract textual information from the images. The extracted text is subsequently processed and integrated into the corresponding image captions through specialized handling procedure. Comprehensive Aesthetics Evaluation. Quantifying aesthetics is inherently challenging due to subjectivity and the generalization limits of existing single-metric models. To address this, we decouple image evaluation into two 10 LongCat-Image Technical Report Figure 6: Data curation pipeline. The pipeline consists of four stages: (1) Filtering: Raw data undergoes deduplication and quality assessment, including watermark and AIGC detection. (2) Meta Information Extraction: We extract comprehensive metadata, such as aesthetic scores, named entities, and OCR text. (3) Multi-Granularity Captioning: Leveraging the extracted metadata and prompt templates, VLM generates captions ranging from entity-level tags to detailed photographic descriptions. (4) Stratification: The dataset is stratified into pyramid structure based on style, quality, and content to support progressive training stages. orthogonal dimensions, namely Quality and Artistry, and employ an ensemble of six complementary assessment methodologies. The overall pipeline is illustrated in Fig. 7. Quality: This dimension measures technical fidelity. We combine low-level signal statistics, including saturation, contrast, and color richness in RGB and HSV spaces, with deep reference-free assessment metrics derived from MUSIQ [Ke et al., 2021] and Q-Align [Wu et al., 2023]. Artistry: This dimension evaluates photographic merit and artistic expression. We leverage VLM-based analysis to assess high-level attributes such as composition, lighting, shadow, and color tonality, complemented by the Q-Align-Aesthetics [Wu et al., 2023] score. 2.1.3 Mutli-Granularity Captioning Image captioning with advanced Vision-Language Models (VLMs) has recently emerged as prominent paradigm. However, there exist three critical limitations: (1) insufficient integration of world knowledge embedded within generated captions; (2) restricted diversity in caption formats; and (3) low information density resulting from verbose captions. The lack of world knowledge often leads to inaccurate or incomplete depictions of named entities, thereby undermining the semantic fidelity of the generated images. Furthermore, captions produced by the same VLM frequently conform to similar structural patterns and lengths, limiting robustness. Finally, verbose descriptions consume valuable token space, thereby reducing the efficiency of content representation within constrained caption lengths. 11 LongCat-Image Technical Report Figure 7: Comprehensive aesthetic scoring. The aesthetic scoring takes image quality and image artistry into account. In this context, quality denotes the signal-related attributes of the image, such as resolution, clarity, and noise levels, while artistry pertains to the perceptual appeal or visual attractiveness of the image as judged by humans or models. To overcome the aforementioned limitations, we introduce Multi-Granularity Captioning (MGC) framework that systematically organizes semantic abstraction into four hierarchical levels. Specifically, the Entity Level Caption aims to identify and describe the principal visual entities present in an image; the Phrase Level Caption encapsulates salient visual attributes using concise linguistic expressions; the Composition Level Caption provides an integrative interpretation that captures the overall semantic structure of the scene; and the Photographic Level Caption offers finest-grained visual depictions, incorporating both the specific content of the image and relevant world knowledge in succinct yet informative manner. At the Entity Level and Phrase Level, we employ Qwen2.5-VL [Bai et al., 2025] to concurrently extract the relevant semantic information from the input image. Subsequently, at the Composition Level, we integrate the image itself, the Entity Level descriptions, and the extracted image meta-information, and feed them into InternVL2.5 [Chen et al., 2024b] to generate comprehensive, Composition Level caption. These example prompts are shown in Fig. 8. At the Photographic Level, we develop customized captioning model, called the Photographic Captioner, based on the Qwen2.5-VL backbone. Empirical analysis reveals that, while this open-source backbone can produce descriptions enriched with extensive world knowledge, the output format exhibits notable inconsistencies. To mitigate this issue, we apply LoRA to fine-tune the model, using meticulously annotated synthetic imagetext pairs. This approach improves the informational density of the captions while retaining their embedded world knowledge. qualitative comparison of caption outputs is provided in Fig. 9. During training, we employ weighted sampling strategy for these multi-granularity captions, prioritizing detailed descriptions to maximize information density. Specifically, the sampling probabilities for the four increasing levels of granularity are set to [0.05, 0.1, 0.2, 0.65], respectively. This distribution enables the model to accommodate diverse prompt formats while robustly encoding complex world knowledge. Fig. 10 illustrates representative training samples across these granularity levels. Figure 8: Prompts for different level captions. Entity Level and Phrase Level captions are generated concurrently using single model, whereas composition-level captions are subsequently produced in sequential stage utilizing separate model. 12 LongCat-Image Technical Report Figure 9: Quality comparison of our Photographic Level Captioner. This captioner produces more concise and information-dense captions compared to baseline. Different color blocks indicate different aspects of the captions. Figure 10: Examples of Multi-Granularity Captioning. 2.1.4 Stratification We stratify the training corpus into distinct subsets tailored to specific training stages, utilizing the extracted metadataspecifically image style, semantic content, and aesthetic scores. Pre-training. Empirical evidence suggests that exposing the model to high concentration of artistic data (e.g., illustrations, cartoons, and anime) during the early pre-training stage biases the model towards learning simplified visual patterns. This tendency can compromise the models ability to generate high-fidelity photorealistic images, effectively causing collapse in the realistic generation subspace. Consequently, we restrict artistic data to approximately 0.5% of the pre-training corpus, deferring the integration of broader stylistic data to the subsequent mid-training phase. Mid-training. The mid-training phase focuses on two objectives: Quality Enhancement and Artistic Style Injection. For quality enhancement, we curate subset of high-resolution images (exceeding 1,024 pixels) from the pre-training corpus, 13 LongCat-Image Technical Report selected for their superior sharpness, balanced composition, and high aesthetic scores. Simultaneously, for artistic style injection, we reintroduced the previously filtered artistic data to unlock the models stylistic capabilities. We gradually increase the proportion of stylized data from 0.5% to 2.5% following calibrated schedule. This progressive integration strategy effectively expands the models stylistic repertoire while preserving its photorealistic foundation. SFT. In the SFT phase, we employ mix of real and synthetic data to align the model with human aesthetic preferences. For real data, human experts manually curate high-fidelity samples from the Mid-training dataset, evaluating dimensions such as composition, lighting, color tonality, and emotional expression, while ensuring balanced categorical distribution. Complementing this, we incorporate model-synthesized images that have undergone rigorous manual filtering to eliminate structural distortions, visual unreality, and aesthetic flaws. The strong stylistic consistency of this synthetic data facilitates the models rapid convergence towards the manifold of human preferences. 2.2 Data Synthesis Synthetic Data Generation. To address the long-tail distribution inherent in real-world datasets, we construct specialized synthetic corpus targeting rare concepts and corner cases. Specifically, we train multiple domain-specific LoRA adapters on limited samples to capture infrequent compositional patterns and distinctive artistic styles. These adapters generate high-quality synthetic images, which are integrated into the mid-training phase at controlled low ratio. This strategy effectively boosts performance on tail categories without compromising the diversity of the generated output space. Text Rendering. Empirical evidence suggests that mastering textual structures synergistically enhances models ability to generate other structured visual elements, thereby improving overall scene coherence. Motivated by this, we integrate synthetic text data into the pre-training corpus. As illustrated in Fig. 11, our pipeline renders text from classical literature onto diverse textures, utilizing varied color palettes and fonts. Figure 11: Process for synthesizing text rendering data."
        },
        {
            "title": "3 Model Design",
            "content": "3.1 Diffusion Model We adopt the transformer architecture of FLUX.1-dev [Labs, 2024], employing double-stream attention mechanism in the initial layers and transitioning to single-stream mechanism in the subsequent layers. To ensure parameter balance, the ratio of double-stream to single-stream blocks is maintained at approximately 1:2. The overall framework design is illustrated in Fig. 12. For the VAE component, we utilize the implementation from FLUX.1-dev. Empirical evaluations demonstrate its superior reconstruction fidelity in challenging scenarios, such as fine typography and intricate textures. Specifically, 14 LongCat-Image Technical Report input images undergo 8 spatial compression; the resulting latents are further processed via 2 2 token merging, yielding final sequence length of HW 1616 before entering the DiT module. Figure 12: Overview of model architecture. 3.2 Text Encoder The text encoder embeds user prompts into continuous representations to condition the DiT model. While prior works [Podell et al., 2023, Team, 2024, Li et al., 2024a, Esser et al., 2024, Labs, 2024] predominantly rely on CLIP [Radford et al., 2021] and T5 [Chung et al., 2024], recent studies [Gao et al., 2025, Wu et al., 2025a] have shifted toward LLMs or MLLMs to enhance multilingual compatibility, particularly for Chinese. Following this paradigm, we adopt Qwen2.5VL-7B [Bai et al., 2025] as our unified text encoder. This choice ensures robust English instruction following while significantly improving Chinese processing capabilities. Furthermore, we discard the conventional injection of text embeddings into timestep embeddings for adaLN [Peebles and Xie, 2022] modulation, as empirical evidence suggests negligible performance gains from this operation. For visual text rendering, we employ character-level tokenizer specifically for content demarcated by quotation marks. This strategy mitigates generation complexity without incurring the computational costs and memory footprint of specialized encoders (e.g., GlyphByT5 [Liu et al., 2024]). Experiments demonstrate that this approach not only improves data efficiency but also accelerates convergence for text rendering tasks. 3.3 Positional Embedding Positional embedding (PE) design is pivotal for handling variable aspect ratios and resolutions. While prior arts [Chen et al., 2024a, Li et al., 2024a, Gong et al., 2025] rely on intricate heuristicssuch as coordinate centering, frequency scaling, or interpolationto align spatial distributions, we adopt the vanilla Multimodal Rotary Position Embedding (MRoPE) [Su et al., 2024, Wang et al., 2024a] without modification. Our empirical observations indicate that the model possesses intrinsic adaptability to varying positional strides across different resolutions, rendering these explicit geometric constraints unnecessary. Consequently, MRoPE enables seamless generalization to unseen resolutions during pretraining without the computational or design overhead of complex adaptation strategies. Specifically, we employ 3D variant of MRoPE. The first dimension is designated for modality differentiation. In the text-to-image task, distinct values are assigned to distinguish tokens belonging to noise latents from those of text latents. For image editing tasks, this dimension further differentiates the latents of reference images from the aforementioned types. The remaining two dimensions encode the 2D spatial coordinates: for images, they correspond to the (x, y) positions, while for text, both coordinates are set to an identical value, analogous to the behavior of 1D-RoPE. This approach not only supports flexible image generation across arbitrary aspect ratios but also facilitates seamless interaction with other modalities, such as text and reference images. 15 LongCat-Image Technical Report Figure 13: Schematic overview of the multi-stage training pipeline. The upper panel delineates the Text-to-Image training trajectory, progressing from progressive pre-training and mid-training to post-training alignment via SFT, GRPO, and DPO. The lower panel illustrates the Image Editing workflow, which initializes from the T2I development checkpoint. 3.4 Prompt Engineering To bridge the gap between the dense captions used in training and the concise, often ambiguous queries provided by users, prompt refinement is essential. While external APIs or large language models (LLMs) may offer superior rewriting capabilities, their integration often introduces deployment constraints and latency issues. To address this, we provide default built-in solution that efficiently repurposes the existing condition encoder, Qwen2.5-VL. This design ensures an out-of-the-box capability for generating high-fidelity images, eliminating dependencies on external services while maintaining ease of use."
        },
        {
            "title": "4 Model Training",
            "content": "We establish comprehensive multi-stage training pipeline, as illustrated in Fig. 13, structured into three distinct phases: Pre-training, Mid-training, and Post-training. Pre-training: This phase adopts progressive multi-resolution strategy, facilitating the efficient acquisition of global semantic knowledge in early iterations while prioritizing high-frequency detail refinement in later stages. Mid-training: Serving as crucial bridge between raw pre-training and alignment, this phase aims to elevate the models baseline generation quality. We leverage large-scale aesthetic assessment model alongside human curation to filter high-fidelity dataset, ensuring the underlying model possesses robust aesthetic priors. Post-training: The final phase focuses on alignment and stylization, comprising SFT and Reinforcement Learning (RL). In the SFT stage, we target stylized data distributions and implement model fusion strategy to synthesize diverse stylistic capabilities. Subsequently, the RL stage incorporates advanced alignment techniquesspecifically DPO and GRPOintegrating ensemble reward models to significantly enhance instruction adherence and quality. The detailed training hyperparameters for each phase are provided in Table 1. 4.1 Pre-training Progressive Mixed-Resolution Training. We implement progressive training curriculum commencing at 256px. To optimize training efficiency and facilitate smooth resolution adaptation, we explicitly retain an intermediate 512px 16 LongCat-Image Technical Report Table 1: Progressive training recipe for LongCat-Image. PT 256px PT 512px PT 512-1024px MT 1e-4 5e-5 Constant Constant 0 900K 4608 0 300K 4608 2e-5 Constant 0 200K SFT 1e-5 DPO GRPO 1e-5 1e-5 1eConstant Consine Consine Consine 0 70K 3072 1000 20K 128 1000 4K 64 0 300 32 0.01 1.0 AdamW (β1 = 0.9, β2 = 0.95) Learning rate LR scheduler Warm-up steps Training steps Global batch size Weight decay Gradient clip Optimizer stage, avoiding the computational instability of transitioning directly from 256px to the final phase. Subsequently, the training culminates in dynamic stage covering continuous resolution range between 512px and 1024px. Throughout these phases, we employ bucket sampling to accommodate variable aspect ratios. Real-time Evaluation Protocol. To facilitate dynamic strategy adjustment and convergence analysis, we implement comprehensive monitoring system tracking validation loss, image-text alignment, aesthetic scores, and OCR-based text rendering accuracy. Empirical observations indicate that while these metrics serve as pivotal indicators during the pre-training phase, their discriminative power and sensitivity notably diminish during the mid-training and post-training stages as performance saturates. Dynamic Sampling of Synthetic Text Rendering Data. The Chinese character set exhibits distinct long-tail distribution, comprising approximately 3,000 common characters and over 5,000 rare ones that appear sparsely in natural data. To address this sparsity, we employ SynthDoG to generate large-scale dataset (over 10 million samples) rendered on simple texturessuch as paper, glass, and blackboardswith high typographic diversity (see Fig. 11). While this synthetic data significantly enhances text rendering accuracy, it inevitably compromises the overall visual harmony. To mitigate this trade-off, we implement dynamic sampling strategy based on real-time character-wise accuracy monitoring. Specifically, we increase the sampling probability for characters with high error rates, while gradually reducing synthetic exposure for well-learned characters in favor of real images. Furthermore, to prevent the model from overfitting to the simplistic synthetic domain, we completely phase out synthetic data in the final stage of pre-training. 4.2 Mid-training While the pre-training phase successfully endows the model with robust global semantic priors and text-to-image mapping capabilities, the resulting visual outputs often lack high-fidelity textures and aesthetic coherence due to the inherent noise in large-scale pre-training data. The Mid-training stage is therefore designed to constrain the learned manifold, guiding the models distribution toward subspace characterized by superior aesthetic quality and visual realism. This refinement serves as critical initialization for subsequent post-training optimization. To achieve this, we implement rigorous data curation protocol that is significantly more stringent than that of the pre-training phase. Our pipeline integrates hierarchical assessment system comprising advanced aesthetic scoring models, image quality estimators, and domain-specific classifiers, culminating in human-in-the-loop verification process. This systematic filtration yields high-fidelity corpus of millions of samples, ensuring balanced representation across diverse domains. Continued training on this curated dataset significantly elevates the models generation quality and visual fidelity. We designate the model derived from this stage as foundational Developer Version. Unlike fully aligned models subjected to extensive RL, this version retains high plasticity and adaptability, avoiding the potential mode collapse or rigidity often introduced by aggressive alignment. We release this model to the community to facilitate downstream fine-tuning and further research. 17 LongCat-Image Technical Report 4.3 Post-training 4.3.1 SFT The primary objective of the SFT phase is to elevate the models visual aesthetics through rigorous data-centric approach. This involves optimizing photorealistic attributes, such as compositional integrity, lighting dynamics, and photographic techniques, while simultaneously ensuring stylistic fidelity across various artistic domains. High-Fidelity Data Curation. We employ hybrid dataset comprising hundreds of thousands of samples that blends real-world imagery with high-quality synthetic data. To prevent the degradation of realism often associated with synthetic artifacts, we enforce strict expert verification protocol. This process ensures that only data possessing superior textural quality and aesthetic value are retained, and it strictly filters out any samples that might compromise the models generation fidelity. Model Weight Averaging. Recognizing that diverse training subsets yield models with complementary strengths, we fine-tune multiple candidate models where each is specialized in distinct visual dimensions, including illumination, portraiture, and artistic style. To synthesize these capabilities, we adopt model parameter averaging strategy. By merging the weights of these specialized models, we effectively balance performance across multiple attributes. This fusion process significantly enhances overall robustness and stability, effectively mitigating the specific biases or deficits inherent in individual single-domain models. Optimization of Timestep Sampling. Unlike the pre-training phase, which prioritizes global structural formation, SFT focuses on refining high-frequency details that typically emerge during the later stages of the diffusion process. Consequently, we transition from the Logit-Normal sampling strategy to Uniform Sampling. This adjustment ensures balanced exposure across all timesteps, specifically increasing the training weight of high-frequency denoising steps to maximize the models capacity for learning intricate textures and fine details. 4.3.2 RLHF We develop fine-grained reward models (RMs), including distortion detection, AIGC detection, human preference assessment, and OCR accuracy, to comprehensively evaluate the models detailed capabilities. Using these RMs, we employ three distinct RL strategies: Direct Preference Optimization (DPO) [Rafailov et al., 2023, Wallace et al., 2024], Group Relative Policy Optimization (GRPO) [Xue et al., 2025], and our proposed Monolithic Policy Optimization (MPO). DPO excels at offline preference modeling for flow-matching models with high computational efficiency, whereas GRPO and MPO perform on-policy sampling during training with reward model evaluation. MPO fundamentally improves upon GRPO by eliminating the group-relative paradigm and its associated synchronization bottlenecks, achieving superior training efficiency and stability. To leverage the scalability advantages of offline preference learning, we conduct relatively large-scale RL with DPO and reserve on-policy methods (MPO or GRPO) for small fine-grained RL refinement. Details of each algorithm are provided below. (A) Direct Preference Optimization (DPO) Data Construction 1) PromptSet: category-balanced PromptSet is constructed from real user queries of public datasets, refined through clustering and data-cleaning techniques to ensure representativeness and diversity. 2) ImagePair: We utilize diverse random initialization seeds to generate 6 candidate images for each prompt sample. An annotation team then assigns subjective quality scores (15) to each sample. To ensure clarity and effectiveness of training, we discard neutral samples (score = 3), treating high-quality (45) samples as positive and low-quality (12) samples as negative, thereby forming win-lose pairs for DPO training. This strategy ensured high confidence and preference discernment in the training data. Algorithm We employ the DPO algorithm to mitigate common structural deficiencies in the model. Based on the SFT model, we construct preference dataset through diversified data sampling and manual curation, optimizing the model as follows: 18 LongCat-Image Technical Report L(θ) = (xw 0 ,xl (cid:32) (cid:32) 0)D,tU (0,T ),xw q(xw xw 0 ),xl tq(xl txl 0) log σ βT ω(λt) vw vθ(xw , t) 2 vw vref(xw , t)2 2 (cid:0)vl vθ(xl t, t)2 2 vl vref(xl t, t)2 (cid:33)(cid:33) (cid:1) . (1) We optimize the model according to Equation 2. During training, we further explore strategies including multi-round DPO iterations, gradient norm-based dirty data skipping, and KL constraints, conducting comparative analyses of different approaches impacts on model performance. Ultimately, DPO significantly reduce the models bad case rate and enhance the robustness of image generation. (B) Group Relative Policy Optimization (GRPO) Algorithm After training with DPO, we perform further fine-grained training using GRPO following the DanceGRPO [Ma et al., 2024] framework. Given text hidden state h, the flow model predicts group of images {xi and the corresponding trajectory {xi 0}G i=1 i=1. Within each group, the advantage function is formulated as: , xi 1, ..., xi R(xi Ai = 0}G 0, h) mean({R(xi 0, h)}G std({R(xi i=1) 0, h)}G i=1) . The training objective of GRPO is: LGRPO(θ) = hD,{xi ,...,xi 0}G i=1πθ (cid:34) 1 G (cid:88) i=1 1 1 (cid:88) (cid:18) t= min (cid:0)ri t(θ)Ai, clip(ri t(θ), 1 ϵ, 1 + ϵ)Ai where ri t(θ) = pθ(xi pθold (xi t1xi txi t,h) t,h) . During trajectory sampling, we reformulate the deterministic flow-matching ODE as an SDE for effective exploration: (cid:18) dxt = vt + σ2 2t (xt + (1 t)vt) dt + σtdw, (cid:19) with Euler-Maruyama discretization: (cid:20) vθ(xt, t, h) + xt+t = xt + (xt + (1 t)vθ(xt, t, h)) + σt (cid:21) tϵ. σ2 2t (4) (5) (C) Monolithic Policy Optimization (MPO) Algorithm MPO employs single-stream on-policy optimization paradigm that eliminates group-relative synchronization bottlenecks inherent in GRPO. For each prompt, MPO generates single trajectory using Stochastic Differential Equation (SDE) sampler and performs one gradient update, achieving superior computational efficiency. The generative process is governed by the SDE: dz = vθ(zt, c, t)dt + g(t)dw, (6) where g(t)dw is the diffusion term enabling exploration within single trajectory. Using Euler-Maruyama discretization: zt+t = zt + vθ(zt, c, t)t + gt tϵt, ϵt (0, I). (7) MPO incorporates three synergistic components for stable variance control: 1. Gaussian Value Tracker with KL-Adaptive Forgetting: persistent Bayesian tracker maintains per-prompt reward estimates V(c) = (µc, σ2 quantifies epistemic uncertainty. Updates follow Kalman filter principles: ). The mean µc serves as stable baseline, while the variance σ2 Kt = σ2 c,t1 c,t1 + σ2 σ2 obs , µc,t µc,t1 + Kt(r µc,t1), c,t (1 Kt)σ2 σ2 c,t1 + Qt, 19 (8) (2) (3) (cid:19)(cid:35) (cid:1) , LongCat-Image Technical Report where Qt = α DKL(πθπθ) adaptively scales process noise with policy drift. 2. Global Advantage Normalization: Raw advantages = µc are normalized using exponential moving averages: = µA (cid:112)σ2 + ϵ . 3. Uncertainty-Powered Curriculum: Prompt sampling probability follows p(c) σc + η/ high-uncertainty prompts. The policy update uses advantage-weighted regression: (9) nc + 1, prioritizing LMPO(θ) = Et,ztτ (cid:104) stop_grad(wc A) (cid:13) (cid:13)vθ(zt, c, t) u(zt, z0)(cid:13) (cid:13) 2(cid:105) , (10) where u() is the target flow-matching vector field and wc = 1 + γ µc/(σc + ϵ). Training Strategy and Implementation Details. GRPO and MPO experiments are initialized from the same DPO base model. We optimize using the AdamW optimizer [Loshchilov and Hutter, 2017] with constant learning rate of 5 106 and global batch size of 64. For MPO and GRPO, we employ 12-step SDE sampler with Euler-Maruyama discretization. The diffusion coefficient gt is linearly annealed from 0.1 to 0 during training. For MPO-specific hyperparameters, we set the EMA decay for advantage normalization to λ = 0.99, the curriculum balance coefficient to η = 1.0, the adaptive scaling factor α = 1.0, and the surprise reweighting factor to γ = 0.5."
        },
        {
            "title": "5 Model Performance",
            "content": "5.1 Benchmarks We conduct comprehensive evaluation using suite of established public benchmarks, including GenEval [Ghosh et al., 2023], DPG-Bench [Hu et al., 2024], and WISE [Niu et al., 2025] for general generative capabilities, as well as GlyphDraw2 [Ma et al., 2025a] and CVTG-2K [Du et al., 2025] for text rendering proficiency. Furthermore, to rigorously assess Chinese character coverage, we construct complete dictionary-based benchmark, ChineseWord, following the protocol of Qwen-Image. Finally, to validate performance in production environments, we introduce proprietary dataset focusing on business-critical scenarios such as poster design and natural scenes with text. 5.1.1 Text-Image Alignment GenEval evaluates the fine-grained controllability of generative models, specifically targeting attribute binding, quantitative relations, and spatial composition. As shown in Table 2, LongCat-Image exhibits superior performance on GenEval, demonstrating robust capabilities in handling complex compositional constraints and entity attributes. DPG-Bench comprises 1,065 dense and structurally complex prompts designed to challenge the semantic alignment of text-to-image models. As presented in Table 3, LongCat-Image achieves competitive alignment performance, ranking closely behind leading models such as Qwen-Image and Seed4.0, thereby validating its proficiency in interpreting verbose captions. WISE comprises 1,000 curated prompts aimed at rigorously testing semantic comprehension and world knowledge. During evaluation, we leverage the off-the-shelf text encoder for intrinsic prompt enhancement. Results indicate that LongCat-Image achieves state-of-the-art (SOTA) scores among open-source diffusion models, underscoring its robust reasoning capabilities and responsiveness to semantic nuances. Table 4 details these findings. 5.1.2 Text Rendering GlyphDraw2 assesses text generation across two distinct subsets. The Poster-Set (200 prompts) evaluates bilingual generation in design contexts, while the Complex-Set challenges models with random combinations drawn from pool of 2,000 frequent Chinese characters to test coverage. As illustrated in Table 5, LongCat-Image excels particularly in the Complex-Set, highlighting its robustness in rendering intricate character structures. CVTG-2K focuses on English text rendering across diverse real-world scenarios, including street views, advertisements, and memes. Each prompt features multi-region layouts (2 to 5 regions) to test spatial text placement. LongCat-Image attains SOTA performance on this benchmark (see Table 5), demonstrating exceptional effectiveness in complex, multi-turn English text rendering tasks. 20 LongCat-Image Technical Report Table 2: Quantitative evaluation results on GenEval. Model Single Object Object Two Counting Colors Position Attribute Binding Overall Show-o [Xie et al., 2024] Emu3 [Wang et al., 2024b] PixArt-α [Chen et al., 2024a] SD-3-Medium [Esser et al., 2024] FLUX.1-dev [Labs, 2024] SD-3.5-large [Stabilityai, 2024] JanusFlow [Ma et al., 2025b] Lumina-Image 2.0 [Qin et al., 2025] Janus-Pro-7B [Chen et al., 2025] HiDream-I1-Full [Cai et al., 2025] GPT Image 1 [High] [OpenAI, 2025] Seedream 3.0 [Gao et al., 2025] Seedream 4.0 [Gao et al., 2025] Qwen-Image [Wu et al., 2025a] HunyuanImage-3.0 [Cao et al., 2025] LongCat-Image 0.95 0.98 0.98 0.98 0.98 0.98 0.97 - 0.99 1.00 0.99 0.99 0.99 0.99 1.00 0.99 0.52 0.71 0.50 0.74 0.81 0.89 0.59 0.87 0.89 0.98 0.92 0.96 0.92 0.92 0.92 0. 0.49 0.34 0.44 0.63 0.74 0.73 0.45 0.67 0.59 0.79 0.85 0.91 0.72 0.89 0.48 0.86 0.82 0.81 0.80 0.67 0.79 0.83 0.83 - 0.90 0.91 0.92 0.93 0.91 0.88 0.82 0.86 0.11 0.17 0.08 0.34 0.22 0.34 0.53 - 0.79 0.60 0.75 0.47 0.76 0.76 0.42 0. 0.28 0.21 0.07 0.36 0.45 0.47 0.42 0.62 0.66 0.72 0.61 0.80 0.74 0.77 0.63 0.73 0.53 0.54 0.48 0.62 0.66 0.71 0.63 0.73 0.80 0.83 0.84 0.84 0.84 0.87 0.72 0.87 Table 3: Quantitative evaluation results on DPG. Model Global Entity Attribute Relation Other Overall PixArt-α [Chen et al., 2024a] Lumina-Next [Zhuo et al., 2024] SDXL [Podell et al., 2023] Playground v2.5 [Li et al., 2024b] Hunyuan-DiT [Li et al., 2024a] Janus [Wu et al., 2025b] PixArt-Σ [Chen et al., 2024c] Emu3-Gen [Wang et al., 2024b] Janus-Pro-1B [Chen et al., 2025] DALL-E 3 [OpenAI, 2023] FLUX.1-dev [Labs, 2024] SD-3-Medium [Esser et al., 2024] Janus-Pro-7B [Ma et al., 2025b] HiDream-I1-Full [Cai et al., 2025] Lumina-Image 2.0 [Qin et al., 2025] Seedream 3.0 [Gao et al., 2025] GPT Image 1 [High] [OpenAI, 2025] Seedream 4.0 [Seedream et al., 2025] Qwen-Image Wu et al. [2025a] HunyuanImage-3.0 [Cao et al., 2025] 74.97 82.82 83.27 83.06 84.59 82.33 86.89 85.21 87.58 90.97 74.35 87.90 86.90 76.44 - 94.31 88.89 94.10 91.32 92.12 79.32 88.65 82.43 82.59 80.59 87.38 82.89 86.68 88.63 89.61 90.00 91.01 88.90 90.22 91.97 92.65 88.94 92.28 91.56 92.53 LongCat-Image 89. 92.54 78.60 86.44 80.91 81.20 88.01 87.70 88.94 86.84 88.17 88.39 88.96 88.83 89.40 89.48 90.20 91.36 89.84 92.75 92.02 89.13 92.00 82.57 80.53 86.76 84.08 74.36 85.46 86.59 90.22 88.98 90.58 90.87 80.70 89.32 93.74 94.85 92.78 92.63 93.67 94.31 92.13 93.28 76.96 81.82 80.41 83.50 86.41 86.41 87.68 83.15 88.30 89.83 88.33 88.68 89.48 91.83 - 88.24 90.96 92.77 92.73 91. 87.50 71.11 74.63 74.65 75.47 78.87 79.68 80.54 80.60 82.63 83.50 83.84 84.08 84.19 85.89 87.20 88.27 85.15 88.25 88.32 86.10 86.80 21 LongCat-Image Technical Report Table 4: Quantitative evaluation results of world knowledge reasoning on WISE. Model Cultural Time Space Biology Physics Chemistry Overall GPT4o [OpenAI, 2025] MetaQuery-XL [Pan et al., 2025] Liquid [Wu et al., 2025c] Emu3 [Wang et al., 2024b] Janus-1.3B [Wu et al., 2025b] JanusFlow [Ma et al., 2025b] Janus-Pro-1B [Chen et al., 2025] Janus-Pro-7B [Chen et al., 2025] Orthus-7B-instruct [Kou et al., 2024] Show-o-512 [Xie et al., 2024] Unified Models 0.71 0.55 0.45 0.45 0.26 0.26 0.28 0.37 0.31 0.40 0.89 0.62 0.48 0.48 0.35 0.28 0.45 0.49 0.38 0.48 0.81 0.56 0.34 0.34 0.16 0.13 0.20 0.30 0.23 0.28 Text-to-Image Models FLUX.1-dev [Labs, 2024] FLUX.1-schnell [Labs, 2024] PixArt-α [Chen et al., 2024a] Playground-v2.5 [Li et al., 2024b] SD-3-medium [Esser et al., 2024] SD-3.5-medium [Stabilityai, 2024] SD-3.5-large [Stabilityai, 2024] Seedream 4.0 [Seedream et al., 2025] Qwen-Image [Wu et al., 2025a] HunyuanImage-3.0 [Cao et al., 2025] LongCat-Image 0.48 0.39 0.45 0.49 0.42 0.43 0.44 0.78 0.62 0.58 0. 0.58 0.44 0.50 0.58 0.44 0.50 0.50 0.73 0.63 0.57 0.61 0.62 0.50 0.48 0.55 0.48 0.52 0.58 0.85 0.77 0.70 0.72 0.83 0.49 0.41 0.41 0.28 0.20 0.24 0.36 0.28 0.30 0.42 0.31 0.49 0.43 0.39 0.41 0.44 0.79 0.57 0.56 0.66 0.79 0.63 0.45 0.45 0.30 0.19 0.32 0.42 0.31 0.46 0.51 0.44 0.56 0.48 0.47 0.53 0.52 0.84 0.75 0.63 0. 0.74 0.41 0.27 0.27 0.14 0.11 0.16 0.26 0.20 0.30 0.35 0.26 0.34 0.33 0.29 0.33 0.31 0.67 0.40 0.31 0.49 0.80 0.55 0.39 0.39 0.23 0.18 0.26 0.35 0.27 0.35 0.50 0.40 0.47 0.49 0.42 0.45 0.46 0.78 0.62 0.57 0.65 Table 5: Quantitative evaluation results of GlyphDraw2. Model Seedream 4.0 [Seedream et al., 2025] Qwen-Image [Wu et al., 2025a] HunyuanImage-3.0 [Cao et al., 2025] LongCat-Image Complex-en Complex-zh 0.99 0.90 0.47 0.94 0.91 0.87 0.85 0.92 Poster-en 0.99 0.95 0.90 0.95 Poster-zh Avg 0.97 0.93 0.78 0.95 0.99 0.98 0.90 0. Table 6: Quantitative evaluation results of CVTG-2K. Model Word Accuracy NED CLIPScore 2 regions 3 regions 4 regions 5 regions average Seedream 4.0 [Seedream et al., 2025] Qwen-Image [Wu et al., 2025a] HunyuanImage-3.0 [Cao et al., 2025] LongCat-Image 0.8898 0.8370 0.8300 0.9129 0.9147 0.8364 0.7635 0. 0.8991 0.8313 0.7384 0.8557 0.8873 0.8158 0.7279 0.8310 0.8917 0.8288 0.7650 0.8658 0.9507 0.9297 0.8765 0.9361 0.7853 0.8059 0.8121 0.7859 ChineseWord To evaluate the full spectrum of Chinese character rendering, especially for long-tail characters, we constructed comprehensive benchmark comprising 8,105 prompts based on the General Standard Chinese Characters 22 LongCat-Image Technical Report Table 7: Quantitative evaluation results of ChineseWord. Model Seedream 4.0 [Seedream et al., 2025] Qwen-Image [Wu et al., 2025a] HunyuanImage-3.0 [Cao et al., 2025] LongCat-Image 94.8 92.5 83.5 98.7 L2 41.2 37.1 31.3 90.8 L3 2.3 6.1 4.1 70.3 Overall 58.5 56.6 49.3 90.7 Table 8: Quantitative evaluation results of internal poster and scene text scenarios. Model Poster Real Scene Seedream 4.0 [Seedream et al., 2025] Qwen-Image [Wu et al., 2025a] HunyuanImage-3.0 [Cao et al., 2025] LongCat-Image 93.2 88.7 89.0 92.0 90.0 89.6 85.1 91.0 Avg 91.6 89.2 87.1 91.5 Table2, aligning with the protocol of Qwen-Image. Each character is embedded in standardized template (e.g., On the blackboard, the word 华 is written in purple Song font.). We employ PPOCRv5 [Cui et al., 2025] for objective accuracy quantification, as existing MLLMs often struggle to recognize rare characters. Results in Table 7 demonstrate that LongCat-Image outperforms all existing models by significant margin. However, we acknowledge that while exhibiting dominance in single-character rendering, the model experiences noticeable decline in stability when generating multi-character sequences, primarily due to the insufficient scale of real-world textual training data. In future work, we aim to address this by rigorously expanding our text-rich dataset collection to enhance robustness in complex multi-character generation tasks. Poster&SceneBench To bridge the gap between academic benchmarks and industrial applications, we curate dataset of 500 prompts covering both poster typography and natural scene text. Unlike flat poster layouts, the latter specifically evaluates the models capability to seamlessly integrate text into real-world environments (e.g., signage on textured surfaces or shop fronts with complex lighting). As indicated in Table 8, LongCat-Image delivers SOTA-level performance, proving its reliability and effectiveness in these practical operational contexts. 5.2 Human Evaluation To assess perceptual quality, we adopt the Mean Opinion Score (MOS) protocol, focusing on four distinct dimensions: text-image alignment, visual plausibility, visual realism, and aesthetics. Text-Image Alignment measures the semantic fidelity of the generated image to the input prompt. It evaluates the accurate depiction of key elements, including entities, attributes, spatial relationships, and stylistic constraints. Plausibility examines the images adherence to physical coherence and logical consistency. This metric penalizes anatomical distortions, unnatural proportions, and spatial anomalies that violate real-world physics. Realism assesses the degree of photorealism and texture fidelity. It serves as proxy for the Turing test of image generation, determining how indistinguishable the synthesized output is from authentic photography, free from AI-generated artifacts. Aesthetics evaluates the artistic quality and perceptual appeal, considering factors such as composition, lighting, color harmony, and overall visual impact. Evaluation dataset. To ensure rigorous and unbiased assessment, we constructed diverse dataset of 400 prompts tailored for black-box evaluation. This corpus spans broad spectrum of difficulty, ranging from fundamental entity depiction to intricate scene synthesis. It comprehensively covers challenging generative dimensions, including multientity interactions, spatial layouts, dynamic actions, artistic creativity, text rendering, and world knowledge reasoning. 2http://www.moe.gov.cn/jyb_sjzl/ziliao/A19/201306/t20130601_186002.html LongCat-Image Technical Report Result Analysis. As illustrated in Fig. 14, LongCat-Image demonstrates comprehensive superiority over HunyuanImage 3.0 across all metrics. Compared to Qwen-Image, our model achieves parity in both Text-Image Alignment and Visual Plausibility. Notably, LongCat-Image excels in Visual Realism, outperforming Qwen-Image and even exhibiting slight advantage over the commercial baseline, Seedream 4.0. While there remains marginal gap in Visual Aesthetics compared to Qwen-Image, the overall human evaluation indicates that LongCat-Image delivers performance comparable to SOTA open-source models. Figure 14: Comparison of human evaluation MOS. 5.3 Qualitative Results In Fig. 15, 16, 17, 18, we present qualitative comparisons between our LongCat-Image and leading SOTA baselines. Visual inspection confirms that our model exhibits robust performance across critical dimensions, including text-image alignment, visual plausibility, realism, and aesthetic quality. Furthermore, it demonstrates exceptional proficiency in text rendering tasks."
        },
        {
            "title": "Image Editing",
            "content": "Extending Text-to-Image foundation model for image editing is well-established paradigm, effectively validated by prior works [Batifol et al., 2025, Wang et al., 2025b, Wu et al., 2025a]. In this section, we detail the adaptation of LongCat-Image into LongCat-Image-Edit, which achieves SOTA performance among open-source models. 6.1 Data Curation Distinct from standard text-to-image pre-training, image editing necessitates source-target image pairs. We curate comprehensive training set from diverse sources, including open-source datasets, synthetic pipelines, video sequences, and interleaved web corpora. To enhance the models instruction-following capabilities, which range from simple descriptions to complex reasoning, we apply extensive instruction rewriting strategies to maximize linguistic diversity. Fig. 19 illustrates the proportional distribution of editing tasks within our training set. 6.1.1 Open-Source Datasets Given the high cost of annotating editing pairs, we prioritize leveraging high-quality open-source repositories, specifically OmniEdit [Wei et al., 2024], OmniGen2 [Wu et al., 2025d], and NHREdit [Kuprashevich et al., 2025]. We implement rigorous data cleaning pipeline on these datasets and rewrite the original instructions. This process efficiently yields high-fidelity training pairs tailored for diverse editing tasks. 6.1.2 Synthesized Data Specialized expert models demonstrate exceptional performance in specific common editing tasks. Leveraging their capabilities, we synthesize high-quality training pairs for tasks such as object manipulation, style transfer, background alteration, and reference-based generation. For each task, we establish dedicated pipeline where MLLMs craft editing instructions, and the corresponding expert models generate the target images. Complementarily, we employ traditional 24 LongCat-Image Technical Report Figure 15: Comparison of overall capability in image generation. 25 LongCat-Image Technical Report Figure 16: Comparison of text rendering capability in image generation. 26 LongCat-Image Technical Report Figure 17: Comparison of text rendering capability in image generation. 27 LongCat-Image Technical Report Figure 18: Comparison of text rendering capability in image generation. 28 LongCat-Image Technical Report image processing algorithms to construct data for low-level adjustments like filter transformations and lighting changes. Furthermore, we incorporate human-in-the-loop verification for critical samples to ensure semantic alignment and visual fidelity. This approach allows us to accumulate substantial volume of high-quality data. 6.1.3 Video Frames Synthetic methods often struggle with complex structural changes, such as human pose and perspective, frequently introducing artifacts. To bridge this gap, we leverage video sequences that naturally capture realistic temporal transitions. Our pipeline employs multimodal models to identify target objects and optical flow estimation to quantify changes between frames. We extract keyframe pairs that exhibit significant yet coherent variations and automatically annotate them with editing instructions. subset of these pairs undergoes manual verification to guarantee accuracy. 6.1.4 Interleaved Corpus While the aforementioned data sources effectively cover standard editing categories, they often lack coverage for long-tail scenarios. To bridge this gap and significantly enrich the diversity of editing instructions, we exploit web-scale interleaved corpora. By mining large-scale image-text sequences with inherent semantic correlations, we extract implicit editing signals from naturally occurring data. These raw pairs undergo rigorous filtering and multimodal-assisted instruction rewriting to ensure their suitability for training. However, mining valid training samples from such massive unstructured corpora is an extremely resource-intensive endeavor. Consequently, the scale of data we have curated to date remains limited. We firmly believe that this represents critical direction for long-term data engineering and encourage broader community participation to further explore this valuable frontier. 6.1.5 Instruction Rewriting Since synthetic instructions often diverge from real-world user prompts and complex reasoning benchmarks, we employ GPT-4o [Hurst et al., 2024] to enhance instruction diversity. We implement one-to-many strategy, associating each editing pair with multiple rewritten variants, including natural language paraphrases and compound commands. This approach aligns training data with diverse inference scenarios, benefit subsequently validated by our experimental results. Figure 19: Overview of image editing pre-training data. LongCat-Image Technical Report Figure 20: Overview of LongCat-Image-Edit model architecture. 6.2 Model Design Building upon the base architecture and drawing inspiration from prior works [Wang et al., 2025b, Wu et al., 2025a], we introduce an image conditioning branch via modifications to VAE features, 3D RoPE embeddings, and token sequencing. Specifically, reference images are encoded into VAE latents and distinguished from noised latents by manipulating the first dimension of the 3D RoPE embeddings, while preserving spatial alignment in the remaining dimensions. These reference tokens are then concatenated with noised latents along the sequence dimension to serve as input for the diffusion visual stream. Furthermore, we feed both the source image and instructions into the multimodal encoder (i.e., Qwen2.5-VL). To differentiate editing tasks from standard text-to-image generation, we implement distinct system prompt during this feature extraction process. The overall schematic of the model architecture is illustrated in Fig. 20. 6.3 Model Training As illustrated in Fig. 13, the training framework comprises three progressive stages: Pre-training, SFT, and DPO. This multi-stage curriculum is designed to systematically enhance the resolution and visual fidelity of the generated images. 6.3.1 Pre-training We initialize the model using mid-training T2I checkpoint, as its unconstrained parameter space offers superior plasticity compared to post-trained models. Our training follows multi-scale strategy: we begin at 512512 resolution with massive, noisy datasets for rapid convergence, then progress to 10241024 with high-quality data to refine details. Simultaneously, we adopt joint training strategy, mixing editing data with T2I mid-training data at balanced batch ratio. Since experiments confirm this approach improves both semantic understanding and image quality, we retain it for the following SFT stage. Furthermore, to enhance instruction generalization, we associate each sample with 3-5 candidate prompts (in Chinese and English) and randomly select one during each training iteration. 6.3.2 SFT During the SFT stage, we curate high-fidelity dataset comprising hundreds of thousands of samples from real photographs, professional manual retouches, and synthetic sources. To guarantee generation stability, we implement rigorous human-in-the-loop filtering protocol, specifically targeting the structural alignment between source and 30 LongCat-Image Technical Report edited images. Our experiments reveal high sensitivity to data quality: even marginal relaxation of these alignment standards leads to precipitous drop in the models ability to maintain consistency. Consequently, we enforce the strictest criteria to ensure precise content preservation. Furthermore, by jointly training this strictly filtered corpus with high-quality T2I SFT data, we achieve significant improvements in both instruction adherence and aesthetic quality. 6.3.3 DPO To further align the model with human aesthetic standards and mitigate persistent structural artifacts, we employ Direct Preference Optimization (DPO) following the SFT stage. We construct high-quality preference dataset via diverse sampling and rigorous manual annotation, optimizing the diffusion DPO objective defined as: L(θ) = (I src,P w,I (cid:32) (cid:32) src,P l)D, tU (0,T ), xw q(xw src,P w), xl tq(xl tI src,P l) log σ βT ω(λt) vw vθ(xw , src, w, t) 2 vw vref(xw , src, w, t)2 2 (cid:0)vl vθ(xl t, src, l, t) 2 vl vref(xl t, src, l, t)2 2 (cid:33)(cid:33) (cid:1) . (11) Data Construction. The preference dataset is curated in two steps. (1) Prompt Curation: We synthesize categorybalanced image-instruction pairs leveraging both Multimodal LLMs and real-world user queries. These inputs undergo clustering and filtration to ensure semantic diversity and representativeness. (2) Preference Annotation: For each unique prompt, we generate five candidate outputs using distinct random seeds. professional annotation team evaluates these candidates to identify the most successful edit (winner) and the failed instances (losers). This rigorous selection process ensures high-confidence preference signals, forming robust win-lose pairs for training. Training Strategy. We optimize the model using Eq. (11). To stabilize training and prevent reward hacking, we incorporate advanced strategies such as gradient-based outlier rejection (to skip noisy data) and KL divergence constraints. Comparative analysis confirms that these techniques are crucial for performance gains. Ultimately, DPO significantly reduces the failure rate (e.g., structural artifacts) and enhances the overall robustness of the image generation. 6.4 Discussion Initially, we aim to unify T2I and image editing into single model to leverage potential task synergies and minimize deployment costs. However, experiments reveal critical data quality mismatch: the heavy reliance on synthetic data during editing pre-training noticeably degrades the photorealism of T2I generation compared to models trained solely on real data. Consequently, we decided to separate the models to ensure optimal performance for each task. We emphasize that this is data-driven issue, not an architectural flaw. We believe that by substituting synthetic datasets with large-scale interleaved corpora, future iterations can successfully merge these capabilities into unified model without sacrificing generation quality. 6.5 Model Performance In this section, we comprehensively evaluate our model across three quantitative benchmarks: CEdit-Bench (Ours)3, GEdit-Bench [Liu et al., 2025], and ImgEdit-Bench [Ye et al., 2025]. Furthermore, we conduct qualitative comparison against leading models on complex editing tasks to demonstrate practical utility. 6.5.1 Benchmarks CEdit. While numerous benchmarks exist for image editing, they often exhibit specific limitations in task coverage and granularity. For instance, GEdit-Bench, despite its popularity, lacks tasks involving reference image generation, structural modification, and viewpoint transformation. Similarly, ImgEdit-Bench offers limited scope, KontextBench [Batifol et al., 2025] suffers from coarse task granularity and low instruction diversity, and Reason-Edit [Huang et al., 2024] prioritizes reasoning over conventional editing capabilities. To address these gaps, we introduce CEdit-Bench, comprehensive evaluation suite derived from the integration and expansion of these existing benchmarks. We curate new data to enrich task diversity, resulting in robust dataset 3https://huggingface.co/datasets/meituan-longcat/CEdit-Bench LongCat-Image Technical Report Figure 21: Task category distribution of CEdit-Bench. comprising 1,464 bilingual (Chinese and English) editing pairs across 15 fine-grained task categories, as illustrated in Fig. 21. This establishes CEdit-Bench as more holistic and rigorous evaluation standard. We benchmark our model against FLUX.1 Kontext [Batifol et al., 2025], Step1X-Edit [Liu et al., 2025], QwenImage-Edit [Wu et al., 2025a], Seedream 4.0 [Seedream et al., 2025], and Nano Banana (Gemini-2.5-flash-image)4 on CEdit-Bench. Following standard evaluation protocols, we employ Semantic Consistency (SQ), Perceptual Quality (PQ), and an Overall Score (O) as metrics, utilizing GPT-4o for automated evaluation. As shown in Table 9, our model achieves SOTA performance among open-source models. GEdit. To benchmark against established standards, we evaluate our model on GEdit-Bench, comparing it with both popular open-source and proprietary products. As reported in Table 10, our model achieves top-tier performance, demonstrating superior instruction-following capabilities in both Chinese and English. ImgEdit. Serving as complement to GEdit, ImgEdit-Bench evaluates models with focus on instruction adherence, editing quality, and detail preservation. We compare our model against competitive baselines using the official metrics provided by the benchmark. The results in Table 11 indicate that our model outperforms all competitors, further validating its comprehensive capabilities across diverse evaluation dimensions. 6.5.2 Human Evaluation To benchmark our model against SOTA open-source models and leading commercial products, we conduct Sideby-Side (SBS) human evaluation. This assessment focuses on two primary dimensions: comprehensive quality and consistency. 4https://aistudio.google.com/models/gemini-2-5-flash-image 32 LongCat-Image Technical Report Table 9: Performance comparison on CEdit-Bench. Model CEdit-Bench-EN CEdit-Bench-CN G_SC G_PQ G_O G_SC G_PQ G_O FLUX.1 Kontext [Pro] [Batifol et al., 2025] GPT Image 1 [High] [OpenAI, 2025] Nano Banana [Google, 2025] Seedream 4.0 [Seedream et al., 2025] FLUX.1 Kontext [Dev] [Batifol et al., 2025] Step1X-Edit [Liu et al., 2025] Qwen-Image-Edit [Wu et al., 2025a] Qwen-Image-Edit [2509] [Wu et al., 2025a] LongCat-Image-Edit 6.79 8.64 7.51 8.12 6.31 6.68 8.07 8.04 8.27 7.80 8.26 8.17 7.95 7.56 7.36 7.84 7.79 7.88 6.53 8.17 7.20 7.58 5.93 6.25 7.52 7.48 7. 1.15 8.67 7.67 8.14 1.25 6.88 8.03 7.93 8.25 8.07 8.26 8.21 7.95 7.66 7.28 7.78 7.71 7.85 1.43 8.21 7.36 7.57 1.51 6.35 7.46 7.37 7. Table 10: Performance comparison on GEdit-Bench. Model GEdit-Bench-EN GEdit-Bench-CN G_SC G_PQ G_O G_SC G_PQ G_O Gemini 2.0 [DeepMind, 2025] FLUX.1 Kontext [Pro] [Batifol et al., 2025] GPT Image 1 [High] [OpenAI, 2025] Nano Banana [Google, 2025] Seedream 4.0 [Seedream et al., 2025] InstructPix2Pix [Brooks et al., 2023] AnyEdit [Yu et al., 2025] MagicBrush [Zhang et al., 2023] UniWorld-v1 [Lin et al., 2025] OmniGen [Xiao et al., 2025] OmniGen2 [Wu et al., 2025d] FLUX.1 Kontext [Dev] [Batifol et al., 2025] BAGEL [Deng et al., 2025] Step1X-Edit [Liu et al., 2025] Qwen-Image-Edit [Wu et al., 2025a] Qwen-Image-Edit [2509] [Wu et al., 2025a] LongCat-Image-Edit 6.73 7.02 7.85 7.86 8.24 3.58 3.18 4.68 4.93 5.96 7.16 6.52 7.36 7.66 8.00 8.15 8.18 6.61 7.60 7.62 8.33 8.08 5.49 5.82 5.66 7.43 5.89 6.77 7.38 6.83 7.35 7.86 7.86 8.00 6.32 6.56 7.53 7.54 7. 3.60 3.21 4.52 4.85 5.06 6.41 6.00 6.52 6.97 7.56 7.54 7.64 5.43 1.11 7.67 7.51 8.19 - - - - - - - 7.34 7.20 7.82 8.05 8.08 6.78 7.36 7.56 8.31 8.14 - - - - - - - 6.85 6.87 7.79 7.88 7.99 5.36 1.23 7.30 7.25 7. - - - - - - - 6.50 6.86 7.52 7.49 7.60 Comprehensive Quality. This metric evaluates the overall performance of image editing across multiple aspects, including instruction adherence, visual plausibility, aesthetics, and the consistency between original and edited images. Annotators provide holistic judgment by categorizing the result as Win, Tie, or Loss. Consistency. We conduct dedicated evaluation for this dimension, distinct from the comprehensive score, to emphasize its critical role in multi-turn editing. This metric specifically scrutinizes whether attributes in non-edited regions, such as layout, texture, color tone, and subject identity, remain invariant unless targeted by the instruction. Evaluation Dataset. We curate diverse dataset comprising approximately 400 samples tailored for black-box evaluation. The dataset covers broad spectrum of difficulty levels and includes various editing tasks, such as global editing, local editing, text modification, and reference-guided editing. 33 LongCat-Image Technical Report Table 11: Performance comparison on ImgEdit-Bench. Model Add Adjust Extract Replace Remove Background Style Hybrid Action Overall FLUX.1 Kontext [Pro] [Batifol et al., 2025] GPT Image 1 [High] [OpenAI, 2025] Nano Banana [Google, 2025] Seedream4.0 [Seedream et al., 2025] MagicBrush [Zhang et al., 2023] InstructPix2Pix [Brooks et al., 2023] AnyEdit [Yu et al., 2025] UltraEdit [Zhao et al., 2024] OmniGen [Xiao et al., 2025] ICEdIt [Zhang et al., 2025] Step1X-Edit [Liu et al., 2025] BAGEL [Deng et al., 2025] UniWorld-V1 [Lin et al., 2025] OmniGen2 [Wu et al., 2025d] FLUX.1 Kontext [Dev] [Batifol et al., 2025] Qwen-Image-Edit [Wu et al., 2025a] Qwen-Image-Edit [2509] [Wu et al., 2025a] LongCat-Image-Edit 4.25 4.61 4.50 4.52 2.84 2.45 3.18 3.44 3.47 3.58 3.88 3.56 3.82 3.57 4.12 4.38 4.32 4.51 4.15 4.33 4.47 4.41 1.58 1.83 2.95 2.81 3.04 3.39 3.14 3.31 3.64 3.06 3.80 4.16 4.36 4.57 2.35 2.90 3.75 2.93 1.51 1.44 1.88 2.13 1.71 1.73 1.76 1.70 2.27 1.77 2.04 3.43 4.04 3. 4.56 4.35 4.64 4.56 1.97 2.01 2.47 2.96 2.94 3.15 3.40 3.30 3.47 3.74 4.22 4.66 4.64 4.76 3.57 3.66 4.51 4.44 1.58 1.50 2.23 1.45 2.43 2.93 2.41 2.62 3.24 3.20 3.09 4.14 4.52 4.60 4.26 4.57 4.44 4.30 1.75 1.44 2.24 2.83 3.21 3.08 3.16 3.24 2.99 3.57 3.97 4.38 4.37 4. 4.57 4.93 4.14 4.76 2.38 3.55 2.85 3.76 4.19 3.84 4.63 4.49 4.21 4.81 4.51 4.81 4.84 4.85 3.68 3.96 4.03 3.33 1.62 1.20 1.56 1.91 2.24 2.04 2.64 2.38 2.96 2.52 3.35 3.82 3.39 4.01 4.63 4.89 4.65 4.36 1.22 1.46 2.65 2.98 3.38 3.68 2.52 4.17 2.74 4.68 4.25 4.69 4.71 4. 4.00 4.20 4.35 4.18 1.90 1.88 2.45 2.70 2.96 3.05 3.06 3.20 3.26 3.44 3.71 4.27 4.35 4.50 Figure 22: Comparison of human evaluation win rates between LongCat-Image-Edit and competing methods. Result Analysis. We calculate the win rate using the formula: (#Win + 0.5 #Tie)/#Total. As illustrated in Fig. 22, LongCat-Image-Edit outperforms Qwen-Image-Edit [2509] and FLUX.1 Kontext [Pro] in both comprehensive quality and consistency. However, performance gap remains when compared to commercial systems such as Nano Banana and Seedream 4.0. 6.5.3 Qualitative Results To comprehensively evaluate our models versatility, we conduct qualitative comparisons against leading instructionbased image editing baselines. We begin by highlighting the models performance in two distinct, high-demand real-world scenarios: Multi-turn Editing and Portrait and Human-Centric Editing. These tasks are selected for their prevalence in practical applications and the rigorous requirements they impose on editing precision. Multi-turn Editing. Sequential editing imposes stringent demands on models ability to preserve visual consistency across iterative steps. We evaluate our model on representative editing chains and further challenge it with compound instructionswhere multiple edits are requested in single prompt. As illustrated in Fig. 23, our model maintains exceptional semantic and structural consistency throughout the entire editing sequence. Remarkably, even when processing complex prompt containing six distinct operations, the model executes all directives accurately. The result aligns closely with the sequential output, underscoring its robust capability in handling both fine-grained iterative updates and complex composite tasks. 34 LongCat-Image Technical Report Figure 23: Visual comparison of multi-turn editing versus one-shot composite editing. The numbered sequence ( 1 6) illustrates the progressive results of multi-turn editing. In contrast, the All in One image (bottom-left) demonstrates the outcome of single complex instruction containing all six operations: Remove the laptop, add mug, change the wall to yellow, add girl sitting on the bed, turn on the TV, and change the style to Ghibli. Portrait and Human-Centric Editing. Fig. 24 validates the models precision in fine-grained portrait editing, confirming its ability to accurately execute diverse facial attribute modifications while preserving identity. Expanding beyond facial details, Fig. 25 demonstrates robust performance in structural body editing. The model successfully handles complex challenges ranging from viewpoint transformation to multi-person interaction synthesis. Collectively, these results highlight the models significant practical utility, indicating its potential for integration into mobile photography pipelines for high-quality post-processing. General Image Editing Capabilities. We conduct comprehensive qualitative evaluation against SeedDream 4.0, Nano Banana, Qwen-Image-Edit [2509], and FLUX.1 Kontext [Pro]. The assessment covers broad spectrum of editing dimensions: object manipulation (addition, removal, extraction), attribute modification, viewpoint transformation, scene text editing, and reference-guided generation. As illustrated in Fig. 26, 27, 28, 29, our model consistently outperforms Qwen-Image-Edit [2509] and FLUX.1 Kontext [Pro] across all dimensions. Notably, in certain challenging scenarios, it also achieves superior results compared to the commercial counterparts. 35 LongCat-Image Technical Report Figure 24: Demonstration of fine-grained portrait editing. From left to right: The original input image, followed by results for blemish removal, hairstyle modification, lighting adjustment, eyelash addition, face slimming, and ID photo generation. The results highlight the models precision in manipulating specific facial attributes while preserving the subjects identity. Figure 25: Qualitative results on Human-centric Editing. The figure displays pairs of input (left) and edited (right) images across three dimensions: Pose & Interaction (top row), involving complex interaction synthesis (e.g., hugging) and large-scale body pose alteration; Viewpoint (bottom-left), transforming subject from side view to front view; and Lighting (bottom-right), simulating directional illumination. Note the preservation of background details and subject identity despite significant structural changes. 36 LongCat-Image Technical Report Figure 26: Qualitative comparison on Style Transfer and Attribute Editing. The upper panel demonstrates the transformation of photorealistic scene into retro-colored illustration style. The lower panel illustrates complex instruction involving both accessory addition (sunglasses) and hand pose modification (heart gesture), highlighting our models ability to preserve facial identity while executing significant structural changes. 37 LongCat-Image Technical Report Figure 27: Qualitative comparison on Object-centric Editing. We evaluate the performance across three distinct scenarios: Object Insertion (top), where an additional creature is added while maintaining scene consistency; Subject Extraction (middle), isolating the foreground subject (the cat) from complex background; Object-Preserved Generation (bottom), where the reference object (the device) is seamlessly integrated into new context (held by robot). 38 LongCat-Image Technical Report Figure 28: Qualitative comparison on Scene Text Editing and Region-Controlled Editing. 39 LongCat-Image Technical Report Figure 29: Qualitative comparison on Camera Control and Viewpoint Transformation. The upper panel shows camera distance adjustment (zooming out), where the view is expanded to place the flowers into pot on table. The lower panel displays camera angle modification, transitioning the view to low-angle perspective looking up at the dog. 40 LongCat-Image Technical Report"
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we present LongCat-Image, 6B-parameter diffusion framework that challenges the prevailing reliance on brute-force scaling by demonstrating that exceptional performance can be achieved through efficient architectural design and refined training methodologies. By integrating hybrid MM-DiT architecture with unified multimodal context encoder, our model establishes an optimal equilibrium between high-fidelity generation and inference efficiency, effectively surpassing the generation quality of numerous open-source models with significantly larger parameters. Across specific domains, LongCat-Image delivers exceptional results. In text-to-image generation, our strategic data curation enables photorealism and Chinese text rendering capabilities that compete with top-tier proprietary systems. In the realm of image editing, our model sets new benchmark for the open-source community. Supported by rigorous data filtering and robust training paradigm, it achieves state-of-the-art performance, exhibiting both precise instruction following and superior visual consistency that significantly outperform existing alternatives. Finally, we distinguish our contribution by democratizing the entire research lifecycle. By open-sourcing not only the final model but also intermediate checkpoints and the complete training codebase, we aim to lower the barriers to entry and foster more transparent, accessible, and collaborative ecosystem for future research. 41 LongCat-Image Technical Report"
        },
        {
            "title": "8 Contributions and Acknowledgments",
            "content": "Contributors are defined as individuals who undertook primary responsibilities in data curation, model design, model training, and relative infrastructures throughout the LongCat-Image complete development cycle. Acknowledgment include those who are working part-time on tasks such as data collection, annotation, model evaluation, and technical discussions. All people are cataloged alphabetically by first name. Names with dagger () are the project leader and sponsors, and names with an asterisk () are former team members. Contributors: Hanghang Ma Haoxian Tan Jiale Huang Acknowledgments: Bingcan Wang Cong Wei Dengsheng Chen Fei Peng Fengjiao Chen Hao Lu"
        },
        {
            "title": "References",
            "content": "Jie Hu Junqiang Wu Jun-Yan He Lishuai Gao Songlin Xiao Xiaoming Wei Xiaoqi Ma Xunliang Cai Yayong Guan Jia Wang Jiajun Liu Kaiwen Wang Lingfeng Tan Liya Ma Man Gao Shengxi Li Tianye Dai Tiezhu Yue Wei Wang Xiaopeng Sun Xiaoyu Li Yanbing Zeng Yingsen Zeng Yuchen Tang Zizhe Zhao Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024a. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Midjourney. Midjourney, 2025. URL https://www.midjourney.com. [Text-to-image model]. Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. Jia Wang, Jie Hu, Xiaoqi Ma, Hanghang Ma, Xiaoming Wei, and Enhua Wu. Image editing with diffusion models: survey. arXiv preprint arXiv:2504.13226, 2025a. Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083, 2025b. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 42 LongCat-Image Technical Report Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV), 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 35:2527825294, 2022. Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024a. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PmLR, 2021. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. JMLR, 25(70):153, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Zeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, and Yuhui Yuan. Glyph-byt5: customized text encoder for accurate visual text rendering. In ECCV, pages 361377. Springer, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 36:5372853741, 2023. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, pages 82288238, 2024. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 43 LongCat-Image Technical Report Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In CVPR, pages 1576215772, 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. NeurIPS, 36:5213252152, 2023. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. Jian Ma, Yonglin Deng, Chen Chen, Nanyang Du, Haonan Lu, and Zhenyu Yang. Glyphdraw2: Automatic generation of complex glyph posters with diffusion models and large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 59555963, 2025a. Nikai Du, Zhennan Chen, Zhizhou Chen, Shan Gao, Xi Chen, Zhengkai Jiang, Jian Yang, and Ying Tai. Textcrafter: Accurately rendering multiple texts in complex visual scenes. arXiv preprint arXiv:2503.23461, 2025. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. Stabilityai. stable-diffusion-3.5-large. https://huggingface.co/stabilityai/stable-diffusion-3. 5-large, 2024. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 77397751, June 2025b. Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. OpenAI. Gpt-image-1, 2025. URL https://openai.com/index/introducing-4o-image-generation/. Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, FuYun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. NeurIPS, 37: 131278131315, 2024. Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024b. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In CVPR, pages 1296612977, 2025b. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, pages 7491. Springer, 2024c. OpenAI. DALLE 3. https://openai.com/research/dall-e-3, September 2023. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable and unified multi-modal generators. IJCV, 2025c. LongCat-Image Technical Report Siqi Kou, Jiachun Jin, Chang Liu, Ye Ma, Jian Jia, Quan Chen, Peng Jiang, and Zhijie Deng. Orthus: Autoregressive interleaved image-text generation with modality-specific heads. arXiv preprint arXiv:2412.00127, 2024. Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, et al. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025. Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2024. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025d. Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, and Aleksandr Gordeev. Nohumansrequired: Autonomous high-quality image editing triplet mining. arXiv preprint arXiv:2507.14119, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83628371, 2024. Google. Gemini 2.5 flash & 2.5 flash image model card. https://storage.googleapis.com/deepmind-media/ Model-Cards/Gemini-2-5-Flash-Model-Card.pdf, 2025. Google DeepMind. Gemini 2.0, 2025. URL https://gemini.google.com/. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1329413304, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025."
        }
    ],
    "affiliations": [
        "Meituan"
    ]
}