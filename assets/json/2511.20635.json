{
    "paper_title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation",
    "authors": [
        "Zhoujie Fu",
        "Xianfang Zeng",
        "Jinghong Lan",
        "Xinyao Liao",
        "Cheng Chen",
        "Junyi Chen",
        "Jiacheng Wei",
        "Wei Cheng",
        "Shiyu Liu",
        "Yunuo Chen",
        "Gang Yu",
        "Guosheng Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 2 5 3 6 0 2 . 1 1 5 2 : r iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation Zhoujie Fu1,2, Xianfang Zeng2,, Jinghong Lan2, Xinyao Liao1,2, Cheng Chen1, Junyi Chen3, Jiacheng Wei1, Wei Cheng2, Shiyu Liu2, Yunuo Chen2,3, Gang Yu,2 Guosheng Lin,1 3Shanghai Jiao Tong University 1Nanyang Technological University 2StepFun https://kr1sjfu.github.io/iMontage-web/ Figure 1. iMontage can flexibly deal with many input images, and can generate many output images with highly consistency. We use three different colors to represent three settings. The dotted-line box images are the input."
        },
        {
            "title": "Abstract",
            "content": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their Project Leader Corresponding Author training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and far more expansive dynamic range. To this end, we introduce iMontage, unified framework designed to repurpose powerful video model into an all-in-one image generator. The framework 1 consumes and produces variable-length image sets, unifying wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-manyout tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Our code and model weights will be made publicly available. 1. Introduction Large-scale diffusion-based generative models[16, 26, 38, 39, 44, 46] have sparked revolution in creative and highquality image generation, accelerating progress in downstream tasks such as image editing. recent trend in the field is to unify diverse image tasks within single framework [33, 35, 40, 59], inspired by the success of Large Language Models (LLMs) and large vision language models (VLMs) [12, 14, 36, 60]. While most unified image models remain specialized for single-in-single-out image tasks, certain commercial model has taken an early lead in extending unified image generation to multi-input, multi-output settings[3] very recently. Accordingly, the many-to-many (multi-input, multi-output) setting warrants systematic exploration by the academic and open-source communities. The many-to-many paradigm splits into two approaches: (i) token-centric models that represent text and images as unified multimodal token stream and autoregressively generate target tokens conditioned on the inputs[59], thereby achieving many-to-many mappings; and (ii) video-centric pipelines that repurpose video diffusion generation as backbones, casting the task as discontinuous video generation and naturally accommodating variable numbers of input and output frames[10, 30]. While the first approach provides an appealing and promising modal-unified solution, its generation quality and instruction following capability is challenged by common sense compared to diffusion paradigm. In contrast, the second approach elegantly leverages pre-trained motion priors to markedly enhance temporal coherence and handle variable-length inputs and outputs. Specifically, [10] trained model of video generation from scratch and constructed large-scale dataset of captioned frame pairs for instruction-tuned editing, demonstrating strong consistency and faithful detail preservation with respect to the input images. Despite these advances for the diffusion-based paradigm, critical question persists: How can model generate highly dynamic multi-image outputs while maintaining temporal and semantic consistency? To our empirical knowledge, image-only models can produce highly diverse images based on the same inputs, yet they struggle with temporal consistency due to limited implicit understanding of world dynamics. Meanwhile, video-based models bring strong motion priors that improve temporal consistency; however, most foundation video models are trained predominantly on contiguous clips, which rarely contains hard cuts, abrupt transitions, or large camera/subject motions, and thereby transferring poorly to highly dynamic content and limiting task versatility. In response, we present iMontage, unified generative model that produces multiple, highly dynamic images conditioned on instructions and arbitrary reference images. Following the video-based paradigm, iMontage builds on large pretrained video model and treats both inputs and outputs as pseudo-frames. We introduce novel rotary positional embedding (RoPE) strategy to prevent conceptual ambiguity between multiple image frames and video frames. Our strategy explicitly maintains the models pretrained capability in modeling temporal coherence, while clearly differentiating the discrete nature of image sets from the continuous flow of video sequences. We further provide data-curation pipeline, which is carefully categorized and filtered for motion diversity and instruction quality, supporting broad, highly dynamic scenario. Finally, we detail training regimen that offers practical insights into multi-task unification. Together, these components marry video generation with many-to-many image generation, achieving both temporal and content consistency. We evaluate iMontage across three settings: one-to-one image editing, many-to-one image generation, and manyto-many image generation. For each setting, we present strong qualitative performances across all sub-tasks, showcasing robust instruction following, high-dynamic outputs, and consistent content generation, as presented in Fig. 1. Furthermore, we provide state-of-the art quantitative metrics on image editing benchmark (one-to-one), in-context learning benchmark (many-to-one) and storyboard generation evaluation (many-to-many). In summary, our contributions are as follows: We introduce iMontage, unified model that handles variable numbers of input and output frames, bridging video generation and highly dynamic image generation. We develop task-agnostic, temporally diverse data curation pipeline paired with multi-task training paradigm, ensuring learnability across heterogeneous tasks and temporal structures and enabling robust many-to-many generalization. Our model showcases convincing results over huge number of variable experiments, including most mainstreaming image generation and editing tasks. Massive visualization results and comprehensive evaluation metrics provide SOTA results in open-source community and even comparable results with commercial models. 2 Figure 2. Overview of iMontage. The model accepts flexible set of reference images and produces outputs conditioned on text prompt. Images are encoded by 3D VAE separately, text by language model, and both token streams are processed by an MMDiT. We concatenate clean reference-image tokens with noisy target tokens before denoising. Right: training uses fixed-length text tokens and variable-length image/noise tokens, transitions from dual stream to single stream blocks. For image branch, we apply Marginal RoPE, headtail temporal indexing that separates input and output pseudo-frames, preserves spatial RoPE, and supports many-to-many generation. In figure, notation and with subscription denote the height/width indices of the 2D RoPE computed at the images native resolution, while notation represents assigned time index for temporal dimension. 2. Related work 2.1. Unified Generation and Editing Models Recent research has increasingly focused on consolidating diverse visual synthesis tasks into single, unified frameworks. Early efforts[17, 28, 58] like OmniGen [59] and ACE++ [35] pioneered monolithic architectures capable of handling generation, editing, and other vision tasks without requiring task-specific modules. This trend evolved with the integration of powerful multimodal large language models (MLLMs) as reasoning engines. Models such as Step1XEdit [33] and Qwen-Image [14] leverage an MLLM to interpret complex user instructions, which then guide diffusion decoder to produce high-fidelity edits. This approach significantly improves instruction-following capabilities. Notably, unified systems in other AIGC area are also emerging, such as [2, 34, 53] in video generation, [32, 50, 63] in audio generation, and even more powerful combining different modalities together[8, 13, 37, 47, 48]. While these unified image models demonstrate impressive versatility, they are predominantly architected for single-input, singleoutput tasks. They lack the inherent capability to manage multiple image inputs and generate set of dynamically varied yet coherent outputs from single prompt, key limitation our work addresses. ios that require handling multiple inputs to produce multiple outputs. significant paradigm shift was introduced by UniReal[10] , which re-frames multi-image generation as discontinuous video generation. By leveraging the powerful temporal priors of video models, this approach naturally accommodates variable number of input and output frames and uses large-scale video data as source of universal supervision for learning real-world dynamics. Following this direction, models like RealGeneral[30] and Frame2Frame[45] also explore video backbones for unified image generation. More recent any-to-any models, such as BAGEL[60] and OmniGen[59], are trained on vast, interleaved multimodal datasets, enabling them to handle arbitrary combinations of inputs and outputs and exhibit emergent world-modeling capabilities. However, critical challenge persists. Foundation video models are typically trained on contiguous video clips, which limits their ability to generate highly dynamic or temporally discontinuous content. This reliance on smooth motion priors hinders their versatility for tasks requiring abrupt scene changes or significant variations between outputs, gap that iMontage is designed to fill. 3. Method 3.1. Model Design 2.2. From One-to-one to Many-to-many Generation The frontier of generative modeling is advancing from single-image tasks to more complex many-to-many scenarNetwork Archtecture. As illustrated in Fig. 2, we adopt hybrid-to-single-stream MMDiT paired with 3D VAE for images and language model for text instruction. All 3 Figure 3. Overview of our dataset: Our dataset is constructed from four sources and is organized into two stages, comprising high-quality foundational data and multiple task-oriented subsets. the components are initialized from HunyuanVideo [24]: MMDiT and 3D VAE are taken from the I2V checkpoint, while the text encoder is taken from the T2V checkpoint. Reference images are encoded by the 3D VAE seperately and then patchified into tokens; textual instructions are encoded by the language model into text tokens. Following the I2V formulation, we concatenate clean reference-image tokens with noisy target tokens and feed the sequence to the image branch block. We train our model to accommodate variable numbers of input and output frames by constructing variable-length attention maps over their image tokens, guided by prompt-engineering cues. During training, we frozen VAE and text encoder, only full-finetune the MMDiT. Position Embedding key objective is to endow the transformer with sensitivity to multiple images without perturbing its original positional geometry. We adopt simple yet effective strategy: cast all input/output images as pseudo-frames along the temporal axis, assign each unique time index, and keep their native spatial resolution and 2D positional encoding intact. Concretely, we preserve the pretrained spatial RoPE and introduce separable temporal RoPE with per-image index offsets, supplying cross-image ordering cues while leaving the spatial distribution unchanged. Inspired by L-RoPE [25], we assign input images to early temporal positions and output images to late positions. In practice, we allocate 3D RoPE with 32 temporal indices, reserving {0, . . . , 7} for inputs and {24, . . . , 31} for outputs, leaving wide temporal margin between them. This headtail layout reduces positional interference between inputs and targets and empirically promotes more diverse output content while preserving temporal coherence. Prompt Engineering We adopt purely text-instruction interface powered by strong LLM encoder, without masks or auxiliary visual embeddings. To unify heterogeneous tasks, we pair set of common prompts with task-specific templates. For the common prompts, we (i) prepend system-style preamble: Please output images according to the instruction: and (ii) use an interleaved multimodal format that explicitly marks image positions via textual placeholders <image n> within the prompt. 3.2. Dataset Creation We divide our data construction into two phases: pretraining dataset and supervised fine-tuning (SFT) dataset. The overview of our dataset construction is refered in Fig. 3. 3.2.1. Pretraining Dataset We partition the pretraining data into two pools: an imageedit pool and video frame-pair pool, sourced from internal corpora. The image-edit pool spans most single-image editing tasks, providing paired (input, edited) images with concise, fine-grained instructions specifying the operation. The video frame-pair pool consists of high-quality frame pairs extracted from videos (with associated captions), curated under stringent quality criteria. We further refine the video Table 1. Comparison metrics of Motion Change and Edit overall on GEdit-GPT4o-EN; Action and Average on ImgEdit. For GEditGPT4o-EN, Semantic Consistency (G SC), Perceptual Quality (G Q), and Overall Score (G O) are reported. Bold means the best performance and underline means the second best performance. Category Models Motion Change - GEdit Edit overall - GEdit ImgEdit SC O SC O Action Average Closed-source Open-source Gemini 2.5[12] GPT-4o[36] Seedream 4.0[3] ICEdit[70] Omnigen[59] Omnigen2[54] Bagel[14] UniWorld-V1[29] HiDream-I1 (E1)[4] HiDream-E1.1[20] Flux-Kontext-dev[26] Step1X-Edit v1.1[33] Open-source iMontage (Ours) 6.87 7.81 5.58 0.93 3.35 4.75 5.25 1.58 1.58 5.55 5.23 4.65 5.25 7.79 8.53 8. 7.98 6.68 8.08 8.03 7.55 7.23 7.80 7.53 8.15 8.43 6.72 7.81 5.53 1.13 3.12 5.13 5.09 1.76 1.66 5.64 4.95 4.73 5.53 8.25 8.74 8. 4.94 5.88 7.16 7.48 4.93 5.66 7.15 7.16 7.66 7.21 8.29 7.67 8.04 7.39 5.87 6.77 6.80 7.43 6.06 6.65 7.37 7.35 7.80 7.89 8.01 7. 4.87 5.01 6.41 6.60 4.85 5.01 6.42 6.51 6.97 6.94 4.61 4.83 4.66 3.68 3.38 4.68 4.17 2.74 3.33 4.18 4.35 3.73 4.48 4.30 4.30 4. 3.05 2.96 3.44 3.20 3.26 3.17 3.97 3.97 3.90 4.11 frame pairs by selecting samples that satisfy the following filtering criteria: For frame pairs drawn from single clip, we apply motion filtering with an optical-flow estimator [49]: for each sample, we compute the average motion magnitude and preferentially retain or upweight high-motion instances to increase their prevalence. To further diversify dynamics, we concatenate segments from the same source video and re-clip them without motionor camera-change heuristics (i.e., not cutting at large motions or pans), thereby producing cross transition frame pairs and mitigating the bias toward quasi-static content. Post-filtering, the dataset comprises 5M image-edit pairs and 15M video frame pairs, providing supervision for highly dynamic content generating and robust instruction following. 3.2.2. Multi Task Dataset Our Multi Task dataset is constructed based on tasks, varying from one-to-one task to many-to-many task. Our data curation pipeline for each task is described as follows: Multi CRef. We crawl web posts to assemble reference images for human, object, and scenario. Human images are filtered to single-person shots via detector [15]; object/scenario images need no extra filtering. VLM [1] composes CRef prompts by randomly combining sources, GPT-4o [36] generates the corresponding images, and the VLM then scores and filters candidates. This pipeline yields around 90k high-quality samples. Conditioned CRef. Different from the CRef dataset, we collect the data from an open-source dataset Echo-4o[67]. We apply some classic ControlNet[69] generation control maps to the target image. We use OpenPose[5] to generate the character poses of the composite image, use DepthAnything-V2[64] to generate the depth map of the target image, and also use the Lineart model[22] as an edge detector. We add these condition pairs to Echo-4o and create new Conditioned CRef dataset about 50k samples. SRef. We curate style-reference data analogously to CRef. We scrape character posts and select human images via VLM aesthetic score [1] as content references, and collect hand-drawn illustrations from open sources as style references. Using subjectstyle models[56, 61], we generate images by randomly pairing content and style. VLM then scores outputs and checks ID consistency with the content image to prevent style leakage. This yields 35k samples. Multi Turn Editing. In this task, we generate multiple responses at the same time according to instruction, where sub-steps instruction cover all editing tasks in pretraining image-edit dataset. Our data is extracted from an internal dataset and we collect around 100k samples. Multi View Generation. We curate our multi-view dataset from the open-source 3D corpus MVImageNet V2 [19]. For each base sample, we randomly select 14 additional viewpoints and, in successive order, use GPT-4o [36] to caption the relative camera motion between adjacent images, yielding concise supervision for multi-view generation. We collect around 90k samples. Storyboard Generation. Storyboard generation is closely related to the storytelling setting [42, 52], but targets high inter-panel diversity, for example, drastic scene changes and distinct character actions across images. Leveraging recent commercial foundation model Seedream4.0[3], we distill high-quality supervision from their outputs to construct instructionimage sequences for training. We begin with an internal character image dataset and apply face-detection filter [15] and an NSFW filter [27] to obtain whole-face character reference images. We then design instruction templates that prompt Seedream4.0 to produce semantically rich, dynamic scenes and multi-panel stories. The generated images are captioned with GPT-4o [36], yielding con5 Table 2. Quantitative comparison on OmniContext grouped by model availability. Char. + Obj. indicates Character + Object. Category Model SINGLE MULTIPLE SCENE Average Closed-source Open-source Gemini 2.5[12] GPT-4o[36] InfiniteYou[23] OmniGen[59] UNO[57] BAGEL[14] OmniGen2[54] Open-source iMontage (Ours) Char. Obj. Char. Obj. 8.62 8.90 6.05 7.21 6.60 5.48 8.05 7.94 9.11 9. 5.71 6.83 7.03 7.58 7.77 8.77 9.07 5.65 2.54 5.17 7.11 6.75 8.88 8. 5.44 5.61 6.64 7.13 7.57 Char. + Obj. 7.39 8.54 4.68 4.39 6.24 7.45 8. Char. Obj. 7.29 8.90 3.59 2.06 4.07 6.38 6.90 7.05 8. 4.32 3.33 5.71 6.71 6.81 Char. + Obj. 6.68 8.60 5.12 4.37 5.47 7.04 7. 7.84 8.80 4.34 4.71 5.73 7.18 7.41 cise storyboard (instruction, images) pairs for supervision. We collect around 29k samples. 3.3. Training Scheme We adopt three-stage training strategy using dynamic mixture of the curated data described above-specifically, Pre-training stage for large-scale pre-training, Supervised Fine-tuning stage, and High-Quality Annealing stage: Pre-training Stage. In this stage, we train on the Pretraining Dataset to instill instruction following and acclimate the model to highly dynamic content. Since we initialize from pretrained backbone, we eschew progressive resolution schedules [7, 16, 18]; instead, we adopt aspect-ratioaware resolution bucketing: for each sample, we select the bestfitting size from set of 37 canonical resolutions and resize accordingly. Batch size in this stage is dynamically adjusted by sequence length, equalizing the token budget across resolutions and yielding smoother, more stable optimization. SFT Stage. We investigate the best solution of unifying multitasks with huge variance in this stage. Our strategy can be concluded as follows: FlatMix: All-in-One Joint Training. Train all tasks together in single mixed pool. StageMix: Curriculum Training. Two-phase schedule: first train on the three many-to-one tasks, then add the three many-out tasks and continue mixed training. CocktailMix: Difficulty-Ordered Fine-Tuning. We witness notable training difficulty variance for each single task, motivating us of mixture of training by difficulty. In practice, we begin with the simplest task, then introduce the second-easiest while reducing the sampling weight of the first. We continue this process by adding one harder task at time and gradually shifting mixture weights until the hardest task is included and receives the largest training share. For the final decision, we choose the CocktailMix training strategy, and discussion about the training is detailed in the ablation study (Sec. 4.4). During all mixture training, we apply weights based on the data amount of each task, ensuring all tasks are treated equally. In this stage, we allow different resolution for input images while fix output resolution for convenience. Since input images can be different resolution, we set batch size per GPU to 1 during all SFT training stage. HQ Stage. In image and video generation, it is widely observed that concluding training with small tranche of highquality data improves final fidelity [39, 65, 72]. We adopt this strategy: using combination of manual review and VLM assistance, we curate high-quality subsets for each task, then perform brief, unified finetuning pass across all tasks after SFT. During this stage, we anneal the learning rate to zero. All our experiments all conducted on 64 NVIDIA H800 GPUs. We apply constant learning rate of 1e-5 for all training stages and the training target follows flow matching[31]. More detailed implementation can be found in Sec. 6. 4. Experiment As unified model, iMontage shows strong performance on various tasks even compared to fixed input/output models. Note that our model only need one inference, with default of 50 diffusion steps. For clarity, we organize results by inputoutput cardinality, spliting into one-to-one editing (Sec. 4.1), many-to-one generation (Sec. 4.2) and many-tomany generation (Sec. 4.3). 4.1. One-to-one Editing We report competitive quantitative metrics and compelling qualitative results on instruction-based image editing. We compare our model against twelve strong baselines, including native image editing models, unified MLLM models and powerful closed-source product. Average metrics on GEdit benchmark[33] and ImgEdit benchmark[68] can be found in Tab. 1. Despite closed-source models and commercial models, iMontage shows strong performance on both benchmark over other models. We also report metric about motion-related sub-task in Tab. 1. Our method demonstrates superior motion-aware editing, exhibiting strong temporal consistency and motion 6 Figure 4. Comparison with three baselines on storyboard generation setting. Single character and many characters samples are presented. priors. These gains are expected: we inherit strong world dynamic knowledge from large pretrained video backbone, then reinforce it with pretrained on high-dynamics videoframe corpus. Please find our one-to-one image editing visualization results in Fig. 6 and Fig. 7. 4.2. Many-to-one Generation The core challenge for multiple inputs is how to preserve all their content and harmony them together. We report our results on the OmniContext benchmark[54], which aims to provide comprehensive evaluation of the models incontext generation abilities. We report our metrics against seven baselines. Detailed metrics can be found in Tab. 2. We also visualize representative results in supplementary materials, showing that iMontage handles diverse tasks while maintaining the source images context. We select In Multichallenging cases to stress control and fidelity. CRef, the model fuses cues from multiple references without altering core content, while being faithful to complex instruction by generating highly detailed background. In Conditioned CRef, it respects the conditioning signal yet preserves the humans details, which is considered to For SRef, we include be hard for generation models. scene-centric and human/object-centric inputs to demonstrate strong style transfer that retains style and identity. 4.3. Many-to-many Generation Generating multiple outputs while preserving consistency is highly challenging. We raise the bar by requiring both cross-output content consistency and temporal consistency. To evaluate capability, we consider three disparate tasks. Multi-view generation. We simulate camera rotations, following [14], and use natural-language descriptions of camera motion to render novel views from single reference image. This temporally continuous setting probes whether Table 3. Storyboard generation metrics over iMontage (ours) and three baselines. Dino feature similarity, Clip feature similarity and VLM rating scores are reported. (a) Identity Preservation."
        },
        {
            "title": "Method",
            "content": "StoryDiffusion UNO (w/ UMO) OmniGen2 (w/ UMO) iMontage (Ours)"
        },
        {
            "title": "Method",
            "content": "StoryDiffusion UNO (w/ UMO) OmniGen2 (w/ UMO) iMontage (Ours) DINO CLIP VLMpref 0.367 0.519 0. 0.585 0.570 0.674 0.619 0.690 3.962 6.625 6.857 7.909 (b) Temporal Consistency. DINO CLIP VLMpref 0.440 0.479 0.460 0.615 0.649 0.676 0. 0.745 7.111 6.556 7.889 9.556 the model preserves identity, geometry, materials, and background context as the viewpoint changes. We report identity/structure consistency across views and visualize long arcs of rotation to stress continuity. All our visualization results can be found in Fig. 10. Multi-turn editing. Most image editors support multi-turn pipelines by running inference sequentially, yet they often drift, overwriting non-target content. We cast multi-turn editing as content-preservation task: given an initial image and sequence of edit instructions, the model should localize changes while maintaining other parts. All our visualization results can be found in Fig. 7. Storyboard generation. This is our most comprehensive setting: temporally, the model must produce smooth, continuous trajectories while also handling highly dynamic transitions such as hard cuts, large camera or subject motions, and scene changes; spatially, it must preserve con7 Table 4. User study metrics on storyboard generation of twenty samples. Rating scores are between 1 and 5, while higher score means better performance. (a) Instruction following (IF) and identity preservation (IP). Method StoryDiffusion UNO (w/ UMO) OmniGen2 (w/ UMO) iMontage (Ours) IF 2.81 3.68 3. 4.46 IP 1.86 2.90 3.07 3.91 (b) Temporal consistency (TC) and overall quality (OQ). Method StoryDiffusion UNO (w/ UMO) OmniGen2 (w/ UMO) iMontage (Ours) TC 2.28 3.05 3.04 4.31 OQ 2.12 3.03 3.23 4.16 tent consistency by maintaining identity, layout, and finegrained appearance across all outputs. As illustrated in visualization results in supplementary material, iMontage delivers coherent yet highly diverse results across all three settings in single forward pass. To the best of our knowledge, this is the first model to unify these tasks within one model and one-shot inference. To better quantify many-out capability, we conduct quantitative study in the storyboard setting, comparing our method against two unified systems (OmniGen2 and UNO) and storytelling-focused baseline, StoryDiffusion [73]. We focus on two axes: ID preservation and temporal consistency. The former measures how closely each generated character matches the reference identity (especially the characters whole body details, such as clothes, skin color, hair), while the latter captures cross-panel coIn our evaluation, herence among the generated images. the evaluated OmniGen2 and UNO models are optimized by UMO[11], which improves identity preservation and other quality measures. For metrics, we use DINO[6] and CLIP[41] feature similarity following [21, 71], along with VLM rating system. We report the comparison score in Tab. 3. We also present visualization comparison in Fig. 4. Detailed conduction of our storyboard evaluation can be found in Sec. 8.1. Furthermore, for more comprehensive evaluation, we conduct user study with 50 professional participants. We show the comparison metrics in Tab. 4. Our method achieves the best performance both at instruction following and identity preservation, outperforms baselines with big margin. Detailed experiments of user study can be found in Sec. 8.2. 4.4. Ablation Study RoPE Strategy. We first ablate our RoPE strategy design. Our default Marginal RoPE assigns inputs to the head of Figure 5. Ablation on different RoPE strategy. We evaluate on subset of the editing data with low resolution, training each strategy for the same number of steps. In the figure, corner numbers indicate provenance: 1 original input, 2 edited ground truth, 3 output from Marginal RoPE, and 4 output from Even RoPE. the temporal index range and outputs to the tail, leaving gap between them; the control, Even RoPE, distributes all images uniformly along the temporal axis. We conduct our ablation study using same setting from pretraining dataset, of which is only small amount of data. We observe late convergence for Even RoPE, with the same training steps. Fig. 5 indicates the visualization of the RoPE ablation study. Training Scheme. As discussed in Sec. 3.3, we ablate three SFT strategies. With FlatMix, the training loss oscillates strongly and does not stabilize. After some updates, the model drifts toward the easier tasks even with inversesize reweighting. We conduct StageMix and CocktailMix experiments at the same time, the former groups training by task type, while the latter organizes the schedule by task difficulty. CocktailMix delivers strong results across all tasks and shows clear advantage on the harder settings, outperforming StageMix by significant margin. We also conduct comparison experiment on Multi CRef, with same training steps for both strategy. The result reveals 12.6% gain on OmniContext for CocktailMix. We show more details in Sec. 8.3. 5. Conclusion and Limitations In conclusion, we introduce iMontage, unified many-tomany image generation model that can create highly dynamic contents while preserving both temporal and content consistency. Adequate experiments demonstrate iMontages superior capabilities in image generation. However, iMontage still face some limitations. First, due to data and compute constraints, we have not explored longcontext many-to-many settings, and the model currently delivers its best quality with up to four inputs and four outputs. Second, several capabilities remain limited. We provide detailed breakdown and failure cases in Sec. 9.2. We also include more discussion about concurrent work in Sec. 9.1. For next step, we view scaling long-context supervision, enhancing data quality and broadening task coverage as primary directions for future work."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 5 [2] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 3 [3] Bytedance. Seedream4.0, 2025. https : / / seed . bytedance.com/en/seedream4_0. 2, [4] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. 5 [5] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose: Realtime multi-person 2d pose IEEE transactions on estimation using part affinity fields. pattern analysis and machine intelligence, 43(1):172186, 2019. 5 [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 8, 1 [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 6 [8] Junyi Chen, Haoyi Zhu, Xianglong He, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Zhoujie Fu, Jiangmiao Pang, et al. Deepverse: 4d autoregresarXiv preprint sive video generation as world model. arXiv:2506.01103, 2025. 3 [9] Lan Chen, Yuchao Gu, and Qi Mao. Univid: Unifying vision tasks with pre-trained video generation models. arXiv preprint arXiv:2509.21760, 2025. [10] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1250112511, 2025. 2, 3 [11] Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, and Qian He. Umo: Scaling multi-identity consistency for image customization via matching reward. arXiv preprint arXiv:2509.06818, 2025. 8 [12] Google Deepmind. Gemini2.5, 2025. https : / / deepmind.google/models/gemini/pro/. 2, 5, [13] Google Deepmind. Veo3, 2025. https://deepmind. google/models/veo/. 3 pretraining. arXiv preprint arXiv:2505.14683, 2025. 2, 3, 5, 6, 7 [15] Arnab Dhar. Yolov8-face-detection, 2024. https : //huggingface.co/arnabdhar/YOLOv8FaceDetection. [16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2, 6 [17] Tsu-Jui Fu, Yusu Qian, Chen Chen, Wenze Hu, Zhe Gan, and Yinfei Yang. Univg: generalist diffusion model for unified image generation and editing. arXiv preprint arXiv:2503.12652, 2025. 3 [18] Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. 6 [19] Xiaoguang Han, Yushuang Wu, Luyue Shi, Haolin Liu, Hongjie Liao, Lingteng Qiu, Weihao Yuan, Xiaodong Gu, Zilong Dong, and Shuguang Cui. Mvimgnet2. 0: larger-scale dataset of multi-view images. arXiv preprint arXiv:2412.01430, 2024. 5 [20] HiDream-ai. https : / / Hidream-e1-1, 2025. huggingface.co/HiDream-ai/HiDream-E1-1. 5 [21] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 8, [22] Huggingface. Controlnet auxiliary models. https : //github.com/huggingface/controlnet_aux? tab=readme-ov-file, 2023. 5 [23] Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, and Xin Lu. Infiniteyou: Flexible photo recrafting while preserving your identity. arXiv preprint arXiv:2503.16418, 2025. 6 [24] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 4, 3 [25] Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, and Wenhan Luo. Let them talk: Audio-driven multi-person conversational video generation. arXiv preprint arXiv:2505.22647, 2025. [26] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX . 1-dev. 2, 5 Flux.1 [dev], 2024. [14] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal [27] LAION. https : Clip-based nsfw detector, 2021. //github.com/LAIONAI/CLIPbasedNSFWDetector. 9 [28] Wei Li, Xue Xu, Jiachen Liu, and Xinyan Xiao. Unimog: Unified image generation through multimodal conditional diffusion. arXiv preprint arXiv:2401.13388, 2024. 3 [29] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 5 [30] Yijing Lin, Mengqi Huang, Shuhan Zhuang, and Zhendong Mao. Realgeneral: Unifying visual generation via temporal in-context learning with video models. arXiv preprint arXiv:2503.10406, 2025. 2, 3 [31] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 6 [32] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32: 28712883, 2024. 3 [33] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 2, 3, 5, 6, [34] Jiabin Luo, Junhui Lin, Zeyu Zhang, Biao Wu, Meng Fang, Ling Chen, and Hao Tang. Univid: The open-source unified video model. arXiv preprint arXiv:2509.24200, 2025. 3 [35] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instructionbased image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 2, 3 [36] OpenAI. Gpt4o, 2024. https://www.openai.com/. 2, 5, 6, 1 [37] OpenAI. Sora2, 2025. https://openai.com/index/ sora-2/. 3 [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, [40] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Luminaimage 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. 2 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 8, 1 [42] Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, and Leonid Sigal. Make-a-story: Visual In Promemory conditioned consistent story generation. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24932502, 2023. 5 [43] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 1 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [45] Noam Rotstein, Gal Yona, Daniel Silver, Roy Velich, David Bensaid, and Ron Kimmel. Pathways on the image manifold: In Proceedings of the Image editing via video generation. Computer Vision and Pattern Recognition Conference, pages 78577866, 2025. 3 [46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [47] Sizhe Shan, Qiulin Li, Yutao Cui, Miles Yang, Yuehai Wang, Qun Yang, Jin Zhou, and Zhao Zhong. Hunyuanvideofoley: Multimodal diffusion with representation alignment arXiv preprint for high-fidelity foley audio generation. arXiv:2508.16930, 2025. 3 [48] Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, et al. Aether: Geometric-aware unified world modeling. arXiv preprint arXiv:2503.18945, 2025. 3 [49] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. 5 [50] Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang, Robert Adkins, William Ngan, et al. Audiobox: Unified audio generation with natural language prompts. arXiv preprint arXiv:2312.15821, 2023. [51] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 3 [52] Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, and Chunhua Shen. Autostory: Generating diverse Internastorytelling images with minimal human efforts. tional Journal of Computer Vision, pages 122, 2024. 5 [53] Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, and Wenhu Chen. Univideo: Unified understanding, generation, and editing for videos. arXiv preprint arXiv:2510.08377, 2025. 3 [54] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal 10 generation. arXiv preprint arXiv:2506.18871, 2025. 5, 6, 7, 1, 3 image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [67] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, et al. Echo-4o: Harnessing the power of gpt4o synthetic images for improved image generation. arXiv preprint arXiv:2508.09987, 2025. 5 [68] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. 6 [69] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 5 [70] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 5 [71] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, WeiShi Zheng, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. 8, 1 [72] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [73] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent selfattention for long-range image and video generation. Advances in Neural Information Processing Systems, 37: 110315110340, 2024. 8 [55] Jay Zhangjie Wu, Xuanchi Ren, Tianchang Shen, Tianshi Cao, Kai He, Yifan Lu, Ruiyuan Gao, Enze Xie, Shiyi Lan, Jose Alvarez, et al. Chronoedit: Towards temporal reasoning for image editing and world simulation. arXiv preprint arXiv:2510.04290, 2025. 3 [56] Shaojin Wu, Mengqi Huang, Yufeng Cheng, Wenxu Wu, Jiahe Tian, Yiming Luo, Fei Ding, and Qian He. Uso: Unified style and subject-driven generation via disentangled and reward learning. arXiv preprint arXiv:2508.18966, 2025. 5, 3 [57] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. 6 [58] Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamomni: Unified image generation and editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2853328543, 2025. 3 [59] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 2, 3, 5, [60] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 2, 3 [61] Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, and Zechao Li. Csgo: Content-style composition in text-to-image generation. arXiv preprint arXiv:2408.16766, 2024. 5, 3 [62] Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, et al. Withanyone: Towards controllable and id consistent image generation. arXiv preprint arXiv:2510.14975, 2025. 3 [63] Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, et al. Uniaudio: An audio foundation model toward universal audio generation. arXiv preprint arXiv:2310.00704, 2023. 3 [64] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. 5 [65] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [66] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-to11 iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Implementation Details For training, we treat all DiT blocks as trainable components in all stages, frozen VAE and text encoders. In pretraining stage, we start with more video clip data and less image editing data, then gradually counting more image editing data for better instruction following capability. The ratio is linear increase of 25% to 75% for image editing data. In this stage, we adopt dynamic resolution grouping. We choose 5122, 7682 and 10242 as base buckets and derive 32-pixel variants on both height and width, yielding candidate resolution of 37 categories. Each training image is assigned to the candidate that best matches its native size (preserving aspect ratio via short-side resize and optional padding), and then resized accordingly. Batch size for 512resolution bucket per gpu is 8, while 4 for 768 bucket and 2 for 1024 bucket. In the SFT stage, due to data constraints, each task is trained at fixed resolution. At inference, however, the model generalizes well to arbitrary resolutions across tasks, exhibiting stable behavior and consistent quality without task-specific resizing rules. In the HQ stage, we collect set of high-aesthetic, higher-resolution multitask samples and mix them with curated subsets from earlier datasets, then perform an annealed finetuning pass. All our training are conducted on NVIDIA H800 gpus, takes about 7 days to cover all training stage on 64 H800 gpus. More detailed hyperparameters used in training stage can be found in Tab. 5. For inference, we conduct classifier-free guidance (CFG) for text embeddings. The default cfg is 6.0 for all inference tasks, and inference steps is set to 50. To align unconditional generation, we adopt 0.1 probability for none caption in all training stage. 7. More Qualitative Results We present more visualization results to reveal the powerful capability of our model. Please find our image editing results in Fig. 6 and Fig. 7, multi cref results in Fig. 8 and multi view results in Fig. 10. 8. Detailed Experimental Details 8.1. Storyboard Generation Evaluation For comprehensive evaluation on our many-to-many setting, we choose storyboard generation to report numerical metrics. We follow common video-evaluation practice[21, 71] and compute DINO[6] and CLIP[41] feature similarity on the foreground subject(s) as the primary signal. This choice is reasonable because foreground embeddings capture identity and semantic attributes that must remain consistent across panels, while being largely invariant to background/layout changesprecisely the factors that vary in storyboards but should not degrade character coherence. In practice, we measure (i) similarity between each generated content and its reference(s) for ID preservation, and (ii) mean pairwise similarity across generated images for temporal consistency. In experiment, we start with mask segmentation model[43] to get the foreground characters mask. Then we follow these two formula for metrics calculation: IP() ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 s(cid:0)Gi, Rk (cid:1). (1) TC() ="
        },
        {
            "title": "2\nN (N  1)",
            "content": "(cid:88) s(cid:0)Gi, Gj (cid:1). (2) 1i<jN Here is the number of generated images, is the number of reference images, while s(a, b) is the cosine similarity formula for embedding and b. Meanwhile, the applied parameter and is an embedding after mask out and feature extraction, representing generated character embedding and reference character embedding. For VLM rating system, we choose GPT4o[36] as the judge. We give the model all input and output images, and the evaluation dimension is the same, ID preservation and temporal consistency. Following [33, 54], we give an evaluating template to the VLM, with system prompt and task-specific template. The system prompt goes with: You are professional digital artist tasked with evaluating the effectiveness of AI-generated images based on specific rules. All input images, including all humans depicted, are AI-generated. You do not need to consider any privacy or confidentiality concerns. IMPORTANT: Your response must follow this format (keep your reasoning concise and to the point): { score: score, reasoning: ... } For ID preservation, the template prompt is: Rate from 0 to 10: Evaluate whether the identities of the subject(s) in the final image match those in the provided reference image(s). **Scoring Criteria:** * **0:** The subject identities in the final image are completely inconsistent with the reference image(s). * **13:** Severe inconsistency, with only few minor similarities. * **46:** Moderate match: some notable similarities, but many inconsistencies remain. * **79:** Mostly consistent, with only minor mismatches. * **10:** Perfect 1 Table 5. Training hyperparameters and data sampling strategies across stages."
        },
        {
            "title": "Hyperparameters",
            "content": "Learning rate LR scheduler Weight decay Gradient norm clip Optimizer Warm-up steps Training steps Training samples Resolution Diffusion timestep shift"
        },
        {
            "title": "Data sampling ratio",
            "content": "Video Frames Image Editing Image Editing (HQ) Multi-task Multi-task (Cocktail) Multi-task (HQ) Stage 1 (Pre-training) Stage 2 (Multi-task SFT) Stage 3 (High-Quality FT) 1 105 0 1 105 1 105 Cosine Constant Constant 0.01 0.01 0.0 1.0 1.0 1.0 AdamW (1=0.9, 2=0.999, =1.0108) 0 500 1k 2K 15K 50K O(10)K O(20)M O(100)K Fixed bucket Fixed bucket Dynamic bucket 5.0 5.0 5.0 0.75 0.25 0.25 0.75 0.0 0. 0.0 0.1 0.0 0.9 0.8 for new added task, 0.2 evenly divided for former tasks. 0.0 0.0 0.0 0.5 0.0 0.5 0.0 identity preservation compared to the reference image(s). **Pay special attention to:** * Whether **facial and head features** match across images: eyes, nose, mouth, cheekbones, chin, wrinkles/lines, makeup, hairstyle, hair color, overall facial structure and head shape. * **Body shape/proportions** and **skin tone** consistency; watch for abnormal anatomical changes. * **Clothing and accessories** if the instruction does not request changes; otherwise do not penalize expected edits. * Distinctive attributes (moles, scars, freckles, tattoos, piercings) that should persist. * If multiple references are given, ensure the correct individual(s) from each reference are present and not confused. **Do not** assess composition, pose, background, or aesthetics unrelated to identity preservation. **Scoring should be strict** avoid giving high scores unless the identity match is clearly strong. Editing instruction: instruction. And for temporal consistency, the template prompt is: Rate from 0 to 10: Evaluate whether the identities of all subject(s) remain consistent across the provided generated images (sequence or set). **Scoring Criteria:** * **0:** Subjects are completely inconsistent across images (identity changes or swaps occur). * **13:** Severe inconsistency; frequent identity drift, swaps, or major attribute changes. * **46:** Moderate consistency; some notable similarities but multiple mismatches across images. * **79:** Mostly consistent identities with only minor mismatches. * **10:** Perfect temporal identity consistency across all images. **Pay special attention to:** * Stable **facial/head features** for the same subject across images (eyes, nose, mouth, facial structure, hairstyle/color). * Consistent **body shape** and **skin tone** for each individual across images. * **Clothing/accessories** stability unless the instruction implies changes; otherwise do not penalize expected edits. * For **multi-person scenes**, ensure each person maintains consistent identity mapping across images (no A/B swapping). **Ignore** differences in pose, composition, viewpoint, background, or lighting that do not affect identity. **Scoring should be strict** do not award high scores unless identity consistency is clear across all images. Editing instruction: instruction 8.2. User Study We invite 50 participants, who are familiar with image and video generative models, to engage in our evaluation on storyboard generation. We curate twenty evaluation samples by first searching some high quality human photos from website, then manually craft some storyboard caption based on them. For fairness, we include reference subjects spanning three racial groups (black, white and yellow) and two genders (female and male), and we vary prompts from simple to complex. Each testing sample provides one or two reference characters and requests generation of two to four storyboard images. We request participants to rate for all results in the same sample. The rating system follows four criteria scored on 5-point Likert scale (1=Poor, 5=Excellent): (i) Instruction Followingwhether the images follow the prompt; (ii) ID Preservationconsistency with the reference character(s), emphasizing facial and fine attributes; (iii) Temporal Con2 pretrained video generator to improve physical plausibility and temporal coherence in edits. UniVid[9] explores complementary route: it adapts pretrained video DiT with lightweight SFT to broad suite of vision tasksboth understanding and generationby casting tasks as visual sentences, thereby avoiding task-specific architectural changes and generalizing across modalities and data sources. Our model focuses on another area, narrowing the gap between image and video generation by casting image synthesis as unified many-to-many problem. We view this as practical technical pathway and plan to extend it into more capable, fully unified system. 9.2. Observed Failure Case Our model still exhibits failure cases on certain tasks, as illustrated in Fig. 11. For image editing, the most salient issue is near-zero ability to render Chinese characters (Fig. 11a), largely inherited from the base backbone HunyuanVideo [24], which lacks robust text-rendering supervision. For SRef, our training data are distilled from other models [56, 61], which is suboptimal; we observe occasional background leakage, which is known challenge in style-reference transfer. Finally, we note head-detail mismatch in some generations. This limitation stems from data constraintsnamely, insufficient training coverage of diverse, high-detail head/face depictions. Two complementary remedies are promising: (i) adopt human-centric identity modules by injecting face embeddings [51, 62, 66]; and (ii) expand coverage of high-quality, head-focused data to strengthen fine-grained facial detail preservation. sistencywhether the same character remains consistent across the generated panels; and (iv) Overall Qualitya holistic judgment beyond adherence and consistency. For each sample, all competing models are rated by the same participant to reduce between-rater variance, and model identities are anonymized and presentation order is randomized. We then report scores for each metric based on the mean rating. We provide showcase of our rating system in Fig. 12. For fair comparison, we use each models recommended inference settings. Specifically: StoryDiffusion at 768768, classifier-free guidance (CFG)=5.0, 50 inference steps; UNO at 768768, CFG=4.0, 25 steps; and OmniGen2 at 10241024, CFG=5.0, 50 steps. Our model follows setting as 1024x640 resolution, CFG=5.0, 50 steps. All experiments are conducted based on random seed. Note that for single sample, other models should be inferred several times with the same seed; iMontage uses one seed, outputting many results for one inference."
        },
        {
            "title": "We present the visualization results of evaluation from",
            "content": "Fig. 13 to Fig. 19. 8.3. Training Scheme Ablation We ablate three scheduling strategies for SFT: FlatMix (all tasks jointly), StageMix (grouped by task type), and CocktailMix (difficulty-ordered curriculum). We begin with FlatMix and then transition to difficulty-aware scheduling. Task difficulty gap. Under shared setup (data, optimizer, steps), we observe clear difficulty spread across tasks: the easiest task, multi-editing, and the hardest task, storyboard generation, differ by roughly 0.2 in training loss. This gap motivates difficulty-aware mixing. StageMix vs. CocktailMix. We train StageMix with the same protocol used for our Stage 2 and Stage 3 runs and compare it head-to-head with CocktailMix. On OmniContext[54], StageMix underperforms by 12.6% relative to CocktailMix. Other tasks all have worser visualization results for StageMix. These observations indicate that difficulty-ordered mixing yields better optimization stability and stronger generalization, especially on the harder tasks. 9. More Discussion 9.1. Concurrent Works Though we are not the first unified image generation model developed upon video models[10, 30], we consider iMontage as the first practical many-to-many system for open-source community. Likewise, two very recent efforts build image capabilities on top of video backbones. ChronoEdit[55] treats the input and edited outputs as the first and last frames of short video and jointly denoises them with temporal-reasoning tokens, leveraging 3 Figure 6. Visualization results for image editing. Zoom in to see more details. 4 Figure 7. Visualization results for image editing. Zoom in to see more details. 5 Figure 8. Visualization results for multi CRef. Zoom in to see more details. 6 Figure 9. Visualization results for conditioned CRef and SRef. Zoom in to see more details. 7 Figure 10. Visualization results for multi view generation, which can be divided to object-centric and scene-centric. Zoom in to see more details. 8 Figure 11. Representative failure case for certain task. Zoom in to see more details. Figure 12. User study template. 9 Figure 13. User study comparison visualization results. Zoom in to see more details. 10 Figure 14. User study comparison visualization results. Zoom in to see more details. Figure 15. User study comparison visualization results. Zoom in to see more details. 12 Figure 16. User study comparison visualization results. Zoom in to see more details. 13 Figure 17. User study comparison visualization results. Zoom in to see more details. Figure 18. User study comparison visualization results. Zoom in to see more details. 15 Figure 19. User study comparison visualization results. Zoom in to see more details."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Shanghai Jiao Tong University",
        "StepFun"
    ]
}