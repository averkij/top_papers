{
    "paper_title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
    "authors": [
        "Jialong Zuo",
        "Haoyou Deng",
        "Hanyu Zhou",
        "Jiaxin Zhu",
        "Yicheng Zhang",
        "Yiwei Zhang",
        "Yongxin Yan",
        "Kaixing Huang",
        "Weisen Chen",
        "Yongtai Deng",
        "Rui Jin",
        "Nong Sang",
        "Changxin Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \\textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle."
        },
        {
            "title": "Start",
            "content": "Is Nano Banana Pro Low-Level Vision All-Rounder? Comprehensive Evaluation on 14 Tasks and 40 Datasets Jialong Zuo, Haoyou Deng, Hanyu Zhou, Jiaxin Zhu, Yicheng Zhang, Yiwei Zhang, Yongxin Yan, Kaixing Huang, Weisen Chen, Yongtai Deng, Rui Jin, Nong Sang, Changxin Gao National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro Low-Level Vision All-Rounder? We conducted comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals distinct performance dichotomy: while Nano Banana Pro demonstrates superior subjective visual quality, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional referencebased quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains significant hurdle. Project Page: https://lowlevelbanana.github.io GitHub: https://github.com/zplusdragon/LowLevelBanana HuggingFace Dataset: https://huggingface.co/datasets/jlongzuo/LowLevelEval 5 2 0 2 7 1 ] . [ 1 0 1 1 5 1 . 2 1 5 2 : r a"
        },
        {
            "title": "1 Introduction",
            "content": "The recent proliferation of Generative AI has fundamentally transformed the landscape of computer vision, with Text-to-Image (T2I) models demonstrating unprecedented capabilities in high-fidelity content creation. Among these, commercial products like Nano Banana Pro [136] have emerged as standouts, garnering significant attention for their versatility. While its prowess in creative synthesis is well-documented, the extent to which such large-scale foundation model can generalize to traditional low-level vision problems remains largely unexplored. This gap presents not only challenge of capability but also one of evaluation, raising the pivotal research question: Is Nano Banana Pro Low-Level Vision All-Rounder? The motivation for this study is rooted in fundamental tension between human perception and traditional metrics. On one hand, the rich visual priors encapsulated within robust generative models should theoretically 1 Figure 1 Exemplary zero-shot results of Nano Banana Pro across 14 low-level vision tasks. For each task, the top row shows the degraded images, and the bottom row presents the corresponding restored outputs generated by Nano Banana Pro using simple text prompts. The visual results demonstrate the models emerging capability for diverse range of low-level vision tasks without task-specific training. The blue box represents image restoration tasks, the green box represents image enhancement tasks, and the yellow box represents image fusion tasks. enable them to hallucinate plausible solutions for restoration, enhancement, and fusion tasks without taskspecific training. On the other hand, this generative nature may conflict with the goal of achieving the strict, pixel-perfect fidelity that is valued by conventional evaluation metrics. To investigate this, we systematically evaluate the zero-shot capabilities of Nano Banana Pro using simple textual prompts, stark contrast to the complex pipeline fine-tuning typically required for specialist models. Our comprehensive empirical study spans 14 distinct low-level vision tasks across 40 datasets, covering image restoration, enhancement, and fusion. As visually exemplified in Fig. 1, Nano Banana Pro frequently produces outputs with remarkable perceptual quality. For instance, in tasks like dehazing or deraining, it can generate sharp edges and realistic textures that are often more aesthetically pleasing to human observer than the results from specialist models. This initial observation immediately highlights critical challenge: model can be subjectively superior yet quantitatively inferior. Our work, therefore, aims not only to benchmark performance but also to delineate the boundaries of its current capabilities through rigorous quantitative and qualitative analysis. It is important to note that the present evaluation reflects conservative estimate of the models capability, as we did not engage in meticulous prompt tuning or employ multi-round inference to cherry-pick optimal outputs. Our fixed, simple prompts represent pragmatic but unoptimized use case. Our findings uncover this anticipated dichotomy in stark detail: Nano Banana Pro excels in perceptual quality but lags in metric-driven fidelity. While it demonstrates remarkable zero-shot potential across wide array of degradations, its outputs consistently score lower on reference-based metrics (e.g., PSNR, SSIM) when compared to domain-specific experts. We attribute this performance gap to the inherent stochasticity of generative models, which prioritize semantic plausibility over the strict pixel-wise alignment demanded by these metrics. In essence, while Nano Banana Pro has not yet achieved the status of perfect all-rounder, it forces us to reconsider the traditional definition of success in low-level vision. It establishes strong baseline for zero-shot restoration, highlighting both its emerging strengths and the critical need for new evaluation paradigms that can reconcile perceptual quality with pixel-level accuracy. The remainder of this report is organized to systematically present these findings. We will detail our experimental results across Image Restoration, Image Enhancement, and Image Fusion, providing in-depth comparative analysis for each task. Finally, we conclude by summarizing the models limitations and discussing potential future directions, including the development of more perception-aligned evaluation methods for generative low-level vision solvers."
        },
        {
            "title": "2 Dehazing",
            "content": "Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 2.2 Quantitative and Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7 Defocus Deblurring",
            "content": "Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1 7.2 Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "11 Low Light Image Enhancement",
            "content": "3 1 5 5 5 7 8 8 8 10 11 12 12 12 13 15 15 15 16 18 18 18 18 19 19 20 21 21 21 22 23 24 24 24 25 25 26 26 27 29 31 31"
        },
        {
            "title": "17.1 Prompts for Each Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17.2 Contributors",
            "content": "34 34 34 35 36 36 36 37 38 39 39 40 40 42 43 43 43 45 45 47 47 47 48 49 50 50 50 50 51 52 52"
        },
        {
            "title": "2.1 Introduction",
            "content": "Real-world Image Dehazing (RID) aims to recover clear and high-fidelity images from hazy observations captured in real-world environments. Unlike synthetic dehazing, where haze is generated under simplified physical assumptions, real-world haze is highly complex and exhibits strong spatial non-uniformity, large depth variations, severe color shifts, sensor noise, and compression artifacts. These factors make RID long-standing and extremely challenging low-level vision problem. The goal of RID is not only to generate visually appealing results, but also to preserve accurate photometric and structural information to support reliable downstream vision tasks such as detection, tracking, and segmentation. Early dehazing methods [118, 132] relied on handcrafted statistical priors, such as the Dark Channel Prior (DCP)[50] and Non-Local Prior (NLP)[8], to constrain the solution space. While these physics-inspired approaches achieved initial success, they often fail to generalize across diverse real-world scenes and frequently introduce visible artifacts. With the rapid development of deep learning, numerous CNN-based and Transformer-based methods[19, 129, 161], have significantly improved dehazing performance on synthetic benchmarks. However, collecting large-scale, perfectly aligned real-world hazy and clean image pairs remains nearly impossible. Although several real-world datasets have been constructed, their scale and diversity are still far from sufficient for training robust deep models. Consequently, most existing methods heavily rely on synthetic data and suffer from severe performance degradation when deployed in real-world scenarios. To bridge this gap, recent studies have increasingly shifted their focus toward real-world dehazing. Some methods reintroduce physical priors to adapt pre-trained networks, while others modify inference strategies to improve generalization. Nevertheless, these approaches remain highly dependent on the quality of pre-training data. Moreover, heavily hazed images often contain severe information loss, fundamentally limiting the capability of traditional enhancement-based methods that lack generative flexibility to recover missing content. Nano Banana is an image generation model developed by Google DeepMind. Its professional version, Nano Banana Pro, further enhances precision and world knowledge understanding. We applied it to real-world image dehazing tasks, with focus on its effectiveness in removing haze, restoring blurred textures, and maintaining scene semantic integrity and tested it on mainstream dehazing benchmarks."
        },
        {
            "title": "2.2 Quantitative and Qualitative Results",
            "content": "Table 1 Quantitative comparison of dehazing methods on multiple datasets. NB Pro achieves excellent NIMA scores on both datasets. It also demonstrates favorable FADE and BRISQUE scores on the RTTS dataset. However, its FADE and BRISQUE metrics on the Fattals dataset are unsatisfactory. The best results are in black bold. Method MBSDN [29] Dehamer [43] DAD [129] PSD [19] D4[178] RIDCP[161] CORUN [32] NB Pro RTTS Fattals FADE BRISQUE NIMA 27.67 1.363 33.24 1.895 32.24 1.130 27.71 0.920 33.21 1.358 17.29 0.944 4.53 4.52 4.31 4.60 4.48 4.97 0.824 0.986 11.96 27.21 5.34 4.95 0.338 0. FADE BRISQUE NIMA 0.579 0.698 0.484 0.416 0.411 0.408 5.43 5.16 14.15 15.53 29.64 23.61 20.33 20.05 14.82 5.46 4.99 5.44 5.43 5.39 22.16 5. To intuitively present the dehazing outcomes of the Nano Banana (NB) Pro model, we provide quantitative and qualitative evaluations of its processing results across the RTTS [73] and Fattals [33] datasets, with comparisons to state-of-the-art baseline methods in real-world dehazing tasks. Tab. 1 show the performance 5 comparison between NB Pro and current mainstream dehazing networks on the RTTS and Fattals datasets, where we evaluated three real-world-oriented dehazing metrics: FADE , BRISQUE and NIMA. Integrating the evaluation results on both the RTTS and Fattals datasets, NB Pro demonstrates outstanding performance in terms of subjective visual quality, achieving top-tier NIMA scores on both benchmarks, which indicates that the generated images possess strong aesthetic appeal and favorable human perceptual quality. However, it performs poorly in terms of image naturalness. Specifically, NB Pro exhibits significantly higher BRISQUE score on the Fattals dataset, suggesting that the outputs may suffer from over-enhancement artifacts. Figure 2 NB Pro dehazing visual results on the RTTS dataset. Especially under heavy hazy conditions, NB Pro can effectively recover and enhance blurred details. Qualitative experimental results demonstrate that for extreme degradation scenarios such as dense fog with severe visibility loss, heavy atmospheric scattering, and complex urban or natural environments, NB Pro can generate perceptually enhanced results. Fig. 2 displays the visualization of exemplary dehazing cases for NB Pro on the RTTS dataset, highlighting both successful and challenging scenarios. For certain severely hazy images, benefiting from its powerful generative capabilities, NB Pro effectively restores intricate detailssuch as fine textures in buildings, vehicles, or vegetationthat are heavily obscured in the input, producing clear, high contrast outputs with impressive visual recovery of distant structures and overall scene coherence. Similarly, for some lightly foggy images, NB Pro performs well in optimization, selectively removing haze from distant backgrounds while preserving foreground sharpness and natural tones, resulting in balanced enhancements that improve visibility without introducing artifacts. Fig. 3 shows the visual comparison of NB and other methods on the RTTS dataset. However, NB Pro also exhibits numerous failure cases, as illustrated in Fig. 4. In these examples, spanning both moderate and heavy haze conditions, NB Pro often restores the images into distorted outputs characterized by unnatural color shifts, such as over-saturated or overly vibrant hues and hallucinatory weather elements, most notably forcing intensely blue skies into scenes that were originally overcast or neutral. These distortions undermine the authenticity of the atmospheric conditions, leading to results that deviate significantly from realistic dehazing expectations. Overall, while NB Pro offers inspirational generative capabilities for ill-posed real-world dehazing, demonstrating the potential of semantic-driven priors in tackling highly ambiguous degradationsits limitations in color fidelity, physical realism, and consistent handling across varying haze densities suggest it is better suited for creative enhancements rather than precise low-level restoration. This prompts future research into hybrid approaches, such as combining NB Pros zero-shot generative strengths with task-specific physical constraints or refined prompt engineering, to better constrain its tendencies toward perceptual appeal over faithful recovery. 6 Figure 3 comparison of visual results between NB pro and other methods. It can be observed that the results produced by NB Pro are noticeably clearer and exhibit superior visual quality; however, they also suffer from obvious over-enhancement artifacts. Figure 4 Anamorphic example images of Nano Pro in dehazing on the RTTS dataset. The original hazy images is on top, and below is the results after hazy removal. The dehazed results exhibit poor color fidelity."
        },
        {
            "title": "2.3 Analyses",
            "content": "Stemming from misalignment between training distributions and restoration objectives, NB Pro struggles to maintain spectral fidelity and atmospheric consistency. The model frequently over-corrects naturally muted, hazy tones into saturated, vibrant colors, introducing artificial chromatic biases that alter the scenes intrinsic mood. In severely hazy scenarios where high-frequency details are obliterated, NB Pros generative priors dominate the restoration process. Rather than solving the inverse physical scattering model, it hallucinates details based on learned statistical patterns. defining characteristic of this behavior is the systematic rendering of vivid blue skies in originally overcast scenes. While visually striking and aligned with subjective preferences for clear weather, this deviation undermines the scenes authenticity and temporal consistency. Consequently, NB Pro is currently best positioned for creative content generationprioritizing perceptual appeal and \"wow-factor\"rather than forensic restoration tasks demanding strict adherence to physical constraints. However, its value remains significant. In the ill-posed domain of real-world dehazing, where traditional physics-based methods often falter due to unknown degradation parameters, NB Pro demonstrates the power of semantic priors to reconstruct plausible details in information-deficient scenarios. This suggests pivotal direction for future research: developing hybrid frameworks. By integrating NB Pro-like generative backbones with task-specific physical constraints (e.g., atmospheric scattering laws) and fidelity anchors, we can bridge the gap between creative hallucination and realistic restoration, aiming for paradigm that balances visual delight with physical truth."
        },
        {
            "title": "3.1 Introduction",
            "content": "Real-World Image Super-Resolution (Real-ISR) aims to restore high-fidelity, high-resolution content from low-resolution inputs degraded by complex, unknown physical processes. Unlike synthetic super-resolution, where degradations are modeled by simple bicubic downsampling, real-world scenarios involve intricate combinations of blur, sensor noise, compression artifacts, and varying camera response functions [12, 160]. This complexity renders traditional regression-based methodswhich rely on pixel-wise optimization (e.g., MSE loss)ineffective, often resulting in over-smoothed textures and lack of high-frequency details [28, 199]. The landscape of generative Real-ISR has evolved rapidly. Generative Adversarial Networks (GANs) [42, 71], represented by methods such as BSRGAN [191] and Real-ESRGAN [150], introduced high-order degradation modeling to synthesize realistic training pairs, significantly improving visual perceptual quality over PSNRoriented baselines. More recently, Denoising Diffusion Probabilistic Models (DDPMs) [51] have emerged as the new state-of-the-art. Methods like StableSR [144] and DiffBIR [92] leverage strong priors from large-scale pre-trained text-to-image models (e.g., Stable Diffusion [? ]) to generate intricate textures. However, these multi-step diffusion models often suffer from high computational costs and slow inference speeds. To mitigate this, acceleration techniques have been proposed, exemplified by SinSR [153], which distills complex diffusion priors into single-step inference models. Despite these advancements, critical challenge remains: the inherent trade-off between perceptual quality and signal fidelity [9], often leading to artifacts or structural hallucinations in the pursuit of sharpness. In this work, we conduct comprehensive quantitative and qualitative evaluation of Nano Banana Pro, novel generative ISR framework, benchmarking it against spectrum of industry-standard algorithms, including GAN-based approaches (BSRGAN [191], Real-ESRGAN [150]), multi-step diffusion models (StableSR [144], DiffBIR [92]), and accelerated diffusion methods (SinSR [153]). Our goal is to rigorously assess where Nano Banana Pro stands within the current Perception-Distortion landscape. To ensure robust evaluation across varying degradation complexities, our experiments are conducted on the large-scale DIV2K-Val dataset (2,994 images) [4] as well as the authentic RealSR [12] and DRealSR [160] benchmarks. Recognizing that pixel fidelity alone is insufficient for characterizing generative performance, we employ comprehensive set of evaluation metrics, incorporating not only standard Full-Reference indicators (PSNR, SSIM [157], LPIPS [194]) but also widely-adopted No-Reference perceptual metrics (NIQE [111], MUSIQ [? ], CLIPIQA [143]). Under this rigorous testing framework, we systematically assess the reconstruction capabilities of Nano Banana Pro in comparison to established GAN-based and diffusion-based baselines. The resulting analysis provides detailed characterization of the models behavior regarding the perception-distortion trade-off, offering valuable insights into its suitability for real-world applications."
        },
        {
            "title": "3.2 Quantitative Results",
            "content": "To comprehensively evaluate Nano Banana Pros performance in Real-ISR tasks, we quantitatively compared it against range of advanced GAN-based and diffusion-based image super-resolution methods. We employed standard full-reference metrics: PSNR and SSIM to evaluate signal fidelity, and LPIPS to assess perceptual similarity. Additionally, No-Reference (NR) metrics NIQE, MUSIQ, and CLIPIQA were utilized to quantify the statistical naturalness and aesthetic quality of the generated images. Results are shown in Tab. 2. Nano Banana Pro significantly underperformed against the comparison methods in terms of traditional fidelity metrics. On the DIV2K-Val dataset, Nano Banana Pro achieved significantly lower PSNR and SSIM than the optimal method, lagging by over 4 dB. similar trend, though less severe, was observed on the RealSR and DRealSR datasets, where its fidelity scores remained consistently behind both GAN-based and diffusionbased baselines. This result clearly indicates that under the standard full-reference evaluation framework, which prioritizes pixel-level accurate reconstruction, Nano Banana Pros generated results exhibit systematic deviations from the ground-truth reference images. Nano Banana Pro fundamentally differs from traditional super-resolution models optimized for specific degradation kernels. The latter typically undergo end-to-end training targeting minimization of pixellevel lossthus inherently excelling in metrics like PSNR and SSIM. In contrast, Nano Banana Pros 8 Table 2 Quantitative comparison on synthetic (DIV2K-Val[4]) and real-world (RealSR[12], DRealSR[160]) benchmarks. The best and second best results are highlighted by black bold and underline. Test Dataset Method Full-Reference Metrics No-Reference Metrics PSNR SSIM LPIPS NIQE MUSIQ CLIPIQA DIV2K-Val RealSR DRealSR BSRGAN [191] Real-ESRGAN [150] StableSR [144] DiffBIR [92] SinSR [153] 24.58 24.29 23.26 23.64 24.41 0.6269 0.3351 0.6371 0.5726 0.5647 0. 0.3112 0.3113 0.3524 0.3240 4.75 4.68 4.76 4.70 6.02 61.20 61.06 65.92 65.81 62.82 0.5071 0.5501 0.6192 0.6210 0. Nano Banana Pro 20.29 0.4720 0.3645 3.52 65. 0.5257 BSRGAN Real-ESRGAN StableSR DiffBIR SinSR 26.39 25.69 24.70 24.75 26.28 0.7654 0.7616 0.7085 0.6567 0.7347 0.2670 0.2727 0.3018 0.3636 0.3188 5.66 5.83 5.91 5.53 6. Nano Banana Pro 23.56 0.6649 0.2978 4.39 BSRGAN Real-ESRGAN StableSR DiffBIR SinSR 28.75 28.64 28.03 26.71 28.36 0.8031 0.2883 0.8053 0.7536 0.6571 0.7515 0.2847 0.3284 0.4557 0.3665 6.52 6.69 6.52 6.31 6. 63.21 60.18 65.78 64.98 60.80 60.18 57.14 54.18 58.51 61.07 55.33 0.5001 0.4449 0. 0.6246 0.5385 0.5199 0.4915 0.4422 0.5601 0.5930 0.4884 Nano Banana Pro 23. 0.6323 0.3809 5.03 59.00 0.5145 generation process prioritizes semantic coherence and visual cleanliness. Its outputs can be viewed as plausible reconstructions of the input image rather than strict pixel-to-pixel mappings. Consequently, generated images may exhibit deviations in local texture alignment and structural positioning compared to reference images, leading to comprehensive score reductions across full-reference metrics. However, notably, on the NIQE metric, Nano Banana Pro consistently achieved the best scores across all three datasets (e.g., 3.52 on DIV2K-Val vs. 4.75 for BSRGAN). This suggests its outputs possess superior statistical naturalness, effectively suppressing artifacts, even though the low-level pixel arrangements have been altered from the original reference. Figure 5 Qualitative visualization of structural recovery in Real-ISR tasks using Nano Banana Pro. 9 Figure 6 Visualization of unintended image boundary extension cases in Real-ISR tasks using Nano Banana Pro. Figure 7 Visualization of generative texture deviations in Real-ISR tasks using Nano Banana Pro."
        },
        {
            "title": "3.3 Qualitative Results",
            "content": "In this section, we examine the generative characteristics of Nano Banana Pro across the DIV2K, RealSR, and DRealSR datasets. The qualitative evaluation is organized into four key scenarios to highlight both the strengths and failure modes of the model: geometric clarity, field-of-view artifacts, texture fidelity, and character reconstruction (Figs. 58). Fig. 5 displays the super-resolution results on scenes with distinct geometric structures. In the examples of the architectural facade and hanging lanterns, Nano Banana Pro effectively sharpens the blurred edges and recovers the linear patterns lost in the low-resolution inputs. The resulting images maintain structural coherence and exhibit reduced noise, offering noticeable improvement in clarity compared to the inputs. 10 Figure 8 This visualization illustrates text reconstruction, showing successful character recovery in the left panel and erroneous character hallucination in the right panel. Fig. 6 illustrates distinct structural anomaly observed in Nano Banana Pro: unintended Field-of-View (FOV) expansion. Comparing the low-resolution input, the Ground Truth (GT), and the Nano Banana Pro generated result reveals that the model fails to strictly adhere to the original spatial boundaries of the input image. Instead of merely super-resolving the existing pixels, Nano Banana Pro erroneously hallucinates additional content along the image periphery. Fig. 7 illustrates the tendency of Nano Banana Pro to synthesize plausible but non-existent details in areas with complex stochastic textures. In the foliage and stone carving examples, while the model produces sharp, high-frequency patterns, these generated textures often deviate from the Ground Truth. Specifically, the arrangement of leaf veins and the granular surface of the stone are reconstructed with altered local structures rather than being faithfully restored, leading to pixel-level discrepancies that lower fidelity scores. Fig. 8 highlights the dependency of Nano Banana Pro on semantic recognizability for text reconstruction. In the examples on the left, where the low-resolution input retains discernible structural features (such as the logo and digits), the model accurately reconstructs sharp and legible characters. Conversely, the examples on the right demonstrate failure cases where severe degradation obscures the original glyphs, particularly with complex Chinese characters. In these instances, the model fails to recover the correct semantic content and instead hallucinates incorrect strokes or non-existent characters, resulting in high-contrast but semantically erroneous text."
        },
        {
            "title": "3.4 Analyses",
            "content": "Our comprehensive evaluation elucidates the operational characteristics of Nano Banana Pro within the Real-ISR domain. Quantitatively, the model trails significantly behind mainstream GAN and diffusion-based methods in fidelity metrics (PSNR/SSIM) across the DIV2K-Val and RealSR datasets; however, it achieves remarkable performance in the no-reference NIQE metric. This discrepancy suggests that, as evidenced in Fig. 7, the model prioritizes learned generative priors to synthesize texture details rather than strictly adhering to the low-resolution input. Qualitatively, unlike traditional restoration models that maintain strict spatial consistency, Nano Banana Pro lacks precise pixel-level alignment with the reference image. Consequently, unintended Field-of-View (FOV) expansion is observed, as shown in Fig. 6. Furthermore, the text reconstruction failures in Fig. 8 reveal the models heavy reliance on feature recognizability. When encountering degraded features such as blurred text, the model exhibits tendency to aggressively generate sharp outputs. This behavior causes feature recognition errors to have catastrophic impact on the result, leading to the hallucination of sharp but semantically incorrect text. Synthesizing these findings, Nano Banana Pro is highly suitable for perception-centric applicationssuch as artistic upscaling, old photo restoration, or casual photographywhere visual purity and noise elimination are paramount. However, due to its propensities for texture hallucination, spatial misalignment, and semantic alteration, it is unsuitable for fidelity-critical scenarios."
        },
        {
            "title": "4.1 Background",
            "content": "Rain is common yet challenging weather degradation that severely obscures scene content and alters the structural appearance of images. Such degradations significantly reduce visual quality and adversely affect the reliability of numerous outdoor vision systems, including intelligent transportation, UAV-based monitoring, and autonomous navigation. Single image deraining, which seeks to restore clean background from rain-contaminated observation, has therefore become an essential task in low-level vision. In recent years, variety of deraining algorithms and benchmark datasets have been developed, achieving remarkable progress in modeling rain streaks, raindrops, and atmospheric veils. Recent advances in large-scale multimodal generative models offer promising new direction for image restoration. Among these models, Nano Banana Pro, Googles latest high-fidelity visual generation system built upon the powerful Gemini 3 Pro multimodal reasoning engine, excels in semantic comprehension, fine-grained visual modeling, and precise structural control. Designed for professional image creation and editing, Nano Banana Pro supports high-resolution synthesis, multi-image fusion, accurate text rendering, and semantically coherent scene manipulation. These capabilities suggest that the model is not only adept at generating novel visual content, but also inherently possesses strong prior knowledge for reconstructing clean structures and textures, rendering it promising candidate for image restoration tasks such as deraining. In this study, we investigate the feasibility of adapting Nano Banana Pro to single-image deraining. In contrast to traditional restoration networks that rely on explicit task-specific modeling, Nano Banana Pro utilizes its rich world model and multimodal reasoning ability to interpret degraded regions, infer plausible background structures, and produce natural, artifact-free reconstructions. By framing deraining as guided generative reconstruction problem, we aim to harness the models semantic priors and sophisticated visual synthesis capabilities to achieve rain removal across diverse scenarios."
        },
        {
            "title": "4.2 Experiment Setup",
            "content": "To thoroughly evaluate the performance and generalization ability of Nano Banana Pro on the single image deraining task, we conduct experiments on three widely used benchmark datasets: two synthetic datasets, Rain200L and Rain200H[176], and one real-world dataset, SPA[148]. These datasets cover broad range of rain patterns and scene complexities, enabling comprehensive assessment of the models restoration capability. Rain200L[176]: Contains 1,800 pairs of synthetic training images and 200 test pairs. The rain streaks in this dataset exhibit single predominant direction and relatively low density, making it suitable for assessing the models basic restoration ability under simple rain conditions. Rain200H[176]: Provides 1,800 training pairs and 200 test pairs, but features rain streaks with higher density and more diverse orientations. This dataset is designed to evaluate the robustness of deraining models when faced with heavy and structurally complex rain degradations. SPA[148]: large-scale real-world rainy image dataset comprising 638,492 training pairs and 1,000 test images. The rain patterns in SPA are highly diverse, and the background scenes vary significantly, making it an appropriate benchmark for measuring cross-domain generalization from synthetic to real rainy conditions. All images from all datasets are given the prompt: This is rainy image. Please remove the rain streaks and raindrops while keeping all other elements, the original color tone, lighting, and atmosphere unchanged. It is worth emphasizing that Nano Banana Pro is evaluated in strictly zero-shot manner: no training images are used for optimization, fine-tuning, or adaptation, and the model is directly applied to the test images via fixed textual prompt."
        },
        {
            "title": "4.3 Quantitative and Qualitative Results",
            "content": "For fair comparison, all metrics are computed under exactly the same evaluation protocol as NeRD-Rain, including image resolution, color space, and PSNR/SSIM computation. Based on the quantitative results reported in Tab. 3, we systematically evaluate the image deraining performance of Nano Banana Pro on two synthetic datasets, Rain200L[176] and Rain200H[176], as well as the real-world SPA-Data dataset[148], and compare it with wide range of representative methods. The compared approaches cover prior-based methods (DSC[102], GMM[89]), CNN-based methods (DDN[35], RESCAN[86], PReNet[125], MSPFN[67], RCDNet[142], MPRNet[181], DualGCN[36], SPDNet[179]), and Transformer-based methods (Uformer[155], Restormer[182], IDT[165], DRSformer[16], NeRD-Rain[17]). Table 3 Quantitative comparison results on three representative benchmarks. The best results are in black bold. Method DSC[102] GMM[89] DDN[35] RESCAN[86] PReNet[125] MSPFN[67] RCDNet[142] MPRNet[181] DualGCN[36] SPDNet[179] Uformer[155] Restormer[182] IDT[165] DRSformer[16] NeRD-Rain[17] Nano Banana Pro Rain200L Rain200H SPA-Data PSNR 27.16 28.66 34.68 36.09 37.80 38.58 39.17 39.47 40.73 40.50 40.20 40.99 40.74 41.23 SSIM PSNR 0.8663 0.8652 0.9671 0.9697 0.9814 0.9827 0.9885 0.9825 0.9886 0.9875 0.9860 0.9890 0.9884 0.9894 14.73 14.50 26.05 26.75 29.04 29.36 30.24 30.67 31.15 31.28 30.80 32.00 32.10 32.17 SSIM PSNR 0.3815 0.4164 0.8056 0.8353 0.8991 0.9034 0.9048 0.9110 0.9125 0.9207 0.9105 0.9329 0.9344 0. 34.95 34.30 36.16 38.11 40.16 43.43 43.36 43.64 44.18 43.20 46.13 47.98 47.35 48.54 41.71 26.05 0.9903 0.7954 32.40 21.10 0.9373 0.6659 49.58 32. SSIM 0.9416 0.9428 0.9457 0.9707 0.9816 0.9843 0.9831 0.9844 0.9902 0.9871 0.9913 0.9921 0.9930 0.9924 0.9940 0.9142 The quantitative performance of Nano Banana Pro is significantly inferior to that of state-of-the-art deraining models across all three datasets. On Rain200L[176], Nano Banana Pro achieves 26.05 dB PSNR and 0.7954 SSIM, which are substantially lower than those of supervised learning-based methods. On the more challenging Rain200H dataset with complex rain patterns, Nano Banana Pro obtains 21.10 dB PSNR and 0.6659 SSIM. Although it outperforms traditional prior-based methods, considerable performance gap remains compared to CNN-based and Transformer-based approaches. On the real-world SPA-Data dataset[148], Nano Banana Pro reaches 32.25 dB PSNR and 0.9142 SSIM, still noticeably below the current best-performing methods. Figure 9 Qualitative comparison on the Rain200H[176] dataset. Close-up views better illustrate the deraining capability. This quantitative degradation is consistent with the visual results in Fig. 9, where large and dense synthetic rain streaks severely obscure background content, leading to color deviations and missing or oversmoothed fine details. Since Nano Banana Pro is not trained specifically for image deraining, local structures are often reconstructed via implicit generative hallucination rather than pixel-wise restoration, which negatively affects PSNR and SSIM. Nevertheless, the model demonstrates notable strength in recovering certain global structures; for example, the cable geometry of the suspension bridge is reconstructed with higher structural coherence and semantic plausibility than several supervised baselines. Furthermore, Fig. 10 shows that the deraining performance of Nano Banana Pro is highly sensitive to rainfall 13 Figure 10 Qualitative comparison under different rain intensities. Under lighter rain, Nano Banana Pro better preserves the original tone and fine details, while heavier rain leads to noticeable color shifts and detail loss. intensity: under low-rain conditions, where more reliable visual information is preserved, both color fidelity and fine details are significantly improved, whereas heavy rainfall introduces severe occlusion and ambiguity, resulting in pronounced color shifts and detail degradation. Overall, these results indicate that while generative multimodal models are disadvantaged in pixel-level fidelity under zero-shot deraining, they retain strong semantic and structural priors, suggesting complementary potential in scenarios with limited supervision or severe information loss. Our qualitative analysis reveals fundamental limitation in the instruction-following behavior of promptconditioned multimodal generative models. As shown in Fig. 11, despite utilizing explicit prompts that constrain the model to preserve non-rain regions, we consistently observe unintended alterations in background elements. This phenomenon stems from an inherent bias in fine-grained semantic understanding. As clearly illustrated in Fig. 12, the model conflates rain streaks with atmospheric haze (i.e., the rain-mist ambiguity). Consequently, it aggressively removes the mist alongside the rain, yielding an output image that appears clearer and visually superior in low-level details. However, this visual enhancement paradoxically leads to lower quantitative scores (e.g., PSNR), as the complete removal of haze introduces significant pixel-wise deviation from the ground truth. This observation underscores the prevalent perception-distortion trade-off in image restoration tasks. Figure 11 Failure case of Nano Banana Pro. From left to right: input rainy image, ground truth (GT), and the output of Nano Banana Pro. The model hallucinates plausible content to fill severely corrupted regions, leading to significant pixel-level discrepancy from the GT. 14 Figure 12 Failure case of Nano Banana Pro. The model inadvertently removes the atmospheric haze accompanying rain streaks; while this enhances visual clarity, it results in lower quantitative metrics due to deviation from the GT."
        },
        {
            "title": "4.4 Analyses",
            "content": "In this study, we investigated the feasibility of Nano Banana Pro for single-image deraining under zero-shot setting. Experimental results indicate that the application of generative models to image restoration presents double-edged sword effect. On one hand, compared to specialized deraining models trained on extensive supervised data, such as NeRD-Rain and Restormer, Nano Banana Pro exhibits significant gap in pixel-level objective metrics like PSNR and SSIM (e.g., achieving PSNR of only 21.10 dB on the Rain200H dataset). This quantitative deficiency is primarily attributed to the inherent tendency of generative models to prioritize semantic reconstruction over strict pixel-wise restoration, resulting in deviations in high-frequency detail preservation and color fidelity. On the other hand, leveraging its robust world model and semantic reasoning capabilities, Nano Banana Pro demonstrates superior structural coherence and visual plausibility compared to traditional methods when handling regions with severe rain occlusion (such as bridge cables). This confirms the unique complementary advantages of generative models in addressing extreme degradation and information loss. Future work will focus on employing prompt tuning to further enhance pixel-level restoration accuracy for low-level vision tasks, while preserving the models strong semantic generation capabilities."
        },
        {
            "title": "5.1 Background",
            "content": "Shadow removal aims to eliminate cast shadows from images and restore consistent illumination between shadow and non-shadow regions [47]. Shadows are caused by partial occlusion of light by scene objects and are ubiquitous in natural environments. Although shadows provide useful geometric cues for human perception, they often introduce strong intensity discontinuities, color distortions, and loss of texture details, which severely degrade the performance of downstream vision tasks such as object detection, tracking, and segmentation. Indeed, removing shadows from an image remains fundamental yet highly challenging low-level vision task. Early shadow removal methods [119] primarily relied on handcrafted priors and carefully designed statistical features, such as illumination consistency, gradient constraints, and region smoothness. These optimizationbased approaches were built upon highly idealized physical and photometric assumptions, which rarely hold in real-world scenes with complex lighting, textured backgrounds, and soft shadow boundaries. Consequently, they often suffer from noticeable artifacts, especially around shadow boundaries, and fail to generalize to diverse real-world scenarios. 15 With the rapid development of deep learning, fully supervised CNN-based shadow removal methods have achieved remarkable progress by learning pixel-wise mappings from shadow images to their shadow-free counterparts using large-scale paired datasets. While these methods significantly improve visual quality on benchmark datasets, they heavily rely on expensive pixel-level annotations and often exhibit severe overfitting with limited generalization capability. More importantly, shadow removal is inherently region-wise corrupted problem that involves strong contextual and structural priors. The recent introduction of Nano Banana Pro demonstrates remarkable capabilities of generative models in visual tasks. In this context, we systematically evaluate the performance of Nano Banana Pro on the single-image shadow removal task. Through comprehensive comparisons with existing representative methods, we provide an in-depth analysis of its strengths and limitations in real-world applications."
        },
        {
            "title": "5.2 Qualitative and Quantitative Results",
            "content": "Figure 13 Some well-performing visual examples of NB Pro on the SRD dataset [119] for the shadow removal task. Fig. 13 presents the well-performing shadow removal results of NB Pro on the SRD dataset [119]. It can be observed that NB Pro effectively removes shadows from the image while highly preserving the original elements without alteration. Tab. 4 presents the quantitative comparison on SRD dataset of NB Pro against state-of-the-art shadow removal methods, using PSNR and SSIM as the primary evaluation metrics. Table 4 Quantitative comparisons on the SRD dataset [119]. The best results are highlighted by black bold."
        },
        {
            "title": "Method",
            "content": "DSC [59] (TPAMI19) DHAN [24](AAAI20) BMNet [209](CVPR22) ShadowFormer[45] (AAAI23) ShadowDiffusion [46] (CVPR23) HomoFormer [166] (CVPR24)"
        },
        {
            "title": "Nano Banana Pro",
            "content": "PSNR 27.76 30.51 31.69 32.90 34.73 35.37 20.67 SSIM 0.903 0.949 0.956 0.958 0.970 0.972 0.682 As shown in Tab. 4, significant discrepancy exists between the visual quality discussed earlier and the numerical fidelity scores. While leading methods such as ShadowDiffusion [46] and HomoFormer [166] achieve PSNR scores exceeding 34 dB and SSIM values above 0.97, NB Pro records comparatively lower scores, with PSNR of 20.67 dB and SSIM of 0.682. This quantitative gap can be attributed to the inherent characteristics of generative models: NB Pro prioritizes perceptual plausibility and visual naturalness over strict pixel-wise alignment with ground truth references. Unlike traditional methods that focus on precise reconstruction, generative approaches like NB Pro tend to produce images with enhanced visual appeal, which may deviate from the exact pixel values of ground truth images, resulting in lower scores on fidelity-based metrics. In addition to its successful cases, we systematically document representative failure modes of NB Pro in shadow removal, as compiled in Fig. 14. These examples reveal two core, recurring limitations that stem from the models generative nature: propensity for over-generation and blindness to subtle shadows. Figure 14 Some visual failure examples of NB Pro on the SRD dataset [119] for the shadow removal task. consistent pattern of failure emerges across the SDR dataset, revealing critical limitations in NB Pros approach. The first failure mode stems from the models inherent generative bias. Driven by compulsion to produce visually complete scenes, NB Pro often prioritizes hallucinated content over fidelity. As illustrated in Fig. 14, while the cast shadow is successfully removed, the model erroneously synthesizes new hand to fill the void, fundamentally compromising the scenes semantic integrity. This highlights tension between the models creative instinct and the strict fidelity required for low-level vision, where structural preservation is paramount. The second failure mode exposes lack of sensitivity in shadow detection. NB Pro frequently overlooks soft, low-contrast, or faint shadows (as seen in the second example), leaving them entirely untreated. This suggests potential bias in the training data or optimization objectives that under-weights subtle illumination changes. Furthermore, the model struggles with color fidelity, frequently exhibiting shifts in tone and saturation that deviate from the ground truth. Collectively, these structural hallucinations, detection failures, and color shifts lead to unsatisfactory quantitative performance."
        },
        {
            "title": "5.3 Analysis",
            "content": "Nano Banana Pro demonstrates remarkable ability to decouple shadow components from the underlying reflectance. However, its effectiveness is fundamentally constrained by its generative nature, which presents significant paradigm mismatch with the strict fidelity requirements of shadow removal. First, the models inherent generative bias prioritizes perceptual plausibility over structural fidelity. As generative model, NB Pro tends to hallucinate details, alter textures, or even synthesize new objects to create visually complete scenes. While this enhances aesthetic appeal, it compromises pixel-level accuracy, causing the output to deviate from truthful reconstruction of the original reflectance. Consequently, NB Pro is better suited for creative applications requiring visual realism rather than low-level vision tasks demanding precise photometric or geometric correspondence. Second, the model exhibits limited sensitivity in shadow detection. This limitation is likely attributable to biases in its training data or optimization objectives. If the training distribution under-represents subtle, soft, or low-contrast shadows, the model fails to learn the necessary features to identify them. As result, NB Pro often leaves faint shadows untreated or, conversely, erroneously alters well-lit areas in an attempt to enforce uniform illumination, introducing artifacts in non-shadow regions. 17 Finally, these limitations are exacerbated by the incompatibility between generative outputs and traditional evaluation metrics. NB Pro typically generates images at fixed, high resolutions (e.g., 1K or 4K). Downsampling these outputs to match benchmark ground-truth resolutions smooths out high-frequency details, artificially depressing scores on pixel-wise metrics like PSNR and SSIM. More critically, paradox arises where the models generative enhancements, such as implicit super-resolution or denoising, are heavily penalized as deviations from the ground truth. This stark divergence between perceptual quality and quantitative scores underscores the inadequacy of current fidelity-based frameworks for evaluating generative restoration models."
        },
        {
            "title": "6.1 Background",
            "content": "Motion blur, caused by camera shake or fast-moving objects, remains one of the most pervasive artifacts degrading image quality in photography and computer vision tasks. Over the past decade, deep learning approaches have revolutionized dynamic scene deblurring. Early CNNs, such as DeepDeblur [114] and DeblurGAN [69], paved the way for more sophisticated architectures. Recently, Transformer-based models like Uformer [155] and Restormer [182] have dominated the field, achieving record-breaking scores in peak PSNR by effectively modeling long-range dependencies. However, these regression-based methods, typically optimized via MSE loss, tend to produce overly smooth results, often sacrificing high-frequency textures in favor of minimizing pixel-level error. To overcome the \"smoothing effect\" and restore realistic details, the focus has shifted toward generative models, including GANs and Diffusion Models (e.g., ID-CDM [154], HI-Diff [20]). These approaches leverage strong generative priors, creating plausible textures for missing details. While they significantly enhance perceptual quality, they introduce critical challenge: the perception-distortion trade-off. As the model strives to generate sharper and more visually pleasing images, it risks drifting away from the ground truth fidelity, potentially creating artifacts or \"hallucinating\" content that does not exist in the original scene. In this technical report, we present comprehensive evaluation of Nano Banana Pro (NB Pro). Our investigation reveals fundamental limitation in this aggressive generative approach. While NB Pro demonstrates exceptional capability in synthesizing sharp textures for static environments and text, specifically in challenging low-light scenarios, it suffers from severe semantic instability. Our quantitative analysis (Tab. 5) and visual inspection confirm that the models pursuit of visual sharpness often comes at the cost of fidelity to the input signal. Specifically, we observe that NB Pro struggles with complex motion trajectory reduction, often misinterpreting motion cues as structural elements, which leads to ghosting artifacts. Furthermore, the model exhibits tendency to alter semantic information, such as facial identities and text characters. These generative behaviors result in comparatively low quantitative scores (PSNR/SSIM). By analyzing NB Pros performance across synthetic (GoPro [114], HIDE [130]) and real-world (RealBlur [126]) benchmarks, this report aims to dissect the failure modes of generative deblurring, highlighting the significant gap between producing visually plausible images and maintaining structural accuracy."
        },
        {
            "title": "6.2 Qualitative Results",
            "content": "We conduct visual analysis of NB Pro on standard benchmark datasets(GoPro [114], HIDE [130] and RealBlur [126]) to evaluate its performance in restoring structural details and handling complex degradations. 6.2.1 Performance on Synthetic Datasets NB Pro demonstrates impressive deblurring capabilities on synthetic datasets, particularly in recovering static environmental details. As observed in Fig. 15, in the second column of GoPro and the first column of HIDE, the model effectively suppresses severe motion blur and restores high-frequency structures with great precision. Architectural elements, such as the building facades and pavement textures, are reconstructed with high fidelity. Notably, the model excels at text preservation in these scenarios; for instance, the \"SEPHORA\" signboard in the HIDE dataset is rendered clearly. This indicates strong spatial adaptability in handling rigid motion and structural edges. 18 However, significant limitations become apparent when processing highly dynamic scenes involving humans. The model struggles to fully eliminate complex synthetic motion trajectories, leading to noticeable residual artifacts. In the first column of GoPro, for example, the clothes hanging on the wall appear duplicated, and the womans headscarf exhibits double-layer ghosting effect, suggesting an incomplete resolution of the motion path. Furthermore, the model tends to hallucinate facial details. While the restored faces in both GoPro (1st column) and HIDE (2nd column) appear visually sharp, they suffer from semantic inconsistencies. The facial features are altered to the extent that the identity of the pedestrians no longer matches the Ground Truth (GT), highlighting critical lack of fidelity in semantic reconstruction. Figure 15 Visual results of Nano Banana Pro on synthetic blur datasets (GoPro and HIDE). 6.2.2 Performance on Real-World Datasets On real-world datasets, NB Pro exhibits strong robustness against complex degradations such as low light and overexposure. As seen in Fig. 16, in the RealBlur-J dataset, the model successfully recovers the legibility of text on posters and signboards. Its ability to handle high dynamic range scenes is particularly noteworthy, in the storefront examples (2nd columns of both RealBlur-J and RealBlur-R), the model manages high-contrast lighting effectively. However, the result generated by NB Pro in the second column of RealBlur-R deviates significantly from the GT properties. While the output appears cleaner, it aggressively removes noise and alters lighting textures, resulting in synthesized appearance that loses the atmosphere of the original scene. Moreover, the models reliance on generative priors introduces substantial perceptual deviations from the ground truth. In the poster examples of RealBlur-J (1st column), the restored facial features differ from the original image, creating \"hallucinated\" face that does not preserve the subjects identity. Similar discrepancies are observed in the text content of the RealBlur-J (2nd column) result, where the generated characters deviate from the GT, such as pink characters on the window. This can be also observed in the second column of RealBlur-R, where the characters on the illuminated sign are significantly altered compared to GT. Additionally, color fidelity is occasionally compromised. For instance, in the first column of RealBlur-R, the skin tone of the person in the result exhibits noticeable color shift compared to the target image. These issues indicate that while NB Pro excels at producing visually pleasing results, it sacrifices faithfulness to the original semantic content."
        },
        {
            "title": "6.3 Quantitative Results",
            "content": "Tab. 5 presents the quantitative comparison of NB Pro against state-of-the-art deblurring methods, including transformer-based models like Uformer and Restormer, as well as recent diffusion-based approaches like HI-Diff and ID-CDM. The evaluation is performed on four standard benchmarks: GoPro [114], HIDEHIDE [130] and RealBlur [126], using PSNR and SSIM as the primary metrics. As observed in Tab. 5, significant divergence exists between the previously discussed visual sharpness and the numerical fidelity scores. While top-performing methods such as ID-CDM and HI-Diff achieve PSNR scores exceeding 33 dB on the GoPro dataset and 36 dB on RealBlur-R, NB Pro records comparatively lower values, such as 21.41 dB on GoPro and 21.35 dB on HIDE. Similarly, the SSIM scores for NB Pro range between 0.645 and 0.778, whereas competing methods consistently score above 0.90. This quantitative gap can be primarily attributed to the models heavy reliance on strong generative priors, which prioritizes perceptual plausibility over strict pixel-wise alignment with the ground truth. 19 Figure 16 Visual results of Nano Banana Pro on real-world blur dataset (RealBlur-J and RealBlur-R). The lower PSNR and SSIM scores directly corroborate the limitations identified in the qualitative analysis. Standard metrics like PSNR are highly sensitive to pixel-level deviations. As noted in the visual evaluation, NB Pro tends to hallucinate high-frequency details, such as altering facial identities or modifying text characters, to maximize sharpness. These generated features, while appearing visually coherent, act as \"errors\" regarding the reference image, leading to heavy penalties in signal-to-noise calculations. Furthermore, the reported semantic inconsistencies, such as color shifts and the \"double-layer ghosting\" effects caused by misinterpreted motion trajectories, significantly disrupt structural similarity, resulting in the observed drop in SSIM. This confirms that the models generated content often diverges from the underlying ground truth signal. Table 5 Quantitative comparison results of Nano Banana Pro and other representative methods on four benchmarks."
        },
        {
            "title": "GoPro",
            "content": "RealBlur-R RealBlur-J"
        },
        {
            "title": "HIDE",
            "content": "PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM 29.08 33.14 28.70 29.55 31.10 32.97 33.08 32.92 30.70 33.20 33.33 33.19 21.41 0.914 0.9284 0.858 0.934 0.942 0.967 0.962 0.961 0.901 0.963 0. 0.970 0.645 32.51 34.00 33.79 35.26 33.78 36.22 36.08 36.19 33.96 36.28 36.34 27.43 0.841 0.9265 0.903 0.944 0.909 0.957 0.954 0.957 0.918 0.958 0.955 27.87 27.97 28.70 24.93 29.06 28.82 28.96 24.21 29.15 28.96 0.778 24.51 0.827 0.834 0.866 0.745 0.884 0.876 0.879 0.729 0.890 0.887 0.747 25.73 24.51 26.61 28.94 30.83 31.03 31.22 31.55 31.46 31.53 21.35 0.874 0.871 0.875 0. 0.952 0.940 0.942 0.947 0.945 0.950 0."
        },
        {
            "title": "Method",
            "content": "DeepDeblur [114] GAMD [101] DeblurGAN [69] DeblurGAN-v2 [70] DBGAN [192] Uformer-B [155] Stripformer [138] Restormer [182] IR-SDE [103] DiffIR [163] HI-Diff [20] ID-CDM [154] Nano Banana Pro"
        },
        {
            "title": "6.4 Analysis",
            "content": "The discrepancy between NB Pros superior visual sharpness and its lower quantitative scores stems primarily from the perception-distortion trade-off. While regression-based methods minimize pixel-wise error to maximize PSNR, often resulting in over-smoothed textures, NB Pro leverages generative priors to create high-frequency 20 details. This strategy produces visually realistic textures but introduces stochastic pixel deviations from the ground truth. Since standard metrics like PSNR penalize any deviation equally, NB Pro receives lower scores despite operating at higher level of perceptual quality. Furthermore, the evaluation on real-world datasets reveals the specific limitations of the models reconstruction capability. While the ground truth in RealBlur sometimes contains noise or light scattering, NB Pros tendency to completely \"clean\" these elements represents deviation from the scenes authentic characteristics. Rather than simply restoring the signal, the model synthesizes new, idealized version of the image. This leads to low quantitative scores not just because of metric limitations, but because the model fails to preserve the original distribution of the input data, effectively altering the scenes atmosphere. Consequently, this reliance on generative priors introduces significant risks regarding semantic fidelity. When motion blur obliterates structural information, the model synthesizes plausible but factually incorrect content, leading to the observed \"identity swaps\" in human faces and ghosting artifacts in complex motion paths. While NB Pro excels in perceptual synthesis, making it suitable for visually-oriented restoration, these semantic inconsistencies highlight its unsuitability for applications requiring strict adherence to the original input signal, such as forensic analysis or high-fidelity surveillance."
        },
        {
            "title": "7.1 Introduction",
            "content": "Defocus blur, an inherent optical phenomenon resulting from limited depth of field and aperture configurations, presents one of the most complex challenges in computational photography and low-level vision. Unlike uniform degradations such as global motion blur, defocus acts as spatially varying aberration where the point spread function (PSF) changes according to the scene depth. This results in non-uniform loss of high-frequency details and edge information, making the restoration process an ill-posed inverse problem that requires estimating spatially adaptive kernels. The field of single-image defocus deblurring has advanced significantly with the advent of deep learning. Early data-driven approaches, such as DPDNet [2], established strong baselines by leveraging dual-pixel data to supervise defocus removal. Subsequent architectures, including IFANet [72] and KPAC [131], introduced iterative filtering and kernel prediction mechanisms to better handle spatially varying blur. More recently, the introduction of Transformer-based architectures, such as Restormer [182], and multi-stage networks like MPRNet [110], has pushed the boundaries of restoration fidelity by capturing long-range dependencies and global context. Current state-of-the-art methods, such as GGKMNet [123], further refine this process by integrating grouped kernel modeling to precisely invert the blurring process across complex depth maps. In this section, we extend our evaluation of the Nano Banana Pro (NB Pro) to the domain of defocus deblurring. Unlike the aforementioned supervised methods, which are trained specifically on paired defocus datasets, our investigation focuses on assessing the NB Pro model in zero-shot inference setting. By benchmarking the model against standard datasets such as DPDD [2] and RealDOF [127], we aim to analyze its efficacy in handling the inverse problem of deblurring without domain-specific fine-tuning. Specifically, we examine whether the models processing pipeline can genuinely recover lost structural information comparable to established specialized networks, or if it merely relies on superficial enhancement techniques."
        },
        {
            "title": "7.2 Quantitative Results",
            "content": "Tab. 6 presents the quantitative evaluation on the DPDD [2] and RealDOF [127] datasets. The results indicate substantial performance gap between the Nano Banana Pro and established defocus deblurring methods. On the DPDD dataset, the Nano Banana Pro yields PSNR of 20.180 dB and an SSIM of 0.635, significantly trailing the state-of-the-art GGKMNet by over 6 dB. similar deficiency is evident on the RealDOF dataset, where NB Pro lags behind even early baselines like DPDNet. These low metrics align consistently with our qualitative findings: the depressed PSNR reflects the models general failure to restore pixel-level sharpness, while the low SSIM corroborates the structural hallucinations and inability to remove blur observed in the visual comparison. However, it is worth noting that the competing methods, such as Restormer and DPDNet, 21 Table 6 Quantitative comparison on DPDD and RealDOF datasets. The best results are highlighted in black bold."
        },
        {
            "title": "Model",
            "content": "DPDNet [2] AIFNet [127] IFANet [72] KPAC [131] GKMNet [120] MDP [3] DRBNet [128] MPRNet [110] Restormer [182] INIKNet [122] NRKNet [121] GGKMNet [123] NB Pro"
        },
        {
            "title": "RealDOF",
            "content": "PSNR 24.348 24.213 25.366 25.221 25.468 25.347 25.485 25.730 25.980 26.055 26.109 26.272 20.180 SSIM PSNR 22.870 0.747 23.093 0.742 24.712 0.789 23.975 0.774 24.254 0.789 23.500 0.763 24.884 0.792 24.541 0.792 25.091 25.231 25.148 0.811 0.803 0.810 0.810 25.355 20.821 0. SSIM 0.670 0.680 0.748 0.762 0.732 0.681 0.751 0.736 0.762 0.765 0.768 0.770 0.641 are supervised models trained directly on the DPDD dataset, whereas NB Pro is evaluated here in zero-shot setting without domain-specific training."
        },
        {
            "title": "7.3 Qualitative Results",
            "content": "We evaluated the perceptual performance of the Nano Banana Pro by conducting comprehensive visual analysis on the DPDD and RealDOF datasets. The results indicate consistent limitation in the models ability to recover high-frequency details from severe defocus, with the model frequently prioritizing global contrast enhancement over effective blur removal. On the DPDD dataset, NB Pro behaves more akin to an image enhancement filter than specialized deblurring network. As seen in Fig. 17, in scenarios such as Case 1 and Case 2, the primary modification to the input is global increase in luminance and contrast. While this improves the visual punch of the image, it fails to address the underlying degradation. Specifically, the severely defocused foreground in Case 2 remains blurry, and the background bokeh in Case 1 is only marginally reduced in spread. Furthermore, the model exhibits instability in structural reconstruction. This is evident in Case 3, where the restoration process hallucinates semantic details, incorrectly recovering the text \"GE CANADA\" as \"OE CANADA\" despite only slight improvement in sharpness. Similarly, in Case 4, while the foreground fence is adequately sharpened, it compromises geometric fidelity, resulting in an unexplained scale alteration of the red vehicle in the background. Figure 17 Some representative qualitative results of Nano Banana Pro on the DPDD dataset. The limitations of NB Pro are even more pronounced in the RealDOF dataset evaluations, where the model demonstrates negligible deblurring effect across multiple test cases. As seen in Fig. 18, in scenes with spatially varying blur, such as the mid-range focus in Case 1 and the foreground focus in Case 2, the model 22 fails to reverse the defocus entirely. The output images are characterized solely by slight boost in contrast, leaving the blurred regions perceptually identical to the input. While the model achieves degree of sharpness recovery in the fully blurred scenario of Case 3, this comes at the cost of introducing high-frequency artifacts, specifically salt-and-pepper noise visible on the building structures. Moreover, the restoration in Case 4 is depth-limited. The model successfully restores the ground texture closest to the lens but fails to extend the depth of field to the background, which remains in state of defocus with only minor reduction in the circle of confusion. Collectively, these qualitative results suggest that the Nano Banana Pro lacks the robustness required for consistent defocus deblurring, often failing to produce discernible improvement in image sharpness. Figure 18 Some representative qualitative results of Nano Banana Pro on the RealDOF dataset."
        },
        {
            "title": "7.4 Analysis",
            "content": "The disparity between the quantitative metrics and the qualitative visual outputs reveals the inherent instability of applying general-purpose generative model to the specific physical constraints of defocus deblurring. Our analysis suggests that the Nano Banana Pro does not perform mathematical inversion of the optical point spread function. Instead, it relies on semantic-aware generative priors to synthesize sharp details. This results in bimodal behavior where the model oscillates between superficial contrast adjustment and aggressive, perceptually driven reconstruction depending on the scenes semantic recognisability. In complex scenes with varying depth and high-frequency clutter, such as those in the DPDD dataset, the models generative mechanism often struggles to identify coherent structural cues. Consequently, the model defaults to global contrast maximization approach. This explains the consistently low PSNR values observed in Tab. 6. While increasing local contrast can improve perceptual punch in slightly out-of-focus regions, it fails to mathematically invert the point spread function. Consequently, the model exhibits inconsistent restoration behaviors, resorting to superficial contrast adjustment when semantic cues are ambiguous, while attempting aggressive reconstruction in scenes with recognizable structures. However, in scenarios with regular, recognizable structures such as the building facade in Fig. 18 Case 3, the model successfully engages its learned priors to \"re-paint\" the geometry, effectively removing the blur. Yet, this reconstruction is perceptually driven rather than physically constrained, leading to high-frequency artifacts. The salt-and-pepper noise observed along the window frames is likely byproduct of the generative process (e.g., instability in the diffusion sampling) attempting to force high-frequency gradients into latent features that do not perfectly align with the degraded input. Conversely, when the defocus aligns with typical photographic aesthetics, the model exhibits passivity. This is clearly observed in Fig. 18 Case 4, where the background exhibits only mild defocus and remains semantically distinguishable, yet the model sharpens only the foreground ground texture while leaving the background blur intact. This divergence strongly suggests that the models priors interpret the slight background defocus as an intended aesthetic attribute, specifically, as natural depth-of-field, rather than degradation requiring correction. Unlike dedicated deblurring networks that aim to minimize the circle of confusion globally, NB Pro appears to prioritize perceptual naturalness, effectively treating the background blur as context to be preserved rather than an error to be inverted. 23 Finally, the lack of fidelity constraints in this zero-shot setting leads to significant structural deviations. Since the model prioritizes perceptual plausibility over pixel-wise accuracy, it introduces semantic errors when input ambiguity is high (such as hallucinating \"OE CANADA\" instead of \"GE CANADA\" in Fig. 18 and altering the geometric scale of the vehicle in Fig. 17). These behaviors confirm that the Nano Banana Pro operates as an image re-synthesis engine rather than dedicated restoration tool, resulting in low quantitative scores (PSNR/SSIM) despite occasional visual successes."
        },
        {
            "title": "8.1 Background",
            "content": "Recent advancements in vision-language models [7, 93, 99] mark significant paradigm shift toward unified architectures capable of integrating diverse modalities and tasks within single framework. Notably, models such as Nano Banana Pro have demonstrated that the synergistic training of understanding and generation objectives can unlock emergent capabilities and enhance cross-task generalization. Despite the architectural elegance and representation efficiency offered by unified models [99], critical question persists: Can these generalist systems rival the precision of dedicated restoration networks [113] in specialized low-level vision tasks? Image denoising, specifically, stands as rigorous litmus test. It evaluates models capacity to preserve fine-grained details, textures, and structural fidelity, which are intrinsic not only to restoration but also to high-fidelity image generation and editing. In this technical report, we conduct systematic evaluation of Nano Banana Pros denoising performance across five established benchmark suites: McMaster [193] for natural image statistics, Kodak24 [34] for photographic quality assessment, Urban100 [61] for challenging high-frequency texture reconstruction, and PolyU [169] and SIDD-small [1] for real-world sensor noise suppression. This study serves dual purpose: first, to determine whether Nano Banana Pros unified training regime yields competitive low-level restoration quality; and second, to elucidate the interplay between generative capabilities and fine-grained reconstruction, providing insights to guide the design of future unified architectures."
        },
        {
            "title": "8.2 Experimental Setup",
            "content": "NanoBanana is closed-source unified multimodal model accessed through its official API. As model capable of image understanding, generation, and text-driven editing, we evaluate its denoising capability by providing noisy images alongside natural language instruction. The prompt used throughout our experiments is: This is noisy image, please remove the noise in this image while keep other elements in this image unchanged. Datasets. We evaluate Nano Banana Pro on five widely-used image denoising benchmarks spanning both synthetic and real-world noise scenarios. For synthetic noise datasets like McMaster [193], Kodak24 [34], and Urban100 [61], we corrupt clean images with additive white Gaussian noise at fixed noise level of  = 50, representing challenging high-noise regime. McMaster contains 18 high-resolution images with rich color and texture, Kodak24 comprises 24 classic photographic images, and Urban100 includes 100 images with complex urban structures and repetitive patterns that stress high-frequency reconstruction. For real-world noise datasets like PolyU [169] and SIDD-small [1], we use the standard noisy/clean image pairs provided by each benchmark without additional synthetic corruption. These datasets capture realistic sensor noise from various camera devices under diverse lighting conditions, presenting more practical evaluation scenario. Resolution and Failure Case Handling. Nano Banana Pro outputs images at approximately 1K resolution, though the exact dimensions vary across samples (e.g., 10241024, 1200896, 7201456). We resize the output images to match the resolution of corresponding ground truth images using bilinear interpolation. All metrics are then computed between the resized outputs and the ground truths. In addition, during evaluation, we observed that Nano Banana Pro occasionally produces outputs that are either semantically irrelevant to the input image or fail to remove noise effectively. In such cases, we regenerate the output by resubmitting the same input and prompt to the API until valid denoised result is obtained. This protocol ensures that our quantitative metrics reflect the models denoising capability under successful generation, while the occurrence of such failures is noted as limitation of applying unified generative models to restoration tasks. 24 Evaluation Metrics. We adopt two complementary metrics to assess denoising quality: PSNR (Peak Signal-toNoise Ratio), which measures pixel-level fidelity. SSIM (Structural Similarity Index): Evaluates perceptual structural similarity. Both are computed on RGB channels"
        },
        {
            "title": "8.3 Quantitative and Qualitative Results",
            "content": "To systematically evaluate the image denoising capabilities of Nano Banana Pro, we invoked the model via its official API and compared its performance against five representative task-specific baselines (DnCNN [190], Restormer [182], MaskDenoising [15], HAT [172], and DIL [88]). The evaluation spans two distinct regimes. First, we employed three synthetic benchmarksMcMaster [193], Kodak24 [34], and Urban100 [61]corrupted with additive Gaussian noise ( = 50) to test reconstruction across varying complexities. Specifically, McMaster assesses basic noise removal in smooth textures; Kodak24 covers diverse natural scenes to balance texture and color fidelity; and Urban100 challenges the models ability to preserve high-frequency details within complex architectural structures. Complementing these synthetic tests, we assessed real-world blind denoising performance using SIDD Val [1] and PolyU [169], where no prior noise information is provided. SIDD Val serves as core benchmark for handling authentic sensor noise captured under varying lighting and device conditions. Furthermore, PolyU is utilized to stress-test the models generalization capabilities on irregular noise distributions characteristic of low-light and complex environments. Table 7 Quantitative results of performance comparison on synthetic and natural noise datasets. The metrics are PSNR and SSIM, where higher values indicate better performance. Noise Types Datasets DnCNN [190] Restormer [182] MaskDenoising [15] HAT [172] DIL [88] NB pro Gauss  = 50 Natural McMaster [193] Kodak24 [34] Urban100 [61] 20.18/0.312 19.78/0.301 19.62/0.420 20.47/0.312 20.12/0.321 19.36/0.437 SIDD Val [1] PolyU [169] -/- -/- -/- -/- 20.63/0.379 20.72/0.368 20.51/0.485 33.14/0.913 24.78/0.812 20.79/0.364 21.04/0.390 20.80/0.492 26.61/0.669 27.46/0.736 25.89/0. 21.57/0.594 20.04/0.517 19.22/0.607 28.58/0.570 37.25/0.948 34.76/0.848 37.65/0.950 26.76/0.681 22.82/0.806 As shown in Tab. 7, Nano Banana Pro exhibits substantial performance deficit compared to all task-specific baselines. On synthetic datasets, it lags significantly behind the state-of-the-art DIL, with PSNR gaps ranging from 5.04 dB to 7.42 dB and SSIM reductions between 0.075 and 0.219. Notably, this disparity persists regardless of texture complexity (from McMaster to Urban100), indicating fundamental lack of competitiveness in Gaussian noise removal. This limitation is further exacerbated in real-world blind denoising tasks. On SIDD Val [1], Nano Banana Pro trails DIL by 8.00 dB in PSNR. The gap widens drastically on PolyU [169], where it underperforms DIL by 14.83 dB and even falls behind the basic MaskDenoising model. These results underscore an inherent inability of Nano Banana Pro to effectively model and remove complex, realistic noise compared to specialized restoration models. Fig. 19 visually compares Nano Banana Pro against state-of-the-art baselines. The results reveal distinct characteristic of the generative approach: trade-off between perceptual clarity and pixel-level fidelity. As shown in the first row, Nano Banana Pro exhibits exceptional perceptual quality on text-rich images. Leveraging its generative priors, it reconstructs the characters with remarkable sharpness. Notably, the output appears even clearer and more legible than the Ground Truth, effectively performing text enhancement alongside denoising. Conversely, the model struggles with consistency in texture and color, as seen in the subsequent rows: In the second row, the model fails to recover the subtle grain of the surface. Instead of preserving the original high-frequency details, it produces an over-smoothed or hallucinated texture that deviates significantly from the Ground Truth. In the third row, the model introduces chromatic deviations. While the noise is removed, the color of the grapes shifts noticeably (appearing brighter and yellower). These cases underscore that while Nano Banana Pro can generate visually pleasing results, it lacks the strict fidelity required for high-precision restoration tasks."
        },
        {
            "title": "8.4 Discussion",
            "content": "Based on the experimental results and architectural characteristics, the suboptimal denoising performance of Nano Banana Pro is attributed to two primary factors: 25 Figure 19 Visual comparison of denoising results. The first row shows successful case, where Nano Banana Pro produces text with sharper edges than the Ground Truth, partly due to its generative priors. The last two rows, however, reveal limitations in fidelity: in the second row, surface texture is not preserved, losing high-frequency details; in the third row, noticeable color distortion shifts the hue of the grapes away from the Ground Truth. Misalignment of Task Objectives: Nano Banana Pro is general-purpose model optimized for high-level multimodal understanding and generation, rather than low-level pixel-wise restoration. It lacks the specialized architectural biases and targeted loss functions that enable baseline models to effectively balance noise removal with detail preservation. Trade-off Between Generative Prior and Pixel Fidelity: As unified model, Nano Banana Pro prioritizes semantic plausibility and visual coherence over strict pixel-level accuracy. This generative nature often leads to the over-smoothing of high-frequency details in pursuit of reasonable content, resulting in inferior quantitative metrics compared to task-specific models trained via strict supervision. In summary, this study evaluated the unified generative model Nano Banana Pro against state-of-the-art specialized models on both synthetic Gaussian noise and real-world blind denoising datasets. Nano Banana Pro significantly underperforms task-specific baselines across all benchmarks, indicating limited competitiveness in direct denoising applications. Direct application of Nano Banana Pro for denoising is not recommended without modification. Enhancing its utility requires targeted adaptations such as prompt engineering, parameter fine-tuning, or integration with post-processing modules. Future research should explore methodologies to align general-purpose generative priors with low-level processing demands."
        },
        {
            "title": "9.1 Background",
            "content": "In fields such as computer vision, clear and interference-free image data is fundamental foundation for subsequent analytical tasks including object detection and semantic segmentation. However, reflective surfaces like glass, water, and metal easily reflect ambient light into images, forming an interfering reflection layer over the real scene. This causes blurred details and obscured target information, directly compromising the reliability and accuracy of subsequent tasks. Single-Image Reflection Removal (SIRR), as core technical solution, aims to accurately separate the transmission layer (real scene) from the reflection layer (interfering component) in single mixed image to restore the true scene. It holds irreplaceable practical value in autonomous driving, security monitoring, consumer electronics, and other areas. 26 SIRR is inherently typical ill-posed inverse problemwithout additional constraints, mixed image decomposition has infinitely many solutions. Early traditional methods relied on manually designed prior knowledge (sparsity, smoothness, and other assumptions) and linear modeling (such as = + R) to simplify the problem, but real-world reflections exhibit complex nonlinear characteristics due to light intensity, shooting angle, surface material, and other factors, leading to limited generalization of these methods. In recent years, deep learning has become the mainstream in SIRR research, forming three core architectures: single-stage approaches that directly output the target layer via single network [159, 196], two-stage approaches that perform intermediate feature estimation followed by refinement [30, 90], and multi-stage approaches that achieve reflection removal through recurrent cascaded iterative optimization [76, 174]. Notably, the rapid development of generative artificial intelligence has injected new vitality into the field, with methods based on diffusion models and Transformers demonstrating potential to break through traditional limitations [52, 180]. Nevertheless, existing approaches face significant bottlenecks: the scarcity of high-quality annotated datasets restricts model generalization, and issues such as scene information loss from strong reflections and overlapping appearance distributions between transmission and reflection layers make it hard to balance thorough reflection removal and detail preservation. Existing research covers traditional methods, deep learning architectures, and generative paradigms, but there remains substantial room for improvement in robustness and detail fidelity under complex real-world scenarios. On one hand, while generative models show promise, their hallucination suppression capabilities and adaptability to complex reflection mechanisms in high-fidelity tasks like SIRR have not been fully verified. On the other hand, efficient solutions for diverse reflection scenarios (diverse material surfaces, extreme lighting, and other scenarios) are lacking, demanding more generalizable generative models. Based on this, this report focuses on the latest generative model Nano Banana Pro. By systematically comparing it with existing baselines using quantitative and qualitative metrics, we investigate its detail restoration, anti-interference performance, and generalization in real reflection removal scenarios. The goal is to reveal its core advantages and limitations, providing practical references for technical optimization and model design in the SIRR field."
        },
        {
            "title": "9.2 Quantitative Results",
            "content": "To evaluate the performance of Googles Nano Banana Pro model on the SIRR task, we conducted experiments using the model as an off-the-shelf solution via API calls. Given the closed-source nature of the model which precludes task-specific fine-tuning, we adopted direct inference strategy. Only the raw reflectioncontaminated images and task-specific prompts were provided as input, without introducing any additional priors or auxiliary guidance. It is worth noting that the resolution of images generated by Nano Banana Pro is fixed at scale of approximately 1024 pixels, which differs from the original input dimensions. To ensure the fairness and accuracy of the quantitative evaluation, all output images were resized to match the original resolution of the corresponding ground-truth images before metric calculation. For comprehensive assessment, we adopted three mainstream datasets in the SIRR domain as our evaluation benchmark. Specifically, we utilized Real20 [196], which contains 20 images from real-world glass reflection scenes; Nature [76], consisting of 20 samples focusing on outdoor natural landscapes; and SIR2 [141], where we evaluated on its Objects, Postcard, and Wild subsets. We compared Nano Banana Pro against 15 state-ofthe-art baseline models, including ERRNet [159], IBCLN [76], YTMT [56], Dong et al. [30], DSRNet [57], RAGNet [90], RRW [210], DSIT [58], RDNet [201], F2T2-HiT [13], Huang et al. [63], L-DiffER [52], DAI [55], Lu et al. [100], and WindowSeat [180]. The evaluation metrics assess both basic image quality and perceptual quality. For pixel-level fidelity, we employed Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM), with comprehensive comparison results summarized in Tab. 8. To better assess visual perception, we further incorporated MultiScale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS). Note that lower LPIPS values indicate better perceptual quality. For these perceptual metrics, we selected recent SOTA methods for comparison (with baseline data sourced from WindowSeat [180]), as presented in Tab. 9. All metrics were calculated on the RGB channels between the resized output and the ground truth. As shown in Tab. 8, Nano Banana Pro exhibits notable performance gap compared to state-of-the-art specialist methods. Quantitatively, it lags behind across all datasets in pixel-wise metrics (PSNR/SSIM). This disparity largely stems from the fundamental difference in optimization objectives: regression-based SOTA 27 Table 8 Quantitative Comparison of Single-Image Reflection Removal Methods. The best and second-best results are highlighted by black bold and underline, respectively. : Training data includes Nature dataset; : Generative AI-based method. Note: indicates higher is better. SIR dataset is divided into Objects, Postcard and Wild subsets; SIR2(454) denotes the commonly used subset, while SIR2(500) denotes the full public dataset. Method ERRNet [159] IBCLN [76] YTMT [56] Dong et al. [30] DSRNet (w/o extra) [57] DSRNet (with extra) [57] RAGNet [90] RRW [210] DSIT (data I) [58] DSIT (data II) [58] RDNet (w/o nature) [201] RDNet (w nature) [201] F2T2-HiT [13] Huang et al. [63] L-DiffER [52] DAI [55] Lu et al. [100] WindowSeat [180] WindowSeat (Qwen-IE) [180] Year 2019 2020 2021 2021 2023 2023 2023 2024 2024 2024 2025 2025 2025 2025 2025 2025 2025 2025 2025 Real20 (20) Nature (20) Objects (200) Postcard (199) Wild (55) SIR2 (454) SIR2 (500) PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM 22.89 21.86 23.26 23.34 24.23 23.91 22.95 23.82 25.06 25.22 24.43 25.58 21.64 25.12 23.77 25.24 - 26.28 26.60 0.803 0.762 0.806 0.812 0.820 0.818 0.793 0.817 0.836 0.836 0.835 0.846 0.766 0.828 0.821 0.840 - 0.856 - 23.57 - 23.45 - - - 25.96 - - - - 26.08 27.03 23.95 27.05 - 27.12 0. 0.655 27.57 21.48 - 0.783 - 0.808 - - - 0.843 - - - - 0.837 0.853 0.831 0.846 - 0.849 24.87 24.87 24.87 24.36 26.28 26.74 26.15 - 26.81 27.27 25.76 26.78 - 27.07 - - - 28.81 0. 28.85 0.896 0.893 0.896 0.898 0.914 0.920 0.903 - 0.919 0.932 0.905 0.921 - 0.930 - - - 0.944 0.938 22.04 23.39 22.91 23.72 24.56 24.83 23.67 - 25.63 25.58 25.95 26.33 - 26.43 - - - 29.17 28.70 0.876 0.875 0.884 0.903 0.908 0.911 0.879 - 0.924 0.922 0.920 0.922 - 0.931 - - - 0.934 0.933 24.25 24.71 25.48 25.73 25.68 26.11 25.52 - 27.06 27.40 27.20 27.70 - 27.96 - - - 28. 0.853 0.886 0.890 0.902 0.896 0.906 0.880 - 0.910 0.918 0.910 0.915 - 0.922 - - - 0.935 29.44 0.936 23.55 24.20 24.08 24.25 25.45 25.83 24.99 25.45 26.32 26.54 26.02 26.69 25.72 26.90 - - - 28.99 28.84 0.882 0.884 0.890 0.901 0.909 0.914 0.890 0.910 0.920 0.926 0.912 0.921 0.903 0.929 - - - 0.939 0. - - - - - - - - - - - - - - 25.18 27.32 28.41 28.75 28.60 - - - - - - - - - - - - - - 0.911 0.931 0.912 0.940 0.937 0.723 21.95 0. 19.29 0.675 23.58 0.798 20.98 0. 21.11 0.730 Nano Banana Pro 2025 20.26 methods are supervised to minimize pixel-level reconstruction error, ensuring precise alignment. In contrast, the generative approach of Nano Banana Pro prioritizes semantic coherence over structural fidelity, often resulting in global intensity scaling and spatial shifts that heavily penalize PSNR, even if the image content is semantically correct. Table 9 Perceptual Quality Comparison (MS-SSIM and LPIPS) on Mainstream Datasets. indicates higher is better, while indicates lower is better. The best and second-best results are highlighted in black bold and underline. Baseline results are sourced from WindowSeat [180]. Method Real20 (20) Nature (20) Objects (200) Postcard (199) Wild (55) MS-SSIM LPIPS MS-SSIM LPIPS MS-SSIM LPIPS MS-SSIM LPIPS MS-SSIM LPIPS DSRNet [57] DAI [55] RDNet [201] DSIT [58] WindowSeat [180] WindowSeat (Qwen-IE) [180] Nano Banana Pro 0.8737 0.9045 0.9081 0.8934 0.9296 0. 0.8013 0.1831 0.1790 0.1442 0.1618 0.1131 0.1074 0.2411 0.9144 0.9309 0.9231 0.9223 0.9435 0. 0.8580 0.1478 0.2161 0.1361 0.1598 0.1368 0.1355 0.1851 0.9564 0.9638 0.9609 0.9586 0.9759 0. 0.0847 0.0689 0.0836 0.0939 0.0470 0.0550 0.9263 0.9567 0.9361 0.9441 0.9693 0.9664 0.1260 0.1029 0.1121 0.1242 0.0504 0. 0.8861 0.1552 0.8373 0.2513 0.9338 0.9423 0.9406 0.9447 0.9625 0. 0.8874 0.1096 0.0941 0.0992 0.0967 0.0632 0.0682 0.1578 Tab. 9 further illustrates the performance in terms of perceptual quality metrics. Despite the generative nature of Nano Banana Pro, which typically favors perceptual scores, it still exhibits high LPIPS values (e.g., 0.2513 on Postcard vs. 0.0549 for SOTA). Unlike PSNR, which penalizes misalignment, the poor LPIPS performance points to deeper issue: semantic and stylistic deviation. The model tends to perform aggressive \"image-to-image translation\" rather than faithful restoration, altering fundamental scene characteristicssuch as modifying illumination, hallucinating textures, or shifting the color domainthereby drifting away from the ground truths perceptual manifold. From an interpretive perspective, the elevated LPIPS and sub-optimal MS-SSIM scores highlight the limitations of general-purpose generative models in high-fidelity restoration tasks. Although the model possesses strong generative capabilities, it lacks precise mechanism for decoupling reflection layers from the background. The high LPIPS values suggest substantial deviation in color distribution, texture details, and high-level semantic features relative to the ground truth. This deviation likely stems from the model partially merging residual reflections into the background or altering the original color and complex textures during the regeneration 28 process. Consequently, while the generated images may appear visually natural, they fail to meet the strict fidelity requirements essential for reflection removal tasks."
        },
        {
            "title": "9.3 Qualitative Results",
            "content": "In this section, we conduct comprehensive qualitative evaluation of the proposed Nano Banana Pro. To establish comparative baseline, we first benchmark our visual results against existing state-of-the-art methods in Fig. 20. Subsequently, we examine the specific restoration capabilities of our model through selected samples in Fig. 21. Finally, to ensure balanced assessment and facilitate future improvements, we provide an analysis of the models limitations by categorizing typical failure cases and degradation patterns in Fig. 22. Figure 20 Qualitative comparison of reflection removal results. Each row shows the input image, predictions from state-of-the-art methods, Nano Banana Pro, and the ground truth (GT) for single sample. Except for the results of Nano Banana Pro, all other method results are sourced from WindowSeat [180]. As illustrated in Fig. 20, the proposed Nano Banana Pro exhibits high variance in performance compared to state-of-the-art methods. In specific instances, our method outperforms existing approaches, yielding results that are visually comparable from the ground truth. However, generally, it lacks the stability of specialized regression-based models. Notably, the model struggles with preserving high-frequency details, leading to the loss of complex textures (e.g., row 1), or suffers from semantic ambiguity where reflection artifacts are erroneously interpreted as background elements and subsequently enhanced (e.g., row 4). These limitations contribute to lower average performance despite the high perceptual quality in successful cases. Fig. 21 showcases selected samples where Nano Banana Pro demonstrates superior restoration capabilities. It is observed that when there is significant semantic or visual distinction between the reflection and transmission layers, the model effectively suppresses the reflection while preserving background integrity. The results indicate that the model possesses high performance upper bound, occasionally achieving reconstruction quality nearly identical to the ground truth. We attribute this potential to the robust generative priors acquired from large-scale pre-training. However, the absence of domain-specific supervision for reflection separation implies trade-off: without explicit guidance, the model may misapply these priors, failing to disentangle the layers or introducing generative hallucinations and noise into the transmission layer. Experimental results suggest that such misuse of priors accounts for considerable portion of the suboptimal outputs. Given the quantitative gap observed in the previous section, explicitly analyzing the failure modes provides critical insights into the misbehavior of generative priors. Based on the distinct characteristics of the introduced artifacts and degradation mechanisms, the suboptimal performance of Nano Banana Pro can be systematically categorized into six types, as visualized in Fig. 22. Figure 21 Visual comparison of selected samples. We present the input images, the restoration results generated by Nano Banana Pro, and the corresponding ground truth. These examples illustrate the visual performance of the method in recovering background content across different scenes. Figure 22 Qualitative analysis of limitations and failure cases. We present typical examples where the proposed method yields suboptimal results, categorized by specific degradation types: (a) incomplete reflection removal due to strong intensity; (b) erroneous enhancement of reflection artifacts mistaken for background details; (c) unintended color deviation in the transmission layer; (d) significant texture distortion; (e) structural deformation compared to the ground truth; and (f) complex scenarios exhibiting compound artifacts involving multiple aforementioned issues. (a) Incomplete Reflection Removal. In these instances, the model fails to effectively decouple the reflection layer from the transmission layer, resulting in significant residual artifacts. This behavior likely stems from conservative inference strategy induced by prompts emphasizing background preservation. When the reflection 30 intensity is high or statistically similar to the background, the model tends to classify the reflection as intrinsic scene content to avoid over-erasing potential background details. (b) Erroneous Enhancement due to Semantic Ambiguity. Despite explicit instructions to suppress reflections, the model occasionally misinterprets reflection artifacts as valid background elements and erroneously enhances them. This phenomenon highlights limitation in current generative priors: the model is driven by semantic plausibility rather than physical layer separation. When reflection (e.g., light source or architectural reflection) aligns semantically with the background scene, the model prioritizes generating \"coherent\" image without contradictions, thereby integrating the artifact as strengthened feature. (c) Unintended Chromatic and Domain Shift. This error is predominantly observed in the Postcard dataset, which features images of paintings or prints (e.g., urban or humanist subjects). The model struggles with domain ambiguity, failing to distinguish between \"picture of scene\" and \"real-world scene.\" Consequently, it attempts to \"restore\" the printed content as realistic photograph, aggressively altering saturation, removing characteristic grain, or modifying illumination. This results in severe color deviations and stylistic inconsistencies compared to the ground truth. (d) Texture Fidelity Loss. In scenarios containing high-frequency details, such as natural foliage or fabric textures (e.g., towels), the generative process often fails to maintain the original texture distribution. The output tends to exhibit either unnatural over-smoothing (loss of fine grain) or artificial sharpening (introduction of high-frequency noise), indicating lack of fine-grained control in the texture reconstruction module. (e) Structural Hallucination and Deformation. In rare but severe cases, the model breaks structural consistency, generating outputs that deviate geometrically from the input. This includes the hallucination of non-existent background structures or the removal of actual objects mistaken for reflections. Such failures represent collapse of the conditioning mechanism, where the strong generative prior overrides the spatial constraints provided by the input image. (f) Compound Degradation. significant portion of low-scoring results exhibits hybrid of the aforementioned failure modes. For example, an image may suffer from incomplete reflection removal while simultaneously undergoing global color shift, or experience structural deformation alongside texture smoothing. These complex scenarios represent the most challenging cases for the current architecture."
        },
        {
            "title": "10.1 Background",
            "content": "Lens flare constitutes fundamental optical phenomenon wherein intense incident light undergoes scattering and reflection within cameras lens system, resulting in parasitic artifacts that degrade image quality. These artifacts manifest predominantly as two distinct categories: scattering flares and reflective flares. Beyond aesthetic degradation, these artifacts critically impair downstream computer vision applications, including stereo matching misestimation, optical flow corruption, and semantic segmentation misclassification, thereby posing substantial risks to safety-critical systems such as autonomous driving and aerial object tracking [64, 87, 162]. Contemporary flare removal methodologies have evolved from traditional detection-based approaches to sophisticated deep learning frameworks enabled by large-scale datasets. The Flare7K++ [25] dataset represents pivotal advancement, providing 7,000 synthetic flares with 25 scattering and 10 reflective patterns, supplemented by 962 real-captured flare images (Flare-R) that capture complex degradation effects unattainable through simulation alone. Recent state-of-the-art approaches demonstrate remarkable progress: DeflareMamba [62] introduces the first State Space Model (SSM)-based architecture, employing hierarchical U-shaped framework with local-enhanced selective scan mechanisms to maintain contextual consistency across global flare patterns and local scene details. Meanwhile, the MiAlgo AI team achieved top performance in the MIPI 2024 Nighttime Flare Removal Challenge through Progressive Perception Diffusion Network (PPDN) [26], combining an IR-SDE diffusion module for comprehensive flare elimination with an AOT Block enhancement stage for detail recovery, employing two-stage progressive strategy to improve visual quality. 31 Performance assessment is conducted on two complementary benchmark suites. The Flare7K++ [25] test set comprises 100 meticulously aligned 512512-resolution real-world flare-corrupted/flare-free image pairs, with manual annotations delineating glare, streak, and light source regions to enable component-specific evaluation via G-PSNR and S-PSNR metrics. Additionally, the MIPI 2024 Challenge introduces FlareReal600 [26], high-resolution dataset featuring 600 aligned training images, with validation and test sets each containing 50 pairs available in both 2K (14401920) and 4K (17743840) resolutions to facilitate comprehensive evaluation across different spatial scales. In this subsection, we will systematically evaluate the flare removal capability of the Nano Banana Pro model on these benchmarks. We will examine its effectiveness in eliminating various nighttime flare artifacts while maintaining the scenes semantic and photometric integrity across different image resolutions, thereby providing reference for the community. Table 10 Quantitative comparisons of Nano Banana Pro and representative specialists on the Flare7K++ dataset. Metric PSNR SSIM Input Restormer [182] Uformer [155] Flare-level [27] DeflareMamba [62] NB Pro 22.56 0.857 27.60 0.897 27.63 0.894 27.05 0. 26.06 0.898 24.92 0.844 Table 11 Quantitative comparisons of Nano Banana Pro and representative specialists on the FlareReal600 dataset. Metric LPIPS [194] PSNR SSIM [157] PPDN [26] NB Pro(2K) NB Pro(4K) 0.143 22.15 0.708 0.287 19.07 0.496 0.361 18.32 0."
        },
        {
            "title": "10.2 Qualitative and Quantitative Results",
            "content": "To comprehensively assess the capabilities of Nano Banana Pro, we organized our experiments into quantitative evaluation and qualitative analysis. Quantitative Evaluation. We first evaluated the model on the Flare7K++ dataset configured to an output resolution of 1K. Performance was measured using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) [157]. As shown in Tab. 10, we compared Nano Banana Pro against state-of-the-art methods trained on the same dataset, including Restormer [182], Uformer [155], and DeflareMamba [62]. Subsequently, we extended our evaluation to the FlareReal600 dataset, processing images at their native resolutions to output 2K and 4K results. In addition to PSNR and SSIM, Learned Perceptual Image Patch Similarity (LPIPS) [194] was included to assess perceptual quality. Tab. 11 benchmarks our results against the MIPI 2024 Challenge champion, MiAlgo AI. Note that while the challenge metrics were derived from an unpublished test set, our evaluation utilized the publicly available validation set. We observed two notable quantitative trends: 1) Resolution Impact: Performance metrics generally decline as output resolution increases. 2) Brightness Sensitivity: On the high-resolution FlareReal600 dataset, higher image brightness results in degraded metrics. However, this trend is not evident in the lower-resolution Flare7K++ dataset. Qualitative Analysis. Visual comparisons in Fig. 23 reveal distinct dichotomy in the models performance: 1. Visual Superiority vs. Stochastic Instability: On optimal inputs, Nano Banana Pro demonstrates exceptional deflaring capabilities, often surpassing SOTA methods in detail restoration. However, this advantage is compromised by the inherent stochasticity of diffusion models. The model exhibits significant variance and is prone to semantic hallucinationssuch as generating unrelated content, suppressing valid light sources, or erroneously illuminating inactive bulbs. While prompt engineering offers partial mitigation, it fails to guarantee the deterministic reliability required for industrial deployment. 32 Figure 23 Qualitative comparison of flare removal results on the Flare7K++ dataset. Nana Banana Pro can preserve image details near light sources and achieves clean removal of streak artifacts. However, it may introduce some brightness changes, as shown in the third row. Figure 24 Examples of some high-scoring and low-scoring samples from Nano Banana Pro on the Flare7K++ dataset. It can be observed that low-scoring samples sometimes appear visually satisfactory, yet their quantitative metric scores may be lower due to certain discrepancies in brightness or color compared to the ground truth. 2. Perceptual-Metric Misalignment: As illustrated in Fig. 24, we observe notable divergence between quantitative metrics and perceptual quality. Instances with low scores sometimes retain high visual fidelity, suggesting that pixel-level metrics may not fully capture the perceptual advantages of generative reconstruction. In conclusion, Nano Banana Pro exhibits high ceiling, low floor characteristic. While it possesses the generative potential to outperform traditional regression-based methods in perceptual quality, it currently sacrifices the stability and consistency essential for robust image restoration."
        },
        {
            "title": "11.1 Background",
            "content": "Low-light image enhancement [11, 158, 177] aims to recover visually pleasing images from photographs captured under insufficient illumination. This task presents significant challenges, including brightness adjustment [14], noise suppression [54], and color restoration, all while preserving structural details and semantic content. Traditional approaches to this problem have relied on handcrafted priors or supervised deep learning methods trained explicitly on paired low-light and normal-light images. Recent advances in unified multimodal models [99], which jointly handle image understanding and generation within single framework, raise an intriguing question: can such models perform low-light enhancement in zero-shot manner, leveraging their broad visual and semantic knowledge without task-specific training? In this chapter, we evaluate Nano Banana Pro, unified generation and understanding multimodal model, on the low-light image enhancement task. Our evaluation spans three widely-used benchmarks, LOLv1 [158], LOLv2real [177], and SICE [11], then we compare Nano Banana Pros zero-shot performance against state-of-the-art supervised and unsupervised methods."
        },
        {
            "title": "11.2 Experiment Setup\nDatasets. We conduct experiments on three established low-light enhancement benchmarks. LOLv1 [158]\ncontains 485 training pairs and 15 testing pairs of real-world low-light and normal-light images captured\nby adjusting camera exposure time and ISO. LOLv2-real [177] extends this with 689 training pairs and 100\ntesting pairs, featuring more diverse indoor and outdoor scenes. SICE [11] is a larger-scale dataset containing\nmulti-exposure sequences, from which low-light and reference pairs are constructed; its test set comprises a\nmore diverse range of scenes and illumination conditions.",
            "content": "Evaluation Metrics. we adopt two standard full-reference image quality metrics: Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). Higher values indicate better reconstruction quality relative to the ground-truth normal-light images. Comparison Methods. We compare Nano Banana Pro against several representative low-light enhancement methods spanning different paradigms: ZeroDCE [44] (zero-reference learning), RUAS [96] (architecture searchbased), LLFlow [152](normalizing flow-based), LLFormer [147] (transformer-based), GSAD [54] (diffusionbased), and Quadprior [149] (diffusion prior-based). These methods represent the current state of the art in supervised and unsupervised low-light enhancement. Nano Banana Pro Configuration. Nano Banana Pro is evaluated in zero-shot setting without any fine-tuning on low-light enhancement data [11, 158, 177]. We provide the model with the following natural language instruction: This is low-light image, please turn this image into normal image while keeping other elements unchanged. No additional inference-time configurations or post-processing steps are applied."
        },
        {
            "title": "11.3 Qualitative and Quantitative Results",
            "content": "Tab. 12 presents the quantitative comparison across all three benchmarks. On LOLv1 [158] and LOLv2real [177], Nano Banana Pros zero-shot performance falls considerably short of the state-of-the-art supervised methods. The gap is particularly pronounced on LOLv2-real [177], where the PSNR of 15.661 dB and SSIM of 0.537 lag behind leading methods by substantial margin. This suggests that without task-specific training, the model struggles to consistently produce enhancements that align with the ground-truth references in these benchmarks. Interestingly, on the SICE [11] dataset, Nano Banana Pro achieves slightly higher metrics than several comparison methods, demonstrating competitive zero-shot performance on this more challenging and diverse benchmark. Fig. 25 presents representative visual comparisons across these three datasets [11, 158, 177]. Nano Banana Pro produces visually reasonable enhancements in many cases, successfully brightening dark regions and 34 Table 12 Quantitative comparisons on the LOL and SICE datasets. The best results are highlighted by black bold. Methods Color Model LOLv1 [158] LOLv2-Real [177] SICE [11] Retinex RetinexNet [158] Retinex KinD [197] RGB ZeroDCE [44] Retinex RUAS [96] LLFlow [152] RGB EnlightenGAN [68] RGB SNR-AW [171] Bread [48] PairLIE [37] LLFormer [147] RetinexFormer [14] Retinex Retinex GSAD [54] Kubelka-Munk QuadPrior [149] - SNR+RGB YCbCr Retinex RGB Nano Banana Pro PSNR 18.915 23.018 21.880 18.654 24.998 20.003 26.716 25.299 23.526 25.278 27.140 27.605 22.849 SSIM LPIPS PSNR 16.097 0.470 0.427 17.544 0.156 0.843 16.059 0.335 0.640 15.326 0.270 0.518 17.433 0.117 0.871 18.230 0.317 0.691 21.480 0.152 0.851 20.830 0.155 0.847 19.085 0.248 0.755 0.167 0.823 20.856 0.129 0.850 22.794 20.153 20.592 0.876 0.800 0.092 0. 0.481 SSIM LPIPS PSNR 12.424 0.543 0.401 - 0.375 0.669 12.452 0.313 0.580 8.656 0.310 0.488 12.737 0.176 0.831 - 0.309 0.617 - 0.163 - 0.174 - 0.317 - 0.211 - 0.171 - - 0.849 0.847 0.778 0.792 0.840 0.846 0.811 0.113 0.202 0.639 0.494 0.617 - - - - - - - - 18. 0.684 15.661 0.537 0.465 14.081 0. - - - - - - - - - - - - - - SSIM LPIPS 0.613 - revealing scene content. However, the model exhibits inconsistent brightness control: in the first row, it tends to overexpose bright regions, while in others, it insufficiently enhances dark areas, leaving the output still underexposed. This inconsistency likely stems from the models reliance on general visual priors rather than explicit illumination modeling. Notably, Nano Banana Pro does not introduce visible artifacts such as color distortion, halo effects, or structural corruption, which is common failure mode of some enhancement methods. Texture preservation remains comparable to other approaches, with fine details in enhanced regions generally retained. The absence of artifacts suggests that the models generative capabilities are well-regularized, even when applied to out-of-distribution tasks like low-light enhancement. Figure 25 Visual comparison examples of Nano Banana Pro and several representative specialists. The first row shows its shortcomings in brightness consistency, while the second row shows its superiority in detail preservation."
        },
        {
            "title": "11.4 Analysis",
            "content": "The evaluation results reveal nuanced picture of Nano Banana Pros zero-shot capabilities for low-light image enhancement. In this section, we provide an in-depth analysis of the observed performance patterns, examine potential underlying causes, and discuss broader implications for applying unified multimodal models to image restoration tasks. notable positive finding is that Nano Banana Pro does not introduce visible artifacts such as color distortion, halo effects around high-contrast edges, amplified noise, or structural corruption. This is significant because artifact introduction is common failure mode of enhancement methods, particularly those based on generative models. The absence of artifacts may partially explain the moderate PSNR/SSIM scores. More aggressive enhancement methods might achieve higher metrics on average by pushing brightness and contrast more strongly, but at the cost of occasional artifacts. Nano Banana Pros conservative approach avoids such failures but may sacrifice peak performance. For practical applications where artifact-free outputs are critical, this trade-off may be acceptable. However, the performance gap on standard benchmarks indicates 35 that zero-shot application is not yet competitive with task-specific methods. The lack of explicit illumination modeling, sensitivity to prompt formulation, and inability to match benchmark-specific ground-truth definitions all limit current performance. Several avenues could potentially improve performance: 1) prompt engineering to provide more specific enhancement guidance; 2) few-shot learning with example image pairs to calibrate the models enhancement behavior; 3) lightweight fine-tuning or adapter-based adaptation to inject task-specific knowledge while preserving general capabilities; and 4) hybrid approaches that combine unified models semantic understanding with task-specific enhancement modules."
        },
        {
            "title": "12.1 Background",
            "content": "Underwater image degradation is an inherent visual quality loss issue in marine environments. As light propagates through water, it undergoes selective absorption by water molecules, multiple scattering by suspended particles such as plankton and sediments, and complex illumination fluctuations, resulting in series of characteristic image defects. These defects manifest primarily in three core types: color distortion, where red light rapidly attenuates in shallow waters, resulting in blue-green color bias; contrast reduction stemming from haze-like obscuration caused by scattered light, significantly lowering the image signal-to-noise ratio; and texture blurring, direct consequence of high-frequency edge information being obscured by scattered particles. Beyond compromising the visual presentation of underwater scenes, these degradations severely undermine the reliability of downstream computer vision tasks. This includes missed detections in underwater object recognition, biased seabed topography reconstruction, and misinterpreted biological behavior analysis. Consequently, it poses significant risks to safety-critical applications such as marine resource exploration, underwater cultural heritage archaeology, unmanned submersible navigation, and seabed infrastructure maintenance. Traditional underwater image enhancement methods primarily rely on passive restoration, typically employing pixel-domain processing [5, 6, 39, 40, 65] or physical modeling [31, 38, 50, 74, 75, 195] to optimize images. However, these methods rely on manually designed rules and exhibit weak generalization capabilities in complex underwater scenes. Deep learning approaches for underwater image enhancement adopt data-driven strategy, leveraging neural network architectures to learn end-to-end mapping relationships between degraded and clear images. They significantly outperform traditional methods in color correction and detail restoration. Diffusion model approaches for underwater image enhancement [135, 200] leverage progressive noise injection and reverse denoising mechanisms to effectively balance global tonal restoration with local texture preservation, emerging as one of the leading techniques in recent years. The UIEB [77] dataset serves as the benchmark for underwater image enhancement. This section utilizes its released 89-image challenge test set, which contains degraded real images and corresponding high-quality reference images to evaluate models adaptability to complex scenarios. The LSUI [117] dataset is large-scale real underwater image dataset. This section adopts the test set partitioning from WF-Diff [200], utilizing 400 images, covering diverse water bodies and target categories, making it suitable for evaluating model generalization. The U45 [78] dataset contains 45 reference-free real underwater images categorized into green cast, blue cast, and haze degradation types, simulating practical application scenarios requiring evaluation without reference images. Performance evaluation is based on the above three datasets. In this subsection, we systematically assess the underwater image enhancement capabilities of the Nano Banana Pro model, focusing on its effectiveness in eliminating blue/green color casts and restoring blurred textures while preserving scene semantic integritysuch as biological morphology and artifact structureand maintaining luminance consistency, including natural transitions between light and dark areas."
        },
        {
            "title": "12.2 Quantitative Results",
            "content": "We conducted experiments using the Nano Banana Pro model configured to output 1K resolution across three datasets, quantitatively evaluating underwater image enhancement performance through reference-free metrics. The Underwater Image Quality Measure (UIQM) [116] comprehensively quantifies underwater image quality by integrating color richness, sharpness, and contrast. Underwater Color Image Quality Evaluation (UCIQE) [175] 36 Table 13 Quantitative comparisons on UIEB, LUSI and U45 datasets. The best results are highlighted by black bold. Method UIEB LUSI U45 IQM CIQE IQM CIQE IQM CIQE UWCNN [77] UIEC-Net [151] U-Shape [117] PUGAN [22] DM-Water [135] WF-Diff [200] 3.8325 3.327 3.332 3.2163 3.8925 3.7388 0.5552 0.609 0. 0.6176 0.5994 0.5867 4.1699 3.9833 4.0334 4.0417 4.0595 4.0308 0.5453 0.5888 0.574 0.5834 0.5883 0.5688 4.387 4.4293 4.3524 4.3377 4.1986 4.2193 0.5622 0.6104 0. 0.6117 0.586 0.5813 NB Pro 3.634 0.5899 4.2993 0. 4.3907 0.5899 addresses non-uniform color shifts and low contrast in underwater scenes by evaluating quality across standard deviation, luminance contrast, and saturation mean dimensions. Both metrics adapt to real-world unreferenced scenarios without requiring reference images. Results were compared against UWCNN [77], UIEC-Net [151], U-Shape [117], PUGAN [22], DM-water [135], and WF-Diff [200]. Relevant details are summarized in Tab. 13. NB Pro demonstrates competitiveness on underwater image reference-free evaluation metrics UIQM and UCIQE that rivals existing mainstream UIE methods. On the UIEB dataset, NB Pros gap with optimal results in reference-free metrics is relatively small. On the larger-scale LSUI dataset with more complex degradation scenarios, NB Pro achieves top performance in both UIQM and UCIQE metrics, fully validating its robustness in challenging environments. On the reference-free dataset U45, NB Pro achieves the best UIQM score and ranks third in UCIQE, demonstrating its practical value in real-world reference-free scenarios. Figure 26 Visualization examples for underwater image enhancement using NB Pro on the UIEB and LSUI datasets."
        },
        {
            "title": "12.3 Qualitative Results",
            "content": "To intuitively present the underwater image enhancement outcomes of the Nano Banana (NB) Pro model, we provide visualizations of its processing results across the UIEB, LSUI, and U45 datasets, with comparisons to state-of-the-art baseline methods. Fig. 26 displays the visualization of exemplary cases for NB Pro in underwater image enhancement tasks on the UIEB and LSUI datasets. Fig. 27 presents the visualization of failed cases for NB Pro in underwater image enhancement tasks on the UIEB and LSUI datasets. Fig. 28 shows the visual comparison of processing results between NB Pro and other mainstream underwater image enhancement methods on the reference-free U45 dataset. Qualitative experimental results demonstrate that for extreme degradation scenarios such as severe green color bias, severe blue color bias, insufficient illumination, and high turbidity, NB Pro can generate high-quality enhanced results without relying on GT images from paired training data. Even in scenarios with multiple severe degradations overlapping, its output images exhibit superior visual quality compared to GT images (as shown in Fig. 26). However, in mildly degraded scenarios with weaker color shifts, NB Pro struggles to effectively identify degradation features. The generated enhanced images exhibit minimal differences from the input images, resulting in visual quality inferior to GT images and failing to fully meet the core objective of underwater image enhancement (as shown in Fig. 27). This conclusion is further validated by visualization results from the reference-free dataset U45  (Fig. 28)  : In the first row depicting diver scene with high 37 turbidity and severe green color cast degradation, NB Pros enhancement significantly outperforms other comparison methods, completely eliminating the color cast while removing foggy blur; In the second row of underwater scenes with blue color cast, NB Pro also performed comparably to existing mainstream methods; however, in the third row of slightly degraded underwater plant scenes, NB Pros restoration results were relatively poor among all comparison methods, failing to effectively optimize image details and contrast. Figure 27 Visualization of failed cases in underwater image enhancement for NB Pro on the UIEB and LSUI datasets. Figure 28 Visual comparison of processing results between NB Pro and other underwater image enhancement methods on the U45 dataset without reference."
        },
        {
            "title": "12.4 Analyses",
            "content": "This study conducted comprehensive evaluation across multiple benchmark datasets representing both synthetic and real underwater environments. Results demonstrate that NB Pro offers unique paradigm for underwater image enhancement. Its core characteristic is distinct trade-off between robust perceptual recovery in complex environments and precise pixel-level fidelity in benign conditions. Quantitative analysis demonstrates that NB Pro excels in the absence of reference images, achieving state-ofthe-art results on large-scale complex datasets like LSUI and real-world datasets such as U45. This indicates 38 that when confronted with severely degraded images, the model efficiently generates images with perceptual clarity, rich color saturation, and optimal contrast. Existing underwater ground truth images often exhibit residual degradation or artifacts. NB Pros generative freedom enables it to surpass the visual quality of these reference images, consistent with its performance in the no-reference evaluation. From qualitative perspective, NB Pro demonstrates unique strengths in handling extremely degraded scenarios. Visual examples prove that NB Pro can successfully reconstruct scenes severely affected by green/blue color casts, low light, and high turbidity. It can simultaneously synthesize clear details and correct severe color shifts, sometimes producing results that visually outperform the ground truth images. This indicates NB Pro possesses strong understanding of underwater degradation and the potential to reverse such degradation. Despite these strengths in high-intensity restoration tasks, the model exhibits instability in mildly degraded scenarios. For scenes with only slight color shifts or minor turbidity, its enhancement effects are suboptimal, reflecting NB Pros sensitivity limitations. When degradation signals are weak, the model struggles to identify or localize degraded features, failing to trigger necessary optimizations for fine details and contrast. Future research should focus on enhancing NB Pros adaptability across the full spectrum of degradation levels. This will ensure it achieves both refined enhancement in mildly blurred environments and thorough reconstruction in severely degraded scenarios, delivering outstanding results in both cases."
        },
        {
            "title": "13.1 Background",
            "content": "High Dynamic Range reconstruction technology aims to address the core limitations of traditional imaging systems. Conventional imaging systems fail to capture the complete spectrum of light intensity information in real-world scenes leading to irreversible loss of highlight details namely overexposed regions and shadow information namely underexposed regions in low dynamic range images. This inherent limitation not only degrades visual quality but also severely impairs the performance of downstream computer vision tasks such as misaligned object detection blurred scene parsing and inaccurate depth estimation posing substantial risks to safety-critical applications including intelligent surveillance environmental perception for autonomous driving and aerial scene analysis. HDR reconstruction methods have evolved steadily forming sophisticated technical framework anchored in large-scale datasets and driven by deep learning. Notably the MIT FiveK[10] dataset serves as classic benchmark for tone mapping and image enhancement offering large-scale professional-grade HDR-LDR image pairs. It includes 5,000 image pairs 4,500 for model training augmented via random cropping flipping rotation and other techniques and 500 for validation and testing. The dataset covers diverse real-world scenarios from indoor and outdoor environments to low-light and high-contrast scenes and supplies professional-grade HDR reference images with exposure parameters and semantic annotations. This enables the reproduction of real-world dynamic range variations and meets evaluation requirements for algorithms focusing on global tone adjustment and local detail preservation. The HDR+[139] dataset is large-scale benchmark for real-world HDR imaging focusing on the actual shooting conditions of consumer cameras. It contains over 100,000 RAW image sequences captured by consumer cameras each paired with aligned HDR ground truth images. Its advantages include capturing natural noise patterns sensor characteristics and complex lighting variations such as backlighting and high contrast simulating real-world tone mapping demands while retaining resolution suitable for practical use. Performance evaluation leverages two benchmark datasets. The FiveK dataset includes 500 aligned LDR and HDR image pairs and the HDR+ benchmark emphasizing real-world applicability includes 250 test image pairs. Evaluations use quantitative metrics including PSNR SSIM and LPIPS complemented by subjective assessments of naturalness and detail fidelity to comprehensively measure model performance. In this section we systematically evaluate the Nano Banana Pro models HDR reconstruction performance on 480p images using the aforementioned benchmarks. The assessment prioritizes verifying the models capacity to recover highlight and shadow details across diverse scenes and spatial scales while ensuring semantic consistency and photometric integrity. These results offer valuable reference data for the research community."
        },
        {
            "title": "13.2 Quantitative Results",
            "content": "Table 14 Quantitative comparison on HDR+ and MIT-FiveK datasets. The best results are highlighted by black bold. Method UPE[145] HDRNet[41] CSRNet[49] DeepLPF[112] LUT[183] sLUT[146] CLUT[186] LLF-LUT++[185] NB Pro HDR+ (480p) MIT-FiveK (480p) PSNR SSIM LPIPS PSNR SSIM LPIPS 23.33 24.15 23.72 25.73 23.29 26.13 26.05 0.852 0.845 0.862 0.904 0.855 0.901 0.892 0.150 0.110 0.104 0.073 0.117 0.069 0. 7.68 7.15 6.67 6.05 7.16 5.34 5.57 21.82 23.31 25.31 24.97 25.10 24.67 24.94 0.839 0.881 0.909 0.897 0.902 0.896 0.898 28.43 14.24 0. 0.467 0.056 0.221 4.54 19.82 26. 19.20 0.912 0.639 0.136 0.075 0.052 0.061 0.059 0.059 0.058 0.054 0. 9.16 7.73 6.17 6.22 6.10 6.39 6.71 4.93 11.14 To comprehensively evaluate Nano Banana Pros performance in HDR tasks, we quantitatively compared it against range of advanced traditional and deep learning-based image enhancement methods. To ensure fair comparison, all images were downsampled to 480p resolution for evaluation. We employed four standard metrics: PSNR and SSIM to evaluate perceptual similarity, LPIPS to assess visual similarity, and to quantify color differences. Results are shown in Tab. 14. NB Pro significantly underperformed against the comparison methods. On the HDR+ dataset, NB Pro achieved lower PSNR and SSIM than the optimal method, while also exhibiting poorer LPIPS and values. On the MIT-FiveK dataset, although its PSNR and SSIM improved, they still lagged significantly behind the optimal method. This result clearly indicates that under the standard full-reference evaluation framework, which prioritizes pixel-level accurate reconstruction and color fidelity, NB Pros generated results exhibit systematic deviations from professionally enhanced or color-graded reference images. NB Pro fundamentally differs from traditional HDR/enhancement models optimized for specific imaging scenarios. The latter typically undergo end-to-end training directly on paired LDR-reference images, targeting minimization of pixel-level loss, thus inherently excelling in metrics like PSNR and SSIM. In contrast, NB Pros generation process prioritizes semantic coherence and overall visual appeal. Its outputs can be viewed as reconstructions of the input image rather than strict pixel-to-pixel mappings. Consequently, generated images may exhibit deviations in luminance distribution, local contrast, and even color style compared to reference images, leading to comprehensive score reductions across full-reference metrics. Notably, on the LPIPS metric, NB Pros performance on the MIT-FiveK dataset remains behind but shows narrowed gap compared to pixel-level metrics. This suggests its outputs may retain some similarity to reference images at higher-level semantic features, while low-level pixel arrangements have been significantly altered."
        },
        {
            "title": "13.3 Qualitative Results",
            "content": "To visually evaluate the performance and potential limitations of NB Pro in HDR imaging tasks, this study conducted qualitative visualization experiments using representative scenes from the HDR+ and MIT-FiveK datasets. The experiments cover three core scenarios: conventional lighting, low-light with minimal detail, and complex dense textures. The corresponding results are presented in Fig. 29, Fig. 30, and Fig. 31, respectively. Fig. 29 presents an illustrative case of NB Pro in HDR imaging tasks. Comparing the low-dynamic-range input image, the true high-dynamic-range reference image, and the NB Pro generated result reveals that in conventionally lit scenes, such as moderately bright indoor settings or outdoor natural light environments, NB Pro effectively restores the scenes dynamic range and color gradation. The overall visual quality of the generated result approaches or even rivals the reference image, fully validating the models effectiveness in fundamental HDR imaging tasks. Fig. 30 further highlights texture anomalies in low-light, low-detail scenarios. When input low-dynamic-range images contain severe shadow areas with inherently weak texture or detail information, NB Pro tends to exhibit two typical defects. One type involves detail loss, such as in the third case where the models output lacks the wall tile texture and sofa outline in the shadow regions of the input image. The second involves 40 Figure 29 Visualization of satisfactory exemplary cases for the HDR imaging task using NB Pro. Figure 30 Visualization of texture detail loss and artificial addition cases in HDR Imaging using NB Pro. artificially redundant additions, such as the extra cookie surface texture generated in the first case that did not exist in the original scene. The second case failed to restore the vegetables original colors and applied redundant diffuse rendering to the green spots. The fourth case generated mountain elements out of thin air in the house background. These issues stem from insufficient detail features in low-light scenes, causing the models texture perception and generation logic to deviate, ultimately reducing detail restoration accuracy. Fig. 31 shows evaluation results for complex, densely textured scenes. When input images contain fine, dense texturessuch as fabric patterns, intricate vegetation textures, or architectural wall texturesNB Pro struggles to precisely balance texture preservation and enhancement scales. Generated results commonly exhibit visual artifacts from excessive sharpening, overly pronounced texture edges, localized artifacts, and even masking of the original textures natural gradations. This phenomenon reflects the models ongoing limitations in perceiving fine textures and controlling enhancement. In summary, qualitative experiments indicate that NB Pro delivers satisfactory visual effects in standard HDR imaging scenarios. However, in low-light, low-detail environments, the model tends to suffer from texture loss or redundant additions. In complex, densely textured scenes, the model exhibits tendency toward excessive sharpening. 41 Figure 31 Visualization of oversharpening cases in the HDR imaging task using NB Pro."
        },
        {
            "title": "13.4 Analyses",
            "content": "The quantitative and qualitative evaluation results systematically reveal the comprehensive performance characteristics of NB Pro in HDR imaging tasks. Quantitatively, the model significantly lags behind mainstream HDR reconstruction methods in all reference evaluation metrics during 480p resolution tests on the HDR+ and MIT-FiveK datasets. However, the gap narrows for the LPIPS metric on the MIT-FiveK dataset, indicating that while its generated results exhibit significant pixel-level deviations, they maintain degree of semantic consistency with reference images. Qualitatively, NB Pro achieves acceptable visual results in standard-illumination HDR scenes. However, it exhibits pronounced limitations in two typical scenarios: low-light environments with sparse details and complex, densely textured scenes. Specific issues include texture information loss, artificial redundant detail generation, color distortion, and over-sharpening artifacts. The evaluation results clearly reveal three core deficiencies of NB Pro when applied to HDR imaging tasks: First, in low-light, low-detail scenes, the model exhibits weaknesses in perceiving and accurately restoring subtle texture features in shadow areas. The weak feature signals in these input shadows struggle to support precise reconstruction, leading to either the omission of shadow texture information or the generation of artificial redundant detail fill. Second, for fine-grained texture scenarios like fabric weaves and dense vegetation patterns, the model lacks adaptive control over texture preservation and enhancement scales. Over-sharpening not only disrupts natural texture gradation but also readily induces edge artifacts. Third, the models color rendering mechanism lacks strong constraints on reference color distributions. In complex color scenes, it prioritizes visual harmony over precise target color reproduction, ultimately causing color distortion. Comprehensive experimental results indicate that NB Pro is only suitable for non-critical HDR imaging scenarios where pixel-level precision is less demanding and visual experience is paramount. It is unsuitable for safety-critical or professional-grade applications requiring stringent detail and color restoration accuracy. Issues such as texture loss, artificial redundant details, and color distortion may cause scene analysis bias or decision misjudgment, failing to meet the core requirements of such scenarios."
        },
        {
            "title": "14.1 Introduction",
            "content": "In the computer vision and digital photography, acquiring fully clear images is crucial for subsequent analysis and processing. However, due to the limited depth of field (DoF) of camera lenses, single shot cannot concurrently focus on all the objects at varying depths. Multi-focus image fusion (MFIF) provides an effective solution to this challenge, aiming to generate an AIF output from multiple images of the same scene with different focal points. This technique has found significant applications in various fields such as medical diagnosis and consumer electronics. In recent years, deep learning has been widely applied to MFIF, typically falling into two categories: decisionbased and reconstruction-based approaches. Decision-based methods learn decision map by classifying pixels in the source images as either in-focus or out-of-focus, and then select the appropriate pixel to assemble the final composite [84, 98, 164]. Reconstruction-based methods employ end-to-end networks to directly extract features from source images and reconstruct the all-in-focus output [85, 109, 198]. However, the former methods normally struggle with focused-defocused boundaries whose focus attributes are ambiguous to distinguish, while the letter ones often suffer from detail loss in other regions away from the boundaries. More recently, the rapid advancement of Generative Artificial Intelligence has opened new avenues for MFIF. Generative models learn the distribution from massive datasets, enabling them to understand and generate complex visual content and structures. Despite the potential for creative fusion, their application in pixelprecise tasks like MFIF remains under-explored. There is critical need to verify whether model designed for creativity can adhere to the strict fidelity requirements of fusion tasks without introducing hallucinations or losing spectral information. This report evaluates the performance of the newest generative model Nano Banana Pro in the task of MFIF, comparing its fusion quality with existing baseline methods using various metrics. The results aim to provide insights into its strengths and limitations in real-world applications."
        },
        {
            "title": "14.2 Quantitative Results",
            "content": "Table 15 Quantitative comparison on the Lytro and MFFW datasets. The best results are highlighted by black bold. Method IFCNN [198] SESF [104] GACN [105] GRFusion [80] ZMFF[60] MUFusion [21] DB-MFIF [189] MFFT [184] DMANet [124] MCCSR [207] Lytro MFFW I EN AG SF 0.9388 1.6808 1.1723 1.1879 0.8795 0.8088 1.0573 1.1533 1.1897 1.1920 7.5318 7.5322 7.5311 7.5329 7. 7.6093 7.5386 7.5620 7.5319 7.5329 8.1473 8.1530 8.1245 8.1823 7.7771 8.1578 8.2415 8.6089 8.2059 8.1757 19.3992 19.4251 19.3247 19.4539 18.8184 18.9240 19.5290 21.3759 19.5129 19.4392 QY 0.9518 0.9879 0.9878 0.9863 0.9321 0.8997 0.9637 0.9523 0.9853 QCB 0.7294 0.8064 0.8062 0.8071 0.6731 0.6819 0.7770 0.7511 0.8054 EN AG SF 0.8206 1.0876 1.0820 1.1426 0.7711 0.7551 0.8699 1.1310 1.1513 7.1688 7.1850 7.1923 7.1711 7.1546 7.2026 7.1935 7.2281 7.1846 7.1688 9.6849 9.8678 9.7328 9.8826 9.2144 8.9247 10.1346 10.1971 10.0948 9.7407 22.0173 22.9291 22.2842 22.6520 21.3405 19.9974 23. 24.5709 23.2370 22.2989 QY 0.8715 0.9588 0.9273 0.9334 0.8474 0.8167 0.8663 0.9336 0.9506 QCB 0.6423 0.7418 0.7192 0.7223 0.6722 0.6205 0.6647 0.6865 0.7276 0. 0.7517 0.9890 0.8084 1.1800 NB Pro 0.7476 7. 8.2977 20.1044 0.7755 0.6603 0.6319 7. 10.2879 23.0223 0.5537 0.5638 We evaluate the performance of MFIF on four benchmark: Lytro [115], MFFW [170], MFI-WHU [188] and SIMIF [137]. The Lytro dataset contains 20 pairs of multi-focus images captured by light field camera. The MFFW dataset includes 13 real image pairs with strong Defocus Spread Effect (DSE). The MFI-WHU dataset is constructed using Gaussian blur and decision maps, consists of larger scale with 120 pairs. The SIMIF dataset is composed of 12 pairs of high-resolution images. Six popular objective metrics are employed for evaluation, including non-reference metrics, EN [66], AG [23], SF [208], and Source-reference metrics [53], QY [173], QCB [18]. These datasets and metrics provide comprehensive assessment from multiple perspectives. 43 Table 16 Quantitative comparison on the MFI-WHU and SIMIF datasets. The best results are in black bold. Method IFCNN [198] SESF [104] GACN [105] ZMFF[60] GRFusion [80] MUFusion [21] DB-MFIF [189] MFFT [184] DMANet [124] MCCSR [207] MFI-WHU SIMIF EN AG SF 0.8993 1.1878 1.2084 0.7028 1.2134 0.7449 1.0693 1.1974 1.2220 1.2246 7.3285 7.3183 7.3127 7.2763 7.3216 7.3744 7.3250 7.3358 7.3158 7. 11.3564 11.5399 11.4633 10.6727 11.6609 9.6015 11.6956 11.7636 11.6003 11.4162 26.1798 26.5894 26.5089 24.7632 26.8362 21.4447 26.8336 27.5736 26.8034 26.4025 QY 0.9404 0.9855 0.9889 0.8387 0.9848 0.8608 0.9466 0.9614 0. 0.9891 QCB 0.7367 0.8166 0.8241 0.6512 0.8212 0.6480 0.7818 0.7648 0.8222 0.8227 EN AG SF 1.0505 1.2995 1.3068 0.8302 1.2898 0.8876 1.1510 1.2751 1.3130 1.3151 7.3897 7.3868 7.3802 7.3942 7.3917 7.3566 7.4073 7.3866 7.3834 7.3782 6.9289 6.9931 6.9750 7.0007 7.0215 6.0633 7.1849 7.1754 7.0634 6. 19.1344 19.4672 19.4241 19.2939 19.4954 16.3885 19.7543 20.5918 19.5735 19.3575 QY 0.9211 0.9807 QCB 0.7364 0. 0.9845 0.8328 0.6259 0.8325 0.8267 0.9771 0.6321 0.8387 0.7612 0.9157 0.7724 0.9492 0.8270 0.9780 0.8298 0.9835 NB Pro 0.5923 7.2986 10.5142 24.1274 0.5610 0. 0.8042 7.4925 7.3561 19.5545 0.5112 0. We compare the Nano Banana Pro (NB Pro) with 10 other state-of-the-art and representative MFIF methods, where ZMFF is Zero-shot method, IFCNN and MUFusion are unsupervised methods, and the rest are supervised methods. According to the comparison results shown in the Tab. 32 and Tab. 33, NB Pro performs exceptionally well on non-reference metrics, achieving results that are close to or even surpassing the current state-of-the-art, indicating the high quality of the generated images themselves. Conversely, on source-reference metrics, NB Pro shows poorer performance, meeting the similar dilemma faced by previous Zero-shot and unsupervised methods. This indicates that during the fusion process, the model failed to adequately preserve consistency between the generated image and the source images in terms of aspects like gradients and structure; it exhibits excessive creativity at the expense of fidelity. Figure 32 Visualization of the fusion images generated by the Nano Banana Pro. The rows from top to bottom correspond to samples from the Lytro, MFFW, MFI-WHI and SIMIF datasets. 44 Figure 33 Visualization of the fusion images of lock sample from the Lytro dataset. Two enlarged views are shown to reveal critical details."
        },
        {
            "title": "14.3 Qualitative Results",
            "content": "Fig. 32 illustrates visualization of the fusion results generated by the Nano Banana Pro. The fusion performance varies across different samples, with some being successful and others subpar. For instance, the first example in the first row showcases favorable fusion result, where the fused image not only preserves the focused foreground person and the background golf course from the source images but also achieves seamless transition between different regions, effectively avoiding artifacts. Notably, it even recovers richer details in the lawn areawhich originally had limited claritythereby enhancing the overall visual quality. Conversely, limitations are observed in some other cases. Specifically, in the second sample of the second row, the white petals at the base of the foreground plant remain blurred. Similarly, in the second sample of the third row, the grass in the bottom-right corner is still defocused. Given that sharp counterparts for these regions exist in the source images, this indicates that the model failed to correctly identify or localize the optimal in-focus regions during the fusion process. To more intuitively verify the capability of NB Pro, Fig. 33 and Fig. 34 compares the fusion results against state-of-the-art MFIF approaches. In the lock example, NB Pro achieves remarkably smooth transitions across regions without introducing artifacts near the lock head, while producing even sharper details on the house wall that was already in focus in the source images. In the coffee cup example, due to the spread effect caused by defocus, some methods generate artifacts at the boundary between the two cup rims, and others produce dark ghosting along the cup wall. In contrast, NB Pro effectively overcomes these issues and delivers visually pleasing fusion result."
        },
        {
            "title": "14.4 Analyses",
            "content": "This comprehensive evaluation, conducted across four diverse benchmarks and benchmarked against ten state-of-the-art methods, establishes NB Pro as paradigm shift in Multi-Focus Image Fusion. The results characterize distinct trade-off between perceptual quality and signal fidelity. Quantitative analysis reveals significant performance divergence. NB Pro excels in non-reference metrics, generating images with exceptional clarity, textural richness, and visual appeal. Conversely, its performance 45 Figure 34 Visualization of the fusion images of the coffee cup sample from the MFFW dataset. Two enlarged views are shown to reveal critical details. on source-reference metrics uncovers critical limitation inherent to zero-shot generative approaches: the prioritization of generative freedom over strict pixel-level adherence to source inputs. While the model can hallucinate plausible high-frequency details (e.g., enhancing lawn textures), it occasionally alters gradients or structures that require preservation. This quantitative discrepancy stems from the fundamental conflict between the strict consistency required by traditional fusion tasks and the stochastic nature of generative models. First, despite prompt-based constraints, the models high degree of freedom can lead to semantic alterations in regions that should be preserved. Second, source images are rarely perfect; NB Pro often performs generative enhancement (e.g., super-resolution or denoising) to supplement details. However, traditional reference-based metrics penalize these visual improvements as errors because they deviate from the imperfect source. Qualitatively, NB Pro demonstrates superior capability in handling complex scenarios, particularly those affected by the Defocus Spread Effect. By effectively mitigating boundary artifacts, dark ghosting, and unnatural transitions common in traditional algorithms, the model showcases superior semantic understanding of scene structure. Despite these strengths, instability in focus detection remains challenge; the model occasionally blurs clear regions, suggesting failures in the attention mechanisms pixel localization. Ultimately, the misalignment between visual superiority and metric penalties suggests that current evaluation frameworks are insufficient for Generative AI. Future work must focus on better constraining the generative process and developing novel metrics capable of distinguishing between hallucinatory errors and generative enhancements."
        },
        {
            "title": "15.1 Introduction",
            "content": "In the field of modern computer vision, multi-modal image fusion technology plays an increasingly critical role. Infrared-Visible Image Fusion (IVIF) aims to synergize the thermal radiation information from infrared images with the texture details and color information from visible images. Infrared sensors capture thermal signatures of objects and are robust against varying lighting conditions and adverse weather (such as smoke or darkness), effectively highlighting targets. Conversely, visible light sensors provide rich high-frequency details and scene descriptions that align with human visual perception. By fusing these two complementary modalities, the resulting images not only possess all-weather scene perception capabilities but also significantly enhance the accuracy and robustness of target detection, autonomous driving navigation, and security surveillance systems. Traditional IVIF methods (such as multi-scale transform [97] and sparse representation [79]) often struggle to balance the saliency of thermal targets with the fidelity of background textures, frequently resulting in artificial artifacts. In recent years, deep learning-based methods (such as CNNs [107, 133, 203], GANs [82, 106, 108]) and transformers [83, 140, 156] have achieved performance breakthroughs but still face challenges in cross-modal feature alignment and detail preservation. With the explosion of generative AI technologies, particularly Diffusion Models, image generation quality has reached unprecedented heights. However, existing high-compute models are often bulky and difficult to run in real-time on edge devices. Furthermore, balancing generative quality with physical fidelity in fusion tasks under specific physical constraints remains pressing problem. Against this backdrop, Googles newly released Nano Banana Pro has garnered widespread attention in the industry. Nano Banana Pro employs optimized Latent Diffusion technology and efficient attention mechanisms, specifically designed to handle high dynamic range and multi-modal inputs. Its uniqueness lies in its ability to understand semantic context more precisely, suggesting that in image fusion tasks, it may preserve the edge information of infrared thermal sources more effectively than its predecessors while naturally integrating visible textures. Although Nano Banana Pro has demonstrated impressive performance in general image generation, its effectiveness in the specific scientific task of Infrared-Visible Image Fusion has not yet been systematically verified. This report aims to bridge this gap by comprehensively evaluating the actual performance of Nano Banana Pro in IVIF tasks through qualitative analysis (visual effects) and quantitative assessment. We will focus on examining its fusion quality, noise control capabilities, and inference efficiency across various lighting scenarios to assess its potential as foundation model for next-generation image fusion."
        },
        {
            "title": "15.2 Quantitative Results",
            "content": "Table 17 Quantitative comparison on the MSRS, RoadScene, and M3FD datasets. The best results are in black bold. Method SDN [187] TarD [94] DeF [91] Meta [202] CDDF [204] LRR [81] MURF [168] DDFM [205] SegM [95] EMMA [206] EN 5.25 5.28 6.46 5.65 6.70 6.19 5.04 6.19 5.95 6.71 MSRS RoadScene M3FD SD SF AG SCD IF EN SD SF AG SCD IF EN SD SF AG SCD IF 17.35 25.22 37.63 24.97 43.38 31.78 16.37 29.26 37.28 44.13 8.67 5.98 8.60 9.99 11.56 8.46 8.31 7.44 11.10 11.56 2.67 1.83 2.80 3.40 3.73 2.63 2.67 2.51 3.47 3.76 0.99 0.71 1.35 1.14 1.62 0.79 0.86 1.45 1.57 1.63 1. 0.50 0.42 0.77 0.31 1.05 0.54 0.40 0.73 0.88 0.97 0.58 7.30 7.26 7.36 6.88 7.52 7.12 6.91 7.24 7.29 7. 7.39 44.06 47.44 47.03 31.97 54.42 39.16 33.34 42.43 46.14 54.81 14.58 11.11 10.99 14.38 14.97 11.41 13.88 10.68 14.47 15.21 5.80 4.14 4.38 5.57 5.81 4.37 5.37 4.15 5.57 5.83 1.37 1.40 1.62 0.92 1.65 1.46 1.04 1.64 1.61 0.61 0.56 0.63 0. 0.66 0.45 0.52 0.62 0.65 6.87 6.80 6.90 6.73 7.04 6.58 6.59 6.82 6.88 36.22 41.77 36.81 30.56 42.02 30.28 28.89 32.68 36.20 15.32 8.65 9.85 16.48 16.56 11.83 11.82 10.07 16.19 5.61 3.17 3.65 6.02 5.84 4.21 4.81 3.71 5.83 1. 0.66 7.12 44.01 16.92 6.23 56. 21.81 7.07 0.83 0.51 6.98 43. 15.68 5.06 1.41 1.35 1.42 1.31 1.41 1.34 1.21 1.35 1.38 1.48 0.75 0.55 0.51 0.58 0.65 0.65 0.54 0.39 0. 0.75 0.66 0.38 NB Pro 6.85 44.95 14. 4.56 Following [206], we conduct experiments on three mainstream benchmarks: MSRS [134], RoadScene [167] and M3FD [94] datasets, and leverage six popular metrics for assignment, including non-reference metrics EN , SD, SF , AG and source-reference metrics SCD, IF . We compare the fusion results of Nano Banana Pro (NB Pro) with 10 other state-of-the-art and representative IVIF methods, where SDNet and DeFusion 47 Figure 35 Visualization examples of image fusion results by Nano Banana Pro in the MSRS dataset. are CNN-based methods, TarDAL is GAN-based method, CDDFuse is CNN-transfomer hybrid method, DDFM is diffusion-based and training-free method, and the remaining ones are model or task driven approaches. As shown in Tab. 17, the results demonstrate that NB Pro exhibits overwhelming superiority in non-reference metrics representing image information content and texture details, particularly on the MSRS and RoadScene datasets. On the MSRS dataset, NB Pro secures the top rank in all four of these metrics. Notably, its EN reaches 6.85 and AG hits 4.56, significantly surpassing the runner-up. On the RoadScene dataset, this advantage is even more pronounced. NB Pro achieves an SF score of 21.81, outperforming the nearest competitor (EMMA, 15.21) by nearly 43%. This indicates that the fused images generated by NB Pro possess extremely high clarity and contrast. It is capable of mining and reconstructing rich high-frequency edge information from source images, attributing to the acute capability of its powerful generative architecture in capturing latent features. However, we also observe an intriguing phenomenon: while NB Pro leads by wide margin in detail metrics, it scores relatively lower in source-reference metrics, specifically SCD and IF . For instance, on the MSRS dataset, its IF is only 0.58, and and it drops to 0.38 on M3FD. This reflects the inherent characteristic of generative models: while the model dramatically enhances texture and human-perceived sharpness, this reconstruction process may introduce stylized features or pixel-level deviations not present in the source images, leading to reduced correlation with the original infrared/visible inputs. In contrast, traditional methods, while not as sharp as NB Pro, demonstrate more robustness in maintaining original pixel fidelity. NB Pros performance varies across different scenarios. It performs best on MSRS (often containing night-time and complex lighting scenes) and RoadScene, suggesting its proficiency in handling high dynamic range scenes requiring edge enhancement. On the M3FD dataset, although it maintains the second-best score in SD(43.44), its overall dominance is less distinct compared to the other two datasets. This implies there may still be room for parameter fine-tuning in specific types of multi-modal target detection scenarios."
        },
        {
            "title": "15.3 Qualitative Results",
            "content": "While NB Pro yields impressive fusion results in most scenarios, certain limitations persist. To intuitively demonstrate the perceptual quality of the fused images, Fig. 35 presents qualitative results on the MSRS dataset. As illustrated in the first row, the method effectively handles extreme lighting conditions. It accurately captures pedestrian targets hidden in low-light regions of the visible spectrum, establishing sharp contours for thermal targets while enhancing overall background illumination. Simultaneously, it restores clear details and textures in over-exposed areas, such as those adjacent to vehicle headlights. However, suboptimal cases are observed in the second row. Although the thermal targets remain highlighted, the first sample exhibits excessive sharpening of the building structure, leading to slight over-exposure. In the second example, distinct halo effect emerges around the pedestrian, manifesting as unnatural bright fringes surrounding the thermal target. Fig. 36 further visualize the fusion performance on the other two datasets. In general, NB Pro demonstrates 48 Figure 36 Visualization examples of image fusion results by Nano Banana Pro in the RoadScene and M3FD datasets. superior performance in terms of overall visual quality and realism, excelling in infrared target recovery and background texture reconstruction. Nevertheless, deficiencies persist in certain fine-grained details. The model may occasionally fail to faithfully utilize and preserve the source information, leading to the introduction of unnatural hallucinations or artifacts."
        },
        {
            "title": "15.4 Analyses",
            "content": "This report presents comprehensive evaluation of Googles Nano Banana Pro in Infrared-Visible Image Fusion. The experimental outcomes reveal pronounced performance dichotomy: On one hand, leveraging powerful generative priors, NB Pro demonstrates overwhelming superiority in non-reference metrics. It successfully circumvents the bottlenecks of traditional methods regarding night-time enhancement and texture reconstruction, yielding fused images of exceptional contrast and clarity. On the other hand, this aggressive generation strategy incurs fidelity cost. Lower scores in source consistency metrics, combined with qualitative artifacts such as excessive sharpening and halo effects, indicate that the model sacrifices pixel-level fidelity to the original physical signals in exchange for perceptual appeal. The performance of NB Pro catalyzes re-evaluation of the current IVIF landscape: Traditional methods fundamentally operate as signal processing routines aiming to preserve pixel intensity. However, NB Pro introduces paradigm of semantic generation. Rather than merely superimposing pixels, it interprets the scene context to re-synthesize the image. This explains its capability to recover astonishing details alongside its propensity for hallucinations. Future research must focus on integrating physical constraints, enforcing strict adherence to thermal distribution laws while exploiting generative capabilities. While visually striking, NB Pros outputs raise concerns for safety-critical applications like autonomous driving. Perceptual pleasantness does not equate to operational reliability. Artifacts or over-sharpening can trigger false positives in detection algorithms or obscure small targets. Consequently, evaluation standards must evolve beyond visual quality to include Machine Perception Metrics, directly validating the utility of fused images on downstream tasks. Our findings highlight the insufficiency of the current evaluation framework. Metrics like SCD and VIF, which penalize pixel-level misalignment, are overly rigid for generative models. As Generative AI becomes prevalent, there is an imperative need for novel No-Reference Image Quality Assessment metrics that prioritize semantic consistency and naturalness over strict pixel-wise alignment."
        },
        {
            "title": "16 Discussion",
            "content": "This comprehensive empirical study, through systematic zero-shot evaluation across 14 diverse low-level vision tasks, elucidates the dual nature of Nano Banana Pro as generalist generative model: it excels in perceptual quality but lags significantly in traditional pixel-fidelity metrics. This core finding not only quantifies the current capabilities of large-scale generative models within the low-level vision domain but also prompts profound reflections on task definitions, evaluation paradigms, and future model evolution."
        },
        {
            "title": "16.1 Generative vs. Regression Paradigms",
            "content": "Our work highlights intrinsic difference between generative and traditional low-level vision models. Traditional methods predominantly follow regression paradigm. They learn deterministic mapping from degraded inputs to clean references via pixel-level supervision, aligning their optimization objective directly with metrics like PSNR and SSIM. In contrast, Nano Banana Pro embodies generative paradigm. Its training core involves learning the joint distribution of large-scale image data and performing conditional synthesis based on semantic priors. Its goal is to produce plausible and visually pleasing images, not to achieve pixel-wise alignment with specific reference. Consequently, in regions with severe information loss, traditional methods, constrained by input information, often yield blurry or insipid results. The generative model, however, can leverage its robust world knowledge to hallucinate plausible details, leading to subjectively superior outputs that constitute deviation from the canonical ground truth."
        },
        {
            "title": "16.2 The Potential Misguidance of Traditional Metrics",
            "content": "Our results strongly challenge the universal applicability of full-reference metrics (e.g., PSNR, SSIM), which are predominant in current low-level vision research. These pixel-difference-based metrics carry strong implicit assumption: the existence of single, pixel-perfect ground truth. This assumption is problematic for evaluating generative solutions: Ground truth is not the unique optimum for generative repair. For regions with catastrophic information loss, multiple visually plausible and contextually correct reconstructions may exist. generative model provides one such possibility, yet it is penalized by the metric as incorrect. The metrics are misaligned with human perception. As shown in previous sections, Nano Banana Pro achieves excellent scores on No-Reference perceptual metrics (e.g., NIQE, NIMA), often surpassing specialized models. This indicates its outputs possess superior statistical naturalness and aesthetic appeal. The drop in PSNR can sometimes be attributed solely to the models reasonable global color adjustment, mild denoising, or detail enhancement, which are improvements that are paradoxically penalized. The quality of dataset ground truth itself. Ground truth images in many real-world datasets contain residual noise, slight blur, or imperfect color balance. generative model producing cleaner version constitutes perceptual enhancement but is scored as fidelity loss. Therefore, judging generative low-level vision models solely by traditional metrics may be both unfair and misleading. This calls for the community to establish new generation of evaluation frameworks."
        },
        {
            "title": "16.3 Operational Scope and Limitations of Nano Banana Pro\nOur evaluation clearly defines the scope of Nano Banana Pro, shaped by a fundamental compromise: it favors\nsemantic plausibility and visual appeal over precise pixel-level fidelity. This positions the model as highly effective\nfor creative and perceptual tasks, such as artistic image enhancement, restoration of severely degraded photos,\nand scenarios where a visually compelling result is more critical than strict accuracy. Its ability to perform\nthese tasks without specialized training also makes it a practical tool for rapid prototyping.",
            "content": "However, these capabilities come with inherent constraints. The model is not suitable for applications demanding rigorous factual accuracy, including forensic examination, scientific imaging, or any context where the output must correspond exactly to the original scene data. Its generative approach can introduce alterations, such as softened boundaries in super-resolution, altered text in deblurring, or non-physical color 50 shifts, that prioritize visual completeness over authentic reproduction. In essence, Nano Banana Pro serves as powerful semantic reconstructor and enhancer for common visual applications, but it is not designed for high-precision tasks where strict fidelity is paramount."
        },
        {
            "title": "16.4 Future Research Directions",
            "content": "The findings of this study point to several critical directions for future work: Exploration of Hybrid Architectures. The future all-rounder may not be purely generative model but generative-regression hybrid. For instance, lightweight regression network could first recover basic structure and color in the front-end, followed by conditional generative model for detail enhancement and beautification in the back-end. This process should be constrained by physics-informed loss functions to curb arbitrariness. Prompt Engineering and Controllable Generation. It is important to note that the present evaluation reflects conservative estimate of the models capability, as we did not engage in meticulous prompt tuning or employ multi-round inference to cherry-pick optimal outputs. Our fixed, simple prompts represent pragmatic but unoptimized use case. Future work should therefore systematically explore how carefully designed textual instructions, visual cues, or interactive refinement can more effectively steer the generative process. Enhancing such controllability will be key to reducing unwanted variability in color, structure, and textureultimately improving the reliability and practical utility of generative models in restoration-sensitive applications. Innovation in Evaluation Frameworks for Generative Models. The rise of generative models calls for fundamental shift in how we evaluate their output. Traditional metrics, which rely on single ground truth, fall short when assessing models that can produce multiple plausible reconstructions from degraded image. We urgently need new benchmarks that reflect this reality, for instance, datasets that include several expert-approved restoration options for given input. At the same time, evaluation should move beyond pixel-level fidelity alone. Developing unified metrics that capture both perceptual quality and distortion would provide more nuanced view of models performance across the qualityfidelity spectrum."
        },
        {
            "title": "16.5 Conclusion",
            "content": "The evaluation of Nano Banana Pro signals paradigm shift: foundational generative models are redefining the boundaries of low-level vision. Functioning less as traditional restoration tool and more as semantic reconstruction engine, the model leverages deep generative priors to synthesize visual content rather than merely recovering pixels. This emergence challenges the community to reconsider the fundamental metric of success: is it absolute pixel fidelity, or the maximization of perceptual plausibility?. Our empirical results confirm that while Nano Banana Pro trails domain-specific experts in zero-shot pixel fidelity, it demonstrates exceptional potential in perceptual quality, particularly when handling extreme degradations and cross-task generalization. Consequently, the trajectory of the field lies not in binary choice between paradigms, but in strategic integration. The next generation of robust vision systems must bridge the semantic imagination of generative models with the physical constraints and precision of specialized networks. Ultimately, Nano Banana Pro is double-edged sword. It has successfully raised the ceiling of perceptual quality for complex visual tasks, yet it has not secured the floor of stability required for forensic precision."
        },
        {
            "title": "17.1 Prompts for Each Task",
            "content": "Dehazing: This is an image with hazy, which reduces clarity and contrast. Please completely remove this atmospheric haze from the image while meticulously preserving all other elements without any alteration. The post-processing must be strictly limited to haze removal only. Ensure that every other aspect of the image remains entirely unchanged, including but not limited to the original composition, all subjects and objects within the scene. The final output should be clear, natural-looking version of the original image with its core content perfectly intact. Super-Resolution: Please upscale this low-resolution image to high-quality 1k (1024x1024) resolution. Perform super-resolution processing to significantly enhance clarity, remove pixelation, and sharpen textures, while strictly preserving the original content, composition, and colors unchanged. Do not alter the subjects features or hallucinate new elements. Deraining: This is rainy image. Please remove the rain streaks and raindrops while keeping all other elements, the original color tone, lighting, and atmosphere unchanged. Shadow Removal: This is an image with prominent cast shadows. Please remove these shadows from the image while ensuring all other elements in the scene remain completely unchanged and consistent. The core requirement is to eliminate the shadow effects while preserving the inherent properties of all objects and the background. The final output should be clean, evenly lit version of the original image, without any dark occlusions or shaded areas. Motion Deblurring: This is an image with blurring and lack of clarity due to motion. Transform this image into sharp, static photograph. Please carefully observe this picture to eliminate motion streaks and recover details in any blurred areasregardless of the motion sourcewhile keeping originally sharp elements consistent. CRITICAL EXPOSURE LOCK: Strictly prohibit any form of exposure correction, HDR tone mapping, or lighting enhancement. Do not attempt to recover details in blown-out highlights or brighten dark/underexposed shadows. You must preserve the exact luminance levels of the original image. Zero changes to brightness, contrast, or exposure are allowed. Defocus Deblurring: This is an image with partial blurring and lack of clarity due to camera defocus. Transform this image into an all-in-focus photograph. Please carefully observe this picture to enhance the sharpness of the blurred areas while keeping all other elements consistent. Maintain the original color palette, lighting, and exposure faithfully. No color shifts or brightness changes. Denoising: This is noisy image, please remove the noise in this image while keep other elements in this image unchanged. Reflection Removal: Strictly remove only the glass reflections and glare overlaying the scene. Do not alter the underlying scene composition, object details, geometry, or color grading in any way. The objective is to make the glass invisible while maintaining pixel-perfect fidelity to the original background. Zero tolerance for hallucinations, artistic interpretation, or style changes. Retain the exact original perspective and lighting conditions of the scene behind the glass. Treat this as forensic image restoration. Flare Removal: Identify and remove lens flare and glare artifacts generated by multiple distinct light sources throughout the entire image. For every single light source causing flare, eliminate the optical interference (such as ghosting, halos, and streaks) while strictly maintaining the natural illumination and intensity of each individual light. Seamlessly restore the obscured background textures behind all flare instances to ensure global consistency. Low Light Image Enhancement: This is low-light image, please turn this image into normal image while keeping other elements unchanged. Underwater Image Enhancement: This is an underwater image with obvious color cast, low contrast, and scattered fog. Please carefully analyze the main scene, eliminate the interference caused by water absorption and suspended particles, and restore clear, fog-free version with true colors. Ensure that all elements except for the degradation factors are consistent with the real underwater environment, without introducing new artifacts or over-enhancement.CRITICAL ELEMENT LOCK: remove only the color shift and fog; do not add, remove, or alter any original object, edge, texture. HDR Imaging: This is an image suffering from limited dynamic range, with lost details in highlights and shadows. Transform this image into high dynamic range photograph. Please carefully analyze this picture to expand its dynamic range, recover details in both clipped highlights and underexposed shadows, while preserving the integrity of properly exposed areas.CRITICAL CONTENT LOCK:Strictly prohibit any form of scene content alteration, object addition, or deletion. Do not change the shape, position, texture, or color relationships of any original elements. Forbid the introduction of any artistic styles, filter effects, or non-physical halos.CRITICAL PROCESSING PRINCIPLES:1. Highlight Recovery: Intelligently reconstruct plausible texture and detail lost in overexposed areas (e.g., sky, windows, light sources), but do not alter their fundamental form or color.2. Shadow Enhancement: Selectively lift the brightness of shadow areas to reveal concealed details, while must retaining the depth and original distribution of the shadows.3. Midtone Preservation: Maintain the natural contrast and color of midtone areas. Avoid introducing an overall flat/gray look or unnatural local contrast.FINAL OBJECTIVE: Produce detail-rich, balanced, and natural-looking image that appears as if the same scene was captured in single shot with professional HDR equipment, not artificially composited. Multi-Focus Image Fusion: Act as an advanced Computer Vision expert specialized in Multi-Focus Image Fusion(MFIF). Your task is to process pair of source images with different focal depths (e.g., near-focus and far-focus) and merge them into single, high-quality all-in-focus image. You must analyze the local sharpness and high-frequency details of each input to accurately identify the clearest regions, constructing precise decision map that selects the best pixels from the respective sources. Ensure to apply boundary refinement techniques to guarantee smooth transitions between fused areas. The final output must be seamless, fully focused image that strictly preserves the original color and structural fidelity while being completely free of visual artifacts such as ghosting, halos, or unnatural stitching seams. Infrared-Visible Image Fusion: Act as an expert in Infrared and Visible Image Fusion (IVIF). Your task is to generate single high-quality fused image based on the provided Infrared (IR) and Visible (VIS) source images. The core objective is to integrate complementary features: you must preserve the high-intensity thermal saliency (such as pedestrians and vehicles) from the IR image to ensure target detectability, while simultaneously injecting the high-frequency textural details (such as edges, vegetation, and building structures) from the VIS image to ensure background clarity. The fusion process must balance the intensity distribution to look perceptually natural, strictly minimizing artifacts like ghosting, halos, or noise. The final output should be sharp, noise-free image that combines the high contrast of the thermal targets with the rich structural details of the visible scene."
        },
        {
            "title": "17.2 Contributors\nJialong Zuo: Project Leader. Haoyou Deng: Document Polish.\nHanyu Zhou: Denoising and Low Light Enhancement. Jiaxin Zhu: Motion Deblurring and Defocus Deblurring.\nYicheng Zhang: Multi-focus Image Fusion and Infrared-Visible Image Fusion.\nYiwei Zhang: Underwater Image Enhancement and HDR Imaging.\nYongxin Yan: Dehazing and Shadow Removal. Kaixing Huang: Super Resolution.\nWeisen Chen: Deraining. Yongtai Deng: Reflection Removal. Rui Jin: Flare Removal.\nNong Sang: Advisor. Changxin Gao: Advisor.",
            "content": ""
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael Brown. high-quality denoising dataset for smartphone cameras. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 16921700, 2018. [2] Abdullah Abuolaim and Michael Brown. Defocus deblurring using dual-pixel data. In European conference on computer vision, pages 111126. Springer, 2020. [3] Abdullah Abuolaim, Mahmoud Afifi, and Michael Brown. Improving single-image defocus deblurring: How dual-pixel images help through multi-task learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 12311239, 2022. [4] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops (CVPRW), pages 126135, 2017. [5] C. Ancuti, C. O. Ancuti, T. Haber, and P. Bekaert. Enhancing underwater images and videos by fusion. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 8188, 2012. [6] C. O. Ancuti, C. Ancuti, C. De Vleeschouwer, and P. Bekaert. Color balance and fusion for underwater image enhancement. IEEE Trans. Image Process., 27(1):379393, 2018. [7] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [8] Dana Berman, Shai Avidan, et al. Non-local image dehazing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 16741682, 2016. [9] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 62286237, 2018. [10] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fredo Durand. Learning photographic global tonal adjustment with database of input / output image pairs. In CVPR 2011, pages 97104, 2011. doi: 10.1109/ CVPR.2011.5995332. [11] Jianrui Cai, Shuhang Gu, and Lei Zhang. Learning deep single image contrast enhancer from multi-exposure images. IEEE Transactions on Image Processing, 27(4):20492062, 2018. [12] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image superresolution: new benchmark and new model. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 30863095, 2019. [13] Jie Cai, Kangning Yang, Ling Ouyang, Lan Fu, Jiaming Ding, Huiming Sun, Chiu Man Ho, and Zibo Meng. F2t2-hit: u-shaped fft transformer and hierarchical transformer for reflection removal. arXiv preprint arXiv:2506.05489, 2025. [14] Yuanhao Cai, Hao Bian, Jing Lin, Haoqian Wang, Radu Timofte, and Yulun Zhang. Retinexformer: One-stage retinex-based transformer for low-light image enhancement. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1250412513, 2023. [15] Haoyu Chen, Jinjin Gu, Yihao Liu, Salma Abdel Magid, Chao Dong, Qiong Wang, Hanspeter Pfister, and Lei Zhu. Masked image training for generalizable deep image denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16921703, 2023. [16] Xiang Chen, Hao Li, Mingqiang Li, and Jinshan Pan. Learning sparse transformer network for effective image deraining. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 58965905, 2023. [17] Xiang Chen, Jinshan Pan, and Jiangxin Dong. Bidirectional multi-scale implicit neural representations for image deraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024. [18] Yin Chen and Rick Blum. new automated quality assessment algorithm for image fusion. Image and vision computing, 27(10):14211432, 2009. 54 [19] Zeyuan Chen, Yangchao Wang, Yang Yang, and Dong Liu. Psd: Principled synthetic-to-real dehazing guided by physical priors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 71807189, 2021. [20] Zheng Chen, Yulun Zhang, Ding Liu, Jinjin Gu, Linghe Kong, Xin Yuan, et al. Hierarchical integration diffusion model for realistic image deblurring. Advances in neural information processing systems, 36:2911429125, 2023. [21] Chunyang Cheng, Tianyang Xu, and Xiao-Jun Wu. Mufusion: general unsupervised image fusion network based on memory unit. Information Fusion, 92:8092, 2023. [22] R. Cong, W. Yang, W. Zhang, C. Li, C.-L. Guo, Q. Huang, and S. Kwong. Pugan: Physical model-guided underwater image enhancement using gan with dual-discriminators. IEEE Transactions on Image Processing, 32: 44724485, 2023. [23] Guangmang Cui, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Detail preserved fusion of visible and infrared images using regional saliency extraction and multi-scale image decomposition. Optics Communications, 341:199209, 2015. [24] Xiaodong Cun, Chi-Man Pun, and Cheng Shi. Towards ghost-free shadow removal via dual hierarchical aggregation network and shadow matting gan. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 1068010687, 2020. [25] Yuekun Dai, Chongyi Li, Shangchen Zhou, Ruicheng Feng, Yihang Luo, and Chen Change Loy. Flare7k++: Mixing synthetic and real datasets for nighttime flare removal and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(11):70417055, 2024. doi: 10.1109/TPAMI.2024.3406821. [26] Yuekun Dai, Dafeng Zhang, Xiaoming Li, Zongsheng Yue, Chongyi Li, Shangchen Zhou, Ruicheng Feng, Peiqing Yang, Zhezhu Jin, Guanqun Liu, and Chen Change Loy. Mipi 2024 challenge on nighttime flare removal: Methods and results. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 11441152, 2024. doi: 10.1109/CVPRW63382.2024.00121. [27] Haoyou Deng, Lida Li, Feng Zhang, Zhiqiang Li, Bin Xu, Qingbo Lu, Changxin Gao, and Nong Sang. Towards blind flare removal using knowledge-driven flare-level estimator. IEEE Transactions on Image Processing, 2024. [28] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning deep convolutional network for image super-resolution. European conference on computer vision (ECCV), 2014. [29] Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang, Fei Wang, and Ming-Hsuan Yang. Multi-scale boosted dehazing network with dense feature fusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 21572167, 2020. [30] Zheng Dong, Ke Xu, Yin Yang, Hujun Bao, Weiwei Xu, and Rynson WH Lau. Location-aware single image reflection removal. In Proceedings of the IEEE/CVF international conference on computer vision, pages 50175026, 2021. [31] P. L. Drews, E. R. Nascimento, S. S. Botelho, and M. F. M. Campos. Underwater depth estimation and image restoration based on single images. IEEE Computer Graphics and Applications, 36(2):2435, 2016. [32] Chengyu Fang, Chunming He, Fengyang Xiao, Yulun Zhang, Longxiang Tang, Yuelin Zhang, Kai Li, and Xiu Li. Real-world image dehazing with coherence-based pseudo labeling and cooperative unfolding network. Advances in Neural Information Processing Systems, 37:9785997883, 2024. [33] Raanan Fattal. Dehazing using color-lines. ACM transactions on graphics (TOG), 34(1):114, 2014. [34] Rich Franzen. Kodak lossless true color image suite, volume 5. https://r0k.us/graphics/kodak/, 1999. [35] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao Ding, and John Paisley. Removing rain from single images via deep detail network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 38553863, 2017. [36] Xueyang Fu, Qi Qi, Zheng-Jun Zha, Yurui Zhu, and Xinghao Ding. Rain streak removal via dual graph convolutional network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13521360, 2021. [37] Zhenqi Fu, Yan Yang, Xiaotong Tu, Yue Huang, Xinghao Ding, and Kai-Kuang Ma. Learning simple low-light image enhancer from paired low-light instances. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2225222261, 2023. 55 [38] A. Galdran, D. Pardo, A. Picon, and A. Alvarez-Gila. Automatic red-channel underwater image restoration. Journal of Visual Communication and Image Representation, 26:132145, 2015. [39] S.-B. Gao, M. Zhang, Q. Zhao, X.-S. Zhang, and Y.-J. Li. Underwater image enhancement using adaptive retinal mechanisms. IEEE Trans. Image Process., 28(11):55805595, 2019. [40] A. S. A. Ghani and N. A. M. Isa. Underwater image quality enhancement through integrated color model with rayleigh distribution. Applied Soft Computing, 27:219230, 2015. [41] Michal Gharbi, Jiawen Chen, Jonathan Barron, Samuel Hasinoff, and Frdo Durand. Deep bilateral learning for real-time image enhancement. ACM Transactions on Graphics (TOG), 36(4):112, 2017. [42] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems (NeurIPS), 2014. [43] Chun-Le Guo, Qixin Yan, Saeed Anwar, Runmin Cong, Wenqi Ren, and Chongyi Li. Image dehazing transformer with transmission-aware 3d position embedding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 58125820, 2022. [44] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and Runmin Cong. Zeroreference deep curve estimation for low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17801789, 2020. [45] Lanqing Guo, Siyu Huang, Ding Liu, Hao Cheng, and Bihan Wen. Shadowformer: Global context helps shadow removal. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 710718, 2023. [46] Lanqing Guo, Chong Wang, Wenhan Yang, Siyu Huang, Yufei Wang, Hanspeter Pfister, and Bihan Wen. Shadowdiffusion: When degradation prior meets diffusion model for shadow removal. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1404914058, 2023. [47] Ruiqi Guo, Qieyun Dai, and Derek Hoiem. Paired regions for shadow detection and removal. IEEE transactions on pattern analysis and machine intelligence, 35(12):29562967, 2012. [48] Xiaojie Guo and Qiming Hu. Low-light image enhancement via breaking down the darkness. International Journal of Computer Vision, 131(1):4866, 2023. [49] Jingwen He, Yihao Liu, Yu Qiao, and Chao Dong. Conditional sequential modulation for efficient global image retouching. In European Conference on Computer Vision, pages 679695. Springer, 2020. [50] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze removal using dark channel prior. IEEE transactions on pattern analysis and machine intelligence, 33(12):23412353, 2010. [51] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pages 68406851, 2020. [52] Yuchen Hong, Haofeng Zhong, Shuchen Weng, Jinxiu Liang, and Boxin Shi. L-differ: Single image reflection removal with language-based diffusion model. In European Conference on Computer Vision, pages 5876. Springer, 2024. [53] Mohammed Hossny, Saeid Nahavandi, and Douglas Creighton. Comments on information measure for performance of image fusion. Electronics letters, 44(18):10661067, 2008. [54] Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, and Hui Yuan. Global structure-aware diffusion process for low-light image enhancement. Advances in Neural Information Processing Systems, 36:7973479747, 2023. [55] Jichen Hu, Chen Yang, Zanwei Zhou, Jiemin Fang, Xiaokang Yang, Qi Tian, and Wei Shen. Dereflection any image with diffusion priors and diversified data. arXiv preprint arXiv:2503.17347, 2025. [56] Qiming Hu and Xiaojie Guo. Trash or treasure? an interactive dual-stream strategy for single image reflection separation. Advances in Neural Information Processing Systems, 34:2468324694, 2021. [57] Qiming Hu and Xiaojie Guo. Single image reflection separation via component synergy. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1313813147, 2023. [58] Qiming Hu, Hainuo Wang, and Xiaojie Guo. Single image reflection separation via dual-stream interactive transformers. Advances in Neural Information Processing Systems, 37:5522855248, 2024. 56 [59] Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, and Pheng-Ann Heng. Direction-aware spatial context features for shadow detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 74547462, 2018. [60] Xingyu Hu, Junjun Jiang, Xianming Liu, and Jiayi Ma. Zmff: Zero-shot multi-focus image fusion. Information Fusion, 92:127138, 2023. [61] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed selfexemplars. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 51975206, 2015. [62] Yihang Huang, Yuanfei Huang, Junhui Lin, and Hua Huang. Deflaremamba: Hierarchical vision mamba for contextually consistent lens flare removal. In Proceedings of the 33rd ACM International Conference on Multimedia, page 80288037, 2025. [63] Yue Huang, Ziang Li, Tianle Hu, Jie Wen, Guanbin Li, Jinglin Zhang, Guoxu Zhou, and Xiaozhao Fang. Single image reflection removal via inter-layer complementarity. arXiv preprint arXiv:2505.12641, 2025. [64] Matthias Hullin, Elmar Eisemann, Hans-Peter Seidel, and Sungkil Lee. Physically-based real-time lens flare rendering. ACM Trans. Graph., 30(4), 2011. [65] K. Iqbal, M. Odetayo, A. James, R. A. Salam, and A. Z. H. Talib. Enhancing the low quality images using unsupervised colour correction method. In IEEE International Conference on Systems, Man and Cybernetics, pages 17031709, 2010. [66] Bernd Jhne. Digital image processing. Springer, 2005. [67] Kui Jiang, Zhongyuan Wang, Peng Yi, Chen Chen, Baojin Huang, Yimin Luo, Jiayi Ma, and Junjun Jiang. Multi-scale progressive fusion network for single image deraining. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83468355, 2020. [68] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision. IEEE transactions on image processing, 30:23402349, 2021. [69] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Ji Matas. Deblurgan: Blind motion deblurring using conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 81838192, 2018. [70] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang. Deblurgan-v2: Deblurring (orders-ofmagnitude) faster and better. In Proceedings of the IEEE/CVF international conference on computer vision, pages 88788887, 2019. [71] Christian Ledig, Lucas Theis, Ferenc Huszr, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Alykhan Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 46814690, 2017. [72] Junyong Lee, Hyeongseok Son, Jaesung Rim, Sunghyun Cho, and Seungyong Lee. Iterative filter adaptive network for single image defocus deblurring. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20342042, 2021. [73] Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking single-image dehazing and beyond. IEEE transactions on image processing, 28(1):492505, 2018. [74] C. Li, J. Guo, B. Wang, R. Cong, Y. Zhang, and J. Wang. Single underwater image enhancement based on color cast removal and visibility restoration. Journal of Electronic Imaging, 25(3):033012, 2016. [75] C.-Y. Li, J.-C. Guo, R.-M. Cong, Y.-W. Pang, and B. Wang. Underwater image enhancement by dehazing with minimum information loss and histogram distribution prior. IEEE Transactions on Image Processing, 25(12): 56645677, 2016. [76] Chao Li, Yixiao Yang, Kun He, Stephen Lin, and John Hopcroft. Single image reflection removal through cascaded refinement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35653574, 2020. 57 [77] Chongyi Li, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui Hou, Sam Kwong, and Dacheng Tao. An underwater image enhancement benchmark dataset and beyond. IEEE Transactions on Image Processing, 29:43764389, nov 2019. doi: 10.1109/TIP.2019.2955241. [78] H. Li, J. Li, and W. Wang. Fusion Adversarial Underwater Image Enhancement Network with Public Test Dataset. arXiv preprint, 2019. doi: 10.48550/arXiv.1906.06819. arXiv:1906.06819. [79] Huafeng Li, Yitang Wang, Zhao Yang, Ruxin Wang, Xiang Li, and Dapeng Tao. Discriminative dictionary learning-based multiple component decomposition for detail-preserving noisy image fusion. IEEE Transactions on Instrumentation and Measurement, 69(4):10821102, 2019. [80] Huafeng Li, Dan Wang, Yuxin Huang, Yafei Zhang, and Zhengtao Yu. Generation and recombination for multifocus image fusion with free number of inputs. IEEE Transactions on Circuits and Systems for Video Technology, 34(7):60096023, 2023. [81] Hui Li, Tianyang Xu, Xiao-Jun Wu, Jiwen Lu, and Josef Kittler. Lrrnet: novel representation learning guided fusion network for infrared and visible images. IEEE transactions on pattern analysis and machine intelligence, 45(9):1104011052, 2023. [82] Jing Li, Hongtao Huo, Chang Li, Renhua Wang, and Qi Feng. Attentionfgan: Infrared and visible image fusion using attention-based generative adversarial networks. IEEE Transactions on Multimedia, 23:13831396, 2020. [83] Jing Li, Jianming Zhu, Chang Li, Xun Chen, and Bin Yang. Cgtf: Convolution-guided transformer for infrared and visible image fusion. IEEE Transactions on Instrumentation and Measurement, 71:114, 2022. [84] Jinxing Li, Xiaobao Guo, Guangming Lu, Bob Zhang, Yong Xu, Feng Wu, and David Zhang. Drpl: Deep regression pair learning for multi-focus image fusion. IEEE Transactions on Image Processing, 29:48164831, 2020. [85] Mining Li, Ronghao Pei, Tianyou Zheng, Yang Zhang, and Weiwei Fu. Fusiondiff: Multi-focus image fusion using denoising diffusion probabilistic models. Expert Systems with Applications, 238:121664, 2024. [86] Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, and Hongbin Zha. Recurrent squeeze-and-excitation context aggregation net for single image deraining. In Proceedings of the European conference on computer vision (ECCV), pages 254269, 2018. [87] Xiaoyu Li, Bo Zhang, Jing Liao, and Pedro V. Sander. Lets see clearly: Contaminant artifact removal for moving cameras. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 19912000, 2021. doi: 10.1109/ICCV48922.2021.00202. [88] Xin Li, Bingchen Li, Xin Jin, Cuiling Lan, and Zhibo Chen. Learning distortion invariant representation for image restoration from causality perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17141724, 2023. [89] Yu Li, Robby Tan, Xiaojie Guo, Jiangbo Lu, and Michael Brown. Rain streak removal using layer priors. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 27362744, 2016. [90] Yu Li, Ming Liu, Yaling Yi, Qince Li, Dongwei Ren, and Wangmeng Zuo. Two-stage single image reflection removal with reflection-aware guidance. Applied Intelligence, 53(16):1943319448, 2023. [91] Pengwei Liang, Junjun Jiang, Xianming Liu, and Jiayi Ma. Fusion from decomposition: self-supervised decomposition approach for image fusion. In European conference on computer vision, pages 719735. Springer, 2022. [92] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Wanli Ouyang, Yu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior. In arXiv preprint arXiv:2308.15070, 2023. [93] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [94] Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng Liu, Wei Zhong, and Zhongxuan Luo. Target-aware dual adversarial learning and multi-scenario multi-modality benchmark to fuse infrared and visible for object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 58025811, 2022. 58 [95] Jinyuan Liu, Zhu Liu, Guanyao Wu, Long Ma, Risheng Liu, Wei Zhong, Zhongxuan Luo, and Xin Fan. Multiinteractive feature learning and full-time multi-modality benchmark for image fusion and segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 81158124, 2023. [96] Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, and Zhongxuan Luo. Retinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1056110570, 2021. [97] Yu Liu, Shuping Liu, and Zengfu Wang. general framework for image fusion based on multi-scale transform and sparse representation. Information fusion, 24:147164, 2015. [98] Yu Liu, Xun Chen, Hu Peng, and Zengfu Wang. Multi-focus image fusion with deep convolutional neural network. Information Fusion, 36:191207, 2017. [99] Yanzuo Lu, Xin Xia, Manlin Zhang, Huafeng Kuang, Jianbin Zheng, Yuxi Ren, and Xuefeng Xiao. Hyper-bagel: unified acceleration framework for multimodal understanding and generation. arXiv preprint arXiv:2509.18824, 2025. [100] Zhengyang Lu, Weifan Wang, Tianhao Guo, and Feng Wang. Single-image reflection removal via self-supervised diffusion models. The Journal of Supercomputing, 81(1):338, 2025. [101] Simin Luan, Cong Yang, Zeyd Boukhers, Xue Qin, Dongfeng Cheng, Wei Sui, and Zhijun Li. Gyroscope-assisted motion deblurring network. CoRR, 2024. [102] Yu Luo, Yong Xu, and Hui Ji. Removing rain from single image via discriminative sparse coding. In Proceedings of the IEEE international conference on computer vision, pages 33973405, 2015. [103] Ziwei Luo, Fredrik Gustafsson, Zheng Zhao, Jens Sjlund, and Thomas Schn. Image restoration with mean-reverting stochastic differential equations. In Proceedings of the 40th International Conference on Machine Learning, pages 2304523066, 2023. [104] Boyuan Ma, Yu Zhu, Xiang Yin, Xiaojuan Ban, Haiyou Huang, and Michele Mukeshimana. Sesf-fuse: An unsupervised deep model for multi-focus image fusion. Neural Computing and Applications, 33(11):57935804, 2021. [105] Boyuan Ma, Xiang Yin, Di Wu, Haokai Shen, Xiaojuan Ban, and Yu Wang. End-to-end learning for simultaneously generating decision map and multi-focus image fusion result. Neurocomputing, 470:204216, 2022. [106] Jiayi Ma, Wei Yu, Pengwei Liang, Chang Li, and Junjun Jiang. Fusiongan: generative adversarial network for infrared and visible image fusion. Information fusion, 48:1126, 2019. [107] Jiayi Ma, Pengwei Liang, Wei Yu, Chen Chen, Xiaojie Guo, Jia Wu, and Junjun Jiang. Infrared and visible image fusion via detail preserving adversarial learning. Information Fusion, 54:8598, 2020. [108] Jiayi Ma, Han Xu, Junjun Jiang, Xiaoguang Mei, and Xiao-Ping Zhang. Ddcgan: dual-discriminator conditional generative adversarial network for multi-resolution image fusion. IEEE Transactions on Image Processing, 29: 49804995, 2020. [109] Jiayi Ma, Linfeng Tang, Fan Fan, Jun Huang, Xiaoguang Mei, and Yong Ma. Swinfusion: Cross-domain long-range learning for general image fusion via swin transformer. IEEE/CAA Journal of Automatica Sinica, 9 (7):12001217, 2022. [110] Armin Mehri, Parichehr Ardakani, and Angel Sappa. Mprnet: Multi-path residual network for lightweight image super resolution. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 27042713, 2021. [111] Anish Mittal, Rajiv Soundararajan, and Alan Bovik. Making completely blind image quality analyzer. IEEE Signal processing letters, 20(3):209212, 2012. [112] Sean Moran, Pierre Marza, Steven McDonagh, Sarah Parisot, and Gregory Slabaugh. Deeplpf: Deep local parametric filters for image enhancement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1282612835, 2020. [113] Vineeth Murali and PV Sudeep. Image denoising using dncnn: An exploration study. In Advances in Communication Systems and Networks: Select Proceedings of ComNet 2019, pages 847859. Springer, 2020. 59 [114] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 38833891, 2017. [115] Mansour Nejati, Shadrokh Samavi, and Shahram Shirani. Multi-focus image fusion using dictionary-based sparse representation. Information fusion, 25:7284, 2015. [116] K. Panetta, C. Gao, and S. Agaian. Human-visual-system-inspired underwater image quality measures. IEEE Journal of Oceanic Engineering, 41(3):541551, 2015. doi: 10.1109/JOE.2015.2410644. [117] L. Peng, C. Zhu, and L. Bian. U-shape transformer for underwater image enhancement. IEEE Transactions on Image Processing, 32:30663079, 2023. doi: 10.1109/TIP.2023.3276332. [118] Yuwei Qiu, Kaihao Zhang, Chenxi Wang, Wenhan Luo, Hongdong Li, and Zhi Jin. Mb-taylorformer: Multibranch efficient transformer expanded by taylor formula for image dehazing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1280212813, 2023. [119] Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, and Rynson WH Lau. Deshadownet: multicontext embedding deep network for shadow removal. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 40674075, 2017. [120] Yuhui Quan, Zicong Wu, and Hui Ji. Gaussian kernel mixture network for single image defocus deblurring. Advances in Neural Information Processing Systems, 34:2081220824, 2021. [121] Yuhui Quan, Zicong Wu, and Hui Ji. Neumann network with recursive kernels for single image defocus deblurring. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57545763, 2023. [122] Yuhui Quan, Xin Yao, and Hui Ji. Single image defocus deblurring via implicit neural inverse kernels. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1260012610, 2023. [123] Yuhui Quan, Zicong Wu, Ruotao Xu, and Hui Ji. Deep single image defocus deblurring via gaussian kernel mixture learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [124] Yuhui Quan, Xi Wan, Zitao Tang, Jinxiu Liang, and Hui Ji. Multi-focus image fusion via explicit defocus blur modelling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 66576665, 2025. [125] Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, and Deyu Meng. Progressive image deraining networks: better and simpler baseline. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 39373946, 2019. [126] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho. Real-world blur dataset for learning and benchmarking deblurring algorithms. In European conference on computer vision, pages 184201. Springer, 2020. [127] Lingyan Ruan, Bin Chen, Jizhou Li, and Miu-Ling Lam. Aifnet: All-in-focus image restoration network using light field-based dataset. IEEE Transactions on Computational Imaging, 7:675688, 2021. [128] Lingyan Ruan, Bin Chen, Jizhou Li, and Miuling Lam. Learning to deblur using light field generated and real defocus images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1630416313, 2022. [129] Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao, and Nong Sang. Domain adaptation for image dehazing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 28082817, 2020. [130] Ziyi Shen, Wenguan Wang, Xiankai Lu, Jianbing Shen, Haibin Ling, Tingfa Xu, and Ling Shao. Humanaware motion deblurring. In Proceedings of the IEEE/CVF international conference on computer vision, pages 55725581, 2019. [131] Hyeongseok Son, Junyong Lee, Sunghyun Cho, and Seungyong Lee. Single image defocus deblurring using kernel-sharing parallel atrous convolutions. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26422650, 2021. [132] Yuda Song, Zhuqing He, Hui Qian, and Xin Du. Vision transformers for single image dehazing. IEEE Transactions on Image Processing, 32:19271941, 2023. [133] Hao Tang, Chengcheng Yuan, Zechao Li, and Jinhui Tang. Learning attention-guided pyramidal features for few-shot fine-grained recognition. Pattern Recognition, 130:108792, 2022. [134] Linfeng Tang, Jiteng Yuan, Hao Zhang, Xingyu Jiang, and Jiayi Ma. Piafusion: progressive infrared and visible image fusion network based on illumination aware. Information Fusion, 83:7992, 2022. 60 [135] Yi Tang, Hiroshi Kawasaki, and Takafumi Iwaguchi. Underwater image enhancement by transformer-based diffusion model with non-uniform sampling for skip strategy. In Proceedings of the 31st ACM International Conference on Multimedia (MM 23), pages 54195427, New York, NY, USA, 2023. Association for Computing Machinery. doi: 10.1145/3581783.3612475. [136] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [137] Chun-Chieh Tsai. Standard images for multifocus image fusion, 2025. [138] Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi Tsai, and Chia-Wen Lin. Stripformer: Strip transformer for fast image deblurring. In European conference on computer vision, pages 146162. Springer, 2022. [139] Yael Vinker, Inbar Huberman-Spiegelglas, and Raanan Fattal. Unpaired learning for high dynamic range image tone mapping. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1463714646, 2021. doi: 10.1109/ICCV48922.2021.01439. [140] Vibashan Vs, Jeya Maria Jose Valanarasu, Poojan Oza, and Vishal Patel. Image fusion transformer. In IEEE International conference on image processing (ICIP), pages 35663570. IEEE, 2022. [141] Renjie Wan, Boxin Shi, Ling-Yu Duan, Ah-Hwee Tan, and Alex Kot. Benchmarking single-image reflection removal algorithms. In Proceedings of the IEEE international conference on computer vision, pages 39223930, 2017. [142] Hong Wang, Qi Xie, Qian Zhao, and Deyu Meng. model-driven deep neural network for single image rain removal. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 31033112, 2020. [143] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 25552563, 2023. [144] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. International Journal of Computer Vision (IJCV), pages 121, 2024. [145] Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68496857, 2019. [146] Tao Wang, Yong Li, Jingyang Peng, Yipeng Ma, Xian Wang, Fenglong Song, and Youliang Yan. Real-time image enhancer via learnable spatial-aware 3d lookup tables. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 24712480, 2021. [147] Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo, Bjorn Stenger, and Tong Lu. Ultra-high-definition In Proceedings of the AAAI low-light image enhancement: benchmark and transformer-based method. conference on artificial intelligence, volume 37, pages 26542662, 2023. [148] Tianyu Wang, Xin Yang, Ke Xu, Shaozhe Chen, Qiang Zhang, and Rynson W.H. Lau. Spatial attentive single-image deraining with high quality real rain dataset. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. [149] Wenjing Wang, Huan Yang, Jianlong Fu, and Jiaying Liu. Zero-reference low-light enhancement via physical quadruple priors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2605726066, 2024. [150] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 19051914, 2021. [151] Yudong Wang, Jichang Guo, Huan Gao, and Huihui Yue. Uiec-net: Cnn-based underwater image enhancement using two color space. Signal Processing: Image Communication, 96:116250, 2021. ISSN 0923-5965. [152] Yufei Wang, Renjie Wan, Wenhan Yang, Haoliang Li, Lap-Pui Chau, and Alex Kot. Low-light image enhancement with normalizing flow. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 26042612, 2022. 61 [153] Yufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu, Yu Qiao, Alex Kot, and Bihan Wen. Sinsr: diffusion-based image super-resolution in single step. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2579625805, 2024. [154] Zhaohan Wang, Chengjun Chen, and Chenggang Dai. Zero-shot realistic image deblurring with consistency model. Complex & Intelligent Systems, 12(1):29, 2026. [155] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: general u-shaped transformer for image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1768317693, 2022. [156] Zhishe Wang, Yanlin Chen, Wenyu Shao, Hui Li, and Lei Zhang. Swinfuse: residual swin transformer fusion network for infrared and visible images. IEEE Transactions on Instrumentation and Measurement, 71:112, 2022. [157] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [158] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, 2018. [159] Kaixuan Wei, Jiaolong Yang, Ying Fu, David Wipf, and Hua Huang. Single image reflection removal exploiting misaligned training data and network enhancements. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81788187, 2019. [160] Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng Zuo, and Liang Lin. Component divide-and-conquer for real-world image super-resolution. In European Conference on Computer Vision (ECCV), pages 101117, 2020. [161] Rui-Qi Wu, Zheng-Peng Duan, Chun-Le Guo, Zhi Chai, and Chongyi Li. Ridcp: Revitalizing real image dehazing via high-quality codebook priors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2228222291, 2023. [162] Yicheng Wu, Qiurui He, Tianfan Xue, Rahul Garg, Jiawen Chen, Ashok Veeraraghavan, and Jonathan T. Barron. How to train neural networks for flare removal. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 22192227, 2021. doi: 10.1109/ICCV48922.2021.00224. [163] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1309513105, 2023. [164] Bin Xiao, Haifeng Wu, and Xiuli Bi. Dtmnet: discrete tchebichef moments-based deep neural network for multi-focus image fusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4351, 2021. [165] Jie Xiao, Xueyang Fu, Aiping Liu, Feng Wu, and Zheng-Jun Zha. Image de-raining transformer. IEEE transactions on pattern analysis and machine intelligence, 45(11):1297812995, 2022. [166] Jie Xiao, Xueyang Fu, Yurui Zhu, Dong Li, Jie Huang, Kai Zhu, and Zheng-Jun Zha. Homoformer: Homogenized transformer for image shadow removal. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2561725626, 2024. [167] Han Xu, Jiayi Ma, Zhuliang Le, Junjun Jiang, and Xiaojie Guo. Fusiondn: unified densely connected network for image fusion. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 1248412491, 2020. [168] Han Xu, Jiteng Yuan, and Jiayi Ma. Murf: Mutually reinforcing multi-modal image registration and fusion. IEEE transactions on pattern analysis and machine intelligence, 45(10):1214812166, 2023. [169] Jun Xu, Hui Li, Zhetong Liang, David Zhang, and Lei Zhang. Real-world noisy image denoising: new benchmark. arXiv preprint arXiv:1804.02603, 2018. [170] Shuang Xu, Xiaoli Wei, Chunxia Zhang, Junmin Liu, and Jiangshe Zhang. Mffw: new dataset for multi-focus image fusion. arXiv preprint arXiv:2002.04780, 2020. [171] Xiaogang Xu, Ruixing Wang, Chi-Wing Fu, and Jiaya Jia. Snr-aware low-light image enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1771417724, 2022. 62 [172] Hanshu Yan, Jingfeng Zhang, Jiashi Feng, Masashi Sugiyama, and Vincent YF Tan. Towards adversarially robust deep image denoising. arXiv preprint arXiv:2201.04397, 2022. [173] Cui Yang, Jian-Qi Zhang, Xiao-Rui Wang, and Xin Liu. novel similarity based quality metric for image fusion. Information Fusion, 9(2):156160, 2008. [174] Jie Yang, Dong Gong, Lingqiao Liu, and Qinfeng Shi. Seeing deeply and bidirectionally: deep learning approach for single image reflection removal. In Proceedings of the european conference on computer vision (ECCV), pages 654669, 2018. [175] M. Yang and A. Sowmya. An underwater color image quality evaluation metric. IEEE Transactions on Image Processing, 24(12):60626071, 2015. doi: 10.1109/TIP.2015.2480136. [176] Wenhan Yang, Robby Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep joint rain detection and removal from single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13571366, 2017. [177] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang, and Jiaying Liu. Sparse gradient regularized deep retinex network for robust low-light image enhancement. IEEE Transactions on Image Processing, 30:20722086, 2021. [178] Yang Yang, Chaoyue Wang, Risheng Liu, Lin Zhang, Xiaojie Guo, and Dacheng Tao. Self-augmented unpaired image dehazing via density and depth decomposition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20372046, 2022. [179] Qiaosi Yi, Juncheng Li, Qinyan Dai, Faming Fang, Guixu Zhang, and Tieyong Zeng. Structure-preserving deraining with residue channel prior guidance. In Proceedings of the IEEE/CVF international conference on computer vision, pages 42384247, 2021. [180] Daniyar Zakarin, Thiemo Wandel, Anton Obukhov, and Dengxin Dai. Reflection removal through efficient adaptation of diffusion transformers. arXiv preprint arXiv:2512.05000, 2025. [181] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1482114831, 2021. [182] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57285739, 2022. [183] Hui Zeng, Jianrui Cai, Lida Li, Zisheng Cao, and Lei Zhang. Learning image-adaptive 3d lookup tables for high performance photo enhancement in real-time. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):20582073, 2020. [184] Hao Zhai, Wenyi Zheng, Yuncan Ouyang, Xin Pan, and Wanli Zhang. Multi-focus image fusion via interactive transformer and asymmetric soft sharing. Engineering Applications of Artificial Intelligence, 133:107967, 2024. [185] Feng Zhang, Haoyou Deng, Zhiqiang Li, Lida Li, Bin Xu, Qingbo Lu, Zisheng Cao, Minchen Wei, Changxin Gao, Nong Sang, et al. High-resolution photo enhancement in real-time: laplacian pyramid network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [186] Fengyi Zhang, Hui Zeng, Tianjun Zhang, and Lin Zhang. Clut-net: Learning adaptively compressed representations of 3dluts for lightweight image enhancement. In Proceedings of the 30th ACM International Conference on Multimedia, pages 64936501. ACM, 2022. [187] Hao Zhang and Jiayi Ma. Sdnet: versatile squeeze-and-decomposition network for real-time image fusion. International Journal of Computer Vision, 129(10):27612785, 2021. [188] Hao Zhang, Zhuliang Le, Zhenfeng Shao, Han Xu, and Jiayi Ma. Mff-gan: An unsupervised generative adversarial network with adaptive and gradient joint constraints for multi-focus image fusion. Information Fusion, 66:4053, 2021. [189] Juncheng Zhang, Qingmin Liao, Haoyu Ma, Jing-Hao Xue, Wenming Yang, and Shaojun Liu. Exploit the best of both end-to-end and map-based methods for multi-focus image fusion. IEEE Transactions on Multimedia, 26: 64116423, 2024. [190] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):31423155, 2017. [191] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind image super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 47914800, 2021. [192] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn Stenger, Wei Liu, and Hongdong Li. Deblurring by realistic blurring. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 27372746, 2020. [193] Lei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li. Color demosaicking by local directional interpolation and nonlocal adaptive thresholding. Journal of Electronic imaging, 20(2):023016023016, 2011. [194] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pages 586595, 2018. [195] W. Zhang, Y. Wang, and C. Li. Underwater image enhancement by attenuated color channel correction and detail preserved contrast enhancement. IEEE Journal of Oceanic Engineering, pages 118, 2022. [196] Xuaner Zhang, Ren Ng, and Qifeng Chen. Single image reflection separation with perceptual losses. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 47864794, 2018. [197] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling the darkness: practical low-light image enhancer. In Proceedings of the 27th ACM international conference on multimedia, pages 16321640, 2019. [198] Yu Zhang, Yu Liu, Peng Sun, Han Yan, Xiaolin Zhao, and Li Zhang. Ifcnn: general image fusion framework based on convolutional neural network. Information Fusion, 54:99118, 2020. [199] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Proceedings of the European conference on computer vision (ECCV), pages 286301, 2018. [200] C. Zhao, W. Cai, C. Dong, and C. Hu. Wavelet-based fourier information interaction with frequency diffusion adjustment for underwater image restoration. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 82818291, Seattle, WA, USA, jun 2024. IEEE/CVF. doi: 10.1109/CVPR52729. 2024.00813. [201] Hao Zhao, Mingjia Li, Qiming Hu, and Xiaojie Guo. Reversible decoupling network for single image reflection removal. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2643026439, 2025. [202] Wenda Zhao, Shigeng Xie, Fan Zhao, You He, and Huchuan Lu. Metafusion: Infrared and visible image fusion via meta-feature embedding from object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1395513965, 2023. [203] Zixiang Zhao, Shuang Xu, Jiangshe Zhang, Chengyang Liang, Chunxia Zhang, and Junmin Liu. Efficient and model-based infrared and visible image fusion via algorithm unrolling. IEEE Transactions on Circuits and Systems for Video Technology, 32(3):11861196, 2021. [204] Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Shuang Xu, Zudi Lin, Radu Timofte, and Luc Van Gool. Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 59065916, 2023. [205] Zixiang Zhao, Haowen Bai, Yuanzhi Zhu, Jiangshe Zhang, Shuang Xu, Yulun Zhang, Kai Zhang, Deyu Meng, Radu Timofte, and Luc Van Gool. Ddfm: denoising diffusion model for multi-modality image fusion. In Proceedings of the IEEE/CVF international conference on computer vision, pages 80828093, 2023. [206] Zixiang Zhao, Haowen Bai, Jiangshe Zhang, Yulun Zhang, Kai Zhang, Shuang Xu, Dongdong Chen, Radu In Proceedings of the IEEE/CVF Timofte, and Luc Van Gool. Equivariant multi-modality image fusion. conference on computer vision and pattern recognition, pages 2591225921, 2024. [207] Kecheng Zheng, Juan Cheng, and Yu Liu. Unfolding coupled convolutional sparse representation for multi-focus image fusion. Information Fusion, 118:102974, 2025. [208] Yufeng Zheng, Edward Essock, Bruce Hansen, and Andrew Haun. new metric based on extended spatial frequency and its application to dwt based fusion algorithms. Information Fusion, 8(2):177192, 2007. [209] Yurui Zhu, Jie Huang, Xueyang Fu, Feng Zhao, Qibin Sun, and Zheng-Jun Zha. Bijective mapping network for shadow removal. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56275636, 2022. 64 [210] Yurui Zhu, Xueyang Fu, Peng-Tao Jiang, Hao Zhang, Qibin Sun, Jinwei Chen, Zheng-Jun Zha, and Bo Li. Revisiting single image reflection removal in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2546825478, 2024."
        }
    ],
    "affiliations": [
        "National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology"
    ]
}