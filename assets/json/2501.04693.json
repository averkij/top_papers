{
    "paper_title": "Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding",
    "authors": [
        "Joshua Jones",
        "Oier Mees",
        "Carmelo Sferrazza",
        "Kyle Stachowicz",
        "Pieter Abbeel",
        "Sergey Levine"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Interacting with the world is a multi-sensory experience: achieving effective general-purpose interaction requires making use of all available modalities -- including vision, touch, and audio -- to fill in gaps from partial observation. For example, when vision is occluded reaching into a bag, a robot should rely on its senses of touch and sound. However, state-of-the-art generalist robot policies are typically trained on large datasets to predict robot actions solely from visual and proprioceptive observations. In this work, we propose FuSe, a novel approach that enables finetuning visuomotor generalist policies on heterogeneous sensor modalities for which large datasets are not readily available by leveraging natural language as a common cross-modal grounding. We combine a multimodal contrastive loss with a sensory-grounded language generation loss to encode high-level semantics. In the context of robot manipulation, we show that FuSe enables performing challenging tasks that require reasoning jointly over modalities such as vision, touch, and sound in a zero-shot setting, such as multimodal prompting, compositional cross-modal prompting, and descriptions of objects it interacts with. We show that the same recipe is applicable to widely different generalist policies, including both diffusion-based generalist policies and large vision-language-action (VLA) models. Extensive experiments in the real world show that FuSeis able to increase success rates by over 20% compared to all considered baselines."
        },
        {
            "title": "Start",
            "content": "Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding Joshua Jones, Oier Mees, Carmelo Sferrazza, Kyle Stachowicz, Pieter Abbeel, Sergey Levine https://fuse-model.github.io 5 2 0 2 4 1 ] . [ 3 3 9 6 4 0 . 1 0 5 2 : r Fig. 1: We introduce FuSe, an approach that enables finetuning large image-based pre-trained generalist policies, including vision-language-action (VLA) models, on heterogeneous robot sensor modalities, such as touch or audio, for which large datasets are not readily available, while leveraging natural language as common cross-modal grounding. Our finetuning recipe enables challenging multimodal and cross-modal prompting tasks in partially-observable scenes and is able to generate zero-shot descriptions of objects it interacts with. Abstract Interacting with the world is multi-sensory experience: achieving effective general-purpose interaction requires making use of all available modalities including vision, touch, and audio to fill in gaps from partial observation. For example, when vision is occluded reaching into bag, robot should rely on its senses of touch and sound. However, state-of-the-art generalist robot policies are typically trained on large datasets to predict robot actions solely from visual and proprioceptive observations. In this work, we propose FuSe, novel approach that enables finetuning visuomotor generalist policies on heterogeneous sensor modalities for which large datasets are not readily available by leveraging natural language as common cross-modal grounding. We combine multimodal contrastive loss with sensory-grounded language generation loss to encode high-level semantics. In the context of robot manipulation, we show that FuSe enables performing challenging tasks that require reasoning jointly over modalities such as vision, touch, and sound in zero-shot setting, such as multimodal prompting, compositional cross-modal prompting, and descriptions of objects it interacts with. We show that the same recipe is applicable to widely different generalist policies, including both diffusion-based generalist policies and large vision-language-action (VLA) models. Extensive experiments in the real world show that FuSe is able to increase success rates by over 20% compared to all considered baselines. Equal contribution, authors listed alphabetically. The authors are members of Berkeley AI Research (BAIR), UC Berkeley, USA. Please email correspondence to the lead authors: {joshuajones, csferrazza, oier.mees}@berkeley.edu. I. INTRODUCTION Intelligent beings have the ability to seamlessly combine variety of sensory feedback that allows them to effectively interact with physical the world. Beyond vision, humans rely on the touch and audio feedback to manipulate objects [1], [2], as they provide rich complementary information about object properties, especially when visual information alone might be insufficient to complete the task, such as when locating keys inside bag [3]. This stands in contrast to state-of-the-art generalist robot policies [4][8] that absorb knowledge from vast amount of robotics datasets [9] [13] but typically rely solely on visual and proprioceptive observations to perform wide range of tasks. The main factor limiting development of generalist robot policies based on truly hetereogeneous data is that, while nearly all robotics datasets include visual and proprioceptive information, only small minority of them include other modalities of sensory data [14][16]. This raises the question: how can we retain the generalization capabilities of generalist robot policies pre-trained on large amounts of data, while connecting their semantic knowledge with heterogeneous sensory data for which large datasets are not readily available?"
        },
        {
            "title": "Prior studies show that natural",
            "content": "language can provide common interface between mixed-modal models, even when they are trained on minimally overlapping data domains [17][23]. Moreover, relating human language to multimodal percepts and actions naturally enables indexing goals using open-vocabulary queries mixing concepts from multiple distinct modalities (pick up the squishy, red object). Nonetheless, incorporating multiple sensing modalities, such as touch or audio, into robotic policies has thus far proved challenging due to data scarcity and in particular lack of data including joint reasoning over multimodal percepts and low-level robotic actions [2], [3], [14][16], [24][27]. In this work, we address these challenges and present recipe to finetune generalist robot policies on smaller-scale datasets comprising modalities complementary to vision, such as touch and sound, and demonstrate that novel capabilities and cross-modal semantic understanding are unlocked through this multimodal finetuning procedure. Our key insight is that by grounding all modalities in single common natural-language modality by way of an auxiliary loss, we can achieve joint reasoning over all modalities. By doing so, we enable our policy to perform challenging manipulation tasks that require reasoning jointly over vision, touch, and sound in zero-shot setting, enabling multimodal prompting, generation of object descriptions upon interaction, and compositional cross-modal prompting. In practice, our policy can successfully fulfill challenging task instructions, such as pick the red object that feels soft and makes loud sound, describe how the grasped object feels like, pick the object that has the same color as the button that plays piano. Our results show that policies leveraging pre-trained generalist robot policy finetuned on multimodal data consistently outperform baselines finetuned only on vision data, or trained from scratch on heterogeneous sensory data. We find that the same general recipe is applicable to generalist policies with widely different architectures, such as Octo [4], large transformer-based policy trained on the Open XEmbodiment [9] (OXE) dataset, and 3B VLA with PaliGemma [28] vision-language-model VLM backbone. For our experiments, we collect dataset consisting of 27K robot trajectories including vision, touch, audio, proprioception, and language instructions on three different real-world robotic manipulation tasks. To the best of our knowledge, this dataset is the first of its kind that also contains robot action data, which is key to perform physically grounded multimodal tasks. We open-source all of our data, code and models to support future research in this area. II. RELATED WORK A. Generalist Robot Policies Generalist robot policies have shown promise of consuming diverse large-scale data to unlock generalization in robotic tasks [4][8], [22], [29]. These policies leverage large robot dataset collections [9], [10], [30] that have recently been made available to the community, and are most often queried with language instructions defining the task. In some instances, robot actions are fused with vision-language model (VLM) backbone [5], [7], [22], [31], improving generalization due to pre-training on internet-scale data. However, while some of the recently introduced models [4], [8] can naturally process flexible observations, the scarcity of datasets that include other sensory modalities, such as touch or audio, limits their capabilities primarily to visual inputs. In contrast, our work shows how such capabilities can be enhanced with much smaller amount of robotic data containing additional heterogeneous modalities to allow jointly reasoning over modalities, such as vision, touch, and sound in zero-shot setting. B. Multimodal Reasoning in Robotics Multimodality aims to exploit complementarity across different sensors to enhance the capabilities of autonomous robot policies. Its advantages have repeatedly been shown in the literature, resulting either in improved performance [2], [3], [3], [25], [32][41], generalization [33], [42], or robustness [39], [43]. Despite this evidence, only minority of works employ sensor modalities in addition to vision and proprioception. This is reflected in the robotics datasets made available to the community. For example, the largest collection of robotics dataset, Open X-Embodiment [9] (OXE), does not include touch or sound as part of their default sensory modalities. Some notable exceptions include recent works [14], [24], [44] that try to align vision, language, and touch for perception tasks. However, most of the available datasets made available through these works do not include robot actions, limiting their applicability for policy training and to perform physically grounded multimodal tasks. Here, we first introduce multi-task dataset that includes vision, touch, audio, inertial measurements, proprioception, as well as robot actions and language instructions. We then leverage this dataset to finetune large generalist robot models, unlocking novel multimodal reasoning capabilities. III. FUSE FINETUNING State-of-the-art generalist robot policies typically rely on vision, language, and robot actions as training modalities, which limits their applicability on partially-observable scenes where tasks cannot be completed solely through vision. We propose recipe, FuSe, to Fuse heterogeneous Sensory data into generalist robot policies. Specifically, we finetune these policies to extend their semantic understanding to include additional sensing modalities, such as touch and sound, while retaining their pre-trained knowledge. By proposing two auxiliary losses, which contrast heterogeneous observations with natural language and generate language from observations, we are able to link variety of sensing modalities with the semantic knowledge of pre-trained generalist robot policies. We use Octo [4], transformer-based pre-trained policy, as the backbone model for the main experiments in this paper, but we also show that the same finetuning recipe is applicable to 3B vision-language-action model based on PaliGemma [28] VLM backbone. The training architecture is depicted in Figure 2. This finetuning strategy introduces three main challenges, namely: (i) the weights of the feature extractors (encoders) for the new modalities generally need to be effectively Fig. 2: Architecture: We finetune pre-trained generalist robot policies by tokenizing all heteregoneous sensing modalities and passing them though pre-trained transformer backbone. Crucially, we apply two auxiliary losses that help connect the semantic knowledge of pre-trained generalist policies with new heterogeneous modalities, such as touch and audio. Concretely, we apply both contrastive loss that aims to maximize mutual information between different views and semantics of the same scene, and language generation loss that predicts high-level semantics for each modality combination. learned from small dataset; (ii) the finetuned model empirically tends to predominantly rely on the pre-training modalities, ignoring the new sensors; (iii) novel cross-modal prompting capabilities rely on modality specific annotations, e.g., the object feels soft and squishy. We detail below the modifications required to address all of these challenges. Tactile encoder. To account for the small finetuning dataset size, we use pre-trained tactile encoder and finetune it together with the backbone Octo architecture. In particular, we use the TVL encoder [14], which was pre-trained via pairwise contrastive learning across vision, language, and tactile modalities. We feed all tactile images (two in our robot setup) separately through the same TVL encoder. Audio encoder. As the raw audio waveform is highly dimensional and noisy, we process the audio data to build spectrogram as reported in previous work [3], [45][47]. The spectrogram is then treated as regular image and fed through ResNet26 encoder [48]. Auxiliary losses. As aforementioned, naıve way of simply finetuning pre-trained generalist policies with meansquare-error (MSE) imitation loss LBC conditioned on additional sensor data, leads to the policy over-relying on its pre-training modalities and ignoring the new modalities. We overcome this limitation by introducing two additional losses that fully leverage multimodality and connect the semantic knowledge of pre-trained generalist policies with unseen sensor modalities: 1) Multimodal Contrastive Loss: We introduce loss that aims to align the various language instructions with the observations via CLIP-style contrastive learning [49]. At high level, it aims to maximize mutual information between different modalities and semantics of the same scene. Concretely, we build an observation embedding by feeding all modalities once more through the transformer and combining them via multi-head attention layer. We then compute CLIP-style loss for each possible instruction resulting from combining the different available modalities. These losses are finally averaged to form combined multimodal contrastive loss Lcontrast. 2) Multimodal Generative Loss: We design generative network that functions as an add-on head to the backbone model. In practice, for each possible modality combination, we build an observation embedding as above, and feed it through the generative head. Then, we compute an auxiliary cross-entropy loss Lgen by comparing the head output with the appropriate language instruction. We use single transformer as the generative head for all possible modality combinations, with modality tokens to distinguish between input modalities. The final loss is given by = LBC +βLgen +λLcontrast, where the contrastive loss and the generative loss are summed to the MSE action loss during training. Language Rephrasing. As discussed previously, crossmodal prompting capabilities require modality specific annotations, e.g., the object feels squishy and looks round. We annotate the robot trajectories we collect with heterogeneous sensors with after-the-fact language annotations. We annotate these trajectories with templated language that enables us to create augmentations based on multiple sensor inputs, the object feels squishy and is red or the object feels metallic and sounds clinking. However, at test time we would like the policy with free-form language. To users to instruct increase the range of possible input instructions, we augment the instructions in the dataset by querying large language model, ChatGPT [50], to generate rephrased versions of the original templates that preserve the original semantic Fig. 3: Visualization of the various sensor modalities on our WidowX robot. meaning. Implementation Details. We train all models for 50,000 steps on v5e-128 TPU pod with batch size of 1024. We use cosine learning rate scheduler with 2000 warmup steps, and peak value of 3104. Our language rephrasing buffer contains 20 different templates for each possible modality combination. We set β = 1 and λ = 1 for all experiments. IV. EXPERIMENTS In this section, we investigate the effectiveness of FuSe to finetune pre-trained generalist robot policies to include additional sensor modalities, while linking them to the policys pre-trained semantic knowledge. We answer the following questions: 1) Does FuSe help perform multimodal prompting tasks in zero-shot manner in partially observable environments? (Section IV-C) 2) Does FuSe enable multimodal prompting to discriminate between objects that would be ambiguous as described by single modality? (Section IV-D) 3) Can the multimodal capabilities of FuSe be applied to compositional reasoning tasks? (Section IV-E) 4) Are the proposed auxiliary cross-modal language grounding losses necessary to achieve high performance when finetuning FuSe? (Section IV-F) 5) Is FuSe applicable to different generalist robot policy architectures? (Section IV-G) A. Real Robot Setup and Training Data All our experiments feature WidowX 250 6-DoF robot arm. The robot is controlled via delta end-effector position commands at frequency of 5 Hz. The system is equipped with third-person view RGB camera, wrist RGB camera, two DIGIT tactile sensors at the gripper fingers, standard microphone, and 9-DoF IMU. We present experiments on three different tasks, which are described below. For the grasping scenarios, we evaluate on the 24 objects present in the training dataset, along with 32 unseen test objects; for the button tasks, we evaluate on the six buttons and 13 of the 18 distractors/grasping targets seen in the training dataset, as well as two unseen buttons and 12 unseen distractors. We visualize the training and test objects used in Figure 4. (a) Objects used for evaluation purposes. (b) Objects included in the training data. Fig. 4: Visualization of objects for real-world experiments, including objects seen (a) and unseen (b) in the multimodal finetuning dataset. Objects differ in shape, appearance, material, hardness, and surface properties. We evaluate each model on several different scenarios (e.g., different objects and distractors) for each of the tasks, by running the same scenario for 5 different rollouts. We collect dataset of 26,866 trajectories collected via teleoperation using Meta Quest 2 VR headset. Each trajectory is labeled with templated language instruction. The two grasping tasks (tabletop and shopping bag) feature visual, tactile, and action data, while the button pressing tasks also includes sound. Visual observations are recorded at resolution of 640x480, while DIGIT images at resolution of 320x240. We follow previous work and subtract static background image from the tactile observations to emphasize the deviation from the zero-deformation state and reduce systematic differences across DIGIT instances [2]. The audio observations comprise 1s of the most recent microphone samples, recorded at frequency of 44,100Hz. We visualize our robot sensory setup in Figure 3. B. Evaluation Tasks We design challenging suite of tasks, which focuses on testing the policies ability to reason jointly over vision, sound, and touch in zero-shot setting: Tabletop Grasping. We set up simple tabletop grasping scenario, where multiple objects are placed on tray and the task is to grasp the right object as prompted via text instruction (e.g., pick the carrot). Shopping Bag. This environment presents more complex grasping scenario, where objects are placed inside paper bag. This scenario generally features occlusions to thirdperson view camera, as well as results in poor lighting conditions for the wrist camera as soon as the gripper enters the bag. Thus, this represents an environment with partiallyobservable visual scenarios. Button Pressing. In this environment, we leverage the sound modality, featuring six sound-making buttons, each playing different sounds upon pressure. The goal is to press the right button depending on the prompt, which can present either visualor audio-related commands (e.g., press the red button, press the button that plays piano, etc.). We also devise two multimodal compositional reasoning tasks in the button-pressing setting, where the objective is either i) to grasp objects that share visual characteristics with one of the buttons (e.g., grab the object that has the same color as the button that plays piano), or ii) to press among the training buttons the one that plays the same sound as an unseen button (e.g., press the button that plays the same sound as the blue button). Fig. 5: FuSe performance on evaluation tasks compared against baselines. Our approach outperforms baselines trained from scratch or finetuned with vision only, especially on the shopping bag task, which presents partially observable visual scenarios. Lighter shades of color represent intermediate task success, i.e., object touched but not fully grasped. C. Finetuning Performance E. Compositional Capabilities We investigate the benefits of our multimodal finetuning recipe, which initializes the policy with the Octo generalist policy and is pre-trained on the large OXE robotics dataset [9]. First, we ask whether pre-training is necessary by comparing our models performance to model with the same architecture, but trained from scratch. The results in Figure 5 show large gap between the models, indicating that training Octo from scratch on our multimodal dataset without our finetuning recipe is challenging due to the limited size of the dataset. In contrast, our approach leverages the knowledge acquired during pre-training and can adapt to the new tasks and modalities with smaller amount of additional data. Finally, we compare against ResNet-based baseline, where language instructions are fed through FiLM conditioning [51] as in [52]. The smaller ResNet26 performs slightly better than training Octo from scratch, but still significantly underperforms our model on all three tasks. To validate the effect of the new modalities on finetuning performance, we compare with recipe that finetunes Octo only using the available pre-trained modalities, i.e., vision and action. The results in Figure 5 show how this baseline is competitive on the simpler tasks (tabletop and button pressing), but it is considerably inferior to our model on the bag task, where visual occlusions make visual features less discriminative when the gripper enters the shopping bag. D. Multimodal Prompting In addition to improving finetuning performance, our training recipe provides the model with additional multimodal capabilities, such as the possibility to provide multimodal prompt that can successfully discriminate objects based not only on visual features but also based on other modalities such as touch or sound. The evaluation prompts contain several instances where the task is to grab an object with an ambiguous description for one modality, but unique for another (e.g., grab the round object that feels squishy, where the scene presents both foam ball and crumpled paper ball). The results are shown in Table for the grasping tasks, on scenarios that present objects sharing the same visual and tactile features, respectively. This experiment demonstrates that our policy can incorporate multimodal instructions to improve over ambiguous descriptions. Finally, we showcase compositional capabilities of our model with two different compositional tasks in the button pressing environment: In simpler task, we prompt the model to grab an object that has the same color as the training button the plays certain sound (e.g., grab the object with the same color as the button that plays piano). In multi-step task, we exploit the generative head to connect between different subtasks. First, we prompt the model to press button not seen at training time, using only visual instructions (e.g., press the blue button). Then, we feed the resulting sound to the generative head, which will generate the instruction related to the corresponding audio (e.g., press the button that plays piano). Finally, we prompt the model with the audio instruction in the training environment, where the model has already associated the visual cues of the button to the corresponding sound, and will execute trajectory that ends up pressing the button that plays the same sound as the button pressed in the first subtask. We report quantitative results in Figure 6, showing that even on the simple compositional task, FuSe outperforms all baselines, exploiting its multimodal reasoning capabilities. For the multi-step task, we compare with Octo trained from scratch on all available sensors and with the same auxiliary losses. Once again, FuSe outperforms the baseline, particularly on the full task completion. In fact, the model trained from scratch shows poor language grounding and Visual Reach 0.43 0.3 0.37 Grasp 0.43 0.25 0.34 Visual, Tactile Grasp Reach 0.43 0.5 0.3 0.55 0.37 0. Tabletop Bag Average (a) Vision-ambiguous objects Tactile Reach 0.4 0.35 0.38 Grasp 0.4 0.3 0.35 Visual, Tactile Grasp Reach 0.4 0.4 0.3 0.5 0.35 0. Tabletop Bag Average (b) Touch-ambiguous objects TABLE I: Multimodal prompting results obtained with the FuSe policy on scenarios that present objects sharing the same visual (a) or tactile (b) features. Our policy incorporate multimodal instructions and improves over ambiguous descriptions. Fig. 6: Results on the compositional tasks devised in the button pressing environment. On the left, the instructions are of the type pick the object that has the same color as the button that play piano. On the right, the whole multi-step task is represented by an instruction like press the train button that plays the same sound as the blue button. does not succeed in fulfilling the audio-based instruction. F. Ablation Study We ablate the different FuSe auxiliary losses in the shopping bag task, which features partially observable visual scenarios. Figure 7 shows that including both losses is key to fully exploit the heterogeneous feedback available on the robot, with the performance particularly deteriorating for the baselines on unseen test objects. G. Vision-Language-Action Model Results We also investigate the effectiveness of FuSe to finetune alternative generalist policies based on off-the-shelf visionlanguage-action (VLA) models. Instead of Octo, we finetune 3B parameter vision-language model to get VLA model capable of producing both robot actions and language grounding. We use the PaliGemma [28] VLM as the backbone, but modify it to easily incorporate arbitrary observation modalities in similar fashion to Octo (but unlike other VLA models like OpenVLA [5]). Such models are also able to incorporate FuSes generative language modeling loss directly rather than requiring an additional language model head, unifying the implementation of action prediction and language-based feature learning. We first pre-train on the OXE dataset with only visual modalities [53], and finetune on our dataset with all sensor modalities. We note that OXE pretraining generally improves low-level grasping compared to using the original PaliGemmas weights. However, overtraining on OXE during the pre-training phrase appears to harm Fig. 7: We study the effect of the proposed losses in an ablation experiment in the shopping bag environment. Our model that includes both contrastive and language generative losses outperforms models trained with only one of the two auxiliary losses or neither. Fig. 8: Performance of PaliGemma FuSe 3B parameter VLA, trained on our multimodal dataset. Our policy achieves robust performance, showcasing the applicability of FuSe to widely different generalist policies. the policys language understanding. We choose checkpoint that pretrains on the OXE dataset for 50,000 steps before finetuning on our multimodal dataset. We show results in Figure 8, where the VLA FuSe policy is competitive with its Octo-based counterpart and outperforms it on the challenging shopping bag task, demonstrating the effectiveness of FuSe across different policy backbones. To our knowledge, VLA FuSe is the first open-source VLA finetuned on heterogeneous (non-visual) sensory inputs. V. CONCLUSIONS In this paper, we introduced FuSe, an approach to finetune large, pre-trained robot policies on heterogeneous robot sensor modalities, such as touch or audio, for which large datasets are not readily available. By leveraging natural language as common cross-modal grounding during training, FuSe enables performing challenging tasks that require reasoning jointly over modalities, such as vision, touch, and sound in zero-shot setting. FuSe enables capabilities such as multimodal prompting, compositional cross-modal prompting, and descriptions of objects it interacts with. We also demonstrate the effectiveness of our recipe (multimodal finetuning and feature learning via cross-modal language grounding) is applicable to widely different generalist policies, including transformer-based Octo model or policy finetuned from generative VLM base model pre-trained on internet-scale data as well as unimodal robot data. limitation of our approach is that training policy with additional modalities requires increasing training resources, which currently limits our observation history to 0.4s. Increasing training efficiency would enable training with longer context length, potentially leading to improved reasoning about sparse signals such as tactile data, and will be subject of future work."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This work was supported in part by the ONR Science of Autonomy Program N000142212121, the SNSF Postdoc Mobility Fellowship 211086, the BAIR Industrial Consortium, ONR N00014-20-1-2383 and the AI Institute. Pieter Abbeel holds concurrent appointments as Professor at UC Berkeley and as an Amazon Scholar. This paper describes work performed at UC Berkeley and is not associated with Amazon."
        },
        {
            "title": "REFERENCES",
            "content": "[1] R. S. Johansson and J. R. Flanagan, Coding and use of tactile signals from the fingertips in object manipulation tasks, Nature Reviews Neuroscience, vol. 10, no. 5, pp. 345359, 2009. [2] R. Calandra, A. Owens, D. Jayaraman, J. Lin, W. Yuan, J. Malik, E. H. Adelson, and S. Levine, More than feeling: Learning to grasp and regrasp using vision and touch, IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 33003307, 2018. [3] M. Du, O. Y. Lee, S. Nair, and C. Finn, Play it by ear: Learning skills amidst occlusion through audio-visual imitation learning, arXiv preprint arXiv:2205.14850, 2022. [4] Octo Model Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, C. Xu, J. Luo, T. Kreiman, Y. Tan, P. Sanketi, Q. Vuong, T. Xiao, D. Sadigh, C. Finn, and S. Levine, Octo: An open-source generalist robot policy, in Proceedings of Robotics: Science and Systems, Delft, Netherlands, 2024. [5] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi et al., Openvla: An open-source vision-language-action model, arXiv preprint arXiv:2406.09246, 2024. [6] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., Rt-1: Robotics transformer for real-world control at scale, arXiv preprint arXiv:2212.06817, 2022. [7] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn et al., Rt-2: Visionlanguage-action models transfer web knowledge to robotic control, arXiv preprint arXiv:2307.15818, 2023. [8] R. Doshi, H. Walke, O. Mees, S. Dasari, and S. Levine, Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation, in Conference on Robot Learning, 2024. [9] O. X.-E. Collaboration, A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan, A. Khazatsky, A. Rai, A. Singh, A. Brohan, A. Raffin, A. Wahid, B. Burgess-Limerick, B. Kim, B. Scholkopf, B. Ichter, C. Lu, C. Xu, C. Finn, C. Xu, C. Chi, C. Huang, C. Chan, C. Pan, C. Fu, C. Devin, D. Driess, D. Pathak, D. Shah, D. Buchler, D. Kalashnikov, D. Sadigh, E. Johns, F. Ceola, F. Xia, F. Stulp, G. Zhou, G. S. Sukhatme, G. Salhotra, G. Yan, G. Schiavi, H. Su, H.- S. Fang, H. Shi, H. B. Amor, H. I. Christensen, H. Furuta, H. Walke, H. Fang, I. Mordatch, I. Radosavovic, I. Leal, J. Liang, J. Kim, J. Schneider, J. Hsu, J. Bohg, J. Bingham, J. Wu, J. Wu, J. Luo, J. Gu, J. Tan, J. Oh, J. Malik, J. Tompson, J. Yang, J. J. Lim, J. Silverio, J. Han, K. Rao, K. Pertsch, K. Hausman, K. Go, K. Gopalakrishnan, K. Goldberg, K. Byrne, K. Oslund, K. Kawaharazuka, K. Zhang, K. Majd, K. Rana, K. Srinivasan, L. Y. Chen, L. Pinto, L. Tan, L. Ott, L. Lee, M. Tomizuka, M. Du, M. Ahn, M. Zhang, M. Ding, M. K. Srirama, M. Sharma, M. J. Kim, N. Kanazawa, N. Hansen, N. Heess, N. J. Joshi, N. Suenderhauf, N. D. Palo, N. M. M. Shafiullah, O. Mees, O. Kroemer, P. R. Sanketi, P. Wohlhart, P. Xu, P. Sermanet, P. Sundaresan, Q. Vuong, R. Rafailov, R. Tian, R. Doshi, R. MartınMartın, R. Mendonca, R. Shah, R. Hoque, R. Julian, S. Bustamante, S. Kirmani, S. Levine, S. Moore, S. Bahl, S. Dass, S. Song, S. Xu, S. Haldar, S. Adebola, S. Guist, S. Nasiriany, S. Schaal, S. Welker, S. Tian, S. Dasari, S. Belkhale, T. Osa, T. Harada, T. Matsushima, T. Xiao, T. Yu, T. Ding, T. Davchev, T. Z. Zhao, T. Armstrong, T. Darrell, V. Jain, V. Vanhoucke, W. Zhan, W. Zhou, W. Burgard, X. Chen, X. Wang, X. Zhu, X. Li, Y. Lu, Y. Chebotar, Y. Zhou, Y. Zhu, Y. Xu, Y. Wang, Y. Bisk, Y. Cho, Y. Lee, Y. Cui, Y. hua Wu, Y. Tang, Y. Zhu, Y. Li, Y. Iwasawa, Y. Matsuo, Z. Xu, and Z. J. Cui, Open X-Embodiment: Robotic learning datasets and RT-X models, in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, 2024. [10] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis et al., Droid: large-scale in-the-wild robot manipulation dataset, arXiv preprint arXiv:2403.12945, 2024. [11] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong, A. He, V. Myers, K. Fang, C. Finn, and S. Levine, Bridgedata v2: dataset for robot learning at scale, in Conference on Robot Learning (CoRL), 2023. [12] E. Rosete-Beas, O. Mees, G. Kalweit, J. Boedecker, and W. Burgard, Latent plans for task agnostic offline reinforcement learning, in Proceedings of the 6th Conference on Robot Learning (CoRL), Auckland, New Zealand, 2022. [13] O. Mees, J. Borja-Diaz, and W. Burgard, Grounding language with the visual affordances over unstructured data, in Proceedings of IEEE International Conference on Robotics and Automation (ICRA), London, UK, 2023. [14] L. Fu, G. Datta, H. Huang, W. C.-H. Panitch, J. Drake, J. Ortiz, M. Mukadam, M. Lambeta, R. Calandra, and K. Goldberg, touch, vision, and language dataset for multimodal alignment, in Forty-first International Conference on Machine Learning. [15] Z. Liu, C. Chi, E. Cousineau, N. Kuppuswamy, B. Burchfiel, and S. Song, Maniwav: Learning robot manipulation from in-the-wild audio-visual data, in 8th Annual Conference on Robot Learning. [16] N. Cheng, Y. Li, J. Gao, B. Fang, J. Xu, and W. Han, Towards comprehensive multimodal perception: Introducing the touch-languagevision dataset, arXiv preprint arXiv:2403.09813, 2024. [17] A. Zeng, M. Attarian, B. Ichter, K. Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani et al., Socratic models: Composing zero-shot multimodal reasoning with language, arXiv preprint arXiv:2204.00598, 2022. [18] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, Code as policies: Language model programs for embodied control, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 94939500. [19] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman et al., Do as can, not as say: Grounding language in robotic affordances, arXiv preprint arXiv:2204.01691, 2022. [20] C. Huang, O. Mees, A. Zeng, and W. Burgard, Visual language maps for robot navigation, in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), London, UK, 2023. [21] , Audio visual language maps for robot navigation, in Proceedings of the International Symposium on Experimental Robotics (ISER), Chiang Mai, Thailand, 2023. [22] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu et al., Palm-e: An embodied multimodal language model, arXiv preprint arXiv:2303.03378, 2023. [23] H. Zhou, X. Yao, O. Mees, Y. Meng, T. Xiao, Y. Bisk, J. Oh, E. Johns, M. Shridhar, D. Shah, J. Thomason, K. Huang, J. Chai, Z. Bing, and A. Knoll, Bridging language and action: survey of language-based robot manipulation, arXiv preprint arXiv:2312.10807, 2024. [24] F. Yang, C. Feng, Z. Chen, H. Park, D. Wang, Y. Dou, Z. Zeng, X. Chen, R. Gangopadhyay, A. Owens et al., Binding touch to everything: Learning unified multimodal tactile representations, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 26 34026 353. [25] R. Calandra, A. Owens, M. Upadhyaya, W. Yuan, J. Lin, E. H. Adelson, and S. Levine, The feeling of success: Does touch sensing help predict grasp outcomes? in Conference on Robot Learning. PMLR, 2017, pp. 314323. [26] T. Bi, C. Sferrazza, and R. DAndrea, Zero-shot sim-to-real transfer of tactile control policies for aggressive swing-up manipulation, IEEE Robotics and Automation Letters, vol. 6, no. 3, pp. 57615768, 2021. [27] C. Wang, S. Wang, B. Romero, F. Veiga, and E. Adelson, Swingbot: Learning physical features from in-hand tactile exploration for dynamic swing-up manipulation, in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 56335640. [28] L. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello et al., Paligemma: versatile 3b vlm for transfer, arXiv preprint arXiv:2407.07726, 2024. [29] H. Etukuru, N. Naka, Z. Hu, S. Lee, J. Mehu, A. Edsinger, C. Paxton, S. Chintala, L. Pinto, and N. M. M. Shafiullah, Robot utility models: General policies for zero-shot deployment in new environments, arXiv preprint arXiv:2409.05865, 2024. [30] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn, Robonet: Large-scale multi-robot learning, arXiv preprint arXiv:1910.11215, 2019. [31] M. Zawalski, W. Chen, K. Pertsch, O. Mees, C. Finn, and S. Levine, Robotic control via embodied chain-of-thought reasoning, in Conference on Robot Learning, 2024. [32] H. Qi, B. Yi, S. Suresh, M. Lambeta, Y. Ma, R. Calandra, and J. Malik, General in-hand object rotation with vision and touch, in Conference on Robot Learning. PMLR, 2023, pp. 25492564. [33] C. Sferrazza, Y. Seo, H. Liu, Y. Lee, and P. Abbeel, The power of the senses: Generalizable manipulation from vision and touch through masked multimodal learning, arXiv preprint arXiv:2311.00924, 2023. [34] O. Mees, A. Eitel, and W. Burgard, Choosing smartly: Adaptive multimodal fusion for object detection in changing environments, in Proceedings of the International Conference on Intelligent Robots and Systems (IROS), Daejeon, South Korea, 2016. [35] J. Mejia, V. Dean, T. Hellebrekers, and A. Gupta, Hearing touch: Audio-visual pretraining for contact-rich manipulation, arXiv preprint arXiv:2405.08576, 2024. [36] H. Li, Y. Zhang, J. Zhu, S. Wang, M. A. Lee, H. Xu, E. Adelson, L. Fei-Fei, R. Gao, and J. Wu, See, hear, and feel: Smart sensory fusion for robotic manipulation, arXiv preprint arXiv:2212.03858, 2022. [37] Y. Yuan, H. Che, Y. Qin, B. Huang, Z.-H. Yin, K.-W. Lee, Y. Wu, S.-C. Lim, and X. Wang, Robot synesthesia: In-hand manipulation with visuotactile sensing, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 65586565. [38] Y. Li, J.-Y. Zhu, R. Tedrake, and A. Torralba, Connecting touch and vision via cross-modal prediction, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 10 60910 618. [39] M. A. Lee, Y. Zhu, P. Zachares, M. Tan, K. Srinivasan, S. Savarese, L. Fei-Fei, A. Garg, and J. Bohg, Making sense of vision and touch: Learning multimodal representations for contact-rich tasks, IEEE Transactions on Robotics, vol. 36, no. 3, pp. 582596, 2020. [40] I. Guzey, Y. Dai, B. Evans, S. Chintala, and L. Pinto, See to touch: Learning tactile dexterity through visual incentives, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 13 82513 832. [41] Y. Tian, D. Krishnan, and P. Isola, Contrastive multiview coding, in Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16. Springer, 2020, pp. 776794. [42] T. Lin, Y. Zhang, Q. Li, H. Qi, B. Yi, S. Levine, and J. Malik, Learning visuotactile skills with two multifingered hands, arXiv preprint arXiv:2404.16823, 2024. [43] P. Miller and P. Leibowitz, Integration of vision, force and tactile sensing for grasping, Int. J. Intell. Mach, vol. 4, pp. 129149, 1999. [44] S. Yu, K. Lin, A. Xiao, J. Duan, and H. Soh, Octopi: Object property reasoning with large tactile-language models, arXiv preprint arXiv:2405.02794, 2024. [45] A. Guzhov, F. Raue, J. Hees, and A. Dengel, Esresne (x) t-fbsp: time-frequency transformation of audio, in 2021 IEEE, Learning robust International Joint Conference on Neural Networks (IJCNN). 2021, pp. 18. [46] , Audioclip: Extending clip to image, text and audio, in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 976980. [47] Y. Gong, Y.-A. Chung, and J. Glass, Ast: Audio spectrogram transformer, arXiv preprint arXiv:2104.01778, 2021. [48] A. Dosovitskiy, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [49] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PMLR, 2021, pp. 87488763. [50] J. Schulman, B. Zoph, C. Kim, J. Hilton, J. Menick, J. Weng, J. F. C. Uribe, L. Fedus, L. Metz, M. Pokorny et al., Chatgpt: Optimizing language models for dialogue, OpenAI blog, vol. 2, no. 4, 2022. [51] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, Film: Visual reasoning with general conditioning layer, in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, 2018. [52] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, Bc-z: Zero-shot task generalization with robotic imitation learning, in Conference on Robot Learning. PMLR, 2022, pp. 9911002. [53] K. Stachowicz, PaliVLA, 2024, GitHub repository. [Online]. Available: https://github.com/kylestach/bigvision-palivla"
        }
    ],
    "affiliations": [
        "Berkeley AI Research (BAIR), UC Berkeley, USA"
    ]
}