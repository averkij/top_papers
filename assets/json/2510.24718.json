{
    "paper_title": "Generative View Stitching",
    "authors": [
        "Chonghyuk Song",
        "Michal Stary",
        "Boyuan Chen",
        "George Kopanas",
        "Vincent Sitzmann"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 1 7 4 2 . 0 1 5 2 : r a"
        },
        {
            "title": "GENERATIVE VIEW STITCHING",
            "content": "Chonghyuk Song1 Michal Stary1 Boyuan Chen1 George Kopanas2 Vincent Sitzmann1 1MIT CSAIL 2RunwayML"
        },
        {
            "title": "ABSTRACT",
            "content": "Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for variety of predefined camera paths, including Oscar Reutersvards Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent developments in diffusion models have significantly advanced video synthesis. However, current video diffusion models generate videos with limited context: most models (Kong et al., 2024; Luma AI Team, 2024; Google DeepMind, 2025; Wan et al., 2025; Runway, 2025) generate videos between 5 and 10 seconds. This is often less than what users want to generate. However, training models with longer context is costly, making methods that can extrapolate pretrained models to generate videos longer than the training context length an attractive prospect. Existing approaches typically involve rolling out short-horizon model autoregressively; they demonstrate temporal coherence and stability over the course of hundreds of frames (Chen et al., 2024; Song et al., 2025). Autoregressive (AR) sampling can be extended with retrieval-based techniques to enable consistency with respect to the distant past (Zhou et al., 2025; Xiao et al., 2025; Yu et al., 2025; Cai et al., 2025). However, AR sampling does not enable consistency with respect to the future, i.e., beyond the current context window: it is not possible to guide the generation to synthesize future goal frame or for current generations to be consistent with future conditioning variables. This arises in applications such as one-shot cinematography (Failes, 2020; Yates, 2023) and synthetic scenario generation for autonomous driving (Hu et al., 2023; Russell et al., 2025), which entail camera-guided video generation with predefined camera trajectory. In this task setting, AR sampling may generate wall that it is later forced to step through due to its inability to plan ahead. This often results in video frames that are out-of-distribution to the model, after which generation quickly collapses (see Fig. 1). To address this, we explore non-autoregressive sampling approach to video length extrapolation, which respects future conditioning signals when generating current frames. We are inspired by previous work in diffusion stitching (Liu et al., 2022; Bar-Tal et al., 2023; Mishra et al., 2023; Kim et al., 2024; Yeo et al., 2025; Goli et al., 2025; Luo et al., 2025), which are sampling methods that generate the entire sequence in parallel, by dividing the sequence into overlapping segments and intertwining their generation processes for consistent connections. Despite their potential for Work done as visiting student at MIT."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Generative View Stitching (GVS) enables stable camera-guided generation of long videos. Given pretrained DFoT video model (Song et al., 2025) with an 8-frame context window and predefined camera trajectory, GVS can generate 120-frame navigation video that is stable, collision-free, faithful to the conditioning trajectory, consistent, and closes loops. On the other hand, Autoregressive sampling diverges due to collisions with the generated scene, is not faithful to the conditioning trajectory, and demonstrates poor loop closure even when augmented with RAG. video length extrapolation, existing stitching methods are not suitable for video generation. For example, StochSync (Yeo et al., 2025), originally designed for generating images such as 360-degree panoramas and 3D mesh textures, lacks the temporal consistency necessary for video generation, as we later show in Sec. 4.1; CompDiffuser (Luo et al., 2025) requires sequence diffusion model specially trained for stitching, but training such custom model for video is costly. Based on these observations, we propose Generative View Stitching (GVS), the first stitching method for camera-guided video generation. GVS is training-free stitching approach that is designed to be compatible with any off-the-shelf video model trained with Diffusion Forcing (Chen et al., 2024), prevalent framework for training sequence diffusion models (Decart et al., 2024; Yin et al., 2025; Song et al., 2025; Chen et al., 2025a; Sand.ai et al., 2025). We then introduce Omni Guidance, which enhances the temporal consistency in stitching by strengthening the conditioning on the past and future and, in turn, enables our proposed loop-closing mechanism for long-range consistency. Our stitching method achieves long-horizon camera-guided video generation that is stable, collision-free, and consistent across both shortand long-term time horizons for variety of predefined camera paths, including Oscar Reutersvards Impossible Staircase (Penrose & Penrose, 1958) (see Fig. 7)."
        },
        {
            "title": "2 RELATED WORK AND PRELIMINARIES",
            "content": "Diffusion models. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021b) are generative models defined by forward process that iteratively corrupts data sample from target distribution into white noise through series of noising steps xk = 1 αkϵ, where {0, 1, . . . , 1} are increasing noise levels, {αk}K1 k=0 is predefined schedule, and ϵ (0, I). The goal of diffusion modeling is to reverse the forward process by learning the score function ϵθ(xk, k), which enables iterative denoising of white noise into sample from the target distribution via: αkx0 + (cid:32) (cid:33) xk 1 αkϵθ(xk, k) (cid:113) + 1 αk1 (σk)2 ϵθ(xk, k) + σkϵ (1) xk1 = αk1 αk (Song et al., 2021a), where σk controls the level of stochasticity i.e., the amount of random noise ϵ (0, I) injected into each denoising step, concept that will become important later on. Long Video Generation. Video diffusion models often fall short in terms of content resolution, especially temporal resolution, resulting in videos only 5 to 10 seconds long (Kong et al., 2024; Luma AI Team, 2024; Google DeepMind, 2025; Wan et al., 2025; Runway, 2025). This is because the typical diffusion model architecture (Peebles & Xie, 2023) uses attention layers that scale quadratically with the number of tokens. One way to avoid this is to retrieve and attend only to select number of tokens that are relevant for generating each frame (Xiao et al., 2025; Yu et al., 2025; Cai et al., 2025). Alternatively, history can be compressed into hidden state (Dalal et al., 2025; Zhang et al., 2025). Training models with large context is an exciting direction, and test-time stitching methods like GVS can piggy-back and extrapolate to even longer sequences by using such backbone models. This"
        },
        {
            "title": "Preprint",
            "content": "highlights an important property of GVS, which modifies only the sampling method and does not require specialized model architectures or training paradigms. Diffusion Forcing and Autoregressive Extension. Diffusion Forcing (DF) (Chen et al., 2024) is prevalent framework for training sequence diffusion models (Decart et al., 2024; Yin et al., 2025; Song et al., 2025; Chen et al., 2025a; Sand.ai et al., 2025). DF trains sequence diffusion models with independent noise levels per token. During sampling, DF models can then selectively mask portions of their context window with noise, enabling conditioning on variable number of context tokens, also referred to as history. DF also gives rise to history guidance (Song et al., 2025), which enables ultra-long camera-guided autoregressive video generation that is stable and consistent. However, the long rollouts presented in these works are only possible because of real-time, user-controlled camera trajectories that prevent collisions with generated scene elements. Autoregressive sampling according to predefined camera trajectory leads to collisions of the camera with the generated scene as the model generates content unaware of the future camera trajectory (see Fig. 1 and Fig. 6). We present stitching-based alternative that (1) is compatible with any DF model and (2) generates videos that are faithful to the full camera trajectory and are thus collision-free and stable."
        },
        {
            "title": "3 GENERATIVE VIEW STITCHING",
            "content": "We motivate and describe the key components of Generative View Stitching (GVS), diffusion stitching method for camera-guided video generation that overcomes the shortcomings of autoregressive sampling. First, we review recent stitching method CompDiffuser (Luo et al., 2025) and its requirement of custom-trained model (Sec. 3.1). We then make the key observation that any video model trained with Diffusion Forcing (DF) already has the necessary affordances to support stitching, and introduce corresponding training-free stitching method (Sec. 3.2). We overcome issues with temporal consistency by combining maximum stochasticity (Yeo et al., 2025), which itself is insufficient (Sec. 3.3), with our novel Omni Guidance, which enhances temporal consistency by strengthening the conditioning on the past and future (Sec. 3.4). Omni Guidance further enables our loop closing mechanism for delivering long-range coherence (Sec. 3.5). 3.1 CHALLENGES IN EXTENDING DIFFUSION STITCHING TO VIDEO GENERATION Diffusion stitching methods (Liu et al., 2022; Bar-Tal et al., 2023; Mishra et al., 2023; Kim et al., 2024; Yeo et al., 2025; Goli et al., 2025; Luo et al., 2025) are sampling methods that enable compositional generalization beyond the context window of the backbone diffusion model. These methods typically involve dividing the target sequence into overlapping chunks {xt}T 1 t=0 and intertwining their denoising processes by synchronizing their intermediate outputs. In particular, CompDiffuser (Luo et al., 2025), stitching method designed for goal-conditioned planning, models each target chunk xt to be dependent only on its temporal neighbors xt1, xt+1, resulting in the following compositional trajectory distribution: pθ(xxstart, xgoal) p0(x0xstart, x1)pT 1(xT 1xT 2, xgoal) 2 (cid:89) t= pt(xtxt1, xt+1), (2) where xstart and xgoal denote the predefined start and goal states, respectively. However, to realize this, CompDiffuser must train customized denoising network ϵθ(xk t+1) that generates target chunk conditioned on its co-evolving, noisy neighboring chunks. These conditioning chunks are treated as separate conditioning inputs, which are embedded via special encoder and then injected to the backbone model via Adaptive LayerNorm. This need for custom model prevents the application of CompDiffuser to off-the-shelf models, and hence makes it infeasible to use for video where training custom model would incur an unacceptable cost. t1, xk , kxk 3.2 GENERAL-PURPOSE VIDEO MODELS ALREADY ENABLE STITCHING We seek to design training-free stitching method that is compatible with off-the-shelf video models. We find that remarkably, widely used training framework, Diffusion Forcing (DF), already provides all the necessary features for stitching. Like CompDiffuser, we represent the distribution of cameraguided videos compositionally: pθ(xp) 1 (cid:89) t=0 pt(xtxt1, xt+1, pt1, pt, pt+1), (3)"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Generative View Stitching (GVS) is training-free diffusion stitching method that is compatible with any off-the-shelf video model trained with Diffusion Forcing (DF). We first partition the target video into non-overlapping chunks shorter than the models context window, then denoise every target chunk jointly with its neighboring chunks to condition on both the past and future. We use the denoised target chunk of every context window to update the noisy stitched sequence while discarding the denoised past and future conditioning chunks. We further enable Omni Guidance (Sec. 3.4), which enhances temporal consistency, by replacing the original score function ϵθ with the guided score function ϵθ in Eq. 8. t1, xk where is the predefined camera trajectory, p1 p0 and pT pT 1, and x1 and xT are pure-noise frames for padding. Unless otherwise stated, we omit the camera trajectory for the sake of brevity. CompDiffuser requires custom backbone model for stitching as it processes neighboring chunks via specialized conditioning path. We instead propose to condition the target chunk xk on its temporal neighbors xk t+1 by jointly denoising them as part of the same input sequence to the model [xk t1, xk t+1], as shown in Fig. 2. We partition the target video into non-overlapping chunks {xt}T 1 t=0 that are shorter than the context window, thereby freeing up space to jointly denoise the conditioning chunks. In practice, only portion of the neighboring chunks fit into the context window. In the model output, the denoised target chunk xk1 is used to update the noisy estimate of the stitched sequence while the denoised conditioning chunks xk1 t+1 are discarded. This stitching procedure, which we refer to as vanilla GVS, is compatible with any DF video model, which is designed for such joint denoising of conditioning and target signals (Song et al., 2025). t1 , xk1 , xk Although vanilla GVS is simple to implement, it achieves poor temporal consistency, as shown in the top row of Fig. 3. We hypothesize that this is because vanilla GVS denoises the target chunk xk using the score function of the joint distribution p(xt1, xt, xt+1) rather than that of the originally intended conditional distribution p(xtxt1, xt+1) in Eq. 3. While in autoregressive sampling, target frames are generally conditioned on past context that is significantly less noisy, vanilla GVS requires that the target chunk xk t+1, leading to weak conditioning signal. is equally noisy as its conditioning neighbors xk t1, xk 3.3 STOCHASTICITY IS NECESSARY BUT INSUFFICIENT FOR CONSISTENCY Prior stitching work StochSync (Yeo et al., 2025) proposes stochasticity as mechanism for enhancing consistency. It introduces maximum stochasticity σk = 1 αk1, which acts as an error correction mechanism by eliminating the predicted noise term and maximizing the random noise term in the denoising equation in Eq. 1. We find that maximum stochasticity also benefits vanilla GVS in terms of temporal consistency (see Table 2) but often results in oversmoothed generations, as shown in Fig. 3, aligning with observations made in prior works (Karras et al., 2022; Yeo et al., 2025). In this case, maximum stochasticity simplifies the task of consistency by oversmoothing the generations."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Effect of Omni Guidance and Stochasticity. Without Omni Guidance and zero stochasticity (η = 0), the generations lack temporal consistency and instead exhibit hazy transitions between different scenes. Increasing stochasticity to its maximum (η = 1.0) enhances consistency but leads to oversmoothing. Our full method with Omni Guidance and partial stochasticity (η = 0.9) enables consistent generation without oversmoothing. 3.4 OMNI GUIDANCE ENHANCES CONSISTENCY To address the shortcomings of stochasticity, we propose more direct way of enhancing temporal consistency: Omni Guidance, guidance technique that aims to steer the score function of the original joint distribution towards that of the desired conditional distribution by strengthening the conditioning on the past and future. One difficulty is that stitching breaks core assumption of standard classifier-free guidance (Ho & Salimans, 2022), which is that the conditioning signal is independent of the model weights. In GVS, the guidance signal for target chunk xk comes from the backbone models own co-evolving, noisy estimate of its temporal neighbors xk t+1, making the guidance signal dependent on the model weights. To address this, we draw inspiration from Inner Guidance (Chefer et al., 2025) and directly modify the original sampling distribution pθ(xk t1:t+1pt1:t+1) for target chunk xt to be consistent with its temporal neighbors (and the predefined camera trajectory): t1, xk t1:t+1pt1:t+1) pθ(xk pθ(xk t1:t+1pt1:t+1)pθ(pt1:t+1xk t1:t+1)γ1pθ(xk pθ(xk t1:t+1pt1:t+1) (cid:20) pθ(xk t1:t+1pt1:t+1) pθ(xk t1:t+1) t1, xk (cid:21)γ1(cid:20) pθ(xk , pt1:t+1)γ2 (cid:21)γ2 t+1xk t1:t+1pt1:t+1) pt1:t+1) t1:t+1pt1:t+1) as follows: pθ(xk (4) (5) (6) This corresponds to modifying the original score function ϵθ(xk ϵθ = (1 + γ1 + γ2)ϵθ(xk , pt1:t+1), (7) where denotes the null condition and guidance scales γ1 and γ2 modulate the adherence to the predefined camera trajectory and consistency of the target chunk with its temporal neighbors, respectively. In practice, we merge the guidance terms to be modulated by single γ to obtain: t1:t+1pt1:t+1) γ1ϵθ(xk t1:t+1, , ) γ2ϵθ(, xk ϵθ = (1 + γ)ϵθ(xk t1:t+1pt1:t+1) γϵθ(, xk (8) The guidance term ϵθ(, xk , , , ) is computed by replacing the noisy neighboring chunks with pure Gaussian noise and setting their noise levels to be maximum, enabled by relying on the Diffusion Forcing backbone (Chen et al., 2024). This can be seen as generalization of Fractional History Guidance (Song et al., 2025), with the key difference that the noise levels of the conditioning neighboring chunks, change throughout the stitching process while they remain fixed in historyguided autoregressive sampling. , , , ). As we show in Sec. 4.2, Omni Guidance enhances temporal consistency across wide range of stochasticity levels and thus provides extra affordance for reducing oversmoothing. Specifically,"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: GVS Requires Explicit Loop Closing. Despite its global theoretical receptive field, our stitching method requires an explicit loop closing mechanism to visually return to the same place. Note that the camera centers are offset from the panoramas rotation center purely for visual clarity. Figure 5: Loop Closing via Cyclic Conditioning. GVS closes loops via cyclic conditioning, whereby target chunks are denoised by two alternating sets of context windows: temporal windows, which condition target chunks on their temporally neighboring chunks, and spatial windows, which condition target chunks on temporally distant but spatially close neighboring chunks. As result, target chunks are conditioned on all relevant neighbors across the entire stitching process. See Fig. 8 for the full set of spatial windows. Omni Guidance enables partial stochasticity σk = η in tandem reduce oversmoothing while maintaining similar levels of consistency. 1 αk1, where η (0, 1), the two of which 3.5 LONG-RANGE CONSISTENCY VIA CYCLIC CONDITIONING In theory, our stitching method described in Sec. 3.2 through 3.4 has global context as the theoretical receptive field of each segment grows with every denoising step, somewhat analogous to the growing receptive field along the depth of CNN (Luo et al., 2017). Therefore, one could reasonably expect GVS to enable zero-shot loop closure, form of global consistency. In practice, we observe that while GVS significantly improves temporal consistency, it does not enforce it globally. Fig. 4 demonstrates that very long generations do not visually return to the same place, suggesting that information does not propagate as widely across the stitched video as necessary. To enable loop closures, we propose adding more factors to the compositional distribution in Eq. 3 by denoising additional diffusion windows that contain temporally distant but spatially close chunks jointly with the original set of diffusion windows. For example, as shown in Fig. 5, the chunk containing the frame at the end of the Panorama 1-loop trajectory is denoised by two diffusion windows: one that conditions the target chunk on its temporal neighbors (temporal window) and one on its spatial neighbors (spatial window). We propose to alternate between these two sets of context windows at every denoising step, procedure we call cyclic conditioning. As result, the generation of the target chunk is conditioned on both of its spatial and temporal neighbors over the course of the entire denoising process, resulting in successful loop closure. We visualize the full set of spatial windows for each trajectory in Fig. 8. Note that some target chunks do not have spatial neighbors and are denoised only by temporal windows."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We empirically evaluate GVS as stitching method for camera-guided video generation and as an alternative to autoregressive sampling for video length extrapolation. Further experimental details and results can be found in Appendix and and video results on our project page."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Qualitative Comparison with Baselines. Autoregressive sampling collides with the generated scene, fails to dream up the desired staircase, and does last-minute loop closure, resulting in discontinuities in scene appearance. StochSync performs better at these tasks, but it generates shape-shifting scenes that lack temporal consistency. GVS, on the other hand, avoids collisions, generates the desired staircase, and closes loops, all the while maintaining temporal consistency. Benchmarks. To evaluate long-horizon, camera-guided video generation against predefined camera trajectory, we create dataset of challenging conditioning trajectories, which are listed in Table 1. These camera trajectories are designed to test various video model capabilities, including video length extrapolation, loop closures, and collision avoidance. Baselines. 1) History-Guided Autoregressive (AR) Sampling (Song et al., 2025): an autoregressive extension method for ultra-long video generation. 2) StochSync (Yeo et al., 2025): diffusion stitching method for panorama generation and 3D mesh texturing. All sampling methods, including ours, are evaluated using the same camera-conditioned video model open-sourced by Song et al. (2025), Diffusion-Forcing Transformer model trained on the RealEstate10K dataset (Zhou et al., 2018) with an 8-frame-long context window. For conditioning trajectories that require loop closing, we augment Autoregressive Sampling with memory mechanism built on field-of-view-based retrieval (Zhou et al., 2025; Xiao et al., 2025) and StochSync with our proposed loop-closing mechanism. Metrics. To evaluate the frame-to-frame consistency (F2FC) of camera-guided video generation, we use MEt3R cosine (Asim et al., 2025) averaged over every pair of consecutive frames. We measure long-range consistency (LRC) also with MEt3R cosine (Asim et al., 2025), now averaged over pairs of frames that are temporally distant but deemed to be spatially close based on the field-of-view overlap of their conditioning cameras. We use the collision detection mechanism in (Schneider et al., 2025) to evaluate collision avoidance (CA), whereby collision is claimed if the inferred metric video depth for any frame (Chen et al., 2025b) falls below threshold. Finally, we evaluate video frame quality using the imaging quality (IQ) and aesthetic quality (AQ) metric proposed in VBench (Huang et al., 2024) and we use the inception score (IS) for certain ablations (Salimans et al., 2016). 4.1 COMPARISON WITH BASELINES As shown in Table 1, our method outperforms both baselines in terms of temporal (frame-to-frame) consistency, long-range (loop) consistency, and collision avoidance, while demonstrating comparable video generation quality. These quantitative results are corroborated in Fig. 6, which demonstrates AR samplings inability to plan and take future conditioning into account: on the Straight Line benchmark, AR sampling collides with the generated scene, after which generations quickly collapse (see frames in green). More notably, on Stairs, AR sampling often fails to dream up the desired staircase and instead collides with the ceiling. On Panorama 2-loop, AR sampling demonstrates loop closing abilities (compare frames in purple) but often loop-closes at the last-minute, stitching together visually inconsistent scenes to return to the same place (compare frames in red). StochSync avoids collisions and generates the desired staircase on Straight Line and Stairs, but it does so by shape-shifting the scene and compromising on temporal consistency. On Panorama 2-loop, StochSync also demonstrates loop closing, but often fails to reconcile high-frequency details, which is reflected in Table 1. GVS, on the other hand, generates samples that are stable, faithful to the"
        },
        {
            "title": "Preprint",
            "content": "Trajectory Autoregressive StochSync Ours (GVS) F2FC() LRC() IQ() AQ() CA() F2FC() LRC() IQ() AQ() CA() F2FC() LRC() IQ() AQ() CA() Panorama 1-loop 0.168 0.339 0.458 0.409 N/A 0.183 0.164 0.515 0.489 N/A 0.138 0.141 0.537 0.461 N/A Panorama 2-loop 0.169 0.171 0.460 0.422 N/A 0.259 0.279 0.500 0.470 N/A 0.155 0.116 0.483 0.376 N/A Circle 1-loop 0.220 0.411 0.432 0.377 0.625 0.204 0.258 0.546 0.459 0 0.160 0.244 0.546 0.432 Circle 2-loop 0.207 0.280 0.459 0.387 0.775 0.252 0.305 0.488 0.419 0 0.182 0.206 0.465 0.358 0 Straight line 0.138 N/A 0.456 0.365 0.325 0.124 N/A 0.544 0.409 0.080 N/A 0.615 0.423 0 Stairs 0.166 N/A 0.513 0.345 0.075 0.204 N/A 0.571 0.417 0 0.139 N/A 0.635 0. Staircase circuit 0.132 0.449 0.397 0.329 0.625 0.179 0.221 0.563 0.438 0 0.129 0.176 0.607 0.419 0 0 Table 1: Comparison with baselines on camera-guided video generation. Our method outperforms both baselines in terms of temporal consistency (F2FC), long-range consistency (LRC), and collision avoidance (CA), while demonstrating comparable video quality (IQ, AQ). Note that while StochSync has zero collisions on paper, it achieves this by shape-shifting the scene, as reflected in its poor temporal consistency. We display results averaged over 40 generations. Straight line Stairs η η Straight line Stairs F2FC() IQ() AQ() IS() F2FC() IQ() AQ() IS() F2FC() IQ() AQ() IS() F2FC() IQ() AQ() IS() 0 0.153 0.537 0.420 2.17 0.201 0.550 0.392 1. 0 0.138 0.553 0.455 2.43 0.177 0.566 0.409 2. 0.5 0.124 0.499 0.407 1.76 0. 0.540 0.397 1.48 0.5 0.110 0.556 0.463 2.06 0.160 0.578 0.419 1. 0.9 0.084 0.458 0.400 1.40 0. 0.539 0.381 1.51 0.9 0.080 0.615 0.423 1. 0.137 0.621 0.401 1.66 1.0 0.061 0.422 0.407 1.54 0. 0.500 0.396 1.40 1.0 0.071 0.610 0.431 1.53 0.130 0. 0.404 1.67 (a) without Omni Guidance (b) with Omni Guidance Table 2: Ablation on Omni Guidance and Stochasticity. Without Omni Guidance (a), increasing stochasticity consistently improves temporal consistency (F2FC) but often results in oversmoothing, which is reflected in the general decline of video quality metrics (IQ, AQ, IS). Omni Guidance (b) complements stochasticity by enhancing consistency across wide range of stochasticity levels, providing our method extra affordance to reduce oversmoothing. conditioning camera trajectory (i.e., avoids collisions and generates the desired staircase), temporally consistent, and visually close loops. Please find more baseline comparisons in Fig. 14, 15, and 16. 4.2 ABLATIONS Omni Guidance and Stochasticity. In Table 2 and Fig. 3, we demonstrate the effectiveness of Omni Guidance and how it complements stochasticity. Table 2 (a) shows that increasing stochasticity consistently improves temporal consistency. However, this comes at the cost of oversmoothed generations, as shown by the second row of Fig. 3 and the general decline in video quality metrics in Table 2 (a). In Table 2 (b), Omni Guidance enhances temporal consistency across wide range of stochasticity levels, providing GVS additional flexibility to reduce oversmoothing; going from maximum stochasticity (η = 1) to partial stochasticity (0 < η < 1) reduces oversmoothing but worsens consistency, which can be compensated for by adding Omni Guidance (compare rows 2 and 3 in Fig. 3 and compare row 4 of Table 2 (a) and row 3 of Table 2 (b), respectively). Note that for Straight Line adding Omni Guidance at maximum stochasticity η = 1 actually hurts temporal consistency, on paper. This is because oversmoothing is so severe that adding Omni Guidance, which generally increases scene complexity, makes the task of consistency harder one. Loop Closure and Omni Guidance. In Table 3 and Fig. 4, we highlight the need for an explicit loop closing mechanism in stitching and illustrate how Omni Guidance bolsters loop closing. As shown in Fig. 4, without our proposed loop closing mechanism our method fails to visually return to the same place, which suggests that the effective receptive field of GVS is not global. This finding is corroborated in Table 3, which shows that even Omni Guidance cannot make up for the absence of explicit loop closing. When our loop closing mechanism is activated, however, adding Omni"
        },
        {
            "title": "Preprint",
            "content": "Loop Closing Omni Guidance Panorama 1-loop Panorama 2-loop F2FC() LRC() IQ() AQ() F2FC() LRC() IQ() AQ() 0.137 0. 0.442 0.423 0.137 0.917 0.442 0.414 0. 0.962 0.554 0.463 0.140 0.917 0.546 0.461 0.138 0.201 0.430 0. 0.166 0.133 0.402 0.355 0.138 0.141 0.537 0. 0.155 0.116 0.483 0.376 Table 3: Ablation on Loop Closing and Omni Guidance. Without an explicit loop closing mechanism, our method fails to display long-range consistency (LRC), even with the help of Omni Guidance. Activating our loop closing mechanism significantly improves long-range consistency, which can be further bolstered by Omni Guidance. Figure 7: GVS can visually navigate through the Impossible Staircase. GVS can generate 120-frame navigation video through our variant of Oscar Reutersvards Impossible Staircase, which is shown on the left. The video forms visually continuous loop between the end points of the conditioning camera trajectory despite their height difference. Guidance significantly improves long-range consistency, demonstrating that both components of our method are crucial for effective loop closure. It should be noted that activating our loop closing mechanism results in oversmoothed generations on Panorama 2-loop for the default stochasticity level η = 0.9 (see Fig. 4 and 6). However, we show in Appendix B.2 that stochasticity can be further reduced to alleviate oversmoothing without compromising consistency, which is made possible by Omni Guidance. 4.3 NEW APPLICATION: THE IMPOSSIBLE STAIRCASE In Fig. 7, we showcase novel application that leverages all of this papers contributions: we generate video that navigates through variant of Oscar Reutersvards impossible staircase (Penrose & Penrose, 1958). The two end points of the conditioning trajectory differ in height, yet we form visually continuous loop by using our proposed loop-closing mechanism. Please find the implementation details in Appendix A.2."
        },
        {
            "title": "5 DISCUSSION",
            "content": "Limitations. We find that conditioning on external images is difficult as GVS struggles to propagate the context frames to the rest of the target video. Furthermore, GVS is dependent on the performance of its Diffusion-Forcing backbone, as it is purely training-free method. For example, GVS fails to loop-close wide-baseline viewpoints and often struggles to distinguish the start of an upward staircase from the end of downward staircase due to the backbones short context window (see Fig. 1). We elaborate on these failure modes in Appendix C. Future work will aim to extend GVS to different forms of conditioning, such as context images and text, and to other domains, such as goal-conditioned robot planning and hierarchical robot planning. Conclusion. In this paper, we introduced Generative View Stitching (GVS), training-free diffusion stitching method for camera-guided video generation. GVS is designed to be compatible with any Diffusion-Forcing video model, which enables Omni Guidance, robust technique that enhances the temporal consistency in stitching, and by extension, enables loop closing. Given its ability to generate camera-guided videos that are consistent, faithful to the conditioning trajectory, and stable, GVS not only establishes itself as competitive video stitching framework, but also presents promising alternative to autoregressive extension for long video generation."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported by the National Science Foundation under Grant No. 2211259, by the Singapore DSTA under DST00OECI20300823 (New Representations for Vision, 3D Self-Supervised Learning for Label-Efficient Vision), by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) under 140D0423C0075, by the Amazon Science Hub, by the MIT-Google Program for Computing Innovation, and by Sony Interactive Entertainment."
        },
        {
            "title": "REFERENCES",
            "content": "Mohammad Asim, Christopher Wewer, Thomas Wimmer, Bernt Schiele, and Jan Eric Lenssen. Met3r: Measuring multi-view consistency in generated images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In International Conference on Machine Learning (ICML), 2023. Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al. Mixture of contexts for long video generation. arXiv preprint arXiv:2508.21058, 2025. Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. VideoJAM: Joint appearance-motion representations for enhanced motion generation in video models. In International Conference on Machine Learning (ICML), 2025. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, and Yahui Zhou. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025a. Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025b. Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, Tatsunori Hashimoto, Sanmi Koyejo, Yejin Choi, Yu Sun, and Xiaolong Wang. One-minute video generation with test-time training. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Decart, Julian Quevedo, Quinn McIntyre, Spruce Campbell, Xinlei Chen, and Robert Wachen. Oasis: universe in transformer. 2024. URL https://oasis-model.github.io/. Ian Failes. The big effects moments in the one-shot 1917. https://beforesandafters. com/2020/01/17/the-big-effects-moments-in-the-one-shot-1917/, 2020. Hossein Goli, Michael Gimelfarb, Nathan Samuel de Lara, Haruki Nishimura, Masha Itkina, and Florian Shkurti. Stitch-ope: Trajectory stitching with guided diffusion for off-policy evaluation. In Advances in Neural Information Processing Systems (NeurIPS), 2025. Google DeepMind. Veo-3-tech-report.pdf. Technical report, 2025. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), 2020."
        },
        {
            "title": "Preprint",
            "content": "Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning (ICML), 2023. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Jaihoon Kim, Juil Koo, Kyeongmin Yeo, and Minhyuk Sung. Synctweedies: general generative framework based on synchronized diffusions. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision (ECCV), 2022. Luma AI Team. Introducing dream machine: Lumas first generative video model, 2024. Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard S. Zemel. Understanding the effective receptive field in deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Yunhao Luo, Utkarsh Mishra, Yilun Du, and Danfei Xu. Generative trajectory stitching through diffusion composition. In Advances in Neural Information Processing Systems (NeurIPS), 2025. Utkarsh Aashu Mishra, Shangjie Xue, Yongxin Chen, and Danfei Xu. Generative skill chaining: Long-horizon skill planning with diffusion models. In Conference on Robot Learning (CoRL), 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE International Conference on Computer Vision (ICCV), 2023. Lionel Sharples Penrose and Roger Penrose. Impossible objects: special type of visual illusion. British journal of psychology, 49:313, 1958. Runway. Introducing Runway Gen-4. introducing-runway-gen-4, 2025. https://runwayml.com/research/ Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. Gaia-2: controllable multi-view generative world model for autonomous driving. arXiv preprint arXiv:2503.20523, 2025. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems (NeurIPS), 2016."
        },
        {
            "title": "Preprint",
            "content": "Sand.ai, Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W. Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, and Yuqiao Li. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Manuel-Andreas Schneider, Lukas Hollein, and Matthias Nießner. Worldexplorer: Towards generating fully navigable 3d scenes. In ACM SIGGRAPH Asia, 2025. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations (ICLR), 2021a. Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. In International Conference on Machine Learning (ICML), 2025. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations (ICLR), 2021b. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Long-term consistent world simulation with memory. In Advances in Neural Information Processing Systems (NeurIPS), 2025. Virginia Yates. Rope sets precedent. https://theasc.com/articles/ rope-sets-a-precedent, 2023. Kyeongmin Yeo, Jaihoon Kim, and Minhyuk Sung. Stochsync: Stochastic diffusion synchronization for image generation in arbitrary spaces. In International Conference on Learning Representations (ICLR), 2025. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In IEEE International Conference on Computer Vision (ICCV), 2023. Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. In ACM SIGGRAPH Asia, 2025. Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T. Freeman, and Hao Tan. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025."
        },
        {
            "title": "Preprint",
            "content": "Jensen Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. Stable virtual camera: Generative view synthesis with diffusion models. In IEEE International Conference on Computer Vision (ICCV), 2025. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. ACM SIGGRAPH, 37, 2018."
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "Algorithm 1: Camera-guided Video Generation with GVS Inputs: cameras p; context windows {wn}N 1 n=0 each specified by target chunk + 2T overlap timesteps Outputs: z: video sample of length aligned with Function GVS(p, {wn}N 1 n=0 ): (0, I) {} for = 0, . . . , 1 do Initialize video sample with pure-noise sequence for = 0, . . . , 1 do if wn is equal to tth target chunk then W[t].insert(wn) Compile context windows containing tth target chunk end end end for = 1, . . . , 1 do ϵk (0, I) for = 0, . . . , 1 do Sample pure-noise sequence for stochasticity term (p), ϵk ) γϵθ(, xk fwk , , , ) (ϵk) wk W[t][k mod W[t]] (z), pk xk fwk ϵθ (1 + γ)ϵθ(xk xk fwk pk 1 αkϵθ αk 1 αk1 αk1x0k σk η xk1 x0k + (cid:112)1 αk1 (σk)2 ϵθ + σkϵk Cycle through context windows Project to context window Predict guided noise Predict clean sample Compute stochasticity Run DDIM denoising step end argminz end (cid:80)T 1 t=0 wk (z) xk1 Update video sample with target chunks A.1 IMPLEMENTATION DETAILS Diffusion-Forcing Backbone. In this paper, we evaluate all sampling methods using the same backbone model: an open-sourced Diffusion-Forcing Transformer model that supports cameraconditioned video generation and is trained on the RealEstate10K dataset (Zhou et al., 2018). The full implementation details of this backbone can be found in Song et al. (2025), but we repeat the most relevant points for completeness sake. This backbone model is U-ViT (Hoogeboom et al., 2023) that accepts 8 256 256 video inputs. All conditioning signals i.e., per-frame noise levels and camera poses, are injected into the model via Adaptive LayerNorm. Importantly, camera conditioning is injected by first computing relative poses with respect to the first frame and then transforming them into high-dimensional ray encodings. Sampling. For history-guided autoregressive sampling (Song et al., 2025), we use the default hyperparameters from its open-sourced implementation: deterministic DDIM sampler, which corresponds to setting σk = 0 in Eq. 1, with linear denoising step schedule over 50 sampling steps (the number of training denoising steps is 1000), history guidance scale of 4, which controls the joint guidance on history and camera conditioning, 4 history frames per context window, and stabilization level of 0.02. We repurpose the StochSync baseline (Yeo et al., 2025), originally designed for arbitrary images such as 360-degree panoramas and 3D mesh textures, for camera-guided video generation; we treat the video output from our backbone model as wide image generated by an image diffusion backbone. We use the default hyperparameters from StochSyncs implementation for 360-degree panorama generation: maximum stochasticity DDIM sampler (σk = 1 αk1), linear denoising step schedule over 25 sampling steps from = 900 to Kstop = 270, multi-step computation of clean samples, which is initially run for 50 steps and linearly decreased as the outer-loop denoising progresses, and two alternating sets of non-overlapping context windows, which are offset from"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Spatial Context Windows that Define Our Methods Cyclic Conditioning Strategy. Note that for Panorama 1-loop and Panorama 2-loop, we intentionally offset the camera centers from the panoramas rotation center to better distinguish between different parts of conditioning trajectory. We visualize Circle 1-loop and Circle 2-loop as spirals also for better clarity. each other by 4 frames. While the default guidance scale is 7.5, we find that this leads to unrealistic generations for the given backbone and task and therefore we lower the guidance scale to 4. Note that this guidance scale only controls camera conditioning as StochSync does not guide with history. τ =tT target chunk Our method (GVS) is outlined in Algorithm 1, where {0, 1, . . . , 1} indexes the tth target chunk, which is comprised of timesteps {τ }t(T target chunk+1)1 , and target chunk, overlap, denote the size of each target chunk, overlap length between context windows, and target video length respectively; denotes the target chunk of context window w, and fw(z) denotes the selection of frames from video estimate that lie within w. Our method uses partial stochasticity DDIM sampler (σk = η 1 αk1) with stochasticity level η = 0.9 and guidance scale of γ = 1. Note that our guidance scale convention represents no guidance with γ = 0, whereas the baseline methods represent no guidance with γ = 1. Our method employs overlapping context windows with overlap = 2 and target chunk = 4. In other words, each 8-frame-long context window is comprised of 2 frames from the past chunk, the target chunk, and 2 frames from the future chunk. We visualize this in Fig. 2 but with half the context window size. Loop-Closing Mechanism. For conditioning camera trajectories that require loop-closing i.e., Panorama 1-loop, Panorama 2-loop, Circle 1-loop, Circle 2-loop, and Staircase circuit, we equip all sampling methods with loop-closing mechanism. Inspired by Xiao et al. (2025); Zhou et al. (2025), we augment history-guided autoregressive sampling with memory mechanism that retrieves previously generated frames whose field-of-view overlap with the current generation exceeds threshold. At every autoregressive step, we generate 1 frame"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: GVS Stably Scales to Longer Videos Given More Test-Time Compute. conditioned on maximum of 3 retrieved history frames, which are placed at the right end of the context window, and the 4 latest history frames, which are placed at the left end; if there are less than 3 retrieved frames, we pad with pure-noise frames. We augment StochSync with our cyclic conditioning mechanism, which we tailor to its nonoverlapping window sampling strategy. For trajectories that form single loop i.e., Panorama 1-loop, Circle 1-loop, and Staircase circuit, we use StochSyncs original strategy of alternating between two sets of context windows, which is form of cyclic conditioning; the second set includes context window that wraps around the loop and thus enforces consistency between the start and end of the camera trajectory. For Panorama 2-loop and Circle 2-loop, we add third set of non-overlapping context windows that enforces consistency between corresponding frames in the first and second loops, similar to Spatial windows 1 12 in Fig. 8. Our method loop closes via cyclic conditioning, whereby target chunk is denoised by two alternating sets of context windows: 1) temporal context windows, which contain timesteps {τ }t(T target chunk+1)+T overlap1 τ =tT target chunkT overlap , and 2) spatial context windows, which we summarize in Fig. 8. A.2 DETAILS ON THE IMPOSSIBLE STAIRCASE (SEC. 4.3) To loop close the two end points of the Impossible Staircase trajectory, we use cyclic conditioning technique similar to that of Staircase Circuit, which is shown in Fig. 8. The key difference is that we modify the conditioning camera segments for the two spatial windows, which condition the first frame on the last frame and vice versa; we replace the original camera segments, which are disconnected due to the height difference between the end points of the trajectory, with continuous straight line to encourage visual continuity. A.3 COMPUTE RESOURCES We run every experiment on single NVIDIA H200 GPU and report the resulting metrics. We also provide scalable implementation of GVS that can be run on lower-VRAM GPU, such as the NVIDIA RTX A6000. At every denoising step, the scalable implementation denoises every context window one-by-one, whereas the default implementation denoises all context windows in parallel."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Lowering Stochasticity Levels Reduces Oversmoothing. Note that we intentionally offset the camera centers from the panoramas rotation center to better distinguish between different parts of the conditioning trajectory. η 0.9 IQ() F2FC() LRC() Panorama 2-loop Omni Guide 0.8 0.9 0.116 2.03 0.8 0.151 0.113 0.483 0.389 2.41 0.402 0.147 0.380 0.355 0. 0.155 0.483 0.376 0.359 0.175 0. AQ() IS() 1.59 1.56 Table 4: Omni Guidance Provides Additional Affordance to Reduce Oversmoothing."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "B.1 APPLICATION: THE INDEFINITE STAIRCASE In Fig. 9, we further highlight the long-horizon stability of GVS by generating 1080-frame video that climbs an 18-story staircase (here there is no loop closure between the end points of the conditioning trajectory). The entire video is collision-free and stable, demonstrating GVS scaling properties and its potential as an alternative to autoregressive extension for long video generation. B.2 ABLATION ON OMNI GUIDANCE AND STOCHASTICITY In Fig. 10 and Table 4, we further demonstrate how Omni Guidance provides additional affordance to reduce oversmoothing without sacrificing consistency. Fig. 10 demonstrates that for Panorama 2-loop, the default stochasticity level η = 0.9 results in oversmoothed generations, which can be alleviated by reducing the stochasticity level to η = 0.8. Table 4 shows that stochasticity can be reduced without sacrificing consistency due to the graceful tradeoff properties afforded by Omni Guidance (compare rows 3 and 4); without Omni Guidance, reducing stochasticity hampers both temporal and long-range consistency (F2FC, LRC). B.3 ADDITIONAL QUALITATIVE COMPARISONS WITH BASELINES In Fig. 14, 15, and 16, we visualize additional qualitative comparisons with baselines, which corroborate our findings in Sec. 4.1: AR sampling collides with the generated scene, after which generation collapses, and often fails to close loops. StochSync avoids collisions, but achieves this by shapeshifting the scene. Our method, GVS, generates stable and temporally consistent samples that are collision-free and closes loops."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: GVS Struggles to Propagate External Context Frames Throughout the Entire Video."
        },
        {
            "title": "C LIMITATIONS AND CHALLENGES",
            "content": "C.1 EXTERNAL IMAGE CONDITIONING We show that GVS does not effectively propagate externally provided context frames to the rest of the target video. In Fig. 11, we apply GVS to single-image-to-video task defined by context frame and corresponding camera trajectory sampled from the RE10K-Mini dataset (Song et al., 2025) and visualize the clean-sample predictions at select denoising steps. After noise level = 999, frame τ = 2 commits to the scene depicted in the context frame τ = 0, while the remaining frames are ambivalent. However, by noise level = 759, future frames τ = 12 and beyond commit to different scene, as there is no direct information propagation from the context frame. By the end of the stitching process, the context frame has only propagated to frame τ = 4, resulting in the awkward transition between these two diverged scenes as shown in frames τ = 6 and τ = 8. Note that GVS without external context frames avoids divergent scenes as neighboring frames can affect each other throughout the stitching process; on the other hand, external context frames are unaffected by the rest of the target video. Autoregressive sampling also precludes divergent scenes as new generations are conditioned on fully denoised history. Controlling information propagation in stitching by modulating the per-frame noise levels of the Diffusion-Forcing backbone is potential solution, which would enable external image conditioning and, in turn, significantly improve video quality and user controllability. We leave this investigation for future work. C.2 LOOP-CLOSING WIDE-BASELINE VIEWPOINTS We find that GVS fails to loop-close wide-baseline viewpoints. To probe the root cause, we benchmark GVS on the Forward-Orbit-Backward trajectory, which is comprised of collinear forward and backward camera segments that are connected by 180-degree orbit camera segment, as shown in Fig. 12. This experiment suggests that this failure mode stems from the specific Diffusion-Forcing backbone used by our method, which is trained on RE10K dataset comprised of camera trajectories with small viewpoint shifts. To elaborate, without our loop closing mechanism, GVS generates video that correctly tracks the camera motion i.e., depicts the generated scene from the correct viewpoint, but whose forward and backward segments do not view the same scene; the brown countertop present in the right side of"
        },
        {
            "title": "Preprint",
            "content": "Figure 12: GVS Fails to Loop-Close Wide-Baseline Viewpoints. Note that the camera centers of the forward and backward segments, which are collinear in implementation, are visually offset for better clarity. Figure 13: Diffusion-Forcing Backbone Fails on Wide-Baseline Camera Trajectories. Note that the camera centers, which are collinear in implementation, are visually offset for better clarity. frame 0 is absent in frame 13, which marks the end of the orbit, and the color of the floor in each frame is different. When our loop closing mechanism is activated, the forward and backward camera segments depict the same scene, but incorrectly do so from the same viewpoint, as opposed to opposite viewpoints. This is because the spatial context window we design for cyclic conditioning, shown in the top-right of Fig. 12, contains wide-baseline cameras in the conditioning trajectory, which is out-of-distribution to the backbone model. Fig. 13 shows that applying full-sequence diffusion on this conditioning trajectory results in the same failure mode, where the generated video fails to track the camera motion. This failure mode may be addressed by training Diffusion-Forcing backbone on multi-view datasets with wider baselines, such as DL3DV (Ling et al., 2024) and ScanNet++ (Yeshwanth et al., 2023), which is another promising direction for future work. C.3 STRUCTURALLY SIMILAR CAMERA TRAJECTORY SEGMENTS In some corner cases, GVS struggles to distinguish camera trajectory segments with similar structure. This is due to the limited context of the Diffusion-Forcing backbone and its use of relative poses for camera conditioning. Fig. 1 presents notable example, where the start of the upward staircase is identical to the end of the downward staircase up to rigid transformation. Due to this ambiguity, GVS often generates small set of ascending steps at the bottom of the downward staircase (note that, otherwise, the generated video faithfully tracks the descending camera motion). Extending GVS to accept additional forms of conditioning, such as context images and text, could help resolve this ambiguity."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Additional Qualitative Comparisons with Baselines. Note that we intentionally offset the camera centers from the panoramas rotation center to better distinguish between different parts of the conditioning trajectory."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: [Continued] Additional Qualitative Comparisons with Baselines. Note that we intentionally offset the camera centers from the panoramas rotation center to better distinguish between different parts of the conditioning trajectory. We visualize Circle 1-loop and Circle 2-loop as spirals also for better clarity."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: [Continued] Additional Qualitative Comparisons with Baselines."
        }
    ],
    "affiliations": [
        "MIT CSAIL",
        "RunwayML"
    ]
}