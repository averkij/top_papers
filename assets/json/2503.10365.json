{
    "paper_title": "Piece it Together: Part-Based Concepting with IP-Priors",
    "authors": [
        "Elad Richardson",
        "Kfir Goldberg",
        "Yuval Alaluf",
        "Daniel Cohen-Or"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Advanced generative models excel at synthesizing images but often rely on text-based conditioning. Visual designers, however, often work beyond language, directly drawing inspiration from existing visual elements. In many cases, these elements represent only fragments of a potential concept-such as an uniquely structured wing, or a specific hairstyle-serving as inspiration for the artist to explore how they can come together creatively into a coherent whole. Recognizing this need, we introduce a generative framework that seamlessly integrates a partial set of user-provided visual components into a coherent composition while simultaneously sampling the missing parts needed to generate a plausible and complete concept. Our approach builds on a strong and underexplored representation space, extracted from IP-Adapter+, on which we train IP-Prior, a lightweight flow-matching model that synthesizes coherent compositions based on domain-specific priors, enabling diverse and context-aware generations. Additionally, we present a LoRA-based fine-tuning strategy that significantly improves prompt adherence in IP-Adapter+ for a given task, addressing its common trade-off between reconstruction quality and prompt adherence."
        },
        {
            "title": "Start",
            "content": "Piece it Together: Part-Based Concepting with IP-Priors Elad Richardson1 Kfir Goldberg1,2 Yuval Alaluf1 Daniel Cohen-Or1 1Tel Aviv University 2Bria AI 5 2 0 2 3 1 ] . [ 1 5 6 3 0 1 . 3 0 5 2 : r Figure 1. Using dedicated prior for the target domain, our method, Piece it Together (PiT), effectively completes missing information by seamlessly integrating given elements into coherent composition while adding the necessary missing pieces needed for the complete concept to reside in the prior domain."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Advanced generative models excel at synthesizing images but often rely on text-based conditioning. Visual designers, however, often work beyond language, directly drawing inspiration from existing visual elements. In many cases, these elements represent only fragments of potential conceptsuch as an uniquely structured wing, or specific hairstyleserving as inspiration for the artist to explore how they can come together creatively into coherent whole. Recognizing this need, we introduce generative framework that seamlessly integrates partial set of userprovided visual components into coherent composition while simultaneously sampling the missing parts needed to generate plausible and complete concept. Our approach builds on strong and underexplored representation space, extracted from IP-Adapter+, on which we train IPPrior, lightweight flow-matching model that synthesizes coherent compositions based on domain-specific priors, enabling diverse and context-aware generations. Additionally, we present LoRA-based fine-tuning strategy that significantly improves prompt adherence in IP-Adapter+ for given task, addressing its common trade-off between reconstruction quality and prompt adherence. Project page can be found at https://eladrich.github.io/PiT/ To create is to recombine Francois Jacob Recent advancements in image generation have significantly enhanced our ability to prototype and visualize new ideas. However, while modern generative models achieve impressive results, they are still typically conditioned on text, assuming that concepts can be fully articulated through language. In practice, however, artists and designers often work visually drawing from references, reconfiguring elements, and refining compositions in ways that cannot always be expressed through text alone [19]. This has led to the development of techniques for conditioning generative models directly on image conditions [51, 52, 76]. However, these methods alone offer limited control on how the visual concepts influence the generated results. Recognizing this, recent works have explored more advanced manipulation of visual concepts using generative models [11, 26, 54, 57, 65], offering more interactive and intuitive creative process. Expanding on this line of work, we ask: Can we simultaneously assemble given visual components and sample plausible completions for missing parts to create coherent whole? To this end, we propose model that dynamically adapts to user inputs, assembling provided elements into coherent structure while inferring missing components in manner consistent with the provided context. 1 The choice of representation space for our model inputs is crucial. Inspiration Tree [65] and ConceptLab [55] use learnable tokens within the text encoders input space [15]. While effective for their respective tasks, this optimizationbased approach would hinder our ability to use our method efficiently at inference time. Alternatively, pOps [54] and IP-Composer [11] operate in CLIP space. This allows them to efficiently encode visual concepts via pretrained CLIP encoder and is well suited for semantic manipulations. However, the CLIP space is limited in its ability to preserve complex concepts, resulting in loss of details [51, 54]. This intuitively stems from the fact that CLIP was never trained to reconstruct images but rather to learn joint representation space for text and images [50]. While this encourages semantic representation, it does not require the representation to encode visual details that cannot be easily described through text. To improve on this, we explore alternative spaces and ultimately converge on the internal representation of IP-Adapter+. This adapter extends the popular model from [76]. While not formally described in paper, it has gained popularity for its improved reconstruction quality. We show that using this IP + space not only results in improved reconstructions but also retains the ability to perform semantic manipulations and thus can serve as new representation for visual concepts, see Figure 2. With our chosen representation in place, we turn to training part-conditioned model on set of generated samples from given target domain. We train our model to sample both conditionally and unconditionally from that domain while interpreting any given input within that context. For instance, model tuned on monsters and creatures would always generate creature from that domain. Importantly, this strong prior allows the artist to reinterpret everyday objects as potential parts within the learned domain (e.g., the broccoli input in Figure 1) and sample multiple plausible results for the same set of inputs. In essence, the trained model captures prior distribution over the target domain in the IP + space and is therefore dubbed an IP-Prior model. Finally, once complete visual concept has been generated using an IP-Prior, it can be rendered as an image by passing it to pretrained image generation model [49]. Ideally, at this stage, we would additionally like to introduce additional conditions for example, incorporating text prompt to place the generated concept within specific scene. Unfortunately, key limitation of IP-Adapter+ is its inherent trade-off between reconstruction quality and prompt adherence. We hypothesize this arises from the expressiveness of the IP + space, which makes the text conditioning redundant during fine-tuning, and propose simple LoRA-based mechanism for re-enabling text conditioning on the generated concepts. Together, this results in PiT, flexible pipeline that first facilitates concept ideation and then renders those concepts as high-quality images. C + Input Scrawny Muscular Figure 2. Semantic Manipulation in CLIP Space vs. IP+ Space. We encode the input image (left) into two different embedding spaces, modify its latent representation by traversing each space, and render the edited image using SDXL [49]. As shown, CLIP struggles to both reconstruct the concept and follow the desired edit, whereas in IP+ space, the rendered images are faithful both to the concept and the desired edit across the entire range. 2. Related Work Image-Conditioned Generation. While text has become the de facto interface for generating visual content [4, 10, 39, 47, 51, 56, 60, 61, 78], its ability to describe specific visual concepts can be limited. As result, various approaches have been proposed to incorporate image inputs into generative models. These techniques can be roughly grouped based on the mechanism they use to incorporate the visual information into the generative network. In optimization-based personalization methods, single visual concept is encoded into the text embedding inputs [1, 16, 66] or directly into the network weights [31, 58, 64]. The model can then generate images of the given concept under different prompts. Numerous efforts have been made to turn these personalization methods optimizationfree. These methods train an encoder to directly map concept into some form of compact representation that is then passed to the model [2, 17, 43, 59, 62, 68, 77, 79]. Specifically, in [76] the input image is encoded through CLIP and passed to set of decoupled cross-attention layers. This has proven to be an effective conditioning mechanism that has gained popularity in the community. Multi-Concept Generation. Successfully depicting multiple concepts in single image remains challenging task, even in text-based generation [8, 9, 21, 35, 42, 46]. This challenge becomes even more pronounced in imageconditioned generation. Consequently, substantial body of work has focused on improving conditioning techniques to support multiple inputs. In essence, these methods build upon existing image-conditioning approaches, extending them to better support multiple visual constraints. Some approaches aim to solve an optimization problem resulting in disentangled representation for each visual concept [3, 18, 31]. Another line of work leverages LoRA [25] to learn new concepts and aims to apply multiple LoRA modules in conjunction [20, 30, 48, 75], often through spatial maps or localized prompts. Others follow 2 the encoder-based approaches and inject representation of the visual information into the model while accounting for the different objects [22, 42, 44, 70]. Alternatively, numerous works utilize the advancement in the research on vision-language models [34, 37, 74] to define multimodal prompts, interleaving multiple reference images with an input text prompt [41, 71, 72, 81]. Crucially, the aforementioned works primarily focus on embedding complete objects into scene where all elements are predefined, eliminating the need for the model to generate novel content. In contrast, our approach generates new concepts from partial user-provided parts, requiring the model to both integrate the given inputs and infer missing details to form single, coherent concept. Visually Inspired and Creative Generation. The exploration of human creativity within computer graphics has been significant area of research, with numerous works investigating how computational tools can enhance the creative design process [13, 14, 23, 28, 40, 67, 73]. Interestingly, fundamental aspect of creativity is the ability to leverage prior knowledge to generate novel ideas [6, 12, 69]. Aligned with these observations, recent works have started to explore how strong generative models can serve to inspire creative generation and exploration. More specifically, Inspiration Tree [65] showed that one can decompose visual concept into distinct attributes, organizing them hierarchically in tree structure by building on the literature of concept personalization. This was extended in [33], which learns disentangled concept representations along language-informed axes. In the context of creative generation, ConceptLab [55] uses guidance from VLM to learn new token embedding representing novel concepts within given broad category. However, while their approach leverages VLMs for the creative generation process, they do not offer control over the visual attributes of the learned concept. In the context of generating images inspired by multiple visual concepts, it has been shown that generative models that are conditioned on the CLIP image embeddings, such as IP-Adapter [76] or Kandinsky [61], allow one to use the CLIP embedding space to manipulate and compose different concepts together [11, 44, 54]. More specifically, in pOps [54], Diffusion Prior model leveraging the CLIP space is used to learn semantic operators (e.g., union, texturing) over given input concepts. Similarly, IP-Composer [11] enables compositional image generation by extracting and integrating visual concepts from multiple reference images. However, while CLIP provides semantically meaningful space, it often struggles to accurately reconstruct desired concepts, limiting its usability. We show that the proposed usage of the IP + space overcomes this limitation while still allowing for semantically meaningful manipulations of image embeddings. 3. Preliminaries Diffusion Prior. Diffusion models are typically trained with conditioning vector, c, directly derived from given text prompt, y. Ramesh et al. [51] introduce two-stage approach that decomposes the text-to-image generation process into two steps. First, diffusion model is trained to generate an image conditioned on an image embedding, c, using the standard diffusion loss: Ldif usion = Ez,y,ε,t (cid:2)ε εθ(zt, t, c) 2 (cid:3) . (1) Here, the denoising network, εθ, learns to remove noise ε added to the latent code zt at time step t, given the conditioning image embedding c. Next, the Diffusion Prior model, Pθ, is trained to recover clean image embedding, e, from its noisy version, et, conditioned on the corresponding text prompt, y. The objective function is given by: Lprior = Ee,y,t (cid:2)e Pθ(et, t, y)2 2 (cid:3) . (2) Once both models are trained separately, they are combined into full text-to-image generation pipeline. In this work, we extend the Diffusion Prior beyond its conventional role of predicting image embeddings from text. Instead, we adapt it to operate over multiple image parts, enabling finergrained control over the synthesis process. 4. Method We begin by laying the foundation of Piece-it-Together (PiT), detailing the representation space in which our IPPrior operates. Next, we describe the design and training process of our generative model within this chosen representation space. Finally, we demonstrate how the generated concepts can be integrated with pretrained image generation models alongside existing conditions. 4.1. In Search of Space The CLIP space is widely used as an image representation for image-conditioned generation and manipulation tasks [11, 54, 61, 77]. Beyond its alignment with the corresponding text encoding, CLIP has several compelling advantages, including an efficient image encoder and semantically rich representation. However, as originally highlighted in [51], the CLIP space struggles to encode distinct visual patterns and is prone to attribute leakage. This limitation stems from CLIPs training objective, which encourages the model to learn joint text-image representation but does not explicitly require it to capture fine-grained visual details that cannot be easily described through text. While this design choice is reasonable for CLIPs intended purpose, it poses clear challenges for image-conditioned tasks. 3 Figure 3. Piece-it-Together Overview. Given an input image, we extract its semantic components (e.g., using SAM [29]) and encode each image patch into the IP+ space using frozen IP-Adapter+ (IP-A+) blocks (shown in yellow). The resulting set of compact image embeddings are then passed together through our IP-Prior model (green), which also receives noised image embedding representing our desired complete concept. The IP-Prior model outputs cleaned image embedding that captures the intended concept, which is subsequently used to generate the final concept image using SDXL [49] (blue). At inference time, users can provide varying number of object-part images to generate new concept that aligns with the learned distribution. Recognizing that the performance of our method is constrained by the reconstruction capabilities of its visual representation, we must first search for an alternative representation that remains compact and inherently semantic while also offering improved reconstructions. To this end, we find that the internal representation of IP-Adapter+ meets these criteria and offers compelling set of properties. This model was released as an extension of the original IP-Adapter [77] and gained popularity in the community for image-conditioned generation. While the original IPAdapter utilizes the already compact CLIP embedding as input, as illustrated in Figure 3, IP-Adapter+ employs Perceiver-like architecture [27] operating over the full internal representation of the CLIP model. This approach produces representation of 16 2048 vectors, explicitly optimized with reconstruction in mind. The resulting representation is then fed into pretrained generative model via set of trainable attention layers. While IP-Adapter+ is usually used end-to-end as method for image conditioning, we propose to conceptually decouple this process into two parts: first, encoding images into compact representation, and second, conditioning the generative model on this representation. We then demonstrate that this intermediate encoding, which we refer to as the IP + space, is semantic representation space with significantly improved reconstructions compared to the standard CLIP image embedding and can serve as an effective representation for our generative model. As we will show, operating in compact latent space rather than directly on pixels makes training efficient and allows for easy manipulation of the generated concepts. 4.2. How to Train Your IP-Prior key requirement for our model is the ability to generate multiple plausible outputs for the same set of input parts. This enables designers to explore range of variations, crucial property for ideation. To achieve this, we follow pOps [54] and design our model as generative operator acting on image embeddings. During training, the model takes sequence of visual concepts representing object parts, each encoded as IP + vectors, and learns to produce representation of the complete object, see Figure 3. Deviating from [54], which fine-tuned pretrained prior model for the desired generative operator, our approach cannot rely on pretrained prior, as no existing model has been trained on IP + vectors. This required us to build and train our network from scratch. Through empirical exploration, we chose to train 4-block Diffusion Transformer (DiT) [45] using rectified flow [36] instead of the standard denoising loss used in [54, 61]. This approach provides an efficient training on single GPU while keeping the model lightweight for both training and inference. 4.3. Defining Prior via Data Having demonstrated how to train prior given corresponding dataset, we now turn to the data-gathering process. While any visual dataset could be used to train an IPPrior, we find that leveraging generated data offers more flexible and efficient approach. Our data generation process is based on the assumption that modern text-based generative models can effectively sample from specific domains. For instance, in character ideation workflow, one could easily use the prompt Concept art of an imaginary creature, Figure 4. Generated Data Samples. We present sample images generated using FLUX-Schnell [5], which are used to train our IPPrior model for the creatures domain. white background to generate diverse character concepts. That is, the domain is often well-within the text-based generative model. However, text offers very specific form of control over the sampled results, making the exploration process tedious. Building on this observation, we first generate large set of images using pretrained text-to-image model, specifically Flux-Schnell [5]. To enhance the diversity of the generated dataset, we randomly append relevant adjectives to the base prompt (e.g., shadowy, cryptic, magnificent). sample training domain generated using this technique is shown in Figure 4. To create the input pairs, we extract semantic parts from the target images using generalpurpose segmentation method [29] and randomly sample subset of them. This process encourages the model to better solve the target task spatially assembling the provided visual concepts while generating the missing parts all within the context of the learned prior. Interestingly, since we operate in an embedding space, we do not encounter the overfitting issues reported in relevant literature for other tasks when using simple segmentation-based data generation process [63]. 4.4. Unleashing Text Adherence with IP-LoRA Once complete visual concept has been generated using an IP-Prior, it can be rendered as an image by passing it to pretrained image generation model [49] via IP-Adapter+. When rendered as-is, the generated concept closely matches its representation in the training data, which in our case often features simple, clean background. Ideally, however, at this point, one would want to explore how the concept fits into different scenarios and scenes. natural approach to achieve this is to leverage the text-conditioning of the pretrained image model alongside our outputted IP vector. In practice, however, when using IP-Adapter+, the model tends to exhibit low prompt adherence, see Section 5.2. common way to address this is by adjusting the scaling factor of the adapter outputs, but this comes at the cost of reconstruction quality, leading to suboptimal trade-off between fidelity and controllability. We hypothesize this Figure 5. Recovering the Text Adherence via IP-LoRA. IPAdapter+ enables rendering generated concepts via SDXL [49] but often struggles with text adherence. To address this, we fine-tune LoRA adapter over paired examples, where the conditioning image has clean background and the target image places the object in scene described using text prompt. This lightweight training (using just 50 prompts) effectively restores text control while maintaining visual fidelity. arises from the expressiveness of the IP + space, which makes the text conditioning redundant during fine-tuning, causing the model to ignore it in inference. Despite this, we argue that since the model functions as an adapter, its text understanding capabilities are still hidden inside the model and simply need to be reactivated. To re-enable text conditioning, we show that training LoRA adapter [25] on small set of examples can effectively restore this capability. In these examples, the conditioning and target images are not identical. Instead, the conditioning image depicts the object on clean background, while the target image places the object in new scene described by text prompt. We refer to this training setup as IP-LoRA and illustrate it in Figure 5. This setup forces the model to incorporate the text prompt while preserving the visual characteristics of the target concept. Remarkably, even when trained on just 50 text prompts, our model generalizes well to unseen prompts describing different backgrounds, showing that the LoRA adapter enables the model to reuse its existing text comprehension abilities. Furthermore, we show that this mechanism can also be leveraged to personalize the model for specific subdomains based on the given image embedding. For example, it can generate an image depicting full character sheet from single generated concept. 4.5. Implementation Details Our IP-Prior consists of approximately 270M parameters. Training is performed with batch size of 64 for 500K steps, requiring 30GB of RAM on single GPU. The IPLoRA training is conducted using the AdamW optimizer with learning rate of 1 104 for 10K steps. During inference, the IP-Prior model runs for 25 steps, while SDXL uses 50 steps. To enhance fine details, we apply SDEdit [38] over Flux-Dev [5] with strength of 0.35. 5 t a s d s Figure 6. PiT Results for Different Priors. We show results generated by our approach across three different domains. For each result, we use varying number of input parts and generate multiple plausible outputs by altering the seed used for learning the representation. 5. Experiments 5.1. IP-Prior Results Qualitative Results. In Figure 6, we present the results of PiT across three different priors. During inference, our model receives sequence of tokens of possibly varying lengths and generates corresponding output for each seed. The parts themselves are gathered from online sources as well as from newly generated images. As shown, our model successfully recognizes the semantic meaning of each given part and integrates it coherently into the generated result. Notably, this process is entirely free of any additional text supervision, requiring the user to provide the desired object. Furthermore, beyond merely integrating the given parts, the model effectively generates meaningful and coherent completions to the missing information, ultimately producing an in-domain result. Note that, as expected, the model exhibits greater variability when provided with fewer inputs, as there is more room for interpretation. This behavior is further highlighted in Figure 7, where we present results generated from single conditioning image. Here, the outputs vary significantly from one another, aligning with the intended use case in which an artist iteratively refines their target concept. To further highlight our models ability to interpret the same elements in different ways based on the trained IPPrior, we show results in Figure 8 using multiple different priors for the same set of inputs. As shown, each model Input Sampled Results Figure 7. Sampling From Single Input. We present concepts generated by PiT using single input part, which encourages greater variation across the generated results. correctly interprets the provided elements and seamlessly integrates missing components to produce complete generation. In the portrait domain, the model is trained on Found Object Portraits and in the Duck domain, the model is trained on customized rubber ducks. Notably, for the portrait domain, the model must interpret each given input as part of facial structure. For instance, in the first row, the hair is interpreted as an eyebrow. 6 Input PiT IP-A (0.2) IP-A (0.4) IP-A (0.6) ...in space with the milky way behind him Input Character Product Ducks Portraits Figure 8. Sampling Across Different Priors Given single input part, we generate concepts across different learned IP-Prior models, highlighting how each model naturally interprets and adapts the part according to its learned distribution. ...in the snow ...in lab C c t F z n c l u Figure 9. Semantic Manipulations in IP+ Space. Using embeddings learned by our IP-Prior, we apply semantic manipulations to edit the concept before rendering the image with SDXL. Semantic Manipulations in IP+. One of the key advantages of working in embedding spaces for part-based generation is the ability to semantically edit and manipulate generated concepts, further enriching the ideation process. We demonstrate this in Figure 9, where we apply various semantic transformations within the IP + space. Each edit direction is found by generating 50 samples corresponding to pair of contrasting words (e.g., cute and scary), embedding them into our space, and computing the vector direction between the mean embeddings of each set. This simple yet effective approach enables meaningful semantic edits, further highlighting both the potential of working in the IP + space and the broader benefit of working with embeddings in PiT. 5.2. IP-LoRA Results In the previous section, all results were rendered as-is using the original IP-Adapter+ over SDXL. As discussed, desirable property of our approach is the ability to take generated concept and integrate it into different scenes or styles, enabling continuous and flexible creative process. 7 ...in bowling alley Figure 10. IP-Lora vs. IP-Adapter+. Given learned representation, we show results of rendering the concept in new scene. Table 1. Quantitative Comparison. We show the average text and visual scores on scale of 1-5, computed using Qwen [74]. PiT IP-A (0.2) IP-A (0.4) IP-A (0.6) IP-A (0.8) Text Score 3.60 Visual Score 4.55 3.70 2.41 2.45 3. 1.37 4.64 1.06 4.84 Text-Conditioning. We first present results using an IPLoRA trained to restore text conditioning for describing background in which we aim to place our generated concept, as shown in Figure 10. In this setup, SDXL is conditioned both on the standard text prompt and on concepts generated using our method, which are passed as encodings to the adapter. As shown, our method effectively balances the text conditioning while remaining faithful to the concept. For comparison, we include the original behavior of IP-Adapter+. When using strong adapter scales (e.g., 0.6), the model completely disregards the text and simply reconstructs the given image embedding. Lowering the scaling value allows the model to incorporate the textual information but significantly degrades reconstruction quality. We now validate this claim quantitatively. common metric for measuring text and image similarity is CLIPspace similarity [24], but it is unsuitable for our evaluation, as we aim to preserve information beyond what CLIP embeddings can capture. Instead, we use Qwen 2 [74]. Specifically, we prompt the VLM to rate each image on scale of 1 to 5 based on its adherence to the given text and its similarity to the reference image. The results, presented in Table 1, align with the visual observation, demonstrating the effectiveness of the IP-LoRA approach. + p - E L - λ i Figure 11. Qualitative Comparisons. We provide visual comparisons with alternative methods across various domains. We present results in Figure 11 across multiple concept domains. As expected, the naive approach of averaging the generated embeddings and using IP-Adapter+ blends the input components together, producing hybrid of their visual characteristics rather than preserving distinct parts. Similarly, λ-ECLIPSE may capture the target domain (e.g., ducks) and the color palette of the input components, but it fails to integrate the specified parts into the output. Finally, OmniGen, exhibits inconsistent behavior across domains: in some cases, it omits certain parts, while in others, it strictly preserves their spatial structure, preventing it from semantically assembling the parts together, as shown in the third column. In contrast, our approach effectively incorporates the provided parts while naturally completing the missing information based on the target domain. 5.4. Additional Inputs While our primary focus is on object parts as input visual concepts, our approach is not inherently limited to this specific semantic meaning. As an example, in Figure 13, we show that our model can also be conditioned on grid-like arrangement of reference images, describing some form of visual style. This allows one to visually describe target look for the generated concept alongside the target parts. We additionally demonstrate in Figure 14 that PiT can also be conditioned on sketches depicting parts of an object, providing users with greater flexibility when specific visual parts are unavailable. In this case, the model interprets each sketch based on the learned prior. Figure 12. Style Generation. We train LoRA to generate character reference sheets when given concept embedding. Styled Generations. Another common use case for model fine-tuning is personalization to specific styles or domains [15, 58]. Similarly, in Figure 12, we present LoRA trained to generate character reference sheets when conditioned on the concept embedding of given character, where paired data is obtained by extracting sample from the reference sheet and using its embedding as conditioning. 5.3. Comparisons Given the nature of our task, there is no direct method that tackles the same task. Still, to offer an analysis of PiT with respect to the state-of-the-art, we compare it to representative set of multi-image baselines, testing their generalization. More specifically, we consider OmniGen [71], strong multi-modal model, λ-ECLIPSE [44] as priorbased technique for multi-image compositions, and IPAdapter+, modified to operate over multiple images by aggregating the generated embeddings. 5.5. Limitations It is important to note that, as with any embedding-based method, the amount of information encoded in the compact embedding space is inherently limited. While our use of the IP+ space significantly improves the tradeoff between reconstruction and editability mitigating prominent issues such as attribute mixing discussed in [51, 54] not all information can be successfully preserved. For example, the model struggles to encode small or high-frequency details, limiting our ability to condition on fine-grained regions of the target concept or small text. Nevertheless, as we demonstrate, embedding-based mechanisms enable simple manipulations that would otherwise be difficult to achieve. Thus, for our ideation task, we find IP+ to be well-balanced choice. 6. Conclusions In this work, we introduced PiT, method for ideating new concepts from sparse set of input parts. Our approach functions as generative operator that receives set of part embeddings and produces an embedding representing plausible and complete concept within given target domain. PiT addresses core limitations of current CLIP-based methods by operating in the more expressive IP + space, enabling improved concept reconstructions while allowing for meaningful semantic manipulations. We hope that PiT not only serves as strong model for photo-inspired generation and ideation but also provides foundation for solving additional tasks that would benefit from operating within the same representation space."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Or Lichter and Yael Vinker for their feedback and insightful comments. This research was supported in part by the Israel Science Foundation (grants no. 2492/20 and 1473/24), Len Blavatnik and the Blavatnik family foundation."
        },
        {
            "title": "References",
            "content": "[1] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. neural space-time representation for text-toimage personalization, 2023. 2 [2] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano. Domainagnostic tuning-encoder for fast personalization of text-toimage models. In SIGGRAPH Asia 2023 Conference Papers, pages 110, 2023. 2 [3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel CohenOr, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from single image. In SIGGRAPH Asia 2023 Conference Papers, pages 112, 2023. 2 [4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Part Style Sampled Results Figure 13. PiT results with reference conditioning. The model is conditioned on both image parts as well as grid representing target look. Input Sampled Results Figure 14. PiT results for sketch conditioning. The model is conditioned on sketch images, allowing one to specify rough shapes without color. Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers, 2023. 2 [5] Black-Forest. Flux: Diffusion models for layered image generation, 2024. Accessed: 2024. 5, 13 [6] Nathalie Bonnardel and Evelyne Marm`eche. Towards supporting evocation processes in creative design: cognitive approach. International journal of human-computer studies, 63(4-5):422435, 2005. 3 [7] Bria-AI. Bria background removal v2.0, 2024. Accessed: 2024. [8] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models, 2023. 2 [9] Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention for multi-subject text-to-image generation. arXiv preprint arXiv:2403.16990, 2024. 2 [10] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems, 35:1689016902, 2022. 2 [11] Sara Dorfman, Dana Cohen-Bar, Rinon Gal, and Daniel Ip-composer: Semantic composition of visual Cohen-Or. concepts. arXiv preprint arXiv:2502.13951, 2025. 1, 2, 3 [12] Claudia Eckert and Martin Stacey. Sources of inspiration: language of design. Design studies, 21(5):523538, 2000. 3 [13] Mohamed Elhoseiny and Mohamed Elfeki. Creativity inspired zero-shot learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5784 5793, 2019. [14] Philippe Esling and Ninon Devis. Creativity in the era of artificial intelligence. arXiv preprint arXiv:2008.05959, 2020. 3 [15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 8 [16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2023. 2 [17] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models, 2023. 2 [18] Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, and Tali Dekel. Tokenverse: Versatile multi-concept personalization in token modulation space, 2025. 2 [19] Milene Goncalves, Carlos Cardoso, and Petra BadkeSchaub. What inspires designers? preferences on inspirational approaches during idea generation. Design studies, 35 (1):2953, 2014. [20] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, Yixiao Ge, Ying Shan, and Mike Zheng Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models, 2023. 2 [21] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized lowrank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36:1589015902, 2023. 2 [22] Shaozhe Hao, Kai Han, Zhengyao Lv, Shihao Zhao, and Kwan-Yee Wong. Conceptexpress: Harnessing diffusion models for single-image unsupervised concept extraction. In European Conference on Computer Vision, pages 215233. Springer, 2024. 3 [23] Aaron Hertzmann. Can computers create art? In Arts, page 18. MDPI, 2018. [24] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. 7 [25] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 2, 5 [26] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. 1 [27] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 46514664. PMLR, 2021. 4 [28] Anna Kantosalo, Jukka Toivanen, Ping Xiao, and Hannu Toivonen. From isolation to involvement: Adapting machine creativity software to support human-computer co-creation. In ICCC, pages 17, 2014. 3 [29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 4, 5, [30] Zhe Kong, Yong Zhang, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, and Wenhan Luo. Omg: Occlusion-friendly personalized multi-concept generation in diffusion models. In European Conference on Computer Vision, pages 253270. Springer, 2024. 2 [31] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. 2023. 2 [32] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. Interna10 tional journal of computer vision, 128(7):19561981, 2020. 13 [33] Sharon Lee, Yunzhi Zhang, Shangzhe Wu, and Jiajun In The Wu. Language-informed visual concept learning. Twelfth International Conference on Learning Representations, 2024. [34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 3 [35] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2251122521, 2023. 2 [36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 4 [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 3 [38] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 5 [39] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [40] Jonas Oppenlaender. The creativity of text-to-image generIn Proceedings of the 25th International Academic ation. Mindtrek Conference. ACM, 2022. 3 [41] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context with multimodal large language models. arXiv preprint arXiv:2310.02992, 2023. 3 [42] Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel CohenObject-level visual prompts Or, and Kfir Aberman. arXiv preprint for compositional arXiv:2501.01424, 2025. 2, 3 image generation. [43] Or Patashnik, Rinon Gal, Daniil Ostashev, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Nested attention: Semantic-aware attention values for concept personalization. arXiv preprint arXiv:2501.01407, 2025. 2 [44] Maitreya Patel, Sangmin Jung, Chitta Baral, and Yezhou Yang. λ-eclipse: Multi-concept personalized text-to-image diffusion models by leveraging clip latent space, 2024. 3, 8 [45] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. 4 [46] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded In Protext-to-image synthesis with attention refocusing. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79327942, 2024. 2 [47] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T. Barron, Amit H. Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, C. Karen Liu, Lingjie Liu, Ben Mildenhall, Matthias Nießner, Bjorn Ommer, Christian Theobalt, Peter Wonka, and Gordon Wetzstein. State of the art on diffusion models for visual computing, 2023. 2 [48] Ryan Po, Guandao Yang, Kfir Aberman, and Gordon Wetzstein. Orthogonal adaptation for modular customization of In Proceedings of the IEEE/CVF Condiffusion models. ference on Computer Vision and Pattern Recognition, pages 79647973, 2024. 2 [49] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 4, 5 [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. [51] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1, 2, 3, 9 [52] Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky: an improved text-to-image synthesis with image prior and latent diffusion. arXiv preprint arXiv:2310.03502, 2023. 1 [53] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 13 [54] Elad Richardson, Yuval Alaluf, Ali Mahdavi-Amiri, and Daniel Cohen-Or. pops: Photo-inspired diffusion operators. arXiv preprint arXiv:2406.01300, 2024. 1, 2, 3, 4, 9 [55] Elad Richardson, Kfir Goldberg, Yuval Alaluf, and Daniel Cohen-Or. Conceptlab: Creative concept generation using vlm-guided diffusion prior constraints. ACM Transactions on Graphics, 43(3):114, 2024. 2, 3 [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. [57] Ciara Rowles, Shimon Vainer, Dante De Nigris, Slava Elizarov, Konstantin Kutsy, and Simon Donne. Ipadapterinstruct: Resolving ambiguity in image-based conditioning using instruct prompts. arXiv preprint arXiv:2408.03209, 2024. 1 [58] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. 2022. 2, 8 11 One single transformer to unify multimodal understanding and generation, 2024. 3 [73] Kai Xu, Hanlin Zheng, Hao Zhang, Daniel Cohen-Or, Ligang Liu, and Yueshan Xiong. Photo-inspired model-driven 3d object modeling. ACM Transactions on Graphics (TOG), 30(4):110, 2011. 3 [74] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 3, [75] Yang Yang, Wen Wang, Liang Peng, Chaotian Song, Yao Chen, Hengjia Li, Xiaolong Yang, Qinglin Lu, Deng Cai, Boxi Wu, et al. Lora-composer: Leveraging low-rank adaptation for multi-concept customization in training-free diffusion models. arXiv preprint arXiv:2403.11627, 2024. 2 [76] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 1, 2, 3 [77] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. 2023. 2, 3, 4 [78] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models, 2024. 2 [79] Yu Zeng, Vishal Patel, Haochen Wang, Xun Huang, TingChun Wang, Ming-Yu Liu, and Yogesh Balaji. Jedi: Jointimage diffusion models for finetuning-free personalized textto-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67866795, 2024. 2 [80] Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral reference for high-resolution dichotomous image segmentation. CAAI Artificial Intelligence Research, 2024. 13 [81] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model, 2024. [59] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65276536, 2024. 2 [60] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 2 [61] Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Vladimir Arkhipkin, Igor Pavlov, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky 2. https: //github.com/ai-forever/Kandinsky-2, 2022. 2, 3, 4 [62] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testtime finetuning, 2023. 2 [63] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and uniarXiv preprint versal control for diffusion transformer. arXiv:2411.15098, 3, 2024. 5 [64] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image personalizaIn ACM SIGGRAPH 2023 Conference Proceedings, tion. 2023. [65] Yael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel Shamir. Concept decomposition for visual exploration and inspiration. ACM Transactions on Graphics (TOG), 42(6): 113, 2023. 1, 2, 3 [66] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-toimage generation, 2023. 2 [67] Haonan Wang, James Zou, Michael Mozer, Linjun Zhang, Anirudh Goyal, Alex Lamb, Zhun Deng, Michael Qizhe Xie, Hannah Brown, and Kenji Kawaguchi. Can ai be as creative as humans? arXiv preprint arXiv:2401.01623, 2024. 3 [68] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023. 2 [69] Merryl Wilkenfeld and Thomas Ward. Similarity and emergence in conceptual combination. Journal of Memory and Language, 45(1):2138, 2001. 3 [70] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multisubject image generation with localized attention. International Journal of Computer Vision, pages 120, 2024. 3 [71] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 3, 8 [72] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: Where outfit is the same list as the one used for toys. In terms of quantity, the data itself was generated on the fly, alongside the training process. We use four steps for generating the image with FLUX-Schnell model [5], followed by segmentation using SAM [29]. A.2. Training Data for IP-LoRA We follow the following process to train the background generation LoRA, where the model is conditioned on an image embedding representing the concept alongside the text prompt. First, we generate images of different creatures using FLUX, and add to the prompt description of background chosen from set of 50 prompts. Then, we use the Bria RMBG2.0 model [7, 80] to separate the creature from the background. This image is used as the input to the IP-Adapter+ during training alongside the text prompt describing the target background. Similarly, for the reference sheet LoRA, we first generate set of images using the prompt character sheet displaying an imaginary fantasy (adjectives) (creatures) creature with eyes mouth, from several angles with 1 large front view in the middle, clean white background. In the background, we can see half-completed, partially colored sketches of different parts of the object. Here, the adjectives and creatures are sampled from the same list as above. Next, we use grounded SAM [53] to find the largest single view of the creature in the generated image. Its segmented and cropped instance then serves as the visual conditioning for the model. In this scenario, the text prompt remains fixed during tuning, as the only control comes from the given image embedding. B. Additional Results and Comparisons Below, we provide additional qualitative results and comparisons, as follows: In Figure 15 and Figure 16 we present additional results for the domain of character ideation, showing our model ability to successfully wide set of inputs. In Figure 17 we present additional results for the product design domain. In Figure 18 and Figure 19 we present additional results for the toy concepting domain. In Figure 20 we present additional results for our reference sheet LoRA, showing its ability to successfully condition on given image embedding and generating an output that resides in the target style while aligning with the given input."
        },
        {
            "title": "Appendix",
            "content": "A. Additional Details A.1. Training Data for IP-Prior As explained in the main text, the data for each domain shown in the paper was generated using Flux-Schnell [5] with dedicated prompt. We next detail the specific prompt used for each such domain Characters - studio photo pixar style concept art, An imaginary fantasy (adjectives) (character) creature with eyes arms legs mouth, white background studio photo pixar asset. Where adjectives is set of 4-6 adjectives randomly sampled from pool of about 200 adjectives, collected using an LLM, and character is set 1-3 characters sampled from list of characters generated by an LLM concatenated with list of 18K classes from OpenImages [32]. Products - product design photo of (attributes) product with (character-like) attributes, integrated together to create one seamless product. It is set against light gray background with soft gradient, creating neutral and elegant backdrop that emphasizes the contemporary design. The soft, even lighting highlights the contours and textures, lending professional, polished quality to the composition Where attributes is similarly generated list of 300 product attributes including materials and features and character-like is an optional description on an object or animal. Toys - Professional studio photo of an extremely cute and friendly smiling (animal) plush toy is sitting in natural frontal position, facing the camera. He is wearing (outfit) (item). It is set against white background with soft, even lighting, lending professional quality to the composition Where, as before, animal is set of texts of different animals, and outfit is set of attributes covering clothing and hairstyle. Portraits - An artistic face collage crafted from sparse and minimal set of large everyday objects, such as (object). The assemblage forms expressive features, such as lips, textured eyes, and sculpted nose, set against pristine white background that highlights the intricate details and creative use of materials. Where object is chosen from set of 60 relevant objects, such as shells and fruits. Rubber Ducks - Professional studio photo of rubber duck. He is wearing (outfit). It is set against white background with soft, even lighting, lending professional quality to the composition 13 Input Sampled Results Input Sampled Results Figure 15. Additional PiT results for character ideation"
        },
        {
            "title": "Sampled Results",
            "content": "Figure 16. Additional PiT results for character ideation"
        },
        {
            "title": "Sampled Results",
            "content": "Figure 17. Additional PiT results for product design"
        },
        {
            "title": "Sampled Results",
            "content": "Figure 18. Additional PiT results for toy concepting"
        },
        {
            "title": "Sampled Results",
            "content": "Figure 19. Additional PiT results for toy concepting 16 Figure 20. Style Generation. We train LoRA to generate character reference sheets when given concept embedding."
        }
    ],
    "affiliations": [
        "Bria AI",
        "Tel Aviv University"
    ]
}