{
    "paper_title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning",
    "authors": [
        "Changtai Zhu",
        "Siyin Wang",
        "Ruijun Feng",
        "Kai Song",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conversational search systems require effective handling of context-dependent queries that often contain ambiguity, omission, and coreference. Conversational Query Reformulation (CQR) addresses this challenge by transforming these queries into self-contained forms suitable for off-the-shelf retrievers. However, existing CQR approaches suffer from two critical constraints: high dependency on costly external supervision from human annotations or large language models, and insufficient alignment between the rewriting model and downstream retrievers. We present ConvSearch-R1, the first self-driven framework that completely eliminates dependency on external rewrite supervision by leveraging reinforcement learning to optimize reformulation directly through retrieval signals. Our novel two-stage approach combines Self-Driven Policy Warm-Up to address the cold-start problem through retrieval-guided self-distillation, followed by Retrieval-Guided Reinforcement Learning with a specially designed rank-incentive reward shaping mechanism that addresses the sparsity issue in conventional retrieval metrics. Extensive experiments on TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly outperforms previous state-of-the-art methods, achieving over 10% improvement on the challenging TopiOCQA dataset while using smaller 3B parameter models without any external supervision."
        },
        {
            "title": "Start",
            "content": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning Changtai Zhu1*, Siyin Wang1, Ruijun Feng3, Kai Song2, Xipeng Qiu1 1Fudan University, 2ByteDance Inc, 3University of New South Wales ctzhu23@m.fudan.edu.cn, {siyinwang20, xpqiu}@fudan.edu.cn, songkai.sk.batman@bytedance.com, ruijun.feng@unsw.edu.au 5 2 0 2 1 2 ] . [ 1 6 7 7 5 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Conversational search systems require effective handling of context-dependent queries that often contain ambiguity, omission, and coreference. Conversational Query Reformulation (CQR) addresses this challenge by transforming these queries into self-contained forms suitable for off-the-shelf retrievers. However, existing CQR approaches suffer from two critical constraints: high dependency on costly external supervision from human annotations or large language models, and insufficient alignment between the rewriting model and downstream retrievers. We present ConvSearch-R1, the first self-driven framework that completely eliminates dependency on external rewrite supervision by leveraging reinforcement learning to optimize reformulation directly through retrieval signals. Our novel two-stage approach combines Self-Driven Policy Warm-Up to address the cold-start problem through retrievalguided self-distillation, followed by RetrievalGuided Reinforcement Learning with specially designed rank-incentive reward shaping mechanism that addresses the sparsity issue in conventional retrieval metrics. Extensive experiments on TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly outperforms previous state-of-the-art methods, achieving over 10% improvement on the challenging TopiOCQA dataset while using smaller 3B parameter models without any external supervision."
        },
        {
            "title": "Introduction",
            "content": "Conversational search aims to fulfill users information needs through multi-turn interactions, unlike traditional information retrieval systems that only consider single-turn, keyword-based queries (Joshi et al., 2017; Kwiatkowski et al., 2019). In these multi-turn scenarios, conversational queries often contain ambiguity, omission, and coreference, *Work done during internship at ByteDance Inc. Corresponding author. 1 Figure 1: Illustration of the CQR task. Given query and its context, the rewriter aims to reformulate the query into stand-alone form, which facilitates the offthe-shelf retriever in finding the most relevant passage. making it difficult for existing retrieval methods to accurately capture user intent (Anantha et al., 2020; Qu et al., 2020; Gao et al., 2022). Given the challenges and computational costs of training multiturn retrievers, Conversational Query Reformulation (CQR) (Elgohary et al., 2019; Vakulenko et al., 2020; Yu et al., 2020) has emerged as practical solution, transforming context-dependent queries into self-contained forms that can be effectively processed by off-the-shelf retrievers (As shown in Figure 1). Existing CQR approaches have explored various strategies to address conversational search challenges. Some methods rely on explicit rewrites as supervision, obtained through either human annotations (Lin et al., 2020; Vakulenko et al., 2020; Del Tredici et al., 2021; Vakulenko et al., 2021) or knowledge distillation (Mao et al., 2023b; Mo et al., 2024a) from large language models (LLMs) like ChatGPT (OpenAI, 2023). More recent approaches attempt to leverage retrieval signals for preference optimization, though still require externally annotated data for initialization and generally limit exploration to pairwise preferences rather than optimizing for actual ranking improvements (Mo et al., 2023; Jang et al., 2024; Zhang et al., 2024; Yoon et al., 2025; Lai et al., 2025). These approaches suffer from two critical constraints: (1) the high dependency on costly and inconsistent external sources for high-quality annotation; and (2) insufficient alignment between the rewriting model and the downstream retriever. The fundamental challenge remains: how to enable query reformulation models to effectively align with retrievers without explicitly annotated reference rewrites, through self-exploration guided by retrieval feedback. To address these challenges, we propose ConvSearch-R1, novel self-driven framework that completely eliminates the dependency on external rewrite supervision. Leveraging retrieval ranking signals as rewards, the model self-discovers effective rewrites through iterative exploration and exploitation. Specifically, we design two-stage framework: (1) Self-Driven Policy Warm-Up (SDPWU), which addresses the cold-start problem by leveraging the models few-shot reasoning capabilities combined with retrieval ranking signals to selfdistill high-quality rewrite data without external supervision; and (2) Retrieval-Guided Reinforcement Learning, which further aligns the rewrite model with the retriever through Group Relative Policy Optimization (GRPO) (Shao et al., 2024). With carefully designed rank-incentive reward shaping, ConvSearch-R1 addresses the sparsity issue in conventional retrieval metrics (like Recall@K and NDCG@K) (Jiang et al., 2025), providing smoother learning signals rather than binary or highly skewed outcomes, enabling stable and efficient exploration of the vast reformulation space. We validate the effectiveness and generalizability of ConvSearch-R1 through extensive experiments using 3B parameter models. Compared to approaches employing 7B models (Zhang et al., 2024; Yoon et al., 2025), our method is not only more cost-efficient but also delivers even better performance. ConvSearch-R1 achieves state-of-theart performance on two widely-used conversational search datasets, TopiOCQA (Adlakha et al., 2021) and QReCC (Anantha et al., 2021). Notably, on the more challenging TopiOCQA dataset under dense retrieval, ConvSearch-R1, using Llama3.2-3B and Qwen2.5-3B as backbones, improves by 10.3% and 10.7% on average across all metrics, respectively, compared to previous state-of-the-art results. This demonstrates that, even without human rewrites or external distilled data, relying solely on selfdistillation combined with RL under the reasoning mode enables the model to perform effectively on the CQR task. Our contributions are summarized as follows: We propose ConvSearch-R1, the first conversational query rewriting approach that completely eliminates dependency on external rewrite supervision, enabling effective alignment with off-the-shelf retrievers without costly human annotations. We introduce novel two-stage alignment framework comprising self-driven policy warm-up and rank-incentive reward shaping that effectively addresses the cold-start problem and reward sparsity challenges inherent in retrieval-aligned optimization. Our extensive experiments across multiple datasets and retrievers demonstrate substantial performance improvements over state-ofthe-art methods, particularly on the challenging TopiOCQA dataset, where ConvSearchR1 achieves over 10% average improvement across all metrics while using smaller language models and no external supervision. To facilitate future research in this area, and models we make datasets, available at https://github.com/BeastyZ/ ConvSearch-R1. code,"
        },
        {
            "title": "2.1 Conversational Search",
            "content": "Conversational search (Gao et al., 2022) is way of searching for information by having natural, back-and-forth dialogue with search system, similar to talking with person. The core challenge of conversational search lies in addressing omission, ambiguity, and coreference present in the current query (Anantha et al., 2020; Qu et al., 2020; Gao et al., 2022). Existing approaches to conversational search can be broadly categorized into two main types: conversational dense retrieval (CDR) and CQR. For CDR, many existing methods (Mao et al., 2024; Kim and Kim, 2022; Mo 2 et al., 2024b) use substantial amount of annotated session-passage pairs to fine-tune an ad-hoc retriever into context-aware conversational retriever, process that is costly and may not fully take the advantages of off-the-shelf retrievers. On the other hand, CQR leverages the strengths of existing ad-hoc retrieval systems by striving to transform context-dependent queries into stand-alone forms. Early studies (Voskarides et al., 2020; Lin et al., 2020) primarily relied on human rewrites to endow models with query rewriting capability. With the advent of LLMs, some recent works have begun to utilize the power of LLMs for query rewriting. Mao et al. (2023b) and Ye et al. (2023) employ ChatGPT to perform query rewriting via training-free, purely prompt-based method. Meanwhile, many approaches (Mo et al., 2023; Zhang et al., 2024; Jang et al., 2024; Mo et al., 2024a; Lai et al., 2025; Yoon et al., 2025) distill high-quality rewrites from ChatGPT or Llama to train rewriters that possess strong rewriting ability from the outset. In contrast, ConvSearch-R1 improves query rewriting capability through self-distillation and RL-based trial and error. ConvSearch-R1 not only significantly reduces the cost of obtaining rewrites but also achieves state-of-the-art performance."
        },
        {
            "title": "2.2 RLVR-based Retrieval",
            "content": "Reinforcement learning from verifiable reward (RLVR) has recently emerged as powerful approach for enhancing language models capabilities without explicit supervision (DeepSeek-AI et al., 2025). While recent work has integrated RLVR with retrieval mechanisms (Jin et al., 2025; Song et al., 2025; Chen et al., 2025; Sun et al., 2025), these efforts primarily focus on improving single-turn question answering by teaching LLMs to better utilize retrieval tools. In contrast, our approach leverages reasoning to optimize query reformulation specifically for conversational search contexts. In the retrieval domain, Jiang et al. (2025) applies RLVR for query generation, but differs from our work in two key aspects: (1) it uses retrieval metrics directly as rewards, while we develop specialized rank-incentive reward function, which significantly enhances retrieval performance; and (2) it addresses only single-turn scenarios, not the conversational challenges of omission, ambiguity, and coreference resolution. Regarding conversational search specifically, existing RL approaches Wu et al. (2021); Chen et al. (2022) rely on human rewrites for reward computation, whereas our method eliminates external supervision requirements while incorporating reasoning capabilities into the rewriter."
        },
        {
            "title": "3.1 Task Formulation",
            "content": "A conversational search session is defined as sequence of alternating user queries and system answers: = {(q1, a1), (q2, a2), . . . , (qt, at)}, where qt denotes the user query at turn t, and at denotes the corresponding system answer. At turn t, the user issues query qt, which is potentially dependent on the previous conversational history Ct1 = {(q1, a1), (q2, a2), . . . , (qt1, at1)}. The task of CQR is, given the current query qt and the preceding conversational history Ct1, to generate context-independent reformulated query rt = (qt, Ct1), where () is the reformulation function that leverages both previous queries and system answers to resolve omission, ambiguity, and coreference in qt. The optimal reformulation function can be defined as: = arg max ES[δ(T (rt), ˆpt)], where ˆpt is the gold-standard passage for qt in the collection, () is the retriever that returns passages given query, and δ(, ) is matching function that measures the relevance between the retrieved passage and the gold passage."
        },
        {
            "title": "3.2 Overview",
            "content": "ConvSearch-R1 employs reasoning-based approach to enable the rewriter to fully grasp the omission, ambiguity, and coreference present in the current user query, thereby generating rewrites that are both context-independent and semantically rich. As illustrated in Figure 2, ConvSearch-R1 does not require any external supervised data (reference rewrite) and consists of the following stages: (1) Self-distill set of format-compliant data from the πinit through few-shot learning, aligning it with the preferences of the retriever. Only the top-ranked (rank-1) rewrites are retained and used to fine-tune the πinit, thus getting the πSF with initial ability to follow the desired output format and perform query rewriting; (2) Further improve the πSF using RL with rank-incentive reward shaping, yielding the final rewriter πθ. The output format of the rewriter is required to strictly adhere to the following structure: <think> reasoning process here </think>n<rewrite> rewrite here </rewrite>. 3 Figure 2: Overview of ConvSearch-R1. In Stage 1, we self-distilled set of high-quality data using few-shot learning and obtained the corresponding SFT model. In Stage 2, we further improved the rewriters performance via RL by refining the reward function."
        },
        {
            "title": "3.3 Self-Driven Policy Warm-Up",
            "content": "Following Chu et al. (2025)s findings on SFTs crucial role in stabilizing output formats for effective RL training, we introduce self-driven policy warm-up strategy that avoids costly knowledge distillation from more powerful LLMs used in previous approaches. outputs, producing πSF with fundamental capabilities in format adherence, reasoning, and query rewriting. Through this process, our model learns to first generate an appropriate reasoning process and then produce context-independent rewrite, conditioned on both the conversation history and the generated reasoning. to by begin applied few-shot using our Self-Distillation. We generpromptating rewrites initial model πinit: ing Dd = {yi = πinit(xi, ρ, instruction) xi D}, where is the original dataset, ρ is fixed set of few-shot examples, and Dd is the distilled dataset. We then filter these outputs to obtain formatcompliant samples: Df = {y Dd g(y) = 1}, where : {0, 1} is format validation function indicating whether the output meets the required constraints. Preference Collection and Supervised FineTuning. We identify high-quality rewrites by retaining only samples whose rewrites rank the gold passage at position 1, creating our self-driven data (SD-DATA). Each sample in SD-DATA is represented as triplet [c,q,r], containing conversational history c, current query q, and the reasoning-rewrite pair r. For more details about data collection, see Appendix B. Finally, we fine-tune πinit on SDDATA to maximize the likelihood of ground-truth"
        },
        {
            "title": "3.4 Retrieval-Guided Reinforcement Learning",
            "content": "Rank-Incentive Reward Shaping. The design of reward functions is critical in RL, directly impacting the learning effectiveness of policy models (Devidze et al., 2021). Unlike Jiang et al. (2025) which directly uses retrieval metrics as reward signals, we propose Rank-Incentive Reward Shaping (RIRS) to address the reward sparsity problem. As shown in Appendix Figure 8, directly using metrics like MRR@3 and NDCG@3 as rewards leads to severe reward sparsity, hindering effective model learning. RIRS utilizes retrieval ranking positions to create more informative reward signal rather than relying on binary retrieval metrics. Considering users typically pay more attention to top positions, RIRS implements piecewise reward function that allocates differentiated reward intensitiesassigning higher rewards for top positions (1-10) while maintaining proportionally smaller but meaningful rewards for moderate positions (11100). This approach preserves semantic thresholds of retrieval quality while ensuring dense feedback signals throughout the policy optimization process. The RIRS reward function is formally defined as: R(ξ) = f[1,10][1,2](ξ) I[1,10](ξ) +f[10,100][0,1](ξ) I(10,100](ξ), (1) where fAB represents function mapping from interval to interval B, IA(ξ) is the indicator function, which equals 1 when ξ is in set and 0 otherwise, ξ is the rank variable. Considering format correctness, the complete reward function is: R(ξ, ϕ) = R(ξ) I(ϕ = 1) + δ I(ϕ = 0), (2) where ϕ 0, 1 is the format compliance indicator, and δ = 0.1 is the penalty term for format noncompliance. GRPO. Equipped with the rewriter πSF obtained from Stage 1 SDPWU and the carefully designed reward function RIR, we adopt GRPO (Shao et al., 2024) as the specific RL algorithm. GRPO is an efficient algorithm that eliminates the need for an explicit reward model and value model. Through RL with Rank-Incentive Reward Shaping, an existing πθ is incentivized to explore the solution space while rollouts {ri}i=1 πθold(x) are used to maximize retrieval performance. The optimization objective is formulated as: JGRP O(θ) =E (cid:104) (cid:88) min (cid:16) πθ(rix) πθold (rix) Ai, clip 1 ϵ, 1 + ϵ (cid:17) (cid:17) Ai βDKL(πθπSF ) , (cid:16) πθ(rix) πθold (rix) (cid:105) (3) where Ai = R(ξi,ϕi)mean(R(ξ,ϕ)) represents the std(R(ξ,ϕ)) normalized advantage of the i-th rewritten query within the current group, calculated using our RIRS reward function. The parameter ϵ controls the clipping threshold, while β regulates the KL divergence penalty."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Settings. We follow prior work (Yoon et al., 2025) in configuring the datasets, retrievers, and metrics. For datasets, we employ TopiOCQA (Adlakha et al., 2021) and QReCC (Anantha et al., 2021), which are widely used in the conversational search task. For retrievers, we utilize BM25 as the sparse retriever for all experiments and ANCE (Xiong et al., 2020) as the dense retriever for all experiments, where ANCE is trained on the MSMARCO (Campos et al., 2016) passage retrieval tasks. Notably, we do not train any retrievers in our experiments. For metrics, we adopt MRR@3, NDCG@3, and Recall@K (referred to as R@K in this paper) for evaluation. See Appendix E.1 & E.2 for more details. Baselines . We consider four categories of baselines in our experiments. The first category comprises basic baselines that do not involve any query rewriting optimization, including Human Rewrite, Raw, DS-R1-Distill-Qwen-7B, Llama3.2-3B, and Qwen2.5-3B. The second category consists of baselines that fine-tune small-scale models (e.g., T5 and BERT), including QuReTec (Voskarides et al., 2020), T5QR (Lin et al., 2020), CONQRR (Wu et al., 2021), ConvGQR (Mo et al., 2023), EDIRCS (Mao et al., 2023a), IterCQR (Jang et al., 2024), ADACQR (Lai et al., 2025), and CHIQ-Fusion (Mo et al., 2024a). The third category involves baselines that fine-tune LLMs (e.g., Llama2-7B) , including RETPO (Yoon et al., 2025) and AdaQR (Zhang et al., 2024). The fourth category is training-free, leveraging prompt-based methods with ChatGPT for query rewriting. This category includes LLM4CS (Mao et al., 2023b) and InfoCQR (Ye et al., 2023). detailed description of each aforementioned baseline is presented in Appendix E.3. Implementation Details. We employ Llama3.23B and Qwen2.5-3B as the backbone models for our rewrite. For training, we utilize verl (Sheng et al., 2024), flexible and efficient RLHF framework. The BM25 retriever is implemented using Pyserini (Lin et al., 2021), while the ANCE retriever is built with Faiss (Johnson et al., 2017). Evaluation metrics are computed with pytrec_eval (Gysel and de Rijke, 2018). More implementation details can be found in Appendix E.4."
        },
        {
            "title": "5.1 Main Results",
            "content": "Table 1 shows the retrieval performance of various methods in dense and sparse settings, leading to the following conclusions: Employing two-stage alignment framework further enhances the final performance in the absence of external supervised data. Directly using the current user query (Raw) for retrieval yields the poorest performance, underscoring the necessity of CQR. Notably, Human Rewrite not only ,"
        },
        {
            "title": "QReCC",
            "content": "MRR@3 NDCG@3 R@10 R@100 MRR@3 NDCG@3 R@10 R@100 ) 5 2 ( a ) A ( e Human Rewrite Raw DS-R1-Distill-Qwen-7B - - - - - - QuReTeC (SIGIR 2020) T5QR CONQRR (EMNLP 2022) ConvGQR (ACL 2023) EDIRCS (ACL 2023) IterCQR (NAACL 2024) ADACQR (COLING 2025) CHIQ-Fusion (EMNLP 2024) RETPO (NAACL 2025) AdaQR (EMNLP 2024) LLM4CS (EMNLP 2023) InfoCQR (EMNLP 2023) LLama3.2-3B + ConvSearch-R1(ours) - - Qwen2.5-3B + ConvSearch-R1(ours) - - Human Rewrite Raw DS-R1-Distill-Qwen-7B - - - - - - QuReTeC (SIGIR 2020) T5QR CONQRR (EMNLP 2022) ConvGQR (ACL 2023) EDIRCS (ACL 2023) IterCQR (NAACL 2024) ADACQR (COLING 2025) CHIQ-Fusion (EMNLP 2024) RETPO (NAACL 2025) AdaQR (EMNLP 2024) LLM4CS (EMNLP 2023) InfoCQR (EMNLP 2023) LLama3.2-3B + ConvSearch-R1(ours) - - Qwen2.5-3B + ConvSearch-R1(ours) - - - 2.1 10.0 8.5 11.3 - 12.4 - 16.5 28.3 25.6 28.3 20.3 18.9 - 4.8 37.8 +9.5 8.8 35.2 +6.9 - 4.1 21.0 11.2 23.0 - 25.6 - 26.3 38.5 38.0 30.0 38.1 27.7 - - 1.8 8.6 7.3 9.8 - 10.7 - 14.9 26.5 23.5 26.5 18.0 17.7 - 4.0 36.2 +9.7 7.5 33.5 +7.0 - 3.8 19.9 10.5 22.2 - 24.3 - 25.1 37.6 37.0 28.9 36.6 26.7 - - 4.0 18.8 16.0 22.1 - 23.8 - 29.3 48.9 44.7 48.3 37.1 33.7 - 8.6 59.6 +10.7 17.3 57.8 +8.9 - 7.5 36.4 20.2 37.6 - 41.8 - 42.6 58.4 61.6 49.6 61.3 43.3 - 12.8 50.5 +12.0 19.2 51.4 +12.9 11.7 50.1 +12.5 18.3 51.3 +13.7 22.6 72.0 +10.4 33.0 72.0 +10. - 9.2 38.3 - 44.7 - 45.6 - 54.1 71.2 - 73.1 66.2 - - 20.7 80.1 +7.0 36.1 79.9 +6.8 - 13.8 53.4 - 54.4 - 58.8 - 62.0 75.0 - 68.7 79.9 - - 38.1 86.3 +6.4 46.7 85.7 +5.8 39.8 6.5 29.4 34.0 33.4 38.3 44.1 41.2 46.7 55.1 54.3 50.0 50.6 47.8 48.9 22.5 55.9 +0.8 27.3 56.5 +1. 38.4 10.2 28.4 35.0 34.5 41.8 42.0 42.1 42.9 45.8 47.2 44 43.4 44.8 43.9 19.1 50.2 +3.0 29.3 49.7 +2.5 36.3 5.5 27.3 30.5 30.2 - 41.0 - 44.1 52.5 51.9 47.3 48.0 45,0 46. 21.0 54.3 +1.8 25.0 54.8 +2.3 35.6 9.3 25.9 32.6 31.8 - 39.1 - 40.2 42.9 44.2 41.1 40.8 42.1 41.3 17.4 48.1 +3.9 27.2 47.7 +3. 62.7 11.1 44.0 55.5 53.8 60.1 64.4 62.7 64.4 76.5 78.5 69.5 69.6 69.1 66.4 33.7 77.2 -1.3 42.3 76.3 -2.2 58.6 15.7 43.9 55.0 53.1 65.1 63.5 65.6 65.5 67.3 70.7 66.7 65.6 66.4 65. 30.4 70.6 -0.1 44.2 69.8 -0.9 98.5 21.5 63.9 - 86.1 88.9 88.0 90.2 85.5 93.7 - 89.5 - - - 49.8 89.0 -4.7 64.6 88.1 -5. 78.1 22.7 59.0 - 72.8 84.7 81.8 85.3 84.1 83.8 - 84.6 - - - 44.1 82.8 -2.5 59.0 81.6 -3."
        },
        {
            "title": "Avg",
            "content": "- 7.7 30.0 - 36.4 - 41.3 - 44.4 56.6 - 54.1 - - - 20.6 61.3 +4.7 28.6 60.3 +3.7 - 10.9 36.0 - 41.2 - 47.1 - 48.6 56.2 - 51.7 - - - 24.5 63.8 +7.6 34.6 63.7 +7.5 Table 1: Results of both dense and sparse retrieval on TopiOCQA and QReCC. NE denotes no external distillation, indicating that no external data was distilled from open-source or closed-source LLMs. NH stands for no human, meaning that no human rewrites were utilized. Bold indicates the best results, and the rest of the tables follow the same convention. Grey indicates the improvements over SOTA baselines. fails to deliver optimal results but also performs significantly worse than many other baselines. This suggests that, in addition to the high annotation costs, human rewrites are not aligned with retriever preferences. Furthermore, directly applying the reasoning model (i.e., DS-R1-Distill-Qwen-7B) to conversational search yields even worse results than Human Rewrite. This indicates that straightforward general-domain reasoning is not silver bullet, and even models equipped with long chainsof-thought capability (i.e., R1-like models) struggle to excel in conversational search scenarios. Methods that rely solely on human rewrites as supervision signals (e.g., QuReTeC, T5QR) exhibit the lowest performance among all baselines, highlighting the clear limitations of human rewrites. In contrast, approaches that utilize retrieval signals for supervision (e.g., CONQRR, EDIRCS, IterCQR, ADACQR, RETPO, AdaQR) achieve notable performance improvements, yet still fall short of our method. These"
        },
        {
            "title": "IterCQR\nADACQR\nRETPO",
            "content": "Sparse(BM25) ConvSearch-R1(ours, Llama3.2-3B) ConvSearch-R1(ours, Qwen2.5-3B)"
        },
        {
            "title": "IterCQR\nADACQR\nRETPO",
            "content": "Dense(ANCE) ConvSearch-R1(ours, Llama3.2-3B) ConvSearch-R1(ours, Qwen2.5-3B)"
        },
        {
            "title": "QReCC",
            "content": "MRR@3 NDCG@3 R@10 R@100 MRR@3 NDCG@3 R@10 R@100 13.7 14.0 17.2 27.6 24.0 17.8 20.1 23.2 36.8 35.2 12.2 12.6 - 25.8 22.1 16.4 18.6 - 35.4 34.2 - - 32.0 48.3 42.4 - - 40. 58.8 57.0 - - 59.1 74.5 69.2 - - 59.4 78.4 75.4 44.9 - 40. 49.9 46.2 40.1 - 40.9 42.8 41.5 42.4 - - 47.6 43.9 37.4 - - 40.2 39.0 - - 62.2 72.0 68.5 - - 61.9 63.5 61.8 - - 86. 87.2 83.8 - - 79.9 78.7 76.6 Table 2: Performance on unseen datasets. results collectively demonstrate the effectiveness of our proposed two-stage alignment framework. Notably, compared to the baselines, ConvSearch-R1 achieves new state-of-the-art performance across most experimental settings without the need for any external supervised data. While the baselines leverage larger language models to achieve competitive performance, our 3B rewriter demonstrates superior results. Many studies (e.g., RETPO, AdaQR, LLM4CS, InfoCQR) have attempted to tackle the CQR task by leveraging larger language models (such as ChatGPT). However, our 3B-parameter model significantly outperforms these approaches and achieves new state-of-the-art results. Both RETPO and AdaQR employ Direct Preference Optimization (Rafailov et al., 2023) to align the retrievers preferences, but this strategy overlooks the potential of enabling the model to fit preferences through trial and error. In contrast, ConvSearch-R1 utilizes RL with Rank-Incentive Reward, which allows the model to explore and refine preference alignment through iterative trial and error, ultimately leading to more optimal solution for retriever preference modeling with reduced parameter usage. (a) Comparison on BM25 (b) Comparison on ANCE"
        },
        {
            "title": "5.2 Generalization on Unseen Datasets",
            "content": "Figure 3: Model scale analysis on TopiOCQA. For the evaluation of generalization ability, we trained models on the TopiOCQA training set and evaluated them on the QReCC test set, and vice versa. As shown in Table 2, our method demonstrates superior generalization performance compared to other approaches. This significant improvement in generalization can be primarily attributed to our use of RL with Rank-Incentive Reward. By employing well-designed reward function, our training paradigm encourages the model to interact with broader and more diverse set of high-quality data through trial and error, thereby enhancing its ability to generalize across different datasets."
        },
        {
            "title": "5.3 Scaling Behavior of Model Performance",
            "content": "To further validate the generalizability of our approach across models of varying parameter scales, we conducted experiments on the TopiOCQA dataset using the Qwen2.5 Series models, as shown in Figure 3. Our results demonstrate that our method consistently outperforms the SOTA base-"
        },
        {
            "title": "QReCC",
            "content": "MRR@3 NDCG@3 R@10 R@100 MRR@3 NDCG@3 R@10 R@100 Sparse(BM25) Dense(ANCE) Qwen2.5-3B + w/ SDPWU + w/ RIRS + ConvSearch-R1 Qwen2.5-3B + w/ SDPWU + w/ RIRS + ConvSearch-R1 8.8 14.8 32.6 35.2 19.2 22.2 49.2 51.4 7.5 13.0 31.0 33. 18.3 20.6 48.6 51.3 17.3 27.8 56.2 57.8 33.0 38.7 70.4 72.0 36.1 50.9 79.0 79.9 46.7 58.3 84.8 85.7 27.3 47.6 51.7 56. 29.3 39.9 47.7 49.7 25.0 45.5 49.9 54.8 27.2 37.5 45.6 47.7 42.3 64.3 71.9 76.3 44.2 59.3 68.3 69.8 64.6 79.8 85.8 88. 59.0 73.8 80.7 81.6 Table 3: Ablation results for both dense and sparse retrieval on the TopiOCQA and QReCC. SDPWU and RIRS denote the stage1 ( 3.3) and stage2 ( 3.4) of ConvSearch-R1, respectively. tial preference-aligned rewriter is obtained through self-distillation and preference-based filtering. In Stage 2 ( 3.4), reinforcement learning with carefully designed rewards is employed to further align the rewriter with retriever preferences. To validate the effectiveness of each component, we conduct ablation studies. As shown in Table 3, each component in ConvSearch-R1 plays critical role across different retrievers and datasets, demonstrating the rationality and effectiveness of our frameworks design. Warm-Up Analysis. To validate the necessity of Stage 1 ( 3.3), we selected checkpoints from various steps during the training process using different retrievers and on different datasets. These checkpoints were then evaluated accordingly. As shown in Figure 4, as the number of training steps increases, the MRR@3 score gradually improves and eventually reaches plateau. At every training step, ConvSearch-R1 consistently outperforms the RIRS-only model (i.e., the model trained without Stage 1) in terms of MRR@3. These results provide strong evidence for the essential role of Stage 1 in enhancing model performance. Reward Analysis. To further validate the necessity of reward shaping in stage 2 ( 3.4), we conducted comparative experiments on different reward functions using the Qwen2.5-3B model implemented with dense retriever architecture. As shown in Figure 5, the rewriter utilizing RankIncentive Reward Shaping consistently achieved the best performance across all settings. These results provide strong evidence for the necessity of reward shaping in our task."
        },
        {
            "title": "5.5 Case Study",
            "content": "(a) Comparison on BM25 (b) Comparison on ANCE Figure 4: Warm-up analysiss using Qwen2.5-3B. line across almost all model sizes. Notably, as the model size increases, the performance gap between our method and the SOTA baseline becomes increasingly pronounced. Interestingly, even when using relatively small 0.5B parameter model under dense (ANCE) retrieval, our approach still significantly surpasses the SOTA baseline. These findings indicate that ConvSearch-R1 exhibits strong generalizability across models of any scale."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "Overall Analysis. ConvSearch-R1 is two-stage alignment framework. In Stage 1 ( 3.3), an iniWhy does ConvSearch-R1 lead to significant performance improvements? To answer this question, we analyze specific case selected from the 8 (a) Comparison on TopiOCQA (b) Comparison on QReCC Figure 5: Comparison of different reward functions for dense retrieval on the TopiOCQA and QReCC datasets. QReCC test set, as shown in Appendix Table 8. In this case, the user asks two questions within single query, both involving coreference to previous context. In the early stages of training, the alignment between the rewriter and the retrievers preferences is limited. We observe that, during the reasoning process, the rewriter abandons the reformulation of the first question, resulting in final rewrite that only addresses the second question. Consequently, the gold passage is not retrieved within the top 100 results. In the later stages of training, after extensive trial and error, the rewriter achieves much higher degree of alignment with the retrievers preferences. We find that the rewriter repeatedly considers both user questions during reasoning and generates rewrite that successfully resolves the coreference for both. Notably, the rewriter even generates pseudo-passages to supplement missing information. Thanks to this comprehensive consideration and the generation of pseudo-passages, the rewriter is able to retrieve the gold passage at the top-1 position for this case in the later training stage. These findings provide strong evidence that our method can effectively align with the retrievers preferences and achieve state-of-the-art performance on the benchmark. reasoning. To the best of our knowledge, we are the first to tackle the CQR task without relying on any form of external supervision. Experiments show state-of-the-art performance on in/out-of-domain datasets, consistent scaling with model size, and meaningful ablation insights."
        },
        {
            "title": "Limitations",
            "content": "While ConvSearch-R1 demonstrates promising results, several limitations remain that warrant further investigation. (1)Limited Exploration of Larger Scales: Due to resource constraints, we did not train extremely large models (e.g., 34B or 72B). Nonetheless, scaling experiments show consistent performance gains with increasing model size, suggesting effectiveness at larger scales. Given the deployment advantages of smaller models, we focused on models below 7B, where ConvSearch- (2) ComputaR1 achieves superior results. tional Overhead: ConvSearch-R1 incorporates reasoning-based paradigm, leading to longer output sequences and potentially higher inference cost. However, its 3B parameter size requires fewer resources than 7B-based methods, while still achieving strong performance."
        },
        {
            "title": "References",
            "content": "In this paper, we propose ConvSearch-R1, novel two-stage alignment framework for CQR that operates without the need for any external supervised data (reference rewrite). First, we generate high-quality SFT data via self-distillation and rankbased filtering. Then, we refine the model using RL with Rank-Incentive Reward, aligning the rewriter with the retrievers preferences through Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, and Siva Reddy. 2021. Topiocqa: Open-domain conversational question answering with topic switching. Transactions of the Association for Computational Linguistics, 10:468483. R. Anantha, Svitlana Vakulenko, Zhucheng Tu, S. Longpre, Stephen G. Pulman, and Srinivas Chappidi. 2020. Open-domain question answering goes conversational via question rewriting. In North American 9 Chapter of the Association for Computational Linguistics. Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021. Open-domain question answering goes conversational via question rewriting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 520534, Online. Association for Computational Linguistics. Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. 2016. Ms marco: human generated machine reading comprehension dataset. ArXiv, abs/1611.09268. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Hua zeng Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. 2025. Research: Learning to reason with search for llms via reinforcement learning. ArXiv, abs/2503.19470. Zhiyu Chen, Jie Zhao, Anjie Fang, Besnik Fetahu, Oleg Rokhlenko, and Shervin Malmasi. 2022. Reinforced question rewriting for conversational question answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 357370, Abu Dhabi, UAE. Association for Computational Linguistics. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of foundation model post-training. ArXiv, abs/2501.17161. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 179 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv, abs/2501.12948. Marco Del Tredici, Gianni Barlacchi, Xiaoyu Shen, Weiwei Cheng, and Adriá de Gispert. 2021. Question rewriting for open-domain conversational qa: In Proceedings of Best practices and limitations. the 30th ACM International Conference on Information & Knowledge Management, CIKM 21, page 29742978, New York, NY, USA. Association for Computing Machinery. Rati Devidze, Goran Radanovic, Parameswaran Kamalaruban, and Adish Singla. 2021. Explicable reward design for reinforcement learning agents. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS 21, Red Hook, NY, USA. Curran Associates Inc. Ahmed Elgohary, Denis Peskov, and Jordan BoydGraber. 2019. Can you unpack that? learning to rewrite questions-in-context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 59185924, Hong Kong, China. Association for Computational Linguistics. Jianfeng Gao, Chenyan Xiong, Paul N. Bennett, and Nick Craswell. 2022. Neural approaches to conversational information retrieval. Neural Approaches to Conversational Information Retrieval. Christophe Van Gysel and M. de Rijke. 2018. Pytrec_eval: An extremely fast python interface to trec_eval. The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. Yunah Jang, Kang-il Lee, Hyunkyung Bae, Hwanhee Lee, and Kyomin Jung. 2024. IterCQR: Iterative conversational query reformulation with retrieval guidance. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 81218138, Mexico City, Mexico. Association for Computational Linguistics. Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han. 2025. Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning. ArXiv, abs/2503.00223. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. ArXiv, abs/2503.09516. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. IEEE Billion-scale similarity search with gpus. Transactions on Big Data, 7:535547. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. ArXiv, abs/1705.03551. Sungdong Kim and Gangwoo Kim. 2022. Saving dense retriever from shortcut dependency in conversational search. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1027810287, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. 10 Yilong Lai, Jialong Wu, Congzhi Zhang, Haowen Sun, and Deyu Zhou. 2025. AdaCQR: Enhancing query reformulation for conversational search via sparse In Proceedings of and dense retrieval alignment. the 31st International Conference on Computational Linguistics, pages 76987720, Abu Dhabi, UAE. Association for Computational Linguistics. Jimmy J. Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense representations. ArXiv, abs/2102.10073. Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy J. Lin. 2020. Conversational question reformulation via sequence-to-sequence architectures and pretrained language models. ArXiv, abs/2004.01909. Kelong Mao, Chenlong Deng, Haonan Chen, Fengran Mo, Zheng Liu, Tetsuya Sakai, and Zhicheng Dou. 2024. ChatRetriever: Adapting large language models for generalized and robust conversational dense retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 12271240, Miami, Florida, USA. Association for Computational Linguistics. Kelong Mao, Zhicheng Dou, Bang Liu, Hongjin Qian, Fengran Mo, Xiangli Wu, Xiaohua Cheng, and Zhao Cao. 2023a. Search-oriented conversational query editing. In Findings of the Association for Computational Linguistics: ACL 2023, pages 41604172, Toronto, Canada. Association for Computational Linguistics. Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou, Haonan Chen, and Hongjin Qian. 2023b. Large language models know your contextual search intent: prompting framework for conversational search. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12111225, Singapore. Association for Computational Linguistics. Fengran Mo, Abbas Ghaddar, Kelong Mao, Mehdi Rezagholizadeh, Boxing Chen, Qun Liu, and Jian-Yun Nie. 2024a. CHIQ: Contextual history enhancement for improving query rewriting in conversational search. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 22532268, Miami, Florida, USA. Association for Computational Linguistics. Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, and Jian-Yun Nie. 2023. ConvGQR: Generative query reformulation for conversational search. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 49985012, Toronto, Canada. Association for Computational Linguistics. Fengran Mo, Chen Qu, Kelong Mao, Tianyu Zhu, Zhan Su, Kaiyu Huang, and Jian-Yun Nie. 2024b. Historyaware conversational dense retrieval. In Annual Meeting of the Association for Computational Linguistics. OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce Croft, and Mohit Iyyer. 2020. Open-retrieval conversational question answering. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. ArXiv, abs/2305.18290. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, JunMei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. Proceedings of the Twentieth European Conference on Computer Systems. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. ArXiv, abs/2503.05592. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, and Yan Zhang. 2025. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588. Svitlana Vakulenko, S. Longpre, Zhucheng Tu, and R. Anantha. 2020. Question rewriting for conversational question answering. Proceedings of the 14th ACM International Conference on Web Search and Data Mining. Svitlana Vakulenko, Nikos Voskarides, Zhucheng Tu, and Shayne Longpre. 2021. comparison of question rewriting methods for conversational passage retrieval. CoRR, abs/2101.07382. Nikos Voskarides, Dan Li, Pengjie Ren, E. Kanoulas, and M. de Rijke. 2020. Query resolution for conversational search with limited supervision. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. Zeqiu Wu, Yi Luan, Hannah Rashkin, D. Reitter, and Gaurav Singh Tomar. 2021. Conqrr: Conversational query rewriting for retrieval with reinforcement learning. In Conference on Empirical Methods in Natural Language Processing. 11 Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. ArXiv, abs/2007.00808. Fanghua Ye, Meng Fang, Shenghui Li, and Emine Yilmaz. 2023. Enhancing conversational search: Large language model-aided informative query rewriting. ArXiv, abs/2310.09716. Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, and Jaewoo Kang. 2025. Ask optimal questions: Aligning large language models with retrievers preference in conversation. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 58995921, Albuquerque, New Mexico. Association for Computational Linguistics. Shih Yuan Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul N. Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020. Few-shot generative conversational query rewriting. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. Tianhua Zhang, Kun Li, Hongyin Luo, Xixin Wu, James R. Glass, and Helen M. Meng. 2024. Adaptive query rewriting: Aligning rewriters through marginal probability of conversational answers. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1344413461, Miami, Florida, USA. Association for Computational Linguistics. ConvSearch-R1 VS Prior Works As depicted in Figure 9, the distinction of ConvSearch-R1 from previous studies lies in the approach to obtaining rewrites. Prior research has always relied on external sources (e.g., human rewrites or powerful LLMs) to generate rewrites and has failed to endow the rewriter with reasoning capability. In contrast, ConvSearch-R1 acquires rewrites through self-distillation and RL, utilizing trial and error, and equips the rewriter with reasoning ability."
        },
        {
            "title": "B Data Collection",
            "content": "We performed self-distillation on Qwen2.5-3B and Llama3.2-3B using the prompts specified in Figure 6 on the training set of TopiOCQA (Adlakha et al., 2021) and QReCC (Anantha et al., 2021). We retained only those samples that conformed to the required format and for which the rewritten query resulted in the gold passage being ranked first. The number of qualified samples for each model on the TopiOCQA and QReCC datasets is summarized in Table 4."
        },
        {
            "title": "Sample Nums",
            "content": "Qwen2.5-3B Llama3.2-3B 5892 6034 7759 6623 Table 4: The number of samples obtained using the SDPWU ( 3.3) on TopiOCQA and QReCC datasets for both Qwen2.5-3B and Llama3.2-3B models. Results on Llama2-7B To ensure fair comparison, we followed the experimental protocols of Yoon et al. (2025) (using Llama2-7B) and Zhang et al. (2024) (using Mistral-7B), applying our method on the Llama27B1 model under the same experimental settings. As shown in Table 5, ConvSearch-R1 consistently achieves state-of-the-art performance on Llama27B. This result provides strong evidence for the effectiveness and generalizability of ConvSearchR1 across different models. It is worth noting that Llama2-7B heavily relies on SFT to acquire robust format adherence capability; without SFT, the stability of RL training can be significantly compromised, resulting in worse performance. 1https://huggingface.co/meta-llama/ Llama-2-7b-chat-hf 12 Type Method TopiOCQA QReCC MRR@3 NDCG@3 R@10 R@100 MRR@3 MRR@3 R@ R@100 Sparse(BM25) Dense(ANCE) Baseline SOTA ConvSearch-R1 w/o SFT Baseline SOTA ConvSearch-R1 w/o SFT 28.3 35.1 13.5 38.5 49.2 23.7 26. 33.8 11.9 37.6 49.0 22.8 48.9 58.0 25.1 61. 70.5 38.1 73.1 79.3 48.4 79.9 84.8 54.8 55. 55.7 34.0 47.2 49.0 33.6 52.5 54.0 31.5 44. 47.1 31.7 78.5 76.8 51.7 70.7 69.8 50.6 93. 88.1 74.4 85.3 82.3 62.5 Avg 57.1 60.1 36. 58.1 62.7 39.7 Table 5: Results of ConvSearch-R1 using Llama2-7B as backbone of the rewriter. For comparison, Baseline SOTA refers to the best performance achieved by any baseline method on each respective metric. Prompt for few-shot learning: Rank-Incentive Reward Given query and its context, you must first think about the reasoning process in the mind to decontextualize the query by resolving coreference Then, provide the user and omission issues. with rewrite that retains its original meaning and is as informative as possible to help search engines retrieve relevant documents effectively. The reasoning process and rewrite should be enclosed within <think> </think> and <rewrite> </rewrite> tags, respectively, i.e., <think> reasoning process here </think>n<rewrite> rewrite here </rewrite>. Here is an example for your reference: ### Example Begin ### {example} ### Example End ### ### Context Begin ### {context} ### Context End ### Query: {query} Figure 6: Prompt for few-shot learning. Prompt for training and inference: Given query and its context, you must first think about the reasoning process in the mind to decontextualize the query by resolving coreference and omission issues. Then, provide the user with rewrite that retains its original meaning and is as informative as possible to help search engines retrieve relevant documents effectively. The reasoning process and rewrite should be enclosed within <think> </think> and <rewrite> </rewrite> tags, respectively, i.e., <think> reasoning process here </think>n<rewrite> rewrite here </rewrite>. ### Context Begin ### {context} ### Context End ### Query: {query} Rewrite: Figure 7: Prompt for training and inference. To further evaluate the generalizability of the RankIncentive Reward (RIR) across different types of functions, we designed experiments involving three distinct categories of functions. The formulations of these functions are as follows: Reward(rank) = rank + b, Piecewise Linear Func e1rank, 1 rank , Exponential Decay Func Reciprocal Func (4) The experimental results on Qwen3.2-3B are presented in Table 6. As shown, all methods achieve comparable performance, and each surpasses the baselines, reaching state-of-the-art results on TopiOCQA. In this paper, we use the Piecewise Linear Function as Rank-Incentive Reward to present the main results."
        },
        {
            "title": "E Experimental Details",
            "content": "E.1 Datasets Details We use QReCC and TopiOCQA as our datasets in the experiments: (1) QReCC focuses on query rewriting. The overall task is relatively simple, and it provides human-rewritten queries. (2) TopiOCQA emphasizes topic shifts within conversations. It generally involves more conversation turns than QReCC and poses higher level of difficulty. However, it does not provide humanrewritten queries. For all datasets, we remove samples that lack gold passages. In the case of QReCC, some samples have gold passages but no corresponding answers for the queries. Following TopiOCQA, we assign UNANSWERABLE as the answer for such queries. For detailed dataset statistics, please refer to Table 7. All datasets used in this paper are supported for academic research. 13 Figure 8: Comparison of Reward Sparsity."
        },
        {
            "title": "TopiOCQA",
            "content": "MRR@3 NDCG@3 R@10 R@100 Sparse(BM25) Dense(ANCE)"
        },
        {
            "title": "Piecewise Linear Func\nExponential Decay Func\nReciprocal Func",
            "content": "28.3 35.2 36.8 34.8 38.5 51.4 50.4 50.6 26.5 33.5 35.5 33. 37.6 51.3 50.0 50.2 48.9 57.8 58.2 57.6 61.6 72.0 70.6 72. 73.1 79.9 79.6 78.6 79.9 85.7 85.5 85."
        },
        {
            "title": "Avg",
            "content": "44.2 51.6 52.5 51.1 54.4 65.1 64.1 64.6 Table 6: Results of different Rank-Incentive Rewards. Baseline SOTA refers to the best performance achieved by any baseline method on each respective metric. Dataset Split #Conv. #Turns(Qry.) #Collection TopiOCQA QReCC train test train test 3,509 205 10,823 2,775 45,450 2,514 29,596 8, 25M 54M Table 7: Statistics of conversational search datasets. E.2 Metrics Details In this study, we employ three widely used metrics to evaluate the performance of our method: Mean Reciprocal Rank at (MRR@K), Normalized Discounted Cumulative Gain at (NDCG@K), and Recall at (Recall@K). These metrics provide complementary perspectives on ranking quality and retrieval effectiveness. MRR@K measures the average reciprocal rank of the first relevant item within the top results across all queries. It emphasizes the importance of retrieving relevant item as high as possible in the result list, rewarding systems that return relevant results earlier. NDCG@K evaluates the ranking quality by considering both the position and the graded relevance of items within the top results. This metric assigns higher importance to relevant items appearing higher in the ranking and accounts for scenarios where relevance is not binary, thus providing more nuanced assessment of ranking effectiveness. Recall@K quantifies the proportion of relevant items that are successfully retrieved within the top results. It reflects the systems ability to cover as many relevant items as possible in the truncated result list, providing insight into the coverage of relevant content. E.3 Baselines Details In our experiments, we compare the following baselines: (1) Human Rewrite: Manually annotated query rewrites; (2) Raw: The users original query within the dialogue context; (3) DS-R1Distill-Qwen-7B: reasoning model distilled from DeepSeek-R1; (4) QuReTeC(Voskarides et al., 2020): bidirectional transformer model for resolving underspecified queries in multi-turn conversational search, using distant supervision for training data generation; (5) T5QR(Lin et al., 2020): sequence-to-sequence model based on T5, finetuned for conversational question reformulation; (6) CONQRR(Wu et al., 2021): reinforcement learning-based approach that rewrites conversational queries to optimize retrieval with any retriever; (7) ConvGQR(Mo et al., 2023): Combines query rewriting and expansion using generative pre-trained models, incorporating knowledge infusion for better retrieval; (8) EDIRCS(Mao et al., 2023a): non-autoregressive, text-editing model that selects most rewrite tokens from dialogue con14 text, trained with search-oriented objectives for efficient query reformulation; (9) IterCQR(Jang et al., 2024): Iteratively optimizes query reformulation based on information retrieval signals without human supervision; (10) ADACQR(Lai et al., 2025): Aligns reformulation models with both sparse and dense retrievers via two-stage training strategy; (11) CHIQ-Fusion(Mo et al., 2024a): Improves conversational history quality with open-source LLMs before query generation; (12) RETPO(Yoon et al., 2025): Fine-tunes language model using large-scale retriever feedback to generate rewrites aligned with retrieval preferences; (13) AdaQR(Zhang et al., 2024): Trains query rewriting models with limited annotations by using conversational answer probability as reward, eliminating the need for passage labels; (14) LLM4CS(Mao et al., 2023b): Uses LLMs to generate and aggregate multiple query rewrites and hypothetical responses, robustly representing user intent; (15) InfoCQR(Ye et al., 2023): Utilizes LLMs as query rewriters and editors, then distills their capabilities into smaller models for efficiency; (16) Qwen2.5-3B: lightweight multi-modal AI model from Alibaba Cloud, delivering strong performance with fewer parameters; (17) Llama2.53B: compact version of Metas Llama, optimized for efficient, edge-device processing with robust text understanding and generation. E."
        },
        {
            "title": "Implementation Details",
            "content": "All experiments are conducted based on the verl(Sheng et al., 2024). The training process consists of two main stages: SFT and RL. During the SFT stage, we apply the following hyperparameters across all experiments: batch size of 64, maximum sequence length of 3072, 2 training epochs, and learning rate of 1e-5. For the RL stage, the hyperparameters are set as follows for all experiments: batch size of 128, maximum prompt length of 1536, maximum response length of 1024, learning rate of 1e-6, 100 learning rate warmup steps, clipping ratio of 0.2, KL loss coefficient of 0.001, and rollout sample size of n=8 with sampling temperature of 0.7. The number of training epochs is set to 6 for TopiOCQA and 9 for QReCC, respectively. For evaluation, we follow the protocol described in Zhang et al. (2024). The BM25 parameters are set to k1 = 0.9 and = 0.4 for TopiOCQA, and k1 = 0.82 and = 0.68 for QReCC. The lengths of the query, concatenated input, and passage are truncated to 64, 512, and 384 tokens, respectively. In this paper, we use the Instruct-tuned versions of Llama3.2-3B2 and Qwen2.5-3B3 for all experiments. 2https://huggingface.co/meta-llama/Llama-3. 2-3B-Instruct 3https://huggingface.co/Qwen/Qwen2. 5-3B-Instruct 15 Figure 9: Comparison of ConvSearch-R1 with previous works."
        },
        {
            "title": "Conversation",
            "content": "### Context Begin ### Q1: What was Wallis relationship with Edward? A1: In January 1934, while Lady Furness was away in New York City, Wallis Simpson allegedly became Prince Edwards mistress. Q2: Were they romantically involved? A2: Edward denied this to his father, despite his staff seeing them in bed together as well as evidence of physical sexual act. Q3: When did they marry? A3: Edward VIII married Wallis Simpson in France on 3 June 1937, after her second divorce became final. Later that year, the couple toured Germany. Q4: How did Wallis and Earl meet? A4: In April 1916, Wallis Simpson met Earl Winfield Spencer Jr., U.S. Navy aviator, at Pensacola, Florida, while visiting her cousin Corinne Mustin. Q5: Did Earl have to leave due to World War I? A5: After the United States entered the First World War in 1917, Earl Spencer was posted to San Diego. Q6: Where did Earl and Wallis live during this time? A6: Earl Spencer was posted to San Diego; they remained there until 1921. Q7: Was their marriage happy? A7: By September 1925, Wallis Simpson and her husband Earl Spencer were living apart. ### Context End ### Query: Did they stay together or did they eventually get divorced? ConvSearch-R1 (Initial Stage) <think> According to A7, Wallis Simpson and Earl Spencer were living apart by September 1925. While the context does not explicitly state that they divorced, their separation suggests the end of their marriage. Since the original question asks whether they stayed together or eventually divorced, and we know they separated by 1925, it is reasonable to refine the question based on available information. </think> <rewrite> Did Wallis Simpson and Earl Spencer eventually get divorced? </rewrite> (Not Found) ConvSearch-R1 (Final Stage) <think> The user is asking about the relationship and eventual outcome of Wallis Simpson and Earl Spencers marriage. The previous context mentions their first meeting and that Wallis visited her cousin in Pensacola, Florida, in 1916. It also notes that Earl Spencer was U.S. Navy aviator and their marriage was unhappy. Wallis and Earls marriage eventually led to divorce as Wallis had her second divorce by 1936, when she became involved with Edward VIII. The question seeks information about whether they were together at all or if they ultimately divorced. Context mentions their marriage was unhappy and they lived apart by 1925. There are no direct statements about getting divorced, but their separation led to their eventual divorce.</think> <rewrite> Did Wallis Simpson and Earl Spencers marriage last? Wallis Simpson was first introduced to Earl Spencer Jr. in 1916 when she visited her cousin in Pensacola, Florida. After the United States entered World War in 1917, Earl Spencer was posted to San Diego. Wallis and Earl were married in 1928, but their marriage was described as unhappy. By 1925, Wallis and her husband separated and by 1936, they were living apart. Wallis had her second divorce in 1936, leading to her marriage to Prince Edward VIII in 1937. Did Wallis and Earls marriage eventually end in divorce? Wallis and Earl Spencers marriage was unhappy and they eventually separated and lived apart. Their first marriage was dissolved by 1936, with Wallis taking her second divorce. She and Earl did not stay together. Later, during her relationship with Prince Edward VIII, Wallis and Earls whereabouts were not known publicly. They were living apart by 1935-1936, which led to their divorce. Did their marriage end with divorce, and how did their relationship with each other and with each others families fare during this period? In 1921, the couple moved to England, but eventually, their marriage broke down. Wallis and Earls marriage was not happy, and they lived separately for long time. After their divorce in 1928, they maintained cordial relationship, but their marriage was indeed marked by separation and eventually divorce. After their divorce, Earl moved to the United States and later returned to England, where he died in 1954. Wallis Simpson went on to marry Prince Edward VIII in 1937, and the couple toured Germany in the same year.</rewrite> (rank 1) Table 8: Case study on example from QReCC using Qwen2.5-3B as backbone."
        }
    ],
    "affiliations": [
        "ByteDance Inc",
        "Fudan University",
        "University of New South Wales"
    ]
}