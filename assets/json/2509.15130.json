{
    "paper_title": "WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance",
    "authors": [
        "Chenxi Song",
        "Yanming Yang",
        "Tong Zhao",
        "Ruibo Li",
        "Chi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence."
        },
        {
            "title": "Start",
            "content": "WORLDFORGE: UNLOCKING EMERGENT 3D/4D GENERATION IN VIDEO DIFFUSION MODEL VIA TRAINING-FREE GUIDANCE Chenxi Song1, Yanming Yang1, Tong Zhao1, Ruibo Li2, Chi Zhang1 1AGI Lab, School of Engineering, Westlake University, Hangzhou, China 2The College of Computing and Data Science, Nanyang Technological University, Singapore {songchenxi, yangyanming, zhaotong68, chizhang}@westlake.edu.cn ruibo001@e.ntu.edu.sg Project Page: https://worldforge-agi.github.io 5 2 0 2 8 1 ] . [ 1 0 3 1 5 1 . 9 0 5 2 : r Figure 1: We present WorldForge, fully training-free framework leveraging pre-trained video diffusion model for various 3D/4D tasks, such as monocular 3D scene generation (up) and dynamic 4D scene re-rendering (down), enabling precise camera trajectory control and high-quality outputs. Corresponding author."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating gap between their strong priors and their practical use in 3D/4D tasks. As result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our methods superiority in realism, trajectory consistency, and visual fidelity. This work introduces novel plug-and-play paradigm for controllable video synthesis, offering new perspective on leveraging generative priors for spatial intelligence."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in generative modeling, particularly video diffusion models (VDM) (Blattmann et al., 2023; Wan et al., 2025; Yang et al., 2024; Google DeepMind, 2025), have greatly expanded the scope and capabilities of spatial intelligence (Cao et al., 2025) tasks such as 3D and 4D scene understanding (Bahmani et al., 2025a;b), reconstruction (Wang et al., 2025a; Wu et al., 2025; Shi et al., 2024), and generation (Yu et al., 2024c; 2025). Trained on massive, diverse video datasets, these models inherently encode rich spatiotemporal priors that capture structural, temporal, and motion-related patterns. Leveraging such priors offers significant advantages for achieving realistic and coherent spatial transformations, enabling applications in novel view synthesis (You et al., 2025; Xiao et al., 2025), panoramic video generation (Wang et al., 2024b; Ma et al., 2024a), 3D scene generation (Liu et al., 2024; Liang et al., 2025; Wang et al., 2024c), and dynamic scene reconstruction (Bai et al., 2025a; Yu et al., 2025; Van Hoorick et al., 2024). Moreover, VDMs are increasingly used to construct world models (Bar et al., 2025; Duan et al., 2025; Bruce et al., 2024), understood as structured internal representations of physical environments that support predictive reasoning, planning, and decision-making in embodied AI systems. Despite their strong generative capabilities, current video diffusion models face fundamental limitations, including limited controllability, spatialtemporal consistency, and geometric fidelity, particularly when applied to 3D or 4D tasks (Wang et al., 2024c; He et al., 2024; Ling et al., 2024; Xing et al., 2024). While they can be loosely guided with text prompts or reference frames (Kong et al., 2024; Wan et al., 2025), they generally lack the ability to follow precise user-defined motion constraints, such as specified 6-DoF camera trajectory or object pose evolution (Hu, 2024; Ma et al., 2024b). This lack of trajectory-level control is especially problematic in tasks requiring spatial consistency across views, such as novel view synthesis or free-viewpoint rendering. Furthermore, these models often entangle scene and camera motion, making it difficult to separate object dynamics from viewpoint changes (Yu et al., 2024c; Liu et al., 2024). Consequently, attempts to synthesize videos with fixed scenes or smooth camera paths often result in unintended object deformations or scene instability. These limitations hinder their applicability in domains requiring structured spatial reasoning or controllable video generation. To handle these limitations, prior works (Jeong et al., 2025; Ren et al., 2025; Yu et al., 2025; Zhang et al., 2025) have explored two main directions. The first involves training or fine-tuning the generative backbone on multiview or motion-conditioned data, often with explicit modules to encode trajectory information (Bai et al., 2025a; Xiao et al., 2024; Bai et al., 2025b). While this approach 2 can improve alignment and control, it is computationally costly, may generalize poorly to diverse scenes, and risks degrading the models pretrained priors during fine-tuning. The second line of work adopts warping-and-repainting strategy (Ma et al., 2025b; Liu et al., 2025; Ma et al., 2025a; You et al., 2025), in which input frames are lifted into partial 3D representation (e.g., via depth estimation (Piccinelli et al., 2024; Yin et al., 2023)), re-projected along user-defined camera path, and then refined by generative model to fill missing regions. Although more flexible, such methods face notable robustness limitations, as pretrained models are not designed to process out-of-distribution (OOD) (Yu et al., 2024a) inputs such as warped or disoccluded images. As result, they often produce artifacts and missing structures (e.g., suspended components or fragmented geometry). Moreover, their bias toward dynamic training data leads to hallucinated motion even in static scenes, undermining view consistency. In summary, the combined effects of OOD inputs and dynamic-data bias make it challenging to balance fine-grained controllability with generation quality and generalization, which remains an open problem. To address this challenge, we aim to inject precise control into VDMs while preserving their valuable priors. For this purpose, we propose general inference-time guidance paradigm that leverages the rich priors of large-scale VDMs (Blattmann et al., 2023; Wan et al., 2025) in spatial intelligence tasks, such as geometry-aware 3D scene generation and video trajectory control. Our method adopts warping-and-repainting pipeline, in which input frames are warped along reference trajectory and then used as conditional inputs in the repainting stage. Building on this, we develop unified, training-free framework composed of three complementary mechanisms, each designed to address specific challenge in trajectory-controlled generation. First, to ensure the generated motion follows the target trajectory derived from depth-based rendering (Wang et al., 2025b; Piccinelli et al., 2024), we introduce Intra-Step Recursive Refinement (IRR). It embeds micro-scale predictcorrect loop within each denoising step: before the next timestep, predicted content in observed regions is replaced with the corresponding ground-truth observations. This incremental correction allows trajectory control signals to be injected at every step, enabling fine-grained, stepwise guidance that keeps the motion aligned with the target trajectory. Second, we observe that different channels of the VAE-encoded (Kingma & Welling, 2013; Foti et al., 2022) latent representation encode different information, with some channels specializing in appearance and others in motion. Directly overwriting all channels when injecting trajectory signals can inadvertently degrade fine-grained visual details. To address this, we propose FlowGated Latent Fusion (FLF), which leverages optical-flow similarity to selectively and dynamically inject trajectory information only into motion-relevant channels, while leaving appearance-relevant channels unmodified. This selective modulation effectively decouples appearance from motion, allowing for precise viewpoint manipulation while preserving content fidelity. Finally, while warping-based rendering effectively enforces user-defined trajectories in view synthesis, it inevitably introduces noise stemming from imperfect depth estimation, occlusions, and scene misalignments. These imperfections often lead to visual artifacts such as ghosting, structural distortions, and degraded temporal coherence (You et al., 2025). To better balance trajectory adherence and generation fidelity, we propose Dual-Path Self-Corrective Guidance (DSG) strategy. Inspired by CFG (Ho & Salimans, 2021), DSG introduces two concurrent denoising pathways during inference: non-guided path that relies on the models internal priors and yields high-fidelity but uncontrolled outputs, and guided path conditioned on the warped input trajectory, which enforces camera motion but is prone to artifacts. By utilizing the difference between these two paths at each denoising step, DSG computes dynamic correction term that softly adjusts the guided path toward the perceptual quality of the non-guided path. This self-corrective mechanism effectively mitigates trajectory-induced degradation while maintaining alignment with the target camera path, thereby improving both the structural integrity and visual quality of the generated video. Together, these three mechanisms form cohesive inference-time guidance framework that achieves robust and precise trajectory control while preserving the generalization ability of VDM priors. Our method is fully training-free and plug-and-play, enabling broad applicability across tasks without model retraining. It is also model-agnostic and readily adapts to backbones such as Wan 2.1 (Wan et al., 2025) and SVD (Blattmann et al., 2023), underscoring its general utility. To validate the effectiveness of the proposed method, we conduct comprehensive experiments on multiple tasks and benchmarks. Results demonstrate consistent improvements in trajectory adherence, geomet3 Figure 2: Overview of our porposed method. Given single image or video frames, vision foundation model reconstructs scene point cloud, which is warped and rendered along user-specified trajectory to produce guidance video. The input image (or first frame) is also converted into textual prompt and latent representation for an image-to-video diffusion model. Trajectory control is injected through training-free strategy comprising IRR, FLF, and DSG (detailed in Sec. 3.23.4), enabling precise control and high-quality synthesis without additional training. ric consistency, and perceptual quality compared to state-of-the-art (SOTA) baselines. Our main contributions are summarized as follows: We introduce novel, training-free paradigm for leveraging video generative priors in spatial intelligence tasks, enabling precise and stable 3D/4D trajectory control without retraining or fine-tuning. We design synergistic inference-time guidance framework integrating Intra-Step Recursive Refinement (IRR) and Flow-Gated Latent Fusion (FLF), achieving accurate trajectory adherence while disentangling motion from content. We propose Dual-Path Self-Corrective Guidance (DSG), self-referential correction mechanism that enhances spatial alignment and perceptual fidelity without auxiliary networks or retraining. We demonstrate, through extensive experiments on diverse datasets and tasks, that our approach achieves state-of-the-art controllability and visual quality, even compared to training-intensive pipelines."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "We review prior work in three areas most relevant to ours: 3D static scene generation, 4D trajectorycontrolled video generation, and guidance strategies for generative models. 3D Static Scene Generation. Recent advances in 3D reconstruction (Mildenhall et al., 2020; Kerbl et al., 2023; Song et al., 2024; Gao et al., 2024; Yu et al., 2024d; Muller et al., 2022; Yao et al., 2018) and object-level generation (Poole et al., 2023; Wei et al., 2024; Xiang et al., 2025; Kwak et al., 2024) have achieved strong results, but they lack scene-level priors and struggle with geometric consistency, limiting their scalability. VDM (Blattmann et al., 2023; Wan et al., 2025; Kong et al., 2024) offer richer priors and have thus become foundation for scene generation. Approaches vary: some decode 3D scenes from single image via latent traversal (e.g., Wonderland (Liang et al., 2025)), others fine-tune VDMs with depth-based warped inputs, (e.g., ViewCrafter(Yu et al., 2024c) and See3D (Ma et al., 2025a)), and some methods such as MotionCtrl (Wang et al., 2024c) and TrajectoryAttention (Xiao et al., 2025) embed camera parameters to guide view synthesis . More efficient, training-free strategies such as NVS-Solver (You et al., 2025) and ViewExtrapolator (Liu et al., 2024) warp input views and modulate frozen models during sampling. Fine-tuning offers 4 controllability but risks eroding pretrained priors and incurs high cost, while training-free methods retain priors and efficiency but must address geometric coherence. Our work follows the trainingfree direction, aiming to ensure both view consistency and controllability. Trajectory-Controlled Dynamic Video Generation. Video synthesis with controllable camera motion generally follows two main paradigms. The first is fine-tuning-based (Ma et al., 2025b; Mou et al., 2024; Yu et al., 2024b; Wang et al., 2024c) , where lightweight adapters aϕ (e.g., LoRA (Hu et al., 2022), ControlNet (Zhang et al., 2023)) are trained on videotrajectory pairs to optimize the standard diffusion (Ho et al., 2020) loss. Examples include ReCamMaster (Bai et al., 2025a), which retrains on 136K annotated videos, TrajectoryCrafter (Yu et al., 2025) with dual-stream conditioning on source videos and 3D point-cloud renders, GCD (Van Hoorick et al., 2024) using synthetic multi-view videos, and DaS (Gu et al., 2025) incorporating 3D tracking signals into pre-trained diffusion model for multi-type motion control. The second paradigm is warp-and-repaint, which projects source frames with depth to target poses and then repaints the occluded regions (Ma et al., 2025b; Liu et al., 2025; Huang et al., 2025; Tian et al., 2025). While flexible, this approach remains vulnerable to noisy warps that cause flicker or distortionas in training-free baselines such as NVS-Solver (You et al., 2025), which modulates frozen video diffusion model using warped views at inference time. In contrast, our method applies more powerful inference-time guidance mechanism that extracts trajectory cues and directly steers the diffusion process, achieving precise motion-consistency control without any training. Guidance and Control for Generative Models. central challenge in diffusion models is how to steer the generative process toward desired outputs. Guidance strategies address this by modifying the sampling trajectory to better satisfy conditioning signals. The most common is Classifier-Free Guidance (CFG) (Ho & Salimans, 2021), which biases generation toward target condition by blending conditional and unconditional score predictions. While effective, high guidance weights can cause distortions. More advanced methods, such as Auto-Guidance and STG (Karras et al., 2024; Hyung et al., 2025; Xu et al., 2023), use an auxiliary model to anticipate and avoid failure modes, whereas Z-sampling (Bai et al., 2025c) alternates denoising and inversion to refine results mid-generation. For 3D and 4D synthesis, enforcing viewpoint consistency often follows the finetuning and warp-and-repaint strategies discussed above. However, the latter remains sensitive to noisy warps (Cai et al., 2024; Wang et al., 2024a), leading to flicker and geometric distortions. To this end, we propose Dual-Path self-corrective guidance, which derives correction signal from the difference between guided and unguided predictions at each step, enhancing trajectory adherence and stability without retraining or per-scene tuning."
        },
        {
            "title": "3 PROPOSED METHODS",
            "content": "We address the challenge of balancing controllability, visual fidelity, and generalization when applying video diffusion models (VDMs) to 3D/4D tasks. Our solution is training-free framework for trajectory-controlled video generation. At its core is an inference-time guidance strategy that steers pre-trained diffusion model along user-defined trajectories while preserving its intrinsic priors and generative quality. As shown in Fig. 2, the framework integrates three complementary components. Intra-Step Recursive Refinement (IRR) injects trajectory guidance from observed regions at each denoising step, ensuring consistent control throughout sampling (see Sec. 3.2). Flow-Gated Latent Fusion (FLF) refines trajectory injection by decoupling motion and appearance features in the latent space, which prevents content drift and preserves fidelity (see Sec. 3.3). Dual-Path Self-Corrective Guidance (DSG) enhances stability by comparing guided and unguided predictions, using their difference as corrective signal to suppress artifacts from noisy priors (see Sec. 3.4). Together, these modules unlock the models latent 3D/4D awareness and enable fine-grained trajectory control without retraining. 3.1 PRELIMINARIES Before detailing our method, we introduce the necessary preliminaries: diffusion models, guidance strategies, and trajectory-controlled video synthesis."
        },
        {
            "title": "3.1.1 DENOISING DIFFUSION MODELS AND GUIDANCE",
            "content": "Diffusion Solvers. Modern generative modeling is dominated by two paradigms: diffusion models (Ho et al., 2020; Song et al., 2020a) and flow-based models (Lipman et al., 2022). Under the SDE view, diffusion models admit deterministic ODE limit that connects them to flow-based formulations through reparameterization (Gao et al., 2025). The detailed derivation of this equivalence is provided in Appendix A. In this section, we take diffusion models as representative examples. Consider the widely used DDIM sampler (Song et al., 2020a): it recovers the clean sample x0 by reversing the forward noising of Gaussian prior xT . Given noise-prediction network ϵθ(xt, t), the sampler estimates an intermediate signal ˆx0 from the current state xt: xt ˆx0(xt, t) = 1 αt ϵθ(xt, t) αt , (1) where αt denotes cumulative noise attenuation. The term ˆx0(xt, t) is key intermediate variable: at each step, it is the one-step denoised estimate from ϵθ, evolving from coarse prediction to sharp final output. The next sample xt1 is then obtained by blending ˆx0 with the predicted noise ϵθ: xt1 = αt1 ˆx0(xt, t) + (cid:112)1 αt1 ϵθ(xt, t). (2) Iterating this update from = to = 0 produces the final sample x0. Our method intervenes at this stage by modifying ˆx0 to enforce trajectory control. Notably, other popular solvers, such as UniPC, EDM, and PNDM (Zhao et al., 2023; Karras et al., 2022; Liu et al., 2022a), also compute ˆx0 directly or can recover it via parameterized transformation, so our framework is broadly compatible. Classifier Free Guidance. To improve fidelity to the condition, CFG (Ho & Salimans, 2021) adjusts the score function during sampling: ϵθ(xt, t) = ϵθ(xt, t, ϕ) + ωCFG [ϵθ(xt, t, c) ϵθ(xt, t, ϕ)] , (3) where ωCFG is the guidance weight, with and ϕ denoting the conditional and unconditional branches, respectively. This interpolates conditional and unconditional scores to steer the sampling trajectory. Our approach extends this principle through self-referential guidance mechanism that dynamically adjusts the guided prediction using the models own unguided output at each step. 3.1.2 TRAJECTORY CONTROL VIA DEPTH-BASED WARPING Our framework uses warping-and-repainting strategy, embedding geometric cues through depthbased view warping. For reliable depth estimation to support warping into new views, we employ depth prediction networks that takes one or more input images {Ii}N i=1 and estimates corresponding camera poses and depth maps: : {Ii}N i=1 {Pi, Di}, (4) where the depth maps Di and poses Pi are then used to warp the source views to target poses. Formally, the warping function projects source frames Isrc with depth Dsrc from pose Psrc to target pose Ptar, producing partial target views tar and validity masks Mtar, which indicate the visible pixels in warped views: (I tar, Mtar) = W(Isrc, Dsrc, Psrc, Ptar). (5) The warped frames tar and masks Mtar provide trajectory-aware observations that serve as the basis for generating videos along arbitrary target poses Ptar, though limited to regions visible in source views. With these preliminaries, we now present our method. Our goal is to use the trajectory control introduced in this section to guide video generation in VDM. To achieve precise and consistent control, we design three modules: Intra-Step Recursive Refinement, Flow-Gated Latent Fusion, and Dual-Path Self-Corrective Guidance. Together, they complement each other to ensure accurate trajectory guidance while preserving high-quality synthesis. 6 3.2 INTRA-STEP RECURSIVE REFINEMENT To enable precise trajectory injection during VDMs inference processing, we introduce Intra-Step Recursive Refinement (IRR). As noted in Sec. 3.1.1, the denoising process produces an intermediate variable ˆx(i) 0 , coarse estimate of the final output and the baseline for later steps. Building on this, IRR modifies ˆx(i) 0 to impose trajectory constraints, ensuring that generation follows the desired path. IRR operates within the update process of Eq. (1) and Eq. (2). Given the one-step denoised sample ˆx(i) from Eq. (1), we fuse it with the trajectory latent Ztraj, obtained by encoding the warped frames 0 from Eq. (5) into the latent space. We then add small Gaussian noise ϵ to obtain modified latent ti: ti = (1 w(σ)) F(ˆx(i) 0 , Ztraj) + w(σ) ϵ, (6) where ϵ = xT (0, I) is the initial Gaussian noise used in sampling. To return the fused result ti into the denoising process while injecting trajectory information, we reintroduce noise ϵ. Here 0 < w() < 1 is user-defined weighting factor derived from the noise schedule σ, which controls the strength of the reintroduced noise. F() is mask-based fusion function defined as F(ˆx(i) 0 , where is the binary validity mask from Eq. (5). It copies observable warped content from Ztraj into the corresponding locations of ˆx(i) 0 , while leaving unobserved regions unchanged. 0 , Ztraj) = Ztraj + (1 M) ˆx(i) In summary, IRR embeds micro predictorcorrector at each denoising step. By updating ˆx(i) 0 with explicit trajectory cues, it continually corrects the sampling path and ensures that synthesis follows the target trajectory precisely. 3.3 FLOW-GATED LATENT FUSION In the IRR process, directly overwriting all latent channels with trajectory information often degrades visual quality. This is because VAE latents tend to encode different types of information: some channels mainly capture appearance, while others relate more to motion. Using Eq. (6) to replace all channels indiscriminately introduces noise into appearance-dominant ones. To avoid this, we introduce Flow-Gated Latent Fusion (FLF), which decouples motion and appearance features in the latent space and selectively injects trajectory guidance into motion-relevant channels. To separate motion from appearance and identify motion-relevant channels, we introduce flowbased scoring scheme. Optical flow captures pixel-wise motion between frames and is widely used to describe temporal dynamics. By comparing predicted and reference flows, we estimate how strongly each latent channel encodes motion. Given the predicted latent ˆx(i) from IRR and the reference trajectory latent Ztraj, we compute 0 per-channel flows using the Farneback algorithm. For each channel c, we obtain predicted flow pred(x, y, τ ) = (u(c) (c) gt (x, y, τ ) = (u(c) gt (x, y, τ ), v(c) gt (x, y, τ )), restricted to valid regions M(c)(x, y, τ ) from Eq. (5). Here (x, y) are pixel coordinates, τ is the time index, and u, are the horizontal and vertical components. pred(x, y, τ )) and reference ground truth (GT) flow (c) pred(x, y, τ ), v(c) To evaluate motion relevance, we design normalized score that combines three masked metrics: Masked End-point Error (M-EPE), Masked Angular Error (M-AE), and Outlier Percentage (Fl-all). M-EPE is the average distance between the vectors of the predicted flow and GT flow; smaller values mean better alignment. M-AE measures the angular difference between flows; smaller values mean more accurate directions. Fl-all is the percentage of pixels with large deviations; lower values mean fewer outliers and higher reliability. Together, these metrics provide robust estimate of each channels motion relevance. The formulas are given below: M-EPE: Given the set Ω of all valid pixels (x, y, τ ) where M(c) = 1, we compute the per-channel L2 distance between predicted and GT flows and normalize by the number of valid pixels Ω: (cid:13) (cid:13)F (c) (cid:13) pred(x, y, τ ) (c) gt (x, y, τ ) M-EPEc = (cid:88) (7) . (cid:13) (cid:13) (cid:13)2 1 Ω (x,y,τ )Ω 7 M-AE: Similar to M-EPE, for each valid pixel in Ω we compute the cosine similarity between (c) pred and (c) gt , and use it to measure angular error: M-AEc = 1 Ω (cid:88) arccos (x,y,τ )Ω (cid:32) (c) (c) pred(x, y, τ ) (c) pred(x, y, τ ) (c) gt (x, y, τ ) gt (x, y, τ ) (cid:33) . (8) Fl-all: valid pixel in Ω is an outlier if its M-EPE exceeds 3 or its relative error exceeds 5%. Fl-all (denoted as Fc) is the percentage of outliers among valid pixels; lower values mean higher reliability. To unify the scales of the three metrics and simplify score computation, we normalize , ANorm M-EPEc, M-AEc, Fc to [0, 1], denoted as ENorm , respectively: , and FNorm c"
        },
        {
            "title": "ENorm\nc",
            "content": "= min(M-EPEc/n1, 1), ANorm = min(M-AEc/n2, 1),"
        },
        {
            "title": "FNorm\nc",
            "content": "= min(Fc/n3, 1), (9) where n1, n2, n3 are normalization factors. We then define the final flow score for each channel as S, which directly reflects the consistency between the predicted and GT flows: (cid:1) , (cid:0)1 FNorm (cid:0)1 ANorm (cid:0)1 ENorm S(c) = γ (cid:1) + γ2 (cid:1) + γ3 (10) c where γ1 + γ2 + γ3 = 1. The flow score S(c) measures the motion relevance of channel c. higher score means its predicted flow is closer to the ground truth and thus encodes more motion. lower score indicates misalignment and suggests that the channel encodes more non-motion information, such as appearance. This score enables us to decouple motion and appearance channels in the latent space. To select motion-relevant channels, we set dynamic threshold: δ(i) = µ(i) λ(i)σ(i) , (11) and σ(i) where µ(i) are the mean and standard deviation of {S(i,c)} at step i. Channels with scores above δ(i) are regarded as motion-relevant and only these channels receive trajectory guidance. The selection sets are: C(i) sel = { S(i,c) δ(i) }. Only these channels are updated with trajectory latents Ztraj. The parameter 0 < λ(i) < 1 controls the strictness of selection and decreases during denoising. Early steps therefore replace more channels for stronger trajectory adherence, while later steps replace fewer channels to preserve appearance details. Finally, the latent update rule for ˆx(i) 0 is: FLF(ˆx(i) 0 , Ztraj)(c) = (cid:40) traj + (cid:0)1 M(c)(cid:1) ˆx(i,c) 0 M(c) Z(c) ˆx(i,c) 0 , , C(i) sel , otherwise. (12) where FLF() denotes the flow-scorebased selective fusion. We use it to replace F() in Eq. (6), which overwrote all channels without distinction. The updated fusion rule is: ti = (1 w(σ)) FLF(ˆx(i) 0 , Ztraj) + w(σ) ϵ, (13) In summary, FLF enables fine-grained trajectory injection while preserving model priors and synthesis quality. Unlike Restart Sampling (Xu et al., 2023), which restarts the schedule, or ViewExtrapolator (Liu et al., 2024), which injects trajectory data directly into ˆx(i) 0 , our method uses IRR to apply fine-grained guidance at every timestep. Combined with FLF, it separates motion from appearance and delivers precise, high-quality trajectory control at inference. 3.4 DUAL-PATH SELF-CORRECTIVE GUIDANCE Trajectory latents Ztraj obtained by warping along the target motion often suffer from distortions due to depth errors, occlusions, or misalignments. Such artifacts are common in warp-based methods and degrade synthesis quality. To address this, we draw inspiration from CFG (Ho & Salimans, 2021), which interpolates between conditional and unconditional predictions. Instead of using conditioning, we exploit the discrepancy between two denoising paths in IRR to obtain more reliable direction. Based on this idea, we propose Dual-Path Self-Corrective Guidance (DSG). At each iteration, IRR produces two velocity fields: the unguided vori ti from the original latent xti, and the 8 guided vtraj ti difference to form correction signal that steers denoising toward higher-quality path. ti obtained by injecting trajectory cues. DSG leverages their from the corrected latent At each step ti, IRR and FLF generate two velocity fields: vori ti . The guided velocity vtraj ti enforces trajectory adherence but may add noise from imperfect warping, while the unguided velocity vori ti stays on the data manifold with higher fidelity but ignores trajectory cues. To exploit these two directions for better denoising path, we propose DSG, which combines them to balance control and fidelity. Unlike CFG (Eq. (3)) (Ho & Salimans, 2021), which interpolates unconditional and conditional predictions, DSG uses two trajectory-specific complementary branches designed for motion control. The corrected velocity vcorr is then computed as: ti and vtraj ti vcorr ti = vtraj ti + ρ βti (cid:0)vtraj ti µ αti vori ti (cid:1), (14) where ρ controls guidance strength, µ normalizes magnitudes, αti = (vtraj ) ti is the cosine similarity between guided and unguided velocities, and βti = sin(arccos(αti)) is the corresponding sine similarity. This adaptive weighting amplifies corrections when the two paths diverge (lower αti, higher βti), pulling the result toward the guided direction, and reduces corrections when they agree (higher αti, lower βti), preserving the models natural prediction. ti vori ti )/(vtraj vori ti ti follows the pretrained data manifold, while vtraj In score-based (Lipman et al., 2022) terms, vori ti may deviate due to injected trajectory signals. DSG applies cosine-weighted interpolation between the two, dynamically combining them to retain trajectory guidance while projecting the velocity back toward the manifold. This improves generation quality by suppressing off-manifold drift. As result, vcorr ti drives the sample along the desired motion path while preserving visual fidelity to the model priors. Subsequent experiments confirm the effectiveness of this mechanism."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we present comprehensive evaluation of our proposed training-free framework. We first outline the implementation details in Sec. 4.1. Subsequently, we demonstrate the performance of our method on 3D scene generation and 4D trajectory control in Sec. 4.2 and Sec. 4.3, respectively. Finally, we conduct series of ablation studies in Sec. 4.4 to validate the effectiveness of each component of our approach. 4.1 IMPLEMENTATION DETAILS Our framework is training-free, inference-time optimization method that steers pre-trained video diffusion models for precise camera control without additional training or fine-tuning. It introduces no significant computational overhead beyond the base models inference requirements. Setup. Experiments are primarily conducted on the Wan2.1 Image-to-Video (I2V-14B) model (Wan et al., 2025). Generation runs on single GPU with 69GB VRAM, producing videos up to 1280720 resolution. The length of each video generated in single pass depends on the capacity limit of the chosen VDM; longer sequences are obtained by concatenation. Inference time increases by 40-50%, mainly due to the IRR mechanism. For ablation and fair comparison, we also evaluate on SVD (Blattmann et al., 2023), which runs on 24GB RTX 4090 for 25-frame inference with similar time overhead. Our pipeline follows warp-and-repaint design. For warping, we test several depth estimation models, such as VGGT (Wang et al., 2025b), UniDepth (Piccinelli et al., 2024), Mega-SaM (Li et al., 2025), and DepthCrafter (Hu et al., 2025). The method adapts well to all, benefiting from the strong world priors of the underlying video model. Test Datasets and Metrics. For single-view 3D scene generation, we adopt the widely used LLFF (Mildenhall et al., 2019), Tanks and Temples (Knapitsch et al., 2017), and MipNeRF 360 (Barron et al., 2022) datasets, selecting half of the scenes from each for evaluation. We also test on diverse internet, real-world, and AI-generated images. Perceptual quality is assessed using FID (Heusel et al., 2017) and CLIPsim (Radford et al., 2021) similarity. For 4D trajectory control, we compare with SOTA methods on their respective benchmarks, which include challenging real-world video clips with varied camera trajectories. 9 Figure 3: Qualitative comparison of novel view synthesis from single input image. The first row shows the input frame and its depth-based warped views, where disoccluded regions appear as holes. Compared to existing SOTA methods, our approach produces more consistent scene content under novel viewpoints, with improved image detail, trajectory accuracy, and structural plausibility. Performance is measured with FVD (Unterthiner et al., 2018) for temporal quality and CLIP-Vsim for sourcetarget consistency. In addition, we assess camera trajectory accuracy using three standard metrics: Absolute Trajectory Error (ATE), which captures the global alignment between estimated and reference paths; Relative Pose Error Translation (RPE-T), which reflects short-term consistency of translational motion; and Relative Pose Error Rotation (RPE-R), which quantifies local orientation accuracy. Together, these metrics provide comprehensive evaluation of both global fidelity and local smoothness of camera trajectories. 4.2 3D SCENE GENERATION We compare our method with SOTA approaches for novel view synthesis, which can be grouped into two categories. The first includes methods requiring model-specific training or fine-tuning, such as ViewCrafter (Yu et al., 2024c), which trains VDM on iteratively refined point clouds; TrajectoryCrafter (Yu et al., 2025), which learns motion control via dedicated training; TrajectoryAttention (Xiao et al., 2025), which injects motion through specialized attention module; and See3D(Ma et al., 2025a), video-based model for scene reconstruction. The second category comprises training-free methods, such as NVS-Solver (You et al., 2025), which modulates pre-trained model using warped views, and ViewExtrapolator (Liu et al., 2024), which employs an annealed guidance strategy to improve trajectory adherence. We evaluate all methods under consistent settings with the same input image, camera trajectory, depth, and target scenes across LLFF (Mildenhall et al., 2019), MipNeRF-360 (Barron et al., 2022), and Tanks-and-Temples (Knapitsch et al., 2017). Results (Figs.3, Table1) show that our inferencetime guidance consistently outperforms both training-based and training-free baselines. Moreover, we further assess human-centric scenes, which are more sensitive to visual consistency and content 10 Table 1: Quantitative results on static 3D and dynamic 4D benchmarks. We evaluate on public datasets such as LLFF and MipNeRF360, as well as Internet images and videos. For static scenes we report CLIPsim and FID, and for dynamic scenes CLIP-Vsim and FVD. Metric details are provided in Appendix B. Our method consistently surpasses SOTA baselines in generation quality."
        },
        {
            "title": "Dynamic",
            "content": "FID CLIPsim FVD CLIP-Vsim See3D (Ma et al., 2025a) ViewCrafter (Yu et al., 2024c) ViewExtrapolator (Liu et al., 2024) TrajectoryAttention (Xiao et al., 2025) TrajectoryCrafter (Yu et al., 2025) NVS-Solver (You et al., 2025) WorldForge (Ours) 123.26 117.50 125.50 122.37 111.49 118.64 96. 0.941 0.930 0.930 0.920 0.910 0.937 0.948 - - 108.48 106.94 97.31 - 93.17 - - 0.913 0.911 0.923 - 0. Table 2: Quantitative comparison of camera-trajectory accuracy on static and dynamic scenes. We evaluate against SOTA methods using ATE, RPE-T, and RPE-R, lower is better. Across both settings, our method achieves the best or second-best results on all metrics. Metric definitions and computation details are provided in Appendix B. Static Dynamic ATE RPE-T RPE-R ATE RPE-T RPE-R See3D (Ma et al., 2025a) ViewCrafter (Yu et al., 2024c) ViewExtrapolator (Liu et al., 2024) 0.091 0.236 0.183 TrajectoryAttention (Xiao et al., 2025) 0.159 0.090 0. TrajectoryCrafter (Yu et al., 2025) NVS-Solver (You et al., 2025) 0.089 0.315 0.260 0.238 0.152 0.268 0.250 0.728 0.882 0.532 0.267 1.056 - - 1.040 0.605 0.431 - - - 1.208 1.238 1.078 - WorldForge (Ours) 0.077 0.086 0.221 0.527 0.826 - - 4.750 3.560 8.950 - 2.690 In contrast, our approach, quality. As shown in Fig. 4, prior methods often fail in such cases. supported by the proposed guidance strategy, preserves model priors and faithfully follows the target trajectory. This offers an effective paradigm for reconciling controllability with generalization and high fidelity. To independently validate the effect of our guidance strategy, we further evaluate on the lighter U-Net-based SVD model (Blattmann et al., 2023); even in this setting, our method attains superior visual quality. Notably, our framework can synthesize high-quality, photorealistic, and structurally consistent 360 views from single input without relying on panoramic intermediates  (Fig. 5)  . This capability extends to complex outdoor and open-world scenes, pushing the limits of single-view novel view synthesis. 4.3 4D TRAJECTORY CONTROL For dynamic trajectory control, we compare against leading video-to-video models. These include ReCamMaster (Bai et al., 2025a), which employs sophisticated conditioning mechanism trained on large-scale synthetic data, TrajectoryCrafter (Yu et al., 2025), with its dual-stream diffusion architecture, TrajectoryAttention (Xiao et al., 2025), which processes motion through an additional attention module, and ViewExtrapolator (Liu et al., 2024). Following the protocols of prior works, we evaluate on diverse and challenging video clips, including arcs, dolly zooms, and composite paths. Our training-free method consistently delivers higher visual fidelity, better trajectory alignment, and more coherent scene completion. As shown in Table 1 and Fig. 6, it matches or surpasses state-of-the-art methods that require costly training, demonstrating the 11 Table 3: Efficiency comparison. We measure inference time on single NVIDIA A100 across methods built on SVD(Blattmann et al., 2023), Wan 2.1Wan et al. (2025), CogVideoX Yang et al. (2024), and custom backbones. ReCamMaster is evaluated at 81 frames; all others use 25 frames. Our method is training-free and plug-and-play, thus incurring zero training cost. Its runtime adds 40% over the base video model, attributable to the IRR recursive refinement. Overall, it achieves comparable or faster inference than prior approaches while avoiding any training overhead."
        },
        {
            "title": "Frames Resolution",
            "content": "Inference Time (min)"
        },
        {
            "title": "Base Video\nModel",
            "content": "See3D (Ma et al., 2025a) ViewCrafter (Yu et al., 2024c) ViewExtrapolator (Liu et al., 2024) TrajectoryAttention (Xiao et al., 2025) TrajectoryCrafter (Yu et al., 2025) NVS-Solver (You et al., 2025) ReCamMaster (Bai et al., 2025a) WorldForge (Ours, 720P) WorldForge (Ours, 480P) WorldForge (Ours, on SVD) 25 25 25 25 25 25 81 25 25 25 576 1024 576 1024 576 1024 576 1024 384 672 576 1024 480 832 1280 720 480 832 576 1.7 1.8 1.6 5.5 1.7 9.3 14.6 17.3 6.8 1.3 Custom Custom SVD SVD CogVideoX SVD Wan 2.1 T2V Wan 2.1 I2V Wan 2.1 I2V SVD TrainingFree Figure 4: Static 3D generation on human-centric scenes. Existing methods struggle, particularly with motion-prone shots (left) and portrait close-ups (right). On the left, baselines introduce artifacts and unintended motion. On the right, most fail to produce plausible results; TrajectoryCrafter (Yu et al., 2025) recovers coarse structure but lacks detail and visual appeal. In contrast, our method maintains scene stationarity under trajectory guidance and produces natural, faithful renderings, achieving both precise control and high perceptual quality. strength of leveraging latent 3D priors in existing models. Notably, our approach excels at plausibly reconstructing unseen regions, while other methods often produce distortions or implausible content. Beyond benchmarks, the framework supports various post-production tasks. It can stabilize videos by smoothing camera motion and control camera paths to enable localized super-resolution or outpainting. In addition, by specifying masked regions, it supports creative video content edits such as object addition or removal, subject replacement, and try-on effects (see Fig. 7). These capabilities highlight its versatility for real-world video re-rendering. 4.4 ABLATION EXPERIMENTS We conducted ablation studies to assess the contribution of each component in our framework. Component Analysis. We examined the effects of removing Intra-Step Recursive Refinement (IRR), Flow-Gated Latent Fusion (FLF), and Dual-Path Self-Corrective Guidance (DSG). As shown in Fig. 8 without IRR, trajectory guidance cannot be injected during inference, and the model fails 12 Figure 5: 360 orbit views from single real-world outdoor image. With precise trajectory control and realistic rendering, our method overcomes the viewpoint limitation of single-image generation and produces ultra-wide views of complex real scenes. Unlike panorama-based approaches, it directly supports object-centric trajectories and achieves higher visual quality. Figure 6: Comparison of 4D trajectory-controlled re-rendering. Baselines often produce implausible artifacts (e.g., flattened faces, floating heads), reflecting limited use of pretrained priors. Our inference-time guidance leverages these latent world priors to re-render realistic, high-quality content along the target trajectory. We compare against state-of-the-art baselines under identical inputs; for ReCamMaster (text-controlled), parameters are adjusted to match the target path. to follow the target path. Removing FLF, i.e., not separating motion and appearance channels, damages the models priors and produces unnatural results. Without DSG, noise from warped trajectories propagates into the generation, causing low quality and artifacts. The full model achieves the best results, confirming that these components work synergistically to ensure robust and precise control. Base Model. We replaced the Wan2.1 (Wan et al., 2025) model with the U-Net-based SVD (Blattmann et al., 2023) model to verify model-agnosticism and transferability (See in Fig. 9). Fair comparisons with other SVD-based methods show that our approach integrates seamlessly into pretrained SVD. Although limited by SVDs weaker priors, our method still reaches state-of-the-art performance within the same model. This demonstrates that our guidance is architecture-independent and suggests even greater potential with stronger future base models. Depth Estimation Model. As shown in Fig. 10 We tested VGGT (Wang et al., 2025b), Mega-SaM (Li et al., 2025), UniDepth (Piccinelli et al., 2024) and DepthCrafter (Hu et al., 2025) for depthbased warping. Each shows distinct strengths and weaknesses, yet our method remains effective across all. Thanks to the generative models strong 3D prior, many warping artifacts (e.g., tearing, 13 Figure 7: Other video effects enabled by our method. Beyond video re-cam, our flexible depthbased warping also supports various video editing operations, such as freezing the camera, stabilizing video, and editing video content. These extensions further broaden the practical scope of our approach. Figure 8: Ablation of the proposed components. IRR enables trajectory injection; without it, the model defaults to prompt-only free generation, and FLF/DSG cannot be applied. FLF decouples trajectory cues from noisy content; removing it introduces noise from warped frames. DSG guides sampling toward high-quality, trajectory-consistent results; without it, detail and plausibility drop. The full model achieves the best fidelity and control, demonstrating their complementary effects. stretching) are automatically corrected during refinement. This robustness enables plug-and-play integration with various SOTA depth estimation techniques without sacrificing performance. 14 Figure 9: Ablation across different VDMs. To rule out the influence of the intrinsic performance advantage of the VDM (Wan2.1 (Wan et al., 2025)) and to verify the methods transferability, we port the proposed guidance to compact U-Netbased SVD model (Blattmann et al., 2023) and compare against SVD-based SOTA baselines. Experiments show that the guidance transfers seamlessly, makes the native SVD controllable, and achieves SOTA performance in content quality, structural plausibility, and trajectory consistency. Figure 10: Depth estimation models ablation. Our method leverages the inherent world knowledge of VDMs to correct errors and fill missing regions even under challenging inputs (left). This strong self-correction ability ensures broad compatibility with different depth estimators (right). Despite variations or noise in depth-based warping, it reliably compensates through learned priors and produces realistic, high-quality results."
        },
        {
            "title": "5 CONCLUSION AND LIMITATION",
            "content": "We present WorldForge, training-free framework for trajectory-controllable generation in both static 3D scenes and dynamic 4D scenes. Our method tackles the persistent challenge in controllable video generation of balancing high visual quality, strong generalization, and precise controllability. At its core is unified guidance strategyIntra-Step Recursive Refinement (IRR), Flow-Gated Latent Fusion (FLF), and Dual-Path Self-Corrective Guidance (DSG)that operates entirely at inference time. By decoupling motion from appearance features and correcting trajectory drift caused by structural noise, it injects fine-grained predefined trajectory constraints while preserving the valuable prior world knowledge embedded in the base model. Extensive experiments demonstrate state-ofthe-art performance on both 3D and 4D trajectory generation tasks, offering new paradigm for exploring spatial intelligence and emergent world models in large-scale generative systems. 15 While our framework can repair many distortions introduced by depth-based warping, it may fail under extremely poor depth estimations (e.g., completely flattened subjects or severe foregroundbackground entanglement). Moreover, due to the global nature of our guidance, control over small objects or fine details remains limited. Future work will explore integrating fine-grained control mechanisms and applying our approach to more powerful generative models."
        },
        {
            "title": "REFERENCES",
            "content": "Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David B. Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025a. Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, David B. Lindell, and Sergey Tulyakov. Vd3d: Taming large video diffusion transformers for 3d camera control. In International Conference on Learning Representations (ICLR), 2025b. Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, and Di Zhang. Recammaster: Camera-controlled generative rendering from single video. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025a. Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. In International Conference on Learning Representations (ICLR), 2025b. Lichen Bai, Shitong Shao, Zikai Zhou, Zipeng Qi, Zhiqiang Xu, Haoyi Xiong, and Zeke Xie. Zigzag diffusion sampling: Diffusion models can self-improve via self-reflection. In International Conference on Learning Representations (ICLR), 2025c. Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1579115801, 2025. Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 54705479, 2022. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In International Conference on Machine Learning (ICML), 2024. Xudong Cai, Yongcai Wang, Zhaoxin Fan, Deng Haoran, Shuo Wang, Wanting Li, Deying Li, Lun Luo, Minhang Wang, and Jintao Xu. Dust to tower: Coarse-to-fine photo-realistic scene reconstruction from sparse uncalibrated images. arXiv preprint arXiv:2412.19518, 2024. Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowei Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, et al. Reconstructing 4d spatial intelligence: survey. arXiv preprint arXiv:2507.21045, 2025. Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983, 2025. Simone Foti, Bongjin Koo, Danail Stoyanov, and Matthew Clarkson. 3d shape variational autoencoder latent disentanglement via mini-batch feature swapping for bodies and faces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1873018739, 2022. 16 Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Ruiqi Gao, Emiel Hoogeboom, Jonathan Heek, Valentin De Bortoli, Kevin Patrick Murphy, and Tim Salimans. Diffusion models and gaussian flow matching: Two sides of the same coin. In The Fourth Blogpost Track at ICLR 2025, 2025. Google DeepMind. Veo 3 tech report. Google Developers Blog, 2025. URL https://storage. googleapis.com/deepmind-media/veo/Veo-3-Tech-Report.pdf. Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation In Proceedings of the Special Interest Group on Computer Graphics and Interactive control. Techniques Conference Conference Papers (SIGGRAPH), pp. 112, 2025. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan arXiv preprint Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv:2404.02101, 2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems (NeurIPS), 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop on Deep Generative Models and Downstream Applications, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 81538163, 2024. Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu, Zhenwei Wang, Junta Wu, Jie Jiang, Hui Li, Rynson WH Lau, Wangmeng Zuo, et al. Voyager: Long-range and world-consistent video diffusion for explorable 3d scene generation. arXiv preprint arXiv:2506.04225, 2025. Junha Hyung, Kinam Kim, Susung Hong, Min-Jung Kim, and Jaegul Choo. Spatiotemporal skip guidance for enhanced video diffusion sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1100611015, 2025. Hyeonho Jeong, Suhyeon Lee, and Jong Chul Ye. Reangle-a-video: 4d video generation as video-tovideo translation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Tero Karras, Miika Aittala, Tuomas Kynkannummi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Benedikt Kerbl, Georgios Kopanas, Till Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 2023. Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems (NeurIPS), 2023. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in Neural Information Processing Systems (NeurIPS), 2021. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (TOG), 36(4):113, 2017. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Min-Seop Kwak, Donghoon Ahn, In`es Hyeonsu Kim, Jin-Hwa Kim, and Seungryong Kim. Geometry-aware score distillation via 3d consistent noising and gradient consistency modeling. arXiv preprint arXiv:2406.16695, 2024. Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1048610496, 2025. Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 798810, 2025. Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Kunhao Liu, Ling Shao, and Shijian Lu. Novel view extrapolation with video diffusion priors. arXiv preprint arXiv:2411.14208, 2024. Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In International Conference on Learning Representations (ICLR), 2022a. Tianqi Liu, Zihao Huang, Zhaoxi Chen, Guangcong Wang, Shoukang Hu, Liao Shen, Huiqiang Sun, Zhiguo Cao, Wei Li, and Ziwei Liu. Free4d: Tuning-free 4d scene generation with spatialtemporal consistency. arXiv preprint arXiv:2503.20785, 2025. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022b. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems (NeurIPS), 2022. Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 20162029, 2025a. Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, and Forrester Cole. Vidpanos: Generative panoramic videos from casual panning videos. In SIGGRAPH Asia 2024 Conference Papers, pp. 111, 2024a. 18 Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 38, pp. 41174125, 2024b. Yue Ma, Kunyu Feng, Xinhua Zhang, Hongyu Liu, David Junhao Zhang, Jinbo Xing, Yinhan Zhang, Ayden Yang, Zeyu Wang, and Qifeng Chen. Follow-your-creation: Empowering 4d creation through video inpainting. arXiv preprint arXiv:2506.04590, 2025b. Ben Mildenhall, Pratul Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 38(4):114, 2019. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision (ECCV), 2020. Chengzhi Mou, Xiang Wang, Linjie Xie, Zhiding Xu, Mohammad Rastegari, and Richard Hartley. T2i-adapter: Learning adapters for controllable text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2024. Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and In Proceedings of the Fisher Yu. Unidepth: Universal monocular metric depth estimation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1010610116, 2024. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In International Conference on Learning Representations (ICLR), 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Mearning (ICML), pp. 87488763, 2021. Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 61216132, 2025. Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In International Conference on Learning Representations (ICLR), 2024. Chenxi Song, Shigang Wang, Jian Wei, and Yan Zhao. Fewarnet: An efficient few-shot view synthesis network based on trend regularization. IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 34(10):92649280, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Fengrui Tian, Tianjiao Ding, Jinqi Luo, Hancheng Min, and Rene Vidal. Voyaging into unbounded dynamic scenes from single view. arXiv preprint arXiv:2507.04183, 2025. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 19 Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: Extreme monocular dynamic novel view synthesis. In European Conference on Computer Vision (ECCV), pp. 313331. Springer, 2024. Team Wan, Ang Wang, Baole Ai, Bin Wen, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Haiping Wang, Yuan Liu, Ziwei Liu, Wenping Wang, Zhen Dong, and Bisheng Yang. Vistadream: arXiv preprint Sampling multiview consistent images for single-view scene reconstruction. arXiv:2410.16892, 2024a. Hanyang Wang, Fangfu Liu, Jiawei Chi, and Yueqi Duan. Videoscene: Distilling video diffusion model to generate 3d scenes in one step. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1647516485. IEEE, 2025a. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 52945306, 2025b. Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, and Jian Zhang. 360dvd: Controllable panorama video generation with 360-degree video diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 69136923, 2024b. Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In Proceedings of the ACM SIGGRAPH Conference on Computer Graphics and Interactive Techniques (SIGGRAPH), 2024c. Min Wei, Jingkai Zhou, Junyao Sun, and Xuesong Zhang. Adversarial score distillation: When score distillation meets gan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, and Huan Ling. Difix3d+: Improving 3d reconstructions with single-step diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2602426035, 2025. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 21469 21480, 2025. FU Xiao, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. In International Conference on Learning Representations (ICLR), 2024. Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Trajectory attention for fine-grained video motion control. In International Conference on Learning Representations (ICLR), 2025. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision (ECCV), pp. 399417, 2024. Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, and Tommi Jaakkola. Restart sampling for improving generative processes. Advances in Neural Information Processing Systems (NeurIPS), 36:7680676838, 2023. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 20 Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In European conference on computer vision (ECCV), pp. 767783, 2018. Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 90439053, 2023. Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. In International Conference on Learning Representations (ICLR), 2025. Han Yu, Jiashuo Liu, Xingxuan Zhang, Jiayun Wu, and Peng Cui. survey on evaluation of out-ofdistribution generalization. arXiv preprint arXiv:2403.01874, 2024a. Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo Attila Jeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via video diffusion models. In Advances in Neural Information Processing Systems (NeurIPS), 2024b. Mark Yu, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, TienTsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024c. Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Aliasfree 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1944719456, 2024d. David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David E. Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, and Nataniel Ruiz. Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Liyuan Zhang, Aojun Rao, and Maneesh Agrawala. Controlnet: Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictorcorrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems (NeurIPS), 36:4984249869, 2023."
        },
        {
            "title": "A PROOF OF THE EQUIVALENCE BETWEEN DIFFUSION AND FLOW MODELS",
            "content": "We consider Flow Matching (Lipman et al., 2022; Liu et al., 2022b) as special case of diffusion modeling (Kingma & Gao, 2023; Gao et al., 2025). In the following, we will first outline the formulation of diffusion models and then substitute the specific parameterization of Flow Matching to demonstrate their compatibility. Given random variable x0 drawn from an unknown data distribution q0(x0), Diffusion Probabilistic Model (DPM) (Ho et al., 2020; Song et al., 2020b; Lu et al., 2022) defines forward process that gradually transforms the data into simple prior distribution, typically Gaussian distribution. The conditional distribution of the noised variable xt at time given the initial data x0 is defined as Gaussian transition kernel (Kingma et al., 2021): qt(xtx0) = (xtαtx0, σ2 I). (15) Equivalently, sample xt at any time [0, ] can be expressed through reparameterization (Kingma et al., 2021; Gao et al., 2025): xt = αtx0 + σtϵ, ϵ (0, I). (16) Here, αt and σt are scalar functions of time, known as the noise schedule, that control the signalto-noise ratio. Typically, αt decreases over time while σt increases, satisfying condition such as α2 + σ2 = 1 in Variance Preserving (VP) SDEs (Ho et al., 2020; Song et al., 2020b). Kingma et al. (2021) proves that the following stochastic differential equation (SDE) has the same transition distribution in Eq. (15) for any [0, ]: dxt = (t)xtdt + g(t)dwt, x0 q0(x0), (17) where wt is standard Wiener process. The drift coefficient (t) and the diffusion coefficient g(t) can be derived using schedule parameters αt and σt (Kingma et al., 2021): (t) = log αt dt , g2(t) = dσ2 dt log αt dt σ2 . (18) The generative process of diffusion models involves reversing this forward process. This can be achieved via corresponding reverse-time SDE (Song et al., 2020b). For more efficient generation, one can utilize the associated probability flow ordinary differential equation (PF-ODE), which shares the same marginal distributions as at each time as that of the SDE (Song et al., 2020b). This PFODE is given by: 1 2 By relating the score function xt log pt(xt) to the noise term via xt log pt(xt) ϵθ(xt,t) , where ϵθ is neural network trained to predict the noise, the ODE becomes (Karras et al., 2022; Zhao et al., 2023): g2(t)xt log pt(xt). = (t)xt dxt dt (19) σt dxt dt = (t)xt + g2(t) 2σt ϵθ(xt, t). (20) Now, let us consider the forward process in Flow Matching (Lipman et al., 2022; Liu et al., 2022b). The path from data point x0 to noise sample ϵ is defined by simple linear interpolation: xt = (1 t)x0 + ϵ, ϵ (0, I), (21) where [0, 1]. By comparing Eq. (21) with the general form of the diffusion forward process in Eq. (16), we can establish direct correspondence by setting the diffusion schedule parameters as: Substituting this specific parameterization into the definitions for (t) and g(t) in Eq. (18), we derive the corresponding coefficients for this Flow Matching SDE: αt = 1 and σt = t. fFM(t) = log(1 t) dt = 1 1 , g2 FM(t) = d(t2) dt 1 1 t2 = 2t 1 . 22 (22) (23) Next, we insert these specific coefficients fFM(t) and g2 FM(t) into the PF-ODE formulation from Eq. (20). To analyze the underlying dynamics, we consider the ideal case where the score is perfectly known, which is equivalent to replacing the model prediction ϵθ(xt, t) with the ground-truth noise ϵ. This yields: dxt dt = fFM(t)xt + ϵ ϵ g2 FM(t) 2σt 2t 2t (1 t) xt + 1 1 ϵ xt 1 ϵ [(1 t)x0 + ϵ] 1 (1 t)ϵ (1 t)x0 1 = = = = = ϵ x0. (24) This resultant vector field, dxt dt = ϵ x0, is precisely the time derivative of the Flow Matching path defined in Eq. (21). This equivalence demonstrates that the process prescribed by Flow Matching is specific instance of the diffusion models, corresponding to the linear noise schedule αt = 1 and σt = t. Therefore, Flow Matching can be formally viewed as subset of the broader diffusion modeling framework (Kingma & Gao, 2023; Gao et al., 2025)."
        },
        {
            "title": "B EVALUATION METRICS",
            "content": "We employ seven complementary metrics to comprehensively evaluate video generation quality: FID and CLIPsim similarity for static scenes, FVD and CLIP-Vsim for dynamic scenes, and ATE, RPE-T, and RPE-R for camera trajectory consistency. These metrics provide objective quantitative assessment across multiple dimensions including image realism, semantic consistency, temporal coherence, and camera motion fidelity. B.1 STATIC SCENE EVALUATION Frechet Inception Distance (FID). FID (Heusel et al., 2017) measures image generation quality by comparing the distribution of real and generated images in the Inception-V3 feature space. We use an ImageNet-pretrained Inception-V3 model and extract 2048-dimensional features from the pool3 layer. The FID score is computed as: FID = µr µg2 + Tr(Σr + Σg 2(ΣrΣg)1/2) (25) where µr and µg are the mean vectors of real and generated image features, and Σr and Σg are the corresponding covariance matrices. CLIP Similarity. CLIP similarity (Radford et al., 2021) evaluates the semantic similarity between generated and real images using vision-language pre-trained representations. We employ the CLIP ViT-B/32 model trained on 400 million image-text pairs. The similarity score is calculated as: CLIPsim = 1 (cid:88) i=1 cos(fr,i, fg,i) (26) where fr,i and fg,i are the L2-normalized 512-dimensional CLIP features of the i-th real and generated image pair. B.2 DYNAMIC SCENE EVALUATION Frechet Video Distance (FVD). FVD (Unterthiner et al., 2018) extends FID to the video domain by measuring distribution differences in spatio-temporal feature space. For computational efficiency, we implement simplified version that treats video frame sequences as image collections and applies 23 Inception-V3 frame-wise feature extraction. The FVD score follows the same Frechet distance formula as FID but operates on video frame features. Video CLIP Similarity (CLIP-Vsim). CLIP-Vsim extends CLIP similarity to the temporal domain by computing frame-level semantic consistency between generated and real videos. The score is calculated as: CLIP-Vsim ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) j="
        },
        {
            "title": "1\nTj",
            "content": "Tj (cid:88) t=1 cos(fr,j,t, fg,j,t) (27) where is the number of video pairs, Tj is the frame count of the j-th video pair, and fr,j,t, fg,j,t are the CLIP features of the t-th frame in the j-th video pair. B.3 CAMERA TRAJECTORY EVALUATION Absolute Trajectory Error (ATE). ATE measures the global consistency between estimated and reference camera trajectories by computing Euclidean distances between corresponding camera positions. The ATE for each timestamp is computed as: where tref,i and test,i are the 3D position vectors of the reference and estimated trajectories at timestamp i, respectively. The root mean square error over all trajectory points is: ATEi = tref,i test,i2 (28) ATE = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) i=1 ATE2 (29) Relative Pose Error - Translation (RPE-T). RPE-T evaluates the local accuracy of camera translation between consecutive frames, reflecting short-term trajectory consistency. For consecutive frames, the relative translation error is: RPE-Ti = tref,i test,i2 (30) where tref,i = tref,i+1 tref,i and test,i = test,i+1 test,i. The RMSE over all 1 consecutive frame pairs is: (cid:118) (cid:117) (cid:117) (cid:116) RPE-T = 1 1 n1 (cid:88) i=1 RPE-T2 (31) Relative Pose Error - Rotation (RPE-R). RPE-R assesses the accuracy of camera orientation changes between consecutive frames. For rotation matrices Rref,i and Rest,i, the angular difference in degrees is computed as: RPE-Ri = arccos (cid:32) trace(RT ref,iRest,i) 1 2 (cid:33) 180 π (32) where Rref,i = Rref,i+1RT pairs is: ref,i and Rest,i = Rest,i+1RT est,i. The RMSE over all consecutive frame RPE-R = (cid:118) (cid:117) (cid:117) (cid:116) 1 1 n1 (cid:88) i=1 RPE-R2 (33) B.4 IMPLEMENTATION DETAILS Preprocessing. Images are resized to 299 299 pixels for FID/FVD computation and 224 224 pixels for CLIP-based metrics, with appropriate normalization applied. For camera trajectory evaluation, images are resized to 720 480 pixels and uniformly sampled to 20 frames while preserving the first and last frames. Videos are uniformly sampled to 25 frames while preserving the first and last frames. For computational efficiency, FVD calculation further samples to 16 frames, and CLIP-Vsim processing is limited to 20 frames for long videos. 24 Evaluation Protocol. For static scenes with multiple reference images, we directly construct the real distribution using all available images. For single-image scenes, we apply minimal augmentation strategies to avoid singular covariance matrices. Dynamic scenes maintain frame correspondence between generated and reference videos to ensure fair comparison. For camera trajectory evaluation, we employ Structure-from-Motion (SfM) to reconstruct camera poses from image sequences, then apply Sim3 alignment to handle scale ambiguity inherent in monocular reconstruction. Trajectory comparisons are performed using the evo toolkit with appropriate alignment and scale correction parameters."
        }
    ],
    "affiliations": [
        "AGI Lab, School of Engineering, Westlake University, Hangzhou, China",
        "The College of Computing and Data Science, Nanyang Technological University, Singapore"
    ]
}