{
    "paper_title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks",
    "authors": [
        "Mohammad Reza Taesiri",
        "Brandon Collins",
        "Logan Bolton",
        "Viet Dac Lai",
        "Franck Dernoncourt",
        "Trung Bui",
        "Anh Totti Nguyen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: https://psrdataset.github.io"
        },
        {
            "title": "Start",
            "content": "Mohammad Reza Taesiri1* mtaesiri@gmail.com Brandon Collins2 blc0063@auburn.edu Logan Bolton2 logan.bolton@auburn.edu Viet Dac Lai3 daclai@adobe.com Franck Dernoncourt3 dernonco@adobe.com Trung Bui3 bui@adobe.com Anh Totti Nguyen2 anh.ng8@gmail.com 1University of Alberta 2Auburn University 3Adobe Research"
        },
        {
            "title": "Abstract",
            "content": "5 2 0 2 2 2 ] . [ 1 1 8 1 6 1 . 5 0 5 2 : r Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of on March 25, 2025. However, what GPT-4o subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes, or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present unique study addressing these questions by analyzing 83k requests with their associated 305k edits from the recent 12 years on the /r/PhotoshopRequest Reddit community. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including ). Interestingly, AI editors perform worse on lowcreativity requests that require precise editing than on more open-ended requests. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently than human judges and may prefer AI edits over human edits. Code and qualitative examples are available at: https://psrdataset.github.io/. , ,"
        },
        {
            "title": "Introduction",
            "content": "GenAI for images has gained enormous research interest [17] and created 2023 market of $300M, which is estimated to multiply [19]. Specifically, text-based image editing is an increasingly highdemand task [45], especially after the recent GPT4o and Gemini-2.5-Pro image generators [36]. How- *While MRT is the lead, all first 3 authors made major contributions to code, running experiments, and analyzing results. 1 Figure 1: We propose PSR, the largest dataset of realworld image-editing requests and human-made edits. PSR enables the community (and our work) to identify types of requests that can be automated using existing AIs and those that need improvement. PSR is the first dataset to tag all requests with WordNet subjects, realworld editing actions, and creativity levels. ever, four important questions remain open: Q1: What are the real everyday image editing requests and needs of users? Q2: According to human judgment, what % of such requests can be satisfied by existing AIs? Q3: What are the improvement areas for AI editors compared to human editors? Q4: Are vision language models (VLMs) judging AI-edited images similarly to human judges? Q1 and Q2 are unanswered partly because many prior datasets (e.g., [4, 44, 42, 58]) contain madeup requests written by either human annotators or AIs based on the source image or the (source, target image) pair (see Tab. 1). Those request distributions may not reflect the actual editing needs of users as well as the challenges posed by the real requests, which may have typos, distraction or ambiguity (e.g., Id love to see how crazy this could get... Thank you in advance!! in this post; Fig. 1). On the other hand, some datasets (e.g., [54, 4, 42, 58]) feature images AI-edited and therefore do not represent the real edits by humans. We aim to answer these four questions by analyzing 305k tuples of (source image, text request, edited image) from the /r/PhotoshopRequest (PSR) Reddit channel, which is the largest public online community [11, 46] that shares diverse, everyday image-editing needs with corresponding edits by PSR wizards1. PSR has 1.7M users and receives an average of 141 new requests per day (our statistics for 2025) with peak as high as 226 per day [9]. To answer Q1, Q2, and Q3, our closed-loop study (a) analyzes 305k tuples, i.e., 83k unique requests 3.67 human-edited images per request; (b) sends all (request, source image) pairs to imageediting models to collect AI edits; (c) performs human study over set of 328 requests (PSR-328) to collect over 4.5k ratings to compare how 1,644 PSR-wizard edits fare against 2,296 AI edits on the same requests to identify areas where AIs perform well and fall short. Our work is the first to compare three state-of-the-art (SOTA) image editors: GPT-4o [21], and [45] as well as 46 other AI models on SeedEdit HuggingFace, for total of 49 AI editors. Furthermore, to address Q4, we compare human ratings against those by 3 SOTA vision-language models [33], and Gemini-2.0-Flash- (VLMs): GPT-4o, o1 Thinking [14]. Our main findings are: [35], Gemini-2.0-Flash 66% of the time, human judges still prefer human edits over AI edits (Sec. 5.1). While SOTA VLMs are excellent at regular visual tasks [8], on image-edit judgment, VLMs can be extremely biased, e.g., o1 prefers GPT4o edits 85% of the time, which is in stark contrast to human judgment (Sec. 5.2). AIs often add extra non-requested changes that improve the aesthetics of the image but also fail to preserve the subjects identity (Sec. 5.3). AIs either tie or win 33.35% of requests, while PSR wizards still outperform AI editors on the remaining 66.65% (Sec. 5.4)."
        },
        {
            "title": "2 Related Work",
            "content": "Online image editing communities have been studied to understand user intent, common editing 1Advanced image editors who are granted to handle paid editing requests in this particular subreddit. 2 patterns, and challenges in automating image editing [31, 27, 43]. However, prior image-editing taxonomies [31] of editing actions often correspond to the low-level functions available in image-editing software (e.g., zoom or select) and do not correspond to the editing intent of humans who request changes (e.g., human would not want to select an object in an image but rather it is the image editor). In contrast, we build our taxonomy of actions intended by users who post requests on the PSR channel and therefore our taxonomy is based on Reddit requests instead of based on what functionalities are available in image editing software [31, 3]. Furthermore, existing taxonomies [31, 3] contain ambiguous, overlapping labels such as (zoom vs. crop), which we resolve in our taxonomy. Our taxonomy also contains 5 labels that reflect more up-to-date operations: super-resolution, relight, specialized, re-color, and merge, which do not exist in prior taxonomies [31, 3]. Editing datasets Synthetic datasets, e.g., InstructPix2Pix [4], UltraEdit [58], and Emu Edit [42], enable large-scale training-set of AI-generated images but fail to capture the real distribution of both requests and edits made by humans. Hybrid datasets like HIVE [56] (with human feedback) and Seed-Edit-Data [13] (synthetic and humanannotated) balance scale and authenticity. Meanwhile, real user-request datasets, such as IER [48] and GIER [43], offer greater authenticity by collecting genuine editing needs from online communities, but remain constrained in scale and diversity (6387% of requests are low -creativity Tab. 1). Unlike the above works, our PSR contains real image-editing requests by Reddit users and also human-edited images by PSR wizards. The most similar dataset to PSR is RealEdit [47], which is concurrent work that scrapes 57k requests also from Reddit. Our dataset is 3 larger than RealEdit in the number of (request, source image, edited image) tuples (Tab. 1). Unlike RealEdit and all previous works, our work is the first to address all four questions Q1Q4, i.e., we (a) analyze everyday image-editing needs submitted by real users; (b) test SOTA image editors on these requests; and (c) compare how SOTA VLMs judge image edits differently than human judges. We label each request with subject labels that correspond to WordNet categories (while RealEdit uses 14 manually-defined subjects) and three creativity levels (which do not exist in RealEdit). PSR features 15 detailed editing actions compared to Table 1: PSR is the largest-scale dataset of real-world requests and PSR-wizard edits. Dataset IER [48] GIER [43] MA5k-Req [44] InstructPix2Pix [4] MagicBrush [54] HIVE [56] EmuEdit [42] AURORA [25] UltraEdit [58] RealEdit [47] PSR (ours) PSR-328 (ours) 2019 2020 2021 2023 2023 2024 2024 2025 2025 2025 2025 2025 No. of No. of Source image edits reddit 4k reddit 6k real 24k requests 4k 6k 24k 454k 10k 454k SD [16] 10k MS COCO DALL-E 2 10M Emu [10] 280k mixed 1.1M 1.1M SD [16] 10M 280k 4M 57k 83k 328 4M MS COCO SD 94k 305k 3.9k reddit reddit reddit Edit generator human human Ps [5] SD [16] Request writer human human Amazon MT GPT-3 Amazon MT BLIP Llama 2 GPT-4o GPT-4o, human human human human human human, AIs human SD [16] Emu [10] mixed Requests based on reddit reddit image pairs image pairs source image image pairs source image & task image pairs source image reddit reddit reddit Creativity 100% Low Med 78% 15% 63% 34% 0% 12% 44% 62% 25% 48% 30% 54% 20% 4% 96% 43% 21% 58% 36% 56% 28% 33% 33% High 7% 3% 0% 42% 12% 22% 26% 0% 36% 6% 16% 33% labels of RealEdit [47]. Automated evaluation in image editing adopts both automated metrics and human evaluation. Automated metrics provide an objective and scalable way to assess edits by measuring fidelity [24, 51], realism [39], alignment with given instructions [55, 29, 23], or multiple facets of an edit [30]. Similarly, we use the LAION Aesthetics score to measure the fidelity of images. Furthermore, to the best of our knowledge, our work is the first work to test SOTA VLMs in rating edited images given (text request, source image) pair and compare their results with human ratings. Human ratings While automatic metrics are convenient, human evaluation remains the most reliable method in capturing subjective qualities such as realism, coherence, and user satisfaction. Public benchmarks facilitate structured human assessments, often using rating scales [22, 1, 50] or pairwise comparisons [20] to evaluate editing performance. Similar to GenAI Arena [20], we also present Win, Lose, and Tie options to human raters. GenAI Arena compares models based on their images generated from scratch while our work tests image editing. Prior human studies used smaller set of imagesEditVal [1], TedBench [22], and EditBench [50] contain 92, 100, and 240 source images. In contrast, PSR-328 test set features 328 real-world requests with controlled creativity levels, uniform subject labels and editing actions."
        },
        {
            "title": "3 PSR Dataset Construction",
            "content": "We source our data from /r/PhotoshopRequest (20132025) via two approaches: (1) PushShift [2] provide historical data (2013 up to Nov 2022) (2) custom crawler written for Reddit to download recent data (Oct 2024Feb 2025)."
        },
        {
            "title": "3.1 Taxonomy",
            "content": "We present taxonomy of requests where we label each request with the following labels: subject, editing action verb, and creativity level. The subject identifies the element being modified, the action verb specifies the edit, and the creativity level distinguishes routine tasks from open-ended ones. This taxonomy enables precise analysis of automated image editing and shows that even routine edits, like object removal, vary significantly by subjectperson, animal, or object. While low-creativity tasks often suit standard automation tools, high-creativity tasks  (Fig. 2)  demand models with greater flexibility. Subject The subject of an image editing request is the specific element being modifiede.g., in request to remove person, the subject is the person. Subjects may include objects, persons, or the entire image. To systematically classify subjects from user instructions, we leverage WordNets taxonomy [12]. We first extract subjects from raw instructions, then match each to the nearest synset (semantic category) in WordNets structured lexical database, providing standardized classification and reduced ambiguity. Editing Action Users may describe their requests vaguely, (e.g., Make this look better) instead of the more technically precise phrasing such as Improve the lighting to make the subject stand out. Because users may not know what editing actions are needed to achieve the result or that they want an surprise, out-of-the-box result. To properly categorize user intent, we develop diverse list of 15 action verbs that cover various editing actions (Tab. 2). Prior editing action taxonomies [31, 48] are tied to low-level tools in popular image editing software, failing to capture high-level user intent and 3 Figure 2: Example cases from PSR dataset where PSR wizard edits were preferred by human raters over the AI edits (a-c) and samples where AI edits was preferred (d-f). (a): The human edit completes the request, but the AI edit removes the people, which was not requested. (b): The human edit completes the request, but the AI edit generates similar image with people resembling those in the source image, although with different identities. (c): The human edit completes the request, but the AI edit does not because the finger does not go through the nose and out the eye. (d): Both edits make the requested removals, but the AI edit makes the image sharper and adds house in the background (which was not requested). (e): The human edit reduces the color cast but leaves behind bluish tint and muted tones, while the AI edit successfully restores realistic colors and contrast. (f): The human edit removes the background and applies soft photo filter that lacks stylization, while the AI edit transforms the dogs into bold, clean cartoons. More results are available at: https://huggingface.co/spaces/PSRDataset/PSR-Battle-Results omitting actions like super-resolution. This limitation motivated us to develop new taxonomy with comprehensive coverage of modern editing techniques. To develop our taxonomy, we feed random and subset of 5,000 edit requests into GPT-4o-mini prompt it to summarize common editing actions. Additionally, we consult image editing experts in the field to refine our list of actions to accurately reflect image-synthesis tasks in computer vision (e.g., super-resolution). Tab. 2 presents the final list of action verbs. Creativity levels We categorize requests into three creativity levels: 1. Low -creativity requests such as remove person or erase an object expect predictable outcome (Fig. 2d). requests 2. Medium -creativity as change the background or adjust lighting to look cinematic allow some room for interpretation and variability in results (Fig. 2e). such 3. High -creativity requests, e.g., make this image look magical or transform this into fantasy scene, require creative ideas and can yield widely different outcomes depending on the editor imagination (Fig. 2f). Our creativity classification differentiates between requests for precise technical edits vs. imaginative, open-ended transformations and enables breakdown analysis across requests, actions, and creativity. For example, the labels in PSR show 4 Table 2: List of image editing action verbs from our taxonomy with sample user requests. See appendix for the VLM prompts that are used to generate these action labels. Editing action Description and Sample Request Editing action Description and Sample Request add apply (Fig. 2f) crop replace move super-resolution relight specialized Insert new elements, objects, text, or effects. e.g. Add copyright watermark to the bottom right. Add filters, styles, or effects. e.g. Apply vintage film effect. Trim edges for smaller image. e.g. Crop to square format for social media. Substitute objects or text. e.g. Please change the pamphlet into dictionary. Reposition elements while keeping the rest unchanged. e.g. Shift the logo 20 pixels up. Increase resolution for clearer details. e.g. Can someone upscale this image to 4K resolution? Adjust lighting conditions. e.g. Can someone make lighting better / remove shadows? Advanced or composite editing tasks. e.g. Can someone vectorize this logo without background? adjust (Fig. 2b) clone delete (Fig. 2a) transform merge re-color zoom (Fig. 2b) Modify properties like tones, contrast, and saturation. e.g. Increase saturation bit on the elephants. Duplicate elements inside the image. e.g. Use cloning tool to blend grass over dirt patches. Remove elements, objects, or imperfections. e.g. Remove the jacket hanging from the girls side. Flip, scale, rotate, or skew elements. e.g. Fix the perspective of the building. Combine elements or effects. panorama from these shots. Change the color of an element, object, or text. e.g. Can someone change the dogs fur to black? Change scale to focus or zoom out. e.g. Zoom in on the man. e.g. Create that delete is indeed the most common action in low -creativity requests (Fig. 3c). That is, users often specify an specific element in the image to be deleted. In contrast, adding new elements into the image has much larger space for creativity: what object, how, and where to add? As the result, we find add to be the most common action in high -creativity requests (Fig. 3c)."
        },
        {
            "title": "3.2 Dataset Annotation Process",
            "content": "We use GPT-4o-mini [34] and InternVL-2.5-38B [7] to annotate our dataset. GPT-4o-mini generates taxonomy labels, while InternVL-2.5 handles image captioning and keyword extraction. We prompt InternVL-2.5 to summarize each image into 36 JSON keys (see Fig. A5), capturing attributes such as image type (e.g., photo or digital art), location, weather, presence of people, and object lists. To filter out posts unrelated to image editing (e.g., requests for image authentication), we prompt GPT4o-mini to output binary flag, image editing relevance, indicating whether the post involves image editing. Prompting details, including model version and temperature, are in Appendix B. Extracting Subject and Action Verbs We use zero-shot prompting (see Fig. A7) to extract actions from the request (Tab. 2). GPT-4o-mini is provided with list of valid actions, their descriptions, the input image, and the user-provided request. We ask the model to first examine the image and then rewrite the instruction in clear and simplified language to eliminate ambiguity, and finally identify the subjects of the edits along with the corresponding editing actions. Mapping Subjects to WordNet We map the extracted subjects from the previous stage to WordNets synsets. Once the subjects are identified, we provide GPT-4o-mini with the image, instruction, and subject, instructing it to select the closest WordNet synset based on the given context. Since the generated synset may not always be valid, we perform search within the WordNet lexical database using NLTK to find the closest matching synset and assign final synset to the subject. We use o1-Pro [37] to summarize WordNet subjects into higher-level semantic categories and organize them into 5 main categories and 12 subcategories (Tab. A2). Since the extracted WordNet synsets from the previous step vary in granularity, reasoning model with long-context capabilities, such as o1-Pro, effectively groups these synsets into structured categories. This approach results in more coherent and meaningful subject categories. Assigning Creativity Levels We use few-shot prompting (Fig. A10) to assign creativity levels. GPT-4o-mini receives the original image and request, along with examples annotated by creativity level, to classify the input accordingly."
        },
        {
            "title": "3.3 PSR dataset statistics",
            "content": "PSR consists of 82,976 requests and 305,806 edited images, categorized by creativity levels as 55.5% low, 28.2% medium, and 16.3% high (Fig. 3d). It includes 49,134 unique subjects, with 53.5% of subjects are under the People and Related category (Fig. 3b). The primary action requested by users is delete (32.9%), typically involving removing individuals or visual clutter to enhance aesthetics or professionalism. And these action distribution remain similar over the years (Fig. 3a). Detailed 5 (a) Editing action popularity per year (b) Most common actions & subjects (c) Action distribution per creativity level (d) Creativity level per year Figure 3: Over 12 years of Reddit data, delete, adjust, and add are the top-3 most wanted actions (a). Specifically, humans, body parts, text, and pets are the most frequent WordNet subjects for such common actions (b). While delete and adjust are the top-2 most common actions in the low - and medium -creativity requests, add takes up the largest share (34.9%) in high -creativity, e.g., inserting some interesting background or objects into the scene (c). Most requests (55.5%) require straightforward edits with low creativity (d). dataset statistics are provided in Appendix A."
        },
        {
            "title": "4 Experiment Setup",
            "content": "We compare AI edits and human edits to understand the preference of human raters and assess how VLMs can match human raters. AI editors We process each request using three generalist SOTA image editing tools: (Image GenerSeedEdit [35].2 For ation Experimental) [21], and GPT-4o each request, we generate two images: one using the original instruction (OI) and one using simplified instruction (SI) generated by GPT-4o-mini. [45], Gemini-2.0-Flash 2Due to access constraints and model safeguards, some edits from , , and are not available. As user-written instructions often include unnecessary details (e.g., Thank you in advance!! in Fig. 1), GPT-4o-mini refines them to focus on the core editing task. Additionally, for each request, we generate three AI-based image edits using 46 off-the-shelf image editing models hosted on HuggingFace [18] (see Tab. A5 for list of models). Spaces PSR-328 dataset Via stratified sampling, we select random PSR subset that has almost the same amount of images in three levels of creativity groups (114 low, 101 medium, and 113 high). On average, each request is edited by an average of five Reddit human editors, resulting in total of 1,644 human edits. We also generate 7 different 6 AI edits per request. In total, our human study has 10,405 unique 4-tuples (source image, request, AI edit, human edit) for evaluationsubstantially larger than prior studies [1, 50, 20], while keeping the human annotation effort manageable. Human study We conduct comparative study to assess whether AI-generated or Reddit user edits better satisfy original requests. To ensure unbiased evaluation, randomly paired AI and human edits are shown to human raters, who vote (Win, Lose or Tie) on which best fulfills the request (see Appendix for the user interface). Automated metrics and VLM judges We use two automated evaluation methods to complement our human study: LAION Aesthetic Score [15, 49, 52] and VLM-as-a-Judge [28, 8, 53, 6]. The Aesthetic Score quantifies visual appeal using classifier trained on large-scale human preference data while VLM-as-a-Judge uses VLMs to evaluate images by articulating visual qualities and explaining judgments. Both metrics act as proxies for human judgment, enabling scalable evaluation of AIand human-edited images. We use LAIONs Aesthetic Score Predictor [41] for calculating aes- [32], thetic metrics, and three VLMs (GPT-4o [14]) as [33], and Gemini-2.0-Flash-Thinking o1 as judges for the image edit rating task."
        },
        {
            "title": "5 Experimental Results",
            "content": "vs. 21.1% for low creativity, (67.6% vs. 25.3%) for medium creativity, and (60.9% vs. 30.7%) for high creativity. Performance by editing actions On average over all 49 models (Tab. A8), AIs win the most on the following editing requests: merge (30.9%), apply (30.6%), and add (30.6%) and the least on zoom (10.7%), crop (15.5%), and move (20.2%). For detailed breakdown, see Appendix E.2. Qualitative analysis We also conduct qualitative analysis (Appendix E.5) to identify patterns in cases where AI edits succeed or fail. From analyzing 206 cases where AI edits win votes, we find that 72% of the time, human edits poorly follow the instructions and AIs more closely follow the requests. In contrast, after analyzing 400 cases where AI loses, we find that 43% of the time, AIs misinterpret the request. In the remaining loss cases, AIs introduce unintended changes, artifacts or facial distortions. key issue is their failure to preserve identity (Fig. 6ab). To quantify the issue of failing to preserve identity, we perform controlled experiment where we repeatedly ask AI editors to only change the shirt color of person of varying gender and ages. However, both and models tend to change the facial identity and even body shape over sequence of multi-turn requests (Fig. 4 and Appendix G)."
        },
        {
            "title": "5.1 Human edits are strongly preferred over",
            "content": "proxy for human raters"
        },
        {
            "title": "AI edits by human raters",
            "content": "We collect 4,359 votes from 122 users and Tab. 3 shows the results. On average, human edits are more preferred (66.0% of the time), while 25.8% of the time AI edits win, and they tie in the remaining 8.2% (Tab. 3; 25.8% ). This result is important given the global interest in the image generation capabilities of Gemini-2.0-Flash and GPT-4o [21]. AI-win-rates by models Human edits are more preferred compared to every single AI editor (Tab. 3; Human win rates 53% ). has the highest overall win rate (37.8%) over human editors, followed by (20.6%). perform the worst (Tab. 3; HuggingFace models 17.8%). AI-win rates by creativity levels Human edits consistently are more preferred across all three levels. However, there is clear trend that AI edits are more preferred as the requests require higher creativity. That is, the gap human-vs-AI becomes smaller as the tasks are more open-ended: 70.1% (32.8%), and then Evaluating edited images is naturally multimodal task that is challenging because it requires understanding the (text request, source image) pair and analyzing how the changes in the edited image satisfies the request. Assessing how SOTA VLMs perform at edit rating is important for (1) benchmarking and advancing future VLMs; and (2) automating the rating efforts currently performed by humans. Experiment We collect over 10k ratings from each of three separate VLMs that serve as judges between human edits and AI edits. We replicate the same setup as in Sec. 5.1. Specifically, given textual request, source image, and two edited images, we ask each VLM to judge which edit better satisfies the request. In this setup, the images are labeled as Source, Edit A, and Edit B. Human-generated and AI-generated edits are randomly assigned to labels and B. The VLM then evaluates the edits and delivers verdict of either Edit is better, Edit is better, or Tie, indicating 7 Table 3: Human and VLM evaluation results for image editing preferences across 114 low -, 101 medium -, and 113 high -creativity requests. Human raters often prefer Human edits over all AI edits, while VLMs show strong preference for edits made by SeedEdit . # denotes the number of votes (i.e., pair-wise and GPT-4o comparisons). Bold numbers are the highest % within each VLM-judge group. AI: the win rate of AI-generated edits (%). H: the win rate of human, PSR-wizard edits (%). a. Human raters b. GPT-4o judge c. o1 judge d. Gemini-2.0-Flash-Thinking Group All data SeedEdit GPT-4o Gemini HF Low Medium High SeedEdit GPT-4o Gemini HF SeedEdit GPT-4o Gemini HF SeedEdit GPT-4o Gemini HF # Human 4,359 886 1,014 681 1,778 1,485 1,282 1,587 335 352 213 585 159 359 211 392 303 256 636 66.0 53.6 61.5 70.0 73.2 Creativity 70.1 67.6 60.9 63.3 71.6 70.9 72. 54.1 61.8 73.9 72.7 45.2 49.5 66.0 73.9 AI 25.8 37.8 32.8 20.6 17.8 21.1 25.3 30. 29.3 22.2 19.7 16.2 39.0 34.5 17.5 18.3 44.6 43.2 23.8 18.9 Tie 8.2 8.6 5.6 9.4 9. 8.8 7.2 8.4 7.5 6.2 9.4 10.9 6.9 3.6 8.5 9.0 10.2 7.3 10.2 7.2 # 10, 1,735 2,609 2,518 3,451 3,468 3,207 3,632 660 869 783 1,156 349 938 869 1,051 724 802 865 1,241 42.1 31.9 11.0 51.5 63.9 Low 45.2 39.6 41.3 40.5 13.7 55.8 64.5 25.2 9.5 51.7 61. 27.5 9.9 47.4 65.4 AI 52.4 60.6 85.3 42.6 30.5 48.2 56.6 52.6 51.4 81.8 36.7 29. 70.5 88.0 44.3 34.3 64.1 85.9 46.2 28.8 Tie 5.5 7.5 3.7 5.9 5.5 6.5 3.7 6. 8.2 4.5 7.5 6.4 4.3 2.6 4.0 4.4 8.4 4.2 6.4 5.7 # 10,352 1,744 2,618 2,524 3, 3,474 3,222 3,650 665 870 779 1,160 350 942 876 1,054 727 806 868 1,249 51. 42.7 15.9 63.9 74.0 AI 47.5 56.6 83.9 35.1 24.6 Medium 56.9 50.9 47. 53.5 21.4 73.3 74.6 46.3 15.2 60.3 76.7 31.1 10.7 59.0 71.3 41.9 48.7 52.0 45.4 78.6 25.0 23.6 53.4 84.5 39.4 22. 68.2 88.8 39.9 27.1 Tie 0.9 0.7 0.3 1.0 1.4 1.2 0.4 1.1 1.1 0.0 1.7 1. 0.3 0.3 0.3 0.6 0.7 0.5 1.2 1.6 # 10,354 1,744 2,623 2,524 3,463 41.9 36.1 17.3 46.8 60.1 AI 52.5 55.0 81.2 46.9 33.6 High 3,486 3,218 3,644 666 874 782 1,164 350 942 875 1,051 726 807 866 1,245 42.4 39.3 43.8 39.9 24.5 46.2 54. 31.1 13.2 47.5 58.6 34.8 14.4 46.4 66.3 44.6 57.9 55.5 40.7 71.7 37.9 30.9 62.9 86.4 49.3 37.8 64.5 85.5 52.8 32. Tie 5.5 8.9 1.4 6.3 6.3 13.1 2.8 0.7 19.4 3.8 16.0 14.4 6.0 0.4 3.2 3. 0.7 0.1 0.8 1.0 that both edits equally satisfy the request. The system message and prompting method are detailed in Fig. A11 and Fig. A12. Result While humans strongly prefer humancreated edits (Sec. 5.1), the trend is mixed for all VLMs. On average, over all human & AI edits, the three VLM judges (GPT-4o, o1, and Gemini-2.0Flash) choose AI edits at near-random chance, i.e., 52.4%, 51.6% and 52.5% of the time (Tab. 3). Interestingly, all three VLM judges prefer edits by GPT-4o at an extremely high rate ( 81%). Additionally, Cohens κ scores between humans and every AI editor (Tab. A18) further confirm weak agreement between human and VLM ratings. Among the three VLMs, o1 is the most consistent with humans but still scores low κ = 0.22. When examining VLM ratings by individual model groups , we observe clear preference among VLMs for edits produced by SeedEdit and GPT-4o. Notably, o1 selects GPT-4o edits 83.9% of the time. In contrast, when comparing human 8 edits vs. those by Gemini-2.0-Flash or Hugging Face models, VLMs generally favor human edits. Still, agreement with human judgments is low, with Cohens κ ranging from 0.14 to 0.25nearing random in some cases (Tab. A19). Qualitative analysis Analyzing the textual chainof-thought responses of VLM judges, we find that they are often blind to critical image details [40] and miss differences between image pairs (Appendix E.4.2). VLMs also overlook key aspects, such as changes in characters identities, or hallucinate nonexistent elements (Appendix E.4.3). These issues underscore the ongoing challenges of using VLMs for judging image edits."
        },
        {
            "title": "5.3 AI editors often improve aesthetics even",
            "content": "when not requested We find strong pattern among all AI editors is their tendency to enhance image aesthetics, even without explicit instruction. Qualitative analysis For example, they ofFigure 4: AIs can significantly alter both persons identity and the overall image quality through iterative imageediting requests. GPT-4o was repeatedly instructed to modify the shirt color, with each steps output serving as the input for the next iteration. Over iterations, facial identity, body shape, and the background shift away from the original person (more details in Appendix G). edits tend to win more (Fig. 7b; 18.7% 30.3%)."
        },
        {
            "title": "5.4 AI editors can satisfactorily handle 1",
            "content": "3 of all PSR requests Given the increase in popularity of SOTA AI editors [32, 21], it is important to estimate how much of the real-world requests (here, in PSR) can be satisfactorily handled by existing AI editors. The answer might inform the area for improvement for future AIs. Experiment Given the requests, AI edits and their ratings provided by humans, we first compute the win rates for humans and AI editors specifically for each edition action. For each request, if it is rated Tie or AI wins by human raters, we consider it to be satisfactorily handled by AI editors (Tab. A8; AI win+Tie). Results by editing actions Given this definition, the top-5 editing actions (see Tab. A8) that AI editors handle the most satisfactorily are clone (48.0%), merge (39.3%), apply (39.3), add (38.0%), and delete (34.9%). This is consistent with the fact that apply, add, and delete are the most common editing actions in large-scale training sets in the literature [10, 4] since the training-set examples can be synthesized using image inpainters or style transfer models. Coincidentally, add and delete are among the top-5 most popular requests in PSR  (Fig. 3)  . To estimate the % of requests that can be handled satisfactorily by AIs over all editing actions, we multiply the combined AI win+Tie rates by the proportion that each action group contributes to the overall dataset  (Fig. 3)  : (cid:80)v v=1 Dv AIv = 33.35% where Dv represents the proportion of dataset requests associated with action verb v, and AIv denotes the combined percentage of AI wins and ties for edits with verb v. That is, 33.35% of all image-editing requests can be handled by existing AI editors. Figure 5: o1 judge occasionally fails to notice details in edited images, here, overlooking the position of the hand and the configuration of the fingers. ten touch up human faces, making skin appear smoother and more polished (data not shown due to concerns of revealing identity of real people). Similarly, AI models enhance pets facial features. For example, when instructed simply to remove the text and foot of dog (Fig. 6a) or image background (Fig. 6b), SeedEdit and GPT-4o perform the requested change and also improve the dogs overall facial aesthetics (Fig. 6b) and even restore damaged eye (Fig. 6a), which is not requested. Quantitative analysis We compute LAION aesthetic scores for all human and AI edits in PSR328. On average, AI-edited images have higher aesthetics scores (µ = 5.56) compared to humanedited images (µ = 5.18) and even source images (µ = 5.32; Fig. 7a). The scores confirm that AI editors tend to increase image aesthetics. AI-generated edits typically have higher aesthetic scores than human edits, regardless of the rating outcome (AI wins, Human wins or Tie; Fig. A17). Categorizing the AI win rates by the increase () in aesthetics scores between the source and edit images, we find strong correlation between the aesthetics score gain and the increase in AI win ratesas the in aesthetics of AI edits increase and surpass that of humans (Human), AI 9 (a) Request: Could someone please remove the text and the foot in the background? (b) Request: Could anybody make the background simple and neutral? Figure 6: AI models tend to increase overall image aesthetics. While both requests (a) and (b) ask for changes in the background, AI editors tend to enhance the subjects facial features. SeedEdit modifies the dogs eyedespite it not being requestedwhereas the human edit keeps the eye intact (a). GPT-4o noticeably changes the dogs ears and the fur (b). (a) Aesthetic scores of source, AI edits, and human edits. (b) Win rate (%) categorized by increase () in aesthetics scores between the (human edit, source image) and (AI edit, source image). Figure 7: AI edits have higher LAION aesthetics scores than the source image and human edits (a). AI edits are more likely to win when they contain greater increase () in aesthetics score (b). modifications outside the target region and inadvertent changes to essential features, such as the subjects identity."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank Peng Wang from the SeedEdit team that has kindly run their models on our PSR-328 to enable this study. We thank Hung H. Nguyen, Pooyan Rahmanzadehgervi, Tin Nguyen, and Giang Nguyen at Auburn University for feedback and discussions of the earlier results. AN was supported by the NSF Grant No. 2145767, and donations from NaphCare Foundation & Adobe Research."
        },
        {
            "title": "Conclusion",
            "content": "Limitations Our dataset annotation and taxonomy rely on multiple large language models to perform labeling, which may introduce biases or inaccuracies. Additionally, some AI image editors were unavailable publicly (e.g., Emu-Edit [42], OmniEdit [57], etc.) during evaluation and are thus excluded from our human study. In this study, we compare generative AI and human edits to understand the gap between current AI capabilities and user needs. AI tools excel at tasks like object removal and outpainting, effectively extending images and filling in missing details. However, in real-world use, current models can adequately handle only about one-third of user requests. Their main limitations are unintended"
        },
        {
            "title": "References",
            "content": "[1] Samyadeep Basu, Mehrdad Saberi, Shweta Bhardwaj, Atoosa Malemir Chegini, Daniela Massiceti, Maziar Sanjabi, Shell Xu Hu, and Soheil Feizi. 2023. Editval: Benchmarking diffusion based textarXiv preprint guided image editing methods. arXiv:2310.02426. 3, 7 [2] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. In Proceedings of the international AAAI conference on web and social media, volume 14, pages 830839. 3 [3] Jacqueline Brixey, Ramesh Manuvinakurike, Nham Le, Tuan Lai, Walter Chang, and Trung Bui. 2018. system for automated image editing from natural language commands. arXiv preprint arXiv:1812.01083. 2 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402. 1, 2, 3, 9 [5] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Frédo Durand. 2011. Learning photographic global tonal adjustment with database of input / output image pairs. In The Twenty-Fourth IEEE Conference on Computer Vision and Pattern Recognition. 3 [6] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024. Mllm-asa-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning. [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, and 1 others. 2024. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. 5 [8] Zhihao Chen, Bin Hu, Chuang Niu, Tao Chen, Yuxin Li, Hongming Shan, and Ge Wang. 2024. Iqagpt: computed tomography image quality assessment with vision-language and chatgpt models. Frontiers in Radiology, 4:131. 2, 7 [9] Marx D. 2025. If was good at photoshop or graphic design, id do this side hustle. Reports r/PhotoshopRequest receives an average of 226 posts per day, based on Subredditstats and Social Rise analytics as of April 2025. 2 [11] Feedspot. 2025. Top 15 Photoshop Forums in 2025 https://forums. [Acforums.feedspot.com. feedspot.com/photoshop_forums/. cessed 14-05-2025]. 2 [12] Christiane Fellbaum. 2010. Wordnet. In Theory and applications of ontology: computer applications, pages 231243. Springer. [13] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. 2024. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007. 2 [14] Google DeepMind. 2024. Gemini 2.0 flash https://deepmind.google/ Experimental AI thinking. technologies/gemini/. model. 2, 7 [15] Simon Hentschel, Konstantin Kobs, and Andreas Hotho. 2022. Clip knows image aesthetics. Frontiers in Artificial Intelligence, 5:976235. 7 [16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. 2023. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations. [17] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, and Liangliang Cao. 2024. Diffusion model-based image editing: survey. arXiv preprint arXiv:2402.17525. 1 [18] Hugging Face. 2023. Hugging Face Spaces. Achttps://huggingface.co/spaces. cessed on March 02, 2025. 6 [19] Fortune Business Insights. 2025. Ai image generator market size, share & industry growth 2030. [Online; accessed 2025-03-06]. 1 [20] Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. 2025. Genai arena: An open evaluation platform for generative models. Advances in Neural Information Processing Systems, 37:7988979908. 3, 7 and"
        },
        {
            "title": "Kampf",
            "content": "[21] Kat 2025. flash //developers.googleblog.com/en/ experiment-with-gemini-20-flash-native-image-generation/. Accessed: 2025-03-21. 2, 6, 7,"
        },
        {
            "title": "Experiment with\nimage",
            "content": "Brichtova. 2.0 https: generation. gemini native [10] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, and 1 others. 2023. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807. 3, 9 [22] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-based real image In Proceedings of editing with diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017. 11 [23] Yoonjeon Kim, Soohyun Ryu, Yeonsung Jung, Hyunkoo Lee, Joowon Kim, June Yong Yang, Jaeryong Hwang, and Eunho Yang. 2024. Augmentationdriven metric for balancing preservation and modification in text-guided image editing. arXiv preprint arXiv:2410.11374. 3 [34] OpenAI. 2025. Gpt-4o mini: advancing costefficient intelligence. Accessed on February 13, 2025. 5 [35] OpenAI. 2025. Introducing 4o image generation. Accessed: 2025-05-10. 2, 6 [24] Jari Korhonen and Junyong You. 2012. Peak signalto-noise ratio revisited: Is simple beautiful? In 2012 Fourth international workshop on quality of multimedia experience, pages 3738. IEEE. [25] Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Chris Pal, and Siva Reddy. 2025. Learning action and reasoning-centric image editing from videos and simulation. Advances in Neural Information Processing Systems, 37:38035 38078. 3 [26] Black Forest Labs. 2024. Flux. https:// github.com/black-forest-labs/flux. 55 [27] Gierad Laput, Mira Dontcheva, Gregg Wilensky, Walter Chang, Aseem Agarwala, Jason Linder, and Eytan Adar. 2013. Pixeltone: multimodal interface for image editing. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 21852194. 2 [28] Seongyun Lee, Seungone Kim, Sue Park, Geewook Kim, and Minjoon Seo. 2024. Prometheusvision: Vision-language model as judge for finegrained evaluation. In Findings of the Association for Computational Linguistics ACL 2024, pages 11286 11315. [29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR. 3 [30] Yiwei Ma, Jiayi Ji, Ke Ye, Weihuang Lin, Zhibin Wang, Yonghan Zheng, Qiang Zhou, Xiaoshuai Sun, and Rongrong Ji. 2024. I2ebench: comprehensive benchmark for instruction-based image editing. In Advances in Neural Information Processing Systems (NeurIPS). 3 [31] Ramesh Manuvinakurike, Jacqueline Brixey, Trung Bui, Walter Chang, Doo Soon Kim, Ron Artstein, and Kallirroi Georgila. 2018. Edit me: corpus and framework for understanding natural language image editing. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). 2, 3 [32] OpenAI. 2024. Hello GPT-4o. https:// openai.com/index/hello-gpt-4o/. Accessed: March 2, 2025. 7, 9 [33] OpenAI. 2024. Openai o1 model series. Accessed 2025-03-02. 2, 7 [36] OpenAI. 2025. Introducing 4o Image Generation openai.com. https://openai.com/index/ introducing-4o-image-generation/. [Accessed 14-05-2025]. 1 2025. [37] OpenAI. pro. introducing-chatgpt-pro/. February 28, 2025. 5 Introducing ChatGPT https://openai.com/index/ Accessed: [38] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, and 1 others. 2023. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193. 55 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR. 3 [40] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. 2024. Vision language models are blind. In Proceedings of the Asian Conference on Computer Vision (ACCV), pages 1834. 8 Schuhmann. [41] Christoph proved //github.com/christophschuhmann/ improved-aesthetic-predictor. predictor. aesthetic 2023. Imhttps: [42] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. 2024. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879. 1, 2, 3, 10 [43] Jing Shi, Ning Xu, Trung Bui, Franck Dernoncourt, Zheng Wen, and Chenliang Xu. 2020. benchmark and baseline for language-driven image editing. In Proceedings of the Asian Conference on Computer Vision. 2, [44] Jing Shi, Ning Xu, Yihang Xu, Trung Bui, Franck Dernoncourt, and Chenliang Xu. 2021. Learning by planning: Language-guided global image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1359013599. 1, 3 [45] Yichun Shi, Peng Wang, and Weilin Huang. 2024. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686. 1, 2, 6 12 metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586 595. 3 [56] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, and 1 others. 2024. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90269036. 2, 3 [57] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. 2024. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093. [58] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. 2025. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093. 1, 2, 3 [46] SubredditStats.com. 2025. r/PhotoshopRequest Subreddit Stats (Photoshop Request) subredditstats.com. https://subredditstats.com/ r/PhotoshopRequest. [Accessed 14-05-2025]. 2 [47] Peter Sushko, Ayana Bharadwaj, Zhi Yang Lim, Vasily Ilin, Ben Caffee, Dongping Chen, Mohammadreza Salehi, Cheng-Yu Hsieh, and Ranjay Krishna. 2025. Realedit: Reddit edits as large-scale empirical dataset for image transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). To appear. 2, 3 [48] Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, and Mohit Bansal. 2019. Expressing visual relationships via language. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 18731883, Florence, Italy. Association for Computational Linguistics. 2, 3 [49] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. 2023. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 25552563. [50] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David Fleet, Radu Soricut, and 1 others. 2023. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1835918369. 3, 7 [51] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612. 3 [52] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtao Zhai, and Weisi Lin. 2024. Q-align: teaching lmms for visual scoring via discrete text-defined levels. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. 7 [53] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024. Llava-critic: Learning arXiv preprint to evaluate multimodal models. arXiv:2410.02712. 7 [54] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. 2023. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449. 2, 3 [55] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual 13 Appendix for: Understanding Generative AI Capabilities in Everyday Image Editing Tasks"
        },
        {
            "title": "A Dataset Statistics",
            "content": "We collect 98,234 posts and 350,609 edited images from Reddit, with 58,624 posts sourced from PushShift between 2013 and 2022 and 39,610 gathered between October 2024 and early 2025. After processing, the final dataset includes 82,976 posts and 305,806 edited images, with 71,027 single-image and 11,949 multiple-image requests. Tab. A1 shows the breakdown of the dataset composition, and Fig. 3 illustrates key statistics and trends within the data. Our dataset contains diverse set of subjects, with 49,134 unique subjects across all requests. The People and Related category is the most common, accounting for 53.5% of the requests (Tab. A2). Subject trends exhibit seasonal variations (Fig. A4), with family-related requests peaking around the holiday season and New Year. Additionally, Fig. A2 illustrates the most common subjects. The most common action is delete, accounting for 32.9% of all edits (Fig. A3), trend that has remained dominant over the years (Fig. 3a). These involve removing people, such as photobombers, for aesthetic or personal reasons. Other deletion requests involve removing objects, like poles, bags, or signs, in order to reduce visual clutter as well as eliminating facial imperfections, such as acne or wrinkles for social media or professional use. The most frequent types of actions applied to subjects are illustrated in Fig. 3b (See Fig. A1 for more details). Fig. 3c shows that 55.5% of user requests fall into the low-creativity categoryindicating that most modifications allow little creative input. In this group, the delete action predominates, accounting for 51.2% of requests. Conversely, high-creativity requests, which involve more complex transformations, are mainly associated with the add action (35.9%). Meanwhile, the medium-creativity category displays more balanced distribution, with add and delete actions representing 25.8% and 20.7% of requests, respectively. This distribution suggests that users generally prioritize simple modifications over intricate creative edits. 14 Table A1: Distribution of user requests and edited images across different data sources in our PSR dataset. Table A2: Distribution of main and subcategories of subjects in image-editing requests."
        },
        {
            "title": "Request Statistics",
            "content": "Count Coverage (%)"
        },
        {
            "title": "Category",
            "content": "Total Requests Single-image Multiple-image Historical (Pushshift) Recent Data (20242025) 82,976 71,027 11,949 58,624 39,"
        },
        {
            "title": "Edit Statistics",
            "content": "Total Edits Historical (Pushshift) Recent Data (20242025) 305,806 90,466 215,340 100.0 84.4 15.6 70.6 29.4 100.0 29.6 70.4 People & Related Humans & Family Body Parts Clothes & Accessories Text Branding & Abstract Text & Logos Special & Misc Abstract & Aesthetic Inanimate Objects Tools & Misc Household & Furnishings Vehicles & Transportation"
        },
        {
            "title": "Animals",
            "content": "Pets & Animals Environment & Background Natural Environment Lighting & Atmosphere"
        },
        {
            "title": "Count",
            "content": "44,416 31,914 9,840 2,662 14,114 8,759 2,979 2,376 10,525 4,021 3,776 2,728 8,411 8,411 5,510 3,904 1,606 (%) 53.5 38.5 11.9 3.2 17.0 10.6 3.6 2.9 12.7 4.8 4.6 3.3 10.1 10.1 6.6 4.7 1."
        },
        {
            "title": "Total",
            "content": "82,976 100.0 15 Figure A1: Distribution of Actions Across Different Subject Subcategories 16 Figure A2: Word cloud visualization of image editing request subjects, mapped to WordNet synsets. Figure A3: Distribution of different action verbs in our dataset 17 Figure A4: Monthly distribution of subject categories in image editing requests, highlighting trends and variations over time"
        },
        {
            "title": "B Model Inference and Prompting Details",
            "content": "In this section, we provide details about the prompts used, the model versions, and the temperature. Table A3: Model Configuration Details"
        },
        {
            "title": "Details",
            "content": "InternVL-2.5-38B GPT-4o-mini o1 Version: OpenGVLab/InternVL2_5-38B-AWQ Temperature: 0.7 Hosted internally using lmdeploy Version: gpt-4o-mini-2024-07-18-global-batch Batch API Temperature: 0 (default) Using the API from Azure OpenAI Service Version: o1-2024-12-17 Temperature: N/A Reasoning Effort: High Using the API from Azure OpenAI Service Gemini-2.0-Flash-Thinking Version: gemini-2.0-flash-thinking-exp-01-21 Temperature: 0.7 (default) Using the API from Google AI Studio"
        },
        {
            "title": "C Prompting Details for Taxonomy Construction",
            "content": "JSON schema for summarizing an image to JSON (Image-to-JSON) { } \"description\": \"Brief description of the main content\", \"image_type\": \"photograph/digital-art/illustration/screenshot/meme\", \"setting\": \"indoor/outdoor/digital/mixed\", \"location\": \"beach/office/park/etc\", \"time_of_day\": \"day/night/unknown\", \"weather\": \"sunny/cloudy/rainy/not-applicable\", \"has_people\": false, \"people_count\": 0, \"has_adults\": false, \"has_children\": false, \"has_elderly\": false, \"has_groups\": false, \"has_animals\": false, \"has_dogs\": false, \"has_cats\": false, \"has_birds\": false, \"has_wildlife\": false, \"other_animals\": [], \"foreground_objects\": [], \"background_objects\": [], \"prominent_objects\": [], \"dominant_colors\": [], \"lighting\": \"bright/dim/dark/natural/artificial\", \"has_text\": false, \"text_content\": \"\", \"text_language\": \"\", \"mood\": [], \"atmosphere\": \"\", \"is_nsfw\": false, \"is_violent\": false, \"has_gore\": false, \"has_nudity\": false, \"is_sensitive\": false, \"image_quality\": \"high/medium/low\", \"orientation\": \"landscape/portrait/square\", \"tags\": [], \"ai_confidence\": \"high/medium/low\" Figure A5: JSON schema for image metadata classification used with InternVL-2.5-38B System message for extracting the requests metadata def create_system_message() -> str: return \"\"\"You are an AI system that analyzes image editing requests. Given textual instruction and an image, evaluate the clarity, complexity, and appropriateness of the editing request . Assess the instructions ambiguity (1-5 scale, where 1 is crystal clear and 5 is completely vague), complexity level (1-5 scale, where 1 is basic editing and 5 is expertlevel), and check for any inappropriate or NSFW content. Verify if the image is valid and usable, and determine if the request is actually related to image editing. Provide your analysis in the following JSON format, including specific reasoning for each field: { \"original_instruction\":\"Preserved exactly as given to maintain reference point - no modifications or interpretations\", \"rewritten_instruction\": { \"text\": \"Clear, structured version of the original instruction\", \"reasoning\": \"Clarified version that removes ambiguity, fills in implied steps, and provides specific direction. Should be actionable without additional context\" }, \"missing_details\": { \"items\": [\"List specific information that would be needed to complete the task but wasnt provided in the original instruction\"], \"reasoning\": \"Identifies gaps that would need to be filled to successfully complete the task\" }, \"external_references\": { \"value\": \"True: References external links or comments False: Self-contained instruction\", \"reasoning\": \"Identifies if critical information is located outside the main instruction\" }, \"nsfw_analysis\": { \"value\": \"True: Contains adult/mature themes False: Safe for general audience\", \"reasoning\": \"Evaluates if content contains mature themes, nudity, or adult subject matter\" }, \"inappropriate_content\": { \"value\": \"True: Contains harmful/offensive/inappropriate content False: Appropriate content\", \"reasoning\": \"Identifies presence of: 1) Harmful content (violence, hate speech, extreme gore) 2) Offensive content (discriminatory themes, extreme political content, severe profanity) 3) Inappropriate but non-harmful content (crude humor, mild toilet humor, silly/whimsical inappropriate gestures, playful trolling). Note: Mild humorous or whimsical content that might be considered silly inappropriate (like tongue-in-cheek jokes, mild pranks, or playful memes) should be marked False unless they cross into actually offensive territory. Consider context and intent - distinguish between harmful inappropriate vs harmless fun\" }, \"image_editing_relevance\": { \"value\": \"True: Related to image manipulation False: Unrelated to image editing\", \"reasoning\": \"Confirms if the instruction pertains to image editing rather than other topics\" }, \"image_validity\": { \"value\": \"True: Image is usable False: Image is blank/corrupted/missing\", \"reasoning\": \"Verifies if provided image is suitable for editing\" }, } \"\"\" Figure A6: Prompt for extracting basic information from the request"
        },
        {
            "title": "System message for extracting action verbs from the request",
            "content": "def create_system_message(categories_desc) -> str: return f\"\"\"Analyze the following image editing instruction and identify which of these specific actions it contains. Available categories with examples: {categories_desc} ANALYSIS GUIDELINES: 1. Evaluate both the original and clarified requests 2. Only include actions that are: - Explicitly stated OR - Logically necessary to achieve the described result 3. Consider the final images appearance to identify implicit actions 4. Exclude actions that are: - Only potentially useful but not required - Vaguely related but not essential - Could be used as alternatives Focus only on actual image manipulation actions that match our predefined categories. Return your response as JSON object with an actions array containing only valid categories from the list provided. \"\"\" Figure A7: Prompt for extracting action verbs from an editing request"
        },
        {
            "title": "System message for assigning a WordNet synset to the subject of an image editing request",
            "content": "def create_system_message() -> str: return \"\"\"You are an AI system designed to analyze image editing requests and map the **subject** of the edit to its corresponding WordNet synset. The **action verb is not mapped**-only the subject needs to be processed. ### **Processing Pipeline:** 1. **Candidate Keyword Selection** - Extract the most relevant subject from the instruction and image. Generate list of candidate keywords that are **highly likely to already exist as WordNet synset**. Prioritize concrete nouns and commonly recognized entities. 2. **WordNet Synset Matching** - Use the refined candidate keywords to query WordNet, -confirming and selecting the best-fitting synset(s) for the subject. ### **Inputs Provided:** - textual instruction describing the image edit. - An image associated with the request. - The previously extracted **subject** (not the action verb). ### **Expected Output:** Generate structured list of **candidate search keywords** that: - Are **highly probable** to already exist as WordNet synset. - Accurately represent the subject in the context of the image editing request . - Can be directly used for WordNet lookup to retrieve the most relevant synset . ### **Important Constraints:** - **Do not process or map the action verb**-focus solely on the subject. - Ensure the **keywords are already strong candidates for WordNet synsets** before attempting lookup. - Prefer concrete, commonly used nouns that align well with WordNes structure . \"\"\" Figure A8: Prompt for assigning WordNet synsets to subjects of editing requests Full Details for Subcategories (Part 1) categories_desc = { \"categories\": [ { \"category\": \"Recoloring\", \"definition\": \"Change the color of an element, object, or text inside the image, but not the whole image\", \"samples\": [ \"Can anyone change the dogs fur to black?\", \"Could somebody change the turquoise on the vanity and mirror to white?\", \"Can someone show me how this truck looks in 3 different colors?\", \"Can someone colorize and touch up my grandma?\", \"Can someone please make this grayscale with only the house blue?\", ], }, { \"category\": \"Relighting\", \"definition\": \"Improve or change the lighting conditions of the scene such as the temperature, color, direction or position of the light source\", \"samples\": [ \"pls get rid of the green light or change it to another colour\", \"Can someone relight this photo, removing all harsh shadows\", \"Can someone make lighting better / remove shadows?\", ], }, { \"category\": \"Superresolution\", \"definition\": \"Modify image so that that the image has higher resolution and showing clearer, fine details\", \"samples\": [ \"How can increase the pixel count on this picture?\", \"Would kind soul be able to clean this up with higher resolution?\", \"Can someone upscale this image to 4K resolution?\", ], }, { \"category\": \"Adjust\", \"definition\": \"Enhance or correct an entire images overall apperance by modifying its common properties\", \"samples\": [ \"Increase saturation bit on the elephants\", \"Brighten the shadows by 40% in the portrait\", \"Can someone adjust the lighting/contrast on this?\", ], }, Figure A9: Full descriptions for each type of action verb in the data set (Part 1/3). 24 Full Details for Subcategories (Part 2) { \"category\": \"Delete\", \"definition\": \"Remove unwanted elements, text, objects, people, or imperfections from the image\", \"samples\": [ \"Remove the jacket hanging from the girls side\", \"Delete the distracting signpost in the background\", \"Please remove the 3rd girl from the left in light blue!\", ], }, { \"category\": \"Crop\", \"definition\": \"Trim the edges of an image to make smaller image to meet specific size requirements\", \"samples\": [ \"Crop the photo to eliminate the space to the left and right\", \"Crop to square format for social media\", ], }, { \"category\": \"Add\", \"definition\": \"Insert new elements, objects, text, or effects that werent in the original image\", \"samples\": [ \"Insert ball hitting the tennis racket\", \"Add copyright watermark to the bottom right\", \"Can someone put believable tattoo on my daughter?\", ], }, { \"category\": \"Replace\", \"definition\": \"Substitute objects or text in the image with something else while keeping the rest of the image unchanged\", \"samples\": [ \"Please change the pamphlet she is holding into dictionary\", \"I hate the backround. would like neat, white background\", \"Can someone replace the ball with planet?\", ], }, { \"category\": \"Apply\", \"definition\": \"Add filters, styles, or effects that modify the overall appearance of the image\", \"samples\": [ \"Add Gaussian blur to the background\", \"Apply vintage film effect\", \"Is someone able to help me turn this into cartoon\", ], }, Figure A9: Full descriptions for each type of action verb in the data set (Part 2/3). 25 Full Details for Subcategories (Part 3) { }, { \"category\": \"Zoom\", \"definition\": \"Adjust the image scale to zoom in on specific area or add new content to mimic zoom-out action\", \"samples\": [ \"Zoom in on the man\", \"Zoom out 50% to show more context\", \"Im happy to tip if someone is able to zoom this out\", ], \"category\": \"Transform\", \"definition\": \"Change the geometric properties (flip, scale, rotate, skew, perspective, distort, warp) of the image or objects\", \"samples\": [ \"Flip the photo horizontally\", \"Fix the perspective of the building\", \"Please rotate the box in my hand\", ], }, { \"category\": \"Move\", \"definition\": \"Change the position of existing elements within the image while keeping the rest of the image unchanged\", \"samples\": [ \"Move the white framed picture to the blue wall\", \"Shift the logo 20 pixels up\", \"Will someone please edit my friend closer to me\", ], }, { }, { \"category\": \"Clone\", \"definition\": \"Make more copies of some existing elements inside the image\", \"samples\": [ \"Can someone clone my cat\", \"Use cloning tool to blend grass to cover any patches of dirt\", \"Can someone multiply me and make it look like my arms are interlocked?\", ], \"category\": \"Merge\", \"definition\": \"Combine multiple elements or effects from multiple images into cohesive final image\", \"samples\": [ \"Can someone combine these 2 photos?\", \"Please combine so Im kissing this moose!\", \"Create panorama from these shots\", ], }, { \"category\": \"Specialized operation\", \"definition\": \"Specialized or composite editing operations that dont fit into standard categories\", \"samples\": [ \"Can someone vectorize this logo for me without background?\", \"Convert to JPEG format\", \"Can someone make collage of 12 photos\", ], }, ] } Figure A9: Full descriptions for each type of action verb in the data set (Part 3/3)."
        },
        {
            "title": "System message for creativity level assignment",
            "content": "def create_system_message() -> str: return \"\"\"As an AI assistant, your task is to assign creativity score to edited images based on the diversity of acceptable final versions for given request. The creativity is measured by considering how many different ways the image could be edited to fulfill the request. - **Low Creativity:** The request leads to similar edited images with limited variations. - **Medium Creativity:** The request allows for some variation but not an extensive range. - **High Creativity:** The request can be fulfilled in many different ways leading to very different images. When evaluating, think about the range of possible acceptable edited images for the request. **Examples:** 1. **Request:** \"Remove the red-eye effect from this photo.\" - The edits will be similar, focusing on correcting the eyes. - **Creativity Score:** Low. 2. **Request:** \"Transform this portrait into work of abstract art.\" - There are countless ways to interpret and edit the image. - **Creativity Score:** High. 3. **Request:** \"Adjust the brightness and contrast to enhance the image.\" - There are some variations in how this can be done. - **Creativity Score:** Medium. 4. **Request:** \"Crop the image to focus on the main subject.\" - Limited variations in the final image. - **Creativity Score:** Low. 5. **Request:** \"Add dramatic sky to this landscape photo.\" - Several ways to interpret dramatic sky. - **Creativity Score:** Medium. 6. **Request:** \"Reimagine this landscape in fantasy setting.\" - Numerous possibilities for how the image could be edited. - **Creativity Score:** High. Provide the creativity score (**Low**, **Medium**, or **High**) along with brief explanation for your assessment. \"\"\" Figure A10: Prompt for assigning creativity level to requests"
        },
        {
            "title": "System message for comparing different edited images using VLMs",
            "content": "def create_system_message() -> str: return \"\"\" You are an image editing evaluation assistant that helps users determine which edited version better fulfills their request. When presented with an original image, an editing request, and two edited versions (A and B), carefully analyze how each edit implements the requested changes. First, examine the specific editing request and how it relates to the original image. Then analyze each edited version, noting strengths, weaknesses, and how closely they match the users intent. Provide clear reasoning that considers technical quality, aesthetic appeal, and faithfulness to the request. Finally, deliver your verdict in one of three ways: \"Image is better\" if the first edit is superior, \"Image is better\" if the second edit is superior, or \"Tie, both edits are equally good\" if they are comparable in quality and adherence to the request. \"\"\" Figure A11: Prompt for judging different edited images using VLMs."
        },
        {
            "title": "Sample Python code for sending multiple images to be evaluated by a judge",
            "content": "response = client.chat.completions.create( model=\"MODEL_NAME\", messages=[ {\"role\": \"system\", \"content\": system_message}, { \"role\": \"user\", \"content\": [ { } \"type\": \"text\", \"text\": f\"Analyze the following image editing request and compare the edits:nnUser instruction: {original_instruction}\", ], }, { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Source Image\"}, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"{source_image_base64}\"}, }, { }, { }, ], \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Edit A\"}, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"{edit1_image_base64}\"}, }, ], \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Edit B\"}, { \"type\": \"image_url\", \"image_url\": {\"url\": f\"{edit2_image_base64}\"}, }, ], }, ], tools=[tool], tool_choice=\"auto\", max_completion_tokens=8192, ) Figure A12: Example code demonstrating how to submit multiple images in one message to vision-language model (VLM) for judgment in the \"VLMs as Judge experiment."
        },
        {
            "title": "D Image Generation Details",
            "content": "In this section, we provide details about how we collected the images for all the models. Table A4: Image Generation Details"
        },
        {
            "title": "Model",
            "content": "GPT-4o"
        },
        {
            "title": "Details",
            "content": "We used ChatGPTs web interface to generate images. Gemini-2.0-Flash We used the official API to generate images. SeedEdit We sent images and prompts to SeedEdits authors to have images generated locally. Hugging Face ( ) We accessed models through Hugging Face Spaces and inserted the image and prompt (if applicable). Masks and bounding boxes were added manually if needed. See Appendix E.1 for full list of the models."
        },
        {
            "title": "E Human Study",
            "content": "In this section, we provide details about the human study. total of 122 different people participated in our study, from North America, representing two universities and one institution. One-third of the participants are professional image editors and are familiar with image editing techniques. Figure A13: The introduction screen for the human study guides users on how to rate images based on the users request. Figure A14: The user interface for the human study displays the original image, the user-provided request, and two edits. Users must decide which edit best satisfies the users request. Before starting the survey, users are shown an example of how to judge quality through Fig. A13. Following the survey introduction, the user is shown the original image, the edit request, and the two edited images (Fig. A14). The user then chooses if image or image is better, or if they are tied in quality. 31 E.1 Details About AI-based Image Editing Tools Table A5: Number of edits for unique posts generated via different AI tools."
        },
        {
            "title": "Model",
            "content": "GPT-4o Gemini-2.0-Flash"
        },
        {
            "title": "CosXL",
            "content": "InstructPix2Pix"
        },
        {
            "title": "LEDITS",
            "content": "FLUX.1-dev-Inpainting-Model-Beta-GPU remove-photo-object BRIA-Eraser-API Finegrain-Object-Eraser"
        },
        {
            "title": "ReplaceAnything",
            "content": "stable-diffusion-xl-inpainting flux-IP-adapter"
        },
        {
            "title": "CodeFormer",
            "content": "old_photo_restoration flux-fill-outpaint leditsplusplus marcosv/InstructIR text-guided-image-colorization B2BMGMT_Sharpening BRIA-2.2-ControlNet-Recoloring foto_filter turbo_edit"
        },
        {
            "title": "Count",
            "content": "fffiloni/diffusers-image-outpaint ameerazam08/Diffusion-Eraser IC-Light textcutobject FLUX.1-Fill-dev diffusers-image-fill FLUX.1-inpaint-dev kornia-image-filtering Finegrain-Object-Cutter Image-to-Line-Drawings 328 327 274 162 153 119 81 52 48 Sketch-Gen 47 AnimeGANv2 45 36 29 NonLinear-Blurr-Image 27 openfree/ColorRevive 22 19 BRIA-Generative-Fill-API 16 image2coloringbook 15 10 remove-photo-object 10 BRIA-2.3-Inpainting 10 9 7 6 yizhangliu/ImgCleaner schirrmacher/ormbg fffiloni/InstantIR sketch2lineart not-lain/background-removal 5 4 3 3 3 3 3 3 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 Table A6: Model Performance Comparison (Win Rate as Judged by Humans %)"
        },
        {
            "title": "Model",
            "content": "remove-photo-object schirrmacher/ormbg Finegrain-Object-Cutter AnimeGANv2 image2coloringbook ameerazam08/Diffusion-Eraser BRIA-Eraser-API openfree/ColorRevive kornia-image-filtering textcutobject IC-Light fffiloni/diffusers-image-outpaint SeedEdit(Simplified Instruction) SeedEdit(Original Instruction) remove-photo-object GPT-4o (Original Instruction) GPT-4o (Simplified Instruction) CodeFormer FLUX.1-dev-Inpainting-Model-Beta-GPU ReplaceAnything turbo_edit BRIA-2.3-Inpainting FLUX.1-Fill-dev Finegrain-Object-Eraser Sketch-Gen Gemini-2.0-Flash (Simplified Instruction) old_photo_restoration MagicQuill BRIA-2.2-ControlNet-Recoloring diffusers-fast-inpaint Image-to-Line-Drawings flux-IP-adapter marcosv/InstructIR Gemini-2.0-Flash (Original Instruction) B2BMGMT_Sharpening flux-fill-outpaint CosXL yizhangliu/ImgCleaner foto_filter text-guided-image-colorization InstructPix2Pix LEDITS leditsplusplus stable-diffusion-xl-inpainting FLUX.1-inpaint-dev fffiloni/InstantIR not-lain/background-removal BRIA-Generative-Fill-API sketch2lineart NonLinear-Blurr-Image"
        },
        {
            "title": "Total Matches Human Edit Win Rate AI Edit Win Rate Tie Rate",
            "content": "100.0 50.0 100.0 83.3 20.0 30.0 35.4 0.0 50.0 25.0 50.0 25.0 39.5 36.2 21.7 33.6 32.1 26.1 25.7 20.9 33.3 33.3 16.7 20.6 33.3 21.1 28.3 21.9 15.4 23.5 14.3 20.9 9.1 20.0 0.0 19.0 14.7 20.0 18.2 16.7 9.6 10.3 8.0 7.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 50.0 0.0 0.0 60.0 30.0 21.5 50.0 0.0 25.0 0.0 25.0 7.0 10.2 21.7 5.3 6.0 10.9 10.9 13.4 0.0 0.0 16.7 12.7 0.0 11.7 3.8 9.8 15.4 5.9 14.3 7.0 18.2 7.1 25.0 4.8 6.6 0.0 0.0 0.0 6.4 5.1 4.0 2.0 7.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 16.7 20.0 40.0 43.1 50.0 50.0 50.0 50.0 50.0 53.5 53.6 56.7 61.1 62.0 63.0 63.4 65.7 66.7 66.7 66.7 66.7 66.7 67.2 67.9 68.4 69.2 70.6 71.4 72.1 72.7 72.9 75.0 76.2 78.7 80.0 81.8 83.3 84.0 84.6 88.0 90.2 92.3 100.0 100.0 100.0 100.0 100.0 3 2 1 6 5 10 65 2 2 4 4 8 443 442 60 512 502 46 101 67 9 3 6 63 3 341 53 215 13 17 7 43 22 340 12 21 334 5 11 24 281 156 25 51 13 1 1 1 1 2 33 E.2 Breakdown of Model Performance by Different Categories Table A7: Win rate breakdown by different models with at least 30 matchups. SeedEdit leads using Simplified Instructions (SI) with +3.3% absolute improvement over the Original Instructions (OI)."
        },
        {
            "title": "Tie Count",
            "content": "SeedEdit (SI) SeedEdit (OI) BRIA-Eraser-API GPT-4o (OI) GPT-4o (SI) Old Photo Restoration FLUX.1-Inpainting MagicQuill Gemini 2.0 Flash (OI) Remove-Photo-Object Gemini 2.0 Flash (SI) ReplaceAnything Finegrain-Object-Eraser CosXL"
        },
        {
            "title": "LEDITS",
            "content": "InstructPix2Pix Stable-Diffusion-XL-Inpainting 39.5 36.2 35.4 33.6 32.1 28.3 25.7 21.9 21.1 21.7 20.0 20.9 20.6 14.7 10.3 9.6 7.8 53.5 53.6 43.1 61.1 62.0 67.9 63.4 68.4 67.2 56.7 72.9 65.7 66.7 78.7 84.6 84.0 90.2 7.0 10.2 21.5 5.3 6.0 3.8 10.9 9.8 11.7 21.7 7.1 13.4 12.7 6.6 5.1 6.4 2.0 443 442 65 512 502 53 101 215 341 60 340 67 63 334 156 281 51 Table A8: % of votes for human edits and AI edits and for the Tie options, categorized by editing actions. AI Win+Tie represents the sum of the AI and Tie columns, indicating the percentage of % requests that can already be handled by the 49 AI models."
        },
        {
            "title": "Action",
            "content": "Adjust Delete Add Recoloring Replace Apply Transform Superresolution Merge Relighting Move Crop Specialized operation Zoom Clone"
        },
        {
            "title": "Human Wins AI Wins",
            "content": "Tie AI Win+Tie no. of votes 67.7 65.1 62.0 68.4 71.2 60.4 70.1 70.2 60.7 66.9 68.1 81.4 62.9 85.3 52.0 25.1 26.3 30.6 25.2 22.3 30.6 20.8 23.6 30.9 24.2 20.2 15.5 26.7 10.7 28.0 7.2 8.6 7.4 6.3 6.6 9.0 9.0 6.2 8.4 9.0 11.7 3.1 10.3 4.0 20.0 32.3 34.9 38.0 31.6 28.8 39.6 29.9 29.8 39.3 33.1 31.9 18.6 37.1 14.7 48. 1293 1123 1114 599 548 399 355 352 191 178 163 129 116 75 25 Table A9: SeedEdit performance (win rate %), categorized by different action verb types"
        },
        {
            "title": "Action",
            "content": "Add Delete Adjust Recoloring Replace Apply Transform Move Relighting Merge Superresolution Specialized operation Crop Zoom Clone"
        },
        {
            "title": "Human Wins AI Wins",
            "content": "Tie AI Win+Tie Count 52.0 46.5 45.7 36.9 43.0 58.6 37.3 31.4 35.7 63.0 47.8 27.3 11.1 40.0 100.0 281 271 197 103 93 87 59 35 28 27 23 22 18 5 1 48.0 53.5 54.3 63.1 57.0 41.4 62.7 68.6 64.3 37.0 52.2 72.7 88.9 60.0 0.0 42.0 38.0 41.1 31.1 34.4 54.0 33.9 14.3 28.6 55.6 30.4 22.7 5.6 20.0 100.0 10.0 8.5 4.6 5.8 8.6 4.6 3.4 17.1 7.1 7.4 17.4 4.5 5.6 20.0 0. 35 Table A10: GPT-4o performance (win rate %), categorized by different action verb types"
        },
        {
            "title": "Action",
            "content": "Adjust Delete Add Recoloring Replace Superresolution Transform Apply Relighting Merge Move Crop Specialized operation Zoom Clone"
        },
        {
            "title": "Human Wins AI Wins",
            "content": "Tie AI Win+Tie Count 59.8 68.6 50.7 63.9 61.5 70.3 64.9 53.9 51.0 54.2 51.5 58.6 39.3 92.0 58.3 35.4 27.7 44.3 30.4 32.3 27.9 28.7 40.8 37.3 39.6 42.4 37.9 42.9 8.0 33.3 4.8 3.7 5.0 5.7 6.2 1.8 6.4 5.3 11.8 6.2 6.1 3.4 17.9 0.0 8.3 40.2 31.4 49.3 36.1 38.5 29.7 35.1 46.1 49.0 45.8 48.5 41.4 60.7 8.0 41.7 336 271 219 158 130 111 94 76 51 48 33 29 28 25 Table A11: Gemini-2.0-Flash performance (win rate %), categorized by different action verb types"
        },
        {
            "title": "Action",
            "content": "Adjust Add Delete Recoloring Replace Transform Apply Superresolution Merge Relighting Move Crop Specialized operation Zoom Clone"
        },
        {
            "title": "Human Wins AI Wins",
            "content": "Tie AI Win+Tie Count 26.4 31.0 25.8 34.6 32.9 25.4 36.4 28.8 28.6 20.0 42.4 25.0 33.3 16.7 28.6 208 158 155 107 85 71 55 52 35 35 33 24 21 18 7 73.6 69.0 74.2 65.4 67.1 74.6 63.6 71.2 71.4 80.0 57.6 75.0 66.7 83.3 71.4 15.9 22.8 21.3 24.3 23.5 12.7 21.8 21.2 25.7 11.4 24.2 16.7 23.8 11.1 14.3 10.6 8.2 4.5 10.3 9.4 12.7 14.5 7.7 2.9 8.6 18.2 8.3 9.5 5.6 14. 36 Table A12: HuggingFace performance (win rate %), categorized by different action verb types"
        },
        {
            "title": "Action",
            "content": "Adjust Add Delete Replace Recoloring Apply Superresolution Transform Merge Relighting Move Crop Specialized operation Zoom Clone"
        },
        {
            "title": "Human Wins AI Wins",
            "content": "Tie AI Win+Tie Count 75.2 73.7 66.9 83.3 75.3 71.3 72.3 74.8 67.9 73.4 82.3 93.1 71.1 85.2 20.0 16.5 19.7 19.7 11.7 19.5 17.7 20.5 13.7 19.8 18.8 9.7 6.9 20.0 11.1 20.0 8.3 6.6 13.4 5.0 5.2 11.0 7.2 11.5 12.3 7.8 8.1 0.0 8.9 3.7 60.0 24.8 26.3 33.1 16.7 24.7 28.7 27.7 25.2 32.1 26.6 17.7 6.9 28.9 14.8 80.0 552 456 426 240 231 181 166 131 81 64 62 58 45 27 37 Table A13: Human preference win rate (%) by main category (all models). AI+Tie denotes share currently handled by AI."
        },
        {
            "title": "Category",
            "content": "Human Wins AI Wins Tie AI Win+Tie Count People And Related Inanimate Objects Animals Environment And Background Text Branding And Abstract 68.5 67.0 59.0 63.5 65.1 24.3 23.1 32.9 28.1 25.2 7.2 9.8 8.1 8.4 9.7 31.5 33.0 41.0 36.5 34.9 1992 864 642 452 404 Table A14: SeedEdit performance (win rate %), categorized by different subjects"
        },
        {
            "title": "Category",
            "content": "Human Wins AI Wins Tie AI Win+Tie Count People And Related Inanimate Objects Animals Environment And Background Text Branding And Abstract 54.2 61.7 41.2 51.0 58.4 37.4 30.6 49.1 40.2 32.7 8.4 7.7 9.7 8.8 8.9 45.8 38.3 58.8 49.0 41.6 321 196 165 102 101 Table A15: Gemini-2.0-Flash performance (win rate %), categorized by different subjects"
        },
        {
            "title": "Human Wins AI Wins",
            "content": "Tie AI Win+Tie Count People And Related Inanimate Objects Animals Text Branding And Abstract Environment And Background 72.9 62.7 69.7 72.1 67.2 19.2 21.2 20.2 22.1 25.4 7.9 16.1 10.1 5.9 7.5 27.1 37.3 30.3 27.9 32.8 328 118 99 68 67 Table A16: GPT-4o performance (win rate %), categorized by different subjects"
        },
        {
            "title": "Human Wins AI Wins",
            "content": "Tie AI Win+Tie Count People And Related Inanimate Objects Animals Environment And Background Text Branding And Abstract 63.6 61.8 53.1 62.7 61.7 31.3 31.4 43.4 26.5 33.3 5.1 6.8 3.4 10.8 4.9 36.4 38.2 46.9 37.3 38.3 514 191 145 83 81 Table A17: HuggingFace performance (win rate %), categorized by different subjects"
        },
        {
            "title": "Human Wins AI Wins",
            "content": "Tie AI Win+Tie Count People And Related Inanimate Objects Animals Environment And Background Text Branding And Abstract 75.4 74.1 70.8 69.0 68.2 16.9 15.3 20.2 23.5 17.5 7.7 10.6 9.0 7.5 14.3 24.6 25.9 29.2 31.0 31.8 829 359 233 200 154 E.3 Analysis of Aesthetic Scores Figure A15: Samples showing that the aesthetic score is not reliable proxy for human evaluation. The AI edit may win or lose user preference, while the aesthetic score can either increase or decrease relative to the source image. 40 Figure A16: Histogram of Aesthetic Improvement Score (Edited Image Score - Source Image Score) Figure A17: Distribution of aesthetic score changes by different rating outcomes. E.4 Additional Results for VLMs-as-a-Judge Table A18: Cohens κ for agreement between human judgment and VLMs, showing generally poor agreement between VLMs and human preferences"
        },
        {
            "title": "Judge",
            "content": "GPT-4o o1 Gemini-2.0-Flash κ 0.195 (n=4332) 0.229 (n=4350) 0.177 (n=4348) Table A19: Cohens κ for agreement between human judgment and VLMs across different model groups. o1 strongly prefers GPT-4o edits, resulting in very poor agreement with humans."
        },
        {
            "title": "Model",
            "content": "GPT-4o κ o1 κ Gemini-2.0-Flash κ SeedEdit GPT-4o Gemini HF 0.226 0.047 0.141 0.203 882 1006 676 0.287 0.054 0.250 0.195 885 1013 679 1773 0.203 0.056 0.144 0.172 882 1014 680 1772 42 E.4.1 Sample Reasoning Outputs from VLM Judges In this section, we provide sample reasoning outputs for each VLM across various agreement settings, compared to human evaluations. Figure A18: Sample reasoning of VLMs-as-a-judge for the case where all three VLMs agree with the human decision. Figure A19: Sample reasoning of VLMs-as-a-judge for the case with mixed results, where some of the VLMs agree with the human decision. 43 Figure A20: Sample reasoning of VLMs-as-a-judge for the case where all three VLMs disagree with the human decision. Figure A21: Sample reasoning of VLMs-as-a-judge for the case with mixed results, where some of the VLMs agree with the human decision. Figure A22: Sample reasoning of VLMs-as-a-judge for the case where all three VLMs agree with the human decision. Figure A23: Sample reasoning of VLMs-as-a-judge for the case where all three VLMs agree with the human decision. 45 Figure A24: Sample reasoning of VLMs-as-a-judge for the case where all three VLMs agree with the human decision. Figure A25: Sample reasoning of VLMs-as-a-judge for the case with mixed results, where some of the VLMs agree with the human decision. E.4.2 VLM Judges Fail to Detect Changes and Alterations in Subject Identity Figure A26: Sample reasoning by VLMs-as-a-judge for cases where all VLMs disagree with human judgments, primarily due to VLMs ignoring subtle differences between images. Figure A27: Sample reasoning by VLMs-as-a-judge for cases where all VLMs disagree with human judgments, primarily due to VLMs ignoring subtle differences between images. 47 Figure A28: Sample reasoning by VLMs-as-a-judge for cases where all VLMs disagree with human judgments, primarily due to VLMs ignoring subtle differences between images. Figure A29: Sample reasoning by VLMs-as-a-judge for cases where all VLMs disagree with human judgments, primarily due to VLMs ignoring subtle differences between images. 48 E.4.3 Hallucinations by VLMs Acting as Judges o1 VLM-as-a-Judge Hallucinations Figure A30: o1 occasionally fails to notice image details when judging different edits. In this example, it thinks only one of the images removed the conical structure, while in reality, both edits removed the building. 49 E.5 Qualitative Analysis of AI Wins and Losses to Identify Patterns When AI Outperforms Human Preference (206 top-rated edits) AI wins primarily due to more accurately reflecting user requests: Overall, 72% of winning AI edits align closely with user instructions. Breakdown by model: GPT-4o: 73% of 62 images Gemini-2.0-Flash: 72% of 32 images SeedEdit: 74% of 76 images Hugging Face models ( ): 69% of 36 images Common Patterns of AI Failures (400 sampled losses) Primary reasons for AI losing include: Misunderstanding user prompts (43% overall): GPT-4o: 16% Gemini-2.0-Flash: 58% SeedEdit: 46% Hugging Face models ( ): 50% Unwanted artifacts or distortions: Facial identity distortions were prominent in GPT-4o (78%) compared to others: * Gemini-2.0-Flash: 14% * SeedEdit: 23% * Hugging Face models ( ): 12% Other unrequested changes were found in 14% to 23% of edits across models. Overall Insight AI models excel when accurately interpreting user requests, yet significant challenges remain in understanding instructions and minimizing unintended modifications. The severity of these issues varies significantly across different AI models. 50 E.6 AI Edit Observations We qualitatively observe that many of the models - in particular text-image models like SeedEdit or CosXL - frequently add irrelevant changes to the original image. We manually inspect the top 50 highest win rate AI edited images and find that 22% of them contain edits that are irrelevant to the original edit request."
        },
        {
            "title": "Human Edit",
            "content": "CosXL Figure A31: This edit adds unnecessary changes to the peoples face, hands and clothes despite only needing to edit the background. User request: Place the two individuals in different background landscape."
        },
        {
            "title": "Human Edit",
            "content": "SeedEdit Figure A32: The request only asks for portions of the image to be removed, but the model adds additional buildings to the city background. User request: Remove Aladdin, Abu, and the play button to focus on the city-scape."
        },
        {
            "title": "Human Edit",
            "content": "MagicQuill Figure A33: The model changes the structure of the ring even though the request says to only change the color. User request: Change the color of the green stones to pale yellow."
        },
        {
            "title": "Human Edit",
            "content": "MagicQuill LEDITS Gemini-2.0-Flash Figure A34: Models fail at edits requiring text replacement. User request: Replace SONIC with ROAD RUNNER and HEDGEHOG with BIRD."
        },
        {
            "title": "Human Edit",
            "content": "Old Photo Restoration BRIA-2.2-ControlNet-Recoloring GPT-4o Figure A35: Models succeed at recoloring and restoring images. User request: Perform color correction and remove spots from the slide scan."
        },
        {
            "title": "Human Edit",
            "content": "SeedEdit InstructPix2Pix Gemini-2.0-Flash GPT-4o Figure A36: Models can recolor specific parts of images. User request: Change the color of the green stones to pale yellow."
        },
        {
            "title": "Human Edit",
            "content": "ReplaceAnything InstructPix2Pix Gemini-2.0-Flash GPT-4o Figure A37: Models can usually change the background properly. User request: Change the background to white or light grey to simulate studio setting."
        },
        {
            "title": "Human Edit",
            "content": "SeedEdit MagicQuill Gemini-2.0-Flash GPT-4o Figure A38: Models can handle requests to replace objects. User request: Replace the metal crate at the bottom of the image with additional sidewalk."
        },
        {
            "title": "Human Edit",
            "content": "SeedEdit CosXL Gemini-2.0-Flash Figure A39: Models can add objects to images, including as part of another object. User request: Add pink tutu dress to the elk in the image."
        },
        {
            "title": "Human Edit",
            "content": "textcutobject Finegrain-Object-Cutter Figure A40: Models excel at removing the backgrounds of images. User request: Create cutout of the raccoon and enhance the image quality to HD."
        },
        {
            "title": "Human Edit",
            "content": "SeedEdit CodeFormer GPT-4o Figure A41: Models can enhance images by removing blur. User request: Remove blur from the subject while keeping the background intact."
        },
        {
            "title": "Human Edit",
            "content": "MagicQuill InstructPix2Pix Gemini-2.0-Flash Figure A42: Models succeed at edits that do not require high amounts of realism. User request: Transform the sky to appear psychedelic."
        },
        {
            "title": "Human Edit",
            "content": "CosXL InstructPix2Pix Gemini-2.0-Flash GPT-4o Figure A43: Models fail at edits that require manipulating several objects within the same image. User request: Arrange the characters into group picture, resizing as necessary and omitting few for aesthetics."
        },
        {
            "title": "Human Edit",
            "content": "CosXL InstructPix2Pix GPT-4o Figure A44: Human users often provide rough reference images as guide for the type of image they want to create. Models typically struggle to understand these reference images when they are not exact representations of what the final output should appear like. User request: Edit the android image to appear as if it is drinking from Java cup."
        },
        {
            "title": "Human Edit",
            "content": "SeedEdit Gemini-2.0-Flash GPT-4o Figure A45: Models struggle with requests requiring humor. User request: Photoshop the soda into humorous scenarios where peoples hands are doing something focused or with extreme concentration."
        },
        {
            "title": "G Generative AI models fail to preserve identity in image editing",
            "content": "We generate an initial image of person wearing white T-shirt across various ages and genders using FLUX-Pro [26]. These images are then processed using GPT-4o and Gemini-2.0-Flash to apply different shirt colors. At each step, we send the image from the previous step along with new color name, repeating this process for eight steps. Finally, we instruct the model to revert the shirt color back to white (the original color). At each step, we compute the L2 distance between the DINOv2_ViT-B/14 [38] feature representations of the modified images and the original image (white shirt) to quantify feature deviations. color: blue red green yellow orange purple pink black white Prompt: Make the shirt {color} Figure A46: GPT-4o fails to preserve the identity of individuals in image editing tasks. When tasked with changing the color of shirt in sequence, the identity of the person changesafter few iterations, the person loses their likeness to the original image. The value indicates the L2 distance between the DINOv2_ViT-B/14 embedding of the image and the original one. 55 color: blue red green yellow orange purple pink black white Prompt: Make the shirt {color} Figure A47: Gemini-2.0-Flash fails to preserve the identity of individuals in image editing tasks. When tasked with changing the color of shirt in sequence, the identity of the person changesafter few iterations, the person loses their likeness to the original image. The value indicates the L2 distance between the DINOv2_ViT-B/14 embedding of the image and the original one. Figure A48: Although changing the shirt color while preserving every other detail in the image affects the DINO embeddings, the magnitude of the change is much smaller than that caused by any edits made by AI models such as GPT-4o and Gemini-2.0-Flash. In this figure, we change the shirt color using an image editing tool while keeping all other elements unchanged. We report the variation in the DINOv2_ViT-B/14 embedding compared to the original image. 56 Figure A49: In sequential image editing request, where model is subsequently asked to change shared color, the distance between the edited image at step and the original image increases as the requests continue. The value for each data point step indicates the L2 distance between the DINOv2_ViT-B/14 embedding of the image and that of the original image."
        },
        {
            "title": "H Dataset License",
            "content": "All data collected in this study comes from publicly available Reddit posts, which fall under Reddits user agreement policy. GPT-4o image edits are produced using the ChatGPT website. Gemini-2.0-Flash image edits are produced using the standard API. SeedEdit images edits are generated by the SeedEdit team. Other AI generated edits were created through the Hugging Face ( ) website."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Auburn University",
        "University of Alberta"
    ]
}