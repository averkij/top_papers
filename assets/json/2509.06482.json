{
    "paper_title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection",
    "authors": [
        "Zhongxiang Xie",
        "Shuangxi Miao",
        "Yuhan Jiang",
        "Zhewei Zhang",
        "Jing Yao",
        "Xuecao Li",
        "Jianxi Huang",
        "Pedram Ghamisi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Change detection from high-resolution remote sensing images lies as a cornerstone of Earth observation applications, yet its efficacy is often compromised by two critical challenges. First, false alarms are prevalent as models misinterpret radiometric variations from temporal shifts (e.g., illumination, season) as genuine changes. Second, a non-negligible semantic gap between deep abstract features and shallow detail-rich features tends to obstruct their effective fusion, culminating in poorly delineated boundaries. To step further in addressing these issues, we propose the Frequency-Spatial Synergistic Gated Network (FSG-Net), a novel paradigm that aims to systematically disentangle semantic changes from nuisance variations. Specifically, FSG-Net first operates in the frequency domain, where a Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates pseudo-changes by discerningly processing different frequency components. Subsequently, the refined features are enhanced in the spatial domain by a Synergistic Temporal-Spatial Attention Module (STSAM), which amplifies the saliency of genuine change regions. To finally bridge the semantic gap, a Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to selectively gate and integrate crucial details from shallow layers. Comprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate the superiority of FSG-Net, establishing a new state-of-the-art with F1-scores of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at https://github.com/zxXie-Air/FSG-Net after a possible publication."
        },
        {
            "title": "Start",
            "content": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection Zhongxiang Xie, Shuangxi Miao, Yuhan Jiang, Zhewei Zhang, Jing Yao, Member, IEEE, Xuecao Li, Jianxi Huang, Senior Member, IEEE, and Pedram Ghamisi, Senior Member, IEEE 1 5 2 0 S 8 ] . [ 1 2 8 4 6 0 . 9 0 5 2 : r AbstractChange detection from high-resolution remote sensing images lies as cornerstone of Earth observation applications, yet its efficacy is often compromised by two critical challenges. First, false alarms are prevalent as models misinterpret radiometric variations from temporal shifts (e.g., illumination, season) as genuine changes. Second, non-negligible semantic gap between deep abstract features and shallow detail-rich features tends to obstruct their effective fusion, culminating in poorly delineated boundaries. To step further in addressing these issues, we propose the Frequency-Spatial Synergistic Gated Network (FSG-Net), novel paradigm that aims to systematically disentangle semantic changes from nuisance variations. Specifically, FSG-Net first operates in the frequency domain, where Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates pseudo-changes by discerningly processing different frequency components. Subsequently, the refined features are enhanced in the spatial domain by Synergistic TemporalSpatial Attention Module (STSAM), which amplifies the saliency of genuine change regions. To finally bridge the semantic gap, Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to selectively gate and integrate crucial details from shallow layers. Comprehensive experiments on the CDD, GZCD, and LEVIR-CD benchmarks validate the superiority of FSG-Net, establishing new state-of-the-art with F1-scores of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at https://github.com/zxXie-Air/FSG-Net after possible publication. Index TermsChange detection, frequency-spatial analysis, feature interaction, gated fusion, remote sensing. I. INTRODUCTION HANGE detection (CD) in remote sensing (RS) images is vital technology for tracking geospatial changes [1], [2], with broad applications in urban planning [3], land monitoring [4], and disaster evaluation [5]. Still, the advent of vast sub-meter resolution datasets from sensors like WorldView, QuickBird, and Gaofen-2 has posed significant challenges to conventional methods like CVA [6] and Random Forest [7], revealing their diminished efficacy [8]. This has paved the way for deep learning-based CD to become the dominant This work was supported by the National Key R&D Program of China under Grant (2023YFB3907600). Z. Xie, S. Miao, Y. Jiang, Z. Zhang, and X. Li are with the College of Land Science and Technology, China Agricultural University, Beijing 100193, China (zx xie@cau.edu.cn; miaosx@cau.edu.cn; jiangyuhan@cau.edu.cn; zhewei@cau.edu.cn; xuecaoli@cau.edu.cn). J. Yao is with the Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China (jasonyao92@gmail.com). J. Huang is with the Faculty of Geosciences and Engineering, Southwest Jiaotong University, Chengdu 60031, China (jxhuang@swjtu.edu.cn). P. Ghamisi is with Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, 09599 Freiberg, Germany, and also with the Lancaster Environment Centre, Lancaster University, LA1 4YR Lancaster, U.K. (e-mail: p.ghamisi@gmail.com). paradigm, owing to the inherent capability of architectures like Convolutional Neural Networks (CNNs) to automatically extract hierarchical features from large-scale, complex data [9]. Unfortunately, even these advanced deep learning models are not without their own set of critical obstacles. Technically, the objective of CD tasks is to identify semantic changes of interest at pixel-wise level from two co-registered RS images acquired at different times. These differences in acquisition time inevitably lead to discrepancies in imaging conditions. Factors such as illumination intensity and seasonal turnover can introduce significant pseudo-changesnonsemantic variations like building shadows or phenological lawns before and after snow changes in vegetation (e.g., cover). Moreover, another challenge lies in the morphological ambiguity between objects of varying scales. For instance, the low-level textural patterns of newly paved footpath might be nearly identical to those of distant, large-scale construction site boundary, despite their clear conceptual disparity. This semantic gap between deep, high-level concepts and shallow, fine-grained details inevitably degrades the precision of boundary delineation. Therefore, to accurately identify changes of interest within complex scenarios, trustworthy CD model is expected to embody the following two capabilities: Pseudo-change suppression. Distinguishing genuine changes of interest from multitude of task-irrelevant interference. Semantics-aware alignment and fusion. Ensuring coherent fusion of multi-level features, aligning abstract semantics with fine-grained spatial details to produce well-delineated boundaries. Aiming at the aforementioned challenges, growing body of research has emerged, exploring solutions from variety of perspectives. For example, Tang et al. [10], and Li [11] have employed techniques like larger convolution kernels and dilated convolutions to incrementally expand the receptive fields of CNN models, thereby emphasizing the significance of environmental alterations [12]. Concurrently, the integration of attention mechanisms, such as self-attention (SelfAtt) [13] and cross-attention (CrossAtt) [14], has also become prevalent strategy for amplifying the most relevant information [15]. Moreover, it is now widely recognized that facilitating feature interaction between bi-temporal representations, before the final difference computation, is paramount for robust change discrimination [16][18]. For this purpose, Liu et al. [16]introduced partial feature exchange mechanism in the channel and spatial dimensions to promote information sharing between images. Fang et al. [18] constructed different temporal novel CD architecture named MetaChanger, which embeds cascade of alternating interaction modules within the backbone feature extractor. Perceiving that purely spatial domain interaction faces bottleneck in sufficiently disentangling authentic changes from background noise, the latest focus has gradually started shifting towards the frequency domain [19][21]. In signal processing, the transformation of an image from the spatial domain to the frequency domain leads to distinct separation of the images spectral content [22]. Low-frequency components predominantly capture the smooth, global characteristics of the image, encapsulating variations in illumination and overarching background changes [23]. Conversely, highfrequency components focus on the finer and more intricate details of the image, including regions of rapid intensity variation, such as edges, textures, and noise, which are critical for discerning detailed structures and boundary [24]. Leveraging this principle, various techniques, including the Discrete Cosine Transform [25], Discrete Wavelet Transform (DWT) [26], and Fast Fourier Transform [27], have been utilized to decouple object identity from stylistic variations, thereby enhancing the models robustness to irrelevant changes. Despite these advances, in tackling the conundrum of pseudo-changes, the preceding methods still fail to establish collaborative mechanism between the frequency and spatial domains. The effective fusion of multi-level features remains challenge, as baseline approaches that rely on simple concatenation, addition, or bilinear pooling often fail to reconcile the representational gap between deep semantic features and shallow, fine-grained details [18], [28]. Consequently, it necessitates the development of bridge mechanism to harmonize features across different hierarchical levels, ensuring proper alignment of high-level concepts and fine-grained details prior to their fusion. Recently, Chen et al. [27] designed U-fusion change perception module, which bidirectionally fuses change features across multiple scales to improve precise boundary delineation. Xu et al. [29] employed coarse-to-fine feature interaction module to progressively fuse multiscale features via hierarchical strategy. Furthermore, another work utilized flow field or deformable convolutions to spatially align multilevel features before their fusion [30], [31]. While effective to some extent, such intricate, multi-scale interaction mechanisms often come at the cost of high computational complexity. Answering this call, we introduce FSG-Net, frethe synergistic framework that quencyspatial change signal from the nuisance and reconciles feature asymmetry in principled manner. The core idea behind FSG-Net lies in: 1) adopting synergistic frequency-spatial separation strategy to discern targets of interest, and 2) leveraging deep semantics to refine multi-level feature details. separates (DAWIM) Concretely, we first introduce Discrepancy-Aware Wavelet that operates on multiInteraction Module resolution wavelet sub-bands, where cross-temporal discrepancy cues drive adaptive sub-band interaction and reweighting. This yields learnable frequency mask that attenuates low-frequency radiometric shifts while preserving high-frequency structural evidence, and the modulated representation is re-projected to the spatial stream through residual path. Building on these refined features, Synergistic 2 Temporal-Spatial Attention Module (STSAM) is proposed to couple the temporally enhanced CrossAtt (to exchange longrange contextual cues between the two times) with coordinate attention (to encode positional priors), thereby amplifying saliency in truly changed regions and stabilizing local structures. Finally, to bridge the semantic gap between deep and shallow featuresa common cause of blurred change boundarieswe propose Lightweight Gated Fusion Unit (LGFU). This module generates semantics-driven gates from deep features to selectively propagate fine-grained details from earlier layers, yielding crisp boundary delineation with minimal computational overhead. To summarize, the main contributions of this work are as follows, 1) We propose the Frequency-Spatial Synergistic Gated Network (FSG-Net), novel architecture that pioneers frequency-spatial synergistic pipeline to systematically disentangle semantic changes from nuisance variations in remote sensing images. 2) To suppress background interference arising from discrepancies in acquisition conditions, we introduce wavelet interaction module, termed DAWIM, that applies tailored strategies to different sub-bands in the frequency domain, effectively mitigating false alarms from radiometric shifts. 3) novel spatial-temporal attention module STSAM is designed to synergistically amplify the saliency of genuine changes by simultaneously modeling global context and preserving fine-grained local details through coupling crossand coordinate-attention. 4) We devise lightweight unit LGFU that bridges the deep-shallow semantic gap to sharp change boundaries. It features semantics-driven gates to selectively fuse features, yielding crisp boundary delineation with high computational efficiency. The remainder of this article is organized as follows. Section II reviews related work in deep learning-based change detection. Section III elaborates on the proposed FSG-Net, detailing the overall architecture and its core modules. Section IV describes the experimental setup, presents comprehensive comparison and analysis against state-of-the-art methods, and includes ablation studies to validate our design choices. Finally, Section concludes the paper and discusses potential future work. II. RELATED WORK A. Deep Learning-Based Change Detection Driven by the powerful feature extraction and representation capabilities of CNNs, deep learning-based CD methods have burgeoned in recent years [32]. Seminal architectures such as FCN, U-Net, and ResNet [33], when augmented with advanced modeling techniques like multi-scale strategies and Transformer, demonstrated impressive performance on various CD tasks [34][36]. From the perspective of information flow and processing paradigms, we broadly categorize existing CD methods into three classes: CNN-based, Transformer-based, and the currently prevalent hybrid architectures. Fig. 1. The overall architecture of the proposed FSG-Net, where respectively. The subscript {1, 2} denotes the temporal phase, while the superscript {1, 2, 3} indicates the feature scale. , and Sj , Dj represent feature maps from the backbone, DAWIM, and STSAM, Daudt et al. [37] first introduced weight-sharing Siamese FCN framework to the CD task, presenting three variants that explored both early fusion (FC-EF) and feature-level fusion through concatenation (FC-Siam-Conc) and differencing (FCSiam-Diff). Techniques such as large-kernel convolutions [10] and dilated convolutions [11] are frequently employed to expand the effective receptive field, thereby enabling CNN models to capture richer contextual information. Simultaneously, various attention mechanisms, including spatial [38] and channel attention [39] have been widely adopted to enhance the saliency of genuine change regions. Despite the extensive exploitation of their potential, CNNs are inherently limited by their inability to model long-range dependencies, shortcoming that leaves clear room for enhancement [40]. The Transformer architecture, originally proposed for natural language processing, has exhibited remarkable success in modeling long-range dependencies through its SelfAtt [41]. Capitalizing on this strength, Dosovitskiy et al. [42] identified the inductive biases of CNNs (locality and translation equivariance) as fundamental constraint on modeling longrange relationships, and pioneered the adaptation of the Transformer to the visual domain with their Vision Transformer (ViT). Subsequently, Bandara et al. [43] proposed Siamese framework integrating hierarchical Transformer encoder and an MLP decoder to efficiently model multi-scale context for CD task. Similarly, Zhang et al. [44] designed pure transformer network, named SwinSUNet, with Siamese Ushaped structure, featuring remarkable 32-block hierarchy. Nevertheless, pure Transformers are often data-hungry and computationally demanding, struggling to capture the finegrained boundary details at which CNNs excel. [45] pioneered hybrid model, BIT, To combine the strengths of both frameworks, Chen et al. that utilizes Transformer encoder to capture global context from CNN features via SelfAtt. Subsequently, hybrid architectures have increasingly emerged as the dominant approach in the field of CD [46], [47]. Yang et al. [46] proposed hybrid model ConvFormer with parallel convolution and multihead SelfAtt enhances the adaptability to complex scenes. GCFormer [48] features Multi-Receptive-Field Convolutional Attention mechanism, specifically designed to extract global context from multiple receptive fields. Informed by prior advancements, we strategically incorporated an enhanced CrossAtt into CNN architecture, aiming to strike balance between detection accuracy and computational efficiency. B. Bi-temporal Feature Interaction in Change Detection fundamental characteristic that distinguishes CD from other dense prediction tasks is the necessity of modeling interactions between bi-temporal features. The importance of feature interaction, well-documented across both homogeneous and heterogeneous data, is especially pertinent to CD due to the domain gap caused by variations in imaging conditions [49]. Numerous approaches have been proposed to address this challenge. Feng et al. [17] devised an intertemporal joint-attention block to manipulate the global feature distribution of each input, thereby stimulating information coupling between intra-level representations. Fang et al. [18] advocated for early feature interaction via co-attention mechanism, demonstrating the critical importance of interaction before fusion by treating features directly as attention maps. Zhao et al. [50] designed the FMCD model to enhance detection in complex scenes by modeling feature context through multi-level interaction module, yielding highly representative features. While these spatial interaction methods have shown promise, they still operate on features where semantic content and stylistic variations are entangled. Recently, the focus of feature interaction research has begun to shift from the spatial domain towards an inclusion of the frequency domain. Xue et al. [25] enhanced feature representations by applying the Discrete Cosine Transform and then using Global Average Pooling on each frequency component to model channel-wise correlations. Tang et al. [21] proposed FDINet for fine-grained object CD, which utilizes Frequency Decoupling Interaction to boost the models discriminative power for different change types. Chen et al. [27] devised the FIMP model, which utilizes the Fourier transform to perform adaptive filtering of bi-temporal features in the frequency thereby isolating and enhancing task-specific feadomain, tures for CD. Regrettably, existing methods have insufficiently probed the feasibility of establishing collaborative mechanism between frequency-domain and spatial-domain feature interactions to fully unlock the potential of their combined strengths. Combining the strengths of existing research, we seek to construct new paradigm for frequency-spatial domain interaction. In this paradigm, frequency-domain interaction is used to suppress pseudo-changes, while spatial domain interaction serves to enhance genuine changes. III. METHODOLOGY A. Method Overview The overall architecture of FSG-Net, illustrated in Fig. 1, is designed as principled, multi-stage pipeline that systematically addresses the core challenges in CD. This process begins with the DAWIM, which purifies the feature representations by suppressing pseudo-changes in the frequency domain. Complementing this, the STSAM then enhances the saliency of authentic change regions in the spatial domain. Finally, the LGFU harmonizes the multi-level features to ensure precise boundary delineation. This cascaded refinement strategy guarantees that both task-irrelevant interference and the semantic gap are collectively mitigated. B. Discrepancy-Aware Wavelet Interaction Module (DAWIM) While the principle of frequency-domain interaction is promising, the key challenge lies in how to process the different frequency components to simultaneously enhance change signals while suppressing noise. The Wavelet Transform offers advantages such as time-frequency localization and multi-frequency subband analysis, making it particularly wellsuited for CD tasks involving scale-sensitive targets [26]. Accordingly, the DAWIM employs distinct processing strategies tailored to the specific characteristics of each frequency component. The operational workflow of this module is illustrated in Fig. 2. Specifically, 2D Haar DWT is first applied to decompose the bi-temporal spatial feature maps, denoted as Fi (where {1, 2}), into four frequency-domain subcomponents, denoted as {LLi, LHi, HLi, HHi}. This decomposition is achieved by applying set of predefined wavelet filters, followed by downsampling (), as formulated below: LLi = (Fi flow LHi = (Fi flow HLi = (Fi fhigh HHi = (Fi fhigh low) , high) , low) , high) , (1) (2) (3) (4) 4 Fig. 2. The operational workflow of the DAWIM. 2, 1/ 2, 1/ 2] and fhigh = [1/ where flow = [1/ 2] are the low-pass and high-pass filters, respectively. The superscript indicates the transpose operation for applying the filters in the vertical direction. Subsequently, each frequency component undergoes an appropriate processing strategy based on its characteristics to maximize the benefits of frequency feature interaction. The low-frequency components LLi primarily represent large-scale spatial patterns and gradual radiometric variations, such as land cover distribution and global illumination trends [51]. Meanwhile, they exhibit relatively stable changes in the time domain, primarily reflecting gradual background variations [23]. To capture these slow temporal dynamics effectively, 3D convolution is applied to extract and then enhance the temporal features embedded within these frequency bands. Specifically, temporal dimension is first inflated to LLi RCHW , forming ˆLLi RC1HW . Then, ˆLL1 and ˆLL2 are concatenated along the temporal dimension to form LL RC2HW and 3D convolution is applied to enhance the temporal relationship modeling between the bitemporal features. The kernel size is set to 233, to perform feature fusion along the temporal dimension, while capturing spatial context from 33 neighborhood. Finally, reshaping operation is performed on the features ˆLL RC1HW obtained from the 3D convolution to embed the temporal dynamics into the channels, resulting in low-frequency interaction features (cid:102)LL R(C1)HW . The entire operation can be formulated as: (cid:102)LL = Reshape(Conv3D233(Concat[ ˆLL1, ˆLL2])), (5) where Concat represents concatenation along the temporal dimension. Compared to the LLi, the mid-frequency components (LHi/HLi) encapsulate more localized temporal variations, particularly abrupt structural changes in the horizontal and vertical directions [52]. We therefore adopt an interaction method similar to that used for components LLi, using 3D convolution to form (cid:103)LH and (cid:103)HL, but set the kernel size to 211 to fuse features along the temporal dimension without compromising the spatial structure. 5 (cid:103)LH = Reshape(Conv3D211(Concat[ ˆLH 1, ˆLH 2])), (cid:103)HL = Reshape(Conv3D211(Concat[ ˆHL1, ˆHL2])). The high-frequency component HHi, typically associated with edges, contours, and fine-grained structural details, is processed using element-wise subtraction to obtain the difference feature [26], formulated as follows: (6) (7) (cid:103)HH = δ(BN(Conv11(HH1 HH2))). (8) After obtaining these frequency interaction features, we introduce an adaptive weighting mechanism based on modified Squeeze-and-Excitation Networks (SENet) [53]. This approach dynamically enhances the saliency of change regions while mitigating the impact of irrelevant background noise. Taking (cid:102)LL as an example, both max pooling (M AP ) and global average pooling (GAP ) are applied to the (cid:102)LL simultaneouslythe former highlights the most significant feature responses, while the latter ensures that the model does not focus solely on extreme variations but instead learns more robust features. Then, global feature vector, obtained by concatenating the max-pooled and average-pooled features, is used to compute the channel-adaptive weight (cid:102)LL. Finally, the computed weight is applied to the original LLi component via the Hadamard product, followed by residual connection. This operation aims to leverage change-aware features, derived from bi-temporal interaction, to guide and refine the original, mono-temporal representations. The formulation is described as follows: (cid:102)LL = σ(W2δ(W1[GAP( (cid:102)LL), MAP( (cid:102)LL)])), LL (cid:102)LL LLi + LLi. = (9) (10) Here, σ is the Sigmoid activation function, δ is the ReLU activation unit, and W1, W2 are the fully connected layer weights, where W1 reduces dimensionality and W2 restores it. The symbol denotes the Hadamard product. Following the interaction between different frequency components, the feature change maps are reconstructed to the spatial domain through the application of the inverse discrete wavelet transform (IDWT). It can be formulated as: = IDWT(LL i, LH i, HL i, HH i). (11) C. Synergistic Temporal-Spatial Attention Module (STSAM) Following the frequency-domain feature interaction, noise interference induced by acquisition conditions is effectively attenuated. Our subsequent objective is to leverage spatial feature interaction to establish an information conduction pathway between the bi-temporal features, thereby mutually steering the allocation of attention to regions with genuine Fig. 3. The internal mechanism of the STSAM. changes. To accomplish this, the STSAM is constructed by integrating CrossAtt augmented with temporal embeddings and coordinate-attention (CoordAtt) [15]. Compared to the SelfAtt, the CrossAtt equally processes the features of both temporal instances, avoiding bias towards single image while simultaneously capturing temporal differences and global changes. Complementing this global temporal interaction, the CoordAtt is further employed to refine the local spatial structure, ensuring that fine-grained details, such as changes in building boundaries, are given adequate attention. As shown in Fig. 3, Fi RCHW are fed into parallel processing stage consisting of two branches: the CrossAtt (branch 1) and the CoordAtt (branch 2). In branch 1, learning temporal embeddings (inspired by position embeddings in Transformer) [41], denoted as Ti RC11, are first introduced and then added to Fi through Pythons broadcasting mechanism, aiming to amplify the temporal discrepancies between inputs. Next, the enhanced features are projected into query (Qi), key (Ki), and value (Vi) spaces using shared 11 convolutions. To maintain computational efficiency and in line with common practice, the channel dimensions of and are compressed to 1/8 of the original, while the dimension of is preserved. Subsequently, the from the counterpart is used to perform similarity computation and weighting operations within its own space, ultimately yielding the final CrossAtt output. This output is then scaled by learnable parameter ω and added back to the original feature Fi via residual connection. The process is described by the following formulas: Qi, Ki, Vi = Linear(Conv11(Fi + Ti)), CrossAtti = Softmax(Qj ) Vi, branch1 = ω CrossAtti + Fi, (12) (13) (14) where i, {1, 2} are temporal indices with = j, and ω is learnable scaling parameter initialized to 0 for training stability. In branch 2, adaptive average pooling is first applied to Fi RCHW along the height and width directions, resulti RCH1 and ing in two directional feature map tensors: xh RC1W (after dimension transposition). Then, xh xw and xw are concatenated along the spatial dimension and processed through 11 convolution to fuse the pooled information from both directions. Next, the fused features are split, convolved, and passed through sigmoid activation to generate the RC1W CoordAtt coefficients ah for the height and width directions, respectively. Finally, the input feature Fi is re-weighted through Hadamard product with the computed attention coefficients ah to perform the weighting operation. RCH1 and aw and aw xh = AvgPoolw(Fi), fi = δ(BN(Convf (Concat[xh xw = AvgPoolh(Fi), )T ]))), , (xw )) σ(Convw(f i )), branch2 = Fi σ(Convh(f (15) (16) (17) where AvgPoolw and AvgPoolh are 1D average pooling operations. Concat denotes concatenation along the spatial dimension, Convf , Convh, and Convw are separate 11 convolutions, δ is ReLU, and σ is Sigmoid. The fused tensors fi are divided into to generate directional attention maps. and The resultant feature maps from both pathways are then integrated through channel-wise concatenation and subsequent convolutional fusion, yielding the final output representation: = Conv11(Concat[F branch1 final , branch2 ]). (18) D. Lightweight Gated Fusion Unit (LGFU) After the previous two steps, the multi-scale difference features are obtained through element-wise subtraction. The subsequent challenge lies in effectively fusing them to generate final change feature map that incorporates both deep semantic information and shallow spatial details. Deliberately bypassing the intricate designs in feature decoders, we introduce LGFU that establishes cross-scale information flow to aggregate both global context and local details. The fundamental principle of the LGFU is to selectively fuse features in the decoders upsampling path. Instead of naively merging high-level semantic features with all available lowlevel detailswhich are often contaminated by noise and irrelevant clutterthe LGFU introduces learnable gating mechanism. This gate adaptively controls the proportion of shallow-feature information that flows through at pixelwise level, effectively filtering out noise while preserving the integrity of meaningful change boundaries. Specifically, given the i-th layers deep difference feature Fdeep RCi(H/2i)(W/2i), it first undergoes 2 upsampling and 1 1 convolution to align with the adjacent difference feature, Fshallow RCi1(H/2i1)(W/2i1), in terms of spatial resolution and channel dimension, respectively. Subsequently, the aligned deep features and the shallow features are concatenated along the channel dimension and then fed into the Gating Unit (GU). The GU consists of 33 convolution, followed by Batch Normalization, ReLU activation, 1 1 convolution, and terminal Sigmoid function, which outputs single-channel gating map, Gmap R1(H/2i1)(W/2i1), with pixel values normalized to the range [0, 1]. The gating map Gmap then modulates the shallow features Fshallow pixelwise by performing Hadamard product, where the singlechannel Gmap is broadcast across all channels of Fshallow via Pythons broadcasting mechanism. This operation gates the 6 model to learn at each spatial location how much fine-grained detail from the shallow layer should be passed through to refine the semantic predictions from the deep layer. Ultimately, the gated shallow features Fshallow are incorporated into the aligned deep features deep using residual-like addition. This can be expressed as: deep = Conv11(Upsample(Fdeep)), deep, Fshallow]), Gmap = GU(Concat[F fused = Gmap Fshallow + deep. (19) (20) (21) This fusion process is iteratively applied in bottom-up manner across the decoder, progressively integrating features from shallower layers until the final high-resolution change map is generated. Finally, the entire network is optimized end-to-end using composite loss function. To address the pervasive class imbalance in CD, we combine the Binary Cross-Entropy (LBCE) and Dice (LDice) losses, leveraging the formers ability to enforce pixel-level accuracy and the latters robustness to skewed distributions [26], [29]. The final objective function is linear combination of the two: LTotal = LBCE + LDice. (22) IV. EXPERIMENTS AND ANALYSIS A. Datasets"
        },
        {
            "title": "Comparative experiments were conducted on three publicly",
            "content": "available datasets: 1) CDD [54] dataset, captured from Google Earth, contains 11 pairs of raw images with varied resolutions (0.03 to 1 m/pixel), comprising both seasonal and disasterinduced changes. This dataset is particularly challenging due to complex scenarios involving shadows, lighting variations, and cloud cover [55]. We used the official pre-processed version, which consists of 10,000, 3,000, and 3,000 image patches of size 256256 for the training, validation, and test sets, respectively. 2) GZ-CD [56] dataset, sourced from Google Earth via BIGEMAP, comprises season-varying, high-resolution (0.55 m/pixel) image pairs of Guangzhous suburbs. The original images were cropped into 256256 nonoverlapping patches, resulting in 1073 clips (originally 1067 in the source paper) after filtering out those containing changed pixels. The final training, validation, and test sets were split with 751, 214, and 108 pairs, respectively. 3) LEVIR-CD [57] dataset, large-scale benchmark for building CD, consists of 637 image pairs (0.5 m/pixel) sized at 10241024. These images document diverse building changes, such as the emergence of villas and warehouses, over 5 to 14-year span in multiple cities across Texas, USA. Following the official protocol, the images are partitioned into 7,120, 1,024, and 2,048 patches of 256256 for training, validation, and testing, respectively. 7 Fig. 4. Visualization of experimental results on the CDD dataset. White indicates true positives, black signifies true negatives, red marks false positives, and green highlights false negatives. Colored dashed boxes highlight the differences among the algorithms. B. Experimental Setup 1) Implementation Details: All experiments were conducted in PyTorch on single NVIDIA RTX 4090 GPU. We optimized the network using the AdamW optimizer with weight decay of 0.01. The initial learning rate was set to 1e-3 for the randomly initialized decoder and head, and 1e-4 for the pre-trained backbone, both decayed to 1e-6 over 100 epochs via cosine annealing schedule. batch size of 32 was used for all datasets. 2) Evaluation Metrics: Quantitative evaluation is conducted using five standard metrics: Precision (Pre.), Recall (Rec.), F1-score (F1), Intersection over Union (IoU), and Overall Accuracy (OA): Pre. = Rec. = F1 = IoU = OA = , , TP TP + FP TP TP + FN 2 Pre Rec Pre + Rec TP TP + FP + FN TP + TN TP + FP + TN + FN , , (23) (24) (25) (26) (27) , where TP, FP, TN, and FN represent the counts of true positives, false positives, true negatives, and false negatives. We prioritize the F1 and IoU for providing more balanced evaluation. 1) BIT [45] pioneers hybrid CNN-Transformer architecture, where Transformer encoder leverages SelfAtt on CNN feature to capture long-range dependencies. 2) ChangeFormer [43] employs Siamese hierarchical Transformer encoder and an MLP decoder to capture multi-scale, long-range contextual details. 3) DMINet [17] constructs an intertemporal joint-attention module for early feature interaction, enabling guided, dual-branch difference acquisition. 4) AMTNet [16] utilizes an integrated CNN-Transformer feature exchange-based domain alignment and for attention-driven multi-scale feature enhancement. 5) WS-Net++ [26] introduces wavelet Siamese network that uses semi-supervised domain adaptation to mitigate appearance shifts and reduce pseudo-changes. 6) CDNeXt [47] proposes Temporospatial Interactive Attention to explicitly model and correct for geometric perspective rotation and temporal style differences. 7) FTransDF-Net [20] features dual architecture with gated module for multi-scale detail fusion and frequency Transformer for long-range dependency capture. 8) ConvFormer [46] integrates parallel convolution, SelfAtt, and novel Temporal Attention mechanism for difference-guided cross-temporal relationship modeling. Above compared models were benchmarked using their publicly available implementations and default parameters. D. Experimental Results C. Comparisons With SOTAs To validate the effectiveness and efficiency of FSG-Net, widely adopted SOTA methods used as benchmarks for bitemporal CD are implemented and compared, including the following. 1) Quantitative Evaluation: Table summarizes the quantitative comparison on the CDD, GZ-CD, and LEVIR-CD datasets, where it is evident that the proposed FSG-Net attains superior results on nearly every metric. Overall, FSG-Net achieves an F1 score of approximately 90% across all three datasets. noticeable performance degradation for all methods 8 Fig. 5. Visualization of experimental results on the GZ-CD dataset. White indicates true positives, black signifies true negatives, red marks false positives, and green highlights false negatives. Colored dashed boxes highlight the differences among the algorithms. TABLE COMPARISON RESULTS ON THE THREE CD DATASETS. THE BEST SCORE IS HIGHLIGHTED IN BOLD. ALL RESULTS ARE DESCRIBED AS PERCENTAGES (%)."
        },
        {
            "title": "Methods",
            "content": "Ref."
        },
        {
            "title": "CDD",
            "content": "GZ-CD LEVIR-CD Pre. Rec. F1 IOU OA Pre. Rec. IOU OA Pre. Rec. F"
        },
        {
            "title": "IOU OA",
            "content": "TGRS21 BIT [45] 90.67 86.44 88.50 79.37 97.26 82.90 80.68 81.77 69.16 90.63 89.24 89.37 89.30 80.67 98.88 ChangeFormer [43] IGARSS22 91.54 89.31 90.41 82.50 97.68 84.59 84.28 84.43 73.06 91.90 91.97 89.03 90.48 82.62 99.02 92.45 90.72 91.58 84.47 97.96 89.04 86.52 87.76 78.19 93.71 92.27 88.94 90.57 82.77 99.03 92.32 92.91 92.61 86.24 98.19 88.51 85.27 86.86 76.77 93.28 91.34 89.76 90.54 82.72 99.02 92.95 93.21 93.08 87.06 98.31 88.32 85.76 87.02 77.02 93.33 92.11 90.05 91.07 83.60 99.08 92.76 93.87 93.31 87.46 98.35 89.21 86.34 87.75 78.17 93.72 92.15 89.69 90.91 83.33 99.06 93.10 94.03 93.56 87.90 98.42 89.13 87.45 88.28 79.02 93.95 92.19 89.73 90.94 83.39 99.07 93.35 93.19 93.28 87.41 98.36 89.11 87.58 88.33 79.11 93.97 92.14 89.69 90.90 83.32 99.06 DMINet [17] AMTNet [16] WS-Net++ [26] CDNeXt [47] FTransDF-Net [20] ConvFormer [46] TGRS23 ISPRS23 TGRS24 JAG24 JAG25 TGRS25 FSG-Net"
        },
        {
            "title": "Ours",
            "content": "93.33 95.01 94.16 88.96 98.56 90.85 88.20 89.51 81.01 94.61 92.53 90.04 91.27 83.94 99.10 is observed on the GZ-CD dataset. This is likely consequence of two compounding factors: first, the data scarcity hinders the models from learning the complex change patterns robustly. Second, the datasets high prevalence of seasonal discrepancies introduces significant ambiguity, further complicating the detection task with limited training data. Even under these demanding conditions, the proposed algorithm still exhibits strong performance, outperforming all comparison models with Pre. of 90.85%, Rec. of 88.20%, F1 of 89.51%, IoU of 81.01%, and OA of 94.61%. This remarkable performance stems from the synergistic interplay between the DAWIM and STSAM. The former mitigates spurious variations by operating in the frequency domain, while the latter amplifies attention towards salient change regions. This collaborative mechanism results in marked increase in the models sensitivity to true changes. Likewise, the competitive performance of FSG-Net extends to the CDD and LEVIR-CD datasets, where it surpasses the recent ConvFormer by 1.55% and 0.62% in IoU, respectively. 2) Qualitative Evaluation: Qualitatively, Figs. 4 to 6 present the predicted maps across different datasets, with distinct colors used to indicate the detection results: TP (white), TN (black), FP (red), and FN (green), highlighting the correctness or errors in the predictions. Colored dashed boxes highlight the differences among the algorithms. Visualization on CDD  (Fig. 4)  : As shown in the 1st and 4th rows, most algorithms struggle with the complex, crisscrossing network of rural paths, leading to either missed detections (e.g., BIT) or false alarms (e.g., WS-Net++). Impressively, FSG-Net demonstrates exceptional resilience in this complex scene. It almost flawlessly traces the full extent of the paths change with high fidelity, notwithstanding few negligible and inevitable misclassifications. During the construction of new agricultural facilities on barren land (5th row), severe illumination artifacts induce most algorithms to generate large, contiguous regions of pseudo-changes. This vulnerability is Fig. 6. Visualization of experimental results on the LEVIR-CD dataset. White indicates true positives, black signifies true negatives, red marks false positives, and green highlights false negatives. Colored dashed boxes highlight the differences among the algorithms. precisely what FSG-Net addresses through its differential frequency-domain interaction, which is designed to disentangle semantic changes from such non-semantic, illuminationinduced noise. Collectively, the aforementioned examples provide compelling evidence of FSG-Net proficiency in both suppressing pseudo-changes and capturing genuine change targets. Visualization on GZ-CD  (Fig. 5)  : closer look at the 1st row reveals that in the scenario where two new buildings are constructed on farmland, FSG-Net not only segments the entire building framework completely but also preserves the boundary integrity with remarkable clarity at the transition between the foreground (changed) and background (unchanged) regions. Such superior performance is intrinsically linked to the contribution of the STSAM. On one hand, the CrossAtt, augmented with temporal embeddings, models long-range contextual relationships across time between the bi-temporal features. On the other hand, the CoordAtt further enhances the focus on the fine-grained details of target edges. In this manner, changes, irrespective of their scale (global or local), are given due consideration and are not overlooked. Visualization on LEVIR-CD  (Fig. 6)  : The dataset, specifically labeled for building CD, is characterized by wide spectrum of change scenes. It features changes ranging from the emergence of large-scale industrial structures, such as factories and warehouses, to the construction of dense residential clusters. The morphological heterogeneity of these structures presents substantial hurdle for accurate boundary delineation. Owing to the LGFU, which narrows the semantic gap between high-level and low-level features, FSG-Net maintains the integrity of building structures and the sharpness of their edges more effectively than all competing algorithms. Consistent with the quantitative metrics in Table I, the qualitative analysis further underscores the SOTA capabilities of FSG-Net on the LEVIR-CD. TABLE II ABLATION STUDY OF DIFFERENT MODULES. ALL RESULTS ARE DESCRIBED AS PERCENTAGES (%). DAWIM STSAM LGFU CDD GZ-CD LEVIR-CD F1 IOU IOU F1 IOU Baseline 90.33 82.37 84.97 73.87 90.29 82.30 91.97 85.13 86.80 76.68 90.45 82.57 91.60 84.50 86.55 76.29 90.71 83.00 91.15 83.74 85.80 75.13 90.62 82.85 92.50 86.05 87.73 78.14 91.05 83.57 92.88 86.71 87.95 78.49 90.73 83.03 93.35 87.53 88.57 79.48 91.01 83.50 Ours 94.16 88.96 89.51 81.01 91.27 83.94 E. Ablation Experiment and Analysis To provide structured and meaningful evaluation, we benchmark against competitive baseline, incorporating proven technologies like basic frequency differencing and SelfAtt (replacing DAWIM and STSAM respectively) [13], [21], [45] with U-Net decoder. This strong baseline serves to demonstrate that the performance gains of FSG-Net are not merely due to the inclusion of generic enhancements, but stem from specific, refined designs. Table II presents the ablation results for each component, focusing on the F1 and IoU. 1) Effectiveness of DAWIM: The primary role of the DAWIM is to suppress pseudo-changes by discerningly processing features in the frequency domain. To validate this, we isolated its effect by adding it to the Baseline. As shown in Table II (row 2 vs. row 1), this integration yields consistent performance boost across all datasets. Most notably, it achieves F1 improvements of 1.64% on CDD and 1.83% on GZCD. The fact that these substantial gains occur on datasets 10 Fig. 7. Visualization of the DAWIMs feature modulation process. Rows 1 & 4 show the features before interaction for Image1 and Image2. Rows 2 & 3 show the features after interaction. The ground truth change mask (Label) and the models final prediction (Pred) are also provided for reference. The columns display the input image, the spatial feature map, and its wavelet components. Fig. 9. Visualization of the LGFUs contribution to refining prediction confidence heatmaps. Columns from left to right show: the bi-temporal images (Image1, Image2), the ground truth (Label), and the predicted heatmaps from Baseline, the model without LGFU (w/o LGFU), and the complete FSG-Net. The color intensity corresponds to the predicted change probability, ranging from black (low) to white (high). benefit greater than the sum of its parts. To provide more intuitive understanding of the DAWIM, as shown in Fig. 7, we visualize the normalized internal feature representations before and after the frequency-domain interaction. The figure is structured in sandwich layout, where the features maps before interaction (Rows 1 and 4 for T1 and T2 respectively) flank after interaction (Rows 2 and 3). From the magnified insets highlighting feature differences before and after interaction, we can draw two key observations. First, the module effectively preserves crucial spatial structures, as evidenced by the overall consistency in feature patterns before and after the interaction. Second, it successfully introduces temporal dynamics, indicated by the subtle yet distinct changes in feature intensity and texture postinteraction, an effect that is quantitatively supported by the results in Table II. 2) Effectiveness of STSAM: The STSAM is designed to refine the representation of genuine changes by creating an information flow pathway between bi-temporal features to mutually steer the attention distribution effectively. As detailed in the rows 6 and 8 of Table II, ablating the STSAM module leads to drop in F1 of 1.28%, 1.56%, and 0.54% on the CDD, GZ-CD, and LEVIR-CD datasets, respectively, compared to the full model. By capturing temporal dependencies globally via CrossAtt augmented with temporal embedding and encoding precise spatial locations locally through CoordAtt, the STSAM ensures that genuinely changed regions, irrespective of their scale or complexity, are saliently represented, thereby achieving complementary temporal-spatial focus on change targets. Fig. 8 illustrates the progressive feature refinement by the STSAM in two representative scenarios featuring building changes amidst seasonal shifts. Evidently, the CoordAtt branch sharpens the feature representation, accentuating fine-grained spatial details and preserving the integrity of building boundaries. The yellow-boxed regions in the fourth column warrant Fig. 8. Visualization of the progressive feature refinement process within the STSAM across two challenging scenarios. Columns from left to right show: the input images (Image1, Image2), and the feature heatmaps after the DAWIM, Coordinate Attention (Coord.), Cross-Attention (Cross.), and the final STSAM stage. For each scenario, the ground truth (Label) and the final prediction (Pred) are provided for comparison. The yellow boxes highlight the reciprocal attention focusing mechanism of the Cross-Attention. interference. This confirms that characterized by significant seasonal variations strongly corroborates the exceptional capability of the DAWIM in mitigating task-irrelevant the strategy of applying distinct, tailored interactions to different frequency components is more effective than simple, uniform differencing approach. Observing the results in rows 1, 2, 3, and 7, it is evident that combining DAWIM and STSAM yields significant performance gain over their individual contributions. This reciprocal effect can be attributed to sequential enhancement mechanism: DAWIMs initial filtering of background noise allows the STSAM to subsequently focus its temporal-spatial attention mechanism on authentic change targets with greater precision, thereby achieving cumulative 11 TABLE III ANALYSIS ON DAWIMS INTERACTION STRATEGIES. THE METRIC IS F1 SCORE (%). TABLE IV COMPARISON ON STSAMS DIFFERENT COMPONENTS. THE METRIC IS F1 SCORE (%). Strategy Weighted Residual CDD GZ-CD LEVIR-CD Component Difference Conv211 Conv233 w/o SE w/o Res DAWIM 92.83 93.47 93.65 93.97 92.57 94. 88.03 88.42 88.92 89.28 87.81 89.51 91.09 91.12 91.17 91.20 90.94 91.27 Cross-Attention Coord-Attention Self-Attention Self & Coord-Attention w/o time embedding STSAM CDD 93.69 92.36 92.88 93.47 94.01 94.16 GZ-CD LEVIR-CD 88.93 87.41 87.95 88.74 89. 89.51 91.02 90.31 90.73 90.91 91.18 91.27 special attention as they highlight key observation: through the temporally-enhanced CrossAtt, the T1 features perceive the new building construction in the T2 image, prompting heightened focus on the counterpart location within itself. Conversely, the T2 features identify the demolition in T1 and similarly amplify attention on their own counterpart region. The final fused feature map showcases the synergy of these two streams: it inherits the high-level semantic focus from the temporal branch while being refined with the high-fidelity structural information from the spatial branch. This fusion enables the model to robustly identify multi-scale changes while preserving sharp boundary integrity, as evidenced in the final segmentation results. The bottom two rows present similar yet more challenging case where new building is camouflaged by snow cover, making it visually blend with the environment. Remarkably, due to the robust frequencyspatial interplay, FSG-Net precisely delineates the changed area despite these challenging camouflage-like artifacts, result clearly visible in the final prediction. 3) Effectiveness of LGFU: Bridging the semantic gap between deep abstract features and shallow detail-rich features is another critical challenge for accurate CD. The LGFU achieves this by leveraging high-level semantic information to guide and enrich shallow features. clear observation from Table II reveals the significant contribution of the LGFU. Integrating the LGFU individually into the baseline boosts the IoU by 1.37%, 1.26%, and 0.55% on the respective datasets. More revealingly, when LGFU is the sole module ablated from the full model, the performance degradation is even more pronounced, with IoU dropping by 1.43% and 1.53% on the first two datasets. This asymmetry is crucial finding: It indicates that the LGFUs contribution is amplified when it operates on the highly refined features generated by DAWIM and STSAM. This validates powerful synergistic effect within our architecture, where the modules not only contribute individually but also enhance each others efficacy, culminating in final model that is greater than the sum of its parts. Fig. 9 visualizes the LGFUs contribution using predicted probability heatmap, where the color of each pixel maps to the models confidence score for the change class. The color intensity signifies the probability of change, ascending from black (lowest) through red and yellow to white (highest). It is easily observed that the baseline captures the general change regions, yet it struggles with blurry boundaries and noisy interferences. The predominantly yellow prediction map further suggests lack of absolute certainty in its identification of changed areas. Only upon the final integration of the LGFU does the model produce exceptionally sharp and welldefined change boundaries that closely adhere to the ground truth. This provides compelling visual evidence that the LGFU is instrumental not just in bridging the semantic gap, but ultimately in translating high-level conceptual understanding into pixel-perfect boundary delineations. 4) Analysis on DAWIMs Interaction Strategies: To validate the internal design of DAWIM, we compared different frequency-domain interaction strategies, including: unified approaches (e.g., Difference, Conv2x1x1, Conv2x3x3) for all components, and the removal of the SE weighting mechanism and the residual connection, as detailed in Table III. Taking the CDD as an example, the DAWIM surpasses the other three unified strategies in F1 by 1.33%, 0.69%, and 0.51% respectively, indicating that tailored interaction strategy could maximize the benefits of frequency-domain feature interaction. Another noteworthy observation is that removing the residual connection leads to catastrophic collapse, with the F1 dropping by 1.59%, 1.70%, and 0.33% on the three datasets, respectively. It can be attributed to well-known consensus in deep learning: removing residual connections hinders model convergence and impedes the smooth flow of gradients and information [41]. This suggests that robust and easily optimizable architecture is prerequisite for fully unleashing the potential of advanced frequency-domain interaction strategies. 5) Comparison on STSAMs Components: Table IV presents an in-depth ablation study, comparing different standard attention mechanisms and the effect of removing the temporal embedding from the complete STSAM. Substituting SelfAtt with CoordAtt slightly decreases detection performance, due to the latters inability to effectively capture longrange dependencies. When the two attention mechanisms are the global modeling capabilities and directioncombined, aware feature encoding yield certain degree of performance improvement. Nevertheless, the inherent functional overlap in this combination keeps the F1 lower by 0.22%, 0.19%, and 0.11% on the three datasets, respectively, compared to using only CrossAtt. The last two rows reveal that explicitly amplifying temporal disparities between bi-temporal features via time embedding can inject new life into the model, particularly when it hits performance ceiling, with the F1 increasing by 0.15%, 0.17%, and 0.09%, respectively. TABLE COMPARISONS OF MODEL ARCHITECTURE AND EFFICIENCY Method Backbone Frequency Interaction Params(M) FLOPs(G) BIT ChangeFormer DMINet AMTNet WS-Net++ CDNeXt FTransDF-Net ConvFormer ResNet18 MiT-b1 ResNet18 ResNet18 ResNet18 CNX-t DFG-UNet w/ ST & BTBlocks Ours ResNet18 3.55 13.94 6.24 16.44 39.42 17.91 37.72 4.35 52.84 14.42 14.71 15.76 9.37 5.14 13.76 6. F. Discussion Synthesizing the quantitative results (Table I) and qualitative evidence (Figs. 4 to 6), it is evident that FSG-Net effectively suppresses pseudo-changes and bridges the semantic gap through the orchestrated interplay of its DAWIM, STSAM, and LGFU modules. The superiority of our proposed FSG-Net is demonstrated by its SOTA performance across all three public datasets, with an F1 and IoU of 94.16% and 88.96% on CDD, 89.51% and 81.01% on GZ-CD, and 91.27% and 83.94% on LEVIR-CD respectively. Providing intuitive evidence for this approach, the visualization results in Fig. 7 and Fig. 8 illustrate that DAWIMs frequency-domain interaction suppresses pseudo-changes, while STSAMs spatial interaction enhances genuine changes. This bidirectional interplay mitigates visual feature interference arising from disparate imaging conditions and promotes information coupling within intra-level representations. Moreover, the evidence in Fig. 9 substantiates the role of the LGFU. It leverages high-level semantics to guide the selection of valuable details from shallow features, resulting in the effective filtering of noise and the generation of clean, well-defined boundaries. However, the current architectures robustness in extreme scenarios could be further enhanced. notable limitation, illustrated in the final row of Fig. 6, is the failure to detect minute, isolated building changes amidst vast homogeneous backgrounds. Table compares the architectural complexity and efficiency. The model parameters (Params) and Floating-Point Operations (FLOPs) were based on unified image input size of 2562563. Beyond incorporating an advanced frequency interaction mechanisma feature absent in most SOTA methodsFSG-Net achieves superior balance in model complexity. While recent architectures like CDNeXt are parameterheavy (39.42M) and ChangeFormer is computationally expensive (52.84G), FSG-Net achieves an exceptional trade-off with only 13.76M parameters and 6.21 GFLOPs. V. CONCLUSION In this article, we have proposed FSG-Net to address two critical concerns in CD tasks: the high false alarm rates caused by pseudo-changes from varying imaging conditions, and the semantic gap between deep, abstract features and shallow, detail-rich features. For the former, FSG-Net introduces synergistic frequency-spatial separation strategy. More specifically, it leverages the DAWIM to optimize feature representations by applying tailored processing to different frequency-domain components, thereby suppressing interference from task-irrelevant factors. Subsequently, the STSAM module establishes an information pathway between the bitemporal features, which in turn mutually guides their attention towards genuine change targets. This synergistic push-pull mechanism is designed to maximize the benefits of feature interaction. To tackle the second challenge, the LGFU leverages high-level semantics to guide and filter shallow-level details, ensuring that change targets are well-represented regardless of their scale. Extensive experiments on three public datasets demonstrate that FSG-Net delivers SOTA performance, exhibiting superior trade-off between accuracy and efficiency. For future research, our efforts will be directed toward lightweight architectures and integrating multi-modal data or constructing knowledge graphs for rule-based constraints to further enhance the models capabilities in extreme and complex scenarios."
        },
        {
            "title": "REFERENCES",
            "content": "[1] D. Peng, X. Liu, Y. Zhang, H. Guan, Y. Li, and L. Bruzzone, Deep learning change detection techniques for optical remote sensing imagery: Status, perspectives and challenges, Int. J. Appl. Earth Observ. Geoinf., vol. 136, p. 104282, 2025. [2] M. Wang, L. Gao, L. Ren, X. Sun, and J. Chanussot, Hyperspectral simultaneous anomaly detection and denoising: Insights from integrative perspective, IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 2024. [3] X. Wen, H. Xie, H. Liu, and L. Yan, Accurate reconstruction of the LoD3 building model by integrating multi-source point clouds and oblique remote sensing imagery, ISPRS Int. J. Geo-Inf., vol. 8, no. 3, p. 135, 2019. [4] L. Pang, J. Yao, K. Li, and X. Cao, SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP, arXiv preprint arXiv:2501.16222, 2025. [5] N. Casagli, E. Intrieri, V. Tofani, G. Gigli, and F. Raspini, Landslide detection, monitoring and prediction with remote-sensing techniques, Nat. Rev. Earth Environ., vol. 4, no. 1, pp. 5164, 2023. [6] F. Bovolo and L. Bruzzone, theoretical framework for unsupervised change detection based on change vector analysis in the polar domain, IEEE Trans. Geosci. Remote Sens., vol. 45, no. 1, pp. 218236, 2006. [7] T. Bai, K. Sun, S. Deng, D. Li, W. Li, and Y. Chen, Multi-scale hierarchical sampling change detection using Random Forest for highresolution satellite imagery, Int. J. Remote Sens., vol. 39, no. 21, pp. 75237546, 2018. [8] J. Lin, G. Wang, D. Peng, and H. Guan, Edge-guided multi-scale foreground attention network for change detection in high resolution remote sensing images, Int. J. Appl. Earth Observ. Geoinf., vol. 133, p. 104070, 2024. [9] Q. Zhu, X. Guo, Z. Li, and D. Li, review of multi-class change detection for satellite remote sensing imagery, Geo-Spat. Inf. Sci., vol. 27, no. 1, pp. 115, 2024. [10] X. Ding, X. Zhang, J. Han, and G. Ding, Scaling up your kernels to 31x31: Revisiting large kernel design in cnns, in in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2022, pp. 11 96311 975. [11] F. Yu and V. Koltun, Multi-scale context aggregation by dilated convolutions, arXiv preprint arXiv:1511.07122, 2015. [12] X. Cai, Q. Lai, Y. Wang, W. Wang, Z. Sun, and Y. Yao, Poly kernel inception network for remote sensing detection, in in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2024, pp. 27 70627 716. [13] M. Hu, C. Wu, and L. Zhang, GlobalMind: Global multi-head interactive self-attention network for hyperspectral change detection, ISPRS J. Photogramm. Remote Sens., vol. 211, pp. 465483, 2024. [14] X. Zhang, S. Cheng, L. Wang, and H. Li, Asymmetric cross-attention hierarchical network based on CNN and transformer for bitemporal remote sensing images change detection, IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 115, 2023. [15] Q. Hou, D. Zhou, and J. Feng, Coordinate attention for efficient mobile network design, in in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2021, pp. 13 71313 722. [16] W. Liu, Y. Lin, W. Liu, Y. Yu, and J. Li, An attention-based multiscale transformer network for remote sensing image change detection, ISPRS J. Photogramm. Remote Sens., vol. 202, pp. 599609, 2023. [17] Y. Feng, J. Jiang, H. Xu, and J. Zheng, Change detection on remote sensing images using dual-branch multilevel intertemporal network, IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 115, 2023. [18] S. Fang, K. Li, and Z. Li, Changer: Feature interaction is what you need for change detection, IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 111, 2023. [19] D. Sun, J. Yao, W. Xue, C. Zhou, P. Ghamisi, and X. Cao, Mask Approximation Net: Novel Diffusion Model Approach for Remote Sensing Change Captioning, IEEE Trans. Geosci. Remote Sens., 2025. [20] Z. Li, Z. Zhang, M. Li, L. Zhang, X. Peng, R. He, and L. Shi, Dual Fine-Grained network with frequency Transformer for change detection on remote sensing images, Int. J. Appl. Earth Observ. Geoinf., vol. 136, p. 104393, 2025. [21] Y. Tang, S. Feng, C. Zhao, Y. Fan, Q. Shi, W. Li, and R. Tao, An object fine-grained change detection method based on frequency decoupling interaction for high-resolution remote sensing images, IEEE Trans. Geosci. Remote Sens., vol. 62, pp. 113, 2023. [22] S. Lee, J. Bae, and H. Y. Kim, Decompose, adjust, compose: Effective normalization by playing with frequency for domain generalization, in in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2023, pp. 11 77611 785. [23] B. Cao, Q. Wang, P. Zhu, Q. Hu, D. Ren, W. Zuo, and X. Gao, Multiview knowledge ensemble with frequency consistency for cross-domain face translation, IEEE Trans. Neural Netw. Learn. Syst., vol. 35, no. 7, pp. 97289742, 2023. [24] J. Huang, D. Guan, A. Xiao, and S. Lu, Fsdr: Frequency space domain randomization for domain generalization, in in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2021, pp. 68916902. [25] D. Xue, T. Lei, S. Yang, Z. Lv, T. Liu, Y. Jin, and A. K. Nandi, Triple change detection network via joint multifrequency and full-scale swintransformer for remote sensing images, IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 115, 2023. [26] F. Xiong, T. Li, Y. Yang, J. Zhou, J. Lu, and Y. Qian, Wavelet siamese network with semi-supervised domain adaptation for remote sensing image change detection, IEEE Trans. Geosci. Remote Sens., 2024. [27] Y. Chen, S. Feng, C. Zhao, N. Su, W. Li, R. Tao, and J. Ren, Highresolution remote sensing image change detection based on Fourier feature interaction and multi-scale perception, IEEE Trans. Geosci. Remote Sens., 2024. [28] J. Yao, B. Zhang, C. Li, D. Hong, and J. Chanussot, Extended vision transformer (ExViT) for land use and land cover classification: multimodal deep learning framework, IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 115, 2023. [29] C. Xu, Z. Ye, L. Mei, H. Yu, J. Liu, Y. Yalikun, S. Jin, S. Liu, W. Yang, and C. Lei, Hybrid attention-aware transformer network collaborative multiscale feature alignment for building change detection, IEEE Trans. Instrum. Meas., vol. 73, pp. 114, 2024. [30] X. Zhu, H. Hu, S. Lin, and J. Dai, Deformable convnets v2: More deformable, better results, in in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2019, pp. 93089316. [31] Z. Huang, Y. Wei, X. Wang, W. Liu, T. S. Huang, and H. Shi, Alignseg: Feature-aligned segmentation networks, IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 1, pp. 550557, 2021. [32] D. Wen, X. Huang, F. Bovolo, J. Li, X. Ke, A. Zhang, and J. A. Benediktsson, Change detection from very-high-spatial-resolution optical remote sensing images: Methods, applications, and future directions, IEEE Geosci. Remote Sens. Mag., vol. 9, no. 4, pp. 68101, 2021. [33] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2016, pp. 770778. [34] M. Wang, J. Zhang, G. Huang, L. Lu, and F. Hua, Change detection based on image standardization and improved residual network for single-polarization SAR images, IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., 2025. [35] C. Wu, L. Zhang, B. Du, H. Chen, J. Wang, and H. Zhong, UNetLike Remote Sensing Change Detection: review of current models and research directions, IEEE Geosci. Remote Sens. Mag., 2024. [36] M. Liu, Z. Chai, H. Deng, and R. Liu, CNN-transformer network with multiscale context aggregation for fine-grained cropland change 13 detection, IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., vol. 15, pp. 42974306, 2022. [37] R. C. Daudt, B. Le Saux, and A. Boulch, Fully convolutional siamese networks for change detection, in in Proc. IEEE Int. Conf. Image Process. (ICIP). IEEE, 2018, pp. 40634067. [38] Y. Liu, C. Pang, Z. Zhan, X. Zhang, and X. Yang, Building change detection for remote sensing images using dual-task constrained deep siamese convolutional network model, IEEE Geosci. Remote Sens. Lett., vol. 18, no. 5, pp. 811815, 2020. [39] S. Fang, K. Li, J. Shao, and Z. Li, SNUNet-CD: densely connected Siamese network for change detection of VHR images, IEEE Geosci. Remote Sens. Lett., vol. 19, pp. 15, 2021. [40] T. Bai, L. Wang, D. Yin, K. Sun, Y. Chen, W. Li, and D. Li, Deep learning for change detection in remote sensing: review, Geo-Spat. Inf. Sci., vol. 26, no. 3, pp. 262288, 2022. [41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Adv. Neural Inf. Process. Syst., vol. 30, 2017. [42] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [43] W. G. C. Bandara and V. M. Patel, transformer-based siamese network for change detection, in in Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS). IEEE, 2022, pp. 207210. [44] C. Zhang, L. Wang, S. Cheng, and Y. Li, SwinSUNet: Pure transformer network for remote sensing image change detection, IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 113, 2022. [45] H. Chen, Z. Qi, and Z. Shi, Remote sensing image change detection with transformers, IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 114, 2021. [46] F. Yang, M. Li, W. Shu, A. Qin, T. Song, C. Gao, and G.-S. Xia, ConvFormer-CD: Hybrid CNN-Transformer with Temporal Attention for Detecting Changes in Remote Sensing Imagery, IEEE Trans. Geosci. Remote Sens., 2025. [47] J. Wei, K. Sun, W. Li, W. Li, S. Gao, S. Miao, Q. Zhou, and J. Liu, Robust change detection for remote sensing images based on temporospatial interactive attention module, Int. J. Appl. Earth Observ. Geoinf., vol. 128, p. 103767, 2024. [48] W. Yu, L. Zhuo, and J. Li, GCFormer: Global context-aware transformer for remote sensing image change detection, IEEE Trans. Geosci. Remote Sens., vol. 62, pp. 112, 2024. [49] Y. Wang, W. Huang, F. Sun, T. Xu, Y. Rong, and J. Huang, Deep multimodal fusion by channel exchanging, Adv. Neural Inf. Process. Syst., vol. 33, pp. 48354845, 2020. [50] C. Zhao, Y. Tang, S. Feng, Y. Fan, W. Li, R. Tao, and L. Zhang, Highresolution remote sensing bitemporal image change detection based on feature interaction and multitask learning, IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 114, 2023. [51] Y. Yang and S. Soatto, Fda: Fourier domain adaptation for semantic segmentation, in in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020, pp. 40854095. [52] D. Quan, H. Wei, S. Wang, Y. Li, J. Chanussot, Y. Guo, B. Hou, and L. Jiao, Efficient and robust: cross-modal registration deep wavelet learning method for remote sensing images, IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens., vol. 16, pp. 47394754, 2023. [53] J. Hu, L. Shen, and G. Sun, Squeeze-and-excitation networks, in in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018, pp. 71327141. [54] M. A. Lebedev, Y. V. Vizilter, O. V. Vygolov, V. A. Knyaz, and A. Y. Rubis, Change detection in remote sensing images using conditional adversarial networks, Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci., vol. 42, pp. 565571, 2018. [55] M. Wang, X. Zhao, and L. Ren, Enhancing cloud-removed regions in multispectral optical images using sar edge features, IEEE Geosci. Remote Sens. Lett., 2025. [56] D. Peng, L. Bruzzone, Y. Zhang, H. Guan, H. Ding, and X. Huang, SemiCDNet: semisupervised convolutional neural network for change detection in high resolution remote-sensing images, IEEE Trans. Geosci. Remote Sens., vol. 59, no. 7, pp. 58915906, 2020. [57] H. Chen and Z. Shi, spatial-temporal attention-based method and new dataset for remote sensing image change detection, Remote Sens., vol. 12, no. 10, p. 1662, 2020."
        }
    ],
    "affiliations": [
        "Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100094, China",
        "College of Land Science and Technology, China Agricultural University, Beijing 100193, China",
        "Faculty of Geosciences and Engineering, Southwest Jiaotong University, Chengdu 60031, China",
        "Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, 09599 Freiberg, Germany",
        "Lancaster Environment Centre, Lancaster University, LA1 4YR Lancaster, U.K."
    ]
}