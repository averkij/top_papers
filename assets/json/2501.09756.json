{
    "paper_title": "SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces",
    "authors": [
        "Sumit Chaturvedi",
        "Mengwei Ren",
        "Yannick Hold-Geoffroy",
        "Jingyuan Liu",
        "Julie Dorsey",
        "Zhixin Shu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SynthLight, a diffusion model for portrait relighting. Our approach frames image relighting as a re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions. Using a physically-based rendering engine, we synthesize a dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting. We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details. Our method generalizes to diverse real photographs and produces realistic illumination effects, including specular highlights and cast shadows, while preserving the subject's identity. Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods. Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects. Project Page: \\url{https://vrroom.github.io/synthlight/}"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 6 5 7 9 0 . 1 0 5 2 : r SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces Sumit Chaturvedi1 Mengwei Ren2 Yannick Hold-Geoffroy2 Jingyuan Liu2 Julie Dorsey1 1Yale University Zhixin Shu2 2Adobe Research Figure 1. SynthLight performs relighting on portraits using an environment map lighting. By learning to re-render synthetic human faces, our diffusion model produces realistic illumination effects on real portrait photographs, including distinct cast shadows on the neck and natural specular highlights on the skin. Despite being trained exclusively on synthetic headshot images for relighting, the model demonstrates remarkable generalization to diverse scenarios, successfully handling half-body portraits and even full-body figurines."
        },
        {
            "title": "Abstract",
            "content": "We introduce SynthLight, diffusion model for portrait relighting. Our approach frames image relighting as re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions. Using physically-based rendering engine, we synthesize dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting. We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details. Our method generalizes to diverse real photographs and produces real- *Work done as an intern at Adobe Research. Corresponding author. istic illumination effects, including specular highlights and cast shadows, while preserving the subjects identity. Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods. Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects. Project Page: https://vrroom.github.io/synthlight/ 1. Introduction Lighting is fundamental to portrait photography, yet manipulating it after capture remains challenging. Recent advances in generative imaging models have demonstrated promising capabilities for controlling lighting in existing images [15, 19, 33, 57, 59]. However, these approaches typically require labeled training data. For portrait relighting specifically, the most effective results have come from training on Light Stage dataportraits rendered with linear combinations of one-light-at-a-time (OLAT) captures. While powerful, Light Stage setups are constrained by physical limitations in light source density and require specialized artificial lighting equipment. In contrast, 3D workflows in VFX and gaming have long treated lighting as relatively straightforward endeavor through modern physically based rendering engines, where light source control is nearly arbitrary. To relight rendering, artists simply adjust the lighting configurations and re-render the scene. Given scene and lighting L1, we denote the rendering as I1 = fr(S, L1). The inverse graphics problem aims to find from I1: = finv(I1, L1) with known or unknown lighting. To relight rendering I1 to I2 under lighting L2, one aims to compute: I2 = fr(S, L2). Given only I1, relighting procedure seeks: I2 = fr(finv(I1), L2) = fre(I1, L2), where fre is the relighting/re-rendering function. Previous approaches [22, 28, 56] have tackled this problem through inverse graphics, either explicitly or implicitly, by estimating lighting-invariant intrinsic scene representations such as depth, surface normals, and albedo. This imposes limitations on subsequent rendering functions and often fails to capture complex illumination effects like inter-reflections, occlusion shadows, and subsurface scattering. In this paper, we propose bypassing inverse rendering entirely by learning the relighting function using physically based 3D renderings of human heads. Specifically, we render pairs of portrait images using Blender (Cycles) (I1, L1) and (I2, L2) and train diffusion model to directly learn to re-render I2 from I1 and L2. However, this approach introduces an inevitable domain gap between simulated 3D renders and real photographs. To address this challenge, we leverage latent diffusion model pretrained on vast internet images for text-to-image generation. We propose to finetune the network with our face renderings and introduce simple yet effective training and testing schemes to narrow the gap between training data and in-the-wild images. During training, we propose multitask training that incorporates in-the-wild images without ground truth relighting information. This allows the model to learn relighting from our synthetic dataset while maintaining knowledge of the real image domain, preventing distributional drift. We further observe that input portraits contain rich textural information. Leveraging the flexibility of diffusion model inference, we design an inference time adaptation scheme that effectively preserves input portrait details in the relit result. We evaluate our methods on in-the-wild portrait images, demonstrating highly detailed illumination effects that accurately capture interactions between the portrait scene and lighting. Our results produce realistic cast shadows and specular highlights on the skin. For the first time, we demonstrate an end-to-end system capable of non-trivial lighting effects including catch lights in eyes, subsurface scattering in ears, and inter-reflections with clothing. Notably, despite training only on simple headshot renderings of 3D faces without accessories, facial hair, or hats, our network generalizes effectively to complex portrait images, including half-body shots and multi-person photographs. We quantitatively evaluate our method on test set of our synthetic faces dataset as well as on Light Stage OLAT dataset. Despite using no Light Stage data for training, our method achieves comparable or superior results to state-ofthe-art portrait relighting methods trained on OLAT data. User studies show that our results are preferred across all evaluated aspects, including perceptual lighting accuracy, identity preservation, and overall image quality. We summarize our contributions as follows: 1. We propose modeling portrait relighting as task of learning to re-render portrait scene in 3D. Using physically based renderings of human heads under varying lighting conditions, we train diffusion model to learn pixel transformations conditioned on lighting. 2. We introduce two techniques enabling synthetic data learning while minimizing domain gap with real images, through the use of training-time multi-task strategy that incorporates real images through text-to-image task, and an inference-time approach based on classifier-free guidance that preserves portrait details in the relit result. 3. Through extensive qualitative and quantitative evaluations, we demonstrate state-of-the-art portrait relighting results, achieving high-quality lighting effects previously unattainable by existing methods. 2. Related Work 2.1. Portrait Relighting Portrait relighting has been explored in both 2D [19, 22, [3, 6, 28, 29, 33, 43, 46, 52, 57, 59] and 3D domains 32, 48, 49, 51, 61], with 2D image-based approaches being more relevant to our work. Since 2D portrait relighting is under-constrained, various priors have been proposed, such as morphable models [4] as 3D face priors in [42], explicit inverse rendering in [2, 40], and style transfer approach for relighting in [41]. Recently, deep learning methods [27, 46] trained on light stage data [9] have driven the state-of-the-art for relighting, with [22, 28] demonstrating widely adopted physicsguided architecture for relighting based on image decomposition into intrinsics such as albedo, normals, diffuse, and specular reflectance maps, conditioned on an HDR environment map lighting representation [8]. However, this formulation presents two main shortcomings. First, the rendering model assumes BRDF-based reflectance model [7, 31], where light is reflected directly from the surface point of incidence, thus neglecting other modes of light transport such as subsurface scattering, which are significant in certain types of human skin (e.g., fair skin) [12, 23, 26]. Addition2 ally, albedo estimation becomes challenging in the presence of face accessories, inter-reflections and face paint [22, 56]. Second, light stage setups inherently limit the types of lighting that can be captured due to restricted light intensity [46] and lighting resolution [47], hindering the ability to learn complex lighting effects such as specular reflections and subsurface scattering. Motivated by these constraints, we employ diffusion models to learn face relighting, without assuming any appearance model, from synthetic dataset rendered with physically based renderer that provides input and relit training pairs for supervision. This enables our method to synthesize interesting illumination effects for human portraits such as hard cast shadows, subsurface scattering and inter-reflections. 2.2. Diffusion Models for Relighting Diffusion models [1, 10, 20, 21, 34, 36, 37, 44, 45] have become the standard framework for tasks ranging from textto-image generation to image-to-image translation and appearance editing. Their ability to scale well with large datasets, coupled with pretrained weights [34] that can be readily adapted to new domains [18, 58], makes them especially suited for these applications. They also offer flexible inference mechanisms, where improved sampling procedures can significantly boost image quality [16, 24]. Several recent works employ diffusion models specifically for relighting. DiLightNet [57] demonstrates finegrained control of object lighting by incorporating radiance hints. However, their multi-step pipeline, depends on scene reconstruction [50], which is error-prone. Similarly, Neural Gaffer [19] focuses on object relighting, leveraging HDR environment maps. For human portrait relighting, Relightful Harmonization [33] and IC-Light [59] train on highquality datasets (including light stage captures, synthetic Objaverse renders, and composited shadow materials) to synthesize background-harmonized portraits. Both methods rely on the background as lighting condition. In contrast, our approach directly tackles portrait-based relighting, using diffusion model that learns to re-render synthetic faces. By starting from pretrained model, and through our multi-task training strategy, we retain rich facial priors, while classifier-free guidance [16] on the input portrait further improves the preservation of texture and detail in the final relit output. 2.3. Domain Adaptation Naively training on synthetic data often creates domain gap for in-the-wild portraits, causing poor identity preservation and reduced photo-realism. Prior diffusion-based domain adaptation approaches [13, 18, 35, 55, 58] mainly target style transfer or focused editing, not relighting. [15] propose training personalized diffusion model per subject, preserving identity but require light-stage capture and dedicated training for each subject. Other methods leverage real data to mitigate the synthetic-to-real gap: SwitchLight [22] pre-trains with masked-autoencoder [14] on real images before training on light-stage data, learning visual features (e.g. structure, color, texture) that are essential for relighting; Relightful Harmonization [33] bootstraps relighting model learned from light-stage data to pseudo-label in-the-wild images, subsequently finetuning on these pseudo-labels for improved photorealism; ICLight [59] uses large-scale data augmentation; and Lumos [56] finetunes its albedo-prediction branch on real images, though its decomposition approach can fail with face paint, accessories, or strong shadows. We propose multi-task training scheme that unifies text-to-image and relighting tasks, enabling the training of our diffusion model with real images along with our synthetic dataset. In addition, our inference scheme based on classifier-free guidance helps preserve fine details from the input portrait. Our user study shows that the resulting relit portraits exhibit superior visual quality, identity, and lighting compared to existing methods. 3. Method Given portrait image captured under unknown illumination conditions, our goal is to synthesize relit version IR under target lighting environment specified by panoramic environment map E. The relit portrait IR should simultaneously: (1) preserve the subjects facial identity and characteristics from the original image I; (2) accurately reflect the illumination effects defined by the target environment map and (3) maintain photorealism in the final rendering. We first simulate this re-rendering to build synthetic dataset for human portraits using Blender. 3.1. Synthetic Data for Relighting Figure 2. Synthetic Faces: Subjects are rendered under various lighting conditions (details in Sec. 3.1). We show two examples, where each pair consists of subject rendered using two different environment maps. The network is trained to re-render synthetic faces by transforming subject rendered with one environment map into its counterpart rendered with the other environment map. We build 3D human portrait generation pipeline similar to [53]. Our system begins with collection of high-quality, artist-created 3D head meshes, which we enhance by incorporating detailed facial components, including eyes, teeth, gums, and hair. We then augment these base 3 the-wild images, resulting in degraded output quality. For instance, when applied to real-world images, the model fails to reproduce critical details, such as textures in clothing, jewelry, and accessories, which are absent in the synthetic data distribution (e.g., as seen in the Base result in Fig. 9). To address this, we propose multitask training strategy to mitigate potential model distribution drifting to synthetic renderings. Similar techniques have been applied in the context of inpainting [54] to combat the lack of diversity in training data. Specifically, we incorporate text-to-portrait generation task, which constraints the diffusion model to produce realistic portrait image given an input prompt. This task is trained alongside the original relighting task, and this helps to improve the photorealism and generalization of the trained model. Since both tasks share the same network architecture, we simply replace the image and LDR inputs with two black images, as illustrated in Fig. 3. To obtain training samples for the text-to-portrait, we curate subset of human portrait images from the LAION [39] dataset by sampling the images filtered by face detector. Details on detection and filtering are provided in the supplementary material (see Appendix B). During training, we empirically set the sampling ratios of the synthetic dataset versus the real dataset as 0.7 and 0.3, respectively. We observe significant benefits from incorporating the real images during training in improving identity preservation and photorealism. This echoes the findings in [33], where bootstrapped dataset helps generalize of image harmonization, emphasizing the benefits of data diversity. models through rigging for pose variation and blendshape deformation for diverse facial expressions. To render realistic appearances, we incorporated set of high quality PBR texture maps, including albedo, normal, roughness, specular, and subsurface scattering maps. We combine the head with random clothing meshes to build portrait scene. The system is built with Blender and the images are rendered with the Cycles renderer. To train our networks, we render images (samples shown in Fig. 2) at 512512 resolution from 350 subjects, each with roughly 10 varied appearance samples, including different hairstyles, skin tones, expressions, clothes, poses, etc. We render each sample with 10 random HDR environment maps, each rotated 36 times evenly with random initial rotation. In total, the dataset contains roughly 1.26 million images. See Fig. 24 in the supplemental material for more examples from the dataset. 3.2. Modeling Relighting with Diffusion Model We build on top of Stable Diffusion [34], text-to-image foundation model pretrained with vast internet data. As shown in Fig. 3, we incorporate the input portrait I, along with the target environment map to the input of the network backbone, by expanding the number of channels in the first convolutional layer of the Unet as per [36]. 1 EHDR To generate training samples (I, E, T, IR), where is text prompt, we render portrait images from subject with different HDR maps EHDR to obtain portraits . We use an off-the-shelf image captioning model [25] to caption these images. Training samples are constructed by sampling two indices i, {1 n} and then using them to select input portrait, environment map, text prompt and target portrait as (I ). In the following, we drop the superscript for the subject to simplify notation. We use the sample to supervise our diffusion model in the following manner. First, we convert the HDR environment map EHDR into LDR ELDR by tone-mapping similar to [19]. The LDR environment map along with the input and target portraits are encoded using the encoder Enc of Stable Diffusions VAE, i.e., ˆIi = Enc(Ii), = Enc(ELDR ), ˆIj = Enc(Ij). , EHDR , , ˆELDR j Following the DDPM formulation [17], we randomly sample Gaussian noise ϵ and diffusion timestep to add noise to the relit image latent ˆIj to obtain the noised latent ˆI , ˆI j. We concatenate ˆIi, along the channel axis and feed it to the Unet, following [19]. The Unet ϵθ is trained with the DDPM objective: ˆELDR min θ ExEnc(IR),t,ϵN (0,I)ϵθ(xt, I, E, ) ϵ (1) 3.3. Multitask Training Training or fine-tuning diffusion model on synthetic dataset creates substantial domain gap when applied to in4 Figure 3. Training pipeline of SynthLight. We first enable the relighting modeling by training the diffusion backbone with synthetic relighting tuples (Task 1, top row), detailed in Sec. 3.2. To further alleviate the domain gap between synthetic and real image domain, we include joint training of the text-to-image task (Task 2, bottom row), detailed in Sec. 3.3. Our model is based on LDM [34] and is composed of VAE and UNet. For simplicity, VAE is omitted in the diagram. Figure 4. We employ the image-conditioning classifier-free guidance during inference to proportionally balance between identity preservation, and relighting effects. The final score estimate is computed as per Eq. (2). 3.4. Inference Time Adaptation We further employ simple yet effective inference time adaptation scheme that proportionally balances between the identity preservation of the input portrait and the relighting Inspired by the dual-conditioning classifier-free strength. guidance [16] proposed in InstructPix2Pix [5], we define an analogous concept in our inference. As illustrated in Fig. 4, at each step of the diffusion inference, the diffusion score is composition of scores from both image-conditional and unconditional output. Specifically, for unconditional inference, we drop the input image while keeping the LDR and text-prompt conditioning identical. Formally, we apply the following score estimate at particular timestep t: ϵt = ϵθ(xt+1, ϕ, E, ϕ) + λT (ϵθ(xt+1, I, E, ) ϵθ(xt+1, I, E, ϕ)) + λI (ϵθ(xt+1, I, E, ϕ) ϵθ(xt+1, ϕ, E, ϕ)) . (2) Here, λT and λI are the guidance parameters, where λT is inherited from the original definition of CFG, which specifies the how much the model respects to the text prompts, while λI specifies the strength of the input portrait guidance. With this score estimate, we use DDIM [45] to obtain the latent at current timestep xt = DDIM(xt+1, ϵt). We empirically find that using guidance value of λI [2, 3] for the input portrait helps achieve balance between the details and identity preservation while performing reasonable relighting. In Fig. 5, we illustrate the effects of varying λI . Smaller values provide the strongest relighting effect while sacrificing some visual quality and losing the facial details of the input. Large values provide much better identity preservation but weaken the relighting effects where lighting information, such as shadows, leaks from the input into the output. Figure 5. Effect of input portrait guidance parameter λI : We show (a) the input portrait, (b) the lighting condition and reference image rendered in Blender with the same lighting, and (c) outputs with varying λI . (d) highlights that λI = 1, equivalent to removing inference-time adaptation, alters the eye shape (in red rectangle). (e) shows that higher λI introduces undesired lighting artifacts, such as shadow artifacts from the input portrait (in yellow rectangle). 4. Experiments 4.1. Setup and Metrics We create three test sets for evaluating our method: (a) 300 Light Stage rendered relighting pairs, (b) held out subset of our synthetic faces dataset consisting of 500 images, (c) in-the-wild portraits for qualitative evaluation of visual quality. For test sets (a) and (b), we use standard quantitative metrics such as SSIM, PSNR, LPIPS [60] to evaluate image fidelity and face embedding distance such as FaceNet [38] for evaluating identity preservation. We train on the entire synthetic dataset but withhold 20% of the environment maps to create the Light Stage test set. We also hold out 10% of the subject identities and 10% of the environment maps for the synthetic test set, ensuring they remain unseen during training. 4.2. Implementation details We implement our model in PyTorch [30] using 32 40GB A100 GPUs. We use batch size of 192, learning rate of 105, and the Adam [11] optimizer. We train our model (and ablations) for 40K steps, which takes around 1 day. We initialized from the IC-Light [59] checkpoint for background conditioned image relighting, which is fine-tuned based on Stable Diffusion 1.5 [34]. We chose this particular checkpoint because we found it to be beneficial for learning our environment map based relighting model compare to text-to-image checkpoint. We show more analysis and comparisons of this choice in supplemental material (see Fig. 17 and Tab. 4). 5 (a) Our method demonstrates the ability to relight subjects effectively in both outdoor (left) and indoor (right) settings. In outdoor scenarios, strong cast shadows are produced due to self-occlusion from facial features and glasses (see inset). For indoor scenes, our method handles complex lighting conditions, such as casting neon lights on the input portrait. (b) Our method captures interesting lighting effects for portraits, synthesizing fine details like catch light in the eye for realistic relighting (left, see inset) and subsurface scattering in the ear under strong backlight conditions, such as sunlight (right, see inset). (c) Our method enables studio-style lighting for portraits, creating dramatic effects in studio-like environments (left). Using hand-designed environment maps, we relight with two presets (right): Backlight, which uses light behind the subject to define edges and produce distinctive rim effect (see inset); and Rembrandt, where light comes from an angle, illuminating one portion of the face while casting the other in shadow to create depth and contrast. The Rembrandt image also highlights inter-reflections from clothing (rightmost, see inset). (d) While trained only on synthetic dataset, our method generalizes to unseen image categories such as clown (left), photograph of two people (middle), and teddy-bear (right). Figure 6. Real-world results showcasing our methods ability to handle diverse lighting scenarios. Each example includes the input portrait (left), the environment map used for relighting (top right), and the relit output (bottom right). The subfigures highlight: (a) relighting under indoor and outdoor environments, (b) capturing interesting lighting effects such as catch lights in eyes and sub-surface scattering on ears, (c) studio-style lighting setups, and (d) generalization across various challenging scenarios. 4.3. Evaluation Results We compare our method against state-of-the-art methods for portrait harmonization [59], portrait relighting [22] and object relighting [19, 57] on both the synthetic and the light stage test set quantitatively (see Tab. 1) and qualitatively (see Fig. 8 and Fig. 7). Quantitative evaluation shows that our method outperforms baselines on the synthetic test set and performs comparably to state-of-the-art portrait relighting methods such as SwitchLight, on the Test Light Stage dataset. Even though our results do not always attain the highest PSNR, they display better visual relighting quality than baselines. We further conduct user study (see Tab. 2) to quantify 6 DiLightNet IC-Light Neural Gaffer Total Relighting SwitchLight Ours In-the-wild portrait results: We display the input portrait, environment map, reference image, rendered in Blender, and Figure 7. baseline comparisons. DiLightNet [57] shows artifacts from 3D reconstruction failures central to its pipeline. Neural Gaffer [19] generates inaccurate shadow contours on relit faces since it isnt trained on human portraits. IC-Light [59] struggles with relighting due to its choice of background as the lighting condition. Total Relighting and SwitchLight [22, 28], trained on light stage data, produce soft shadows even under strong sunlight and alter skin tones. In contrast, our method achieves superior relighting while preserving subject identity. Inputs DiLightNet IC-Light Neural Gaffer Total Relighting SwitchLight Ours GT Figure 8. Light Stage test results: We compare our method against baselines on the input portrait (bottom left) from the Light Stage test set relit with target environment map (top left). human perceptual preference for relighting. For each pair (our method vs. baseline), participants are asked three questions: (1) which method has better lighting (2) which has better image quality (3) which better preserves idenTest Synthetic Test Light Stage Method LPIPS SSIM PSNR FaceNet LPIPS SSIM PSNR FaceNet Ours SwitchLight IC-Light DiLightNet Neural Gaffer 0.063 0.088 0.108 0.128 0.102 0.945 0.911 0.874 0.860 0.900 29.572 21.432 20.283 22.991 25.327 0.165 0.198 0.284 0.333 0.357 0.165 0.141 0.172 0.245 0.196 0.813 0.853 0.789 0.703 0. 19.698 20.299 17.440 16.619 19.311 0.173 0.152 0.195 0.576 0.247 Table 1. Comparisons: We compare against baselines on held-out set of our synthetic dataset and data rendered through Light Stage. While trained only on synthetic data, our model performs comparably to SwitchLight, commercial relighting method trained with Light Stage data. Base Base + Multi-Task Base + Inference Adaptation Ours + Light Stage Ours Figure 9. Ablations: We display the input portrait with its lighting condition and reference image rendered in Blender (left). The Base configuration fails to reproduce the portraits textures and alters its identity. In contrast, Base + Multi-Task recovers some details, such as realistic skin tone (bottom row, yellow rectangle). The Base + Inference Adaptation configuration struggles with unseen textures and accessories (e.g., the cigarette, top row, red rectangle) and produces unnatural textures for sleeveless skin (bottom row, yellow rectangle). Meanwhile, Ours + Light Stage enhances details but inherits biases from Light Stage data and cannot remove strong shadows (neck region, bottom row, red rectangle). Finally, Ours achieves plausible lighting, harmonizes well with the background, and preserves key details from the input portrait. IC-Light SwitchLight Neural Gaffer Lighting Quality Identity 0.92 0.57 0.52 0.56 0.64 0.70 0.65 0.73 0.65 Table 2. User Study: Preference rates indicate how often our method was preferred over baselines. For example, rate of 0.92 under Lighting means our method was preferred 92% of the time over IC-Light. Based on 482 responses from 20 participants, our method consistently outperforms baselines in lighting, image quality, and subject identity, since all preference rates exceed 0.5. This highlights superior image quality over relighting methods [19, 22] and better lighting over harmonization methods [59]. tity. All questions are presented as 2-alternative forced choice (2AFC). We collect 482 responses from 20 participants with diverse backgrounds, ranging from design to computer science. Results show that our methods outperforms baselines in perceived image lighting, quality, and identity preservation. Refer to the supplementary material for screenshots, Fig. 22 and Fig. 23, showcasing the precise format of our user study. 4.4. Ablations We conduct an ablation study to evaluate the contribution of our two key methods for domain adaptation: multi-task training (See Sec. 3.3) and inference-time adaptation (See Sec. 3.4). We start with Base configuration that excludes both multi-task training and inference time adaptation. Next, we examine the individual impact of each component by separately adding multi-task training, denoted as Base + MultiTask, and inference time adaptation, denoted as Base + In8 Test Synthetic Test Light Stage Method LPIPS SSIM PSNR FaceNet LPIPS SSIM PSNR FaceNet Base Base + Multi-Task Base + Inference Adaptation Ours Ours + Light Stage 0.066 0.066 0.062 0.063 0. 0.942 0.942 0.946 0.945 0.942 29.131 29.049 29.638 29.572 29.126 0.193 0.196 0.163 0.165 0. 0.210 0.186 0.178 0.165 0.156 0.790 0.797 0.810 0.813 0.822 18.919 19.184 19.484 19.698 20. 0.295 0.242 0.179 0.173 0.149 Table 3. Ablations highlight the contributions of each component i.e. Multi-Task training and Inference-time Adaptation (Sec. 3.3 and Sec. 3.4 respectively). Adding Light Stage data during training improves performance on Light Stage Test set, and qualitatively improves details but brings lighting biases (See Fig. 9). input portrait but still lacks photo-realism. Ours + Light Stage addresses identity and texture issues but inherits lighting biases from the Light Stage dataset. For example, under strong sunlight, it yields oversaturated images (see Fig. 20, in the supplementary material). Similar artifacts appear in other methods (e.g., SwitchLight) that are trained on Light Stage data. It also struggles to remove strong shadows, which are rarely present in Light Stage captures. Finally, Ours, generates images with plausible lighting, that are well harmonized with background and preserve important details from the input portrait. These findings are corroborated by our quantitative evaluation in Sec. 4.3. 4.5. Environment map better than background We train two variants of our model, one using background and the other using an environment map as lighting condition. We observe that while in many cases, the backgroundconditioned model produces plausible lighting and appears well harmonized with the background, when we continuously rotate the environment map, lighting inconsistencies appear. See Fig. 10 for lighting inaccuracies in background-conditioned method. Despite these, leveraging our synthetic dataset makes our background-conditioned model generate plausible self-occlusions, whereas harmonization methods such as [59] fail in this use case. 5. Limitations & Discussion Despite the advances proposed by our method both in terms of simplicity and image quality, it bears some limitations. In particular, our rendering pipeline could achieve higher level of realism if we specialized it for rendering humans. Of note, it does not model unseen occluders casting shadows on the subjects face, accessories such as hats, glasses, or even facial hair, which limits the diversity of lighting our method saw during training. Despite this, our method achieves great generalization capabilities. Furthermore, user editing of the light is cumbersome in the current representation; we could improve this aspect by proposing parametric representation of the light, such as 3D point Figure 10. Background vs. Environment Map as Lighting Conditions: The background provides limited lighting cues, leading the background-conditioned model to produce inaccurate lighting (note the wrong lighting direction in (1)-(c)). Even so, by utilizing our Synthetic Faces dataset, the background-conditioned model is able to generate plausible lighting, characterized by strong cast shadows, whereas harmonization methods such as IC-Light [59] fall short. See Fig. 7, Row 3 for the input portrait. ference Adaptation. Our full configuration, combining both techniques is referred to as Ours. Finally, we explore the role of Light Stage data, by adding fraction of it to each training batch, denoted as Ours + Light Stage. Please refer to the supplementary material, Appendix A, for more details on the Light Stage data. Fig. 9 shows the effect of each configuration. Base loses important details from the input and fails to produce textures in clothing or accessories. Base + Multi-Task shows partial detail recovery, and Base + Inference Adaptation enhances finer details by leveraging information present in the lights or spherical Gaussians, that is easier to understand and edit for users. Additional qualitative examples illustrating the limitations of our method are provided in the supplementary material, see Fig. 25. 6. Conclusion We present SynthLight, Portrait Relighting Diffusion that relights in-the-wild images while garnering model It underlighting supervision only from synthetic data. scores the potential of using synthetic data to achieve plausible portrait relighting, enabling interesting lighting effects such as strong cast shadows, catch light in the eyes, and inter-reflections."
        },
        {
            "title": "Acknowledgement",
            "content": "We thank Weijie Lyu, Ziwen Chen, Haian Jin, Vikas Thamizharasan, Natalia Pacheco-Tallaj, Ryusuke Sugimoto and Christophe Bolduc for their insightful discussions and the many participants in our user study. We also thank Kalyan Sunkavalli and Nathan Carr for their support."
        },
        {
            "title": "References",
            "content": "[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 3 [2] Jonathan Barron and Jitendra Malik. Shape, illumination, and reflectance from shading. IEEE transactions on pattern analysis and machine intelligence, 37(8):16701687, 2014. 2 [3] Sai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon, Shih-En Wei, Kevyn Mcphail, Ravi Ramamoorthi, Yaser Sheikh, and Jason Saragih. Deep relightable appearance models for animatable faces. ACM Transactions on Graphics (ToG), 40(4):115, 2021. 2 [4] Volker Blanz and Thomas Vetter. morphable model for the synthesis of 3d faces. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 157164. 2023. 2 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 5 [6] Ziqi Cai, Kaiwen Jiang, Shu-Yu Chen, Yu-Kun Lai, Hongbo Fu, Boxin Shi, and Lin Gao. Real-time 3d-aware portrait video relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62216231, 2024. [7] Robert Cook and Kenneth E. Torrance. reflectance model for computer graphics. ACM Transactions on Graphics (ToG), 1(1):724, 1982. 2 [8] Paul Debevec. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. In Acm siggraph 2008 classes, pages 110. 2008. 2 [9] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar. Acquiring the In Proceedings of the reflectance field of human face. 27th annual conference on Computer graphics and interactive techniques, pages 145156, 2000. 2 [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 3 [11] Kingma Diederik. Adam: method for stochastic optimization. (No Title), 2014. [12] Craig Donner and Henrik Wann Jensen. spectral bssrdf for shading human skin. Rendering techniques, 2006:409418, 2006. 2 [13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 3 [15] Mingming He, Pascal Clausen, Ahmet Levent Tasel, Li Ma, Oliver Pilarski, Wenqi Xian, Laszlo Rikker, Xueming Yu, Ryan Burgert, Ning Yu, et al. Diffrelight: DiffusionarXiv preprint based facial performance relighting. arXiv:2410.08188, 2024. 1, 3 [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3, 5 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 4 [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [19] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah Snavely. Neural gaffer: Relighting any object via diffusion, 2024. 1, 2, 3, 4, 6, 7, 8 [20] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 3 [21] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the In Proceedings of training dynamics of diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. 3 [22] Hoon Kim, Minje Jang, Wonjun Yoon, Jisoo Lee, Donghyun Na, and Sanghyun Woo. Switchlight: Co-design of physicsdriven architecture and pre-training framework for human 10 In Proceedings of the IEEE/CVF Conportrait relighting. ference on Computer Vision and Pattern Recognition, pages 2509625106, 2024. 2, 3, 6, 7, [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 3, 4, 5 [23] Theodore Kim, Holly Rushmeier, Julie Dorsey, Derek Nowrouzezahrai, Raqi Syed, Wojciech Jarosz, and AM Darke. Countering racial bias in computer graphics research. In ACM SIGGRAPH 2022 Talks, pages 12. 2022. 2 [24] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. 3 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 4 [26] Tomohiro Mashita, Yasuhiro Mukaigawa, and Yasushi Yagi. Measuring and modeling of multi-layered subsurface scatIn Virtual and Mixed Reality-New tering for human skin. Trends: International Conference, Virtual and Mixed Reality 2011, Held as Part of HCI International 2011, Orlando, FL, USA, July 9-14, 2011, Proceedings, Part 4, pages 335344. Springer, 2011. 2 [27] Thomas Nestmeyer, Jean-Francois Lalonde, Iain Matthews, and Andreas Lehrmann. Learning physics-guided face reIn Proceedings of the lighting under directional light. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51245133, 2020. 2 [28] Rohit Pandey, Sergio Orts-Escolano, Chloe Legendre, Christian Haene, Sofien Bouaziz, Christoph Rhemann, Paul Debevec, and Sean Ryan Fanello. Total relighting: learning to relight portraits for background replacement. ACM Trans. Graph., 40(4):431, 2021. 2, [29] Sylvain Paris, Francois Sillion, and Long Quan. Lightweight face relighting. In 11th Pacific Conference onComputer Graphics and Applications, 2003. Proceedings., pages 4150. IEEE, 2003. 2 [30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 5 [31] Bui Tuong Phong. Illumination for computer generated pictures. In Seminal graphics: pioneering efforts that shaped the field, pages 95101. 1998. 2 [32] Pramod Rao, Gereon Fox, Abhimitra Meka, Mallikarjun BR, Fangneng Zhan, Tim Weyrich, Bernd Bickel, Hanspeter Pfister, Wojciech Matusik, Mohamed Elgharib, et al. Lite2relight: 3d-aware single image portrait relightIn ACM SIGGRAPH 2024 Conference Papers, pages ing. 112, 2024. 2 [33] Mengwei Ren, Wei Xiong, Jae Shin Yoon, Zhixin Shu, Jianming Zhang, HyunJoon Jung, Guido Gerig, and He Zhang. Relightful harmonization: Lighting-aware portrait background replacement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64526462, 2024. 1, 2, 3, 4, [35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 [36] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 3, 4 Image-to-image diffusion models. [37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [38] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815823, 2015. 5 [39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 4 [40] Soumyadip Sengupta, Angjoo Kanazawa, Carlos Castillo, and David Jacobs. Sfsnet: Learning shape, reflectance and illuminance of facesin the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 62966305, 2018. 2 [41] YiChang Shih, Sylvain Paris, Connelly Barnes, William Freeman, and Fredo Durand. Style transfer for headshot portraits. 2014. [42] Zhixin Shu, Sunil Hadap, Eli Shechtman, Kalyan Sunkavalli, Sylvain Paris, and Dimitris Samaras. Portrait lighting transfer using mass transport approach. ACM Transactions on Graphics (TOG), 36(4):1, 2017. 2 [43] Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli, Eli Shechtman, and Dimitris Samaras. Neural face editing In Proceedings of the with intrinsic image disentangling. IEEE conference on computer vision and pattern recognition, pages 55415550, 2017. 2 [44] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 3 [45] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3, 5 and Stefano Ermon. arXiv preprint [46] Tiancheng Sun, Jonathan Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay 11 Busch, Paul Debevec, and Ravi Ramamoorthi. Single image portrait relighting. ACM Transactions on Graphics (TOG), 38(4):112, 2019. 2, 3 Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 3 [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Ic-light github page, 2024. 1, 2, 3, 5, 6, 7, 8, [60] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [61] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David Jacobs. Deep single-image portrait relighting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 71947202, 2019. 2 [47] Tiancheng Sun, Zexiang Xu, Xiuming Zhang, Sean Fanello, Christoph Rhemann, Paul Debevec, Yun-Ta Tsai, Jonathan Light stage superBarron, and Ravi Ramamoorthi. ACM resolution: continuous high-frequency relighting. Transactions on Graphics (TOG), 39(6):112, 2020. 3 [48] Tiancheng Sun, Kai-En Lin, Sai Bi, Zexiang Xu, and light-transport field Nelf: Neural Ravi Ramamoorthi. for portrait view synthesis and relighting. arXiv preprint arXiv:2107.12351, 2021. 2 [49] Feitong Tan, Sean Fanello, Abhimitra Meka, Sergio OrtsEscolano, Danhang Tang, Rohit Pandey, Jonathan Taylor, Ping Tan, and Yinda Zhang. Volux-gan: generative model In ACM SIGfor 3d face synthesis with hdri relighting. GRAPH 2022 Conference Proceedings, pages 19, 2022. 2 [50] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 3 [51] Yifan Wang, Aleksander Holynski, Xiuming Zhang, and Xuaner Zhang. Sunstage: Portrait reconstruction and reIn Proceedings of lighting using the sun as light stage. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2079220802, 2023. 2 [52] Zhibo Wang, Xin Yu, Ming Lu, Quan Wang, Chen Qian, and Feng Xu. Single image portrait relighting via explicit multiple reflectance channel modeling. ACM Transactions on Graphics (ToG), 39(6):113, 2020. [53] Erroll Wood, Tadas Baltruˇsaitis, Charlie Hewitt, Sebastian Dziadzio, Thomas Cashman, and Jamie Shotton. Fake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international conference on computer vision, pages 36813691, 2021. 3 [54] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2242822437, 2023. 4 [55] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3 [56] Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz, Ming-Yu Liu, and Ting-Chun Wang. Learning to relight portrait images via virtual light stage and synthetic-to-real adaptation. ACM Transactions on Graphics (TOG), 41(6): 121, 2022. 2, 3 [57] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Fine-grained lightIn ACM ing control for diffusion-based image generation. SIGGRAPH 2024 Conference Papers, pages 112, 2024. 1, 2, 3, 6, 7 [58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. 12 SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Results We present additional results on input portraits from various stock websites such as Adobe Stock, Unsplash and Pexels as well as from our internal Light-Stage captures. In-the-wild Test Portraits We demonstrate portrait relighting in the presence of strong sunlight to produce effects such as strong cast shadow from facial features, rim-effects in hair and specular highlights in Fig. 11. In Fig. 12, we demonstrate applying studio environment map on in-thewild test portraits to accentuate prominent features such as facial contours and expressions in the portraits. In Fig. 13, we showcase that SynthLight generalises to several to several challenging cases such as 2D cartoon, boy with face paint and full body portrait, beyond the diversity present in the synthetic training data. Comparison with Baselines We evaluate SynthLight against several baseline methods on in-the-wild portraits. As shown in Fig. 14, SynthLight achieves lighting effects, such as the rim-light effect in hair and subsurface scattering in the ears. Additionally, Fig. 15 illustrates specular highlights on darker skin tones, capability not replicated by baseline methods. These limitations in baselines can be attributed to the nature of the underlying methods. For instance, IC-Light, being an image harmonization technique, is not trained on physically based rendered data and hence struggles with achieving these effects. Surprisingly, even relighting approaches, such as Neural Gaffer and SwitchLight fall short. While Neural Gaffer is trained on rendered images, it is not explicitly trained on human facial data, leading to limited effectiveness in such scenarios. Even SwitchLight, despite leveraging Light Stage data, does not capture these intricate lighting effects. Ablations Fig. 19 showcases additional examples from our ablation study, illustrating the contribution of each component to the final qualitative results. The Base model struggles with identity preservation and fails to capture key details present in the input portrait. Adding either Base + Multi-Task or Base + Inference Adaptation improves detail recovery but remains insufficient for reproducing complex accessories, materials, and textures. For example, in Fig. 19, the cigarette in the input portrait (top) and the specularity of the choker necklace or the accurate dress color (bottom) are not faithfully replicated. In contrast, our method successfully addresses these challenges, achieving superior results. We train an additional model, Ours + Light Stage, where Light Stage-rendered data is combined with the synthetic dataset for relighting. The Light Stage data is same as in [33], and consists of roughly 6000 light stage captures, rendered under 100 environment maps. Fig. 20 illustrates spectrum of overexposure issues. Models trained purely on Light Stage data, such as SwitchLight, often suffer from severe overexposure, resulting in unnatural yellowish skin tones. Ours + Light Stage reduces this issue due to the inclusion of physically-based rendered synthetic data, though some overexposure persists. In contrast, our method trained exclusively on physically-based rendered synthetic data avoids this problem entirely, producing natural and balanced skin tones. Comparison with Background-Conditioned Models In trained on our synFig. 21, we compare SynthLight, thetic physically-based rendered data using environment maps with comprehensive 360 lighting information, to background-conditioned variant of SynthLight, and ICLight. SynthLight excels at capturing nuanced lighting effects, such as cast shadows and self-occlusion, due to its precise environmental lighting inputs. The backgroundconditioned model, while able to generate these lighting effects, generates inaccurate lighting. IC-Light, an image harmonisation method, neither generates these effects nor generates accurate lighting. B. Dataset Synthetic Dataset In Fig. 24 we show more examples from our synthetic dataset of subjects rendered under different environment maps. Each group of 4 visualizes subject rendered under 4 lighting conditions, highlighting variety across race and gender. LAION Data Filtration We filter subset of LAION by first running face detector. Since this results in large number of false positives, we additionally curate set of query phrases whose matching images we seek to avoid. We filter the set of images further by evaluating the CLIP score of each image against the query words and retaining only those images whose CLIP score is below threshold. Emperically, we set this threshold to 0.15. C. Additional Implementation Details Network Architecture The inputs to SynthLight are portrait image and an environment map, both with resolution of 512 512. The environment map is transformed 13 Figure 11. In order to demonstrate portrait lighting effects in the presence of strong sunlight such as strong cast shadows by facial features, rim-effects in hair and specular highlights, we show in-the-wild portraits relit using outdoor environment maps. 14 Figure 12. To demonstrate SynthLights ability to enhance portraits with studio-style lighting, we present in-the-wild portraits relit using studio environment map, where the studio lights accentuate prominent features such as facial contours and expressions. Figure 13. We show challenging in-the-wild portraits featuring 2D cartoon characters, child wearing face paint and full body portrait, demonstrating that our method can generalize beyond the synthetic dataset seen during training. 16 IC-Light Neural Gaffer SwitchLight Ours IC-Light Neural Gaffer SwitchLight Ours Figure 14. We show the input portrait, the environment map used to relight and reference synthetic data rendering from Blender (left) and results from our method and baselines (right). SynthLight achieves lighting effects such as rim-light on hair (top) and subsurface scattering in ears (bottom). These cannot be generated by baselines. IC-Light Neural Gaffer Figure 15. We highlight lighting effects that our method achieves in contrast to baselines such as specular highlights in response to lighting direction. SwitchLight Ours Test Synthetic Test Light Stage Method LPIPS SSIM PSNR Ours (init SD 1.5) Ours (init IC-Light) 0.061 0.057 0.945 0.948 30.002 30.268 FN 0.143 0.125 LPIPS SSIM PSNR 0.177 0.165 0.808 0.813 19.317 19.698 FN 0.188 0.173 Table 4. Ablating initial checkpoint: We evaluate our method, initialized with IC-Light, against initialization with SD 1.5. All tables in both main paper and supplementary, including non-inference specific ablations, are generated with classifier-free guidance parameters, λT = 2, λI = 3. See main paper for detailed descriptions of them. from high-dynamic range to low-dynamic range through the following sequence of operations: clipping, normalization, and exponentiation by 1 2.2 . These inputs are encoded into latents of shape 64 64 4 using the VAE from Stable Diffusion. SynthLight extends Stable Diffusion 1.5 by adding 8 additional channels to the first convolutional layer of the UNet, yielding total of 12 channels (4 each for the denoising latent, input portrait, and environment map). The weights for these extra channels are initialized to 0. Training and Inference We evaluate the performance of training with SD 1.5 initialization compared to ICinitialization (see Tab. 4 and Fig. 17). While Light IC-Light initialization yields slightly better test set perit as our primary formanceprompting us to report methodour approach is not reliant on IC-Light. As shown in Fig. 17, even without IC-Light, our method generates advanced lighting effects, such as strong cast shadows and subsurface scattering in the ear. Conversely, without our training and inference procedures, IC-Light alone cannot produce the nuanced lighting effects (e.g. rim-effects, subsurface scattering and specular highlights) as illustrated in Fig. 14 and Fig. 15. During training, foreground mask is applied to the input portrait. Each conditioninput portrait, environment map, and text promptis randomly dropped with probability of 0.1. For inference, classifier-free guidance is applied with λI = 3 and λT = 2, using the prompt nice person. Ablation Details Base serves as the baseline model, trained solely on the synthetic dataset. During inference, 18 Figure 16. We show the input portrait, the environment map used to relight and reference synthetic data rendering from Blender (left) and results from our method and ablations (right). We demonstrate the impact of fine-tuning with our synthetic dataset. The base model, IC-Light, without this fine-tuning, is unable to relight images using an environment map. Without Finetune (IC-Light) With Finetune (Ours) it omits inference time adaptation, meaning no classifierfree guidance is applied to the input portrait. Base + MultiTask incorporates additional training with LAION data using text-to-image task, where the input portrait and environment maps are randomly dropped. The relighting and text-to-image tasks are mixed in 7:3 ratio. Base + Inference time Adaptation applies classifier-free guidance on input portrait, while keeping the same training configuration as Base. Finally, Ours combines both strategies. We train an additional model where Light Stage-rendered data complement the synthetic dataset for relighting Ours + Light Stage. D. User Study We provide additional details about our user study. Screenshots illustrating the setup can be found in Fig. 22 and Fig. 23. The user study is conducted in three phases, with each phase focusing on specific aspect of evaluation: Phase 1: Visual Quality In the first phase, participants are asked to specify their preference between our method and the baseline in terms of visual quality. Each comparison is presented as two-option forced choice. Phase 2: Lighting In the second phase, participants evaluate the lighting of the renderings. To aid their judgment, we provide synthetic reference rendered in Blender under the same environment map. This phase also uses twooption forced choice format. Finetuning with IC-Light initialization Finetuning with SD 1.5 initialization Figure 17. We show the input portrait, the environment map used for relighting, and reference synthetic data rendering from Blender (left). On the right, we present results with IC-Light and SD 1.5 initialization for finetuning on our synthetic dataset. We note that while IC-Light initialization yields slightly better performance on our Light Stage Test set, both are comparable in terms of visual quality and achieve realistic lighting effects such as shadows and subsurface scattering. E. Limitations Fig. 25 highlights some limitations observed with our method. We notice minor loss of detail, particularly in small or intricate facial features. This can be attributed to limited camera pose diversity in our synthetic dataset, i.e. headshot-only renderings, and the reliance on Stable Diffusion 1.5, which causes our method to inherit image reconstruction artifacts from Stable Diffusions VAE. These issues can be mitigated by leveraging larger models with with better VAEs, such as those in Flux or Stable Diffusion 3, and incorporating greater camera pose variation in our synthetic dataset. Fig. 25 illustrate another failure mode where our method struggles with accurately capturing cloth textures. While this limitation is rare, it arises from the restricted range of materials and textures used for clothing in the synthetic dataset. Expanding the diversity and quality of the datasets cloth-related materials could effectively address this issue. Phase 3: Identity In the final phase, participants assess the identity of the renderings. reference input portrait is provided, and users judge which option better preserves the subjects identity. As with the previous phases, this is conducted as two-option forced choice task. General Instructions Participants are instructed to choose at random if making selection is too difficult. At the beginning of each phase, tutorial question is presented, where the answer is obvious. For example, in these cases: One example has severe degradation in visual quality. The lighting in one example is clearly incorrect. One rendering fails to match the reference identity. The correct answer and the reasoning are explained to participants to familiarize them with the task. Study Statistics The study consists of 30 questions in total, including three tutorial questions (one per phase). Participants can opt to exit the study at any time. In total, we collected 482 responses from 20 participants over oneweek period. DiLightNet IC-Light Neural Gaffer Total Relighting SwitchLight Ours Figure 18. We show additional comparisons against baselines, illustrating, that unlike baselines, our method produces accurate lighting, that matches given reference, while preserving identity and maintaining high visual quality. Base Base + Multi-Task Base + Inference-time Adaptation Ours Base Base + Multi-Task Base + Inference-time Adaptation Ours Figure 19. We show the input portrait, the environment map used to relight and reference synthetic data rendering from Blender (left) and results from our method and ablations (right). Examples show the contributions of each component in our proposed method. The Base model struggles with identity preservation and detail reproduction. Base + Multitask and Base + Inference-Time Adaptation improve detail recovery but fail to replicate complex features like accessories and textures. Our method successfully preserves identity and reproduces intricate details, such as the cigarette (top) and specularity of the necklace (bottom). 22 Figure 20. Overexposure issues due to Light Stage data. SwitchLight, trained purely on Light Stage data, suffers from severe overexposure and unnatural skin tones. Ours + Light Stage reduces this issue but retains some artifacts. Ours, trained on synthetic data alone, avoids these problems entirely. SwitchLight Ours + Light Stage Ours 23 Reference SynthLight Background Conditioned Model IC-Light Figure 21. Background vs Environment Map as Lighting Condition: We compare SynthLight with background conditioned model and IC-Light and show reference model rendered in blender (top row). Background contains insufficient lighting cues, causing background conditioned model to generate inaccurate lighting (columns 3-4). By leveraging our synthetic dataset, the background conditioned model can still generate lighting effects like strong cast shadows, whereas harmonization methods, for example, IC-Light can neither reproduce these effects or relight accurately. 24 Figure 22. User Study: We ask users to pick between our method and baseline on visual quality of image (top) and lighting, with given reference (bottom). 25 Figure 23. User Study: We ask users to judge identity preservation by providing reference identity and asking them to select between our method and baseline. Figure 24. More examples from synthetic dataset: Each group of four represents subject rendered under four different lighting conditions. 26 (a) We observe minor detail loss in facial features, such as the eyes, arising from limited camera pose diversity and Stable Diffusion 1.5s VAE artifacts. Mitigations include using improved VAEs (e.g., Flux, Stable Diffusion 3) and enhancing pose variation in the dataset. Figure 25. Limitations of our method include minor detail loss in full-body portraits and inaccuracies in cloth texture."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Yale University"
    ]
}