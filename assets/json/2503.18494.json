{
    "paper_title": "Verbal Process Supervision Elicits Better Coding Agents",
    "authors": [
        "Hao-Yuan Chen",
        "Cheng-Pong Huang",
        "Jui-Ming Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex software engineering challenges. This work introduces CURA, a code understanding and reasoning agent system enhanced with verbal process supervision (VPS), achieving a 3.65\\% improvement over baseline models on challenging benchmarks like BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and VPS techniques, attains state-of-the-art performance. This work represents a step forward in integrating reasoning-driven architectures with LLM-based code generation, enabling agentic reasoning for language models to solve complex software engineering tasks."
        },
        {
            "title": "Start",
            "content": "Hao-Yuan (Mark) Chen1,2, Cheng-Pong Huang3, and Jui-Ming Yao3 1Mindify AI, United States 2University of London, United Kingdom 3National Taiwan University of Science and Technology, Taiwan {mark, chengpong}@mindifyai.dev, b11132009@mail.ntust.edu.tw 5 2 0 2 4 2 ] A . [ 1 4 9 4 8 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex software engineering challenges. This work introduces CURA, code understanding and reasoning agent system enhanced with verbal process supervision (VPS), achieving 3.65% improvement over baseline models on challenging benchmarks like BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and VPS techniques, attains state-of-the-art performance. This work represents step forward in integrating reasoning-driven architectures with LLM-based code generation, enabling agentic reasoning for language models to solve complex software engineering tasks. \"Journey before Destination.\" Brandon Sanderson, The Way of Kings"
        },
        {
            "title": "Introduction",
            "content": "With the emergence of large-scale pre-trained language models (Brown et al., 2020; Touvron et al., 2023), there has been growing interest in their ability to act as autonomous agents capable of reasoning (Yao et al., 2023), planning (Wang et al., 2023), reflection (Shinn et al., 2023a) and executIn code generation, LLMs ing complex tasks. have demonstrated capable performance on various benchmarks. Yet, they often struggle with multi-step reasoning (Lightman et al., 2023), debugging, and adapting to real execution feedback (Chen et al., 2022). Traditional approaches rely on static datasets, limiting the models capacity to refine its outputs dynamically(Shinn et al., 2023a). Therefore, this work introduces Code Generation and Reasoning Agent (CURA)a reasoning framework enhanced by novel verbal process supervi1 sion techniqueto further improve the code generation capabilities of base reasoning and chat models. Verbal Process Supervision (VPS) is novel supervision mechanism that enables language models to generate verbal process reward signals (Lightman et al., 2023), guiding the reasoning process of the system, i.e., CURA. The research explores whether iterative verbal process supervision, combined with agentic reasoning (Lightman et al., 2023) pipeline like CURA, can serve as direct, fine-tuning-free method to reinforce model behavior and enhance reasoning capabilities through verbal reward signals to contrast the conventional reinforcement learning methods (Kumar et al., 2024). To evaluate this, the study leverages BigCodeBench to assess large-scale frontier models (DeepSeek-AI et al., 2025; OpenAI et al., 2025), examining whether this approach improves performance over raw base model without agentic or iterative reasoning. Additionally, it introduces various chat models to determine whether verbal process supervision and the CURA architecture remain effective in smaller open-source models."
        },
        {
            "title": "2 Related Works",
            "content": "In the related work section, the study investigates various notable works in the areas of code generation and agent reasoning framework to understand the frontier state of the areas while paving the foundation for the research to construct concise yet precise definition of the research hypothesis."
        },
        {
            "title": "2.1 Agent Frameworks",
            "content": "In recent works such as ReAct (Yao et al., 2023) and Reflexion (Shinn et al., 2023b), various attempts have been made to integrate agentic reasoning frameworks to enhance the performance of reasoning and coding models in solving complex software engineering tasks, including benchmarks like BigCodeBench (Zhuo et al., 2024) and HumanEval (Chen et al., 2021). Reflexion, in particular, introduces novel reinforcement learning approach by incorporating verbal reward signals into the learning loop, diverging from conventional policy gradient methods. This research serves as the theoretical foundation for the present study, which focuses on process-based supervision in reasoning frameworks such as ReAct and CURA. In contrast to Reflexions outcome-based approach, this study explores complementary paradigm that emphasizes structured process-based reasoning supervision. 2.2 Code Generation Benchmarks Evaluating the code generation capabilities of large language models (LLMs) has been key area of research, leading to the development of various benchmarks. Traditional benchmarks such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) primarily assess function-level code generation, where models generate self-contained functions given natural language prompt. In contrast, BigCodeBench (Zhuo et al., 2024) evaluates LLMs ability to generate code that integrates multiple function calls across diverse libraries and domains. Unlike ClassEval (Du et al., 2023), which focuses on object-oriented programming, BigCodeBench assesses models on broader range of practical programming tasks, including data analysis, networking, and system automation. It also introduces BigCodeBench-Instruct, variant that reformulates task descriptions into natural language instructions to test LLMs instructionfollowing capabilities. Empirical studies reveal that while LLMs perform well on isolated function calls, their accuracy deteriorates significantly when required to correctly compose multiple function calls. Both benchmarks highlight the limitations of current LLMs in handling complex, real-world programming tasks, underscoring the need for improved reasoning and compositional abilities in LLM-driven code generation."
        },
        {
            "title": "3 Methods",
            "content": "The proposed work introduces CURA (Code Understanding and Reasoning Agent), novel code generation framework incorporating verbal process supervision. This supervision mechanism enables language models to generate step-level reAlgorithm 1 CURA Reasoning Framework Require: Problem statement, model parameters Ensure: Generated and verified code solution 1: Initialize CURA reasoning framework 2: while recursion limit not reached do 3: Understanding: Interpret the problem Process Supervision: Guide reasoning Test Generation: Construct test cases Process Supervision: Guide reasoning Solution Reasoning: Generate code Process Supervision: Guide reasoning Code Execution: Run in sandbox Verification: Evaluate correctness if solution is correct then return final code solution else Process Supervision: Refine approach 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end if 15: 16: end while 17: return solution ward signals, guiding them toward improved codegeneration outcomes."
        },
        {
            "title": "3.1 CURA Architecture",
            "content": "The CURA architecture introduces an iterative, process-supervised reasoning framework for code generation, leveraging verbal reward signals to refine model behavior. The pipeline begins with code understanding, where the model interprets the given problem. Next, test case generation ensures the creation of diverse evaluation cases before transitioning to solution reasoning (code generation) to produce executable code. This code is then evaluated in code-testing sandbox to verify correctness. At each stage, process reward model provides state-based supervision, guiding the model through intermediate reasoning steps rather than relying solely on final execution results. Additionally, reward signal from the code testing phase reinforces correct behavior, enabling fine-tuning-free improvements in reasoning and problem-solving. This structured, agentic approach enhances model performance by integrating iterative feedback and verbal process supervision throughout the entire reasoning pipeline (see Figure 1) and algorithm 1."
        },
        {
            "title": "3.2 Verbal Process Supervision (VPS)",
            "content": "In this study, verbal process supervision (VPS) enables iterative feedback at each step of the reasoning process. In contrast, the Reflexion agent (Shinn 2 Figure 1: The CURA architecture: process-supervised reasoning framework incorporating verbal reward signals. et al., 2023a) refines its policy based on verbal reward signal derived from the outcome. Overall, the VPS method comprises two models and an external environment. The first model is responsible for problem-solving and reasoning within the CURA pipeline, while the second model provides verbal rewards based on the current state of the processing pipeline. In code generation, the external environment acts as an isolated sandbox where executable code is rigorously tested. The resulting execution outcomes are then relayed to the reward model, which generates supervisory signals that steer and enhance the reasoning process of the code generation models, ultimately leading to improved final outputs. Context: You are given task description along with related outputs (such as task understanding, generated test cases, code, or error messages). Goal: Provide critique of the current output and suggest improvements if needed. You need to provide detailed critique of the current output and suggest improvements to enhance the quality of the output. Task: {task} Understanding: {task_understanding} Code: {code} Test Code: {test_code} Error Message: {error_message}"
        },
        {
            "title": "3.3 Prompt Engineering of Verbal Process",
            "content": "Supervision (VPS)"
        },
        {
            "title": "4 Experiments",
            "content": "This structured verbal supervision approach ensures that each step is guided with natural language feedback, allowing iterative refinement before reaching the final solution. This methodology enhances reasoning capabilities, reduces hallucination errors, and enables process-level improvements in code generation tasks. This study validates the effectiveness of the CURA architecture combined with the VPS method on the BigCodeBench and HumanEval datasets. Using GPT-4o-mini and o3-mini models, the results demonstrate that CURA, when integrated with VPS, consistently improves performance across two different code generation tasks. Identity: You are an expert AI assistant specializing in programmatic reasoning, problem decomposition, reflective reasoning, and solution verification."
        },
        {
            "title": "4.1 BigCodeBench with Reasoning Model",
            "content": "The bar chart 2 compares the performance of the o3mini Baseline and o3-mini - CURA with VPS on the BigCodeBench - Hard benchmark across three 3 evaluation categories: Complete, Instruct, and Average. The vertical axis represents the score in percentage, ranging from 30% to 50%. The results indicate that the o3-mini-VPS model outperforms the o3-mini Baseline in the Complete and Average categories while slightly underperforming in the Instruct category. Specifically, in the Complete category, o3-mini - VPS achieves score of 45.9%, significantly higher than the o3-mini Baselines 37.8%. However, in the Instruct category, the baseline model attains score of 33.1%, which is marginally better than the VPS models 32.4%. When considering the Average score across all categories, o3-mini-VPS still demonstrates an overall improvement, scoring 39.1% compared to the baselines 35.5%. These results suggest that the VPS-enhanced model provides substantial benefits in code completion tasks but may require further refinements for instruction-based tasks with pure natural language understanding. parison includes GPT-4o-mini and Mistral Large Latest, each evaluated at temperature 0 (deterministic setting) and temperature 1 (stochastic setting). The results indicate that models generally perform better at temperature 0, with Mistral Large Latest (Temp=0) achieving the highest score in the Complete category (31.8), surpassing GPT-4o-mini (Temp=0) (28.4). However, in the Instruct category, the gap is smaller, with GPT-4o-mini (Temp=0) scoring 22.3, while Mistral Large Latest (Temp=0) scores 23.6. The average performance follows similar trend, where higher temperatures negatively impact scores, with Mistral Large Latest (Temp=1) demonstrating the lowest performance across all categories. This suggests that deterministic decoding strategies yield more reliable results, particularly in structured code generation tasks, whereas higher temperatures introduce randomness that degrades model effectiveness. These findings highlight the trade-offs between diversity and reliability in model outputs and suggest that Mistral Large Latest excels in deterministic code completion, while GPT-4o-mini remains competitive across categories. Figure 2: Comparison of o3-mini Baseline vs. o3-mini CURA with VPS on the BigCodeBench (Hard) dataset. The y-axis shows the score (in %), while the x-axis shows three different evaluation modes (Complete, Instruct, and the Average of all modes). Notice that o3mini VPS shows an improvement in all categories, with the largest gain in the Complete mode."
        },
        {
            "title": "4.2 BigCodeBench with Various Chat Models",
            "content": "on Different Temperatures The performance evaluation of different language models on the BigCodeBench - Hard Benchmark at varying temperature settings reveals key insights into their effectiveness in code generation tasks, which is shown in the figure 3. The benchmark assesses models across three categories: Complete, which measures the ability to generate complete code segments; Instruct, which evaluates instruction-following capabilities; and Average, representing the overall performance. The comFigure 3: Performance comparison of GPT-4o-mini and Mistral Large Latest on the BigCodeBench using CURA architecture with VPS technique - Hard Benchmark across different temperature settings. The models are evaluated in three categories: Complete, Instruct, and Average. Results indicate that deterministic decoding (Temp=0) generally leads to higher scores, particularly in the Complete category where Mistral Large Latest outperforms other configurations. Increasing temperature (Temp=1) negatively impacts performance across all categories, highlighting the trade-offs between deterministic and stochastic decoding in code generation tasks."
        },
        {
            "title": "5 Discussion",
            "content": "The results demonstrate that the CURA framework, when combined with verbal process supervision 4 (VPS), significantly improves code generation performance across multiple benchmarks. The observed improvements are particularly evident in the BigCodeBench - Hard benchmark, where the CURA-enhanced models outperform their respective baselines in the Complete and Average evaluation categories. This indicates that process supervision effectively refines intermediate reasoning steps, leading to better overall performance. However, the slight underperformance in the Instruct category suggests that VPS may need further adaptation for tasks that heavily depend on direct instruction-following rather than iterative reasoning. 5.1 Influence of Temperature Settings key insight from the experiments is the influence of temperature settings on model performance. The results indicate that deterministic decoding (temperature = 0) generally yields more reliable outputs compared to stochastic decoding (temperature = 1). Mistral Large Latest achieves the highest scores in the Complete category when temperature is set to zero, while performance degrades as randomness increases. This suggests that structured code generation benefits from deterministic approaches, whereas higher temperatures introduce variability that can be detrimental to maintaining coherence and correctness. The performance comparison between different models also reveals that while Mistral Large Latest excels in deterministic code completion, GPT-4omini maintains competitive performance across all categories. This indicates that while some models benefit more from structured reasoning, others balance instruction-following and code generation more effectively. The trade-offs between reliability and diversity in outputs suggest that future improvements could incorporate adaptive temperature strategies to optimize generation for different tasks."
        },
        {
            "title": "Technique",
            "content": "The effectiveness of VPS in guiding intermediate steps also raises questions about its scalability to larger code-generation scenarios. The current setup applies VPS at fine-grained level, providing immediate feedback at each stage of the reasoning process. While this enhances model refinement, it also introduces potential computational overhead. Future work should explore methods to balance the granularity of supervision with efficiency, potentially by leveraging hierarchical reward structures or selective intervention strategies. 5.3 Opportunities of CURA Method with VPS Technique Another consideration is the broader applicability of CURA beyond code generation. The principles of verbal process supervision could be extended to other reasoning-intensive tasks, such as theorem proving, data analysis automation, or even multimodal reasoning. By incorporating process-level feedback rather than relying solely on final output evaluation, language models could develop more robust problem-solving capabilities across various domains."
        },
        {
            "title": "6 Conclusion",
            "content": "This work introduces CURA, novel reasoning architecture designed as code understanding and reasoning agent, complemented by verbal process supervision (VPS) techniques. CURA leverages iterative, step-level VPS to guide large language models (LLMs), enabling them to significantly surpass their base performance. By integrating agentic reasoning with inference-time computation, this approach establishes new paradigm for enhancing code generation and software engineering tasks. However, VPS remains constrained in its ability to precisely convey improvement directions to the code generation model. Future research will explore advanced prompting strategies and test-time computing techniques to further refine CURA and enhance the effectiveness of VPS."
        },
        {
            "title": "Limitations",
            "content": "While CURA with VPS demonstrates significant improvements in structured code generation, several limitations remain. One major challenge is the computational cost associated with iterative process supervision. Since VPS introduces multiple feedback loops, the overhead may slow down inference and limit the feasibility of real-time applications, especially in large-scale production environments. Another limitation is the reliance on verbal feedback models, which may not always align with optimal reasoning strategies. Although verbal process supervision helps refine intermediate steps, it may occasionally introduce biases or incorrect guidance, particularly if the reward model is not well-calibrated. Further improvements in aligning 5 VPS signals with human-expert feedback could enhance its effectiveness. Additionally, the performance of CURA depends on the underlying language models capabilities. While VPS helps refine reasoning, it does not fully mitigate the inherent limitations of pre-trained models, such as sensitivity to prompt variations and potential hallucinations in code generation. Addressing these challenges may require integrating stronger retrieval mechanisms or external knowledge bases to improve factual accuracy. Finally, the adaptability of VPS across different domains remains an open question. Although the method is designed for code generation, its effectiveness in other complex reasoning tasks, such as mathematical theorem proving or multimodal reasoning, requires further investigation. Future work should explore domain-specific adaptations and evaluate VPS in broader range of AI-assisted reasoning applications."
        },
        {
            "title": "Acknowledgments",
            "content": "The research was co-led by Mark Chen and ChengPong Huang during their time at Mindify AI, whereas Jui-Ming Yao provided support and conducted an analysis of the empirical study of the work."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models. Preprint, arXiv:2108.07732. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Preprint, arXiv:2005.14165. Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet: Code generation with generated tests. Preprint, arXiv:2207.10397. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. Preprint, arXiv:2107.03374. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, 6 Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Preprint, arXiv:2307.09288. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models. Preprint, arXiv:2305.04091. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. Preprint, arXiv:2210.03629. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, and Leandro Von Werra. 2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. Preprint, arXiv:2406.15877. Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. 2023. Classeval: manually-crafted benchmark for evaluating llms on class-level code generation. Preprint, arXiv:2308.01861. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. 2024. Training language models to self-correct via reinforcement learning. Preprint, arXiv:2409.12917. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Preprint, 2023. arXiv:2305.20050. Lets verify step by step. OpenAI, :, Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, and Wenda Zhou. 2025. Competitive programming with large reasoning models. Preprint, arXiv:2502.06807. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023a. Reflexion: Language agents with verbal reinforcement learning. Preprint, arXiv:2303.11366. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023b. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems, volume 36, pages 86348652. Curran Associates, Inc. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,"
        }
    ],
    "affiliations": [
        "Mindify AI, United States",
        "National Taiwan University of Science and Technology, Taiwan",
        "University of London, United Kingdom"
    ]
}