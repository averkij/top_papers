{
    "paper_title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning",
    "authors": [
        "Wenyi Hong",
        "Wenmeng Yu",
        "Xiaotao Gu",
        "Guo Wang",
        "Guobing Gan",
        "Haomiao Tang",
        "Jiale Cheng",
        "Ji Qi",
        "Junhui Ji",
        "Lihang Pan",
        "Shuaiqi Duan",
        "Weihan Wang",
        "Yan Wang",
        "Yean Cheng",
        "Zehai He",
        "Zhe Su",
        "Zhen Yang",
        "Ziyang Pan",
        "Aohan Zeng",
        "Baoxu Wang",
        "Boyan Shi",
        "Changyu Pang",
        "Chenhui Zhang",
        "Da Yin",
        "Fan Yang",
        "Guoqing Chen",
        "Jiazheng Xu",
        "Jiali Chen",
        "Jing Chen",
        "Jinhao Chen",
        "Jinghao Lin",
        "Jinjiang Wang",
        "Junjie Chen",
        "Leqi Lei",
        "Leyi Pan",
        "Mingzhi Zhang",
        "Qinkai Zheng",
        "Sheng Yang",
        "Shi Zhong",
        "Shiyu Huang",
        "Shuyuan Zhao",
        "Siyan Xue",
        "Shangqin Tu",
        "Shengbiao Meng",
        "Tianshu Zhang",
        "Tianwei Luo",
        "Tianxiang Hao",
        "Tianle Gong",
        "Wenkai Li",
        "Wei Jia",
        "Xin Lyu",
        "Xuancheng Huang",
        "Yanling Wang",
        "Yadong Xue",
        "Yanfeng Wang",
        "Yifan An",
        "Yifan Du",
        "Yiming Shi",
        "Yiheng Huang",
        "Yilin Niu",
        "Yuan Wang",
        "Yuanchang Yue",
        "Yuchen Li",
        "Yutao Zhang",
        "Yuxuan Zhang",
        "Zhanxiao Du",
        "Zhenyu Hou",
        "Zhao Xue",
        "Zhengxiao Du",
        "Zihan Wang",
        "Peng Zhang",
        "Debing Liu",
        "Bin Xu",
        "Juanzi Li",
        "Minlie Huang",
        "Yuxiao Dong",
        "Jie Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking."
        },
        {
            "title": "Start",
            "content": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning GLM-V Team Zhipu AI & Tsinghua University (For the complete list of authors, please refer to the Contribution section)"
        },
        {
            "title": "Abstract",
            "content": "We present GLM-4.1V-Thinking, vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop capable vision foundation model with significant potential through largescale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking. 5 2 0 2 1 ] . [ 1 6 0 0 1 0 . 7 0 5 2 : r Figure 1: (A) GLM-4.1V-Thinking matches or outperforms the much larger Qwen2.5-VL-72B and the closed-source GPT-4o across range of tasks. (B) Reinforcement learning substantially boosts the models performance, with gains of up to +7.3%."
        },
        {
            "title": "Introduction",
            "content": "Vision-language models (VLMs) have become crucial cornerstone of modern intelligent systems, enabling the perception and understanding of visual information beyond text. Over the past decade, as the intelligence level of models has advanced dramatically [31; 41; 17; 3], the complexity of corresponding multimodal intelligence tasks has increased accordingly. From solving scientific problems [52; 27; 29] to developing autonomous agents [18; 49; 37], the demands on models have far surpassed simple visual content perception [26], with an increasing emphasis on advanced reasoning capabilities. Recently, numerous studies have shown that long-form reasoning [48] and scalable reinforcement learning [36] can significantly enhance the ability of large language models (LLMs) to solve complex problems [20; 14]. Several previous works have attempted to enhance the reasoning capabilities of VLMs using similar paradigms [40; 28], but they mainly focus on specific domains. The open-source community currently lacks multimodal reasoning model that consistently outperforms traditional non-thinking models of comparable parameter scale across broad range of tasks. In this report, we share our key findings in the development of GLM-4.1V-Thinking, vision-language model (VLM) designed to advance general-purpose multimodal reasoning. Our training framework is structured around unified objective: to comprehensively enhance the models reasoning capabilities through scalable reinforcement learning. For pre-training, we curate broad and diverse corpus of knowledge-intensive multimodal data to equip the model with strong foundational capabilities, including (a) massive image-text pairs with accurate factual knowledge; (b) self-curated academic corpus with interleaved image and text; (c) annotated documents and diagrams, instructional videos, and grounding data spanning both natural and synthetic images. This foundation model serves as high-potential multimodal reasoning base for subsequent reinforcement learning. In the supervised fine-tuning phase, we construct carefully designed, domain-specific datasets that teach the model to perform effective reasoning with standardized format across wide range of tasks. Finally, we introduce Reinforcement Learning with Curriculum Sampling (RLCS) to drive large-scale, cross-domain reasoning capabilities. RLCS is multi-domain reinforcement learning framework that combines curriculum learning with difficulty-aware sampling to improve training efficiency by selecting tasks and samples suited to the models current competence. Our reinforcement learning process enhances training effectiveness and stability, and systematically improves the models reasoning abilities through interaction and feedback across diverse domains. To advance research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves stateof-the-art performance among models of comparable size. In comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks, and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. For example, our model achieves 85.8 on MMBenchV11-EN, 72.9 on MMStar, and 87.9 on AI2D, surpassing both Qwen2.5-VL-72B and other baselines. In particularly challenging areas such as MMMU-Pro and ChartMuseum, it outperforms even 72B-scale models by 6.0 and 9.2 points, respectively. On long document understanding, it scores 42.4 vs. 35.2 (Qwen2.5-VL-72B), exhibiting strong multi-page document reading and reasoning abilities. It also leads on multimodal agent (WebQuest-SingleQA: 72.1 vs. 60.5) and coding tasks (Flame-VLM-Code: 72.5 vs. 46.3). Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities, such as MMStar (72.9 vs. 66.2), MUIRBENCH (74.7 vs. 69.7), and MathVista (80.7 vs. 64.0). We also open-source the pre-trained base model, GLM-4.1V-9B-Base, to provide strong foundation for fellow researchers to develop and extend their own models. We summarize some of our key findings from the development process below and provide more detailed explanations in the following sections. Multi-domain reinforcement learning demonstrates robust cross-domain generalization and mutual facilitation. Training on one domain boosts performance in others, and joint training across domains yields even greater improvements in each. (See Section 6.3) Dynamically selecting the most informative rollout problems is essential for both efficiency and performance. Therefore, we propose strategies including Reinforcement Learning with 2 Figure 2: The architecture of GLM-4.1V-Thinking. The proposed model consists of three components: (1) ViT Encoder to process and encode images and videos, (2) an MLP Projector to align visual features to tokens, (3) Large Language Model as Language Decoder to process multimodal tokens and yield token completions. GLM-4.1V-Thinking can perceive images and videos as their native resolutions and aspect ratios. For video inputs, additional time index tokens are inserted behind each frame to enhance the models temporal understanding capability. Curriculum Sampling (RLCS) and dynamic sampling expansion via ratio-based EMA. (See Section 5.3) robust and precise reward system is critical for multi-domain RL. When training unified VLM across diverse skills, even slight weakness in the reward signal for one capability can collapse the entire process. (See Section 5.2) In summary, our contributions are as follows: We present GLM-4.1V-Thinking, VLM designed to advance general-purpose multimodal reasoning. We introduce the model design and the reasoning-centric training framework, along with key insights and challenges encountered during the development process. We open-source the reasoning model GLM-4.1V-9B-Thinking, the pre-trained foundation model GLM-4.1V-9B-Base, and other useful components such as domain-specific reward systems, to facilitate research in this area. Code, models and more information are released at https:// github.com/THUDM/GLM-4.1V-Thinking. Comprehensive experiments demonstrate the superiority of the proposed model: GLM-4.1V-9BThinking achieves state-of-the-art performance among models of its size and is even comparable to much larger models on numerous benchmarks."
        },
        {
            "title": "2 Overview and Architecture",
            "content": "Figure 2 shows the architecture of GLM-4.1V-Thinking, composed of three core components: vision encoder, an MLP adapter, and large language model (LLM) as the decoder. We employ AIMv2Huge [7] as the vision encoder and GLM [11] as the LLM. Within the vision encoder, we adopt strategy similar to Qwen2-VL [45], replacing the original 2D convolutions with 3D convolutions. This enables temporal downsampling by factor of two for video inputs, thereby improving model efficiency. For single-image inputs, the image is duplicated to maintain consistency. 3 Figure 3: Comparison of pass@k performance on subset of MathVista consisting of non-multiplechoice questions. To support arbitrary image resolutions and aspect ratios, we introduce two adaptations. First, we integrate 2D-RoPE [38], enabling the model to effectively process images with extreme aspect ratios (over 200:1) or high resolutions (beyond 4K). Second, to preserve the foundational capabilities of the pre-trained ViT, we retain its original learnable absolute position embeddings. During training, these embeddings are dynamically adapted to variable-resolution inputs via bicubic interpolation. Specifically, for an input image divided into grid of Hp Wp patches, the integer coordinates = (w, h) of each patch are first normalized to continuous grid gnorm spanning [1, 1]: gnorm = (wnorm, hnorm) = 2 (cid:18) + 0.5 Wp , + 0.5 Hp (cid:19) 1 (1) These normalized coordinates are then used to sample from the original position embedding table Porig using bicubic interpolation function Ibicubic to generate the final adapted embedding Padapted for that patch: Padapted(g) = Ibicubic(Porig, gnorm) (2) To further enhance spatial awareness on the language side, we extend RoPE to 3D-RoPE in the LLM. This extension provides superior spatial understanding for multimodal contexts, while preserving the original models text-based capabilities. After addressing spatial adaptation, we turn to temporal modeling in video inputs. For videos, we insert time index token after each frame token, where the time index is implemented by encoding each frames timestamp as string. Unlike multi-image inputs, video frames form temporally coherent sequence. This design explicitly informs the model of the real-world timestamps and temporal distances between frames, thereby boosting its temporal understanding and grounding capabilities."
        },
        {
            "title": "3 Pre-training",
            "content": "To develop more powerful visual language foundation model, we are incorporating diverse range of datasets, including extensive academic corpora and knowledge-rich, interleaved image-text data, while also training the model on pure text data to preserve its language capabilities. As shown in Figure 3, GLM-4.1V-9B-Base achieves significantly better results on the pass@k metric compared with the state-of-the-art pre-trained base model of similar scale. This superior base performance arguably sets the upper bound for the final results after reinforcement learning (RL). Below, we first describe the construction of pre-training data and then outline the training procedure. 4 Figure 4: Examples of recaption model results. The recaptioning process eliminates noise and hallucinated content from the original data, while fully retaining factual knowledge. 3.1 Pre-training Data Image caption data. High-quality image-text captions are crucial for imbuing visual-language models with world knowledge and enhancing their generalization capabilities. To this end, we construct large-scale, high-quality caption dataset through meticulous curation pipeline. The process begins with the aggregation of an initial pool of over 10 billion image-text pairs from diverse sources, including public datasets like LAION[34], DataComp [10], DFN[6], and Wukong [12], supplemented by data from web search engines. To ensure data integrity, we implement multi-stage refinement process: 1. Heuristic-based filtering: We first apply series of rule-based filters to discard overtly lowquality samples. These rules include minimum image resolution, solid color detection, caption length constraints, and image-level deduplication. 2. Relevance filtering: To enforce semantic consistency between modalities, we employ pretrained CLIP model to calculate image-text similarity, retaining only pairs with CLIP-Score above threshold of 0.3. 3. Concept-balanced resampling: To mitigate the inherent long-tail distribution of concepts in web-scale data, we adopt resampling strategy inspired by MetaCLIP [50]. Using comprehensive vocabulary rich in visual concepts and proper nouns, we re-weight the filtered data to enhance conceptual coverage and balance. 4. Factual-centered recaptioning: Furthermore, to improve the descriptive quality and information density of the captions, we iteratively train factual-centered recaptioning model. As shown in Figure 4, this model is designed to denoise and enrich the original captions, generating new, more precise, and detailed descriptions while preserving the factual accuracy of the source text. For the final training set, we blend the curated original data with the recaptioned data at specific ratio, creating final dataset that balances the breadth of world knowledge with descriptive depth. Interleaved image-text data. Rich interleaved image-text vision-language data can be found in corpora such as web pages and books. On the one hand, its volume is immense, far exceeding that of existing image-caption datasets which rely primarily on alt-text. On the other hand, it encodes rich information beyond simple image descriptions, including complex logical relationships between text and images and covering broad spectrum of domain knowledge. However, such data is often extremely noisy: many samples lack genuine image-text alignment, and the distribution of information density is highly skewed (a large fraction of the corpus is uninformative). As result, prior work seldom leverages these resources at scale to boost vision-language capabilities, typically only using small amounts to help models adapt to multi-image, interleaved-text layouts. To address this, we design and implement specialized processing pipelines tailored to each data source, successfully extracting large volume of high-quality interleaved image-text data and substantially enhancing the models foundational image-text understanding and reasoning abilities. The specific pipelines are as follows: 1. Web data processing pipeline: Our pipeline for web data begins with the aggregation of raw content from large-scale open-source datasets, including MINT [2], MMC4 [57], and OmniCorpus [24]. This initial pool undergoes multi-stage cleaning and filtering process. First, we discard images that are semantically irrelevant to the surrounding article context using CLIP-Score threshold. We then remove common noise elements like advertisements and QR codes, which are typically located at the end of articles, using combination of heuristic rules 5 and purpose-built image classifier for enhanced precision. Furthermore, we exclude samples characterized by high density of images but sparse textual content, such as online photo albums. To actively enrich the dataset with high-information-value content, we iteratively train highknowledge-density image classifier. This model is engineered to identify and prioritize images of significant informational value, such as academic charts, scientific illustrations, engineering schematics, instructional diagrams, and maps. 2. Academic book processing pipeline: As another core data source, we collect over 100 million digitized books. To ensure content relevance and quality, we first filter this collection to select books pertaining to key domains, including science, technology, engineering, and mathematics (STEM). Subsequently, we employ PDF parsing tool to perform deep parsing of these PDF documents, enabling the extraction of high-quality interleaved image-and-text content. OCR data. To bolster the models OCR capabilities, we construct large-scale pre-training dataset comprising 220 million images. This dataset is meticulously composed of three distinct components, each designed to address specific aspect of text recognition: 1. Synthetic document images: We render text from language pre-training corpora using varied fonts, sizes, colors, and orientations. These rendered texts are then composited onto diverse image backgrounds sourced from the LAION dataset, producing synthetic images that cover broad spectrum of practical application scenarios. 2. Natural scene text images: We utilize the Paddle-OCR toolkit to process vast collection of natural images, automatically extracting textual content and their corresponding bounding boxes. The resulting data is subsequently filtered to retain only images containing at least one valid OCR detection, thereby enriching the dataset with authentic, real-world text instances. 3. Academic documents: We adopt processing methodology inspired by Nougat [4]. large corpus of papers is sourced from arXiv, where the LaTeX source code is first normalized and converted to HTML format using the LaTeXML tool. The HTML is then parsed and transformed into lightweight markup language. Finally, this content is segmented according to the original PDF page breaks and rasterized, creating high-quality dataset of paired PDF page renderings and their corresponding structured source markup. Grounding data. To endow the model with precise visual localization capabilities, we construct hybrid grounding dataset spanning two primary domains: natural images and graphical user interfaces (GUIs). 1. Natural image grounding: In the domain of natural images, we utilize LAION-115M [23] as foundational dataset. Leveraging the GLIPv2 [54] model, we parse the caption of each image and automatically predict the corresponding bounding boxes for every noun phrase. To ensure the quality and richness of the grounding data, we apply filter to retain only those samples containing at least two valid bounding boxes. This pipeline results in final dataset of 40 million high-quality grounding annotations for natural images. 2. GUI grounding: For the GUI domain, we construct novel, large-scale dataset from scratch. We begin by extracting URLs from recent CommonCrawl snapshot and capturing corresponding webpage screenshots via automated tools. Going beyond static captures, we employ the Playwright framework to deeply interact with webpages. This enables us to compile and parse all visible DOM elements along with their precisely rendered bounding boxes on the page. In general, to enhance the models interactive and comprehension abilities within GUI environments, we generate over 140 million question-answer pairs for Referring Expression Generation and Comprehension tasks specific to GUIs. Video data. To support advanced video understanding, we construct large-scale, high-quality video-text dataset. We curate diverse corpus from academic, web, and proprietary sources. To address the hallucinations and omissions common in standard captions, we develop pipeline with fine-grained human annotation to accurately capture complex actions and in-scene text. Furthermore, to encode deeper visual narratives, we annotate key cinematic elements such as camera motion and shot composition using human-in-the-loop workflow. To ensure data purity, we implement rigorous filtering protocol. We begin with integrity checks to remove corrupted or invalid files. Subsequently, we employ multimodal, embedding-based 6 deduplication strategy, discarding pairs where both video and text embeddings show high similarity to another entry. This process effectively eliminates semantic redundancy, resulting in clean and efficient training corpus. Instruction tuning data. To enhance model versatility and generalization, we diversify high-quality instruction tuning data. Three targeted strategies are implemented: 1. Task coverage and taxonomy: We design fine-grained taxonomy to optimize data sampling for expanded world knowledge coverage. This taxonomy allows us to organize prompts according to their semantic structure and task objective, enabling category-specific preprocessing and balanced sampling strategies. 2. Complex scenario augmentation: To address gaps in existing open-source datasets (e.g., GUI interactions, long-document comprehension), we integrate synthetically generated data with rigorous structural constraints. These methods help us expand the dataset in diverse areas and improve its overall complexity. 3. Data contamination check: To prevent data leakage from public evaluation benchmarks, we conduct both manual and automated reviews of all open-source datasets. The resulting 50 million samples include general visual perception and understanding, multimodal reasoning (e.g., STEM problem-solving), document-intensive contexts, GUI agent operations, and UI coding. It provides comprehensive coverage for the full-scenario reinforcement learning pipeline. 3.2 Training Recipe Our models training is conducted in two sequential stages: multimodal pre-training and long-context continual training. Multimodal pre-training. The initial stage aims to build strong foundation of general multimodal capabilities. We train all model parameters for 120,000 steps using 2-way tensor parallelism strategy. The training utilizes sequence length of 8,192 and global batch size of 1,536. The dataset for this stage consists of carefully curated mixture of all data modalities described in Section 3.1, with the exception of video. To maximize computational efficiency, we employ data packing strategy where multiple variable-length samples are concatenated into single sequences approaching the maximum length. Long-context continual training. Following pre-training, we perform continual training stage to extend the models capabilities to high-resolution imagery, video, and extended contexts. We augment the training data with video inputs and long-sequence interleaved data exceeding 8k tokens. To accommodate these longer inputs, we increase the sequence length to 32,768 and adopt hybrid parallelism approach, combining 2-way tensor parallelism with 4-way context parallelism. This stage is run for an additional 10,000 steps, while maintaining the global batch size of 1,536."
        },
        {
            "title": "4 Supervised Fine-Tuning",
            "content": "The supervised fine-tuning (SFT) stage functions as bridge to reinforcement learning, transforming base vision-language model (VLM) into one capable of long chain-of-thought (CoT) inference. Our long-CoT corpus is carefully curated to enhance reasoning style and human alignment, spanning both verifiable domains (e.g., STEM problems) and non-verifiable tasks (e.g., instruction following, open-ended writing). Unlike prior workflows [47; 17; 13] that apply SFT to short CoT data, we deliberately omit this step: rather than injecting new knowledge, we view SFTs role as aligning the models existing vision-language understanding with more effective thinking and response style. This alignment primes the model for stronger cold start, enabling more efficient and stable reinforcement learning in the next phase. 4.1 Supervised Fine-Tuning Data To facilitate subsequent reinforcement learning, we curate high-quality dataset of long CoT reasoning examples. This dataset is designed to train models to produce coherent, multi-step solutions in standardized format, thereby underpinning stable and scalable RL training. 7 Data composition. Our reasoning dataset spans wide spectrum of domains, with primary focus on verifiable tasks whose outcomes can be rigorously assessed and refined via reinforcement learning. We also include non-verifiable tasks, such as open-ended visual question answering, to broaden and strengthen the models general reasoning capabilities across diverse contexts. The dataset is primarily composed of data in Chinese and English, with small proportion in other languages. We employ our pre-trained model to filter out instances that are either too easy or excessively hard, maintaining moderate overall difficulty level suitable for training. Response formatting. Each response follows standardized structure: <think> {think_content} </think> <answer> {answer_content} </answer> The <think> part captures the models reasoning process, including strategies such as reflection, backtracking, retrying, and verification. The <answer> part presents concise, complete and logically sound solution. For verifiable tasks with specific final answer, the final result in the <answer> part is required to be wrapped with <begin_of_box> and <end_of_box>, and only one boxed span is acceptable. This annotation facilitates more accurate answer extraction during the RL phase. Note that we include <think>, </think>, <answer>, </answer>, <begin_of_box>, <end_of_box> to the tokenizers vocabulary as special tokens to facilitate easier and accurate online parsing. Response curation. The quality of the cold-start dataset is critical to the stability of RL training. In practice, we find that poorly constructed data can lead to training instability or even collapse. To mitigate this, we implement rigorous data cleaning pipeline. This process enforces strict adherence to formatting conventions (e.g., correct usage of <think> and <answer> tags) and removes examples with inconsistent or noisy reasoning styles. Also, we filter out responses containing mixed-language phrasing or redundant thought patterns. Iterative data enhancement. To improve the quality and challenge level of the cold-start dataset, we incorporate high-quality and informative examples sampled from RL checkpoints back into the cold-start dataset. This iterative enhancement helps expose the model to more useful reasoning patterns discovered during RL, and can provide stronger foundation for subsequent rounds of RL training. 4.2 Training Recipe We perform full-parameter fine-tuning with sequence length of 32,768 tokens and global batch size of 32. The training corpus includes the long-form reasoning data described in 4.1, spanning multiple domains. In addition to multimodal data, we also incorporate high-quality text-only long-form examples covering math problem solving, multi-turn conversation, agent planning, and instruction following. These examples help preserve the models core language understanding and general reasoning abilities throughout multimodal fine-tuning. Interestingly, we observe that even when cold-start training uses noisy reasoning data, which contain formatting inconsistencies or repetitive patterns, subsequent RL remains effective. This suggests that imperfect reasoning traces can still provide useful guidance. Nonetheless, models initialized with clean and consistent data show more stable RL convergence and achieve higher overall performance."
        },
        {
            "title": "5 Reinforcement Learning: What’s Challenging and What Works",
            "content": "After the supervised fine-tuning phase, we primarily rely on reinforcement learning (RL) to enhance the models performance. We employ combination of Reinforcement Learning with Verifiable Rewards (RLVR) and Reinforcement Learning with Human Feedback (RLHF) to conduct large-scale RL across all multimodal domains and capabilities, including STEM problem solving (such as mathematics, physics, chemistry), grounding, optical character recognition (OCR), video understanding, GUI agents, chart and document understanding, logical reasoning, and instruction following. Our RL framework comprises the following components: Data preparation: Define sub-tasks in each multimodal domain that are suitable for verifiable rewards (for RLVR) or model-based rewards (for RLHF) as supervision signals, and curate large volumes of high-quality data with appropriate difficulty levels and broad coverage. 8 Reward system: Precise rewards are the key to RLVRs effectiveness. When scaling RL across all multimodal domains, it becomes challenging yet critical to assign accurate rewards to as many tasks as possible within each subdomain. We design multi-domain, unified reward system that shares common evaluation logic while enabling targeted optimization of robust verifiers for each subdomain. Training: Building on our solid foundation of data and reward system, we meticulously refine our RL training recipes toward improved effectiveness, efficiency and stability. We propose and incorporate improvements including Reinforcement Learning with Curriculum Sampling (RLCS), dynamic sampling expansion with ratio EMA, larger batch size, discarding KL and entropy loss, etc. Infrastructure: To efficiently utilize compute resources for large-scale RL training, we develop an in-house high-performance, stable RL infrastructure. It flexibly supports diverse training configurations across multimodal domains (e.g., custom verifiers, data sampling ratios) and incorporates comprehensive optimizations in sampling, training, and beyond. We find that challenges arise at every layerfrom data preparation and reward system to training and infrastructure. Failure in any single dimension can lead to severe degradation in the efficiency of the RL stage or even its collapse. In this section, we first outline the core workflow of each component, and then share the challenges we encounter during our exploration as well as the best practices we discover. 5.1 Data Preparation The objective of data preparation is to select or synthesize as much verifiable data as possible in each subdomain that can be efficiently improved through RL. To this end, we carry out the following stages in sequence. Task identification. We first define set of candidate tasks for verification in each multimodal subdomain. For example, while video captioning is open-ended and difficult to evaluate strictly, temporal grounding lends itself to clear correctness judgments. Data curation. We then filter or generate question-answer pairs from these tasks that verifier can assess with high precision. This process includes converting multiple-choice questions with unique answers into the fill-in-the-blank format to eliminate noise from random guessing during RL. Quality validation and offline difficulty grading. Next, we carry out thorough correctness checks and run passk evaluations using multiple existing or prior RL models, combining these results with human difficulty labels to achieve fine-grained difficulty grading. Pilot RL experiments. Finally, we perform preliminary RL experiments in each subdomain to confirm the datas quality and the models potential for performance gains. 5.2 Reward System We establish reward system compatible with both RLVR and RLHF and tailor it for every multimodal domain. For RLVR tasks, the system first extracts the segment containing the final answer from the rollout outputs, then compares this key answer against the reference answer to determine correctness, and finally returns reward value in binary (0/1) or continuous form. For RLHF tasks, it directly takes the answer segment of the output and scores it using the reward model. As its core, reinforcement learning is an optimization process driven by the feedback from the reward system. Therefore, its critical to enhance the accuracy and robustness of the reward system. The possibility of assigning precise rewards is also one of the key reasons why RLVR is currently delivering outstanding results. For GLM-4.1V-Thinking, to achieve multimodal reinforcement learning across all domains, we employ meticulously crafted reward system to supervise every facet of the models abilitiesvisual perception (OCR, grounding), comprehension (document, chart and video understanding), reasoning (academic and logical problem solving), and agent behaviorthereby necessitating the development of comprehensive, precise, and robust reward system. Although some studies report that even random or imperfect feedback can sometimes yield benefits by steering models toward effective output patterns [35], we discover that when training unified 9 Figure 5: Training reward curves (top) and evaluation metrics (bottom) when low-quality verifiers exist in some multimodal sub-domains. The STEM verifier is finely tuned, but the other-single-image and other-multi-image verifiers are not, causing: (a) Reward noise@other-single-image: The model tweaks outputs to drive rewards up without improving actual accuracy. (b) Reward hacking@othermulti-image: The model learns shortcuts that repeatedly fool the verifier, inflating rewards. After step 150, STEM reward growth stalls, the overall multimodal benchmark declines, and STEM-related benchmarks (MMMU, MathVista, AI2D) drop sharply. VLM across diverse skills, any weakness in the reward signal for single capability can derail the entire training. As Figure 5 illustrates, even if the STEM subdomain is provided with high-quality reward, flaw in the reward for the multi-image QA task led to model collapse across all domains. This highlights that stable, effective RL demands finely tuned, hack-resistant verifiers in every domainany weak verifier can destabilize and collapse the entire training. We highlight several challenges and difficulties we identified in reward model design during our experiments below, and present our corresponding optimizations and solutions. The extraction of the final answer in RLVR. For RLVR, we first extract the final answer from the models response and then conduct correctness comparison. There are generally two extraction methods: rule-based extraction according to box markers and extraction via LLMs. The latter is more flexible, which doesnt force the model to emit explicit box markers around the key answer, avoiding cumbersome format tuning and preserving the original user-friendly response format. We find that for simple academic questions (where the answer is usually single number) or single-category tasks (where the answer follows fixed-range format), its straightforward to design prompts that enable an LLM to extract the answer precisely. However, in our multimodal, open-domain RL setting, the diversity of questions and answers increases dramatically, making extraction significantly more complex with numerous corner cases. LLM-based extraction is often proved to be inaccurate, causing errors in the subsequent correctness judgment. Moreover, in some cases, the answer segment would loop or become excessively long, which is difficult or out-of-distribution for LLMs to extract the final answer, further undermining model-based extraction accuracy. To address these issues, we require the model during RLVR to explicitly mark the final answer with box tokens, and compare only the boxed content against the reference answer. It is worth noting that many prior works use the boxed{} label to denote final answers. However, when reference answers become complex (for example, the results of GUI agent tasks are expressed as complex function calls), boxed{} can be ambiguous and difficult to parse automatically. Therefore, we introduced special tokens into the vocabulary and instead mark answer spans as: <begin_of_box>{FINAL_ANSWER}<end_of_box>. Avoid reward hacking. coarse or incomplete reward design can lead the model to discover shortcuts for boosting its reward rather than truly improving its task performance. For example, in pilot experiments, we find that after more than 500 training iterations, an imprudently designed 10 verifier could be hacked as follows: for counting problem, the model would answer correct number between 0 and 10, and for relativity question about speed, it would answer velocity very close to the speed of light responses that successfully fool some LLM-based reward models and get high reward. Domain-specific reward system. The optimal verifier varies between multimodal subdomains and is tightly coupled to the taskfor instance, 43 and 43.0 are equivalent in math problem but not in an OCR context. To deliver more accurate rewards across all domains, we develop domain-specific reward system with the following features: Shared verification functions: Common checkssuch as format validation, boxed content extraction, and exact matchingare implemented as reusable functions to streamline development. Domain-specific modules: Each domain has its own submodule supporting complex verification logic, including branching workflows, functional evaluations, and model-based judgments driven by custom judge prompts and hyperparameters. Unit testing: To validate the reward system in each domain, we recommend defining unit tests that target that domains output distribution and iteratively refining the reward logic based on test results. For example, in chart QA, numeric answers are verified against relative tolerance threshold; for textual answers, we first check for an exact match and, if none is found, fall back to an LLM-based semantic equivalence assessment. We summarize some of our domain-specific verifiers below and will open-source this reward system to support further academic research. Table 1: Domain-specific reward design in the reward system of GLM-4.1V-Thinking. Rule Model Binary Reward design details Numeric: numeric matching via Sympy with tolerance; Others: exact matching or LLM judge. If physical units present, use LLM judgment; otherwise, similar to Math. If chemical units present, use LLM judgment; otherwise, similar to Math. Category Domain STEM Chart & Doc Math Physics Chemistry OCR Chart Long Doc Grounding Grounding Spatial Reasoning GUI Agent GUI Agent Counting Geo Guess Video Video Using edit distance, reward = 1 dedit(ans, gt) max(ans, gt) . Numeric: similar to Math (except Year); Textual: exact match or LLM judge. Semantic matching via LLM. Reward = #boxes with IoU > τ divided by total boxes. Exact matching with Sympy. Evaluates address accuracy and location name at various levels with continuous score. Action prediction: action+IoU; Grounding: IoU; QA: exact or semantic matching. Exact matching, or semantic matching via LLM judge. Beyond the domain-specific content checking above, we also build format-and-style-checking In format checking, reward system using both rule-based and model-based judgments. any response to non-verifiable data whose <answer> content contains the special markers <begin_of_box> <end_of_box> is penalized with minimal reward. For style checking, we similarly assign low reward if the <think> content or <answer> content includes extensive mixed Chinese and English segments or large blocks of repetitive text. In addition, text-based reward model evaluates the <answer> content for instruction compliance and fluency, encouraging outputs that adhere closely to the prompt while remaining coherent and logically rigorous. 5.3 Reinforcement Learning with Curriculum Sampling (RLCS) During RL training, we blend data from each multimodal domain in predetermined proportions, verify every domain using the reward system described in section 5.2, and optimize with GRPO [36] objective. To ensure that GLM-4.1V-Thinking reaches its full potential for each multimodal subdomain, we first run pilot experiments on each subdomain to evaluate its training difficulty, performanceimprovement potential and the required training tokens in the corresponding dataset. These insights guide the allocation of training data proportions in the RL process. One challenge is that, as RL training progresses, the models effective learning efficiency inevitably declines as its capabilities improve: many examples become too trivial to drive further learning. For 11 instance, in GRPO, rollout batch in which all samples are answered correctly yields no useful gradient. In our pilot experiment, over half of all prompts achieve accuracy over 90% after just 200 training steps. Meanwhile, rollout efficiency remains the primary bottleneck: the bulk of training time is consumed by rollouts. Consequently, it is essential to select the most informative, appropriately challenging problems for rollout. To maximize learning efficiency, we propose Reinforcement Learning with Curriculum Sampling (RLCS), which applies the insight of curriculum learning to online sampling. We employ an adaptive curriculum that continuously adjusts the difficulty of training samples to match the models evolving capabilities, ensuring each update is maximally informative. To achieve this, we evaluate sample difficulty both offline and online. Before training, we assess the inherent difficulty of every sample by running pass@k evaluations across the full dataset with several established vision-language models (or earlier RL checkpoints) and merging those quantitative scores with expert human difficulty annotations. This process yields set of fine-grained difficulty labels that partition our data into multiple tiers, from very easy through very hard. During training, we perform online difficulty grading. For each generated rollout, we record the pass@k outcome, map it to its corresponding difficulty tier, and merge these results with our offline labels. This online difficulty distribution also offers valuable insights into the models current performance. By leveraging these difficulty labels alongside the models subcategory performance, we continuously re-weight the sampling ratios of different difficulty categories at the granularity of training iterations. The core idea is to down-sample examples that are too trivial, as well as those that currently prove too challenging, and boost exposure to the mid-range difficulties where the model gains the most. Empirically, we observe that RLCS significantly accelerates model improvement and consistently leads to performance gains. Besides, we make particular efforts to improve the effectiveness, efficiency, and stability of RLCS. Here, we share as comprehensively as possible the lessons learned, insights gained, and methods developed in optimizing the training process. 5.3.1 Improving Effectiveness To improve the performance upper bound of multimodal reinforcement learning, we propose and incorporate the following enhancements: Larger batch size. When mixing multi-domain multimodal data during training, relatively large batch size is recommended to achieve higher performance ceiling in the long run. Dynamic sampling expansion via ratio EMA (Exponential Moving Average). In GRPO, when both entropy and KL losses are removed, rollout batch composed entirely of correct or entirely of incorrect samples provides no useful gradient. In other words, the all-correct/incorrect prompts reduce the usable batch size. As the proportion of these all-correct or all-incorrect batches grows or fluctuates, the effective batch size can vary wildly, degrading training stability. To address this, we perform rollouts with an intentional oversampling factor expansion_ratio, and then select the subset of samples whose difficulty is most balanced (i.e., with the numbers of correct and incorrect responses as close as possible). Concretely, for each rollout, we compute the expansion ratio as expansion_ratio = 1/(1 not_valid_sample_rate), where not_valid_sample_rate denotes the fraction of samples that are all-correct or all-incorrect in the last iteration. We then maintain an exponential moving average, expansion_ratio_ema, of this ratio and use it as the oversampling coefficient in the next iteration. Compared to [51], this method predetermines the total number of rollout samples, facilitating parallel sampling and balanced rollout allocation, which aligns more closely with the underlying large-scale RL infrastructure for greater efficiency. Force answering. When the thinking process becomes excessively long, it may be truncated by the rollout length limit. Because the model then fails to produce an answer, it is typically assigned reward of zero. However, such lengthy reasoning isnt necessarily incorrect for difficult questions, the generated part of an overlong thinking path can be perfectly valid. Truncating in this way not only wastes rollout budget but also injects noise into training. To address this, we enforce forced truncation by inserting </think> token followed by an <answer> token, which prompts the model to emit final answer and allows us to award fair reward for its reasoning. We found that this method encourages the model to learn how to provide an appropriate answer after any amount of thinking, facilitating dynamic control of the thinking budget at test time. 12 Discard KL loss. Compared to text-only models, vision-language models usually experience faster increase in KL divergence during reinforcement learning. However, when we apply KL loss to explicitly suppress this increase, the models capabilities are noticeably constrained. Therefore, we remove the KL loss. Clip-higher. In multimodal RL, similar to [51], increasing the upper clipping bound of the importance sampling ratio also proved useful for improving both off-policy performance and preventing excessive entropy collapse. We also share our observations and insights gained from large-scale cross-domain reinforcement learning. We believe that these insights can help improve the RL effectiveness in the future. The peak performance in the RL phase does not perfectly correlate with cold-start SFT models performance. For instance, in one of our experiments, increasing cold-start training from 1,000 to 2,000 steps boosts the post-cold-start performance by about two points on average. However, after RL, both checkpoints converge to nearly the same performance. In other words, higher score after cold-start does not guarantee greater RL potential; with proper training, RL can elevate base models of equal inherent potential to the same peak. Domain interference in RL is less pronounced than in SFT. Performance on underor untrained domains that are orthogonal to the RL training domain can be preserved quite well. Moreover, cross-domain generalization, where improvements in one domain transfer to others, is frequently observed. 5.3.2 Improving Stability To enhance training robustness and prevent collapse during RL, we identified several key factors that significantly impact stability throughout the training pipeline: The quality of cold-start SFT data has critical impact on training stability. Therefore, it is strongly recommended to maintain the cold-start data quality above certain threshold. For example, if the cold-start data contains large amount of meaningless thinking paths, the resulting model exhibits severe instability during RL training or even leads to training collapse. We found that incorporating an entropy loss to promote diversity could cause the model to produce garbled output, which eventually leads to training collapse, so we removed the entropy loss. During rollouts, using top-p = 1 instead of smaller value produces more stable RL training. Although some prior works advocate lowering top-p (e.g. to 0.9) to reduce variance and stabilize rollouts, we observe that this actually increases the risk of garbling over time. In contrast, setting top-p to 1 eliminates the garbled outputs that tend to appear in later iterations. We hypothesize that top-p = 1 ensures full vocabulary coverage, preventing the under-learning of rare tokens and thus maintaining clean output. While this choice does introduce more randomness during sampling, it ultimately enhances both stability and performance in the RL phase. We compared per-sample (averaging tokens loss within each sample, then averaging across samples) and per-token (averaging all tokens loss within batch, then averaging across batches) loss computation methods and observed no significant difference in mean reward, but per-sample loss computation yielded more stable training. Although format-based rewards during RL can help nudge outputs toward the correct structure, we strongly recommend that the model fully learn the required output format during the cold-start phase rather than depending on RL. In our experience, if the format errors frequently occur, the mixture of format and correctness reward may destabilize training. 5. Infrastructure To maximize RL training efficiency and performance, we extensively optimize our RL infrastructure, focusing on the following components: Load balancing of sequence lengths across DP ranks. Since the rollout length of each sample is unknown beforehand, some ranks may be assigned many extremely long sequences (e.g., video or long-document prompts, or difficult problems with long responses). Without balancing total sequence length across DP (Data Parallel) ranks, the slowest rank dictates the overall training speed. 13 To address this, after rollout and before assigning training samples to each DP rank, we balance both sequence length and compute load across ranks so that forward-backward passes per rank remain within tight range, thereby maximizing throughput. Intra-rank training with sequence packing and gradient accumulation. Because sample lengths vary unpredictably in RL, we cannot know in advance how many forward passes each DP rank will perform. We solve this by combining sequence packing with gradient accumulation: each optimization step comprises multiple micro-steps of forward and backward passes, where each micro-step packs several samples into fixed-length sequence of length context_length = 32K, padding unused positions. During training, we weight and average micro-step gradients by sample count, which is mathematically equivalent to computing gradients over the entire rollout batch at once. Sample packing and reorganization within DP ranks. Building on the previous strategy, training is decoupled from the number of samples per forward-backward pass. We therefore apply an efficient sample re-packing heuristic to complete all samples in as few micro-steps as possible. In practice, this optimization halves our forward-backward time. Dynamic sampling expansion via ratio EMA. To oversample and then select moderately difficult examples, we propose and implement dynamic sampling expansion via ratio EMA algorithm, which is detailed in section 5.3.1. Our approach precomputes the required sample count for parallel rollout, greatly improving efficiency."
        },
        {
            "title": "6 Evaluation",
            "content": "In this section, we present the evaluation details and results of GLM-4.1V-9B-Thinking. In 6.1, we show the comprehensive evaluation setting; the full quantitative comparison results are listed in 6.2. 6.1 Evaluation Setting Benchmarks. To comprehensively assess the capabilities of GLM-4.1V-9B-Thinking, we conduct evaluations across 28 public benchmarks, covering eight distinct categories: General VQA, STEM, OCR & Chart, Long Document, Visual Grounding, GUI Agents, Coding, and Video Understanding. The following benchmarks are used for evaluation: General VQA: MMBench-V1.1 [25], MMStar [5], BLINK [9], MUIRBENCH [43]; STEM: MMMU [52], MMMU-Pro [53], VideoMMMU [19], MathVista [27], WeMath [32], AI2D [22]; OCR & Chart: OCRBench [26], ChartQAPro [30], ChartMuseum [39] Long Document: MMLongBench-Doc [29]; Visual Grounding: RefCOCO-avg (val) [21]; GUI Agents: OSWorld [49], Android World [33], WebVoyager Some [15], Webquest-QA [44]; Coding: Design2Code [37], Flame-VLM-Code [1]; Video Understanding: VideoMME [8], MMVU [55], LVBench [46], MotionBench [16]; Setting. We use vLLM 1 as the backend for model inference. The maximum output length for each model response is set to 8,192 tokens. For visual input configuration, we set the maximum expected length for image inputs to 6,144 tokens, and 30,000 tokens for video benchmarks. The predicted answer is extracted as the string enclosed within special boxed tokens (<begin_of_box>...<end_of_box>), which we define as the models final output. For benchmarks that require answer extraction or scoring by language model, we consistently use GPT-4o (2024-11-20) [31] for this purpose. To ensure fairness, all modelsincluding GLM-4.1V-9BThinking and its open-source counterpartsare evaluated using the same toolchain, policies, and prompt templates. For each model, we enforce minimum successful request rate of 95% on every benchmark. Samples that fail due to generation errors or API issues are excluded from scoring, ensuring that final metrics reflect only valid outputs. 1https://github.com/vllm-project/vllm 14 Task Benchmark GLM-4.1V -9B-Thinking Qwen2.5-VL 7B InternVL3 9B Kimi-VL A3B-Thinking MiMo-VL 7B-RL Qwen2.5-VL 72B GPT-4o 2024-11General VQA STEM MMBench-V1.1-EN MMBench-V1.1-CN MMStar BLINK MUIRBENCH MMMU MMMU-Pro VideoMMMU AI2D MathVista WeMath ChartQAPro OCR & Chart ChartMuseum OCRBench Long Document MMLongBench-Doc Visual Grounding RefCOCO-avg (val) OSWorld AndroidWorld GUI Agents WebVoyageSom Coding Webquest-SingleQA Webquest-MultiQA Design2Code Flame-VLM-Code VideoMME (w/o) VideoMME (w/) Video Understanding MMVU LVBench MotionBench 85.8 84.7 72.9 65.1 74.7 68. 57.1 61.0 87.9 80.7 63.8 59. 48.8 84.2 42.4 87.4 14.9 41. 69.0 72.1 54.7 64.7 72.5 68. 73.6 59.4 45.1 59.0 82.7 81. 71.6* 79.4* 88.0 84.4* 80.1* 80.9* 70.2* 80.3* 86.7* 83.2* 63.9 45.7* 53.2* 58.6 38.3 47.4 83.8* 68. 31.0* 66.3 58.6 51.4 57.7 62.3* 69.3* 70.8 66.2* 53.5* 62.4 58.0* 66.4* 56.8* 64.8* 62.9* 69.7* 61. 66.7 42.1* 45.5* 53.1* - 84. 71.5 33.8 - 78.1* 71.3 36.0* 43.3 83.5 81.5 66.3 70.2 51. 60.2 69.1* 54.6* 61.2* 87.6* 84.8* 74.8 64.0* 46.0* 44.4* 38.0* 36.1* 44.1* 53.6* 46.7* 49.4* 27.2* 21.5* 29.3* 44.4* 39.6* 42.7* 84.5* 87. 78.7* 86.6 85.1* 81.1* 25.1* 20.4* 35.1 24.9* 35.2* 41.0* 87.1 1.9* 27.61 88.7 1.4* 1.9* - 8. - 89.6 1.9* 90.2 8.8 - 5.0 10.8* 35.0 34.52 14.1* 19.5* 1.8* 34.0* 40.4* 59.4* 53.5* 39.3* 56.8* 64.0* 60.5* 57.0* 39.4* 26.4* 42.0* 47.5* 52.1* 52.8 29.1* 15.3* 38.8* 28.7* 41.9* 35.3* 25.0* 11.3* 36.3* 65.1 71.6 50.1 45.3 - 66.7 68.9 67.8 72.6 - - - - - - 8.8* 67. 72.8* 52.4* 37.1* 48.4* 46.3* 75.0* 73.3 79.1 62.9 47.3 - 71. 77.2 61.4* 48.9 58.0* Table 2: Comparison of GLM-4.1V-9B-Thinking with other models on diverse visual-language benchmarks. Results marked with * were obtained under our evaluation framework to ensure consistency, while those marked with are reported by third-party sources. The best results among open-source models under 10B parameters are bolded. 1 Tested with predefined set of marks (SoM). 2 Tested with the input of screenshot and accessibility tree. 15 6.2 Comparison to Other Advanced MLLMs We compare GLM-4.1V-9B-Thinking against wide range of open source state-of-the-art MLLMs, including MiMo-VL [40], Kimi-VL [42], InternVL3 [56], and Qwen-VL series [3]. As shown in Table 2, GLM-4.1V-9B-Thinking sets new state-of-the-art across 23 out of 28 benchmarks among models under 10B parameters, demonstrating consistent and robust performance across wide spectrum of multimodal tasks. In the domain of General VQA, GLM-4.1V-9B-Thinking surpasses all competing open-source models under 10B on five widely-used benchmarks, covering both single-image and multi-image settings. This underscores the models strong general-purpose visual reasoning capabilities and its adeptness in both factual and inferential question answering across varying visual contexts. Within the STEM category, our model achieves the highest performance on challenging science and engineering benchmarks such as MMMU_Val, MMMU_Pro, VideoMMMU, and AI2D. These results indicate particularly strong capacity for structured and domain-specific reasoning. On mathematicscentric tasks such as MathVista and WeMath, GLM-4.1V-9B-Thinking remains competitive, although slightly behind MiMo-VL, suggesting potential room for enhancement in symbolic and arithmetic reasoning. In the domain of OCR & Chart, GLM-4.1V-9B-Thinking sets new state-of-the-art scores on both ChartQAPro and ChartMuseum, demonstrating strong capabilities in structured data extraction from plots and charts. On OCRBench, it performs competitively, slightly behind InternVL3 and MiMo-VL, indicating solid but improvable text recognition in natural images. For Long Document Understanding, GLM-4.1V-9B-Thinking outperforms all other models on MMLongBench, revealing strong capacity for reasoning over extended sequences, maintaining cross-page coherence, and handling complex document layouts. GLM-4.1V-9B-Thinking also establishes new state-of-the-art results in emerging tasks involving GUI Agents and Multimodal Coding. Its significant margin over competitors in these areas highlights its strong cross-modal reasoning ability and semantic alignment between visual interfaces and code representations. In the area of Video Understanding, GLM-4.1V-9B-Thinking demonstrates robust performance, leading on benchmarks such as VideoMME, MMVU, and MotionBench. These results emphasize its advanced spatio-temporal reasoning abilities, crucial for interpreting dynamic and multi-frame visual content. Meanwhile, for Visual Grounding, the model delivers solid results on RefCOCO, performing on par with or better than many larger-scale competitors. Remarkably, despite its relatively compact size, GLM-4.1V-9B-Thinking outperforms the much larger Qwen2.5-VL-72B model on 18 out of 28 benchmarks, including particularly challenging tasks such as MMStar, MUIRBENCH, MMMU_Pro, and ChartMuseum. This illustrates the superior efficiency and capability of our model, making it compelling choice for real-world deployment where computational resources are constrained. Furthermore, compared with the proprietary GPT-4o, GLM-4.1V-9B-Thinking achieves superior results on most of tasks (e.g., MMStar, MUIRBENCH, AI2D, MMMU-Pro, MathVista, and MotionBench), despite GPT-4os significantly larger scale and closed-source advantage. These findings emphasize that our model offers an excellent trade-off between performance and efficiency, making it practical and powerful solution for real-world deployment under resource constraints. 6.3 Investigating Cross-Domain Generalization in Reinforcement Learning While multi-domain RL successfully improves the overall performance, there is one remaining question: In the course of RL, is it possible for the various multimodal domains to generalize to and reinforce one another, or will they instead antagonize and interfere with each other? To explore this question, we selected four representative domains: STEM, OCR & Chart, Grounding, and GUI agents. While each domain relies on common toolkit of visual perception, reasoning, they stress different abilities. For example, Grounding demands fine-grained pixel-level perception; OCR & Chart emphasizes text recognition and the interpretation of abstract figures; STEM tasks center on complex visual reasoning; and GUI agents require blend of UI understanding, real-world knowledge, and dynamic decision-making. 16 Figure 6: Cross-domain generalization in reinforcement learning. We evaluate the SFT-stage models across five RL data settings: STEM, OCR & Chart, grounding, GUI agent, and combined Mix-all. Each model is tested on five benchmark suites corresponding to these domains. The values in the grid show the average performance improvement per domain (negative values indicate decline), and the cell colors are normalized within each domain. All experiments are conducted based on the SFT-stage checkpoint of GLM-4.1V-9B-Thinking. We compare 5 groups of RL training data: (1) STEM, (2) OCR & Chart, (3) Grounding, (4) GUI Agent, and (5) Mix-all which mixes all four data groups above with the RL training ratio of GLM-4.1V-9BThinking. We adopt the mix-all setup as our reference for training data volume: for each standalone RL experiment on given sub-domain, the number of samples processed is exactly the same as the number that sub-domain sees in the mix-all experiment. After training, we evaluate models on 5 categories of benchmarks, each category contains multiple benchmarks: (1) STEM, (2) OCR & Chart, (3) Grounding, (4) GUI Agent, and (5) General image VQA that does not have corresponding training set. The results are shown in Figure 6, where the numbers represent the improvement of the average group score compared to the initial SFT checkpoint. The results demonstrate robust cross-domain generalization and mutual facilitation in most domains: Training on one domain boosts performance in others. For example, reinforcement learning on STEM data not only improves STEM-specific skills but also enhances performance on visual grounding, GUI-agent interaction, and general VQA tasks. Similarly, training on OCR & Chart data yields gains in STEM, GUI-agent, and general VQA benchmarks. This crossdomain effect reveals that shared underlying capabilities such as visual understanding, text recognition, and reasoning can be co-activated and refined through single-domain RL signal. Intriguingly, RL applied exclusively to GUI-agent tasks produces improvements across all evaluated domains, indicating that GUI-agent challenges intrinsically require comprehensive mix of text recognition, visual grounding, and logical reasoning that transfers broadly. Joint training across domains yields even greater improvements in each. This synergy likely underpins GLM-4.1V-9B-Thinkings extraordinary performance. Among all configurations, the mix-all setting where the model is trained simultaneously on every domain delivers clear gains over any single-domain RL in three out of five areas (STEM, OCR & Chart, and general VQA). Notably, however, mixed-domain training does not improve grounding or GUIagent performance, suggesting that these domains may require more targeted or specialized multi-domain strategies and warrant further exploration. Interestingly, the cross-domain RL results also reveal how closely related these tasks are. For example, training on GUI-agent data markedly improves grounding performance and vice versa, highlighting their shared reliance on visual grounding capabilities. Likewise, OCR & Chart and GUI-agent tasks boost each others performance, reflecting their common demand for accurate text recognition."
        },
        {
            "title": "7 Discussion: Limitations and Future Work",
            "content": "GLM-4.1V-Thinking represents step toward general-purpose multimodal reasoning. Developed under reasoning-centric training framework that unifies pretraining, supervised fine-tuning, and reinforcement learning around shared objective, the model learns to reason across visual, textual, mathematical, scientific, and agentic domains. The resulting 9B-parameter model achieves strong performance across diverse benchmarks, often outperforming significantly larger models, such as those with over 70B parameters. We open-source GLM-4.1V-9B-Thinking to support further research in multimodal reasoning. Despite notable progress, several limitations remain. First, although RL enhances task completion rates, it does not consistently improve reasoning quality. In certain instances, the model produces correct answers but relies on incorrect reasoning steps. This issue arises because current reward models typically evaluate final outcomes without assessing intermediate reasoning steps. Consequently, flawed or hallucinated reasoning chains may inadvertently be reinforced if they produce correct answers. This emphasizes the importance of designing reward mechanisms that explicitly evaluate the reasoning process, not merely the outcome. Second, RL training can exhibit instability. Early experiments demonstrated that minor changes in setup could lead to substantial variations in reasoning depth or output style. Although advancements in later versions, such as improved reward design and enhanced cold-start data, have led to more stable training, the remaining sensitivity indicates deeper challenges in large-scale RL optimization. Further refinement of RL algorithms is needed to enhance consistency and robustness. Third, despite GLM-4.1V-9B-Thinkings strong performance across diverse tasks, it still struggles with complex scenarios. Situations involving cluttered images, occluded objects, or ambiguous visual details could lead the model to perceptual errors that undermine its reasoning capability. Under these conditions, the model may resort to guesswork or generic assumptions rather than engaging in grounded inference. This suggests that improvements in perceptual accuracy and logical reasoning must progress simultaneously, as these components are intricately interconnected. Looking ahead, key direction is to improve how we supervise and evaluate model reasoning. Future reward models should assess not only final answers but also intermediate reasoning steps, actively detecting hallucinations and flagging logical inconsistencies. Additionally, for tasks with subjective evaluations, it is crucial to explore strategies for preventing reward hacking, necessary step toward achieving general-purpose intelligence. We are also interested in the potential benefits of multimodal training for text-only reasoning tasks. For instance, understanding whether visual reasoning tasks, such as interpreting code in images, can enhance the performance of text-only coding tasks is promising research direction. Exploring how vision and language modalities mutually reinforce each other may lead to significant advancements in general reasoning capabilities. Finally, as model capabilities improve, evaluation frameworks must evolve correspondingly. Many current benchmarks are approaching saturation or fail to effectively identify critical errors, like hallucination in reasoning chains. Future benchmarks should be both more challenging and diagnostic, designed explicitly to detect more failure modes such as shortcut reasoning or hallucination. We hope GLM-4.1V-9B-Thinking can help inspire new standards for evaluating and improving general-purpose multimodal reasoning."
        },
        {
            "title": "8 Contribution",
            "content": "The contributors names are sorted in alphabetical order of the first name. Core Contributors Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan Contributors Aohan Zeng, Baoxu Wang, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing 18 Chen, Jiazheng Xu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Leyi Pan, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianle Gong, Wenkai Li, Wei Jia, Xin Lyu, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuxuan Zhang, Zhanxiao Du, Zhenyu Hou, Zhao Xue, Zhengxiao Du, Zihan Wang Project Leads Wenyi Hong, Wenmeng Yu, Xiaotao Gu Academic Advisors Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang"
        },
        {
            "title": "References",
            "content": "[1] Flame-code-vlm. https://github.com/Flame-Code-VLM/Flame-Code-VLM. [2] A. Awadalla, L. Xue, O. Lo, M. Shu, H. Lee, E. Guha, S. Shen, M. Awadalla, S. Savarese, C. Xiong, et al. Mint-1t: Scaling open-source multimodal data by 10x: multimodal dataset with one trillion tokens. Advances in Neural Information Processing Systems, 37:3680536828, 2024. [3] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] L. Blecher, G. Cucurull, T. Scialom, and R. Stojnic. Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418, 2023. [5] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [6] A. Fang, A. M. Jose, A. Jain, L. Schmidt, A. Toshev, and V. Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. [7] E. Fini, M. Shukor, X. Li, P. Dufter, M. Klein, D. Haldimann, S. Aitharaju, V. G. T. da Costa, L. Béthune, Z. Gan, et al. Multimodal autoregressive pre-training of large vision encoders. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 96419654, 2025. [8] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv:2405.21075, 2024. [9] X. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin, D. Roth, N. A. Smith, W.-C. Ma, and R. Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. [10] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36:2709227112, 2023. [11] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Rojas, G. Feng, H. Zhao, H. Lai, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. [12] J. Gu, X. Meng, G. Lu, L. Hou, N. Minzhe, X. Liang, L. Yao, R. Huang, W. Zhang, X. Jiang, et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems, 35:2641826431, 2022. [13] D. Guo, F. Wu, F. Zhu, F. Leng, G. Shi, H. Chen, H. Fan, J. Wang, J. Jiang, J. Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [14] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [15] H. He, W. Yao, K. Ma, W. Yu, Y. Dai, H. Zhang, Z. Lan, and D. Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. [16] W. Hong*, Y. Cheng*, Z. Yang*, W. Wang, L. Wang, X. Gu, S. Huang, Y. Dong, and J. Tang. Motionbench: Benchmarking and improving fine-grained video motion understanding for vision language models, 2024. [17] W. Hong, W. Wang, M. Ding, W. Yu, Q. Lv, Y. Wang, Y. Cheng, S. Huang, J. Ji, Z. Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. 20 [18] W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [19] K. Hu, P. Wu, F. Pu, W. Xiao, Y. Zhang, X. Yue, B. Li, and Z. Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. 2025. [20] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [21] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. [22] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [23] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [24] Q. Li, Z. Chen, W. Wang, W. Wang, S. Ye, Z. Jin, G. Chen, Y. He, Z. Gao, E. Cui, et al. Omnicorpus: unified multimodal corpus of 10 billion-level images interleaved with text. arXiv preprint arXiv:2406.08418, 2024. [25] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023. [26] Y. Liu, Z. Li, M. Huang, B. Yang, W. Yu, C. Li, X.-C. Yin, C.-L. Liu, L. Jin, and X. Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12), Dec. 2024. [27] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [28] Y. Ma, L. Du, X. Shen, S. Chen, P. Li, Q. Ren, L. Ma, Y. Dai, P. Liu, and J. Yan. One rl to see them all: Visual triple unified reinforcement learning, 2025. [29] Y. Ma, Y. Zang, L. Chen, M. Chen, Y. Jiao, X. Li, X. Lu, Z. Liu, Y. Ma, X. Dong, P. Zhang, L. Pan, Y.-G. Jiang, J. Wang, Y. Cao, and A. Sun. Mmlongbench-doc: Benchmarking longcontext document understanding with visualizations, 2024. [30] A. Masry, M. S. Islam, M. Ahmed, A. Bajaj, F. Kabir, A. Kartha, M. T. R. Laskar, M. Rahman, S. Rahman, M. Shahmohammadi, M. Thakkar, M. R. Parvez, E. Hoque, and S. Joty. Chartqapro: more diverse and challenging benchmark for chart question answering, 2025. [31] OpenAI. Gpt-4o. 2024. [32] R. Qiao, Q. Tan, G. Dong, M. Wu, C. Sun, X. Song, Z. GongQue, S. Lei, Z. Wei, M. Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. [33] C. Rawles, S. Clinckemaillie, Y. Chang, J. Waltz, G. Lau, M. Fair, A. Li, W. Bishop, W. Li, F. Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv:2405.14573, 2024. [34] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:25278 25294, 2022. 21 [35] R. Shao, S. S. Li, R. Xin, S. Geng, Y. Wang, S. Oh, S. S. Du, N. Lambert, S. Min, R. Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. [36] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [37] C. Si, Y. Zhang, Z. Yang, R. Liu, and D. Yang. Design2code: How far are we from automating front-end engineering?, 2024. [38] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. [39] L. Tang, G. Kim, X. Zhao, T. Lake, W. Ding, F. Yin, P. Singhal, M. Wadhwa, Z. L. Liu, Z. Sprague, et al. Chartmuseum: Testing visual reasoning capabilities of large vision-language models. arXiv preprint arXiv:2505.13444, 2025. [40] C. Team, Z. Yue, Z. Lin, Y. Song, W. Wang, S. Ren, S. Gu, S. Li, P. Li, L. Zhao, L. Li, K. Bao, H. Tian, H. Zhang, G. Wang, D. Zhu, Cici, C. He, B. Ye, B. Shen, Z. Zhang, Z. Jiang, Z. Zheng, Z. Song, Z. Luo, Y. Yu, Y. Wang, Y. Tian, Y. Tu, Y. Yan, Y. Huang, X. Wang, X. Xu, X. Song, X. Zhang, X. Yong, X. Zhang, X. Deng, W. Yang, W. Ma, W. Lv, W. Zhuang, W. Liu, S. Deng, S. Liu, S. Chen, S. Yu, S. Liu, S. Wang, R. Ma, Q. Wang, P. Wang, N. Chen, M. Zhu, K. Zhou, K. Zhou, K. Fang, J. Shi, J. Dong, J. Xiao, J. Xu, H. Liu, H. Xu, H. Qu, H. Zhao, H. Lv, G. Wang, D. Zhang, D. Zhang, D. Zhang, C. Ma, C. Liu, C. Cai, and B. Xia. Mimo-vl technical report, 2025. [41] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen, E. Pitler, T. Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan, B. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira, K. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka, B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez, M. Khalman, J. Sygnowski, A. Frechette, C. Smith, L. Culp, L. Proleev, Y. Luan, X. Chen, J. Lottes, N. Schucher, F. Lebron, A. Rrustemi, N. Clay, P. Crone, T. Kocisky, J. Zhao, B. Perz, D. Yu, H. Howard, A. Bloniarz, J. W. Rae, H. Lu, L. Sifre, M. Maggioni, F. Alcober, D. Garrette, M. Barnes, S. Thakoor, J. Austin, G. Barth-Maron, W. Wong, R. Joshi, R. Chaabouni, D. Fatiha, A. Ahuja, R. Liu, Y. Li, S. Cogan, J. Chen, C. Jia, C. Gu, Q. Zhang, J. Grimstad, A. J. Hartman, M. Chadwick, G. S. Tomar, X. Garcia, E. Senter, E. Taropa, T. S. Pillai, J. Devlin, M. Laskin, D. de Las Casas, D. Valter, C. Tao, L. Blanco, A. P. Badia, D. Reitter, M. Chen, J. Brennan, C. Rivera, S. Brin, S. Iqbal, G. Surita, J. Labanowski, A. Rao, S. Winkler, E. Parisotto, Y. Gu, K. Olszewska, Y. Zhang, R. Addanki, A. Miech, A. Louis, L. E. Shafey, D. Teplyashin, G. Brown, E. Catt, N. Attaluri, J. Balaguer, J. Xiang, P. Wang, Z. Ashwood, A. Briukhov, A. Webson, S. Ganapathy, S. Sanghavi, A. Kannan, M.-W. Chang, A. Stjerngren, J. Djolonga, Y. Sun, A. Bapna, M. Aitchison, P. Pejman, H. Michalewski, T. Yu, C. Wang, J. Love, J. Ahn, D. Bloxwich, K. Han, P. Humphreys, T. Sellam, J. Bradbury, V. Godbole, S. Samangooei, B. Damoc, A. Kaskasoli, S. M. R. Arnold, V. Vasudevan, S. Agrawal, J. Riesa, D. Lepikhin, R. Tanburn, S. Srinivasan, H. Lim, S. Hodkinson, P. Shyam, J. Ferret, S. Hand, A. Garg, T. L. Paine, J. Li, Y. Li, M. Giang, A. Neitz, Z. Abbas, S. York, M. Reid, E. Cole, A. Chowdhery, D. Das, D. Rogozinska, V. Nikolaev, P. Sprechmann, Z. Nado, L. Zilka, F. Prost, L. He, M. Monteiro, G. Mishra, C. Welty, J. Newlan, D. Jia, M. Allamanis, C. H. Hu, R. de Liedekerke, J. Gilmer, C. Saroufim, S. Rijhwani, S. Hou, D. Shrivastava, A. Baddepudi, A. Goldin, A. Ozturel, A. Cassirer, Y. Xu, D. Sohn, D. Sachan, R. K. Amplayo, C. Swanson, D. Petrova, S. Narayan, A. Guez, S. Brahma, J. Landon, M. Patel, R. Zhao, K. Villela, L. Wang, W. Jia, M. Rahtz, M. Giménez, L. Yeung, H. Lin, J. Keeling, P. Georgiev, D. Mincu, B. Wu, S. Haykal, R. Saputro, K. Vodrahalli, J. Qin, Z. Cankara, A. Sharma, N. Fernando, W. Hawkins, B. Neyshabur, S. Kim, A. Hutter, P. Agrawal, A. Castro-Ros, G. van den Driessche, T. Wang, F. Yang, S. yiin Chang, P. Komarek, R. McIlroy, M. Luˇcic, G. Zhang, W. Farhan, M. Sharman, P. Natsev, P. Michel, Y. Cheng, Y. Bansal, S. Qiao, K. Cao, S. Shakeri, C. Butterfield, J. Chung, P. K. Rubenstein, S. Agrawal, A. Mensch, K. Soparkar, K. Lenc, T. Chung, A. Pope, L. Maggiore, J. Kay, P. Jhakra, S. Wang, J. Maynez, M. Phuong, T. Tobin, A. Tacchetti, 22 M. Trebacz, K. Robinson, Y. Katariya, S. Riedel, P. Bailey, K. Xiao, N. Ghelani, L. Aroyo, A. Slone, N. Houlsby, X. Xiong, Z. Yang, E. Gribovskaya, J. Adler, M. Wirth, L. Lee, M. Li, T. Kagohara, J. Pavagadhi, S. Bridgers, A. Bortsova, S. Ghemawat, Z. Ahmed, T. Liu, R. Powell, V. Bolina, M. Iinuma, P. Zablotskaia, J. Besley, D.-W. Chung, T. Dozat, R. Comanescu, X. Si, J. Greer, G. Su, M. Polacek, R. L. Kaufman, S. Tokumine, H. Hu, E. Buchatskaya, Y. Miao, M. Elhawaty, A. Siddhant, N. Tomasev, J. Xing, C. Greer, H. Miller, S. Ashraf, A. Roy, Z. Zhang, A. Ma, A. Filos, M. Besta, R. Blevins, T. Klimenko, C.-K. Yeh, S. Changpinyo, J. Mu, O. Chang, M. Pajarskas, C. Muir, V. Cohen, C. L. Lan, K. Haridasan, A. Marathe, S. Hansen, S. Douglas, R. Samuel, M. Wang, S. Austin, C. Lan, J. Jiang, J. Chiu, J. A. Lorenzo, L. L. Sjösund, S. Cevey, Z. Gleicher, T. Avrahami, A. Boral, H. Srinivasan, V. Selo, R. May, K. Aisopos, L. Hussenot, L. B. Soares, K. Baumli, M. B. Chang, A. Recasens, B. Caine, A. Pritzel, F. Pavetic, F. Pardo, A. Gergely, J. Frye, V. Ramasesh, D. Horgan, K. Badola, N. Kassner, S. Roy, E. Dyer, V. Campos, A. Tomala, Y. Tang, D. E. Badawy, E. White, B. Mustafa, O. Lang, A. Jindal, S. Vikram, Z. Gong, S. Caelles, R. Hemsley, G. Thornton, F. Feng, W. Stokowiec, C. Zheng, P. Thacker, Çaglar Ünlü, Z. Zhang, M. Saleh, J. Svensson, M. Bileschi, P. Patil, A. Anand, R. Ring, K. Tsihlas, A. Vezer, M. Selvi, T. Shevlane, M. Rodriguez, T. Kwiatkowski, S. Daruki, K. Rong, A. Dafoe, N. FitzGerald, K. Gu-Lemberg, M. Khan, L. A. Hendricks, M. Pellat, V. Feinberg, J. Cobon-Kerr, T. Sainath, M. Rauh, S. H. Hashemi, R. Ives, Y. Hasson, Y. Li, E. Noland, Y. Cao, N. Byrd, L. Hou, Q. Wang, T. Sottiaux, M. Paganini, J.-B. Lespiau, A. Moufarek, S. Hassan, K. Shivakumar, J. van Amersfoort, A. Mandhane, P. Joshi, A. Goyal, M. Tung, A. Brock, H. Sheahan, V. Misra, C. Li, N. Rakicevic, M. Dehghani, F. Liu, S. Mittal, J. Oh, S. Noury, E. Sezener, F. Huot, M. Lamm, N. D. Cao, C. Chen, G. Elsayed, E. Chi, M. Mahdieh, I. Tenney, N. Hua, I. Petrychenko, P. Kane, D. Scandinaro, R. Jain, J. Uesato, R. Datta, A. Sadovsky, O. Bunyan, D. Rabiej, S. Wu, J. Zhang, G. Vasudevan, E. Leurent, M. Alnahlawi, I. Georgescu, N. Wei, I. Zheng, B. Chan, P. G. Rabinovitch, P. Stanczyk, Y. Zhang, D. Steiner, S. Naskar, M. Azzam, M. Johnson, A. Paszke, C.-C. Chiu, J. S. Elias, A. Mohiuddin, F. Muhammad, J. Miao, A. Lee, N. Vieillard, S. Potluri, J. Park, E. Davoodi, J. Zhang, J. Stanway, D. Garmon, A. Karmarkar, Z. Dong, J. Lee, A. Kumar, L. Zhou, J. Evens, W. Isaac, Z. Chen, J. Jia, A. Levskaya, Z. Zhu, C. Gorgolewski, P. Grabowski, Y. Mao, A. Magni, K. Yao, J. Snaider, N. Casagrande, P. Suganthan, E. Palmer, G. Irving, E. Loper, M. Faruqui, I. Arkatkar, N. Chen, I. Shafran, M. Fink, A. Castaño, I. Giannoumis, W. Kim, M. Rybinski, A. Sreevatsa, J. Prendki, D. Soergel, A. Goedeckemeyer, W. Gierke, M. Jafari, M. Gaba, J. Wiesner, D. G. Wright, Y. Wei, H. Vashisht, Y. Kulizhskaya, J. Hoover, M. Le, L. Li, C. Iwuanyanwu, L. Liu, K. Ramirez, A. Khorlin, A. Cui, T. LIN, M. Georgiev, M. Wu, R. Aguilar, K. Pallo, A. Chakladar, A. Repina, X. Wu, T. van der Weide, P. Ponnapalli, C. Kaplan, J. Simsa, S. Li, O. Dousse, F. Yang, J. Piper, N. Ie, M. Lui, R. Pasumarthi, N. Lintz, A. Vijayakumar, L. N. Thiet, D. Andor, P. Valenzuela, C. Paduraru, D. Peng, K. Lee, S. Zhang, S. Greene, D. D. Nguyen, P. Kurylowicz, S. Velury, S. Krause, C. Hardin, L. Dixon, L. Janzer, K. Choo, Z. Feng, B. Zhang, A. Singhal, T. Latkar, M. Zhang, Q. Le, E. A. Abellan, D. Du, D. McKinnon, N. Antropova, T. Bolukbasi, O. Keller, D. Reid, D. Finchelstein, M. A. Raad, R. Crocker, P. Hawkins, R. Dadashi, C. Gaffney, S. Lall, K. Franko, E. Filonov, A. Bulanova, R. Leblond, V. Yadav, S. Chung, H. Askham, L. C. Cobo, K. Xu, F. Fischer, J. Xu, C. Sorokin, C. Alberti, C.-C. Lin, C. Evans, H. Zhou, A. Dimitriev, H. Forbes, D. Banarse, Z. Tung, J. Liu, M. Omernick, C. Bishop, C. Kumar, R. Sterneck, R. Foley, R. Jain, S. Mishra, J. Xia, T. Bos, G. Cideron, E. Amid, F. Piccinno, X. Wang, P. Banzal, P. Gurita, H. Noga, P. Shah, D. J. Mankowitz, A. Polozov, N. Kushman, V. Krakovna, S. Brown, M. Bateni, D. Duan, V. Firoiu, M. Thotakuri, T. Natan, A. Mohananey, M. Geist, S. Mudgal, S. Girgin, H. Li, J. Ye, O. Roval, R. Tojo, M. Kwong, J. Lee-Thorp, C. Yew, Q. Yuan, S. Bagri, D. Sinopalnikov, S. Ramos, J. Mellor, A. Sharma, A. Severyn, J. Lai, K. Wu, H.-T. Cheng, D. Miller, N. Sonnerat, D. Vnukov, R. Greig, J. Beattie, E. Caveness, L. Bai, J. Eisenschlos, A. Korchemniy, T. Tsai, M. Jasarevic, W. Kong, P. Dao, Z. Zheng, F. Liu, F. Yang, R. Zhu, M. Geller, T. H. Teh, J. Sanmiya, E. Gladchenko, N. Trdin, A. Sozanschi, D. Toyama, E. Rosen, S. Tavakkol, L. Xue, C. Elkind, O. Woodman, J. Carpenter, G. Papamakarios, R. Kemp, S. Kafle, T. Grunina, R. Sinha, A. Talbert, A. Goyal, D. Wu, D. Owusu-Afriyie, C. Du, C. Thornton, J. Pont-Tuset, P. Narayana, J. Li, S. Fatehi, J. Wieting, O. Ajmeri, B. Uria, T. Zhu, Y. Ko, L. Knight, A. Héliou, N. Niu, S. Gu, C. Pang, D. Tran, Y. Li, N. Levine, A. Stolovich, N. Kalb, R. Santamaria-Fernandez, S. Goenka, W. Yustalim, R. Strudel, A. Elqursh, B. Lakshminarayanan, C. Deck, S. Upadhyay, H. Lee, M. Dusenberry, Z. Li, X. Wang, K. Levin, R. Hoffmann, D. Holtmann-Rice, O. Bachem, S. Yue, S. Arora, E. Malmi, D. Mirylenka, Q. Tan, C. Koh, S. H. Yeganeh, S. Põder, S. Zheng, F. Pongetti, 23 M. Tariq, Y. Sun, L. Ionita, M. Seyedhosseini, P. Tafti, R. Kotikalapudi, Z. Liu, A. Gulati, J. Liu, X. Ye, B. Chrzaszcz, L. Wang, N. Sethi, T. Li, B. Brown, S. Singh, W. Fan, A. Parisi, J. Stanton, C. Kuang, V. Koverkathu, C. A. Choquette-Choo, Y. Li, T. Lu, A. Ittycheriah, P. Shroff, P. Sun, M. Varadarajan, S. Bahargam, R. Willoughby, D. Gaddy, I. Dasgupta, G. Desjardins, M. Cornero, B. Robenek, B. Mittal, B. Albrecht, A. Shenoy, F. Moiseev, H. Jacobsson, A. Ghaffarkhah, M. Rivière, A. Walton, C. Crepy, A. Parrish, Y. Liu, Z. Zhou, C. Farabet, C. Radebaugh, P. Srinivasan, C. van der Salm, A. Fidjeland, S. Scellato, E. Latorre-Chimoto, H. Klimczak-Plucinska, D. Bridson, D. de Cesare, T. Hudson, P. Mendolicchio, L. Walker, A. Morris, I. Penchev, M. Mauger, A. Guseynov, A. Reid, S. Odoom, L. Loher, V. Cotruta, M. Yenugula, D. Grewe, A. Petrushkina, T. Duerig, A. Sanchez, S. Yadlowsky, A. Shen, A. Globerson, A. Kurzrok, L. Webb, S. Dua, D. Li, P. Lahoti, S. Bhupatiraju, D. Hurt, H. Qureshi, A. Agarwal, T. Shani, M. Eyal, A. Khare, S. R. Belle, L. Wang, C. Tekur, M. S. Kale, J. Wei, R. Sang, B. Saeta, T. Liechty, Y. Sun, Y. Zhao, S. Lee, P. Nayak, D. Fritz, M. R. Vuyyuru, J. Aslanides, N. Vyas, M. Wicke, X. Ma, T. Bilal, E. Eltyshev, D. Balle, N. Martin, H. Cate, J. Manyika, K. Amiri, Y. Kim, X. Xiong, K. Kang, F. Luisier, N. Tripuraneni, D. Madras, M. Guo, A. Waters, O. Wang, J. Ainslie, J. Baldridge, H. Zhang, G. Pruthi, J. Bauer, F. Yang, R. Mansour, J. Gelman, Y. Xu, G. Polovets, J. Liu, H. Cai, W. Chen, X. Sheng, E. Xue, S. Ozair, A. Yu, C. Angermueller, X. Li, W. Wang, J. Wiesinger, E. Koukoumidis, Y. Tian, A. Iyer, M. Gurumurthy, M. Goldenson, P. Shah, M. Blake, H. Yu, A. Urbanowicz, J. Palomaki, C. Fernando, K. Brooks, K. Durden, H. Mehta, N. Momchev, E. Rahimtoroghi, M. Georgaki, A. Raul, S. Ruder, M. Redshaw, J. Lee, K. Jalan, D. Li, G. Perng, B. Hechtman, P. Schuh, M. Nasr, M. Chen, K. Milan, V. Mikulik, T. Strohman, J. Franco, T. Green, D. Hassabis, K. Kavukcuoglu, J. Dean, and O. Vinyals. Gemini: family of highly capable multimodal models, 2023. [42] K. Team, A. Du, B. Yin, B. Xing, B. Qu, B. Wang, C. Chen, C. Zhang, C. Du, C. Wei, C. Wang, D. Zhang, D. Du, D. Wang, E. Yuan, E. Lu, F. Li, F. Sung, G. Wei, G. Lai, H. Zhu, H. Ding, H. Hu, H. Yang, H. Zhang, H. Wu, H. Yao, H. Lu, H. Wang, H. Gao, H. Zheng, J. Li, J. Su, J. Wang, J. Deng, J. Qiu, J. Xie, J. Wang, J. Liu, J. Yan, K. Ouyang, L. Chen, L. Sui, L. Yu, M. Dong, M. Dong, N. Xu, P. Cheng, Q. Gu, R. Zhou, S. Liu, S. Cao, T. Yu, T. Song, T. Bai, W. Song, W. He, W. Huang, W. Xu, X. Yuan, X. Yao, X. Wu, X. Li, X. Zu, X. Zhou, X. Wang, Y. Charles, Y. Zhong, Y. Li, Y. Hu, Y. Chen, Y. Wang, Y. Liu, Y. Miao, Y. Qin, Y. Chen, Y. Bao, Y. Wang, Y. Kang, Y. Liu, Y. Dong, Y. Du, Y. Wu, Y. Wang, Y. Yan, Z. Zhou, Z. Li, Z. Jiang, Z. Zhang, Z. Yang, Z. Huang, Z. Huang, Z. Zhao, Z. Chen, and Z. Lin. Kimi-vl technical report, 2025. [43] F. Wang, X. Fu, J. Y. Huang, Z. Li, Q. Liu, X. Liu, M. D. Ma, N. Xu, W. Zhou, K. Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. [44] M. Wang, S. Sunkara, G. Baechler, J. Lin, Y. Zhu, F. Zubach, L. Shu, and J. Chen. Webquest: benchmark for multimodal qa on web page sequences, 2024. [45] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [46] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, S. Huang, B. Xu, Y. Dong, M. Ding, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. [47] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. [48] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Proc. of Neural Information Processing Systems, 2022. [49] T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, J. H. Toh, Z. Cheng, D. Shin, F. Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2025. [50] H. Xu, S. Xie, X. E. Tan, P.-Y. Huang, R. Howes, V. Sharma, S.-W. Li, G. Ghosh, L. Zettlemoyer, and C. Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. [51] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, W. Dai, T. Fan, G. Liu, L. Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [52] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proc. of Computer Vision and Pattern Recognition, 2024. [53] X. Yue, T. Zheng, Y. Ni, Y. Wang, K. Zhang, S. Tong, Y. Sun, B. Yu, G. Zhang, H. Sun, Y. Su, W. Chen, and G. Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. [54] H. Zhang, P. Zhang, X. Hu, Y.-C. Chen, L. Li, X. Dai, L. Wang, L. Yuan, J.-N. Hwang, and J. Gao. Glipv2: Unifying localization and vision-language understanding. Proc. of Neural Information Processing Systems, 35:3606736080, 2022. [55] Y. Zhao, L. Xie, H. Zhang, G. Gan, Y. Long, Z. Hu, T. Hu, W. Chen, C. Li, J. Song, Z. Xu, C. Wang, W. Pan, Z. Shangguan, X. Tang, Z. Liang, Y. Liu, C. Zhao, and A. Cohan. Mmvu: Measuring expert-level multi-discipline video understanding, 2025. [56] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, H. Tian, Y. Duan, W. Su, J. Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [57] W. Zhu, J. Hessel, A. Awadalla, S. Y. Gadre, J. Dodge, A. Fang, Y. Yu, L. Schmidt, W. Y. Wang, and Y. Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. Advances in Neural Information Processing Systems, 36:89588974, 2023."
        },
        {
            "title": "A Qualitative Examples",
            "content": "In this section, we demonstrate the capabilities and advantages of our model through various qualitative examples. A.1 UI Code Generation Figure 7: case showing the ability of generating the front-end code from UI snapshot. 26 A.2 Video Description Figure 8: case showing the ability of giving detailed and precise description for video. 27 A.3 Video Description (in Chinese) Figure 9: case showing the ability of giving detailed and precise description for video. 28 A.4 Video QA Figure 10: case showing the ability of answering question related to video using perception, knowledge and reasoning. 29 A.5 GUI Agent Figure 11: case showing the ability of GUI recognition and operation. 30 A.6 Chart QA Figure 12: case showing the ability of chart understanding and question answering. 31 A.7 Geolocation Figure 13: case showing the ability of inferring the geographic position from picture. 32 A.8 OCR + Coding Figure 14: case showing the integrated ability of code recognition, debugging and correction. 33 A.9 Chemistry Problem Solving Figure 15: case showing the ability of solving Chemistry problem. 34 A.10 Math Problem Solving Figure 16: case showing the ability of solving complex Math problem."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "Zhipu AI"
    ]
}