{
    "paper_title": "Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model",
    "authors": [
        "Gregor Geigle",
        "Florian Schneider",
        "Carolin Holtermann",
        "Chris Biemann",
        "Radu Timofte",
        "Anne Lauscher",
        "Goran Glavaš"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language. Existing efforts mitigate these issues by adding multilingual training data, but do so in a largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages. In this work, we present a comprehensive investigation into the training strategies for massively multilingual LVLMs. First, we conduct a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pre-training as well as (3) instruction-tuning data. Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce a new benchmark for the task. Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50\\% of non-English data, to greatly improve multilingual performance while retaining strong English performance. We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding. Finally, we put all our findings together and train Centurio, a 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 2 2 1 5 0 . 1 0 5 2 : r Centurio: On Drivers of Multilingual Ability of Large Vision-Language Model Gregor Geigle12 Florian Schneider3 Carolin Holtermann4 Chris Biemann3 Radu Timofte2 Anne Lauscher4 Goran Glavaš1 1WüNLP, 2Computer Vision Lab, CAIDAS, University of Würzburg 3Language Technology Group, 4Data Science Group, University of Hamburg (gregor.geigleflorian.schneider-1)@uni-(wuerzburghamburg).de gregor-ge.github.io/Centurio"
        },
        {
            "title": "Abstract",
            "content": "Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language. Existing efforts mitigate these issues by adding multilingual training data, but do so in largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages. In this work, we present comprehensive investigation into the training strategies for massively multilingual LVLMs. First, we conduct series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pretraining as well as (3) instruction-tuning data. Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce new benchmark for the task. Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50% of non-English data, to greatly improve multilingual performance while retaining strong English performance. We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding. Finally, we put all our findings together and train Centurio, 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages."
        },
        {
            "title": "Introduction",
            "content": "Large Vision-Language Models (LVLMs) extend Large Language Models (LLMs) (Brown et al., 2020) to natively understand images as input (Li et al., 2023; Liu et al., 2023b). This leverages the impressive language generation and reasoning * Equal contribution. 1 abilities of recent LLMs (Llama Team, 2024; Yang et al., 2024) for vision-language tasks like image captioning or visual question answering. However, most models are trained with just English data (Liu et al., 2023a; Dai et al., 2023; Liu et al., 2024). This limits the access for speakers of other languages as the resulting models have several limitations even if the underlying LLMs exhibit multilingual capabilities: the models fail to understand non-English instructions (Schneider and Sitaram, 2024), struggle with non-English text content in images (Tang et al., 2024), and often fail to reply in the correct language, i.e., they have problems with language fidelity (Hinck et al., 2024). To ameliorate these issues, LVLMs need to be trained on multilingual data composition. As the amount of data one can train on is always limitedby time, computing resources, financial costs, or other constraintsan effective distribution of the data across different languages is paramount. Existing multilingual LVLM work has, however, given minimal consideration to this central question of optimal training data composition (e.g., Geigle et al., 2023a; Sun et al., 2024a; Maaz et al., 2024b). In this work, we comprehensively investigate the space of language distributions of LVLM training mixes, focusing on the presumed trade-off between the number of included languages and performance across languagesgrouped by the amount of data available for themunder fixed training budget. We train several models with different data compositions obtained by machine-translating high-quality English data and benchmark them across 13 downstream tasks covering 43 diverse languagesfrom low-resource languages like Igbo to high-resource languages like German. We focus on four research questions, each building on the previous one, designed to identify an optimal multilingual training mix: RQ1: What is the optimal number of training languages? RQ2 & RQ3: What is the optimal distribution of data across languages Figure 1: Exploring drivers of multilingual ability: (1) Languages in the training data; (2) Distribution of languages in the training data; (3) Incorporating multilingual OCR samples to understand non-English text in images. in (RQ3) pre-training data and (RQ2) instructiontuning? RQ4: How to improve the understanding of multilingual text in images? To measure progress for RQ4, we introduce SMPQA (Synthetic Multilingual Plot Question Answering), novel dataset for testing multilingual OCR capabilities, spanning 11 languages and 7 scripts. Our findings are encouraging, albeit surprising. 1. We do not observe the infamous curse of multilinguality (Conneau et al., 2020; Pfeiffer et al., 2022b) and find that gradually increasing the number of languages incurs only negligible performance tax: scaling from 7 to 100 languages greatly improves performance for languages newly added to the training data, especially with respect to language fidelity, while largely retaining performance levels for all previously added languages. 2. We find that exposure to language matters more than increasing the training portion of the language or, in particular, that the majority of the training data can still be in English, which lowers the cost of acquiring training data in other languages (e.g., via machine translation). Concretely, we find that turning 25 to 50% of training data multilingual yields strong performance, with more data sometimes even degrading performance; in pre-training, having larger share of multilingual data is more beneficial, but it also saturates after 50%. 3. We obtain mixed results for text-in-image problems: while incorporating (synthetic) OCR data with 5k samples per language rapidly boosts the performance for Latin-script languages, the same does not hold for languages with other scripts. Finally, to demonstrate the practical impact of our findings, we train Centurio, massively multilingual LVLM with 100 languages, following what we found to be an optimal data distribution across Centurio languages for both training stages. achieves state-of-the-art results over 14 tasks , matching the performance of popular multilingual open-weight LVLMs like Qwen2-VL (Wang et al., 2024b), InternVL 2.5 (Chen et al., 2024d) and Pangea (Yue et al., 2024a) on English and other high-resource languages while outperforming them on low(er)-resource languages."
        },
        {
            "title": "2 Drivers of Multilingual Ability",
            "content": "The design space for training (multilingual) LVLMs is extensive, ranging from the choice of the image encoder and the alignment module between the image encoder and LLM to the selection of training data. (Karamcheti et al., 2024; Laurençon et al., 2024a; Tong et al., 2024). Exhaustively searching through the cross-product of all choices is not feasible. In this work, we focus on extensive evaluation of language distributions of training data in both pre-training and instructiontuning. Intuitively, this should be major factor affecting the multilingual ability of an LVLM. Figure 1 illustrates the scope of our analysis. We keep adding groups of languagesfrom highestto lowest-resourced, following the resourceness tiers of (Joshi et al., 2020)into the training mix while keeping the data size fixed. Besides the number of languages, our main focus is on the division of the training budget between English and all other languages. Finally, we posit that, besides understanding instructions and generating outputs in different languages, truly multilingual LVLMs must be able to understand multilingual text in images. We thus pay special attention to training adaptions for multilingual text-in-image problems."
        },
        {
            "title": "2.1 Experimental Setup",
            "content": "Architecture. For our experiments, we adopt the popular LLaVA architecture (Liu et al., 2023b,a): An image encoder (SigLIP SO400/384 (Zhai et al., 2023)) encodes images into sequence of visual tokens which are projected with 2-layer MLP 2 into the LLM embedding space; these tokens are then concatenated to the text tokens and fed to the LLM. We choose Phi 3.5 (Abdin et al., 2024b) as our LLM because it exhibits strong multilingual performance while its small size (3.8B parameters) allows for more computationally efficient experimentation. To show that our findings generalize to other LLMs, we repeat subset of the analysis experiments with Llama 3 (8B) (Llama Team, 2024) as the LLM backbone (see Appendix D.1). Training Setup. Following previous work (Liu et al., 2023a; Tong et al., 2024), we split the training into two phases : 1) pre-training: the model is trained only on image captioning, with dense image captions; 2) instruction tuning: the model is trained on mix of diverse vision-language tasks using several public datasets. While pre-training benefits downstream performance, it is not strictly necessary for the LVLM to perform well on the downstream tasks (Karamcheti et al., 2024). To reduce the computational cost of our analysis (i.e., to avoid coupling each language distribution over pretraining data with every language distribution of instruction-tuning data), we skip pre-training while searching for an optimal language distribution for instruction tuning. Then, with instruction-tuning data fixed, we search for an optimal language distribution for pre-training data. In both stages, we freeze the image encoder and only update the MLP and LLM (with LoRA (Hu et al., 2022)) weights. We provide further details in Appendix A. Training Data. Our controlled experiments require comparable data over wide range of languages. Existing multilingual datasets, available only for some tasks and in handful of languages1 thus do not meet our needs. Instead, we resort to machine translation (MT) and use the open NLLB model (Costa-jussà et al., 2022)2 to translate readily available English datasets.3 While MT results in lower data quality, especially for lower-resource languages, it is the only option to obtain multilingual vision-language training data at scale. Moreover, gains from low-quality MT data are guaranteed to be met or even surpassed with higher-quality translations (e.g., commercial MT or human translators). Our instruction-tuning data is adapted from LLaVA-Next (Liu et al., 2024) and contains 0.77M 1See, for example, datasets collected by Yue et al. (2024a) 2nllb-200-distilled-1.3B 3We do not translate text-in-image datasets as that would result in mismatches between the instruction/output language and the English text in the image. samples. For pre-training, we use the 1.3M dense captions from ShareGPT4v (Chen et al., 2024b). We provide further details in Appendix B. Evaluation. We curate an extensive test suite of 13 tasks covering 43 languages to assess the multilingual abilities of our models. Following Joshi et al. (2020), we cluster the tested languages into five tiers, with T5 encompassing the high-resource languages (e.g., German, Chinese) and T1 extremely low-resource languages (e.g., Maori, Telugu). The tasks contained in our test suite are twofold: (1) discriminative tasks with questions that require binary (yes/no) or multiple-choice answers and (2) open generation tasks, where the models need to generate output in the target language (e.g., an image caption or free-form answer). Generative tasks additionally evaluate models language fidelity, i.e., the ability to generate the answer in the language of the instruction. The full list of evaluation tasks and languages, along with further details, is in Appendix C. We report the results for language tiers (T1T5), averaging the scores over all tasks and all tier languages.4 We separately report English performance and exclude it from T5."
        },
        {
            "title": "2.2 RQ1: Number of Training Languages",
            "content": "We first investigate on how many languages to actually train with: does training on few high-resource languages and (zero-shot) cross-lingual transfer to unseen languages suffice, as suggested, e.g., by Shaham et al. (2024a); Chen et al. (2024c); Kew et al. (2023), or do we need to explicitly include each targeted languages? Conversely, does training with more languages harm the per-language performance, with smaller portion of the training data now allocated to each language? Setup. We focus on the instruction-tuning step: 50% of the data is kept in English5, while the other 50% split between other languages equally, i.e., each language gets 50 % of the data budget. We gradually increase , starting with the highestresource tier (T5) and then including tiers of lowerresource languages (T4 to T1), one at time. This results in the following setups: T5 (N = 6), T5T4 (N = 24), T5-T3 (N = 52), T5-T2 (N = 69), and finally L100 (N = 99). In L100, in addition to languages from T5-T2, we include T1 languages 4While tasks use different measures, all are on the 0-100% scale, so no task skews the average. 5More specifically, 50% of the 80% of non-text-in-image data, which is excluded from translation. 3 Train Lang. T1 T2 T3 T5 en English % T1 All tasks English T5 T5-4 T5-3 T5-2 L100 14.4 16.5 17.4 17.7 17.0 19. 30.4 31.0 30.6 31.4 34.5 32.6 24.4 26.3 27.9 32.1 30.0 30.7 23.6 26.7 29.6 29.0 28.2 28.9 Tasks unaffected by language fidelity English T5 T5-T4 T5-T3 T5-T2 L100 33.0 35.3 35.8 35.9 35.2 36. 32.5 33.2 32.6 33.6 36.5 34.3 36.3 36.4 37.8 40.5 38.5 39.1 38.5 38.7 40.1 39.7 39.5 39.8 28.5 34.0 33.5 34.1 33.4 34.4 42.9 42.4 42.2 42.6 42.8 42.7 53.6 53.7 51.5 52.7 54.1 52. 55.7 56.0 55.7 56.3 55.5 54.6 1 10 25 50 75 90 19.1 18.1 19.7 19.3 18.5 15.9 T2 30.3 32.4 35.5 32.6 31.5 31.2 28.8 29.4 29.9 30.7 30.7 27.6 T4 27.1 27.4 27.9 28.9 28.4 26.9 T5 31.7 32.5 33.0 34.4 34.6 34.1 en 48.9 50.1 50.3 52.6 54.1 54.8 Table 2: RQ2 (2.3) results for models trained with different ratios of English to multilingual data in the instruction-tuning phase. Scores are averaged over results from all tasks grouped by language tier. (a) Scores are averaged over results from all tasks grouped by language tier. The performance on the following tasks is affected by language fidelity: XM3600, MaXM, MTVQA. Appendix confirms these trends observed with Phi 3.5: in fact, we see even larger gains over all tasks when training with more languages. Train Lang. English T5 T5-T4 T5-T3 T5-T2 T1 0.2 39.1 61.8 72.9 68.5 72.9 T2 0.2 36.1 84.6 84.4 99.0 98.2 T3 0.1 82.2 87.5 98.2 97.9 95. T4 2.4 83.9 99.2 95.2 98.4 97.8 T5 6.2 99.1 98.4 97.9 98.1 98.2 en 100.0 100.0 100.0 100.0 100.0 100. (b) Average language fidelity on XM3600 in %. Table 1: RQ1 (2.2) results for models trained with different sets of languages. We emphasize the best and second-best result in each column. to cover XM3600 (Thapliyal et al., 2022) and otherwise randomly to reach 99 languages. Results. Table 1 summarizes the results. Expectedly, we find that including language (tier) in instruction-tuning improves their performance (Table 1a, top half). Nevertheless, the (negative) effect of adding new languages on performance of previously included languages is negligible, if at all present. This makes training massively multilingual LVLMs feasible with only minor performance drawbacks for any given language. In-language training leads to dramatic improvements in language fidelity (i.e., the model producing the output in the correct language), as shown in Table 1b. Interestingly, the more multilingual the training, the larger the fidelity gains also for languages not included in training; explicit in-language training, expectedly, then further improves fidelity for any given language (see Table 27 in the Appendix for detailed per language results). Even when excluding tasks where language fidelity plays role (Table 1a bottom), we observe the same trends: steady improvements from in-language training, with negligible (if any) performance drops for other languages. subset of experiments with Llama 3 (setups: English, T5, and L100) in Table 13 in the"
        },
        {
            "title": "2.3 RQ2: Language Distribution in",
            "content": "Instruction-Tuning RQ1 experiments show that massively multilingual instruction-tuning data is beneficial across the board. We now analyze how much of the training data should be multilingual. On the one hand, intuitively, increasing the non-English portion of the training data budget could then lead to further gains. On the other hand, the gains from more multilingual training are, at some point, likely to be offset by the fact that we are adding noisy (MT-obtained) data at the expense of clean (English) data. Setup. We opt for the full set of 100 languages (L100) in this experiment due to their robust multilingual performance. However, we adjust the language distribution by keeping E% of the data budget in English and splitting the remaining 100 E% equally across the other 99 languages6. We consider the following six setups: {1, 10, 25, 50, 75, 90}. Results. We present the results in Table 2. We observe peak performance for all language tiers when between 25% and 75% of the training data is in English. For some tasks (e.g., XM3600, MaXM, BINMC), we see weaker performance with more English data, while for others (e.g., MTVQA, xGQA, MaRVL) more multilingual data leads to slight performance drops (see per-task results in F.1). Overall, lower-resource languages benefit from more multilingual data and, conversely, higher-resource languages benefit from more English data. However, this is in part also consequence of language coverage across tasks: XM3600 and BINMC profit from more multilingual training mix; 6We observed no benefits from an unequal allocation that up-samples low(er)-resource languages; see D.2) 4 English % No pre-training 100 50 1 T1 19. 19.3 22.8 22.7 T2 32.6 33.3 39.5 38.9 T3 30. 32.1 33.8 33.7 T4 28.9 29.4 30.8 31.2 T5 34. 34.5 35.7 35.4 en 52.6 55.2 54.9 55.1 Table 3: RQ3 (2.4) results with different English-tomultilingual ratios (L100) for pre-training. All variants are identically instruction-tuned (L100, 50% En.). at the same time, they are the tasks that encompass the most low(er)-resource languages. Results obtained with the Llama 3 backbone (see Table 14 in the Appendix) follow the same pattern: we observe the best performance in T1 and T2 with = 10; and for T5 and English with = 90; = 50 yields the best results overall, considering all tiers. Our findings align with concurrent work by Yue et al. (2024a), who found that anywhere between 20 and 80% of English data yields good global performance. Following these results, we choose = 50 as robust value for the training."
        },
        {
            "title": "2.4 RQ3: Language Distribution in",
            "content": "Pre-Training As hinted by (Liu et al., 2023b, 2024) and explicitly demonstrated by Tong et al. (2024), pre-training on image-caption pairs improves the LVLMs performance. We therefore, after identifying an effective distribution of instruction-tuning data, next explore the effect of different distributions of pre-training data across languages. Specifically, we test if balancing out the English and multilingual portions delivers better performance than unbalanced distributions, that assign more training budget to English or the multilingual mix, respectively. Setup. In these experiments, we fix the instructiontuning mix to L100 with EIT = 50% of data in English, which we found in the previous section to produce overall most balanced results. For the pre-training data mix, we select the same 100 languages, varying the portion of English imagecaption pairs, EPT {100%, 50%, 1%}; as in instruction-tuning, the non-English data budget is equally distributed across the other 99 languages. Results. Scores in Table 3 reveal that while English-only pre-training yields downstream benefits on English tasks, it has largely negligible effect on other languages. The multilingual mixes substantially improve the performance for virtually all language tiers, with gains being the most prominent for lowest-resource languages from T2 and T1. In contrast to instruction-tuning, very low proportion of clean English data does not result in tangible performance degradation, but it generally does not improve the multilingual performance either. We thus select EPT = 50% as the optimal choice for subsequent experiments. Experiments with Llama 3, with 1% and 100% of English data (see Table 15 in the Appendix) support this finding that having highly multilingual pre-training benefits multilingual downstream performance."
        },
        {
            "title": "2.5 RQ4: Improving on Multilingual",
            "content": "Text-in-Image Tasks Finally, we focus on the models multilingual understanding of text in images and how to improve it. Unlike tasks based on natural images, text-in-image tasks cannot be translated trivially from English: even if the prompt and output text are translated, the text in the image remains in English. Because of this, we test how synthetic multilingual OCR data, which can be generated at scale in any number of languages, can help improve performance. Evaluation. To this end, we introduce SMPQA (Synthetic Multilingual Plot QA) new multilingual evaluation dataset, which focuses on two fundamental skills required in text-in-image tasks: 1) reading (and outputting) the text from an image and 2) grounding the input text (given as part of the prompt) to the corresponding text in the image (via balanced yes/no questions, e.g., Is the bar with label $Label the largest?). We provide further details on the construction and examples in Appendix C.5.7 We construct SMPQA to cover (i) 5 Latin-script languages, one from each tier, and (ii) 6 major languages with different non-Latin scripts. Setup. We generate multilingual synthetic text-inimage data for training following the Synthdog approach (Kim et al., 2022) (see B.3 for details). We again adopt the training setup L100 with 50% English data, both in pre-training and fine-tuning, now adding 500k Synthdog samples to pre-training and subset of 50k instances to the instruction-tuning mix. As before, we select {100, 50, 1}% English samples, distributing the rest of the budget equally over the other 99 languages. We test an additional Latin-down distribution: we double the budget allocated to 32 non-Latin-script languages 7While MTVQA and M3Exam also require OCR capabilities, they also require input image resolution that is far greater than what we use in our experiments (384px); SMPQA uses bigger letters, making performance effects from multilingual training on text-in-image understanding easier to measure. Setup SMPQA Ground en Latin other SMPQA Read en Latin other No pre-training No OCR 100% Eng. 50% Eng. 50% (frozen) 1% Eng. Latin down 69.6 76.1 78.4 81.2 76.1 81.0 78.9 67.2 73.0 74.7 76.7 70.8 78.3 74. 51.9 55.3 57.9 60.0 56.3 64.1 59.5 33.4 41.8 55.8 53.8 47.2 54.8 54.6 12.8 23.1 39.9 41.8 34.1 43.5 41. 0.1 0.2 3.9 7.1 3.5 8.0 9.9 Table 4: RQ4 (2.5) results of models trained with additional synthetic OCR data on SMPQA for English, Latin-script languages, and languages with other scripts. No pre-training: from Table 2; No OCR: from Table 3; frozen: image encoder frozen; N% Eng.: % of OCR data is English, rest uniform distributed over L100 languages; Latin down: 2.5k samples for all Latin-script languages, 10k samples for others. and cut the training budget for Latin-script languages (other than English) in half. Importantly, in these experiments we unfreeze the image encoder and fine-tune its parameters as well. Results. Table 4 summarizes the results. The models from prior experiments, No pre-training and No OCR, succeed for English and other Latin-script languages but utterly fail on non-Latin scripts with near-random performance. We note that the model with the pre-training step (without the additional OCR data) already performs better than the model trained just via instruction-tuning; this is likely due to the presence of images with text coupled with captions that explicitly mention this text. Training with synthetic data greatly improves the performance across all languages even if all of the OCR data is in English (100% Eng.). Nonetheless, using multilingual synthetic OCR data is very effective and, importantly, does not degrade English SMPQA performance even if English constitutes only 1% of the training data. We note that unfreezing and training the image encoder is critical for optimal performance in all scripts. Despite all this, we still observe large performance gap between Latinand non-Latin-script languages, even if we skew the training budget towards the non-Latin scripts (Latin-down). We hypothesize that orders of magnitude more text-in-image training data for other scripts are required for adequate performance.8 Centurio: Applying Lessons Learned Our answers to RQ1RQ4 (see 2) point to the feasibility of training massively multilingual LVLMs 8Concurrently, in preliminary exploration of text-inimage capabilities, Yue et al. (2024a) noted steady gains with 50k samples per language but also observed worse performance for non-Latin-script languages. 6 supporting 100 languages with sweet spot of roughly 50% of the English data being MTtranslated to the languages covered. For improving multilingual OCR capabilities, training on largescale synthetic data with an unfrozen image encoder has proven effective. Demonstrating the practicability of our findings, we now train state-ofthe-art multilingual LVLMs applying our lessons learned, which we call Centurio. We briefly describe further design choices below."
        },
        {
            "title": "3.1 Design Choices",
            "content": "Text Encoder. The choice of the LLM greatly impacts multilingual performance. We benchmark several LLMs (with 7-9B parameters) following the evaluation setup described in 2 for L100 languages and translations for 50% of the English instruct data to find candidates for Centurio (details in Appendix D.3). The best performances where obtained with Aya-Expanse (Dang et al., 2024) and Qwen 2.5 (Yang et al., 2024) as backbones. Image Tiling and Projection. Image tiling methods (Lin et al., 2024; Liu et al., 2024) increase the image resolution by concatenating encodings of non-overlapping tiles of an input image together, which significantly helps with reading small text in images. However, they also greatly increase the input length: 22 tiling would yield 3,645 tokens per image with our model.9 Instead, we adopt the method by Shi et al. (2024), which concatenates the tokens of the whole image and the tiles along the feature dimension before projection by the MLP. This gives an efficient trade-off between computing costthe number of tokens stays constantand performance gains for fine-grained content. Training Data. We increase the amount of the pretraining and instruct tuning data to further improve performance beyond our analysis setup. For pretraining, we add the 0.7M ALLaVA captions (Chen et al., 2024a) to the ShareGPT-4V captions and we use all synthetic OCR data generated in 2.5 (1.16M total: 500k English, 5k for Latin-script language, 10k for other scripts). For instructiontuning, we incorporate additional datasets from the Cambrian collection (Tong et al., 2024) along with several text-only instruction-tuning datasets (see Appendix B.2 for list). We translate the data to the L100 50% En. setup, excluding text-heavy datasets and others that are problematic for MT. 9The whole image plus four tiles, each with 729 tokens. Parrot PALO 7B PALO 13B Llama-Vision 3.2 11B Maya Pixtral 12B Phi 3.5 Vision Qwen2VL 2B MiniCPM 2.6 InternVL 2.5 4B InternVL 2.5 8B Qwen2VL 7B Pangea Centurio Aya Centurio Qwen Parrot PALO 7B PALO 13B Llama-Vision 3.2 11B Maya Pixtral 12B Phi 3.5 Vision Qwen2VL 2B MiniCPM 2.6 InternVL 2.5 4B InternVL 2.5 8B Qwen2VL 7B Pangea Centurio Aya Centurio Qwen AVG. 25.8 28.7 29.9 *32.3 33.4 38.1 39.5 41.2 41.7 45.3 47.4 47.7 48.2 48.5 51.6 XM en mul MTfid. VQA SMPQA G. en mul SMPQA N. M3Exam en mul en mul xMMMU en mul VQA C5.6 65.9 67.3 35.9 55.9 26.5 32.3 68.8 87.5 38.9 38.3 50.3 70.1 78.4 79.1 0.4 13.5 17.0 7.2 14.6 22.1 6.3 5.2 14.2 17.5 15.7 24.6 34.6 39.2 34.4 25.0 72.0 60.1 33.3 65.7 96.8 40.8 13.2 92.3 91.0 91.1 90.0 87. 95.7 95.2 2.0 5.8 6.3 15.2 5.3 14.1 11.1 19.0 16.1 25.1 25.0 23.2 19.3 11.1 11.9 51.0 55.5 54.0 91.1 51.4 91.1 92.2 85.0 89.0 87.0 91.0 91.2 87.2 83.1 84.8 49.9 52.8 51.5 84.8 50.9 71.0 79.4 83.5 74.3 78.3 79.2 90.9 72. 74.2 76.1 0.0 22.4 25.6 58.4 14.6 85.0 84.8 68.8 80.8 77.8 80.6 85.0 72.0 60.0 65.2 0.0 2.7 4.0 22.8 1.8 35.9 35.9 47.4 39.3 47.5 48.2 64.9 23.8 30.1 31.7 46.6 41.0 45.2 49.2 49.4 56.3 47.9 55.0 63.2 67.0 56.1 58. 53.0 61.2 36.2 29.1 28.3 36.3 33.7 40.7 40.5 48.2 50.3 53.3 49.7 45.5 41.2 46.9 35.3 31.8 32.4 37.9 30.3 41.7 36.8 39.1 49.2 50.7 43.0 43.1 37.6 46.4 32.4 30.9 28. 41.1 37.1 39.6 38.8 39.8 33.5 40.9 33.6 34.1 48.1 48.6 37.6 55.2 33.3 26.2 37.4 35.5 36.5 42.7 45.2 40.7 42.0 37.2 43.0 49.4 52.9 MAXM en mul xGQA en mul BIN-MC en mul XVNLI en mul MaRVL en mul VGR en mul VLOD en mul 28.2 54.0 51.7 0.0 55.4 59.4 43.6 53.7 53.4 46.0 45.6 54.7 61.4 55.7 60.1 3.6 22.5 33.1 4.7 17.3 43.4 17.9 26.5 22.3 42.5 38.2 31.2 55.0 49.3 47.7 37.7 59.1 58.0 39.3 58.2 59.9 65.2 60.5 57.9 63.6 63.4 62.5 64.6 59.1 60. 21.2 36.6 27.8 27.6 49.1 3.8 38.0 38.2 45.7 28.0 32.0 49.3 60.4 53.2 54.8 30.5 58.7 61.4 75.6 54.0 71.0 63.1 78.2 72.6 68.4 70.3 80.7 70.3 69.7 72.7 25.7 38.6 41.1 50.8 43.2 54.2 36.8 47.2 47.4 45.4 44.2 57.5 52.1 54.7 56. 28.7 58.0 56.6 50.1 60.9 58.9 61.9 71.9 69.0 73.5 62.1 69.0 65.0 75.4 31.4 53.4 53.6 43.9 52.7 53.3 56.2 65.4 58.7 66.4 59.6 65.2 62.4 70.2 63.5 62.7 63.8 60.3 67.7 73.4 67.9 70.2 74.9 83.0 69.8 75.8 85.0 89. 55.1 24.1 33.1 56.3 60.7 46.4 55.9 57.9 59.0 63.3 60.2 70.5 77.9 81.7 59.2 48.3 63.3 46.7 55.8 81.7 61.7 52.5 72.5 87.5 60.0 69.2 82.5 87.5 52.9 25.6 26.2 42.3 47.7 50.3 50.5 49.1 49.7 51.6 52.9 58.9 66.8 73. 0.0 5.8 2.5 20.0 9.2 45.8 22.5 9.2 24.2 57.5 5.8 0.0 12.5 28.3 0.0 6.8 4.9 20.1 12.4 31.5 20.4 14.6 21.0 29.0 13.2 6.7 20.7 27.0 Table 5: Comparison of Centurio and 13 other LVLMs across 14 tasks. We highlight the best and second-best results. Scores are accuracy (CIDEr for XM3600). en & mul are the English and averaged multilingual results. XM3600 fid. is the language fidelity over all languages; SMPQA G. & are Grounding and Naming. *: supports only single-image input. AVG.: average over all tasks. Details on the setup and models are provided in Appendix C. Model Centurio Aya Centurio Qwen T1 35.1 38.1 InternVL 2.5 8B 29.9 30.6 Qwen2VL 7B 38.5 Pangea T2 46.4 51. 37.0 36.8 38.6 T3 47.0 48.3 37.4 40.5 46.9 T4 46.7 47. 41.0 46.2 44.2 T5 48.3 50.9 50.5 48.0 49.9 Without multi-image tasks (MaRVL, VGR, VLOD): Centurio Aya Centurio Qwen 35.1 38.1 InternVL 2.5 8B 29.9 30.6 Qwen2VL 7B 38.5 Pangea 44.5 49.5 40.4 38.7 46.5 45.7 45.6 35.2 40.8 47. 46.2 45.8 39.4 46.8 44.4 47.7 49.6 49.7 48.3 49.9 en 60.6 66. 64.4 56.8 59.8 60.7 66.0 62.3 61.7 64.9 Table 6: Comparison between Centurio and the top-3 models of Table 5. Scores are averages over results from all 14 tasks grouped by language tier."
        },
        {
            "title": "3.2 Results",
            "content": "We compare our Centurio models against 13 other multilingual LVLMs across the 13 tasks used in 2, and additionally evaluate them on CVQA10, testing the models capabilities across 56 languages. We provide details for all models in Appendix C.6. On average, Centurio achieves the best results across the 14 tasks on their multilingual portions 10CVQA has private test set and only allows limited submissions hence we left it out for our analysis experiments. and additionally performs strongly on English  (Table 5)  . These results prove the effectiveness of our training composition: we are able to retain high English performances while maximizing the models multilingual capabilities. When analyzing these results grouped by language tier  (Table 6)  , we find that our models shine in the low-resource tiers T111 and T2, with competitive results for higherresource languageseven when excluding multiimage tasks (VGR, MaRVL, VLOD), where our models greatly outperform most others. Only for text-heavy tasks (primarily MTVQA and SMPQA), Centurio falls behind. While we show the importance of multilingual OCR trainingCenturio succeeds at the SMPQA reading task in more languages than, for example, Pangeathe limited input resolution and magnitudes less OCR data compared to Qwen2-VL and others result in comparably poor performance."
        },
        {
            "title": "4 Related Work",
            "content": "Multilingual LVLMs. Building on the success of monolingual LVLMs like BLIP-2 (Li et al., 2023) 11Despite 4/7 T1 CVQA languages not in our training data. 7 and LLaVA (Liu et al., 2023b,a), researchers extended the English training protocols to include multilingual data for obtaining massively multilingual LVLMs (e.g., Maaz et al., 2024b; Geigle et al., 2023a). As such, Googles PaLI models (Chen et al., 2022, 2023) were the first closed-weight models trained on multilingual captions and VQA data with the recent open-weight PaliGemma (Beyer et al., 2024) following similar training strategy. Geigle et al. (2023a) presented with mBLIP the first open model, trained with image captions and limited mix of instruct data translated to 98 languages. Subsequent models similarly followed an established procedure by directly translating parts of the English training data (Maaz et al., 2024b; Hu et al., 2024; Alam et al., 2024). For the concurrent Pangea, Yue et al. (2024a) optimized for multicultural aspects and used mix of machine-translated data, existing multilingual data, and synthetically generated data. While they analyze the ratio between English and multilingual data, they do not vary the number of languages, fixing it at 39. Interestingly, most researchers either (i) did not properly motivate their multilingual data mix (e.g., Geigle et al., 2023a; Alam et al., 2024; Beyer et al., 2024), or (ii) did not provide any details on the training data composition (e.g., Wang et al., 2024b; Yao et al., 2024a; Chen et al., 2024d)) Multilingual OCR with LVLMs. While OCR recently gained popularity for English LVLMs (Lu et al., 2024; Tong et al., 2024), multilingual OCR has largely been neglected in prior work. As an exception, Qwen2-VL (Wang et al., 2024b) and InternVL 2.5 (Chen et al., 2024d) exhibit excellent multilingual OCR capabilities, but no training details are known. Towards open knowledge on improving multilingual OCR, Yue et al. (2024a) performed preliminary experiments leveraging data in 10 languages. However, such efforts are still hindered by the lack of evaluation resources: MTVQA (Tang et al., 2024) and M3Exam (Zhang et al., 2023a) only cover up to 9 languages and conflate language understanding (in the text input) with understanding text on images. In this work, we push multilingual OCR research by presenting the novel SMPQA dataset dedicated to evaluation of multilingual OCR. We further explore how synthetic training data can improve models capabilities. Multilingual Instruction Tuning of LLMs.. While older LLMs struggled in multilingual tasks (Ahuja et al., 2024), more recent ones like Qwen 2.5 (Yang et al., 2024), Llama 3 (Llama Team, 2024), Gemma 2 (Gemma Team, 2024), or Aya (Aryabumi et al., 2024) have greatly improved in that respect, making them usable in many languages besides English. Still, current LLMs often fail to respond faithfully to the prompting language if that language is not English, especially for lowresource languages (Holtermann et al., 2024; Kew et al., 2024; Marchisio et al., 2024). To mitigate this issue, several works have analyzed the importance of multilingual instruction tuning. Weber et al. (2024) demonstrated that multilingual training is crucial for downstream performance even if the base models are pre-trained on multilingual data mixtures. Others showed that just small set of languages is sufficient to improve cross-lingual transfer for multilingual downstream tasks significantly (Shaham et al., 2024b; Chen et al., 2024c; Kew et al., 2024). However, they focus on small set of primarily higher-resource languages, while we consider the problem in the vision-language context for wider language selection. In (Soykan and Sahin, 2024), the authors propose methods to select the optimal mix of languages for instruction tuning in linguisticallyinformed manner. However, they find no general best selection, and instead taskand modeldependent selection is necessary. Therefore, in our work, we do not apply these techniques and instead choose languages based on the taxonomy introduced by Joshi et al. (2020)."
        },
        {
            "title": "5 Conclusion",
            "content": "In this study, we systematically investigated the optimal data composition for training multilingual LVLM through four progressively refined analysis setups. Our findings reveal that massively multilingual training with 100 languages is highly effective, achieving comparable results to configurations with fewer languages. Moreover, only 2550% of the training data needs to be non-English, keeping the cost of multilingual data production low. To enhance multilingual text understanding in images, we introduced novel evaluation benchmark and demonstrated the importance and effectiveness of including multilingual synthetic OCR data in the training mix. Finally, we apply our findings to train Centurio, massively multilingual LVLMs trained with 100 languages, and achieve state-of-the-art results on our evaluation suite covering 14 tasks and 56 language tasks against 13 other LVLMs."
        },
        {
            "title": "6 Limitations",
            "content": "Lack of Explicit Multicultural Training The focus of this work is on language understanding in massively multilingual setup, that is, how to train the model to maximize its ability to understand and generate text in various languages. We do not consider the multicultural aspect, that is, training model so that it is also more knowledgeable about concepts from the countries whose languages it can understand as measured by benchmarks like CVQA or CulturalVQA (Nayak et al., 2024). While the two aspects multilingual and multicultural knowledge can be intermingled in practice, they require distinct approaches in training: Multilingual data is necessary for multilingual language understanding, as we have shown. However, multicultural knowledge can be learned from multilingual resources as created by, for example, by Yue et al. (2024a), but also from fully English resources like Wikipedia (Srinivasan et al., 2021). Using Machine-Translated Training Data We train our model using machine-translated (MT) data derived from high-quality English datasets. This is advantageous because it allows us to create comparable setups for our analyses with full control over the languages and their proportions. While the data proves effective in increasing multilingual performance, MT data, especially for low-resource languages, can be of low quality and, even in higherresource languages, might exhibit unwanted translationese artifacts. This can negatively impact the quality of generated text in way that the metrics employed in our evaluation suite do not adequately measure. While native multilingual training data is available, it is not available for all tasks or languages equally, or, for most languages, not at all. Future work should consider evaluation setups to quantify the effect the MT data has on the final model, work on better MT pipelines, or create more data through native speakers. Using Synthetically Generated OCR Data The text-heavy, real-world tasks in some datasets of our instruction tuning mix, which cover diverse image types such as plots, scans, application screenshots, or screenshots of webpages, are still entirely in English. Due to the issues that arise when translating such samples, we do not translate them. Hence, our methods to improve the understanding of multilingual texts in images are limited to only using synthetically generated images. While we have seen that our synthetic data positively impacts the performance of models on the respective tasks, future work should explore methods for collecting or generating more diverse data in different languages beyond our synthetic OCR data. Another limitation regarding OCR capabilities is our relatively small image input resolution compared to models like Qwen2-VL or InternVL 2.5 both of which support image inputs in native resolution at the cost of thousands of tokens per image , which limits the performance of Centurio for images with small text."
        },
        {
            "title": "Acknowledgments",
            "content": "Simulations were performed with computing resources granted by WestAI under project 9148. Simulations were performed with computing resources from Julia 2. Julia 2 was funded as DFG project as Forschungsgroßgerät nach Art 91b GG under INST 93/1145-1 FUGG"
        },
        {
            "title": "To work of Gregor Geigle was in part supported",
            "content": "by the Alexander von Humboldt Foundation. The work of Carolin Holtermann and Anne Lauscher is funded under the Excellence Strategy of the German Federal Government and the States."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. 2024a. Phi-3 Technical Report: Highly Capable Language Model Locally on Your Phone. arXiv preprint arXiv:2404.14219. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, 9 Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024b. Phi-3 Technical Report: Highly Capable Language Model Locally on Your Phone. _eprint: 2404.14219. Manoj Acharya, Kushal Kafle, and Christopher Kanan. 2019. TallyQA: Answering Complex Counting Questions. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 80768084. AAAI Press. Željko Agic and Natalie Schluter. 2018. Baselines and Test Data for Cross-Lingual Inference. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. 2024. Pixtral 12B. arXiv preprint arXiv:2410.07073. Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2024. MEGAVERSE: Benchmarking large language models across languages, modalities, models and tasks. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 25982637, Mexico City, Mexico. Association for Computational Linguistics. AI@Meta. 2024. Llama 3 Model Card. Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, SM Uddin, Shayekh Bin Islam, et al. 2024. Maya: An Instruction Finetuned Multilingual Multimodal Model. arXiv preprint arXiv:2412.07112. Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan N. Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. 2024. Aya 23: Open Weight Releases to Further Multilingual Progress. CoRR, abs/2405.15032. ArXiv: 2405.15032. Jonas Belouadi, Anne Lauscher, and Steffen Eger. 2024. AutomaTikZ: Text-Guided Synthesis of Scientific In The Twelfth InterVector Graphics with TikZ. national Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey A. Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bosnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier J. Hénaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. 2024. PaliGemma: versatile 3B VLM for transfer. CoRR, abs/2407.07726. ArXiv: 2407.07726. Ali Furkan Biten, Rubèn Tito, Andrés Mafla, Lluís Gómez Bigorda, Marçal Rusiñol, C. V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. 2019. In 2019 Scene Text Visual Question Answering. IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 42904300. IEEE. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. large annotated corpus for learning natural language inferIn Proceedings of the 2015 Conference on ence. Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 632642. The Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs]. ArXiv: 2005.14165. Emanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva Reddy, Desmond Elliott, Edoardo Maria Ponti, and Ivan Vulic. 2022. IGLUE: Benchmark for Transfer Learning across Modalities, Tasks, and Languages. CoRR, abs/2201.11732. ArXiv: 2201.11732. Soravit Changpinyo, Linting Xue, Idan Szpektor, Ashish V. Thapliyal, Julien Amelot, Michal Yarom, Xi Chen, and Radu Soricut. 2022. MaXM: Towards arXiv Multilingual Visual Question Answering. preprint arXiv:2209.05401. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. 2024a. ALLaVA: Harnessing GPT4V-synthesized Data for Lite Vision-Language Model. CoRR, abs/2402.11684. ArXiv: 2402.11684. 10 Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024b. ShareGPT4V: Improving Large Multi-modal Models with Better Captions. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XVII, volume 15075 of Lecture Notes in Computer Science, pages 370387. Springer. Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Andrey Kutuzov, Barry Haddow, and Kenneth Heafield. 2024c. Monolingual or multilingual instruction tuning: Which makes better alpaca. In Findings of the Association for Computational Linguistics: EACL 2024, pages 13471356, St. Julians, Malta. Association for Computational Linguistics. Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, A. J. Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. 2023. PaLI-X: On Scaling up Multilingual Vision and Language Model. CoRR, abs/2305.18565. ArXiv: 2305.18565. Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. 2022. PaLI: JointlyScaled Multilingual Language-Image Model. CoRR, abs/2209.06794. ArXiv: 2209.06794. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2024d. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Preprint, arXiv:2412.05271. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. 2024e. Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling. arXiv preprint arXiv:2412.05271. Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R. Routledge, and William Yang Wang. 2021. FinQA: Dataset of Numerical Reasoning over Financial Data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 711 November, 2021, pages 36973711. Association for Computational Linguistics. Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, and Dongmei Zhang. 2022. HiTab: Hierarchical Table Dataset for Question Answering and Natural Language Generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 10941110. Association for Computational Linguistics. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 84408451. Association for Computational Linguistics. Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No Language Left Behind: Scaling Human-Centered Machine Translation. CoRR, abs/2207.04672. ArXiv: 2207.04672. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. CoRR, abs/2305.06500. ArXiv: 2305.06500. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak 11 Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, and Sara Hooker. 2024. Aya expanse: Combining research breakthroughs for new multilingual frontier. Preprint, arXiv:2412.04261. J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li FeiFei. 2009. ImageNet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255. Peter Devine. 2024. Tagengo: Multilingual Chat Dataset. CoRR, abs/2405.12612. ArXiv: 2405.12612. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. 2023. G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model. CoRR, abs/2312.11370. ArXiv: 2312.11370. William Gaviria Rojas, Sudnya Diamos, Keertan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. 2022. The Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World. Advances in Neural Information Processing Systems, 35:1297912990. Gregor Geigle, Abhay Jain, Radu Timofte, and Goran Glavas. 2023a. mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. CoRR, abs/2307.06930. ArXiv: 2307.06930. Gregor Geigle, Radu Timofte, and Goran Glavas. 2023b. Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations. CoRR, abs/2306.08658. ArXiv: 2306.08658. Gregor Geigle, Radu Timofte, and Goran Glavas. 2024. African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 26532669. Association for Computational Linguistics. Gemma Team. 2024. Gemma 2: Improving Open CoRR, Language Models at Practical Size. abs/2408.00118. ArXiv: 2408.00118. Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. 2017. Making the in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 63256334. Bigham. 2018. VizWiz Grand Challenge: Answering Visual Questions From Blind People. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 36083617. Computer Vision Foundation / IEEE Computer Society. Musashi Hinck, Carolin Holtermann, Matthew L. Olson, Florian Schneider, Sungduk Yu, Anahita Bhiwandiwalla, Anne Lauscher, Shao-Yen Tseng, and Vasudev Lal. 2024. Why do LLaVA Vision-Language Models Reply to Images in English? In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 1340213421. Association for Computational Linguistics. Carolin Holtermann, Paul Röttger, Timm Dill, and Anne Lauscher. 2024. Evaluating the elementary multilingual capabilities of large language models with MultiQ. In Findings of the Association for Computational Linguistics: ACL 2024, pages 44764494, Bangkok, Thailand. Association for Computational Linguistics. Yu-Chung Hsiao, Fedir Zubach, Maria Wang, and Jindong Chen. 2022. ScreenQA: Large-Scale QuestionAnswer Pairs over Mobile App Screenshots. CoRR, abs/2209.08199. ArXiv: 2209.08199. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation In The Tenth Interof Large Language Models. national Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Drew A. Hudson and Christopher D. Manning. 2019. GQA: New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 67006709. Computer Vision Foundation / IEEE. Harsh Jhamtani and Taylor Berg-Kirkpatrick. 2018. Learning to Describe Differences Between Pairs of Similar Images. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 40244034. Association for Computational Linguistics. Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. 2017. CLEVR: Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 19881997. IEEE Computer Society. Tannon Kew, Florian Schottmann, and Rico Sennrich. 2024. Turning English-centric LLMs into polyglots: How much multilinguality is needed? In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1309713124, Miami, Florida, USA. Association for Computational Linguistics. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The State and Fate of Linguistic Diversity and Inclusion in the NLP World. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 62826293. Association for Computational Linguistics. Kushal Kafle, Brian L. Price, Scott Cohen, and Christopher Kanan. 2018. DVQA: Understanding Data Visualizations via Question Answering. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 56485656. Computer Vision Foundation / IEEE Computer Society. Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. 2024. Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models. CoRR, abs/2402.07865. ArXiv: 2402.07865. Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. 2023. GeomVerse: Systematic Evaluation of Large Models for Geometric Reasoning. CoRR, abs/2312.12241. ArXiv: 2312.12241. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara L. Berg. 2014. ReferItGame: Referring In to Objects in Photographs of Natural Scenes. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, meeting of SIGDAT, Special Interest Group of the ACL, pages 787798. ACL. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Min Joon Seo, Hannaneh Hajishirzi, and Ali Farhadi. 2016. Diagram is Worth Dozen Images. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pages 235251. Springer. Aniruddha Kembhavi, Min Joon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Are You Smarter Than Sixth Grader? Textbook Question Answering for Multimodal MaIn 2017 IEEE Conference chine Comprehension. on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 53765384. IEEE Computer Society. Tannon Kew, Florian Schottmann, and Rico Sennrich. 2023. Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed? CoRR, abs/2312.12683. ArXiv: 2312.12683. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2022. OCR-Free Document Understanding Transformer. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXVIII, volume 13688 of Lecture Notes in Computer Science, pages 498517. Springer. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language and Vision Using Int. J. Crowdsourced Dense Image Annotations. Comput. Vision, 123(1):3273. Place: USA Publisher: Kluwer Academic Publishers. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. 2024a. What matters when building vision-language models? _eprint: 2405.02246. Hugo Laurençon, Léo Tronchon, and Victor Sanh. 2024b. Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset. CoRR, abs/2403.09029. ArXiv: 2403.09029. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. CoRR, abs/2301.12597. ArXiv: 2301.12597. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024. Multimodal ArXiv: Dataset for Improving Scientific Comprehension of Large Vision-Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1436914387. Association for Computational Linguistics. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, volume 8693 of Lecture Notes in Computer Science, pages 740755. Springer. Ziyi Lin, Dongyang Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Yu Qiao, and Hongsheng Li. 2024. SPHINX: Mixer of Weights, Visual Embeddings and Image Scales for Multi-modal Large 13 Language Models. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXII, volume 15120 of Lecture Notes in Computer Science, pages 3655. Springer. Adam Dahlgren Lindström and Savitha Sam Abraham. 2022. CLEVR-Math: Dataset for Compositional Language, Visual and Mathematical Reasoning. In Proceedings of the 16th International Workshop on Neural-Symbolic Learning and Reasoning as part of the 2nd International Joint Conference on Learning & Reasoning (IJCLR 2022), Cumberland Lodge, Windsor Great Park, UK, September 28-30, 2022, volume 3212 of CEUR Workshop Proceedings, pages 155170. CEUR-WS.org. Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. 2021. Visually Grounded Reasoning across Languages and Cultures. CoRR, abs/2109.13238. ArXiv: 2109.13238. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved Baselines with Visual Instruction Tuning. CoRR, abs/2310.03744. ArXiv: 2310.03744. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual Instruction Tuning. CoRR, abs/2304.08485. ArXiv: 2304.08485. Llama Team. 2024. The Llama 3 Herd of Models. CoRR, abs/2407.21783. ArXiv: 2407.21783. Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. 2024. DeepSeek-VL: Towards Real-World VisionLanguage Understanding. CoRR, abs/2403.05525. ArXiv: 2403.05525. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. 2021a. Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 6774 6786. Association for Computational Linguistics. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023. Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Pan Lu, Liang Qiu, Jiaqi Chen, Tanglin Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. 2021b. IconQA: New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao Anwer, Tim Baldwin, Michael Felsberg, and Fahad Khan. 2024a. PALO: Polyglot Large Multimodal Model for 5B People. arXiv preprint arXiv:2402.14818. Muhammad Maaz, Hanoona Abdul Rasheed, Abdelrahman M. Shaker, Salman H. Khan, Hisham Cholakkal, Rao Muhammad Anwer, Tim Baldwin, Michael Felsberg, and Fahad Shahbaz Khan. 2024b. PALO: Polyglot Large Multimodal Model for 5B People. CoRR, abs/2402.14818. ArXiv: 2402.14818. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, and Kevin Murphy. 2016. Generation and Comprehension of Unambiguous Object Descriptions. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 1120. IEEE Computer Society. Kelly Marchisio, Wei-Yin Ko, Alexandre Berard, Théo Dehaze, and Sebastian Ruder. 2024. Understanding and Mitigating Language Confusion in LLMs. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 66536677. Association for Computational Linguistics. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. OK-VQA: Visual Question Answering Benchmark Requiring External Knowledge. arXiv:1906.00067 [cs]. ArXiv: 1906.00067. 14 Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. 2022. ChartQA: Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 22632279. Association for Computational Linguistics. Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V. Jawahar. 2022. InfographicVQA. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022, Waikoloa, HI, USA, January 3-8, 2022, pages 2582 2591. IEEE. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. 2021. DocVQA: Dataset for VQA on Document Images. In IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021, pages 21992208. IEEE. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. 2019. OCR-VQA: Visual Question Answering by Reading Text in Images. In 2019 International Conference on Document Analysis and Recognition, ICDAR 2019, Sydney, Australia, September 20-25, 2019, pages 947952. IEEE. Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd van Steenkiste, Lisa Anne Hendricks, Karolina Stanczak, and Aishwarya Agrawal. 2024. Benchmarking Vision Language Models for Cultural In Proceedings of the 2024 ConUnderstanding. ference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 57695790. Association for Computational Linguistics. Jason Obeid and Enamul Hoque. 2020. Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model. In Proceedings of the 13th International Conference on Natural Language Generation, INLG 2020, Dublin, Ireland, December 15-18, 2020, pages 138147. Association for Computational Linguistics. Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, JanMartin O. Steitz, Stefan Roth, Ivan Vulic, and Iryna Gurevych. 2022a. xGQA: Cross-Lingual Visual Question Answering. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 24972511. Association for Computational Linguistics. Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. 2022b. Lifting the curse of multilinguality by pre-training modular transformers. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 34793495. David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, Bontu Fufa Balcha, Chenxi Whitehouse, Christian Salamea, Dan John Velasco, David Ifeoluwa Adelani, David Le Meur, Emilio Villa-Cueva, Fajri Koto, Fauzan Farooqui, Frederico Belcavello, Ganzorig Batnasan, Gisela Vallejo, Grainne Caulfield, Guido Ivetta, Haiyue Song, Henok Biadglign Ademtew, Hernán Maina, Holy Lovenia, Israel Abebe Azime, Jan Christian Blaise Cruz, Jay P. Gala, Jiahui Geng, Jesús-Germán Ortiz-Barajas, Jinheon Baek, Jocelyn Dunstan, Laura Alonso Alemany, Kumaranage Ravindu Yasas Nagasinghe, Luciana Benotti, Luis Fernando DHaro, Marcelo Viridiano, Marcos Estecha-Garitagoitia, Maria Camila Buitrago Cabrera, Mario Rodríguez-Cantelar, Mélanie Jouitteau, Mihail Mihaylov, Mohamed Fazli Mohamed Imam, Muhammad Farid Adilazuarda, Munkhjargal Gochoo, Munkh-Erdene Otgonbold, Naome A. Etori, Olivier Niyomugisha, Paula Mónica Silva, Pranjal A. Chitale, Raj Dabre, Rendi Chevi, Ruochen Zhang, Ryandito Diandaru, Samuel Cahyawijaya, Santiago Góngora, Soyeong Jeong, Sukannya Purkayastha, Tatsuki Kuribayashi, Thanmay Jayakumar, Tiago Timponi Torrent, Toqeer Ehsan, Vladimir Araujo, Yova Kementchedjhieva, Zara Burzo, Zheng Wei Lim, Zheng Xin Yong, Oana Ignat, Joan Nwatu, Rada Mihalcea, Thamar Solorio, and Alham Fikri Aji. 2024. CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark. CoRR, abs/2406.05967. ArXiv: 2406.05967. Florian Schneider and Sunayana Sitaram. 2024. M5 - Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 43094345. Association for Computational Linguistics. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. LAION-5B: An open large-scale dataset for training next generation image-text models. CoRR, abs/2210.08402. ArXiv: 2210.08402. Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-OKVQA: Benchmark for Visual Question Answering Using World Knowledge. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII, volume 13668 of Lecture Notes in Computer Science, pages 146162. Springer. Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. 2024a. Multilingual Instruction Tuning With Just Pinch of Multilinguality. CoRR, abs/2401.01854. ArXiv: 2401.01854. Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, and Matan Eyal. 2024b. Multilingual instruction tuning with just pinch of multilinguality. In Findings of the Association for Computational Linguistics: ACL 2024, pages 2304 2317, Bangkok, Thailand. Association for Computational Linguistics. Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor Darrell. 2024. When Do We Not Need Larger Vision Models? In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part VIII, volume 15066 of Lecture Notes in Computer Science, pages 444462. Springer. Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. 2024. Design2Code: How Far Are We From Automating Front-End Engineering? CoRR, abs/2403.03163. ArXiv: 2403.03163. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Minh Vu Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker. 2024. Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1152111567. Association for Computational Linguistics. Gurkan Soykan and Gözde Gül Sahin. 2024. Linguistically-Informed Multilingual Instruction Tuning: Is There an Optimal Set of Languages to Tune? ArXiv, abs/2410.07809. Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. 2021. WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 21, pages 24432449, New York, NY, USA. Association for Computing Machinery. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. Corpus for Reasoning about Natural Language Grounded in Photographs. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pages 64186428. Association for Computational Linguistics. Zhang, De-Chuan Zhan, and Han-Jia Ye. 2024a. Parrot: Multilingual Visual Instruction Tuning. CoRR, abs/2406.02539. ArXiv: 2406.02539. Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, et al. 2024b. Parrot: Multilingual Visual Instruction Tuning. arXiv preprint arXiv:2406.02539. Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. 2021. VisualMRC: Machine Reading Comprehension on Document Images. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 1387813888. AAAI Press. Benny J. Tang, Angie W. Boggust, and Arvind Satyanarayan. 2023. VisText: Benchmark for Semantically Rich Chart Captioning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 7268 7298. Association for Computational Linguistics. Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, Yanjie Wang, Yuliang Liu, Hao Liu, Xiang Bai, and Can Huang. 2024. MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering. Ashish V. Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. 2022. Crossmodal-3600: Massively Multilingual Multimodal Evaluation Dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 715729, Abu Dhabi, United Arab Emirates. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. 2024. Cambrian-1: Fully Open, VisionCentric Exploration of Multimodal LLMs. CoRR, abs/2406.16860. ArXiv: 2406.16860. Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. 2023. How Many Unicorns Are in This Image? Safety Evaluation Benchmark for Vision LLMs. CoRR, abs/2311.16101. ArXiv: 2311.16101. Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 45664575. IEEE Computer Society. Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. 2023. To See is to 16 Believe: Prompting GPT-4V for Better Visual Instruction Tuning. CoRR, abs/2311.07574. ArXiv: 2311.07574. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. 2024a. Measuring Multimodal Mathematical Reasoning with MATHVision Dataset. CoRR, abs/2402.14804. ArXiv: 2402.14804. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024b. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. CoRR, abs/2409.12191. ArXiv: 2409.12191. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024c. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Alexander Arno Weber, Klaudia Thellmann, Jan Ebert, Nicolas Flores-Herr, Jens Lehmann, Michael Fromm, and Mehdi Ali. 2024. Investigating multilingual instruction-tuning: Do polyglot models demand for In Proceedings of the multilingual instructions? 2024 Conference on Empirical Methods in Natural Language Processing, pages 2082920855, Miami, Florida, USA. Association for Computational Linguistics. Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. 2019. Visual Entailment: Novel Task for Fine-grained Image Understanding. arXiv preprint arXiv:1901.06706. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024. Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing. CoRR, abs/2406.08464. ArXiv: 2406.08464. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 Technical Report. CoRR, abs/2407.10671. ArXiv: 2407.10671. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024a. MiniCPMV: GPT-4V Level MLLM on Your Phone. CoRR, abs/2408.01800. ArXiv: 2408.01800. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. 2024b. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. MMMU: Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. CoRR, abs/2311.16502. ArXiv: 2311.16502. Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. 2024a. Pangea: Fully Open Multilingual Multimodal LLM for 39 Languages. CoRR, abs/2410.16153. ArXiv: 2410.16153. Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. 2024b. Pangea: Fully Open Multilingual Multimodal LLM for 39 Languages. arXiv preprint arXiv:2410.16153. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid Loss for Language Image Pre-Training. CoRR, abs/2303.15343. ArXiv: 2303.15343. Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. 2019. RAVEN: Dataset for Relational and Analogical Visual REasoNing. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 53175327. Computer Vision Foundation / IEEE. Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. 2023a. M3Exam: Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. Advances in Neural Information Processing Systems, 36:5484 5505. Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023b. LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding. CoRR, abs/2306.17107. ArXiv: 2306.17107. 17 Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, and Dragomir Radev. 2023. RobuT: Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 60646081. Association for Computational Linguistics. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: Question Answering Benchmark on Hybrid of Tabular and Textual Content in Finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 32773287. Association for Computational Linguistics. Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei. 2016. Visual7W: Grounded Question In 2016 IEEE Conference Answering in Images. on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 49955004. IEEE Computer Society."
        },
        {
            "title": "A Training Setup",
            "content": "All models are trained with the following hyperparameters: AdamW optimizer (Loshchilov and Hutter, 2019) with cosine learning rate schedule and 3% linear warmup; LORA (Hu et al., 2022) is used with rank 256 and α512 and applied to all matrices in the LLM the LLM is otherwise frozen; the image encoder is frozen in the first three experiments and jointly trained with the model otherwise; weight decay is 0; batch size is 32 using gradient accumulation; learning rate is 1e 6 for the image encoder, 1e 4 for LORA and the MLP in general except when training Centurio, we use 5e 5 during pretraining and 3e 5 during instruct tuning. Models are always trained for one epoch on the entire data. The training loss is causal language modeling and we mask both image and input prompt tokens for calculating the loss. For going from the pretraining to the instruct tuning phase, we found it best to continue training the same LORA adapter; merging the LORA weights after pretraining and initializing new adapter gave worse results. Hyperparameter (LORA rank & α, learning rates, weight decay) were tuned for Phi 3.5 and transferred to the other LLMs. All models were trained with 4 H100 GPUs. Training Centurio took 6 days (half for pretraining, half for instruct tuning). Training of one Phi 3.5 model for 2 takes 810h for instruct tuning, and for pre-training 12 to 20h (with synthetic OCR data and unfrozen image encoder). A.1 Training Languages We list the 100 languages used in training in Table 7."
        },
        {
            "title": "B Training Data",
            "content": "B.1 For Analysis Experiments The collections of datasets used in the instruct tuning phase for the analysis experiments (2) is adapted from LLaVA-Next (Liu et al., 2024). As multiple evaluation datasets contain multiple images in the input (MaRVL, VGR, VLOD, M3Exam, xMMMU), we include additional datasets to improve capabilities for this situation. See Table 8 for the full list. B.2 For Training Centurio For training Centurio, we combine the datasets from Table 8 with additional datasets listed in Table 9. B.3 Synthetic OCR Data We use the official Synthdog code12 to generate the samples using the Google Noto font and with images from the ImageNet train split as background. Text is sampled from Wikipedias of the respective languages. We consider the following 32 languages as not using the Latin script: am, ar, as, azb, be, bg, bn, bo, el, fa, he, hi, ja, ka, kk, km, ko, lo, mr, my, pa, ru, sa, sd, sr, ta, te, th, ti, uk, ur, zh."
        },
        {
            "title": "C Evaluation Setup",
            "content": "This section describes the details of our evaluation setup. C.1 Generation Parameters In all experiments of our greedy use do_sample=False;). decoding test suite, we (temperature=0.0; 12https://github.com/clovaai/donut/tree/master/ synthdog 18 Name Script ISOFlores-200 Tier Name Script ISO-639 FloresTier Arabic Chinese English French German Japanese Spanish Basque Catalan Croatian Czech Dutch Finnish Hindi Hungarian Italian Korean Persian Polish Portuguese Russian Serbian Swedish Turkish Vietnamese Afrikaans Bangla Belarusian Bosnian Bulgarian Cebuano Danish Egyptian Arabic Estonian Galician Georgian Greek Indonesian Kazakh Latin Latvian Lithuanian Malay Romanian Slovak Slovenian Tagalog Tamil Thai Ukrainian Arabic Trad. Han Latin Latin Latin Japanese Latin Latin Latin Latin Latin Latin Latin Devanagari Latin Latin Hangul Arabic Latin Latin Cyrillic Cyrillic Latin Latin Latin Latin Bengali Cyrillic Latin Cyrillic Latin Latin Arabic Latin Latin Georgian Greek Latin Cyrillic Latin Latin Latin Latin Latin Latin Latin Latin Tamil Thai Cyrillic ar zh en fr de ja es eu ca hr cs nl fi hi hu it ko fa pl pt ru sr sv tr vi af bn be bs bg ceb da ar-eg et gl ka el id kk la lv lt ms ro sk sl tl ta th uk arb_Arab zho_Hant eng_Latn fra_Latn deu_Latn jpn_Jpan spa_Latn eus_Latn cat_Latn hrv_Latn ces_Latn nld_Latn fin_Latn hin_Deva hun_Latn ita_Latn kor_Hang pes_Arab pol_Latn por_Latn rus_Cyrl srp_Cyrl swe_Latn tur_Latn vie_Latn afr_Latn ben_Beng bel_Cyrl bos_Latn bul_Cyrl ceb_Latn dan_Latn arz_Arab est_Latn glg_Latn kat_Geor ell_Grek ind_Latn kaz_Cyrl NO lvs_Latn lit_Latn zsm_Latn ron_Latn slk_Latn slv_Latn tgl_Latn tam_Taml tha_Thai ukr_Cyrl 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Urdu Uzbek Hebrew Amharic Haitian Hausa Icelandic Irish Lao Maltese Marathi Punjabi Sanskrit Swahili Tigrinya Tswana Wolof Xhosa Yoruba Zulu Albanian Assamese Azerbaijani Bambara Burmese Esperanto Igbo Javanese Khmer Kikuyu Lingala Luxembourgish Maori Norwegian Occitan Quechua Samoan Sango Sardinian Scottish Gaelic Sindhi Somali Swati Telugu Tibetan Tok Pisin Tsonga Twi Waray Welsh Arabic Latin Hebrew Ethiopic Latin Latin Latin Latin Lao Latin Devanagari Gurmukhi Devanagari Latin Ethiopic Latin Latin Latin Latin Latin Latin Bengali Arabic Latin Myanmar Latin Latin Latin Khmer Latin Latin Latin Latin Latin Latin Latin Latin Latin Latin Latin Arabic Latin Latin Telugu Tibetan Latin Latin Latin Latin Latin ur uz iwhe am ht ha is ga lo mt mr pa sa sw ti tn wo xh yo zu sq as azb bm my eo ig jv km ki ln lb mi no oc qu sm sg sc gd sd so ss te bo tpi ts tw war cy urd_Arab uzn_Latn heb_Hebr amh_Ethi hat_Latn hau_Latn isl_Latn gle_Latn lao_Laoo mlt_Latn mar_Deva pan_Guru san_Deva swh_Latn tir_Ethi tsn_Latn wol_Latn xho_Latn yor_Latn zul_Latn als_Latn asm_Beng azb_Arab bam_Latn mya_Mymr epo_Latn ibo_Latn jav_Latn khm_Khmr kik_Latn lin_Latn ltz_Latn mri_Latn nob_Latn oci_Latn quy_Latn smo_Latn sag_Latn srd_Latn gla_Latn snd_Arab som_Latn ssw_Latn tel_Telu bod_Tibt tpi_Latn tso_Latn twi_Latn war_Latn cym_Latn 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Table 7: The list of 100 languages used in our training experiments. The Tier column represents the tier in the taxonomy proposed by Joshi et al. (2020), where higher tier indicates more available resources, i.e., data, in the respective language. 19 Dataset Size (Images) Translated? Dataset Size (Images) Translated? Natural Image: LLaVA Instruct (Liu et al., 2023b) VQAv2 (Goyal et al., 2017) GQA (Hudson and Manning, 2019) OKVQA (Marino et al., 2019) A-OKVQA (Schwenk et al., 2022) RefCOCO (Kazemzadeh et al., 2014; Mao et al., 2016) VG (Krishna et al., 2017) MSCOCO (Lin et al., 2014) Multiple Images: NLVR (Suhr et al., 2019) Spot-the-difference (Jhamtani and Berg-Kirkpatrick, 2018) OCR: OCRVQA (Mishra et al., 2019) DocVQA (Mathew et al., 2021) AI2D (Kembhavi et al., 2016) ChartQA (Masry et al., 2022) DVQA (Kafle et al., 2018) ScienceQA (Lu et al., 2022) Total 160k 83k 72k 9k 30k 48k 86k 50k (subset) 86k 8k 50k (subset) 10k 3k 18k 50k (subset) 6k 766k yes yes yes yes yes yes yes yes yes yes no no no no no no Table 8: List of datasets included in the instruct tuning phase in our analysis experiments. All sizes are based on unique images; examples about the same image are packed into one sequence. C.2 Metrics Depending on the dataset and task, we employ either CIDEr (Vedantam et al., 2015), the exact match accuracy, or relaxed match accuracy (see Table C.4). For the relaxed match accuracy, we consider an answer correct if it starts with the correct choice letter. For example, answers like A. are also counted as correct if the gold label is A. C.3 Prompts We list the prompts for each dataset in our test suite used for all models in Figure 2. C.4 Datasets In the following, datasets included in our test suite are briefly introduced. An overview is provided in Table 10. Details about the languages covered by the datasets are listed in Table 11. xGQA The xGQA dataset (Pfeiffer et al., 2022a) is cross-lingual visual question-answering dataset. It extends the well-known English-only GQA dataset (Hudson and Manning, 2019) by manually translating the questions in the balanced test-dev set. Each of the 9666 questions is available in eight languages covering five scripts, while the answers are in English only. The dataset holds 300 unique images from Visual Genome (Krishna et al., 2017). MaXM The MaXM dataset was introduced by Changpinyo et al. (2022) and is VQA dataset comprising seven languages in five scripts. In MaXM, the questions and their respective answers are in the same language. The images are subset Natural Image: ALLaVA Instruct1 (Chen et al., 2024a) LVIS Instruct4V (Wang et al., 2023) Visual7W (Zhu et al., 2016) VizWiz QA (Gurari et al., 2018) TallyQA (Acharya et al., 2019) SketchyVQA (Tu et al., 2023) OODVQA (Tu et al., 2023) OCR: ScienceQA (Cambrian version) AI2D (Cambrian version) Rendered Text 2 ScreenQA (Hsiao et al., 2022) LLaVAR (Zhang et al., 2023b) ArxivQA (Li et al., 2024) Chart2Text (Obeid and Hoque, 2020) InfographicVQA (Mathew et al., 2022) VisText (Tang et al., 2023) TQA (Kembhavi et al., 2017) STVQA (Biten et al., 2019) TAT-QA (Zhu et al., 2021) TabMWP (Lu et al., 2023) HiTab (Cheng et al., 2022) IconQA (Lu et al., 2021b) VisualMRC (Tanaka et al., 2021) RobuT (Zhao et al., 2023) FinQA (Chen et al., 2021) Math & Code: WebSight (Laurençon et al., 2024b) Design2Code (Si et al., 2024) DaTikz (Belouadi et al., 2024) CLEVR (Johnson et al., 2017) CLEVR-Math (Lindström and Abraham, 2022) Geo170k (Gao et al., 2023) GeomVerse (Kazemi et al., 2023) Inter-GPS (Lu et al., 2021a) MathVision (Wang et al., 2024a) Raven (Zhang et al., 2019) Text (no images): Aya Dataset (Singh et al., 2024) Tagengo-GPT4 (Devine, 2024) Magpie2 (Xu et al., 2024) Total 760k 223k 14k 21k 133k 4k 3k 6k 4k 10k 33k 20k 54k 25k 2k 10k 1k 17k 2k 23k 2k 27k 3k 113k 5k 10k 0k 48k 70k 70k 9k 9k 1k 3k 42k 202k 70k 400k 2.47M yes yes no no yes yes no no no no no no no no no no no no no no no no no no no yes yes no yes yes no no no no no Table 9: Datasets used on top of the datasets from Table 8 for the instruct tuning phase of Centurio. 1: also contains web-scraped images from LAION (Schuhmann et al., 2022) which contain textual 2:https://huggingface.co/datasets/ elements. wendlerc/RenderedText. 2: Combining magpie-ultrav0.1 (50k), Magpie-Qwen2-Pro-200K-English (200k), Magpie-Llama-3.1-Pro-MT-300K-Filtered (150k subset). of the XM3600 (Thapliyal et al., 2022) dataset and are chosen to match region where the language of the question-answer pair is spoken. This ensures cultural diversity in the images in addition to the language diversity in the question-answer texts. XVNLI The XVNLI dataset (Bugliarello et al., 2022) introduces the task of Cross-lingual Visual Natural Language Inference where model needs to predict whether textual hypothesis entails, contradicts, or is neutral concerning visual premise. XVNLI comprises five languages covering three scripts and 357 unique images from Visual Genome. It is based on combination of the text-only SNLI (Bowman et al., 2015) dataset 20 SMPQA <IMG>{QUESTION}nAnswer the question using single word or phrase. CVQA <IMG>{QUESTION}nThere are several options:nA. {OPTION A}nB. {OPTION B}nC. {OPTION C}nD. {OPTION D}nAnswer with the options letter from the given choices directly. xMMMU {QUESTION}nThere are several options:nA. {OPTION A}nB. {OPTION B}nC. {OPTION C}nD. {OPTION D}nAnswer with the options letter from the given choices directly. MTVQA <IMG>{QUESTION}nAnswer the question using single word or phrase.nAnswer in {LANGUAGE}. M3Exam {QUESTION}nOptions:nA. {OPTION A}nB. {OPTION B}nC. {OPTION C}nD. {OPTION D}n Answer with the options letter from the given choices directly. BIN-MC <IMG>Which of these choices (in English) is shown in the image?n Choices:nA. {CHOICE A}nB. {CHOICE B}nC. {CHOICE C}nD. {CHOICE D}n Answer with the letter from the given choices directly. xGQA <IMG>{QUESTION}?nAnswer the question using single word or phrase.nAnswer in English. MaXM <IMG>{QUESTION}?nAnswer the question using single word or phrase.nAnswer in {LANGUAGE}. MaRVL <IMG>Given the two images <IMG><IMG>, is it correct to say {HYPOTHESIS}? Answer yes or no. XVNLI <IMG>Is it guaranteed true that {HYPOTHESIS}? Yes, no, or maybe? Answer in English. M5-VGR Given the two images <IMG><IMG>, is it correct to say {HYPOTHESIS}? Answer yes or no. M5-VLOD Based on the 5 images <IMG><IMG><IMG><IMG><IMG> ordered from top-left to bottom-right, which image does not match the hypothesis {HYPOTHESIS}? Choose one from [A, B, C, D, E] and only output single letter: XM3600 Briefly describe the image in {LANGUAGE} in one sentence. Figure 2: Prompts used for the different datasets of our test suite. For M3Exam and xMMMU, the questions contain images at individual positions, and also the options can consist of images. In total, sample of M3Exam can contain up to 8 images and 8 options, and sample of xMMMU can contain up to 4 images and 4 options. Dataset Task Visual Input Textual Input Target Output Metric #Lang. Single-Image Single-Image Single-Image Multi-Image Multi-Image Multi-Image Single-Image Single-Image Single-Image MaXM xGQA XVNLI M5B-VLOD M5B-VGR MaRVL MTVQA SMPQA - Name SMPQA - Ground M3Exam MMMU xMMMU BabelImageNet-MC MC VQA MC VQA CVQA Captioning XM3600 Question (TL) VQA Question (TL) VQA Hypothesis (TL) VNLI Hypothesis (TL) VLOD Hypothesis (TL) VGR Hypothesis (TL) VGR Question (TL) TH VQA Question (TL) TH VQA TH VGR Question (TL) TH MC VQA Single or Multi-Image Question (TL) TH MC VQA Single or Multi-Image Question (EN) TH MC VQA Single or Multi-Image Question (TL) Question (TL) Question (TL) Prompt (EN) Single-Image Single-Image Single-Image WoP (TL) WoP (EN) yes / no / maybe LoC yes / no yes / no WoP (TL) WoP (TL) yes / no LoC LoC LoC LoC LoC Caption (TL) E. Acc. E. Acc. E. Acc. R. Acc. E. Acc. E. Acc. E. Acc. E. Acc. E. Acc. R. Acc. R. Acc. R. Acc. R. Acc. R. Acc. CIDEr 6 8 5 12 12 6 9 11 11 7 1 7 20 39 36 Table 10: List of datasets contained in our test suite. In the Task column, VQA VNLI, VLOD, VGR, TH, and MC are acronyms for Visual Question Answering, Visual Natural Language Inference, Visio-Linguistic Outlier Detection, Visually Grounded Reasoning, Text-Heavy, and Multiple-Choice, respectively. In the Textual Input and Target Output columns, the acronyms WoP, LoC, TL, and EN stand for (Single) Word or Phrase, Letter of the correct Choice, Target Language, and English, respectively. Further, E. Acc. is Exact Accuracy and R. Acc. is Relaxed Accuracy. CVQA is not used in 2 due to its hidden test set with limited submissions. and its cross-lingual (Agic and Schluter, 2018) and cross-modal (Xie et al., 2019) equivalents. MaRVL The MaRVL dataset (Liu et al., 2021) aims to benchmark models on Multicultural Reasoning over Vision and Language. task sample comprises two images, textual statement, and binary true or false answer grounded in the images. MaRVL comprises five languages covering three scripts and 4914 culturally diverse images that match the respective languages. The images in sample are chosen to match the culture of the annotator who has written the textual statement in his or her native language. XM3600 The XM3600 dataset (Thapliyal et al., 2022) is large multilingual image captioning dataset comprising 36 languages with 261375 captions covering 13 different scripts for 100 unique images per language. The images are selected to match the languages cultural background, ensuring cultural and linguistic diversity. The captions were not automatically translated but manually created by professional annotators who are native speakers of the respective language. We only use subset of 500/3600 images (selected randomly) per language when evaluating XM3600 due to its size. Babel-ImageNet (multiple-choice) (BIN-MC) Babel-ImageNet (Geigle et al., 2023b) translates the labels of ImageNet (Deng et al., 2009) to nearly 300 languages, which allows us to test if models are capable of recognizing and linking the diverse objects of ImageNet to their correct label in the tested language. Testing all 300 languages would be too expensive, instead we use it to deepen our evaluation in languages appearing in only 1 or 2 other datasets, plus English and select few high-resource languages. Also, we only use 10 images per class instead of 50, again, to keep computational cost reasonable. We formulate the task as multiple-choice problem, following the approach by Geigle et al. (2024) to mine hard negative options from the total label pool. This avoids problems of unclear or underspecified answers that appear in traditional open-ended VQA formulation. We mine negatives with the English labels, filtering out all candidates not translated by Babel-ImageNet in the target language, that is, in the end, we select the three most similar negative labels that appear in the BabelImageNet labels of given language. SMPQA We propose SMPQA (Synthetic Multilingual Plot QA) as novel test dataset for evaluating multilingual OCR capabilities in images bar plots and pie charts to be specific in 11 languages, covering different scripts and resource levels. See C.5 for details. M5B-VGR The M5B-VGR dataset is Visually Grounded Reasoning dataset similar to MaRVL and was introduced by (Schneider and Sitaram, 2024). sample comprises two images, textual statement, and binary true or false answer 22 Name Tier ISO-639-3 ISO-639-1 Datasets Afrikaans Amharic Arabic Bengali Berber (macrolanguage) Breton Bulgarian Chinese Croatian Cusco Quechua Czech Danish Dutch Egyptian Arabic English Filipino Finnish French German Hausa Hebrew Hindi Hungarian Igbo Indonesian Irish Italian Japanese Javanese Kanuri Kinyarwanda Korean Malay (macrolanguage) Maori Mi-gkabau Modern Greek Mongolian Norwegian Oromo Persian Polish Portuguese Romanian Russian Sinhala Spanish Sundanese Swahili (macrolanguage) Swedish Tamil Telugu Thai Turkish Ukrainian Urdu Vietnamese Zulu 3 2 5 3 0 1 3 5 4 1 4 3 4 3 5 3 4 5 5 2 3 4 4 1 3 2 4 5 1 0 1 4 3 1 1 3 1 1 1 4 4 4 3 4 0 5 1 2 4 3 1 3 4 3 3 4 2 afr amh ara ben ber bre bul zho hrv quz ces dan nld arz eng fil fin fra deu hau heb hin hun ibo ind gle ita jpn jav kau kin kor msa mri min ell mon nor orm fas pol por ron rus sin spa sun swa swe tam tel tha tur ukr urd vie zul af am ar bn - br bg zh hr - cs da nl - en - fi fr de ha he hi hu ig id ga it ja jv kr rw ko ms mi - el mn no om fa pl pt ro ru si es su sw sv ta te th tr uk ur vi zu BabelImageNet-MC, M3Exam BabelImageNet-MC, CVQA, M5B-VGR, M5B-VLOD MTVQA, SMPQA, XM3600, xMMMU, XVNLI CVQA, M5B-VGR, M5B-VLOD, xGQA, XM3600 M5B-VGR, M5B-VLOD CVQA CVQA CVQA, M3Exam, MaRVL, MaXM, SMPQA, xGQA, XM3600 BabelImageNet-MC, XM3600 XM3600 BabelImageNet-MC, XM3600 XM3600 BabelImageNet-MC, XM3600 CVQA BabelImageNet-MC, M3Exam, M5B-VGR, M5B-VLOD, MaRVL, MaXM, MME, MMMU, SMPQA, xGQA, XM3600, xMMMU, XVNLI CVQA, M5B-VGR, M5B-VLOD, XM3600 BabelImageNet-MC, XM3600 MaXM, MTVQA, XM3600, xMMMU, XVNLI M5B-VGR, M5B-VLOD, MTVQA, SMPQA, xGQA, XM3600 BabelImageNet-MC, M5B-VGR, M5B-VLOD XM3600 M5B-VGR, M5B-VLOD, MaXM, SMPQA, XM3600, xMMMU BabelImageNet-MC, XM3600 CVQA CVQA, MaRVL, SMPQA, xGQA, XM3600, xMMMU CVQA M3Exam, MTVQA, SMPQA, XM3600 BabelImageNet-MC, CVQA, MTVQA, XM3600, xMMMU CVQA CVQA CVQA CVQA, SMPQA, xGQA, XM3600 CVQA BabelImageNet-MC, XM3600 CVQA BabelImageNet-MC, XM3600 CVQA BabelImageNet-MC, CVQA, XM3600 CVQA BabelImageNet-MC, XM3600 BabelImageNet-MC, XM3600 CVQA, M3Exam, xGQA, XM3600, xMMMU BabelImageNet-MC, CVQA, MaXM, XM3600 CVQA, M5B-VGR, M5B-VLOD, MTVQA, SMPQA, xGQA, XM3600, XVNLI CVQA BabelImageNet-MC, CVQA, XM3600, XVNLI CVQA CVQA, M5B-VGR, M5B-VLOD, MaRVL, XM3600 XM3600 BabelImageNet-MC, CVQA, MaRVL BabelImageNet-MC, CVQA, XM3600 M3Exam, M5B-VGR, M5B-VLOD, MaXM, MTVQA, SMPQA, XM3600 MaRVL, XM3600 XM3600 CVQA M3Exam, MTVQA, XM3600 BabelImageNet-MC, M5B-VGR, M5B-VLOD, SMPQA Unique Languages 56 (43 without CVQA) Table 11: List of languages covered in the datasets of our test suite. The Tier column represents the tier in the taxonomy proposed by Joshi et al. (2020), where higher tier indicates more available resources, i.e., data, in the respective language. CVQA is not used in 2 due to its hidden test set with limited submissions. grounded in the images. It comprises 12 languages covering 7 scripts and culturally diverse photos taken in regions where the respective language is spoken. The images are sampled from the Dollar Street (Gaviria Rojas et al., 2022) dataset. For each language, there are 120 samples. M5B-VLOD The M5B-VLOD (Visio-Linguistic Outlier Detection) dataset was introduced by (Schneider and Sitaram, 2024). sample comprises five images and textual statement that is true for all but one of the images. The task is to find the outlier image, that is, the image that does not match the statement. It comprises the same 12 languages as M5B-VGR and images sampled with similar strategy from the same dataset. For each language, there are 120 samples. MTVQA The MTVQA dataset was introduced by (Tang et al., 2024) and comprises text-heavy Visual Question Answering (VQA) tasks. It features human expert annotations across 9 diverse languages, consisting of total of 6778 questionanswer pairs across 2116 images. The images primarily contain text in the respective language and the question (and answer) related to that text. The images are sampled from different publicly available datasets. CVQA The CVQA dataset was introduced by (Romero et al., 2024) and is multilingual, culturally nuanced VQA benchmark that includes diverse set of languages, many of them underrepresented and understudied in NLP. It consists of 10000 questions across 30 countries, covering 31 languages, and in 39 distinct country-language pairs (e.g., the dataset includes 7 different splits for Spanish because it contains 7 countries where Spanish is spoken). The images in the dataset were manually gathered by human annotators to match and depict the culture of the respective countrylanguage pair. sample consists of one image and question related to the image in the respective language. The authors did not release the test set publicly but allowed up to 5 daily submissions to their leaderboard to obtain evaluation results. M3Exam The M3Exam dataset was introduced by (Zhang et al., 2023a). It contains real-world exam questions in 9 languages, which are either text-only or multi-modal. In our test suite, we only consider samples that require at least one image. Further, due to the low number of resulting samples for Swhalili and Javanese, we only include the remaining 7 languages. The remaining samples consist of multiple-choice questions in the target language and up to 8 images that can appear both in the question and the answer options. Further, the number of options ranges from 4 to 8 depending on the individual sample. xMMMU The xMMMU was introduced by (Yue et al., 2024b) and consists of college-level multiplechoice VQA samples across seven languages. It was automatically translated using GPT4o from subset of 300 randomly selected questions from the MMMU (Yue et al., 2023) validation split. C.5 Details for SMPQA (a) Example of bar plot in SMPQA for English. Questions for Grounding: \"Is the bar with label reward the biggest?\", \"Is the bar with label incredible the biggest?\", \"Is the bar with label reverse the smallest?\", \"Is the bar with label sunset the smallest?\", \"Is the bar with label closed colored in yellow?\", \"Is the bar with label closed colored in purple?\", \"Is the bar with label twitter colored in purple?\", \"Is the bar with label twitter colored in red?\" Questions for Reading: \"What is the label of the biggest bar?\", \"What is the label of the smallest bar?\", \"What is the label of the yellow bar?\", \"What is the label of the red bar?\", \"What is the label of the purple bar?\" (b) The same plot in Indonesian. Note that all questions refer to the same parts of the plot as the English version just with different words for labels. Questions for Grounding: \"Is the bar with label tertulis the biggest?\", \"Is the bar with label hidangan penutup the biggest?\", \"Is the bar with label korupsi the smallest?\", \"Is the bar with label segel the smallest?\", \"Is the bar with label penting colored in yellow?\", \"Is the bar with label penting colored in purple?\", \"Is the bar with label engkau colored in purple?\", \"Is the bar with label engkau colored in red?\" Questions for Reading: \"What is the label of the biggest bar?\", \"What is the label of the smallest bar?\", \"What is the label of the yellow bar?\", \"What is the label of the red bar?\", \"What is the label of the purple bar?\" Figure 3: Examples of one plot configuration in SMPQA for English and Indonesian. We propose SMPQA (Synthetic Multilingual Plot QA) as test dataset for evaluating multilingual OCR capabilities, that is capabilities to identify and read text in various languages in images, specifically bar plots and pie charts. We test the capabilities in two directions: i) grounding requires the model to ground given label in the user prompt to the corresponding part in the plot to answer yes/no question (Is the bar with label $X the biggest?); ii) reading requires the model to output the label of specified part of the plot (What is the label of the biggest slice?). 24 The questions are simple by design, requiring minimal reasoning, math, or multi-hop capabilities, as to isolate solely the OCR capabilities in the tested language. We show example plots and questions in Figure 3. We use exact match accuracy for both tasks. For reading, edit distance to the correct word would be fine-grained alternative but since word lengths differ between languages Chinese can be 1-2 characters while Indonesian can be >10 we opt against it to more easily compare results between languages. To have fair comparison between languages, we construct the dataset in way that plots and questions about them are identical between languages (except for labels in the respective languages, obviously). Construction: SMPQA is constructed with deterministic pipeline yielding identical results for each language. 1. We define list of diverse pie charts and bar plots by randomly sampling the number of bars/slices, the size of each, their colors, the plot size and aspect ratio, and vertical/horizontal orientation for bar plots, and exploding some slices in pie charts. For each plot type, we define 50 configurations, so we have 100 plots/images in total per language. 2. Using word lists of common words in the languages, we sample words for use as labels for the bars and pie slices to fill and ultimately render the pre-defined plots. This means the plots are identical between languages except for the labels and some size adjustments caused by different word lengths. 3. For each plot, we use templates to generate 5 questions for reading and 8 questions for grounding (with balanced yes and no as answers). The questions are always the same for plot, so each language has the same questions, just with different labels. Language Selection: We selected the languages as follows: For Latin-script languages, we chose English and one language from Tier 5 to 2 to have both highand low-resource languages: German, Italian, Indonesian, and Zulu. For non-Latin scripts, we select 6 languages to represent scripts with high usage in the world: Russian (Cyrillic), Chinese, Korean (Hangul), Hindi (Devanagari), Arabic, and Thai. HuggingFace Model ID Params Qwen/Qwen2-VL-2B-Instruct (Wang et al., 2024c) Qwen/Qwen2-VL-7B-Instruct (Wang et al., 2024c) microsoft/Phi-3.5-vision-instruct (Abdin et al., 2024a) neulab/Pangea-7B-hf (Yue et al., 2024b) openbmb/MiniCPM-V-2_6 (Yao et al., 2024b) meta-llama/Llama-3.2-11B-Vision-Instruct (AI@Meta, 2024) mistralai/Pixtral-12B-2409 (Agrawal et al., 2024) AIDC-AI/Parrot-7B (Sun et al., 2024b) MBZUAI/PALO-7B (Maaz et al., 2024a) MBZUAI/PALO-13B (Maaz et al., 2024a) OpenGVLab/InternVL2_5-4B (Chen et al., 2024e) OpenGVLab/InternVL2_5-8B (Chen et al., 2024e) maya-multimodal/maya (Alam et al., 2024) 2B 7B 4B 7B 8B 11B 12B 7B 7B 13B 4B 8B 8B Table 12: List of models considered in our evaluation experiments. Train Lang. T2 T3 T4 T5 en English T5 16.1 19.1 31.1 34.7 32.5 43.0 26.3 29.3 39.4 24.3 27.2 35.9 26.2 35.5 36.4 Without tasks affected by language fidelity: English T5 39.6 40.2 42.8 37.1 34.8 44.0 39.0 40.1 45.0 36.6 38.8 46.3 40.0 40.4 42.9 56.4 54.3 56. 54.6 53.5 55.3 Table 13: Experimental setup of Table 1 repeated with Llama 3 and the setups: just English, T5 languages, and L100 languages. scripts (Telugu, Greek, Hebrew, ...) or languages using the Latin script with heavy use of diacritics (Vietnamese, Turkish, ...). This makes SMPQA an ideal starting point for probing OCR capabilities in diverse languages. C.6 Baseline Models the evaluated baseline models in TaWe list ble 12. In all baseline evaluation experiments, we use greedy decoding (temperature=0.0; do_sample=False;). Further, we do not preprocess the images in any way and use the provided code for inference with the respective model. We use relaxed match accuracy for all tasks even if Centurio uses exact match for fairer comparison because some models struggled with replying just yes/no and similar issues."
        },
        {
            "title": "D Additional Experiments",
            "content": "D.1 Analysis Results with Llama 3 We note that our dataset construction can easily be extended to other languages if needed (as long as word lists are available) to test, for example, more We report the results with Llama 3 when repeating the experiments of 2.2, 2.3, 2.4 in Table 13, 14, 15. 25 English % T1 T3 T4 T5 en Distribution T2 T3 T4 T5 en 10 50 32.9 31.1 26.9 43.1 43.0 38.7 38.7 39.4 36.9 35.4 35.9 34.2 35.4 36.4 35.8 54.2 56.6 56. Uniform Stratified-1 Stratified-2 18.9 18.6 19.2 32.6 32.5 32.6 30.7 30.7 29.5 28.8 28.0 27.4 34.4 33.8 33. 52.6 53.0 52.0 Table 14: Experimental setup of Table 2 repeated with Llama 3 and the setups: 10, 50, and 90% English instruct tune data. Table 16: Comparison between our uniform allocation of data compared to two stratified allocations that upsample low-resource languages. English % No pretrain 100 1 31.1 33.9 37.8 T2 43.0 44.7 47.4 T3 39.4 43.3 45.0 35.9 39.9 41.1 T5 36.4 39.9 40.7 en 56.6 60.8 61.4 Table 15: Results of Table 3 repeated with Llama 3 and the setups: 1 and 100% English pre-train data. D.2 Non-Uniform Language Allocation In our experiments in 2, we distribute the nonEnglish portion of the data uniformly over all languages. We now consider two stratified distributions that upsample low-resource languages. language with taxonomy will get allocated the following portion of the non-English data: p(i) = (i) jTrainLanguages (j) (cid:80) (1) for Stratified-1 and (i) = 1 with (i) = 1 exp for Stratified-2. This effectively doubles the allocated data for T1 languages, and divides the data for T5 languages by factor 3 or 20 (depending on Stratified-1 or -2). Results are reported in Table 16. We do observe small decrease for T5 and T4 languages, and also for T3 with Stratified-2, but results for T1 and T2 languages stay relatively constant despite more data. This suggests that higher-resource languages can be quite sample efficient even with what amounts to few hundred samples (at least in the instruct tuning phase) but as the stratified distributions fail to improve lower-resource languages, there is little reason in practice to not use the uniform distribution which makes no assumptions about the resource-level of language. D.3 LLM Comparison We train several recent 7-9B parameter LLMs on the instruct tuning data mix used in our analysis with L100 languages and 50% English. All models are trained with the same hyperparameters. We LLM T1 T2 T3 T4 T5 en Phi-3.5-mini-instruct gemma-2-9b-it Meta-Llama-3-8B-Instruct Qwen2.5-7B-Instruct aya-expanse-8b 18.9 29.2 31.1 30.7 28.3 32.6 40.9 43.0 43.7 42.5 30.7 36.4 39.4 42.0 43.0 28.8 33.5 35.9 38.1 39.8 34.4 35.3 36.4 40.5 40. 52.6 52.8 56.6 62.7 59.9 Table 17: Comparison between different LLM backbones all trained with the instruct tuning data with L100 languages and 50% English (as in 2.3). compare Llama 3 (Llama Team, 2024)13, Gemma 2 (Gemma Team, 2024), Aya-Expanse (Aryabumi et al., 2024; Dang et al., 2024), and Qwen 2.5 (Yang et al., 2024). Table 17 shows that Qwen and Aya yield the overall best results with Aya slightly ahead in T3T5 and Qwen (with Llama 3) better in T1 and T2. Qwen achievs by far the best English results. Qwen is also notably strong in the exam tasks (M3Exam, xMMMU) and is the only model with better-thanguessing results on average for VLOD."
        },
        {
            "title": "E Qualitative Examples",
            "content": "We provide some qualitative examples of our Centurio models. Figure 4, 5, and 6 show results with (non-English) text in images with English prompts. Figure 7 and 8 show examples for multilingual prompts (and responses)."
        },
        {
            "title": "F Full Results",
            "content": "We report the full results of all tasks with all language-specific results. Avg. refers to the average without English. Metric are the same as in C. F.1 Analysis Experiments We report the full results for all models trained for 2 (and also the LLMs tested for D.3). The following holds for all Tables: Models of the form Phi 3.5 - T5 50 are to be interpreted as using the LLM Phi 3.5 with the T5 languages and 13While Llama 3.1 has officially better multilingual capabilities, we found Llama 3 to work better. 26 Can you explain the joke of this German meme? Centurio Aya: The humor in this image comes from juxtaposing biblical figure, Jesus, with an everyday scene that is both ordinary and unexpected. The mans appearance as Jesus, complete with long hair and beard, contrasts with the mundane setting of caravan park populated by caravans (trailers). This creates humorous effect because it takes something familiar (Jesus) and places it into context that is not only unfamiliar but also somewhat banal or trivial. It plays on the idea of finding divinity in the most unexpected places, which can be seen as commentary on how we perceive holiness or significance. Additionally, the caption \"Jesus und seine Anhänger\" further emphasizes the absurdity by referencing followers, implying that even in such an unremarkable location, one could find disciples or supporters, thus adding another layer to the comedic effect. Centurio Qwen: The joke in this German meme plays on pun. The phrase \"Jesus und seine Anhänger\" translates to \"Jesus and his followers.\" However, the word \"Anhänger,\" which means \"followers,\" is also slang for caravans or RVs (recreational vehicles) in some contexts. Therefore, the image humorously suggests that Jesus has modern-day followers who travel together in caravans instead of following him spiritually as he did with his original disciples. This juxtaposition creates humorous effect by combining religious iconography with contemporary culture. Figure 4: Centurio explaining German meme. Both models correctly read the caption but only Qwen understands the pun and gets the explanation (mostly) correct. Summarize the table for me. Centurio Aya: This is detailed summary of the table: The table compares Centurio and 13 other LLM models on 14 different tasks, using CIDEr as an evaluation metric. It presents accuracy scores (AVG., MTXMUL) and their standard deviations (SD.) across various datasets like SQuAD & N, Image captioning, etc. Models are listed in descending order based on AVG score, with Paratrol being at the top followed by others like PALO-TB, Lama Vision, etc. Theres also information about model names, task descriptions, and additional metrics like BIN_MC, XNLI, MARVL, VGRU, VLOD, and C.Q.A. note explains that only one image input was used for evaluating language fidelity over all languages, and it mentions \"CENTURIO QEN\" which might be specific version or variant of the Centurio model. The bottom part of the table provides further details about the evaluation setup and models performance on these tasks. Overall, this table seems to evaluate the Centurio model against several others in terms of its ability to perform well on multiple NLP tasks. Centurio Qwen: Here summary: The best model is Centurio Aya. The worst model is Palo 7B. The average score of all models is around 50%. The most accurate model (Centurio Aya) has an accuracy over 63% in all tasks, while the least accurate one (Palo 7B) has an accuracy under 28%. In multilingual results, the best model is also the most accurate one and vice versa. The most accurate model has an overall language fidelity over 94%, whereas the least accurate one has an overall language fidelity below 13%. All models have an input supported AVG. above 40%. The highest number of errors made by any model was 128, which occurred on the Inten/VL 2.5 SB task. The lowest number of errors made by any model was 1, which occurred on the MiniCPM 2.6 task. What model is best? Explain why. Centurio Aya: Centurio! QEN, with an AVG., MAMM/U 32.0 and C.QAL 41.8. It has the highest average over all tasks compared to other models listed in table 6, indicating its superior performance across multiple linguistic tests. Centurio Qwen: Centurio Aya CenturioQueen. (20.3). It has the highest accuracy score (65.7) and lowest error rate Figure 5: Centurio struggle with images containing small text. 28 What is written on this bottle? Centurio Aya: The text on the bottle, in German, reads \"Sasxia Natürliches Mineralwasser\" which translates to \"Sasxia Natural Mineral Water.\" Below that, it says \"Wort am Rhein,\" likely referring to specific source or brand name associated with the Rhine River. The phrase \"ohne Kohlensäure\" indicates that the water contains no carbonation. Theres also some additional information such as size (\"0,5L\") and possibly an expiration date or batch number at the bottom of the label, but these are not clearly visible in the image provided. Centurio Qwen: The text on the label of the bottle reads \"Saskia,\" followed by description in German that translates to \"Natural Mineral Water\" and \"Mineral water from Württemberg am Rhein.\" Below, it states \"still,\" indicating the type of mineral water. The bottom part mentions \"ohne Kohlenstoffe,\" which means \"without carbonation,\" confirming that the water is still (non-carbonated). Additionally, theres logo or emblem above the brand name \"Saskia,\" but due to the angle and focus of the image, specific details about the logo cannot be discerned. There are also some numbers at the very top right corner of the label, likely related to product information such as batch number or size, but they are not clearly visible. The overall design suggests natural, possibly organic origin for the mineral water. However, without clearer visibility of all elements, more detailed information would require further examination or context. If youre looking for precise nutritional facts or chemical composition, those specifics might not be available from the visual content alone. For accurate information regarding the contents or any health-related aspects of the beverage, consulting the official product packaging or manufacturers website would be advisable. Please note: This analysis focuses solely on what can be seen within the provided image and does not include speculative or unverifiable claims beyond what is directly observable. Any additional interpretation goes beyond the scope of the given visual data. If you need assistance with something else or have another question, feel free to ask! In summary: Brand Name: Saskia Type: Natural Mineral Water Origin: Württemberg am Rhein Carbonation Status: Still (NonCarbonated) Additional Information: Without Carbon Dioxide (Kohlensäure) Label Design Suggests Organic/Healthy Origin Specific Nutritional Facts or Chemical Composition Not Clearly Visible in Image For comprehensive details, refer to the actual product packaging or manufacturers resources. Figure 6: Centurio summarizing the label of water bottle. While mostly correct, there are some OCR mistakes, e.g., with the city name (Wörth) 29 Opišite sliku što detaljnije. (Describe the image in as much detail as possible.) Centurio Aya: Slika prikazuje mirno živopisno prizoru vodopada koji se nalazi bujnim zelenim krajoliku. Vodopad, više padina, stvara dinamiˇcnu scenu sa ˇcistim plavom vodi koja kontrastira sa okolnom zelenilom. Okolište je gusto naseljen drvecem, nekoliko oblaka može se vidjeti na nebu, dodajuci dubinu sceni. (The image depicts peaceful and picturesque scene of waterfall set in lush green landscape. The waterfall, with multiple slopes, creates dynamic scene with its clear blue water contrasting with the surrounding greenery. The surroundings are densely populated with trees, and few clouds can be seen in the sky, adding depth to the scene.) Centurio Qwen: Slika prikazuje mirnu vodopadsku scenu, karakterizirana svojim više padavina koji se spuštaju ˇcisto plavo-zeleno jezero ispod njih. Voda izgleda glatko neometano, sugerirajuci dubinu ˇcistost. Okolišna vegetacija je bogata živahna, ukazujuci na dobro održavan ekosistem sa obiljem svježe vode. Nebo je jasno plavo, minimalnim oblaci, što sugeriše sunˇcan dan bez vidljive atmosferske promene ili udaljenog doga daja. Sama vodopada nema ljudskih elemenata kao što su mostovi, klupi ili barijeri, što bi moglo znaˇciti da je lokacija priloživa možda nepovremena za putnike. (The image depicts tranquil waterfall scene, characterized by its multiple falls cascading into clear blue-green lake below. The water appears smooth and undisturbed, suggesting depth and purity. The surrounding vegetation is rich and vibrant, indicating well-maintained ecosystem with an abundance of fresh water. The sky is clear and blue, with minimal clouds, suggesting sunny day with no visible atmospheric change or distant event. The waterfall itself has no human elements such as bridges, benches, or barriers, which could indicate that the location is approachable and perhaps uninhabited by travelers.) Figure 7: Centurio describing an image in Croatian. Translations by Google Translate are provided in brackets for the reader. 30 Cine este prezentat în acest portret? (Who is shown in this portrait?) Centurio Aya: Portretul prezinta un om care pare fi figura istorica sau fictiva, probabil de rang înalt datorita îmbracamintei sale regale s, bijuteriilor. Stilul picturii sugereaza ca ar putea fi din perioada Renas, terii sau baroca, având în vedere detaliile meticuloase ale costumului s, tehnica picturilor cu ulei. Fara context suplimentar, nu este posibil sa se identifice persoana exacta reprezentata, dar hainele s, accesoriile sunt caracteristice celor care au det,inut pozit,ii semnificative în istoria europeana. (The portrait depicts man who appears to be historical or fictional figure, likely of high rank due to his regal attire and jewelry. The style of the painting suggests that it may be from the Renaissance or Baroque period, given the meticulous detail of the costume and the oil painting technique. Without further context, it is not possible to identify the exact person depicted, but the clothing and accessories are characteristic of those who held significant positions in European history.) Centurio Qwen: Persoana din portret pare fi un om de natare, probabil figura istorica sau nobila, având în vedere îmbracamintea s, simbolismul capului. În mod specific, el poarta coroana cu modele complicate, care ar putea indica putere sau statut. Stila îmbracamintei sugereaza ca ar putea fi din secolul al XVIII-lea, ceea ce indica faptul ca figura poate fi asociata cu aceasta epoca. Fara context suplimentar, nu este posibil sa se determine identitatea exacta, dar caracteristicile sale distincte sugereaza ca ar putea fi un lider politic, un regiune sau un alt individ important dintr-o perioada istorica. Portretul serves, te pentru transmite autoritatea s, statusul sa prin utilizarea elementelor visuale ale îmbracamintei s, accesoriilor. Este important de ment,ionat ca, des, detaliul exact al persoanei este neclar, natura portretului sugereaza ca ea are un rol semnificativ în istoria respectiva. Portretul serves, te pentru transmite autoritatea s, statusul sa prin utilizarea elementelor vizuale ale îmbracamintei s, accesoriilor. (The person in the portrait appears to be man of character, perhaps historical or noble figure, given the clothing and symbolism of the head. Specifically, he is wearing an intricately patterned crown, which could indicate power or status. The style of the clothing suggests that it may be from the 18th century, indicating that the figure can be associated with this era. Without additional context, it is not possible to determine the exact identity, but his distinctive features suggest that he may be political leader, regional leader, or another important individual from historical period. The portrait serves to convey his authority and status through the use of visual elements of his clothing and accessories. It is important to note that while the exact detail of the person is unclear, the nature of the portrait suggests that he has significant role in the history in question. The portrait serves to convey his authority and status through the use of visual elements of his clothing and accessories.) Figure 8: Centurio answering question in Romanian at length. Still, neither model correctly identifies the famous portrait of Vlad III Dracula and both models are wrong with their guess of creation time (16th century). Translations by Google Translate are provided in brackets for the reader. 50% English with analog interpretation for other rows. Phi 3.5 - PT 1 means the model was pretrained with 1% English and then instruct-tuned with the L100 50% English mix (see 3). Phi 3.5 - OCR 1 means the model was pretrained with 50% English for the captions and 1% English for the OCR data and then instruct-tuned with the L100 50% English mix (see 4). BIN-MC Table 18 M3Exam Table 19 VGR Table VLOD Table 21 MaRVL Table 22 MaXM Table 23 MTVQA Table 24 xGQA Table 25 XM3600 Table 26. Language fidelity for 2.2 in Table 27. XVNLI Table 28 xMMMU Table 29 SMPQA - Ground Table 30 SMPQA - Name Table 31 F.2 Comparison with Centurio BIN-MC Table M3Exam Table 33 VGR Table 34 VLOD Table 35 MaRVL Table 36 MaXM Table 37 MTVQA Table xGQA Table 39 XM3600 Table 40. Language fidelity Table 41. XVNLI Table 42 xMMMU Table 43 CVQA Table 44 SMPQA - Ground Table SMPQA - Name Table 46 32 Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen en 64.7 66.0 65.2 65.5 64.8 64.7 65.4 63.9 66.2 63.1 62.7 63.3 64.7 65.4 64.7 65.9 66.2 64.4 64.7 66.3 65.7 65.8 66.2 69.6 68.7 60.5 66.2 68.2 67.6 69.7 72.7 avg. 38.1 39.6 40.6 40.6 39.1 39.9 40.9 43.7 48.8 39.7 39.4 40.4 39.9 39.8 37.5 49.8 48.8 45.3 39.9 38.9 42.2 42.8 48.8 55.5 53.6 44.8 48.8 50.6 52.0 54.7 56.2 af 43.3 46.0 46.8 50.0 47.2 48.1 44.0 50.6 55.3 47.4 47.1 48.0 48.1 47.1 43.8 58.4 55.3 52.5 48.1 48.4 50.0 50.1 55.3 62.4 63.4 49.1 55.3 62.4 62.2 63.6 65.3 am 29.7 30.3 29.6 28.8 25.6 28.2 28.2 28.7 35.1 26.8 27.1 29.0 28.2 26.0 24.1 38.1 35.1 26.8 28.2 25.0 37.8 35.1 35.1 44.0 36.8 42.5 35.1 37.1 31.0 29.4 47.4 cs 41.5 43.1 44.6 43.3 41.9 42.8 46.9 49.2 54.2 42.9 43.1 43.3 42.8 42.0 40.3 55.0 54.2 51.0 42.8 42.7 44.2 44.8 54.2 60.4 59.6 47.5 54.2 57.9 65.3 66.2 62.2 el 35.5 36.3 37.9 37.4 35.8 36.8 42.2 46.4 51.2 36.7 36.8 37.7 36.8 37.1 35.8 50.9 51.2 47.2 36.8 36.0 40.0 38.9 51.2 60.4 58.1 45.3 51.2 50.8 65.5 67.8 56.7 es 55.9 56.3 59.1 58.6 57.9 57.2 53.0 54.6 56.2 56.2 56.5 56.5 57.2 57.4 57.1 58.5 56.2 54.8 57.2 57.3 57.8 56.9 56.2 62.9 62.5 52.0 56.2 63.4 63.2 65.1 67.0 fa 33.6 33.4 36.7 34.4 34.0 34.7 42.4 46.6 47.6 34.3 34.4 35.2 34.7 34.7 31.8 49.3 47.6 45.9 34.7 33.2 36.0 37.9 47.6 55.3 54.1 44.8 47.6 49.6 58.9 60.0 53.6 fi 36.4 36.5 37.5 38.4 36.0 37.0 38.7 41.7 46.2 35.9 36.5 36.7 37.0 36.9 35.7 45.7 46.2 44.0 37.0 36.5 36.5 37.5 46.2 51.7 50.8 41.6 46.2 42.6 39.8 43.3 48.8 ha 24.5 35.1 29.0 33.0 29.8 28.2 31.1 35.4 37.2 33.5 29.3 32.4 28.2 32.2 25.0 40.7 37.2 29.5 28.2 22.3 33.0 41.2 37.2 40.4 37.5 30.9 37.2 28.7 33.2 37.5 36.7 hr 43.3 45.1 46.4 46.1 44.8 44.7 47.6 50.7 56.1 46.8 43.8 46.9 44.7 44.2 43.1 59.4 56.1 54.1 44.7 44.8 45.2 49.1 56.1 63.0 63.1 50.7 56.1 61.0 60.8 63.6 65.4 hu 39.0 40.7 42.5 41.4 39.5 40.4 46.3 50.8 54.1 40.5 40.9 40.7 40.4 40.4 39.2 56.3 54.1 50.0 40.4 39.8 41.8 42.1 54.1 62.1 61.6 47.6 54.1 48.3 46.3 49.8 54.1 ja 49.8 50.9 52.0 50.9 50.0 51.2 48.6 51.9 53.3 49.0 49.8 50.7 51.2 51.3 49.1 54.1 53.3 51.2 51.2 49.9 49.3 49.6 53.3 59.9 60.7 51.4 53.3 63.1 65.1 66.7 67.6 mi 27.8 30.1 31.1 31.2 30.5 31.6 30.1 30.0 33.7 32.7 29.8 33.0 31.6 31.5 24.9 34.9 33.7 31.3 31.6 31.3 35.0 33.4 33.7 36.6 36.9 32.8 33.7 33.5 33.1 37.0 39.1 nl 47.3 48.4 50.7 49.7 47.7 47.8 48.2 51.2 54.6 48.4 47.2 49.3 47.8 49.4 47.9 53.7 54.6 52.1 47.8 48.6 49.0 49.6 54.6 59.4 59.9 49.8 54.6 58.8 61.3 62.4 63.7 no 44.2 46.2 47.4 47.0 45.8 46.4 47.4 50.9 54.3 46.7 48.2 47.2 46.4 46.8 44.4 56.8 54.3 51.8 46.4 46.4 48.1 48.2 54.3 62.1 61.0 51.1 54.3 58.2 55.5 59.1 63.6 pl 41.4 41.1 43.0 41.9 41.2 40.9 44.0 47.0 50.8 41.3 41.4 41.8 40.9 41.6 39.3 51.8 50.8 48.6 40.9 41.3 42.0 43.6 50.8 58.0 58.0 47.2 50.8 57.2 60.2 62.6 60.4 ro 42.8 43.1 43.5 43.8 42.4 43.8 44.9 48.4 51.9 43.1 43.6 44.4 43.8 42.9 42.8 51.3 51.9 49.8 43.8 43.2 44.1 45.9 51.9 58.6 58.0 47.5 51.9 55.4 61.9 64.0 58.5 ta 30.0 31.0 31.5 32.5 30.4 30.6 31.6 31.1 43.6 29.0 30.2 31.9 30.6 31.1 28.7 42.6 43.6 36.5 30.6 30.5 33.7 34.9 43.6 50.6 46.5 41.8 43.6 36.8 43.5 46.9 45.2 te 27.1 29.6 29.0 29.6 29.2 30.1 32.5 35.6 50.9 29.9 28.0 31.2 30.1 28.5 27.7 51.9 50.9 48.3 30.1 30.8 37.7 36.1 50.9 60.6 54.0 45.1 50.9 55.6 43.2 50.9 63.4 zu 31.1 29.1 32.4 32.7 33.9 37.5 29.3 30.1 40.8 34.2 34.4 31.1 37.5 34.2 31.9 36.0 40.8 34.7 37.5 30.6 40.6 38.5 40.8 45.7 34.9 32.1 40.8 40.6 37.2 42.6 49.5 Table 18: BIN-MC 33 Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen en 52.9 51.2 52.2 51.3 49.2 50.8 46.1 45.0 46.6 50.3 48.8 50.8 50.8 48.0 51.7 43.7 46.6 43.3 50.8 50.3 49.7 48.4 46.6 50.2 52.9 42.5 46.6 53.6 49.3 53.0 61.2 avg. 32.7 35.3 34.2 35.3 33.7 36.0 32.5 33.8 34.2 35.1 33.9 36.5 36.0 36.1 35.1 33.6 34.2 34.6 36.0 35.8 33.1 33.8 34.2 37.9 37.1 33.4 34.2 39.6 36.5 41.2 46.9 af 32.5 39.9 40.5 43.6 39.3 39.3 38.6 40.5 44.2 39.9 35.0 41.7 39.3 44.2 36.8 41.7 44.2 37.4 39.3 41.7 41.1 41.7 44.2 44.8 50.3 43.6 44.2 46.0 46.6 52.8 50. zh 37.0 35.9 32.4 34.0 32.9 36.1 32.6 34.3 31.0 35.4 33.6 37.0 36.1 35.9 38.0 29.4 31.0 32.2 36.1 37.5 36.1 35.9 31.0 34.7 33.8 33.6 31.0 44.7 36.8 40.3 64.1 Table 19: M3Exam it 49.6 46.4 49.1 47.4 45.1 50.9 41.6 41.9 42.4 46.6 48.1 51.6 50.9 47.1 48.1 44.9 42.4 44.9 50.9 49.4 44.4 46.4 42.4 48.1 46.6 41.6 42.4 50.6 51.9 51.4 55.6 pt 39.7 39.7 38.6 37.3 38.4 40.1 35.0 34.1 34.6 39.2 36.1 35.9 40.1 38.4 36.8 35.3 34.6 35.3 40.1 36.6 35.0 34.8 34.6 40.6 37.5 30.4 34.6 42.4 39.0 47.7 49.0 th 25.4 28.2 25.2 27.9 22.2 26.2 25.9 25.7 27.2 23.9 24.7 27.7 26.2 26.7 26.4 23.7 27.2 30.2 26.2 24.2 21.7 23.2 27.2 31.4 30.2 27.7 27.2 29.7 26.2 27.4 31.9 vi 12.2 21.7 19.1 21.7 24.3 23.5 20.9 26.1 26.1 25.2 26.1 25.2 23.5 24.3 24.3 27.0 26.1 27.8 23.5 25.2 20.0 20.9 26.1 27.8 24.3 23.5 26.1 24.3 18.3 27.8 29.6 Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen en 80.8 75.8 83.3 83.3 81.7 76.7 82.5 77.5 80.0 65.0 73.3 73.3 76.7 80.0 79.2 77.5 80.0 82.5 76.7 80.8 80.0 80.0 80.0 87.5 85.0 77.5 80.0 91.7 92.5 82.5 87.5 en 16.7 23.3 17.5 25.8 21.7 18.3 12.5 20.8 12.5 24.2 23.3 25.0 18.3 16.7 22.5 13.3 12.5 12.5 18.3 23.3 20.0 25.0 12.5 19.2 13.3 14.2 12.5 26.7 12.5 12.5 28. avg. 54.1 50.9 55.1 56.6 57.5 56.4 56.3 55.9 64.8 47.5 54.5 60.3 56.4 56.7 54.6 65.4 64.8 63.0 56.4 58.6 63.2 62.0 64.8 71.2 68.8 61.8 64.8 71.2 69.9 66.8 73.1 avg. 21.3 20.0 18.2 19.8 20.5 19.5 20.8 20.1 20.6 19.3 19.2 18.3 19.5 18.0 19.0 20.4 20.6 19.9 19.5 20.0 18.6 19.4 20.6 20.5 20.8 21.1 20.6 27.3 20.7 20.7 27.0 fil 55.8 55.0 65.8 69.2 64.2 60.0 50.8 63.3 61.7 58.3 60.0 55.0 60.0 67.5 60.0 58.3 61.7 58.3 60.0 56.7 63.3 62.5 61.7 63.3 61.7 51.7 61.7 77.5 85.0 59.2 78. fil 25.8 16.7 17.5 20.0 27.5 20.0 24.2 25.0 24.2 29.2 20.8 16.7 20.0 16.7 16.7 23.3 24.2 22.5 20.0 25.0 14.2 16.7 24.2 23.3 23.3 25.0 24.2 27.5 24.2 24.2 29.2 am ber 45.0 49.2 51.7 43.3 45.8 46.7 66.7 47.5 58.3 42.5 43.3 54.2 46.7 51.7 43.3 65.0 58.3 45.8 46.7 44.2 58.3 55.8 58.3 70.0 65.8 64.2 58.3 76.7 52.5 71.7 77.5 50.8 49.2 43.3 50.8 52.5 46.7 30.8 49.2 47.5 50.0 50.0 47.5 46.7 53.3 50.0 45.0 47.5 39.2 46.7 49.2 50.0 50.0 47.5 50.8 49.2 52.5 47.5 50.0 54.2 54.2 49.2 bn 41.5 40.7 49.2 50.8 44.1 54.2 49.2 49.2 64.4 38.1 51.7 58.5 54.2 55.1 44.9 63.6 64.4 66.1 54.2 56.8 55.1 51.7 64.4 65.3 67.8 48.3 64.4 69.5 55.9 59.3 62.7 de 71.7 72.5 70.8 74.2 73.3 71.7 77.5 71.7 75.8 65.0 67.5 72.5 71.7 70.8 80.8 76.7 75.8 80.8 71.7 78.3 78.3 81.7 75.8 79.2 80.8 70.8 75.8 81.7 80.8 73.3 80.8 Table 20: VGR am ber 20.8 15.0 19.2 16.7 21.7 16.7 18.3 18.3 19.2 15.0 23.3 20.8 16.7 15.0 20.0 18.3 19.2 18.3 16.7 16.7 18.3 21.7 19.2 15.8 18.3 18.3 19.2 25.0 18.3 18.3 18. 20.8 18.3 20.8 17.5 18.3 20.8 21.7 19.2 20.8 21.7 15.0 18.3 20.8 20.0 16.7 21.7 20.8 21.7 20.8 16.7 17.5 22.5 20.8 19.2 21.7 22.5 20.8 21.7 21.7 21.7 20.0 bn 19.2 20.8 13.3 21.7 16.7 19.2 20.0 17.5 20.0 17.5 16.7 16.7 19.2 19.2 15.8 19.2 20.0 15.0 19.2 24.2 15.0 19.2 20.0 22.5 20.0 20.8 20.0 26.7 20.0 20.0 33.3 de 16.7 21.7 20.8 21.7 22.5 25.8 10.8 16.7 10.8 20.0 21.7 20.8 25.8 19.2 20.0 10.8 10.8 10.8 25.8 20.0 15.8 22.5 10.8 15.0 12.5 10.8 10.8 27.5 10.8 11.7 32.5 Table 21: VLOD 35 ha 41.7 42.5 42.5 42.5 39.2 45.0 48.3 42.5 67.5 40.0 45.0 58.3 45.0 41.7 42.5 70.8 67.5 68.3 45.0 47.5 60.0 60.0 67.5 83.3 70.0 64.2 67.5 57.5 72.5 65.0 76.7 ha 28.3 20.0 16.7 21.7 27.5 16.7 29.2 21.7 30.0 22.5 20.8 20.8 16.7 23.3 23.3 26.7 30.0 28.3 16.7 21.7 21.7 15.8 30.0 23.3 29.2 28.3 30.0 25.0 29.2 29.2 22.5 hi 62.7 54.2 61.9 57.6 59.3 57.6 63.6 62.7 64.4 45.8 51.7 60.2 57.6 63.6 55.9 64.4 64.4 61.9 57.6 65.3 61.9 66.1 64.4 68.6 66.9 58.5 64.4 72.9 79.7 71.2 72.9 hi 17.0 23.2 21.4 23.2 17.9 25.0 15.2 24.1 15.2 17.9 20.5 17.9 25.0 17.0 21.4 17.9 15.2 15.2 25.0 19.6 17.9 20.5 15.2 17.9 17.0 16.1 15.2 29.5 15.2 15.2 25.0 ru 85.0 60.8 70.8 76.7 73.3 70.8 75.8 73.3 73.3 58.3 63.3 72.5 70.8 75.8 77.5 74.2 73.3 75.0 70.8 75.0 76.7 75.0 73.3 82.5 85.0 71.7 73.3 83.3 83.3 75.8 85.0 ru 12.5 27.5 26.7 20.8 21.7 20.8 12.5 15.0 10.8 14.2 24.2 21.7 20.8 13.3 23.3 10.0 10.8 10.8 20.8 15.0 23.3 21.7 10.8 13.3 10.8 11.7 10.8 25.0 10.8 10.8 22.5 sw 35.8 37.5 38.3 43.3 60.0 57.5 46.7 45.8 59.2 39.2 53.3 64.2 57.5 38.3 45.0 63.3 59.2 63.3 57.5 47.5 55.0 50.0 59.2 66.7 70.0 54.2 59.2 71.7 63.3 67.5 70.0 sw 25.0 22.3 16.1 18.8 17.0 13.4 28.6 19.6 28.6 16.1 10.7 14.3 13.4 17.9 16.1 28.6 28.6 28.6 13.4 20.5 20.5 16.1 28.6 25.9 28.6 27.7 28.6 29.5 28.6 28.6 30.4 th 68.3 60.8 75.0 71.7 60.8 65.8 70.0 70.8 67.5 42.5 63.3 59.2 65.8 69.2 55.8 69.2 67.5 75.0 65.8 73.3 75.0 66.7 67.5 85.8 74.2 70.8 67.5 80.8 78.3 72.5 81.7 th 26.7 15.8 10.0 16.7 13.3 17.5 29.2 23.3 27.5 22.5 15.8 16.7 17.5 15.8 15.8 28.3 27.5 28.3 17.5 20.0 20.8 15.0 27.5 28.3 28.3 30.0 27.5 40.0 29.2 29.2 30.0 zu 36.2 37.9 36.2 42.2 59.5 44.0 39.7 38.8 73.3 42.2 50.0 61.2 44.0 36.2 44.8 69.0 73.3 59.5 44.0 50.9 61.2 62.1 73.3 68.1 65.5 73.3 73.3 62.1 63.8 65.5 69.0 zu 22.0 18.6 17.8 18.6 21.2 18.6 19.5 20.3 19.5 16.1 22.0 16.9 18.6 20.3 19.5 19.5 19.5 19.5 18.6 20.3 19.5 22.0 19.5 21.2 19.5 20.3 19.5 22.9 19.5 19.5 33.1 Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen en 82.1 81.5 81.2 81.5 79.7 79.6 85.2 84.5 83.7 71.9 74.1 76.0 79.6 81.7 83.1 80.0 83.7 85.1 79.6 82.0 82.5 81.9 83.7 87.5 86.5 82.5 83.7 89.6 87.0 85.0 89.6 avg. 61.4 61.8 64.3 65.9 66.4 64.4 65.0 67.1 74.2 61.4 63.4 61.6 64.4 64.7 64.3 72.9 74.2 71.1 64.4 65.6 69.9 67.9 74.2 80.4 78.9 73.0 74.2 79.4 80.2 77.9 81. id 65.6 66.4 68.7 70.8 70.2 69.0 68.8 73.8 75.3 65.1 66.8 63.4 69.0 71.3 70.7 71.9 75.3 73.4 69.0 68.6 75.2 74.0 75.3 82.5 81.3 72.6 75.3 84.8 83.9 79.5 85.0 sw 50.8 53.4 52.3 56.4 62.2 59.0 52.5 55.7 71.4 56.1 58.1 57.6 59.0 54.4 56.3 70.8 71.4 63.7 59.0 59.4 64.0 64.0 71.4 75.5 73.0 71.4 71.4 73.9 75.6 70.9 76.8 Table 22: MaRVL ta 53.3 53.7 54.3 56.7 57.5 53.6 54.3 53.6 68.4 54.3 57.2 56.9 53.6 56.1 53.8 71.7 68.4 65.1 53.6 57.9 64.1 60.2 68.4 77.1 75.1 68.3 68.4 65.2 71.7 73.4 76.0 tr 63.8 61.6 70.2 68.9 66.7 67.5 69.7 72.7 79.8 65.2 65.1 64.0 67.5 64.8 62.8 75.7 79.8 75.7 67.5 67.6 71.1 68.0 79.8 84.5 83.4 76.4 79.8 86.6 86.9 83.4 84.2 zh 73.2 73.8 76.2 76.7 75.4 73.0 79.8 79.6 76.0 66.1 70.0 66.3 73.0 77.0 77.8 74.2 76.0 77.6 73.0 74.5 74.9 73.4 76.0 82.3 81.5 76.2 76.0 86.6 83.0 82.4 86.7 Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen en 53.0 51.3 51.0 53.7 53.4 54.4 55.4 41.3 52.7 48.0 52.0 50.7 54.4 51.0 54.7 49.0 52.7 52.7 54.4 54.0 53.4 55.7 52.7 55.0 58.1 51.7 52.7 58.7 53.4 55.7 60.1 avg. 9.2 25.6 33.1 36.7 35.9 36.6 7.7 20.2 42.3 33.8 35.4 35.1 36.6 32.5 29.7 41.9 42.3 40.6 36.6 36.2 39.0 39.7 42.3 48.5 47.4 41.5 42.3 45.8 47.2 49.3 47.7 fr 14.3 41.0 45.4 41.0 42.3 43.0 9.2 45.1 42.3 39.9 44.7 44.0 43.0 42.0 41.6 37.9 42.3 43.3 43.0 44.0 45.7 44.7 42.3 47.4 44.7 39.6 42.3 46.4 46.4 45.1 47.1 hi 11.9 30.6 50.7 45.9 48.0 48.0 10.9 12.6 54.4 45.2 45.6 44.6 48.0 36.4 28.2 53.4 54.4 52.7 48.0 48.6 49.3 52.0 54.4 57.1 54.8 52.4 54.4 51.4 58.8 62.9 56.8 Table 23: MaXM he 7.9 17.5 27.0 33.0 33.3 30.8 6.7 2.9 40.6 32.4 34.6 29.8 30.8 29.8 27.3 45.7 40.6 36.2 30.8 32.4 39.4 41.0 40.6 56.2 54.0 44.1 40.6 50.2 59.4 58.7 45. ro 7.2 15.6 23.7 36.6 35.1 35.1 4.5 24.3 40.5 32.4 36.0 33.0 35.1 33.3 28.5 41.4 40.5 40.2 35.1 33.9 36.6 40.8 40.5 47.4 47.1 39.3 40.5 41.7 49.9 51.1 47.7 th 7.0 27.5 32.5 40.4 32.8 39.1 8.3 14.6 52.6 32.8 29.5 38.1 39.1 31.8 30.5 51.0 52.6 49.0 39.1 36.8 40.7 40.1 52.6 57.3 57.3 48.7 52.6 57.9 41.4 46.7 57.0 zh 7.2 21.5 19.5 23.5 23.8 23.5 6.8 21.8 23.1 19.9 22.1 21.2 23.5 21.8 21.8 21.8 23.1 22.1 23.5 21.5 22.1 19.9 23.1 25.7 26.4 24.8 23.1 27.0 27.4 31.6 32. 37 Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen avg. 3.2 5.7 5.9 5.8 6.6 6.3 3.2 5.6 6.0 4.7 5.7 6.2 6.3 6.3 7.0 5.3 6.0 6.5 6.3 6.9 6.1 6.2 6.0 6.9 8.3 4.3 6.0 6.4 6.2 11.1 11.9 ar 0.9 1.7 2.7 2.0 5.3 2.8 0.3 2.0 2.1 2.0 3.0 3.6 2.8 2.6 2.6 1.6 2.1 2.1 2.8 3.7 1.8 1.6 2.1 2.4 2.6 1.7 2.1 5.5 3.7 6.7 4. de 6.5 12.0 14.0 13.5 15.9 15.8 6.9 14.2 11.9 12.0 12.1 14.0 15.8 13.8 14.7 11.3 11.9 14.0 15.8 16.0 14.8 14.9 11.9 17.1 18.7 11.1 11.9 12.0 13.2 19.9 22.7 fr 9.3 15.9 15.1 14.6 15.1 16.8 8.0 15.0 15.8 9.4 14.2 15.8 16.8 18.3 19.3 13.8 15.8 17.8 16.8 15.9 15.8 15.9 15.8 16.6 19.6 8.1 15.8 13.0 13.9 22.5 26.5 Table 24: MTVQA it 8.1 10.1 9.6 9.4 9.4 8.9 8.7 9.1 7.2 7.5 8.6 8.7 8.9 8.7 10.4 7.5 7.2 9.7 8.9 11.3 10.5 11.1 7.2 9.1 11.4 7.1 7.2 10.3 9.5 16.7 18.6 ja 0.8 2.4 3.5 3.9 4.1 3.9 0.7 1.9 2.1 3.4 4.6 3.1 3.9 4.3 3.6 2.9 2.1 2.5 3.9 3.4 3.5 3.7 2.1 3.4 4.0 3.0 2.1 3.0 3.0 5.0 5.9 ko 0.7 3.8 3.8 3.8 3.8 2.7 0.5 1.4 3.2 3.4 4.1 3.8 2.7 2.9 4.1 3.4 3.2 3.8 2.7 3.2 2.9 3.0 3.2 4.5 4.3 2.3 3.2 3.2 3.4 9.0 9.9 ru 1.6 2.6 1.9 2.4 2.5 2.8 0.7 2.6 2.4 1.9 2.1 3.3 2.8 2.8 3.2 2.6 2.4 2.8 2.8 2.9 2.6 1.7 2.4 2.5 4.0 2.1 2.4 2.9 3.4 5.2 5.0 th 0.0 0.9 0.9 0.9 0.4 0.4 0.4 1.3 4.8 0.9 0.9 0.9 0.4 0.9 3.5 0.9 4.8 2.2 0.4 2.2 0.9 0.9 4.8 1.7 4.8 1.7 4.8 2.2 1.7 5.2 5.2 vi 1.1 1.8 1.6 2.0 2.7 2.9 2.7 2.8 4.1 2.3 1.5 2.5 2.9 2.8 1.5 3.5 4.1 3.5 2.9 3.5 2.1 2.7 4.1 5.2 5.3 1.7 4.1 5.2 3.6 9.7 8.9 Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen en 59.7 54.1 52.0 54.8 57.8 56.6 61.9 49.3 60.6 48.4 51.8 53.8 56.6 58.6 58.5 54.9 60.6 61.9 56.6 58.0 58.3 58.3 60.6 61.1 61.6 56.5 60.6 60.3 60.5 59.1 60.6 avg. 37.2 34.1 37.4 40.6 45.3 45.1 39.2 33.8 51.0 40.3 42.2 42.9 45.1 45.8 42.1 45.0 51.0 51.4 45.1 46.1 47.6 47.0 51.0 55.1 53.0 47.5 51.0 51.9 52.5 53.2 54.8 bn 4.9 2.6 5.7 22.7 27.4 27.0 13.2 5.9 46.7 28.2 27.6 29.1 27.0 26.4 14.2 40.5 46.7 42.5 27.0 29.5 35.4 37.6 46.7 52.8 50.4 43.9 46.7 44.2 45.2 43.4 49.9 de 47.8 44.6 45.6 46.5 50.3 51.4 49.0 43.8 54.1 43.9 46.3 47.6 51.4 52.4 53.0 46.4 54.1 56.2 51.4 52.8 52.8 52.6 54.1 56.6 54.9 51.6 54.1 54.8 54.6 56.9 57.0 Table 25: xGQA id 33.2 34.3 38.7 42.1 46.0 44.8 35.6 38.0 51.2 41.1 43.0 43.4 44.8 44.4 39.8 45.7 51.2 51.7 44.8 46.1 48.7 46.8 51.2 56.0 53.6 47.6 51.2 53.1 53.8 54.4 54. ko 38.2 36.3 40.4 39.8 46.4 44.9 39.1 32.4 49.4 40.6 42.2 42.2 44.9 45.4 43.3 42.5 49.4 50.1 44.9 44.5 45.5 44.1 49.4 53.9 52.4 44.2 49.4 51.3 51.7 53.6 53.5 pt 47.1 43.8 45.2 46.0 48.6 50.8 44.9 41.7 53.4 43.0 45.7 46.6 50.8 51.9 51.4 46.5 53.4 54.6 50.8 51.7 52.5 51.5 53.4 56.0 53.0 50.1 53.4 54.3 54.7 56.2 57.2 ru 42.1 36.4 43.4 43.6 49.5 48.2 44.1 37.3 51.2 42.8 44.8 45.9 48.2 49.9 45.7 46.2 51.2 52.5 48.2 49.5 49.6 48.1 51.2 55.4 53.1 47.5 51.2 53.2 53.9 54.0 55. zh 47.2 41.0 42.7 43.6 48.9 48.7 48.4 37.4 51.3 42.8 45.6 45.4 48.7 50.0 47.7 47.4 51.3 52.1 48.7 48.3 48.6 48.1 51.3 55.0 53.4 47.5 51.3 52.8 53.4 54.3 55.6 Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen en 33.6 33.0 25.2 32.7 29.9 31.0 75.6 76.1 72.6 43.3 38.9 31.5 31.0 36.5 34.2 74.8 72.6 73.6 31.0 35.9 37.1 33.1 72.6 80.8 77.9 66.6 72.6 74.8 75.6 78.4 79.1 avg. 1.2 9.5 11.8 13.6 11.1 13.2 1.1 12.6 28.5 13.3 12.7 13.2 13.2 12.0 9.4 28.9 28.5 23.0 13.2 13.5 17.3 17.4 28.5 35.3 31.8 27.5 28.5 27.8 33.4 39.2 34.4 ar 0.0 7.8 6.9 6.5 4.5 5.6 0.1 27.9 25.6 5.2 4.7 5.2 5.6 4.4 4.0 23.0 25.6 18.2 5.6 5.3 7.7 6.3 25.6 30.6 26.1 24.5 25.6 28.6 40.4 40.4 36.6 bn 0.0 0.6 1.0 5.9 5.5 3.3 0.0 0.5 14.0 4.3 4.1 5.0 3.3 2.5 1.9 11.9 14.0 7.8 3.3 5.0 9.0 9.3 14.0 15.4 14.4 17.9 14.0 13.9 12.2 18.5 17.1 cs 0.7 3.9 13.8 13.8 10.7 10.9 1.3 1.6 30.5 11.1 11.4 11.6 10.9 9.1 6.7 25.8 30.5 24.1 10.9 13.9 16.5 17.2 30.5 35.5 35.4 28.6 30.5 26.1 39.4 33.9 29.7 da 1.1 8.2 10.8 18.2 12.4 18.0 2.3 20.7 38.1 15.3 12.5 15.1 18.0 13.6 9.2 43.6 38.1 36.7 18.0 15.7 21.2 22.1 38.1 51.3 43.5 35.1 38.1 35.4 37.7 40.0 43.1 de 1.7 24.7 24.4 25.4 20.4 26.4 1.6 29.2 27.6 24.5 23.6 22.6 26.4 25.0 21.8 26.0 27.6 23.0 26.4 26.5 27.8 26.9 27.6 34.0 33.4 26.0 27.6 29.6 32.7 38.6 32.0 el 0.0 0.6 1.4 7.0 6.4 4.5 0.1 0.6 23.3 5.3 5.9 6.9 4.5 3.0 2.8 24.6 23.3 19.5 4.5 5.9 9.3 8.2 23.3 28.2 27.0 18.2 23.3 11.6 31.9 35.3 19.2 es 10.5 34.4 27.3 31.0 20.4 30.9 2.1 61.6 56.0 25.4 28.1 30.9 30.9 25.7 23.4 53.7 56.0 54.2 30.9 29.6 38.0 37.5 56.0 65.4 60.7 54.7 56.0 58.7 69.2 69.7 69.2 fa 0.0 0.4 8.6 7.1 5.7 4.1 0.1 0.4 24.3 5.4 4.8 4.9 4.1 3.4 2.0 24.9 24.3 17.6 4.1 5.4 8.2 9.1 24.3 32.3 23.0 29.4 24.3 17.1 41.6 55.8 31.2 fi 0.4 1.8 7.0 5.7 5.5 4.4 0.8 1.1 13.8 6.1 5.3 6.1 4.4 3.8 3.8 16.0 13.8 10.5 4.4 4.1 7.0 7.2 13.8 17.9 14.6 13.7 13.8 10.2 8.4 11.0 12.0 fil 1.6 1.9 3.6 11.9 10.1 11.1 2.9 2.9 29.0 13.3 14.2 12.0 11.1 7.1 4.1 30.2 29.0 24.0 11.1 9.1 15.2 13.9 29.0 36.3 31.7 26.8 29.0 26.0 26.2 34.0 33.6 fr 4.4 39.1 31.3 30.7 24.8 38.5 3.4 58.2 50.9 37.1 33.3 39.2 38.5 33.6 26.2 52.6 50.9 51.9 38.5 33.5 42.4 40.6 50.9 62.5 58.9 54.3 50.9 55.6 67.6 71.3 67.6 he 0.0 3.4 3.5 7.6 7.4 6.7 0.0 0.0 15.5 7.0 8.5 8.2 6.7 6.3 4.4 17.1 15.5 9.7 6.7 8.3 10.9 12.2 15.5 24.6 18.2 22.8 15.5 22.2 42.5 47.1 27.6 hi 0.0 3.5 9.7 7.2 6.7 6.3 0.0 0.4 22.5 7.2 8.6 7.7 6.3 5.2 3.7 20.1 22.5 20.2 6.3 6.9 10.4 9.1 22.5 27.4 24.6 21.6 22.5 16.3 24.5 26.3 20.3 hr 0.5 2.6 9.2 9.4 8.0 7.3 0.7 0.9 19.7 7.9 8.2 8.2 7.3 5.9 4.7 20.5 19.7 15.4 7.3 8.8 11.8 11.5 19.7 26.9 22.2 17.7 19.7 19.8 19.1 24.9 22.0 hu 0.6 4.0 10.5 8.2 7.1 7.8 1.3 1.6 18.0 6.7 6.3 5.1 7.8 7.0 4.9 18.5 18.0 15.3 7.8 7.1 9.7 11.1 18.0 24.7 22.6 20.1 18.0 11.7 12.5 19.6 18.7 id 1.5 7.9 8.4 22.9 17.5 22.4 2.1 19.6 39.9 22.6 22.6 22.4 22.4 20.2 12.5 43.3 39.9 33.0 22.4 22.3 28.5 28.9 39.9 49.2 45.2 43.8 39.9 40.3 50.3 58.3 50.4 it 9.2 28.5 24.9 26.0 18.5 30.6 1.5 26.8 44.1 28.1 23.3 24.7 30.6 29.7 21.3 40.3 44.1 38.0 30.6 30.3 33.9 34.8 44.1 51.6 49.2 39.7 44.1 43.8 53.6 60.4 53.7 ja 0.1 27.6 27.9 27.3 27.0 23.7 0.2 35.8 33.8 25.4 25.2 25.8 23.7 24.8 21.3 35.0 33.8 25.6 23.7 25.7 26.2 30.5 33.8 38.7 35.9 36.3 33.8 39.0 39.5 49.1 43.5 ko 0.0 1.6 3.1 3.0 2.5 2.5 0.0 0.1 13.4 2.1 2.4 3.3 2.5 2.0 2.0 13.9 13.4 10.4 2.5 2.7 3.2 2.9 13.4 15.2 14.0 11.5 13.4 13.4 18.4 21.3 13.4 mi 0.0 0.1 2.1 1.8 2.0 2.6 0.2 0.2 24.9 4.0 3.0 4.8 2.6 3.3 1.2 29.4 24.9 17.5 2.6 3.9 7.2 7.9 24.9 35.9 26.5 21.5 24.9 21.8 20.1 33.7 34.9 nl 1.6 20.4 21.7 28.5 18.9 24.2 3.0 34.9 50.1 22.3 23.0 24.8 24.2 23.0 12.7 53.4 50.1 46.1 24.2 21.6 30.0 27.7 50.1 59.1 55.3 46.2 50.1 48.9 59.6 61.7 56.2 no 1.5 8.8 12.0 14.1 11.2 18.8 1.8 15.7 41.5 17.7 15.6 18.5 18.8 17.1 11.8 41.9 41.5 33.1 18.8 20.1 24.7 26.4 41.5 49.2 45.8 38.7 41.5 36.6 36.3 42.5 41.4 pl 0.7 4.8 14.1 13.6 10.0 10.8 1.1 11.3 26.9 12.0 12.0 13.6 10.8 9.8 7.3 25.6 26.9 19.8 10.8 12.0 16.1 14.9 26.9 32.5 32.3 25.1 26.9 25.0 35.5 37.9 30.0 pt quz 1.9 30.2 24.0 22.1 18.1 29.0 2.6 12.5 45.1 24.9 25.8 17.7 29.0 27.8 22.5 44.8 45.1 41.2 29.0 21.8 29.1 31.2 45.1 51.1 51.6 45.3 45.1 52.3 50.6 59.3 59.9 0.1 0.7 0.5 0.4 0.4 1.1 0.8 1.3 1.3 1.4 0.7 0.7 1.1 0.8 0.8 1.6 1.3 0.2 1.1 0.9 2.5 2.3 1.3 2.9 0.9 1.8 1.3 0.9 0.7 1.7 2.1 ro 0.7 5.4 6.0 11.0 7.6 8.1 1.2 5.4 22.6 10.7 8.2 9.1 8.1 6.2 5.9 19.8 22.6 17.1 8.1 9.5 14.7 14.4 22.6 30.7 25.2 21.7 22.6 16.9 31.3 34.6 23.4 ru 0.4 17.5 22.5 16.7 21.1 18.6 0.7 12.8 23.5 21.4 20.0 17.8 18.6 19.6 16.3 25.3 23.5 20.6 18.6 19.5 21.3 22.4 23.5 32.3 30.7 24.0 23.5 33.6 31.9 38.0 39.2 sv 1.1 11.8 24.1 23.8 17.8 17.8 2.0 18.3 42.7 17.8 15.9 17.5 17.8 15.2 16.1 44.0 42.7 38.1 17.8 18.9 24.1 23.8 42.7 51.8 47.7 37.9 42.7 36.7 34.9 45.9 42.7 sw 0.6 1.3 2.1 1.5 7.0 8.1 0.8 0.6 28.9 11.1 9.9 9.3 8.1 5.0 5.6 30.3 28.9 14.6 8.1 8.5 15.3 14.7 28.9 38.0 29.3 27.2 28.9 18.9 21.3 29.9 30.2 te 0.0 0.0 0.0 0.0 0.0 1.5 0.0 0.0 11.0 1.4 1.8 2.3 1.5 1.4 0.6 13.8 11.0 5.8 1.5 1.4 4.3 4.4 11.0 17.4 12.4 13.1 11.0 8.0 9.2 15.1 13.5 th 0.2 4.0 5.8 12.9 12.6 10.2 0.4 4.4 27.7 12.6 11.8 13.3 10.2 10.9 7.5 28.8 27.7 23.2 10.2 13.6 18.6 18.4 27.7 36.2 32.0 28.5 27.7 38.1 19.0 26.0 42.3 tr 0.3 3.2 9.4 7.6 7.9 7.2 0.5 6.1 21.8 7.6 6.7 6.5 7.2 5.7 3.9 22.1 21.8 16.3 7.2 7.5 8.0 10.8 21.8 29.3 27.0 19.0 21.8 17.4 29.8 30.6 23.3 uk 0.1 5.7 4.6 10.0 10.3 8.3 0.3 0.2 21.9 8.2 11.9 9.6 8.3 8.9 7.8 21.1 21.9 0.3 8.3 8.5 9.8 10.5 21.9 26.4 25.2 0.2 21.9 18.1 27.7 30.6 20.3 vi 0.6 2.6 18.8 15.9 12.7 14.7 0.5 10.1 49.7 17.2 14.7 15.4 14.7 13.0 8.8 47.4 49.7 43.9 14.7 14.9 19.4 18.8 49.7 58.1 56.0 50.1 49.7 59.5 68.2 72.7 69.4 zh 0.0 12.8 10.6 20.2 9.5 15.6 0.3 17.2 16.9 16.9 11.8 14.4 15.6 18.5 20.3 20.2 16.9 13.3 15.6 23.9 22.3 21.1 16.9 16.5 15.2 18.5 16.9 19.9 26.1 56.9 33.8 Table 26: XM Phi 3.5 - L100 Phi 3.5 - T5-2 Phi 3.5 - T5-3 Phi 3.5 - T5-4 Phi 3.5 - T5 Phi 3.5 - English ar 93.8 94.7 92.2 96.5 98.2 0.0 bn 99.0 100.0 99.8 96.1 97.3 0.0 cs 99.0 99.4 99.2 99.2 78.7 0.0 da 91.8 98.4 97.9 78.3 87.5 0.2 de 100.0 100.0 100.0 100.0 100.0 0.2 el 88.3 99.8 99.8 98.4 50.2 0.0 en es fa fi 100.0 100.0 100.0 100.0 100.0 100. 100.0 100.0 100.0 100.0 99.8 31.4 98.8 100.0 100.0 100.0 2.3 0.0 100.0 100.0 100.0 100.0 76.4 0.8 fil 96.7 99.6 99.6 99.6 49.4 0.0 fr he 100.0 100.0 100.0 100.0 100.0 5.3 99.6 100.0 99.8 99.4 96.7 0.0 hi 97.7 98.8 99.0 98.6 99.2 0.0 hr 76.2 79.1 77.9 89.8 73.6 0.2 hu 100.0 100.0 100.0 100.0 99.2 0.0 id 95.3 88.5 91.8 98.0 95.1 0.6 it ja ko mi nl 100.0 100.0 100.0 100.0 100.0 31.2 99.2 99.8 100.0 100.0 100.0 0. 99.8 100.0 100.0 100.0 99.8 0.0 100.0 97.3 95.9 95.7 1.6 0.0 100.0 100.0 100.0 100.0 99.8 0.2 no 98.4 85.7 75.2 71.5 92.0 1.0 pl pt 100.0 100.0 100.0 100.0 96.5 0.0 97.7 99.2 52.1 100.0 100.0 2.5 quz 0.2 27.5 37.5 12.7 0.2 0.0 ro ru sv 100.0 100.0 100.0 100.0 96.3 0.0 99.8 99.8 100.0 100.0 100.0 0.0 98.6 99.6 99.2 100.0 98.6 0.0 sw 98.2 99.0 84.4 84.6 36.1 0.2 te 93.2 63.5 82.8 67.2 62.5 0.0 th tr 99.8 100.0 100.0 99.0 90.2 0. 100.0 100.0 100.0 100.0 94.9 0.6 uk 92.6 96.9 96.5 30.9 91.6 0.0 vi 100.0 100.0 100.0 100.0 39.8 0.0 zh 96.5 93.9 95.3 94.1 96.7 0.0 Table 27: XM3600 language fidelity (1b) 39 Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen en 59.6 59.9 58.9 58.6 58.5 59.6 46.1 45.4 59.7 55.2 58.3 58.2 59.6 61.9 60.0 60.8 59.7 57.8 59.6 54.3 58.9 56.8 59.7 61.7 60.3 59.9 59.7 57.8 58.2 65.0 75. avg. 55.0 51.8 48.3 50.5 53.6 53.3 36.3 37.5 54.8 48.2 53.4 48.4 53.3 54.5 50.5 55.3 54.8 51.1 53.3 45.4 52.5 49.7 54.8 59.4 57.3 55.0 54.8 52.6 54.7 62.4 70.2 ar 52.3 49.7 47.7 46.6 50.7 49.9 33.4 36.6 53.0 42.2 50.5 43.5 49.9 50.3 43.6 56.3 53.0 48.9 49.9 40.1 49.0 46.8 53.0 58.8 56.5 53.1 53.0 55.7 54.7 61.7 68.8 es 54.9 51.3 47.1 51.3 54.7 53.7 37.0 36.1 54.7 50.6 54.3 50.3 53.7 56.2 53.8 52.6 54.7 52.3 53.7 47.8 53.8 49.6 54.7 59.0 56.5 54.6 54.7 47.5 54.0 61.0 70. fr 57.6 55.2 51.2 52.6 55.1 56.8 36.5 38.7 56.3 51.1 55.7 52.7 56.8 57.5 55.1 55.0 56.3 52.1 56.8 49.4 54.6 53.9 56.3 60.0 58.3 57.1 56.3 52.5 56.4 64.3 70.5 ru 55.2 51.0 47.3 51.3 54.0 52.6 38.2 38.7 55.4 48.8 53.1 47.3 52.6 54.2 49.5 57.1 55.4 51.0 52.6 44.4 52.5 48.6 55.4 59.7 57.8 55.1 55.4 54.8 53.5 62.7 70.8 Table 28: XVNLI Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 Centurio Aya Centurio Qwen en 38.4 36.7 37.0 37.3 37.6 36.6 33.2 33.4 33.0 37.3 36.1 34.4 36.6 36.2 37.1 32.6 33.0 32.7 36.6 33.4 35.0 36.0 33.0 38.6 36.9 32.8 33.0 39.8 36.8 37.6 46.4 avg. 36.2 36.2 33.9 35.8 35.1 32.0 32.4 32.4 31.7 34.1 30.9 31.9 32.0 33.2 31.9 30.0 31.7 33.5 32.0 30.2 33.4 31.3 31.7 35.2 36.1 32.0 31.7 39.7 35.4 37.2 43.0 ar 36.2 31.5 33.2 32.5 32.2 28.5 30.9 34.9 31.5 32.5 27.5 28.5 28.5 31.9 30.5 26.8 31.5 30.5 28.5 28.5 30.9 26.5 31.5 33.9 34.6 32.5 31.5 38.6 34.9 36.2 39.6 fr 41.9 38.9 39.9 39.3 40.3 35.9 34.2 36.6 34.6 40.3 33.9 35.9 35.9 38.9 35.6 31.5 34.6 35.9 35.9 32.9 39.3 35.9 34.6 34.2 36.2 30.9 34.6 40.3 35.2 38.9 45.0 Table 29: xMMMU hi 29.9 31.6 29.2 32.3 32.6 27.8 30.6 28.9 34.0 30.9 28.2 29.2 27.8 29.2 25.8 26.8 34.0 30.9 27.8 28.9 33.7 29.2 34.0 34.0 36.8 33.0 34.0 34.4 37.5 38.8 41. id 35.4 37.0 32.3 37.0 34.7 32.0 32.7 31.3 31.6 31.3 28.6 30.3 32.0 32.7 31.0 31.6 31.6 35.4 32.0 30.0 31.0 32.0 31.6 35.0 36.7 30.6 31.6 40.7 36.4 39.7 44.1 ja 34.2 34.9 31.2 36.8 32.3 31.2 30.5 30.9 27.9 31.6 32.7 33.5 31.2 29.0 33.8 32.0 27.9 31.2 31.2 27.5 30.5 28.3 27.9 36.1 36.1 32.7 27.9 38.7 34.6 34.2 43.5 pt 39.7 43.4 37.7 37.0 38.7 36.7 35.7 31.6 30.6 37.7 34.7 34.0 36.7 37.4 34.7 31.3 30.6 37.0 36.7 33.3 35.0 36.0 30.6 38.0 36.0 32.0 30.6 45.5 33.7 35.4 44. 41 Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Phi 3.5 - OCR English Phi 3.5 - OCR 50 Phi 3.5 - OCR 1 Phi 3.5 - OCR Latin-down Phi 3.5 - OCR 50 (frozen) Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 modelname Aya modelname Qwen Phi 3.5 - English Phi 3.5 - T5 50 Phi 3.5 - T5-4 50 Phi 3.5 - T5-3 50 Phi 3.5 - T5-2 50 Phi 3.5 - L100 50 Llama 3 - English Llama 3 - T5 50 Llama 3 - L100 50 Phi 3.5 - L100 1 Phi 3.5 - L100 10 Phi 3.5 - L100 24 Phi 3.5 - L100 50 Phi 3.5 - L100 75 Phi 3.5 - L100 90 Llama 3 - L100 10 Llama 3 - L100 50 Llama 3 - L100 90 Phi 3.5 - L100 50 Phi 3.5 - PT 100 Phi 3.5 - PT 50 Phi 3.5 - PT 1 Llama 3 - L100 50 Llama 3 - PT 1 Llama 3 - PT 100 Phi 3.5 - OCR English Phi 3.5 - OCR 50 Phi 3.5 - OCR 1 Phi 3.5 - OCR Latin-down Phi 3.5 - OCR 50 (frozen) Gemma 2 - L100 50 Llama 3 - L100 50 Qwen 2.5 - L100 50 Aya-Expanse - L100 50 modelname Aya modelname Qwen en 65.8 75.2 74.2 70.4 68.4 69.6 72.0 73.4 72.0 58.4 56.9 60.4 69.6 74.5 71.6 65.9 72.0 73.1 69.6 79.5 76.1 78.1 72.0 76.9 79.9 78.4 81.2 81.0 78.9 76.1 59.9 72.0 82.8 79.1 83.1 84.8 en 36.2 36.4 35.0 34.6 35.8 33.4 41.0 41.4 39.2 22.0 24.6 26.0 33.4 38.4 39.8 32.0 39.2 40.0 33.4 44.0 41.8 42.2 39.2 48.4 48.8 55.8 53.8 54.8 54.6 47.2 28.6 39.2 48.8 46.6 60.0 65.2 avg. 55.8 60.2 60.8 58.7 56.2 58.0 60.5 62.2 58.4 52.6 51.6 54.0 58.0 61.2 59.4 56.6 58.4 59.4 58.0 63.3 62.4 64.5 58.4 65.1 65.2 64.6 66.7 69.8 65.4 62.1 53.5 58.4 62.5 63.5 74.2 76.1 avg. 5.0 5.4 5.8 5.8 5.8 5.2 8.5 8.2 7.3 4.0 4.1 3.8 5.2 6.0 6.5 6.3 7.3 7.5 5.2 9.9 9.4 9.5 7.3 11.4 10.5 18.3 21.0 22.2 22.4 15.7 3.8 7.3 10.1 10.2 30.1 31.7 avg. Latin avg. other 62.3 70.9 71.4 67.7 64.2 67.2 69.6 72.5 67.9 55.7 54.9 58.8 67.2 71.6 69.2 62.6 67.9 68.4 67.2 74.8 73.0 74.5 67.9 74.4 77.4 74.7 76.7 78.3 74.2 70.8 57.1 67.9 75.1 75.2 80.9 82.7 51.5 53.1 53.7 52.8 50.8 51.9 54.4 55.4 52.1 50.5 49.4 50.8 51.9 54.2 52.9 52.6 52.1 53.3 51.9 55.6 55.3 57.7 52.1 58.9 57.0 57.9 60.0 64.1 59.5 56.3 51.1 52.1 54.0 55.7 69.7 71.8 ar 50.2 50.2 52.2 51.6 49.5 49.9 53.5 54.5 51.6 50.2 48.5 51.8 49.9 53.2 51.0 51.5 51.6 51.0 49.9 52.8 52.4 57.0 51.6 55.0 52.6 59.1 61.4 66.8 57.8 59.2 49.6 51.6 51.5 53.9 75.9 76.9 de 63.5 70.8 71.5 66.9 64.5 68.0 69.9 72.2 69.6 55.2 54.8 58.9 68.0 71.2 70.2 62.1 69.6 67.4 68.0 75.8 72.2 74.0 69.6 74.8 77.6 77.1 78.6 78.0 75.5 73.2 59.1 69.6 76.4 77.2 82.1 83.5 hi 58.5 65.4 65.5 61.0 58.4 62.4 67.2 67.1 62.0 53.5 49.6 54.5 62.4 63.8 60.5 59.5 62.0 65.8 62.4 68.5 69.6 72.8 62.0 73.0 73.4 70.9 72.1 76.8 67.6 63.2 56.5 62.0 66.5 71.4 80.1 82.4 id 61.4 71.8 72.8 69.6 65.4 69.0 71.1 74.4 65.9 57.5 55.1 58.1 69.0 74.0 69.4 62.6 65.9 71.0 69.0 76.2 73.6 76.8 65.9 75.5 78.1 73.6 76.0 78.5 75.0 66.2 56.8 65.9 76.5 75.6 81.4 83.8 it 64.0 71.6 73.1 67.2 64.9 67.9 70.9 71.5 70.4 56.4 56.8 60.0 67.9 70.5 71.2 65.8 70.4 69.0 67.9 76.5 73.8 75.0 70.4 74.4 78.2 74.5 77.1 79.1 75.0 76.1 58.9 70.4 76.5 75.0 80.6 83.1 ko 49.0 49.8 51.1 50.0 50.0 48.6 48.9 50.5 49.9 49.6 50.5 50.0 48.6 50.5 49.6 50.8 49.9 50.6 48.6 50.8 49.2 52.8 49.9 53.4 51.0 50.6 51.5 56.9 56.4 50.0 49.9 49.9 50.1 50.6 68.8 72.4 ru 52.1 54.1 53.9 53.6 50.5 52.5 57.5 56.6 52.0 50.9 48.2 51.1 52.5 54.2 54.1 54.5 52.0 52.6 52.5 59.6 59.9 62.2 52.0 65.9 64.0 66.5 71.5 73.2 67.8 68.0 51.0 52.0 55.2 56.0 73.5 75.6 th 49.1 51.0 49.6 48.9 48.8 49.6 50.1 51.6 48.8 48.2 49.6 48.4 49.6 51.9 50.4 50.4 48.8 49.9 49.6 50.9 50.0 51.4 48.8 52.5 49.1 51.1 52.1 58.6 55.0 47.8 50.6 48.8 51.0 51.1 66.5 64.4 Table 30: SMPQA Ground avg. Latin avg. other 12.4 13.6 14.4 14.4 14.5 12.8 21.1 20.4 18.2 10.1 10.3 9.5 12.8 15.1 16.1 15.6 18.2 18.8 12.8 24.5 23.1 23.7 18.2 27.9 25.0 39.9 41.8 43.5 41.0 34.1 9.4 18.2 25.1 25.4 49.8 54.3 0.0 0.0 0.0 0.0 0.0 0.1 0.0 0.0 0.0 0.0 0.0 0.1 0.1 0.0 0.0 0.1 0.0 0.0 0.1 0.2 0.2 0.1 0.0 0.4 0.8 3.9 7.1 8.0 9.9 3.5 0.1 0.0 0.1 0.1 17.0 16. ar 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5.2 14.4 17.2 20.2 5.2 0.0 0.0 0.0 0.0 29.2 21.4 de 17.4 21.2 20.0 16.0 18.0 17.4 24.4 25.2 21.6 12.0 11.6 12.2 17.4 21.0 21.0 17.8 21.6 21.2 17.4 31.4 27.8 27.2 21.6 29.6 28.8 38.6 42.2 43.8 41.6 36.4 13.8 21.6 32.0 27.4 50.2 53.2 hi 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 2.6 2.4 6.4 6.2 7.0 3.8 0.0 0.0 0.0 0.0 17.6 21. id 12.6 13.2 14.6 16.6 14.8 14.0 21.6 21.8 18.8 9.0 10.0 8.4 14.0 14.8 17.0 15.8 18.8 21.0 14.0 25.6 24.4 24.4 18.8 30.6 26.2 43.2 45.8 46.4 42.6 37.2 10.4 18.8 23.8 28.8 52.6 55.4 it 15.2 16.0 16.6 20.4 19.6 14.6 23.8 23.4 21.6 14.0 14.2 12.6 14.6 18.6 21.8 19.2 21.6 20.4 14.6 26.8 25.0 29.0 21.6 30.6 28.4 41.6 42.6 42.8 43.0 33.0 8.4 21.6 29.0 27.4 51.2 56.6 ko 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.0 0.2 1.2 2.8 0.0 0.0 0.0 0.0 0.0 11.2 16. ru 0.0 0.0 0.0 0.0 0.0 0.2 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.2 0.0 0.0 0.0 0.0 0.2 1.2 1.2 0.4 0.0 1.6 1.8 15.2 21.2 21.4 25.6 11.8 0.0 0.0 0.2 0.0 38.2 34.8 th 0.0 0.0 0.0 0.0 0.0 0.2 0.2 0.2 0.2 0.0 0.0 0.4 0.2 0.0 0.0 0.4 0.2 0.2 0.2 0.2 0.2 0.0 0.2 0.4 0.4 0.4 0.6 1.8 3.4 0.2 0.4 0.2 0.2 0.4 4.8 5.2 zh 49.8 48.0 49.6 51.4 47.9 48.4 49.4 51.9 48.4 50.5 50.0 49.2 48.4 51.8 51.8 48.8 48.4 50.1 48.4 51.0 50.6 50.2 48.4 53.8 51.8 49.0 51.6 52.4 52.5 49.8 49.2 48.4 49.8 51.0 53.4 58. zh 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.6 0.0 0.0 0.0 0.0 0.0 0.8 0.6 zu 60.2 69.4 68.4 67.0 62.0 64.1 66.5 72.0 65.6 53.5 52.9 58.0 64.1 70.6 66.1 59.9 65.6 66.2 64.1 70.8 72.2 72.4 65.6 72.9 75.8 73.6 75.0 77.6 71.1 67.8 53.6 65.6 71.1 73.1 79.5 80.2 zu 4.4 3.8 6.4 4.8 5.6 5.2 14.8 11.2 10.8 5.2 5.4 4.8 5.2 5.8 4.8 9.6 10.8 12.6 5.2 14.0 15.0 14.0 10.8 20.6 16.6 36.4 36.4 40.8 36.8 29.6 5.0 10.8 15.6 18.0 45.2 52. Table 31: SMPQA Name 42 en Centurio Aya 69.7 Centurio Qwen 72.7 30.5 Parrot 61.4 PALO 13B PALO 7B 58.7 InternVL 2.5 4B 68.4 InternVL 2.5 8B 70.3 78.2 Qwen2-VL 2B 80.7 Qwen2-VL 7B 54.0 Maya 75.6 Llama-Vision 63.1 Phi 3.5 Vision 71.0 Pixtral 12B 70.3 Pangea 72.6 MiniCPM 2.6 avg. 54.7 56.2 25.7 41.1 38.6 45.4 44.2 47.2 57.5 43.2 50.8 36.8 54.2 52.1 47. af 63.6 65.3 26.0 48.4 44.2 53.2 54.4 56.6 68.9 50.6 65.1 40.9 62.3 61.4 56.0 am 29.4 47.4 22.8 25.9 28.4 31.3 29.1 30.3 37.2 27.1 30.6 28.7 34.3 34.3 29.9 cs 66.2 62.2 26.1 47.9 43.6 53.2 52.8 56.7 68.5 53.3 61.3 41.0 61.6 59.6 55. el 67.8 56.7 25.5 35.8 33.5 42.3 43.3 47.2 62.2 53.6 42.9 34.7 58.3 54.2 46.6 es 65.1 67.0 27.3 53.2 49.9 60.8 57.8 64.0 72.6 52.7 65.1 52.7 66.1 64.4 62.1 fa 60.0 53.6 25.9 37.5 36.9 45.4 40.5 48.7 59.8 48.7 49.9 33.5 57.3 54.9 48. fi 43.3 48.8 26.4 42.7 39.1 38.3 41.3 41.7 55.1 35.3 51.5 34.9 52.0 45.4 41.8 ha 37.5 36.7 23.7 26.1 24.5 26.3 25.8 26.1 27.1 23.7 31.1 27.1 27.7 27.9 22.9 hr 63.6 65.4 25.3 52.3 49.6 55.2 55.6 57.1 72.2 50.5 60.9 40.5 67.1 63.0 59. hu 49.8 54.1 25.6 47.9 45.4 42.1 44.9 48.0 61.8 39.3 65.0 36.8 60.4 49.8 44.9 ja 66.7 67.6 26.7 49.1 48.8 60.5 57.3 62.2 71.8 55.2 46.3 45.9 64.8 65.5 62.9 mi 37.0 39.1 25.4 31.0 27.8 29.5 30.0 30.0 29.5 28.6 32.8 28.2 31.9 29.6 29. nl 62.4 63.7 28.0 48.9 45.1 56.6 51.8 59.2 69.5 51.4 61.5 43.6 58.6 61.0 57.8 no 59.1 63.6 26.6 51.2 45.8 53.7 54.8 57.8 69.6 46.4 61.8 44.4 62.1 64.1 55.2 pl 62.6 60.4 26.5 46.1 42.0 53.1 50.3 54.6 67.5 50.0 55.7 38.5 59.8 59.5 54. ro 64.0 58.5 26.8 46.5 44.0 49.7 48.9 54.5 65.6 51.3 57.3 39.8 59.0 60.6 52.7 ta 46.9 45.2 25.5 28.9 26.7 35.3 33.2 31.9 42.7 31.9 42.0 30.9 56.7 42.4 34.5 te 50.9 63.4 23.9 32.2 30.1 50.1 41.2 43.4 62.3 36.9 51.6 28.1 64.5 62.7 53. zu 42.6 49.5 24.0 28.3 28.3 26.5 27.3 27.6 29.3 33.4 31.9 28.1 25.0 29.3 33.4 Table 32: BIN-MC en Centurio Aya 53.0 Centurio Qwen 61.2 46.6 Parrot 45.2 PALO 13B PALO 7B 41.0 InternVL 2.5 4B 63.2 InternVL 2.5 8B 67.0 47.9 Qwen2-VL 2B 56.1 Qwen2-VL 7B 49.2 Maya 56.3 Phi 3.5 Vision 49.4 Pixtral 12B 58.0 Pangea 55.0 MiniCPM 2.6 avg. 41.2 46.9 36.2 28.3 29.1 50.3 53.3 40.5 49.7 36.3 40.7 33.7 45.5 48.2 af 52.8 50.9 38.0 33.1 34.4 46.0 57.7 38.0 50.9 48.5 51.5 39.9 50.3 44.2 zh 51.4 55.6 37.8 31.3 31.5 60.9 61.7 51.6 58.6 46.4 54.4 53.6 58.6 54.6 it 47.7 49.0 36.8 36.5 32.7 50.3 53.2 36.4 46.8 36.6 44.1 34.4 49.0 44.3 Table 33: M3Exam pt 27.4 31.9 25.9 19.3 21.8 34.9 33.0 36.2 34.7 25.9 25.2 19.5 32.2 36.9 th 27.8 29.6 23.5 20.2 21.1 39.1 39.1 26.1 38.3 20.0 24.3 7.0 27.8 38. vi 40.3 64.1 55.1 29.2 33.4 70.4 75.2 54.9 69.0 40.3 44.4 47.7 55.3 70.8 en Centurio Aya 82.5 Centurio Qwen 87.5 Parrot 59.2 PALO 13B 63.3 48.3 PALO 7B InternVL 2.5 4B 72.5 InternVL 2.5 8B 87.5 61.7 Qwen2-VL 2B 60.0 Qwen2-VL 7B 46.7 Maya 81.7 Phi 3.5 Vision 55.8 Pixtral 12B 69.2 Pangea 52.5 MiniCPM 2.6 avg. 66.8 73.1 52.9 26.2 25.6 49.7 51.6 50.5 52.9 42.3 50.3 47.7 58.9 49. am ber 71.7 77.5 45.0 25.0 40.8 43.3 43.3 44.2 48.3 43.3 45.8 51.7 45.8 45.0 54.2 49.2 64.2 55.0 75.0 50.0 50.0 50.0 50.0 48.3 49.2 32.5 90.0 55.8 bn 59.3 62.7 53.4 0.8 0.0 40.7 41.5 43.2 46.6 33.9 56.8 47.5 53.4 49.2 de 73.3 80.8 63.3 44.2 0.0 62.5 64.2 65.0 60.0 50.8 73.3 63.3 61.7 45.8 fil 59.2 78.3 49.2 47.5 49.2 56.7 49.2 53.3 50.0 51.7 54.2 51.7 55.0 48.3 ha 65.0 76.7 41.7 40.0 40.0 41.7 41.7 41.7 46.7 40.8 41.7 44.2 41.7 40.8 hi 71.2 72.9 62.7 0.0 0.0 42.4 59.3 61.0 48.3 42.4 56.8 16.1 60.2 44.1 ru 75.8 85.0 62.5 5.8 0.0 63.3 75.8 52.5 63.3 45.8 85.8 54.2 74.2 59.2 sw 67.5 70.0 35.8 32.5 39.2 35.8 36.7 38.3 58.3 34.2 38.3 65.8 54.2 48.3 th 72.5 81.7 67.5 0.0 0.0 74.2 68.3 67.5 60.8 38.3 15.0 53.3 75.8 65.8 zu 65.5 69.0 36.2 37.1 37.9 36.2 37.1 38.8 49.1 35.3 36.2 44.0 36.2 37.9 Table 34: VGR 43 en Centurio Aya 12.5 Centurio Qwen 28.3 0.0 Parrot 2.5 PALO 13B PALO 7B 5.8 InternVL 2.5 4B 24.2 InternVL 2.5 8B 57.5 22.5 Qwen2-VL 2B 5.8 Qwen2-VL 7B 20.0 Maya 45.8 Phi 3.5 Vision 9.2 Pixtral 12B 0.0 Pangea 9.2 MiniCPM 2.6 avg. 20.7 27.0 0.0 4.9 6.8 21.0 29.0 20.4 13.2 20.1 31.5 12.4 6.7 14.6 am ber 18.3 18.3 0.0 6.7 8.3 18.3 25.0 17.5 14.2 20.0 27.5 17.5 0.0 11.7 21.7 20.0 0.0 5.0 9.2 26.7 22.5 20.0 15.8 25.8 29.2 13.3 0.8 19. bn 20.0 33.3 0.0 6.7 10.0 17.5 25.8 13.3 13.3 19.2 23.3 10.0 0.0 12.5 de 11.7 32.5 0.0 5.0 5.8 20.8 38.3 26.7 11.7 20.8 36.7 16.7 20.8 10.8 fil 24.2 29.2 0.0 5.8 6.7 20.0 36.7 25.0 10.0 15.0 30.0 10.0 24.2 10. ha 29.2 22.5 0.0 2.5 4.2 23.3 25.8 24.2 15.0 25.8 31.7 16.7 15.8 22.5 hi 15.2 25.0 0.0 3.6 9.8 22.3 41.1 20.5 12.5 17.9 33.9 3.6 6.2 10.7 ru 10.8 22.5 0.0 4.2 5.0 20.0 35.8 16.7 12.5 23.3 29.2 14.2 0.8 12. sw 28.6 30.4 0.0 5.4 4.5 23.2 15.2 21.4 13.4 21.4 37.5 8.9 3.6 19.6 th 29.2 30.0 0.0 5.0 5.8 20.0 30.0 15.8 13.3 15.8 35.8 12.5 0.8 11.7 zu 19.5 33.1 0.0 4.2 5.1 18.6 22.9 23.7 13.6 16.1 31.4 13.6 0.8 19. Table 35: VLOD en Centurio Aya 85.0 Centurio Qwen 89.6 63.5 Parrot 63.8 PALO 13B PALO 7B 62.7 InternVL 2.5 4B 74.9 InternVL 2.5 8B 83.0 67.9 Qwen2-VL 2B 69.8 Qwen2-VL 7B 60.3 Maya 73.4 Phi 3.5 Vision 67.7 Pixtral 12B 75.8 Pangea 70.2 MiniCPM 2.6 avg. 77.9 81.7 55.1 33.1 24.1 59.0 63.3 55.9 60.2 56.3 46.4 60.7 70.5 57.9 id 79.5 85.0 56.6 58.7 33.6 65.7 63.2 60.9 61.1 60.3 56.4 62.5 74.3 57.8 sw 70.9 76.8 51.2 50.9 47.8 50.7 51.4 51.8 53.1 50.7 51.3 54.4 70.9 54.2 Table 36: MaRVL ta 73.4 76.0 50.7 2.6 0.4 50.9 54.6 52.2 60.9 50.6 50.8 61.8 66.6 57. tr 83.4 84.2 58.6 53.1 38.5 64.2 67.2 59.0 65.3 58.9 58.0 65.5 71.1 63.3 zh 82.4 86.7 58.2 0.2 0.0 63.5 79.9 55.8 60.7 61.2 15.7 59.1 69.6 57.2 44 en Centurio Aya 55.7 Centurio Qwen 60.1 28.2 Parrot 51.7 PALO 13B PALO 7B 54.0 InternVL 2.5 4B 46.0 InternVL 2.5 8B 45.6 53.7 Qwen2-VL 2B 54.7 Qwen2-VL 7B 55.4 Maya Llama-Vision 0.0 43.6 Phi 3.5 Vision 59.4 Pixtral 12B 61.4 Pangea 53.4 MiniCPM 2.6 avg. 49.3 47.7 3.6 33.1 22.5 42.5 38.2 26.5 31.2 17.3 4.7 17.9 43.4 55.0 22.3 fr 45.1 47.1 2.7 42.0 39.9 45.7 51.2 40.3 38.6 19.1 0.0 23.5 46.8 47.4 14.3 hi 58.7 45.1 2.9 17.5 9.2 37.1 27.9 10.8 18.7 13.0 0.6 12.1 31.7 61.0 12.1 Table 37: MaXM he 62.9 56.8 1.4 53.4 30.6 38.8 24.5 9.5 13.9 21.1 2.4 16.3 54.4 53.7 5.1 ro 51.1 47.7 1.2 34.2 16.8 31.5 35.7 15.6 37.2 18.0 0.3 7.8 44.1 52.9 19. th 46.7 57.0 3.0 20.9 12.3 51.0 36.4 38.1 42.1 11.6 24.8 20.9 44.4 67.2 53.6 zh 31.6 32.2 10.7 30.6 26.4 50.8 53.4 44.6 36.8 20.8 0.0 27.0 39.1 47.9 29.3 avg. Centurio Aya 11.1 Centurio Qwen 11.9 2.0 Parrot 6.3 PALO 13B PALO 7B 5.8 InternVL 2.5 4B 25.1 InternVL 2.5 8B 25.0 19.0 Qwen2-VL 2B 23.2 Qwen2-VL 7B 5.3 Maya 15.2 Llama-Vision 11.1 Phi 3.5 Vision 14.1 Pixtral 12B 19.3 Pangea 16.1 MiniCPM 2. ar 6.7 4.6 1.4 2.6 1.8 11.2 11.5 6.1 16.9 2.8 7.4 3.3 4.3 8.3 2.3 de 19.9 22.7 1.9 15.6 14.3 34.4 33.8 26.8 27.3 13.1 24.0 18.2 25.7 29.5 23.9 fr 22.5 26.5 0.9 12.1 13.3 38.4 37.4 30.9 31.7 12.2 18.7 20.2 27.3 35.2 27. it 16.7 18.6 1.6 10.4 8.3 33.5 35.3 30.7 35.2 6.6 25.3 25.2 25.2 29.2 32.7 ja 5.0 5.9 1.6 4.0 3.4 18.4 19.7 13.5 16.1 2.8 9.4 5.6 5.9 9.3 11.7 ko 9.0 9.9 2.7 4.3 3.2 29.0 30.3 21.1 24.6 4.8 14.5 8.8 9.1 14.5 12. ru 5.2 5.0 2.0 4.0 3.6 9.8 10.4 9.3 10.8 2.9 6.1 5.4 7.5 7.4 7.3 th 5.2 5.2 5.2 0.0 0.4 16.5 16.5 10.0 15.6 0.4 15.2 3.0 5.2 10.8 10.0 vi 9.7 8.9 0.9 4.2 4.1 34.6 30.4 22.4 30.7 2.3 15.8 10.5 16.6 29.2 16. Table 38: MTVQA 45 en"
        },
        {
            "title": "37.7\nParrot\n58.0\nPALO 13B\n59.1\nPALO 7B\nInternVL 2.5 4B 63.6\nInternVL 2.5 8B 63.4\n60.5\nQwen2-VL 2B\n62.5\nQwen2-VL 7B\n58.2\nMaya\n39.3\nLlama-Vision\n65.2\nPhi 3.5 Vision\n59.9\nPixtral 12B\n64.6\nPangea\n57.9\nMiniCPM 2.6",
            "content": "avg. 21.2 27.8 36.6 28.0 32.0 38.2 49.3 49.1 27.6 38.0 3.8 60.4 45.7 bn 20.2 26.3 42.8 28.1 17.4 18.6 37.4 40.1 26.0 5.0 0.7 59.1 33.9 de 23.2 14.7 34.5 29.2 23.8 43.2 51.1 53.2 29.2 51.9 5.4 61.6 49. id 19.8 29.6 30.0 15.4 25.0 32.6 48.4 49.7 26.8 37.3 14.0 60.7 46.3 ko 22.8 30.9 40.8 38.3 38.2 39.0 50.3 47.2 24.9 35.6 0.3 58.8 42.1 pt 21.7 17.8 27.7 27.2 27.6 39.9 51.8 52.5 27.9 50.6 3.6 62.1 51. ru 19.7 30.9 32.2 31.5 36.4 44.1 52.1 50.6 30.7 45.9 0.4 60.7 48.7 zh 21.2 44.1 47.9 25.9 55.2 50.3 54.1 50.1 27.9 39.5 1.9 59.6 48.6 Table 39: xGQA en Centurio Aya 78.4 Centurio Qwen 79.1 5.6 Parrot 67.3 PALO 13B PALO 7B 65.9 InternVL 2.5 4B 38.9 InternVL 2.5 8B 38.3 68.8 Qwen2-VL 2B 50.3 Qwen2-VL 7B 55.9 Maya 35.9 Llama-Vision 32.3 Phi 3.5 Vision 26.5 Pixtral 12B 70.1 Pangea 87.5 MiniCPM 2.6 avg. 39.2 34.4 0.4 17.0 13.5 17.5 15.7 5.2 24.6 14.6 7.2 6.3 22.1 34.6 14.2 ar 40.4 36.6 0.6 23.5 17.3 12.1 7.9 0.8 17.9 20.6 0.0 2.8 18.6 33.3 6.7 bn 18.5 17.1 0.0 22.9 18.8 3.7 4.0 0.0 11.5 18.4 0.0 0.0 9.6 30.8 3.3 cs 33.9 29.7 0.2 7.9 5.8 9.4 10.7 1.7 23.8 11.4 0.9 0.6 16.8 19.4 8.5 da 40.0 43.1 0.0 30.3 18.5 13.6 19.2 7.2 32.3 10.6 15.5 10.5 24.4 25.2 8.7 de 38.6 32.0 0.0 32.4 23.3 28.0 27.8 7.0 36.1 23.6 22.4 21.3 33.2 39.4 27.5 el 35.3 19.2 0.0 0.2 0.1 2.0 2.9 0.2 13.5 10.7 0.5 0.1 8.9 13.0 1.7 es 69.7 69.2 3.3 57.0 48.3 39.7 35.0 5.1 38.9 38.2 14.7 21.9 36.5 61.4 44.0 fa 55.8 31.2 2.3 1.5 1.5 10.1 8.7 9.0 23.6 1.5 0.0 0.1 20.5 25.4 5.8 fi 11.0 12.0 0.0 6.6 4.0 2.6 5.0 1.2 8.0 0.5 4.0 0.9 10.4 4.2 3.2 fil 34.0 33.6 0.0 8.4 2.7 6.2 10.9 2.9 8.3 2.1 13.1 2.5 15.3 6.7 5.0 fr 71.3 67.6 0.3 66.2 59.1 49.2 47.0 9.4 50.6 47.3 32.1 32.5 47.8 69.7 52.1 he 47.1 27.6 0.0 0.6 0.2 8.5 8.2 0.4 13.7 18.9 0.0 1.0 18.0 42.7 1.5 hi 26.3 20.3 0.0 25.0 21.2 5.4 8.3 0.0 6.7 15.0 0.0 0.1 6.3 21.5 3.0 hr 24.9 22.0 0.0 9.9 2.8 5.6 7.9 1.4 11.6 2.0 2.9 1.5 18.7 9.5 6.1 hu 19.6 18.7 0.0 2.7 6.3 3.8 5.3 2.1 15.5 0.9 13.2 2.6 15.6 3.6 5.8 id 58.3 50.4 0.0 22.7 20.2 39.9 24.7 8.9 45.4 19.4 2.2 4.2 44.6 70.9 24.6 it 60.4 53.7 0.2 40.4 31.0 33.1 27.5 5.9 38.7 34.4 33.5 23.6 32.8 53.5 24.6 ja 49.1 43.5 0.0 19.7 29.8 33.1 22.0 8.4 32.0 26.3 0.2 8.0 21.8 63.3 18.6 ko 21.3 13.4 0.0 0.2 2.4 8.9 6.7 1.0 9.1 8.9 0.1 0.3 12.0 20.3 4.4 mi 33.7 34.9 0.0 0.3 0.3 0.8 0.9 0.3 0.9 0.3 0.8 0.2 5.9 0.3 2.2 nl 61.7 56.2 1.6 36.5 29.8 29.3 26.8 2.9 39.1 28.8 30.1 19.8 29.7 44.9 27.8 no 42.5 41.4 0.0 31.0 16.5 14.2 16.6 5.2 35.7 9.4 2.8 10.7 26.0 48.5 12.0 pl 37.9 30.0 0.0 9.1 8.4 12.9 12.0 1.5 30.1 15.8 2.4 1.7 19.6 24.1 12.0 pt quz 59.3 59.9 4.0 13.8 8.9 39.0 35.0 21.0 48.8 16.4 15.7 25.8 42.4 64.6 36.0 1.7 2.1 0.2 0.8 0.5 0.2 0.8 1.0 0.9 0.6 0.2 0.4 1.0 1.7 0.2 ro 34.6 23.4 0.4 14.5 2.6 9.9 12.0 3.7 19.0 22.0 23.4 3.0 20.2 38.7 10.0 ru 38.0 39.2 0.4 21.3 19.7 23.4 22.6 1.1 37.9 19.9 0.3 0.5 33.8 47.3 20.0 sv 45.9 42.7 0.0 33.9 23.3 17.1 20.5 13.0 43.1 11.4 11.2 10.2 30.0 20.1 17.0 sw 29.9 30.2 0.0 0.8 0.0 0.6 1.0 1.1 2.4 0.5 6.8 0.5 10.4 40.7 1.5 te 15.1 13.5 0.8 0.0 0.0 1.1 2.6 0.0 3.9 0.0 0.0 0.0 6.2 21.8 0.5 th 26.0 42.3 0.0 0.5 0.0 27.9 7.2 1.3 31.2 0.2 1.2 1.0 23.8 61.4 20.9 tr 30.6 23.3 1.0 0.6 0.5 7.8 9.3 0.9 15.8 13.5 0.6 1.7 14.9 30.2 8.0 uk 30.6 20.3 0.0 2.6 0.1 7.1 4.7 0.6 16.6 1.5 0.1 0.1 18.4 20.7 7.5 vi 72.7 69.4 0.0 15.6 17.4 61.3 46.2 7.9 55.6 31.8 0.8 2.6 51.7 81.3 25.8 zh 56.9 33.8 0.0 37.0 29.7 44.1 40.1 49.5 41.8 26.9 0.0 8.1 28.1 50.7 39.4 Table 40: XM3600 en Centurio Aya 100.0 Centurio Qwen 99.8 100.0 Parrot 100.0 PALO 13B PALO 7B 100.0 InternVL 2.5 4B 100.0 InternVL 2.5 8B 100.0 100.0 Qwen2-VL 2B 100.0 Qwen2-VL 7B 100.0 Maya 100.0 Llama-Vision 100.0 Phi 3.5 Vision 100.0 Pixtral 12B 99.8 Pangea 99.8 MiniCPM 2.6 avg. 95.7 95.2 25.0 60.1 72.0 91.0 91.1 13.2 90.0 65.7 33.3 40.8 96.8 87.9 92. ar bn 93.6 95.1 100.0 98.6 99.6 96.7 99.4 8.2 96.5 99.0 0.0 58.4 99.8 98.8 94.7 100.0 100.0 0.0 93.9 98.8 93.9 95.3 0.0 98.2 96.1 0.0 0.6 99.6 99.0 96.5 cs 97.7 98.6 0.0 47.1 47.5 97.1 97.7 0.0 93.9 67.6 4.9 1.4 98.8 97.9 95. da 96.7 93.9 0.0 87.5 93.4 82.8 82.8 9.6 86.1 85.5 68.8 85.4 95.9 19.1 96.3 de el es fa fi fil fr he hi 100.0 100.0 0.0 100.0 100.0 100.0 100.0 12.9 99.8 98.6 95.5 99.2 100.0 99.6 100. 100.0 100.0 0.0 60.7 58.2 99.0 100.0 0.2 99.4 92.0 7.0 16.2 99.4 99.8 99.8 99.8 99.4 100.0 99.6 99.8 99.8 99.4 5.9 99.2 99.8 52.7 99.4 100.0 99.2 99.8 100.0 100.0 100.0 0.0 0.0 98.8 97.9 58.4 99.2 0.2 0.0 0.0 100.0 98.4 99.0 99.8 100.0 0.0 74.0 91.8 96.1 98.2 0.2 95.7 12.1 35.0 15.2 99.8 91.6 98.4 100.0 100.0 0.0 71.5 52.7 95.3 96.3 10.9 96.3 1.0 80.7 30.1 99.8 68.9 97.9 100.0 100.0 0.0 99.8 100.0 100.0 100.0 10.0 98.2 100.0 88.3 99.8 100.0 100.0 100. 99.8 98.8 0.0 35.4 30.7 96.7 98.4 4.5 98.2 77.0 0.0 14.8 99.8 100.0 62.9 99.6 99.0 0.0 98.2 98.8 88.9 95.1 0.0 60.2 98.4 0.0 4.7 100.0 98.2 92.6 hr 84.6 80.9 0.0 70.1 27.0 91.4 83.2 3.1 79.1 20.7 17.0 25.2 93.4 67.8 77.3 hu 99.8 100.0 0.0 9.0 90.8 96.1 98.2 0.2 75.4 60.7 91.0 56.4 100.0 93.6 94. id 99.2 96.7 0.0 66.4 96.9 96.9 96.7 12.7 86.9 40.6 1.6 31.6 99.6 97.9 93.6 it ja ko mi nl 99.6 99.8 0.0 99.2 99.4 99.6 100.0 3.9 99.0 99.6 94.7 99.0 100.0 100.0 100.0 98.8 98.8 0.0 61.5 99.2 100.0 99.8 19.3 98.8 99.8 0.0 58.6 100.0 99.4 98.4 100.0 100.0 0.0 9.8 91.6 99.2 99.2 18.0 99.0 91.4 0.0 9.0 100.0 100.0 99.2 98.8 100.0 0.0 0.0 0.0 48.2 66.8 0.2 64.5 0.0 93.0 1.0 99.6 0.8 99.2 100.0 100.0 100.0 99.8 99.4 99.6 99.2 0.0 99.2 99.8 99.6 93.6 100.0 99.6 99. no 97.3 95.7 0.0 92.2 95.1 83.0 86.5 5.3 94.1 80.1 9.0 89.8 95.5 95.7 95.5 pl pt 99.8 99.6 0.0 41.2 95.5 99.6 99.6 0.0 95.7 92.0 9.2 27.3 100.0 99.6 96.5 100.0 99.4 100.0 27.5 27.0 100.0 99.8 34.0 95.3 43.9 48.2 63.9 100.0 100.0 99. quz 1.8 3.7 0.0 0.0 0.0 7.0 1.2 0.0 0.2 0.0 0.8 0.0 9.4 0.0 10.0 ro ru sv 100.0 100.0 0.0 95.5 69.1 98.8 99.8 15.4 98.2 95.7 92.6 56.8 99.6 100.0 98. 99.6 99.4 100.0 68.2 100.0 99.2 99.8 0.0 99.4 100.0 0.2 0.0 100.0 99.6 97.3 98.8 98.2 0.0 94.9 96.9 97.7 98.2 23.6 97.9 96.7 33.8 85.0 100.0 67.0 99.4 sw 90.6 86.5 0.0 1.6 0.0 34.6 54.7 4.5 72.1 1.6 73.6 2.9 95.9 82.4 66.4 te th tr uk vi 100.0 100.0 100.0 26.4 56.8 90.8 99.4 0.0 95.1 0.0 0.0 13.7 99.4 99.8 85.5 99.6 99.8 0.0 68.0 91.6 98.2 99.2 1.0 98.8 47.9 2.3 3.5 100.0 100.0 96.3 100.0 99.6 100.0 9.4 84.4 97.7 98.4 1.0 89.8 96.3 0.2 38.1 99.8 99.8 95. 99.8 99.2 0.0 11.3 0.0 95.7 40.6 0.8 83.2 7.2 0.0 0.0 100.0 91.0 99.2 100.0 100.0 0.0 54.7 99.6 96.1 100.0 12.7 98.2 62.5 0.2 51.8 100.0 99.8 90.6 zh 89.6 86.5 0.0 88.9 99.8 99.8 99.8 98.8 99.0 99.8 0.0 37.9 99.8 99.0 97.1 Table 41: XM3600 Language Fidelity en Centurio Aya 65.0 Centurio Qwen 75.4 28.7 Parrot 56.6 PALO 13B PALO 7B 58.0 InternVL 2.5 4B 69.0 InternVL 2.5 8B 73.5 61.9 Qwen2-VL 2B 62.1 Qwen2-VL 7B 50.1 Maya 58.9 Phi 3.5 Vision 60.9 Pixtral 12B 69.0 Pangea 71.9 MiniCPM 2.6 avg. 62.4 70.2 31.4 53.6 53.4 58.7 66.4 56.2 59.6 43.9 53.3 52.7 65.2 65.4 ar 61.7 68.8 34.0 51.8 52.5 55.7 61.8 52.9 59.2 45.3 49.7 36.0 64.5 61. es 61.0 70.9 24.3 52.7 52.3 58.8 68.0 55.3 58.9 42.7 52.7 57.9 64.3 67.5 fr 64.3 70.5 30.0 54.9 53.7 61.4 68.4 58.6 60.0 45.8 56.4 59.0 66.3 67.0 ru 62.7 70.8 37.4 55.0 55.1 59.0 67.3 57.9 60.3 41.8 54.3 58.1 65.7 66. en Centurio Aya 37.6 Centurio Qwen 46.4 35.3 Parrot 32.4 PALO 13B PALO 7B 31.8 InternVL 2.5 4B 49.2 InternVL 2.5 8B 50.7 36.8 Qwen2-VL 2B 43.0 Qwen2-VL 7B 37.9 Maya 41.7 Phi 3.5 Vision 30.3 Pixtral 12B 43.1 Pangea 39.1 MiniCPM 2.6 Table 42: XVNLI avg. 37.2 43.0 32.4 28.9 30.9 42.7 45.2 35.5 40.7 33.3 37.4 26.2 42.0 36.5 ar 36.2 39.6 31.9 24.2 28.2 41.6 40.3 31.5 36.9 32.6 34.9 19.1 37.6 30.5 fr 38.9 45.0 34.9 34.9 33.6 45.6 48.7 41.3 42.6 36.6 44.3 28.5 43.0 38.9 Table 43: xMMMU hi 38.8 41.6 26.1 24.2 27.3 33.7 41.2 30.2 38.5 31.3 29.2 19.2 38.5 33. id 39.7 44.1 31.3 31.6 30.6 43.4 43.1 36.7 41.1 31.6 37.7 27.3 46.8 37.7 ja 34.2 43.5 34.9 26.4 32.3 44.2 47.6 36.1 41.3 32.0 35.7 28.6 41.6 37.2 pt 35.4 44.1 35.4 32.3 33.3 47.8 50.2 37.0 43.8 36.0 42.4 34.7 44.8 40. avg. amh-ethiopia arz-egypt ben-india bre-france bul-bulgaria fil-philippines gle-ireland hin-india ibo-nigeria ind-indonesia jav-indonesia jpn-japan kin-rwanda kor-south korea mar-india min-indonesia mon-mongolia msa-malaysia nor-norway orm-ethiopia por-brazil ron-romania rus-russia sin-sri lanka spa-argentina spa-chile spa-colombia spa-ecuador spa-mexico spa-spain spa-uruguay sun-indonesia swa-kenya tam-india tel-india urd-india urd-pakistan zho-china zho-singapore Centurio Aya 49.4 Centurio Qwen 52.9 41.1 Parrot 39.6 PALO 13B PALO 7B 37.1 InternVL 2.5 4B 48.1 InternVL 2.5 8B 48.6 33.6 Qwen2-VL 2B 37.6 Qwen2-VL 7B 39.8 Maya 38.8 Llama-Vision 40.9 Phi 3.5 Vision 33.5 Pixtral 12B 55.2 Pangea 34.1 MiniCPM 2.6 32.1 38.0 31.6 26.1 20.5 36.3 29.5 27.4 31.2 30.3 32.1 28.6 22.6 35.5 26.9 52.7 52.7 35.5 31.5 25.1 38.9 41.4 31.0 35.5 41.9 5.4 38.4 27.1 49.3 31.5 45.8 54.9 33.9 33.9 30.1 42.7 42.3 33.6 31.5 38.8 60.1 28.7 21.7 53.5 27.6 30.4 30.6 31.1 31.6 27.7 33.1 29.4 25.9 31.1 30.6 13.6 28.9 24.9 33.1 25.9 50.1 49.6 38.8 35.3 32.6 42.0 47.4 32.9 35.6 36.7 22.4 33.7 30.5 52.3 32. 48.8 51.7 45.3 45.3 43.3 47.8 46.8 32.0 40.9 35.0 43.8 45.3 35.5 56.7 31.5 41.7 48.5 45.1 41.1 37.1 45.7 47.5 31.3 37.1 34.4 35.9 40.2 38.7 53.1 37.1 67.2 65.7 41.8 40.3 43.3 51.7 50.2 34.8 39.3 46.8 46.3 42.8 41.3 66.2 32.3 31.0 30.5 35.5 32.5 31.0 29.0 33.5 35.5 31.0 31.0 28.5 38.0 26.5 40.0 36.0 53.6 54.9 43.4 41.0 37.9 54.6 54.9 36.9 40.8 36.2 42.0 40.8 36.9 60.0 36.2 41.1 46.1 37.7 35.4 33.7 44.4 44.8 31.6 32.3 34.7 34.0 35.4 32.3 50.5 31. 44.8 44.8 34.5 33.0 29.1 39.4 41.4 25.1 36.0 29.1 25.1 36.9 27.6 42.9 33.5 32.8 42.1 32.8 28.9 29.4 34.9 32.8 31.5 30.2 31.5 18.3 36.2 25.1 33.6 30.6 61.7 66.2 47.6 42.8 42.4 65.9 56.9 37.6 43.4 50.0 45.9 44.1 39.7 68.3 31.0 56.9 55.9 32.2 37.6 33.7 48.0 43.6 24.3 31.2 42.6 38.1 33.7 24.3 57.9 32.2 42.6 41.8 36.3 37.8 31.1 41.0 44.6 27.9 33.5 33.9 34.3 39.0 28.3 48.2 29.9 29.2 33.3 34.3 26.3 25.6 27.6 33.0 31.1 34.0 31.1 27.9 35.3 24.7 40.7 29. 52.7 55.9 42.9 44.1 36.8 53.0 54.3 32.1 43.5 44.4 40.6 41.3 36.8 60.3 34.6 55.5 58.2 49.5 54.5 47.5 52.8 55.2 40.5 42.8 47.5 48.5 48.2 32.1 58.2 40.5 36.4 35.0 34.6 29.4 33.6 34.6 32.2 33.2 37.4 29.9 23.4 33.6 21.5 36.4 36.0 65.1 70.8 60.9 53.9 51.1 66.5 64.4 39.8 47.9 53.5 38.7 62.0 41.2 69.7 45.1 61.6 57.0 44.0 52.6 49.3 49.7 60.3 32.5 34.8 51.3 47.7 43.7 28.8 62.9 33.4 65.5 67.5 45.0 44.0 45.5 65.5 62.5 33.0 47.5 42.0 52.0 45.0 40.5 73.5 37. 28.9 42.7 28.0 24.9 24.0 34.7 29.8 24.9 30.7 30.2 48.4 29.3 23.6 36.0 26.7 60.0 63.8 47.9 47.5 47.5 60.4 60.4 40.0 44.5 44.9 47.9 55.1 49.1 63.8 37.7 58.5 66.2 52.1 50.0 49.1 59.8 65.4 40.2 47.9 47.4 55.1 59.0 54.3 67.1 44.4 56.0 59.8 48.5 47.3 45.2 54.4 56.4 40.7 40.7 45.2 51.0 51.9 43.6 61.4 40.7 55.2 58.8 43.9 47.8 45.9 56.6 59.7 39.0 42.3 45.6 48.1 52.8 48.3 62.4 38.7 52.6 61.9 44.3 44.9 42.4 53.9 57.0 35.0 40.2 39.3 48.3 48.0 40.6 60.7 35. 68.2 70.4 65.7 63.2 57.5 68.2 72.3 42.1 47.8 55.7 70.1 64.2 48.4 73.0 42.5 40.3 42.5 36.2 39.4 36.2 44.8 44.8 34.6 41.6 34.3 37.1 45.1 40.3 44.8 36.2 42.0 41.5 31.5 37.5 31.5 44.0 41.5 33.5 31.5 33.5 29.5 32.0 27.0 49.0 29.0 50.2 56.4 40.7 39.6 31.1 45.8 50.9 37.7 34.1 38.8 52.0 46.5 42.1 65.6 34.1 49.5 43.0 36.9 31.3 29.0 35.5 35.5 25.2 26.6 32.2 60.7 29.4 22.9 46.7 24.3 37.0 50.5 29.5 29.5 28.5 41.0 39.5 27.0 26.5 29.0 62.5 32.5 19.0 55.0 28. 47.3 52.7 31.8 35.9 34.1 47.7 44.1 30.5 37.3 43.2 24.5 29.5 27.7 57.7 32.7 50.5 56.9 32.9 40.3 40.3 41.7 39.8 31.9 31.0 48.1 15.3 26.9 23.6 65.7 22.7 64.6 71.1 64.3 46.6 46.3 74.6 78.5 44.1 51.4 50.5 36.3 51.1 47.3 71.7 48.2 65.6 73.1 55.2 39.6 42.0 66.5 72.6 41.5 43.4 50.9 23.1 40.6 39.6 68.4 46.2 Table 44: CVQA en Centurio Aya 83.1 Centurio Qwen 84.8 51.0 Parrot 54.0 PALO 13B PALO 7B 55.5 InternVL 2.5 4B 87.0 InternVL 2.5 8B 91.0 85.0 Qwen2-VL 2B 91.2 Qwen2-VL 7B 51.4 Maya 91.1 Llama-Vision 92.2 Phi 3.5 Vision 91.1 Pixtral 12B 87.2 Pangea 89.0 MiniCPM 2.6 avg. 74.2 76.1 49.9 51.5 52.8 78.3 79.2 83.5 90.9 50.9 84.8 79.4 71.0 72.2 74.3 avg. Latin avg. other 80.9 82.7 50.5 52.7 55.4 86.9 88.7 83.4 90.1 51.6 89.9 90.2 90.5 85.7 88.0 69.7 71.8 49.5 50.7 51.0 72.6 72.8 83.5 91.4 50.4 81.5 72.2 58.0 63.1 65.2 ar 75.9 76.9 50.4 50.9 50.4 54.9 55.8 70.6 83.4 50.4 63.2 53.1 50.4 51.5 52.0 de 82.1 83.5 51.6 53.2 56.9 87.6 89.8 84.4 90.5 53.4 90.1 91.9 91.5 86.6 89. hi 80.1 82.4 49.6 51.2 51.0 59.8 54.9 86.5 94.8 50.1 91.1 83.8 53.6 69.4 53.1 id 81.4 83.8 51.0 52.5 55.0 87.0 89.1 84.1 91.0 51.5 89.5 89.2 91.1 86.2 87.9 it 80.6 83.1 49.8 52.8 54.1 88.2 89.1 83.5 90.8 50.0 91.9 90.9 90.9 87.1 89. ko 68.8 72.4 50.4 51.0 51.6 89.4 92.5 88.1 93.8 49.9 87.4 77.9 49.5 71.4 54.8 ru 73.5 75.6 50.5 49.5 51.1 86.4 86.9 78.8 87.5 49.5 83.0 86.6 88.2 79.2 84.0 th 66.5 64.4 48.2 51.0 51.4 55.1 53.1 86.4 94.1 51.1 84.8 55.5 52.9 54.4 53. zh 53.4 58.9 47.8 50.7 50.2 90.4 93.6 90.4 94.9 51.6 79.5 76.5 53.4 52.9 94.5 zu 79.5 80.2 49.5 52.1 55.8 84.8 86.9 81.8 88.2 51.6 88.0 88.8 88.4 82.9 86.0 Table 45: SMPQA Ground en Centurio Aya 60.0 Centurio Qwen 65.2 0.0 Parrot 25.6 PALO 13B PALO 7B 22.4 InternVL 2.5 4B 77.8 InternVL 2.5 8B 80.6 68.8 Qwen2-VL 2B 85.0 Qwen2-VL 7B 14.6 Maya 58.4 Llama-Vision 84.8 Phi 3.5 Vision 85.0 Pixtral 12B 72.0 Pangea 80.8 MiniCPM 2.6 avg. 30.1 31.7 0.0 4.0 2.7 47.5 48.2 47.4 64.9 1.8 22.8 35.9 35.9 23.8 39.3 avg. Latin avg. other 49.8 54.3 0.0 9.9 6.7 67.7 68.1 60.0 76.2 4.3 46.6 69.4 73.3 54.4 67. 17.0 16.6 0.1 0.1 0.1 34.0 34.9 39.0 57.4 0.1 6.9 13.5 10.9 3.4 20.6 ar 29.2 21.4 0.0 0.0 0.0 0.0 0.0 0.2 1.8 0.0 0.0 0.2 0.0 0.0 0.0 de 50.2 53.2 0.0 12.0 8.4 71.0 69.2 61.2 80.6 8.2 55.4 70.8 71.8 58.6 67.2 hi 17.6 21.4 0.0 0.0 0.0 0.0 0.0 24.8 58.6 0.0 2.4 12.0 0.0 0.2 0.0 id 52.6 55.4 0.0 10.2 7.0 69.8 70.4 59.4 75.8 3.6 38.4 69.4 75.4 57.2 69.8 it 51.2 56.6 0.0 12.4 7.0 69.6 70.8 61.2 79.2 4.6 37.2 76.6 81.6 64.4 71.4 ko 11.2 16.2 0.4 0.4 0.4 69.0 67.2 66.0 77.6 0.4 8.4 15.4 0.4 0.4 1.0 ru 38.2 34.8 0.0 0.0 0.0 54.4 61.2 46.8 70.6 0.0 13.0 40.4 64.6 19.2 38.4 th 4.8 5.2 0.0 0.0 0.0 0.2 0.2 24.0 43.8 0.0 6.0 0.2 0.4 0.4 0.4 zh 0.8 0.6 0.0 0.0 0.0 80.2 80.8 72.0 92.0 0.0 11.8 12.8 0.0 0.0 83.6 zu 45.2 52.2 0.0 5.0 4.4 60.4 62.2 58.2 69.2 0.8 55.4 61.0 64.6 37.4 61.6 Table 46: SMPQA Name"
        }
    ],
    "affiliations": [
        "Data Science Group, University of Hamburg",
        "Language Technology Group",
        "WüNLP, Computer Vision Lab, CAIDAS, University of Würzburg"
    ]
}