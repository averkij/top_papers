{
    "paper_title": "Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval",
    "authors": [
        "Fanheng Kong",
        "Jingyuan Zhang",
        "Yahui Liu",
        "Hongzhi Zhang",
        "Shi Feng",
        "Xiaocui Yang",
        "Daling Wang",
        "Yu Tian",
        "Victoria W.",
        "Fuzheng Zhang",
        "Guorui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, a systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, a universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides a foundational blueprint for future research in multimodal systems. Our project is available at https://friedrichor.github.io/projects/UNITE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 2 0 5 6 9 1 . 5 0 5 2 : r Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval Fanheng Kong1,2*, Jingyuan Zhang2*, Yahui Liu2, Hongzhi Zhang2, Shi Feng1, Xiaocui Yang1, Daling Wang1, Yu Tian2, Victoria W., Fuzheng Zhang2, Guorui Zhou2 1Northeastern University 2Kuaishou Technology kongfanheng426@gmail.com, fengshi@cse.neu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides foundational blueprint for future research in multimodal systems. Our project is available at https://friedrichor.github.io/projects/UNITE. Figure 1: Performance comparison on instruction-based retrieval benchmarks (left: MMEB [32] and right: WebVid-CoVR [79]). Our UNITE achieves leading performance on various tasks, even surpassing models with larger parameter scales."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Information Retrieval (MIR) is critical research topic [69, 28], aiming to satisfy users information requirements for diverse media, such as text, images, and videos. As multimedia * Equal contributions. Corresponding author. Work done during an internship at Kuaishou Technology. Preprint. Under review. applications continue to progress and develop, series of more complex and demanding tasks come to the fore, which are collectively referred to as fused-modal retrieval such as the retrieval of composite images or videos [53, 108, 79]. These tasks require highly sophisticated approaches for handling interleaved multimodal queries and candidates, highlighting the necessity for the development of one-piece framework for unified multimodal representations. Recently, large multimodal models (LMMs) have shown powerful capabilities on various visionlanguage tasks, such as visual question answering (VQA) [3, 34, 83, 59] and multimodal factchecking [98, 1]. In MIR field, several methods [32, 52, 24] have explored adapting large language models (LLMs) for retrieval tasks via contrastive learning, aiming to produce unified embeddings. For example, E5-V [31] finetunes LLaVA-NeXT [49] with text-only NLI [22] data, demonstrating the portability of LMMs for multimodal retrieval. GME [109] achieves leading performance in various image-text retrieval by finetuning Qwen2-VL [82] on diverse image-text datasets. InternVideo2 [86] stands out prominently in text-video retrieval, due to its training process that involves on hundred millions of video-text pairs. Although achieving notable success in specific domains, these models are hindered by their limited modalities, which inherently restrict their ability to fully capitalize on the potential of LMMs for generating unified multimodal embeddings. Despite ongoing researches [63, 106, 120] exploring training strategies for LMMs in MIRincluding model architectures, training methodologies, and dataset considerationscritical questions remain unresolved. Specifically, the optimal data composition and proportions, as well as the nuanced impact of different modal data configurations across various retrieval tasks, have yet to be comprehensively understood. In our empirical investigations, we find that inappropriate combinations of multimodal data or data training sequences can easily disrupt the harmonious integration of diverse data modalities, causing the model to misinterpret the relationships between different types of information. In this paper, through meticulous analysis of how various data compositions impact retrieval results, we make efforts to achieve balance among the three modalities of text, image, and video. In particular, we find that introducing small number of fine-grained video-text pairs during the retrieval adaptation stage can significantly enhance the fine-grained retrieval performance of LMMs. Meanwhile, existing works [31, 109] have proven that there is substantial distribution gap in the data of various modalities within the feature space. Simply mixing data from different modalities for contrastive learning will affect the quality of representation learning (i.e., introducing noise). Therefore, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to balance the competitive relationships among the instances of various modalities. Finally, we develop UNIversal mulTimodal Embedder, named UNITE, that effectively handles text, images, videos, and their combinations. During the training, we employ an evolving training strategy to gradually unlock the retrieval capability of LMMs. After two training stages (i.e., retrieval adaptation and instruction tuning), we validate our approach through comprehensive evaluation on 40+ diverse retrieval tasks, spanning coarse-grained, fine-grained, and instruction-based retrieval across text, images, and video. Experimental results show that UNITE achieves state-of-the-art performance in various tasks, and outperforms existing specialized domain-specific models in numerous scenarios. In summary, our contributions are as follows: We unveil the appropriate method for curating modality data during the process of learning unified multimodal embeddings, so as to balance the gap between the feature spaces of different modalities. We propose MAMCL to mitigate the competitive relationships among the instances of various modalities. Specially, the MAMCL strategy can serve as general method and be applied to any extended modal scenarios. To the best of our knowledge, UNITE is the first model capable of enabling text, image, video and fused-modal to concurrently focus on fine-grained and instruction-based retrieval tasks. Notably, UNITE achieves state-of-the-art performance in 40+ different tasks, surpassing specialized domainspecific models in numerous scenarios, as shown in Figure 1."
        },
        {
            "title": "2 Related Work",
            "content": "Large Multimodal Models. LMMs have mushroomed, showcasing impressive capabilities in multimodal information understanding [41, 107]. Pioneering efforts such as LLaVA [50, 48], MiniGPT4 [117], InternVL [15, 118], and Qwen-VL [82, 5], bridge visual encoders and LLM with lightweight 2 intermediate architectures. Recent advances [40, 88, 120, 95, 58, 43] have extended these techniques from static images to sequential videos, demonstrating promising results in video understanding through frame-based processing. These developments have catalyzed the widespread adoption of LMMs across various applications [51, 66, 110, 37]. Cross-Modal Retrieval. Traditional multimodal retrieval primarily focuses on cross-modal scenarios, particularly text-image retrieval [81, 84] and text-video retrieval [116, 29]. Foundational methods, such as CLIP [69], Align [28], BLIP [42] and CoCa [101], separately encode text and images through dual-encoder structures, and learn multimodal representations by contrastive learning on large-scale image-text pairs. ImageBind [23], OmniBind [89] and VAST [13] expand this approach to accommodate more modalities with similar architectures. Instruction-based Retrieval. Recently, the community has witnessed growing demand for multimodal information retrieval in complex scenarios, including composed image retrieval [53, 72, 108], composed video retrieval [79, 26], multimodal document retrieval [62, 57, 19], multimodal knowledge retrieval [55, 54], and multimodal retrieval-augmented generation [14, 113, 7, 56]. While CLIP-based models struggle with complex multimodal queries and candidates, LMMs naturally excel at processing fused-modal inputs within universal framework. Recent methods [109, 52, 44, 32, 39, 46, 39] contrastively train LMMs (e.g., Qwen2-VL [82] and LLaVA-NeXT [49]) to uniformly embed images and text, boosting complex fused-modal retrieval. However, these models still face limitations in video-related tasks compared to video-specialized methods, highlighting the need for universal multimodal embedder that excels across text, image, video, and their combinations."
        },
        {
            "title": "3 UNITE",
            "content": "3.1 Task Formulation Unified multimodal retrieval addresses queries and candidates across text, image, and their combinations [91, 109]. We extend prior formulations to include video, enabling more comprehensive multimodal embedding alignment. We define query Rd representing an embedding that is sampled from specific modality. Specifically, qt, qi and qv denote the text, image and video embeddings, respectively. In practice, the query can be any single modality or combination of various modalities (e.g., (qt, qi), (qv, qt)). Similarly, we define retrieval candidate Rd presenting the representation embeddings of specific modality. Thus, ct, ci and cv denote the text, image and video embeddings, respectively. The main purpose of MIR is to maximize the correlation between the related query and candidate pairs in the repsentation space Q, while ensuring that the correlation between the unrelated query and candidate pairs is as low as possible. In practice, for each query q, we can collect its corresponding positive candidate c+ and set of negative candidates = {c K}, where consists of both in-batch negatives and hard negatives, refers to the number of negative candidates. As aforementioned, both queries and candidates can encompass various modalities. We employ contrastive learning with the InfoNCE loss [65] to simultaneously minimize the distance between matched query-candidate pairs (q, c+) while maximizing the distance between unmatched pairs (q, c): 1 , . . . , LCL = log exp(cos(q, c+)/τ ) exp(cos(q, c+)/τ ) + (cid:80) C exp(cos(q, c)/τ ) (1) where τ is the temperature hyper-parameter. 3.2 Model Architecture Generally, LMMs are composed of three essential components: large language model, vision encoder, and vision projector, as shown in Figure 2 (a). This architectural design empowers LMMs to handle text, images, videos, and their integrated forms with remarkable fluidity and efficiency. This native multimodal processing provides new promise for unified multimodal embeddings. Following recent methods [44, 32, 109], we extract target embeddings from the hidden state of the final token in the last layer. Inspired by PromptEoL [30] and E5-V [31], we adapt the Explicit One word Limitation (EoL) method. Specifically, we use prompt template as follows: <vision>n<text>nSummarize above <modalities> in one word: 3 Figure 2: Overview of UNITE: (a) Model architecture utilizing LMM as the backbone, supporting multimodal inputs (text, images, videos, and their combinations). (b) Similarity matrix after applying MAMCL, which enables focused contrastive learning by restricting comparisons to samples sharing the same target modality, thus reducing inter-modal interference. where <vision> and <text> are placeholders for visual content (i.e., images, videos) and textual sentences, respectively, and <modalities> specifies the input modality types. For instance, videotext input would utilize the following prompt: <video>n<text>nSummarize above video and sentence in one word:. 3.3 Training Strategy We employ two-stage training scheme: retrieval adaptation and instruction tuning. In the first stage, we adapt LMMs to general retrieval tasks, while we enhance the capabilities of LMMs for instruction-based complex retrieval scenarios in the second stage. Retrieval Adaptation. LMMs have showcased remarkable prowess in multimodal understanding, adeptly handling and interpreting diverse forms of information. However, retrieval tasks remain uncharted territory for these models. Thus, in the first stage, we focus on building robust fundamental retrieval capabilities by exposing LMMs to various information scenarios. It enables the LMMs to learn and adapt to the varying characteristics and requirements of different retrieval tasks. For example, we utilize single-modal and multimodal retrieval data, including text-text, image-text and video-text pairs, in the experiments. Instruction Tuning. To enhance generalization across varied multimodal retrieval scenarios, we employ instruction tuning using comprehensive datasets like MMEB [32], which covers 36 datasets across 4 multimodal tasks. This stage introduces complex fused-modal retrieval scenarios via instruction-guided samples. This goes beyond the basic single-modal and multimodal retrieval tasks, enabling more sophisticated and nuanced understanding of retrieval tasks. 3.4 Modal-Aware Masked Contrastive Learning Traditional multimodal retrieval models typically employ standard InfoNCE loss [65] (i.e., Eq. (1)) in the contrastive learning, it treats all negative pairs equally regardless of the modality compositions. However, this strategy overlooks the inherent distinctiveness of diverse modal combinations in retrieval tasks. For instance, embeddings derived solely from text and those from multimodal sources typically display substantial disparities in their distributions within the feature space. When subjected to joint contrastive learning, the model struggles to balance the diverse information from different modalities, resulting in representations that fail to fully capture the semantic richness of each modality. This leads to our hypothesis: Contrastive learning performed across instances featuring disparate target modalities has the potential to introduce noise and give rise to negative effects. To address this crucial problem, we propose Modal-Aware Masked Contrastive Learning (MAMCL), introducing modality-aware constraint to mitigate competitive relationships between various target modal instances. Given batch of samples, we compute the similarity matrix RN (1+K), where 1 + refers to the number of all positive and negative candidates, Snk represents the cosine 4 Table 1: Zero-shot performance of fine-grained video-text retrieval on CaReBench [96]. LLaVA-NV refers to LLaVA-NeXT-Video [111]. indicates that these models have been equipped with feature representation capabilities through contrastive learning. Rel. represents the relative performance improvement of UNITEbase 7B compared to UNITEbase 2B. Results in bold and underline denote the best and second-best performances. Model TV VT TV VT TV VT CaRe-General CaRe-Spatial CaRe-Temporal R@1 R@ R@1 R@5 R@1 R@5 R@1 R@ R@1 R@5 R@1 R@5 CLIP-based Models CLIP L/14 [69] LanguageBind [115] Long-CLIP L/14 [105] InternVideo2stage2 1B [86] LMM-based Models LLaVA-NV 7B [111] MiniCPM-V 2.6 [99] InternVL2 8B [15] Tarsier 7B [80] Qwen2-VL 7B [82] CaRe 7B [96] UNITEbase 2B UNITEbase 7B Rel. 51.2 64.3 62.7 72.5 66.9 71.0 72.1 71.0 76.6 77.0 78.1 86.0 83.4 91.0 88.8 93.7 89.4 92.2 92.6 93.8 95.3 95. 95.5 96.8 54.7 59.5 60.3 69.5 62.7 69.3 73.6 70.6 77.4 79.0 80.8 86.9 86.9 88.0 88.8 94.6 89.2 92.8 93.4 94.2 95.6 96. 96.4 98.3 49.0 64.7 65.6 72.4 68.0 71.7 76.1 70.2 78.2 76.8 79.6 86.5 81.9 90.8 90.9 94.2 92.0 93.6 94.1 94.0 95.5 96. 95.4 96.9 55.4 61.0 61.0 62.7 65.0 67.6 74.3 67.4 75.4 78.1 78.0 84.8 85.6 87.2 88.3 90.5 90.0 92.3 94.5 93.5 95.0 95. 95.4 98.0 33.5 39.8 33.2 46.0 43.3 50.5 48.1 50.1 51.9 50.7 45.3 52.4 70.3 77.3 68.8 80.8 76.9 82.9 76.8 84.1 84.8 85. 77.6 82.5 39.7 42.2 34.5 46.6 40.1 46.1 47.6 50.0 52.7 53.4 50.0 55.4 76.2 77.6 71.9 82.5 75.4 80.9 78.2 84.7 85.4 86. 83.6 86.5 +10.1% +1.4% +7.5% +2.0% +8.7% +1.6% +8.7% +1.6% +15.7% +6.3% +10.8% +3.5% similarity between query qn and candidate ck: Snk = cos(qn, cnk)/τ (2) where {1, . . . , } and {1, . . . , 1+K}, cnk refers to the k-th candidate of the n-th query sample. To incorporate modality awareness, we introduce modality mask matrix MMM {0, 1}N (1+K), where MMMnk indicates whether candidate cnk shares the same target modality combination as the positive candidate c+ of query qn: MMMnk = 1[Φ(c+ (3) where Φ() refers to the operation of extracting the modality type of the embeddings. This extraction can be directly accomplished using the prior knowledge of the inputs. In this manner, we ensure that each query only considers candidates with the same modality as its target candidate during the contrastive learning. Then, we update the masked similarity matrix as follows: ) = Φ(cnk)] Snk = Snk MMMnk + () (1 MMMnk) (4) visualization of the similarity matrix is shown in Figure 2 (b). Finally, we expand the Eq. (1) to incorporate negatives, constantly ensuring that the model remains sensitive to different modalities: LMAMCL = 1 (cid:88) n=1 log exp(Snp) exp(Snp) + (cid:80)K+1 k=1,k=p exp(Snk) (5) where refers to the index of the positive candidate."
        },
        {
            "title": "4 Experiments",
            "content": "Experimental Setup For the retrieval adaptation stage, we curate diverse 7M-instance dataset spanning four categories: (1) text-text pairs from MSMARCO [6], NLI [22], NQ [38], HotpotQA [97], Fever [77], TriviaQA [33] and SQuAD [70]; (2) image-text pairs from CapsFusion [102], LAIONArt [73] and MSCOCO [45]; (3) video-text pairs from InternVid-10M-FLT [85]; (4) fine-grained video-caption pairs Tarsier2-Recap-585K [103]. We name the model trained in this stage as UNITEbase. For the instruction tuning, we use the combination of MMEB [44] and WebVid-CoVR [78] as our training set. Thus, we name the model trained in this stage as UNITEinstruct. We will explain the underlying rationale for this data composition in Section 5. Our specific training data composition and allocation are shown in Appendix A.1. We used Qwen2-VL [82] as the backbone of our model to conduct experiments on models with both 2B and 7B parameters. All evaluation datasets are described in Appendix A.2. All experimental results are reported in Recall@1 unless otherwise specified. We refer to Appendix for more implementation details. 5 Table 2: Results of fine-grained image-text retrieval on ShareGPT4V, Urban1K, and DOCCI. Table 3: Results on the WebVid-CoVR. We take the best variant of baselines in the table. Model ShareGPT4V Urban1K DOCCI TI IT TI IT TI IT Model WebVid-CoVR-Test (TVV) R@1 R@5 R@ R@50 CLIP-based Models 83.6 CLIP L/14 [69] OpenCLIP L/14 [16] 81.8 Long-CLIP L/14 [105] 97.3 93.1 EVA-CLIP 8B [75] - FineLIP [4] LMM-based Models MATE [27] E5-V 7B [31] VLM2Vec 7B [32] UniME 7B [24] UNITEbase 2B UNITEbase 7B - 85.1 90.7 93.9 89.7 93. 84.2 84.0 97.2 91.2 - - 82.1 85.8 97.2 86.7 93.2 - 55.6 68.3 65.8 63.1 47.0 47.0 86.1 82.7 78.6 66.5 80.4 77.8 94.1 93.2 86.0 84.5 - - - - - 84.6 76.6 88.9 83.2 90.8 84.7 95.2 95. - - - - - - 91.5 89.2 81.4 72.3 95.5 95.6 87.2 85.8 Zero-shot Setting LanguageBind [115] 43.2 66.3 45.5 70.5 CoVR [79] 45.7 71.7 CoVR-2 [78] 51.7 75.3 TFR-CVR [26] Finetuning Setting CoVR [79] CoVR-2 [78] ECDE [76] UNITEinstruct 2B UNITEinstruct 7B 53.1 79.9 59.8 83.8 60.1 84.3 69.1 88.4 72.5 90.8 75.2 79.5 81.3 80.7 86.9 91.3 91.3 93.2 95.3 - 93.3 94.8 - 97.7 98.2 98.7 99.1 99.5 Table 4: Results on the MMEB benchmark [32]. We average the scores in each meta-task. We compare numerous recent works, across diverse model scales. IND refers to the in-distribution dataset, and OOD denotes the out-of-distribution dataset. Model #Parameters #Datasets Zero-shot Setting Magiclens (ViT-L/14) [108] CLIP (ViT-L/14) [69] OpenCLIP (ViT-L/14) [16] BLIP2 (ViT-L/14) [41] Finetuning Setting CLIP (ViT-L/14) [69] OpenCLIP (ViT-L/14) [16] E5-V (LLaVA-1.6) [31] MMRet-MLLM (LLaVA-1.6) [114] VLM2Vec (Qwen2-VL, high-res) [32] UniME (LLaVA-1.6) [24] CAFe (LLaVA-OV) [100] mmE5 (Llama-3.2-Vision) [11] IDMR (InternVL2.5) [46] UNITEinstruct 2B UNITEinstruct 7B 0.4B 0.4B 0.4B 1.2B 0.4B 0.4B 7B 7B 7B 7B 7B 11B 26B 2B 7B Per Meta-Task Score Average Score Classification VQA Retrieval Grounding IND OOD Overall 10 10 12 4 20 36 38.8 42.8 47.8 27.0 55.2 56.0 39.7 56.0 62.6 60.6 65.2 67.6 66.3 63.2 68.3 8.3 9.1 10.9 4.2 19.7 21.9 10.8 57.4 57.8 52.9 65.6 62.8 61. 55.9 65.1 35.4 53.0 52.3 33.9 53.2 55.4 39.4 69.9 69.9 67.9 70.0 70.9 71.1 65.4 71.6 26.0 51.8 53.3 47.0 62.2 64.1 60.2 83.6 81.7 85.1 91.2 89.7 88. 75.6 84.8 31.0 37.1 39.3 25.3 47.6 50.5 34.2 68.0 72.2 68.4 75.8 72.3 73.4 65.8 73.6 23.7 38.7 40.2 25.1 42.8 43.1 33.4 59.1 57.8 57.9 62.4 66.7 63. 60.1 66.3 27.8 37.8 39.7 25.2 45.4 47.2 33.9 64.1 65.8 66.6 69.8 69.8 69.2 63.3 70.3 4.1 Main Results Fine-grained Retrieval. Table 1 demonstrates that our UNITEbase achieves state-of-the-art performance and outperforms the existing methods with substantial margins, particularly on CaRe-General and CaRe-Spatial. This excellence stems from the incorporation of fine-grained video-caption pairs during retrieval adaptation, enhancing the feature representation capabilities of LMMs. While our 2B model outperforms all baselines on general and spatial retrieval tasks, its temporal retrieval performance remains moderate. After scaling the model size to 7B, we obtain significant improvements on general, spatial and temporal tasks. Notably, comparing our 2B and 7B models, we observe that model scaling achieves the most substantial relative improvements in temporal retrieval (e.g., 15.7% and 10.8%). It indicates key insight: Models with larger sizes are likely to be more advantageous for retrieval tasks related to the temporal aspects of videos. Moreover, compared with the level that has been achieved in spatial tasks, there is still great deal of room for improving temporal tasks. In addition, our UNITEbase achieves leading performance in fine-grained image-text retrieval tasks, as shown in Table 2. It indicates that our carefully designed data composition and allocation strategy indeed enables effective alignment across text, image, and video modalities. Instruction-based Retrieval. As shown in Table 3, UNITEinstruct 2B substantially outperforms existing models on the WebVid-CoVR-Test [79]. Similarly, increasing the scale of model size to 7B can significantly boost the improvement margins. In Table 4, the evaluation on the MMEB 6 benchmark, encompassing 36 datasets across four meta-tasks, demonstrates superior performance of UNITEinstruct against various existing models at different parameter scales. For example, UNITE surpasses both larger-scale models (e.g., mmE5 11B [11] and IDMR 26B [46]) and models trained with more extensive datasets (e.g., MMRet [114] with 26M image-text retrieval samples). These compelling results across text, image, and video retrieval scenarios can be attributed to our evolving training strategy and MAMCL strategy (See details in Section 4.2). Coarse-grained Cross-Modal Retrieval. We also assess our the performance of UNITE on coarsegrained retrieval tasks. As demonstrated in Table 5 and Table 6, our models achieve competitive results on coarse-grained cross-modal retrieval tasks, including text-image and text-video scenarios. It proves that our data composition strategy effectively balances performance across different modalities on various retrieval tasks. Table 5: Zero-shot coarse-grained imagetext retrieval results on Flickr30K [67]. Table 6: Zero-shot coarse-grained video-text retrieval results on MSR-VTT [94], MSVD [10], and DiDeMo [2]. Model TI IT R@1 R@5 R@1 R@5 Model MSR-VTT MSVD DiDeMo TV VT TV VT TV VT OpenCLIP-L [16] MagicLens-L [108] 75.0 92.5 88.7 98.4 79.7 95.0 89.6 98.7 CAFe 7B [100] VLM2Vec 7B [32] LamRA-Ret 7B [52] 82.8 81.9 UniME 7B [24] 75.3 92.6 87.5 98.2 80.3 95.0 94.6 99.5 - - 92.7 93.4 - - UNITEbase 2B UNITEbase 7B 80.9 95.3 89.6 98.4 86.1 96.9 94.4 99. 4.2 Ablation Study InternVideo [87] 40.7 LanguageBind [115] 42.1 42.4 ViCLIP [85] VLM2Vec 7B [32] LamRA 7B [52] CaRe 7B [96] UNITEbase 2B UNITEbase 7B 43.5 44.7 43.9 43.8 46. 39.6 65.9 41.3 - - 41.7 41.7 45.2 43.4 40.1 49.1 49.5 52.4 52.6 50.0 50. 67.6 65.4 75.1 - - 74.6 73.1 76.1 31.5 35.6 18.4 - - 41.4 37.9 43. 33.5 35.6 27.9 - - 39.1 37.5 40.3 Table 7: Ablation study of our proposed MAMCL. We show results of various settings on the MMEB and WebVid-CoVR test sets. Gray indicates without fine-tuning on the corresponding training set. Avg refers to the average of the overall score on MMEB and the R@1 score on WebVid-CoVR. Setting MMEB WebVid-CoVR MMEB CoVR MAMCL IND OOD Overall R@ R@5 R@10 2B parameters 64.4 65.5 (+1.1) 60.1 60.2 (+0.1) 62.5 63.1 (+0.6) 36.1 34.9 35. 49.6 48.0 69.2 74.5 73.4 89.4 82.1 80.9 93. 64.8 65.8 (+1.0) 60.3 60.1 (-0.2) 62.8 63.3 (+0.5) 67.4 69.1 (+1.7) 88.0 88.4 (+0.4) 92.6 93.2 (+0.6) 65.1 66.2 (+1.1) 7B parameters 73.3 73.6 (+0.3) 65.8 66.3 (+0.5) 70.0 70.3 (+0.3) 71.4 72.5 (+1.1) 91.1 90.8 (-0.3) 94.7 95.3 (+0.6) 70.7 71.4 (+0.7) Avg - - - ID 1 2 3 4 5 6 7 Modal-Aware Masked Contrastive Learning. We evaluate the effectiveness of Modal-Aware Masked Contrastive Learning (MAMCL) through comprehensive ablation studies across diverse instruction-based retrieval scenarios. As shown in Table 7 (34), we observe that there exists significant performance improvements on MMEB after integrating the MMEB training set. However, the performance degrades on WebVid-CoVR. It verifies our hypothesis that cross-modal interfere might occur among samples with distinct target modalities. Results in Table 7 (12, 45, 67) reveal that MAMCL successfully mitigates these inter-modal effects. Specifically, MAMCL yields substantial improvements in in-distribution (IND) scenarios, validating its effectiveness in scenarios where test samples align with the training distribution. While exhibiting minor fluctuations on out-of-distribution (OOD) datasets, the performance indicates that its generalization capabilities remain well. 7 Hard Negatives. Previous methods [71, 93, 68] have evidenced the importance of hard negatives for retrieval tasks. We conduct an ablation experiment on our UNITEinstruct 2B model. As shown in Table 8, model without the hard negative leads to performance degradation, which underscores the critical role it plays in the training of effective retrieval models. Table 8: Ablation study on hard negative. We report the MMEB overall and WebVid-CoVR R@1 scores. Setting MMEB CoVR Avg UNITEinstruct 2B w/o hard-negative 63.3 62.4 (-0.9) 69.1 68.0 (-1.1) 66.2 65.2 (-1.0)"
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we present systematic investigation of two critical aspects: (1) the impact of various training data on different retrieval tasks, and (2) efficient training strategies for enhancing the fine-grained retrieval capabilities of LMMs. Training Data Composition Understanding the optimal composition of training data for text-imagevideo retrieval scenarios remains an open research problem that warrants systematic investigation. We conduct comprehensive experiments utilizing Text-Text (TT), Text-Image (TI), and Text-Video (TV) datasets, both independently and in various combined scenarios (See details in Appendix B.3). Finally, the evaluation covers coarse-grained, fine-grained, and instruction-based retrieval tasks. Video-text pairs prove to be superior training data for general cross-modal retrieval. The first striking finding is that the TV-only training pattern consistently outperforms all other configurations across the entire spectrum of cross-modal retrieval tasks, as shown in the Coarse and Fine results in Table 9 (See more details in Table 18 in Appendix C.2). Notably, in image-text retrieval tasks, training solely with TV data outperforms training that uses only TI data. This result represents novel discovery, as it contradicts the established findings in conventional image-text research, challenging existing assumptions about the optimal data sources for such retrieval tasks. This unexpected outcome underscores the importance of reevaluating traditional data selection strategies and opens new avenues for exploring how different data types interact with model training in cross-modal retrieval scenarios. Table 9: Results of various training data composition on different retrieval tasks in retrieval adaptation stage. To ensure fairness, the total data size for all configurations is 600K. All scores of cross-modal retrieval are the average of R@1 in zero-shot setting. The tested datasets include (1) coarse-grained image-text datasets (Flickr30K, MSCOCO), video-text datasets (MSR-VTT, MSVD); (2) fine-grained image-text dataset (DOCCI), video-text dataset (CaReBench); and (3) instruction-based datasets (MMEB, WebVid-CoVR). Setting Coarse I-T Coarse V-T Fine I-T Fine V-T MMEB CoVR TT TI TV TI IT TV VT TI IT TV VT IND OOD Overall R@1 64.1 68.9 73.8 67.6 65.1 71.8 67.8 37.8 40.5 44. 41.6 43.4 43.7 43.0 45.8 51.0 56.0 50.8 54.6 55.9 54.6 69.1 75.8 79. 74.8 76.3 77.8 76.5 65.4 71.8 74.9 70.3 70.1 73.0 70.9 45.5 57.9 65. 56.8 62.3 65.7 61.3 52.4 62.9 68.7 58.8 64.1 68.4 61.2 61.9 62.6 62. 63.8 62.1 63.0 62.7 58.5 58.3 59.1 59.9 57.7 59.2 59.0 60.4 60.7 61. 62.1 60.2 61.3 61.0 64.4 66.5 65.6 65.4 65.6 65.8 64.8 53.5 55.4 60. 55.6 56.9 58.3 58.1 Text-text and text-image pairs are essential for instruction-following tasks. To further investigate the impact of data composition on broader retrieval tasks, we conducted comprehensive experiments on instrcution-based retrieval tasks. TT+TI training overall outperforms other combinations on instruction-based retrieval tasks, including TV-only configuration that excel in general cross-modal retrieval tasks, as shown in the MMEB and CoVR results in Table 9 (See more details in Table 19 in Appendix C.2). This observation can be attributed to two key factors: (1) Text-text pairs enhance linguistic understanding and logical reasoning capabilities, establishing solid and comprehensive foundation for interpreting complex retrieval instructions. (2) Text-image pairs provide precise multimodal alignment information, empowering more focused semantic connections compared to video content. These factors enable the model to capture detailed vision-language correspondences that are crucial for instruction following. 8 Effectively Utilizing Fine-Grained Video-Caption Data Recent advances in video LMMs have produced powerful captioning models (e.g., Tarsier [80], AuroraCap [8]) and fine-grained datasets like LLaVA-Video-178K [112]. While CaRe has shown that fine-tuning LMMs with these videocaption pairs prior to retrieval adaptation can significantly boost the performance of fine-grained video retrieval, notable limitation exists in its retrieval adaptation phase, which depends solely on text-text pairs. This raises two critical questions: (1) Does the fine-grained alignment continue to yield effective results when video-text pairs are integrated into the retrieval adaptation process? and (2) How can we optimally utilize fine-grained video-text pairs to achieve the greatest possible enhancements in performance? Table 10: Ablation study on fine-grained alignment stage (Align) across distinct retrieval-adaptation data (Retrieval). The reported scores are Recall@1 of zero-shot results. The composition and volume of retrieval-adaptation data used for ID 1-6 are consistent with those in Table 9 in Appendix C.2. Setting Coarse-grained Video-Text Retrieval Fine-grained Video-Text Retrieval MSR-VTT MSVD CaRe-General CaRe-Spatial CaRe-Temporal Avg TV VT TV VT TV Align Retrieval TV VT TV TT TT TV TV TT+TI+TV TT+TI+TV 32.9 33.5 (+0.6) 41.5 39.6 (-1.9) 40.0 38.1 (-1.9) Fine TV 34.8 TV + Fine TV 39.9 31.5 30.8 (-0.7) 41.7 40.4 (-1.3) 39.2 39.0 (-0.2) 35. 39.1 42.7 42.5 (-0.2) 47.0 47.4 (+0.4) 46.0 45.6 (-0.4) 42.3 40. VT 60.1 60.1 (-) 70.3 70.3 (-) 45.5 50.1 (+4.6) 65.8 68.5 (+2.7) 69.9 69.1 (-0.8) 61.3 59.1 (-2.2) 69.6 69.6 81.2 79.2 52.4 51.5 (-0.9) 68.7 68.1 (-0.6) 61.2 60.2 (-1.0) 79.9 79.0 47.7 51.3 (+3.6) 51.3 51.4 (+0.1) 33.1 34.8 (+1.7) 68.3 67.6 (-0.7) 63.0 62.1 (-0.9) 67.3 65.9 (-1.4) 59.6 58.8 (-0.8) 42.0 41.6 (-0.4) 40.5 39.7 (-0.8) 81.0 79.4 80.5 76.7 42. 41.8 VT 37.7 37.7 (-) 47.1 45.3 (-1.8) 42.7 43.1 (+0.4) 46. 45.7 43.5 44.4 (+0.9) 56.0 55.5 (-0.5) 52.3 51.5 (-0.8) 59.4 59. ID 1 2 3 4 5 6 7 To solve these two questions, we conduct extensive experiments across diverse settings (See details in Appendix B.4). Our research endeavors focus on evaluating the effectiveness of fine-grained alignment. This is achieved by performing next token prediction fine-tuning on 500K fine-grained video-caption instances sourced from Tarsier2-Recap-585K [103] with various configurations. Initial experiments validate the findings in CaRe [96]: when text-text (TT) data is employed for retrieval adaptation, fine-grained alignment significantly boosts performance, especially in fine-grained videotext retrieval tasks (See Table 10, 12). However, when text-video (TV) pairs are involved in retrieval adaptation, fine-grained alignment surprisingly leads to performance degradation (See Table 10, 34, 56). Our results reveal several key insights: During the retrieval adaptation process, leveraging TV pairs yields far more significant performance improvements than those obtained through fine-grained alignment. The exclusive employment of fine-grained video-text pairs during the retrieval adaptation process leads to substantial enhancement in CaReBench performance. However, it causes severe degradation in the models coarse-grained retrieval capabilities (See Table 10, 7). By incorporating fine-grained TV pairs into general TV data (See Table 10, 8), we can achieves balanced performance. It enables the model to obtain competitive results in both coarseand fine-grained video-text retrieval tasks. Thus, these observations lead to crucial insight: During the retrieval adaptation, the direct incorporation of fine-grained video-caption pairs has been shown to be far more effective than the implementing isolated fine-grained alignment stage."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce UNITE, universal multimodal embedding framework that enables the seamlessly integration of text, image, and video modalities. Through systematic analysis of how training with varied data compositions affects the final retrieval performance, we observe novel insights that have not received limited exploration in image-text and video-text retrieval scenarios. Based on these insights, we propose an data composition and allocation strategy, and introduce MAMCL to mitigate inter-instance competition while maintaining balance the representation learning across text, images, and videos. Extensive experimental results demonstrate that UNITE achieves state-of-the-art results on 40+ tasks spanning coarse-grained, fine-grained, and instruction-based retrieval scenarios. We believe that our work advances the development of unified multimodal retrieval and provides valuable insights for future research."
        },
        {
            "title": "References",
            "content": "[1] Mubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo, Oana Cocarascu, Elena Simperl, and Andreas Vlachos. Multimodal automated fact-checking: survey. In Findings of the Conference on Empirical Methods in Natural Language Processing, 2023. 2 [2] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2017. 7, 19, 21, 25 [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2015. 2 [4] Mothilal Asokan, Kebin Wu, and Fatima Albreiki. Finelip: Extending clips reach via finegrained alignment with longer text inputs. arXiv preprint arXiv:2504.01916, 2025. 6 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. 5, 19, 22 [7] Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [8] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. 9 [9] Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 19 [10] David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2011. 7, 19, 20, 22, 25 [11] Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, and Zhicheng Dou. mme5: Improving multimodal multilingual embeddings via high-quality synthetic data. arXiv preprint arXiv:2502.08468, 2025. 6, 7, 24, [12] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision (ECCV), 2024. 18, 20 [13] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: vision-audio-subtitle-text omni-modality foundation model and dataset. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 3 [14] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. arXiv preprint arXiv:2210.02928, 2022. 3 [15] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 2, 5 10 [16] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 6, 7, [17] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 19 [18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 19 [19] Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, and Yong Liu. Mmdocir: Benchmarking multi-modal retrieval for long documents. arXiv preprint arXiv:2501.08828, 2025. 3 [20] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: retrospective. International Journal of Computer Vision, 111:98136, 2015. 19 [21] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. 19 [22] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021. 2, 5, 19, [23] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 3 [24] Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, and Jiankang Deng. Breaking the modality barrier: Universal embedding learning with multimodal llms. arXiv preprint arXiv:2504.17432, 2025. 2, 6, 7, 24, 26 [25] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. 21 [26] Thomas Hummel, Shyamgopal Karthik, Mariana-Iuliana Georgescu, and Zeynep Akata. Egocvr: An egocentric benchmark for fine-grained composed video retrieval. In European Conference on Computer Vision (ECCV), 2024. 3, 6 [27] Young Kyun Jang, Junmo Kang, Yong Jae Lee, and Donghyun Kim. Mate: Meet at the embedding-connecting images with long texts. In Findings of the Conference on Empirical Methods in Natural Language Processing, 2024. 6 [28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning (ICML), 2021. 1, [29] Jie Jiang, Shaobo Min, Weijie Kong, Hongfa Wang, Zhifeng Li, and Wei Liu. Tencent textvideo retrieval: hierarchical cross-modal interactions with multi-level representations. IEEE Access, 2022. 3 [30] Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. Scaling sentence embeddings with large language models. In Findings of the Conference on Empirical Methods in Natural Language Processing, 2024. 3 11 [31] Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. E5-v: Universal embeddings with multimodal large language models. arXiv preprint arXiv:2407.12580, 2024. 2, 3, 6, 24 [32] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160, 2024. 1, 2, 3, 4, 6, 7, 18, 20, 22, 24, 25, 26 [33] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2017. [34] Kushal Kafle and Christopher Kanan. Visual question answering: Datasets, algorithms, and future challenges. Computer Vision and Image Understanding, 163:320, 2017. 2 [35] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 20 [36] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 19 [37] Fanheng Kong, Jingyuan Zhang, Hongzhi Zhang, Shi Feng, Daling Wang, Yu Tian, Linhao Yu, Xingguang Ji, Victoria W, and Fuzheng Zhang. Tuna: Comprehensive fine-grained temporal understanding evaluation on dense dynamic videos. arXiv preprint arXiv:2505.20124, 2025. 3 [38] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. 5, [39] Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, and Jinsong Su. Llave: Large language and vision embedding models with hardness-weighted contrastive learning. arXiv preprint arXiv:2503.04812, 2025. 3 [40] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 3 [41] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In International Conference on Machine Learning (ICML), 2023. 2, 6, 24 [42] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning (ICML), 2022. 3 [43] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Videollava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 3 [44] Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. Mm-embed: Universal multimodal retrieval with multimodal llms. arXiv preprint arXiv:2411.02571, 2024. 3, 5, [45] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), 2014. 5, 19, 22, 25 [46] Bangwei Liu, Yicheng Bao, Shaohui Lin, Xuhong Wang, Xin Tan, Yingchun Wang, Yuan Xie, and Chaochao Lu. Idmr: Towards instance-driven precise visual correspondence in multimodal retrieval. arXiv preprint arXiv:2504.00954, 2025. 3, 6, 7, 24, 26 12 [47] Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. Visual news: Benchmark and challenges in news image captioning. arXiv preprint arXiv:2010.03743, 2020. 19 [48] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [49] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. 2, 3 [50] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 2 [51] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In European Conference on Computer Vision (ECCV), 2024. 3 [52] Yikun Liu, Pingan Chen, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiangchao Yao, Yanfeng Wang, and Weidi Xie. Lamra: Large multimodal model as your advanced retrieval assistant. arXiv preprint arXiv:2412.01720, 2024. 2, 3, 7, [53] Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. Image retrieval In Proceedings of the on real-life images with pre-trained vision-and-language models. IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 2, 3, 19 [54] Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou. Generative multi-modal knowledge retrieval with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2024. 3 [55] Man Luo, Zhiyuan Fang, Tejas Gokhale, Yezhou Yang, and Chitta Baral. End-to-end knowledge retrieval with multi-modal queries. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2023. 3 [56] Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, and Rongrong Ji. Video-rag: Visually-aligned retrieval-augmented long video comprehension. arXiv preprint arXiv:2411.13093, 2024. 3 [57] Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying multimodal retrieval via document screenshot embedding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. [58] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024. 3 [59] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2, 19 [60] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Annual Meeting of the Association for Computational Linguistics, 2022. 19 [61] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2022. 19 [62] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2021. 3, 19 [63] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et al. Mm1: methods, analysis and insights from multimodal llm pre-training. In European Conference on Computer Vision (ECCV), 2024. 13 [64] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, et al. Docci: Descriptions of connected and contrasting images. In European Conference on Computer Vision (ECCV), 2024. 18, 19, 20, 22, 25 [65] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 3, 4 [66] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmosg: Generating images in context with multimodal large language models. arXiv preprint arXiv:2310.02992, 2023. 3 [67] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2015. 7, 19, 20, 22, [68] Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon Vandenhende, Yash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan. Filtering, distillation, and hard negatives for vision-language pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 8 [69] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. 1, 3, 5, 6, 24, 25 [70] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016. 5, 19 [71] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples. arXiv preprint arXiv:2010.04592, 2020. 8 [72] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Pic2word: Mapping pictures to words for zero-shot composed image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [73] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems (NeurIPS), 2022. 5, 19 [74] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European Conference on Computer Vision (ECCV), 2022. 19 [75] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 6 [76] Omkar Thawakar, Muzammal Naseer, Rao Muhammad Anwer, Salman Khan, Michael Felsberg, Mubarak Shah, and Fahad Shahbaz Khan. Composed video retrieval via enriched context and discriminative embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 6 [77] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: large-scale dataset for fact extraction and verification. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 809819, 2018. 5, [78] Lucas Ventura, Antoine Yang, Cordelia Schmid, and Gül Varol. Covr-2: Automatic data IEEE Transactions on Pattern Analysis and construction for composed video retrieval. Machine Intelligence, 2024. 5, 6 14 [79] Lucas Ventura, Antoine Yang, Cordelia Schmid, and Gül Varol. Covr: Learning composed video retrieval from web video captions. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2024. 1, 2, 3, 6, 18, 19, 20, 22 [80] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. 5, 9 [81] Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and Liang Wang. comprehensive survey on cross-modal retrieval. arXiv preprint arXiv:1607.06215, 2016. 3 [82] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 3, 5, 21, 22 [83] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Fvqa: Fact-based visual question answering. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(10):24132427, 2017. 2 [84] Tianshi Wang, Fengling Li, Lei Zhu, Jingjing Li, Zheng Zhang, and Heng Tao Shen. Crossmodal retrieval: systematic review of methods and future directions. Proceedings of the IEEE, 2025. 3 [85] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. In International Conference on Learning Representations (ICLR), 2023. 5, 7, 19, 22, 25 [86] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision (ECCV), 2024. 2, [87] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 7, 25 [88] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 3 [89] Zehan Wang, Ziang Zhang, Hang Zhang, Luping Liu, Rongjie Huang, Xize Cheng, Hengshuang Zhao, and Zhou Zhao. Omnibind: Large-scale omni multimodal representation via binding spaces. arXiv preprint arXiv:2407.11895, 2024. 3 [90] Zhen Wang, Xu Shan, Xiangxie Zhang, and Jie Yang. N24news: new dataset for multimodal news classification. In Proceedings of the Language Resources and Evaluation Conference (LREC), 2022. 19 [91] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. In European Conference on Computer Vision (ECCV), 2024. 3, 24 [92] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2010. [93] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020. 8 [94] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 7, 19, 20, 22, 25 15 [95] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 3 [96] Yifan Xu, Xinhao Li, Yichun Yang, Rui Huang, and Limin Wang. Fine-grained video-text retrieval: new benchmark and method. arXiv preprint arXiv:2501.00513, 2024. 5, 7, 9, 18, 19, 22, 23, 25 [97] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. 5, [98] Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. End-to-end multimodal fact-checking and explanation generation: challenging dataset and models. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2023. 2 [99] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 5 [100] Hao Yu, Zhuokai Zhao, Shen Yan, Lukasz Korycki, Jianyu Wang, Baosheng He, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, and Hanchao Yu. Cafe: Unifying representation and generation with contrastive-autoregressive finetuning. arXiv preprint arXiv:2503.19900, 2025. 6, 7, 24 [101] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. 3 [102] Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 5, 19, 22 [103] Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, and Yuan Lin. Tarsier2: Advancing large vision-language models from detailed video description to comprehensive video understanding. arXiv preprint arXiv:2501.07888, 2025. 5, 9, [104] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 24 [105] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In European Conference on Computer Vision (ECCV), 2024. 5, 6, 18, 19, 20, 25 [106] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. 2 [107] Jingyuan Zhang, Hongzhi Zhang, Zhou Haonan, Chenxi Sun, Jiakang Wang, Fanheng Kong, Yahui Liu, Qi Wang, Fuzheng Zhang, et al. Data metabolism: An efficient data design schema for vision language model. arXiv preprint arXiv:2504.12316, 2025. 2 [108] Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and MingWei Chang. Magiclens: Self-supervised image retrieval with open-ended instructions. arXiv preprint arXiv:2403.19651, 2024. 2, 3, 6, 7, 24 [109] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Gme: Improving universal multimodal retrieval by multimodal llms. arXiv preprint arXiv:2412.16855, 2024. 2, 3, [110] Yiqun Zhang, Fanheng Kong, Peidong Wang, Shuang Sun, SWangLing SWangLing, Shi Feng, Daling Wang, Yifei Zhang, and Kaisong Song. Stickerconv: Generating multimodal empathetic responses from scratch. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2024. 3 16 [111] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024. 5 [112] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 9 [113] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. Retrieving multimodal information for augmented generation: survey. In Findings of the Conference on Empirical Methods in Natural Language Processing, 2023. [114] Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, and Yongping Xiong. Megapairs: Massive data synthesis for universal multimodal retrieval. arXiv preprint arXiv:2412.14475, 2024. 6, 7, 24, 26 [115] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852, 2023. 5, 6, 7, 25 [116] Cunjuan Zhu, Qi Jia, Wei Chen, Yanming Guo, and Yu Liu. Deep learning for video-text retrieval: review. International Journal of Multimedia Information Retrieval, 12(1):3, 2023. 3 [117] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 [118] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 2 [119] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [120] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in large multimodal models. arXiv preprint arXiv:2412.10360, 2024. 2, 3 17 Figure 3: We develop universal multimodal embedder UNITE, allowing for unified representation of arbitrary multimodal contents. Figure 4: Performance comparison on fine-grained video-text benchmark (CaReBench [96]) and image-text benchmarks (ShareGPT4V [12], Urban1K [105], DOCCI [64]). Our UNITE achieves the overall optimal performance."
        },
        {
            "title": "A Data",
            "content": "A.1 Training Data Composition Our thoroughly analysis across coarse-grained, fine-grained, and instruction-based retrieval tasks reveals that text-video pairs are particularly effective for cross-modal retrieval, while text-text and text-image pairs are essential for instruction-based retrieval (Section 5). Based on these findings, we carefully curate the retrieval adaptation data to achieve balanced performance across different modalities. The detailed data composition is illustrated in Figure 5. For instruction tuning, surpassing existing works that solely rely on supervised learning on MMEB [32], we combine 20 instruction-based image-text retrieval datasets from MMEB with subset of WebVid-CoVR [79] to optimize performance across text, image, and video modalities. The detailed data composition is illustrated in Figure 6. 18 Text-Text Pairs (21.6%) NLI [22] (275.6 K) MSMARCO [6] (532.8 K) NQ [38] (100.2 K) HotpotQA [97] (170.0 K) Fever [77](140.1 K) TriviaQA (73.3 K) SQuAD [70] (87.6 K) Image-Text Pairs (39.2%) CapsFusion [102] (1.0 M) LAION-Art [73] (1.0 M) MSCOCO [45] (500 K) Video-Text Pairs (36.1%) InternVid-FLT [85] (2.0 M) Tarsier2-Recap (500.0 K) Figure 5: Retrieval Adaptation 6.4M. Left: Data Distribution within Each Category. The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets. Right: The detailed quantities of datasets. ImageImage (1.3%) NIGHTS [21] (15.9 K) TextImage (25.5%) MSCOCO [45] (100.0 K) VisDial [17] (123.3 K) VisualNews [47] (99.9 K) ImageText (27.6%) HatefulMemes [36] (8.5 K) ImageNet-1K [18] (100.0 K) MSCOCO [45] (113.3 K) SUN397 [92] (19.9 K) VisualNews [47] (100.0 K) VOC2007 [20] (7.9 K) TextImage+Text (1.4%) WebQA [9] (17.2 K) Image+TextText (14.8%) A-OKVQA [74] (17.1 K) ChartQA [60] (28.3 K) DocVQA [62] (39.5 K) InfographicsVQA [61] (23.5 K) OK-VQA [59] (9.0 K) Visual7W [119] (69.8 K) Image+TextImage (13.8%) CIRR [53] (26.1 K) MSCOCO [45] (100.0 K) N24News [90] (49.0 K) Video+TextVideo (15.8%) WebVid-CoVR [79] (200.0 K) Figure 6: Instruction Tuning 1.3M. Left: Data Distribution within Each Category. The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets. Right: The detailed quantities of datasets. A.2 Evaluation Datasets We provide brief descriptions of all evaluation benchmarks, and their statistics are shown in Table 11. Table 11: Evaluation benchmark statistics. MMEB is comprehensive benchmark and its statistics are shown in Table 12. #Text/Instruction and #Image/Video denote the test text/instruction count and image/video pool size, respectively. Benchmark QueryTarget Zero-shot #Text/Instrcution #Image/Video Coarse-grained Retrieval Flickr30K [67] MSR-VTT [94] MSVD [10] DiDeMo [2] Fine-grained Retrieval ShareGPT4V [67] Urban1K [105] DOCCI [64] CaRe [96] TI, IT TV, VT TV, VT TV, VT TI, IT TI, IT TI, IT TV, VT Instruction-based Retrieval MMEB [67] WebVid-CoVR [105] 36 tasks T+VV 1,000 1,000 670 1,004 1,000 1,000 5,000 1,000 - 2,556 5,000 1,000 27,763 1,004 1,000 1,000 5,000 1,000 - 2, A.2.1 Fine-grained Retrieval Datasets CaReBench [96] consists of 1,000 video-caption pairs with hierarchical annotations covering overall summary, static objects, dynamic actions, and miscellaneous aspects. Its distinctive feature lies in the 19 manually annotated spatial and temporal information. We evaluate fine-grained video-text retrieval through three tasks: General, Spatial, and Temporal. ShareGPT4V [12] features comprehensive image-text pairs with rich descriptions of world knowledge, object properties, spatial relationships, and aesthetic elements. The dataset comprises 100K GPT4-Vision generated captions and 1.2M model-expanded captions, averaging 942 characters in length. Following Long-CLIP [105], we utilize 1K instances for testing. Urban1K [105] is derived from Urban-200 and expanded to 1,000 urban scene image-text pairs. Each image is paired with GPT-4V generated caption (averaging 101 words) detailing object types, colors, and spatial relationships. The dataset challenges models with visually similar urban scenes, requiring fine-grained cross-modal understanding. DOCCI [64] (Descriptions of Connected and Contrasting Images) contains 15,000 images with human-annotated descriptions (averaging 136 words). Curated by single researcher, it evaluates spatial relations, counting, text rendering, and world knowledge comprehension. The dataset features contrast sets with subtle variations in object arrangements. We evaluate using the official 5K test split. A.2.2 Instruction-based Retrieval Datasets MMEB [32] is comprehensive multimodal embedding benchmark comprising 36 datasets across four meta-tasks: classification, visual question answering, retrieval, and visual grounding. The benchmark is strategically divided into 20 in-distribution training datasets and 16 out-of-distribution evaluation datasets. All tasks are formulated as ranking problems with 1,000 candidates, where models process instruction-guided queries (text, images, or both) to select correct targets. The meta-tasks are structured as follows: Classification: Queries combine instructions with images (and optional text) to predict class labels. Visual Question Answering: Queries include instructions, images, and questions, with answers as targets. Information Retrieval: Both queries and targets can be multimodal combinations. Visual Grounding: Queries pair instructions with full images to locate specific objects, using cropped regions as candidates. MMEB spans diverse domains (common, news, Wikipedia, web, fashion) and supports various instruction types from object recognition to retrieval tasks. Performance is evaluated using Precision@1, measuring the accuracy of selecting the correct candidate from 1,000 options. The benchmarks comprehensive design makes it an ideal testbed for universal multimodal embeddings. WebVid-CoVR-Test [79] is manually verified benchmark for Composed Video Retrieval (CoVR), containing 2,435 high-quality video-text-video triplets. Each triplet consists of query video, modification text describing desired changes, and target video. The benchmark is derived from WebVid10M, specifically excluding WebVid2M content to ensure evaluation integrity. The dataset features diverse modification texts (averaging 4.8 words) and videos (averaging 16.8 seconds), covering wide range of content variations. A.2.3 Coarse-grained Retrieval Datasets Flickr30K [67] consists of 31K images collected from Flickr, with each image paired with five human-annotated captions describing its content. The dataset covers diverse everyday scenarios and human activities in natural settings. Following common practice [35], we use the standard split with 1,000 images for testing. MSRVTT [94] comprises 10K video clips paired with 200K captions, covering diverse topics including human activities, sports, and natural landscapes. For text-to-video retrieval evaluation, we adopt the standard 1K-A split following prior works. MSVD [10] features 1,970 videos, with approximately 40 captions annotated per video. 20 Table 12: The statistics of MMEB: 36 datasets across 4 meta-task categories, with 20 in-distribution datasets used for training and 16 out-of-distribution datasets used exclusively for evaluation. Meta-Task Dataset QueryTarget Distribution Type #Training #Eval #Candidates Classification (10 Tasks) VQA (10 Tasks) Retrieval (12 Tasks) ImageNet-1K N24News HatefulMemes VOC2007 SUN Place365 ImageNet-A ImageNet-R ObjectNet Country-211 OK-VQA A-OKVQA DocVQA InfographicVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA VisDial CIRR VisualNews_t2i VisualNews_i2t MSCOCO_t2i MSCOCO_i2t NIGHTS WebQA OVEN FashionIQ EDIS Wiki-SS-NQ MSCOCO Visual Grounding (4 Tasks) Visual7W-Pointing RefCOCO RefCOCO-Matching IT + TI IT IT IT IT IT IT IT IT + TT + TT + TT + TT + TT + TT + TT + TT + TT + TT TI + TI TI IT TI IT II TI + + TI + + TI TI + I + TI + TI + TI + TI + IND IND IND IND IND OOD OOD OOD OOD OOD IND IND IND IND IND IND OOD OOD OOD OOD IND IND IND IND IND IND IND IND OOD OOD OOD OOD IND OOD OOD OOD 100K 49K 8K 8K 20K - - - - - 9K 17K 40K 24K 28K 70K - - - - 123K 26K 100K 100K 100K 113K 16K 17K - - - - 100K - - - 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 24 2 20 397 365 1000 200 313 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 DiDeMo [2] features 10K long-form Flickr videos, where each video is annotated with four temporally ordered sentences. Following previous works, we concatenate these sentences for paragraph-to-video retrieval evaluation using the official split."
        },
        {
            "title": "B More Implementation Details",
            "content": "We simply incorporate LoRA [25] module into LMM, with rank of 8, and gradually unlock its retrieval capabilities. We employ temperature of 0.03 for contrastive learning, with learning rates of 1e-4 and 2e-5 for retrieval adaptation and instruction tuning phases, respectively. Following Qwen2-VL [82], we process images with dynamic resolution, constraining the number of image tokens to (256, 1,280). For video processing, we sample frames at 1fps with 12-frame cap, limiting each frames maximum number of tokens to 98. Our experiments are conducted on 64 NVIDIA A100 GPUs for training and 8 NVIDIA H100 GPUs for evaluation. The following experiments adopt these default settings unless specified otherwise. B.1 Stage1: Retrieval Adaptation During retrieval adaptation, we follow the standard InfoNCE loss (i.e., Eq. (1)) and employ bidirectional contrastive learning strategy to maximize the utilization of training data. Taking image21 text pairs as an example, we simultaneously optimize both text-to-image and image-to-text retrieval directions. The final loss function is computed as the average of these bidirectional retrieval processes: Lbi = (cid:34)"
        },
        {
            "title": "1\nN",
            "content": "1 2 (cid:88) n=1 log exp (cos(qn, c+ j=1 exp (cos (qn, cj) /τ ) )/τ ) (cid:80)N"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) n=1 log exp (cos(c+ j=1 exp (cid:0)cos (cid:0)c+ , qn)/τ ) , qj (cid:80)N (cid:1) /τ (cid:1) (cid:35) (6) The training data distribution and hyperparameters are shown in Figure 5 and Table 13, respectively. B.2 Stage2: Instruction Tuning During instruction tuning, we observe that jointly training on instruction-based retrieval data from different modalities (image-text and video-text) may lead to performance fluctuations, with potential degradation in certain domains. To address this challenge and maintain consistent performance across modalities, we propose MAMCL (Modality-Aware Masked Contrastive Learning) to balance the learning dynamics between different modal data. This approach helps optimize the overall multimodal retrieval capabilities while preserving domain-specific performance. The detailed data distribution and training parameters are presented in Figure 6 and Table 14. Table 13: Training hyperparameters and computational requirements for UNITEbase. Table 14: Training hyperparameters and computational requirements for UNITEinstruct. Hyperparameter UNITE 2B UNITE 7B Hyperparameter UNITE 2B UNITE 7B Stage1: Retrieval Adaptation for UNITEbase Stage2: Instruction Tuning for UNITEinstruct Training Samples Batch Size Learning rate Optimizer Learning Rate Decay Warmup Ratio LoRA Rank LoRA Alpha Temperature τ Epochs GPU Configuration Training Time 6.4M 4,096 1,024 1104 AdamW cosine 0.03 8 16 0.03 1 64A100 7 hours 21 hours Training Samples Batch Size Learning rate Optimizer Learning Rate Decay Warmup Ratio LoRA Rank LoRA Alpha Temperature τ Epochs GPU Configuration Training Time 1.3M 4,096 1,024 2105 AdamW cosine 0.03 8 64 0.03 1 64A100 2 hours 6 hours B.3 Analysis: Training Data Composition We conduct comprehensive experiments utilizing Text-Text (TT), Text-Image (TI), and Text-Video (TV) datasets, both independently and in various combined scenarios. Specifically, we collect TT data from MSMARCO [6] and NLI [22], TI data from CapsFusion [102], and TV data from InternVid [85]. To ensure fair comparisons, we maintain consistent total dataset size of 600K instances across all configurations, with mixed-pattern settings employing equal distribution of different constituent types (e.g., 200K per dataset in TT+TI+TV). The experimental pipeline consists of two phases. In the first phase, we finetune Qwen2-VL-2B [82] using 600K instances across seven distinct configurations, following the hyperparameters specified in Table 13. The second phase involves instruction tuning, where we independently finetune the retrieval-adapted model using the complete MMEB [32] training set and 500K instances from WebVid-CoVR [79], respectively, adhering to the hyperparameters outlined in Table 14. To thoroughly investigate the impact of various data compositions on retrieval performance, we evaluate across three distinct retrieval scenarios, including (1) coarse-grained image-text datasets (Flickr30K [67], MSCOCO [45]), video-text datasets (MSR-VTT [94], MSVD [10]); (2) fine-grained image-text dataset (DOCCI [64]), video-text dataset (CaReBench [96]); and (3) instruction-based datasets (MMEB [32], WebVid-CoVR [79]). The raw results are shown in Table 18 and Table 19. B.4 Analysis: Effectively Utilizing Fine-Grained Video-Caption Data To investigate efficient strategies for fine-grained video-caption training, we conduct preliminary experiments using Tarsier2-Recap-585K [103]. Extending the exploration scope of CaRes [96] work, 22 we examine the effectiveness of fine-grained alignment across broader data compositions (TT, TV, and TT+TI+TV). This expanded investigation aims to understand the generalizability of fine-grained alignment (i.e., fine-tuning LMM through next token prediction using fine-grained video-caption pairs) in diverse multimodal scenarios. The experimental configurations in Table 10 (IDs 1-6) utilize 500K instances from Tarsier2-Recap for fine-grained alignment, maintaining consistent retrieval adaptation data as described in Section B.3. Configuration ID 7 omits the fine-grained alignment, instead directing all 500K fine-grained videocaption pairs to retrieval adaptation. Similarly, ID 8 bypasses fine-grained alignment and employs balanced training set with 600K instances, comprising 300K instances each from InternVid and Tarsier2-Recap. For fine-grained alignment, we leverage ms-swift1 to train LoRA modules of Qwen2-VL-2B, configured with rank of 16 and an alpha of 32. We set the batch size to 128 and the learning rate to 1e-4. For video items, we sample frames at 1fps with 16-frame cap, limiting each frames maximum number of tokens to 192. Notably, unified contrastive learning demonstrates superiority not only in performance but also in computational efficiency. Our empirical analysis reveals significant training speed advantages: processing 500K instances from Tarsier2-Recap requires 4.2 hours for fine-grained alignment and merely 0.9 hours for retrieval adaptation, highlighting the methods computational efficiency while maintaining superior performance."
        },
        {
            "title": "C More Experimental Results",
            "content": "C.1 Detailed Main Results Detailed experimental results on CaReBench, MMEB, and coarse-grained video-text retrieval benchmarks are provided in Tables 15, 16, and 17, in correspondence with Tables 1, 4, and 6. Table 20 provides the results of MMEB specifically for the 36 tasks. Figure 4 shows the visualization of the fine-grained retrieval results. Table 15: Detailed results of zero-shot performance on CaReBench [96]. Model TV VT TV VT TV VT CaRe-General CaRe-Spatial CaRe-Temporal R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 CLIP-based Models 51.2 83.4 CLIP L/14 64.3 91.0 LanguageBind Long-CLIP L/14 62.7 88.8 InternVideo2stage2 1B 72.5 93.7 LMM-based Models LLaVA-NV 7B MiniCPM-V 2.6 InternVL2 8B Tarsier 7B Qwen2-VL 7B CaRe 7B 66.9 89.4 71.0 92.2 72.1 92.6 71.0 93.8 76.6 95.3 77.0 95.6 UNITEbase 2B UNITEbase 7B 78.1 95.5 86.0 96.8 90.6 96.3 95.7 97.3 96.0 97.0 96.8 97.8 98.7 98.7 97.8 98.9 54.7 86.9 59.5 88.0 60.3 88.8 69.5 94.6 62.7 89.2 69.3 92.8 73.6 93.4 70.6 94.2 77.4 95.6 79.0 96. 80.8 96.4 86.9 98.3 93.6 95.0 94.9 97.8 95.4 97.1 97.4 98.0 98.7 99.1 98.7 99.7 49.0 81.9 64.7 90.8 65.6 90.9 72.4 94.2 68.0 92.0 71.7 93.6 76.1 94.1 70.2 94.0 78.2 95.5 76.8 96. 79.6 95.4 86.5 96.9 91.4 96.8 96.0 97.4 96.2 98.0 97.6 98.2 98.5 98.7 98.3 99.2 55.4 85.6 61.0 87.2 61.0 88.3 62.7 90.5 65.0 90.0 67.6 92.3 74.3 94.5 67.4 93.5 75.4 95.0 78.1 95. 78.0 95.4 84.8 98.0 93.0 94.5 94.4 95.9 95.9 97.7 97.6 97.4 98.1 99.3 97.9 99.4 33.5 70.3 39.8 77.3 33.2 68.8 46.0 80.8 43.3 76.9 50.5 82.9 48.1 76.8 50.1 84.1 51.9 84.8 50.7 85. 45.3 77.6 52.4 82.5 84.0 90.5 81.6 91.9 88.9 92.1 89.0 92.8 94.9 94.4 89.9 92.2 39.7 76.2 42.2 77.6 34.5 71.9 46.6 82.5 40.1 75.4 46.1 80.9 47.6 78.2 50.0 84.7 52.7 85.4 53.4 86. 50.0 83.6 55.4 86.5 87.9 91.7 86.6 92.5 88.7 93.3 90.3 94.9 95.2 94.0 92.6 95.3 C.2 Detailed Results for Analysis Tables 18, 19 show the raw results of Table 9."
        },
        {
            "title": "Limitations",
            "content": "While UNITE demonstrates superior performance across text, image, and video modalities, audio emerges as another potential modality with the evolution of social media. Our exploration reveals that 1https://github.com/modelscope/ms-swift 23 Table 16: Detailed results on the MMEB benchmark [32]. We average the scores in each meta-task. Model #Datasets Per Meta-Task Score Average Score Classification VQA Retrieval Grounding IND OOD Overall 10 12 4 20 16 36 Zero-shot Setting CLIP [69] BLIP2 [41] SigLIP [104] OpenCLIP [16] Magiclens [108] E5-V 8B (LLaVA-NeXT) [31] MMRet-MLLM 7B (LLaVA-1.6) [114] mmE5 11B (Llama-3.2-Vision) [11] 42.8 27.0 40.3 47.8 38.8 21.8 47.2 60.6 9.1 4.2 8.4 10.9 8.3 4.9 18.4 55.7 53.0 33.9 31.6 52.3 35.4 11.5 56.5 54.7 51.8 47.0 59.5 53.3 26.0 19.0 62.2 72.4 Partially Supervised Finetuning Setting (Finetuning on M-BEIR) UniIR (BLIP_FF) [91] UniIR (CLIP_SF) [91] MM-Embed 7B (LLaVA-1.6) [44] GME 2B (Qwen2-VL) [109] 42.1 70.5 48.1 56.9 15.0 16.2 32.3 41.2 60.1 61.8 63.8 67.8 62.2 65.3 57.8 53.4 Supervised Finetuning Setting (Finetuning on MMEB) CLIP [69] OpenCLIP [16] E5-V 4.2B (Phi3.5-V) [31] E5-V 7B (LLaVA-1.6) [31] VLM2Vec 4.2B (Phi-3.5-V) [32] VLM2Vec 7B (LLaVA-1.6) [32] VLM2Vec 2B (Qwen2-VL) [32] VLM2Vec 7B (Qwen2-VL) [32] MMRet-MLLM 7B (LLaVA-1.6) [114] UniME 4.2B (Phi3.5-V) [24] UniME 7B (LLaVA-1.6) [24] CAFe 0.5B (LLaVA-OneVision) [100] CAFe 7B (LLaVA-OneVision) [100] mmE5 11B (Llama-3.2-Vision) [11] IDMR 8B (InternVL2.5) [46] IDMR 26B (InternVL2.5) [46] UNITEinstruct 2B (Qwen2-VL) UNITEinstruct 7B (Qwen2-VL) 55.2 56.0 39.1 39.7 54.8 61.2 59.0 62.6 56.0 54.8 60.6 59.1 65.2 67.6 58.3 66.3 63.2 68.3 19.7 21.9 9.6 10.8 54.9 49.9 49.4 57.8 57.4 55.9 52.9 49.1 65.6 62.8 58.6 61.9 55.9 65. 53.2 55.4 38.0 39.4 62.3 67.4 65.4 69.9 69.9 64.5 67.9 61.0 70.0 70.9 68.7 71.1 65.4 71.6 62.2 64.1 57.6 60.2 79.5 86.1 73.4 81.7 83.6 81.8 85.1 83.0 91.2 89.7 85.6 88.6 75.6 84.8 37.1 25.3 32.3 39.3 31.0 14.9 43.5 57.2 44.7 47.1 - - 47.6 50.5 33.1 34.2 66.5 67.5 66.0 72.2 68.0 68.2 68.4 64.3 75.8 72.3 70.5 73.4 65.8 73.6 38.7 25.1 38.0 40.2 23.7 11.5 44.3 62.9 40.4 41.7 - - 42.8 43.1 31.9 33.4 52.0 57.1 52.6 57.8 59.1 52.7 57.9 53.7 62.4 66.7 57.9 63.9 60.1 66. 37.8 25.2 34.8 39.7 27.8 13.3 44.0 58.6 42.8 44.7 50.0 55.8 45.4 47.2 32.6 33.9 60.1 62.9 60.1 65.8 64.1 64.2 66.6 59.6 69.8 69.8 64.9 69.2 63.3 70.3 balancing multiple modalities remains challenging, suggesting the need for further investigation into modality expansion. Additionally, while comprehensive benchmarks exist for image-text retrieval, developing unified benchmark that encompasses text, image, video, and potentially audio modalities represents valuable future direction. Table 17: Detailed results of zero-shot performance on MSR-VTT [94], MSVD [10], and DiDeMo [2]. Model TV VT TV VT TV VT R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 MSR-VTT MSVD DiDeMo CLIP-based Models CLIP L/14 [69] 36.7 58.8 68.0 32.8 54.7 66.2 Long-CLIP L/14 [105] 40.9 65.5 74.6 36.2 62.2 71.5 InternVideo [87] LanguageBind [115] ViCLIP [85] 40.7 42.1 65.9 75.5 40.1 65.4 73.9 42.4 39.6 41.3 - - - - - - - - 41.1 68.8 77.5 68.1 85.5 91.8 46.5 73.5 82.0 69.3 86.0 90.3 43.4 50.0 77.7 85.6 75.1 90.0 94.2 49.1 67.6 75.1 - - - - - - - - 24.1 48.0 58.2 23.8 44.9 54.0 32.4 56.2 65.2 28.5 54.1 64.7 31.5 35.6 63.6 71.7 35.6 62.8 71.8 18.4 33.5 27.9 - - - - - - - - LMM-based Models VLM2Vec [32] LamRA 7B [52] CaRe 7B [96] UNITEbase 2B UNITEbase 7B 46.8 71.1 80.0 44.7 68.6 78.6 43.9 67.0 75.7 41.7 68.1 76.2 - - - - - - 52.9 80.1 87.0 52.4 79.8 87.0 52.6 79.2 86.6 74.6 87.9 92.4 - - - - - - - - - - - - - - - - - - 41.4 68.5 77.1 39.1 66.0 75.8 43.8 67.6 76.8 41.7 66.2 75.9 46.5 69.4 78.0 45.2 70.3 79. 50.0 77.8 85.1 73.1 90.7 94.3 50.4 78.2 86.4 76.1 91.3 94.6 37.9 64.1 73.7 26.4 37.5 63.5 43.5 69.6 77.3 40.3 68.7 78.1 Table 18: Impact study of training data composition on general retrieval in the retrieval adaptation stage. We report zero-shot cross-modal retrieval results on coarse-grained cross-modal datasets (Flickr30K [67], MSCOCO [45], MSR-VTT [94], MSVD [10]) and fine-grained cross-modal datasets (DOCCI [64], CaReBench-General[96]) with Recall@1. Setting Coarse Image-Text Coarse Video-Text Fine Image-Text Fine Video-Text Flickr30K COCO MSR-VTT MSVD DOCCI CaRe-General Avg TT TI TV TI IT TI IT TV VT TV VT TI IT TV VT 78.2 79.5 86. 78.9 82.0 84.2 82.0 40.8 42.6 47.0 43.5 42.8 45.0 44.4 49.9 58.2 60. 56.3 48.2 59.3 53.6 32.9 37.2 41.5 38.5 40.4 41.3 40.0 31.5 35.6 41. 35.2 39.5 41.4 39.2 42.7 43.7 47.0 44.6 46.4 46.0 46.0 60.1 66.4 70. 66.3 69.7 70.3 69.9 69.1 75.8 79.8 74.8 76.3 77.8 76.5 65.4 71.8 74. 70.3 70.1 73.0 70.9 45.5 57.9 65.8 56.8 62.3 65.7 61.3 52.4 62.9 68. 58.8 64.1 68.4 61.2 52.9 58.3 63.1 57.6 59.4 62.0 59.7 66.1 68.1 73. 67.7 70.9 71.6 71.7 Table 19: Results of diverse training data composition on instruction-based retrieval in the retrieval adaptation stage. Setting MMEB WebVid-CoVR TT TI TV Classification VQA Retrieval Grounding IND OOD Overall R@1 R@5 R@10 61.9 62.6 62.6 63.8 62.1 63.0 62.7 58.5 58.3 59. 59.9 57.7 59.2 59.0 60.4 60.7 61.1 62.1 60.2 61.3 61.0 64.4 66.5 65. 65.4 65.6 65.8 64.8 86.3 87.2 87.8 87.0 87.4 87.4 86.8 92.1 92.3 92. 92.1 92.6 92.6 92.7 60.9 60.5 60. 53.8 55.2 55.0 61.1 61.2 61.9 73.7 73.3 74.1 61.9 59.7 61.5 61.2 55.4 54.4 55. 54.7 62.8 60.7 61.6 61.8 77.1 74.2 74.3 74.6 Table 20: The detailed results of the baselines and our UNITE on MMEB, which includes 20 indistribution (IND) datasets and 16 out-of-distribution (OOD) datasets. The out-of-distribution datasets are highlighted with yellow background in the table. For each model, we report its best variant with fully available performance metrics, including VLM2Vec 7B (LLaVA-1.6) [32], MMRet 7B (LLaVA-1.6) [114], UniME 7B (LLaVA-1.6) [24], mmE5 11B (Llama-3.2-Vision) [11], and IDMR 26B (InternVL2.5) [46]. Superior versions might exist but are excluded due to incomplete score reporting. CLIP VLM2Vec MMRet UniME mmE5 IDMR UNITE 2B UNITE 7B Classification (10 tasks) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country-211 All Classification VQA (10 tasks) OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA All VQA Retrieval (12 tasks) VisDial CIRR VisualNews_t2i VisualNews_i2t MSCOCO_t2i MSCOCO_i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS All Retrieval Visual Grounding (4 tasks) MSCOCO RefCOCO RefCOCO-matching Visual7W-pointing All Visual Grounding Final Score (36 tasks) All All IND All OOD 55.8 34.7 51.1 50.7 43.4 28.5 25.5 75.6 43.4 19.2 42.8 7.5 3.8 4.0 4.6 1.4 4.0 9.4 8.2 41.3 7.0 9.1 30.7 12.6 78.9 79.6 59.5 57.7 60.4 67.5 11.4 55.0 41.1 81.0 53.0 33.8 56.9 61.3 55.1 51. 37.8 37.1 38.7 74.5 80.3 67.9 91.5 75.8 44.0 43.6 79.8 39.6 14.7 61.2 69.0 54.4 52.0 30.7 34.8 49.8 42.1 43.0 61.2 62.0 49.9 80.9 49.9 75.4 80.0 75.7 73.1 65.5 87.6 16.2 60.2 56.5 87.8 67.4 80.6 88.7 84.0 90.9 86.1 62.9 67.5 57. 71.3 79.5 64.6 90.4 75.9 45.6 45.5 78.4 36.4 18.7 60.6 68.3 58.7 67.6 37.0 33.4 51.7 40.5 42.7 63.6 65.2 52.9 79.7 52.2 74.8 78.8 74.9 73.8 66.2 89.8 16.5 66.6 55.7 86.2 67.9 76.5 89.3 90.6 84.1 85.1 66.6 68.4 57.9 77.8 81.7 64.2 91.0 77.7 43 56.3 86.3 62.5 35.4 67. 67.6 56.1 90.3 56.5 50.5 51.9 55.8 52.8 61.7 83.3 62.6 74.1 54.7 77.6 83.3 76.4 73.2 68.3 88.0 28.8 65.8 77.5 83.7 71.0 53.7 92.7 88.8 92.3 89.6 69.8 72.3 66.7 80.6 81.6 72.3 92.7 78.8 38.9 63.6 84 50.5 20.3 66.3 71.0 59.2 75.1 44.6 64.6 54.9 54.7 47.1 71.0 77.0 61. 81.5 57.6 78.5 80.6 79.1 75.4 68.6 89.0 21.0 66.9 67.4 87.6 71.1 81.5 91.7 88.1 93.1 88.6 69.2 73.4 63.4 77.6 66.8 57.4 85.0 75.2 41.3 48.3 88.8 66.6 24.8 63.2 57.3 46.0 87.0 52.8 45.7 47.7 41.1 49.8 53.0 78.9 55.9 70.0 43.5 70.6 74.1 73.6 71.1 65.1 85.0 18.1 65.0 65.9 82.2 65. 64.6 79.5 84.5 73.9 75.6 63.3 65.8 60.1 80.2 80.3 67.1 84.9 78.7 44.5 59.2 90.5 68.1 29.5 68.3 67.1 58.0 92.7 71.3 63.2 54.9 51.2 53.4 56.8 82.3 65.1 80.5 51.6 79.3 82.4 78.2 74.3 66.0 87.0 26.3 72.2 73.1 88.3 71.6 73.9 89.2 90.1 86.1 84. 70.3 73.6 66.3 58.8 71.3 53.7 85.0 70.0 43.0 36.1 71.6 55.8 14.7 56.0 73.3 56.7 78.5 39.3 41.7 49.5 45.2 51.7 59.0 79.0 57.4 83.0 61.4 74.2 78.1 78.6 72.4 68.3 90.2 54.9 24.9 87.5 65.6 69.9 76.8 89.8 90.6 77.0 83.6 64.1 59.1 68."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Northeastern University"
    ]
}