{
    "paper_title": "Risk-Averse Reinforcement Learning with Itakura-Saito Loss",
    "authors": [
        "Igor Udovichenko",
        "Olivier Croissant",
        "Anita Toleutaeva",
        "Evgeny Burnaev",
        "Alexander Korotin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Risk-averse reinforcement learning finds application in various high-stakes fields. Unlike classical reinforcement learning, which aims to maximize expected returns, risk-averse agents choose policies that minimize risk, occasionally sacrificing expected value. These preferences can be framed through utility theory. We focus on the specific case of the exponential utility function, where we can derive the Bellman equations and employ various reinforcement learning algorithms with few modifications. However, these methods suffer from numerical instability due to the need for exponent computation throughout the process. To address this, we introduce a numerically stable and mathematically sound loss function based on the Itakura-Saito divergence for learning state-value and action-value functions. We evaluate our proposed loss function against established alternatives, both theoretically and empirically. In the experimental section, we explore multiple financial scenarios, some with known analytical solutions, and show that our loss function outperforms the alternatives."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 5 2 9 6 1 . 5 0 5 2 : r Risk-Averse Reinforcement Learning with Itakura-Saito Loss Igor Udovichenko Skolkovo Institute of Science and Technology Vega Institue Foundation Moscow, Russia i.udovichenko@skoltech.ru Olivier Croissant Natixis Foundation Paris, France Anita Toleutaeva Skolkovo Institute of Science and Technology Moscow, Russia Evgeny Burnaev Skolkovo Institute of Science and Technology Artificial Intelligence Research Institute Moscow, Russia Alexander Korotin Skolkovo Institute of Science and Technology Artificial Intelligence Research Institute Moscow, Russia a.korotin@skoltech.ru"
        },
        {
            "title": "Abstract",
            "content": "Risk-averse reinforcement learning finds application in various high-stakes fields. Unlike classical reinforcement learning, which aims to maximize expected returns, risk-averse agents choose policies that minimize risk, occasionally sacrificing expected value. These preferences can be framed through utility theory. We focus on the specific case of the exponential utility function, where we can derive the Bellman equations and employ various reinforcement learning algorithms with few modifications. To address this, we introduce numerically stable and mathematically sound loss function based on the Itakura-Saito divergence for learning state-value and action-value functions. We evaluate our proposed loss function against established alternatives, both theoretically and empirically. In the experimental section, we explore multiple scenarios, some with known analytical solutions, and show that our loss function outperforms the alternatives."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has achieved remarkable success in domains where the primary goal is to learn policies by interacting with an environment [34, 42]. The goal is often formalized through Markov decision process (MDP), which aims to find policy that maximizes the expected cumulative reward received during the interaction with the environment [42]. However, agents must prioritize risk mitigation alongside performance in high-stakes applications such as finance, healthcare, and autonomous systems [4, 44, 33]. Traditional risk-neutral RL frameworks, which optimize for expected returns, often fail to account for the variability and tail risks inherent in these settings. Risk-averse RL addresses this by incorporating preferences that penalize uncertainty, typically formalized through utility theory or coherent risk measures [23]. Preprint. Under review. Among the various utility-based methods, the exponential (or entropic) utility function stands out for its convenient properties [40, 28]. Yet, existing exponential-utility RL approaches typically require exponentiation of the value function at each step and can suffer from significant numerical instabilities [17]. These instabilities often prevent reliable convergence. This paper introduces novel approach to risk-averse RL by leveraging the Itakura-Saito (IS) divergence [31], specific case of Bregman divergence [8, 3] historically used in signal processing [13] and non-negative matrix factorization [22]. Our IS-based loss function is numerically stable, invariant to absolute value scales, and preserves the theoretical guarantees of the exponential utility framework. Contributions To tackle this problem, we propose to replace the exponentiated Bellman target with loss derived from the Itakura-Saito (IS) divergence [31], classical but underused member of the Bregman divergence family [8, 3]. Our main contributions are: 1. Novel Loss for Exponential Utility: We introduce new simple loss function (IS) based on the Itakura-Saito divergence to learn the value function in risk-averse MDPs. 2. Theoretical Guarantees: We show that this IS loss recovers the exponential utilitys Bellman equation under mild conditions, ensuring that the resulting value estimate is correct and that the method is scale-invariant. 3. Empirical Validation: Across range of benchmarksfrom analytically tractable portfolio examples 4.1 to deep hedging task 4.1 and robust combinatorial RL problem 4.2the proposed IS approach outperforms existing baselines."
        },
        {
            "title": "2 Background and Related Works",
            "content": "In this section, we briefly review the essentials of reinforcement learning (RL), emphasizing how risk aversion arises in decision-making processes, and why exponential utility proves useful for risk-sensitive control. 2.1 Markov Decision Process (MDP) Consider MDP of the form (S, A, r, p, s0), where and are sets of states and actions, respectively. Here r(s, a, s) is the reward function, dependent on the current state, action, and next state. Statetransition probability (or density) is denoted as p(s s, a). The initial state at = 0 is s0. We assume the finite time horizon, so the time index = 0, . . . , , where < . The discount factor γ = 1 for simplicity. Extending our ideas on the case with γ < 1 and infinite time horizon is straightforward [28]. The timestamp is assumed to be part of the state to avoid notation overload. By Π we denote the set of Markov policies π(a s). We restrict our considerations to the class of Markov policies, because the optimal policy lies in it [28]. We define the trajectory π as random sequence of states and actions according to policy π: π def= (cid:0)s0, a0, s1, a1, . . . , sT (cid:1) , trajectory part started at state is denoted as π also fixed rather than sampled from π. We define the random return of policy π as follows: s,a if the action at is at π( st), . Furthermore, we write π = 0, . . . , 1. (1) Rπ(cid:14)Rπ(s)(cid:14)Rπ(s, a) = 1 (cid:88) τ =t r(sτ , aτ , sτ +1), (cid:0)st, at, . . . , sT (cid:1) π(cid:14)T π (cid:14)T π s,a, (2) where = 0 for Rπ or is timestamp of for Rπ(s) and Rπ(s, a). The standard goal of RL is to find policy that maximizes the expected return: π = arg max πΠ ET π [Rπ] . (3) 2.2 Learning Optimal Value Functions Many RL algorithms rely on the state-value function π(s) (or simply -function) or action-value function Qπ(s, a) (Q-function) defined as follows: π(s) def= ET π [Rπ(s)] , Qπ(s, a) def= ET π s,a [Rπ(s, a)] , (4) 2 π(s) = 0 and Qπ(s, a) = 0 in all terminal states s. We denote the -function of the optimal policy π (optimal value function) by (), and the optimal Q-function as Q(, ). Thanks to the tower property of the conditional expectation operator, value and optimal value functions satisfy the famous Bellman equations [42]: π(s) = Ea,s[r(s, a, s) + π(s)] , (s) = max aA Es[r(s, a, s) + (s)] , Qπ(s, a) = Ea,s[r(s, a, s) + Qπ(s, a)] , Q(s, a) = Es (cid:20) r(s, a, s) + max aA Q(s, a) (cid:21) , (VV) (VV*) (QQ) (QQ*) where the expectation is taken over the variables sampled from the policy or the state-transition law. Many deep learning algorithms in RL involve learning either the Qor the -function using Bellman equations. Policy gradient methods often rely on learning the value function for some policy π [34]. Policy evaluation step [42] aims to find π. The π θ is NN parametrized by weight vector θ. The NN is trained by optimizing the MSE objective that regresses π θ (s), δV (θ) = π θ on the rhs of (VV). Define θ (s) r(s, a, s) π (5) the difference between the current approximation of the -function and its target from the corresponding Bellman equation. By θ we denote the target networks weights. Then 2 δV (θ)2(cid:3) , (cid:2) 1 (MSE) θ = . It follows from the fundamental property of conditional expectation θ satisfies (VV). The expectation is optimized when π being the optimal L2-predictor, so the minimum is attained when π is taken over tuples (s, a, s) collected during the interaction of an agent an environment. LMSE(θ) = E(s,a,s) 2.3 Formalizing the Risk Aversion Consider two alternative returns an agent can choose, one is deterministic zero reward, and the other is either 1 or 1 with equal probabilities. For objective (3) they are equal, because their expected values are equal, but for some applications the deterministic reward is preferable, because it is less risky. There are many possible ways to formalize the preferences of random outcomes. The most straightforward one is through the von NeumannMorgenstern (VNM) utility theorem. It states that under 4 VNM-rationality axioms [44, 23], the utility function can describe the agents preferences, i.e., random outcome is preferable to , if E[u(X)] > E[u(Y )]. The utility function is defined up to affine transformations, e.g. u() and + bu() describe the same preferences for and > 0. natural assumption, not implied by the VNM theorem, is that u() is strictly increasing function, which can be interpreted as there is no such thing as too much money. Under this assumption, one can define the certainty equivalent (CE) as u1(E[u(X]), non-random reward that is equivalent to random one from the VNM agents point of view. The exponential (also called entropic) utility function u(x) = α1(1 eαx) represents significant specific example. Coefficient α > 0 defines the agents risk aversion. In some applications, one can also consider the case α < 0, in which the agent is said to be risk seeking. If α 0, the agent becomes indifferent to risk and treats outcomes with the same expected values as equal in the limit. As α , the agent treats all positive returns equally regardless of their magnitude and does not tolerate any losses. The certainty equivalent (CE) for random variable is defined as the guaranteed amount that an agent would accept instead of taking risk. For exponential utility, this is expressed as Eα[X] def= α1 log E(cid:2)eαX (cid:3) , (6) where α > 0 is the risk aversion parameter. Operator Eα[] shares many properties with expectation, hence the notation. The key ones are [23]: P. 1 Normalization: Eα[0] = 0. P. 2 Monotonicity: If a.s., then Eα[X] Eα[Y ]. P. 3 Translation invariance: Eα[X + c] = Eα[X] + c, R. 3 P. 4 Tower property: Eα(cid:104)Eα[Y X] (cid:105) = Eα[Y ] Unlike the expectation, Eα[] is not linear, but concave, which is weaker property: P. 5 Concavity: Eα[λX + (1 λ)Y ] λEα[X] + (1 λ)Eα[Y ] , λ [0, 1]. These unique properties allow us to derive Bellman equations [28] for the exponential utility similar to those widely used to solve risk-neutral MDPs. 2.4 Entropic MDP and its limitations Risk-averse MDP aims to maximize the anticipated future return adjusted for unwillingness to bear excess risks. Due to our focus on the exponential utility, we formalize the objective as follows: π = arg max πΠ Eα π [Rπ] . (7) This objective is analogous to (3), but the expectation operator E[] is replaced with the CE operator Eα[] of exponential utility. We can define the value functions analogously to (4): π(s) def= Eα[Rπ(s)] , Qπ(s, a) def= Eα[Rπ(s, a)] , The Bellman equations become [28]: π(s) = Eα a,s (cid:104) r(s, a, s) + π(s) (cid:105) , (s) = max aA Qπ(s, a) = Eα Eα (cid:104) (cid:105) (cid:104) r(s, a, s) + (s) , (cid:105) r(s, a, s) + Qπ(s, a) , Q(s, a) = Eα a,s (cid:20) r(s, a, s) + max aA Q(s, a) (cid:21) . (8) (EVV) (EVV*) (EQQ) (EQQ*) The seminal work on risk-sensitive MDP considered exponential utility [30]. Recently, works [5, 6, 7, 35, 37, 21, 15, 19, 20, 18, 36, 38, 28, 17, 25] also considered MDPs with exponential utility specifically. Many of these methods rely on learning the optimal Qor -function. The value function is often auxiliary in RL algorithms, since the ultimate goal is to learn policy. However, in some applications learning the precise value function is critical. For example in finance it represents the portfolio value or the price of the derivative being hedged [10, 11, 9, 32, 26]. Since the CE operator replaces the expectation, note that objective (MSE) does not learn the correct value function for entropic MDP. The majority of the works mentioned above rely on the following objective, which we call exponential MSE loss: (cid:20) 1 θ (s)(cid:9) exp(cid:8)αr(s, a, s) α π LEMSE(θ) = E(s,a,s) θ (s)(cid:9)(cid:17)2(cid:21) exp(cid:8)α π α2(cid:16) (EMSE) . The optimizer of this loss is correct value function for the risk-averse MDP. By the Taylor expansion (EMSE) can be rewritten as: LEMSE = 1 2 exp(cid:8)2α π (cid:2)δ (θ)2 + o(cid:0)δ (θ)2(cid:1)(cid:3) , θ (s)(cid:9)Ea,s so, the objective (EMSE) reduces to MSE loss for risk-tolerant agents or small error δ (θ). Note that θ (s)(cid:9). We argue that it depends on θ not only through the δ (θ) because of the factor exp(cid:8)2α π such dependence is highly undesirable. First, the loss vanishes for high positive values of π θ (s) and explodes for high negative values. Works [25, 17] note its numerical instability. Second, from the translation invariance property P. 3 of Eα[], the learning of π θ should not depend on the absolute levels of its values. Another loss was proposed in [15], which we call softplus because of the term log(cid:0)1 + exp{αz}(cid:1): (9) LSP(θ) = 2δ (θ)α1 log(cid:0)1 + exp(cid:8)αδ (θ)(cid:9)(cid:1) + 2α2li2 (cid:0) exp(cid:8)αδ (θ)(cid:9)(cid:1) + π2(cid:14)(6α2), (SP) 4 Figure 1: Comparison of loss penalties for one-step value prediction error δ (θ) when α = 1. positive δ (θ) > 0 means the current estimate Vθ(s) underestimates the true CE value (the return is higher than expected). Risk-averse losses heavily penalize underestimation (δ (θ) > 0) since underestimating the value implies unaccounted risk, whereas overestimation (δ (θ) < 0) is penalized less. MSE, being risk-neutral, is symmetric. EMSE (exponential MSE) grows with the absolute value of , leading to numerical instability for large values. where li2(z) is Spences dilogarithm function li2(z) = (cid:82) dz. It appears as an objective, whose gradient coincides with the heuristic stochastic approximation rule, introduced in [35]. This loss depends on θ only through the δ (θ), which makes it numerically more stable than (SP). However, it is not convex and only learns the correct value function when the target has Gaussian distribution. log(1z) 0 In summary, the known losses have the following limitations: EMSE is numerically unstable, SP is optimized by the value function only in specific case. In the following section, we propose the objective that addresses these limitations. 3 Itakura-Saito Loss for Learning Risk-Averse Value Function In this section, we propose an objective for learning the value function that is mathematically correct and numerically stable. While MSE loss minimizes the expectation, there are other objectives with similar properties, especially for risk-sensitive settings. 3.1 Bregman Divergence and Itakura-Saito Loss Recall the definition of Bregman divergence (BD) [8]: dφ(x, y) = φ(x) φ(y) (cid:10)x y, φ(y)(cid:11), (10) where φ() is differentiable convex function. It measures the discrepancy induced by convex function φ; The important property is that the true mean minimizes the expected divergence: E[X] = arg min E[dφ(X, y)] . (11) In other words, the expectation is the best prediction under any Bregman loss, generalization of the fact that mean minimizes MSE. Moreover, BD is an exhaustive class of loss functions for which the expectation is the optimizer [2]. BD with φ(z) = 2 z2 reduces to the MSE loss. BD with φ(z) = log is known as Itakura-Saito (IS) distance [31] dIS(x, y) = x/y log(x/y)1. It is widely used in audio processing [13] and non-negative matrix factorization [22]. We propose the Itakura-Saito loss to learn the value function. The IS loss is given by: LIS(θ) def= α2E(s,a,s) In Appendix we formally state and prove the following proposition. (cid:2)exp(cid:8)αδ (θ)(cid:9) αδ (θ) 1(cid:3) . (IS) Proposition 1. Under mild assumptions the value function that minimizes (IS) satisfies (EVV). First, note that (IS) depends on θ only through δ (θ). Second, by the Taylor expansion: (cid:2) 1 2 δ (θ)2 + (cid:0)δ (θ)2(cid:1)(cid:3) , LIS = E(s,a,s) (12) so, for risk-tolerant agent or small discrepancies between the -function and its target, the ItakuraSaito loss reduces to the MSE loss. We compare visually all losses in Figure 1. Notably, our IS loss casts the risk-sensitive Bellman criterion into form suitable for stochastic gradient descent circumventing the bias issues identified in past risk-sensitive Q-learning attempts [35]."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we empirically compare the proposed loss (IS) against (EMSE) and (SP). We choose the financial problems as our primary experimental setups for the following reasons: 1. The very concept of risk aversion originates in economics and finance [4, 44, 23]. 2. RL is widely considered in financial literature [33, 27]. 3. The proposed setups admit ground truth analytical solutions (see Appendix A) or theoretical references, which allow us to highlight the advantages of the proposed method. Also, we compared all losses in more complex setup considered in [17], where the authors propose to use risk-averse RL to increase the robustness of the learned policy against distribution shifts. We disclose all technical details in Appendix B. 4.1 Portfolio Optimization and Hedging Consider the problem of optimal stock trading in several setups. The state space is represented by the stock price augmented with timestamp: = (t, St) {0, . . . , } R. Each time, an agent can buy or sell any amount of stock, so the action space = is continuous. We consider the discrete-time Bachelier model [1] of stock price dynamics 1, so the price increments are independent and normally distributed. Let Zt (µ, σ2), = 1, . . . , be the iid Gaussian variables with mean µ and variance σ2. The state transition law is: = (t + 1, St + Zt+1), = 0, . . . , 1, so the state transition law is independent of the action taken. The initial stock price is S0, so s0 = (0, S0). The reward function is specified differently for each setup. We parametrize πϕ(s) and π θ (s) as multi-layer perceptrons. We use the TD(0) learning with function approximation [42] to learn the -functions. Authors in [28] prove that the optimal policy is deterministic in this case, so the action is non-random output of πϕ(s). To learn the optimal policy, we minimize the following objective: L(ϕ) = Es (cid:104) α1 exp(cid:8)α(cid:0)r(s, πϕ(s), s) + π θ (s) π θ (s)(cid:1)(cid:9)(cid:105) , (13) which estimates the gradient of α1 exp(cid:8)α Qπ(cid:0)s, πϕ(s)(cid:1)(cid:9) using one TD(0) sample. It learns the correct policy, since the function α1 exp{αx} is monotonically decreasing. In the first experiment we set µ > 0 and r(s, a, s) = a(St+1 Analytically Tractable Cases St), = 0, . . . , 1, so the rewards come from stock trading solely. Return Rπ(s0) = (cid:80)T 1 t=0 atZt+1 is distributed normally as sum of Gaussian random variables, so the application of objective (SP) is mathematically sound here. The optimal -function and optimal policy can be derived analytically (see Appendix A): π(s) = µ ασ2 , (s) = µ2(T t) 2ασ . (14) Next, we set µ = 0 and we consider the reward function of the form: r(s, a, s) = (cid:26)a(St+1 St), a(ST ST 1) + g(ST ), g(x) = 1 2 (x S0)2 = 0, . . . , 2, = 1. (15) 1Although the model may seem too simplistic compared to those used in practice [24], it allows us to evaluate our method quantitatively. The extension to more complex model falls beyond the scope of the current work. (a) Gaussian reward (b) Quadratic reward Figure 2: Error in learning the obtained approximation of in the Gaussian and quadratic cases. Each experiment was run five times with different random seeds. In the Gaussian case, losses perform on par. Loss (SP) does not learn the correct value function for the non-Gaussian return. (a) Training process with α = 10. We depict the loss value during training for five random seeds for each loss. Objectives (SP) and (IS) converge successfully, while all runs with (EMSE) failed. (b) We run (SP) and (IS) five times for each value of risk aversion α. The filling covers the area 1 standard deviation around the mean value. Although losses converge to the theoretical risk-neutral reference, (IS) is more stable for large values of α than (SP). Figure 3: Loss performance on the Deep Hedging problem [10]. Our loss shows more stable and reliable convergence than the alternatives. It is similar to the previous one, except the agent receives quadratic reward at the last moment. The return is not Gaussian anymore, so the loss (SP) learns the incorrect value function. An analytical solution exists in this case: π(s) = α(St S0), (s) = α(St S0)2 + (T t) log (cid:0)1 ασ2(cid:1) 2α . (16) We compare the proposed loss (IS) with the alternatives in Figure 2. We use RMSE between the learned value function and the analytical solution RMSE = (cid:112)Es[(V (s) π θ (s))2]. Note that the expectation does not depend on a, because the state-transition law is independent of a. Deep Hedging The European call option is the simplest non-linear derivative contract, which has the payoff of the form h(x) = max{x K, 0}, where is the stock price at pre-determined moment and is the contract parameter, called the strike price [23]. We consider the following reward function: r(s, a, s) = (cid:26)a(St+1 St), a(ST ST 1) h(ST ), h(x) = max{x K, 0} = 0, . . . , 2, = 1. (17) Similar problems are widely considered in the financial literature [10, 16, 12, 32, 41]. The main goal is to calculate the price of the derivative, (cid:0)s0(cid:1) in our notations. In our case, the closed-form solution is available only for the risk-neutral case (α = 0): (cid:0)s0(cid:1) = σ(cid:112)T /2π [1, 14]. Interestingly, in the risk-neutral case, every policy for which the expected return is well defined mathematically is optimal, because the price process is martingale. 7 (a) Validation performance of risk-sensitive SAC under undiscounted returns (γ = 1). (b) Validation performance of RSSAC under discounted returns (γ = 0.99). Figure 4: Loss performance on the RSSAC problem [17]. Learning curves depict the mean validation return during the training process. Each line represents the average over three random seeds, with shaded areas indicating 1 standard deviation. The (EMSE) loss destabilizes training. We show the results in Figure 3. First, Figure 3a supports our speculations (9) about the unstable nature of (EMSE) loss. Second, Figure 3b shows that (SP) and (IS) succeeded in converging to the theoretical risk-neutral reference. However, the (SP) loss shows higher variance across random seeds than our loss. We speculate that the primary cause is the non-convex nature of (SP), so the optimization procedure can get stuck in local minimum. Also, the return is not Gaussian in this case (although close), so the usage of (SP) is not justified. 4.2 Risk-Averse Soft Actor-Critic (RSSAC) for Robust Combinatorial Optimization This experiment aims to show that our loss can act as performance-enhancing drop-in replacement of known losses in complex RL algorithms. We adopt the experimental setup of [17], which resembles the warehouse management problem. The environment is 5 5 grid. Each time, items randomly and independently appear in grid cells according to some probability distribution unknown to the agent. The agent can move up, down, left, right, or stay. Any movement costs 1 to the agent. When the agent reaches the cell with an item, it picks it. If the agent delivers the item to the specific cell, it receives +15 reward. If an item is not picked during some period after its spawn, it disappears. The agent can carry at most one item at time. The duration of episodes is constant. The authors study the problem of learning policy robust to distribution shifts. They propose the risk-averse soft actor-critic algorithm with exponential utility to learn such policies. The algorithms learns Q-function as critic. The authors rely on the approximate equality E[X γ] E[X]γ, where γ is discount factor, and derive the Bellman equation of the form: Qπ θ (s, a) = Eα (cid:2)r(s, a, s) + γκH(πϕ (cid:0) s)(cid:1) + γQπ θ(s, a)(cid:3) , s,a (18) where H(cid:0)πϕ( s)(cid:1) is the entropy of the policy πϕ and κ is the entropic regularization coefficient. The authors note the unstable nature of (EMSE) loss and propose to regress Qπ θ (s, a) on sampling-based estimation of the rhs of (18). They replace the expectation over with single from the replay buffer and directly compute the expectation over the next actions to estimate the regression target. As noted by the authors, this results in biased estimation of the Q-function. Nevertheless, they do it because they do not aim to recover the correct Q-function, but to learn the close-to-optimal policy. We run the proposed soft actor-critic algorithms with minor modifications. Instead of relying on the direct estimation of the rhs of (18), we learn the Q-function using the unbiased objective. We compare the losses in Figure 4. The figure shows the validation return during the training process. To measure the robustness, the return is computed with the probability of item appearance different from the one used during training. The objective (EMSE) destabilizes the training process. The proposed loss (IS) performs on par with (SP) and consistently performs during training. Our loss outperforms the (EMSE) objective in complex RL algorithms and environments."
        },
        {
            "title": "5 Discussion",
            "content": "This paper proposes Itakura-Saito loss, simple loss function to learn the value function in risk-averse MDPs. We proved that the minimizer of this loss is indeed the correct value function. Numerical 8 experiments show that alternatives either destabilize training or do not recover the correct value function. Itakura-Saito loss can be used as drop-in replacement in complex RL algorithms. Limitations The proposed methodology only applies to the exponential utility. Though we do not see this as problem, since many methods rely on solving the exponential MDP [28, 29, 39]. Broader impact While our work is methodological in nature and tested in synthetic settings, we believe that developing reliable mathematical tools for optimization under uncertainty is necessary to make AI more acceptable and dependable in real-world applications."
        },
        {
            "title": "References",
            "content": "[1] Louis Bachelier. Théorie de la spéculation. In: Annales scientifiques de lÉcole normale supérieure. Vol. 17. 1900, pp. 2186. [2] Arindam Banerjee, Xin Guo, and Hui Wang. On the optimality of conditional expectation as Bregman predictor. In: IEEE Transactions on Information Theory 51.7 (2005), pp. 2664 2669. [3] Arindam Banerjee et al. Clustering with Bregman divergences. In: Journal of machine learning research 6.Oct (2005), pp. 17051749. [4] Daniel Bernoulli. Commentarii academiae scientiarum imperialis petropolitanae. In: Petropoli. Chap. De vibrationibus et sono laminarum elasticarum 27 (1751), p. 28. [5] Vivek Borkar. sensitivity formula for risk-sensitive cost and the actorcritic algorithm. In: Systems & Control Letters 44.5 (2001), pp. 339346. [6] Vivek Borkar. Q-learning for risk-sensitive control. In: Mathematics of operations research 27.2 (2002), pp. 294311. [7] Vivek Borkar and Sean Meyn. Risk-sensitive optimal control for Markov decision processes with monotone cost. In: Mathematics of Operations Research 27.1 (2002), pp. 192 209. [8] Lev Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. In: USSR computational mathematics and mathematical physics 7.3 (1967), pp. 200217. [9] Hans Buehler, Phillip Murray, and Ben Wood. Deep bellman hedging. In: arXiv preprint arXiv:2207.00932 (2022). [10] Hans Buehler et al. Deep hedging. In: Quantitative Finance 19.8 (2019), pp. 12711291. [11] Hans Buehler et al. Deep hedging: learning to remove the drift under trading frictions with minimal equivalent near-martingale measures. In: arXiv preprint arXiv:2111.07844 (2021). Jay Cao et al. Deep Hedging of Derivatives Using Reinforcement Learning. In: The Journal of Financial Data Science 3.1 (2021), pp. 1027. [12] [13] Alan HS Chan. Advances in industrial engineering and operations research. Vol. 5. Springer [14] Science & Business Media, 2008. Jaehyuk Choi et al. BlackScholes users guide to the Bachelier model. In: Journal of Futures Markets 42.5 (2022), pp. 959980. [15] Grégoire Delétang et al. Model-free risk-sensitive reinforcement learning. In: arXiv preprint [16] arXiv:2111.02907 (2021). Jiayi Du et al. Deep Reinforcement Learning for Option Replication and Hedging. In: The Journal of Financial Data Science (2020). [17] Tobias Enders, James Harrison, and Maximilian Schiffer. Risk-sensitive soft actor-critic learning under distribution shifts. In: arXiv preprint for robust deep reinforcement arXiv:2402.09992 (2024). [18] Yingjie Fei and Ruitu Xu. Cascaded gaps: Towards logarithmic regret for risk-sensitive reinforcement learning. In: International Conference on Machine Learning. PMLR. 2022, pp. 63926417. [19] Yingjie Fei, Zhuoran Yang, and Zhaoran Wang. Risk-sensitive reinforcement learning with function approximation: debiasing approach. In: International Conference on Machine Learning. PMLR. 2021, pp. 31983207. [20] Yingjie Fei et al. Exponential bellman equation and improved regret bounds for risk-sensitive reinforcement learning. In: Advances in neural information processing systems 34 (2021), pp. 2043620446. 9 [21] Yingjie Fei et al. Risk-sensitive reinforcement learning: Near-optimal risk-sample tradeoff in regret. In: Advances in Neural Information Processing Systems 33 (2020), pp. 2238422395. [22] Cédric Févotte, Nancy Bertin, and Jean-Louis Durrieu. Nonnegative matrix factorization with the Itakura-Saito divergence: With application to music analysis. In: Neural computation 21.3 (2009), pp. 793830. [23] Hans Föllmer and Alexander Schied. Stochastic finance: an introduction in discrete time. Walter de Gruyter, 2011. Jim Gatheral. The volatility surface: practitioners guide. John Wiley & Sons, 2011. [24] [25] Alonso Granados, Reza Ebrahimi, and Jason Pacheco. Risk-Sensitive Variational ActorCritic: Model-Based Approach. In: The Thirteenth International Conference on Learning Representations. 2025. Igor Halperin. QLBS: Q-Learner in the Black-Scholes (-Merton) Worlds. In: Journal of Derivatives 28.1 (2020), pp. 99122. [26] [27] Ben Hambly, Renyuan Xu, and Huining Yang. Recent advances in reinforcement learning in [28] finance. In: Mathematical Finance 33.3 (2023), pp. 437503. Jia Lin Hau, Marek Petrik, and Mohammad Ghavamzadeh. Entropic risk optimization in discounted MDPs. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2023, pp. 4776. [29] Pierre Henry-Labordere. From (martingale) Schrodinger bridges to new class of stochastic volatility model. In: Available at SSRN 3353270 (2019). [30] Ronald Howard and James Matheson. Risk-sensitive Markov decision processes. In: Management science 18.7 (1972), pp. 356369. [31] Fumitada Itakura. Analysis synthesis telephony based on the maximum likelihood method. In: Reports of the 6th Int. Cong. Acoust., 1968 (1968). [32] Petter Kolm and Gordon Ritter. Dynamic replication and hedging: reinforcement learning approach. In: The Journal of Financial Data Science 1.1 (2019), pp. 159171. [33] Petter Kolm and Gordon Ritter. Modern perspectives on reinforcement learning in finance. In: Modern Perspectives on Reinforcement Learning in Finance (September 6, 2019) (2019). [34] Yuxi Li. Deep Reinforcement Learning. 2018. arXiv: 1810.06339 [cs.LG]. URL: https: //arxiv.org/abs/1810.06339. [35] Oliver Mihatsch and Ralph Neuneier. Risk-sensitive reinforcement learning. In: Machine learning 49 (2002), pp. 267290. [36] Mehrdad Moharrami et al. policy gradient algorithm for the risk-sensitive exponential cost mdp. In: Mathematics of Operations Research 50.1 (2025), pp. 431458. [37] David Nass, Boris Belousov, and Jan Peters. Entropic risk measure in policy search. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. 2019, pp. 11011106. [38] Erfaun Noorani, Christos Mavridis, and John Baras. Exponential TD Learning: Risk-Sensitive Actor-Critic Reinforcement Learning Algorithm. In: 2023 American Control Conference (ACC). IEEE. 2023, pp. 41044109. [39] Marcel Nutz, Johannes Wiesel, and Long Zhao. Martingale Schr\"odinger bridges and optimal [40] semistatic portfolios. In: Finance and Stochastics 27.1 (2023), pp. 233254. Irina Penner. Dynamic convex risk measures: time consistency, prudence, and sustainability. In: Humboldt-Universität zu Berlin (2007). [41] Zoran Stoiljkovic. Applying Reinforcement Learning to Option Pricing and Hedging. In: arXiv preprint arXiv:2310.04336 (2023). [42] Richard Sutton and Andrew Barto. Reinforcement learning: An introduction. MIT press, 2018. [43] Xiaolu Tan and Nizar Touzi. Optimal transportation under controlled stochastic dynamics. [44] In: The annals of probability (2013), pp. 32013240. John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior, 2nd rev. In: (1947)."
        },
        {
            "title": "A Analytical Solutions",
            "content": "We provide here the closed-form derivations for the ground-truth benchmarks reported in the experiments (cf. Section 4 in the main text). All results are obtained under the discrete-time Bachelier dynamics St+1 = St + Zt+1, Zt+1 (µ, σ2), = 0, . . . , 1, and use the exponential-utility certainty equivalent (cid:101)Eα[X] = 1 α log E[exp{αX}], α > 0. For any Gaussian variable (m, v) one has exp(cid:8)αG(cid:9)] = exp(cid:8)αE[G] + α2 2 Var(G)(cid:9) = exp(cid:8)αm + α 2 v(cid:9) fact used repeatedly below. A.1 Pure trading with Gaussian reward Consider single-period reward rt = at(St+1 St) = atZt+1. Since (Zt) are i.i.d. Gaussians, the optimal deterministic policy π is time-independent and the value function does not depend on St. For any fixed action the one-step certainty-equivalent return is (cid:101)Eα[aZt+1] = 1 α log E[exp{αaZt+1}] = µa 1 2 αa2σ2. Maximising this quadratic over gives maxa (cid:101)Eα[aZt+1] = µ2(T t) µ2(T t)/(2ασ2). Hence, the optimal policy is constant in time and satisfies ασ2 and the corresponding single-step optimum . Summing over τ periods yields the value function (st) = 2ασ2 = µ π(st) = µ ασ2 , (st) = µ2(T t) 2ασ2 , as quoted in Eq. (14) in Sec. 4.1. No additional integrability condition is required here because exp(αaZ) is integrable for all α. A.2 Trading with quadratic terminal penalty (µ = 0) Let xt = St S0 be the centred price. Rewards are rt = atZt+1 for < 1, while at = 1 the agent additionally incurs terminal penalty rT 1 = aT 1ZT 1 . We look for solution of the form with boundary conditions: 2 x2 Vt(x) = 1 2 Ktx2 + Ct, KT = 1, CT = 0. Since xt+1 = xt + Zt+1 conditional on xt, xt+1 is Gaussian, so (cid:104) exp (cid:110) α{aZt+1 1 2 Kt+1(xt + Zt+1)2 + Ct+1} (cid:111)(cid:105) (cid:110) = exp αCt+1 + αKt+1 2 x2 (cid:111) (1 αKt+1σ2) 1 2 exp (cid:18) = ασ2 2(1 αKt+1σ2) (a Kt+1xt)2 (cid:19) . By direct calculation of the certainty equivalent at each step, one finds that the exponential inside the expectation is maximized when = Kt+1xt, so the optimal policy is and the maximized Bellman update is dynamic recursion = Kt+1xt Vt(x) = Ct+1 1 2 Kt+1x2 + 1 2α log(1 αKt+1σ2). Matching coefficients gives Kt = Kt+1 and KT = 1, hence Kt 1 for all t. Meanwhile, the scalars Ct satisfy Ct = Ct+1 + 1 2α log(1 ασ2), with boundary CT = 0, which solves to Ct = 2α log(1 ασ2). The recursion is well-defined only if ασ2 < 1, ensuring the Gaussian moment-generating function exists. Under the above condition the optimal policy and value are Provided ασ2 < 1 (so that the required moment-generating function is well-defined), the optimal policy and value function are π(st) = St S0, (st) = 1 2 (St S0)2 + 2α log(1 ασ2), ασ2 < 1, which coincides with Eq. (16) in Sec. 4.1."
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Portfolio Optimization and Hedging Throughout all experiments in this section, we take = 10. The code is written in pure PyTorch 2.7.0. We approximate the state-value function and policy with multi-layer perceptrons with Mish activation and 2 hidden layers, 64 neurons each. We used the Adam optimizer (lr=1e-4, β1 = 0.99, β2 = 0.999) and applied the following learning rate schedule: During the first 1k iterations, the learning rate grows linearly with start factor of 0.01. It then remains constant for the next 49k (149k for Deep Hedging experiments) iterations. Afterwards, we apply cosine decay with T_max equal to 50k (150k for Deep Hedging). Each training batch has size of 1024, with gradient values clipped at 1 and the gradient norms clipped at 10. Each experiment is repeated with five independent seeds that affect weight initialization and mini-batch sampling. The other parameters are listed in the table: Experiment µ σ α Gaussian return Quadratic penalty Deep Hedging 0. 0 0.2 10 1 100 {0.1, 0.3, 1, 3, 10} Our experiments typically run on an A100 GPU, with up to three runs in parallel on single device. For the Gaussian and quadratic tasks, we use 100k training iterations, requiring between 25 and 65 minutes of wall-clock time depending on concurrent jobs. The Deep Hedging setup generally requires 50 minutes to 2 hours for each run, owing to its longer 300k iteration schedule. B.2 Risk-Averse Soft Actor-Critic (RSSAC) for Robust Combinatorial Optimization We employ the open-source code provided by the authors of [17]2 and leave all network, replay-buffer and optimiser hyper-parameters unchanged except for the three modifications below. 1. We took β = 0.1, which is α = 0.1 in our notations; 2. The discount factor is γ {0.99, 1}; 3. The loss function is simpler than in [17] and do not involve computing the expectation over the next actions. Every RSSAC run (500 environment steps) takes 2 on single A100 GPU. No concurrent jobs are scheduled on the same device. 2https://github.com/tumBAIS/RiskSensitiveSACforRobustDRLunderDistShifts. 12 Statement and Proof of Proposition 1 Before stating the result we recall the ItakuraSaito loss (IS) LIS(θ) def= α2E(s,a,s) (cid:2)exp(cid:8)αδ (θ)(cid:9) αδ (θ) 1(cid:3) and δV (θ) = π θ (s) r(s, a, s) π θ (s). Proposition. Assume the following hold: (19) (20) 1. The target network weights θ are copy of the main network weights θ, π θ(s) = stop gradient(cid:0)V π θ (s)(cid:1) in (20); 2. Both (cid:104) exp(cid:8)αr(s, a, s) α π θ (s)(cid:9)(cid:105) (cid:104) and r(s, a, s) + π (cid:105) θ (s) exist; 3. LIS attains its minimum at θ. Then, π θ satisfies the riskaverse Bellman equation (EVV): π(s) = Eα a,s (cid:104) r(s, a, s) + π(s) (cid:105) . (21) Proof. First, note that LIS(θ) = α2Es,a,s (cid:16) (cid:104) dIS exp (cid:110) αr(s, a, s) α π θ (s) (cid:111) , exp (cid:110) α π θ (s) (cid:111) (cid:17)(cid:105) , where α2 is scaling coefficient. Thanks to the assumption 2, one can apply Theorem 1 in [43], so for the minimizer it holds: (cid:110) exp α π θ (s) (cid:111) = Ea,s (cid:104) exp (cid:110) αr(s, a, s) α π θ (s) (cid:111)(cid:105) . Taking the logarithm of both sides, dividing by α and recalling the definition of Eα[] we obtain: θ (s) = Eα π a,s (cid:104) r(s, a, s) + π θ (s) (cid:105) . Geometric and Field-Theoretic Interpretations of the IS Loss This appendix explores the deeper mathematical structure behind the Itakura-Saito (IS) loss and its regularization. While the main paper focuses on empirical performance and optimization theory, here we reinterpret the IS loss in the language of variational calculus and field theory. We show that, under regularization, the IS loss induces an energy functional with features reminiscent of classical scalar field theories. In particular, we analyze its behavior in the linearized regime, where it yields propagator with long-range correlations and, in the high-target limit, reveals an emergent conformal symmetry. These structures illuminate why the IS loss promotes global consistency and smoothness in prediction. We then speculate on broader analogy: drawing inspiration from the AdS/CFT correspondence in theoretical physics, we suggest that the robustness and coherence induced by IS-trained models may be seen as form of holographic encoding where local losses reflect and preserve global structure. No advanced physics background is required to follow this appendix only curiosity for the beautiful human adventure of understanding the universe through mathematics, symmetry, and learning. 13 D.1 Conformal Symmetry and Long-Range Correlations Induced by Itakura-Saito Loss To understand the structural properties of the IS loss beyond pointwise optimization, we adopt variational viewpoint by constructing an action functional. This approach is standard in physics and calculus of variations: instead of minimizing discrete loss over samples, we define functional that integrates local loss density over continuous domain. In this setting, the learned function ϕ(x) becomes field, and the loss becomes an energy functional whose minimizers reflect both local fit and global coherence. This field-theoretic perspective allows us to uncover the long-range smoothing and scale-invariance properties implicit in the IS loss when combined with regularization. We analyze the field-theoretic formulation of the IS loss with smoothness regularization and show how conformal symmetry emerges in the continuum limit. D.2 Two-Point Correlation from the IS-Induced Action We consider the action functional: (cid:90) S[ϕ] = (cid:34) dx λ (cid:19) (cid:18) dϕ dx + y(x) ϕ(x) log (cid:19) (cid:18) y(x) ϕ(x) (cid:35) 1 (22) To study long-range correlations, we simplify and linearize around constant input y(x) = y0, assuming: ϕ(x) = y0 + ε(x), ε(x) (23) Expanding the IS potential term: y0 ϕ (cid:19) (cid:18) y0 ϕ log = 1 1 + ε/y0 1 (cid:19)2 ε y0 + (cid:18) ε (cid:18) = log 1 + (cid:19) ε y0 (cid:32) ε y0 1 2 (cid:18) ε y0 (cid:19)2(cid:33) So the potential becomes: The linearized action reads: VIS(y0, ϕ) (cid:19)2 1 2 (cid:18) ε (cid:90) (cid:34) dx λ S[ε] (cid:19) (cid:18) dε dx + 1 2y2 0 (cid:35) ε2(x) This is standard Gaussian field theory with mass m2 = 1 2y2 0 . D.3 Propagator and Emergence of Conformal Limit (24) (25) The propagator satisfies: (cid:18) (cid:19) 1 2y2 0 Here, δ(x) denotes the Dirac delta function, generalized distribution satisfying (cid:82) δ(x)f (x) dx = (0) for any smooth test function . It models pointwise source and is used to define Greens functions in field theory. G(x) = δ(x) d2 dx2 + λ (26) Solving in 1D yields: G(x) = 2λy0 y0 2λ In the conformal limit y0 or λ 0: This reflects: G(x) 1 14 (27) (28) Absence of intrinsic length scale, Power-law correlation decay, Conformal symmetry and long-range propagation. D.4 Implications for Optimization Dynamics We now show how IS loss, being scale-invariant, improves optimization convergence. Let θ Rd be model parameters, with outputs (θ) Rn, and targets y. Assume gradient descent: θt+1 = θt ηL(θt) (29) We compare: MSE: LMSE(θ) = 1 IS: LIS(θ) = (cid:80) 2 (θ) y2 (cid:16) yi fi(θ) log (cid:16) yi fi(θ) (cid:17) (cid:17) Conditioning and Hessian Geometry: MSE: Hessian is = where = /θ. Large spread in leads to poor conditioning. IS: Penalizes relative differences (θ)/y. Built-in normalization leads to better conditioned Hessian and stable steps. Local Approximation in 1D: Let = + ε with small ε. Then: LIS(f ) = 1 2 log (cid:19) (cid:18) (cid:19)2 (cid:18) ε Hence IS behaves locally like rescaled MSE: LIS(f ) 1 2y2 ε2 (30) Large lower weight, small higher weight. This performs implicit preconditioning, akin to natural gradients. Natural gradients arise in information geometry as an improvement over standard (Euclidean) gradients. Instead of computing updates in the raw parameter space, the natural gradient rescales the direction of steepest descent using the inverse Fisher information matrix. This aligns updates with the intrinsic curvature of the loss landscape, leading to faster convergence and better conditioning. In the context of IS loss, the local reweighting of errors by 1/y2 mimics this effect: low-output regions receive stronger updates, similar to how natural gradients emphasize directions of low Fisher variance [45]."
        },
        {
            "title": "E Conformal Invariance Improves Statistical Conditioning",
            "content": "In this appendix, we formalize the intuition that conformal invariance, as induced by the Itakura-Saito (IS) loss, statistically improves the conditioning of the optimization problem. We do so by analyzing the distribution of Hessian spectra under Gaussian prior over models. Remark. This theorem highlights novel geometric interpretation of scale-invariance: it not only regularizes the optimization surface but also statistically preconditions the curvature providing mathematical basis for the empirical advantages of IS loss. We propose theoretical explanation for the improved optimization behavior observed when using the Itakura-Saito (IS) loss, through the lens of statistical conditioning and symmetry constraints. 15 Theorem (Improved Spectral Conditioning under Conformal Invariance). Let ϕ be Gaussian ensemble of models (e.g., fields or neural networks), and let Gconf be the subset of models such that the IS-induced action SIS[ϕ] is conformally invariant. Define spectral conditioning measure λ(ϕ), such as the variance or entropy of the eigenvalue spectrum of the Hessian at ϕ. Then: EϕGconf[Var(λ(ϕ))] < EϕG[Var(λ(ϕ))] (31) Sketch of proof. The IS loss induces field theory with conformal symmetry in the limit y0 , leading to scale-free correlations. When restricting to Gconf, the underlying symmetry enforces statistical regularity in the structure of the Hessian. In contrast, over the full Gaussian ensemble G, more arbitrary fluctuations are allowed. By known results from random matrix theory and information geometry, imposing such symmetries reduces the variance and entropy of the spectral distribution. This leads to improved average conditioning. This supports the empirical observation that the IS loss improves the landscape geometry for gradient descent by constraining optimization trajectories to submanifold of better-conditioned models. E.1 Discussion and Related Work While general formal proof connecting conformal invariance to better conditioning is still open, similar ideas appear in: Theoretical physics, where scale-invariant theories exhibit smoother correlation functions and long-range order [49, 53]. Studies of the conformal bootstrap, which show how scale invariance constrains fluctuations and narrows operator spectra [56]. Random matrix theory and kernel methods, where flattened spectra correspond to improved generalization. Recent numerical studies that identify conformal symmetry in real physical transitions as signature of underlying flatness and universality [60, 59]. These connections reinforce the insight that conformal invariance leads to better distributed Hessian spectrum measurable through variance and entropy and thus enhances the robustness and convergence of gradient-based learning. E.2 The particular case of flattened spectra and RMT We talk about flattened spectra when Hessians or kernel matrices have lower spectral variance or more uniform eigenvalue distribution. This is usually associated with improved generalization in both kernel methods and deep networks. In random matrix theory (RMT), this corresponds to ensembles where the eigenvalue density concentrates, reducing the effect of high-curvature directions that may cause overfitting. In kernel methods [46] and [50] show that flatter spectra lead to more stable interpolation and better generalization in the overparameterized regime. Similar ideas appear in deep learning: models trained with flatter loss landscapes, reflected in the Hessian spectrum (e.g., fewer large outliers), often exhibit better generalization [58, 57]. In our setting, the conformal IS loss effectively preconditions the Hessian, reducing its spectral variance, thus echoing these theoretical insights. The flattening of the Hessian spectrum induced by conformal invariance also resonates with results from random matrix theory (RMT). In high-dimensional models, RMT provides statistical framework to describe the eigenvalue distribution of Hessians, Fisher matrices, or kernel operators. common benchmark is the MarchenkoPastur law, which characterizes the spectrum of sample covariance matrices in the absence of structure. Deviations from this law such as heavy tails, outliers, or sharp spectral peaks often signal overfitting or poor generalization [51, 54]. In contrast, flatter or more regular spectra (e.g., those with lower variance and fewer extreme outliers) tend to reflect better generalization performance. In our setting, the IS-induced conformal symmetry reduces the spectral variance of the Hessian, effectively steering the system toward more stable and bulk-like spectrum, thus aligning with favorable RMT regimes. 16 E.3 CFT Robustness and the Holographic Analogy In the high-target limit y0 , we have shown that the IS loss regularized by smoothness penalty induces an effective action (cid:90) S[ϵ] = dx λ (cid:19)2 (cid:18) dϵ dx (32) This corresponds to massless scalar field theory conformal field theory (CFT) in one dimension with long-range power-law correlations G(x) 1/x and no intrinsic length scale. Beyond its mathematical elegance, this structure echoes the foundational role of CFTs in the AdS/CFT correspondence [52]. In this duality, gravitational theory defined in (d + 1)-dimensional Anti-de Sitter (AdS) bulk is fully determined by d-dimensional CFT living on its boundary. Perturbations and dynamics in the bulk geometry are encoded in boundary correlators and operator insertions of the CFT. Robustness as Boundary Consistency. Within this holographic framework, the robustness and stability of the boundary CFT are essential for the well-posedness of the dual gravitational theory. Small fluctuations on the boundary propagate into coherent and physically meaningful bulk geometries. Similarly, in our context, we observe that the IS loss by enforcing conformal invariance in the large-y0 regime leads to stable, globally consistent learning dynamics. The long-range coherence of the predictions resembles the behavior of holographic boundary theory robustly encoding bulk structure. Speculative Correspondence. We may interpret the IS-trained model as system where: The prediction layer behaves like boundary CFT, enforcing smooth, scale-invariant structure. The internal representation space (e.g., hidden layers or latent variables) acts as discrete, dynamically evolving bulk, whose geometry is regularized through this induced boundary behavior. This analogy opens speculative but intriguing avenue: robust generalization in learning systems may reflect kind of holographic encoding, where local losses induce structured global behavior. The IS loss, in this view, acts as holographically robust preconditioner: it ensures that perturbations do not remain confined, but are smoothed in globally consistent manner. Future Directions. While our model remains in 1D and is far from full-fledged CFT, let alone holographic dual, the structural parallels motivate future exploration. It may be fruitful to investigate: Learning systems that approach RG fixed points under IS-like training, mirroring conformal fixed points. Architectures where latent representations exhibit AdS-like metrics, optimized via boundaryconsistent losses. Explicit bulk-boundary decompositions in model design, inspired by holographic renormalization. This connection between optimal loss design, conformal symmetry, and holographic stability invites deeper geometric and dynamical understanding of learning systems. Conclusion The IS loss with regularization is mathematically equivalent to conformal scalar field theory. It induces power-law correlations and better-conditioned optimization landscapes. This explains both its empirical stability and its ability to propagate information globally in high-risk or low-signal regimes. Thus, scale-invariant losses improve convergence by embedding learning into the geometry of critical systems with long-range structure [47, 48, 61, 55]."
        },
        {
            "title": "Additional References",
            "content": "[45] Shun-Ichi Amari. Natural gradient works efficiently in learning. In: Neural computation 10.2 (1998), pp. 251276. 17 [46] Mikhail Belkin et al. Reconciling modern machine learning and the bias-variance trade-off. In: Proceedings of the National Academy of Sciences 116.32 (2019), pp. 1584915854. John Cardy. Scaling and renormalization in statistical physics. Vol. 5. Cambridge university press, 1996. [47] [48] Philippe Francesco, Pierre Mathieu, and David Sénéchal. Conformal field theory. Springer Science & Business Media, 2012. [49] Malte Henkel. Conformal Invariance and Critical Phenomena. Springer, 1999. DOI: 10.1007/ 978-3-662-03937-3. [50] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In: NeurIPS (2018). [51] Camille Louart, Zhenyu Liao, and Romain Couillet. random matrix approach to neural [52] networks. In: Annals of Applied Probability 28.2 (2018), pp. 11901248. Juan Maldacena. The large-N limit of superconformal field theories and supergravity. In: International journal of theoretical physics 38.4 (1999), pp. 11131133. [53] Hidetoshi Nishimori and Gerardo Ortiz. Elements of Phase Transitions and Critical Phenom- [54] ena. Oxford University Press, 2010. ISBN: 9780199577224. Jeffrey Pennington and Pratik Worah. The emergence of spectral universality in deep networks. In: Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS). 2018, pp. 19241932. [55] Michael Peskin. An Introduction to quantum field theory. CRC press, 2018. [56] David Poland, Slava Rychkov, and Alessandro Vichi. The Conformal Bootstrap: Theory, Numerical Techniques, and Applications. arXiv preprint arXiv:1805.04405. 2018. arXiv: 1805.04405 [hep-th]. [57] Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singularity and beyond. In: arXiv preprint arXiv:1611.07476 (2016). [58] Zhewei Yao et al. Hessian-based analysis of large batch training and robustness to adversaries. In: Advances in Neural Information Processing Systems 31 (2018). [59] You Zhou et al. Emergent Symmetry in Quantum Phase Transitions. In: Fundamental Research 3.4 (2023), pp. 2738. DOI: 10.1016/j.fmre.2023.02.003. [60] Wei Zhu et al. Uncovering conformal symmetry in the 3D Ising transition: state-operator correspondence from quantum fuzzy sphere regularization. In: Physical Review 13.2 (2023), p. 021009. Jean Zinn-Justin. Quantum field theory and critical phenomena. Vol. 171. Oxford university press, 2021. [61]"
        }
    ],
    "affiliations": [
        "Artificial Intelligence Research Institute",
        "Natixis Foundation",
        "Skolkovo Institute of Science and Technology",
        "Vega Institute Foundation"
    ]
}