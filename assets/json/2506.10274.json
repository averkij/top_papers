{
    "paper_title": "Discrete Audio Tokens: More Than a Survey!",
    "authors": [
        "Pooneh Mousavi",
        "Gallil Maimon",
        "Adel Moumen",
        "Darius Petermann",
        "Jiatong Shi",
        "Haibin Wu",
        "Haici Yang",
        "Anastasia Kuznetsova",
        "Artem Ploujnikov",
        "Ricard Marxer",
        "Bhuvana Ramabhadran",
        "Benjamin Elizalde",
        "Loren Lugosch",
        "Jinyu Li",
        "Cem Subakan",
        "Phil Woodland",
        "Minje Kim",
        "Hung-yi Lee",
        "Shinji Watanabe",
        "Yossi Adi",
        "Mirco Ravanelli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks.They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 4 7 2 0 1 . 6 0 5 2 : r Discrete Audio Tokens: More Than Survey! Pooneh Mousavi1,2, Gallil Maimon*3, Adel Moumen*4, Darius Petermann*5, Jiatong Shi*6, Haibin Wu*7, Haici Yang*5, Anastasia Kuznetsova*5, Artem Ploujnikov8,2, Ricard Marxer9, Bhuvana Ramabhadran10, Benjamin Elizalde11, Loren Lugosch11, Jinyu Li7, Cem Subakan12,2,1, Phil Woodland4, Minje Kim14, Hung-yi Lee13, Shinji Watanabe6, Yossi Adi3, Mirco Ravanelli1,2,8 1Concordia University, 2Mila-Quebec AI Institute, 3The Hebrew University of Jerusalem, 4University of Cambridge, 5Indiana University, 6Carnegie Mellon University, 7Microsoft, 8Université de Montréal, 9Université de Toulon, 10Google, 11Apple 12Laval University, 13National Taiwan University, 14University of Illinois at Urbana-Champaign"
        },
        {
            "title": "Abstract",
            "content": "Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks. They provide practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack unified comparison across various benchmarks. This paper presents systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/."
        },
        {
            "title": "1 Introduction",
            "content": "Audio compression has been well-established research topic since the foundations of digital communication (Shannon, 1948; Nyquist, 1928). Traditional audio codecs, such as linear predictive coding (LPC) (Itakura, 1968; Atal, 1970), modified discrete cosine transform (MDCT) (Wang & Vilermo, 2003), and Code Excited Linear Prediction (CELP) (Schroeder & Atal, 1985; Jage & Upadhya, 2016), were designed to reduce redundancy and remove perceptually irrelevant information. These models have been effective in compressing raw audio signals into compact bitstreams (encoding) and then restoring them to the original signal domain (decoding). Codecs like USAC (Quackenbush, 2013), Opus (Valin et al., 2012), and EVS (Dietz et al., 2015) combine these techniques to support range of content types, bitrates, and sampling rates while ensuring low latency for real-time communication. These approaches rely heavily on domain knowledge, combining signal processing pipelines with hand-crafted components to achieve efficient but lossy compression. Traditional codecs are efficient and optimized for perceptual quality, but their design requires substantial manual effort, including parameter tuning and subjective listening tests (Valin et al., 2012; Dietz et al., 2015). This has motivated shift toward data-driven approaches with deep learning, known as neural Project lead, Corresponding author (pooneh.mousavi@mail.concordia.ca) * Equal contribution, Core Team. 1 codecs. Neural codecs consist of an encoder, decoder, and quantization module, closely resembling standard autoencoders. The key difference is that neural codecs produce discrete representations (audio tokens) instead of continuous ones. The discretization is performed by differentiable quantizer, such as residual vector quantization (RVQ) (Zeghidour et al., 2021), which enables end-to-end training by allowing gradients to propagate through the quantization step. Neural codecs are often trained using combination of losses. For example, reconstruction losses in the time and frequency domains (Kankanahalli, 2018), optionally combined with psychoacoustic calibration (Zhen et al., 2020), direct guide signal reconstruction. Adversarial losses (Zeghidour et al., 2021) and generative models (Kleijn et al., 2018; Valin & Skoglund, 2019b; Gârbacea et al., 2019) indirectly improve the perceptual quality of the reconstructed signal. Finally, auxiliary losses are often introduced to improve the learning process and often act as regularizers or encode inductive bias (Zhang et al., 2024a; Défossez et al., 2024; Har-Tuv et al., 2025). Discrete tokens have several useful properties. As they are normally compact, audio tokens enable more efficient storage and transmission than continuous embeddings. They also simplify audio generation by converting tasks that involve modeling continuous distributions, such as regression, into discrete classification problems (Wu et al., 2024f; Mousavi et al., 2024a). More importantly, they help bridge the gap between text and audio processing, making them natural choice for multimodal models and core component of many recent multimodal LLMs (Peng et al., 2024; Cui et al., 2024; Ji et al., 2024a; Latif et al., 2023; Liu et al., 2023; Wu et al., 2024a; Tian et al., 2025). Driven by these advantages, discrete audio tokens have already been adopted as an alternative to continuous features in wide range of downstream tasks: automatic speech recognition (Chang et al., 2023; Du et al., 2023), speech-to-speech translation (Popuri et al., 2022; Inaguma et al., 2023; Wu et al., 2023a; Chang et al., 2024), voice conversion (Maimon & Adi, 2023; Wang et al., 2024d), text-to-speech synthesis (Ju et al., 2024; Chen et al., 2025a; Hayashi & Watanabe, 2020), speech enhancement (Wang et al., 2024e; Yang et al., 2024f; Xue et al., 2024), and source separation (Shi et al., 2021c; Erdogan et al., 2023; Mousavi et al., 2024b; Bie et al., 2025; Yip et al., 2024). Discrete tokens are also used in music and general audio tasks, including music generation (Copet et al., 2023; Chen et al., 2024a), environmental sound synthesis (Yang et al., 2023b; Kreuk et al., 2023), and multimodal generation (Borsos et al., 2023b; Liu et al., 2023; Ziv et al., 2024; Rubenstein et al., 2023; Wang et al., 2024b). Recent studies have introduced variety of tokenization methods, often grouped into two main categories: acoustic and semantic1 (Borsos et al., 2023b; Zhang et al., 2024a; Har-Tuv et al., 2025; Guo et al., 2025b). Acoustic tokens are typically learned through encoder-decoder architectures optimized for waveform reconstruction (Zeghidour et al., 2021; Défossez et al., 2023; Kumar et al., 2023; Yang et al., 2023a). Semantic tokens are derived from pretrained self-supervised learning (SSL) models (Lakhotia et al., 2021; Mousavi et al., 2024b) or encoders trained in supervised manner (Du et al., 2024b) designed to capture phonetic or linguistic content for discriminative tasks such as speech recognition and translation. Some recent approaches aim to combine both types, introducing hybrid tokenizers (Zhang et al., 2024a; Défossez et al., 2024) that balance acoustic and phonetic properties. We argue the common division of discrete tokens into acoustic and semantic categories has notable limitations. Acoustic tokenizers can capture semantic information (Défossez et al., 2024; Du et al., 2023; Zhang et al., 2024a; Bai et al., 2024), while semantic tokenizers have been effectively used in generative tasks (Polyak et al., 2021; Wang et al., 2024e; Nguyen et al., 2025; Lakhotia et al., 2021; Maimon et al., 2025a; Hassid et al., 2023; Mousavi et al., 2024b; Wu et al., 2025). This overlap blurs the boundary between the two categories and suggests that the acoustic-semantic distinction alone is insufficient. Moreover, as tokenization methods continue to evolve, traditional classifications fail to capture key architectural differences and practical tradeoffs. To address this limitation, we introduce refined taxonomy that captures key design choices, including encoder-decoder, quantization techniques, training paradigms, streamability, and application domains. Another notable gap in the literature is that existing surveys and benchmark papers have primarily focused on speech applications (Cui et al., 2024; Kim & Skoglund, 2024; Ji et al., 2024a; Anees, 2024; Guo et al., 2025b; 1It is important to clarify that the term semantic in the speech context does not align with its conventional linguistic meaning. In the speech context, these discrete tokens are more accurately described as phonetic units (Sicherman & Adi, 2023; Choi et al., 2024) and typically do not carry semantic content (Arora et al., 2025). In this paper, to maintain consistency across different domains (speech, audio, music) and with established terminology such as semantic distillation, we consistently use the term semantic. 2 Figure 1: Overview of our empirical study, covering three domains: speech, music, and general audio, with four evaluation components: Downstream Evaluation (Section 3.2) using the DASB benchmark, Reconstructed Audio Evaluation (Section 3.1) using Codec-SUPERB and Versa, Acoustic LLM Evaluation (Section 3.3) using SALMon and the Zero-Resource benchmark, Tokenizer Training Ablation Study (Section 4) using ESPnet-Codec. Arora et al., 2025; Vashishth et al., 2024), often overlooking tokenization methods for music and general audio. As result, the current literature lacks unified study that covers multiple domains and diverse evaluation criteria. Moreover, rather than providing holistic comparison, most existing works focus on single aspect, such as reconstruction quality in Codec-SUPERB (Wu et al., 2024c;b), downstream task performance in DASB (Mousavi et al., 2024a), controlled evaluation settings in ESPnet-Codec (Shi et al., 2024c), or audio language modeling in SALMon (Maimon et al., 2025c). These limitations persist even in the latest surveys. For example, Guo et al. (2025b) focuses on reconstruction and voice conversion, while Cui et al. (2024); Peng et al. (2024) explores integration with LLMs. To help bridge this gap, we present comprehensive benchmark of discrete audio tokenizers. Our benchmark covers three audio domains: speech, music, and general audio. It considers multiple evaluation criteria, including signal reconstruction, downstream task performance, and acoustic language modeling. These aspects are analyzed jointly to provide more robust and comprehensive assessment. An additional issue in current benchmarks is that tokenizers are often trained under inconsistent conditions, such as different datasets, domains, or sampling rates. These inconsistencies make direct and fair comparisons difficult. To ensure fair comparisons, we support our analysis with ablation studies that examine different quantization methods under controlled experimental settings. Our contribution is organized into three core studies, as illustrated in Figures 1 and 3: Study 1: Audio Tokenizer Taxonomy (Section 2). We propose comprehensive taxonomy of discrete audio tokenization methods based on key architectural and functional criteria. 3 Figure 2: Overall architecture of standard audio tokenizer. The input signal is encoded into latent representation zt, which is then discretized by quantizer Q(). The decoder reconstructs the signal ˆx from the quantized representations ˆzt. Training typically involves combination of reconstruction (LRecon), adversarial (LGAN, LFeats), and vector quantization losses (LV Q). Study 2: Benchmark Evaluation (Section 3). We evaluate existing tokenizers using multiple benchmarks. Codec-SUPERB2 and VERSA3 (Shi et al., 2025) are used for reconstruction. DASB4 is used for downstream tasks. SALMon5 and the Zero-resource speech benchmark6 (Nguyen et al., 2020) are used for acoustic language modeling. All evaluations are conducted under consistent conditions. Study 3: Ablation Studies (Section 4). We perform controlled experiments to isolate the effects of specific design choices for training audio tokenizers, including sampling rate and single-domain versus multi-domain training using ESPnet-Code7 (Shi et al., 2024c). This survey provides unified and practical perspective on discrete audio tokenization and its role in speech, music, and general audio processing. We aim to clarify key design trade-offs, highlight current limitations, and offer guidance for future research in this evolving field."
        },
        {
            "title": "2.1 Overall architecture",
            "content": "As shown in Figure 2, audio tokenizers typically comprise three components: An encoder that converts the input waveform into sequence of frame-wise embeddings = t=1 using an encoder function fe, such that, = fe(x), where each zt RD is continuous {zt}T embedding at time step t. quantization module that maps each embedding zt to quantized vector ˆzt and set of discrete indices qt = [q1,t, . . . , qM,t] using quantization function Q, i.e. (ˆzt, qt) = Q(zt). Here, denotes the number of codebooks used in the quantizer. The full sequence of quantized embeddings is denoted as ˆZ = {ˆzt}T t=1. 2https://codecsuperb.github.io/ 3https://github.com/wavlab-speech/versa/tree/main/egs/survey. 4https://poonehmousavi.github.io/DASB-website/ 5https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ 6https://github.com/zerospeech/zerospeech2021_baseline 7https://github.com/espnet/espnet 4 decoder that reconstructs the waveform from the sequence of quantized embeddings using decoder function fd, i.e., ˆx = fd(ˆz), where ˆx is the reconstructed waveform. To address the taxonomy issues outlined in the introduction, this section proposes refined taxonomy based on three core dimensions: the quantization method, the encoder-decoder architecture, and the training paradigm (e.g., joint or end-to-end training, and the use of auxiliary components). We also provide more fine-grained categories, including streamability and the target domain of each tokenizer. The proposed taxonomy is illustrated in Figure 3 and the classification of existing audio tokenizers according to this taxonomy is shown in Table 1. The following subsection summarizes the most popular methods according to the proposed categorization. Figure 3: Taxonomy of audio tokenizers based on: encoder-decoder architecture (Section 2.3), quantization method (Section 2.2), training paradigm (Section 2.4), and target domain and streamability (Section 2.5). CNN denotes Convolutional networks, represents Transformer models, and RNN refers to any recurrent neural network including LSTM and GRU. RVQ stands for Residual Vector Quantization, GVQ for Group Vector Quantization, SVQ for Single Vector Quantization, MSRVQ stands for Multi-Scale Residual Vector Quantization, CSRVQ stands for Cross-Scale Residual Vector Quantization, PQ stands for Product Quantization, FSQ for Finite Scalar Quantization. K-Means signifies that the tokenizer is trained independently of the encoder and the decoder pipeline. Objectives include adversarial learning (GAN), diffusion-based generation (Diff), and masked prediction (MP) as generative training strategy, feature matching loss (Feats), and reconstruction loss (Recon). The interactive version of this figure can be accessed through https://dates-tokens.github.io/taxonomy_interactive.html"
        },
        {
            "title": "2.2 Quantization Method",
            "content": "Quantization is key component of the tokenization pipeline, transforming continuous frame-wise features (vectors) zt RD into discrete tokens qt = [q1,t, . . . , qM,t] and corresponding quantized vectors ˆzt RD. More formally, quantization maps data into smaller representation space with lower cardinality. It is defined as two-step procedure involving encoding and decoding, which are distinct from the encoder and decoder modules described in Figure 2. The encoder of the quantizer, denoted Eq : RD {1, . . . , K}M , maps continuous embedding zt to tuple of discrete indices qt, where each qm,t Im = {1, 2, . . . , K} corresponds to quantization layer (codebook) m. These indices refer to entries in set of codebooks = {C1, . . . , CM }, where each codebook Cm contains learnable dimensional continuous vectors. The decoder of the quantizer, Dq : {1, . . . , K}M RD, reconstructs the quantized embedding ˆzt by retrieving the selected codewords from the codebooks and combining them, typically via averaging or summation. 5 Table 1: Comprehensive overview of audio tokenizers, organized alphabetically by tokenizer name. The table covers core design choices across five major dimensions: application domains (Speech, Music, Audio), encoder-decoder architecture (including encoder/decoder type and feature representation), quantization (technique and bitrate strategy), training paradigms (objective, auxiliary loss, and joint optimization), and streaming capability."
        },
        {
            "title": "Frame",
            "content": "Encoder-Decoder"
        },
        {
            "title": "Architecture",
            "content": "Rep. Tech. Bit. Objective(s) Aux."
        },
        {
            "title": "Joint",
            "content": "EnCodec (Défossez et al., 2023) 75, 150 CNN+RNN ESC (Gu & Diao, 2024) FACodec (Ju et al., 2024) FunCodec (Du et al., 2023) HARP-Net (Petermann et al., 2021) HiFi-Codec (Yang et al., 2023a) 150 80 T-F CSRVQ CNN+RNN CNN+RNN GRVQ 1.25, 25, 50 CNN+RNN CNN+RNN T-F 44100 CNN CNN 50, 75, 100 CNN+RNN CNN+RNN APCodec (Ai et al., 2024) AudioDec (Wu et al., 2023b) Best-RQ (Chiu et al., 2022) BigCodec (Xin et al., 2024) DAC (Kumar et al., 2023) Discrete SSL (Mousavi et al., 2024b) Disen-TF-Codec (Jiang et al., 2023) dMel (Bai et al., 2024) 150 160 25 75 50 19 40 HILCodec (Ahn et al., 2024) LaDiffCodec (Yang et al., 2024e) Language Codec (Ji et al., 2024b) LFSC (Casanova et al., 2025) LLMCodec (Yang et al., 2024b) LSCodec (Guo et al., 2025a) MDCTCodec (Jiang et al., 2024) Mimi (Défossez et al., 2024) MMM (Shi et al., 2024b) NAST (Messica & Adi, 2024) NDVQ (Niu et al., 2024) PAST (Har-Tuv et al., 2025) PQ-VAE (Guo et al., 2024b) Prompt Codec (Pan et al., 2024) RepCodec (Huang et al., 2024) S-TFNet (Jiang et al., 2022a) S3 (Du et al., 2024a) SD-Codec (Bie et al., 2025) SemantiCodec (Liu et al., 2024a) Single Codec (Li et al., 2024) SingOMD (Tang et al., 2024b) 50 75 21.5 57 25, 50 12.5 50 50 75 75 75 50 - - 50 23 50 SOCODEC (Guo et al., 2024a) SoundStream (Zeghidour et al., 2021) Spectral Codecs (Langman et al., 2024) SpeechTokenizer (Zhang et al., 2024a) SQ-Codec (Yang et al., 2024d) TAAE (Parker et al., 2025) TFNet (Jiang et al., 2022b) Ti-Codec (Ren et al., 2024b) TS3-Codec (Wu et al., 2024d) USM (Zhang et al., 2023) Vocos (Siuzdak, 2024) 20 75 86. 50 50 25 120"
        },
        {
            "title": "CNN",
            "content": "CNN+T"
        },
        {
            "title": "CNN",
            "content": "- CNN+RNN CNN+RNN"
        },
        {
            "title": "CNN",
            "content": "CNN+T CNN+RNN -"
        },
        {
            "title": "CNN",
            "content": "T"
        },
        {
            "title": "CNN",
            "content": "CNN CNN CNN CNN+RNN CNN CNN CNN CNN CNN CNN+T CNN+T CNN CNN CNN CNN CNN+T CNN+T CNN+T CNN CNN+T CNN+T CNN+RNN CNN+RNN CNN+T CNN CNN CNN CNN+RNN CNN+RNN CNN CNN+RNN CNN CNN+T CNN CNN CNN CNN+T CNN CNN CNN CNN+RNN CNN CNN CNN CNN CNN CNN CNN CNN T-F T-F T T-F T-F T T,T-F T T-F T T-F T-F T T-F T-F CNN+RNN CNN+RNN T-F"
        },
        {
            "title": "SVQ",
            "content": "RVQ RVQ FSQ GRVQ RVQ RVQ RVQ FSQ SVQ RVQ RVQ K-means FSQ RVQ RVQ PQ GRVQ SVQ SVQ RVQ RVQ SVQ K-Means PQ RVQ FSQ RVQ FSQ FSQ T-F CSRVQ"
        },
        {
            "title": "RVQ",
            "content": "PQ"
        },
        {
            "title": "RVQ",
            "content": "K-means F F GAN, Feat, Rec, VQ GAN, Feat, Rec, VQ MP GAN, Feat, Rec, VQ GAN, Feat, Rec, VQ GAN, Feat, Rec, MP - - - - - -"
        },
        {
            "title": "GRVQ",
            "content": "A GAN, Feat, Rec, (Pred)"
        },
        {
            "title": "Dis",
            "content": "F F F F GAN, Feat, Rec, VQ GAN, Feat, Rec, VQ Rec, VQ GAN, Feat, Rec, VQ GAN, Feat, Rec, VQ Rec GAN, Feat, Rec, VQ GAN, Feat, Rec, VQ Diff GAN, Feat, Rec, VQ GAN, Feat, Rec MS-RVQ GAN, Rec - - - Dis SD - - - - - SD SD - - - - SST Dis SD - - - F F F F F GAN, Feat, Rec Dis, SD GAN, Feat, Rec, VQ GAN, Feat, Rec, VQ GAN, Feat, Rec, MP Rec, VQ, MP GAN, Feat, Rec, VQ SD - SD - GAN, Feat, Rec, VQ SST Rec, VQ GAN, Feat, Rec, VQ Rec, VQ , MP GAN, Rec, VQ Dif GAN, Rec,Feat, VQ Dif, VQ GAN, Rec, VQ GAN, Feat, Rec, MP F F F F F GAN, Rec, VQ Dis GAN, Rec,Feat GAN, Feat, Rec GAN, Rec,Feat, VQ GAN, Rec GAN, Feat, Rec Rec, VQ GAN, Feat, Rec, GAN, Feat, Rec, MP GAN, Feat, Rec GAN, Feat, Rec, MP GAN, Feat, Rec GAN, Rec, MP - - SD - SD -"
        },
        {
            "title": "Dis",
            "content": "- - - - - - SD - CNN+T CNN+T CNN+RNN CNN T-F GRVQ CNN+RNN CNN+RNN 40, 50 T - CNN+T"
        },
        {
            "title": "CNN",
            "content": "T T-F T-F T T"
        },
        {
            "title": "SVQ",
            "content": "PQ"
        },
        {
            "title": "SVQ",
            "content": "PQ"
        },
        {
            "title": "RVQ",
            "content": "K-means WavTokenizer (Ji et al., 2024c) 40, 75 CNN+T CNN+T Wav2Vec-BERT (Chung et al., 2021) WMCodec (Zhou et al., 2024) X-Codec (Ye et al., 2025) XEUS (Chen et al., 2024b) 25 75 50 CNN+T CNN+T"
        },
        {
            "title": "CNN",
            "content": "CNN+T -"
        },
        {
            "title": "CNN",
            "content": "- 6 SNAC (Siuzdak et al., 2024) Variable CNN+RNN MS-RVQ GAN, Feat, Rec, VQ Depending on the design choices, quantization methods vary along two important axes: (1) the specific quantization algorithm used to convert continuous features into discrete tokens, such as k-means, product quantization, or residual vector quantization (RVQ), (2) whether the bitrate is fixed or adaptive. These aspects are described in the following subsections."
        },
        {
            "title": "2.2.1 Quantization Algorithm",
            "content": "K-means. K-means clustering is frequently used for post-training quantization. While many recent codecbased acoustic tokenizers tend to adopt the joint-training quantization techniques, k-means is still prevalent in extracting tokens from SSL models (Mousavi et al., 2024b; Chang et al., 2023; Polyak et al., 2021; Wang et al., 2024e). Typically, layer or multiple layers from pretrained SSL model are selected, and representations are clustered using offline trained k-means to create discrete tokens. Such tokenizers natively lack built-in decoder, as they are primarily used for discriminative tasks like ASR. Nevertheless, recent studies have investigated training separate decoders to reconstruct speech from discrete representations, such as employing modified HiFi-GAN (Yang et al., 2023a). Additionally, Mousavi et al. (2024b) introduced multi-layer training strategy with dropout mechanisms, enabling the decoder to flexibly handle varying bitrates during inference. The assignment of each embedding zt to its nearest centroid ck is performed using the standard K-means rule: qt = arg min k{1,...,K} zt ck2 (1) Here, zt RD denotes the continuous embedding at time step from frozen SSL model, ck is the k-th cluster centroid, and qt {1, . . . , K} is the resulting discrete token index. Residual Vector Quantization (RVQ). RVQ maps each frame-wise feature to the closest entry in codebook and then refines this process by computing the residual after quantization. The remaining residual is compressed by sequentially applying series of quantizers, each refining the residuals left by the previous one. The first neural network-based RVQ method was first introduced in SoundStream (Zeghidour et al., 2021) and has since been widely adopted in other models (Kumar et al., 2023; Défossez et al., 2023; 2024; Zhang et al., 2024a). Many approaches (Kumar et al., 2023; Défossez et al., 2023) also incorporate bitrate scalability by performing variable bandwidth training, where the number of codebooks is randomly selected during training to support different bandwidths during inference. variant of RVQ, called Residual Normal Distribution Vector Quantization (RNDVQ), is used in (Niu et al., 2024). Unlike standard RVQ, which selects the nearest neighbor deterministically, RNDVQ formulates quantization as probabilistic selection problem. This addresses issues such as low codebook utilization and sensitivity to noise, making the model more robust to minor variations in input data. The procedure is defined recursively as: Algorithm 1 Residual Vector Quantization (RVQ) m=1 zt 1: Input: Embedding zt, Codebooks {C(m)}M 2: Initialize residual: r(1) 3: for = 1 to do q(m) arg mink c(m) ˆz(m) q(m) r(m) r(m+1) ˆz(m) c(m) (cid:13) (cid:13)r(m) (cid:13) (cid:13) 2 (cid:13) (cid:13) 4: 5: t 6: 7: end for 8: Output: ˆzt PM m=1 ˆz(m) Here, zt is the input embedding at time t, q(m) ˆzt is the final quantized vector produced by summing the quantized outputs from each residual stage. is the discrete index selected from the m-th codebook, and 7 Single Vector Quantization (SVQ). SVQ uses single codebook for quantization, where each framewise embedding is mapped to single code, unlike RVQ, which uses multiple codes. SVQ can be viewed as simplified form of RVQ with single codebook (i.e., =1), without iterative residual refinement, similar to VQ-VAW (Gârbacea et al., 2019). has gained popularity due to the architectural complexity introduced by multiple codebooks in acoustic language models, such as managing multiple codebook streams. SVQ, by contrast, is simpler and particularly useful for training acoustic language models. To compensate for the potential loss of information caused by using single codebook, some SVQ-based codec models adopt larger codebook sizes. Examples of SVQ-based codecs include BigCodec (Xin et al., 2024), TS3-Codec (Wu et al., 2024d), WavTokenizer (Ji et al., 2024c). Group Vector Quantization (GVQ). One limitation of RVQ is that most of the information tends to be captured in the first-layer codebook, with later codebooks contributing minimally. To address this, GVQ (Yang et al., 2023a) increases capacity at the first quantization stage by dividing the latent feature vector zt RD into non-overlapping groups: zt = z(2) z(1) . . . z(G) , (2) where each z(g) RD/G represents segment of the input feature, and denotes concatenation. Each group is quantized independently using separate RVQ module, producing group-wise quantized embedding ˆz(g) . The final quantized vector is formed by concatenating the quantized outputs from all groups: ˆzt = ˆz(2) ˆz(1) . . . ˆz(G) . (3) This grouped structure improves performance while reducing the number of required codebooks. Finite Scalar Quantization (FSQ). Unlike traditional vector quantization, FSQ maps each dimension of feature vector to fixed set of scalar values (Mentzer et al., 2024). Specifically, each feature value zt is first squashed into the range [1, 1] using non-linear function such as tanh, and then quantized into scalar latent space by computing round(zt S)/S, where is hyperparameter controlling the quantization resolution. This procedure results in 2S + 1 distinct scalar values per dimension, ensuring uniform coverage of the latent space. FSQ has been adopted in various recent models. SQ-Codec (Yang et al., 2024d) achieves this by creating scalar latent space, while Spectral Codecs (Langman et al., 2024) use FSQ to encode mel-spectrogram features into flat codebook. FSQ is often used with diffusion models for highquality audio generation. HARP-Net (Petermann et al., 2021) similarly applies FSQ but directly maps bottleneck features i.e., single scalar values rather than vectors, to set of learned scalar bins. Unlike other approaches, HARP-Net maintains the original input frame rate (44.1 kHz) by avoiding temporal decimation, instead expanding the feature dimension in intermediate layers before collapsing to scalar quantization. FocalCodec (Della Libera et al., 2025) instead uses variant of FSQ called Binary Spherical Quantization (BSQ), which relies on two scalar values. Multi-Scale RVQ (MSRVQ). MSRVQ (Siuzdak et al., 2024) extends standard RVQ by applying quantizers at different temporal resolutions. This hierarchical structure enables the model to efficiently capture both coarse and fine-grained details. The initial VQ layers operate at higher frame rates to encode fine details, while later layers work at lower temporal resolutions to refine the residuals using fewer tokens. At each stage i, the residual r(i) is downsampled by factor Wi, quantized, and then upsampled back to length : ˆz(i) = Upsample (cid:16) Q(i) (cid:16) Downsample(r(i) , Wi) (cid:17)(cid:17) (4) This strategy reduces the number of tokens while preserving essential information in the representation. Cross-Scale RVQ (CSRVQ). CSRVQ (Gu & Diao, 2024; Jiang et al., 2022a) extends RVQ by integrating multi-scale features that progressively encode coarse-to-fine information. Unlike conventional RVQ, which applies residual quantization only at single and lowest-resolution layer, CSRVQ encodes residuals between 8 encoder and decoder features at multiple hierarchical levels. During decoding, each level is conditioned on quantized residuals from coarser scales, allowing the model to refine reconstructions in coarse-to-fine manner. This structure enables the preservation of low-level detail often lost in high-level-only representations. In practice, CSRVQ can include quantization modules across different decoder layers, with each layer incorporating its own quantizer. ESC (Gu & Diao, 2024) adopts this design via hierarchical transformers and stepwise decoding, progressively improving reconstruction fidelity without requiring extra fusion networks between encoder and decoder. Product Quantization (PQ). Product quantization is commonly used in self-supervised learning (SSL) models to discretize continuous speech representations. PQ can be viewed as group of independent vector quantization modules (Chung et al., 2021; Guo et al., 2024a), partitioning embeddings into smaller subvectors and quantizing each separately. The quantized sub-vectors are then concatenated to form the final output. Other variations include Random-Projection Quantization, as seen in models like Best-RQ and USM (Chiu et al., 2022; Zhang et al., 2023). This method maps speech signals into discrete labels using randomly initialized projection matrix. Unlike other quantization methods, most PQ-based approaches do not have built-in decoder, as they are primarily designed for SSL models rather than direct waveform reconstruction."
        },
        {
            "title": "2.2.2 Fixed vs. Adaptive Bitrate",
            "content": "Depending on the system design, the bitrate of quantized representations can be either fixed or adaptive. In fixed-allocation schemes, such as those based on codebooks, the bitrate is determined by the number of bits required to represent each code index, irrespective of the actual token distribution. In contrast, adaptive bitrate refers to entropy-based coding schemes that assign variable-length codes based on the statistical frequency of tokens (Agustsson et al., 2017; Kankanahalli, 2018). More frequent tokens are encoded using fewer bits, while rarer tokens require more, leading to improved compression efficiency. Standard methods such as Huffman coding or arithmetic coding are commonly employed to exploit this redundancy. Importantly, any quantization method, regardless of its original design, can benefit from post-hoc entropy coding to further reduce the effective bitrate. It is also important to distinguish between adaptive bitrate and scalable bitrate. Adaptive bitrate dynamically adjusts the number of bits per token according to the token distribution (e.g., via entropy coding (Jiang et al., 2023; Petermann et al., 2021)). In contrast, scalable bitrate refers to systems capable of operating at multiple fixed bitrates, typically achieved by varying the number of active codebooks. This bitrate level is selected manually or defined as hyperparameter, but it remains fixed per run and does not adapt token-wise at runtime. For instance, Encodec (Défossez et al., 2023) enables scalable bitrate by employing codebook dropout strategy during training."
        },
        {
            "title": "2.3 Encoder-Decoder",
            "content": "This section describes the main encoder and decoder architectures, along with the encoder input and decoder output representations used across different designs."
        },
        {
            "title": "2.3.1 Architecture",
            "content": "Convolutional (CNN). CNN extracts and downsamples audio waveforms into lower frame-rate features using CNN layers. CNN is the most widely applied architecture among early neural codecs (Kumar et al., 2023; Zeghidour et al., 2021). CNN models are generally more compact, thus can be readily integrated with different system sizes and are especially useful in resource-constrained environments. However, CNN tokenizers cannot capture long-range dependencies. Convolutional + RNN (CNN+RNN). Some tokenizers (Défossez et al., 2023; Xin et al., 2024) combine CNN-based feature extraction with LSTMs or GRUs for sequential modeling. RNN provides mechanism for longer-range dependency than CNN, although it can still suffer from memory loss when the sequences reach certain length. RNNs can easily add algorithm and system complexity to both training and inference. Therefore, the number of RNN layers used in tokenizers is usually small, and they are combined with CNN modules. 9 Transformer (T). This category refers to the fully transformer-based models without convolutional components (Wu et al., 2024d). This type of tokenizer is less common than others. They may achieve impressive compression and reconstruction performance, but they generally demand large amount of training data and much heavier computational resources, which limit their practicality in tokenization. Convolutional + Transformer (CNN+T). This group of tokenizers (Mousavi et al., 2024b; Chiu et al., 2022; Yang et al., 2024b) uses CNN-based feature extraction followed by attention mechanisms to capture long-range dependencies. Transformers have recently started to appear in audio tokenizers, as replacement for RNN, due to their effectiveness in capturing long-range dependencies."
        },
        {
            "title": "2.3.2 Input and Output Representations",
            "content": "Encoders can process audio inputs in either the time domain or the frequency domain. In the time domain approach, raw waveforms are directly passed to the encoder. In the frequency-domain approach, precomputed mel-spectrograms or other spectral features are used as inputs. The output representation can follow two approaches: (1) time domain waveforms, where the decoder directly upsamples the discrete representation into waveforms (Défossez et al., 2023); or (2) time-frequency domain features, where the decoder outputs time-frequency domain features (Siuzdak, 2024), and the Inverse Short-Time Fourier Transform (ISTFT) is applied for upsampling. In this case, the decoded features typically have frame rate similar to that of the codec tokens. Siuzdak (2024) argues that assigning the upsampling to the ISTFT reduces the burden on the decoder and leads to better performance."
        },
        {
            "title": "2.4 Training Paradigm",
            "content": "In this section, we discuss three key dimensions of audio tokenizer training: the training strategy, the main training objectives used to optimize the model, and the auxiliary components that further enhance the learned representations."
        },
        {
            "title": "2.4.1 Training Strategies",
            "content": "Training strategies for audio tokenizers can be categorized into two broad approaches: separate (post-training) and joint (end-to-end training). These differ in how the encoder, quantizer, and decoder modules are optimized in relation to each other. In both cases, the encoder may be randomly initialized or initialized using pretrained model, such SSL model (e.g., wav2vec 2.0 (Baevski et al., 2020), HuBERT (Hsu et al., 2021), WavLM (Chen et al., 2022)) or an ASR model (Radford et al., 2023)). Separate (Post-Training). In this approach, the encoder and decoder are optimized independently from the quantization module. This is common in semantic tokenizers and earlier neural codecs. The encoder is often initialized from pretrained SSL model such as wav2vec 2.0 (Baevski et al., 2020), HuBERT (Hsu et al., 2021), or WavLM (Chen et al., 2022), and typically kept frozen during quantizer training (Lakhotia et al., 2021; Mousavi et al., 2024b; Shi et al., 2024b). The quantizer is trained offline using methods such as k-means clustering on latent representations. Discrete SSL (Lakhotia et al., 2021; Mousavi et al., 2024b; Shi et al., 2024b), for example, uses k-means clustering to quantize the latent semantic features after pretraining. LPCNet-based codecs8 (Valin & Skoglund, 2019a;b; Yang et al., 2023c) use combination of scalar quantization and multi-stage vector residual quantization on the pre-extracted features (pitch and cepstra) with k-means, involving different levels of feature predictions. µ-law is also popular quantization technique used in earlier autoregressive vocoders (van den Oord et al., 2016), and some neural audio codecs (Kleijn et al., 2018). Decoders are then trained independently to reconstruct waveforms or features from discrete tokens, often using HiFi-GAN (Polyak et al., 2021; Kong et al., 2020) or diffusion models (Du et al., 2024b; Zeng et al., 2024). Joint (End-to-End Training). In joint training, the encoder, quantizer, and decoder are optimized simultaneously within unified end-to-end framework. This approach is commonly adopted by acoustic 8https://github.com/xiph/LPCNet 10 tokenizers (Zeghidour et al., 2021; Défossez et al., 2023). The full model is optimized using combination of reconstruction losses (e.g., MSE) and often adversarial losses (Goodfellow et al., 2020) to promote both signal fidelity and perceptual quality. To address the non-differentiability of quantization, several gradient approximation techniques are used: (1) straight-through estimators (van den Oord et al., 2017), which copy gradients across the quantizer; (2) soft-to-hard quantization with annealing (Agustsson et al., 2017; Kankanahalli, 2018); and (3) Gumbel-softmax relaxation (Jang et al., 2017; Maddison et al., 2017). Joint training also allows for incorporating auxiliary objectives (see Section 2.4.3) to improve downstream task utility, robustness, or bitrate flexibility (Niu et al., 2024; Kumar et al., 2023)."
        },
        {
            "title": "2.4.2 Main Training Objectives",
            "content": "Audio tokenizers are optimized using different main objectives, depending on the targeted application, as depicted in Figure 2. Reconstruction (Recon). The most common objective for training audio tokenizers is to reconstruct the original audio input from discrete tokens. This is achieved using regression loss, such as the mean squared error (MSE) or mean absolute error (MAE) between the input and the reconstructed output ˆx (Défossez et al., 2023; Zeghidour et al., 2021): LRecon ="
        },
        {
            "title": "T\nX",
            "content": "t=1 xt ˆxt2 . (5) Vector Quantization (VQ). In the straight-through estimator (van den Oord et al., 2017) used for vector quantization, gradients bypass the codebook, requiring additional losses to align the embeddings with the encoder outputs. One example is the soft-to-hard scheme (Agustsson et al., 2017), where quantization loss is applied during training to encourage the softmax-based quantization approximation to closely match the original continuous representation z: LV = ˆz, ˆz ="
        },
        {
            "title": "M\nX",
            "content": "m=1 αm cm, (6) where denotes the total number of codebooks, cm is the continuous representation that corresponds to the mth codebook (as defined in Section 2.2), and αm are their corresponding softmax weights. Another example is the commitment loss, which encourages the encoder outputs to align with the selected and its quantized counterpart ˆz(m) codebook embeddings. The loss is computed between each residual z(m) from the m-th codebook, with gradients blocked from flowing through the quantized values: t LVQ ="
        },
        {
            "title": "M\nX",
            "content": "t=1 m=1 (cid:13) (cid:13)z(m) (cid:13) sg ˆz(m) i(cid:13) 2 (cid:13) (cid:13) , (7) where sg[] denotes the stop-gradient operator. This loss penalizes discrepancies between the encoder outputs and their corresponding quantized embeddings while ensuring that gradients do not update the codebook entries directly. Modern approaches often replace this loss with Exponential Moving Average (EMA) updates for the codebook, which improve training stability and mitigate codebook collapse. Adversarial (GAN). Acoustic tokenizers often apply adversarial losses to improve perceptual quality. discriminator network is trained to distinguish between real signals and reconstructed signals ˆx, while the tokenizer (generator) is optimized to fool the discriminator. The adversarial loss is defined as hinge loss over the logits of the discriminator, averaged over multiple discriminators and over time. The generator loss is: LG ="
        },
        {
            "title": "K\nX",
            "content": "k=1 max(1 Dk(ˆx), 0) (8) 11 The discriminator loss is: LD ="
        },
        {
            "title": "K\nX",
            "content": "k=1 [max(1 Dk(x), 0) + max(1 + Dk(ˆx), 0)] (9) where Dk() denotes the output of the k-th discriminator. To improve the quality of the generated audio, multiple discriminators are commonly used, including multi-resolution and multi-scale short-time Fourier transform (STFT) discriminators from neural vocoder research (Kong et al., 2020; Défossez et al., 2023; Zeghidour et al., 2021). Perceptual losses are applied using variety of discriminators, including multi-scale STFT-based (MS-STFT), multi-period (MPD), and multi-scale (MSD) variants. Feature Matching (Feat). To encourage the original and reconstructed signals to exhibit similar abstractions (or to align closely in the latent space), stabilize adversarial training, and encourage more natural reconstructions, feature-matching loss is often applied. This loss compares intermediate activations from the discriminator for real and reconstructed signals. It is defined as: LFeats ="
        },
        {
            "title": "L\nX",
            "content": "k=1 l=1 k(x) Dl Dl mean(Dl k(ˆx)1 k(x)1) , (10) where is the number of discriminators, is the number of layers in each discriminator, and Dl k() denotes the output of the l-th layer of the k-th discriminator. Feature matching encourages the generator to match higher-level statistics of real signals, improving stability and perceptual quality. Diffusion (Diff). Diffusion loss is used when the decoder is modeled as conditional denoising diffusion process. diffusion model (Rombach et al., 2022) progressively adds noise ϵt to the latent representation zt during the forward process. conditioned neural network, parameterized by θ, is trained to predict the noise at each timestep. The training objective minimizes the expected difference between the true noise ϵt and the network prediction ϵθ(zt, t, zq), conditioned on the discrete tokens zq: Ldiffusion = Ez0,t,zq [ϵt ϵθ(zt, t, zq)] , (11) where zq represents the discrete conditioning tokens provided during both training and generation. This approach enables the model to recover acoustic features directly from discrete tokens (Yang et al., 2024e; San Roman et al., 2023; Du et al., 2024b). Masked Prediction (MP). Masked prediction loss is commonly used in tokenizers where the encoder and decoder are trained separately. The encoder is trained to predict masked portions of the input sequence, typically capturing phonetic information rather than reconstructing the full waveform. Following the masked language modeling (MLM) paradigm (Devlin et al., 2019), portion of the input is randomly masked, and the model is optimized to predict the masked frames from the surrounding context. This approach is widely used in speech pretraining models such as HuBERT (Hsu et al., 2021) and WavLM (Chen et al., 2022) and is adopted in several semantic tokenizer designs (Lakhotia et al., 2021; Mousavi et al., 2024b)."
        },
        {
            "title": "2.4.3 Auxiliary Components",
            "content": "Beyond the main training objectives, neural audio tokenizers often combine auxiliary components to enhance generalization, improve representation learning, and refine specific features. These auxiliary components fall into three categories: disentanglement, semantic distillation, and supervised semantic tokenization9. Disentanglement. Disentanglement methods separate different speech attributes into distinct representations, reducing redundancy while allowing independent control over acoustic properties and simplifying 9We here inherit the original terminology from the referenced papers (i.e., semantic distillation, and supervised semantic tokenization) . However, it is important to note that both methods typically extract or learn information from SSL features, which predominantly encode phonetic information. 12 downstream tasks. One line of disentanglement focuses on separating speech and background audio in the codec embedding space, enabling better bitrate, entropy control (Yang et al., 2021) or speech enhancement (Omran et al., 2023). Another line separates the conceptual and fundamental components of speech. Early attempts obtained efficient and low-bitrate speech coding through speaker and phoneme disentanglement, utilizing separate training (Polyak et al., 2021) or joint training (Jiang et al., 2023). More recently, TiCodec (Ren et al., 2024b) minimizes token usage by separately quantizing time-invariant global embeddings (e.g., timbre) and time-varying features (e.g., phonetic information). FACodec (Ju et al., 2024) decomposes speech into subspaces such as content, prosody, timbre, and acoustic details through supervised techniques. The timbre extractor in FACodec is optimized with speaker classification loss, while distinct RVQ modules process other components before supervised decoupling. LSCodec (Guo et al., 2025a) introduces lowbitrate, speaker-decoupled speech codec using three-stage training framework with speaker perturbation. VQ layer is applied after VAE that disentangles speaker attributes in continuous space, followed by training token vocoder on the quantized codes. Unlike most acoustic tokens that redundantly encode speaker timbre across time steps, LSCodec minimizes this inefficiency by isolating timbre from content and prosody. SoCodec (Guo et al., 2024a) employs multi-stream phonetic sequences and ordered product quantization to encode speech into phonetic and time-variant token sequences using HuBERT as pretrained SSL model. An ECAPA-TDNN-based encoder (Desplanques et al., 2020) extracts an utterance-level global embedding to retain time-invariant information, such as speaker identity, global speaking style, and acoustic environment. SD-Codec (Bie et al., 2025) integrates audio coding with source separation by assigning different audio domains (such as speech, music, and sound effects) to distinct codebooks using multiple parallel RVQ modules. Semantic Distillation. Semantic distillation enhances codec representations by incorporating phonetic information into specific codebooks. Various approaches have been explored to distill phonetic knowledge into tokenization while maintaining good reconstruction. Pretrained model guidance is common approach, where models like SpeechTokenizer (Zhang et al., 2024a), X-Codec (Ye et al., 2025), and Mimi (Défossez et al., 2024) use SSL features to guide specific RVQ layers to learn information from such SSL features. This distillation is implemented by applying regression or classification loss on the first RVQ output to align it with continuous SSL embeddings or discrete SSL tokens. In this way, the first RVQ layers are trained to learn more phonetic information, while later layers focus on acoustic details. Another method injects semantic knowledge directly into the quantizer codebook. LLM-Codec (Yang et al., 2024b) follows this approach by initializing codebooks with token embeddings from LLaMa2 (Touvron et al., 2023) and keeping them frozen during training. This strengthens the ability of the codec to encode meaningful linguistic representations. Some models integrate semantic features into the encoder-quantizer pipeline by combining pretrained SSL representations with acoustic features through concatenation. SemantiCodec (Liu et al., 2024a) and X-Codec (Ye et al., 2025) adopt dual-encoder-decoder architecture to process SSL semantic tokens independently from acoustic features. Supervised Semantic Tokenization. Some tokenizers explicitly capture phonetic detail through supervised training. For example, Supervised Semantic Speech (S3) (Du et al., 2024a;b) employ single-codebook VQ layer and FSQ, positioned between two transformer encoder modules. Recently, Har-Tuv et al. (2025) proposed adding phonetic classification auxiliary loss over the first codebook of the RVQ. These models optimize representations using an automatic speech recognition (ASR) loss. Additionally, they utilize optimaltransport conditional flow matching (OT-CFM) (Tong et al., 2024) to model and generate Mel spectrogram distributions conditioned on the produced discrete speech tokens. These supervised approaches produce discrete tokens that effectively preserve phonetic information, making them more aligned with content information and suitable for understanding tasks in speech LMs (Zeng et al., 2024)."
        },
        {
            "title": "2.5 Streamability and Domain Categorization",
            "content": "Beyond architecture and training paradigms, audio tokenizers also differ in their support for streaming and their domain of application. 13 Table 2: Audio tokenizers, their characteristics, and abbreviations used throughout the study. As abbreviations, we denote tokenizers as [name]-[domain(s)]-[sample rate]."
        },
        {
            "title": "Domain",
            "content": "SR Frame #Codes Params MACs Link"
        },
        {
            "title": "Speech Music Audio",
            "content": "(kHz) Rate (Mil)"
        },
        {
            "title": "DAC",
            "content": "Enc-SMA-24 Enc-M-32 Enc-A-16 DAC-SMA-44 DAC-SMA-24 DAC-SMA-"
        },
        {
            "title": "SpeechTokenizer",
            "content": "ST-S-"
        },
        {
            "title": "Mimi",
            "content": "Mimi-S-24 Discrete-WavLM DWavL-S-16 SQ-Codec SQ-SMA-"
        },
        {
            "title": "WavTokenizer",
            "content": "WT-SMA-24 WT-S-24 24 32 16 24 16 16 24 16 24 24 75 50 50 75 50 50 12.5 50 75 40 1024 2048 2048 1024 1024 1024 2048 1000 4096 4096 14.9 56.9 56.8 76. 74.7 74.1 103.7 79.3 331.9 23. 80.6 80.9 (G) 6.1 14.4 14. 147.0 83.4 55.6 17.1 8.1 21. 14.7 6.3 3."
        },
        {
            "title": "Link",
            "content": "Streamability. Streamability refers to the ability of tokenizer to process and generate audio in real-time with minimal latency, using little or no future context. This property is critical for low-latency applications such as real-time communication or streaming. Latency can be analyzed from two main perspectives: Algorithmic latency, determined by the look-ahead windowi.e., how much future information is needed to compute the current frame. CNN-based models (Défossez et al., 2023) support streamability via causal convolutions, while Transformer-based (Wu et al., 2024d) models require causal attention mechanisms. Computational complexity, which becomes especially important when deploying models on resource-constrained systems like mobile or edge devices. Traditional and early neural audio codecs generally maintain low complexity for real-time feasibility. For instance, LPCNet (Valin & Skoglund, 2019b) achieves real-time performance with fewer than 2M parameters at 1.6 kbps. In contrast, more recent models like Encodec scale up to 14M parameters to support 1.5 kbps, while BigCodec pushes further to 159M parameters at just 1.04 kbps to improve quality at low bitrates. Many SSL-based tokenizers rely on non-causal encoders, which limits their use in real-time settings. Thus, achieving streamability with high-quality and efficient causal architectures remains an open research challenge. Target Domain. Some models (Xin et al., 2024; Zhang et al., 2024a; Mousavi et al., 2024b; Défossez et al., 2024) are specifically designed for speech tasks such as ASR and TTS. Others are optimized for music (Petermann et al., 2021; Tang et al., 2024b) generation and enhancement, capturing tonal and harmonic structures. Some tokenizers (Yang et al., 2024b) are designed for general audio, including environmental sounds and non-speech signals. few models (Ji et al., 2024c; Défossez et al., 2023; Kumar et al., 2023) are trained to handle multiple domains."
        },
        {
            "title": "3 Benchmark Evaluation",
            "content": "Given the wide range of available tokenizers, researchers and practitioners may wonder which existing tokenizers are best suited for given use case. This depends not only on the expected performance for 14 given task but also on computational efficiency and, in some cases, additional factors such as streamability or the ability to generalize across diverse domains. Several benchmarks have been proposed to evaluate audio tokenizers, offering some guidance on which tokenizers are best suited for different applications and tasks (Wu et al., 2024c; Mousavi et al., 2024a; Shi et al., 2024c; Maimon et al., 2025c). Nevertheless, drawing solid insights from current benchmarks is challenging, as each focuses on specific aspect or domain and holistic comparison of audio tokenizers is missing. Furthermore, while each existing benchmark is internally consistent in its evaluation protocol, they differ significantly in the set of tokenizers they consider, some focus exclusively on acoustic models, while others evaluate semantic tokenizers or even different configurations of the same model (e.g., EnCodec-16k vs. EnCodec-24k). This lack of alignment makes it difficult to derive unified or comparable conclusions across studies. This section contributes to filling this gap by considering diverse set of publicly available, pre-trained tokenizers across speech, music, and general audio tasks. Unlike previous benchmarks, we perform joint evaluation across multiple dimensions: 1. Reconstruction Evaluation and Complexity Analysis. We assess the quality of resynthesized audio using the original decoder trained for each tokenizer, following protocols from CodecSUPERB and VERSA. We also evaluate the computational efficiency of each tokenizer based on model size (parameters), frame rate, token rate, and multiply-accumulate operations (MACs). 2. Downstream Evaluation. We assess the effectiveness of tokenized representations when used directly as input to lightweight models for both discriminative and generative tasks using DASB benchmark. 3. Acoustic Language Modeling. We analyze the effectiveness of each tokenizer in training acoustic language models, using the SALMon and Zero-resource benchmarks. summary of all tokenizers included in the benchmark evaluation is provided in Table 2. We utilize pretrained checkpoints released by the original authors. An overview of our benchmark evaluation pipeline is illustrated in Figure 1."
        },
        {
            "title": "3.1 Evaluation for Reconstructed Audio Quality and Complexity",
            "content": "Background. We evaluate audio reconstruction quality and examine key properties of audio tokenizers, such as computational complexity, bitrate, and token rate. Reconstruction quality is particularly important for applications like transmission, where preserving signal fidelity is crucial. Moreover, high reconstruction quality might be useful proxy for selecting effective tokenizers for downstream tasks, especially those that rely directly on the decoder for audio generation, such as speech enhancement and source separation. In such cases, the reconstruction performance of the tokenizer can impact the overall task performance. Experimental Setup. The input audio is first compressed using an audio tokenizer and then resynthesized through its corresponding decoder. The resynthesized audio is assessed from both signal-level and application-level perspectives, providing insights into how well each tokenizer preserves information. To ensure comprehensive analysis, we integrate the evaluation scripts from Codec-SUPERB and VERSA (Wu et al., 2024c;b; Shi et al., 2024c; 2025). We extend the evaluation across three domains (music, general audio, and speech) to examine how different tokenizers perform in diverse acoustic scenarios. Dataset. For speech evaluation, we use the LibriSpeech test-clean set (Panayotov et al., 2015). For music, we use the MUSDB dataset (Rafii et al., 2017), which consists of approximately 10 hours of full-length and professionally-recorded musical tracks at 44.1kHz. Lastly, for general audio we opt for the Audioset (Gemmeke et al., 2017) test-set, which accounts for approximately 55 hours of audio clips extracted from YouTube. Evaluation Setup. Table 3 reports the reconstruction metrics for resynthesized audio. Beyond quality, it is important to jointly consider factors often overlooked in the literature, such as computational efficiency and tokenizer properties. To address this, we also include these metrics as Table 2: (1). Model parameters (Params): The total number of parameters in the audio tokenizer. (2). Computational complexity (MACs): Number of arithmetic operations performed by tokenizer. MACs are computed for one-second audio sample using PyFlops10. For components incompatible with PyFlops, such as streaming self-attention, dot 10https://github.com/sovrasov/flops-counter.pytorch/tree/master Table 3: Summary of evaluation metrics on resynthesized audio."
        },
        {
            "title": "Speech Music Audio",
            "content": "Signal-level"
        },
        {
            "title": "SDR",
            "content": "SI-SNR"
        },
        {
            "title": "UTMOS",
            "content": "Signal-to-distortion Ratio Scale-invariant signal-to-noise ratio"
        },
        {
            "title": "Perceptual Evaluation of Speech Quality",
            "content": "UTokyo-SaruLab System for VoiceMOS 2022 DNSMOS P808 Deep Noise Suppression MOS Score of P.808 DNSMOS P835 Deep Noise Suppression MOS Score of P."
        },
        {
            "title": "Spk Sim",
            "content": "Packet Loss Concealment-focus MOS Short-Time Objective Intelligibility"
        },
        {
            "title": "Singing voice MOS",
            "content": "Application-level Word Error Rate (beam=5)"
        },
        {
            "title": "Speaker Similarity",
            "content": "[0, inf) [-1, 1] (-inf, inf) (-inf, inf) [1, 5] [1, 5] [1, 5] [1, 5] [1, 5] [0, 1] [1, 5] [1, 5] Table 4: Reconstruction performance of audio tokenizers (speech)."
        },
        {
            "title": "Tokenizer",
            "content": "#Q kbps Token"
        },
        {
            "title": "SDR",
            "content": "SI-"
        },
        {
            "title": "PESQ UTMOS DNSMOS DNSMOS PLCMOS STOI WER Spk",
            "content": "rate SNR P808 P"
        },
        {
            "title": "Ground truth",
            "content": "Enc-SMA-24 DAC-SMA-24 ST-S-16 Mimi-S-24 DWavL-S-16 SQ-SMAWT-SMA-24 WT-S-24 - 2 8 2 8 32 2 8 32 2 6 4 1 - 1.5 6 24 1.5 24 1 4 1.1 4.4 3 3 .98 .52 - 290. 55.92 4.64 4.09 3.84 150 2400 150 600 2400 100 100 400 100 300 200 40 0.82 6.50 9.75 -0.57 1. 2.20 1.53 4.83 7.90 8.40 9. 9.47 -7.10 14.46 3.01 3.43 9.32 0. 1.19 7.45 -13.96 37.23 -12.69 35.43 1.91 2. 0.17 8.61 0.79 3.16 1.56 2. 3.71 1.48 3.40 4.45 1.21 2. 2.22 3.38 1.13 1.19 3.31 1. 2.05 1.58 3.09 3.74 1.68 3. 4.05 2.32 3.84 3.60 3.92 3. 3.32 3.90 3.77 3.89 3.21 3. 3.74 3.24 3.69 3.78 3.37 3. 3.68 3.74 3.68 3.72 3.83 3. 3.82 3.18 2.39 2.96 3.19 2. 3.16 3.20 2.78 3.17 3.17 3. 3.13 3.13 3.28 3.18 3.27 4.16 3.44 4.08 4.29 3.27 4. 4.40 2.96 4.00 4.27 4.40 3. 4.05 4.13 4.41 4.38 Sim 1.00 0.85 0.94 0.97 0. 0.95 0.99 0.77 0.92 0.90 0. 0.75 0.75 0.96 0.87 0.89 2. 5.44 2.78 2.77 9.59 3.53 2. 4.20 2.41 3.72 2.96 4.97 4. 1.00 0.42 0.72 0.78 0.45 0. 0.80 0.35 0.86 0.70 0.85 0. 0.35 2.37 0.87 8.10 8.91 0. 0.61 product, calculations are performed manually. (3). Bitrate: The number of bits per second, representing balance between audio quality and compression efficiency. (4). Frame rate: The number of temporal frames used to encode one second of audio. (5). Token rate: Number of tokens required to encode one second of audio, an important factor for acoustic language modeling applications. Results and Discussion. 16 Speech. From Table 4, we make the following observations: (1) For both EnCodec and DAC, the reconstruction quality consistently degrades as the bitrate decreases from 24k to 6k and 1.5k. This trend confirms that higher bitrates better preserve acoustic detail, resulting in improved reconstruction quality across all evaluated metrics. (2) For SpeechTokenizer (4k vs. 1k) and Mimi (4.4k vs. 1.1k), which both apply semantic distillation to the first codebook, all objective metrics decline at lower bitrates. However, the WER does not drop as drastically, indicating that semantic distillation effectively preserves linguistic content even when the overall reconstruction quality decreases. (3) Discrete WavLM exhibits significantly lower SDR, SI-SNR, PESQ, STOI, and Spk-Sim scores. Since these metrics rely on reference ground truth signals, the poor performance indicates these models are not optimized for precise waveform reconstruction. Metrics such as UTMOS, DNSMOS, and PLCMOS, however, remain reasonable, suggesting these tokenizers still preserve speech quality. This discrepancy indicates that discrete tokenizers focus more on high-level representations than on exact waveform reconstruction. (4) SQ-SMA-16 performs comparable or even better than large bitrate codec models (e.g., Mimi-S-24 4.4kbps, and DAC-SMA-24 6kbps). (5) Finally, we find that SDR and SI-SNR are less reliable indicators. possible reason is that the signal is over-compressed, the generation of neural codec (especially in low-bitrate), usually have less consistency in the local sample-level information. It is likely due to non-linear shifts or amplitude variations. General Audio and Music. Table 5 summarizes the reconstruction results for the general audio and music domains. As in the speech domain, reconstruction quality generally decreases with lower bitrates. The results also reveal notable trends related to both the training domain and optimization objectives for each tokenizer. EnCodec achieves the best overall reconstruction performance across SDR, SI-SNR, and perceptual metrics (VISQOL and SingMOS), particularly at higher bitrates. In contrast, DAC shows surprisingly poor performance in time-domain metrics, with negative SI-SNR values in most settings. This suggests that DAC relies on adversarial or perceptual optimization strategies that do not prioritize time-domain reconstruction loss. Nonetheless, despite poor time-domain fidelity, DAC maintains strong VISQOL and SingMOS scores, indicating its reconstructions remain perceptually plausible. Similar to DAC, WavTokenizer is not explicitly optimized with time-domain waveform reconstruction loss, resulting in poor SDR and SI-SNR scores. However, its perceptual metrics (VISQOL and SingMOS) remain relatively strong. This further highlights the limitations of time-domain metrics in evaluating token-based representations, as previously observed in the speech reconstruction results. SQ-Codec, the only fully outof-domain model (trained only on speech data), gives the poorest performance in SDR and SI-SNR metrics. Nevertheless, its perceptual metrics remain comparable to in-domain models at certain bitrates, indicating that despite limited signal fidelity, key perceptual characteristics can still be preserved. Summary. Overall, these results underscore the importance of evaluating audio tokenizers beyond traditional waveform fidelity measures. Models optimized for perceptual or downstream tasks may exhibit low signal reconstruction performance, yet still produce subjectively high-quality audio reconstructions."
        },
        {
            "title": "3.2 Downstream Evaluation",
            "content": "Background. Evaluating token quality solely based on reconstruction performance raises an important question: how much task-relevant information is preserved in the tokens, independent of the decoders capacity? This distinction is critical in multimodal language modeling settings, where audio tokens are used directly as input to large language models. These models must perform both discriminative tasks (e.g., ASR, emotion recognition) that map audio to text, and generative tasks (e.g., speech synthesis, speech-to-speech translation) that output audio. Experimental Setup. We evaluate discrete audio tokenizers using the DASB benchmark (Mousavi et al., 2024a), built on the SpeechBrain toolkit (Ravanelli et al., 2024), which isolates the representational quality of the tokens for downstream modeling. For each task, the encoder is frozen, and task-specific classification head is trained. We use lightweight classification heads to avoid hiding weaknesses in the token representations. Generative tasks additionally use the frozen decoder. All token embeddings are projected to fixed dimensionality of 1024 to ensure consistency across models. This value corresponds to the largest embedding size among the tokenizers in our benchmark. For tokenizers with multiple codebooks, weighted sum of 17 Table 5: Reconstruction performance of audio tokenizers for both general audio and music experiments."
        },
        {
            "title": "Tokenizer",
            "content": "#Q kbps Token"
        },
        {
            "title": "SDR",
            "content": "CISI-"
        },
        {
            "title": "SDR",
            "content": "CISI-"
        },
        {
            "title": "Sing",
            "content": "rate SDR SNR MOS SDR SNR MOS"
        },
        {
            "title": "Ground truth",
            "content": "Enc-SMA-24 DAC-SMA-24 SQ-SMA-16 WT-SMA-24 WT-S-24 - 2 8 32 2 8 4 1 1 - 1.5 24 1.5 6 24 3 . .52 - 252.75 84.90 57.96 4. 2.70 254.24 87.26 60.26 4.73 2. 150 600 2400 150 600 1.29 1.28 4.31 4.28 7.72 4.10 7. 2.33 5.64 2.60 2.55 11.55 1.35 2. 1.22 10.28 2.22 9.91 200 2.33 2.33 10.50 40 4.55 4.45 9.78 11.00 10.85 20.91 3. 4.25 4.36 3.99 4.35 4.59 4. 3.96 3.85 2.59 2.60 2.60 2. 2.61 2.60 2.62 2.56 2.53 2. 7.32 2.13 7.17 11.04 10.75 1. 4.67 5.37 1.75 4.82 5.56 3. 0.46 5.87 9.19 2.21 1.25 1. 3.39 0.38 14.30 14.28 23.09 19.91 19.89 45.55 4.05 4. 4.50 3.94 4.30 4.56 4.34 3. 3.33 2.67 2.66 2.66 2.70 2. 2.66 2.68 2.60 2.42 Table 6: Datasets, metrics, and downstream models for the DASB evaluation."
        },
        {
            "title": "Architecture",
            "content": "Metric(s)"
        },
        {
            "title": "Data Link",
            "content": "Speech (Discriminative) ASR (En) LibriSpeech (Korvas et al., 2014)"
        },
        {
            "title": "Branchformer",
            "content": "ASR (Low-resource) CommonVoice 17.0 (Ardila et al., 2020)"
        },
        {
            "title": "BiLSTM",
            "content": "Speaker ID / Verification VoxCeleb1 (Nagrani et al., 2017)"
        },
        {
            "title": "Emotion Recognition",
            "content": "IEMOCAP (Busso et al., 2008)"
        },
        {
            "title": "Keyword Spotting",
            "content": "Speech Commands (Warden, 2018) ECAPA-TDNN ECAPA-TDNN ECAPA-TDNN"
        },
        {
            "title": "Intent Classification",
            "content": "SLURP (Bastianelli et al., 2020) BiLSTM+Linear"
        },
        {
            "title": "WER",
            "content": "Accuracy / EER"
        },
        {
            "title": "Accuracy",
            "content": "Speech (Generative)"
        },
        {
            "title": "Speech Enhancement",
            "content": "VoiceBank (Valentini-Botinhao et al., 2016)"
        },
        {
            "title": "Conformer",
            "content": "DNSMOS / dWER"
        },
        {
            "title": "Speech Separation",
            "content": "Libri2Mix (Cosentino et al., 2020)"
        },
        {
            "title": "Conformer",
            "content": "DNSMOS / dWER / SpkSim"
        },
        {
            "title": "Music",
            "content": "Music Genre Classification GTZAN (Tzanetakis & Cook, 2002) ECAPA-TDNN"
        },
        {
            "title": "Music Source Separation",
            "content": "MUSDB (Rafii et al., 2017)"
        },
        {
            "title": "Conformer",
            "content": "SDR / SIR / SAR Sound Event Classification ESC-50 (Piczak, 2015)"
        },
        {
            "title": "Audio Separation",
            "content": "FUSS (Wisdom et al., 2021) ECAPA-TDNN"
        },
        {
            "title": "Link",
            "content": "codebook embeddings is computed, with weights learned jointly with the downstream head (Chen et al., 2022; Zaiem et al., 2023). For SQ-Codec, which uses scalar quantization and group vector quantization, we apply ternary matrix-based embedding and concatenate four 256-dimensional group vectors to match the 1024-dimensional standard. Each tokenizer is evaluated across multiple bitrate settings (low, high, and recommended). We tune the most relevant hyperparameters, such as learning rate and model capacity, using the Tree-structured Parzen Estimator (TPE) (Bouthillier et al., 2022) with 20 trials. To obtain more robust performance estimate, we average the results of each tokenizer over three downstream training runs with different random seeds. For ASR tasks, both character-level and byte pair encoding (BPE) segmentations are considered, and the better-performing configuration is reported. Table 6 summarizes the benchmark tasks and their corresponding downstream models. When multiple-domain tokenizer checkpoints are available, we use the multi-domain version for consistency. The impact of domain-specific vs. multi-domain training is further analyzed in our ablation study (Section 4). Additional implementation details are provided in the DASB paper (Mousavi et al., 2024a). 18 Dataset. We evaluate audio tokenizers across diverse tasks and domains, including speech discriminative tasks such as ASR, low-resource ASR (L-R ASR), speaker identification and verification (SID, SV), emotion recognition (ER), intent classification (IC), and keyword spotting (KS). For generative tasks, we include speech enhancement (SE) and speech separation (SS). In the music and general audio domains, we evaluate music genre classification (MG), music source separation (MSS), general sound separation (ASS), and sound event classification (SEC). full summary of datasets and tasks is provided in Table 6. Evaluation Setup For continuous baselines, we follow Zaiem et al. (2023) by using weighted sum of WavLM-large layers as input across most tasks. To ensure fair comparison between discrete and continuous representations, we adopt identical downstream architectures for both settings. While neither WavLM nor the chosen downstream architecture may represent the state-of-the-art for every task, using consistent setup across all experiments allows us to isolate the effect of representation quality. For instance, in speech separation, well-established baselines such as Conv-TasNet (Luo & Mesgarani, 2019) and Transformer-based models (Saijo et al., 2024) are excluded. Libri2Mix has become saturated benchmark, with many approaches reaching near-ceiling performance, and adding stronger backbones would not yield meaningful insights. Instead, our focus is on isolating the effects of discrete versus continuous representations under shared architecture. There are two exceptions: for music and general audio separation, tasks that remain more challenging, we use stronger, task-specific architectures for the continuous baselines. Specifically, we adopt DEMUCS (Rouard et al., 2023) for music source separation and TDCN++ (Kavalerov et al., 2019) for general sound separation, as these models are better suited to the complexity of these domains. We evaluate each task using standard, task-specific metrics  (Table 6)  : ASR is evaluated using Word Error Rate (WER); SV uses Equal Error Rate (EER); classification tasks including ER, SID, IC, MG, and SEC are evaluated using classification accuracy (ACC). For SE and SS, we report DNSMOS (Reddy et al., 2022) for perceptual audio quality, differential WER (dWER) using Whisper (Radford et al., 2023) for intelligibility, and speaker similarity (SpkSim) based on cosine similarity of WavLM-derived embeddings. Music source separation is evaluated using signal-to-distortion ratio (SDR), signal-to-artifact ratio (SAR), and signal-to-interference ratio (SIR) via the BSSEval toolkit (Vincent et al., 2006), while general sound separation is assessed using SDR on FUSS (Wisdom et al., 2021). All results are averaged over three runs with different random seeds to ensure robustness. Results and Discussion Tables 7, 8, and 9 summarize performance across discriminative and generative tasks for speech, music, and general audio. Below, we outline key findings from our experiments. Speech Tasks. Discrete WavLM consistently performs best in discriminative tasks, likely due to its strong ability to preserve phonetic content. SpeechTokenizer, which uses semantic distillation, ranks second. In speaker recognition, however, DAC achieves the best results, suggesting that reconstruction-based objectives help preserve speaker identity. For speech separation and enhancement, WavLM performs well at low and medium bitrates but shows poor results in speaker similarity metrics. This aligns with previous findings (van Niekerk et al., 2022) that SSL-based tokenizers tend to lose speaker-related information. Another notable observation is that in many cases, the reconstructed DNSMOS score, representing the upper bound set by the codec alone without any separation, does not surpass the score obtained by using the raw mixture as the estimate (i.e., the lower bound), suggesting that limitations in reconstruction quality may constrain downstream performance, particularly for high-fidelity tasks like speech separation. Audio and Music Tasks. For general audio and music tasks, EnCodec consistently outperforms other tokenizers across all bitrates and domains, while DAC lags behind. Although DAC is known for strong perceptual quality, its signal-level fidelity is generally lower, which likely impacts its separation performance. The difficulty of these tasks is evident from the SI-SDR of the unprocessed mixtures, for example, approximately -16 dB for general audio and -7.7 dB for music. Even the best-performing model (EnCodec at medium bitrate) only reaches about -7 dB SI-SDR for audio and -5.7 dB for music. While high-bitrate settings have proven to be challenging for downstream tasks, they perform particularly poorly in music separation, emphasizing that increasing bitrate alone does not improve separation quality and may even degrade performance. This may be due to the inherently polyphonic and less sparse nature of music (in contrast to speech and general audio), which results in highly overlapping sources that are harder to disentangle from detailed but semantically entangled representations. 19 Impact of Codebook Size. Increasing the number of codebooks (e.g., 2, 8, 32) improves signal reconstruction but often reduces downstream task performance. This trade-off suggests that while more codebooks enhance fidelity, they often degrade performance for both discriminative and generative tasks by increasing output dimensionality and modeling complexity. In RVQ-based models, earlier codebooks capture more phonetic information, while later ones often add redundancy, which may explain this trade-off. This highlights an important design principle for tokenizers: optimizing for reconstruction alone does not guarantee better performance on downstream tasks. Medium bitrate settings typically provide the best balance between audio reconstruction quality and task performance. Discrete vs Continuous. While discrete tokens show promise, they face notable limitations in complex scenarios such as polyphonic music separation or noisy environments. Continuous features consistently outperform discrete tokens due to the information loss inherent in quantization, which affects critical attributes like phonetics, emotion, and speaker identity. These limitations are further exacerbated in low-resource settings. For instance, although Discrete WavLM performs competitively at low and medium bitrates for low-resource ASR, it still lags behind the continuous baseline. RVQ-based tokenizers struggle even more, especially on smaller datasets such as Welsh, ESC-50, and GTZAN, with high bitrate amplifying these issues. Performance improves with more data: Discrete WavLM, for example, achieves 6.0% WER on LibriSpeech (960h), 22.0% on Basque (116h), and 58.9% on Welsh (8h) at low bitrate using BiLSTM head, illustrating strong correlation between data scale and ASR accuracy. From the hyperparameter tuning experiments (not reported here for brevity), we noticed that larger downstream models help improve convergence and performance, particularly for acoustic tokenizers, which are more sensitive to both data scale and model capacity. Semantic tokenizers are generally more robust in low-resource settings but still fall short of continuous representations with extremely limited data. Overall, careful tuning and appropriate scaling of both data and model are essential for an effective use of discrete representations, especially acoustic tokens. Summary. Semantic tokenizers are generally more robust, especially in low-resource settings, but still fall short of continuous representations when data is limited. Training downstream models with semantic or semantically-distilled tokenizers tends to be more stable and reliable compared to acoustic tokenizers, which often require larger datasets and more careful model scaling. Overall, discrete tokenizers are more sensitive to architectural choices and hyperparameters of the downstream head, whereas continuous features typically yield more consistent performance across configurations. Therefore, careful tuning and appropriate scaling of both data and model architecture are crucial for effectively leveraging discrete representations. While discrete tokens offer advantages in efficiency and modularity, continuous representations still lead in overall performance. Bridging this gap is essential for the successful integration of audio tokens into future multimodal language models."
        },
        {
            "title": "3.3 Acoustic Language Models Evaluation",
            "content": "Following the rise of LLMs, researchers have extended the generative auto-regressive framework beyond text to discrete representations of speech (Lakhotia et al., 2021), audio (Borsos et al., 2023b), and music (Copet et al., 2023). This modeling approach has proven highly effective across domains, enabling powerful genIn this section, we begin by examining unconditional speech eration capabilities (Défossez et al., 2024). generation (Speech Language Models) and text-conditioned generation (text-to-speech (TTS)). We then explore audio generation and finally turn to the music modality."
        },
        {
            "title": "3.3.1 Speech Language Modeling",
            "content": "Background. Speech Language Models (SLMs) have gained significant interest (Arora et al., 2025; Wu et al., 2024a; Peng et al., 2024; Cui et al., 2024; Ji et al., 2024a; Latif et al., 2023), demonstrating remarkable performance in traditional speech tasks (Chen et al., 2025a; Elmakies et al., 2025), diverse generative applications (Yang et al., 2024c;b), and reasoning over speech and audio signals (Chu et al., 2023; Wang et al., 2025a; Yosha et al., 2025). SLMs can generally be classified into two main categories: (i) generative SLMs that are conditioned on previous speech/text tokens and generate speech/text (Défossez et al., 2024; Cuervo et al., 2025; Nguyen 20 Table 7: DASB results for discriminative tasks (speech)."
        },
        {
            "title": "Tokenizer",
            "content": "#Q"
        },
        {
            "title": "Continuous",
            "content": "Enc-SMA-24 2 8 ASR-En WER ASR-LR WER"
        },
        {
            "title": "Basque",
            "content": "ER IC KS SI SV ACC ACC ACC ACC EER 4.07 6. 41.77 14.32 63.10 86.10 99.00 99. 2.10 12.700.37 29.090.13 90.900.32 51.000.98 45.500. 42.900.16 77.733.12 89.815.46 18.330.26 8.430.13 21.77 0. 84.531.90 45.360.57 44.730.02 40.030.29 74.301.69 94.263. 13.540.57 32 9.951.17 23.24 1.22 97.391.19 58.210. 42.960.02 33.662.65 69.103.42 91.121.92 10.126.66 DAC-SMA2 8 14.840.25 33.880.20 95.210.84 68.930. 45.200.01 29.830.19 67.271.56 97.880.79 21.801.00 10.73 0. 25.39 0.20 97.200.14 62.451.40 44.730.02 23.970.41 65.272. 87.3310.98 15.865.26 32 13.130.16 28.470.19 98.960. 73.571.56 43.200.02 44.6039.19 68.672.91 87.694.99 17.12 0. ST-S-16 Mimi-S-24 DWavL-S-16 SQ-SMA-16 SQ-SMA-16* WT-SMA2 8 8 9.480.10 22.680.10 71.360. 42.170.05 54.860.01 56.800.08 94.110.63 73.160.37 24.230. 9.06 0.45 21.720.23 68.360.44 35.350.22 55.000.01 53.830. 94.110.07 96.780.45 10.450.43 9.730.61 22.650.41 91.590. 59.188.52 51.130.02 53.830.19 92.180.20 79.500.43 18.680. 32 10.840.56 24.100.36 96.890.07 58.156.90 46.760. 50.730.50 91.310.19 63.9313.64 23.914.60 2 4 4 1 4.780.25 10.580.17 58.980. 22.020.17 61.530.02 76.330.17 96.820.92 76.570.33 22.410. 5.070.17 9.570.20 48.940.38 19.660.33 63.200.01 78.730. 95.890.50 92.310.09 13.470.22 91.570.49 92.900.41 94.800. 94.241.24 41.300.06 58.130.26 92.740.42 97.380.03 9.690. 11.630.08 30.910.17 16.110.18 35.480.35 97.410. 75.820.20 43.430.02 15.250.15 59.132.10 85.902.48 19.380. Table 8: DASB results for generative tasks (speech). ModelsTasks #Q SE SS - Speech"
        },
        {
            "title": "DNSMOS DNSMOS",
            "content": "dWER Sep 3.68 9."
        },
        {
            "title": "Spk",
            "content": "Sim 0."
        },
        {
            "title": "DNSMOS",
            "content": "dWER 3.49 4."
        },
        {
            "title": "Spk",
            "content": "Sim 0.93 3.150.01 34.950.64 0.860.00 3.080. 22.701.84 0.880.00 2.780.01 65.706.09 0.800.01 3.260. 54.851.82 0.860.00 3.510.01 29.443.93 0.900.01 2.930. 30.660.97 0.880.00 3.190.02 29.980.58 0.860.00 3.490. 21.650.57 0.870.00 3.250.01 67.562.21 0.850.00 2 8 32 2 8 2 8 8 32 3.180.01 102.612. 0.820.00 2 6 4 1 3.560.01 25.882.15 0.880.00 3.570.01 9.430.33 0.890. 3.280.01 122.338.74 0.830.00 3.330.01 67.5310.65 0.850."
        },
        {
            "title": "Continuous",
            "content": "Enc-SMA-24 DAC-SMA-24 ST-S-16 Mimi-S-24 DWavL-S-16 SQ-SMAWT-SMA-"
        },
        {
            "title": "Mixture",
            "content": "Rec 3.19 3.54 3.72 3. 3.67 3.76 3.20 3.72 3.65 3. 3.57 3.75 3.77 3.57 3.130. 80.331.77 0.880.00 3.080.00 53.370.65 0.900.00 2.970. 92.420.97 0.850.00 3.010.00 101.191.99 0.850.00 3.300. 52.772.48 0.930.00 2.670.01 92.070.05 0.880.01 3.130. 84.940.63 0.870.00 3.430.01 60.900.77 0.910.00 3.290. 109.303.30 0.870.00 3.000.00 137.002.16 0.820.00 3.560. 49.570.64 0.850.00 3.750.01 30.390.45 0.910.00 3.190. 136.003.58 0.830.00 3.420.00 118.334.50 0.860.00 3. et al., 2025), and (ii) speech-aware LMs that are conditioned on speech/text and generate text (Chu et al., 2023; Tang et al., 2024a; Mousavi et al., 2025). This work focuses on the first category of SLMs as there 21 Table 9: DASB results for generative and discriminative tasks (music and general audio)."
        },
        {
            "title": "Tokenizer",
            "content": "#Q SS - Audio SS - Music"
        },
        {
            "title": "SEC",
            "content": "SI-SDRi SI-SDRi SAR SIR ACC"
        },
        {
            "title": "MGC",
            "content": "ACC"
        },
        {
            "title": "Continuous",
            "content": "Enc-SMA-24 DAC-SMA-24 SQ-SMA-16 WT-SMA-"
        },
        {
            "title": "Mixture",
            "content": "2 8 32 2 32 4"
        },
        {
            "title": "Rec",
            "content": "0.76 3."
        },
        {
            "title": "Sep",
            "content": "15.07 7.030.49 9.530."
        },
        {
            "title": "Rec",
            "content": "3.36 7."
        },
        {
            "title": "Sep",
            "content": "13.29 9.56 11.99 92.91 87.00 1.492. -2.801.68 5.961.52 34.830.47 70.331.70 1.980.36 -1.950. 5.260.22 37.000.73 54.673.86 5.76 -1.730.09 11. -11.720.35 -15.000.02 -0.420.01 35.431.45 39.671.25 0. 3.33 4.73 3.62 3.840.48 5.620.21 -4.920. 2.37 6.66 8.54 1.010.17 -3.590.09 5.920. 31.031.84 50.000.82 -11.770.1 -10.622.35 -5.523.68 28.600. 47.673.09 -11.320.12 -12.700.17 -2.050.41 36.670.92 50.000. 6.540.22 5.53 -3.620.87 -5.840.86 1.420.32 31.371. 42.670.47 -24.05 -16.720.08 -2.66 -4.520.04 -8.320. 2.650.11 34.500.82 48.001.41 -16.5 -7.71 50.01 -inf is growing interest from the research community to study generative SLMs (Nguyen et al., 2025; Maimon et al., 2025b; Rubenstein et al., 2023). Several SLMs operate over discrete speech representations derived from pre-trained SSL model (Nguyen et al., 2025; Lakhotia et al., 2021; Cuervo et al., 2025). Others employ semantically distilled acoustic tokenizers (Défossez et al., 2024) or adopt cascading, mixed-resolution strategy (Borsos et al., 2023b), modeling speech hierarchically from coarse semantic content to fine acoustic details, using language models conditioned on previously generated streams. More recently, supervised semantic tokenizers have gained popularity. These methods typically quantize the output layer of pre-trained ASR system to produce discrete tokens (Zeng et al., 2024; 2025). In this study, we analyze the impact of these choices by comparing SLMs trained under controlled setup with different audio tokenizers presented in Table 2. We focus on SLMs that model the joint probability of sequence of speech tokens as: (q = q1, . . . , qn) = i=1 P(qi q<i), (12) where qi Vq, and Vq denotes the vocabulary of speech tokens. These models are typically implemented as decoder-only transformers (Vaswani et al., 2017) and trained to minimize the negative log-likelihood: LLM = i=1 P(qiq<i). (13) Each token is embedded via matrix RVsd, where denotes the embedding dimension. The resulting sequence is processed by stack of causal transformer layers, yielding contextual representations Rnd. final linear projection RdVs maps these to logits, which are converted to probability distribution over the vocabulary via softmax: p(qi+1 ci). Experimental Setup. Motivated by Maimon et al. (2025a), each SLM is built upon the Qwen-2.5 architecture (Yang et al., 2024a) (357M parameters in total, after removing the text embedding tables) and initialized using TWIST (Hassid et al., 2023). The textual embedding tables are replaced with new audio embedding tables corresponding to the audio codebooks. The models are trained using the standard crossentropy loss reported in Eq. 13. To ensure all SLMs processed comparable amount of audio during training, we dynamically adjusted the tokens per batch according to their tokenizers frame rate. To accommodate multiple codebooks, the delay pattern from MusicGen (Copet et al., 2023) is applied across all SLM models. 22 Table 10: SLM results considering spoken content and acoustic elements using subset of SALMon tasks."
        },
        {
            "title": "Tokenizer",
            "content": "#Q sBLIMP sWUGGY sSC tSC Gender Sent. Spk Sentiment"
        },
        {
            "title": "Acoustic Consistency",
            "content": "Sem.-Ac. Align. HuBERT 25Hz Enc-SMA-24 DAC-SMA-16 ST-S-16 ST-S-16* Mimi-S-24 Mimi-S-24* DWavL-S-16 SQ-SMA-16 WT-SMA-24 8 8 8 8 8 6 4 1 60.89 70.51 53. 71.46 69.50 62.50 69.00 53.00 51. 51.51 51.08 52.75 52.25 60.17 53. 51.58 51.22 51.29 50.73 56.89 63. 62.21 67.57 69.10 51.41 54.60 50. 48.20 70.50 56.50 65.00 48.95 51. 81.00 60.00 77.00 48.42 47.56 51. 51.68 55.74 60.60 54.30 68.51 66. 67.00 77.50 76.50 58.00 59.50 71. 77.00 65.00 65.50 78.00 76.00 51. 62.42 92.00 70.00 86.50 51.79 55. 83.00 64.00 84.50 52.00 52.75 81. 78.50 69.00 50.00 50.00 49.50 50. 52.00 52.00 49.00 50.50 50.50 The models are trained for total of 50, 000 optimizer steps, with context length set to 1024. The audio target batch size is set to include about 2.9 hours of speech per backpropagation step. We used the Adam optimizer coupled with linear learning rate scheduler, applying 1% warmup ratio (corresponding to 500 steps). All input samples are fed to the SLMs using packing strategy by concatenating all samples together until having sequence of the target length. To ensure fairer evaluations across different tokenizers, the number of codebooks is restricted to maximum of 8, promoting better alignment between them. All code was developed using the SpeechBrain toolkit (Ravanelli et al., 2024), and Hugging Face Transformers (Wolf et al., 2019). Dataset. We use the publicly available dataset LibriHeavy (Kang et al., 2024) containing 56k hours of transcribed speech, and the official validation and test sets of LibriSpeech (Panayotov et al., 2015). Evaluation Setup. To evaluate our SLMs, we use the ZeroSpeech (Dunbar et al., 2021) sBLIMP and sWUGGY evaluation. The sBLIMP task assesses model perplexity on pairs of syntactically correct and incorrect sentences (e.g., the dog sleeps vs. It evaluates the models understanding of the dogs sleeps). core grammatical phenomena in English. Similarly, sWUGGY measures whether the model assigns higher probability to real word over phonologically similar non-word (e.g., brick vs. blick), thus testing lexical discrimination. We further assess semantic understanding using Spoken Story-Cloze (sSC) and Topic Story-Cloze (tSC) tasks (Hassid et al., 2023), derived from the spoken variant of the StoryCloze dataset (Mostafazadeh et al., 2016). In sSC, the model must choose between the correct continuation and randomly sampled, semantically incompatible adversarial one. This setup probes the models ability to capture fine-grained causal and temporal commonsense relations. Similarly, in tSC, the adversarial continuation is taken from different topic, so success reflects the models capacity to maintain topical coherence. To evaluate acoustic modeling such as prosody, speaker identity, and sentiment we adopt the SALMon evaluation suite (Maimon et al., 2025c). This includes several metrics that test whether the SLM retains and models key acoustic attributes. We focus on two aspects: (1) acoustic consistency, which evaluates the models sensitivity to changes in speaker, gender, and sentiment; and (2) sentiment-acoustic alignment, which tests whether the model assigns higher scores to utterances where the acoustic sentiment aligns with the spoken content. This comprehensive suite allows us to assess both the linguistic and paralinguistic modeling capacities of our SLMs. Results and Discussions. The results of the SLM experiments are presented in Table 10. To establish clear baseline aligned with the current literature, we train an SLM using the HuBERT 25 Hz tokenizer, 23 originally introduced in TWIST (Hassid et al., 2023), using the same configuration as previously described. Following the methodology of Défossez et al. (2024), all hybrid tokenizers marked with an asterisk (\"*\") correspond to SLMs trained with semantic stream overweight factor of 100. This training setup prioritizes the semantic content over the acoustic content, and could be required for better disentanglement of the semantic distillation. We maintain the same weighting strategy during evaluation. The results reveal significant differences in performance across semantic and acoustic evaluation tasks. On semantically-oriented benchmarks (sBLIMP, sWUGGY, sSC, and tSC), most tokenizers exhibit limited semantic capacity. The tokenizer achieving the strongest semantic performance is HuBERT, followed by the semantically distilled weighted tokenizers that demonstrate the second-highest semantic performance. Specifically, by overweighting the semantic stream, Mimi improves from 52.25% to 60.17% accuracy on sBLIMP. similar trend is observed across all semantic evaluations, with an average performance gain of 6.91 accuracy points. The same pattern holds when comparing ST-S-16 to ST-S-16*, where the overweighted version achieves superior semantic results. Since semantic information is not clearly localized in specific stream, WavLM is evaluated without any weighting strategy. Despite this, it still ranks second among unweighted tokenizers, just after HuBERT. This semantic trend is consistent with our expectations of HuBERT, Mimi, SpeechTokenizer, and WavLM, since they primarily encode or have specific phonetic streams (via distillation or self-supervised learning objectives), thereby enhancing performance on linguistic understanding tasks. These results suggest that, when carefully tuned, semantic distillation approaches can rival SSL-based models like HuBERT, though they still fall slightly behind on sSC and tSC. Further investigation is needed to determine the optimal weighting between semantic and acoustic streams. In contrast, purely acoustic tokenizers such as Encodec and DAC show negligible semantic capability, rendering them unsuitable for this SLM configuration. Similarly, SQCodec and WavTokenizer offer limited semantic utility. We note that contrary to Encodec and DAC, SQCodec and WavTokenizer obtained stronger semantic scores. One explanation could be due to the type of quantization (i.e. RVQ vs. SVQ / FSQ) or the limited number of streams. Overall, the performance varies substantially across tokenizers, with only Mimi-S-24 approaching HuBERTs baseline on semantic tasks. WavLM achieves the best acoustic performance on average, with accuracies of 92.00%, 70.00%, and 86.50% on gender, sentiment, and speaker consistency, respectively. This indicates that the model effectively captures and processes acoustic attributes in comparison to other tokenizers. Interestingly, Mimi also shows strong acoustic performance, outperforming HuBERT on each task. Purely acoustic tokenizers such as DAC and EnCodec, or SQ-codec and WavTokenizer also outperform HuBERT on acoustic evaluations and achieve results comparable to the semantically distilled tokenizers. However, no method yields substantial results on semantic-acoustic alignment, highlighting limitation of current approaches in jointly modeling and reasoning over both modalities. Summary. Our study reveals that semantic and acoustic performance in SLMs varies significantly across tokenizer types. HuBERT remains the strongest performer on semantic tasks, while WavLM leads in acoustic consistency. Semantically distilled tokenizers, particularly those with semantic stream overweighting, showed promising results by narrowing the semantic gap with HuBERT. These gains, however, come with trade-offs, emphasizing the importance of carefully balancing semantic and acoustic objectives. Overall, our findings suggest that, for now, there is no single tokenizer that excels across all spoken and acoustic tasks."
        },
        {
            "title": "3.3.2 Text-to-Speech",
            "content": "Background. Text-to-Speech (TTS) is one of the primary applications of audio tokens. Traditional TTS systems typically rely on neural networks that predict mel spectrograms from text (Shen et al., 2018; Ren et al., 2019), followed by neural vocoders (Morise et al., 2016; Kong et al., 2020; van den Oord et al., 2016) to synthesize waveforms. The introduction of discrete audio tokens offers several advantages. Notably, it reframes waveform generation as classification task over fixed vocabulary, rather than regression over continuous values. This shift enables optimization via categorical distributions and negative log-likelihood loss, which is typically more stable and tractable than regression objectives. Second, off-the-shelf neural codec decoders can reconstruct waveforms directly from token sequences, removing the need to train separate 24 vocoders. Third, discrete tokens reduce sequence length, improving inference efficiency compared to µ-law quantization (van den Oord et al., 2016). Prior to the adoption of discrete representations, TTS was largely dominated by non-autoregressive (NAR) models (Ren et al., 2021; 2019; Kim et al., 2021) due to their inference speed and stability relative to autoregressive (AR) models (Shen et al., 2018). However, the emergence of neural codecs has renewed interest in AR architectures (Wang et al., 2024c; Chen et al., 2025a; Yang et al., 2024c), which have demonstrated strong performance in expressive and zero-shot generation settings. Meanwhile, NAR models have also benefited from discrete token supervision, with recent advances incorporating diffusionand flow-matchingbased methods (Ju et al., 2024; Yang et al., 2024d) to further enhance synthesis quality. Experimental Setup. Our models are based on customized adaptation of the ESPnet (Tian et al., 2025) implementation of VALL-E (Chen et al., 2025a). To facilitate convergence, we adopt staged training strategy that allows independent optimization of the AR and NAR components. We first perform 10 epochs of AR-only training, followed by 90 epochs of joint training with both AR and NAR layers to improve convergence for some tokenizers. All models use 12-layer architecture for both AR and NAR decoders, with an attention dimension of 1024 and dropout rate of 0.2. Following ESPnet conventions, we use looped nominal epochs, with 50,000 samples per epoch. The approach involves using fixed number of data samples per epoch taken sequentially from the dataset until the end of the dataset is reached, at which point training restarts from the beginning. The epoch achieving the lowest validation dWER is chosen for evaluation. Dataset. We train our model on the LibriTTS dataset (Zen et al., 2019), using corresponding phoneme transcriptions obtained from LibriTTS with Forced Alignment dataset (McAuliffe et al., 2017). The model is trained to generate discrete audio tokens conditioned on phoneme prompts (representing content) and acoustic code prompts (capturing target speaker characteristics), enabling it to synthesize speech that matches both the textual input and the target speakers voice. Evaluation Setup. We evaluate all models on all samples in the test split that fall within the length limit. To evaluate the vocal quality of synthesized speech, we use pretrained UTMOS model (Takaaki et al., 2022). For assessing pronunciation accuracy, we transcribe both the ground-truth and generated utterances using the Whisper Large model (Radford et al., 2023) with greedy decoding. We compute the degraded WordError-Rate (dWER) by treating the ASR prediction of the ground-truth audio as the reference instead of the original transcription. We report mean UTMOS scores and micro dWER values; that is, the Levenshtein edit distance computed over the entire dataset. To measure speaker fidelity, we compute the cosine similarity (SpkSim) between X-vectors extracted from the generated and reference audio using the base variant of WavLM (Chen et al., 2022), fine-tuned for speaker verification. Following ESPnet-Codec, to mitigate the variability of samples arising from using text-conditioned sampling-based generative model, we generate 10 samples simultaneously for each prompt and choose the best one based on the WER calculated with the original label as the ground truth using Whisper Small, while final dWERs are calculated using the large model. To establish clear baseline aligned with current literature, we train VALL-E using ESPNets in-distribution retraining of EnCodec11, which is trained on LibriTTS data only instead of mix of speech, audio, and music as in the original model. We perform grid search over sampling temperature values 0.7, 0.8, 1.0, 1.2, 1.3 and top-k values 10, 20, 30, where controls the sampling temperature and limits the number of highest-probability tokens considered during top-k sampling. The optimal hyperparameters are selected based on the lowest dWER on filtered subset of the validation set (67 samples selected from an initial random pool of 100, based on sequence length). These optimal values are then used for evaluation on the test set. Results and Discussion. Table 11 presents the performance of various tokenizers on the TTS task. The highest audio quality is achieved by ESPNet EnCodec (Enc-S-24), which obtains UTMOS score of 3.77. This model is trained on speech-only data, likely enabling it to better capture fine-grained speaker 11https://huggingface.co/espnet/libritts_encodec_24k Table 11: Text-to-Speech synthesis results when using the VALL-E speech language model conditioned on phoneme annotations."
        },
        {
            "title": "Tokenizer",
            "content": "#Q UTMOS dWER SpkSim Enc-SMA-24 Enc-S-24 DAC-SMA-24 ST-S-16 Mimi-SDWavL-S-16 WT-SMA-24 8 8 8 8 6 1 2.31 3.77 2. 2.91 2.60 3.42 2.85 4.77 5. 11.71 5.35 7.93 4.32 4.67 0. 0.91 0.88 0.91 0.91 0.90 0. characteristics. Notably, the original EnCodec model performs significantly worse, with UTMOS of only 2.31. We further investigate the impact of training data in Section 4. The best adherence to the text is achieved with WavLM (Ji et al., 2024c) with dWER of 4.32, which is likely attributable to the preservation of higher-level semantic information. It is followed by WavTokenizer at 4.67, which employs single codebook with larger vocabulary. The model also achieves competitive audio quality at UTMOS of 2.85. This result may be attributed to the expressive power of the larger vocabulary and the simplicity of single-stream generation. Discrete WavLM6, achieves the second-best audio quality with UTMOS of 3.42. It should be noted that during training, this tokenizer showed the most stable and robust results and early convergence, particularly in low-data regimes, and reasonable speaker similarity at 0.90. These findings indicate that semantic representations derived from self-supervised models are well-suited for TTS, supporting both natural-sounding and phonetically faithful speech synthesis. In contrast, general-purpose acoustic tokenizers such as EnCodec (Enc-SMA-24) and DAC (DAC-SMA-24) result in decreased audio quality, and occasionally, as in the case of the latter, decreased pronunciation fidelity as well. These models likely require the TTS model to learn high-level speech abstractions from raw acoustic features, adding complexity to the generation process. Another possible contributing factor is that these tokenizers were trained on multi-domain audio data, whereas all other tokenizers evaluated were trained exclusively on speech. Finally, SQ-Codec, originally designed for diffusion-based generation with large vocabulary ( 20k), failed to converge in our AR/NAR setup, likely due to the challenges of modeling such large token space in an autoregressive setting. All models achieve comparable levels of speaker similarity (ranging from 0.88 to 0.91). Summary. Overall, achieving strong TTS performance with discrete tokenizers remains challenging, especially under constrained training conditions. Training with semantic tokenizers leads to more robust and effective TTS performance compared to acoustic or semantically distilled tokenizers; however, in high-data regimes with deep models, acoustic tokenizers, such as EnCodec, can be competitive with or even outperform semantic ones, particularly if they are trained on similar speech data, such as shown with Enc-S-24 trained on the same LibriTTS dataset as the TTS itself."
        },
        {
            "title": "3.3.3 Audio Generation",
            "content": "Background. Generating realistic audio is long-standing goal in generative AI, with applications in media, accessibility, and human-computer interaction. Previous works have explored audio generation under various conditions, including text (Liu et al., 2024b; Dong et al., 2023; Saito et al., 2024; Kumar et al., 2024), image (Sheffer & Adi, 2023; Wang et al., 2024a), video (Luo et al., 2023; Pascual et al., 2024; Zhang et al., 26 2024b; Wang et al., 2025b), and multimodal inputs (Jeong et al., 2025; Chen et al., 2025b), as well as in unconditional settings. In this study, we focus on both unconditional and text-conditioned generation, as these represent the most common and well-benchmarked paradigms. Audio generation can be very broadly categorized into two main categories: diffusion-based methods (Yang et al., 2023b; Huang et al., 2023; Liu et al., 2023), and language model based approaches (Kreuk et al., 2023; Borsos et al., 2023b; Ziv et al., 2024). In this study, we focus on the latter, as the use of discrete audio tokens is more prevalent in such generation approaches. While both autoregressive (Kreuk et al., 2023; Borsos et al., 2023b) and non-autoregressive (Ziv et al., 2024) methods have been proposed for audio generation, we focus on AR models in this study. This choice is motivated by their typically superior generation quality and their prevalence in recent work, despite the trade-off of slower inference. Experimental Setup. We adopt the AudioCraft toolkit (Copet et al., 2023), which provides training pipeline for text-to-audio synthesis based on MusicGen (Copet et al., 2023) and AudioGen (Kreuk et al., 2023). The framework uses T5 model (Raffel et al., 2020) to encode the text prompt into latent conditioning tensor RTlenD. This tensor is passed to causal decoder Transformer, where each block consists of causal self-attention layer over previously generated audio tokens, followed by cross-attention layer on the conditioning tensor. Following the original setup, we adopt delay pattern across streams and apply Classifier-Free Guidance (CFG). We use the base model configuration with approximately 300M parameters and T5-Base as the text encoder. This framework is well-established and supports both multistream and single-stream audio tokens. To enable unconditional generation with the same model, we apply CFG dropout during 10% of training steps, strategy shown to also enhance robustness in both conditional and unconditional settings. We use the same architecture across all tokenizers to ensure fair comparison. To balance consistency and computational efficiency, each model is trained for 100,000 steps using batch size of 128 audio samples (each 10 seconds long), with mix-up augmentation. This corresponds to half the batch size and training steps used in the original AudioGen, while still achieving competitive performance. Note that the effective number of tokens and training time may vary depending on the tokenizers frame rate and number of codebooks. For generation, we use fixed sampling parameters across all tokenizers, specifically top-k sampling with = 250, without tuning them for individual models. Additional experiments confirm that the observed trends remain consistent under different sampling hyperparameters, though detailed results are omitted for brevity. Dataset. We use several audio datasets for training and evaluation, many of which are part of LAIONAUDIO-630K (Wu et al., 2023c). Specifically, we include AudioCaps (Kim et al., 2019), AudioStock12, BBC Sound Effects13, EpidemicSound14, FreeSound15, Free to Use Sounds16, MACS (Morato & Mesaros, 2021), and Odeon Sound Effects17. We follow Kreuk et al. (2023) and use the official splits of AudioCaps for validation and testing. All other datasets are used for training. These datasets vary in sample rate and format. We resample all audio and convert it to mono to match the input requirements of each tokenizer. In total, the training data contains approximately 4,050 hours of audio. Evaluation Setup. We evaluate both text-conditioned generation and audio continuation with 2.5 second audio prompt (and no text condition). We use three objective metrics that provide complementary perspectives for evaluation. First, we compute the Fréchet Audio Distance (FAD) using the FadTK toolkit (Gui et al., 2024) with the VGGish model. This metric is computed similarly to AudioGen (Kreuk et al., 2023), where FAD is calculated against the AudioCaps test set to measure the overall quality of synthesized audio. Second, we assess semantic consistency using KL Divergence. Specifically, we follow Yang et al. (2023b) and compare the output distribution of pre-trained audio classifier, specifically PASST (Koutini et al., 2022), on real samples versus model-generated samples for the same conditions. Finally, we evaluate text-audio 12https://audiostock.net/sfx 13https://sound-effects.bbcrewind.co.uk 14https://www.epidemicsound.com/sound-effects/ 15https://freesound.org 16https://www.freetousesounds.com/all-in-one-bundle/ 17https://www.paramountmotion.com/odeon-sound-effects 27 Table 12: Comparing performance of audio LMs over different tokenizers. We report FAD, KL divergence, and CLAP score on the AudioCaps test set. We also provide metrics for audio reconstruction. We note that ground truth audio in AudioCaps gets CLAP= 0.311, proving topline. For more information about the tokenizers see Section 2."
        },
        {
            "title": "Tokenizer",
            "content": "#Q Text Cond. Generation Uncond. Generation"
        },
        {
            "title": "Reconstruction",
            "content": "FAD KLD CLAP FAD KLD CLAP FAD KLD CLAP Enc-SMA-24 Enc-MEnc-A-16 DAC-SMA-44 DAC-SMA-24 SQ-SMA-16 WT-SMA-24 4 4 9 9 4 3.771 10.110 1.955 6.929 7.708 1. 1.788 1.576 1.959 1.966 7.733 3. 2.594 1.463 .279 .295 .300 . .253 .151 .291 5.996 1.897 13. 3.548 6.732 8.196 2.840 2.064 2. 2.183 5.977 2.301 4.441 2.224 . .175 .205 .212 .199 .175 . 3.806 12.611 1.816 0.456 1.387 0. 2.206 4.124 0.242 0.446 3.460 0. 5.018 0.892 .281 .251 .273 . .281 .268 .253 alignment using the CLAP score (Wu et al., 2023c; Huang et al., 2023). This measures how well the generated audio matches the input prompt. All evaluation metrics used here are generative in nature and are therefore influenced not only by the language models ability to predict tokens but also by the quality of the vocoder used to synthesize audio. As such, poor metric scores may reflect limitations in the vocoder rather than issues in the encoder or language model. Results and Discussion. Table 12 summarizes the performance for both text-conditioned generation and audio continuation. Since all Audio LM evaluations rely on generative outputs, final performance is often influenced by vocoder quality. As result, even language model with strong next-token prediction capabilities may underperform if paired with suboptimal vocoder. To better isolate the effect of vocoding, we report reconstruction quality metrics for each tokenizer without involving any language model training. These results highlight notable differences across tokenizers. The 44kHz variant of DAC performs particularly well, reaching quality levels comparable to the version of EnCodec trained specifically on audio. In contrast, the music-only EnCodec model shows poor reconstruction quality, as expected given its domain-specific training. Apart from the music-only EnCodec, which was not trained on general audio, WavTokenizer exhibits the weakest reconstruction performance among all evaluated tokenizers. The Audio LM trained on the music-only tokenizer (ENC-M-32) shows weak performance, particularly in terms of FAD. This may be attributed to limitations in the vocoder or the sensitivity of distribution-based metrics. For example, the model may be effective at next-token prediction and produce acoustically coherent samples, yet still deviate from the reference waveform distribution. The relatively strong CLAP and KLD scores for text-conditioned generation support this possibility, even though current evaluation metrics are insufficient to fully diagnose the cause of the performance drop. In contrast, the general-audio-trained version of EnCodec achieves the best FAD scores and consistently strong performance across all evaluation settings, emphasizing the value of domain-specific training for audio tokenizers. WavTokenizer achieves strong performance in text-to-audio generation, despite its relatively poor reconstruction quality. One possible explanation is that its single-token stream format simplifies the modeling task for the language model, potentially enabling faster convergence. This performance gap may narrow with additional training compute. In contrast, both DAC variants and SQCodec exhibit weaker results in text-conditioned generation compared to their strong reconstruction performance. For SQCodec, the large and potentially redundant token vocabulary may make next-token prediction more difficult, reducing generation quality. Similarly, while the DAC models offer excellent compression, their structure may be more challenging for autoregressive modeling, resulting in reduced generation performance. This gap may be due to its higher token rate leading to longer sequences, or modeling difficulties specific to DAC. 28 Summary. Our findings highlight the critical role of domain-specific training for audio tokenizers. Training the language model alone on in-domain data is not sufficient: tokenizers must also be trained on the same domain to ensure strong performance. Our results also show that the best reconstruction performance does not correlate with the best modeling performance. In the future, we encourage the development of evaluation metrics that disentangle modeling ability from vocoder performance, as is common in the speech domain. We also emphasize the need for more robust modeling metrics (Chung et al., 2025)."
        },
        {
            "title": "3.3.4 Music Generation",
            "content": "Background. Music generation is particularly challenging due to the structural complexity of musical compositions, which involve diverse instrumentation, long-term dependencies, and high expectations for both acoustic quality and aesthetic coherence. Recent advances in generative models, especially diffusionbased approaches and LLM-based architectures, have shown strong potential for producing coherent and high-quality music, often conditioned on melodic (Borsos et al., 2023b) or text (Huang et al., 2022) prompts. dominant paradigm in text-to-music generation involves latent diffusion models that operate over VAEderived continuous representations (Evans et al., 2025; Chen et al., 2024a; Lam et al., 2023). In contrast, the use of autoregressive language models for music generation over discrete tokens remains an evolving area. Early work such as Jukebox (Dhariwal et al., 2020) employed VQ-VAE (van den Oord et al., 2017) to obtain quantized features for autoregressive modeling. More recent developments in neural audio codecs have shown that their latent spaces can serve as compact and expressive discrete representations of music. Building on this, several studies (Borsos et al., 2023b;a; Rouard et al., 2024) have explored LM-based music generation using various codec tokenizers and decoding strategies. In this section, we evaluate the effectiveness of discrete tokenizers in LM-based music generation, considering both text-conditioned synthesis and unconditional generation (i.e., music continuation). Experimental Setup. Our experimental setup for text-to-music synthesis largely follows the same configuration described in the audio generation section. We use the AudioCraft toolkit (Copet et al., 2023) with decoder-only transformer and cross-attention over T5-Base (Raffel et al., 2020) text encoder. The model has approximately 300M parameters and is trained using classifier-free guidance (CFG). Key differences from the audio setup include the use of higher CFG dropout rate of 30% (vs. 10% for audio), which allocates more emphasis to unconditioned (self-conditioned) music generation. Additionally, no mix-up augmentations are applied, and models are trained for 200,000 steps using 4A100 40GB GPUs, each with batch size of 32 samples (10 seconds each). This results in significantly less computing compared to the original MusicGen configuration (192 samples for 1M steps). As in the audio experiments, we use the same architecture across all tokenizers to ensure fair comparison. We adopt autoregressive modeling, which, while less efficient than masked non-autoregressive methods (Garcia et al., 2023; Ziv et al., 2024), is more widely used and known to produce better perceptual quality. Datasets. For training, we use the genre-balanced Free Music Archive (FMA) dataset (Defferrard et al., 2017), following the setup of stable-audio-open (Evans et al., 2025). All samples are 30 seconds long, and we follow the official split provided in the dataset repository18. The training set consists of 84,213 samples, totaling 702h hours. We combine artist, album, keywords, genres, and titles from the metadata to build the text prompt of the model. For evaluation, we use two datasets: (1)MusicCaps (Agostinelli et al., 2023)19, which contains 5,347 samples of 10 seconds each, annotated with descriptive text; and (2) the FMA test split, originally containing 11,235 samples of 30 seconds. To reduce evaluation time while preserving genre coverage, we randomly select 10 clips from each of the 156 genres, resulting in genre-balanced subset of 1,560 samples. Evaluation Setup. We evaluate music generation models on two tasks: text-conditioned generation and unconditioned generation, also referred to as continuation, where 2-second audio clip is used as prompt to extend the content in coherent manner. In addition, we assess the reconstruction performance of each 18https://github.com/mdeff/fma 19https://www.kaggle.com/datasets/googleai/musiccaps 29 Table 13: Comparing the performance of text-to-music LMs over different tokenizers on MusicCaps and FMA-test. The abbreviation of the tokenizer column are shown in Table 2. #Q denotes the number of quantization layers used in the tokenizer."
        },
        {
            "title": "Tokenizer",
            "content": "#Q Text Cond. Generation Uncond. Generation"
        },
        {
            "title": "Reconstruction",
            "content": "FAD KLD CLAP FAD KLD CLAP FAD KLD CLAP Enc-SMA-24 Enc-M-32 DAC-SMADAC-SMA-24 SQ-SMA-16 WT-SMA-24 Enc-SMA-24 Enc-M-32 DAC-SMADAC-SMA-24 SQ-SMA-16 WT-SMA-24 8 4 9 4 1 8 4 9 4 1 11.173 2.246 4. 2.006 8.398 9.403 2.214 2.127 14. 2.810 17.050 2.792 15.380 2.161 8. 1.299 8.115 8.789 1.543 1.746 9. 2.412 16.511 1."
        },
        {
            "title": "MusicCaps",
            "content": ".108 .150 .119 .093 .064 . .059 .078 .062 .039 .048 . 4.632 2.715 3.724 4.001 0.904 0. 0.784 0.820 5.163 0.979 5.550 1."
        },
        {
            "title": "FMA",
            "content": "14.478 1.827 8.357 1.006 6.398 7. 1.100 1.405 4.690 1.592 6.890 1. .275 .282 .282 .277 .270 . .065 .079 .075 .043 .070 . 2.209 1.995 0.259 0.356 0.927 0. 1.335 0.209 2.078 0.258 1.984 0. 1.013 0.784 0.287 0.344 0.494 0. 0.708 0.222 0.956 0.327 0.631 0. .358 .339 .340 .358 .338 . .141 .153 .158 .125 .133 . tokenizer on both test datasets, providing an upper bound on the potential quality of generated outputs. Our evaluation protocol follows the same setup as in the text-to-audio experiments. Specifically, we use three objective metrics: FAD(Gui et al., 2024) computed with the VGGish model to assess audio quality, KLD between PASST classifier outputs(Koutini et al., 2022) for semantic consistency, and CLAP score (Wu et al., 2023c; Huang et al., 2023) to measure alignment with textual prompts. Results and Discussion. The evaluation scores on MusicCaps and FMA test set are presented in Table 13. Surprisingly, the evaluation score on the MusicCaps is consistently better than those on the FMAtest, despite the theoretical similarity between the FMA-test and the FMA training data. Compared to MusicCaps, the FMA training set only provides very limited text prompts and compromised sound quality. However, thanks to its large dataset size and publicity, it is natural choice for open-sourced model training (Evans et al., 2025). We observe that models trained on FMA are to some extent adaptable to other datasets. We believe the quality of the FMA dataset explains the lower evaluation scores on the FMA-test. On this issue, we also want to call for efforts within this community to make prompt-rich text-to-music datasets publicly available. Regarding reconstruction scores, EnCodec-32k tokenizer (despite being trained exclusively on music) does not consistently produce the highest quality. WavTokenizer achieves the best FAD score for reconstructed audio. DAC-44k and DAC-24k achieve the lowest KLD scores, indicating strong preservation of acoustic content. For text consistency, EnCodec-24k and DAC-24k perform best based on CLAP scores. Overall, DAC tokenizers at both sampling rates show the strongest reconstruction performance across metrics. For text-conditioned generation, the music-specific tokenizer (EnCodec-32k) demonstrates clear advantages across all metrics and evaluation datasets. Among the multi-domain tokenizers, DAC-44k consistently outperforms DAC-24k, EnCodec-24k, and the single-stream WavTokenizer. This performance gap may stem 30 (a) Speech Ranking (b) Audio Ranking (c) Music Ranking Figure 4: Average ranking of audio tokenizers across three domains: speech, general audio, and music. from DAC-44ks higher bitrate, which likely enables it to produce richer and more expressive representation, an essential factor for effective language modeling in music generation tasks. For the unconditional generation task, EnCodec-32k and DAC-44k generally produce higher-quality outputs. An exception is observed on the FMA dataset, where SQ-Codec achieves better generation quality, as indicated by the FAD score. Across both datasets, unconditional generation consistently outperforms text-conditioned generation. We attribute this to the higher CFG dropout rate used during training, which exposes the models to more unconditioned scenarios and improves their ability to generate coherent continuations. Additionally, the poor quality of text prompts in the FMA dataset, also evident from its lower reconstruction scores, likely hinders the models performance on text-conditioned tasks. Summary. For music LM, we observe that tokenizers with higher sample rates and multi-codebook, associated with higher bitrates, tend to perform better. This contrasts with audio and speech generation, where higher bitrate tokenizers were harder to model. We hypothesize that music, with its complex harmonic and temporal structure, benefits more from detailed representations, whereas such granularity may be excessive or less critical for general audio tasks. Additionally, unconditional generation consistently outperforms textconditioned generation, emphasizing the benefits of providing melody prompts in music generation tasks."
        },
        {
            "title": "3.4 General Trend",
            "content": "Figure 4 summarizes the overall ranking of audio tokenizers in three domains: speech, general audio, and music. These rankings are computed by first sorting the performance of each tokenizer per metric within each 31 task (with rank 1 as worst and rank as best for tokenizers), then averaging the ranks across all tasks in the respective category. For tasks with multiple metrics, rankings are computed separately for each metric and then averaged. The resulting radar charts illustrate the average rank of each tokenizer across the following high-level categories, with higher values indicating better performance. For speech, we distinguish between two types of reconstruction: Reconstruction (Signal), which captures low-level fidelity (e.g., UTMOS), and Reconstruction (Application), which reflects the performance of resynthesized audio in downstream tasks (e.g., WER for ASR, speaker similarity for SV). Downstream tasks are categorized as either Discriminative or Generative. In speech, Discriminative tasks are further split into Content-level, which require higher-level semantic understanding (e.g., ASR, intent classification, keyword spotting), and Acoustic-level, which depend more on fine-grained acoustic cues (e.g., emotion recognition, speaker ID, speaker verification). Language modeling tasks are grouped into two types: Text-conditioned and Unconditional (i.e., continuation-based generation). For speech, we include SpeechLM with separate subcategories for phonetic and acoustic metrics. The radar charts highlight general trends in tokenizer strengths and weaknesses across domains. No tokenizer consistently outperforms others on all axes. The performance is strongly taskand domain-dependent. Some models excel at reconstruction but fall short in semantic modeling, while others achieve strong downstream results despite poorer signal fidelity. These plots are not meant to give strict recommendations, but rather to provide high-level overview of performance trends. For real-world applications, we encourage referring to the full benchmark tables and task-specific analyses to identify the most appropriate tokenizer for the target use case."
        },
        {
            "title": "4 Ablation Studies",
            "content": "While the above discussion has extensively evaluated publicly available discrete audio tokens across various applications, conducting fair comparisons between these codecs remains challenging. The development of audio tokenizers inherently involves numerous hyperparameters, including codebook setups, quantization algorithms, and training data composition, all of which significantly influence performance. This variability in model design and implementation creates substantial obstacles for researchers attempting to make meaningful comparisons, ultimately hindering both comprehensive understanding of existing approaches and systematic exploration of new audio tokenizer designs. In this section, we aim to mitigate this issue by conducting experiments in carefully controlled setup. Specifically, we use ESPnet-Codec (Shi et al., 2024c) as the base training framework to evaluate the effects of training data, codebook setups, quantization methods, and pre-trained model distillation."
        },
        {
            "title": "4.1 Experimental Setups",
            "content": "To ensure reproducibility and isolate key variables in our experimental investigation, we establish methodical framework that controls for potential confounding factors. First, we present our training data in various domains and sampling rates. Next, we establish uniform implementation protocols for model architecture and hyperparameter configuration, enabling direct performance comparisons across model variants. Building on this controlled foundation, we present comprehensive set of experimental models, each systematically varying only the target parameters under investigation. Finally, we detail our evaluation methodology, including metrics and testing environments, to provide consistent basis for assessing model performance and drawing meaningful conclusions about codec effectiveness. Training Dataset. We prepare the dataset in three major domains, including speech, general audio, and music. All data in the three domains is sourced from the AMUSE dataset discussed in ESPnet-Codec (Shi et al., 2024c). The AMUSE dataset is combination of high-quality datasets for codec training purposes. For speech, it contains DAPS, DNS Challenge 4, Commonvoice, VCTK, AISHELL3, Googlei18n-TTS corpora, and Mexican endangered languages (Dubey et al., 2024; Kuhn et al., 2014; Yamagishi et al., 2019; Shi et al., 2021a;b; Amith et al., 2021; Amith & López Francisco, 2022; Amith & Castillo Castillo, 2021; Ardila et al., 2020; Shi et al., 2021d). For general audio, it contains all data from the AudioSet unbalanced training set (Gemmeke et al., 2017). For music, it contains MusDB, Jamendo, OpenSinger, StyleSing111, M4Singer, Kiritan-singing, Oniku Kurumi Utagoe database, Natsume Singing database, Opencpop, ACE32 Table 14: Summary of models used in the ablation study across 16kHz and 44.1kHz setups. Models are grouped by quantization method, RVQ, SVQ, FSQ, and Unit-based, with all models using the DAC backbone except D, which adopts Uni-HifiGAN. S, A, and denote training on speech, general audio, and music, respectively. The + symbol indicates the use of distillation from SSL-based representations. Checkmarks () indicate domain-specific training data used for each model variant."
        },
        {
            "title": "Speech Audio Music",
            "content": "RVQ-S RVQ-S+ RVQ-A RVQ-M RVQ-3 SVQ-S SVQ-S+ SVQ-A SVQ-M SVQ-3 FSQ-S FSQ-A FSQ-M FSQ-"
        },
        {
            "title": "FSQ",
            "content": "K-means-S Unit-HifiGAN K-means - - - - - - - - - - - - - KiSing (excluding original voices), PJS, and JSUT singing (Rafii et al., 2017; Bogdanov et al., 2019; Huang et al., 2021; Dai et al., 2023; Zhang et al., 2022; Ogawa & Morise, 2021; Wang et al., 2022; Shi et al., 2024a; Koguchi et al., 2020; Takamichi et al., 2020). To ensure equal consideration of the three domains and reduce the effect of unbalanced data distributions, we randomly sample 1k hours of data from each domain over the AMUSE dataset. The following experiments are conducted in either one of the domain-specific datasets or combination of all three subsets. For each set of training data, we provide both 16 kHz version and 44.1 kHz version to consider the effect of different sampling rates. Model Implementation. For our experiments, we utilize the Descript Audio Codec (DAC) framework implemented in ESPnet-Codec (Shi et al., 2024c; Kumar et al., 2023) as the foundation for neural-codec training. To evaluate discrete SSL approaches, we employ the discrete unit-based HiFiGAN vocoder as implemented by Yan et al. (2023). Our ablation studies focus on two key aspects: quantization techniques and semantic distillation. For quantization, we compare three distinct approaches: RVQ, single-layer VQ, and FSQ. Additionally, following recent research demonstrating the efficacy of self-supervised learning representations as distillation targets for quantizer training (see Section 2 for more discussion), we incorporate semantic distillation variants for both our RVQ and VQ-based models to systematically evaluate its impact. Summary of Models. Candidate models are summarized in Table 14. For each model listed in the table, we conduct the training at 16 kHz and 44.1 kHz with the corresponding dataset. Here, we mostly follow the previous literature discussed in Section 2, where we ignore model setups with no or limited related work, such as the scenarios of using SSL-based distillation in audio or music domains or the use of Uni-HifiGAN for higher sampling rates. While we standardize core training parameters across all experimental conditions to ensure fair comparisons, we implement targeted customization for specific model variants to integrate 33 Table 15: Ablation experiments on audio tokenizer reconstruction performance (speech domain). SDR SI-SNR PESQ UTMOS DNSMOS P835 WER Spk Sim ModelSR(kHz)"
        },
        {
            "title": "Ground truth",
            "content": "RVQ-S RVQ-S+ RVQ-A RVQ-M RVQ-3 SVQ-S SVQ-S+ SVQ-A SVQ-M SVQ-3 FSQ-S FSQ-A FSQ-M FSQ-3 16 - 4.08 1. 0.59 2.80 2.46 -4.90 -4.45 -11. -5.19 -5.90 3.89 1.14 44.1 - 8.24 8.40 6.92 6.74 7.50 0. -0.64 -5.04 -4.70 -3.09 4.58 1. -1.08 -2.09 2.41 1.29 16 - 1.15 -1.77 -3.36 -0.68 -1.12 -13. -11.74 -33.46 -11.17 -15.13 1.75 -1. -4.39 0.01 44.1 16 44.1 44.1 16 - - - 4. 4.09 3.18 6.38 2.59 3.24 3. 3.62 5.67 4.27 4.61 4.63 -2. -3.64 -9.56 -7.89 -7.12 2.47 -1. -4.61 -1.28 2.22 2.02 2.00 2. 1.43 1.42 1.19 1.20 1.29 2. 1.94 1.39 1.97 1.05 3.30 2. 2.41 3.06 1.69 1.63 1.18 1. 1.79 2.10 1.74 1.22 1.79 - 3.12 1.81 1.64 2.71 2.19 2. 1.25 1.29 1.44 3.29 2.84 1. 3.06 2.28 3.65 3.15 2.33 3. 2.61 2.40 1.26 1.24 2.57 3. 2.41 1.26 2.57 - 3.16 3. 2.58 2.64 2.96 3.09 3.00 1. 2.19 3.10 3.21 3.15 2.66 3. 2.46 44.1 3.18 3.16 3.14 3. 2.68 3.08 3.10 3.07 1.97 1. 3.01 3.12 2.97 2.04 3.01 - 16 44.1 16 44.1 2.83 2. - - 2.04 2.63 0.67 0. 2.12 2.47 3.20 2.22 13.16 13. 31.40 14.88 22.24 3.57 5.12 3. 2.92 2.85 2.66 8.28 7.89 34. 33.87 6.71 4.02 7.59 24.46 20. 4.35 6.71 6.78 - 0.69 0. 0.44 0.61 0.35 0.36 0.19 0. 0.26 0.48 0.43 0.17 0.44 0. 0.89 0.73 0.56 0.87 0.53 0. 0.14 0.11 0.49 0.66 0.39 0. 0.49 - K-means-S -18.21 - -42. - different ablation factors. Complete documentation of both the standardized parameters and model-specific adjustments is available in our released model checkpoints20. Evaluation Setup. We follow the reconstruction evaluation protocol outlined in Section 3.1, adapting our methodology to accommodate the specific requirements of different data domains. For evaluation data, we utilize the LibriSpeech test-clean set (speech), the AudioSet test set (general audio), and the MUSDB test set (music). Sampling rate considerations necessitated domain-specific evaluation approaches. For speech reconstruction, all evaluations are conducted at 16 kHz, even when testing 44.1 kHz neural codecs, to maintain consistency with the source LibriSpeech dataset. For music evaluation, we use different sampling rates based on model capabilities: 16 kHz for models designed at that native rate, and 24 kHz for 44.1 kHz neural codecs, reflecting the upper-frequency limitations in the MUSDB test set. For general audio evaluation, we align the evaluation with the codec models. These adjustments ensure fair comparisons while respecting both technical constraints and the inherent characteristics of each dataset."
        },
        {
            "title": "4.2 Results and Discussion",
            "content": "The results of our ablation study are shown in Table 15 and Table 16. We summarize our findings as follows: Data Domains. Domain alignment between training and testing data has emerged as critical determinant of performance in discrete audio representation modeling. Our experiments confirm that reconstruction quality consistently peaks when models are evaluated on domains matching their training data. More significantly, we observe that even with carefully balanced multi-domain training datasets, models still exhibit notable performance degradation when assessed on individual domains compared to domain-specific training. These challenges have become increasingly relevant in light of recent audio foundation models, which aim for broad generalization across diverse audio types. Our findings highlight the need for two crucial research directions: developing more effective methodologies for balancing domain-specific optimization, and addressing the fundamental challenges of cross-domain generalization in discrete audio representation learning. 20https://huggingface.co/collections/espnet/codec-survey-pre-trained-models-67ce8e09568b741d1c4483c8 34 Table 16: Ablation experiments on audio tokenizer reconstruction performance (audio and music domain). ModelSR(kHz) 16 44.1 16 44.1 16 44. 16 44.1 16 44.1 16 44. 16 44.1 16 44.1 16 44. 16 44.1 SDR CI-SDR SI-SNR VISQOL SingMOS SDR CI-SDR SI-SNR VISQOL SingMOS Audio Music RVQ-S RVQ-S+ RVQ-A RVQ-M RVQ-3 SVQ-S SVQ-S+ SVQ-A SVQ-M SVQFSQ-S FSQ-A FSQ-M FSQ-3 -4.05 -8. -0.59 0.65 0.14 -14.80 -14.74 -13. -6.53 -9.28 -7.32 -2.79 -3.37 -3. 2.85 2.52 3.65 3.76 3.99 -8. -9.39 -6.80 -5.50 -7.47 -4.26 -2. -3.25 -2.36 -4.02 -8.83 -0.61 0. 0.11 -14.72 -14.66 -13.00 -6.47 -9. -7.26 -2.76 -3.33 -3.19 2.51 2. 3.21 3.35 -10.07 -17.57 -5.56 -4. 3.50 -4.47 -0.02 -1.14 0.48 1. 0.70 -8.07 -9.00 -6.46 -5.19 -7. -4.04 -1.00 -3.05 -2.24 -31.86 -14. -30.69 -16.37 -30.70 -12.80 -13.43 -10. -19.50 -14.32 -14.22 -7.05 -8.23 -7. -8.12 -5.37 -6.85 -6.01 K-means-S -21. - -20.89 - -47.37 - 4. 4.13 4.22 4.13 4.17 3.83 3. 3.86 3.94 3.88 4.05 4.09 4. 4.06 3.14 3.81 3.79 3.78 3. 3.83 3.28 3.23 3.32 3.33 3. 3.32 3.53 3.41 3.53 2.59 2. 2.63 2.63 2.61 2.55 2.54 2. 2.59 2.52 2.60 2.63 2.62 2. 2.66 2.63 2.65 2.63 2.65 2. 2.59 2.59 2.58 2.62 2.63 2. 2.60 2.62 0.45 -6.73 5.78 6. 5.75 -14.25 -15.10 -6.54 -0.35 -2. 6.80 6.60 8.24 8.33 8.54 -4. -5.24 -0.49 0.52 -1.35 0.44 -6. 5.70 6.25 5.68 -14.22 -15.06 -6. -0.35 -2.73 6.75 6.55 8.17 8. 8.46 -4.76 -5.23 -0.40 0.52 -1. -5.04 -0.37 -5.02 -0.37 0.90 1. 0.89 2.62 2.70 2.75 0.80 1. 0.88 2.60 2.77 2.73 -2.29 -11. 2.87 3.82 3.32 -27.16 -27.69 -14. -2.94 -6.75 -8.42 -1.14 -0.67 -1. 4.83 4.46 5.97 6.48 6.34 -7. -8.80 -3.21 -1.80 -4.63 -2.84 0. 0.66 0.19 - 2.78 - -19. - -19.49 - -46.03 - 4. 4.17 4.07 4.24 4.12 3.99 4. 3.64 3.60 3.52 3.33 3.54 3. 3.92 3.54 3.74 4.21 4.13 4. 3.68 3.69 3.68 3.87 3.72 3. 4.00 3.99 3.96 2.82 2.67 2. 2.71 2.70 2.68 2.59 2.58 2. 2.70 2.63 2.69 2.70 2.71 2. 2.60 2.70 2.72 2.68 2.72 2. 2.68 2.69 2.63 2.73 2.71 2. 2.66 2.68 - 2.87 - Sampling Rate. While prior research has rarely examined sampling rate effects on discrete audio representation, this gap is largely due to methodological challenges in creating controlled comparisons across different rate conditions. Our systematic ablation study addresses this limitation and reveals sampling rate as significant factor in model performance. As shown in Table 15 and Table 16, RVQ-based models consistently demonstrate performance improvements across multiple evaluation metrics when trained at 44.1 kHz, even when the reconstructed audio is downsampled to 16 kHz for evaluation. These benefits, however, are not universal across all quantization approaches. Models utilizing Finite Scalar Quantization (FSQ) actually exhibited performance degradation on several metrics when trained at higher sampling rates. This contrasting behavior indicates that the relationship between sampling rate and model effectiveness is contingent on the specific quantization methodology employed. Based on these findings, we recommend that future research on discrete audio representation should incorporate sampling rate as critical design parameter, with careful optimization based on the selected quantization approach and target application domain. Distillation Effect. Prior studies involving distillation from pre-trained models have frequently demonstrated that such distillation supports comparable or improved signal reconstruction, while providing substantial performance benefits for downstream tasks (Du et al., 2023; Défossez et al., 2024; Zhang et al., 2024a). In our controlled ablation analyses, we similarly observed that incorporating distillation from pretrained speech representations can enhance model performance on certain metrics for signal reconstruction, as demonstrated by our comparison between models trained with and without distillation (e.g., A-S vs. A-S+). However, it should be noted that the domain-specific nature of the pre-trained model may limit generalization, especially when applied to broader domains such as general audio or music, as evidenced in Table 16. This highlights potential trade-off between achieving high performance in specialized tasks and maintaining broader generalization capabilities. Quantization Methods. Our experiments demonstrate that different quantization methods significantly impact codec performance. The RVQ modeling consistently outperforms other quantization approaches across most reconstruction metrics. Conversely, SVQ models typically yield the poorest results. An interesting exception emerges with FSQ at 16 kHz, which surpasses RVQ in speech quality metrics measured by UTMOS and DNSMOS. Generally, RVQ demonstrates higher potential for audio quality due to its highfidelity reconstruction capabilities. However, we caution readers that the performance alignment between audio reconstruction and downstream applications is not guaranteed. The metrics observed in this study may not directly translate to broader application performance, and further research is needed to establish definitive correlations."
        },
        {
            "title": "5 Conclusion and Future Directions",
            "content": "Discrete units offer several advantages in audio representation. They provide compact, modular, and scalable abstractions that are particularly well-suited for generative tasks such as speech synthesis, music generation, and general audio modeling. By converting regression-based waveform modeling into classification tasks, discrete tokenization simplifies both training and inference. These representations are also more efficient in terms of storage and transmission, making them advantageous for resource-constrained deployment and streaming applications. Additionally, discrete tokens align naturally with large language model architectures, facilitating integration into multimodal systems. While discrete representations may currently underperform continuous features on certain discriminative tasks, particularly in low-resource or semantically fine-grained scenarios, recent advances show that they can match or even outperform continuous representations in some applications, such as text-to-speech, especially when trained on large-scale data. These trends highlight the growing promise of discrete audio tokenization and motivate continued research to improve their robustness, expressiveness, and generalization across diverse downstream tasks. Based on our comprehensive analysis, we identify several key observations and open challenges in the design, evaluation, and application of discrete audio tokenizers. These point to promising future research directions: Scaling Limitations and Generalizability: While our experiments used moderately sized models and datasets, this choice was intentional to ensure fair and controlled comparisons across tokenizers. These settings reflect realistic constraints for many academic and open-source efforts. Importantly, our key findings, such as the superior performance of semantic over acoustic tokenizers for semantic tasks, are aligned with trends observed in larger-scale systems, suggesting that these insights are likely to hold as models and datasets scale up. Correlation between Reconstruction and Downstream Performance: We observed clear trade-off between high-fidelity signal reconstruction and downstream task performance. Optimizing tokenizers purely for reconstruction often fails to preserve task-relevant features such as phonetics or semantic content. This is especially evident in tasks where the decoder is not involved (e.g., ASR or SLU). Future research should aim to jointly optimize for both signal fidelity and semantic utility, possibly using multi-task or adversarial training. Fair and Consistent Evaluation: Tokenizers vary significantly in training data, sampling rate, and domain scope (e.g., speech-only vs. multi-domain). These discrepancies complicate benchmarking. Our study highlights the importance of standardizing evaluation pipelines to allow fair comparison across tokenizers. Establishing unified benchmarks with consistent experimental settings remains an urgent need. Benchmark vs. Reported Performance Gap: Results reproduced under controlled benchmark settings often fall short of the originally reported numbers. This indicates that some improvements reported in prior works may rely on favorable hyperparameter tuning or large-scale resources. There is need for reproducibility-focused evaluations and scaling studies to better understand real-world tokenizer performance. Semantic Distillation Beyond Speech: Most existing distillation-based tokenizers focus exclusively on speech. Extending semantic distillation techniques to music and general audio domains is an underexplored direction that could substantially improve discrete token quality across diverse audio tasks. Discrete vs. Continuous Representations: While discrete tokenizers have shown promising progress, continuous representations often remain superior for speech-language understanding tasks that rely on preserving fine-grained acoustic cues such as prosody, emotion, and speaker traits. However, this performance gap may not be universal. For instance, discrete tokens can be more suitable in settings involving autoregressive or masked generative modeling, where classificationbased objectives are advantageous. Conversely, models based on score matching or diffusion may still benefit from continuous conditioning inputs. Bridging these differences remains an open challenge for integrating audio tokenizers into multimodal LLMs that require semantic richness. 36 Toward Unified Tokenizers: Future systems may benefit from unified tokenizers that can support both generative and discriminative tasks across multiple audio domains. Achieving this will likely require architectures that balance streamability, semantic alignment, reconstruction quality, and domain generalization, possibly via modular or hierarchical designs. Trustworthiness: Trustworthy concerns such as bias (Ren et al., 2024a) and deepfakes (Wu et al., 2024e; Du et al., 2025) are required to be considered. Modern discrete-unit-based speech generation can mimic voices with human-like realism (Chen et al., 2025a), raising risks of misuse by malicious actorse.g., generating fake news using public figures voice. In summary, while some questions about audio tokens remain open, our evaluation provides comprehensive perspective that highlights general trends, key challenges, and guidelines for selecting tokenizer. We hope our study offers valuable insights to the research community and helps pave the way for future advancements in this rapidly progressing field."
        },
        {
            "title": "Author Contributions",
            "content": "All core members participated equally in the project. While this section outlines the primary responsibilities of each core contributor, many also contributed to other aspects of the project. Pooneh Mousavi led the overall survey and benchmarking effort. Pooneh Mousavi, Haibin Wu, and Anastasia Kuznetsova co-led the design of the tokenizer taxonomy, with collaboration with Haici Yang and Gallil Maimon. Haibin Wu, Jiatong Shi, and Darius Petermann conducted the reconstruction experiments. Pooneh Mousavi led the development of the downstream section, with contributions from Darius Petermann, and Anastasia Kuznetsova. Adel Moumen led the development of the SpeechLM evaluations. Artem Ploujnikov was responsible for the TTS evaluation experiments. Gallil Maimon conducted the AudioLM experiments. Haici Yang led the MusicLM evaluation. Jiatong Shi performed the ablation study in controlled setup. All authors participated in writing, editing, and refining the paper. The advisors provided guidance and critical feedback throughout the project."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Jinchuan Tian for valuable discussions with TTS. We also greatly thank Dongchao Yang for sharing his in-depth, hands-on experience with SQ-Codec development and thoughts about the tokenizer taxonomy design, which helped clarify important aspects of our survey. We are grateful to Huizhong Lu for computing support. We thank the NVIDIA Academic Grant Program for donating GPU hours used for this project. This work was supported by the Cambridge Commonwealth, European & International Trust (scholarship to Adel Moumen); the Natural Sciences and Engineering Research Council of Canada (NSERC); the Digital Research Alliance of Canada (alliancecan.ca); The Israel Science Foundaton, grant 2049/22 (scholarship to Gallil Maimon); an Amazon Research Award (ARA); Electronics and Telecommunications Research Institute (ETRI) grant funded by the Korean government [24ZC1100, The research of the basic media/contents technologies]. We also thank Jean Zay GENCI-IDRIS for their support in computing (Grant 2024-AD011015344 and Grant 2024A0161015099). Cem Subakan is supported by Discovery Grant RGPIN 2023-05759. Part of the experiments used the Bridges2 at PSC and Delta/DeltaAI NCSA computing systems through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, supported by National Science Foundation grants 2138259, 2138286, 2138307, 2137603, and 2138296."
        },
        {
            "title": "References",
            "content": "Andrea Agostinelli et al. MusicLM: Generating music from text. arXiv preprint arXiv:2301.11325, 2023. Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and Luc Gool. Soft-to-hard vector quantization for end-to-end learning compressible representations. In Proc. NeurIPS, 2017. 37 Sung Hwan Ahn, Beom Jun Woo, Mingrui Han, Chan Yeong Moon, and Nam Soo Kim. Hilcodec: Highfidelity and lightweight neural audio codec. IEEE Journal of Selected Topics in Signal Processing, 18: 15171530, 2024. Yang Ai, Xiao-Hang Jiang, Ye-Xin Lu, Hui-Peng Du, and Zhen-Hua Ling. Apcodec: neural audio codec with parallel amplitude and phase spectrum encoding and decoding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. Jonathan D. Amith and Rey Castillo Castillo. Audio corpus of yoloxóchitl mixtec with accompanying timecoded transcriptions in elan, 2021. URL https://www.openslr.org/89/. Jonathan D. Amith and Osbel López Francisco. Audio corpus of totonac recordings from northern puebla and adjacent areas of veracruz, 2022. URL https://www.openslr.org/107/. Jonathan D. Amith, Amelia Domínguez Alcántara, Hermelindo Salazar Osollo, Ceferino Salgado Castañeda, and Eleuterio Gorostiza Salazar. Audio corpus of sierra nororiental and sierra norte de puebla nahuat(l) with accompanying time-code transcriptions in elan, 2021. URL https://www.openslr.org/92/. Mohamed Anees. Speech coding techniques and challenges: comprehensive literature survey. Multimedia Tools and Applications, 83(10):2985929879, 2024. Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common Voice: massively-multilingual speech corpus. In Proc. LREC, 2020. Siddhant Arora, Kai-Wei Chang, Chung-Ming Chien, Yifan Peng, Haibin Wu, Yossi Adi, Emmanuel Dupoux, Hung-Yi Lee, Karen Livescu, and Shinji Watanabe. On the landscape of spoken language models: comprehensive survey. arXiv preprint arXiv:2504.08528, 2025. Bishnu Atal. Speech analysis and synthesis by linear prediction of the speech wave. The journal of the acoustical society of America, 47(1A_Supplement):6565, 1970. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. In Proc. NeurIPS, 2020. He Bai, Tatiana Likhomanenko, Ruixiang Zhang, Zijin Gu, Zakaria Aldeneh, and Navdeep Jaitly. Dmel: Speech tokenization made simple. arXiv preprint arXiv:2407.15835, 2024. Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser. SLURP: spoken language understanding resource package. In Proc. EMNLP, 2020. Xiaoyu Bie, Xubo Liu, and Gaël Richard. Learning source disentanglement in neural audio codec. In Proc. ICASSP, 2025. Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra. The MTG-Jamendo dataset for automatic music tagging. In Proc. ICML, 2019. Zalán Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco Tagliasacchi. SoundStorm: Efficient parallel audio generation. arXiv preprint arXiv:2305.09636, 2023a. Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. AudioLM: language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:25232533, 2023b. Xavier Bouthillier, Christos Tsirigotis, François Corneau-Tremblay, Thomas Schweizer, Lin Dong, Pierre Delaunay, Fabrice Normandin, Mirko Bronzi, Dendi Suhubdy, Reyhane Askari, Michael Noukhovitch, Chao Xue, Satya Ortiz-Gagné, Olivier Breuleux, Arnaud Bergeron, Olexa Bilaniuk, Steven Bocco, Hadrien Bertrand, Guillaume Alain, Dmitriy Serdyuk, Peter Henderson, Pascal Lamblin, and Christopher Beckham. Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization, Aug 2022. URL https://doi.org/10.5281/zenodo.3478592. 38 Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette IEMOCAP: Interactive emotional dyadic motion Chang, Sungbok Lee, and Shrikanth Narayanan. capture database. Language resources and evaluation, 42:335359, 2008. Edresson Casanova, Ryan Langman, Paarth Neekhara, Shehzeen Hussain, Jason Li, Subhankar Ghosh, Ante Jukić, and Sang-gil Lee. Low frame-rate speech codec: codec designed for fast high-quality speech LLM training and inference. In Proc. ICASSP, 2025. Kai-Wei Chang, Haibin Wu, Yu-Kai Wang, Yuan-Kuei Wu, Hua Shen, Wei-Cheng Tseng, Iu-thing Kang, Shang-Wen Li, and Hung-yi Lee. Speechprompt: Prompting speech language models for speech processing tasks. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, and Shinji Watanabe. Exploration of efficient end-to-end ASR using discretized input from self-supervised learning. In Proc. Interspeech, 2023. Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Musicldm: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies. In Proc. ICASSP, 2024a. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. WavLM: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518, 2022. Sanyuan Chen, Chengyi Wang, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Neural codec language models are zero-shot text to speech synthesizers. IEEE Transactions on Audio, Speech and Language Processing, 33:705718, 2025a. William Chen, Wangyou Zhang, Yifan Peng, Xinjian Li, Jinchuan Tian, Jiatong Shi, Xuankai Chang, Soumi Maiti, Karen Livescu, and Shinji Watanabe. Towards robust speech representation learning for thousands of languages. In Proc. EMNLP, 2024b. Ziyang Chen, Prem Seetharaman, Bryan Russell, Oriol Nieto, David Bourgin, Andrew Owens, and Justin Salamon. Video-guided foley sound generation with multimodal controls. In Proc. CVPR, 2025b. Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. Self-supervised learning with random-projection quantizer for speech recognition. In Proc. ICML, 2022. Kwanghee Choi, Ankita Pasad, Tomohiko Nakamura, Satoru Fukayama, Karen Livescu, and Shinji Watanabe. Self-supervised speech representations are more phonetic than semantic. In Proc. Interspeech, 2024. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. Yoonjin Chung, Pilsun Eu, Junwon Lee, Keunwoo Choi, Juhan Nam, and Ben Sangbae Chon. Kad: No more fad! an effective and efficient evaluation metric for audio generation. arXiv preprint arXiv:2502.15602, 2025. Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. w2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In Proc. ASRU, 2021. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez. Simple and controllable music generation. In Proc. NeurIPS, 2023. Joris Cosentino, Manuel Pariente, Samuele Cornell, Antoine Deleforge, and Emmanuel Vincent. LibriMix: An open-source dataset for generalizable speech separation. arXiv preprint arXiv:2005.11262, 2020. Santiago Cuervo, Adel Moumen, Yanis Labrak, Sameer Khurana, Antoine Laurent, Mickael Rouvier, and Ricard Marxer. Text-speech language models with improved cross-modal transfer by aligning abstraction levels. arXiv preprint arXiv:2503.06211, 2025. Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, and Irwin King. Recent advances in speech language models: survey. arXiv preprint arXiv:2410.03751, 2024. Shuqi Dai, Siqi Chen, Yuxuan Wu, Ruxin Diao, Roy Huang, and Roger Dannenberg. Singstyle111: multilingual singing dataset with style transfer. In Proc. ISMIR, 2023. Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. FMA: dataset for music analysis. In Proc. ISMIR, 2017. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. Transactions on Machine Learning Research, 2023. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Luca Della Libera, Francesco Paissan, Cem Subakan, and Mirco Ravanelli. Focalcodec: Low-bitrate speech coding via focal modulation networks. arXiv preprint arXiv:2502.04465, 2025. Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck. ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification. In Proc. Interspeech, 2020. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL, 2019. Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: generative model for music. arXiv preprint arXiv:2005.00341, 2020. Martin Dietz, Markus Multrus, Vaclav Eksler, Vladimir Malenovsky, Erik Norvell, Harald Pobloth, Lei Miao, Zhe Wang, Lasse Laaksonen, Adriana Vasilache, et al. Overview of the EVS codec architecture. In Proc. ICASSP, 2015. Hao-Wen Dong, Xiaoyu Liu, Jordi Pons, Gautam Bhattacharya, Santiago Pascual, Joan Serrà, Taylor BergKirkpatrick, and Julian McAuley. Clipsonic: Text-to-audio synthesis with unlabeled videos and pretrained language-vision models. In Proc. WASPAA, 2023. Jiawei Du, Xuanjun Chen, Haibin Wu, Lin Zhang, Lin, Chiu, Wenze Ren, Yuan Tseng, Yu Tsao, JyhShing Roger Jang, et al. Codecfake-omni: large-scale codec-based deepfake speech dataset. arXiv preprint arXiv:2501.08238, 2025. Zhihao Du, Shiliang Zhang, Kai Hu, and Siqi Zheng. FunCodec: fundamental, reproducible and integrable open-source toolkit for neural speech codec. arXiv preprint arXiv:2309.07405, 2023. Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024a. Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024b. Harishchandra Dubey, Ashkan Aazami, Vishak Gopal, Babak Naderi, Sebastian Braun, Ross Cutler, Alex Ju, Mehdi Zohourian, Min Tang, Mehrsa Golestaneh, et al. ICASSP 2023 deep noise suppression challenge. IEEE Open Journal of Signal Processing, 2024. 40 Ewan Dunbar, Mathieu Bernard, Nicolas Hamilakis, Tu Anh Nguyen, Maureen de Seyssel, Patricia Rozé, Morgane Rivière, Eugene Kharitonov, and Emmanuel Dupoux. The zero resource speech challenge 2021: Spoken language modelling. In Proc. Interspeech, 2021. Avishai Elmakies, Omri Abend, and Yossi Adi. Unsupervised speech segmentation: general approach using speech language models. arXiv preprint arXiv:2501.03711, 2025. Hakan Erdogan, Scott Wisdom, Xuankai Chang, Zalán Borsos, Marco Tagliasacchi, Neil Zeghidour, and John Hershey. TokenSplit: Using discrete speech representations for direct, refined, and transcriptconditioned speech separation and recognition. In Proc. Interspeech, 2023. Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. In Proc. ICASSP, 2025. Cristina Gârbacea, Aäron van den Oord, Yazhe Li, Felicia SC Lim, Alejandro Luebs, Oriol Vinyals, and Thomas Walters. Low bit-rate speech coding with vq-vae and wavenet decoder. In Proc. ICASSP, 2019. Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, and Bryan Pardo. Vampnet: Music generation via masked acoustic token modeling. In Proc. ISMIR, 2023. Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio Set: An ontology and human-labeled dataset for audio events. In Proc. ICASSP, 2017. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11): 139144, 2020. Yuzhe Gu and Enmao Diao. Esc: Efficient speech coding with cross-scale residual vector quantized transformers. In Proc. EMNLP, 2024. Azalea Gui, Hannes Gamper, Sebastian Braun, and Dimitra Emmanouilidou. Adapting frechet audio distance for generative music evaluation. In Proc. ICASSP, 2024. Haohan Guo, Fenglong Xie, Kun Xie, Dongchao Yang, Dake Guo, Xixin Wu, and Helen Meng. Socodec: semantic-ordered multi-stream speech codec for efficient language model based text-to-speech synthesis. In Proc. SLT, 2024a. Haohan Guo, Fenglong Xie, Dongchao Yang, Hui Lu, Xixin Wu, and Helen M. Meng. Addressing index collapse of large-codebook speech tokenizer with dual-decoding product-quantized variational auto-encoder. In Proc. SLT, 2024b. Yiwei Guo, Zhihan Li, Chenpeng Du, Hankun Wang, Xie Chen, and Kai Yu. Lscodec: Low-bitrate and speaker-decoupled discrete speech codec. In Proc. Interspeech, 2025a. Yiwei Guo, Zhihan Li, Hankun Wang, Bohan Li, Chongtian Shao, Hanglei Zhang, Chenpeng Du, Xie Chen, Shujie Liu, and Kai Yu. Recent advances in discrete speech tokens: review. arXiv preprint arXiv:2502.06490, 2025b. Nadav Har-Tuv, Or Tal, and Yossi Adi. Past: Phonetic-acoustic speech tokenizer. arXiv preprint arXiv:2505.14470, 2025. Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. Textually pretrained speech language models. In Proc. NeurIPS, 2023. Tomoki Hayashi and Shinji Watanabe. Discretalk: Text-to-speech as machine translation problem. arXiv preprint arXiv:2005.05525, 2020. 41 Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. HuBERT: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:34513460, 2021. Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel PW Ellis. Mulan: joint embedding of music audio and natural language. In Proc. ISMIR, 2022. Rongjie Huang, Feiyang Chen, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. Multi-singer: Fast multisinger singing voice vocoder with large-scale corpus. In Proc. ACMMM, 2021. Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. In Proc. ICML, 2023. Zhichao Huang, Chutong Meng, and Tom Ko. RepCodec: speech representation codec for speech tokenization. In Proc. ACL, 2024. Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, and Juan Pino. Unity: Two-pass direct speech-to-speech translation with discrete units. In Proc. ACL, 2023. Fumitada Itakura. Analysis synthesis telephony based on the maximum likelihood method. Reports of the 6th Int. Cong. Acoust., 1968. Rhutuja Jage and Savitha Upadhya. Celp and melp speech coding techniques. In Proc. WiSPNET, 2016. E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. In Proc. ICLR, 2017. Yujin Jeong, Yunji Kim, Sanghyuk Chun, and Jiyoung Lee. Read, watch and scream! sound generation from text and video. In Proc. AAAI, 2025. Shengpeng Ji, Yifu Chen, Minghui Fang, Jialong Zuo, Jingyu Lu, Hanting Wang, Ziyue Jiang, Long Zhou, Shujie Liu, Xize Cheng, et al. Wavchat: survey of spoken dialogue models. arXiv preprint arXiv:2411.13577, 2024a. Shengpeng Ji, Minghui Fang, Ziyue Jiang, Rongjie Huang, Jialung Zuo, Shulei Wang, and Zhou Zhao. Language-codec: Reducing the gaps between discrete codec representation and speech language models. arXiv preprint arXiv:2402.12208, 2024b. Shengpeng Ji, Ziyue Jiang, Xize Cheng, Yifu Chen, Minghui Fang, Jialong Zuo, Qian Yang, Ruiqi Li, Ziang Zhang, Xiaoda Yang, et al. Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling. In Proc. ICLR, 2024c. Xiao-Hang Jiang, Yang Ai, Ruixin Zheng, Hui-Peng Du, Ye-Xin Lu, and Zhenhua Ling. Mdctcodec: lightweight mdct-based neural audio codec towards high sampling rate and low bitrate scenarios. In Proc. SLT, 2024. Xue Jiang, Xiulian Peng, Huaying Xue, Yuan Zhang, and Yan Lu. Cross-scale vector quantization for scalable neural speech coding. In Proc. Interspeech, 2022a. Xue Jiang, Xiulian Peng, Chengyu Zheng, Huaying Xue, Yuan Zhang, and Yan Lu. End-to-end neural speech coding for real-time communications. In Proc. ICASSP, 2022b. Xue Jiang, Xiulian Peng, Yuan Zhang, and Yan Lu. Disentangled feature learning for real-time neural speech coding. In Proc. ICASSP, 2023. Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Eric Liu, Yichong Leng, Kaitao Song, Siliang Tang, et al. Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models. In Proc. ICML, 2024. 42 Wei Kang, Xiaoyu Yang, Zengwei Yao, Fangjun Kuang, Yifan Yang, Liyong Guo, Long Lin, and Daniel Povey. Libriheavy: 50,000 hours asr corpus with punctuation casing and context. In Proc. ICASSP, 2024. Srihari Kankanahalli. End-to-end optimized speech coding with deep neural networks. In Proc. ICASSP, 2018. Ilya Kavalerov, Scott Wisdom, Hakan Erdogan, Brian Patton, Kevin W. Wilson, Jonathan Le Roux, and John R. Hershey. Universal sound separation. In Proc. WASPAA, 2019. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proc. NAACL, 2019. Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. In Proc. ICML, 2021. Minje Kim and Jan Skoglund. Neural speech and audio coding. arXiv preprint arXiv:2408.06954, 2024. W. Bastiaan Kleijn, Felicia S. C. Lim, Alejandro Luebs, Jan Skoglund, Florian Stimberg, Quan Wang, and Thomas C. Walters. Wavenet based low rate speech coding. In Proc. ICASSP, 2018. Junya Koguchi, Shinnosuke Takamichi, and Masanori Morise. PJS: Phoneme-balanced japanese singing-voice corpus. In Proc. APSIPA ASC, 2020. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: generative adversarial networks for efficient and high fidelity speech synthesis. In Proc. NeurIPS, 2020. Matěj Korvas, Ondřej Plátek, Ondřej Dušek, Lukáš Žilka, and Filip Jurčíček. Free English and Czech telephone speech corpus shared under the CC-BY-SA 3.0 license. In Proc. LREC, 2014. Khaled Koutini, Jan Schlüter, Hamid Eghbal-zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. In Proc. Interspeech, 2022. Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. AudioGen: Textually guided audio generation. In Proc. ICLR, 2023. Nicolas Kuhn, Emmanuel Lochin, Ahlem Mifdaoui, Golam Sarwar, Olivier Mehani, and Roksana Boreli. DAPS: Intelligent delay-aware packet scheduling for multipath transport. In Proc. ICC, 2014. Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved RVQGAN. In Proc. NeurIPS, 2023. Sonal Kumar, Prem Seetharaman, Justin Salamon, Dinesh Manocha, and Oriol Nieto. Sila: Signal-tolanguage augmentation for enhanced control in text-to-audio generation. arXiv preprint arXiv:2412.09789, 2024. Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:13361354, 2021. Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma, Xuchen Song, et al. Efficient neural music generation. In Proc. NeurIPS, 2023. Ryan Langman, Ante Jukić, Kunal Dhawan, Nithin Rao Koluguri, and Boris Ginsburg. Spectral codecs: Spectrogram-based audio codecs for high quality speech synthesis. arXiv preprint arXiv:2406.05298, 2024. Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Yi Ren, Heriberto Cuayáhuitl, Wenwu Wang, Xulong Zhang, Roberto Togneri, Erik Cambria, et al. Sparks of large audio models: survey and outlook. arXiv preprint arXiv:2308.12792, 2023. 43 Hanzhao Li, Liumeng Xue, Haohan Guo, Xinfa Zhu, Yuanjun Lv, Lei Xie, Yunlin Chen, Hao Yin, and Zhifei Li. Single-codec: Single-codebook speech codec towards high-performance speech generation. In Proc. Interspeech, 2024. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. In Proc. ICML, 2023. Haohe Liu, Xuenan Xu, Yi Yuan, Mengyue Wu, Wenwu Wang, and Mark Plumbley. Semanticodec: An IEEE Journal of Selected Topics in Signal ultra low bitrate semantic audio codec for general sound. Processing, 2024a. Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. AudioLDM 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024b. Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. In Proc. NeurIPS, 2023. Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal timefrequency magnitude masking for speech separation. IEEE/ACM Transactions on audio, speech, and language processing, 27(8):12561266, 2019. Chris Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: continuous relaxation of discrete random variables. In Proc. ICLR, 2017. Gallil Maimon and Yossi Adi. Speaking style conversion in the waveform domain using discrete self-supervised units. In Proc. EMNLP, 2023. Gallil Maimon, Avishai Elmakies, and Yossi Adi. Slamming: Training speech language model on one gpu in day. arXiv preprint arXiv:2502.15814, 2025a. Gallil Maimon, Michael Hassid, Amit Roth, and Yossi Adi. Scaling analysis of interleaved speech-text language models. arXiv preprint arXiv:2504.02398, 2025b. Gallil Maimon, Amit Roth, and Yossi Adi. Salmon: suite for acoustic language model evaluation. In Proc. ICASSP, 2025c. Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger. Montreal forced aligner: Trainable text-speech alignment using kaldi. In Proc. Interspeech, 2017. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. In Proc. ICLR, 2024. Shoval Messica and Yossi Adi. Nast: Noise aware speech tokenization for speech language models. In Proc. Interspeech, 2024. Irene Martin Morato and Annamaria Mesaros. Diversity and bias in audio captioning datasets. In Proc. DCASE, 2021. Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: vocoder-based high-quality speech synthesis system for real-time applications. IEICE TRANSACTIONS on Information and Systems, 99(7):18771884, 2016. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. corpus and cloze evaluation for deeper understanding of commonsense stories. In Proc. NAACL, 2016. Pooneh Mousavi, Luca Della Libera, Jarod Duret, Artem Ploujnikov, Cem Subakan, and Mirco Ravanelli. Dasbdiscrete audio and speech benchmark. arXiv preprint arXiv:2406.14294, 2024a. Pooneh Mousavi, Jarod Duret, Salah Zaiem, Luca Della Libera, Artem Ploujnikov, Cem Subakan, and Mirco Ravanelli. Semantic token tuning: How should we extract discrete audio tokens from self-supervised models? In Proc. Interspeech, 2024b. Pooneh Mousavi, Shubham Gupta, Cem Subakan, and Mirco Ravanelli. Listen: Learning soft token embeddings for neural audio llms. arXiv preprint arXiv:2505.18517, 2025. Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: large-scale speaker identification dataset. In Proc. Interspeech, 2017. Tu Anh Nguyen, Maureen de Seyssel, Patricia Rozé, Morgane Rivière, Evgeny Kharitonov, Alexei Baevski, Ewan Dunbar, and Emmanuel Dupoux. The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language modeling. In Proc. NeurIPS, 2020. Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Mary Williamson, Gabriel Synnaeve, Juan Pino, Benoît Sagot, and Emmanuel Dupoux. SpiRit-LM: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13: 3052, 2025. Zhikang Niu, Sanyuan Chen, Long Zhou, Ziyang Ma, Xie Chen, and Shujie Liu. NDVQ: Robust neural audio codec with normal distribution-based vector quantization. In Proc. SLT, 2024. Harry Nyquist. Certain topics in telegraph transmission theory. Transactions of the American Institute of Electrical Engineers, 47:617644, 1928. Itsuki Ogawa and Masanori Morise. Tohoku Kiritan singing database: singing database for statistical parametric singing synthesis using japanese pop songs. AST, 42(3):140145, 2021. Ahmed Omran, Neil Zeghidour, Zalán Borsos, Félix de Chaumont Quitry, Malcolm Slaney, and Marco Tagliasacchi. Disentangling speech from surroundings with neural embeddings. In Proc. ICASSP, 2023. Yu Pan, Xiang Zhang, Yuguang Yang, Jixun Yao, Yanni Hu, Jianhao Ye, Hongbin Zhou, Lei Ma, and Jianjun Zhao. Pscodec: series of high-fidelity low-bitrate neural speech codecs leveraging prompt encoders. arXiv preprint arXiv:2404.02702, 2024. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus based on public domain audio books. In Proc. ICASSP, 2015. Julian Parker, Anton Smirnov, Jordi Pons, CJ Carr, Zack Zukowski, Zach Evans, and Xubo Liu. Scaling transformers for low-bitrate high-quality speech coding. In Proc. ICML, 2025. Santiago Pascual, Chunghsin Yeh, Ioannis Tsiamas, and Joan Serrà. Masked generative video-to-audio transformers with enhanced synchronicity. In Proc. ECCV, 2024. Jing Peng, Yucheng Wang, Yu Xi, Xu Li, Xizhuo Zhang, and Kai Yu. survey on speech large language models. arXiv preprint arXiv:2410.18908, 2024. Darius Petermann, Seungkwon Beack, and Minje Kim. Harp-net: Hyper-autoencoded reconstruction propagation for scalable neural audio coding. In Proc. WASPAA, 2021. Karol Piczak. Esc: Dataset for environmental sound classification. In Proc. ACM, 2015. Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled self-supervised representations. In Proc. Interspeech, 2021. Sravya Popuri, Peng-Jen Chen, Changhan Wang, Juan Pino, Yossi Adi, Jiatao Gu, Wei-Ning Hsu, and Ann Lee. Enhanced direct speech-to-speech translation using self-supervised pre-training and data augmentation. In Proc. Interspeech, 2022. 45 Schuyler Quackenbush. Mpeg unified speech and audio coding. IEEE MultiMedia, 20(2):7278, 2013. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In Proc. ICML, 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Zafar Rafii, Antoine Liutkus, Fabian-Robert Stöter, Stylianos Ioannis Mimilakis, and Rachel Bittner. The MUSDB18 corpus for music separation, December 2017. URL https://doi.org/10.5281/zenodo. 1117372. Mirco Ravanelli, Titouan Parcollet, Adel Moumen, Sylvain de Langen, Cem Subakan, Peter Plantinga, Yingzhi Wang, Pooneh Mousavi, Luca Della Libera, Artem Ploujnikov, et al. Open-source conversational ai with speechbrain 1.0. Journal of Machine Learning Research, 25(333):111, 2024. Chandan KA Reddy, Vishak Gopal, and Ross Cutler. DNSMOS P.835: non-intrusive perceptual objective speech quality metric to evaluate noise suppressors. In Proc. ICASSP, 2022. Wenze Ren, Yi-Cheng Lin, Huang-Cheng Chou, Haibin Wu, Yi-Chiao Wu, Chi-Chun Lee, Hung-yi Lee, Hsin-Min Wang, and Yu Tsao. Emo-codec: An in-depth look at emotion preservation capacity of legacy and neural codec models with subjective and objective evaluations. In Proc. APSIPA ASC, 2024a. Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech: Fast, robust and controllable text to speech. In Proc. NeurIPS, 2019. Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast and high-quality end-to-end text to speech. In Proc. ICLR, 2021. Yong Ren, Tao Wang, Jiangyan Yi, Le Xu, Jianhua Tao, Chu Yuan Zhang, and Junzuo Zhou. Fewer-token neural speech codec with time-invariant codes. In Proc. ICASSP, 2024b. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proc. CVPR, 2022. Simon Rouard, Francisco Massa, and Alexandre Défossez. Hybrid transformers for music source separation. In Proc. ICASSP, 2023. Simon Rouard, Yossi Adi, Jade Copet, Axel Roebel, and Alexandre Défossez. Audio conditioning for music generation via discrete bottleneck features. In Proc. ISMIR, 2024. Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, et al. AudioPaLM: large language model that can speak and listen. arXiv preprint arXiv:2306.12925, 2023. Kohei Saijo, Gordon Wichern, François G. Germain, Zexu Pan, and Jonathan Le Roux. Tf-locoformer: Transformer with local modeling by convolution for speech separation and enhancement. In Proc. IWAENC, 2024. Koichi Saito, Dongjun Kim, Takashi Shibuya, Chieh-Hsin Lai, Zhi Zhong, Yuhta Takida, and Yuki Mitsufuji. SoundCTM: Uniting score-based and consistency models for text-to-sound generation. In Proc. NeurIPS Workshops, 2024. Robin San Roman, Yossi Adi, Antoine Deleforge, Romain Serizel, Gabriel Synnaeve, and Alexandre Défossez. From discrete tokens to high-fidelity audio using multi-band diffusion. In Proc. NeurIPS, 2023. Manfred Schroeder and Atal. Code-excited linear prediction (celp): High-quality speech at very low bit rates. In Proc. ICASSP, 1985. 46 Claude Shannon. mathematical theory of communication. The Bell system technical journal, 27(3): 379423, 1948. Roy Sheffer and Yossi Adi. hear your true colors: Image guided audio generation. In Proc. ICASSP, 2023. Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, Rif A. Saurous, Yannis Agiomvrgiannakis, and Yonghui Wu. Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions. In Proc. ICASSP, 2018. Jiatong Shi, Jonathan Amith, Xuankai Chang, Siddharth Dalmia, Brian Yan, and Shinji Watanabe. In Proc. Highland Puebla Nahuatl speech translation corpus for endangered language documentation. AmericasNLP, 2021a. Jiatong Shi, Jonathan Amith, Rey Castillo García, Esteban Guadalupe Sierra, Kevin Duh, and Shinji Watanabe. Leveraging end-to-end ASR for endangered language documentation: An empirical study on Yolóxochitl Mixtec. In Proc. EACL, 2021b. Jiatong Shi, Yueqian Lin, Xinyi Bai, Keyi Zhang, Yuning Wu, Yuxun Tang, Yifeng Yu, Qin Jin, and Shinji Watanabe. Singing voice data scaling-up: An introduction to ACE-Opencpop and ACE-KiSing. In Proc. Interspeech, 2024a. Jiatong Shi, Xutai Ma, Hirofumi Inaguma, Anna Sun, and Shinji Watanabe. MMM: Multi-layer multiresidual multi-stream discrete speech representation from self-supervised learning model. In Proc. Interspeech, 2024b. Jiatong Shi, Jinchuan Tian, Yihan Wu, Jee-weon Jung, Jia Qi Yip, Yoshiki Masuyama, William Chen, Yuning Wu, Yuxun Tang, Massa Baali, et al. ESPnet-Codec: Comprehensive training and evaluation of neural codecs for audio, music, and speech. In Proc. SLT, 2024c. Jiatong Shi, Hye-jin Shim, Jinchuan Tian, Siddhant Arora, Haibin Wu, Darius Petermann, Jia Qi Yip, You Zhang, Yuxun Tang, Wangyou Zhang, et al. VERSA: versatile evaluation toolkit for speech, audio, and music. In Proc. NAACL, 2025. Jing Shi, Xuankai Chang, Tomoki Hayashi, Yen-Ju Lu, Shinji Watanabe, and Bo Xu. Discretization and re-synthesis: an alternative method to solve the cocktail party problem. arXiv preprint arXiv:2112.09382, 2021c. Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li. AISHELL-3: multi-speaker mandarin TTS corpus. In Proc. Interspeech, 2021d. Amitay Sicherman and Yossi Adi. Analysing discrete self supervised speech representation for spoken language modeling. In Proc. ICASSP, 2023. Hubert Siuzdak. Vocos: Closing the gap between time-domain and fourier-based neural vocoders for highquality audio synthesis. In Proc. ICLR, 2024. Hubert Siuzdak, Florian Grötschla, and Luca Lanzendörfer. SNAC: Multi-scale neural audio codec. arXiv preprint arXiv:2410.14411, 2024. Saeki Takaaki et al. UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022. In Proc. Interspeech, 2022. Shinnosuke Takamichi, Ryosuke Sonobe, Kentaro Mitsui, Yuki Saito, Tomoki Koriyama, Naoko Tanji, and Hiroshi Saruwatari. JSUT and JVS: Free japanese voice corpora for accelerating speech synthesis research. AST, 41(5):761768, 2020. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang. SALMONN: Towards generic hearing abilities for large language models. In Proc. ICLR, 2024a. 47 Yuxun Tang, Yuning Wu, Jiatong Shi, and Qin Jin. SingOMD: Singing oriented multi-resolution discrete representation construction from speech models. In Proc. Interspeech, 2024b. Jinchuan Tian, Jiatong Shi, William Chen, Siddhant Arora, Yoshiki Masuyama, Takashi Maekaku, Yihan Wu, Junyi Peng, Shikhar Bharadwaj, Yiwen Zhao, et al. ESPnet-SpeechLM: An open speech language model toolkit. In Proc. NAACL, 2025. Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian FATRAS, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. George Tzanetakis and Perry Cook. Musical genre classification of audio signals. IEEE Transactions on speech and audio processing, 10(5):293302, 2002. Cassia Valentini-Botinhao, Xin Wang, Shinji Takaki, and Junichi Yamagishi. Investigating RNN-based speech enhancement methods for noise-robust text-to-speech. In Speech Synthesis Workshop, pp. 146152, 2016. Jean-Marc Valin and Jan Skoglund. LPCNet: Improving neural speech synthesis through linear prediction. In Proc. ICASSP, 2019a. Jean-Marc Valin and Jan Skoglund. real-time wideband neural vocoder at 1.6 kb/s using lpcnet. In Proc. Interspeech, 2019b. Jean-Marc Valin, Koen Vos, and Terriberry. Rfc 6716: Definition of the opus audio codec, 2012. Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Proc. NeurIPS, 2017. Benjamin van Niekerk, Marc-André Carbonneau, Julian Zaïdi, Matthew Baas, Hugo Seuté, and Herman Kamper. comparison of discrete and soft speech units for improved voice conversion. In Proc. ICASSP, 2022. Shikhar Vashishth, Harman Singh, Shikhar Bharadwaj, Sriram Ganapathy, Chulayuth Asawaroengchai, Kartik Audhkhasi, Andrew Rosenberg, Ankur Bapna, and Bhuvana Ramabhadran. STAB: Speech tokenizer assessment benchmark. arXiv preprint arXiv:2409.02384, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. NeurIPS, 2017. E. Vincent, R. Gribonval, and C. Fevotte. Performance measurement in blind audio source separation. IEEE Transactions on Audio, Speech, and Language Processing, 14(4):14621469, 2006. Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, and Weidong Cai. V2a-mapper: In Proc. AAAI, lightweight solution for vision-to-audio generation by connecting foundation models. 2024a. Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu Wei. Viola: Conditional language models for speech recognition, synthesis, and translation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 37093716, 2024b. 48 Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, and Takuya Yoshioka. Speechx: Neural codec language model as versatile speech transformer. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:33553364, 2024c. Ye Wang and Mikka Vilermo. Modified discrete cosine transform: Its implications for audio coding and error concealment. Journal of the Audio Engineering Society, 51(1/2):5261, 2003. Yingzhi Wang, Pooneh Mousavi, Artem Ploujnikov, and Mirco Ravanelli. What are they doing? joint audio-speech co-reasoning. In Proc. ICASSP, 2025a. Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao. Frieren: Efficient video-to-audio generation network with rectified flow matching. In Proc. NeurIPS, 2025b. Yu Wang, Xinsheng Wang, Pengcheng Zhu, Jie Wu, Hanzhao Li, Heyang Xue, Yongmao Zhang, Lei Xie, and Mengxiao Bi. Opencpop: high-quality open source chinese popular song corpus for singing voice synthesis. In Proc. Interspeech, 2022. Zhichao Wang, Yuanzhe Chen, Xinsheng Wang, Lei Xie, and Yuping Wang. Streamvoice: Streamable context-aware language modeling for real-time zero-shot voice conversion. In Proc. ACL, 2024d. Ziqian Wang, Xinfa Zhu, Zihan Zhang, YuanJun Lv, Ning Jiang, Guoqing Zhao, and Lei Xie. SELM: Speech enhancement using discrete tokens and language models. In Proc. ICASSP, 2024e. Pete Warden. Speech Commands: dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209, 2018. Scott Wisdom, Hakan Erdogan, Daniel PW Ellis, Romain Serizel, Nicolas Turpault, Eduardo Fonseca, Justin Salamon, Prem Seetharaman, and John Hershey. Whats all the fuss about free universal sound separation data? In Proc. ICASSP, 2021. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingfaces transformers: State-of-the-art natural language processing, 2019. Haibin Wu, Kai-Wei Chang, Yuan-Kuei Wu, and Hung-yi Lee. Speechgen: Unlocking the generative power of speech language models with prompts. arXiv preprint arXiv:2306.02207, 2023a. Haibin Wu, Xuanjun Chen, Yi-Cheng Lin, Kai-Wei Chang, Ho-Lam Chung, Alexander H. Liu, and Hung-yi Lee. Towards audio language modeling-an overview. arXiv preprint arXiv:2402.13236, 2024a. Haibin Wu, Xuanjun Chen, Yi-Cheng Lin, Kai-wei Chang, Jiawei Du, Ke-Han Lu, Alexander Liu, Ho-Lam Chung, Yuan-Kuei Wu, Dongchao Yang, et al. Codec-SUPERB@ SLT 2024: lightweight benchmark for neural audio codec models. In Proc. SLT, 2024b. Haibin Wu, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, Xuanjun Chen, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander Liu, and Hung-yi Lee. Codec-SUPERB: An in-depth analysis of sound codec models. In Proc. ACL, 2024c. Haibin Wu, Naoyuki Kanda, Sefik Emre Eskimez, and Jinyu Li. Ts3-codec: Transformer-based simple streaming single codec. arXiv preprint arXiv:2411.18803, 2024d. Haibin Wu, Yuan Tseng, and Hung-yi Lee. Codecfake: Enhancing anti-spoofing models against deepfake audios from codec-based speech synthesis systems. In Proc. Interspeech, 2024e. 49 Haibin Wu, Yuxuan Hu, Ruchao Fan, Xiaofei Wang, Kenichi Kumatani, Bo Ren, Jianwei Yu, Heng Lu, Lijuan Wang, Yao Qian, et al. Towards efficient speech-text jointly decoding within one speech language model. arXiv preprint arXiv:2506.04518, 2025. Yi-Chiao Wu, Israel D. Gebru, Dejan Marković, and Alexander Richard. Audiodec: An open-source streaming high-fidelity neural audio codec. In Proc. ICASSP, 2023b. Yuning Wu, Chunlei Zhang, Jiatong Shi, Yuxun Tang, Shan Yang, and Qin Jin. Toksing: Singing voice synthesis based on discrete tokens. In Proc. Interspeech, 2024f. Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Largescale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In Proc. ICASSP, 2023c. Detai Xin, Xu Tan, Shinnosuke Takamichi, and Hiroshi Saruwatari. Bigcodec: Pushing the limits of lowbitrate neural speech codec. arXiv preprint arXiv:2409.05377, 2024. Huaying Xue, Xiulian Peng, and Yan Lu. Low-latency speech enhancement via speech token generation. In Proc. ICASSP, 2024. Junichi Yamagishi, Christophe Veaux, Kirsten MacDonald, et al. CSTR VCTK corpus: English multispeaker corpus for CSTR voice cloning toolkit (version 0.92). University of Edinburgh. The Centre for Speech Technology Research (CSTR), pp. 271350, 2019. Brian Yan, Jiatong Shi, Yun Tang, Hirofumi Inaguma, Yifan Peng, Siddharth Dalmia, Peter Polak, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, et al. ESPnet-ST-v2: Multipurpose spoken language translation toolkit. In Proc. ACL, 2023. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Dongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and Yuexian Zou. HiFi-Codec: Group-residual vector quantization for high fidelity audio codec. arXiv preprint arXiv:2305.02765, 2023a. Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:17201733, 2023b. Dongchao Yang, Haohan Guo, Yuanyuan Wang, Rongjie Huang, Xiang Li, Xu Tan, Xixin Wu, and Helen M. Meng. UniAudio 1.5: Large language model-driven audio codec is few-shot audio task learner. In Proc. NeurIPS, 2024b. Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Haohan Guo, Xuankai Chang, Jiatong Shi, Jiang Bian, Zhou Zhao, et al. UniAudio: Towards universal audio generation with large language models. In Proc. ICML, 2024c. Dongchao Yang, Dingdong Wang, Haohan Guo, Xueyuan Chen, Xixin Wu, and Helen Meng. Simplespeech: In Proc. Towards simple and efficient text-to-speech with scalar latent transformer diffusion models. Interspeech, 2024d. Haici Yang, Kai Zhen, Seungkwon Beack, and Minje Kim. Source-aware neural speech coding for noisy speech compression. In Proc. ICASSP, 2021. Haici Yang, Wootaek Lim, and Minje Kim. Neural feature predictor and discriminative residual coding for low-bitrate speech coding. In Proc. ICASSP, 2023c. 50 Haici Yang, Inseon Jang, and Minje Kim. Generative de-quantization for neural speech codec via latent diffusion. In Proc. ICASSP, 2024e. Haici Yang, Jiaqi Su, Minje Kim, and Zeyu Jin. Genhancer: High-fidelity speech enhancement via generative modeling on discrete codec tokens. In Proc. Interspeech, 2024f. Zhen Ye, Peiwen Sun, Jiahe Lei, Hongzhan Lin, Xu Tan, Zheqi Dai, Qiuqiang Kong, Jianyi Chen, Jiahao Pan, Qifeng Liu, et al. Codec does matter: Exploring the semantic shortcoming of codec for audio language model. In Proc. AAAI, 2025. Jia Qi Yip, Shengkui Zhao, Dianwen Ng, Eng Siong Chng, and Bin Ma. Towards audio codec-based speech separation. In Proc. Interspeech, 2024. Iddo Yosha, Gallil Maimon, and Yossi Adi. StressTest: Can your speech LM handle the stress? arXiv preprint arXiv:2505.22765, 2025. Salah Zaiem, Youcef Kemiche, Titouan Parcollet, Slim Essid, and Mirco Ravanelli. Speech self-supervised representation benchmarking: Are we doing it right? In Proc. Interspeech, 2023. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. SoundStream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 495507, 2021. Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. LibriTTS: corpus derived from librispeech for text-to-speech. In Proc. Interspeech, 2019. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. GLM-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Yuxiao Dong, Jie Tang, et al. Scaling speech-text pre-training with synthetic interleaved data. In Proc. ICLR, 2025. Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, Yi Ren, Jinzheng He, Rongjie Huang, Jieming Zhu, Xiao Chen, et al. M4singer: multi-style, multi-singer and musical score provided mandarin singing corpus. In Proc. NeurIPS, 2022. Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. SpeechTokenizer: Unified speech tokenizer for speech large language models. In Proc. ICLR, 2024a. Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024b. Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, et al. Google USM: Scaling automatic speech recognition beyond 100 languages. arXiv preprint arXiv:2303.01037, 2023. Kai Zhen, Mi Suk Lee, Jongmo Sung, Seungkwon Beack, and Minje Kim. Psychoacoustic calibration of loss functions for efficient end-to-end neural audio coding. IEEE Signal Processing Letters, 27:21592163, 2020. doi: 10.1109/LSP.2020.3039765. Junzuo Zhou, Jiangyan Yi, Yong Ren, Jianhua Tao, Tao Wang, and Chu Yuan Zhang. Wmcodec: End-to-end neural speech codec with deep watermarking for authenticity verification. arXiv preprint arXiv:2409.12121, 2024. Alon Ziv, Itai Gat, Gaël Le Lan, Tal Remez, Felix Kreuk, Jade Copet, Alexandre Défossez, Gabriel Synnaeve, and Yossi Adi. Masked audio generation using single non-autoregressive transformer. In Proc. ICLR, 2024."
        }
    ],
    "affiliations": [
        "Apple",
        "Carnegie Mellon University",
        "Concordia University",
        "Google",
        "Indiana University",
        "Laval University",
        "Microsoft",
        "Mila-Quebec AI Institute",
        "National Taiwan University",
        "The Hebrew University of Jerusalem",
        "University of Cambridge",
        "University of Illinois at Urbana-Champaign",
        "Université de Montréal",
        "Université de Toulon"
    ]
}