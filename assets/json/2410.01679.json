{
    "paper_title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment",
    "authors": [
        "Amirhossein Kazemnejad",
        "Milad Aghajohari",
        "Eva Portelance",
        "Alessandro Sordoni",
        "Siva Reddy",
        "Aaron Courville",
        "Nicolas Le Roux"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 1 9 7 6 1 0 . 0 1 4 2 : r VINEPPO: UNLOCKING RL POTENTIAL FOR LLM REASONING THROUGH REFINED CREDIT ASSIGNMENT Amirhossein Kazemnejad 1, Milad Aghajohari 1, Eva Portelance1,6, Alessandro Sordoni1,2, Siva Reddy1,3,4, Aaron Courville 1,4,5, Nicolas Le Roux 1,4 1Mila 2Microsoft Research 3McGill University 4Canada CIFAR AI Chair 5Universite de Montreal 6HEC Montreal {amirhossein.kazemnejad,aghajohm}@mila.quebec"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform random baseline when comparing alternative steps. To address this, we propose VinePPO, straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPOs potential as superior alternative1."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) are increasingly used for tasks requiring complex reasoning, such as solving mathematical problems (OpenAI, 2024), navigating the web (Zhou et al., 2024), or editing large codebases (Jimenez et al., 2024). In these settings, LLMs often engage in extended reasoning steps, executing multiple actions to arrive at solution. However, not all steps are equally importantsome contribute significantly, while others are irrelevant or detrimental. For example, in Figure 1.a, only step s2 provides key insight. Indeed, most reasoning steps generated by model do not affect the chance of it solving the problem (Figure 1.b). Identifying the contribution of each action is crucial for improving model performance. However, this is inherently difficult due to the significant delay between actions and their eventual effect. This issue, known as the credit assignment problem, is core challenge in reinforcement learning (RL, Sutton and Barto 1998). Proximal Policy Optimization (PPO, Schulman et al. 2017; Ouyang et al. 2022), state-of-the-art algorithm for RL-based finetuning of LLMs (Xu et al., 2024; Ivison et al., 2024), tackles credit assignment using value network (or critic). This network, typically separate model initialized from pretrained checkpoint, is trained during PPO finetuning to estimate the expected cumulative rewards (or value) of an intermediate action. In Figure 1.b, an ideal value network would assign high value to step s2 and subsequent steps, where the model predicted critical action. PPO uses these value estimates to measure the advantage of each action and update the model accordingly. Equal contribution. Equal advising. 1Code available at https://github.com/McGill-NLP/VinePPO 1 ˆp(corrects:t) Prompt (s0) Let and be nonzero real numbers such that (2 7i)(a + bi) is pure imaginary. Find . Response s1 s2 s3 s4 We can expand the left-hand side to get (2 7i)(a + bi) = (2a + 7b) + (7a + 2b)i. This is pure imaginary if and only if the real part is 0, i.e. 2a + 7b = 0. Then = 7 2 b, so b = 7 2 . 0.4 0.4 1.0 1.0 1. 1.0 Sample Response ˆp(corrects:t+1) ˆp(corrects:t) Figure 1: (Left) response generated by the model. The notation ˆp(corrects:t) represents the estimated probability of successfully solving the problem at step t. Here, only step s2 is critical; after this, the model completes the solution correctly. (Right) The delta in probability of successful completion between response steps. Most steps show little or no advantage over the preceding step. Accurately modeling valuepredicting future rewards from an incomplete responserequires the value network to understand both the space of correct solutions (the very task the policy model is trying to learn) and predict the models future behavior, both of which are inherently challenging. In fact, there are hints in the literature that standard PPO implementations for LLMs have inaccurate value estimations. Ahmadian et al. (2024) and Trung et al. (2024) find that value networks often serve best as just baseline in policy gradient2. Shao et al. (2024) show that the value network can be replaced by averaging rewards of responses to given problem without degradation in performance. Since errors in value estimation can lead to poor credit assignment and negatively impact convergence and performance (Greensmith et al., 2001), natural question to ask is: how accurately do value networks actually perform during LLM finetuning? If we could improve credit assignment, to what extent would it enhance LLM performance? While recent studies (Hwang et al., 2024; Setlur et al., 2024) have begun to highlight the importance of identifying incorrect reasoning steps and incorporating them via ad-hoc mechanisms in RL-free methods (Rafailov et al., 2023), the broader question of how improving credit assignment might boost RL fine-tuning for LLMs remains open. In this work, we evaluate the standard PPO pipeline in mathematical reasoning tasks across various model sizes. We find that value networks consistently provide inaccurate estimates and struggle to rank alternative steps correctly, suggesting that current PPO finetuning approaches for LLMs operate without effective credit assignment. To address this issue and illustrate the effect of accurate credit assignment, we propose VinePPO (Figure 2). Instead of relying on value networks, VinePPO computes unbiased value estimates of intermediate states by using independent Monte Carlo (MC) samples and averaging their respective return. This straightforward modification to PPO takes advantage of special property of the language environment: the ability to easily reset to any intermediate state along the trajectory. VinePPO consistently outperforms standard PPO and RL-free baselines, especially on more challenging datasets. Despite its slower per-iteration speed, it reaches and surpasses PPOs peak performance with fewer gradient updates, resulting in less wall-clock time and lower KL divergence from the base model. Our findings highlight the importance of precise credit assignment in LLM finetuning and establishes VinePPO as straightforward alternative to value network-based approaches. Our contributions are as follows: We demonstrate the suboptimal credit assignment in standard PPO finetuning by analyzing the value network, showing that it provides inaccurate estimates of intermediate state values and barely outperforms random baseline when ranking alternative steps (see Section 7 for details). 2setting the Generalized Advantage Estimation (GAE, Schulman et al. 2016) parameter λ = 1 2 ... Value Prediction in PPO ˆVϕ(x; y<t) = ValNet(x; y<t) 0.42 ... Value Prediction in VinePPO ˆVMC(x; y<t) = R(τ k) 1/K (cid:80) 0.33 yt (a) yt ... yt1 (b) yt ... τ 1 τ 2 . . . τ Figure 2: (a) PPO finetunes the model by adjusting action probabilities based on their advantage, which is primarily guided by the value networks value estimates. (b) VinePPO modifies standard PPO and obtains values estimates by simply resetting to intermediate states and using MC samples. We propose VinePPO, introduced in Section 4, which takes advantage of the flexibility of language as an RL environment to compute unbiased value estimates, eliminating the need for large value networks and reducing memory requirements (up to 112GB for 7B LLM). VinePPO highlights the significance of credit assignment: It outperforms PPO and other baselines, especially on more challenging datasets. It achieves PPOs peak performance with fewer iterations (up to 9x), less wall-clock time (up to 3.0x), and better KL-divergence trade-off. See Section 6."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Credit Assignment in Post-Training of LLM PPO, as applied in RL from Human Feedback (RLHF, Ouyang et al. 2022), pioneered RL finetuning of LLMs. However, its computational overhead and hyperparameter sensitivity led to the development of simpler alternatives. RL-free methods such as DPO (Rafailov et al., 2023) operate in bandit setting, treating the entire response as single action. Similarly, rejection sampling methods like RestEM (Singh et al., 2024) finetune on full highreward responses. RLOO (Ahmadian et al., 2024) and GRPO (Shao et al., 2024) abandon PPOs value network, instead using average reward from multiple samples as baseline. Recent work has emphasized finer credit assignment, with Hwang et al. (2024) and Setlur et al. (2024) introducing MC-based methods to detect key errors in reasoning chains for use as ad-hoc mechanisms in DPO. Our work, by contrast, fully embraces the RL training, with the target of unlocking PPOs potential. Parallel efforts have also focused on building better verifiers and reward models for per-step feedback, with recent attempts to automate their data collection using MC rollouts (Ma et al., 2023; Uesato et al., 2022; Luo et al., 2024; Wang et al., 2024). Our method is orthogonal to these methods, operating within PPO-based training to optimize given reward, instead of designing new ones. Value Estimation in RL and Monte Carlo Tree Search (MCTS) Deep RL algorithms are typically categorized into value-based and policy-based methods. Policy-based methods like PPO usually employ critic networks for value prediction. An exception is the Vine variant of TRPO (Schulman et al., 2015), which uses MC samples for state value estimation. The authors, however, note that the Vine variant is limited to environments that allow intermediate state resets, rare in typical RL settings3. However, language generation when formulated as RL environment enables such intermediate reset capabilities. In domains with similar reset capabilities, such as Go and Chess, MC-heavy methods like AlphaGo (Silver et al., 2016) and AlphaZero (Silver et al., 2017) have emerged. AlphaGos architecture includes policy, trained using expert moves and self-play, and value network that predicts game outcomes. At inference, it employs tree search guided by MC rollouts and value network to select optimal moves. AlphaZero advances this approach by distilling MCTS outcomes into the policy. Recent works have adapted AlphaZeros principles to LLMs, employing similar search techniques during inference to improve responses and during training to find better trajectories for distillation (Xie et al., 2024; Chen et al., 2024; Wan et al., 2024; Zhang et al., 3This is reflected in the design of Gym (Towers et al., 2024), which only allows resets to the initial state. 3 Figure 3: VinePPO outperforms standard PPO and other RL-free baselines on Pass@1 performance on MATH and GSM8K datasets, while also exhibiting scalability across different model sizes. 2024; Hao et al., 2023). While this is promising direction, our method is not an MCTS approach; it uses MC samples solely for value estimation during PPO training to improve credit assignment."
        },
        {
            "title": "3 BACKGROUND",
            "content": "We focus on the RL tuning phase in the RLHF pipeline, following Ouyang et al. (2022); Shao et al. (2024). In this section, we provide an overview of actor-critic finetuning as implemented in PPO. RL Finetuning In this setup, the policy πθ represents language model that generates response = [y0, . . . , yT 1] autoregressively given an input = [x0, . . . , xM 1]. The goal of RL finetuning is to maximize the expected undiscounted (γ = 1) finite-horizon return, while incorporating KLdivergence constraint to regularize the policy and prevent it from deviating too far from reference policy πref (typically the initial supervised finetuned, SFT, model). The objective can be written as: J(θ) = ExD,yπ(x) [R(x; y)] β KL[πθπref ], where is the dataset of prompts, R(x; y) is the complete sequence-level reward function, and β controls the strength of the KL penalty. Note that the policy πθ is initialized from πref . (1) Language Environment as an MDP Language generation is typically modeled as token-level Markov Decision Process (MDP) in an actor-critic setting, where each response is an episode. The state at time step t, st S, is the concatenation of the input prompt and the tokens generated up to that point: st = x; y<t = [x0, . . . , xM 1, y0, . . . , yt1]. At each time step, the action at corresponds to generating the next token yt from fixed vocabulary. The process begins with the initial state s0 = x, and after each action, the environment transitions to the next state, st+1 = st; [at], by appending the action at to the current state st. In this case, since states are always constructed by concatenating tokens, the environment dynamics are known and the transition function is deterministic, i.e., (st+1st, at) = 1. During the generation process, the reward rt is set to zero for all intermediate actions ats, with the sequence-level reward R(x; y) only applied at the final step when the model stops generating. trajectory τ = (s0, a0, s1, a1, . . . ) is therefore sequence of state-action pairs, starting from the input prompt until the terminal state. Finally, we define the cumulative return of trajectory τ as R(τ ) = (cid:80)T 1 t=0 rt = rT 1 = R(x; y). Policy Gradient Given this MDP formulation, policy gradient methods like PPO maximize Equation 1 by repeatedly sampling trajectories and taking step in the direction of the gradient 4 Impact of number of sampled trajectories for estimating ˆVMC(st), evaluated on Figure 4: RhoMath 1.1B models. Increasing the number of rollouts improves task performance consistently. gpg := θJ(θ) to update the parameters. Policy gradient gpg takes the following form: gpg = Eτ πθ (cid:34)T 1 (cid:88) t= θ log πθ(atst)A(st, at) , where st = x; y<t, at = yt, (2) (cid:35) where A(st, at) is the advantage function. If A(st, at) > 0, gpg will increase the probability of action at in state st, and decrease it when A(st, at) < 0. Intuitively, the advantage function quantifies how much better action at is compared to average actions taken in state st under the policy. Formally, it is defined as: A(st, at) = Q(st, at) (st) = rt + γV (st+1) (st), (3) where Q(st, at) is the state-action value and (st) is the per-state value function4. The value function, (st) : R, offers long-term assessment of how desirable particular state is under the current policy. Formally, it represents the expected cumulative reward obtained from starting in state st and following the policy thereafter5: (st) = Eτ πθ [R(τ ) s0 = st] . PPO uses the same advantage-weighted policy gradient as in Equation 2, but constrains policy updates through clipping to ensure stable training. For full details, see Appendix A. In practice, the advantage A(st, at) is not known Estimating Advantage via Value Networks beforehand and is typically estimated by first using value network ˆVϕ to approximate the true value function (st), then substituting the learned values into Equation 3 or alternative methods like GAE (Schulman et al., 2016). The value network is parameterized by ϕ and trained alongside the policy network πθ. The training objective for the value network minimizes the mean squared error between the predicted value and the empirical return: LV (ϕ) = Eτ πθ (cid:34) 1 (cid:88) 1 2 (cid:35) ( ˆVϕ(st) Gt)2 , (4) where Gt = (cid:80)T 1 t=t rt is the empirical return from state st. PPO uses the same objective for ˆVϕ but enhances stability by applying clipping during training (see Appendix A.1 for details). In RL-tuning of LLMs, the value network is often initialized using the initial SFT policy πref (or the reward model when available), with the language modeling head swapped out for scalar head to predict values (Zheng et al., 2023). This setup leverages the prior knowledge of the pretrained model."
        },
        {
            "title": "4 ACCURATE CREDIT ASSIGNMENT WITH VINEPPO",
            "content": "As outlined in Section 3, step in the PPO gradient update aims to increase the probability of better-than-average actions while decreasing the probability of those that perform worsea process quantified by the advantage A(st, at). However, the true advantage is generally unknown and must be estimated, typically by substituting estimates from value network into Equation 3. As we will 4Such derivation is possible as the language environment is deterministic. 5We drop the dependency on πθ for brevity. 5 Figure 5: Comparison of the training behavior between VinePPO and PPO. VinePPO demonstrates consistently higher accuracy (as measured on the test set of MATH dataset) throughout the training. Refer to Appendix for more detailed plots. elaborate in Section 7, value networks are often inaccurate and result in biased value computation. Fortunately, the language environment as an MDP (Section 3) offers useful property that allows for unbiased estimation of (st). Since states are simply concatenated tokens, we can prompt the language model πθ to generate continuations from any intermediate state. This flexibility allows us to explore alternative future paths from arbitrary points in generation Moreover, recent advancements in LLM inference engines (Kwon et al., 2023; Zheng et al., 2024) have dramatically increased the speed of on-the-fly response generation6. This computational efficiency makes it feasible to conduct fast environment simulation, opening up unique opportunities for RL training of LLMs.VinePPO uses this property and estimates advantage via MC sampling. It only modifies the way advantages are estimated, leaving the rest of the standard PPO pipeline intact (Figure 2). We start by estimating the true value (st). Instead of relying on value network, for any intermediate state st, we sample independent trajectories τ ks. The average return across these trajectories serves as the value estimate: ˆVMC(st) := 1 (cid:88) k=1 R(τ k), where τ 1, . . . , τ πθ( st). (5) This is MC estimate of (st) = [R(τ ) s0 = st] . Note that these trajectories are not trained on. Once the value ˆVMC(st) is computed, we estimate the advantages of each action using Equation 3: ˆAMC(st, at) := r(st, at) + γ ˆVMC(st+1) ˆVMC(st). (6) For any 1, the policy gradient computed using the advantage estimator ˆAMC is an unbiased estimate of the gradient of expected return gpg. To enhance the efficiency of ˆAMC, we group states within reasoning step and compute single advantage, which is assigned to all tokens in that step (examples in Appendix B). This trades off granularity for efficiency, allowing finer resolution with more compute, or coarser estimates with limited resources. The parameter also offers another trade-off between computational cost (i.e. more MC samples per state) and the variance of the estimator. As shown in Section 6.1, even = 1 performs well. In essence, VinePPO is straightforward modification to the PPO pipeline, altering only the advantage computation. This minimal adjustment allows us to leverage PPOs benefits while enabling systematic evaluation of the effect of unbiased advantage estimation and improved credit assignment. In the following sections, we compare various aspects such as task performance, computational efficiency, KL divergence, and robustness to shed light on the nature of these approaches."
        },
        {
            "title": "5 EXPERIMENTAL SETUP",
            "content": "Datasets and Pretrained LLMs We conduct our experiments using LLMs that show strong performance on mathematical reasoning: DeepSeekMath 7B (Shao et al., 2024) and RhoMath 1.1B 6up to 5K tokens/second on single Nvidia A100 GPU for 7B LLM loaded in bfloat16. 6 Figure 6: Task accuracy as function of KL divergence during training on the MATH dataset. VinePPO achieves higher accuracy, reflecting more efficient credit assignment and focused updates. (Lin et al., 2024), both of which have been trained on diverse mathematical and natural language corpora. Having different sized models allows evaluating the effect of scaling. We focus on mathematical reasoning datasets MATH (Hendrycks et al., 2021), consisting of competition-level mathematical problems, and GSM8K (Cobbe et al., 2021), containing simpler grade-school level math word problems. Both datasets are well-established and present range of difficulty levels that allow for comprehensive evaluation. For each dataset, we finetune the base LLMs on its respective training sets to obtain the initial SFT policy (πref ). In all experiments, we employ full-parameter finetuning to allow utilization of models full capacity (Sun et al., 2023; Biderman et al., 2024). Evaluation We evaluate model performance on the test sets of each dataset, using accuracy (Pass@1) as our primary metric, which measures the correctness of the final answers produced by the models. As our baseline, we adopt the standard PPO framework, as commonly implemented for LLM finetuning (Ouyang et al., 2022; Huang et al., 2024). Additionally, we compare them against RL-free methods that doesnt have explicit credit assignment mechanisms: RestEM (Singh et al., 2024), form of Iterative Rejection Finetuning (Yuan et al., 2023; Anthony et al., 2017) and DPO+ (Pal et al., 2024), variant of DPO with strong performance on reasoning tasks. All methods are initialized from the same SFT checkpoint to ensure fair comparison. Training Details and Hyperparameters To ensure standard PPO (and its value network) has healthy training and our evaluation reflects its full potential, we first focus our hyperparameter search on PPO parameters (such as KL penalty coefficient, batch size, minibatch size, GAE λ, number of epochs per iteration) and apply all well-known techniques and best practices (Huang et al., 2024; Ivison et al., 2024) in PPO tuning (Refer to Appendix C.2 for the full list). Following previous work (Pal et al., 2024; Singh et al., 2024), we set the task reward to be binary function that only checks final answer against the ground truth. VinePPO borrows the exact same hyperparameters from PPO and only modifies the advantage A(st, at) estimation, keeping the rest of the pipeline unchanged. This allows us to isolate the effect of accurate credit assignment. We found that sampling = 9 trajectories in ˆVMC performs well; the effect of varying is fully analyzed in Section 6.1. For the other baseline, we closely follow the original setup while ensuring consistency in training conditions for fair comparison. We choose the best checkpoint based on held-out validation set for all experiments. Full implementation details, including all hyperparameters and training procedures, are provided in Appendix C.6."
        },
        {
            "title": "6 RESULTS",
            "content": "We evaluate the effect of accurate credit assignment on four key measures of model finetuning efficiency and success: task performance, KL divergence, temperature tolerance, and computational efficiency. Our experimental setup is designed to control for and isolate the impact of credit assignment on each of these measures. 7 Figure 7: Accuracy vs. Wall Clock Time for both methods measured on the same hardware (shown only up to PPOs final performance). Despite VinePPO taking longer per iteration (up to 2x for 7B and 5x for 1.1B models), it passes PPOs peak performance in fewer iterations and less overall time."
        },
        {
            "title": "6.1 TASK PERFORMANCE",
            "content": "VinePPO consistently outperforms standard PPO throughout training (Figure 5) and other baselines (Figure 3). More importantly, its performance gap widens in MATH which is much more challenging reasoning task. Unlike VinePPO and PPO, DPO+ and RestEM lacks any explicit mechanisms for credit assignment, opting instead to finetune the model on the full trajectory. Our experiments show that these RL-free methods lags behind both PPO-based methods. For RestEM, the absence of targeted credit assignments likely leads to overfitting (Appendix C.5). To assess the impact of K, the number of MC samples used to estimate the value, we run an ablation on RhoMath 1.1B, varying from 1 to 3 and then to 9. As shown in Figure 4, VinePPO demonstrates improved performance with higher values, as more MC samples reduce the variance of the ˆAMC estimator. Notably, increasing provides reliable approach to leveraging additional computational resources for better performance. 6.2 KL DIVERGENCE The RL objective (Equation 1) balances maximizing task performance while constraining deviations from the initial policy πref , measured by KL divergence. We analyze how VinePPO and PPO navigate this trade-off by plotting task accuracy against KL divergence KL[πθπref ] throughout training (Figure 6). Results show VinePPO consistently achieves higher accuracy at same KL divergence, indicating more efficient use of the KL budget. This efficiency stems from VinePPOs more precise credit assignment. As shown in Figure 1, many advantages are zero, and VinePPO excludes these steps from the loss. By avoiding unnecessary updates on non-contributing tokens, VinePPO reduces non-essential parameter adjustments that would inflate KL. See Appendix D.1 for full results. 6.3 TEMPERATURE TOLERANCE Sampling temperature is critical hyperparameter controlling the randomness of sampled trajectories. At higher temperatures models generates more diverse trajectories, accelerating early training through increased exploration. However, this diversity challenges PPOs value network, requiring generalization over wider range of states. We compared VinePPO and PPO using temperatures {0.6, 0.8, 1.0} over the initial third of training steps. Figure 8 shows VinePPO consistently benefits from higher temperatures, achieving faster convergence. Conversely, PPO fails to benefit from increased exploration and even diverges at = 1.0, where trajectories are most diverse. Figure 8: Test set accuracy during training with higher temperature presented for DeepSeekMath 7B and MATH dataset. VinePPO can tolerate higher temperatures. 8 Figure 9: Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training for DeepSeekMath 7B on MATH dataset, highlighting the nature of errors. VinePPO achieves much lower Mean Absolute Error (MAE). 6.4 COMPUTATIONAL EFFICIENCY VinePPO and PPO require different resources: PPO uses separate value network, requiring two times more GPU memory (up to 112GB for 7B LLM, considering both model and its optimizer); VinePPO, conversely, relies on MC samples. This skips value networks memory requirements, but shifts the computational burden to increased LLM inferences, making VinePPO generally slower per iteration (up to 5x for RhoMath 1.1B and 2x for DeepSeekMath 7B). However, the effect of VinePPOs accurate credit assignment is substantial. Although slower per iteration, VinePPO achieves PPOs peak accuracy in fewer gradient steps and less wall-clock time. Figure 7 shows RhoMath 1.1B and DeepSeekMath 7B require about 3.0x and 1.51x less time and 9x and 2.8x fewer steps. This improvement occurs despite all hyperparameters being tuned for PPO. Therefore, switching to VinePPO offers way to enhance performance within the same compute budget and serves as the only option when memory is constrained."
        },
        {
            "title": "7 VALUE PREDICTION ANALYSIS",
            "content": "In this section, we explore the underlying reasons for the performance gap between PPO and VinePPO by closely analyzing the value prediction of both methods. First, we establish ground truth value at each reasoning step within trajectories by running many MC samples (256 in our case) and averaging the returns. This provides low-variance reference value. We then compare the value predictions in both methods against this ground truth. We present the results for DeepSeekMath 7B on the MATH dataset (full analysis with other models and datasets in Appendix D.2). Accuracy Figure 9 presents the distribution of value predictions at each reasoning step. The errors produced by VinePPO and PPO differ significantly. VinePPOs estimates are unbiased, with variance peaking at 0.5 and dropping to zero at 0 and 1. PPOs value network shows high bias, often misclassifying bad states (ground truth near 0) as good and vice versa. To further visualize accuracy, we classify value prediction as correct if it falls within 0.05 of the ground truth. The accuracy of this formulation is shown in Figure 11.a. PPOs value network starts with low accuracy, gradually improving to 65%. VinePPO, however, consistently achieves 70-90% accuracy throughout training. Top Action Identification In value-based RL, ranking actions correctly is more crucial than absolute value accuracy. While PPO, as policy gradient method, requires accurate value estimates to compute meaningful advantages, it is still compelling question whether PPOs value network, despite its bias, can maintain correct action ranking. To investigate, we sample five new next steps 9 Figure 10: Visualizing the Mean Absolute Error (MAE) of the value predictions at different point of the reasoning chain. Value Network in PPO fails to generalize as the reasoning chain progresses, while VinePPOs value estimates become more accurate as the model become more deterministic. Figure 11: (a) Value prediction accuracy formulated as classification problem, where prediction is considered correct if it falls within 0.05 of the ground truth. (b) Accuracy of identifying the top action in set of five possible next states. VinePPO consistently outperforms the value network. from the same initial state and evaluate if the method correctly identifies the resulting next state with the highest ground truth value. As shown in Figure 11.b, PPOs value network performs near chance levels for much of the training, with slight improvements over time. In contrast, VinePPO consistently identifies the top action with high accuracy throughout training. Error Per Reasoning Step To understand value computation mechanisms, we visualize the prediction error at each reasoning step within trajectory. As shown in Figure 10, PPOs estimation error increases as reasoning progresses. We hypothesize this occurs because early steps have lower diversity and resemble training data more, allowing the value network to rely on memorization. Later, as space of states become much larger, they become unfamiliar and the network struggles to generalize. VinePPOs prediction error decreases with reasoning progression. We attribute this to the model becoming more deterministic in later steps as it conditions on bigger and longer context. This determinism enables more accurate estimates from the same number of MC samples."
        },
        {
            "title": "8 DISCUSSION",
            "content": "Accurate credit assignment has profound implications on the performance of RL tuning of LLMs. As weve demonstrated, standard PPO, despite outperforming most RL-free baselines, suffers from suboptimal value estimation. More importantly, its scaling behavior is concerning; PPO struggles with increasingly diverse trajectories and tends to perform worse as tasks become more complex. VinePPO, on the other hand, is viable alternative. As shown in Section 6.4, it offers lowered memory requirements and better performance with the same computational budget. VinePPO could also be particularly attractive option for frontier LLMs as even doubling the post-training compute is negligible compared to their pre-training costs (Ouyang et al., 2022)7. Given the major investments in pre-training compute and data collection of these models, it is imperative for model developers to employ post-training methods that provide more accurate updates, avoiding the high-variance adjustments caused by inferior credit assignment. Additionally, VinePPO offers straightforward 7For example, InstructGPT used nearly 60 times more compute for pre-training (Ouyang et al., 2022). 10 scaling axis: increasing the number of MC samples directly enhances performance with additional compute. Unlike recent approaches that focus on increasing inference-time compute to boost performance (OpenAI, 2024; Bansal et al., 2024), VinePPOs training compute is amortized over all future inferences. Note that the computational workload of VinePPO is highly parallelizable with linear scalability, making it well-suited for large-scale training. The unique properties of the language environment are what enabled VinePPO to be viable credit assignment option; it may have limited practical use in traditional deep RL policy gradient methods. This suggests that adapting RL techniques to LLMs requires careful consideration and perhaps reevaluation of underlying assumptions. Overall, our work highlights the potential of well-tuned RL finetuning strategies with proper credit assignment, and we hope it encourages further research into optimizing RL post-training pipelines for LLMs."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Matheus Pereira for his efforts on facilitating experimentation. AC and NR are supported by CIFAR AI Chair. SR is supported by Facebook CIFAR AI Chair and NSERC Discovery Grant program. We thank Mila IDT team and Digital Research Alliance of Canada for the compute provided for experimentation."
        },
        {
            "title": "REFERENCES",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. 2024. Back to Basics: Revisiting REINFORCE-style Optimization for Learning from Human Feedback in LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, pages 1224812267, Bangkok, Thailand. Association for Computational Linguistics. Thomas Anthony, Zheng Tian, and David Barber. 2017. Thinking Fast and Slow with Deep Learning and Tree Search. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,, pages 53605370, USA. Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, and Mehran Kazemi. 2024. Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-optimal Sampling. CoRR, abs/2408.16737. Dan Biderman, Jose Javier Gonzalez Ortiz, Jacob Portes, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John P. Cunningham. 2024. LoRA Learns Less and Forgets Less. CoRR, abs/2405.09673. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024. AlphaMath Almost Zero: process Supervision without process. CoRR, abs/2405.03553. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. CoRR, abs/2110.14168. Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. 2001. Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning. In Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, pages 15071514, Vancouver, British Columbia, Canada. MIT Press. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with Language Model is Planning with World Model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, pages 81548173, Singapore. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring Mathematical Problem Solving With the MATH Dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. 11 Shengyi Huang, Michael Noukhovitch, Arian Hosseini, Kashif Rasul, Weixun Wang, and Lewis Tunstall. 2024. The N+ Implementation Details of RLHF with PPO: Case Study on TL;DR Summarization. CoRR, abs/2403.17031. Hyeonbin Hwang, Doyoung Kim, Seungone Kim, Seonghyeon Ye, and Minjoon Seo. 2024. Selfexplore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Finegrained Rewards. CoRR, abs/2404.10346. Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, and Hannaneh Hajishirzi. 2024. Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback. CoRR, abs/2406.09279. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. 2024. SWE-bench: Can Language Models Resolve Real-world Github Issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria. OpenReview.net. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, pages 611626, Koblenz, Germany. ACM. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving Quantitative Reasoning Problems In Advances in Neural Information Processing Systems 35: Annual with Language Models. Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets Verify Step by Step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria. OpenReview.net. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. 2024. Rho-1: Not All Tokens Are What You Need. CoRR, abs/2404.07965. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Improve Mathematical Reasoning in Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. 2024. Language Models by Automated Process Supervision. CoRR, abs/2406.06592. Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. 2023. Lets reward step by step: Step-level reward model as the Navigators for Reasoning. CoRR, abs/2310.10080. OpenAI. 2024. OpenAI o1 System Card. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA. Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. 2024. Smaug: Fixing Failure Modes of Preference Optimisation with DPO-positive. CoRR, abs/2402.13228. Qwen. 2024. Qwen2.5-Math: The worlds leading open-sourced mathematical LLMs. https: //qwenlm.github.io/blog/qwen2.5-math/. Accessed: 2024-09-23. 12 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct Preference Optimization: Your Language Model is Secretly Reward Model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA. John Schulman. 2020. Notes on the KL-divergence Approximation. http://joschu.net/ blog/kl-approx.html. Accessed: 2024-09-23. John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. 2015. Trust Region Policy Optimization. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 1889 1897, Lille, France. JMLR.org. John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. 2016. Highdimensional Continuous Control Using Generalized Advantage Estimation. In 4th International Conference on Learning Representations, ICLR 2016Proceedings, San Juan, Puerto Rico. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. CoRR, abs/1707.06347. Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. 2024. RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-fold. CoRR, abs/2406.14532. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. CoRR, abs/2402.03300. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. 2016. Mastering the game of Go with deep neural networks and tree search. Nat., 529(7587):484489. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. 2017. Mastering Chess and Shogi by Self-play with General Reinforcement Learning Algorithm. CoRR, abs/1712.01815. Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron T. Parisi, Abhishek Kumar, Alexander A. Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2024. Beyond Human Data: Scaling Self-training for Problem-solving with Language Models. Transactions on Machine Learning Research, 2024. Xianghui Sun, Yunjie Ji, Baochang Ma, and Xiangang Li. 2023. Comparative Study between Fullparameter and LoRA-based Fine-tuning on Chinese Instruction Data for Instruction Following Large Language Model. CoRR, abs/2304.08109. Richard S. Sutton and Andrew G. Barto. 1998. Introduction to Reinforcement Learning. In Introduction to Reinforcement Learning. Richard S. Sutton, David A. McAllester, Satinder Singh, and Yishay Mansour. 1999. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In Advances in Neural Information Processing Systems 12, [NIPS Conference, pages 10571063, Denver, Colorado, USA. The MIT Press. 13 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, et al. 2023. Llama 2: Open Foundation and Fine-tuned Chat Models. CoRR, abs/2307.09288. Mark Towers, Ariel Kwiatkowski, Jordan Terry, John Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulao, Andreas Kallinteris, Markus Krimmel, Arjun KG, et al. 2024. Gymnasium: standard interface for reinforcement learning environments. arXiv preprint arXiv:2407.17032. Luong Quoc Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. 2024. ReFT: Reasoning with Reinforced Fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, pages 76017614, Bangkok, Thailand. Association for Computational Linguistics. Jonathan Uesato, Nate Kushman, Ramana Kumar, H. Francis Song, Noah Y. Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcome-based feedback. CoRR, abs/2211.14275. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2024. AlphaZero-like Tree-search can Guide Large Language Model Decoding and In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Training. Austria. OpenReview.net. Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. 2024. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR, abs/2406.06592. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024. Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning. CoRR, abs/2405.00451. Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. 2024. Is DPO Superior to PPO for LLM Alignment? Comprehensive Study. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria. OpenReview.net. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023. Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. CoRR, abs/2308.01825. Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024. ReST-MCTS*: LLM Self-training via Process Reward Guided Tree Search. CoRR, abs/2406.03816. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2024. Sglang: Efficient execution of structured language model programs. CoRR, abs/2312.07104. Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, and Xuanjing Huang. 2023. Secrets of RLHF in Large Language Models Part I: PPO. CoRR, abs/2307.04964. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. WebArena: Realistic Web Environment for Building Autonomous Agents. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria. OpenReview.net. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. 2019. Fine-tuning Language Models from Human Preferences. CoRR, abs/1909.08593."
        },
        {
            "title": "A REVIEWING PPO",
            "content": "PPO, as used in RL tuning of LLMs, formulates language generation as token-level MDP (Section 3), where each response is an episode. The state at time step t, st S, is the concatenation of the prompt and the tokens generated so far: st = x; y<t = [x0, . . . , xM 1, y0, . . . , yt1]. The action at corresponds to generating the next token yt from the models vocabulary. Given prompt x, an episode of this MDP starts from the initial state s0 = x, and with each action taken, the environment moves to subsequent state, st+1 = st; [at], by adding the action at to the existing state st. In the language environment, because states are always formed by concatenating tokens, the environment dynamics are fully known, and the transition function is deterministic, meaning (st+1st, at) = 1. Throughout the generation process, the reward rt is set to zero for all intermediate actions at, with the sequence-level reward R(x; y) applied only at the final step when the model stops the generation. That is: (cid:26)R(x; y) 0 rt = r(st, at) = if = 1, where st+1 = is terminal, otherwise. trajectory τ = (s0, a0, s1, a1, . . . ) thus represents sequence of state-action pairs that begins at the input prompt and continues until reaching the terminal state. Finally, the cumulative return of trajectory τ is defined as R(τ ) = (cid:80)T 1 t=0 rt = rT 1 = R(x; y). The goal of RL tuning is to maximize the expected return of the models responses to prompts in the dataset, as defined by the reward function (Equation 1). PPO, similar to other policy gradient methods, achieves this goal by repeatedly sampling trajectories for batch of prompt sampled from and taking multiple optimization steps in the direction of the gradient gppo to update the parameters. PPO gradient gppo is defined as the gradient of the following loss: (cid:32) (7) (cid:33) (cid:35) Aθk , clip(θ)Aθk β KL[πθ πref ] (8) Lppo(θ) = Eτ πθk (cid:34) 1 (cid:88) t=0 min πθ(at st) πθk (at st) where πθk is the policy at the previous iteration, ϵ is the clipping parameter, β is the KL penalty coefficient, Aθk = Aθk (st, at) is the advantage estimate for policy πθk , and the clip(θ) function is: clip(θ) = clip , 1 ϵ, 1 + ϵ . (9) (cid:18) πθ(at st) πθk (at st) (cid:19) Note that the KL penalty could be also added to the reward function R. We follow the more recent implementations (Shao et al., 2024; Qwen, 2024), where it is added to the loss function. The KL term can be computed using the following unbiased estimator (Schulman, 2020): ˆKL(θ) = πref (at st) πθ(at st) log πref (at st) πθ(at st) 1, (10) where πref denotes the reference model (initial SFT). A.1 VALUE NETWORK In addition to the policy πθ, PPO also trains separate value network ˆVϕ to obtain an estimate the true values (st) of states st. Parameterized by ϕ, the value network is trained alongside the policy network πθ using the following loss: (cid:34) (cid:35) max (cid:16)(cid:13) ˆVϕ(st) Gt (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) , (cid:13) (cid:13) (cid:13)clip(ϕ) Gt (cid:13) (cid:13) (cid:13) 2(cid:17) (11) LValNet(ϕ) = Eτ πθ 1 2 1 1 (cid:88) t=0 where ˆVϕk is the value network at the previous iteration, Gt = (cid:80)T 1 from state st, ϵ is value clipping parameter, and the clip(θ) is defined as: (cid:16) ˆVϕ(st), ˆVϕk (st) ϵ, ˆVϕk (st) + ϵ(cid:17) clip(ϕ) = clip t=t γttrt is the empirical return . (12) In RL-tuning of LLMs, the value network is typically initialized from the initial policy πref (or the reward model, if available), replacing the language modeling head with scalar output head to predict values (Zheng et al., 2023) This approach takes advantage of the base models prior knowledge for value estimation. 15 Prompt If we express 3x2 + 4 in the form a(x h)2 + k, then what is k? Response We complete the square. First, we factor 3 out of the terms 3x2 + to get 3 (cid:0)x2 + 3 (cid:1). We can square + 1 to get x2 + 3 + 1 36 , so (cid:18) 3 x2 + (cid:19) 3 (cid:20)(cid:18) + = 3 (cid:21) (cid:19)2 1 6 1 36 (cid:18) (cid:18) = 3 = 3 + + 1 6 1 (cid:19)2 3 36 (cid:19)2 1 12 , and (cid:18) 3 x2 + (cid:19) 3 4 = 3 = 3 49 12 . We see that = (cid:18) (cid:18) + + (cid:19) (cid:19)2 1 6 1 6 1 12 4 49 12 . Prompt Suppose the roots of the polynomial x2 mx + are positive prime integers (not necessarily distinct). Given that < 20, how many possible values of are there? Response Let and be the prime roots. Then, we know that = + and = pq. Since < 20, the primes and must both be less than 20. The primes less than 20 are 2, 3, 5, 7, 11, 13, 17, 19. Now we list all possible pairs (p, q) such that + < 20, remembering to also include the cases in which = q: (2, 2), (2, 3), (2, 5), (2, 7), (2, 11), (2, 13), (2, 17) (3, 3), (3, 5), (3, 7), (3, 11), (3, 13) (5, 5), (5, 7), (5, 11), (5, 13) (7, 7), (7, 11) There are 7 + 5 + 4 + 2 = 18 pairs in total. Each pair produces value for n, and furthermore, these values are all distinct, because every positive integer has unique prime factorization. Thus, there are 18 possible values for n. Figure B.1: Examples of solutions separated into its reasoning steps on the MATH dataset. Steps are highlighted using distinct colors. denotes the reasoning step boundary. Advantage Estimation Once the estimated values ˆVϕ(st) are obtained, the advantages A(st, at) are computed using the GAE (Schulman et al., 2016): A(st, at) ˆAGAE(st, at) (cid:16) ˆA(1) = (1 λ) + λ ˆA(2) + λ2 ˆA(3) + . . . (cid:17) = = (cid:88) l=0 (cid:88) l=0 (γλ)lδt+l (γλ)l (cid:16) (cid:17) rt+l + γ ˆVϕ(st+l+1) ˆVϕ(st+l) (13) (14) (15) (16) where δt = rt + γ ˆVϕ(st+1) ˆVϕ(st) is the temporal difference error, λ is the GAE parameter, and γ is the discount factor. Also, we have: ˆA(k) := k1 (cid:88) l=0 γlδt+l = rt + γrt+1 + + γk1rt+k1 + γk ˆVϕ(st+k) ˆVϕ(st). (17) Adjusting the GAE parameter λ allows for trade-off between bias and variance in the advantage estimates. However, as we discuss in Appendix C.6, we found that λ = 1 works best in our experiments (similar to the findings of Trung et al. (2024) and Ahmadian et al. (2024)). In this case, the GAE simplifies to the following form (assuming γ = 1): ˆAGAE(st, at) = (cid:80)T 1 t=t rt ˆVϕ(st). Prompt Tobias is buying new pair of shoes that costs $95. He has been saving up his money each month for the past three months. He gets $5 allowance month. He also mows lawns and shovels driveways. He charges $15 to mow lawn and $7 to shovel. After buying the shoes, he has $15 in change. If he mows 4 lawns, how many driveways did he shovel? Response He saved up $110 total because 95 + 15 = 110 Prompt Tim rides his bike back and forth to work for each of his 5 workdays. His work is 20 miles away. He also goes for weekend bike ride of 200 miles. If he can bike at 25 mph how much time does he spend biking week? Response He bikes 20 2 = 40 miles each day for work. He saved $15 from his allowance because 3 5 = 15 So he bikes 40 5 = 200 miles for work He earned $60 mowing lawns because 4 15 = 60 That means he bikes total of 200+200 =400 miles for work He earned $35 shoveling driveways because 110-60-15 = 35 He shoveled 5 driveways because 35 / 7 = 5. So he bikes total of 400 / 25=16 hours #### 16 #### 5 Figure B.2: Examples of solutions separated into its reasoning steps on the GSM8K dataset. Steps are highlighted using distinct colors. denotes the reasoning step boundary."
        },
        {
            "title": "B REASONING STEP SEPARATION EXAMPLES",
            "content": "In this section, we outline the methodology used to segment solutions into discrete reasoning steps for the MATH and GSM8K datasets, as illustrated in Figures B.1 and B.2. For the MATH dataset, we begin by splitting solutions based on clear natural boundaries such as newline characters or punctuation marks (e.g., periods or commas). Care is taken to avoid splitting within mathematical expressions, ensuring that mathematical formulas remain intact. After this initial segmentation, if any resulting step exceeds 100 characters, we further try to divide it by identifying logical breakpoints, such as equal signs (=) within math mode. For the GSM8K dataset, we take simpler approach, segmenting the reasoning steps by newlines alone as with this task newlines already serve as natural delimiters."
        },
        {
            "title": "C EXPERIMENTAL DETAILS",
            "content": "C.1 DATASETS We focus on mathematical reasoning datasets that require step-by-step solutions and are widely used to evaluate the reasoning capabilities of LLMs. Below is brief overview of the datasets used in our experiments: MATH (Hendrycks et al., 2021) The MATH dataset contains problems from high school math competitions, covering wide range of topics such as algebra, geometry, and probability. For our experiments, we use the OpenAI split provided by Lightman et al. (2024), which consists of 500 problems for testing and 12,500 problems for training. We further divide the training set into 11,500 problems for training and 500 problems for validation. Each problem includes step-by-step solution, ending in final answer marked by boxed{} in the solution (e.g., ..so the smallest possible value of is π ). This marking allows for verification of the correctness of model-generated responses by comparing the final answer to the ground truth. We use the scripts provided by Lewkowycz et al. (2022), Lightman et al. (2024), and Shao et al. (2024) to extract and compare the final answers to the ground truth. GSM8K (Cobbe et al., 2021) The GSM8K dataset comprises high-quality grade-school math problems, requiring basic arithmetic or elementary algebra to solve. Although simpler than the MATH dataset, GSM8K is still widely used to assess the reasoning capabilities of LLMs. It contains 1,319 problems for testing and 7,473 for training. To create validation set, we further split the training set into 7,100 problems for training and 373 for validation. Verifying the correctness of 17 Table 1: Summary of PPO hyperparamters used in the experiments. Parameter Value TRAINING Optimizer Adam Parameters (β1, β2) Learning rate Weight Decay Max Global Gradient Norm for Clipping Learning Rate Scheduler Warm Up # Train Steps For MATH dataset # Train Steps For GSM8K dataset AdamW (0.9, 0.999) 1 106 0.0 1.0 Polynomial 3% of training steps 1000 steps (around 8 dataset epochs) 650 steps (around 8 dataset epochs) Maximum Response Length Maximum Sequence Length for RhoMath 1.1B Maximum Sequence Length for DeepSeekMath 7B 1024 tokens 2048 tokens 2500 tokens GENERAL # Responses per Prompt # Episodes per PPO Step # Prompts per PPO Step Mini-batch Size # Inner epochs per PPO Step Sampling Temperature Discount Factor γ GAE Parameter λ KL Penalty Coefficient β Policy Clipping Parameter ϵ Value Clipping Parameter ϵ PPO 8 512 512/8 = 64 64 2 0.6 1.0 1.0 1e-4 0.2 0.2 Search Space: {8, 16, 32} Search Space: {256, 512} Search Space: {1, 2} Search Space: {0.6, 0.8, 1.0} Search Space: [0.95 1.0] Search Space: {1e-1, 1e-2, 3e-3, 1e-4} Table 2: Summary of RestEM hyperparamters used in the experiments. Parameter Optimizer Adam Parameters (β1, β2) Learning rate Weight Decay Max Global Gradient Norm for Clipping Learning Rate Scheduler Warm Up Value TRAINING AdamW (0.9, 0.999) 1 106 0.0 1.0 Polynomial 3% of training steps RESTEM # iterations # Sampled Responses per Prompt Sampling Temperature Checkpoints every # iteration Checkpoint Selection 10 8 0.6 500 step until validation improves Search Space: {until validation improves, best validation} Search Space: {8, 32} Search Space: {0.6, 0.8, 1.0} model responses is straightforward, as the final answer is typically an integer, marked by #### in the solution. C.2 PPO IMPLEMENTATION To ensure our PPO implementation is robust, and our evaluation reflects its full potential, we have applied set of well-established techniques and best practices from the literature (Huang et al., 2024; 18 Table 3: Summary of DPO-Positive hyperparameters used in the experiments. Parameter Optimizer Adam Parameters (β1, β2) Learning rate Weight Decay Max Global Gradient Norm for Clipping Learning Rate Scheduler Warm Up Value TRAINING AdamW (0.9, 0.999) 1 106 0.0 1.0 Polynomial 3% of training steps DPO-POSITIVE # DPO-β # DPO-Positive-λ # Epochs # Sampled Responses per Prompt # Pairs per prompt Sampling Temperature 0.1 for MATH, 0.3 for GSM8K 50. 3 64 64 0.6 Search Space: {3, 8} Search Space: {8, 64} Search Space: {8, 64} Ivison et al., 2024; Zheng et al., 2023). Below, we outline the key implementation details that were most effective in our experiments: Advantage Normalization: After calculating the advantages, we normalize them to have zero mean and unit variance, not only across the batch but also across data parallel ranks. This normalization step is applied consistently in both our PPO and VinePPO implementations. Reward Normalization: We follow Ivison et al. (2024) and do not normalize the rewards, as the reward structure in our task is already well-defined within the range of [0, 1]. Specifically, correct responses are assigned reward of 1, while incorrect responses receive 0. End-of-Sequence (EOS) Trick: As detailed in Appendix A, rewards are only applied at the final token of response, which corresponds to the EOS token when the response is complete. For responses that exceed the maximum length, we truncate the response to the maximum length and apply the reward to the last token of the truncated sequence. We also experimented with penalizing truncated responses by assigning negative reward (-1), but this did not lead to performance improvements. Dropout Disabling: During the RL tuning phase, we disable dropout across all models. This ensures that the log probabilities remain consistent between different forward passes, thereby avoiding stochastic effects that could hurt training stability. Fixed KL Coefficient We use constant coefficient for the KL penalty. Although the original PPO implementation for finetining language models (Ziegler et al., 2019) utilized an adaptive KL controller, more recent implementations typically do not use this approach (Ouyang et al., 2022; Touvron et al., 2023; Xu et al., 2024). C.3 SFT MODELS To ensure systematic and reproducible evaluation, we create our SFT models πref by finetuning the base pretrained LLMs (as opposed to their Instruct version) on the training splits of the respective datasets. Specifically, we produce four distinct SFT models: two base LLM (DeepSeekMath 7B and RhoMath 1.1B ) across two datasets (MATH and GSM8K). The base models are finetuned using the Adam optimizer without weight decay. We employ learning rate warm-up over 6% of the total training steps. Each model is trained for three epochs with batch size of 64, and the best checkpoint is selected based on validation accuracy. For each SFT model, we conduct hyperparameter sweep over learning rates in the range {1 107, 3 107, 1 106, 3 106, 1 105, 3 105, 8 105, 1 104} to ensure optimal performance. We then use these SFT models as the initial checkpoint for training the methods mentioned in our paper. 19 C.4 EVALUATION We evaluate each methods performance on the test sets of each dataset. For example, when we report that PPO achieves 42.8% accuracy on the MATH dataset for the DeepSeekMath 7B model, this means the PPO training was initialized with the SFT model specific to DeepSeekMath 7B on the MATH dataset, and accuracy was measured on the MATH test set. Our primary evaluation metric is accuracy, specifically Pass@1, which reflects the percentage of correctly answered problems on the first attempt. This metric is crucial because it represents realistic user interaction, where the model is expected to deliver correct answer without the need for multiple tries. For each evaluation, we sample response from the model for given prompt, using maximum token length of 1024 and temperature of 0.35. response is considered correct if its final answer matches the ground truth final answer, as detailed in Appendix C.1. Furthermore, each accuracy score is averaged over 16 evaluation rounds, each conducted with different random seeds. This will ensure robust and low variance assessment of model performance. C.5 BASELINES DPO+ (DPO-Positive) (Pal et al., 2024) The original DPO method has failure mode when the edit distance between positive (correct) and negative (incorrect) responses is small. In these cases, the probability of both responses tends to decrease. This issue is especially common in reasoning and mathematical tasks, where multiple solution paths may involve similar equations or steps. Although DPO achieves its goal by reducing the probability of the incorrect response more than the correct one, it ultimately still lowers the likelihood of generating the correct response. This undermines model performance, making it failure mode despite partially fulfilling the DPO objective. (Pal et al., 2024; Hwang et al., 2024). While previous methods mitigated this issue by maintaining high edit distance between positive and negative response pairs, DPO-Positive (Pal et al., 2024) addresses it more effectively. It introduces an additional term to the DPO objective, penalizing any reduction in the probability of the correct response below its probability under the reference model. This ensures that the correct response is not overly suppressed, even when the edit distance is small. The final objective of DPO-Positive is:: LDPO-Positive(πθ; πref) = E(x,yw,yl)D log σ β (cid:124) (cid:34) (cid:32) (cid:32) log πθ(ywx) πref(ywx) log πθ(ylx) πref(ylx) (cid:123)(cid:122) DPO Original term (cid:33) (cid:125) (cid:18) λ max 0, log πref(ywx) πθ(ywx) (cid:33)(cid:35) (cid:19) (cid:125) (18) (cid:124) (cid:123)(cid:122) DPO-Positive additional term where λ is hyperparameter controlling the weight of the additional term keeping the probabilities of correct responses high. We chose DPO-Positive as baseline due to its strong performance in (Setlur et al., 2024). RestEM (Singh et al., 2024) RestEM is an iterative method where, in each iteration, the base model is trained on correct, self-generated responses from the chosen checkpoint of the previous iteration. RestEM takes gradient steps to maximize this objective until the fine-tuned models accuracy drops on validation split. The objective of the fine-tuning process is to maximize the log-likelihood of correct responses. Training the model with maximum likelihood objective on correct responses is mathematically equivalent to training the model with REINFORCE (Sutton et al., 1999), without baseline, where the entire response is treated as single action. The reward is 1 when the response is correct, and 0 otherwise. Specifically, we have: ExD,yπ(x),R(x;y)=1 [θ log Pθ(yx)] (cid:124) (cid:125) (cid:123)(cid:122) max log-likelihood on correct responses = ExD,yπ(x) [θ log Pθ(yx)R(x; y)] (cid:123)(cid:122) (cid:125) REINFORCE (19) (cid:124) Therefore, maximizing log-likelihood training on correct responses is equivalent to train with policy gradient without precise credit assignment, such as without advantages for specific actions. In our experiments, we observe the impact of this limitation in both Figure C.3 and Figure C.4 where RestEM overfits on the training data. 20 Figure C.3: Performance comparisons across different models and datasets: (a) RhoMath 1.1B on GSM8K, (b) RhoMath 1.1B on MATH, (c) DeepSeekMath 7B on GSM8K, and (d) DeepSeekMath 7B on MATH. The yellow points are chosen checkpoints based on the RestEM rule. Within each iteration, we train on the generated data of the chosen checkpoint for eight epochs and then we choose the first place where performance on validation split drops following Singh et al. (2024) C.6 HYPERPARAMETERS In this section, we present comprehensive overview of the hyperparameters used in our experiments. Its important to note that the number of training samples was carefully selected to ensure that the amount of training data remained consistent across all methods. PPO Finetuning LLMs using PPO is known to be sensitive to hyperparameter selection, and finding the optimal settings is critical for achieving strong performance. To ensure the robustness of our study, we explored hyperparameter values reported in recent studies (Shao et al., 2024; Zheng et al., 2023; Ivison et al., 2024; Huang et al., 2024) and conducted various sweeps across wide range of values to identify the best configuration for our tasks and models. The full set of hyperparameters, along with their respective search spaces, is detailed in Table 1. VinePPO We utilized the same hyperparameter setup as in the PPO implementation  (Table 1)  for VinePPO. As outlined in Section 5, the number of MC samples, K, was set to 9 for all experiments. 21 Figure C.4: scatter plot showing the relationship between achieved training accuracy and test accuracy at various checkpoints throughout training. This plot highlights the dynamics of overfitting and generalization across different methods. As we progress from no credit assignment to accurate credit assignmentfrom RestEM to DPO+, PPO, and finally VinePPOgeneralization improves and overfitting decreases. In other words, by treating the training dataset as resource, VinePPO achieves higher test accuracy per unit of training data consumed. Note that all these are fully trained. Note that the training accuracy does not reach 100 percent due to several factors, including mechanisms like the KL penalty in DPO+, PPO, and VinePPO, the reset to the base model in RestEM, or the absence of any correct self-generated responses for certain questions. RestEM To ensure fair comparison we equalize the number of sampled responses for training between our RestEM run and our PPO runs. Therefore, in each RestEM iteration we sample 8 responses per prompt and train for 8 epochs on the correct responses. To enhance RestEMs performance, we also conducted sweep of other reasonable parameters  (Table 2)  . This included increasing the number of samples to expand the training data and reducing the number of correct responses per question to minimize overfitting.However, we observed no significant improvement . DPO+ (DPO-Positive) We adopted the same hyperparameters as those used by Setlur et al. In addition, we conducted search for the optimal value of β to see if using the same (2024). β as in our PPO experiments would yield better performance than the values they recommended. To maintain fair comparison, we ensured that the number of training samples in our DPO+ runs matched those in our PPO run where we trained for eight epochs, with each epoch consisting of training on eight responses per question. To match this, we generated 64 pairs of positive and negative responses given 64 self-generated responses from the base model.  (Table 3)  C.7 TRAIN VS. TEST DURING TRAINING When training on reasoning datasets, the training data can be viewed as finite resource of learning signals. Algorithms that exhaust this resource through memorization tend to generalize less effectively on the test set. As we move from RL-free methods or less accurate credit assignment towards more accurate credit assignment, or full reinforcement learningfrom RestEM to DPO, PPO, and finally VinePPOthe model demonstrates higher test accuracy gains per unit of training data consumed. This trend is illustrated in Figure C.4. 22 Table 4: Average time spent per each training step for different methods and models measured for MATH dataset Model Hardware Average Training Step Time (s) RhoMath 1.1B RhoMath 1.1B 4 Nvidia A100 80GB 4 Nvidia A100 80GB DeepSeekMath 7B 8 Nvidia H100 80GB DeepSeekMath 7B 8 Nvidia H100 80GB 80 312 583 Method PPO VinePPO PPO VinePPO C.8 COMPUTE All experiments were conducted using multi-GPU training to efficiently handle the computational demands of large-scale models. For the RhoMath 1.1B model, we utilized node with 4 Nvidia A100 80GB GPUs to train both PPO and VinePPO. For the larger DeepSeekMath 7B model, we employed more powerful setup, using node with 8 Nvidia H100 80GB GPUs. Additionally, for training DeepSeekMath 7B models with the RestEM approach, we used node with 4 Nvidia A100 80GB GPUs. The average training step time for each method on the MATH dataset is presented in Table 4. C.9 SOFTWARE STACK Both PPO and VinePPO require robust and efficient implementation. For model implementation, we utilize the Huggingface library. Training is carried out using the DeepSpeed distributed training library, which offers efficient multi-GPU support. Specifically, we employ DeepSpeed ZeRO stage 0 (vanilla data parallelism) for RhoMath 1.1B and ZeRO stage 2 (shared optimizer states and gradients across GPUs) for DeepSeekMath 7B . For trajectory sampling during RL training, we rely on the vLLM library (Kwon et al., 2023), which provides optimized inference for LLMs. Additionally, VinePPO leverages vLLM to generate Monte Carlo samples for value estimation. This software stack ensures that our experiments are both efficient and reproducible. For instance, during VinePPO training, we achieve an inference speed of up to 30K tokens per second using 8 Nvidia H100 GPUs with the DeepSeekMath 7B model. C.10 REPRODUCIBILITY In this study, all experiments were conducted using open-source libraries, publicly available datasets, and open-weight LLMs. To ensure full reproducibility, we will release both Singularity and Docker containers, equipped with all dependencies and libraries, enabling our experiments to be run on any machine equipped with NVIDIA GPUs, now or in the future. Additionally, we will make our codebase publicly available on GitHub at https://github.com/McGill-NLP/VinePPO."
        },
        {
            "title": "D FULL RESULTS",
            "content": "D.1 TRAINING PLOTS In this section, we present additional training plots for both PPO and VinePPO on the GSM8K dataset, as shown in Figure D.5. Figure D.6 further illustrates the trade-off between accuracy and KL divergence, while Figure D.7 highlights the computational efficiency of the models8. We observe consistent patterns with the results reported in Section 6. Although the performance gap for the DeepSeekMath 7B model is narrower on GSM8K, VinePPO still higher accuracy with significantly lower KL divergence and faster per-iteration time (this happens because responses to GSM8K problems are typically shorter, making MC estimation quite fast). 8For GSM8K runs of RhoMath 1.1B , different hardware was used, making direct comparison of wall-clock time not feasible. Figure D.5: Comparison of the training behavior between VinePPO and PPO. VinePPO demonstrates consistently higher accuracy throughout the training on the GSM8K dataset. Refer to Figure 5 for MATH dataset. Figure D.6: Task accuracy as function of KL divergence during training on the GSM8K dataset. VinePPO significantly higher accuracy per KL. Refer to Figure 6 for MATH dataset. D.2 VALUE PREDICTION ANALYSIS In this section, we provide additional plots for value analysis. Specifically, Figures D.8 to D.11 demonstrates these plots for on the MATH dataset, and Figures D.12 to D.15 on the GSM8K dataset. Furthermore, we present the prediction error per step in Figures D.16 to D.19. Figure D.7: Accuracy vs. Wall Clock Time for both methods measured on the same hardware throughout the entire training. Since the responses to GSM8K problems are short, VinePPO is even faster per-iteration in our setup and it reaches PPOs peak performance in fewer iterations and less overall time. Figure D.8: Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE). 25 Figure D.9: Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE). Figure D.10: Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE). Figure D.11: Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE). Figure D.12: Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE). 27 Figure D.13: Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE). Figure D.14: Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE). Figure D.15: Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE). Figure D.16: Visualizing the Mean Absolute Error (MAE) of the value predictions in different point of reasoning chain, plotted for DeepSeekMath 7B on MATH dataset. Figure D.17: Visualizing the Mean Absolute Error (MAE) of the value predictions in different point of reasoning chain, plotted for DeepSeekMath 7B on GSM8K dataset. 29 Figure D.18: Visualizing the Mean Absolute Error (MAE) of the value predictions in different point of reasoning chain, plotted for RhoMath 1.1B on MATH dataset. Figure D.19: Visualizing the Mean Absolute Error (MAE) of the value predictions in different point of reasoning chain, plotted for RhoMath 1.1B on GSM8K dataset."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "HEC Montreal",
        "McGill University",
        "Microsoft Research",
        "Mila",
        "Universite de Montreal"
    ]
}