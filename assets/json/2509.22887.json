{
    "paper_title": "Infusing Theory of Mind into Socially Intelligent LLM Agents",
    "authors": [
        "EunJeong Hwang",
        "Yuwei Yin",
        "Giuseppe Carenini",
        "Peter West",
        "Vered Shwartz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 7 8 8 2 2 . 9 0 5 2 : r Infusing Theory of Mind into Socially Intelligent LLM Agents"
        },
        {
            "title": "INFUSING THEORY OF MIND\nINTO SOCIALLY INTELLIGENT LLM AGENTS",
            "content": "EunJeong Hwang1,2, Yuwei Yin1, Giuseppe Carenini1, Peter West1, Vered Shwartz1,2 1University of British Columbia, 2Vector Institute for AI {ejhwang,yuweiyin,carenini,pwest,vshwartz}@cs.ubc.ca"
        },
        {
            "title": "ABSTRACT",
            "content": "Theory of Mind (ToM)an understanding of the mental states of othersis key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (TOMA), ToM-focused dialogue agent. TOMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over range of baselines. Comprehensive analysis shows that TOMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest step forward in integrating ToM for building socially intelligent LLM agents."
        },
        {
            "title": "INTRODUCTION",
            "content": "Success in social interactions defined by goal achievement, adherence to social norms, and more depends not just on expressing our own intentions and beliefs, but also on understanding our conversation partners. Theory of Mind (ToM), the cognitive ability to understand the mental states of others (Premack & Woodruff, 1978; Baron-Cohen et al., 1985), captures this intuition and allows social reasoning and strategic behavior (Apperly & Butterfill, 2009). Here, we study whether ToM can serve as similarly powerful element in social LLM agents. The extent to which LLMs already possess ToM is debatable (Kosinski, 2024; Shapira et al., 2024), despite the deployment of LLMs in settings where understanding the user is crucial (e.g. job interviews, customer service). Methods for improving LLMs ToM abilities range from chain-of-thought prompting (Wilf et al., 2024; Shinoda et al., 2025), through neuro-symbolic methods that combine LLMs with symbolic belief tracking (Sclar et al., 2023), to Bayesian Inverse Planning (Ying et al., 2023), and inference-time hypothesis generation (Kim et al., 2025). However, past work on ToM for LLMs typically evaluates this ability directly on QA setups (Kim et al., 2023; Chen et al., 2024), rather than its usefulness in social situations. Meanwhile, existing research in interactive social environments like Sotopia (Zhou et al., 2024) has largely focused on training models to generate utterances that lead to successful conversations (Kong et al., 2025; Yu et al., 2025), overlooking the role of explicit mental state modeling. In this work, we address the question of how to equip LLMs with Theory of Mind abilities that can effectively improve their social reasoning. We demonstrate that even simply prompting LLMs to generate mental states between dialogue turns can significantly contribute to goal achievement. To maximize this benefit, we propose ToMAgent (TOMA), method for goal-oriented social reasoning in dialogues that combines ToM predictions with conversation outcome prediction to select the best trajectory for training. As illustrated in Figure 1, given social scenario such as Two friends are camping in the cold and there is only one blanket and opposing agent goals (e.g., Agent1 wants Equal contribution. 1 Code, training data, and models are available at https://github.com/eujhwang/toma. 1 Infusing Theory of Mind into Socially Intelligent LLM Agents Figure 1: Overview of TOMA. We sample scenarios, goals, and conversation histories from SotopiaPi (Step 1), generate candidate mental stateutterance pairs and simulate dialogues (Steps 23), evaluate goal achievement to select high-utility pairs (Step 5), and train the model after collecting these training pairs (Steps 67). to keep it for themselves while Agent2 wants to share), the target agent (Agent1) is asked to (i) make multiple hypotheses about the other agents mental states, (ii) generate the corresponding next utterances, and (iii) simulate the remaining dialogue and estimate the likelihood that each dialogue leads to goal completion. We then use the most successful conversations to fine-tune the same LLM to generate the partners mental states (e.g., they are cold and uncomfortable) and the strategic utterances that are likely to result in goal achievement (e.g., suggesting compromise). TOMA is evaluated on the Sotopia dataset (Zhou et al., 2024; Wang et al., 2024), an open-ended social reasoning environment that includes diverse goal-oriented social scenarios such as collaboration, negotiation, persuasion, and competition. Our experimental results demonstrate that TOMA achieves score improvements by up to 18.9% and 6.9% compared to the best base model variant for Qwen2.5-3B and Qwen2.5-7B, respectively, and is also competitive with GPT-5 nano baseline. Furthermore, we provide comprehensive analysis of our results, including the success and failure factors across different scenarios and the ToM dimensions that are generated by the model. The analysis shows that TOMA exhibits more strategic, goal-oriented, and long-horizon behavior than the baselines, while also achieving better personal relationships with the partner. Our findings highlight that social reasoning in LLMs cannot be achieved through optimizing their performance on general reasoning benchmarks (Leaderboard, 2025) alone; it requires explicit modeling of mental states to enable safe, fair, and effective interactions with humans."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "In this section, we introduce TOMA, look-ahead training framework that improves agents ToM ability in social interactions to achieve their goals. Conditioned on scenario (e.g., two friends are camping in the cold and there is only one blanket in Figure 1) and the agents private goals (e.g., sharing the only blanket available vs. keeping the blanket for yourself), the goal is to reach mutually agreeable solution, such as taking turns or sharing the blanket, through dialogue. Our proposed training protocol consists of generating training examples and fine-tuning an LLMbased agent, as illustrated in Figure 1. First, we sample conversation contexts (2.1). At each step of the dialogue, we use an LLM to first elicit multiple ToM hypotheses corresponding to the mental state of each agent (i.e., self and first-order beliefs), and then generate an appropriate utterance conditioned on these mental states (2.2). To identify useful mental states and utterances that eventually contribute to goal achievement, we run short-horizon simulations and keep pairs that achieve the highest score on the simulated conversations (2.2). Finally, we use the identified set of mental states and utterance pairs as training examples for fine-tuning the LLM to generate both the latent mental states and utterances (2.3). 2.1 SAMPLING CONVERSATIONS TO SEED SCENARIOS AND AGENTS To train models capable of socially grounded, goal-oriented reasoning in diverse contexts, it is imperative to use data that captures the complexity of real-world social interactions. To this end, we adopt the Sotopia-Pi dataset (Wang et al., 2024), which provides diverse set of scenarios and social 2 Infusing Theory of Mind into Socially Intelligent LLM Agents goals, allowing us to simulate complex social interactions during training. We first randomly sample 500 episodes from Sotopia-Pi, where each episode provides social scenario, two agents with their own goals, and multi-turn dialogue between them. Then, for each scenario, we randomly sample two conversations provided by Sotopia-Pi and truncate each to at most four turns to ensure the context is early enough that the social goals have not yet been achieved. We denote each resulting instance, comprising scenario, agents social goals, and partial conversation history, as H, which is referred to as the context in subsequent steps. These contexts serve as the default input set for eliciting useful mental states and utterances. 2.2 GENERATING AND SCORING TOM HYPOTHESES AND UTTERANCES The goal of this phase is to generate plausible mental states and utterances that help an agent advance its own social goal, which can be used to train goal-oriented ToM-aware agents. Specifically, we ask the target model (Agent1 in Figure 1), which is the model to be trained, to generate its own latent ToM states, produce corresponding utterances, and utilize these pairs for training. Exploring mental states and utterances. The first key steps (23 in Figure 1) toward socially intelligent behavior is to explore range of plausible mental states and corresponding utterances that align with the agents social goals and conversational context. For this purpose, from each context H, which includes the scenario, the agents private social goals, and the partial conversation history up to that point, we prompt an LMtarget to generate mental state hypotheses, where each hypothesis may consist of multiple sentences capturing different aspects of the current (target) agents internal state: mk LMtarget(m H). The model is asked to ensure that each generated hypothesis covers at least three out of the five ToM dimensions: beliefs, desires, intentions, emotions, and knowledge. For each mental state hypothesis mk, we sample utterances: uk,j LMtarget(u mk, H). This gives us candidate set of mental state and utterance pairs CH = {(mk, uk,j)}k=1..K, j=1..J . Running simulations to evaluate downstream utility. To identify the most useful mental state and utterance pairs for training that most effectively contribute to successful goal achievement, we perform short-horizon simulation to look ahead into the future trajectory of the dialogue and assess how each pair influences the goal achievement of agents throughout the conversation (Steps 45 in Figure 1). In the first turn, the target agent produces utterance uk,j conditioned on the mental state hypothesis mk and the context H. Then the conversation continues for up to four future turns, simulating the partner agent using LMpartner. Once the simulation is done, we compute the goal achievement score (010) for each agent, Starget and Spartner, reflecting the degree to which each agent successfully advanced its objectives. Since successful conversation is supposed to contribute to both agents goals, the average goal score is calculated: ˆS(h, mk, uk,j) = 1 2 (Starget + Spartner). We retain all pairs with an average score 9. If none meet this threshold, we keep the top-scoring pair. The resulting high-scoring pairs form training set that we use for fine-tuning. See Appendix for the prompts and the training instance format. 2.3 FINE-TUNING ON TOM STATES AND UTTERANCES To instill Theory of Mind reasoning into the model, we fine-tune it on high-scoring mental state and utterance pairs identified through dialogue simulation that are maximally useful to advance their goals (Step 7 in Figure 1). From each selected pair (m, u) and its context (i.e., scenario, private goal, and dialogue history), we construct two types of training examples: one where the model is prompted with and trained to generate (i.e., mental-state prediction), and another where the model is prompted with both and to generate (i.e., utterance prediction). Together, we train the model to align with the joint behavior (u, H) = (u m, H) (m H) that led to high goal scores. We finetune the model LMtarget using standard cross-entropy loss over next-token prediction. The resulting objective can be formalized as: LCE(ϕ) = E(H,m,u)D CE(m, ϕ(H)) + CE(u, ϕ(H, m)) (1) (cid:105) (cid:104) = log Pϕ(m H) log Pϕ(u H, m), (2) where CE(y, ϕ(x)) denotes the token-level cross-entropy loss for target given input under model ϕ. This way, the model learns to associate contexts with latent mental states and utterances that were empirically effective during simulation. This implicitly improves its internal mechanism over (m H) and (u m, H), aligning them to achieve their goals in various social situations. 3 Infusing Theory of Mind into Socially Intelligent LLM Agents"
        },
        {
            "title": "3 EXPERIMENTAL SETUP",
            "content": "We follow the setup defined in Sotopia (Zhou et al., 2024). Each instance in Sotopia provides the scenario for the current social interaction between two agents, as well as their names and social goals. Models evaluated on Sotopia take the role of one agent, and they are tasked with having dialogue with the other agent that results in achieving their own social goals. We describe the evaluation setup (3.1) and training settings (3.2). See Appendix for more experiment details and Appendix for all LLM prompts. 3.1 EVALUATION Data. We adopt the Sotopia-Pi dataset (Wang et al., 2024), which provides multiple social scenarios for the agents to simulate conversations dynamically. We use both the all and hard sets to evaluate models. The all set includes 90 scenarios combined with 5 agent pairs, resulting in total of 450 testing instances. Each pair among the five shares the same scenario description and agent goals, but the agent names and profiles are different. The hard set consists of 14 scenarios that are challenging to GPT-4 (Achiam et al., 2023), yielding 70 testing instances. Metrics. We follow Sotopia-Eval (Zhou et al., 2024), suite of multi-dimensional evaluation metrics, and use LLM-as-a-Judge (Gu et al., 2024; OpenAI, 2025) to assess an entire conversation. We focus on the following central criteria from the original setup: (1) Goal: the extent to which the agent achieved their goals (010); (2) Relationship (Rel): whether the interactions between the agents help preserve or enhance their personal relationships prior to the conversation (-55); and (3) Knowledge (Know): whether the agent gained new and important information through the interaction (010). The LLM judge provides both the rating score and its rationale on each dimension and for each agent. We use GPT-5-mini (OpenAI, 2025) as the evaluator. Partner Agent. We follow the original Sotopia evaluation setup which evaluates both agents on their goal achievement and social awareness, and reports the average scores of the two agents. In this Self-Play setup, both agents are instantiated as model with the same complexity (e.g., base with base, TOMA with TOMA, etc.). Baselines. We consider two base settings as follows: (1) Base: Using the vanilla language model (without fine-tuning), as lower bound for the LLMs ability to hold social dialogue; and (2) Base+MS: where we apply two-step prompt to the base model. We first generate mental states based on the context and then generate an utterance conditioned on the context and mental states. This setup quantifies both the quality and the utility of the mental states generated by the base model. 3.2 TRAINING Data. We use the scenarios and the agents names and social goals from Sotopia-Pi (Wang et al., 2024) to seed our conversations, as shown in Figure 1, Step 1. We instantiate each agent with an instance of the pre-trained LLM (which we will later fine-tune on the training set described here). Then, we generate conversation between the two agents using the simulation protocol provided by Sotopia, which defines the action types and schedules the agents to speak iteratively, and modify it to introduce mental states as latent variable. Before generating each utterance, we prompt the agent to generate or update their own mental states and their first-order beliefs about the mental states of the other agent. We set the number of mental state hypotheses to = 2 and the number of utterance candidates per hypothesis to = 2. Models. We use Qwen2.5-3B and Qwen2.5-7B (Qwen, 2024a) as the backbone LLM in our experiments. We use 4-bit quantized version of Qwen2.5-14B as LMpartner to ensure the partner generates reasonable utterances in simulations independent of the model size being trained. Finally, Gemini-Flash (Comanici et al., 2025) is used to score the simulated conversations. Fine-tuning. Utilizing the paired utterances (Uttr) and mental states (MS) from the generated multi-turn conversations, we conduct supervised fine-tuning (Pareja et al., 2025) over low-rank adapters (Hu et al., 2022) of small language models (i.e., Qwen2.5-3B and Qwen2.5-7B) with the data obtained in 2.2. We consider the following three training objectives: (1) FT+Uttr: Fine-tuning models only on utterance generation, ablating the mental states supervision to assess its contribution 4 Infusing Theory of Mind into Socially Intelligent LLM Agents to the conversation success; (2) FT+MS: Fine-tuning models to generate mental states, ablating the utterance generation to assess its contribution to the conversation success; and (3) FT+MS+Uttr (TOMA): Fine-tuning models on both utterance generation and mental states alignment, as explained in 2.3. For the evaluation of FT+MS and TOMA, the model generates mental states first and then produces the utterances to respect the causal constraint between the two."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We compare the performance of TOMA to the baselines (4.1). Then, we analyze the effect of different partner agents on goal achievement (4.2), the performance across scenario types (4.3), and the success and failure factors in goal achievement (4.4). Finally, we present statistical analysis of TOMAs performance across different evaluation dimensions (Appendix B.1). 4.1 DOES THEORY OF MIND HELP WITH SOCIAL REASONING? Method Base Base+MS FT+Uttr FT+MS FT+MS+Uttr (TOMA) Qwen2.5-3B Rel Know Goal Avg. Qwen2.5-7B Rel Know Goal Avg. 0.97 1.54 1.92 2.37 2.18 3.29 3.48 4.01 3.81 4.22 5.25 5. 6.60 6.69 6.84 3.17 3.65 4.18 4.29 4.41 2.07 2.47 2.42 2.73 2.70 4.54 4. 4.78 4.40 4.77 7.26 7.30 7.43 7.46 7.67 4.62 4.74 4.88 4.86 5.05 Table 1: Overall performance in terms of Rel, Know, and Goal dimensions on the all split. Method Base Base+MS FT+Uttr FT+MS FT+MS+Uttr (TOMA) Qwen2.5-3B Rel Know Goal Avg. Qwen2.5-7B Rel Know Goal Avg. GPT-5-nano Rel Know Goal Avg. 0.18 1.04 1.22 1.70 1.90 4.20 4.05 4.10 4.08 4.22 4.96 5.27 5.23 5.42 5. 3.11 3.45 3.52 3.73 4.00 0.58 2.17 1.36 2.40 2.33 4.21 4.51 4.43 4.33 4. 5.26 5.86 5.70 6.30 6.32 3.35 4.18 3.83 4.34 4.48 0.77 1.51 4.39 5. 6.24 6.67 3.80 4.46 - - - - - - - - - - - - Table 2: Overall performance in terms of Rel, Know, and Goal dimensions on the hard split. TOMA outperforms the baselines. Tables 1 and 2 present the performance of models on the all and hard subsets of the Sotopia test set, respectively. On both subsets, TOMA consistently outperforms all other model variants across the relationship, knowledge, and goal completion dimensions. Moreover, TOMA performs competitively with strong GPT-5-nano baseline (Base+MS in Table 2), even though GPT-5-nano surpasses Qwen2.5-7B on several general reasoning benchmarks (White et al., 2025; Leaderboard, 2025). Specifically, TOMA (and even slightly more FT+MS) substantially outperforms GPT-5 nano on the relationship dimension, indicating it generates utterances with better sensitivity to the other partners feelings. Compared to the best base model variant (Base+MS), TOMA achieves score improvements of 16.8% and 6.6% on both datasets for 3B and 7B models, respectively, averaged across the all and hard sets. Mental-state conditioning improves relationship modeling. We observe that models that generate utterances without explicit mental-state conditioning (Base and FT+Uttr) perform significantly worse on the relationship dimension than models that use mental-state representations (Base+MS, FT+MS, and TOMA). This may suggest that explicitly considering the partner agents mental state can help the target agent preserve positive relationship with them. Training on utterances alone (FT+Uttr) generally improves the knowledge and goal scores compared to the Base models on all split. The improvement in goal completion is unsurprising given that our fine-tuned models are supervised to maximize goal completion. However, this goal-directed behavior may come at the expense of interpersonal sensitivity, as indicated in its lower relationship scores compared to models conditioned on mental states. Fine-tuning on mental states does not hurt utterance effectiveness. Training the model only on mental states (FT+MS) could potentially decrease its general generation ability. However, our 5 Infusing Theory of Mind into Socially Intelligent LLM Agents fine-tuned model is still able to produce reasonably effective utterances, achieving higher goal and relationship scores than the base models across both splits. TOMA, trained to jointly improve the prediction of latent mental states and the corresponding appropriate utterances, achieves the best of both worlds, effectively maintaining relationships, knowledge seeking, and goal-oriented behavior. 6 5 6 6.5 5. 5.5 c o Qwen2.5-3B Qwen2.5-7B Theory of Mind enables long-horizon adaptation. Figure 2 compares how efficiently agents achieve their goals under different maximum turn limits. Surprisingly, the goal score of Base decreases as the number of turns increases. This is likely because the base model often repeats the same argument, making no progress across turns, which the GPT-5 judge penalizes. Base+MS shows slight improvement, but starts declining again for conversations longer than 15 turns. In contrast, TOMA consistently improves its goal completion score as the number of turns increases, suggesting that Figure 2: Goal completion scores across it may be adapting its strategy over time to achieve the 520 turns on the hard split. goal more effectively. This adaptivity and long-horizon planning behavior can make ToM-informed agents better suited for real-world social interactions that often require longer and more flexible responses. Base+MS TOMA Turns Turns Base 20 10 10 15 20 5 5 4.2 HOW DO DIFFERENT PARTNERS AFFECT GOAL ACHIEVEMENT? 7B 7B All B 32B 14B 14B 32B a Metric 3.4 3.64 5.72 5.75 5.11 5.36 3.49 3.73 4.81 5.00 5.28 5. 5.83 5.86 3.85 4.01 3.17 3.35 4.94 5.23 3.38 3.53 3.29 3. 5.27 5.41 3.56 3.67 4.99 4.96 Goal Goal Target Model Base+MS TOMA Base+MS TOMA Base+MS TOMA (Target=7B) Partner=Base3B (Target=3B) Partner=Base3B Our main evaluations follow the original self-play setup where both agents are instances of the same model (e.g., TOMA with Qwen2.5-3B). Here, we address the question of how different partner can imthe performance of pact the target agent. To that end, we test how target agent based on the best model variants of each of base (Base+MS) and TOMA fares when paired with partner model of different complexity (Base) and size (332B). We conduct the evaluation on the hard split. For each scenario, we use the original 5 distinct role pairs and swap the agent roles (e.g., agent 1 as target and agent 2 as partner, and vice versa), resulting in 10 role pairs. We report both the goal completion score as well as the average across goal, relationship, and knowledge scores; once for the target agent and once for the average of both agents, in Table 3. Table 3: Performance of the target agent (Target) and average performance of both agents (Both) with respect to goal completion (Goal) and the average across goal, relationship, and knowledge scores (All). We use the hard split and vary the size of the partner agent (Base). Base+MS TOMA 4.35 4.34 4.35 3.95 3.63 3.48 3.84 4. 2.93 3.18 3.93 4.01 3.18 3.42 2.76 2.96 3.58 3.64 2.88 3. 3.79 3.86 4.12 4.27 3.26 3.28 3.1 3.04 3.77 4.14 3.2 3. 2.92 3.1 All TOMA target agent not only improves its own goal completion, but also their partners. The target agent trained with our method performs best across most settings  (Table 3)  . TOMA results in consistently better combined outcomes (Table 3, top) between target and partner, suggesting that our agent with improved ToM ability not only benefits itself, but also helps the other agent, likely reaching agreeable solutions for both agents. As we show in 4.4, this effect is likely due to the agents ability to employ more effective strategies across broader range of interaction scenarios (e.g., coordination, negotiation, persuasion, etc.). The individual outcome for the target is somewhat more complex (Table 3, bottom). For the larger target size (7B), TOMA results in consistently better target outcomes. For the 3B target size, the winner on goal achievement is inconsistent between TOMA and Base+MS; we hypothesize that its harder for small target agent to achieve their goal when conversing with larger and socially unaware partner. With that said, it is worth noting that TOMA wins at All metrics in most cases, meaning it is less likely than Base+MS to sacrifice relationships or knowledge. Coordination dynamics depend on both agent and partner sizes. We observe that when the partner is larger, the overall conversation outcome as measured by the average scores for both 6 Infusing Theory of Mind into Socially Intelligent LLM Agents agents improves. Looking at the target agent scores shows that the factors behind this improvement differ between the 3B and 7B TOMA target agents. The 7B target agent shows consistent performance improvement with partner size across all dimensions, suggesting that it can benefit from more powerful partner. Conversely, the scores for the 3B target agent dont consistently improve with the partners size, again suggesting that in that case, larger partner leads to higher scores primarily for the partner. We observe that the 3B TOMA agent is more likely to achieve its goal when paired with an equal-size partner than with considerably larger partner (14B or 32B). 4.3 HOW DOES TOMA PERFORM ACROSS DIFFERENT CONVERSATION TYPES? Categorizing scenarios into types. We are also interested in the performance and behavior of TOMA across different types of social interaction, where the agents goals may be either aligned or competing. We manually examined the 90 scenarios in the all split and categorized them into four conversation types: cooperation - win-win situation where both agents can fully achieve their goals without conflicts or compromises (36 scenarios); negotiation - positive-sum game where the agents can reach their goals to satisfactory extent with certain compromises (28 scenarios); persuasion - positive-sum game where the target agent tries to convince the partner to act in way that promotes the target agents goals (13 scenarios); and conflict - zero-sum or even negative-sum game where their goals are in conflict and can hardly be solved through compromise (13 scenarios). See Appendix for full details of each scenario group. TOMA outperforms the base model under all scenario types. We analyzed 450 conversations: five conversations for each of the 90 scenarios in the all split. Figure 3 looks at the average goal achievement score of the target agent in each conversation type, comparing agents implemented as the base model vs. TOMA. Data points on the right of the orange dotted line (x = 0 neutral line) correspond to conversations on which TOMA outperformed the base model. As observed, the first quartile (Q1) of each box is on the neutral line, indicating that TOMA outperforms base in at least 75% of the conversations of each type. Considering the inter-quartile range (IQR), TOMA brings greater gains in conflicts, where ToM may be more necessary for the target agent to achieve goal that goes against their partner. Furthermore, the lower boundary (Q1-1.5IQR) is about -5 while the upper boundary (Q3+1.5IQR) is nearly 10 (i.e., TOMA obtains an average score of 10 while the base model scores 0), showing that our method can largely outperform base, but not the other way around. Figure 3: Performance gains of TOMA over Base w.r.t. scenarios. 4.4 WHAT STRATEGIES DOES TOMA EMPLOY? Figure 4: Top 7 goal success and failure factors for the Base model and, using the 3B model. To understand the different strategies that agents with varying levels of ToM capabilities employ in order to achieve their goals, we analyze the factors contributing to successful conversations (goal completion score 7) and the barriers leading to failed conversations (goal completion score < 4) across different model variants. 7 Infusing Theory of Mind into Socially Intelligent LLM Agents Categorizing success and failure reasons. To identify successful strategies, we provide Gemini with the full conversation, as well as the target agents name and social goal, and prompt it to explain the reasons for success. Using the reasons from all the successful conversations, we prompt the LLM to categorize the reasons and provide concise definition for each reason. To reduce redundancy, we further instruct the LLM to cluster and merge similar reasons into 25 representative ones, each manually verified by the authors for validity. Finally, we prompt the LLM to classify the reasons provided for each conversation into these canonical categories. We repeat the same process to obtain the failure reasons from the failed conversations. Figure 4 presents the top factors most frequently associated with success and failure outcomes of the 3B models, with the respective prefixes or . Each label is further broken down by scenario types (Details in 4.3). See Appendix for complete definitions of the labels and scenario categories. TOMA enables more strategic reasoning across diverse scenarios. In successful conversations, the base model relies heavily on interpersonal strategies, such as rapport building and relationship building, and direct goal-pursuit approaches, such as persistent request and direct request. In contrast, TOMA adopts long-horizon goal-oriented strategic behavior by employing compromise, accommodation, and solution offering, while still maintaining comparable levels of rapport building and cooperative response to the base model. In terms of conversation types, both models achieve success mainly in cooperative conversations, where its easy for both agents to achieve high goal completion score. Compared to the base model, TOMA also has high levels of success in competitive settings (negotiation, persuasion, and conflict), especially when using the strategies of compromise, accommodation, and solution offering. The results of the 7B model (in Appendix B.3) similarly show that TOMA applies strategic behaviors which lead to success across different scenario types, and this strategic behavior seems to increase with model size. TOMA exhibit more active behaviors in failure modes. The base model often fails due to being too passive (failed to initiate; lack of action; lack of information provision). Conversely, TOMA employs active strategies that sometimes fail (e.g., failure to persuade) as well as goal-oriented approaches that fail to account for the role of relationship building in goal achievement (ignored preferences; prioritizing self). In the 7B version of TOMA, these failures are significantly reduced while the lack of action frequency is increased. We hypothesize this is the result of increased sensitivity to the partners emotional state compared to the 3B model (as shown in the relationship score in Tables 1 and 2), which reduces the selfish ignored preferences and prioritizing self occurrences (see Appendix B.3). Size Model 0th-order (%) 1st-order (%) 3B 7B Base+MS TOMA Base+MS TOMA 28.1 21. 22.3 17.6 71.9 78.2 77.7 82.4 Figure 5: Distribution of mental state dimensions on the 3B model. See Appendix B.3 for 7B. Table 4: Zerovs. first-order reasoning percentage on Base+MS and TOMA. TOMA prioritizes intentions over emotions in mental state generation. To investigate the effect of TOMA across different mental states, we categorize the generated ToM hypotheses into five dimensions and then compare the mental states distributions given by Base+MS and TOMA. Figure 5 shows that TOMA generates more hypotheses about intentions and relies less on emotions, while maintaining similar levels for beliefs, desires, and knowledge. This is in line with the finding that the base model is focused on rapport-building strategies, which require hypothesizing about the other agents emotions as opposed to TOMAs strategic and goal-oriented behavior that requires reasoning about the other agents intentions. We observe similar trends in the 7B model. TOMA generates more 1st-order mental states than the baseline. Table 4 shows the distribution of 0th-order (target agents own beliefs) and 1st-order (target agents beliefs about others) mental states generated by Base+MS and TOMA. Although both models are prompted to produce 8 Infusing Theory of Mind into Socially Intelligent LLM Agents these states in equal proportions, TOMA consistently generates more 1st-order beliefs by an average of +6.3% and +5.0% on the 3B model and 7B model, respectively, compared to Base+MS. This suggests that TOMA is better at inferring others mental states, potentially contributing to more strategic and socially aware behaviors during interaction."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Theory of Mind in LLMs. With the advent of LLMs, research on ToM in AI is experiencing strong momentum. Studying the extent that LLMs have ToM abilities can inform research on building AI agents with human-like communication and empathy skills, as well as protecting against AI manipulation and deception. Current findings are conflicting: LLMs achieve good performance on various ToM benchmarks and tests designed for humans, which some researchers interpret as having developed theory of mind (Kosinski, 2023; 2024; Strachan et al., 2024); Yet others show that this ability is inconsistent and superficial (Ullman, 2023; Shapira et al., 2024; Amirizaniani et al., 2024; Nickel et al., 2024; Soubki & Rambow, 2025). To improve LLMs ToM capabilities, one approach is to prompt models in chain-of-thought setup to explicitly reason about beliefs and mental states before making prediction (Wilf et al., 2024; Shinoda et al., 2025). An alternative neuro-symbolic approach combines LLMs with symbolic belief tracking (Sclar et al., 2023) or Bayesian Inverse Planning (Ying et al., 2023). While less brittle than pure LLM-based approaches, these methods are typically limited in scope and only applied to specific setups. Another promising (but computationally expensive) approach generates and explores multiple hypotheses about the agents mental states during inference (Kim et al., 2025). In contrast, we propose training approach that saves inferencetime costs. Crucially, most existing work evaluates LLMs on static and artificial ToM benchmarks, requiring models to answer questions as an observer rather than participant in dynamic environment (Wagner et al., 2025; Xiao et al., 2025; Lupu et al., 2025). Instead, we evaluate our method on Sotopia, measuring the contribution of modeling ToM for social conversations between AI agents. Look-Ahead Simulation in Self-Training Agents. In this work we leverage look-ahead, planning technique where an agent simulates the potential outcomes several steps into the future to make more informed decisions in the present. In text generation, look-ahead search was employed for decoding, prioritizing tokens that lead to better overall generated text (Lu et al., 2022; Fu et al., 2024) or faster inference (Leviathan et al., 2023; Chen et al., 2023). More recently, look-ahead signals were used in GRPO (Guo et al., 2025), an RL algorithm used in LLM preference tuning. GRPO obviates the need for human-labeled data by generating multiple outputs, simulating their outcomes with an LLM-as-a-judge (Gu et al., 2024), and rewarding outputs that yield better outcomes. In general, many simulation-based methods focus on outcome alignment using RL (Xi et al., 2024; Pang et al., 2024). Conversely, we use simulation to generate training examples, similarly to Hoang et al. (2025). In the context of social dialogues, prior work targeting Sotopia employed similar approach of generating conversations, simulating their outcome with an LLM judge (e.g., in terms of goal achievement), and using this signal to select positive training examples or as reward in RL (Wang et al., 2024; Kong et al., 2025; Yu et al., 2025). Instead of directly optimizing utterances that lead to goal achievement or other desirable outcomes which could lead to reward hacking we explicitly train our model to use ToM in social dialogues; we improve both the models ability to reason about mental states, as well as the capacity to consider this information when generating utterances."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced TOMA, training framework that integrates ToM-driven mental state and utterance prediction with conversation simulation to select interaction trajectories that best support goal achievement. Experiments on the Sotopia interactive evaluation benchmark demonstrate the effectiveness of our approach across range of baselines, achieving competitive performance with GPT5-nano. Comprehensive analysis demonstrates that TOMA, infused with ToM ability, can better infer others mental states, leading to more strategic and goal-oriented behavior, as well as supporting long-horizon adaptation and improving relationship management. In conclusion, TOMA represents significant step toward building socially intelligent LLM agents through explicit modeling of social reasoning and internal agent mechanisms. 9 Infusing Theory of Mind into Socially Intelligent LLM Agents ACKNOWLEDGMENTS This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chairs program, and an NSERC discovery grant. We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC). Nous remercions le Conseil de recherches en sciences naturelles et en genie du Canada (CRSNG) de son soutien."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.org/abs/2303. 08774. Maryam Amirizaniani, Elias Martin, Maryna Sivachenko, Afra Mashhadi, and Chirag Shah. Can llms reason like humans? assessing theory of mind reasoning in llms for open-ended questions. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pp. 3444, 2024. URL https://dl.acm.org/doi/abs/10.1145/3627673. 3679832. Ian Apperly and Stephen Butterfill. Do humans have two systems to track beliefs and belief-like states? Psychological review, 116(4):953, 2009. URL https://psycnet.apa.org/buy/ 2009-18254-013. Simon Baron-Cohen, Alan Leslie, and Uta Frith. Does the autistic child have theory of mind? Cognition, 21(1):3746, 1985. URL https://www.sciencedirect.com/science/ article/abs/pii/0010027785900228. Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www. wandb.com/. Software available from wandb.com. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. URL https://arxiv.org/abs/2302.01318. Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, and Minlie Huang. ToMBench: Benchmarking theory of mind in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1595915983, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 847. URL https://aclanthology.org/2024.acl-long.847/. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. URL https://arxiv.org/abs/2507. 06261. Michael Han Daniel Han and Unsloth team. Unsloth, 2023. URL http://github.com/ unslothai/unsloth. Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of LLM inference using lookahead decoding. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1406014079. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/fu24a.html. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. URL https://arxiv.org/abs/2411.15594. 10 Infusing Theory of Mind into Socially Intelligent LLM Agents Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv. org/abs/2501.12948. Thai Quoc Hoang, Kung-Hsiang Huang, Shirley Kokane, Jianguo Zhang, Zuxin Liu, Ming Zhu, Jake Grigsby, Tian Lan, Michael Ryoo, Chien-Sheng Wu, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong, and Juan Carlos Niebles. LAM SIMULATOR: Advancing data generation for large action model training via online exploration and trajectory feedback. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 1292112934, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-2565. doi: 10.18653/v1/2025.findings-acl.670. URL https://aclanthology.org/2025. findings-acl.670/. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=rygGQyrFvH. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. FANToM: benchmark for stress-testing machine theory of mind in interactions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1439714413, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.890. URL https://aclanthology.org/2023. emnlp-main.890/. Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, and Yejin Choi. Hypothesis-driven theory-of-mind reasoning for large language models. In Second Conference on Language Modeling, 2025. URL https://openreview. net/forum?id=yGQqTuSJPK. Aobo Kong, Wentao Ma, Shiwan Zhao, Yongbin Li, Yuchuan Wu, Ke Wang, Xiaoqian Liu, Qicheng Li, Yong Qin, and Fei Huang. SDPO: Segment-level direct preference optimization for social agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1240912423, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.607. URL https://aclanthology.org/2025.acl-long.607/. Michal Kosinski. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083, 4:169, 2023. URL https://arxiv.org/abs/2302. 02083v2. Michal Kosinski. Evaluating large language models in theory of mind tasks. Proceedings of the National Academy of Sciences, 121(45):e2405460121, 2024. URL https://www.pnas.org/ doi/10.1073/pnas.2405460121. LLM Leaderboard. Llm leaderboard. https://llm-stats.com/, 2025. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1927419286. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/leviathan23a.html. Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. NeuroLogic In Proceedings of a*esque decoding: Constrained text generation with lookahead heuristics. 11 Infusing Theory of Mind into Socially Intelligent LLM Agents the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 780799, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.57. URL https: //aclanthology.org/2022.naacl-main.57/. Andrei Lupu, Timon Willi, and Jakob Foerster. The decrypto benchmark for multi-agent reasoning and theory of mind. arXiv preprint arXiv:2506.20664, 2025. URL https://arxiv.org/ abs/2506.20664. Christian Nickel, Laura Schrewe, and Lucie Flek. Probing the robustness of theory of mind in large language models. arXiv preprint arXiv:2410.06271, 2024. URL https://arxiv.org/abs/ 2410.06271. OpenAI. Gpt-5 system card. OpenAI Blog, 2025. URL https://cdn.openai.com/ gpt-5-system-card.pdf. Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, and Siheng Chen. Self-alignment of large language models via monopolylogue-based social scene simIn Forty-first International Conference on Machine Learning, 2024. URL https: ulation. //openreview.net/forum?id=l7shXGuGBT. Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wang, Krishnateja Killamsetty, Shivchander Sudalairaj, Wenlong Zhao, Seungwook Han, Abhishek Bhandwaldar, Guangxuan Xu, Kai Xu, Ligong Han, Luke Inglis, and Akash Srivastava. Unveiling the secret recipe: guide for supervised fine-tuning small LLMs. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=eENHKMTOfW. David Premack and Guy Woodruff. Does the chimpanzee have theory of mind? Behavioral and Brain Sciences, 1(4):515526, 1978. doi: 10.1017/S0140525X00076512. URL https: //www.cambridge.org/core/journals/behavioral-and-brain-sciences/ article/does-the-chimpanzee-have-a-theory-of-mind/ 1E96B02CD9850016B7C93BC6D2FEF1D0. Qwen. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. URL https://arxiv. org/abs/2407.10671. Qwen. Qwen2.5: party of foundation models, September 2024b. URL https://qwenlm. github.io/blog/qwen2.5/. Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. Minding language models (lack of) theory of mind: plug-and-play multi-character belief tracker. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1396013980, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.780. URL https://aclanthology.org/ 2023.acl-long.780/. Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. Clever hans or neural theory of mind? stress testing social In Proceedings of the 18th Conference of the European reasoning in large language models. Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2257 2273, St. Julians, Malta, March 2024. Association for Computational Linguistics. doi: 10.18653/ v1/2024.eacl-long.138. URL https://aclanthology.org/2024.eacl-long.138/. Kazutoshi Shinoda, Nobukatsu Hojo, Kyosuke Nishida, Yoshihiro Yamazaki, Keita Suzuki, Hiroaki Sugiyama, and Kuniko Saito. Lets put ourselves in sallys shoes: Shoes-of-others prefixing improves theory of mind in large language models. arXiv preprint arXiv:2506.05970, 2025. URL https://arxiv.org/abs/2506.05970. Adil Soubki and Owen Rambow. Machine theory of mind needs machine validation. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 1849518505, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/ v1/2025.findings-acl.951. URL https://aclanthology.org/2025.findings-acl. 951/. 12 Infusing Theory of Mind into Socially Intelligent LLM Agents James WA Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, et al. Testing theory of mind in large language models and humans. Nature Human Behaviour, 8(7):12851295, 2024. URL https://www.nature.com/articles/s41562-024-01882-z. Tomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399, 2023. URL https://arxiv.org/abs/2302.08399. Eitan Wagner, Nitay Alon, Joseph Barnby, and Omri Abend. Mind your theory: Theory In Findings of the Association for Computational Linof mind goes deeper than reasoning. guistics: ACL 2025, pp. 2665826668, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.1368. URL https://aclanthology.org/2025.findings-acl.1368/. Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Yonatan Bisk, Graham Neubig, and Hao Zhu. SOTOPIA-π: Interactive learning of socially intelligent language agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1291212940, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.698. URL https: //aclanthology.org/2024.acl-long.698/. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sreemanti Dey, Shubh-Agrawal, Sandeep Singh Sandha, Siddartha Venkat Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: challenging, contamination-limited LLM benchmark. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=sKYHBTAxVa. Alex Wilf, Sihyun Lee, Paul Pu Liang, and Louis-Philippe Morency. Think twice: PerspectiveIn Proceedings of the taking improves large language models theory-of-mind capabilities. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 82928308, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.451. URL https://aclanthology.org/2024. acl-long.451/. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural In Proceedings of the 2020 Conference on Empirical Methods in Natulanguage processing. ral Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https: //aclanthology.org/2020.emnlp-demos.6/. Jiajun Xi, Yinong He, Jianing Yang, Yinpei Dai, and Joyce Chai. Teaching embodied reinforcement learning agents: Informativeness and diversity of language use. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 40974114, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.237. URL https://aclanthology.org/2024.emnlp-main.237/. Yang Xiao, Jiashuo Wang, Qiancheng Xu, Changhe Song, Chunpu Xu, Yi Cheng, Wenjie Li, and Pengfei Liu. Towards dynamic theory of mind: Evaluating LLM adaptation to temporal evolution of human states. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2403624057, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1171. URL https://aclanthology.org/2025.acl-long.1171/. Lance Ying, Katherine Collins, Megan Wei, Cedegao Zhang, Tan Zhi-Xuan, Adrian Weller, Joshua Tenenbaum, and Lionel Wong. The neuro-symbolic inverse planning engine (nipe): Modeling probabilistic social inferences from linguistic inputs. arXiv preprint arXiv:2306.14325, 2023. URL https://arxiv.org/abs/2306.14325. 13 Infusing Theory of Mind into Socially Intelligent LLM Agents Haofei Yu, Zhengyang Qi, Yining Zhao, Kolby Nottingham, Keyang Xuan, Bodhisattwa Prasad Majumder, Hao Zhu, Paul Pu Liang, and Jiaxuan You. Sotopia-rl: Reward design for social intelligence. arXiv preprint arXiv:2508.03905, 2025. URL https://arxiv.org/abs/2508. 03905. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. SOTOPIA: Interactive evaluation for social intelligence in language agents. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= mM7VurbA4r. 14 Infusing Theory of Mind into Socially Intelligent LLM Agents"
        },
        {
            "title": "A EXPERIMENT DETAILS",
            "content": "A.1 MODEL SETTINGS For open-weight models such as Qwen (Qwen, 2024b), we load the model checkpoint and tokenizer provided by Hugging Face Transformers (Wolf et al., 2020). We load all models in the brain floating-point format (bfloat16). The maximum context length is set to 4096, random seed to 42, generation temperature to 0.7, and we use top-p sampling (Holtzman et al., 2020) with = 0.9. For proprietary LLMs (GPT-5 (OpenAI, 2025) and Gemini (Comanici et al., 2025)), we call the respective API using default generation temperature of 1.0. Table 5 provides the model sources. Type Role Model Link Open-weight LLM Speaker (fine-tuning) Open-weight LLM Speaker (fine-tuning) Open-weight LLM Proprietary LLM Proprietary LLM Proprietary LLM Partner (frozen) Partner (frozen) Evaluator (frozen) Evaluator (frozen) Qwen2.5-3B Model Link Qwen2.5-7B Model Link Qwen2.5-14B Model Link API Link GPT-5-nano API Link GPT-5-mini API Link Gemini-Flash Table 5: The sources of models used in this work. A.2 TRAINING DETAILS We adopt LoRA (Hu et al., 2022) for fine-tuning and apply grid search, provided by wandb (Biewald, 2020), on the learning rate and LoRA configurations (rank and alpha), and select the best model checkpoint based on the performance on the validation set. During validation, the model is evaluated on 20 randomly sampled testing instances and is asked to generate 10 turns of conversation per instance. In addition, we employ an early stopping strategy to end the training session when the best validation score does not change for 3 consecutive updates. The key training hyper-parameters are presented in Table 6. Hyper-parameters Values # epochs batch size gradient accumulation steps learning rate lr scheduler weight decay warmup steps max seq len LoRA rank LoRA alpha LoRA dropout 3 2 4 1e-4; 5e-05 cosine 0 10 4,096 8; 16; 32; 64 32; 64; 128 0 Table 6: The training hyper-parameters. A.3 EXPERIMENTAL COSTS For constructing the training data containing mental states and utterances, the API calls of Gemini (gemini-2.0-flash-lite-001) cost less than 5 USD. For the comprehensive evaluation in our experiments, the cost of GPT-5 (gpt-5-mini) was roughly 100 USD. Each experiment session involving open-weight LLMs was conducted on single NVIDIA L40S GPU, and we employ unsloth (Daniel Han & team, 2023) for fast training, reducing each training session to about 4 hours. 15 Infusing Theory of Mind into Socially Intelligent LLM Agents"
        },
        {
            "title": "B ANALYSIS DETAILS",
            "content": "B.1 HOW DOES TOMA PERFORM ACROSS DIFFERENT EVALUATION DIMENSIONS? To investigate the performance gains of TOMA over Base in different evaluation dimensions (i.e., Goal, Relationship, and Knowledge), we visualize the paired scores in Figure 6 and Figure 7, where each point (x, y) means the Base performance is and TOMA performance is for one instance. The 45-degree dot line (neutral line) stands for draw, and darker color of the points represents higher frequency. We observe that more points are distributed above the neutral line, meaning TOMA outperforms Base for more instances, especially for Goal and Rel dimensions. In addition, considering the four quadrants of the Goal dimension in Figure 6(a) and Figure 7(a), many points lie in the upper-left region, meaning TOMA is much better than Base, while hardly any points lie in the lower-right corner. For the Relationship dimension in Figure 6(b) and Figure 7(b), most points of TOMA and Base are above the y=0 line, meaning the relationship between two agents is preserved and even enhanced through after the conversation. Figure 6(c) and Figure 7(c) show that both methods help agents gain new or important information through interaction, and TOMA often brings more knowledge gains. Figure 6: Comparisons between TOMA and Base over different dimensions using Qwen2.5-3B. Figure 7: Comparisons between TOMA and Base over different dimensions using Qwen2.5-7B. In addition, we present the correlation coefficients between the results of different dimensions in Table 7, which shows that the three dimensions are positively correlated with each other. We observe that the Goal-Rel pair shows the strongest correlation, indicating that the improved goal completion performance is related to the preservation and enhancement of the agents relationship throughout the conversation, which supports the importance of enabling Theory of Mind. GoalRel GoalKnow RelKnow Pearson Spearman Pearson Spearman Pearson Spearman 3B 7B 0.224 (2e-6) 0.284 (2e-9) 0.376 (5e-16) 0.370 (2e-15) 0.228 (1e-6) 0.120 (0.013) 0.213 (7e-6) 0.136 (5e-3) 0.288 (9e-10) 0.107 (0.026) 0.222 (3e-6) 0.062 (0.195) Table 7: The Pearson and Spearman correlation coefficients (with p-values) between dimensions. Infusing Theory of Mind into Socially Intelligent LLM Agents B.2 HOW DOES TOMA PERFORM ACROSS DIFFERENT CONVERSATION TYPES? Figure 8 provides the performance (Goal) gains of TOMA over Base with respect to different scenario types using the Qwen2.5-7B model, and the analysis of the 3B model (Figure 3) is described in 4.3. Table 8, Table 9, Table 10, and Table 11 showcase five instances per scenario type: cooperation, negotiation, persuasion, and conflict. Figure 8: The Goal gains of TOMA over Base regarding different scenario types using Qwen2.5-7B. Scenario Description Speakers Goal Partners Goal Conversation between two individuals in romantic relationship Ask the other to go to dinner with you (Extra information: you want to have romantic dinner at fancy restaurant) Two friends are meeting at coffee shop, where one of them is having trouble keeping up with their bills. Help your friend with their financial trouble (Extra information: you are aware that your friend is struggling with their bills and you want to help.) Two friends are having conversation in coffee shop Seek emotional support (Extra information: you have been feeling down lately and you need someone to talk to) conversation between two friends at park Break bad luck (Extra information: you found out that you have been having bad luck because you have been unknowingly carrying cursed artifact.) Maintain balanced diet while enjoying the dinner (Extra information: you are on diet and you have daily limit on calorie intake.) Maintain your pride if your friend offers you money(Extra information: you are the friend who is struggling financially, but you dont want to seem like you are taking advantage of the situation. Hint: you can express gratitude and discuss how you plan to pay it back in the future.) Provide emotional support but also avoid enabling victim mentality (Extra information: you believe that your friend needs to face their problems instead of dwelling on them.) Help friend prepare the ritual to break the bad luck, although you are an atheist and not superstitious at all. (Extra information: you know that your friend believes in the ritual deeply, and the placebo effect might give them more confidence.) Two strangers are meeting at party. You are trying to figure out whether you have mutual friend with the other person. You should not simply list their names. You are trying to figure out whether you have mutual friend with the other person. You should not simply list their names. Table 8: Five instances of the cooperation scenario type. Infusing Theory of Mind into Socially Intelligent LLM Agents Scenario Description Speakers Goal Partners Goal Two friends are camping in the wilderness and the temperature drops significantly at night. One person is offering BMW Z3 for $5000.0, while another individual is interested in purchasing it. Heres brief description of the car: The BMW Z3 is in need of good home. It operates smoothly without any problems and is known for its fuel efficiency and sporty appearance. The mileage is relatively low for its age. Its an opportunity not to be missed, so come and get it before its gone. smog certificate is readily available. Two roommates deciding on how to split up items after garage sale. The items are 3 books, 2 hats, and 1 ball. Each item has different sentimental value for each person, which translates into points. Share the only blanket available (Extra information: you are extremely cold and need the blanket to keep warm Hint: you can stress the benefit of sharing.) Keep the blanket for yourself as you are also very cold (Extra information: you have lower tolerance for cold, and you feel that you need the blanket more) You are the buyer for this item and your target price is $4600. You should be aware that if you purchase it at price significantly higher than your target, you will incur penalty. However, if you manage to secure it for less than the target price, you will receive bonus. You are the seller of this item, with target price set at $3260. Please note, you may face penalty if this item is sold for price significantly lower than the target. However, you stand chance to earn bonus if you successfully sell it for price higher than the target. Maximize the points you have (Extra information: you value the books at 3 points each, the hats at 2 points each, and the ball at 1 point) Maximize the points you have (Extra information: you value the books at 2 points each, the hats at 3 points each, and the ball at 1 point) between Conversation two friends, one who has written play based on their parents life, and the other owns production company. Sell the rights of the play to your friend (Extra information: you believe that the play is worth lot and you would like fair price for it) conversation between two individuals, one is the homeowner and the other is their cousins partner, who has been staying at the house for while. Tell the cousins partner that they are no longer welcome in your home (Extra information: they have been causing lot of trouble and not respecting house rules Hint: you may want to discuss the issues and propose possible solutions) Acquire the rights of the play while maintaining budget (Extra information: you only have limited budget for new plays this season. Hint: you can not spend all your budget on this play since you still need to maintain your financial stability.) Attempt to remain in the home while maintaining good relationship with the cousin and the homeowner (Extra information: you have nowhere else to go at the moment and you believe the issues can be resolved.) Table 9: Five instances of the negotiation scenario type. 18 Infusing Theory of Mind into Socially Intelligent LLM Agents Scenario Description Speakers Goal Partners Goal Two friends meet at bar. Two roommates living together and sharing household chores. One of them, who is responsible for cooking, finds out that the other one refuses to eat anything they cook Two friends discussing their schedules at coffee shop between two Conversation business reviewing partners their companys financial status (Gourmet Delights Inc. is globally recognized for producing and distributing diverse line of exceptional quality food products.) conversation between two individuals at charity gala Convince the friend to drink less (Extra information: your friend has been drinking lot recently and youre worried about their health) Convince the roommate to try the food (Extra information: you have spent lot of time and effort on cooking) Convince the friend to take more classes (Extra information: you think your friend is not challenging themselves enough) Convince the partner to cut costs (Extra information: you have indicating the found report companys profits are decreasing. And you think the company should start by switching suppliers of ingredients. Hint: you can propose ways to cut costs that wont affect the status of the company.) Convince the other individual to donate to cause supporting young children (Extra information: this cause can greatly improve the lives of many young children and you think it is worthy cause. The minimum donation for this charity organization is $3000) You want to keep drinking but dont want to upset your friend (Extra information: youve been drinking to cope with stress recently, but dont want to worry your friend) Express your concerns about the food without hurting the roommates feelings (Extra information: you are worried about the taste and nutrition of the food) Maintain manageable schedule while preserving the friendship (Extra information: you are already overwhelmed with your current workload Hint: you can express your concerns about the workload and suggest other ways of challenging oneself.) Maintain the quality of the companys products and services while agreeing to cost cut (Extra information: you are concerned about the impact of cost-cutting on the quality of your product.) Maintain financial stability while supporting causes (Extra information: you have certain budget for donations this year and have already made several donations. You are only thinking of donating $2000 max) Table 10: Five instances of the persuasion scenario type. 19 Infusing Theory of Mind into Socially Intelligent LLM Agents Scenario Description Speakers Goal Partners Goal Conversation friends at trivia night between two Get the answer for the trivia question from the friend (Extra information: your friend knows the answer to the trivia question but hasnt shared it with you yet) Avoid revealing the answer to the trivia question (Extra information: You think that your friend should try to figure it out on their own to improve their trivia skills) Two roommates are having disagreement over noise levels in their shared apartment. roommate to Convince your lower the noise (Extra information: you have an important online tomorrow morning and you need quiet environment to prepare) meeting Two friends are playing video game together. Conversation between two individuals who share common dislike for third person Win the game (Extra information: you just need five more minutes to accomplish your goal in the game) Injure the third person (Extra information: You have deepseated hatred for the third person and you are willing to take the risk to hurt them) Two criminals are arrested and placed in separate cells. The police have enough evidence to convict both criminals of minor crime but need their confessions to establish evidence for more serious crime. Now they get chance to briefly chat with each during lunchtime. Minimize the jail year (Extra information: your lawyer told you that 1. if you report the other person, you would be free, the other person would go to jail for 10 years, and vice versa. 2. if both of you remain silent, you will only receive fine. 3. If both of you reported each other, you both would go to jail for 10 years. Hint: You may want to share your information to reach mutual understanding) Defend your right to play music in your own room, but also keep the peace in the apartment (Extra information: you find playing music relaxing and its your way to unwind after long day. Hint: you may want to suggest compromise, like using headphones or playing music only during certain hours.) Stop the game because its too late (Extra information: you feel tired and need to go to bed early but dont want to upset your friend) Prevent physical harm but also express your dislike (Extra information: You also dislike the third person, but you are not willing to resort to violence, you prefer to express your dislike in nonviolent way. Hint: you can try to convince the other person about the potential consequences of engaging in violence) Minimize the jail year (Extra information: your lawyer told you that 1. if you report the other person, you would be free, the other person would go to jail for 10 years, and vice versa. 2. if both of you remain silent, you will only receive fine. 3. If both of you reported each other, you both would go to jail for 10 years. Hint: You may want to share your information to reach mutual understanding) Table 11: Five instances of the conflict scenario type. 20 Infusing Theory of Mind into Socially Intelligent LLM Agents B.3 WHAT STRATEGIES DOES TOMA EMPLOY? Figure 9 presents the Top-7 goal success and failure reasoning labels on Base and TOMA on the Qwen2.5-7B model, and the reasoning of the 3B model (Figure 4) is described in 4.4. Table 12 and Table 13 provide the canonical labels for success and failure reasons, respectively. Figure 10 presents the distribution of mental state dimensions for 7B model. Figure 9: Top 7 goal success and failure reasoning labels on Base and TOMA on 7B model. Figure 10: Distribution of mental state dimensions on 7B model. Success Labels Definition Establishing connection, empathy, and openness. Collecting details to understand needs, preferences, and context. Starting the process of discussion and bargaining. Discussing and adjusting the price or value. Demonstrating willingness to compromise on terms. Establishing clear objectives and intentions. Offering solutions and support to address requests. Proposing concrete steps to move forward. Making clear and detailed proposal or offer. Making clear, straightforward demand or question. Consistently pursuing goal or request. Avoiding commitment, connection, or engagement. Explaining the steps or methods involved. Organizing and scheduling actions to move forward. Convincing others through offers or logic. Conveying the worth or benefits. rapport building information gathering negotiation initiation price negotiation flexible negotiation goal setting cooperative response actionable suggestion offer establishment direct request persistent request avoidance behavior process clarification coordination persuasion value communication resource management Managing finances, items, time, or space. relationship building risk management compromise initiative budget influence solution offering direct statement accommodation Developing connections and fostering trust. Addressing and mitigating potential concerns. Finding mutually agreeable solution. Taking proactive steps or offering suggestions. Considering and working within financial constraints. Providing or suggesting concrete methods to resolve issues. Making clear and unambiguous pronouncements. Meeting the needs or preferences of the other party. Table 12: Canonical labels for success reasons. 21 Infusing Theory of Mind into Socially Intelligent LLM Agents Failure Labels Definition emotional reactivity information gathering failure weak argumentation prioritizing self price negotiation failure lack of information provision lack of empathy and consideration inadequate proposal failed to initiate failed to persuade missed opportunities lack of shared understanding communication ineffectiveness lack of rapport building unresponsiveness poor introduction inconsistent behavior unclear strategy ignored preferences avoidance of subject lack of action constraint violation failed to offer solutions unrealistic expectations repetitive communication Displays of anger, hostility, or defensiveness that disrupt cooperation. Insufficient attempts to collect or exchange necessary information. Inability to provide strong reasoning, counterarguments, or supporting evidence. Focus on personal needs/comfort over the shared goal or others needs. Inability to reach desired price or bargain effectively. Failure to provide crucial details needed for decision. Failing to understand or acknowledge the other partys feelings/perspective. Presenting proposal that is vague or lacks essential details. Failing to start the conversation or propose actions. Failure to convince or motivate the other party. Failing to capitalize on advantageous chances or options. Failure to establish or confirm mutual agreement on key points. Using ineffective or misunderstood communication styles. Failing to establish positive relationship or connection. The other party did not respond or engage. Focusing on self-interests or an impersonal approach in the introduction. Actions or statements that contradict each other, creating distrust. Absence of defined plan or approach to achieve the desired outcome. Failing to address the other partys expressed preferences. Intentionally evading topic or issue. Failure to take necessary steps or follow-up after rejection/issue. Breaking established rules, boundaries, or constraints. Inability to provide concrete actions or support. Setting goals that are not achievable or aligned with the context. Getting stuck in loop of unproductive exchanges. Table 13: Canonical labels for failure reasons. 22 Infusing Theory of Mind into Socially Intelligent LLM Agents"
        },
        {
            "title": "C LLM PROMPTS",
            "content": "Figure 11, 12, and 13 present the prompts used in 4.4, analyzing the factors behind agents successes and failures in achieving their goals. Figure 14 shows the prompt used to calculate goal scores of simulated dialogues during the training data construction stage (2). Figure 15 and 16 present the prompts used to generate mental state hypotheses and utterances, respectively. Figure 17 provides an example training instance used to finetune our model. In this instance, scenario, an agents social goal and its mental state, and the conversation history are provided as input, and the model is trained to produce an utterance. For the mental state generation task, we use the same inputs except that the mental state is excluded, and the model is trained to generate mental state hypotheses. Prompt for Generating Reasons for Success Task: You will be given scenario, the social goal of the target agent, and conversation between agents. Your goal is to identify the main reasons the target agent **succeeded** (including partial success) in achieving their goals. Focus only on success factors. Rules: - Return **13** distinct, non-overlapping reasons. If no success reasons exist, return None. - Be concise using less than 30 words per reason. - No speculation, suggestions, failure reasons, or chain-of-thought. Inputs: Scenario: {{scenario}} Target Agent: {{agent name}} Target Agents social goal: {{social goal}} Conversation: {{conversation}} Proceed to identify the main success reasons in natural language. Figure 11: prompt used to generate reasoning for success. Prompt for Generating Reasons for Failure Task: You will be given scenario, social goal of the target agent, conversation between agents. Your goal is to identify the main reasons the target agent **failed** (including partial failure) in achieving their goals. Focus only on failure factors. Rules: - Return **13** distinct, non-overlapping reasons. If no success reasons exist, return None. - Be concise using less than 30 words per reason. - No speculation, suggestions, failure reasons, or chain-of-thought. Inputs: Scenario: {{scenario}} Target Agent: {{agent name}} Target Agents social goal: {{social goal}} Conversation: {{conversation}} Proceed to identify the main success reasons in natural language. Figure 12: prompt used to generate reasoning for failure. 23 Infusing Theory of Mind into Socially Intelligent LLM Agents Prompt for Generating Topic Labels for Success and Failure Reasons Task: You are analyzing an explanation of why the agent succeeded in achieving the goal or why the agent failed to achieve the goal. Your job is to extract the main reasons that explain the outcome. Return 13 reasons. Each reason MUST be about {{type}} reasons. Use canonical labels if they fit; otherwise you may create new labels. Here are the identified categories for {{category name}} (use these if they fit): {{category name}} CATEGORIES: {{category list}} Rules: - Try your best to use **canonical labels** if any fit. Always use the **exact code strings** from the list above. - Do not create overly generic, overly specific, or duplicate labels. - New labels must be concise (fewer than 5 words). - Only if **none** of the canonical labels fit the text, create NEW * label. When creating NEW * labels: - For {{category name}}: {{prefix}} - Name new codes in snake case (e.g., {{example}}). - If the code starts with NEW *, you MUST include short definition ( 20 words). - Otherwise, leave the definition empty. Here is the identified reason: {{text}} Example output JSON schema (and nothing else): { reasons: <=20 words only when code starts with NEW ; otherwise empty } ] } [{ code: {{prefix}} canonical code OR {{new prefix}}, definition: Proceed to generate your label in JSON format. Figure 13: prompt used to generate topic labels for success and failure reasons. 24 Infusing Theory of Mind into Socially Intelligent LLM Agents Prompt for Measuring Goal Score of Conversation during Training Based on the scenario, the agents social goal, and conversation history, you are evaluating how well the agent achieves their goals. Here is the scenario: {{scenario}} Agent: {{agent}} {{agent}}s goal: {{social goal}} Here is the conversation history: {{history}} Please first reiterate the agents social goals. Then provide comprehensive analysis about the extent to which the agent has managed to achieve these goals. In the reasoning field, provide comprehensive account of the logic or thought process that led you to your conclusion. Further, provide an integer score ranging from 0 and 10 in the score field. 0 represents minimal goal achievement, 10 represents complete goal achievement, and higher score indicates that the agent is making progress towards their social goals. Please follow the format: The output should be formatted as valid JSON instance that conforms to the following JSON schema: { reasoning: Explanation about how the agents actions align, or do not, with their social goals., score: Integer from 0 to 10, indicating how fully the social goal was achieved. } Proceed to generate the output. Figure 14: prompt used to measure the goal score of the conversation during training. 25 Infusing Theory of Mind into Socially Intelligent LLM Agents Prompt for Generating Mental States Role: You are {{person}}. You recently had conversation with {{another person}}. Your social goal is: {{social goal}}. Task: Prepare the ground for your very next utterance by articulating compact mental states that can guide what you say next. Stay grounded in the scenario and conversation; avoid guessing beyond the evidence. Here are example mental state dimensions: - Beliefs: facts the speaker accepts as true or false about the world or events. - Desires: outcomes or states the speaker wants to bring about. - Intentions: specific actions or plans the speaker aims to carry out. - Emotions: feelings or affective states the speaker is experiencing. - Knowledge gaps: information the speaker does not have but may want to obtain. - Others: other mental states that may useful to understand other person and shape the next utterance. Here are the scenario and recent conversation: Scenario: {{scenario}} Recent conversation: {{history}} Write one short paragraph (5-6 sentences) in natural prose. Mix your own states with firstorder inferences about {{another person}} in roughly equal proportion. Use natural cues for partner inferences (e.g., think {{another person}} believes.. It seems {{another person}} intends.., hear {{another person}} feels..). Cover at least three dimensions across both sides. Avoid lists; Stop after the paragraph. Figure 15: prompt used to generate mental states. 26 Infusing Theory of Mind into Socially Intelligent LLM Agents Prompt for Generating Mental States Imagine you are {{speaker}}, your task is to act/speak exactly as {{speaker}} would, keeping in mind {{speaker}}s social goal. You can find {{speaker}}s goal and private notes in the Here is the context of the interaction field. Note that {{speaker}}s goal and internal notes are only visible to you. You should try your best to achieve {{speaker}}s goal in way that aligns with their character traits. Additionally, maintain naturalness and realism (do not repeat what other people have already said). Here is the context of the interaction: - Scenario: {{scenario}} - {{speaker}}s social goal (private): {{social goal}} - {{speaker}}s internal mental states (private): {{ms text}} Recent conversation: {{history}} You are at Turn #{{turn number}}. Your available action types are \"none\", \"speak\", \"non-verbal communication\", \"action\", \"leave\". IMPORTANT: - If there is NO prior history, you MUST START the conversation with one concise opening line that advances your goal. - Keep your output to single turn. Note: You can leave this conversation if 1) you achieved your social goal, 2) you feel uncomfortable, 3) you lose patience/interest, or 4) for any other reason. Please only generate JSON string including the action type and the argument. Your action should follow the given format: Output EXACTLY one JSON object. No extra text. Schema: { mental state: single-paragraph text per the guidelines below, action type: [none, speak, non-verbal communication, action, leave], argument: content or empty } Rules for mental state: - Write plain text (no markdown). Keep it to one paragraph; avoid newlines and unescaped quotes. Rules for action type and argument: - Allowed values for action type: none, speak, non-verbal communication, action, leave (lowercase; match exactly). - When action type == none: you are done / no further action now. Set argument to (empty). - When action type == speak: argument must be your next utterance ONLY (no speaker labels, no markdown, no quotes). - When action type == non-verbal communication: argument is brief stage direction, e.g., *nods*, *sighs*, *shrugs* (no speaker labels, 120 chars). - When action type == action: argument is brief physical action, e.g., hands over the receipt (no speaker labels, 120 chars). - When action type == leave: you exit the conversation (e.g., you achieved your goal, you felt uncomfortable, or you think the conversation has ended). Set argument to (empty). - Keep everything concise; avoid newlines and unescaped quotes in argument. Proceed to generate your reply in the above JSON format. Figure 16: prompt used to generate utterances. 27 Infusing Theory of Mind into Socially Intelligent LLM Agents Training data instance used for FT+MS+Uttr User: Scenario: {{scenario}} Social Goal: {{social goal}} Mental State: {{mental text}} Recent Conversation: {{history}} Assistant: Figure 17: Training data instance used for TOMA"
        }
    ],
    "affiliations": [
        "University of British Columbia",
        "Vector Institute for AI"
    ]
}