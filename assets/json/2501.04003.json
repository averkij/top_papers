{
    "paper_title": "Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives",
    "authors": [
        "Shaoyuan Xie",
        "Lingdong Kong",
        "Yuhao Dong",
        "Chonghao Sima",
        "Wenwei Zhang",
        "Qi Alfred Chen",
        "Ziwei Liu",
        "Liang Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Vision-Language Models (VLMs) have sparked interest in their use for autonomous driving, particularly in generating interpretable driving decisions through natural language. However, the assumption that VLMs inherently provide visually grounded, reliable, and interpretable explanations for driving remains largely unexamined. To address this gap, we introduce DriveBench, a benchmark dataset designed to evaluate VLM reliability across 17 settings (clean, corrupted, and text-only inputs), encompassing 19,200 frames, 20,498 question-answer pairs, three question types, four mainstream driving tasks, and a total of 12 popular VLMs. Our findings reveal that VLMs often generate plausible responses derived from general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs. This behavior, concealed by dataset imbalances and insufficient evaluation metrics, poses significant risks in safety-critical scenarios like autonomous driving. We further observe that VLMs struggle with multi-modal reasoning and display heightened sensitivity to input corruptions, leading to inconsistencies in performance. To address these challenges, we propose refined evaluation metrics that prioritize robust visual grounding and multi-modal understanding. Additionally, we highlight the potential of leveraging VLMs' awareness of corruptions to enhance their reliability, offering a roadmap for developing more trustworthy and interpretable decision-making systems in real-world autonomous driving contexts. The benchmark toolkit is publicly accessible."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 3 0 0 4 0 . 1 0 5 2 : r Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives Shaoyuan Xie Lingdong Kong,, Yuhao Dong, Chonghao Sima, Wenwei Zhang Qi Alfred Chen Ziwei Liu Liang Pan,(cid:66) University of California, Irvine Shanghai AI Laboratory National University of Singapore S-Lab, Nanyang Technological University The University of Hong Kong Code & Demo: drive-bench.github.io Dataset & Benchmark: huggingface.co/datasets/drive-bench/arena Figure 1. Overview of DriveBench. Our benchmark evaluates the reliability and visual grounding of Vision-Language Models (VLMs) in autonomous driving across four mainstream driving tasks perception, prediction, planning, and explanation under diverse spectrum of 17 settings (clean, corrupted, and text-only inputs). It includes 19, 200 frames and 20, 498 QA pairs spanning three question types: multiple-choice, open-ended, and visual grounding. By addressing diverse tasks and conditions, we aim to reveal VLM limitations and promote reliable, interpretable autonomous driving."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Vision-Language Models (VLMs) have sparked interest in their use for autonomous driving, particularly in generating interpretable driving decisions () Project lead. ((cid:66)) Corresponding author. through natural language. However, the assumption that VLMs inherently provide visually grounded, reliable, and interpretable explanations for driving remains largely unexamined. To address this gap, we introduce DriveBench, benchmark dataset designed to evaluate VLM reliability across 17 settings (clean, corrupted, and text-only inputs), encompassing 19, 200 frames, 20, 498 question1 answer pairs, three question types, four mainstream driving tasks, and total of 12 popular VLMs. Our findings reveal that VLMs often generate plausible responses derived from general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs. This behavior, concealed by dataset imbalances and insufficient evaluation metrics, poses significant risks in safety-critical scenarios like autonomous driving. We further observe that VLMs struggle with multi-modal reasoning and display heightened sensitivity to input corruptions, leading to inconsistencies in performance. To address these challenges, we propose refined evaluation metrics that prioritize robust visual grounding and multi-modal understanding. Additionally, we highlight the potential of leveraging VLMs awareness of corruptions to enhance their reliability, offering roadmap for developing more trustworthy and interpretable decision-making systems in realworld autonomous driving contexts. The benchmark toolkit is publicly accessible. 1. Introduction With recent advancements in Vision-Language Models (VLMs) [1, 5, 12, 13, 4244, 67], there has been increasing research interest in applying VLMs to autonomous driving applications [20, 48, 58, 59, 62, 68, 69, 74]. These efforts span both the design of end-to-end frameworks [58, 59, 62, 74] and the integration of VLMs to facilitate interpretable interactions and decisions through natural language [28, 48, 53]. Such interpretability is believed to enhance transparency, trustworthiness, and user confidence in autonomous systems [76]. However, previous studies highlight significant limitations in evaluating end-to-end autonomous driving models in open-loop settings [39]. Instead of focusing on trajectory prediction with potentially unreliable open-loop end-to-end VLMs [28, 50, 59, 74], we address another fundamental yet underexplored question that has been widely assumed [50, 58, 62, 76]: Are existing VLMs capable of providing reliable explanations grounded on visual cues for driving? To investigate, we examine whether driving decisions generated by VLMs are genuinely grounded in sensory information from the physical environment or reflect general knowledge and fabricated responses from textual cues. Model Reliability. To address the fundamental question, we examine VLM reliability through an out-ofdistribution (OoD) robustness lens. For this purpose, we introduce DriveBench, benchmark encompassing four mainstream driving tasks and 15 types of data corruptions, including 19, 200 images and 20, 498 QA pairs tailored to real-world autonomous driving scenarios. To assess robustness under extreme conditions, we push visual degraFigure 2. Do VLMs provide reliable explanations based on visual cues in driving? We study this from perspectives on reliability, data, and metrics. We find VLMs can fabricate quality answers to driving questions when visual information is absent. The fabricated answers can bypass current metrics, even GPT scores, due to imbalance, lack of context dataset, and problematical evaluation protocols. Our observations challenge the passive assumption that VLMs are more reliable than task-specific models in driving decisions [26] because of visual-grounded interpretable responses. dation to its limits by using text-only prompts. Surprisingly, VLMs demonstrate comparable performance to their outputs under clean visual inputs, even when no visual cues are available (as illustrated in Fig. 2). This observation starkly contrasts with human drivers, who would struggle under such degraded conditions. closer analysis reveals that this apparent resilience is often byproduct of imbalanced datasets and suboptimal evaluation protocols rather than inherent model robustness. Datasets. We perform an in-depth analysis of existing Driving with Language benchmarks [9, 29, 55, 59, 72] and identify critical shortcomings, particularly concerning dataset imbalance. Many of these benchmarks, built on popular driving datasets such as nuScenes [55], BDD [78], and Waymo Open [61], inherit limitations from their original designs [39]. For instance, imbalanced data distributions skew evaluations, enabling overly simplistic answers such as Going Ahead to achieve over 90% accuracy for motion-related queries. Furthermore, the reliance on singleframe questions often reliant on temporal context creates challenges even for human annotators. Consequently, these benchmarks exhibit inherent biases and persistent negative samples, which diminish the interpretability and reliability 2 of evaluation outcomes. Metrics. We also reevaluate existing metric designs critically. Language-driven interactions in driving applications are often assessed using traditional pattern-matching metrics such as ROUGE [40], BLEU [54], and CIDEr [65], which were originally developed for summarization and translation tasks. However, as noted in [3, 4, 18, 63], these metrics face significant limitations in evaluating nuanced language-based driving decisions. Even modern evaluators like GPT-based scoring [10, 22, 45, 59] provide limited insights without task-specific rubrics. These constraints underscore the need for metrics that effectively capture reasoning, contextual understanding, and safety-critical aspects. We advocate for the development of advanced evaluation metrics that incorporate task-specific rubrics, structured question formats, and contextual driving information to more accurately assess VLMs in the real world. Through series of comprehensive experiments, we derive several key insights from our analysis, spanning 17 settings (i.e., clean, text-only, and various corrupted inputs), 12 VLMs (including both open-sourced and commercial models), 5 tasks (perception, prediction, planning, behavior, and corruption identification), and 3 evaluation metrics (accuracy scores, traditional language metrics [40, 54], and GPT scores). These findings shed light on the current challenges in integrating VLMs into driving scenarios: 1 Fabricated responses under degradation: VLMs often produce plausible yet fabricated responses under degraded visual conditions, including scenarios where no visual cues exist. This raises concerns about their reliability and trustworthiness, as such behaviors are difficult to detect using existing datasets and evaluation protocols. 2 Awareness of visual corruptions: While VLMs exhibit some awareness of visual corruptions, they only explicitly acknowledge these issues when directly prompted. This highlights the models limited capacity to autonomously assess the reliability of visual inputs and provide scenariospecific, safety-focused responses. 3 Impact of dataset biases: Highly biased datasets and suboptimal evaluation protocols can create misleading perceptions of VLM performance. In many cases, VLMs rely on general knowledge rather than actual visual cues to generate responses, which can unexpectedly achieve high scores with existing metrics. 4 Need for tailored metrics: Current evaluation metrics, including traditional language-based metrics [40, 54] and GPT scores [10, 59], fail to capture the nuanced requirements of autonomous driving tasks. There is an urgent need for the development of specialized metrics that account for reasoning, contextual understanding, and safety-critical aspects to evaluate VLMs more effectively."
        },
        {
            "title": "Our findings through DriveBench not only highlight the\nneed for improved datasets and evaluation protocols but also",
            "content": "pave the way for developing safer, more interpretable VLMs for real-world autonomous systems. 2. Related Work Driving with Language. VLMs [1, 5, 4244, 67] have demonstrated remarkable human-level reasoning and understanding across diverse domains [6, 8, 11, 15, 17, 25, 41, 47, 60, 62, 73, 75]. This capability has raised the prospect of utilizing VLMs to manage complex and unpredictable scenarios in autonomous driving [76]. Additionally, the language-based interaction that VLMs offer can help mitigate the black-box nature of deep neural networks by providing explanatory feedback that accompanies their decisions. Driven by these advantages, growing body of research has begun investigating the deployment of VLMs in autonomous driving [20, 48, 59, 62, 69, 74]. Early works [20, 69] leveraged LLMs for decision-making in simplified driving simulators (e.g., HighwayEnv [34]) by offering context-driven descriptions. More recent advancements in VLM architectures [48, 59, 62, 74] enable these models to interact directly with environments through multimodal (visual and language) inputs. However, despite these advancements, the robustness and reliability of VLMs in complex, real-world autonomous driving tasks remain largely untested, especially given that reliable performance across diverse driving situations is fundamental requirement for their application in autonomous driving. Datasets & Metrics. To support the integration of VLMs in autonomous driving, several multimodal datasets have been proposed [16, 29, 55, 59, 70, 72]. These datasets typically augment established driving benchmarks, such as BDD [78] and nuScenes [55], with language-based annotations that enable language-driven perception and decisionmaking. Certain datasets, such as DriveLM [59], incorporate advanced structures like graphs [55, 59] to further support the reasoning capabilities of VLMs, thus offering richer context for nuanced decision-making. DriveLM [59] is notable for its extensive, human-annotated language data based on the nuScenes dataset, covering the full range of autonomous driving tasks, including perception, prediction, planning, and control. Nonetheless, despite these advances, current datasets and metrics may still lack the comprehensive scope needed to capture the full spectrum of real-world driving complexities, particularly in evaluating open-ended questions generated by VLMs, where responses require detailed understanding of diverse scenarios. VLM Reliability. Deep neural networks have historically struggled with out-of-distribution (OoD) data, limitation of particular concern in autonomous driving, where failing to handle rare or unexpected scenarios could result in severe safety risks [30, 31, 71]. Large-scale models such as CLIP [56], trained on extensive internet-sourced datasets, have shown enhanced robustness to such challenging corner 3 (a) Training Set (b) Validation Set Figure 3. The behavior distributions of steering and speed in DriveLM-nuScenes [59]. The majority actions of vehicle behaviors are Going Ahead, which has also been noted in [39]. (a) Perception Task (b) Behavior Task Figure 4. The accuracy scores of behavior tasks under different visual inputs. The results are from DriveLMAgent [59]. No Pix. and No Fea. denote zero image pixel and zero feature, respectively. perception and cases [19, 51], which suggests that VLMs, trained on vast, diverse datasets may have the inherent common-sense reasoning capabilities to address these challenges better than traditional, task-specific models [35, 76]. However, this hypothesis remains under-investigated in two critical areas: 1) the reliability of VLMs to maintain accurate reasoning when exposed to visual corruptions, and 2) their capacity to detect and interpret potential visual anomalies that could impact safe vehicle maneuvering. In this work, we provide systematic evaluation of the reliability of current VLMs under conditions of visual corruption, identifying potential limitations that impact their applicability in real-world driving contexts. 3. DriveBench: Driving with VLMs In this section, we detail the construction of our benchmark designed to assess the reliability of VLMs within the domain of autonomous driving. 3.1. Datasets In this section, we start building our DriveBench with representative driving with language datasets [59]. We choose DriveLM [59] as it is one of the most representative datasets 4 Figure 5. Challenging cases in existing dataset. The results are from GPT4-o [2]. (a): The black sedan is turning left, indicated (b): The black sedan is turning right. The by the turn lights. model predicts both Going Ahead. The examples show challenging cases for Turn choice, where the visual cues are too subtle or rely on temporal context for correct predictions. (c) and (d) are both Turning Right, but the model fails to locate the objects based on center pixel positions due to the existence of overlapping or occlusion. Zoomed-in for more details. for driving with languages, considering the impact that the dataset serves as one of the benchmarks in Foundation Models for Autonomous Systems Workshop [14, 52]. The dataset spans five tasks, including perception, prediction, planning, behavior, and control. For each task, different sets of questions are applied, such as multiple-choice questions (MCQs), and visual question answering (VQA). The comparison between our dataset and related benchmarks can be seen in Tab. 1. Distribution Bias. Through detailed examination, we identify significant distribution bias in the dataset, which is naturally inherited from the nuScenes dataset [7, 39]. Specifically, in behavior-based multiple-choice questions (MCQs) that inquire about the future movement of the ego vehicles, approximately 78.6% of responses are labeled as Going Straight as shown in Fig. 3. Consequently, randomly selecting Going Straight as an answer can yield accuracy levels exceeding 70%, which is concerning since the fine-tuning processes [59] further encourage the model to memorize majority choices. To address this imbalance, in DriveBench, we carefully re-sampled the data to create more balanced distribution among different options. Challenging Cases. Furthermore, we evaluate the advanced GPT-4o [2] and analyze the failure cases, as illustrated in Fig. 5. We find that annotations such as Turn Left or Turn Right are factually accurate but often require temporal context or subtle indicators (e.g., turn signal lights) to be correctly interpreted. Additionally, vehicles overlap with each other in some cases, making the pixel position-based question too nuanced for current VLMs to distinguish (e.g., Whats the object at (540, 600) and object at (530, 610), respectively). As result, existing VLMs demonstrate conTable 1. Comparisons among evaluation benchmarks for driving. Per., Pre., Beh., Pla., Rob. refer to the Perception, Prediction, Behavior, Planning, and Robustness tasks, respectively. GPTctx represents GPT scores augmented with context information."
        },
        {
            "title": "Benchmark",
            "content": "Per. Pre. Beh. Pla. Rob. # Frames (Test) # QA Pairs (Test)"
        },
        {
            "title": "Evaluation Metrics",
            "content": "BDD-X [29] BDD-OIA [72] nuScenes-QA [55] Talk2Car [16] nuPrompt [70] DRAMA [49] Rank2Tel [57] DirveMLLM [23] DriveVLM [62] DriveLM [59]"
        },
        {
            "title": "DriveBench",
            "content": "- - 36, 114 1.8k 36k - - 880 - 4, 794 - - 83, 337 2, 447 6k 14k - - - 15,"
        },
        {
            "title": "None\nNone\nNone\nNone\nNone\nChain\nChain\nNone\nNone\nGraph",
            "content": "Language F1 Score Acc Acc AMOTA Language Acc, Language Acc GPTctx Language, GPT 19, 200 20, 498 Graph Acc, Language, GPT, GPTctx 3.2. Driving Tasks Our DriveBench covers four mainstream driving tasks, inplanning, and cluding perception, prediction, behavior, examples are shown in Fig. 1. The perception task focuses on identifying the moving status of targeted objects and analyzing the surrounding environments. The prediction task focuses on predicting the future movement of these objects. The planning task focuses on suggesting actions to navigate the vehicle safely and efficiently in complex driving scenarios. Finally, the behavior task is designed to predict the future steering and speed of the ego vehicles. By addressing these tasks, the dataset ensures comprehensive coverage of the critical aspects required for evaluating the capabilities of driving VLMs in handling complex and dynamic scenarios. 3.3. Corruption Data Autonomous driving is an application where numerous types of corruption can exist [24, 3032, 37, 71]. Previous works find that task-specific models are inherently vulnerable to OoD corruptions, which prohibit precise perception of surroundings. On the other hand, large models [19, 51] are shown to be resilient toward OoD corruption, given their vast amount of training data [56]. Therefore, given the safety-critical applications and promising robustness of large models, one natural question is, how reliable are existing VLMs towards visual corruption in driving? Our DriveBench crafts total of 15 different corruption types (cf . Fig. 1), spanning across weather conditions (1Brightness, 2Dark, 3Fog, 4Snow, and external disturbances (6Water Splash 5Rain), sensor failures (8Camera and 7Lens Obstacle), Crash, 9Frame Lost, and 10Saturate), motion blurs (11Motion Blur and 12Zoom Blur), and data transmission errors (13Bit Error, 14Color Quant, Figure 6. The word cloud collected from the QA pairs in the proposed benchmark, highlighting the main focus on different autonomous driving tasks in DriveBench. The larger the font size, the higher the frequency of occurrence. siderable difficulty in accurately interpreting cues that rely on temporal information or subtle visual indicators, leading to disproportionate number of negative outcomes in evaluations. To prevent such samples from skewing our findings, we eliminate instances highly dependent on temporal context or present significant interpretive challenges for current VLMs. Our data selection strategy prioritizes instances that GPT-4o [2] can correctly interpret, thereby indicating the availability of sufficient visual cues for single-framebased predictions. We also analyze other failure cases of GPT-4o [2], as shown in Fig. 7. In these examples, humans can answer with correct answers while GPT-4o [2] heavily relies on the object relation position to the frames for decision-making. Thus, we keep those examples in our dataset. 5 Figure 7. GPT-4o failure cases. (a): GPT-4o reasons the moving status of the pedestrian by the moving position related to the frame, instead of the coordinate of the moving object itself, thus leading to wrong perception results. (b): The model struggles to distinguish the correct direction based on the coordinate of the target object. (c): GPT-4o reasons the moving status of the SUV by the relative location of the object to the current frame causes the wrong perception results. (d): GPT-4o fails to perceive the orientation of the car. (e): The dataset contains examples that need multiple frames to reason successfully, GPT-4o fails to address these examples with single image input. (f): GPT-4o reasons the moving status of the SUV by the relative location of the object to the current frame causes the wrong perception results. and 15H.265 Compression). The corruption encompasses range of potential OoD scenarios the vehicles might encounter. From reliability perspective, These corruptions are the key to our evaluation and insights into VLMs visual-grounded driving capabilities. For more detailed definitions and examples, please refer to Appendix A.1. It should be noted that while existing research has explored VLM hallucinations and trustworthiness [27, 36, 64, 66], it has not yet been rigorously examined within the context of driving applications. Autonomous driving requires approaching the reliability of VLMs from different aspects, as language-based driving decisions are naturally linked to physical and context-specific real-world scenarios. 3.4. Vision-Language Models (VLMs) Recent advancements in VLMs applied to autonomous driving mainly include: 1) leveraging VLMs to generate and interpret driving decisions in high-level language [28, 48, 62], and 2) employing VLMs for end-to-end autonomous driving in low-level commands in open-loop [28, 50, 59, 62, 68, 74] or closed-loop [58] settings. fundamental motivation and underlying assumption across this domain is that VLMs can generate interpretable and explainable re6 Table 2. Evaluations of VLMs across different driving tasks (perception, prediction, planning, and behavior). Clean represents clean image inputs. Corr. represents corruption image inputs, averaged across fifteen corruptions. T.O. represents text-only evaluation. For humans, we only evaluate MCQ questions in perception and behavior tasks. The evaluations are based on GPT scores, where we tailored detailed rubrics for each task and question type. We highlight scores higher than clean performance under corruption."
        },
        {
            "title": "Clean",
            "content": "Corr. T.O."
        },
        {
            "title": "Clean",
            "content": "Corr. T.O."
        },
        {
            "title": "Clean",
            "content": "Planning Corr. T.O."
        },
        {
            "title": "Clean",
            "content": "Behavior Corr. T.O."
        },
        {
            "title": "Human",
            "content": "- - 47.67 38.32 - - - - - - - 69. 54.09 - GPT-4o [2] - Commercial 35.37 35. 36.48 51.30 49.94 49.05 75.75 75. 73.21 45.40 44.33 50.03 LLaVA-1.5 [43] LLaVA-1.5 [43] LLaVA-NeXT [44] InternVL2 [12] Phi-3 [1] Phi-3.5 [1] Oryx [46] Qwen2-VL [67] Qwen2-VL [67] DriveLM [59] Dolphins [48]"
        },
        {
            "title": "Specialist\nSpecialist",
            "content": "23.22 23.35 24.15 32.36 22.88 27.52 17.02 28.99 30.13 16.85 9.59 22.95 23.37 19.62 32.68 23.93 27.51 15.97 27.85 26.92 16.00 10.84 22.31 22.37 13.86 33.60 28.26 28.26 18.47 35.16 17.70 8.75 11. 22.02 36.98 35.07 45.52 40.11 45.13 48.13 37.89 49.35 44.33 32.66 17.54 37.78 35.89 37.93 37.27 38.21 46.63 39.55 43.49 39.71 29.88 14.64 23.98 28.36 48.89 22.61 4.92 12.77 37.77 5.57 4.70 39. 29.15 34.26 45.27 53.27 60.03 31.91 53.57 57.04 61.30 68.71 52.91 31.51 34.99 44.36 55.25 61.31 28.36 55.76 54.78 63.07 67.60 53.77 32.45 38.85 27.58 34.56 46.88 46.30 48.26 41.66 53.35 65.24 60. 13.60 32.99 48.16 54.58 45.20 37.89 33.92 49.07 51.26 42.78 8.81 13.62 32.43 39.44 40.78 44.57 49.13 33.81 47.68 49.78 40.37 8.25 14.91 32.79 11.92 20.14 28.22 39.16 23.94 54.48 39.46 27.83 11. in the explanatory quality beyond simple answer selection. We employ GPT-3.5-turbo for GPT Score evaluation. To better capture subtleties between responses, we prompt the model with detailed rubrics that account for answer correctness, coherence, and the alignment of explanations with the final answer. Rubrics are adapted for each specific task and question type to better reflect human-preferred responses. Detailed information on the evaluation prompts and rubrics can be found in Appendix C.3. 4. Experiments We conduct extensive benchmark experiments and analyses in DriveBench, with detailed discussions leading to our observations and conclusions by step. 4.1. Experimental Setups We set the temperature to 0.2 and top-p to 0.2, with maximum output token limit of 512. For DriveLMAgent [59], we adhere to the configurations outlined in [14]. Specifically, we utilize LLaMA-Adapter-V2 [21] as the base model, fine-tuned on the DriveLM-nuScenes dataset. The fine-tuning process is conducted on eight A800 GPUs with batch size of 4, over 4 epochs. For other open-source models, we download the official model weight from HuggingFace and inference using the vLLM [33] framework. More details about the used model weight can be found in Appendix F. For GPT-4o, we query the official APIs from OpenAI with the same configuration mentioned above. Figure 8. Illustration of performance degradation. After applying each corruption, we evaluate the MCQs accuracy changes compared with clean inputs. We observe that human performance largely decreases while most VLMs remain unchanged. sponses, thereby reducing the opaque black-box nature inherent in traditional task-specific models [26, 38]. To encompass the full scope of existing advanced VLMs, the current version of DriveBench evaluates diverse set of 12 popular VLMs, including both commercial and opensource models, as well as models fine-tuned specifically for autonomous driving applications [48, 59]. This selection reflects the latest developments in state-of-the-art VLMs for driving. To ensure consistency, we apply standardized system prompt across all models (further details are provided in the Appendix C.2). The prompt explicitly instructs the VLMs to generate auxiliary explanations, enabling GPT-based evaluation of MCQs, which only have selection alone by default, as detailed in the next section. 3.5. Evaluation Metrics In DriveBench, we consider several metrics following [59], including accuracy, BLEU [54], ROUGE-L [40], and GPT score [10, 59]. For MCQs, we utilize both accuracy, as the most direct measure, and GPT scores to capture nuances 7 4.2. Observations & Discussions 4.2.1. Corruption Resilience The primary results, evaluated using GPT, are summarized in Tab. 2 and Fig. 9. These findings reveal that, even in the presence of image corruption, the model performance remains largely unaffected, demonstrating notable resilience to such perturbations. Specifically, as illustrated in Fig. 9, the performance trends align closely with those observed in other benchmarks. For instance, GPT-4o and Qwen2VL72B consistently achieve state-of-the-art results. Furthermore, when evaluating the resilience against corruption, most vision-language models (VLMs) maintain comparable performance to that observed with clean image inputs, even in open-ended visual question answering (VQA) tasks. To understand the source of this resilience, we investigate whether it stems from the inherent robustness of these VLMs, due to their pre-training on extensive webscale datasets, or if other factors contribute to this phenomenon. Human Evaluations. To further validate that the applied corruptions indeed impact the driving scenario and to explore the performance gap between humans and VLMs, we conduct human evaluation. Specifically, we sub-sample the dataset and design user interface to facilitate human performance assessment (more details in Appendix C.4). The results are shown in Tab. 2. Since human responses include only choice without time-intensive written explanation, GPT scores, which we design to reward detailed explanations, may introduce unfairness in comparison. Therefore, we also report accuracy degradation, as shown in Fig. 8. Interestingly, we observe significant accuracy drop for human participants under corrupted conditions, whereas most VLMs exhibit subtle performance variations across different corruption types. We next explore if it is due to model robustness [19] or other possible reasons. Text-Only Prompts. Given the above results, we further investigate the effects of extreme corruption by providing VLMs with fully black images, reducing the input to text-only prompts with no visual information. The results, shown in Tab. 2, reveals an intriguing pattern: GPT scores for text-only prompts are closely aligned with those obtained with clean image inputs. This trend persists across different tasks and models, suggesting that the phenomenon is not solely due to model robustness. Considering the GPT score also takes the quality of explanations into account in addition to answer correctness, we further analyze accuracy on MCQs to isolate the potential scoring advantages due to explanations (discussed in Sec. 3.4). Given that MCQs in perception tasks have three answer choices and MCQs in behavior tasks have four, the expected random-guessing accuracy would be approximately 33% and 25%, respectively. We report results only Figure 9. Radar chart comparisons among different models. The performance for each input corruption type is averaged across all the 1, 261 questions spanning four different driving tasks. The evaluation metric used here is the GPT score. for models that exceed this random baseline on clean inputs, as shown in Tab. 3. Interestingly, significant portion of the models show minimal or no accuracy degradation, even in the absence of visual cues. The results are even more concerning when considering open-ended questions (i.e., prediction and planning). For instance, responses from the state-of-the-art GPT-4o [2] under text-only conditions maintain approximately 95% of the performance seen with clean image prompts. Upon further examination shown in Fig. 10, we observe that the high performance of VLMs under text-only conditions is likely influenced by the extensive general knowledge acquired during training. For instance, the models can guess the moving status of one surrounding object based on text cues referring to which camera it has been seen and the corresponding position in that image (examples can be found in Fig. 2). These observations yield two key insights: VLMs are capable of producing plausible responses to driving-related questions based solely on natural language prompts. This capability is likely attributed to the extensive general knowledge and common sense reasoning capabilities developed during their pre-training. The current evaluation protocols for assessing VLMs in autonomous driving reveal significant shortcomings. Even advanced evaluation methods, such as those leveraging GPT-based scoring, fail to capture the nuances of the reliability of VLMs responses. To investigate the first insight further, we pose the ques8 Table 3. Comparisons of accuracy scores between clean and fully black (no image) inputs. We observe large portion of models have no clear performance degradation even when the visual information is absent, suggesting the VLMs response might mainly be based on majority biases (e.g., Going Ahead, in most driving scenarios), instead of leveraging visual cues from sensors. Task Image Human GPT-4o [2] LLaVA-NeXT [44] LLaVA-1.513B [43] Phi-3 [1] Phi-3.5 [1] Qwen2-VL7B [67] Qwen2-VL72B [67] Perception Behavior Clean No Image Clean No Image 93.3 - 69.5 - 59.0 59.5 0. 25.5 24.0 1.5 55.0 34.5 20.5 33.5 24.0 9.5 50.0 50.0 0.0 32.5 33.0 0.5 54.5 17.5 37. 26.5 30.0 3.5 56.5 58.5 2.0 36.5 40.0 3.5 59.0 56.5 2.5 30.0 23.0 7.0 60.0 23.5 36. 23.0 36.5 13.5 Table 4. Comparisons of accuracy changes before and after prompting VLMs with explicit corruption context. We notice clear trend of performance degradation after mentioning the corruption type in the question. The results suggest VLMs are aware of the current corruption and acknowledge they can not respond to the severely degraded visual information when explicitly prompted. Method Bright Dark Snow Fog Rain Lens Water Cam Frame Saturate Motion Zoom Bit Quant H.265 GPT-4o 8.69 12.98 8.25 9.00 6. 3.81 5.82 12.94 10.99 8.52 6.98 0.57 8. 4.79 14.30 0.00 0.00 0.25 0.25 1.04 1.04 0.26 0.26 LLaVA-1.57B LLaVA-1.513B LLaVA-NeXT 5.83 20.63 31.95 14.00 18.50 31.39 36.97 6.13 18.29 17.67 2.46 9.50 8.51 8.49 10. 8.92 10.74 9.50 9.00 7.75 7.48 5.00 10.00 11.31 33.22 0.75 8.67 9.50 9.75 7.96 5.93 9.00 9.50 11.00 8.04 8.25 8.96 6.24 17.51 0.23 7.48 16.37 9.31 8.29 3.03 6.98 20.94 29.85 11.01 7.19 6.90 InternVL8B 7.24 Phi-3.5 9.78 Phi-3 -4.22 Qwen2-VL7B 9.74 Qwen2-VL72B 6.70 7.50 8.50 7.54 8.60 2.79 1. 8.97 0.26 0.51 0.51 1.40 1.96 0.00 0.00 2.60 2.60 2.22 2. 0.52 1.04 2.66 1.32 2.57 2.07 0.26 2.57 21.19 24.85 33.29 19.50 5.89 6.67 7.00 2.35 4.65 7.71 6.94 11.29 11.16 8.07 8.48 18.81 17.89 3.57 5.42 -13.12 9.38 11.07 5.06 3.00 8.46 9.29 13.07 6.61 2.93 7.44 Figure 10. Prediction spatial distribution of Qwen2-VL7B [67] under text-only prompts. The model can potentially guess the MCQ answers without visual information by leveraging text cues, e.g., camera and coordinate positions in the questions. tion: Are driving VLMs aware of the underlying corruptions in images when they fabricate their answers? This question serves as the basis for the expanded analysis presented below. 4.2.2. Corruption Awareness We explore whether the fabricated reasonable answer of VLMs under corruption might stem from lack of awareness regarding potential visual corruptions. To investigate this, we conduct two experiments: E-1) involves explicit corruption reference when prompting the model, e.g., what are the important objects in the snowy day, and E2) we directly ask the model to identify the current type of image corruption, e.g., what is the current corruption. In E-1, we analyze changes in MCQ accuracy when Figure 11. Evaluation results when using different metrics. The language metrics, such as ROUGE-L [40] and BLEU-4 [54], exhibit high consistency; while the GPT score demonstrates noticeable gaps. We also observe that fine-tuned process benefits DriveLM [21, 59] significantly in regulating its response format, thus leading to misleading high performance under language metrics. VLMs are explicitly prompted with references to visual corruption. As shown in Tab. 4, the results demonstrate notable trend of decreasing accuracy across various models and corruption types. Certain models exhibit substantial performance declines in the presence of corruption prompts; for example, LLaVA-NeXT7B [44] experiences an accuracy reduction of approximately 19.62%. closer examination of model responses reveals increased uncertainty when the corruption context is included in the prompt. For instance, the model may respond with statements such as based on the image provided, it is not possible to accurately de9 Table 5. Evaluations on corruption awareness. MCQ represents questions that directly ask about the current corruption types. VQA represents questions in perception, prediction, and planning but augmented with explicit corruption context information, averaged across three driving tasks. CAP represents captioning questions that ask detailed descriptions at both object-level and corruption-level."
        },
        {
            "title": "CAP",
            "content": "GPT-4o [2] 57.20 57.28 54.90 29.25 56. 61.98 44.25 54.95 56.53 34.25 59. 56.25 36.83 53.95 57.57 LLaVA-1.57B [43] LLaVA-1.513B [43] LLaVA-NeXT [44] InternVL2 [12] Phi-3 [1] Phi-3.5 [1] Oryx [46] Qwen2-VL7B [67] Qwen2-VL72B [67] DriveLM [59] Dolphins [48] 69.70 61.60 69.70 59.90 40.00 60.60 53.20 76.70 59.80 21.20 54.30 35.49 39.76 36.96 48.72 40.59 41.82 40.43 49.33 51.05 42.86 30.21 35.91 37.76 48.52 48.60 45.61 45.97 48.95 45.12 48.55 20.04 31. 26.50 15.50 48.50 50.75 25.00 21.25 45.00 37.50 45.50 21.25 3.00 29.17 34.55 30.32 47.74 31.44 36.89 40.68 47.62 50.57 37.49 30.42 34.95 37.83 57.18 57.82 45.99 30.95 56.06 51.24 57.25 21.92 29. 18.83 24.08 21.83 29.92 16.83 25.58 50.50 22.83 52.25 9.00 9.42 30.64 35.48 30.40 45.06 35.58 34.66 36.71 39.45 45.89 36.68 26.83 33.15 36.08 44.37 51.14 43.71 39.30 48.55 47.23 48.59 15.56 26. 71.25 79.75 66.00 68.25 31.25 33.00 72.50 57.00 58.25 22.25 9.25 33.43 36.46 34.20 49.51 42.92 46.03 40.01 47.40 50.85 42.05 29.82 35.18 36.42 50.44 49.67 48.43 49.33 48.33 47.74 47.88 17.07 28. 10.17 15.50 11.83 30.00 27.67 39.67 39.67 35.83 44.83 17.50 21.50 27.28 32.53 29.43 43.42 33.04 33.47 36.98 42.31 46.23 39.56 28.86 34.38 34.33 53.50 54.24 41.35 39.67 49.87 48.60 50.50 10.37 27. tonomous driving depends on accurate situational awareness and precise interpretation of environmental cues. The observed behavior suggests that current VLMs may lack the capacity for actively corruption-aware reasoning, defaulting instead to generalized responses and common sense knowledge (e.g., going ahead, maintain safe distance, be cautious, etc.). This tendency can be considered form of hallucination [27], which undermines the visual grounding reliability of VLMs within the safety-critical context of autonomous driving. Conversely, models such as LLaVA-1.5 [43] with 7B and 13B parameters exhibit minimal performance changes even when corruption-specific prompts are provided. This observation, when combined with the previous findings, suggests two possible explanations: 1) these models may lack the capability to detect image corruption, or 2) while aware of the corruption, their responses remain dominated by general knowledge rather than visual cues, even in clean situations, thus lead to unchanged performance. To investigate the first hypothesis, we conduct E-2, in which we explicitly prompt the VLMs to identify the type of visual corruption or determine the number of corrupted cameras in scenarios involving camera crash and frame lost corruptions [71]. The results, summarized under the MCQ columns in Tab. 5, indicate that LLaVA-1.5 [43] achieves high accuracy in identifying corruption types, particularly in weather and motion corruption scenarios, suggesting that it indeed possesses corruption awareness. To evaluate the second hypothesis, we analyze the confusion matrix of responses from LLaVA-1.5 [43]. Remarkably, the model consistently outputs Going ahead across all corruption scenarios, regardless of the actual visual context (shown in Fig. 21f in Appendix). This uniformity in answering indicates the model response is based on general knowledge rather than counting on visual inputs. Combining with the findings in Sec. 4.2.1, we conclude below: (a) Open-Ended Questions (b) Multiple Choice Questions Figure 12. Correlations when using different metrics. We study how well accuracy or ROUGE-L [40] matches the GPT scores for open-ended questions and multiple-choice questions (MCQs), respectively. We find that ROUGE-L [40] fails to reflect semantic information (e.g., key object) that is critical in driving. Contrarily, accuracy aligns well with the GPT score for MCQ while the GPT score can further capture nuanced differences in explanation when the answer is correct. termine the moving status of the object (480, 520) given the camera crash corruption. These findings suggest that some models exhibit degree of corruption awareness when explicitly prompted, recognizing potential unreliability in their responses under conditions of severe visual degradation. The differences in responses between prompts with and without explicit corruption references lead to two significant insights: VLMs demonstrate limited capacity to identify visual corruption when not explicitly prompted. In the absence of corruption-specific prompts, VLMs frequently rely on general knowledge rather than on the degraded visual information in the current driving scene, often generating responses that feign an understanding of the visual context. These insights raise critical concerns, as reliable au10 (a) Rubric-Aware GPT Evaluation (b) Question-Aware GPT Evaluation (c) Context-Aware GPT Evaluation Figure 13. Comparisons among different evaluation types (rubric, question-aware, and context-aware). The GPT scores vary depending on the rubric, question, and physical driving context. With more information added, the results become more distinguishable. Figure 14. Examples of GPT-4o [2] responses to four tasks and the corresponding evaluation results under the dark condition. We observe that GPT-4o [2] is aware of the low-light environment and can identify the bus and pedestrian from the image, showing resilience. 11 Figure 15. Examples of GPT-4o [2] responses to four tasks and the corresponding evaluation results under the motion blur condition. We observe that GPT-4o [2] are influenced by this type of corruption and tend to predict driving fast based on it. The example shows the potential of visual corruption to influence high-level driving decisions. Advanced VLMs tend to rely predominantly on textbased cues to generate responses under conditions of visual degradation, even though they are aware of it. Less advanced models demonstrate stronger dependence on general knowledge acquired during training, resulting in responses dominated by learned priors rather than situational visual information. 4.2.3. Fine-Tuned VLM Models Thus far, our analysis has primarily focused on generalpurpose VLMs applied in driving. While this focus is essential, given that these models serve as foundational architectures for task-specific fine-tuning, it remains insufficient for evaluating driving-specific capabilities, as these general models are not tailored or optimized for autonomous driving tasks. In this section, we shift our attention to VLMs fine-tuned specifically on driving datasets, reflecting the growing body of research dedicated to this area [48, 59, 62]. Specifically, we select DriveLM [59] and Dolphin [48] as representative models for our analysis, as both exhibit promising results and are explicitly fine-tuned to enhance visual-grounded driving decision-making abilities. DriveLM is fine-tuned on the nuScenes dataset [55], while Dolphin is fine-tuned on the BDD dataset [78]. This distinction in fine-tuning datasets offers unique opportunity to investigate the transferability of driving-specific VLMs across different datasets, as well as their answer reliability to visual corruption. The main results are summarized in Tab. 2 and Tab. 5. key observation is that Dolphin [48], which is primarily fine-tuned on the BDD [78] dataset, demonstrates significant difficulty in answering questions from the nuScenes [55] dataset. Given the general capabilities of VLMs to ad12 Figure 16. Examples of different VLM responses under the frame lost condition. We observe that GPT-4o [2] responses with visible objects while LLaVA-NeXT [44] and DriveLM [59] tend to hallucinate objects that cannot be seen from the provided images. dress questions across diverse domains, this result is both surprising and concerning, highlighting the limited generalizability of driving-specific VLMs when exposed to datasets or question formats that differ from their fine-tuning conditions. Regarding DriveLM [59], we further investigate how the model benefits from in-distribution fine-tuning in the following section. This analysis aims to elucidate the potential advantages and limitations of fine-tuning on specific language-annotated driving dataset. 4.2.4. Metrics Language Metrics. The insights presented thus far rely primarily on accuracy and GPT-based scores. However, due to the prohibitive costs associated with large-scale evalua13 Figure 17. Examples of different VLM responses under the water splash condition. We observe that, under severe visual corruptions, VLMs respond with ambiguous and general answers based on their learned knowledge, without referring to the visual information. Most responses include traffic signals and pedestrians, even though they are not visible in the provided images. tions using GPT APIs, traditional language metrics, such as BLEU [54] and ROUGE-L [40] remain widely employed in existing benchmarks [29, 59]. To better understand their applicability, we evaluate the validity of these metrics in the context of language-based driving decision tasks. In Fig. 11, we present the prediction task performance under clean image inputs, evaluated across different metrics. As anticipated, the language metrics demonstrate high internal consistency: models with high BLEU scores also tend to achieve high ROUGE-L scores, as both metrics emphasize pattern-matching between predicted responses and ground-truth answers. Furthermore, we visualize how the same responses are scored under ROUGE-L and GPT scores at scale in Fig. 12a. The results further reveal how the language score fails to reflect underlying key information: when ROUGE-L remains around 0.2, GPT scores vary across large range. DriveLM [59], while surpassing other VLMs with large margins under ROUGE-L evaluation, still lags behind Qwen2-VL72B [67] and GPT-4o [2] in GPT evaluation. The observation indicates that the main improvement of in-distribution fine-tuning on the current small-scale driving dataset largely comes from the answering template. Accuracy. In terms of accuracy, we also study how accuracy and GPT scores are related. The results are presented in Fig 12b. The GPT evaluation highly aligns with accuracy since we prompt the GPT to assign certain scores if the answer is correct. The divination is because GPT assigns another portion of scores to the coherence of explanation dimensions, capturing nuanced differences between answers. GPT Evaluation. critical question remains: is GPT evaluation currently the optimal approach? The answer is nuanced. GPT-based scoring can effectively capture human preferences and emphasize critical elements in driving scenarios, yet this capability is highly contingent on the provision of comprehensive driving contextual information. We empirically compare how the same response is scored given different information, shown in Fig. 13a. When GPT evaluation is prompted solely with GT and model response, the resulting scores are highly homogeneous while the inclusion of specific rubrics, questions, and specific-driving context yields greater score diversity. 5. Conclusion This work identifies and addresses key challenges in the deployment of Vision-Language Models (VLMs) for autonomous driving, with an emphasis on their visualgrounding reliability in complex real-world scenarios. Our findings reveal that VLMs frequently generate plausible yet unsupported responses when subjected to severe visual degradation, casting doubt on their reliability in critical decision-making tasks. Furthermore, imbalanced datasets and suboptimal evaluation protocols amplify these concerns, contributing to an overestimation of VLM reliabilities. To mitigate these challenges, we advocate for future efforts to prioritize the development of well-balanced, context-aware datasets and advanced evaluation metrics that rigorously assess the quality, contextual reasoning, and safety of driving decisions."
        },
        {
            "title": "Appendix",
            "content": "A.2. Corruption Definitions . . . . . . . . . . . . . . . . . . . A.3. Overall Statistics . . . . . . . . . . . . . . . . . . A.4. License . . . . B. Benchmark Study B.1. DriveLM-nuScenes . . . . . . . . B.2. BDD-X . . . . . . . . . . . . . . . . . . . . . . C. Additional Implementation Details . . . . . . . . . C.1. VLM Configurations . . C.2. VLM Prompts . . . . . . . . . . . . . . C.3. GPT Evaluations . . . . . . . . . . . . . C.4. Human Evaluations . . . . . . . . . . . . D. Detailed Experiment Results D.1. GPT Scores . . . D.2. Accuracy Scores . . . . D.3. ROUGE-L Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. Broader Impact & Limitations E.1. Broader Impact . . . . . . . . . . . . . . E.2. Potential Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F. Public Resource Used A. Benchmark Setup 15 17 17 17 17 17 17 18 18 19 19 19 19 19 37 37 37 37 In this section, we elaborate in detail on the procedures and protocols used to establish the DriveBench in this work. A.1. Benchmark Construction We detailed the benchmark construction process in this section. Our DriveBench is primarily inspired by DriveLM [59] given its impact and representativeness. Given its public availability, we subsample 200 keyframes from the DriveLM [59] training dataset. These keyframes are selected to balance the ground truth distribution, which can more accurately reflect the models performance and prevent bias in most common cases (e.g., Going Ahead). Each keyframe has multiple questions related to different tasks, spanning perception, prediction, planning, and behavior. For each task, we follow the question type design in DriveLM [59], including multiple-choice questions (MCQs) and visual question answering (VQAs), as shown in Tab. 6. When evaluating corruption awareness, we add information about corruption context to the question and modify the answer accordingly if necessary. We generate the corruption-related question-answering pairs by prompting GPT4 based on original QA pairs, which we refer to the robustness dataset as shown in Tab. 7. A.2. Corruption Definitions A. Benchmark Setup A.1. Benchmark Construction . . . . . . . . . . . 15 15 In this section, we detailed our settings for generating image corruption. DriveBench encompasses five distinctive 15 Figure 18. Examples of GPT evaluators with different information. The evaluator can revise the score and give more accurate evaluation based on more contextual information in the driving scenarios. For more details please refer to Fig. 32. Table 6. Detailed distribution of the curated benchmark dataset. #"
        },
        {
            "title": "Driving Task Question Type",
            "content": "# Samples"
        },
        {
            "title": "Total",
            "content": "Perception Prediction Planning Behavior MCQ & VQA VQA VQA MCQ 400 61 600 200 1, 261 Table 7. Detailed distribution of the proposed robustness benchmark dataset. The total number is summed across all the corruption types. Corrupt. Rec. represents corruption recognition, which asks the model to identify the current corruption types. Corrupt. Desc. represents questions related to the description of the current corrupted environment. #"
        },
        {
            "title": "Question Type",
            "content": "# Samples"
        },
        {
            "title": "Total",
            "content": "Corrupt. Rec. Perception Prediction Planning Corrupt. Desc. MCQ MCQ & VQA VQA VQA CAP 4, 000 5, 475 799 5, 999 3, 000 19, 237 corruption categories, each with multiple different types of corruptions reflecting the real-world scenarios. Weather & Lighting Conditions (5 Types): The simulations of diverse environmental weather and lighting conditions are used in the driving scenarios. In this benchmark, we include the 1Brightness, 2Dark, 3Snow, 4Fog, and 5Rain corruptions. External Disturbances (2 Types): The simulations of situations where camera lenses are In this benchoccluded by external objects or stains. mark, we include the 6Water Splash and 7Lens Obstacle corruptions. Sensor Failures (3 Types): The simulations of sensor failures. In this benchmark, we include the 8Camera Crash, 9Frame Lost, and 10Saturate corruptions. Motion Blurs (2 Types): The simulations of the blurs caused by the ego vehicles high-speed motion. In this benchmark, we include the 11Motion Blur and 12Zoom Blur corruptions. Data Transmission Errors (3 Types): The simulations of the errors happening during the video transmission process. In this benchmark, we include the 13Bit Error, 14Color Quantization, and 15H.265 Compression corruptions. All the 15 corruption types are generated by applying high-fidelity image processing algorithms developed in previous works [30, 31, 71, 77]. Here, we detail how each corruptions are synthesized as follows: 1Brightness: Adjusts the brightness values of the camera images by scaling pixel intensity upwards. 2Dark: Simulates low-light conditions by scaling down the images brightness using gamma-adjusted mapping. Additionally, it introduces Poisson noise to mimic photon shot noise and Gaussian noise to simulate sensor noise. 3Snow: Generates synthetic snow layer using random noise, applies motion blur to simulate falling snow, and blends it with the original image. 4Fog: Simulates fog by blending fractal noise-based fog layer over the image. 5Rain: Adds streak-like artifacts to the image, created through line patterns combined with motion blur, to sim16 ulate rain. 6Water Splash: Simulates water splashes by overlaying transparent circular droplet patterns on the image, followed by Gaussian blur to mimic water distortion effects. 7Lens Obstacle: Creates lens obstruction effects by blending blurred and unblurred regions of the image using randomly placed and shaped elliptical mask to emulate obstructions on the lens surface. 8Camera Crash: Simulates camera crash by replacing the affected image frames with black frames, representing complete loss of data for specific viewpoints or cameras. 9Frame Lost: Emulates frame loss by randomly setting some frames to black, indicating partial data corruption or temporary transmission failure. 10Saturate: Modifies the images color saturation by manipulating the saturation channel in the HSV color space, either enhancing or reducing the vibrancy of colors. 11Motion Blur: Applies linear motion blur to the image, simulating movement during exposure, with the blur radius and direction determined by severity. 12Zoom Blur: Applies radial zoom effect to the image, creating focused blur that simulates rapid movement toward or away from the lens, controlled by severity. 13Bit Error: Introduces random bit-level noise in the image data, mimicking digital corruption, with severity influencing the extent of errors. 14Color Quantization: Reduces the images color palette to limited set of levels, simulating low-quality color quantization, where severity controls the number of colors. 15H.265 Compression: Applies heavy H.265 video compression artifacts to the image, with severity increasing the compression level and artifact visibility. A.3. Overall Statistics The results in the main paper are based on the curated datasets, which contain 1, 461 questions, the detailed distribution of the questions is shown in Tab. 6. Specifically, each keyframe has two perception questions: one for MCQ and the other for VQA, four VQA questions for the planning task, and one behavior MCQ for the behavior task. In terms of the prediction task, not all keyframes have corresponding prediction questions. A.4. License The proposed benchmark is released under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License1. B. Benchmark Study In this section, we include detailed information on the the representative driving-withdataset distribution of language dataset. These datasets advance the development of driving with language models. B.1. DriveLM-nuScenes We visualize the dataset distribution in perception and behavior tasks in the DriveLM-nuScenes dataset [59], as shown in Fig. 3. The majority of choices for vehicle behaviors are going straight, which is also shown in [39]. Along with the DriveLM-Agent results in the main paper, we find that highly imbalanced data can cause several problems. When fine-tuning VLMs on this dataset, the model tends to memorize the majority of choices, and thus answer with it during inference even if the visual cues are absent, which prevents effective evaluation of the model reliability. The dataset mainly adopts language metrics [40, 54] and naive GPT score: prompt with only the answer and ground truth, for evaluation. Our results in the main paper also show the limitations of these metrics in evaluating language-based driving decisions. B.2. BDD-X To show the general existence of the limitations of existing benchmarks, we also study the BDD-X [29] dataset. We observe similar limitations as those in DriveLM [59] dataset, where the data is highly imbalanced. Most actions of the car are Stop or Going Ahead, where the random guessing of VLMs can achieve high accuracy since weve shown they potentially guessed the answer based on common knowledge and general case in Sec. 4.2.1. The observation shows that the limitations observed in our work are not individual but general drawbacks of existing drivingwith-language benchmarks. C. Additional Implementation Details In this section, we provide more details in terms of implementations and evaluation to facilitate the reproduction of this work. C.1. VLM Configurations GPT-4o [2], developed by OpenAIs, offers GPT-4-level intelligence with enhanced speed and multimodal capabilities, including voice, text, and image processing. It is designed to provide faster and more efficient responses across various applications. 1https://creativecommons.org/licenses/by-nc-sa/ 4.0. Figure 19. BDD-X dataset [29]: detailed distribution of car actions. Only the actions with frequency larger than 80 are visualized. The Stop actions and Turn actions are highlighted. We observe similar data distribution in balance as those in DriveLM [59], where turning actions only account for small portion of all actions. Phi-3 [1] is language model developed by Microsoft, focusing on efficiency and performance in natural language understanding and generation tasks. It is designed to handle wide range of applications, from conversational agents to content creation. Phi-3.5 [1] is an advanced version of Microsofts Phi series, offering improved reasoning and mathematical capabilities comparable to larger models like GPT-4o. It maintains efficiency while managing complex AI tasks across different languages. LLaVA-1.5 [43] is an open-source large multimodal model that integrates vision and language understanding. With 13 billion parameters, it achieves state-of-the-art performance across multiple benchmarks, rivaling models like GPT-4. LLaVA-NeXT [44] is an evolution of the LLaVA series, enhancing multimodal capabilities by supporting multiimage, video, and 3D tasks within unified large language model. It achieves state-of-the-art performance on wide range of benchmarks, demonstrating strong video understanding through task transfer from images. InterVL [12] is an open-source multimodal dialogue model developed by OpenGVLab. It closely approximates the performance of proprietary models like GPT4o, excelling in tasks that integrate visual and linguistic information, such as visual question answering and image captioning. Oryx [17] is unified multimodal architecture created by researchers from Tsinghua University and Tencent. It is designed for spatial-temporal understanding of images, videos, and multi-view 3D scenes, offering on-demand processing of visual inputs with arbitrary spatial sizes and temporal lengths. Qwen2-VL [5, 67] is large language model developed by Alibaba Cloud, available in both chat and pretrained versions. It delivers high-quality language generation and understanding capabilities, optimized for tasks requiring nuanced comprehension and generation of human language. DriveLM-Agent [59] is model from OpenDriveLab tailored for autonomous driving applications, focusing on graph-based visual question answering. It addresses challenges in driving scenarios by integrating language understanding with visual perception, enhancing decisionmaking processes in autonomous systems. Dolphins [48] is multimodal language model developed by NVIDIA for driving applications. It adeptly processes inputs such as video data, text instructions, and historical control signals to generate informed outputs, facilitating comprehensive understanding of complex driving scenarios. C.2. VLM Prompts We use the same system prompt for all the candidate VLMs, as shown in Fig. 20. We prompt the VLMs to explain for MCQs to facilitate the GPT evaluation based on their explanations. C.3. GPT Evaluations We include the detailed prompt we used for GPT evaluation here, we use the following prompt to evaluate MCQ questions in the perception task as shown in Fig. 22. The DESC is used to prompt context information for accurate evaluation. Limited to the current drive-with-language dataset, we extract the natural language description of critical objects in the current environment to provide context information. The prompt for the open-ended perception task is shown in Fig. 23. Since the ground truth for open-ended questions 18 You are smart autonomous driving assistant responsible for analyzing and responding to driving scenarios. You are provided with up to six camera images in the sequence [CAM FRONT, CAM FRONT LEFT, CAM FRONT RIGHT, CAM BACK, CAM BACK LEFT, CAM BACK RIGHT]. Each image has normalized coordinates from [0, 1], with (0,0) at the top left and (1,1) at the bottom right. Instructions: 1. Answer Requirements: For multiple-choice questions, provide the selected answer choice along with an explanation. For is or is not questions, respond with Yes or No, along with an explanation. For open-ended perception and prediction questions, related objects to which the camera. 2. Key Information for Driving Context: When answering, focus on object attributes (e.g., categories, statuses, visual descriptions) and motions (e.g., speed, action, acceleration) relevant to driving decision-making Use the images and coordinate information to respond accurately to questions related to perception, prediction, planning, or behavior, based on the question requirements. Figure 20. Inference system prompt. already included the visual description and moving status of important objects, we only prompt with PRED and GT with detailed rubrics. C.4. Human Evaluations In this subsection, we elaborate in more detail on how we conduct the human evaluation experiments in our benchmark. Procedures. Considering the large number of questions in the dataset, we subsample 15 out of 200 keyframes from our curated dataset. To ensure no overlaps between different corruptions, which might cause information leakage, we lower the probability if the same keyframes are sampled before. Then, we design user interface for human evaluation, focusing on MCQs. The interface is shown in Fig. 24. As with evaluating VLMs, we only prompt single-view images if the questions are related to only one of the cameras. Ethic Declaration. According to the Federal Policy of Human Subjective Research2, our research involves conducting anonymous visual recognition tasks, where participants respond to questions about visual stimuli without any interventions or demographic data collection. It qualifies for Exempt Research 2(i)3 because it solely involves surveylike procedures with no physical or psychological risks to 2https://www.federalregister.gov/d/201701058/ p-1315 3https://www.federalregister.gov/d/201701058/ p-1375 19 participants. Specifically, it meets the requirements of Exempt Research 2(i) as the data is recorded in manner ensuring that participants identities cannot be readily ascertained, directly or through linked identifiers. It does not fall under Exempt Research 2(ii) or 2(iii) because no identifiable information is recorded, and no IRB review is required to ensure these protections. We also submit an IRB review records to the corresponding institutions and receive the official confirmation of IRB review exemption. D. Detailed Experiment Results In this section, we include the detailed benchmark results evaluated by the GPT score metric. We also include the prediction spatial distribution in Fig. 21 for each model. D.1. GPT Scores We include the detailed GPT scores in Tab. 8, Tab. 10, Tab. 12, Tab. 13, and Tab. 15 for different tasks. The observation and conclusion in the main paper are primarily derived from GPT scores. Therefore, we focus more on the discussion of accuracy and language scores in the following sections. D.2. Accuracy Scores We include accuracy scores for MCQs in addition to GPT scores in Tab. 9 and Tab. 11. Compared with GPT scores, we find that the accuracy score metric is more homogeneous. For example, the LLaVA-1.5 models have 50% accuracy under all the input types, suggesting they are merely output Going Ahead for perception MCQs, which is also observed in the prediction spatial distribution in Fig. 21e and 21f. Moreover, we find that most models have no accuracy degradation under corruptions or even text-only inputs. This raises concerns about whether VLMs are indeed leveraging visual information to make decisions about the specified spatial location or naively guessing based on their general knowledge. D.3. ROUGE-L Scores We also present the detailed language scores, i.e., ROUGEL [40] here for open-ended questions in Tab. 14 and Tab. 16. As discussed in the main paper, we find the fine-tuning process can significantly benefit the ROUGE-L score, as indicated by the fact that DriveLM [59] output performs other models with large margin. On the contrary, GPT-4o [2], which generates more detailed answers punished by the answer length. The ROUGE-L score of GPT-4o [2] is lower than most of the models even though the GPT scores are much higher. (a) Ground Truth (b) GPT-4o [2] (c) Phi-3 [1] (d) Phi-3.5 [1] (e) LLaVA-1.57B [43] (f) LLaVA-1.513B [43] (g) Qwen2-VL72B [67] (h) InterVL8B [12] Figure 21. Prediction spatial distributions from VLMs. The locations represent the object positions in the image within each camera, which is input to the model as text description. We only visualize the data point where the model response aligns with the provided multiple choices (e.g., Going ahead, Turn Left, and Turn Right). Please evaluate the multiple-choice answer on scale from 0 to 100, where higher score reflects precise alignment with the correct answer and well-supported reasoning. Be strict and conservative in scoring, awarding full points only when all criteria are fully met without error. Deduct points for minor inaccuracies, omissions, or lack of clarity. Distribute the Total Score across the following criteria: 1. Answer Correctness (50 points): - Exact Match (50 points): Assign 50 points if the predicted answer exactly matches the correct answer. - No Match (0 points): Assign 0 points if the predicted answer does not match the correct answer, regardless of explanation quality. 2. Object Recognition (10 points): - Award up to 5 points for accurately identifying all relevant object(s) in the scene. - Award up to 5 points for correct descriptions of the identified object(s), including attributes like colors, materials, sizes, or shapes. - Guideline: Deduct points for any missing, misidentified, or irrelevant objects, particularly if they are crucial to the driving context. Deduct points if any important visual details are missing, incorrect, or overly generalized, especially if they affect comprehension or recognition. 3. Object Location and Orientation (15 points): - Score up to 5 points for precise description of the objects location, orientation, or position relative to the ego vehicle. - Award up to 5 points for acknowledging environmental factors, such as lighting, visibility, and other conditions that influence perception. - Score up to 5 points based on how well the answer reflects an understanding of situational context, such as obstacles, traffic flow, or potential hazards. - Guideline: Deduct points for inaccuracies or omissions in spatial information that could affect scene understanding. Deduct points if the answer fails to consider factors impacting object visibility or situational awareness. Deduct points for overlooked or misinterpreted contextual factors that may impact driving decisions. 4. Environmental Condition Awareness (15 points): - Award up to 15 points if the explanation considers environmental conditions (e.g., weather or sensor limitations) that could impact perception. - Guideline: Deduct points if relevant environmental conditions are ignored or inadequately addressed. 5. Clarity of Reasoning (10 points): - Award up to 5 points for clear, logically structured reasoning that is easy to understand. - Assign up to 5 points for grammatical accuracy and coherent structure. - Guideline: Deduct points for vague or confusing explanations that hinder comprehension. Deduct points for grammar or syntax issues that impact clarity or logical flow. Assign 0 points from criteria 2 to 5 if no explanation is provided. Here is the multiple-choice question: QUESTION Here is the ground truth object visual description: DESC Here is the correct answer: GT Here is the predicted answer and explanation (if any): PRED Please fill in the following scoring sheet, and then provide brief summary supporting the score: 1. Answer Correctness (50 points): 2. Object Recognition (10 points): 3. Object Location and Orientation (15 points): 4. Environmental Condition Awareness (15 points): 5. Clarity of Reasoning (10 points): Total Score: Brief Summary: Figure 22. GPT evaluation prompts for MCQs in our benchmark. 21 Please evaluate the predicted answer on scale from 0 to 100, where higher score reflects precise alignment with the correct answer and well-supported reasoning. Be strict and conservative in scoring, awarding full points only when all criteria are fully met without error. Deduct points for minor inaccuracies, omissions, or lack of clarity. Distribute the Total Score across the following criteria: 1. Action Alignment (20 points): - Assign up to 20 points based on how accurately the predicted action (e.g., forward, turn left, turn right) matches the correct answer. - Guideline: Award full points only for exact matches or highly similar actions. Deduct points for any inaccuracies or missing elements. Assign 0 points if no action prediction is provided. 2. Motion Precision (20 points): - Award up to 20 points based on how closely the predicted motion (e.g., speed up, decelerate) aligns with the correct motion in the answer. - Guideline: Deduct points if the predicted motion fails to match the type or intensity of the correct answer. Ensure that the intended speed or deceleration aligns accurately with the driving context. Assign 0 points if no motion prediction is provided. 3. Driving Context Appropriateness (15 points): - Score up to 15 points for the relevance of the predicted answer to the driving context implied by the correct answer, emphasizing logical alignment with the situation. - Guideline: Award higher scores only if the answer fully reflects an accurate understanding of the driving context. Deduct points if the action or motion is illogical or does not align with the scenarios requirements. 4. Situational Awareness (15 points): - Award up to 15 points for demonstrated awareness of environmental factors (e.g., traffic participants, obstacles) relevant to the action or motion. - Guideline: Deduct points if the answer misses key situational details that may lead to unsafe or incorrect predictions. 5. Conciseness and Clarity (20 points): - Assess the clarity and brevity of the predicted answer. Answers should be concise, clear, and easy to understand, effectively communicating the intended actions and motions. - Guideline: Deduct points for verbosity, ambiguity, or lack of focus that could hinder quick comprehension. 6. Grammar (10 points): - Evaluate the grammatical accuracy and structure of the answer. Assign up to 5 points for clarity and logical flow, and up to 5 points for grammatical accuracy. - Guideline: Deduct points for grammar or syntax issues that reduce readability or coherence. Here is the predicted answer: PRED Here is the correct answer: GT Please fill in the following scoring sheet, and then provide brief summary supporting the score: 1. Action Alignment (20 points): 2. Motion Precision (20 points): 3. Driving Context Appropriateness (15 points): 4. Situational Awareness (15 points): 5. Conciseness and Clarity (20 points): 6. Grammar (10 points): Total Score: Brief Summary: Figure 23. GPT evaluation prompts for Open-Ended Questions in our benchmark. 22 (a) (c) (e) (b) (d) (f) Figure 24. Illustrative examples from our human evaluation interfaces. 23 (a) GPT-4o (b) Phi3 (c) Phi-3.5 (d) LLaVA-1.57B (e) LLaVA-1.513B (f) LLaVA-NeXT (g) InternVL8B (h) Oryx (i) Qwen2VL7B (j) Qwen2VL72B (k) Dolphin (l) DriveLM Figure 25. Model performance comparisons using radar graphs. The performance for each input corruption type is averaged across all the 1, 261 questions spanning four different tasks using GPT scores. The gray dash line represents the performance of text-only input. We observe VLMs have subtle performance changes under corruptions. For some models, the GPT scores under only text input are even higher than the performance when the visual information is available. 24 Perception task. Under the text-only condition (right example), the model can fabricate Figure 26. Example of Qwen2-VL72B for the answers based on the coordinate information in the question. The GPT evaluator can not reflect the fabricated answer even with detailed rubrics, resulting in the same score as prompted with clean image. 25 Prediction task. Under the snow condition (right example), the model can still observe Figure 27. Example of Qwen2-VL72B for the some visible objects in the images. Since most objects are severely occluded, the third object noticed becomes more general: the building, compared to the clean inputs, where all the objects are visible vehicles. However, the GPT score is even higher. The example illustrates that even the GPT evaluator can not accurately reflect the language quality generated by VLMs. 26 Planning task. Under the text-only condition (right example), the model fails to perceive Figure 28. Example of Qwen2-VL72B for the motorcycle to stop sign, giving their close distance on the image. When the image is absent, the model guesses the object to be pedestrian or stationary object on the road, unexpectedly leading to an answer with higher scores. 27 Perception task. Under the bit error condition (right example), the visual information for the Figure 29. Example of GPT-4o for target object is completely corrupted but the model can guess the answer correctly. However, the GPT evaluator here captures the nuance in the explanation and assigns higher score to the more detailed answer with clean image inputs. 28 Planning task. In both low-light (left example) and normal (right example) conditions, the ground Figure 30. Example of GPT-4o for truth is to turn left. However, in the left case, the vehicle is prepared to turn left. Turning right will cause potential collision with the white sedan on the front right camera. On the right image, the road is empty and the predicted slow-down action will not cause safety issues. However, the GPT evaluator assigns higher score on the left case, ignoring the potential safety consequences as there are no high-fidelity physical context prompts. 29 Figure 31. Examples of the GPT evaluator with different prompts. With the same question and answer, the GPT evaluator assigns different scores based on the rubrics. Without detailed information on the physical context, the scoring point, the evaluator fails to capture critical information for driving decisions in the answer but simply based on semantic similarities. 30 Figure 32. Examples of the GPT evaluators with different information. We gradually add more information about the question and the visual description of the target objects. The evaluator gives more accurate score based on more information. 31 Table 8. Detailed GPT score results of MCQs for the Perception task. Clean represents clean image inputs. T.O. represents textonly evaluation. The Corrupt settings range from weather conditions, external disturbances, sensor failures, motion blur, and transmission errors. The benchmarked VLMs include commercial, open-sourced, and driving specialist models, respectively. l Method . . s h B D S F R a O L l e s a a o a t t u o M B Z r i n r C s p 5 6 2 . Human 47.67 - 43.33 26.67 45.00 18. 40.00 35.33 37.33 42.00 20.00 25. 43.00 20.00 33.67 25.33 31.33 GPT-4o 41.87 43.59 43.84 44.82 45.18 44. 46.20 45.69 44.10 38.12 40.08 39. 41.40 36.67 37.54 38.22 39.37 Phi-3 Phi-3.5 LLaVA-1.57B LLaVA-1.513B LLaVA-NeXT InternVL8B Oryx Qwen2VL7B Qwen2VL72B 35.51 40.22 32.40 33.58 32.98 46.60 17.98 42.64 38.15 6.50 Dolphin DriveLM 22.38 32.65 38.33 32.68 33.25 4.20 52.46 20.87 37.76 21.53 8.35 12.45 35.28 39.22 32.48 33.25 33.85 43.65 16.48 43.08 36.24 10.18 20. 34.15 36.46 32.95 33.25 20.43 44.15 16.88 37.29 37.77 11.08 25.30 38.88 41.41 31.95 33.15 11.62 43.58 16.63 39.72 35.91 10.70 18.98 38.22 43.04 32.43 33.50 16.58 46.02 16.79 41.67 35.78 9.53 24. 37.70 41.40 32.30 33.53 27.33 42.38 14.31 40.87 37.13 10.58 25.95 38.39 40.66 32.88 32.95 18.24 41.48 16.35 40.69 38.14 9.93 22.03 36.75 40.83 32.18 32.93 32.30 43.38 15.85 39.89 38.97 9.80 21. 34.93 37.59 32.93 33.48 26.80 45.32 16.49 39.75 29.48 10.08 21.95 33.89 36.91 31.63 33.40 20.83 49.08 21.44 39.17 25.63 9.95 16.28 37.53 39.86 32.50 33.25 23.50 43.98 21.38 40.85 36.87 11.20 19. 37.80 40.33 32.43 33.45 34.00 41.30 16.36 41.32 36.91 9.85 22.98 39.72 42.30 32.93 33.68 17.75 41.50 21.04 39.62 37.01 10.10 20.93 36.49 36.71 32.48 33.33 24.50 38.25 17.65 34.28 30.90 8.80 19. 37.15 41.49 32.18 33.25 18.03 44.84 18.13 39.90 36.05 10.00 16.25 37.03 41.48 32.63 33.38 26.24 42.13 19.51 41.20 41.48 11.10 26.48 Perception task. Clean represents clean image inputs. T.O. represents Table 9. Detailed Accuracy score results of MCQs for the text-only evaluation. The Corrupt settings range from weather conditions, external disturbances, sensor failures, motion blur, and transmission errors. The benchmarked VLMs include commercial, open-sourced, and driving specialist models, respectively. l Method . . s h B D n i l s n s S a a r C L r r S B t u o o E B u l o e o 5 6 2 . Human 93.33 - 80.00 53.33 80.00 33. 80.00 66.67 73.33 80.00 40.00 46. 80.00 40.00 60.00 46.67 53.33 GPT-4o 59.00 59.50 60.50 63.50 59.00 61. 59.50 58.00 59.00 50.00 51.50 56. 57.50 52.00 51.00 54.00 57.50 Phi3 Phi-3.5 LLaVA-1.57B LLaVA-1.513B LLaVA-NeXT InternVL8B Oryx Qwen2VL7B Qwen2VL72B 54.50 56.50 50.00 50.00 55.00 56.50 51.00 59.00 60.00 7.00 Dolphin DriveLM 40.00 17.50 58.50 50.00 50.00 34.50 23.50 19.00 56.50 23.50 9.00 23.00 55.00 59.00 50.00 50.00 53.50 57.50 53.50 60.00 58.00 5.50 37. 33.00 58.00 50.00 50.00 42.50 61.00 50.50 59.00 60.00 5.50 45.00 50.00 58.00 50.00 50.00 41.50 62.00 52.00 60.00 58.50 5.50 35.00 55.00 59.00 50.00 50.00 44.00 59.50 52.50 59.50 59.50 6.00 46. 59.00 58.50 50.00 50.00 55.00 59.50 52.50 59.00 61.50 4.50 45.50 56.00 60.00 50.00 50.00 42.00 59.50 50.00 59.00 60.00 6.50 40.50 57.50 59.50 50.00 50.00 52.00 60.00 48.00 59.00 59.50 5.50 37. 32.50 58.50 50.00 50.00 31.00 50.50 32.50 58.00 39.00 5.50 39.50 23.50 57.00 50.00 50.00 26.50 31.00 29.00 56.00 29.50 7.50 29.50 58.00 59.50 50.00 50.00 50.50 56.50 52.00 59.00 60.00 6.50 34. 57.50 59.00 50.00 50.00 53.50 56.00 48.00 59.50 59.00 6.00 42.00 49.00 59.50 50.00 50.00 39.00 59.00 36.00 55.00 55.50 5.50 36.50 36.00 57.50 50.00 50.00 44.50 60.00 50.50 54.50 55.50 6.00 36. 49.50 60.50 50.00 50.00 37.50 58.50 52.00 57.00 58.50 5.50 30.50 57.50 58.50 50.00 50.00 49.00 58.50 52.00 59.00 61.00 5.50 47.00 32 Table 10. Detailed GPT score results of MCQs for the Behavior task. Clean represents clean image inputs. T.O. represents textonly evaluation. The Corrupt settings range from weather conditions, external disturbances, sensor failures, motion blur, and transmission errors. The benchmarked VLMs include commercial, open-sourced, and driving specialist models, respectively. e Method . . s h B a S i l s n s S a s a a L r r S B t u o r E n r C s p 5 6 2 . Human 65.00 - 40.00 40. 66.67 51.00 73.33 53.33 56.00 60. 36.67 54.00 67.67 34.67 53.33 73. 53.33 GPT-4o 45.40 50.03 46.27 49. 42.54 41.79 46.55 45.30 47.78 45. 40.44 47.89 39.91 32.25 48.40 50. 41.07 Phi-3 Phi-3.5 LLaVA-1.57B LLaVA-1.513B LLaVA-NeXT InternVL8B Oryx Qwen2VL7B Qwen2VL72B 45.20 36.75 13.60 32.99 48.16 54.58 33.92 49.07 51.26 8.81 Dolphin DriveLM 42.78 40.91 39.16 14.91 32.79 11.92 20.14 23.94 46.93 39.46 7.11 27. 45.98 37.15 12.79 33.34 48.84 32.54 34.19 46.81 52.13 7.17 47.18 44.48 38.14 12.83 33.10 38.82 36.95 37.77 48.75 51.24 9.54 36.30 47.91 37.19 15.57 33.10 15.90 42.10 33.02 48.04 51.64 9.02 40. 45.17 39.53 12.63 31.96 39.13 56.72 32.89 47.64 49.75 6.48 39.18 47.45 38.40 14.06 32.44 47.07 31.53 32.56 48.45 53.18 8.05 40.93 44.22 36.79 13.99 32.56 20.72 31.09 34.16 46.80 52.46 7.95 43. 44.02 37.36 12.79 32.49 47.02 41.65 34.83 49.24 50.81 7.10 40.98 43.65 36.83 14.68 31.87 48.20 50.17 34.51 47.95 51.25 9.29 39.95 44.01 37.98 13.65 31.55 36.67 32.77 34.82 47.19 47.44 8.94 38. 43.51 39.09 13.12 31.84 39.69 43.66 34.05 49.58 51.22 8.02 40.08 42.48 37.70 13.55 33.17 47.36 34.82 33.95 48.83 48.87 8.02 45.68 41.05 38.91 13.83 31.14 39.60 34.90 29.61 41.27 35.72 9.42 38. 43.83 38.27 13.98 33.40 46.99 50.41 35.25 49.72 52.76 8.37 41.10 46.60 38.23 13.44 31.78 28.13 50.78 33.27 47.07 49.77 10.07 33.50 44.30 36.85 13.48 33.72 47.55 41.66 32.33 47.90 48.52 6.32 39. Behavior task. Clean represents clean image inputs. T.O. represents Table 11. Detailed Accuracy score results of MCQs for the text-only evaluation. The Corrupt settings range from weather conditions, external disturbances, sensor failures, motion blur, and transmission errors. The benchmarked VLMs include commercial, open-sourced, and driving specialist models, respectively. l Method . . s h r r o o R a O L a r W r e s m t t u o M l o r B u l n s m 5 6 2 . Human 66.67 - 40.00 46. 66.67 53.33 73.33 53.33 53.33 60. 40.00 53.33 66.67 33.33 53.33 73. 53.33 GPT-4o 25.50 24.00 25.50 25. 21.50 23.50 25.00 26.00 24.00 26. 28.50 24.50 23.50 24.00 26.00 22. 21.50 Phi3 Phi-3.5 LLaVA1.57B LLaVA1.513B LLaVA-NeXT InternVL8B Oryx Qwen2VL7B Qwen2VL72B 26.50 36.50 10.00 32.50 23.00 27.50 21.00 30.00 23.00 0.50 Dolphin DriveLM 44.00 30.00 40.00 9.50 33.00 15.00 21.50 21.00 23.00 36.50 3.50 25. 29.50 37.00 8.50 33.00 23.00 9.00 21.50 29.00 25.50 1.50 48.50 29.50 36.00 8.00 33.00 24.00 14.50 21.50 28.00 24.50 0.00 37.00 28.00 37.00 8.00 32.50 22.00 20.50 21.50 25.00 25.50 1.00 41. 29.50 38.50 7.50 32.50 23.00 25.50 20.50 28.50 22.00 0.00 40.00 28.50 38.00 8.00 32.00 22.50 13.00 21.50 27.50 29.50 0.00 42.50 27.00 36.50 8.50 32.50 24.50 11.50 21.50 25.00 26.00 1.00 43. 30.00 35.50 8.00 32.50 25.50 15.00 21.50 28.50 22.50 1.00 41.00 32.50 37.00 11.00 32.00 27.50 25.00 21.50 31.50 27.00 1.50 41.00 31.00 39.00 10.00 32.50 24.50 17.50 22.00 28.50 25.00 2.50 39. 28.50 39.00 7.50 32.50 26.50 21.00 21.50 33.50 26.00 0.50 40.50 29.50 37.50 9.00 33.00 24.00 11.50 22.00 26.00 22.50 1.00 46.50 23.50 36.50 7.50 31.00 23.00 12.00 21.00 21.50 22.00 1.00 40. 27.50 39.00 11.00 34.00 24.00 28.00 22.50 27.00 28.50 1.00 43.00 31.50 36.50 8.50 31.00 21.50 23.50 21.50 28.50 23.50 0.50 35.00 27.50 36.00 8.00 33.50 25.50 18.50 21.50 30.00 23.50 0.50 41. 33 Table 12. Detailed GPT score results of open-ended questions for the Perception task. Clean represents clean image inputs. T.O. represents text-only evaluation. The Corrupt settings range from weather conditions, external disturbances, sensor failures, motions, and transmission errors. The benchmarked VLMs include commercial, open-sourced, and driving specialist models, respectively. l Method . . s t r D S i e t s s S a r e s m t t r n o B Z r B u r C s p 5 6 2 . GPT-4o 28.87 29. 29.51 28.15 30.19 28.89 28.63 28. 29.42 27.12 28.16 27.96 29.82 32. 27.25 26.44 29.89 Phi-3 Phi-3.5 LLaVA-1.57B LLaVA-1.513B LLaVA-NeXT InternVL8B Oryx Qwen2VL7B Qwen2VL72B 10.26 14.83 14.03 13.13 15.33 18.12 16.07 15.33 22.10 12.68 Dolphin DriveLM 11. 10.38 18.20 11.94 11.50 23.53 14.73 16.07 32.56 13.88 13.67 5.05 10.44 14.24 13.53 12.72 15.49 20.05 15.72 16.75 20.59 12.07 11.30 11.27 13.08 13.31 13.39 14.95 18.82 13.46 14.95 17.00 10.34 10. 10.97 15.36 13.31 12.86 16.61 23.14 11.98 15.16 14.64 12.37 11.21 10.81 16.09 13.61 13.38 15.62 14.28 15.54 15.66 18.77 11.46 9.71 10.99 18.04 13.75 13.05 16.05 27.75 13.73 15.02 20.00 11.73 10. 10.64 16.53 13.91 13.23 15.66 23.43 17.75 15.66 24.19 12.33 10.71 11.32 14.39 13.48 13.59 15.66 26.67 15.86 15.48 20.34 11.04 11.01 10.67 16.42 13.95 15.13 15.19 11.88 13.35 17.31 17.55 12.34 11. 10.01 14.47 12.90 14.98 16.04 17.11 13.29 17.65 15.45 11.39 10.13 11.03 12.88 12.98 12.39 15.16 26.26 14.45 14.52 19.13 11.47 9.38 11.37 14.97 13.35 13.60 16.06 22.02 13.74 15.29 16.10 11.34 10. 12.31 12.96 13.05 13.85 17.92 27.25 13.90 15.20 18.58 10.17 9.03 11.38 14.74 13.36 12.72 15.27 11.54 13.17 15.17 15.97 11.40 11.39 10.33 13.27 12.83 13.43 15.10 29.57 13.40 14.63 18.78 11.35 10. 10.58 18.66 14.28 12.26 15.86 29.77 14.66 17.89 16.36 11.65 10.73 Table 13. Detailed GPT score results of the open-ended questions for Prediction. Clean represents clean image inputs. T.O. represents text-only evaluation. The Corrupt settings range from weather conditions, external disturbances, sensor failures, motions, and transmission errors. The benchmarked VLMs include commercial, open-sourced, and driving specialist models, respectively. l Method . . s h B D S n l s n s S a r e s m e u u o M B Z r B a o o e o 5 6 2 . GPT-4o 51.30 49. 52.15 50.28 47.97 49.66 51.39 50. 53.30 46.62 45.95 49.18 51.90 49. 48.59 47.56 54.36 Phi-3 Phi-3.5 LLaVA-1.57B LLaVA-1.513B LLaVA-NeXT InternVL8B Oryx Qwen2VL7B Qwen2VL72B 40.11 45.13 22.02 36.98 35.07 45.52 48.13 37.89 49.35 32.66 Dolphin DriveLM 44. 22.61 4.92 14.64 23.98 28.36 48.89 12.77 37.77 5.57 39.98 4.70 45.21 46.57 24.79 36.00 37.15 45.73 49.52 40.82 43.89 29.85 46.82 26.28 9.67 20.95 35.59 35.31 40.71 44.33 35.90 43.25 32.31 43. 35.54 48.02 15.97 40.51 37.59 35.75 47.67 38.92 43.74 24.64 42.33 44.05 45.08 15.30 39.23 37.62 38.43 45.77 44.15 44.49 29.92 35.84 34.87 52.16 18.98 38.90 35.44 33.18 47.20 40.15 45.57 31.38 44. 40.90 42.75 16.28 38.11 37.00 38.69 45.90 41.89 41.61 33.41 44.00 33.56 47.02 24.16 38.92 35.87 40.71 50.30 41.57 47.46 31.79 42.59 44.26 47.18 6.11 36.25 36.25 45.00 42.18 36.61 40.89 29.05 46. 38.74 24.66 13.90 36.54 30.10 45.75 44.59 35.87 34.75 30.93 33.56 41.39 23.26 20.30 37.57 40.56 30.03 42.26 41.25 39.38 30.49 29.69 35.08 49.16 25.20 38.10 34.66 39.52 48.21 40.89 48.21 31.59 42. 36.46 28.38 10.56 39.74 39.36 36.55 52.54 39.23 51.15 26.38 19.00 32.25 41.39 11.10 34.28 31.74 40.12 45.56 36.69 40.49 30.13 38.20 32.82 18.02 15.61 37.16 34.07 28.40 43.77 40.52 40.82 25.64 44. 37.57 49.87 23.92 36.31 35.66 30.31 49.61 38.84 46.67 30.62 42.87 34 Table 14. Detailed ROUGE-L score results of open-ended questions for the Predicion task. Clean represents clean image inputs. T.O. represents text-only evaluation. The Corrupt settings range from weather conditions, external disturbances, sensor failures, motion blur, and transmission errors. The benchmarked VLMs include commercial, open-sourced, and driving specialist models, respectively. l Method . . s h B D S F R a O L l e s a a o a t t u o M B Z r i n r C s p 5 6 2 . GPT-4o 19. 18.58 19.77 19.58 19.67 19.71 19. 20.22 19.94 19.89 19.66 19.70 19. 19.48 19.47 19.90 19.67 Phi3 Phi-3.5 LLaVA1.57B LLaVA1.513B qwen2-7b Qwen2VL72B LLaVA1.6-7b InternVL8B Oryx Qwen2VL7B Qwen2VL72B 17.76 19.36 21.18 24.12 25.49 23.42 16.09 13.92 21.03 25.49 23."
        },
        {
            "title": "25.18\nDolphin\nDriveLM 40.00",
            "content": "14.71 18.76 23.21 24.79 24.15 16.10 13.93 13.25 15.43 23.15 16.10 23.64 23.00 17.21 18.37 21.75 24.03 25.64 20.05 16.29 20.57 18.79 25.64 20.05 24.71 37.00 21.55 5.78 22.64 24.04 24.80 18.46 16.99 15.14 17.45 24.80 18.46 24.84 45. 25.81 17.28 22.21 24.09 25.21 17.97 17.02 14.67 16.99 25.21 17.97 25.95 35.00 15.28 15.74 22.17 24.37 24.82 19.08 17.14 13.64 18.04 24.82 19.08 24.98 46.50 23.77 18.24 22.19 24.14 24.86 19.59 16.54 15.15 18.19 24.86 19.59 26.19 45. 18.56 18.38 19.61 24.18 25.17 18.68 16.27 16.39 17.81 25.17 18.68 25.09 40.50 19.27 17.98 22.05 24.35 25.64 18.51 17.08 15.45 17.68 25.64 18.51 25.18 37.50 17.59 17.10 22.53 23.91 25.62 22.05 16.25 13.92 20.38 25.62 22.05 24.52 39. 17.42 12.63 22.36 23.91 25.99 21.17 16.59 16.12 19.56 25.99 21.17 24.75 29.50 16.90 10.84 22.65 23.84 24.74 17.97 18.05 14.47 16.54 24.74 17.97 24.48 34.00 16.75 17.85 21.71 24.04 25.11 18.98 16.69 16.29 18.21 25.11 18.98 24.99 42. 14.36 12.46 20.28 24.59 23.73 19.03 16.93 14.79 17.84 23.73 19.03 24.63 36.50 25.23 17.12 22.46 23.93 25.09 18.89 16.58 14.02 18.20 25.09 18.89 24.87 36.00 26.88 8.50 22.57 24.31 25.31 19.35 17.33 14.26 17.92 25.31 19.35 24.34 30. 22.55 17.03 21.17 24.12 25.35 18.13 16.46 14.07 17.03 25.35 18.13 24.95 47.00 Table 15. Detailed GPT score results of the open-ended questions for the Planning. Clean represents clean image inputs. T.O. represents text-only evaluation. The Corrupt settings range from weather conditions, external disturbances, sensor failures, motions, and transmission errors. The benchmarked VLMs include commercial, open-sourced, and driving specialist models, respectively. n i r w o R a O L l e h C m s m t t u o M B Z r t n r C s p 5 6 2 . l Method . . GPT-4o 75.75 73. 77.56 74.33 73.00 76.58 76.53 75. 74.65 84.08 74.54 74.84 77.34 73. 74.30 76.36 76.82 Phi-3 Phi-3.5 LLaVA-1.57B LLaVA-1.513B LLaVA-NeXT InternVL8B Oyrx Qwen2VL7B Qwen2VL72B 60.03 31.91 29.15 34.26 45.27 53.27 53.57 57.04 61.30 52.91 Dolphin DriveLM 68. 46.88 46.30 32.45 38.85 27.58 34.56 48.26 41.66 53.35 60.98 65.24 61.48 30.57 31.52 33.63 45.64 54.70 55.46 54.19 62.01 51.85 67.25 59.14 28.64 31.49 34.36 44.54 60.02 58.60 58.37 65.31 55.39 67. 64.59 32.88 32.58 35.06 43.55 63.89 58.91 52.70 66.69 53.09 65.72 64.81 28.38 32.42 39.43 44.17 53.69 57.93 58.18 68.15 54.78 63.08 63.47 34.70 31.00 35.18 45.08 60.64 55.04 55.85 67.83 53.92 69. 61.83 30.88 29.81 33.01 44.62 54.31 57.01 53.98 65.67 51.79 69.04 62.31 29.53 31.72 34.80 44.21 56.68 58.35 55.64 65.26 53.57 67.97 58.32 23.16 33.70 37.61 45.69 52.14 52.28 54.71 57.42 55.73 67. 54.63 24.53 35.95 39.32 44.51 46.12 51.05 53.18 56.34 57.81 66.47 63.04 25.47 29.93 32.77 41.17 54.44 55.76 51.93 62.06 55.78 66.25 61.61 32.73 31.20 34.74 45.30 55.94 55.88 55.22 64.20 51.42 67. 58.93 23.00 30.65 32.99 44.57 54.93 53.63 51.73 61.66 50.95 70.17 58.11 28.24 30.05 33.75 43.67 49.08 52.94 56.79 58.23 53.35 68.46 64.04 23.42 30.00 33.33 43.57 57.02 56.00 53.16 59.86 54.89 68. 63.40 29.31 30.61 33.67 45.13 55.13 57.57 56.04 65.31 52.17 68.59 35 Planning task. Clean represents clean image inputs. Table 16. Detailed ROUGE-L score results of open-ended questions for the T.O. represents text-only evaluation. The Corrupt settings range from weather conditions, external disturbances, sensor failures, motion blur, and transmission errors. The benchmarked VLMs include commercial, open-sourced, and driving specialist models, respectively. l Method . . s h B D S F R a O L l e s a a o a t t u o M B Z r i n r C s p 5 6 2 . GPT-4o 6. 6.42 6.54 6.34 6.54 6.54 6. 6.40 6.52 6.47 6.35 6.39 6. 6.40 6.35 6.37 6.67 Phi3 Phi-3.5 LLaVA1.57B LLaVA1.513B LLaVA-NeXT InternVL8B Oryx Qwen2VL7B Qwen2VL72B 9.39 6.19 8.76 8.06 5.69 4.06 21.03 9.61 12. 12.90 Dolphin DriveLM 53.12 10.05 8.27 9.98 8.68 7.75 9.45 15.43 8.33 13.17 14.82 46.83 9.56 6.52 8.93 8.14 5.72 10.37 18.79 9.31 12.13 12.90 51.55 9.26 7.97 8.99 8.09 5.66 11.65 17.45 8.70 11. 13.01 51.26 9.51 6.30 9.18 8.10 5.50 12.07 16.99 8.14 11.45 12.61 49.26 9.62 6.10 8.96 8.06 5.49 4.05 18.04 8.33 10.97 12.82 46.74 9.47 6.12 8.76 8.10 5.60 10.81 18.19 8.69 11. 12.45 52.02 9.65 6.57 9.00 8.03 5.67 10.97 17.81 8.32 11.50 12.81 53.38 9.60 6.59 9.04 7.99 5.60 9.88 17.68 8.82 11.67 12.74 52.77 10.01 7.18 9.06 8.68 5.62 4.16 20.38 9.15 12. 13.56 52.44 10.01 7.59 9.24 8.64 5.88 9.29 19.56 8.81 11.43 13.91 50.91 9.90 5.98 8.88 8.43 5.59 11.02 16.54 8.69 12.13 12.82 48.66 9.40 6.17 8.94 8.10 5.64 10.04 18.21 8.64 11. 12.86 53.28 9.42 7.87 9.27 8.04 5.45 9.23 17.84 9.05 11.55 12.89 52.20 9.07 6.64 9.25 8.39 5.64 4.03 18.20 9.13 12.06 13.22 49.88 10.02 6.04 9.16 8.33 5.64 10.24 17.92 8.77 11. 13.29 52.79 9.36 6.37 8.85 8.24 5.58 9.65 17.03 7.92 11.93 12.69 51.43 36 E. Broader Impact & Limitations In this section, we discuss the broader implications of our study and acknowledge its potential limitations. E.1. Broader Impact Our research focuses on evaluating the reliability of VLMs in autonomous driving, emphasizing three critical perspectives: the models robustness, data quality, and evaluation metrics. The findings reveal concerning tendency of VLMs to fabricate explanations, particularly under conditions of visual degradation. This issue is not limited to autonomous driving but is likely relevant to other VLMembodied systems, such as robotics and other safety-critical cyber-physical systems. For example, VLM-based robots could generate misleading task explanations or actions based on hallucinated understanding, potentially compromising safety and operational reliability. The implications of our work extend beyond autonomous driving, calling for reassessment of benchmark and metric designs to better evaluate the trustworthiness of VLMs in real-world applications. Current benchmarks often fail to account for the complexity and variability of real-world scenarios, particularly in environments where system malfunctions could result in life-threatening consequences. Our study highlights the urgency of addressing these gaps to develop robust, reliable, and interpretable VLMs that can be safely integrated into such systems. Finally, the design of benchmarks, testbeds, and evaluation metrics that accurately capture the reliability and safety implications of applying VLMs to real-world physical systems is of paramount importance. These tools must go beyond traditional performance metrics to consider the nuanced requirements of autonomous systems, such as contextual understanding, interpretability, and robustness against adversarial conditions. E.2. Potential Limitations While this study provides valuable insights, it is essential to recognize its limitations to contextualize the findings: The experimental results are derived exclusively from the DriveLM [59] dataset due to the prohibitive computational cost of large-scale VLM inference and GPT-based evaluations. While DriveLM is comprehensive dataset, its scope may limit the generalizability of our findings to other driving benchmarks or real-world settings. Future work should expand the analysis to additional datasets and environments to validate the observed trends. The lack of detailed contextual data in the DriveLM dataset poses constraint on our evaluations. For instance, the GPT-based assessments rely on limited visual descriptions of key objects, which may not comprehensively capture the broader situational context required for accurate and nuanced evaluations. Expanding datasets to include richer temporal and spatial contexts could improve evaluation fidelity. This study primarily investigates the language-based explanations generated by VLMs. While these insights are crucial for understanding VLM reliability, it remains unclear whether the observations generalize to action models that generate vehicle trajectories or other nonlanguage outputs. Exploring how VLMs visual grounding affects action-oriented tasks, such as trajectory prediction or manipulation control, represents an important direction for future research. The study evaluates finite set of 12 VLMs across specific tasks, metrics, and settings. Although the insights are significant, the scalability of these findings to emerging VLM architectures or more diverse driving scenarios warrants further investigation. By addressing these limitations in future studies, we aim to build more comprehensive understanding of the challenges and opportunities in applying VLMs to autonomous driving and other safety-critical domains. F. Public Resource Used In this section, we acknowledge the use of the following public resources, during the course of this work: nuScenes4 . . . . . . . . . . . . . . . . . . . . . . . . CC BY-NC-SA 4.0 nuScenes-devkit5 . . . . . . . . . . . . . . . . . Apache License 2.0 DriveLM6 . . . . . . . . . . . . . . . . . . . . . . . .Apache License 2.0 Phi-3.5-vision-instruct7 . . . . . . . . . . . . . . . . . . MIT License Phi-3-mini-4k-instruct8 . . . . . . . . . . . . . . . . . . MIT License LLaVA-1.5-7B-hf9 . . . . . . . . . . . . . . . . Apache License 2.0 LLaVA-1.5-13B-hf10 . . . . . . . . . . . . . . Apache License 2.0 LLaVA-v1.6-mistral-7B11 . . . . . . . . . Apache License 2.0 InternVL2-8B12 . . . . . . . . . . . . . . . . . . Apache License 2.0 Qwen2-VL-7B13 . . . . . . . . . . . . . . . . . Apache License 2.0 Qwen2-VL-72B14 . . . . . . . . . . . . . . . . Apache License 2.0 4https://www.nuscenes.org/nuscenes. 5https://github.com/nutonomy/nuscenes-devkit. 6https://github.com/OpenDriveLab/DriveLM 7https : / / huggingface . co / microsoft / Phi - 3 . 5 - vision-instruct 8https://huggingface.co/microsoft/Phi3mini4k-instruct 9https://huggingface.co/llava-hf/llava-1.5-7bhf 10https : / / huggingface . co / llava - hf / llava - 1 . 5 - 13b-hf 11https://huggingface.co/liuhaotian/llavav1.6mistral-7b 12https://huggingface.co/OpenGVLab/InternVL2-8B 13https : / / huggingface . co / Qwen / Qwen2 - VL - 7B -"
        },
        {
            "title": "Instruct",
            "content": "14https : / / huggingface . co / Qwen / Qwen2 - VL - 72B -"
        },
        {
            "title": "Instruct",
            "content": ""
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 2, 3, 7, 9, 10, 18, 20 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 17, 19, 20 [3] Mousumi Akter, Naman Bansal, and Shubhra Kanti Karmaker. Revisiting automatic evaluation of extractive summarization task: Can we do better than rouge? In Findings of the Association for Computational Linguistics, pages 1547 1560, 2022. 3 [4] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image capIn European Conference on Computer Vition evaluation. sion, pages 382398, 2016. 3 [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 2, 3, 18 [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 3 [7] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1162111631, 2020. 4 [8] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. [9] Kai Chen, Yanze Li, Wenhua Zhang, Yanxin Liu, Pengxiang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, Dit-Yan Yeung, Huchuan Lu, and Xu Jia. Automated evaluation of large vision-language models on self-driving corner cases. arXiv preprint arXiv:2404.10595, 2024. 2 [10] Long Chen, Oleg Sinavski, Jan Hunermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. Driving with llms: Fusing object-level vector modality for explainable autonomous driving. In IEEE International Conference on Robotics and Automation, pages 1409314100, 2024. 3, 7 [11] Yuntao Chen, Yuqi Wang, and Zhaoxiang Zhang. Drivinggpt: Unifying driving world modeling and planning with arXiv preprint multi-modal autoregressive transformers. arXiv:2412.18607, 2024. 3 [12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 2, 7, 10, 18, 20 [13] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [14] DriveLM contributors. Drivelm: Driving with graph vihttps : / / github . com / sual question answering. OpenDriveLab/DriveLM, 2023. 4, 7 [15] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, and Ziran Wang. Drive as you speak: Enabling human-like interaction with large language models in autonomous vehicles. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 902909, 2024. 3 [16] Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Luc Van Gool, and Marie-Francine Moens. Talk2car: Taking control of your self-driving car. In Conference on Empirical Methods in Natural Language Processing, pages 20882098, 2019. 3, 5 [17] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. 3, 18 [18] Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov, and Timofey Bryksin. Out of the bleu: how should we assess quality of the code generation models? Journal of Systems and Software, 203:111741, 2023. 3 [19] Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language In International Conference on image pre-training (clip). Machine Learning, pages 62166234. PMLR, 2022. 4, 5, [20] Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. Drive like human: RethinkIn ing autonomous driving with large language models. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 910919, 2024. 2, 3 [21] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 7, 9 38 [22] Ali Goli and Amandeep Singh. Frontiers: Can large language models capture human preferences? Marketing Science, 2024. 3 [23] Xianda Guo, Ruijun Zhang, Yiqun Duan, Yuhang He, Chenming Zhang, Shuai Liu, and Long Chen. Drivemllm: benchmark for spatial understanding with multimodal large arXiv preprint language models in autonomous driving. arXiv:2411.13112, 2024. 5 [24] Xiaoshuai Hao, Mengchuan Wei, Yifan Yang, Haimei Zhao, Hui Zhang, Yi Zhou, Qiang Wang, Weiming Li, Lingdong Kong, and Jing Zhang. Is your hd map constructor reliable under sensor corruptions? In Advances in Neural Information Processing Systems, 2024. [25] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. 3 [26] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1785317862, 2023. 2, 7 [27] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138, 2023. 6, 10 [28] Bo Jiang, Shaoyu Chen, Bencheng Liao, Xingyu Zhang, Wei Yin, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Senna: Bridging large vision-language modarXiv preprint els and end-to-end autonomous driving. arXiv:2410.22313, 2024. 2, 6 [29] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. Textual explanations for self-driving vehicles. In European Conference on Computer Vision, pages 563578, 2018. 2, 3, 5, 14, 17, 18 [30] Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu. Robo3d: Towards robust and reliable 3d perception against corruptions. In IEEE/CVF International Conference on Computer Vision, pages 1999420006, 2023. 3, 5, 16 [31] Lingdong Kong, Shaoyuan Xie, Hanjiang Hu, Lai Xing Ng, Benoit Cottereau, and Wei Tsang Ooi. Robodepth: Robust out-of-distribution depth estimation under corruptions. Advances in Neural Information Processing Systems, 36, 2024. 3, 16 [32] Lingdong Kong, Shaoyuan Xie, Hanjiang Hu, Yaru Niu, Wei Tsang Ooi, Benoit R. Cottereau, Lai Xing Ng, Yuexin Ma, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu, Weichao Qiu, Wei Zhang, Xu Cao, Hao Lu, Ying-Cong Chen, Caixin Kang, Xinning Zhou, Chengyang Ying, Wentao Shang, Xingxing Wei, Yinpeng Dong, Bo Yang, Shengyin Jiang, Zeliang Ma, Dengyi Ji, Haiwen Li, Xingliang Huang, Yu Tian, Genghua Kou, Fan Jia, Yingfei Liu, Tiancai Wang, Ying Li, Xiaoshuai Hao, Yifan Yang, Hui Zhang, Mengchuan Wei, Yi Zhou, Haimei Zhao, Jing Zhang, Jinke Li, Xiao He, Xiaoqiang Cheng, Bingyang Zhang, Lirong Zhao, Dianlei Ding, Fangsheng Liu, Yixiang Yan, Hongming Wang, Nanfei Ye, Lun Luo, Yubo Tian, Yiwei Zuo, Zhe Cao, Yi Ren, Yunfan Li, Wenjie Liu, Xun Wu, Yifan Mao, Ming Li, Jian Liu, Jiayang Liu, Zihan Qin, Cunxi Chu, Jialei Xu, Wenbo Zhao, Junjun Jiang, Xianming Liu, Ziyan Wang, Chiwei Li, Shilong Li, Chendong Yuan, Songyue Yang, Wentao Liu, Peng Chen, Bin Zhou, Yubo Wang, Chi Zhang, Jianhang Sun, Hai Chen, Xiao Yang, Lizhong Wang, Dongyi Fu, Yongchun Lin, Huitong Yang, Haoang Li, Yadan Luo, Xianjing Cheng, and Yong Xu. The robodrive challenge: Drive anytime anywhere in any condition. arXiv preprint arXiv:2405.08816, 2024. 5 [33] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 7 [34] Edouard Leurent. An environment for autonomous driving decision-making. https://github.com/eleurent/ highway-env, 2018. 3 [35] Chunyi Li, Jianbo Zhang, Zicheng Zhang, Haoning Wu, Yuan Tian, Wei Sun, Guo Lu, Xiaohong Liu, Xiongkuo Min, Weisi Lin, et al. R-bench: Are your large multimodal arXiv preprint model robust to real-world corruptions? arXiv:2410.05474, 2024. 4 [36] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Conference on Empirical Methods in Natural Language Processing, pages 292305, 2023. 6 [37] Ye Li, Lingdong Kong, Hanjiang Hu, Xiaohao Xu, and Xiaonan Huang. Is your lidar placement optimized for 3d scene understanding? In Advances in Neural Information Processing Systems, 2024. [38] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning birds-eye-view representation from multi-camera images via spatiotemporal transformers. In European Conference on Computer Vision, pages 118, 2022. 7 [39] Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose Alvarez. Is ego status all you need for openIn IEEE/CVF Conloop end-to-end autonomous driving? ference on Computer Vision and Pattern Recognition, pages 1486414873, 2024. 2, 4, 17 [40] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, 2004. 3, 7, 9, 10, 14, 17, 19 [41] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yansong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondences elicit 3d spacetime understanding in multimodal language model. arXiv preprint arXiv:2408.00754, 2024. 3 [42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, pages 3489234916, 2023. 2, 3 [43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Improved baselines with visual instruction tuning. Lee. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 7, 9, 10, 18, 20 [44] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 3, 7, 9, 10, 13, 18 [45] Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. Aligning with human judgement: The role of pairwise preference in large language model evaluators. arXiv preprint arXiv:2403.16950, 2024. 3 [46] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 7, 10 [47] Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Jiwen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024. 3 [48] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. Dolphins: Multimodal language model for driving. arXiv preprint arXiv:2312.00438, 2023. 2, 3, 6, 7, 10, 12, 18 [49] Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, and Jiachen Li. Drama: Joint risk localization and captioning in driving. In IEEE/CVF Winter Conference on Applications of Computer Vision, pages 10431052, 2023. [50] Jiageng Mao, Yuxi Qian, Junjie Ye, Hang Zhao, and Yue Wang. Gpt-driver: Learning to drive with gpt. arXiv preprint arXiv:2310.01415, 2023. 2, 6 [51] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality not quantity: On the interaction between dataset design and robustness of clip. Advances in Neural Information Processing Systems, 35:2145521469, 2022. 4, 5 [52] OpenDriveLab. Foundation models for autonomous systems, 2024. Accessed: 2024-11-11. 4 [53] Chenbin Pan, Burhaneddin Yaman, Tommaso Nesti, Abhirup Mallik, Alessandro Allievi, Senem Velipasalar, and Liu Ren. Vlp: Vision language planning for autonomous driving. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1476014769, 2024. 2 [54] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Annual Meeting of the Association for Computational Linguistics, pages 311318, 2002. 3, 7, 9, 14, 17 [55] Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. Nuscenes-qa: multi-modal visual question answering benchmark for autonomous driving scenario. In AAAI Conference on Artificial Intelligence, pages 4542 4550, 2024. 2, 3, 5, [56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. 3, 5 [57] Enna Sachdeva, Nakul Agarwal, Suhas Chundi, Sean Roelofs, Jiachen Li, Mykel Kochenderfer, Chiho Choi, and Behzad Dariush. Rank2tell: multimodal driving dataset In IEEE/CVF for joint importance ranking and reasoning. Winter Conference on Applications of Computer Vision, pages 75137522, 2024. 5 [58] Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven Waslander, Yu Liu, and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1512015130, 2024. 2, 6 [59] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with In European Conference graph visual question answering. on Computer Vision, pages 256274, 2024. 2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 17, 18, 19, 37 [60] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, et al. Open-world object manipulation using pre-trained vision-language models. arXiv preprint arXiv:2303.00905, 2023. 3 [61] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24462454, 2020. [62] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024. 2, 3, 5, 6, 12 [63] Ngoc Tran, Hieu Tran, Son Nguyen, Hoan Nguyen, and Tien Nguyen. Does bleu score work for code migration? In IEEE/ACM International Conference on Program Comprehension, pages 165176, 2019. 3 [64] Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. How many unicorns are in this image? safety evaluation benchmark for vision llms. arXiv preprint arXiv:2311.16101, 2023. 6 [65] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluIn IEEE/CVF Conference on Computer Vision and ation. Pattern Recognition, pages 45664575, 2015. 3 [66] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126, 2023. 6 [67] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 3, 7, 9, 10, 15, 18, [68] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose Alvarez. 40 Omnidrive: holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. arXiv preprint arXiv:2405.01533, 2024. 2, 6 [69] Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, and Yu Qiao. Dilu: knowledge-driven approach to autonomous driving with large language models. In International Conference on Learning Representations, 2024. 2, 3 [70] Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Language prompt Xiangyu Zhang, and Jianbing Shen. for autonomous driving. arXiv preprint arXiv:2309.04379, 2023. 3, 5 [71] Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu. Benchmarking and improving birds eye view perception robustness in autonomous driving. arXiv preprint arXiv:2405.17426, 2024. 3, 5, 10, 16 [72] Yiran Xu, Xiaoyin Yang, Lihang Gong, Hsuan-Chu Lin, TzYing Wu, Yunsheng Li, and Nuno Vasconcelos. Explainable object-induced action decision for autonomous vehicles. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95239532, 2020. 2, 3, [73] Yi Xu, Yuxin Hu, Zaiwei Zhang, Gregory Meyer, Siva Karthik Mustikovela, Siddhartha Srinivasa, Eric Wolff, and Xin Huang. Vlm-ad: End-to-end autonomous driving through vision-language model supervision. arXiv preprint arXiv:2412.14446, 2024. 3 [74] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee Wong, Zhenguo Li, and Hengshuang Zhao. Drivegpt4: Interpretable end-to-end autonomous driving via large language model. IEEE Robotics and Automation Letters, 9(10):81868193, 2024. 2, 3, 6 [75] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Haoran Tan, Chencheng Jiang, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language In European programmer from environmental feedback. Conference on Computer Vision, pages 2038. Springer, 2025. 3 [76] Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. survey of large language models for autonomous driving. arXiv preprint arXiv:2311.01043, 2023. 2, 3, 4 [77] Chenyu Yi, Siyuan Yang, Haoliang Li, Yap-peng Tan, and Alex Kot. Benchmarking the robustness of spatial-temporal models against corruptions. In Advances in Neural Information Processing Systems, 2021. 16 [78] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: diverse driving dataset for heterogeneous multitask learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26362645, 2020. 2, 3,"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "S-Lab, Nanyang Technological University",
        "Shanghai AI Laboratory",
        "The University of Hong Kong",
        "University of California, Irvine"
    ]
}