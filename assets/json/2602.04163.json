{
    "paper_title": "BPDQ: Bit-Plane Decomposition Quantization on a Variable Grid for Large Language Models",
    "authors": [
        "Junyu Chen",
        "Jungang Li",
        "Jing Xiong",
        "Wenjie Wang",
        "Qingyao Yang",
        "He Xiao",
        "Zhen Li",
        "Taiqiang Wu",
        "Mengzhao Chen",
        "Zhen Peng",
        "Chaofan Tao",
        "Long Shi",
        "Hongxia Yang",
        "Ngai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization a fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce a shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs a variable quantization grid via bit-planes and scalar coefficients, and iteratively refines them using approximate second-order information while progressively compensating quantization errors to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on a single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry. Code: github.com/KingdalfGoodman/BPDQ."
        },
        {
            "title": "Start",
            "content": "BPDQ: Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models Junyu Chen 1 2 3 Jungang Li 4 Jing Xiong 2 Wenjie Wang 1 3 Qingyao Yang 2 He Xiao 2 Zhen Li 5 Taiqiang Wu 2 Mengzhao Chen 2 Zhen Peng 6 Chaofan Tao 2 Long Shi 1 3 Hongxia Yang 5 Ngai Wong 2 6 2 0 2 4 ] . [ 1 3 6 1 4 0 . 2 0 6 2 : r Abstract Large language model (LLM) inference is often bounded by memory footprint and memory bandwidth in resource-constrained deployments, making quantization fundamental technique for efficient serving. While post-training quantization (PTQ) maintains high fidelity at 4-bit, it deteriorates at 2-3 bits. Fundamentally, existing methods enforce shape-invariant quantization grid (e.g., the fixed uniform intervals of UINT2) for each group, severely restricting the feasible set for error minimization. To address this, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs variable quantization grid via bit-planes and scalar coefficients, and iteratively refines them using approximate second-order information while progressively compensating quantization errors to minimize output discrepancy. In the 2-bit regime, BPDQ enables serving Qwen2.5-72B on single RTX 3090 with 83.85% GSM8K accuracy (vs. 90.83% at 16-bit). Moreover, we provide theoretical analysis showing that the variable grid expands the feasible set, and that the quantization process consistently aligns with the optimization objective in Hessian-induced geometry. Code: github.com/KingdalfGoodman/BPDQ. 1. Introduction Large language models (LLMs) demand substantial memory and compute resources, making efficiency major research 1Southwestern University of Finance and Economics 2The University of Hong Kong 3Artificial Intelligence and Digital Finance Key Laboratory of Sichuan Province 4The Hong Kong University of Science and Technology (Guangzhou) 5The Hong Kong Polytechnic University 6Sun Yat-sen University. Correspondence to: Junyu Chen <223081200039@smail. swufe.edu.cn>, Long Shi <shilong@swufe.edu.cn>, Hongxia Yang <hongxia.yang@polyu.edu.hk>, Ngai Wong <nwong@eee.hku.hk>. Preprint. February 5, 2026. 1 focus in academia and industry (Miao et al., 2023; Zhu et al., 2024). Among efficiency approaches, quantization is fundamental technique that reduces memory footprint and alleviates memory bandwidth bottlenecks during inference (Gong et al., 2024; Zhou et al., 2024). Accordingly, many recent open-source models release low-bit checkpoints. For example, Qwen3 offers an official 4-bit quantized variant (Qwen Team, 2025), suggesting that 4-bit weight-only quantization preserves high fidelity. Specifically, quantizationaware training (QAT) demonstrates promising performance by learning directly in the low-bit space, yet incurs prohibitive training cost (Liu et al., 2023; Chen et al., 2024). Furthermore, quantization-aware fine-tuning (QAF) can improve low-bit performance by fine-tuning quantized model, but it requires two-stage pipeline (Dettmers et al., 2023; Xu et al., 2023; Chen et al., 2025a). For post-training quantization (PTQ), distribution-aware methods utilize weight or activation statistics to reduce distortion induced by outliers (Lin et al., 2024; Ashkboos et al., 2024), which rely on handling outliers during inference. In contrast, optimizationbased methods such as GPTQ (Frantar et al., 2022; Zhang et al., 2025a) are theoretically well grounded by minimizing output discrepancy under an output-aligned objective (e.g., WX (cid:99)WX) (Zhang et al., 2025a; Chen et al., 2025b), while preserving hardware-friendly inference. Nevertheless, pushing precision down to 2-3 bits remains challenging because of limited cardinality (e.g., 2-bit offers only four distinct values), which causes significant representational loss and severe degradation in model quality. In the exploration of low-bit quantization, distribution-aware methods (Huang et al., 2024a;b; Li et al., 2024) apply hybrid formats or mixed precision to protect salient weights, leading to irregular memory access patterns. Vector Quantization (VQ) methods (Liu et al., 2024; Egiazarian et al., 2024) achieve high fidelity by mapping weights to codebooks, but suffer from prohibitive computational costs during codebook optimization. While bit-plane methods (Park et al., 2025; Tran & Nguyen, 2025) enable accelerator-friendly bit-parallel arithmetic, they lack rigorous output-aligned objective and rely on fine-tuning to preserve fidelity. Despite these explorations, optimization-based PTQ mainBit-Plane Decomposition Quantization on Variable Grid for Large Language Models Figure 1. (a) Fixed grids (Uniform/Non-Uniform) enforce shape invariance, where the relative spacing of quantization levels is shared across groups (scaled by s). BPDQ breaks this limitation by constructing variable grid per group using bit-plane coefficients (c1, c2), expanding the feasible set. (b) Performance comparison of 2-bit quantized Qwen2.5-72B. tains rigorous theoretical formulation but fails in the lowbit regime. We attribute this failure to critical misalignment between the optimization objective and the rigid quantization grid. Essentially, the problem is not failure of the optimization objective, but the rigidity of the quantizers feasible set under that objective. As illustrated in Figure 1 (a), fixed uniform grid restricts the per-group feasible values to scale {0, 1, 2, 3}, while fixed non-uniform grid employs scale {a0, a1, a2, a3}. Although the scale varies across groups, the relative spacing pattern of the four levels is shared across all groups, making each group only magnified or shrunken copy of the same template. This shape invariance can be overly restrictive, since the output-aligned objective is the nearest-point projection in the Hessian-induced geometry. To address this limitation, we propose Bit-Plane Decomposition Quantization (BPDQ), which constructs variable grid via bit-planes and scalar coefficients. The right side of Figure 1 (a) shows that BPDQ allows the relative spacing pattern to vary across groups using distinct coefficients. This breaks the shape invariance constraint and enlarges the feasible set under the output-aligned objective. Specifically, BPDQ initializes the variable grid through bit-plane decomposition and closed-form solution for scalar coefficients. Within the Hessian-induced geometry, we iteratively refine the bit-planes and scalar coefficients. Moreover, to maintain error-propagation consistency, we introduce delta correction, ensuring iterations align with the optimization objective. Appendix formalizes how this variable grid expands the feasible solution set, and Appendix formalizes the consistency of BPDQ with Hessian-induced optimality. We validate BPDQ on the Qwen-3/2.5 family (0.6B-72B) and Ministral-3 (3B, 8B) across five language modeling and commonsense benchmarks. Furthermore, we demonstrate BPDQs robustness on quantization-sensitive reasoning tasks and long-context benchmarks. Figure 1 (b) compares 2-bit quantization on Qwen2.5-72B, where GPTQ and AWQ suffer severe degradation (e.g., dropping below 41% on GSM8K) while BPDQ preserves the high fidelity of the full-precision baseline (e.g., 86.13% on GSM8K). In terms of deployment efficiency, BPDQ enables serving the quantized 72B model (W2-G256) on single RTX 3090 (22.69 GB VRAM) with 83.85% accuracy on GSM8K. By implementing bit-plane look-up table (LUT) kernel (Park et al., 2022), we achieve low-latency decoding for real-time interactive generation. Analysis of activation statistics confirms that BPDQ inherently preserves essential outliers, which is crucial for maintaining model quality. Our contributions are summarized as follows: Insight: We identify the shape invariance of fixed quantization grids as the fundamental constraint on optimization-based PTQ in low-bit regimes. To address this, we propose BPDQ, which decomposes weights into bit-planes to construct variable quantization grid, theoretically expanding the feasible solution set for output-aligned error minimization. Methodology: We formulate rigorous optimization framework that extends optimization-based PTQ to variable grids. By iteratively refining bit-planes and scalar coefficients within the Hessian-induced geometry, BPDQ ensures that the optimization process consistently aligns with the optimization objective, as formally established in our theoretical analysis. Performance: Extensive experiments across language understanding, reasoning, and long-context tasks demonstrate BPDQs consistently high fidelity in lowbit regimes. Furthermore, we provide system efficiency profiling to validate the hardware efficiency of bit-plane methods, and confirm that BPDQ inherently preserves critical outliers through activation analysis. 2 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models Figure 2. Overview of the 2-bit BPDQ quantization procedure. 2. Related Work Low-bit Quantization for LLMs. To achieve extreme compression rates, QAT methods optimize in the Boolean domain or utilize factorized representations (Tran & Nguyen, 2025; Lee et al., 2025), albeit at substantial training costs. Among PTQ methods, vector quantization (VQ) maps weights to codebooks (Egiazarian et al., 2024; Liu et al., 2024), preserving high fidelity but suffering from prohibitive quantization overheads. Alternatively, distribution-aware methods employ hybrid formats or mixed precision to protect salient weights (Huang et al., 2024a;b; Li et al., 2024), often causing irregular memory access patterns. Recently, bit-plane and ternary decomposition methods have emerged to enable accelerator-friendly arithmetic (Xiao et al., 2025; Yan et al., 2025; Park et al., 2025). However, these approaches typically rely on progressive residual correction or fine-tuning, lacking rigorous output-aligned objective. Optimization-based PTQ for LLMs. Optimizationbased PTQ minimizes output discrepancy under objectives such as WX (cid:99)WX (Zhang et al., 2025a; Chen et al., 2025b). This perspective traces back to second-order sensitivity analyses (OBD/OBS) (LeCun et al., 1989; Hassibi et al., 1993) and is further developed by Optimal Brain Compression (OBC) (Frantar & Alistarh, 2022). GPTQ employs efficient approximate second-order information for LLM quantization (Frantar et al., 2022). Recent theoretical advances connect GPTQ to Babais nearest-plane algorithm on Hessian-induced lattice, offering geometric interpretation of its error propagation (Chen et al., 2025b). Furthermore, theoretical analyses have established provable error bounds for these procedures (Zhang et al., 2025a), while enhanced sequential solvers like Qronos (Zhang et al., 2025b) have been introduced to integrate past-error correction to further minimize reconstruction loss. However, the rigidity of fixed quantization grids restricts the feasible solution set for optimization-based PTQ, leading to degradation in the low-bit regime. To overcome this restriction, BPDQ constructs variable grid that expands the feasible set, achieving high fidelity at extreme compression rates. 3. Methodology BPDQ follows the optimization objective while replacing the fixed quantization grid with variable grid. Within the Hessian-induced geometry, BPDQ initializes via bit-plane decomposition (BPD) and closed-form scalar coefficient fitting. It then iteratively refines the grid through columnwise bit-plane update and group-wise scalar coefficient refitting with Hessian-aware error compensation. Finally, it applies delta correction to maintain error-propagation consistency. The formal consistency of this procedure with Hessian-induced optimality is established in Appendix B. Specifically, the variable grid quantized weight (cid:99)W is formed by bit-planes and scalar coefficients: (cid:99)W = REP(C0) + (cid:88) i=1 REP(Ci) Bi, (1) where Bi {0, 1}doutdin (for = 1, . . . , k) is the i-th bitplane, Ci Rdout(din/g) is the group-wise scale coefficient with group size g, and C0 Rdout(din/g) is the group-wise bias coefficient. The operator REP() expands Ci along the input dimension by repeating each group coefficient across its columns. The integer is the number of non-bias bit-planes, and is element-wise multiplication. 3.1. Preliminaries Optimization Objective. Consider linear layer with weight Rdoutdin and input activations RdinN 3 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models computed from calibration samples. The quantized weight (cid:99)W is obtained by minimizing the output reconstruction error: them introduces only small truncation error, providing low-error initialization. (cid:99)W = argmin (cid:102)WQ = argmin (cid:102)WQ (cid:13)(W (cid:102)W)X(cid:13) (cid:13) 2 (cid:13) tr(cid:0)(W (cid:102)W) (W (cid:102)W)(cid:1), (2) where = XX is an approximate Hessian metric induced by calibration data, and denotes the set of admissible low-bit weight matrices. Quantization Error Compensation. Due to the enormous parameter space of LLMs, repeatedly updating the inverse Hessian is prohibitively expensive. To address this, GPTQ (Frantar et al., 2022) operates within the Hessianinduced geometry by using the upper-triangular Cholesky factorization = chol(H1) (i.e., H1 = UU), and performs error propagation via triangular updates to compensate the induced quantization error on the remaining free coordinates. When quantizing the l-th column, let W:,l and (cid:99)W:,l denote the current working and quantized column vectors, respectively. Then define the error coordinate: the Scalar Coefficient Fitting. When bit-planes {(Bi):,s:(s+g)}k i=1 are fixed, (cid:99)W is affine in the scalar coefficients, enabling closed-form fit under the Hessianinduced geometry to align with the optimization objective in Eq. (2). Concretely, consider column group W:,s:(s+g), and let Uloc = Us:(s+g),s:(s+g) Rgg be the local triangular factor of this group. For r-th row, define Br = (cid:104) {0, 1}g(k+1), 1, (B1) where 1 is the all-ones column vector. The group-wise coefficient vector cr Rk+1 is obtained by the following row-wise weighted least-squares fit: r,s:(s+g), . . . , (Bk) r,s:(s+g) (cid:105) cr = argmin cRk+ (cid:13) (cid:13)U (cid:13) loc (cid:0)Br r,s:(s+g) (cid:1)(cid:13) 2 (cid:13) (cid:13) 2 . (6) This scalar coefficient fitting is an optimal projection under the Hessian-induced geometry for the fixed bit-planes, yielding coefficients consistent with the output reconstruction objective and inducing the variable grid by cr. In implementation, damping factor α = 104 is applied for numerical stability (omitted in Eq. 6 for brevity). E:,l = W:,l (cid:99)W:,l Ul,l . The Hessian-aware compensation update is: W:, l: W:, l: E:,l Ul, l:, (3) (4) 3.3. Iteration under the Optimization Objective For each group, BPDQ alternates bit-plane update and coefficient refitting, with delta correction for propagation-state consistency. In our experiments, we consistently set the number of iterations to 10, retaining the iterate that minimizes the group-wise propagation error E:, s:(s+g)2 . which maintains the optimization-based PTQ errorpropagation state under the Hessian-induced geometry. 3.2. Variable Grid Initialization Bit-Plane Selection. Consider contiguous column group W:,s:(s+g) Rdoutg, where is the starting column index and is the group size. Applying per-group affine quantizer with round-to-nearest (RTN) to W:, s:(s+g) obtains an unsigned 8-bit integer matrix {0, . . . , 255}doutg. Then admits the bit-plane decomposition (BPD): i=0, the bit-planes {(Bi)}k Bit-plane Update. Given the fixed group-wise scalar coefficients {(Ci):,s/g}k i=1 are updated column by column via exact enumeration under the Hessian-induced geometry. For column {s, . . . , + 1} and row r, enumerating bit vectors = (b1, . . . , bk) {0, 1}k generates 2k candidate values: vr(b) = (C0)r,s/g + (cid:88) i=1 (Ci)r,s/g bi. (7) = 7 (cid:88) i= 2i Pi, (5) The optimal bit vector is selected to minimize the local reconstruction error: where Pi {0, 1}doutg is i-th bit-plane of Z. For bitplane initialization, select the most significant bit (MSB) planes. The bit-planes (Bi):,s:(s+g) = P7k+i for {1, . . . , k}, since the MSB planes capture the dominant magnitude information. The remaining least significant bit (LSB) planes {P7k, . . . , P0} are discarded. Removing = argmin b{0,1}k (cid:0)W r,l vr(b)(cid:1)2 , (8) where :,l denotes the current working column after previous propagation updates. This minimization is executed in parallel for all rows to determine the quantized column vector (cid:99)W:,l. The update is performed column-wise and 4 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models followed by an error propagation step at each column, consistent with the formulation in Eq. (4). Coefficient Refitting. After completing the bit-plane update for the entire group {s, . . . , + 1}, the groupwise scalar coefficients {(Ci):,s/g}k i=0 are refit to match the original weights by solving the row-wise weighted leastsquares problem in Eq. (6) with the updated bit-planes {(Bi):,s:(s+g)}k i=1 fixed. This closed-form refit updates the variable grid while remaining aligned with the Hessianinduced objective in Eq. (2). Delta Correction. After refitting the coefficients, we apply delta correction to keep the error-propagation state consistent. Refitting the coefficients changes the quantized weight block from (cid:99)Wold Rdoutg to (cid:99)Wnew Rdoutg for the current group. Here, (cid:99)Wold is obtained from the bit-plane update step, and (cid:99)Wnew is the updated block after refitting the scalar coefficients. This discrepancy renders the accumulated propagation state inconsistent. Thus, we compute correction using the local triangular factor Uloc: Uloc = (cid:99)Wold (cid:99)Wnew, (9) where Rdoutg is the group-wise correction in the propagation coordinates. The coordinates are updated as :,s:(s+g) = E:,s:(s+g) + E, where E:,s:(s+g) represents the propagation error vectors computed during the bitplane update phase. This delta correction maintains errorpropagation consistency within the Hessian-induced geometry, ensuring that all iterates adhere to the same outputaligned optimization objective. formal equivalence proof is given in Appendix B.3. 4. Experiments 4.1. Experimental Setup Models and Tasks. Experiments are conducted on several large language models, including the Qwen-3 family (0.6B, 4B, 8B, 14B, 32B) (Yang et al., 2025), Qwen-2.5 (7B, 72B) (Qwen Team, 2024), and Ministral-3 (3B, 8B) (Mistral AI, 2025). Quantization quality is assessed using lm-evaluation-harness (Gao et al., 2024) across the following benchmarks: WikiText-2 (Merity et al., 2016), GSM8K (5-shot) (Cobbe et al., 2021), MATH500 (4-shot) (Lightman et al., 2023), ARC-C (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2020), and LongBench (Bai et al., 2024). Baselines and Hyperparameters. BPDQ is implemented within the GPTQModel library (ModelCloud.ai, 2024), which also supports GPTQ (Frantar et al., 2022) and AWQ (Lin et al., 2024). All methods employ asymmetric quantization, calibrated on 1024 samples from the C4 dataset (Raffel et al., 2019). To ensure fair comparisons at similar bits-perweight (BPW), larger group sizes are used in BPDQ to offset the storage overhead introduced by per-bit-plane scalar coefficients. Specifically, GPTQ and AWQ use group size of = 64 for 4-bit and {32, 64} for 2/3-bit, whereas BPDQ uses = 128 for 4-bit and {64, 128} for 2/3-bit. Regarding error propagation, GPTQ utilizes desc act to sort channels in descending order of approximate Hessian values. Meanwhile, BPDQ employs Group-Aware Reordering (GAR) (Gafni et al., 2025) to preserve group integrity for scalar derivation, with the damping factor α set to 104 and iterations set to 10 across all experiments. Additionally, the recent bit-plane method AnyBCQ (Park et al., 2025) and the vector quantization (VQ) method VPTQ (Liu et al., 2024) are included. AnyBCQ follows its paper-recommended settings with fixed-precision configurations at 2-4 bits, while VPTQ is evaluated using officially released checkpoints. 4.2. Main Results Benefits of Variable Grid. Table 1 shows the results across three model sizes (8B, 32B, 72B) and five quantization settings on seven benchmarks. BPDQ yields the best performance in most cases, exhibiting significant lead over GPTQ and AWQ, particularly in the 2-bit regime. Specifically, on reasoning tasks such as GSM8K and MATH500, 2-bit AWQ suffers catastrophic collapse (e.g., 0.00% for the 2-bit 72B model), and 2-bit GPTQ shows severe deterioration. Conversely, BPDQ preserves reasoning capabilities, achieving 87.72% on GSM8K (Qwen2.5-72B W2-G64) and far surpassing GPTQs 63.46%. Notably, although AWQ performs competitively at 3-4 bits by focusing on outlier preservation, its failure at 2-bit suggests that outlier protection alone is insufficient when the quantization grid is extremely coarse. Meanwhile, GPTQ, which shares the same Hessian-based optimization framework as BPDQ, outperforms AWQ at 2-bit but remains constrained by the fixed uniform grid. By relaxing this restriction with variable grid, BPDQ attains superior performance by expanding the feasible solution set, which allows the Hessian-based solver to align more closely with the optimization objective. This comparison validates our insight: the primary restriction at ultra-low bitwidths is not failure of the optimization objective, but the rigidity of the fixed grid. In the extreme compression scenario of W2-G256, BPDQ compresses Qwen2.5-72B to 22.69 GB, unlocking deployment on single RTX 3090. Concurrently, it achieves 83.85% on GSM8K, retaining 92.32% of the baseline accuracy. Moreover, it maintains high fidelity across diverse domains, preserving over 91.01% of the baseline performance on general benchmarks (BoolQ, ARC-C, HellaSwag, MMLU), with BoolQ peaking at 99.15%. 5 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models Table 1. Evaluation results of Ministral3-8B, Qwen3-32B, and Qwen2.5-72B across seven benchmarks. Best and second-best results are highlighted in bold and underlined, respectively. Additional results for other model sizes are provided in Appendix C. Model BPW Wiki2 GSM8K MATH500 ARC-C BoolQ HellaS MMLU Ministral3-8B GPTQ-W4-G64 AWQ-W4-G64 BPDQ-W4-G128 GPTQ-W3-G32 AWQ-W3-G32 BPDQ-W3-G64 GPTQ-W3-G64 AWQ-W3-G64 BPDQ-W3-G128 GPTQ-W2-G32 AWQ-W2-G32 BPDQ-W2-G64 GPTQ-W2-G64 AWQ-W2-G64 BPDQ-W2-G Qwen3-32B GPTQ-W4-G64 AWQ-W4-G64 BPDQ-W4-G128 GPTQ-W3-G32 AWQ-W3-G32 BPDQ-W3-G64 GPTQ-W3-G64 AWQ-W3-G64 BPDQ-W3-G128 GPTQ-W2-G32 AWQ-W2-G32 BPDQ-W2-G64 GPTQ-W2-G64 AWQ-W2-G64 BPDQ-W2-G128 Qwen2.5-72B GPTQ-W4-G64 AWQ-W4-G64 BPDQ-W4-G128 GPTQ-W3-G32 AWQ-W3-G32 BPDQ-W3-G64 GPTQ-W3-G64 AWQ-W3-G64 BPDQ-W3-G128 GPTQ-W2-G32 AWQ-W2-G32 BPDQ-W2-G64 GPTQ-W2-G64 AWQ-W2-G64 BPDQ-W2-G128 BPDQ-W2-G256 16 4.31 4.31 4.63 3.59 3.59 4.00 3.30 3.30 3.50 2.56 2.56 2.75 2.28 2.28 2.38 16 4.31 4.31 4.63 3.59 3.59 4.00 3.30 3.30 3.50 2.56 2.56 2.75 2.28 2.28 2.38 16 4.31 4.31 4.63 3.59 3.59 4.00 3.30 3.30 3.50 2.56 2.56 2.75 2.28 2.28 2.38 2.19 9.72 9.94 9.97 9.95 10.56 10.75 10.49 10.85 11.03 10.68 19.20 5.3E+5 14.69 26.15 1.5E+6 15. 9.34 9.53 10.23 9.52 9.95 10.11 9.86 10.14 10.34 9.97 14.64 8.2E+2 12.34 18.26 3.3E+7 12.97 4.72 5.01 5.64 4.95 5.76 5.57 5.55 6.04 5.85 5.73 10.01 4.0E+7 8.35 12.47 1.6E+7 8.66 8.94 85.90% 84.84% 83.40% 84.99% 79.83% 81.50% 80.06% 76.80% 77.41% 79.15% 12.36% 0.00% 42.46% 1.52% 0.00% 38.74% 74.15% 72.18% 74.47% 76.95% 56.86% 80.14% 86.13% 46.40% 66.19% 67.85% 44.20% 0.00% 80.89% 5.91% 3.18% 70.43% 90.83% 90.52% 91.28% 92.65% 91.74% 90.90% 91.21% 90.07% 90.75% 90.67% 63.46% 0.00% 87.72% 40.49% 0.00% 86.13% 83.85% 54.00% 51.20% 52.40% 51.80% 43.60% 48.60% 45.40% 38.80% 46.60% 43.60% 2.40% 0.00% 17.40% 2.80% 0.00% 12.40% 54.00% 50.40% 53.00% 53.60% 49.60% 50.60% 52.60% 47.40% 51.60% 53.20% 15.80% 0.00% 42.40% 3.80% 7.20% 33.60% 55.80% 56.00% 59.20% 55.40% 51.00% 58.60% 56.40% 50.80% 58.60% 56.60% 28.40% 0.00% 51.20% 14.40% 0.00% 47.60% 39.40% 64.08% 63.82% 62.97% 63.74% 59.47% 61.35% 60.49% 59.73% 60.32% 60.84% 41.47% 35.84% 51.62% 35.58% 26.45% 50.09% 61.01% 60.84% 60.35% 62.54% 57.25% 59.90% 61.18% 56.57% 57.94% 60.41% 39.93% 27.13% 56.83% 32.68% 31.40% 52.56% 63.05% 63.99% 61.26% 62.88% 63.05% 62.54% 62.71% 59.98% 63.74% 62.80% 53.16% 41.47% 59.47% 41.89% 46.50% 60.75% 60.24% 85.78% 85.84% 85.50% 85.90% 84.86% 85.47% 86.88% 84.80% 85.75% 85.84% 62.51% 52.14% 84.04% 53.70% 38.07% 83.52% 86.39% 87.83% 88.35% 87.55% 87.77% 84.13% 88.01% 84.62% 86.24% 88.17% 74.16% 80.64% 86.85% 60.46% 60.09% 87.13% 90.49% 90.76% 90.52% 90.70% 90.24% 90.52% 90.52% 90.12% 90.58% 90.52% 86.21% 68.75% 90.37% 79.79% 72.11% 90.06% 89.72% 78.80% 78.37% 78.24% 78.40% 77.66% 76.18% 76.91% 77.28% 75.88% 76.78% 66.66% 42.80% 68.06% 60.20% 28.50% 67.15% 82.56% 82.36% 81.96% 82.31% 81.34% 80.93% 81.56% 81.14% 80.65% 81.16% 69.95% 67.20% 76.67% 63.84% 47.96% 75.10% 87.35% 87.04% 86.92% 87.21% 86.40% 86.63% 86.73% 86.22% 86.35% 86.36% 78.60% 58.09% 82.71% 74.69% 67.86% 82.20% 81.69% 73.02% 72.71% 72.50% 72.91% 70.48% 69.89% 70.23% 70.07% 69.26% 69.28% 45.39% 26.16% 60.13% 31.90% 26.61% 58.59% 80.69% 79.92% 78.46% 80.07% 78.26% 78.70% 78.83% 77.06% 76.95% 77.89% 50.08% 58.14% 73.24% 36.77% 50.14% 71.31% 83.38% 82.77% 82.14% 83.09% 82.19% 82.01% 81.59% 81.55% 81.58% 81.65% 69.59% 56.94% 77.14% 62.18% 60.23% 76.73% 75.89% Comparison with Bit-Plane and Vector Quantization Methods. In addition to GPTQ and AWQ, the recent bitplane method AnyBCQ (Park et al., 2025) and the vector quantization baseline VPTQ (Liu et al., 2024) are included. In the 2-bit regime  (Table 2)  , BPDQ, AnyBCQ, and VPTQ consistently outperform GPTQ and AWQ. While VPTQ achieves the highest accuracy, it incurs prohibitive quantization overhead ( 40 quantization time relative to GPTQ). 6 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models Table 2. Evaluation of BPDQ, GPTQ, AWQ, AnyBCQ (bit-plane method), and VPTQ (vector quantization) on Qwen2.5-7B. Model SIZE(GB) Wiki2 GSM8K MATH500 ARC-C BoolQ HellaS MMLU Qwen2.5-7B GPTQ-W4-G64 AWQ-W4-G64 AnyBCQ-W4-G128 VPTQ-W4 BPDQ-W4-G128 GPTQ-W3-G32 AWQ-W3-G32 AnyBCQ-W3-G64 VPTQ-W3 BPDQ-W3-G64 GPTQ-W3-G64 AWQ-W3-G64 AnyBCQ-W3-G128 BPDQ-W3-G128 GPTQ-W2-G32 AWQ-W2-G32 AnyBCQ-W2-G64 VPTQ-W2 BPDQ-W2-G64 GPTQ-W2-G64 AWQ-W2-G64 AnyBCQ-W2-G128 BPDQ-W2-G 14.19 5.31 5.31 6.30 5.46 5.54 4.77 4.76 5.82 4.51 5.07 4.54 4.54 5.06 4.69 3.98 3.98 4.30 4.32 4.12 3.77 3.76 3.92 3.84 9.42 9.72 10.35 11.18 9.62 9.66 10.04 10.70 12.24 10.32 10.31 10.27 11.28 12.44 10.55 21.66 N/A 19.20 14.38 15.09 42.59 N/A 22.57 16.85 75.97% 78.32% 78.29% 29.26% 79.45% 78.24% 72.48% 57.16% 26.61% 78.17% 76.42% 63.53% 65.58% 25.63% 71.27% 0.38% 2.43% 9.63% 67.63% 44.50% 0.00% 0.00% 4.47% 35.48% 46.00% 42.20% 45.20% 33.60% 47.00% 41.60% 44.40% 44.60% 25.60% 46.60% 44.00% 39.40% 38.00% 27.40% 40.60% 3.00% 0.00% 5.80% 33.40% 13.60% 1.40% 0.00% 5.80% 10.40% 86.39% 80.44% 55.29% 86.82% 80.00% 54.52% 79.93% 86.24% 55.12% 79.17% 84.83% 50.68% 79.87% 86.73% 54.27% 80.03% 55.80% 86.42% 78.72% 54.78% 83.91% 86.36% 78.09% 51.71% 77.21% 82.39% 50.34% 78.17% 85.96% 51.28% 78.43% 85.90% 54.35% 78.45% 84.68% 52.82% 77.27% 84.95% 50.17% 52.39% 76.77% 82.97% 54.27% 86.21% 77.92% 66.12% 65.02% 34.04% 34.64% 48.98% 45.99% 68.24% 69.97% 45.48% 52.56% 86.79% 73.60% 69.98% 85.50% 48.29% 58.51% 59.14% 29.27% 32.14% 38.81% 25.17% 44.54% 65.83% 77.52% 45.90% 84.62% 68.76% 71.76% 71.16% 71.08% 69.50% 71.33% 71.19% 68.97% 68.58% 66.99% 69.78% 69.90% 67.53% 67.23% 66.59% 69.53% 37.12% 28.30% 54.26% 65.81% 57.51% 27.10% 26.03% 48.54% 57.46% In contrast, BPDQ remains highly efficient ( 3) with 10 iterations across all experiments. Furthermore, as fellow bit-plane method, AnyBCQ also outperforms fixed-grid baselines in the extreme W2-G128 scenario. This confirms that the variable-grid structure offers stronger representation capabilities than fixed-grid data types at ultra-low bits. At 3-bit, BPDQ and VPTQ retain marked lead on reasoning tasks (GSM8K, MATH500), whereas performance gaps narrow on general benchmarks. At 4-bit, most methods achieve high fidelity, with the exception of AnyBCQ, which still faces notable degradation on reasoning tasks. 4.3. Further Analysis of BPDQ System Efficiency Profile. Experiments were conducted on single NVIDIA H20 GPU. As shown in Table 3, BPDQ requires 3 the quantization time of GPTQ due to iteration (10 rounds), yet is far faster than VPTQ, which incurs an estimated 40 overhead. For inference, BPDQ utilizes the Look-Up Table (LUT) kernel (Park et al., 2022) adapted to support its bit-plane format, enabling efficient per-token decoding (Batch Size=1), which targets realtime interactive generation scenario. In contrast, GPTQ utilizes optimized kernels (ExllamaV2 for W4, Torch/Triton for W3/W2). Overall, BPDQ achieves superior decoding latency in 2/3-bit regimes compared to GPTQ. While GPTQW4 also demonstrates competitive latency, it consumes higher VRAM (6.63 GB) due to ExllamaV2s pre-allocated scratch buffers. VPTQ maintains consistent latency across bit-widths but suffers from prohibitive quantization costs. Activation Outlier Statistics. We analyze activation outliers using 128 WikiText-2 sequences and report the results in Table 3. For outlier intensity, DiagR is defined as the max-to-median ratio per layer, and we report the 95th percentile (P95) across all layers. For outlier quantity, Cnt10 counts the number of channels exceeding 10 the median, summed across all layers. Specifically, GPTQ-W2 exhibits severe suppression of outlier features (DiagR -32.89%, Cnt10 -23.61%). In contrast, VPTQ and BPDQ effectively retain these essential outliers under 2-bit quantization. While VPTQ employs expensive outlier protection, BPDQ inherently preserves outliers by extending the feasible set on variable grid. Comparing the 2-bit results in Table 2 and Table 3, we observe positive correlation between outlier preservation and downstream performance, consistent with (Lin et al., 2024; Gu et al., 2024). Long-Context Capabilities. As illustrated in Figure 3, we evaluated the quantized models on subset of LongBench covering retrieval (PassageRetrieval), summarization (GovReport, SAMSum), code completion (RepoBench-P), and classification (TREC). At 3-4 bits, all quantization methods show strong robustness on most tasks, maintaining performance generally comparable to the baseline. significant challenge arises at 2-bit, particularly in the retrieval task, 7 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models Table 3. System efficiency profile and activation outlier statistics on Qwen2.5-7B. Model Qwen2.5-7B GPTQ-W4-G64 VPTQ-W4 BPDQ-W4-G128 GPTQ-W3-G32 VPTQ-W3 BPDQ-W3-G64 GPTQ-W2-G32 VPTQ-W2 BPDQ-W2-G64 Efficiency Profile Outlier Statistics Cost (min) VRAM (GB) Latency (ms) DiagR (P95) DiagR Cnt10 Cnt10 N/A 16 4160 47 16 4160 40 17 4170 40 14.19 6.63 5.46 5.55 4.54 4.51 4.69 3.77 4.32 3.86 14.42 18.74 20.07 18.20 47.67 17.34 18.21 33.91 18.24 18.09 3.01E4 3.28E4 2.83E4 2.95E4 2.82E4 2.59E4 2.96E4 2.02E4 2.68E4 2.86E N/A 4.32E4 +8.97% 4.28E4 -5.98% 4.30E4 -1.99% 4.28E4 -6.31% 4.12E4 -13.95% 4.38E4 -1.66% 4.29E4 -32.89% 3.30E4 -10.96% 4.44E4 -4.98% 4.24E4 N/A -0.93% -0.46% -0.93% -4.63% +1.39% -0.69% -23.61% +2.78% -1.85% VPTQ costs from the paper require 4 GPUs and 10 time relative to the single-GPU GPTQ baseline. Figure 3. LongBench performance comparison on Qwen2.5-7B. which acts as stress test for long-range dependency. GPTQ suffers severe degradation (score drops to 4.98%), indicating the loss of retrieval capabilities. In contrast, BPDQ sustains the performance at 53.75%, whereas VPTQ achieves higher resilience but at the cost of prohibitive quantization overhead. Furthermore, in summarization and classification tasks, BPDQ performs competitively with the baseline under such extreme compression. 5. Conclusion In this paper, we present Bit-Plane Decomposition Quantization (BPDQ) to relax the constraint of shape-invariant grids that hampers optimization-based PTQ in low-bit regimes. Specifically, BPDQ constructs variable quantization grid via bit-plane decomposition, which theoretically expands the feasible solution set and allows for rigorous refinement process within the Hessian-induced geometry. Consequently, BPDQ unlocks high-fidelity 2-bit inference for 72B models on consumer-grade GPUs. By relaxing the rigidity of the quantization grid while maintaining hardwarefriendly format, BPDQ offers promising direction for extreme model compression and efficient deployment. 6. Limitations and Future Work Fidelity Gap and Enhancements. While BPDQ achieves strong performance, fidelity gap remains compared to vector quantization, which often has high overhead and limited hardware support. Future work could address this by incorporating rotation techniques (Ashkboos et al., 2024), or by integrating enhanced sequential solvers like Qronos, to maximize the potential of the optimization framework. Hardware Efficiency on FPGA/ASIC. The binary nature of bit-planes ({0, 1}) is inherently suitable for FPGA or ASIC deployment (Zeng et al.; Hong et al., 2022). This suits custom hardware, as it allows replacing expensive floatingpoint multiplications with simple additions, significantly improving energy and area efficiency. Mixedand Multi-Precision. BPDQs unified basis inherently surpasses conventional mixed-precision schemes. Instead of requiring hardware support for diverse data types, BPDQ achieves mixed precision simply by allocating more or fewer bit-planes. Furthermore, this structure naturally supports multi-precision serving (Park et al., 2025), enabling dynamic accuracy-latency trade-offs by serving multiple precisions from single on-device model. 8 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Ashkboos, S., Mohtashami, A., Croci, M., Li, B., Cameron, P., Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems, 37: 100213100240, 2024. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: Long papers), pp. 31193137, 2024. Chen, J., Li, J., Peng, Z., Wang, W., Ren, Y., Shi, L., and Hu, X. Lota-qaf: Lossless ternary adaptation for quantizationaware fine-tuning. arXiv preprint arXiv:2505.18724, 2025a. Chen, J., Shabanzadeh, Y., Crnˇcevic, E., Hoefler, T., and Alistarh, D. The geometry of llm quantization: Gptq arXiv preprint as babais nearest plane algorithm. arXiv:2507.18553, 2025b. Chen, M., Shao, W., Xu, P., Wang, J., Gao, P., Zhang, K., and Luo, P. Efficientqat: Efficient quantizationaware training for large language models. arXiv preprint arXiv:2407.11062, 2024. large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024. Frantar, E. and Alistarh, D. Optimal brain compression: framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35:44754488, 2022. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022. Gafni, T., Karnieli, A., and Hanani, Y. Dual precision quantization for efficient and accurate deep neural networks inference. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 32593269, 2025. Gao, L., Tow, J., Abbasi, B., et al. The language model evaluation harness, 07 2024. URL https://zenodo. org/records/12608602. Gong, R., Ding, Y., Wang, Z., Lv, C., Zheng, X., Du, J., Qin, H., Guo, J., Magno, M., and Liu, X. survey of low-bit large language models: Basics, systems, and algorithms. arXiv preprint arXiv:2409.16694, 2024. Gu, X., Pang, T., Du, C., Liu, Q., Zhang, F., Du, C., Wang, Y., and Lin, M. When attention sink emerges in language models: An empirical view. arXiv preprint arXiv:2410.10781, 2024. Hassibi, B., Stork, D., and Wolff, G. Optimal brain surgeon: Extensions and performance comparisons. Advances in neural information processing systems, 6, 1993. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36, 2023. Egiazarian, V., Panferov, A., Kuznedelev, D., Frantar, E., Babenko, A., and Alistarh, D. Extreme compression of Hong, S., Moon, S., Kim, J., Lee, S., Kim, M., Lee, D., and Kim, J.-Y. Dfx: low-latency multi-fpga appliance for accelerating transformer-based text generation. In 2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO), pp. 616630. IEEE, 2022. Huang, W., Liu, Y., Qin, H., Li, Y., Zhang, S., Liu, X., Magno, M., and Qi, X. Billm: Pushing the limit of post-training quantization for llms. arXiv preprint arXiv:2402.04291, 2024a. Huang, W., Qin, H., Liu, Y., Li, Y., Liu, Q., Liu, X., Benini, L., Magno, M., Zhang, S., and Qi, X. Slim-llm: Saliencedriven mixed-precision quantization for large language models. arXiv preprint arXiv:2405.14917, 2024b. 9 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models LeCun, Y., Denker, J., and Solla, S. Optimal brain damage. Advances in neural information processing systems, 2, 1989. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github. io/blog/qwen2.5/. Lee, B., Kim, D., You, Y., and Kim, Y. Littlebit: Ultra lowbit quantization via latent factorization. arXiv preprint arXiv:2506.13771, 2025. Li, Z., Yan, X., Zhang, T., Qin, H., Xie, D., Tian, J., Kong, L., Zhang, Y., Yang, X., et al. Arb-llm: Alternating refined binarizations for large language models. arXiv preprint arXiv:2410.03129, 2024. Lightman, H., Kosaraju, V., Burda, Y., et al. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. Liu, Y., Wen, J., Wang, Y., Ye, S., Zhang, L. L., Cao, T., Li, C., and Yang, M. Vptq: Extreme low-bit vector posttraining quantization for large language models. arXiv preprint arXiv:2409.17066, 2024. Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. Merity, S., Xiong, C., Bradbury, J., and Socher, R. arXiv preprint Pointer sentinel mixture models. arXiv:1609.07843, 2016. Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Jin, H., Chen, T., and Jia, Z. Towards efficient generative large language model serving: survey from algorithms to systems. arXiv preprint arXiv:2312.15234, 2023. Mistral AI. Introducing mistral 3, 2025. URL https: //mistral.ai/news/mistral-3. ModelCloud.ai. Gpt-qmodel. com/modelcloud/gptqmodel, 2024. qubitium@modelcloud.ai. https://github. Contact: Park, G., Park, B., Kim, M., Lee, S., Kim, J., Kwon, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022. Qwen Team. Qwen documentation, 2025. URL https: //qwen.readthedocs.io. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. Exploring the limits of transfer learning with unified text-to-text transformer. arXiv: Learning,arXiv: Learning, 2019. Tran, B.-H. and Nguyen, V. M. Highly efficient and effective llms with multi-boolean architectures. arXiv preprint arXiv:2505.22811, 2025. Xiao, H., Yang, R., Yang, Q., Xu, W., Li, Z., Su, Y., Liu, Z., Yang, H., and Wong, N. Ptqtp: Post-training quantization to trit-planes for large language models. arXiv preprint arXiv:2509.16989, 2025. Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen, Z., Zhang, X., and Tian, Q. Qa-lora: Quantizationaware low-rank adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023. Yan, X., Bao, C., Li, Z., Zhang, T., Yang, K., Qin, H., Xie, R., Sun, X., and Zhang, Y. Pt2-llm: Post-training ternarization for large language models. arXiv preprint arXiv:2510.03267, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zeng, S., Liu, J., Dai, G., et al. Flightllm: Efficient large language model inference with complete mapping flow on fpgas. In Proceedings of the 2024 ACM/SIGDA International Symposium on Field Programmable Gate Arrays. Zhang, H., Zhang, S., Colbert, I., and Saab, R. Provable post-training quantization: Theoretical analysis of optq and qronos. arXiv preprint arXiv:2508.04853, 2025a. Zhang, S., Zhang, H., Colbert, I., and Saab, R. Qronos: Correcting the past by shaping the future... in post-training quantization. arXiv preprint arXiv:2505.11695, 2025b. Zhou, Z., Ning, X., Hong, K., et al. survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294, 2024. Park, G., Bae, J., Kwon, B., Kim, B., Kwon, S. J., and Lee, D. Anybcq: Hardware efficient flexible binary-coded quantization for multi-precision llms. arXiv preprint arXiv:2510.10467, 2025. Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. survey on model compression for large language models. Transactions of the Association for Computational Linguistics, 12:15561577, 2024. 10 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models A. Analysis of the Variable Grid A.1. Optimization-based PTQ as an H-Metric Projection Consider weight vector Rg within quantization group of size g. Let Rgg denote the corresponding Hessian eHe. Optimization-based PTQ determines quantized vector matrix. We define the Hessian-induced norm as eH = (cid:98)w from feasible set Rg that minimizes the output discrepancy. This is equivalent to finding the projection of onto under the H-metric: (cid:98)w = Π(H) (w) = argmin (cid:101)wQ (cid:101)w2 = argmin (w (cid:101)w)H(w (cid:101)w). (cid:101)wQ (10) Eq. (10) establishes that optimization-based PTQ is nearest-point projection problem. Consequently, quantization quality is restricted by the geometric richness of the feasible set Q. In the low-bit regime (e.g., 2-3 bits), fidelity degradation stems not from failure of the objective, but from the rigidity of the feasible set induced by shape-invariant grid. A.2. Feasible Set Comparison at 2-Bit Precision The distinction between fixed and variable grids lies in the geometric degrees of freedom defining the quantization levels within each group. fixed grid constrains the levels to shape-invariant template governed by scaling factor, restricting the solution to lower-dimensional manifold. In contrast, BPDQ constructs the grid using independent scalar coefficients for each group, decoupling the quantization intervals and expanding the feasible set to higher-dimensional geometry. Fixed Grid (Rigid Template). Given canonical template = [t0, t1, t2, t3] R4 (e.g., [0, 1, 2, 3] for UINT2), fixed grid restricts the quantization levels R4 to scaling factor of this template. For group scale R: Qfix(s) = {t0, t1, t2, t3}, (11) where varies across groups, but the relative ratios between levels (e.g., q2/q1 = (s t2)/(s t1) = t2/t1, t1 = 0) remain frozen. This rigidity restricts the feasible level vectors to one-dimensional ray in R4. Variable Grid (Adaptive Geometry). BPDQ constructs the grid via two bit-planes weighted by coefficients c1, c2 R: Qvar(c1, c2) = {0, c1, c2, c1 + c2}, (12) where c1 and c2 are independent coefficients determined per group, enabling dynamic relative ratios (i.e., c2/c1 is flexible, c1 = 0). This flexibility expands the feasible level vectors to two-dimensional plane in R4. Proposition 1: Strict Inclusion of Uniform Grids. The feasible set of BPDQ strictly contains the feasible set of standard UINT2 grids. Proof. Without loss of generality, factoring out the group-wise bias (available to both schemes), we consider the canonical zero-based template tuni = {0, 1, 2, 3}. Accordingly, any uniform grid is essentially scaled instance of this template, given by {0, 1, 2, 3} = {0, s, 2s, 3s}. To show inclusion (Quniform QBPDQ), we set the coefficients c1 = and c2 = 2s: Qvar(s, 2s) = {0, s, 2s, + 2s} = {0, s, 2s, 3s} Quni fix (s). (13) This confirms that BPDQ can exactly reproduce any UINT2 grid. Furthermore, the inclusion is strict because BPDQ admits non-uniform spacings (e.g., when c2 = 10c1) that no linear scale can represent. Consequently, the quantization error of BPDQ is upper-bounded by that of UINT2: min qQvar q min qQuni fix q2 H. 11 (14) Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models Proposition 2: Strict Error Reduction via Variable-Grid Expressivity. Compared to shape-invariant fixed-template quantization grids parameterized only by per-group bias c0 and scale s, 2-bit BPDQ induces additional degrees of freedom and can realize feasible points unattainable by rigid templates. Consequently, there exists non-empty open set of weight vectors Rg where BPDQ achieves strictly lower quantization error. Proof. Assume group size 3 and Hessian 0. Define the feasible sets for the fixed grid (Sfix) and BPDQ (Svar): Sfix = {c01 + sz c0, R, {t0, . . . , t3}g}, Svar = {c01 + c1b1 + c2b2 c0, c1, c2 R, b1, b2 {0, 1}g}. (15) (16) For any specific pattern (or bit-planes b1, b2), the generated vectors form an affine subspace. Since the number of patterns is finite (4g), both Sfix and Svar are finite unions of affine subspaces, and are thus closed sets. Construction of v: Define the finite set of difference ratios for = [t0, t1, t2, t3] R4: R(t) = (cid:26) ti tj ti tk (cid:12) (cid:12) (cid:12) ti, tj, tk {t0, . . . , t3}, ti = tj, ti = tk, tj = tk (cid:27) . (17) For any Sfix, if attains three distinct values x, y, across three coordinates (so ti = tj, ti = tk and tj = tk when = 0), they must satisfy = c0 + sti, = c0 + stj, = c0 + stk. The bias and scale cancel out in the difference ratio: (x y)/(x z) = (ti tj)/(ti tk) R(t). In contrast, BPDQ can generate vector containing values {c0, c0 + c1, c0 + c2} by selecting bit-planes such that three coordinates take patterns (0, 0), (1, 0), (0, 1). Let these three values be = c0, = c0 + c1, = c0 + c2. The difference ratio becomes: = c0 (c0 + c1) c0 (c0 + c2) = c1 c2 . (18) Since R(t) is finite set, we can choose c1, c2 such that the ratio c1/c2 / R(t) and the resulting values x, y, are distinct. Suppose for contradiction that Sfix. As attains the distinct values x, y, at the chosen coordinates, the fixed-grid constraint would necessitate that their difference ratio falls within R(t), which contradicts our construction. Thus, Svar but / Sfix. Strict Inequality over an Open Set: Consider weight vector = v, for which the BPDQ error is zero (i.e., Fvar(v) = 0). Since 0 induces norm equivalent to the Euclidean norm on Rg, and Sfix is closed set with / Sfix, the distance is strictly positive: Ffix(v) = inf qSfix q = δ > 0. (19) Define the error difference function (w) = Ffix(w) Fvar(w). Since Sfix and Svar are closed sets, their respective squareddistance functions Ffix(w) and Fvar(w) are continuous. Specifically, the distance-to-set function d(w, S) is 1-Lipschitz under H, and squaring preserves continuity. Therefore, (w) is continuous, so there exists an open neighborhood around where (w) > 0 (i.e., Ffix(w) > Fvar(w)). This confirms the existence of strict inequality over an open set. Remark: Geometric Degrees of Freedom. Proposition 1 establishes strictly nested feasibility for uniform grids (Suni SBPDQ), and Proposition 2 addresses general fixed templates. Geometrically, specific choice of bit-planes (b1, b2) in BPDQ yields an affine subspace of dimension up to 3 (spanning 1, b1, b2), whereas fixed templates yield subspaces of dimension at most 2 (spanning 1, z). In the generic case where (1, b1, b2) are linearly independent, this provides an additional coefficient degree of freedom (3 parameters (c0, c1, c2) vs. 2 parameters (c0, s)). Crucially, the fixed grid enforces global rigidity across all groups, whereas the variable grid enables adaptability tailored for each group. 12 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models B. Consistency in Hessian-Induced Geometry B.1. Consistency of Coefficient Fitting Proposition. The coefficient fitting objective in Eq. (6) is theoretically equivalent to minimizing the local contribution to the optimization objective in Eq. (2) corresponding to the current group, under the Hessian-induced geometry defined by the upper-triangular Cholesky factor. Proof. The optimization objective minimizes the output reconstruction error defined by the Hessian H: = tr (cid:0)(W (cid:99)W)H(W (cid:99)W)(cid:1). (20) Substituting the Cholesky factorization of the inverse Hessian H1 = UU (i.e., = U1U), we rewrite the objective using the definition of the Frobenius norm: = (cid:13) = (cid:13) (cid:13)(W (cid:99)W)U1(cid:13) 2 (cid:13) (cid:13)U(W (cid:99)W)(cid:13) 2 . (cid:13) (21) This reveals that the error measures the magnitude of the weight residual projected onto the geometry defined by the inverse Cholesky factor. When determining the coefficients for column group W:,s:(s+g), we consider the corresponding local block Uloc = Us:(s+g),s:(s+g) Rgg. Accordingly, the local contribution to the error is: Lloc = (cid:13) (cid:13)U loc (W:,s:(s+g) (cid:99)W:,s:(s+g))(cid:13) 2 . (cid:13) (22) Since the Frobenius norm decomposes row-wise, we minimize the error for each row independently. In BPDQ, the r,s:(s+g) = Brcr, where cr Rk+1 is the coefficient vector. Substituting this quantized row vector is parameterized as (cid:99)W parameterization and the original weight vector r,s:(s+g) into the row-wise objective yields: argmin crRk+1 (cid:13) (cid:13)U loc (Brcr r,s:(s+g))(cid:13) 2 2. (cid:13) (23) This matches exactly the weighted least-squares problem defined in Eq. (6). Thus, solving Eq. (6) minimizes the optimization objective over the group coefficients within the Hessian-induced geometry. In implementation, damping factor α = 104 is applied to the diagonal for numerical stability (omitted in the derivation for brevity). B.2. Consistency of Bit-Plane Update Proposition. During the bit-plane update (with fixed coefficients), the element-wise enumeration within column minimizes the column-wise error term of the optimization objective in Eq. (2). Under the error propagation mechanism, this elementwise Euclidean projection ensures consistency with the Hessian-induced geometry. Proof. As derived in Eq. (21), the objective is equivalent to minimizing the projected residual norm = (W (cid:99)W)U12 . The error propagation mechanism in Eq. (4) establishes the relationship (W (cid:99)W) = EU, where is the matrix collecting the error coordinates E:,l defined in Eq. (3). Substituting this relationship into the objective L: = (cid:13) (cid:13)(EU)U1(cid:13) = (cid:13) 2 (cid:13) (cid:13)E(cid:13) 2 = (cid:13) din(cid:88) l=1 E:,l2 2. (24) For the l-th column, the objective reduces to minimizing the error term E:,l2 Based on the definition in Eq. (3), the error coordinate for the current working column 2 conditioned on the current propagation state. :,l is: E:,l = :,l (cid:99)W:,l Ul,l . (25) Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models Assuming 0, the diagonal element Ul,l is strictly positive scalar constant independent of the quantization choice (cid:99)W:,l. Therefore, minimizing the column-wise error coordinate E:,l2 2 is strictly equivalent to minimizing the Euclidean distance in the weight space: (cid:99)W:,l = argmin (cid:102)W:,l (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) :,l (cid:102)W:,l Ul,l (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 = argmin :,l (cid:102)W:,l2 2. (cid:102)W:,l (26) Since the group-wise coefficients are fixed, this problem decouples into dout independent scalar nearest-neighbor projections. For each coordinate of the column, the value (cid:102)Wr,l must be selected from the set of values vr(b) generated by the bit vectors {0, 1}k following Eq. (7). Thus, the problem simplifies to finding the optimal bit vector for each coordinate: = argmin b{0,1}k (W r,l vr(b))2. (27) This matches Eq. (8), confirming that the element-wise Euclidean nearest-neighbor projection minimizes the column-wise error, and maintains consistency with the Hessian-induced geometry. B.3. Consistency of Delta Correction Proposition. The delta correction in Eq. (9) preserves the error-propagation consistency following coefficient refitting. Proof. We partition the global index space into the current group columns {s : (s + g)} and the tail columns {(s + g) : din}. Recall the propagation invariant (W (cid:99)W) = EU from Appendix B.2 and let Uloc = Us:(s+g),s:(s+g) Rgg. To preserve the local consistency within the group columns, we utilize the upper triangular structure to decompose the residual: W:,s:(s+g) (cid:99)W:,s:(s+g) = (EU):,s:(s+g) = E:,:sU:s,s:(s+g) + E:,s:(s+g)Uloc. (28) By rearranging Eq. (28), we isolate the components that remain constant during coefficient refitting (i.e., original weights and historical errors): W:,s:(s+g) E:,:sU:s,s:(s+g) (cid:124) (cid:123)(cid:122) (cid:125) Constant = (cid:99)W:,s:(s+g) + E:,s:(s+g)Uloc. (29) Since the left side of Eq. (29) is constant, the right side must be equal for the bit-plane update state (old) and the refitted state (new). Subtracting the expression for the new state from the old state: (Enew :,s:(s+g) Eold :,s:(s+g))Uloc = (cid:99)Wold (cid:99)Wnew. (30) This strictly derives the delta correction Uloc = (cid:99)Wold (cid:99)Wnew in Eq. (9). To preserve the tail consistency, we expand the tail working weights the decomposition analogous to Eq. (28): :,(s+g): based on the error propagation in Eq. (4) and :,(s+g): = W:,(s+g): (cid:88) E:,jUj,(s+g): (cid:124) j<s (cid:123)(cid:122) History (Constant) (cid:125) E:,s:(s+g)Us:(s+g),(s+g): (cid:125) (cid:123)(cid:122) Current Group (Varying) (cid:124) . (31) The first term accounts for the original weights and errors from preceding groups (j < s), which remain constant during the current groups coefficient refitting. The change in the working weights is derived by differencing Eq. (31) between the new and old states: Wnew :,(s+g): Wold :,(s+g): = (Enew :,s:(s+g) Eold :,s:(s+g))Us:(s+g),(s+g): = Us:(s+g),(s+g):. Thus, applying the update exactly synchronizes the propagation state for the tail columns. 14 (32) Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models C. Additional Evaluation Results Table 4. Evaluation results of Qwen3-0.6B, Ministral3-3B, and Qwen3-4B across seven benchmarks. Best and second-best results are highlighted in bold and underlined, respectively. Model BPW Wiki2 GSM8K MATH500 ARC-C BoolQ HellaS MMLU Qwen3-0.6B GPTQ-W4-G64 AWQ-W4-G64 BPDQ-W4-G128 GPTQ-W3-G32 AWQ-W3-G32 BPDQ-W3-G64 GPTQ-W3-G64 AWQ-W3-G64 BPDQ-W3-G128 GPTQ-W2-G32 AWQ-W2-G32 BPDQ-W2-G64 GPTQ-W2-G64 AWQ-W2-G64 BPDQ-W2-G128 Ministral-3-3B GPTQ-W4-G64 AWQ-W4-G64 BPDQ-W4-G128 GPTQ-W3-G32 AWQ-W3-G32 BPDQ-W3-G64 GPTQ-W3-G64 AWQ-W3-G64 BPDQ-W3-G128 GPTQ-W2-G32 AWQ-W2-G32 BPDQ-W2-G64 GPTQ-W2-G64 AWQ-W2-G64 BPDQ-W2-G128 Qwen3-4B GPTQ-W4-G64 AWQ-W4-G64 BPDQ-W4-G128 GPTQ-W3-G32 AWQ-W3-G32 BPDQ-W3-G64 GPTQ-W3-G64 AWQ-W3-G64 BPDQ-W3-G128 GPTQ-W2-G32 AWQ-W2-G32 BPDQ-W2-G64 GPTQ-W2-G64 AWQ-W2-G64 BPDQ-W2-G128 16 4.31 4.31 4.63 3.59 3.59 4.00 3.30 3.30 3.50 2.56 2.56 2.75 2.28 2.28 2.38 16 4.31 4.31 4.63 3.59 3.59 4.00 3.30 3.30 3.50 2.56 2.56 2.75 2.28 2.28 2. 16 4.31 4.31 4.63 3.59 3.59 4.00 3.30 3.30 3.50 2.56 2.56 2.75 2.28 2.28 2.38 26.09 28.61 29.53 28.58 35.15 37.43 34.38 38.14 44.48 38.40 3.7E+2 5.6E+6 1.2E+2 1.2E+3 5.8E+7 1.3E+2 11.70 12.09 12.22 12.10 13.30 13.83 13.16 13.90 14.41 13.52 37.76 9.3E+5 21.01 69.48 1.8E+6 23.46 13.07 13.28 13.60 13.57 14.05 14.95 14.67 14.53 16.02 14.79 23.37 1.8E+8 21.40 34.80 8.5E+8 23.93 41.02% 30.10% 36.24% 31.54% 18.95% 9.63% 14.03% 5.69% 3.64% 7.43% 0.00% 0.00% 0.15% 0.00% 0.00% 0.30% 73.16% 70.43% 70.05% 70.13% 65.05% 61.26% 65.43% 61.79% 54.21% 58.83% 1.06% 0.00% 21.99% 0.61% 0.00% 17.36% 85.75% 85.44% 85.44% 81.50% 70.13% 79.91% 77.71% 73.39% 79.38% 61.87% 0.61% 0.30% 34.19% 0.00% 0.00% 24.87% 33.70% 30.80% 31.91% 32.08% 31.23% 29.10% 27.90% 29.86% 26.71% 29.52% 23.29% 26.28% 23.55% 22.70% 27.30% 21.93% 60.41% 59.56% 59.56% 58.11% 54.86% 54.69% 56.40% 55.55% 55.55% 53.75% 26.71% 25.09% 41.47% 26.54% 24.57% 40.27% 58.62% 57.76% 57.51% 59.22% 56.23% 54.01% 57.42% 55.03% 49.15% 55.12% 33.96% 33.45% 42.92% 28.24% 26.62% 40.19% 63.82% 64.71% 66.91% 57.43% 66.70% 49.79% 55.87% 56.73% 58.50% 64.13% 40.49% 47.19% 62.29% 39.11% 51.25% 60.95% 84.10% 83.33% 83.06% 84.13% 82.51% 79.60% 83.33% 81.01% 80.83% 81.44% 49.69% 51.35% 72.08% 56.18% 38.01% 78.38% 84.74% 84.28% 83.88% 84.86% 84.22% 82.14% 84.56% 83.15% 81.87% 83.52% 52.81% 55.81% 78.78% 50.15% 46.36% 80.61% 47.30% 46.33% 45.57% 45.88% 42.86% 42.67% 43.06% 41.90% 40.81% 42.07% 27.95% 27.05% 34.09% 26.04% 26.07% 32.66% 73.45% 72.88% 72.50% 72.78% 71.19% 69.93% 70.46% 70.55% 68.87% 70.14% 46.40% 35.90% 59.31% 37.70% 27.43% 57.23% 69.08% 68.60% 68.58% 68.87% 67.19% 65.52% 66.44% 66.24% 64.32% 65.81% 52.04% 45.85% 56.81% 44.36% 28.97% 55.09% 40.39% 34.05% 41.57% 29.65% 25.35% 36.13% 36.88% 30.49% 34.90% 28.03% 24.61% 24.79% 23.35% 24.73% 25.92% 25.36% 67.41% 65.25% 65.79% 66.09% 62.74% 63.35% 63.39% 61.87% 62.41% 61.95% 25.23% 24.18% 49.67% 23.68% 25.51% 45.24% 70.57% 69.29% 69.35% 69.80% 66.19% 66.71% 67.06% 65.96% 64.46% 66.70% 27.45% 36.03% 53.67% 24.47% 25.42% 52.49% 28.60% 22.40% 22.40% 18.60% 6.40% 6.80% 6.00% 4.00% 5.00% 4.60% 1.80% 0.00% 2.40% 1.60% 0.20% 1.60% 40.00% 41.40% 42.60% 37.40% 30.60% 30.00% 34.80% 28.00% 29.60% 32.80% 3.00% 0.00% 5.60% 2.40% 0.00% 6.20% 52.60% 50.00% 48.80% 51.20% 44.80% 47.80% 46.40% 38.00% 43.80% 40.60% 1.80% 0.00% 8.80% 1.20% 0.00% 4.80% 15 Bit-Plane Decomposition Quantization on Variable Grid for Large Language Models Table 5. Evaluation results of Qwen3-8B and Qwen3-14B across seven benchmarks. Model BPW Wiki2 GSM8K MATH500 ARC-C BoolQ HellaS MMLU Qwen3-8B GPTQ-W4-G64 AWQ-W4-G64 BPDQ-W4-G128 GPTQ-W3-G32 AWQ-W3-G32 BPDQ-W3-G64 GPTQ-W3-G64 AWQ-W3-G64 BPDQ-W3-G128 GPTQ-W2-G32 AWQ-W2-G32 BPDQ-W2-G64 GPTQ-W2-G64 AWQ-W2-G64 BPDQ-W2-G128 Qwen3-14B GPTQ-W4-G64 AWQ-W4-G64 BPDQ-W4-G128 GPTQ-W3-G32 AWQ-W3-G32 BPDQ-W3-G64 GPTQ-W3-G64 AWQ-W3-G64 BPDQ-W3-G128 GPTQ-W2-G32 AWQ-W2-G32 BPDQ-W2-G64 GPTQ-W2-G64 AWQ-W2-G64 BPDQ-W2-G128 16 4.31 4.31 4.63 3.59 3.59 4.00 3.30 3.30 3.50 2.56 2.56 2.75 2.28 2.28 2.38 16 4.31 4.31 4.63 3.59 3.59 4.00 3.30 3.30 3.50 2.56 2.56 2.75 2.28 2.28 2.38 12.22 12.52 12.78 12.66 12.97 13.49 13.29 13.50 13.75 13.71 22.05 2.4E+7 18.83 30.30 8.9E+10 20. 10.78 10.98 11.29 11.05 11.37 11.86 11.51 11.64 12.32 11.84 16.31 3.1E+6 15.32 20.09 4.8E+8 16.69 87.11% 87.04% 86.28% 86.43% 82.64% 84.84% 85.67% 79.45% 83.40% 83.85% 0.38% 1.44% 52.99% 0.00% 0.00% 40.79% 88.02% 89.08% 88.02% 88.17% 84.46% 87.64% 89.16% 87.26% 88.78% 88.86% 23.81% 0.53% 71.80% 4.09% 0.00% 59.74% 56.74% 55.38% 55.81% 55.72% 53.24% 54.10% 54.78% 48.12% 52.56% 55.80% 30.80% 34.47% 44.37% 27.05% 27.30% 42.58% 60.32% 61.09% 61.26% 60.24% 59.56% 59.04% 60.84% 59.81% 56.66% 58.62% 35.49% 41.30% 51.11% 29.78% 26.96% 50.43% 86.61% 86.24% 86.38% 86.85% 85.26% 86.15% 85.93% 85.66% 85.54% 86.09% 63.98% 65.90% 84.31% 52.51% 61.68% 80.83% 89.30% 88.87% 89.36% 89.24% 87.52% 88.87% 88.78% 87.89% 87.86% 89.08% 72.20% 64.92% 87.40% 64.40% 59.94% 87.58% 74.90% 74.57% 73.55% 74.13% 73.05% 71.69% 72.23% 72.58% 71.65% 71.51% 57.18% 44.90% 62.15% 49.91% 27.92% 61.34% 78.82% 78.52% 78.32% 78.33% 77.69% 76.95% 76.91% 77.04% 76.09% 76.61% 66.23% 51.20% 69.01% 60.28% 27.20% 67.86% 73.02% 72.33% 72.18% 72.39% 70.06% 69.96% 69.95% 68.54% 68.84% 70.01% 32.47% 46.07% 58.70% 25.35% 23.05% 55.13% 77.14% 76.51% 76.02% 75.99% 74.48% 75.15% 74.22% 74.16% 73.27% 74.95% 50.84% 30.14% 66.08% 29.90% 26.21% 64.02% 53.00% 52.40% 50.60% 48.80% 47.60% 50.80% 47.60% 46.20% 45.20% 47.80% 2.60% 3.40% 14.80% 1.40% 0.00% 10.20% 53.00% 52.80% 52.00% 54.20% 48.40% 50.80% 52.80% 49.80% 50.20% 52.20% 8.20% 0.00% 34.80% 3.80% 0.00% 25.40%"
        }
    ],
    "affiliations": [
        "Artificial Intelligence and Digital Finance Key Laboratory of Sichuan Province",
        "Southwestern University of Finance and Economics",
        "Sun Yat-sen University",
        "The Hong Kong Polytechnic University",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "The University of Hong Kong"
    ]
}