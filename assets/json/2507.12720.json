{
    "paper_title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models",
    "authors": [
        "Abraham Toluase Owodunni",
        "Orevaoghene Ahia",
        "Sachin Kumar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10\\% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens"
        },
        {
            "title": "Start",
            "content": "FlexıTokens: Flexible Tokenization for Evolving Language Models Abraham Toluase Owodunni1 Orevaoghene Ahia2 Sachin Kumar1 1The Ohio State University 2University of Washington owodunni.1@osu.edu"
        },
        {
            "title": "Abstract",
            "content": "Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces fixed compression rate across the training corpus, introducing new kind of rigidity. We propose FlexıTokens, simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FlexıTokens consistently reduces token over-fragmentation and achieves up to 10% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens"
        },
        {
            "title": "1 Introduction",
            "content": "Tokenizationthe process of segmenting text into discrete unitshas been shown to significantly influence language model performance [13]. Widely used subword tokenization algorithms [4, 5] often overfragment sequences in unseen domains, languages, and scripts. This oversegmentation not only leads to poor downstream performance, increased sequence lengths contribute to higher computational overhead, memory usage, and inference costs [6, 7]. In addition, such tokenizers are inherently static and tightly coupled with the language model; they do not adapt when the language model is finetuned. As result, even if model is adapted to new distribution, its tokenization remains fixed, limiting its performance, e.g., fine-tuning Llama 2 models is subpar for coding tasks [8, 9], and unseen scripts [10]. Eliminating the reliance on static subword tokenizers has, thus, gained momentum in recent literature by directly modeling bytes [1113]. To address the increase in sequence length in byte-level language models, various papers introduce tokenization module within the LM to segment bytes into patches [1419]. As opposed to subword tokenizers, this module is typically learned via gradients alongside the LM with an auxiliary loss to achieve desired compression rate of the input sequence during training. This compression rate, while controllable, is predetermined and fixed during pretraining, which again hampers adaptation to new distributions (see Figure 1). For example, an LM trained with fixed compression rate on general domain may over-tokenize samples in specialized domains like Medicine or morphologically rich languages like Turkish that contain longer words. Conversely, it may undertokenize samples in programming languages or logographic languages like Chinese where distinct semantic units may be inappropriately merged. Preprint. Under review. Figure 1: We present an example of tokenized medical text, where FlexıTokens produces less fragmented sequence of tokens than BPE. Unlike BPE which applies fixed tokenization, FlexıTokens adapts its tokenization to the medical domain, capturing domain-specific patterns more effectively. To enable flexible adaptation of gradient-based tokenizers, we propose new training objective, which relaxes the need to have fixed compression rate. Instead of an expected compression rate, we define lower bound on the compression rate that every input sequence should have. We introduce hinge-like loss to optimize the tokenizer with this rate. By not penalizing the tokenizer when the compression rate is higher than this rate, our method allows for the segmentation to be flexible to the input sequence. When the LM is fine-tuned, this loss allows the tokenization to effectively adjust to the target distribution without leading to overfragmentation. We call our method FlexıTokens. We evaluate our proposed approach on multiple multilingual benchmarks and morphologically diverse tasks (4). FlexıTokens consistently shows superior performance compared to baselines while improving average compression rate thereby improving inference runtime. We also show that while maintaining fairer fragmentation rate across all our pretraining languages, FlexıTokens can be easily adapted to unseen languages and scripts without leading to overfragmentation. Our analysis shows that our method often updates the tokenizer to recover semantically meaningful tokens relevant to the task or domain after adaptation whereas the baselines, being not updatable, overtokenize."
        },
        {
            "title": "2 FlexıTokens",
            "content": "We build byte-level LM with learnable tokenization module integrated within the model. FlexıTokens allows the model to adjust its learned tokenization strategy to the structure and distribution of the task and input data. Our model uses hourglass transformers [15] as backbone, originally introduced to efficiently handle long sequences in tokenizer-free models [18, 16]. Despite being learnable, the resulting tokenization modules in prior work remain bound to the decisions made during pretraining, even when the model is trained or finetuned further. This inherently limits their ability to adapt to new domains, languages, or evolving data distributions, where the originally learned segmentation might no longer be optimal.1 Below, we describe the key components of the hourglass architecture (2.1) and introduce the modifications we make to enable dynamic and equitable tokenization (2.2)."
        },
        {
            "title": "2.1 Hourglass Architecture",
            "content": "The hourglass architecture [15] was designed to scale byte-level language models to handle long sequences by incorporating an internal tokenization process. It consists of three modules; tokenization submodule, language modeling block, and an upsampling layer. The tokenization submodule processes input byte sequences using lightweight transformer that maps each byte in an input byte sequence x1, . . . , xN to hidden states. boundary predictor then estimates the probability ˆbt [0, 1] of predicting segment boundary at each position t. It is implemented using an MLP followed by sigmoid function. To obtain discrete boundary decisions bt {0, 1} while preserving differentiability, we employ hard Gumbel sigmoid reparameterization of the Bernoulli distribution. Since this module is differentiable, the segmentations are learned along with the rest of the model. 1This issue is also present in subword tokenizers like BPE. Prior work typically handles this issue with heuristics like retraining and replacing the entire tokenizer during adaptation [9]. 2 Given the predicted boundaries, the language modeling module pools hidden states between segment boundaries to construct sequence of token-level representations. These representations are then passed through the middle block of transformer layers to obtain another sequence of hidden representations. Finally, the upsampling module converts the outputs from the middle LM block to byte-level probabilities. The token-level representations from the middle block are first upsampled to match the original input resolution via duplication and combined with initial byte-level representations using skip connections. These are then passed through lightweight transformer, an unembedding layer, and softmax to compute the language modeling loss. We refer the reader to [18] for detailed description. To prevent the boundary predictor from collapsing and trivially predicting each position as boundary, prior work [18, 16] added regularizer to the LM objective: log Binomial(α; N, k) where, (cid:19) Binomial(α; N, k) = αk(1 α)N k, and = bt. (1) (cid:18)N (cid:88) α [0, 1] is hyperparameter that controls the expected boundary rate. This loss is lowest when is close to αN which is the mode of the Binomial distribution. In other words, α controls the compression rate of the input sequence to approximately 1 α . Setting α = 0 will cause no boundaries to be predicted and with α = 1, the model learns to predict every position to be boundary. This loss is added to cross-entropy loss for next-byte prediction to train the model and tokenizer in an end-to-end fashion."
        },
        {
            "title": "2.2 FlexıTokens",
            "content": "In contrast with subword based models like BPE, LMs with gradient-based tokenization can learn to segment input text in way that best represents the underlying data distribution. Furthermore, prior work has shown that it allows better controllability over segmentation rates over different languages when training multilingual models by simply employing different boundary predictors with different compression rates per language or script [16] leading to more equitable tokenization [7]. However, even within language, different subsets such as different domains might require different compression rates to optimally encode the input. But the expected compression rate is predetermined by the hyperparameter α with little room for variation. Furthermore, when adapting the LM to new distributions such as new domain or new language, bound by the binomial loss in Equation 1, the compression rate does not update to the requirements of the target distribution. The ideal solution to address this issue is to get rid of the hyperparameter α (and the binomial loss) and simply minimize the predicted number of boundaries per byte, that is, . If optimized well, this loss will find the right balance between compression and minimizing the LM loss. However, in our early experiments, we observe that this loss quickly decreases to 0, predicting no boundaries. To prevent this behavior, we modify this loss to max (cid:18) (cid:19) β, , where β = α λσ α (2) σ represents the standard deviation of tokenization rates over multiple samples in given language. λ is hyperparameter. This loss introduces lower bound on the boundary rate at α λσ. If the boundary rate reduces to less than this value, this loss will become 0 reducing further incentive to compress but does not penalize it. In contrast, the binomial loss forces the rate to be close to α penalizing both increase or decrease. Indeed, we observe in our experiments that there is higher variance in the segmentation rates of different samples. Furthermore, during finetuning, we observe changes in the compression rates showing that the tokenization indeed adapts to the task. We refer to the flexible tokens learned through our proposed loss and the resulting model that predicts flexible tokens as FlexıTokens.2 To encode the same information, different languages require different number of bytes, where non-Latin languages (e.g., Indian languages) may require up to 4 bytes per character. When training multilingual models, setting one α for all languages will lead to text in some languages getting segmented into much longer sequences. To alleviate this issue, Ahia et al. [16] proposed adding different boundary predictor per language with its own α defined to make the compression rates uniform across languages. unique boundary predictor per language, however, requires determining or predicting the input language to route the input to the appropriate predictor. It also makes it challenging when the input 2We use the term interchangeably to refer to our model and proposed loss. 3 text contains multiple languages (in case of code-mixed text). Our experiments reveal that training one shared boundary predictor with different hyperparameter αL for each language leads to the same performance. Hence, we train multilingual model with the following training objective objective. = (cid:88) i= log pθ(xi x<i) (cid:88) I(language(x) = L) max (cid:18) (cid:19) βL, 0 (3) where is the set of all languages in the training set. Determining βL We define an anchor language A3 and set αA as hyperparameter. We assume access to an n-way parallel corpus4 between and every other language in our training set.5 We compute the mean sequence length (in bytes) µA, µL and standard deviation σA, σL over this dataset. We set αL to be αA , and define the lower bound βL as αL λσL. Intuitively, if uses more bytes to represent the same information as A, its compression rate should be higher (and hence α lower). µA µL"
        },
        {
            "title": "3 Experimental Setup",
            "content": "3.1 Datasets We validate our proposed approach in multilingual setting. We train models with four scripts and six languages: Latin script (English and Spanish), Cyrillic (Russian and Ukrainian), Devanagari (Hindi), and Telugu script (Telugu). These scripts cover diverse range of typologies and byte complexities. For example, Latin script needs 1 byte per character in Unicode, whereas Russian and Telugu characters need up to 2 and 3 bytes respectively. To make tokenization rates similar across all languages, all these languages require different amounts of compression. For pretraining, we sample the first 2.06M documents from FineWeb [20] for English and Spanish, using the first 10K documents as the validation set. For all other languages, we sample the first 1.65M documents from FineWeb 2 [21], again using the first 10K documents for validation. breakdown of the training set sizes is shown in Figure 6 (in Appendix E). For downstream evaluations, we finetune on the following tasks: (1) XNLI [22]: natural language inference, (2) SIB-200 [23]: topic classification, (3) Multilingual Sentiment [24]: multi-domain sentiment analysis, (4) WikiANN [25]: named entity recognition, (5) Indo-Aryan Language Identification (ILI)6 [26]: dialect classification, (6) Medical Abstracts Text Classification [27] and (7) Irony detection in Tweets containing emojis [28] We provide more details on each dataset in Appendix E."
        },
        {
            "title": "3.2 Hyperparameters",
            "content": "To understand the impact of sequence compression on models performance, we explore multiple compression rate configurations. Our main results use 3 compression rate for our anchor language, English (i.e. α = 1/3). We also compare with 5 and 10. The corresponding values of αL and σL for all languages is in Table 1. We compute βL using the FLORES-200 dataset [29], which contains parallel sentences in 200 languages. We empirically set λ = 3; we show comparisons with other values in 4. In our experiment with adapting our model to an unseen script (for Urdu), we set it β to have the same value as Telugu, which has the highest compression rate of all the languages we experimented on, assuming no available training dataset in the unseen language. Model Architecture and Pretraining We pretrain two model sizes: small (119M parameters) and large (1B parameters). For our small model, we follow Ahia et al. [16] to create 16-layer hourglass transformer. The tokenization and upsampling submodules each consist of 2 transformer layers, while the language modeling submodule contains 12 transformer layers. The input embedding dimension is 768. All transformer layers have hidden size of 768, with feed-forward intermediate dimension of 3We choose as English in all our experiments. This choice is arbitrary; choosing another language will change the β values but will not influence the final results). 4This computation can also be done with pairwise parallel dataset with the anchor language with slight modifications. 5This parallel dataset is not used for training the model. 6https://github.com/kmi-linguistics/vardial2018 4 3072, and we use 12 attention heads in the self-attention mechanism. All other parameters follow Ahia et al. [16], except for the boundary predictor: instead of multiple predictors, we use single 2-layer MLP as the boundary predictor. See Appendix for the parameters of our large (1B) model. During pretraining, we use chunk size of 512 bytes. We train for 100K steps with cumulative batch size of 512 across 2 H100 GPUs with 9000 warmup steps. Optimization is performed with Adam [30], cosine learning rate scheduler (with maximum learning rate of 5e-5), and gradient clipping set to 0.25. Finetuning During finetuning, we increase the sequence length to 2048 bytes to better capture longer sequences in the finetuning dataset.7 For the NER task, we first concatenate token sequences using whitespaces before tokenization and label whitespaces as non-entity. - We set gradient clipping to 1.0 and apply warmup ratio of 10%. All tasks are finetuned for 5 and 3 epochs for our 119M and 1B parameter models respectively. We use task-specific batch sizes based on data availability. We perform monolingual finetuning on each language. Please refer to Table 6 in the Appendix for full finetuning parameters. Table 1: αL and σL values for each language in our training dataset, computed using FLORES-200. The upper bound βL in Equation 3 is computed as αL λσL) Configuration en es ru uk hi te FlexıTokens 10 0.1 / 10 FlexıTokens 5 0.2 / 5 FlexıTokens 3 0.333 / 3 0.08 / 12.12 0.17 / 6.06 0.28 / 3.64 0.05 / 19.92 0.1 / 9.96 0.167 / 5.98 0.053 / 18.70 0.107 / 9.35 0.178 / 5.61 0.039 / 25.62 0.078 / 12.81 0.13 / 7. 0.037 / 26.91 0.074 / 13.45 0.124 / 8.07 σ 0.023 0.019 0.011 0. 0.009 0."
        },
        {
            "title": "3.3 Baselines",
            "content": "We consider two baselines: (1) model trained with BPE tokenizer and (2) byte-level model whose boundary predictor is trained with binomial loss as described in Nawrot et al. [2023] [18] (bınomıal). For fair comparison with the BPE-based model, we match its overall parameter size with FlexıTokens. We train BPE tokenizer with vocab size of 50K on the same amount of dataset from each language. This achieves compression rate of 4.4 on English.8 To match total parameters (embeddings + transformer layers), we train the language model with 5 Transformer layers.9 Figure 2: FineWeb Test BPB (), Compression rate () and Compression variance () of FlexıTokens compared to the bınomıal variant with αA = 0.3 and λ = 3. Higher compression rates result in fewer tokens, which in turn leads to more efficient model. Overall, FlexıTokens 1B model achieves the best score across all metrics 7We use shorter sequence length during pretraining due to computational constraints. 8Note that BPE models cannot be controlled to have desired compression rates across all languages due to their inherent frequency based training process [6]. 9We conducted early experiments with training BPE-based models by matching Englishs compression rate to 3 compression rate but they resulted in vocabulary sizes of 10K which performed poorly in early experiments."
        },
        {
            "title": "4 Results and Analyses\nWe evaluate our pretrained model using bits per byte (BPB) [31] and the finetuned models using task\nspecific metrics, mostly accuracy and F1-score. We provide a summary of the results for the pretrained\nmodels in Figure 2 and Figure 3, and for the finetuned models inTable 3, Table 2, and Figure 4, with\ndetails in Appendix F.",
            "content": "Pretraining with FlexıTokens leads to better compression As shown in Figure 2, our method maintains the BPB performance as bınomıal on the FineWeb test sets while achieving substantially higher average compression rate, which in turn increases inference speed by requiring fewer tokens. We also observe higher variance in compression rates of FlexıTokens implying higher flexibility in how input sequences are fragmented. This variationwhich is much lower in baseline modelsalongside the higher compression rate on average underscores FlexıTokens ability to dynamically adapt its tokenization patterns to its input. In Figure 3, we compare average number of tokens required to represent the same information in different languages by different tokenization methods. Our method remains as equitable as bınomıal using similar number of tokens for all languages . In comparison, BPE shows high variability with included languages like Hindi and Telugu requiring twice as many tokens. An unseen language (Urdu) requires 6 times as much. Figure 3: Average number of tokens per sample obtained in the FLORES dataset with different tokenization algorithms. FlexıTokens consistently produces the least number of tokens while maintaining balance across languages, even for the unseen language Urdu. BPE over-fragments seen (Hindi, Telugu) as well as unseen languages (Urdu). FlexıTokens adapts tokenization and boosts performance across tasks and domains. In Tables 2 and 3, we report task-specific metrics after finetuning our pretrained models on several downstream tasks across different domains and the corresponding compression rates per language and task in Figure 4. FlexıTokens outperforms all baselines on majority of tasks, even the BPE baseline with much higher compression rate. Our method obtains performance improvements of over 3 points on SIB-200 and XNLI with bınomıal while improving compression across all tasks. Moreover, as we increase λ, performance tends to also increase. This is because higher λ allows wider margin for model to find the optimal compression rate resulting in over 9 points improvements in SIB-200. Also, we observe that by increasing FlexıTokenss model size to 1B parameters, we consistently outperform all other baselines and model sizes on all our tasks by 2.2 points on average. This indicates that even more performance improvements can be obtained by further scaling FlexıTokens to larger model sizes with more training data. We leave this exploration for future work. Table 2: Accuracy on ILI, Medical Abstracts, and Irony tasks. FlexıTokens outperforms across all tasks. Analyzing compression rates across tasks and languages in Figure 4, we observe that bınomıal maintains rates closer to the initial α, but this effect diminishes for non-Latin languages such as Hindi and Telugu, which are structurally distant from Latin scripts. These languages show both higher average compression and greater variance with FlexıTokens. Qualitative analysis reveals consistent tokenization patterns across topic classification tasks like SIB-200 and Medical Abstracts, where compression remains stable across examples. In contrast, tasks such as XNLI exhibit compression spikes across all languages, indicating that some tasks benefit from more compression than others. In the Irony Classification task, FlexıTokens effectively tokenizes emojis with higher compression, preserving their semantic meaning. Following adaptation to the medical domain (Figure 1), we also find that medical terms are tokenized in unison as whole words, reducing fragmentation and better aligning with expected domain-specific vocabulary. FlexıTokens λ1 68.37 FlexıTokens λ2 68.75 FlexıTokens λ3 69.26 FlexıTokens λ3 1B 67.22 ILI (hi) Med. Abs. (en) BPE bınomıal 89.58 90.33 89.55 62.92 62.74 63.19 67.86 67. 89.06 89.47 57.68 62.81 Irony (en) Model 89.32 64. 6 Table 3: WikiANN (NER), XNLI and SIB-200 F1 Score and Accuracy and for 3 Compression Rate. FlexıTokens outperforms all baselines on XNLI and NER respectively. Notably, it achieves approximately 3 point gain on XNLI for Urduan unseen language scriptcompared to BPE. NER F1 Score Model BPE bınomıal en es ru uk hi 52.30 63.80 67.7 0 75. 64.94 67.59 74.99 78.06 60.23 61.21 te 48.18 48.31 Avg 61.39 65.67 FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 FlexıTokens λ3 1B 64.61 62.26 62.24 61.97 51.74 48.13 50.88 68.30 67.55 68.25 66.57 66.02 66. 77.94 77.99 78.01 76.12 76.23 75.45 63.07 63.96 63.73 52.77 69.69 79. 63.61 67.97 77.60 XNLI Accuracy Model BPE bınomıal en 73.09 72.87 es ru hi te ur (OOD) 69.9 70.28 65.95 65.93 61.48 62.26 68.00 66.11 54.11 54. Avg 65.42 65.37 FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 FlexıTokens λ3 1B 75.17 62.42 62.16 62.36 73.51 73.21 73.35 66.47 66.97 66. 70.22 70.84 70.22 56.99 57.58 57.33 67.11 66.71 67.82 66.12 66.25 66.31 68.60 57. 64.41 69.62 67.98 72.44 SIB-200 Accuracy Model BPE bınomıal en 80.88 79.41 es ru uk hi 81.37 74.02 81.37 71.08 76.96 68.63 60.78 64.71 te 72.55 69.61 Avg 75.65 71.24 FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 FlexıTokens λ3 1B 85.78 75.49 74.51 73.04 72.55 75.98 77. 69.61 71.57 72.55 70.67 72.55 74.35 66.18 66.18 71.08 61.27 69.12 71.08 78.92 77.94 80.88 86. 81.86 84.31 77.94 83.33 83.82 Adaptive tokenization to unseen scripts boosts performance without overfragmentation In Table 3, we extend our evaluation to Urdu, low-resource Indo-Aryan language that shares linguistic commonalities with Hindi but uses different script, not included in our pretraining dataset. We see that FlexıTokens outperforms BPE with more than 3 points after finetuning. Qualitative evaluation on the XNLI inputs  (Table 5)  reveals that our approach finds more compressed and semantically meaningful tokens compared to baselines (numbers and words). BPE tokenizer tokenizes Urdu with more 6 tokens than FlexıTokens which follows the same result patterns from Figure 3. Note that FlexıTokens adapts well to unseen scripts because we use script-agnostic boundary predictor as opposed to Ahia et al. [16] which introduced the idea of equitable tokenization via script-specific boundary predictor for every language script included during pretraining. Also, compound or rare words (especially medical terms or foreign-origin words like hypertrophic) are split into meaningful subwords, enabling the model to learn more meaningful representations. Impact of scaling model size: We experimented with scaling FlexıTokens by adding more layers to the tokenization, language modeling and upsampling module. Overall, we observe (see Figure 5) that increasing our models parameters by adding more layers to each module improves performance. We also note that the compression rate increases as we add more layers to the model. This pattern is observed in all models sizes, including our 1B model (Figure 2). Surprisingly, we find that scaling the non-language modeling modules also improves performance. We presume that this is because more layers allow the model to create richer representations prior to tokenization. We note that for the choice of which module gains the most from layer addition, increasing the LM module with 2 layers (2,14,2) outperforms adding more layers to other parts of the model (3,12,3). These results provide insightful directions for future research on scaling FlexıTokens. 7 Figure 4: Compression rate changes with FlexıTokens across multiple tasks. Initial is the base compression rate before pretraining. Compression rate for bınomıal remains relatively low while we also see spike for task like XNLI Figure 5: FineWeb Test results for ablating the number layers in FlexıTokens. Adding more layers results to lower BPC and higher compression rate across all model sizes. FlexıTokens (2,12,2) is equivalent to 2, 12 and 2 transformer layers in the tokenization, LM and upsampling module respectively. Relationship between compression and model performance: We explore various configurations of α and how it impacts performance and show average results across all tasks in Table 4 (see Appendix for breakdown of performance on each language). As we scale the compression rate from 3 to 5 and 10 , we observe slight decline in performance indicating that too much compression may result in loss of information hurting the model. We speculate that this issue might be attributed to the small model size used in our main experiments. Recent work has argued that larger models can handle larger vocabularies better [32]. Its analogue in our case is training larger model with more layers in the tokenization module, which we show improves performance in FlexıTokens (3,12,3) and FlexıTokens 1B. Table 4: Ablation for α: Average Accuracy and Compression Results Across Multiple Languages Model SIB-200 WikiANN Multi. Senti. XNLI ILI Med. Abs. Avg Accuracy FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x 53.76 71.16 72. 64.35 64.92 66.02 72.99 72.54 72.74 65.23 65.48 66.25 89.07 89.28 90.33 62.95 63.47 62.74 68.06 71.14 71. Compression Rate Std FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x 28.89 11.06 10.72 1.54 6.19 0.53 28.01 14.14 11.17 3.69 6.26 1.33 27.41 12.12 11.25 2.86 6.17 1.03 29.06 8.55 12.15 1.76 6.83 0. 38.80 38.80 14.82 14.82 8.35 8.35 13.22 2.15 5.63 0.33 3.21 0.15 27.56 14.47 10.96 4.17 6.17 2.00 Table 5: Tokenization outputs with different methods (Urdu, Telugu, English) Tokenizer Sentence and Segmentation ur BPE bınomıal 3 FlexıTokens 3 39-year-old SpongeBob was diagnosed with hypertrophic cardiomyopathy in Mumbai. 39Ø³ØÙĦÛģ ØØ³Ù¾ÙĨØ ØØØ ÚÙĪ ÙħÙħØØÛĮ ÙħÛĮÚº ÛģØØÙ¾ØÙ¹ØØÙģÚ ÚØØÚĪÛĮÙĪÙħÛĮÙĪÙ¾ÛĮØªÚ¾ÛĮ ÚÛĮ ØªØØÛĮØµ ÛģÙĪØÛĮÛĶ 39سالہﺍسپنجباﺏکوممبئمیںہائپرٹرﺍفککاﺭڈیومیوپیتھیکیتشخﺹہوئی 39سالہﺍسپنجباﺏکوممبئیمیںہائپرٹرﺍفککاﺭڈیومیوپتھکیتشخیصہوئی te BPE He spent the whole night watching Netflix. He fell asleep early. àħàààģ àà¾ààįààĤàà¾ ààĨàŁàįâĢĮààįà²ààķàįààį àļàĤààįààĤ àĹàààªà¾ààģ. àħàààģ ààįàµààĹà¾ ààààįààªàĭàà¾ààģ. అడ రర న ఫలస చస గడ. అతడ తవరగ నరప. bınomıal 3 FlexıTokens 3 అతడ రతరత న ఫలకస చస గడపడ:. అతడ తరగ నదరపయడ:. en BPE Binomial 3 Influenza and pneumonia were identified as major causes of mortality in children. Influenza and pneumonia were identified as major causes of mortality in children. Influenza and pneumonia were identified as major causes of mortality in children. FlexıTokens 3 Influenza and pneumonia were identified as major causes of mortality in children. en BPE Binomial3 FlexıTokens 3 Oh no, another surprise bonus at work. Just what didnt need Oh no, another surprise bonus at work. Just what didnâĢĻt need ðŁĺĢðŁĺĤðŁĺĤðŁĻĭðŁı½âĢįâĻĤïıðŁðŁı¾âĢ įâĻĢïıłħłħķłĦIJłħijłħłħķłĦIJłħĵłħŁłħŁłħĽłħķłħĶ. Oh no, another surprise bonus at work. Just what didnt need . . Oh no, another surprise bonus at work. . Just what didnt need #Tokens 107 21 17 37 22 20 25 20"
        },
        {
            "title": "5 Related Work",
            "content": "Tokenizer-free language modeling Several works have explored the possibilities of training language models without relying on subword tokenization, instead representing text directly as sequence of bytes [1113, 33] or pixels [3436]. To address the efficiency challenges of processing raw characters or byte sequences on tokenizer free LMs, alternative architectures have proposed to either segment byte sequences into fixed-length [15, 3739, 19] or dynamic segments [18, 16, 17]. However, these models are pretrained with fixed target compression rate, which limits their ability to adapt to shifts in data distribution. 9 Adapting tokenizers to new distributions There has been little research on adapting tokenizer-free LMs to new data distributions. Mofijul Islam et al. [40] propose character-based tokenizer by distilling segmentation information from heuristic-based subword tokenization. In contrast, several studies have explored adaptation strategies for subword tokenizers, both at inference time and during fine-tuning. For instance, prior work has shown that improved segmentation of large numbers can enhance performance on arithmetic tasks without retraining [41, 42]. In multilingual and domain-specific settings, various approaches have been proposed to adapt subword tokenizers during fine-tuning. These involve refining the tokenizer vocabulary with new tokens from the target distribution and initializing the corresponding embeddings to better capture linguistic and domain-specific characteristics [4347]. However, our experiments indicate that subword tokenizers often underperform in low-resource and non-Latin script languages due to over-segmentation."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced FlexıTokens, flexible, gradient-based tokenization approach that enables language models to adapt their segmentation patterns during finetuning. Unlike prior methods that enforce static or fixed compression rates, our method promotes dynamic tokenization aligned with the structure of the target distribution. Through multilingual and domain-diverse evaluations, FlexıTokens consistently reduces token over-fragmentation, improves downstream task performance, and achieves higher compression without sacrificing accuracy. Our results highlight the importance of adaptable tokenization strategies for building more efficient and generalizable language models."
        },
        {
            "title": "References",
            "content": "[1] Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max Lübbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper Buschhoff, et al. Tokenizer choice for llm training: Negligible or crucial? In Findings of the Association for Computational Linguistics: NAACL 2024, pages 39073924, 2024. [2] Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom Goldstein. Coercing llms to do and reveal (almost) anything. arXiv preprint arXiv:2402.14020, 2024. [3] Sander Land and Max Bartolo. Fishing for magikarp: Automatically detecting under-trained tokens in large language models. arXiv preprint arXiv:2405.05417, 2024. [4] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162/. [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [6] Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah Smith, and Yulia Tsvetkov. Do all languages cost the same? tokenization in the era of commercial language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 99049923, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 614. URL https://aclanthology.org/2023.emnlp-main.614/. [7] Aleksandar Petrov, Emanuele La Malfa, Philip Torr, and Adel Bibi. Language model tokenizers introduce unfairness between languages. Advances in neural information processing systems, 36: 3696336990, 2023. [8] Gautier Dagan, Gabriel Synnaeve, and Baptiste Rozière. Getting the most out of your tokenizer for pre-training and domain adaptation. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. 10 [9] Benjamin Minixhofer, Edoardo Maria Ponti, and Ivan Vulić. Zero-shot tokenizer transfer. arXiv preprint arXiv:2405.07883, 2024. [10] Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. Bactrian-x: Multilingual replicable instruction-following models with low-rank adaptation. arXiv preprint arXiv:2305.15011, 2023. [11] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. ByT5: Towards token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291306, 2022. doi: 10.1162/tacl_a_00461. URL https://aclanthology.org/2022.tacl-1.17/. [12] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. In AAAI Conference on Artificial Intelligence, 2018. URL https://api.semanticscholar.org/CorpusID:52004855. [13] Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander Rush. Mambabyte: Token-free selective state space model. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=X1xNsuKssb. [14] Yi Tay, Vinh Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via gradient-based subword tokenization. arXiv preprint arXiv:2106.12672, 2021. [15] Piotr Nawrot, Szymon Tworkowski, Michał Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, and Henryk Michalewski. Hierarchical transformers are more efficient language models. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Findings of the Association for Computational Linguistics: NAACL 2022, pages 15591571, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. findings-naacl.117. URL https://aclanthology.org/2022.findings-naacl. 117/. [16] Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Valentin Hofmann, Tomasz Limisiewicz, Yulia Tsvetkov, and Noah Smith. Magnet: Improving the multilingual fairness of language models with adaptive gradient-based tokenization. Advances in Neural Information Processing Systems, 37: 4779047814, 2024. [17] Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, and Srinivasan Iyer. Byte latent transformer: Patches scale better than tokens, 2024. URL https://arxiv.org/abs/2412.09871. [18] Piotr Nawrot, Jan Chorowski, Adrian Lancucki, and Edoardo Maria Ponti. Efficient transformers with dynamic token pooling. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 64036417, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.353. URL https://aclanthology.org/ 2023.acl-long.353/. [19] LILI YU, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. MEGABYTE: Predicting million-byte sequences with multiscale transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=JTmO2V9Xpz. [20] Guilherme Penedo, Hynek Kydlíček, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. [21] Guilherme Penedo, Hynek Kydlíček, Vinko Sabolčec, Bettina Messmer, Negar Foroutan, Martin Jaggi, Leandro von Werra, and Thomas Wolf. Fineweb2: sparkling update with 1000s of languages, December 2024. URL https://huggingface.co/datasets/ HuggingFaceFW/fineweb-2. 11 [22] Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. arXiv preprint arXiv:1809.05053, 2018. [23] David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba Alabi, Yanke Mao, Haonan Gao, and Annie En-Shiun Lee. Sib-200: simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. arXiv preprint arXiv:2309.07445, 2023. [24] clapAI. Multilingualsentiment: multilingual sentiment classification dataset, 2024. URL https://huggingface.co/datasets/clapAI/MultiLingualSentiment. [25] Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. Crosslingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19461958, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1178. URL https://www.aclweb.org/anthology/P17-1178. [26] Marcos Zampieri, Preslav Nakov, Nikola Ljubešić, Jörg Tiedemann, Shervin Malmasi, and Ahmed Ali, editors. Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018), Santa Fe, New Mexico, USA, August 2018. Association for Computational Linguistics. URL https://aclanthology.org/W18-3900/. [27] Tim Schopf, Daniel Braun, and Florian Matthes. Evaluating unsupervised text classification: zero-shot and similarity-based approaches. In Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval, pages 615, 2022. [28] Omid Rohanian, Shiva Taslimipoor, Richard Evans, and Ruslan Mitkov. Wlv at semeval-2018 task 3: Dissecting tweets in search of irony. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 553559, 2018. [29] Marta Costa-Jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. [30] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [31] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [32] Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, and Ngai Wong. Scaling laws with vocabulary: Larger models deserve larger vocabularies, 2024. URL https://arxiv.org/abs/2407.13623. [33] Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, and Luke Zettlemoyer. MYTE: Morphology-driven byte encoding for better and fairer multilingual language modeling. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1505915076, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.acl-long.804. URL https://aclanthology.org/2024.acl-long.804/. [34] Jonas Lotz, Elizabeth Salesky, Phillip Rust, and Desmond Elliott. Text rendering strategies for pixel language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1015510172, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.628. URL https://aclanthology.org/2023.emnlp-main.628/. [35] Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, and Desmond Elliott. Language modelling with pixels. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= FkSp8VW8RjH. 12 [36] Elizabeth Salesky, Neha Verma, Philipp Koehn, and Matt Post. Multilingual pixel representations for translation and effective cross-lingual transfer. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1384513861, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.854. URL https://aclanthology. org/2023.emnlp-main.854/. [37] Jonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an efficient tokenization-free encoder for language representation. Transactions of the Association for Computational Linguistics, 10:7391, 2022. doi: 10.1162/tacl_a_00448. URL https://aclanthology.org/2022.tacl-1.5/. [38] Nathan Godey, Roman Castagné, Éric de la Clergerie, and Benoît Sagot. MANTa: Efficient gradient-based tokenization for end-to-end robust language modeling. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 28592870, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.207. URL https: //aclanthology.org/2022.findings-emnlp.207/. [39] Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via gradient-based subword tokenization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=JtBRnrlOEFN. [40] Md Mofijul Islam, Gustavo Aguilar, Pragaash Ponnusamy, Clint Solomon Mathialagan, Chengyuan Ma, and Chenlei Guo. vocabulary-free multilingual neural tokenizer for end-to-end task learning. In Spandana Gella, He He, Bodhisattwa Prasad Majumder, Burcu Can, Eleonora Giunchiglia, Samuel Cahyawijaya, Sewon Min, Maximilian Mozes, Xiang Lorraine Li, Isabelle Augenstein, Anna Rogers, Kyunghyun Cho, Edward Grefenstette, Laura Rimell, and Chris Dyer, editors, Proceedings of the 7th Workshop on Representation Learning for NLP, pages 9199, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.repl4nlp-1.10. URL https://aclanthology.org/2022.repl4nlp-1.10/. [41] Aaditya K. Singh and DJ Strouse. Tokenization counts: the impact of tokenization on arithmetic in frontier llms, 2024. URL https://arxiv.org/abs/2402.14903. [42] Ashutosh Sathe, Divyanshu Aggarwal, and Sunayana Sitaram. Improving consistency in LLM inference using probabilistic tokenization. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Findings of the Association for Computational Linguistics: NAACL 2025, pages 47664778, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-889176-195-7. URL https://aclanthology.org/2025.findings-naacl.268/. [43] Chanjun Park, Sugyeong Eo, Hyeonseok Moon, and Heuiseok Lim. Should we find another model?: Improving neural machine translation performance with ONE-piece tokenization method without model modification. In Young-bum Kim, Yunyao Li, and Owen Rambow, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers, pages 97104, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-industry.13. URL https://aclanthology.org/2021.naacl-industry.13/. [44] Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow. Adapting pretrained language models to African languages via multilingual adaptive fine-tuning. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Conference on Computational Linguistics, pages 43364349, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https://aclanthology. org/2022.coling-1.382/. [45] Benjamin Minixhofer, Fabian Paischer, and Navid Rekabsaz. WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models. In Marine 13 Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39924006, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.293. URL https: //aclanthology.org/2022.naacl-main.293/. [46] Vin Sachidananda, Jason Kessler, and Yi-An Lai. Efficient domain adaptation of language models via adaptive tokenization. In Nafise Sadat Moosavi, Iryna Gurevych, Angela Fan, Thomas Wolf, Yufang Hou, Ana Marasović, and Sujith Ravi, editors, Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing, pages 155165, Virtual, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.sustainlp-1.16. URL https: //aclanthology.org/2021.sustainlp-1.16/. [47] Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, and Rada Mihalcea. Taskadaptive tokenization: Enhancing long-form text generation efficacy in mental health and beyond. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1526415281, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.944. URL https://aclanthology.org/2023.emnlp-main.944/."
        },
        {
            "title": "A Limitations",
            "content": "Our limited computational budget prevents us from training larger models with more language on larger datasets. We anticipate the results will improve with scaling potentially providing even higher compression. We leave this exploration to future work. While we aimed for diversity of languages and scripts in our experiments, we acknowledge we do not cover vast majority of linguistic diversity. But our methods are general and we believe our results should translate to more languages. We also acknowledge tradeoff between the performance and compression rate of the languages with higher compression leading to slight decline in performance with some languages being more sensitive than others. FlexıTokens shares limitations of other segmentation methods in that it may not be suitable for languages where morphemes are discontinuous and vowels are interspersed between consonant roots for inflection or sometimes omitted such as Semitic languages or other languages with Templatic morphologies."
        },
        {
            "title": "B Broader Impacts Statement",
            "content": "Through this work, we demonstrate that tokenization can be performed in non-rigid but adaptive manner that is more equitable, efficient, and performant across multiple domains. This flexibility opens new opportunities for incorporating low-resource and out-of-distribution (OOD) languages into state-of-the-art multilingual language models, particularly those being developed at industrial scale. FlexıTokens enables easier adaptation of models to new domains, even in data-scarce settings, creating pathways for easier and more targeted model adaptation. We also acknowledge limitation in scaling the α, and we encourage the research community to further explore strategies for tuning this parameter that best suits their target domains and languages. We include our code in this submission and upon acceptance, we will release our code and training recipes to support reproducibility and foster adoption of FlexıTokens in future research."
        },
        {
            "title": "C Proof for optimizing the Binomial PMF",
            "content": "We begin by revisiting the boundary regularization term based on the Binomial distribution. Rather than minimizing the negative log-likelihood (NLL) of the Binomial, we simplify the form as follows: log (k N, α) = log α + (N k) log(1 α) (4) Here, is the number of predicted boundaries, is the sequence length, and α is the boundary prior. Taking the derivative with respect to α: dα log (k N, α) = α 1 α Setting this gradient to zero yields the maximum likelihood estimate (MLE): α = 1 α k(1 α) = (N k)α α = (5) (6) This shows that the optimal α aligns with the empirical boundary rate . Therefore, instead of explicitly computing the Binomial loss, we may directly regularize the deviation between the predicted and expected boundary rates. To encourage compression and avoid over-segmentation, we introduce one-sided penalty: max (cid:18) (cid:19) α, 0 (7) This penalizes only when the boundary rate exceeds the prior α, allowing lower rates without penalty. However, to prevent trivial collapse (i.e., 0), we relax this constraint by defining soft upper bound: where σ is the standard deviation of boundary rates over multiple samples and λ is tunable margin. This leads to the final loss term: β = α λσ (8) Lboundary = max (cid:18) (cid:19) β, 0 (9) This is the expression used in FlexıTokens from Equation 3. It replaces the rigid binomial constraint with margin-aware compression regularizer that adapts across languages, scripts, and domains during training. Model Architecture and Hyperparamters (1B) For our large (1B parameters) model, we create 24-layer hourglass transformer. The tokenization and upsampling submodules each consist of 2 transformer layers, while the language modeling submodule contains 20 transformer layers. The input embedding dimension is 2048. All transformer layers have hidden size of 2048, with feed-forward intermediate dimension of 8192, and we use 16 attention heads in the self-attention mechanism. We also use maximum sequence length of 2048. All other parameters follow the same architecture as our small model. We pretrain this model for 50,000 steps which is equivalent to training for 1 epoch on our training data as with our small model. We use 9000 warm steps and learning rate of 3e-4."
        },
        {
            "title": "E Hyperparameters",
            "content": "We extend our hyperparameter section (3.2) and present the exact batch size used for finetuning all the models used in our experiments on downstream task (see Table 6). In Figure 6, we also show distribution of the training dataset size we used for each language in our experiments training corpus. In addition to English, we keep the number of samples for all other languages the same to avoid any bias that could be caused by data imbalance in our models. Table 6: Batch Sizes per Dataset and Language"
        },
        {
            "title": "Dataset",
            "content": "XNLI SIB-200 WikiANN Multi. Sentiment ILI Medical Abstract Irony detection en 64 8 16 128 - 16 32 es 64 8 16 32 - - - ru 64 8 16 32 - - - uk 64 8 16 - - - - hi 64 8 16 8 32 - - te 64 8 16 - - - - ur 64 - - - - - -"
        },
        {
            "title": "F Results and Analyses",
            "content": "In this section, we present the full results discussed in 4 across all our selected downstream tasks as seen in Table 8, 9, 10, and 2. We also present the full results for our multilingual sentiment analysis evaluation  (Table 7)  . All Results in this section contain values for performance metrics like accuracy and F1 score, compression rates and standard deviation of the compression rates. 16 Figure 6: Number of training documents sampled by language Table 7: Multilingual Sentiment Accuracy and Compression Results for 3x Configurations"
        },
        {
            "title": "Model",
            "content": "es ru hi"
        },
        {
            "title": "Avg",
            "content": "BPE Binomial 3x FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 77.89 77.75 77.77 77."
        },
        {
            "title": "Accuracy",
            "content": "87.20 87.33 87.33 87.13 53.63 53.42 53.12 53.01 72.91 72.83 72.74 72. Compression Rate Std Binomial 3.61 0.48 5.97 0.98 7.98 1.90 5.85 1. FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 3.78 0.27 3.90 0.28 4.04 0.37 6.22 0.53 6.44 0.61 6.67 0.75 8.26 1.82 8.16 1.65 8.83 1.84 6.09 1.11 6.17 1.03 6.51 1.17 Table 8: WikiANN NER F1 Score and Compression Results for 3x Configurations Model en es ru uk hi te Avg F1 Score BPE Binomial FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 52.30 63. 63.07 63.96 63.73 67.7 0 75.06 76.12 76.23 75.45 64.94 67.59 68.30 67.55 68.25 74.99 78. 77.94 77.99 78.01 60.23 61.21 62.26 62.24 61.97 48.18 48.31 51.74 48.13 50.88 61.39 65. 66.57 66.02 66.38 Compression Rate Std Binomial 3x 3.05 0.47 3.88 0.76 6.37 1. 5.75 1.11 8.74 3.27 8.56 2.29 6.06 1.86 FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 3.18 0.43 3.27 0.44 3.42 0. 3.84 0.54 3.93 0.58 4.18 0.66 6.31 1.15 6.58 1.38 6.64 1.29 5.92 0.90 6.12 1.00 6.30 1.07 8.42 1.68 8.52 1.49 8.76 1.77 8.64 1.55 9.15 2.21 8.99 2.07 6.05 1.14 5.66 1.33 6.38 1. 17 Table 9: SIB-200 Accuracy and Compression Results for with 3x Configurations Model en es ru uk hi te Avg Accuracy BPE Binomial FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 80.88 79.41 78.92 77.94 80.88 81.37 74.02 72.55 75.98 77.45 81.37 71. 75.49 74.51 73.04 76.96 68.63 69.61 71.57 72.55 60.78 64.71 61.27 69.12 71.08 72.55 69. 66.18 66.18 71.08 75.65 71.24 70.67 72.55 74.35 Compression Rate Std Binomial 3.04 0. 3.70 0.34 5.97 0.64 6.26 0.70 6.59 0.48 10.16 1.34 5.95 0. FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 3.13 0.25 3.32 0.27 3.34 0.35 3.81 0.29 3.92 0.31 4.19 0.38 6.35 0.64 6.49 0.56 6.55 0.75 6.40 0.64 6.06 0.54 6.36 0.81 8.46 0.82 8.35 0.54 8.36 0. 8.44 0.61 9.00 0.79 9.65 1.28 6.10 0.58 6.19 0.53 6.41 0.76 Table 10: XNLI Accuracy and Compression Results for 3x Configurations Model en es ru hi te ur (OOD) Avg Accuracy BPE Binomial FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 73.09 72.87 73.51 73.21 73.35 69.9 70.28 70.22 70.84 70. 65.95 65.93 66.47 66.97 66.75 61.48 62.26 62.42 62.16 62.36 68 66.11 67.11 66.71 67. 54.11 54.79 56.99 57.58 57.33 65.42 65.37 66.12 66.25 66.31 Compression Rate Std Binomial 3.13 0.30 3.79 0.48 6.10 0.74 9.85 1.28 8.37 1.21 8.58 0. 6.64 0.88 FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 3.17 0.19 3.36 0.26 3.56 0.31 3.89 0.26 4.10 0.30 4.32 0.34 6.47 0.53 6.98 0.60 7.45 0.72 7.99 0.75 9.18 0.85 10.06 1. 8.39 0.58 8.62 0.65 8.95 0.74 8.52 0.71 8.73 0.73 9.07 0.80 6.40 0.55 6.83 0.60 7.24 0.74 Table 11: ILI, Medical Abstracts, and Irony (for 3 Configuration)"
        },
        {
            "title": "Model",
            "content": "ILI (hi) Med. Abs. (en) Irony (en)"
        },
        {
            "title": "Accuracy",
            "content": "BPE bınomıal FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 89.06 89.47 89.58 90.33 89.55 57.68 62.81 62.92 62.74 63. 67.86 67.60 68.37 68.75 69.26 Compression Rate Std Binomial 3x 8.02 1.38 3.01 0. 3.05 0.14 FlexıTokens λ1 FlexıTokens λ2 FlexıTokens λ3 8.04 0.89 8.35 0.87 8.77 1.21 3.11 0.13 3.21 0.15 3.43 0.18 3.09 0.08 3.22 0.31 3.36 0.13 In Table 12, we present results of finetuning our 119M and 1B parameter models for 3 epochs each. We observe that the FlexıTokens 1B model consistently outperforms FlexıTokens 119M on most tasks. 18 Table 12: Performance Comparison: FlexıTokens 119M vs FlexıTokens 1B across Multiple Tasks Task Model en es ru uk hi te Avg NER SIB-200 XNLI ILI Med. Abs Irony NER F1 Score / Accuracy FlexıTokens 119M FlexıTokens 1B FlexıTokens 119M FlexıTokens 1B FlexıTokens 119M FlexıTokens 1B FlexıTokens 119M FlexıTokens 1B FlexıTokens 119M FlexıTokens 1B FlexıTokens 119M FlexıTokens 1B 63.02 64.61 80.88 85.78 72.67 75.17 63.82 64. 68.37 67.22 73.81 77.66 75.49 83.82 70.24 72.44 66.87 69.69 74.51 86.27 66.01 68.64 77.55 79.53 73.53 84.31 57.64 63.61 66.67 77.94 62.36 64.41 90.43 89.32 48.62 52.77 67.16 81.86 65.77 69.62 Compression Rate Std 64.58 67.97 73.04 83.33 67.41 70.05 FlexıTokens 119M 3.44 0.31 3.46 0.34 FlexıTokens 1B 4.23 0.37 4.18 0.57 6.80 1.03 6.83 2.45 6.33 1.27 6.34 1. 8.82 3.37 8.91 3.37 9.16 2.55 9.17 2.55 6.46 1.62 6.48 1.76 SIB-200 FlexıTokens 119M 3.37 0.10 3.42 0.07 FlexıTokens 1B 4.27 0.20 4.14 0. 6.39 0.42 6.71 0.30 6.35 0.38 6.28 0.25 8.82 0.77 8.63 0.37 9.12 0.99 8.91 0.62 6.39 0.48 6.35 0.29 FlexıTokens 119M 3.55 0.05 3.30 0.03 FlexıTokens 1B 4.40 0.10 4.00 0.05 7.22 0.34 6.56 0.14 9.42 0.58 8.60 0.37 8.97 0.44 9.03 0.29 6.71 0.30 6.30 0.18 8.49 0.36 8.75 0.34 XNLI ILI FlexıTokens 119M FlexıTokens 1B Med. Abs FlexıTokens 119M 3.34 0.13 3.33 0.11 FlexıTokens 1B Irony FlexıTokens 119M 3.37 0.07 3.38 0.96 FlexıTokens 1B"
        },
        {
            "title": "G Full Ablation Results",
            "content": "We present the full ablation results as discussed in 4 in Table 4. All results in this section (13, 14, 15, 16, and 17) contain values for performance metrics like accuracy and F1 score, compression rates and standard deviation of the compression rates. Table 13: SIB-200 α Ablation: Accuracy and Compression Results Model en es ru uk hi te FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x 57.35 78.92 77.94 59.80 78.92 75. Accuracy 55.88 74.51 74.51 50.98 73.04 71.57 Compression Rate Std 47.06 62.75 69.12 51.47 58.82 66. Avg 53.76 71.16 72.55 FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x 19.37 8.23 5.75 0.65 3.32 0.27 16.23 4.45 6.78 0.71 3.92 0.31 24.57 6.82 12.58 1.91 6.49 0. 28.69 8.88 10.62 1.70 6.06 0.54 40.06 14.68 13.42 1.63 8.35 0.54 44.43 17.47 15.17 2.04 9.00 0.79 28.89 11.06 10.72 1.54 6.19 0.53 Table 14: WikiANN α Ablation: F1 Score and Compression Results Model en es ru uk hi te F1 Score FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x 61.81 62.84 63.96 75.48 75.81 76.23 66.90 67.48 67.55 76.90 77.68 77. 59.88 60.02 62.24 45.15 45.66 48.13 Compression Rate Std Avg 64.35 64.92 66.02 FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x 14.15 6.07 5.83 1.23 3.27 0.44 16.87 6.39 7.26 2.01 3.93 0.58 40.03 19.10 15.30 5.90 8.52 1.49 27.91 11.95 11.93 3.59 6.58 1.38 42.52 21.82 15.92 4.68 9.15 2.21 26.55 11.73 10.80 2.57 6.12 1. 28.01 14.14 11.17 3.69 6.26 1.33 19 Table 15: XNLI α Ablation: Accuracy and Compression Results Model en es ru hi te ur Avg Accuracy FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x 71.42 72.97 73.21 68.60 70.38 70.84 65.59 65.47 66.97 62.22 61.88 62.16 66.05 65.49 66. 57.52 56.71 57.58 65.23 65.48 66.25 Compression Rate Std FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x 13.41 2.88 6.06 0.72 3.36 0.26 15.88 3.12 7.59 0.88 4.10 0. 25.20 6.07 13.02 2.08 6.98 0.60 41.81 12.06 15.44 2.16 9.18 0.85 37.23 8.77 15.10 1.60 8.62 0.65 40.84 12.71 15.67 2.40 8.73 0.73 29.06 8.55 12.15 1.76 6.83 0.60 Table 16: Multilingual Sentiment α Ablation: Accuracy and Compression Results"
        },
        {
            "title": "Model",
            "content": "FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x es 77.67 77.74 77.77 ru"
        },
        {
            "title": "Accuracy",
            "content": "87.07 87.17 87.33 hi 54.24 52.71 53.12 Compression Rate Std avg 72.99 72.54 72. FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x 16.07 4.53 6.83 0.77 3.90 0.28 26.55 8.33 11.45 2.11 6.44 0.61 39.60 21.99 15.47 5.21 8.16 1.65 27.41 12.12 11.25 2.86 6.17 1.03 Table 17: ILI (hi) and Medical Abstract (en) λ Ablation: Accuracy and Compression Results"
        },
        {
            "title": "Model",
            "content": "ILI (hi) Med. Abstract (en) FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x"
        },
        {
            "title": "Accuracy",
            "content": "89.07 89.28 90.33 62.95 63.47 62.74 Compression Rate Std FlexıTokens 10x FlexıTokens 5x FlexıTokens 3x 38.80 16.75 14.82 3.00 8.35 0.87 13.22 2.15 5.63 0.33 3.21 0."
        }
    ],
    "affiliations": [
        "The Ohio State University",
        "University of Washington"
    ]
}