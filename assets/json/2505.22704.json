{
    "paper_title": "Training Language Models to Generate Quality Code with Program Analysis Feedback",
    "authors": [
        "Feng Yao",
        "Zilong Wang",
        "Liyuan Liu",
        "Junxia Cui",
        "Li Zhong",
        "Xiaohan Fu",
        "Haohui Mai",
        "Vish Krishnan",
        "Jianfeng Gao",
        "Jingbo Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Code generation with large language models (LLMs), often termed vibe coding, is increasingly adopted in production but fails to ensure code quality, particularly in security (e.g., SQL injection vulnerabilities) and maintainability (e.g., missing type annotations). Existing methods, such as supervised fine-tuning and rule-based post-processing, rely on labor-intensive annotations or brittle heuristics, limiting their scalability and effectiveness. We propose REAL, a reinforcement learning framework that incentivizes LLMs to generate production-quality code using program analysis-guided feedback. Specifically, REAL integrates two automated signals: (1) program analysis detecting security or maintainability defects and (2) unit tests ensuring functional correctness. Unlike prior work, our framework is prompt-agnostic and reference-free, enabling scalable supervision without manual intervention. Experiments across multiple datasets and model scales demonstrate that REAL outperforms state-of-the-art methods in simultaneous assessments of functionality and code quality. Our work bridges the gap between rapid prototyping and production-ready code, enabling LLMs to deliver both speed and quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 0 7 2 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Training Language Models to Generate Quality Code\nwith Program Analysis Feedback",
            "content": "Feng Yao1 Zilong Wang1 Liyuan Liu2 Junxia Cui1 Li Zhong1 Xiaohan Fu1 Haohui Mai3 Vish Krishnan1 Jianfeng Gao2 Jingbo Shang1 1University of California, San Diego 2Microsoft Research, 3CausalFlow Inc. {fengyao, zlwang, jshang}@ucsd.edu {lucliu, jfgao}@microsoft.com"
        },
        {
            "title": "Abstract",
            "content": "Code generation with large language models (LLMs), often termed vibe coding, is increasingly adopted in production but fails to ensure code quality, particularly in security (e.g., SQL injection vulnerabilities) and maintainability (e.g., missing type annotations). Existing methods, such as supervised fine-tuning and rule-based post-processing, rely on labor-intensive annotations or brittle heuristics, limiting their scalability and effectiveness. We propose REAL, reinforcement learning framework that incentivizes LLMs to generate production-quality code using program analysis-guided feedback. Specifically, REAL integrates two automated signals: (1) program analysis detecting security or maintainability defects and (2) unit tests ensuring functional correctness. Unlike prior work, our framework is prompt-agnostic and reference-free, enabling scalable supervision without manual intervention. Experiments across multiple datasets and model scales demonstrate that REAL outperforms state-of-the-art methods in simultaneous assessments of functionality and code quality. Our work bridges the gap between rapid prototyping and production-ready code, enabling LLMs to deliver both speed and quality."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have revolutionized code generation, enabling rapid workflows colloquially termed vibe coding. Coding assistants like Copilot [GitHub, 2021], Cursor, and Windsurf [Xu et al., 2022]) exemplify this shift, with developers increasingly relying on LLMs to automate tasks from prototyping to production, highlighting the critical need to ensure code quality. In production settings, code quality extends beyond functional correctness to encompass: security (e.g., resistance to injection attacks or misuse of unsafe functions) and maintainability (e.g., proper type annotations, consistent style, and modular structure). These properties are crucial for long-term reliability and team collaboration. However, LLMs are known to generate code that is syntactically plausible but flawed in subtle or dangerous ways [Yang et al., 2024, Wan et al., 2024]. Existing methods improve code quality either through supervised fine-tuning on large corpora of manually curated, vulnerability-free code [He et al., 2024] or by applying rule-based post-processing at inference to enforce security constraints [Fu et al., 2024, Nazzal et al., 2024]. The former incurs high annotation costs, while the latter depends on hand-crafted constraints specific to each coding task. Both strategies exhibit limited scalability and effectiveness in real-world production scenarios. Equal contribution. Preprint. Under review. Figure 1: Overview of the REAL framework. Given coding task, the LLM policy generates candidate program, which is then evaluated along two automated axes: (1) Vulnerability Detector applies program analysis to flag security and maintainability defects, (2) Functionality Verifier runs unit tests to assess correctness. The two reward signals are averaged and fed into policy-gradient update, steering the LLM toward high-quality, functionally correct code with minimal human effort. We introduce REAL (Reinforcement rEwards from Automated program anaLysis), reinforcement learning framework that trains LLMs to generate quality code through program analysis-guided feedback. Unlike prior methods that either teach LLMs to mimic human-verified code examples or correct their outputs post hoc with brittle heuristics, REAL employs verifiable and reference-free reward signals to incentivize quality code generation with minimal human efforts. As illustrated in Figure 1, REALs compound reward combines: (1) program analysisbased detection of vulnerabilities in security or maintainability, and (2) unit-testbased verification of functional correctness. To demonstrate REALs efficacy, we evaluate it across multiple benchmarks spanning diverse production scenarios, assessing code quality along two key dimensions. (1) For security evaluation, we augment SecCodePLT dataset [Yang et al., 2024] with program analysis-based detector built by us that effectively identifies 17 Common Weakness Enumerations (CWEs) A.1, resulting in an enhanced benchmark we term SecCodePLT+. To enable fine-grained evaluation of high-impact vulnerabilities, we additionally introduce SafeSQL, targeted dataset featuring realistic database query tasks susceptible to SQL injection attacks. (2) For maintainability assessment, we augment APPS dataset [Hendrycks et al., 2021] to APPS+ with comprehensive static analysis, including type checking, unreachable code detection, and function signature verification for Python code. In addition to code quality evaluation, we assess functional correctness using unit tests. To provide holistic evaluation, we introduce composite metric that jointly measures both functionality and quality. This resolves critical gap: structurally sound code that fails functionally should not be prioritized. By integrating both objectives, our evaluation reflects the real-world needs, where code quality and functionality are inseparable. Extensive experiments across diverse benchmarks and model sizes demonstrate that REAL consistently outperforms state-of-the-art baselines, confirming its scalability and effectiveness in delivering reliable and production-quality code generation. To summarize, our contributions2 are three-fold: We propose REAL, novel reinforcement learning framework that integrates program analysis as automated feedback, enabling LLMs to generate quality code with minimal manual intervention. We contribute three datasets for quality code generation: (1) SecCodePLT+, enhancing [Yang et al., 2024] with detectors for 17 CWEs, (2) APPS+, augmenting [Hendrycks et al., 2021] with static analysis for maintainability, and (3) SafeSQL, targeted dataset for SQL injection vulnerabilities. We design holistic evaluation protocol that jointly prioritizes functionality and code quality, resolving the oversight of prior work that treats these objectives independently. 2Our code and datasets are released at https://github.com/yaof20/ReaL.git"
        },
        {
            "title": "2 Problem Formulation",
            "content": "In this section, we first formalize the concept of quality code we investigate in this paper ( 2.1), then outline the existing paradigms and their limitations that motivate our work ( 2.2), and finally introduce our task formulation and holistic evaluation protocol for quality code generation ( 2.3)."
        },
        {
            "title": "2.1 Quality Code: Beyond Functional Correctness",
            "content": "In real-world production, quality code should be both functionally correct and vulnerability resistant. In this work, we concentrate on two classes of vulnerabilities that critically impact production code: Security Vulnerabilities. These include exploits such as SQL injection and Cross-Site Request Forgery (CSRF), corresponding to entries in the Common Weakness Enumeration (CWE) [MITRE, 2024]. These vulnerabilities pose significant risks and critical threats in production environments. Maintainability Vulnerabilities. In dynamically typed languages (e.g., Python), the absence of explicit type information, unreachable code paths, or inconsistent function signatures often leads to latent bugs, runtime errors, and degrade long-term code reliability and maintainability."
        },
        {
            "title": "2.2 Limitations of Existing Approaches",
            "content": "Existing methods for quality code generation have primarily targeted reducing security vulnerabilities, with limited attention to maintainability aspects. We further identify some other critical limitations in both their evaluation protocols and methodological frameworks as follows. First, prior work suffers from incomplete evaluation protocols, manifesting in three key shortcomings: Quality-Functionality Isolation. Code quality and functionality are evaluated separately using disjoint datasets [He et al., 2024] or omitting functionality entirely [Bhatt et al., 2023], failing to capture real-world production requirements of satisfying both criteria at the same time. Single-Vulnerability Assumption. Evaluations by default assume each coding problem contains only one vulnerability type and rely on this false premise for methodology design [Fu et al., 2024], ignoring production scenarios where the code can involve multiple defects simultaneously. Limited Detection Paradigms. Existing evaluations rely on two constrained strategies: (1) Unreliable static analyzers like CodeQL [GitHub, 2019], which we found unexpectedly ineffective for SecCodePLT+, and (2) Handcrafted unit tests [Yang et al., 2024], which inherently have limited coverage and do not support maintainability issues (e.g., type checking). Second, existing methods for quality code generation follow two paradigms with inherent trade-offs: Over-Reliance on Human Annotations. Data-driven approaches like supervised finetuning heavily depend on extensive human annotations of vulnerability-free code examples, which are labor-intensive, costly, and impractical for scaling [He and Vechev, 2023, He et al., 2024]. Presumption of Vulnerability Knowledge. Training-free methods enforce predefined rules for each coding problem (e.g., filtering code with insecure patterns) that inherently presume prior knowledge of potential vulnerabilities [Fu et al., 2024, Nazzal et al., 2024]. This creates paradox: avoiding known vulnerabilities renders generation redundant, while unknown ones evade detection. 2."
        },
        {
            "title": "Investigation Setup",
            "content": "Task Formulation. We investigate quality code generation in two scenarios: (1) security-sensitive tasks requiring robust mitigation of vulnerabilities (e.g., generating database queries resistant to SQL injection shown in Figure 1), and (2) maintainability-aware tasks demanding adherence to structural best practices (e.g., Python code with type annotations). These scenarios reflect real-world demands where code must simultaneously achieve functional correctness and defect resistance. Holistic Evaluation. Our protocol addresses prior evaluation limitations through three principles: (1) jointly assess quality-functionality: we evaluate code on quality and functionality simultaneously, (2) detect multiple vulnerabilities: we detect multiple security vulnerabilities and maintainability issues via program analysis ( 3.1), and (3) propose unified metrics: we introduce holistic metrics that prioritize functionality and quality jointly (detailed in 4.1)."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we propose REAL to address the limitations discussed in 2.2 by integrating program analysis as feedback in model training. We first present the development of our vulnerability detector the program analysis tool tailored for code quality ( 3.1), and then describe how its outputs are combined with functionality unit tests to form hybrid reward for reinforcement learning ( 3.2)."
        },
        {
            "title": "3.1 Vulnerability Detector",
            "content": "As noted in 2.1, quality code extends beyond functional correctness to vulnerability resistance. While correctness can be validated with unit tests, vulnerability detection is significantly more challenging: unit tests offer limited coverage and are expensive to craft for each problem. In contrast, program analysis provides scalable and general solution, capable of identifying broad range of issues without task-specific design. To leverage this, we develop dedicated vulnerability detectors based on program analysis techniques, targeting both security and maintainability. Security Vulnerability. It encompasses wide spectrum of weaknesses in software development. In REAL, we target total of 18 CWEs ( A.1) covered by SecCodePLT+ and SafeSQL, such as path traversal, command injection, and cross-site scripting. We develop static analysis to identify vulnerabilities in the code. On high level, most of the analysis follows the tactics of information flow analysis [Myers, 1999]: it systematically checks whether there exists code path where sensitive information or unsanitized inputs (i.e., sources) go to undesired destinations (i.e., sinks). Take SQL injection as an example. It arises when unsanitized user inputs are directly embedded into database queries, allowing attackers to execute arbitrary SQL statements. To detect such issues, the detector transforms the program into Static Single Assignment (SSA) form [Aho et al., 2006] to analyze control flow and data dependencies. It treats user inputs and database APIs as sources and sinks, then traverses the control flow graph to identify data flows connecting unsanitized inputs to APIs without proper sanitation. In Figure 1, path connects the user inputs (max, min) to the database API (cur.execute()) without safeguards such as parameterized queries, indicating vulnerability. Compared to other tools [GitHub, 2019, PyCQA, 2014, Bearer, 2021] that emphasize precisions, REALs analysis focuses more on soundness (i.e., identifying vulnerabilities more comprehensively) to guide the RL process to generate codes that are easier to reason about. We find that REALs context-insensitive, flow-sensitive analysis is sufficient for detecting vulnerabilities, which is typically short and self-contained. REAL currently uses heuristics to conservatively identify sanitation and applies the same analysis principles to other types of vulnerability. Maintainability Vulnerability. Beyond security vulnerabilities, we also address maintainability requirements essential to modern software development, where generated code must follow strict standards to ensure long-term reliability. Such requirements include, but are not limited to, enforcing proper type annotations throughout the codebase, eliminating unused or redundant code segments, and following consistent naming conventions across modules and functions. To assess maintainability, we use MyPy [Lehtosalo, 2025], static analysis tool for Python that inspects the abstract syntax tree and performs type inference to detect missing annotations, type mismatches, implicit conversions, and other quality issueswithout executing the code. We apply MyPy to model-generated code to extract rich signals that guide reinforcement learning toward producing maintainable code. Figure 2: Maintainability issues detected by MyPy"
        },
        {
            "title": "3.2 Reinforcement Learning with Hybrid Rewards",
            "content": "REAL incorporates both code quality and functionality rewards into the reinforcement learning framework, guiding the model to generate code that is both correct and vulnerability resistant. 4 Motivation. While our vulnerability detectors can automatically identify defects and provide feedback during training, optimizing for code quality alone leads to reward hacking. In early experiments on the PurpleLLaMA dataset [Bhatt et al., 2023], the model learned to produce trivial outputssuch as empty code or commentsthat maximize quality scores while lacking functionality. This failure mode underscores the need to jointly optimize for both code quality and functionality during training. To this end, we introduce REAL, reinforcement learning framework with hybrid rewards that balances these two objectives, promoting code that is both safe and functionally meaningful. Framework. We adopt Proximal Policy Optimization (PPO) [Schulman et al., 2017] as our reinforcement learning algorithm, following standard framework guided by our enhanced reward design. In REAL, candidate programs are generated by the policy model, i.e., ˆy = πθ(x), where πθ is the policy parameterized by θ, and represents the problem description. The generated programs ˆy are then evaluated from two perspectives: code quality and functional correctness. Quality Reward. We pass the generated candidate program ˆy through our curated vulnerability detector to check whether it is safe in terms of security or maintainability. We denote the reward provided by the detector as rquality rquality = Detector(ˆy) = (cid:26)1, if no vulnerabilities are detected 0, otherwise where Detector(ˆy) is binary reward function that assigns positive reward only when the generated program ˆy passes the vulnerability checks without any detected security or maintainability issues. Functionality Reward. we follow prior work and use unit tests as verifiable reward signal for functionality [Guo et al., 2025]. Correctness evaluates the functionality of each specific task, making it hard to develop universal detectors similar to those for security and maintainability. Finally, we measure the pass rate of the unit tests as our functionality reward: rfunction ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 1 {fˆy(inpi) = outi} where is the total number of unit tests, 1{fˆy(inpi, outi)} denotes whether the candidate program ˆy generates ground truth result outi against the i-th unit test inpi as expected. Hybrid Reward. Taking both vulnerability concerns and functional correctness into account, our final hybrid reward is formulated as, rhybrid = α rquality + (1 α) rfunction where α [0, 1] is weighting coefficient that controls the trade-off between code quality and functionality. If the policy model πθ fails to generate runnable code (e.g., due to syntax errors, runtime exceptions, etc.), we assign penalty reward of 1. After obtaining the hybrid reward, we estimate the advantage using Generalized Advantage Estimation (GAE) and update the policy model with the PPO clipped loss to ensure stable learning. This process iteratively improves the models ability to generate quality yet functional code."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we verify the effectiveness of REAL in both security-sensitive and maintainabilityaware tasks. We first introduce the experimental setup ( 4.1), then present comparative results ( 4.2) and ablation studies ( 4.3) to validate our design choices, and conclude with case study ( 4.4)."
        },
        {
            "title": "4.1 Experiment Settings",
            "content": "We evaluate REAL across three curated code generation benchmarks that cover broad range of security-sensitive and maintainability-aware coding problems. Each task requires the model to generate functionally correct code while meeting specific quality constraints. 5 Table 1: Overview of the benchmarks. Task/solution lengths are averaged value measured in tokens."
        },
        {
            "title": "Source",
            "content": "SecCodePLT+ SafeSQL APPS+ 655 339 2,038 164 85 519 224 337 373 128 203 152 Maintainability-Aware Security-Sensitive Security-Sensitive"
        },
        {
            "title": "Enriched\nConstructed\nEnriched",
            "content": "Benchmarks. Given the scarcity of benchmarks for evaluating the overall quality of generated code, we curate the benchmarks used in our experiments by extending existing datasets or evolving data with large language models [Luo et al., 2024]. Specifically, we employ SecCodePLT+ and SafeSQL to study security vulnerabilities, and APPS+ to study maintainability concerns. The details of these datasets are described below, and summary statistics are provided in Table 1. SecCodePLT+: We enhance the original SecCodePLT [Yang et al., 2024] dataset by integrating dedicated vulnerability detectors ( 3.1) for each associated CWE category, resulting in unified and comprehensive evaluation platform for assessing security risks in code generation. SafeSQL: We construct SafeSQL dataset by evolving seed programs using GPT-4.1 [OpenAI, 2025], focusing on SQL injection vulnerabilities. Each task involves generating code that constructs SQL queries resistant to injection attacks while retrieving correct results from given database. APPS+: We construct APPS+ by filtering and verifying subset of APPS [Hendrycks et al., 2021], then augmenting it with maintainability checker ( 3.1). The benchmark evaluates whether models can solve algorithmic problems while producing clear, maintainable, and robust code. Evaluation. We evaluate REAL along two key dimensions: functionality and quality. For each dimension, we report the Pass Rate as the metric, representing the percentage of tasks that pass all the unit tests or pass the vulnerability detector, respectively. To jointly assess both dimensions, we compute the Pass Rate by requiring the generated code to meet both criteria simultaneously. Baselines. For the security-sensitive scenario, we consider two categories of state-of-the-art secure code generation methods: (1) Data-driven methods, including SVEN [He and Vechev, 2023] and SafeCoder [He et al., 2024], which finetune LLMs on curated vulnerability-free code, and supervised finetuning (SFT) baseline trained directly on ground-truth safe solutions from our dataset. (2) Trainingfree methods, such as CodeGuard+[Fu et al., 2024], which constrains decoding to favor secure outputs, and PromSec[Nazzal et al., 2024], which refines prompts using GAN-based feedback. For the maintainability-aware scenario, where no existing methods target maintainable code generation, we introduce: (1) prompt-based baseline that explicitly instructs the model to generate maintainable code, and (2) an SFT baseline trained on ground-truth maintainable solutions. All methods use Qwen2.5-Coder-Instruct [Hui et al., 2024] as the backbone, evaluated at 0.5B, 3B, and 7B scales."
        },
        {
            "title": "4.2 Quantitative Results",
            "content": "We evaluate the performance of REAL and baseline methods across both security-sensitive and maintainability-aware scenarios. The results are summarized in Table 2 and Table 3, respectively. Security-Sensitive Scenario. The results in Table 2 highlight three key findings. First, REAL consistently achieves the best overall performance on the SafeSQL benchmark across all model sizes, outperforming all baselines in functionality, security quality, and joint metrics. Second, on SecCodePLT+, REAL leads in security quality and joint metrics at the 3B and 7B scales. However, at the 0.5B scale, supervised finetuning (SFT) slightly outperforms REAL. This gap is relatively small and can be attributed to the limited capacity of the 0.5B modelreinforcement learning often requires reasonably strong base model. Third, training-free methods like CodeGuard+ and PromSec perform poorly across most metrics, highlighting the limitations of decoding-time interventions for secure code generation. Overall, REAL demonstrates strong scalability and robust balance between functionality and security, validating the its effectiveness in security-sensitive tasks. Maintainability-Aware Scenario. According to Table 3, REAL achieves the best overall performance across all metrics and model sizes. It surpasses both prompt-based and supervised finetuning (SFT) baselines in functionality, maintainability quality, and their joint measurement. The improvements are especially clear in the joint metrics, where REAL significantly outperforms all alternatives, 6 Table 2: Performance comparison of REAL and baseline models on security-sensitive tasks across different model scales. (Bold indicates the best performance; underline indicates the second-best.) # Params"
        },
        {
            "title": "Method",
            "content": "SecCodePLT+"
        },
        {
            "title": "Quality",
            "content": "Func.-Qual."
        },
        {
            "title": "Quality",
            "content": "Func.-Qual. 0.5B 3B 7B Vanilla SafeCoder SVEN CodeGuard+ PromSec SFT"
        },
        {
            "title": "REAL",
            "content": "Vanilla SafeCoder SVEN CodeGuard+ PromSec SFT"
        },
        {
            "title": "REAL",
            "content": "Vanilla SafeCoder SVEN CodeGuard+ PromSec SFT"
        },
        {
            "title": "REAL",
            "content": "0.1280 0.1524 0.1707 0.0732 0.1341 0.8720 0.5854 0.2805 0.3476 0.3476 0.2927 0.2134 0.8902 0.7378 0.2988 0.3293 0.3171 0.2988 0.2195 0.8659 0. 0.3598 0.3963 0.3780 0.5061 0.3537 0.5061 0.7988 0.3354 0.4146 0.4024 0.3963 0.4146 0.5061 0.8476 0.3902 0.3902 0.4024 0.3476 0.4878 0.5122 0. 0.0366 0.0488 0.0549 0.0061 0.0366 0.4573 0.3963 0.1098 0.1585 0.1646 0.1280 0.0854 0.4573 0.6037 0.1098 0.1402 0.1280 0.1098 0.0976 0.4634 0. 0.4118 0.3412 0.3176 0.3176 0.3529 0.5765 0.7647 0.6000 0.4824 0.4471 0.5882 0.2000 0.7529 0.8471 0.6471 0.4706 0.5059 0.6353 0.0941 0.8118 0. 0.5412 0.8824 0.8824 0.4471 0.7412 0.8706 0.8941 0.4588 0.8706 0.9294 0.4824 0.9647 0.9176 1.0000 0.8824 0.8941 0.9176 0.8824 0.8588 0.8941 1. 0.2000 0.3294 0.3059 0.0824 0.2235 0.5527 0.6941 0.2706 0.4471 0.4353 0.2471 0.1882 0.6824 0.8471 0.5882 0.4353 0.4706 0.5647 0.0824 0.7294 0. demonstrating its effectiveness in generating not only correct but also clean and maintainable code. Furthermore, REAL scales well with model size delivering consistent gains in both functionality and joint performance as the model capacity increases from 0.5B to 7B. These results collectively demonstrate the effectiveness of REAL across wide range of real-world production scenarios, including both security-sensitive and maintainability-aware tasks. By jointly optimizing for correctness and code quality through reinforcement learning with program analysis feedback, REAL consistently improves performance across all metrics and model sizesoutperforming strong baselines without sacrificing either dimension significantly."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Table 3: Performance comparison of REAL and baselines on maintainability-sensitive tasks."
        },
        {
            "title": "REAL",
            "content": "0.5B 3B 7B APPS+"
        },
        {
            "title": "Function Quality",
            "content": "Func.-Qual. 0.1965 0.1888 0.2274 0.3064 0.4990 0.4913 0.4586 0.5549 0.5896 0.5645 0. 0.6667 0.2177 0.1888 0.7476 0.9557 0.1407 0.1946 0.8189 0.9268 0.1580 0.2312 0. 0.9229 0.0501 0.0597 0.1888 0.2909 0.0983 0.1272 0.4046 0.5241 0.1079 0.1599 0. 0.6204 In this section, we analyze the impact of key design choices in our proposed REAL framework, focusing on (1) the hybrid reward balancing code quality and functionality, and (2) program analysis versus unit tests for quality supervision during reinforcement learning. Hybrid Reward vs. Single Reward Our proposed REAL framework incorporates hybrid reward that combines correctness unit tests (for functionality) with program analysis (for quality). To better understand the contribution of each component, we conduct an ablation study where we isolate the reward signal to either functionality-only (unit tests) or quality-only (program analysis), keeping the overall training pipeline unchanged. The results are presented in Table 4. We observe that when using only the functionality reward, the model achieves strong functional correctness but suffers from notable decline in security quality. Conversely, when using only the quality reward, the model generates safer code but at the expense of reduced functional correctness. In both cases, the combined metric drops significantly, indicating lack of balance. In contrast, adopting the hybrid reward leads to substantial improvement in the joint functionalityquality metric. While there is slight trade-off in each individual dimension compared to their respective single-reward counterparts, the hybrid approach enables the model to achieve balanced optimization. This balance results in significantly better overall performance, demonstrating that our method effectively harmonizes the competing objectives of functionality and quality. Table 4: Comparison of training strategies using functionality-only, quality-only, and hybrid rewards in both security-sensitive and maintainability-aware scenarios. (Bold indicates the best performance; underline indicates the second-best.) Method Vanilla w/ rFunction w/ rquality REAL Vanilla w/ rFunction w/ rquality REAL Vanilla w/ rFunction w/ rquality REAL 0.5B 3B 7B SecCodePLT+ SafeSQL APPS+ Function Quality Func.-Qual. Function Quality Func.-Qual. Function Quality Func.-Qual. 0.1280 0.7317 0.0732 0.5854 0.2805 0.7683 0. 0.7378 0.2988 0.7805 0.3415 0.7561 0.3598 0.1402 0.9268 0.7988 0.3354 0.2805 0. 0.8476 0.3902 0.3476 0.8354 0.8293 0.0366 0.0854 0.0732 0.3963 0.1098 0.2073 0. 0.6037 0.1098 0.2866 0.2866 0.6159 0.4118 0.7765 0.3529 0.7647 0.6000 0.8588 0. 0.8471 0.6471 0.8706 0.6353 0.8588 0.5412 0.0471 1.0000 0.8941 0.4588 0.0824 1. 1.0000 0.8824 0.0588 1.0000 1.0000 0.2000 0.0235 0.3529 0.6941 0.2706 0.0588 0. 0.8471 0.5882 0.0353 0.6353 0.8588 0.1965 0.2505 0.0443 0.3064 0.4990 0.5607 0. 0.5549 0.5896 0.6667 0.5645 0.6667 0.2177 0.1734 0.9981 0.9557 0.1407 0.1464 0. 0.9268 0.1580 0.1503 0.9441 0.9229 0.0501 0.0751 0.0443 0.2909 0.0983 0.1137 0. 0.5241 0.1079 0.1214 0.5414 0.6204 Program Analysis vs. Unit Test While our REAL framework uses program analysis as feedback to train models for generating quality code, prior work has explored using unit tests to evaluate security properties [Yang et al., 2024, Dai et al., 2025]. Although not originally intended for training, these unit tests can also be repurposed as reward signals in reinforcement learning. To assess the effectiveness of our design choice, we compare these two reward strategies: (1) using safety unit tests, and (2) using our program analysisbased detector. We conduct experiments on the SecCodePLT+ and SafeSQL datasets, both of which provide safety unit tests for each coding problemallowing for fair comparison. As illustrated in Table 5, models trained with our program analysisbased feedback consistently outperform those trained with unit tests across functionality, quality, and their conjunction, demonstrating the robustness and scalability of our approach across all model sizes. Table 5: Comparison of training with program analysisbased rewards (REAL) versus safety unit tests across different model sizes on the SecCodePLT+ and SafeSQL datasets. REAL consistently outperforms unit testbased training across functionality, security quality, and their conjunction. (Bold indicates the best performance.) # Params rquality SecCodePLT+"
        },
        {
            "title": "Function Quality",
            "content": "Func.-Qual."
        },
        {
            "title": "Function Quality",
            "content": "Func.-Qual. 0.5B 3B 7B w/ Safety Unit Tests w/ Detector (REAL) w/ Safety Unit Tests w/ Detector (REAL) w/ Safety Unit Tests w/ Detector (REAL) 0.4573 0.5854 0.6524 0.7378 0.6341 0.7561 0.3415 0.7988 0.4634 0. 0.4695 0.8293 0.1341 0.3963 0.2439 0.6037 0.3171 0.6159 0.4588 0.7647 0.8235 0. 0.8471 0.8588 0.8235 0.8941 0.8824 1.0000 0.8941 1.0000 0.3882 0.6941 0.7412 0. 0.7647 0."
        },
        {
            "title": "4.4 Case Study",
            "content": "In Figure 3, we illustrate the code generated by REAL at different training stages of reinforcement learning with hybrid rewards on the SafeSQL benchmark. We observe that the individual reward components within the hybrid reward framework converge at different rates, with the code quality reward saturating earlier than the unit test-based correctness reward. In this example, the task requires the model to correctly interpret the constraints described in the task prompt and construct an appropriate SQL query to retrieve the desired results from the database. At the initial stage, the generated code is vulnerable and incorrect. It suffers from SQL injection vulnerability by directly incorporating user inputs into the SQL query through string formatting without proper sanitization. Additionally, the semantics of the query are problematic, as the task specifies the use of an OR condition for the price constraint rather than an AND condition. As training progresses, the model gradually learns to satisfy the security requirement but the constructed query remains semantically incorrect. 8 The generated code adopts parameterized queries, leveraging the implicit sanitization provided by the sqlite3 library (e.g., cursor.execute(query, (room_type, ...))). In parameterized execution, placeholders (i.e., ?) are used in the SQL query, and the corresponding user inputs are safely injected through the provided parameters, automatically handling escaping and preventing injection attacks (e.g., escaping special characters and enforcing correct data types). By the final phase of training, the model successfully learns to both generate correct SQL query while securely using parameterized execution. Notably, the hybrid reward further guides the model to apply explicit input sanitization by enforcing proper type conversion (e.g., max_price = float(input(...))), further demonstrating the effectiveness of our proposed REAL in producing high-quality, secure code and enabling seamless, confident vibe coding experience. Figure 3: Examples of code generated by REAL 0.5B at different training stages on SafeSQL with hybrid rewards. Initially, the model produces incorrect and insecure code, misinterpreting \"or\" as AND and directly incorporating unsanitized user inputs. Later on, it adopts parameterized execution (using ? placeholders in the query with separate parameter binding) to implicitly address vulnerabilities. Finally, it corrects the query logic and explicitly sanitizes user inputs with proper type conversion (using float() to enforce correct type conversion of the input)."
        },
        {
            "title": "5 Related Work",
            "content": "Secure Code Generation Existing secure code generation methods generally follow two paradigms: (1) Data-driven approaches apply supervised fine-tuning to train LLMs on large corpora of secure code [He et al., 2024, Yang et al., 2024], under the assumption that mimicking clean code is sufficient for generalization. While effective against known vulnerabilities, these methods struggle with novel defects and require extensive human annotation. (2) Training-free approaches rely on rule-based post-processing, either enforcing security constraints during decoding [Fu et al., 2024] or using feedback or in-context examples to iteratively refine prompts [Nazzal et al., 2024, Zhang et al., 2024]. However, such heuristics are task-specific, brittle to unseen vulnerabilities, and vulnerable to vulnerability-aware leakage, where models exploit patterns in the rules to evade detection. Some recent work explores reinforcement learning to fix vulnerabilities, but these methods still depend on ground-truth code for each problem and define rewards based on semantic equivalence to those referencesthus still requiring costly annotations [Islam et al., 2024]. In contrast, our method does not rely on ground-truth annotations or prior knowledge of specific vulnerabilities. Reinforcement Learning for Code Generation Reinforcement learning has recently shown to be highly effective in improving model capabilities in tasks that come with \"verifiable rewards\" such as math and coding [Guo et al., 2025, OpenAI, 2025]. For code generation, reward signals can be determined by executing the candidate code against unit tests or testcases [Le et al., 2022, Yang et al., 2024, Yu et al., 2024, Gehring et al., 2025, Wei et al., 2025]. Since the availability of unit tests may be limited in practice, [Li et al., 2024] explores generating unit tests for specific coding tasks automatically in scale. In addition to unit tests derived rewards, [Dou et al., 2024] incorporates compiler feedback to address the challenge of long code sequence, while [Xie et al., 2025] explores using LLM themselves as critic to improve the code generation. However, these work all focus on enhancing the functionality/correctness of the generation, but not code quality."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented REAL, reinforcement learning framework that leverages program analysis to guide LLMs toward generating quality code that is both functionally correct and vulnerability resistant. By integrating feedback from program analysis and functionality verification, REAL enables scalable training without relying on human-written references or handcrafted rules. Extensive experimental results demonstrate the effectiveness of REAL. While our current vulnerability detectors are designed to prioritize soundness and generality, they rely on heuristic approximations and do not yet cover the full breadth of CWE types. In the future, we will explore more robust and comprehensive detectors to expand coverage, enabling even more reliable feedback for large-scale training."
        },
        {
            "title": "References",
            "content": "Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. Compilers: Principles, Techniques, and Tools (2nd Edition). Addison-Wesley Longman Publishing Co., Inc., USA, 2006. ISBN 0321486811. Bearer. Bearer: Static application security testing (sast) tool. https://github.com/Bearer/ bearer, 2021. Accessed: 2025-05-15. Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, et al. Purple llama cyberseceval: secure coding benchmark for language models. arXiv preprint arXiv:2312.04724, 2023. Shih-Chieh Dai, Jun Xu, and Guanhong Tao. comprehensive study of llm secure code generation. arXiv preprint arXiv:2503.15554, 2025. Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Wei Shen, Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan, et al. Stepcoder: Improve code generation with reinforcement learning from compiler feedback. arXiv preprint arXiv:2402.01391, 2024. Yanjun Fu, Ethan Baker, Yu Ding, and Yizheng Chen. Constrained decoding for secure code generation, 2024. URL https://arxiv.org/abs/2405.00218. Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2025. URL https://arxiv.org/abs/2410.02089. GitHub. Codeql: Semantic code analysis engine. https://codeql.github.com/, 2019. Accessed: 2025-05-15. GitHub. Github copilot:"
        },
        {
            "title": "Your",
            "content": "ai pair programmer. https://github.blog/ 2021-06-29-github-copilot-ai-pair-programmer/, 2021. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jingxuan He and Martin Vechev. Large language models for code: Security hardening and adversarial testing. In ACM CCS, 2023. URL https://arxiv.org/abs/2302.05319. Jingxuan He, Mark Vero, Gabriela Krasnopolska, and Martin T. Vechev. Instruction tuning for secure code generation. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= MgTzMaYHvG. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. NeurIPS, 2021. 10 Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Nafis Tanveer Islam, Mohammad Bahrami Karkevandi, and Peyman Najafirad. Code security vulnerability repair using reinforcement learning with large language models. arXiv preprint arXiv:2401.07031, 2024. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning, 2022. URL https://arxiv.org/abs/2207.01780. Jukka Lehtosalo. Mypy: Optional static typing for python. https://mypy-lang.org/, 2025. Version accessed: May 2025. Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. Acecoder: An effective prompting technique specialized in code generation. ACM Transactions on Software Engineering and Methodology, 33 (8):126, 2024. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. In The Twelfth International Conference on Learning Representations, 2024. MITRE. Common weakness enumeration (cwe). MITRE Corporation, 2024. URL https://cwe. mitre.org/index.html. Accessed: 2025-05-14. Version 2025a. Andrew C. Myers. Jflow: practical mostly-static information flow control. In Proceedings of the 26th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 99, pages 228241, New York, NY, USA, 1999. Association for Computing Machinery. ISBN 1581130953. doi: 10.1145/292540.292561. URL https://doi.org/10.1145/292540.292561. Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, and NhatHai Phan. Promsec: Prompt optimization for secure generation of functional source code with large language models In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Com- (llms). munications Security, CCS 24, pages 22662280, New York, NY, USA, 2024. Association ISBN 9798400706363. doi: 10.1145/3658644.3690298. URL for Computing Machinery. https://doi.org/10.1145/3658644.3690298. OpenAI. Gpt-4.1, 2025. https://openai.com/index/gpt-4-1/. PyCQA. Bandit: Security linter for python source code. https://github.com/PyCQA/bandit, 2014. Accessed: 2025-05-15. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shengye Wan, Cyrus Nikolaidis, Daniel Song, David Molnar, James Crnkovich, Jayson Grace, Manish Bhatt, Sahana Chennabasappa, Spencer Whitman, Stephanie Ding, Vlad Ionescu, Yue Li, and Joshua Saxe. Cyberseceval 3: Advancing the evaluation of cybersecurity risks and capabilities in large language models, 2024. URL https://arxiv.org/abs/2408.01605. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution, 2025. URL https://arxiv.org/abs/2502. 18449. Zhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, et al. Teaching language models to critique via reinforcement learning. arXiv preprint arXiv:2502.03492, 2025. Frank Xu et al. Windsurf: Empowering developers with ai-assisted code completion. arXiv preprint arXiv:2212.10943, 2022. Yu Yang, Yuzhou Nie, Zhun Wang, Yuheng Tang, Wenbo Guo, Bo Li, and Dawn Song. Seccodeplt: unified platform for evaluating the security of code genai, 2024. URL https://arxiv.org/ abs/2410.11096. 11 Zishun Yu, Yunzhe Tao, Liyu Chen, Tao Sun, and Hongxia Yang. B-coder: Value-based deep reinforcement learning for program synthesis, 2024. URL https://arxiv.org/abs/2310. 03173. Boyu Zhang, Tianyu Du, Junkai Tong, Xuhong Zhang, Kingsum Chow, Sheng Cheng, Xun Wang, and Jianwei Yin. Seccoder: Towards generalizable and robust secure code generation. arXiv preprint arXiv:2410.01488, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 CWE List We provide list of Common Weakness Enumerations (CWEs) we cover in the paper in A."
        },
        {
            "title": "CWE ID",
            "content": "74 77 79 89 94 200 327"
        },
        {
            "title": "CWE RISKY SCENARIOS",
            "content": "Improper Neutralization of Special Elements in Output Used by Downstream Component (Injection) Improper Neutralization of Special Elements used in Command (Command Injection) Improper Neutralization of Input During Web Page Generation (Cross-site Scripting) Improper Neutralization of Special Elements used in an SQL Command (SQL Injection) Improper Control of Generation of Code (Code Injection) Improper Neutralization of Directives in Dynamically Evaluated Code (Eval Injection) Exposure of Sensitive Information to an Unauthorized Actor Use of Broken or Risky Cryptographic Algorithm Improper Verification of Cryptographic Signature Cross-Site Request Forgery (CSRF) The product constructs all or part of command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to downstream component. The product constructs all or part of command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to downstream component. The product does not neutralize or incorrectly neutralizes user-controllable input before it is placed in output that is used as web page that is served to other users. The product constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to downstream component. Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted as SQL instead of ordinary user data. The product constructs all or part of code segment using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the syntax or behavior of the intended code segment. The product receives input from an upstream component, but it does not neutralize or incorrectly neutralizes code syntax before using the input in dynamic evaluation call (e.g. eval). The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information. The product uses broken or risky cryptographic algorithm or protocol. The product does not verify, or incorrectly verifies, the cryptographic signature for data. The web application does not, or can not, sufficiently verify whether well-formed, valid, consistent request was intentionally provided by the user who submitted the request."
        },
        {
            "title": "CWE RISKY SCENARIOS",
            "content": ""
        },
        {
            "title": "Deserialization of Untrusted Data",
            "content": "601 770 URL Redirection to Untrusted Site (Open Redirect)"
        },
        {
            "title": "Allocation of Resources Without\nLimits or Throttling",
            "content": ""
        },
        {
            "title": "Missing Authorization",
            "content": ""
        },
        {
            "title": "Incorrect Authorization",
            "content": "915 918 Improperly Controlled Modification of Dynamically-Determined Object Attributes Server-Side Request Forgery (SSRF)"
        },
        {
            "title": "Inefficient Regular Expression\nComplexity",
            "content": "The product deserializes untrusted data without sufficiently verifying that the resulting data will be valid. web application accepts user-controlled input that specifies link to an external site, and uses that link in Redirect. This simplifies phishing attacks. The product allocates reusable resource or group of resources on behalf of an actor without imposing any restrictions on the size or number of resources that can be allocated, in violation of the intended security policy for that actor. The product does not perform an authorization check when an actor attempts to access resource or perform an action. The product performs an authorization check when an actor attempts to access resource or perform an action, but it does not correctly perform the check. This allows attackers to bypass intended access restrictions. The product receives input from an upstream component that specifies multiple attributes, properties, or fields that are to be initialized or updated in an object, but it does not properly control which attributes can be modified. The web server receives URL or similar request from an upstream component and retrieves the contents of this URL, but it does not sufficiently ensure that the request is being sent to the expected destination. The product uses regular expression with an inefficient, possibly exponential worst-case computational complexity that consumes excessive CPU cycles. A."
        },
        {
            "title": "Implementation Details",
            "content": "We implement REAL based on the VeRL framework3 and conduct all experiments on server node equipped with 8 NVIDIA H100 GPUs. The backbone model used in our experiments is Qwen2.5-Coder-Instruct, with pretrained weights obtained from the public HuggingFace platform4. For reinforcement learning, we adopt the Proximal Policy Optimization (PPO) algorithm with hybrid reward design that balances functional correctness and code quality, focusing on both security and maintainability. The policy model is initialized from the Qwen2.5-Coder-Instruct checkpoint and fine-tuned using PPO with learning rate of 1e-6, batch size of 256, and KL divergence penalty coefficient of 1e-3 to ensure stable policy updates. Advantage estimates are computed using Generalized Advantage Estimation (GAE) with discount factor of 1.0 and GAE lambda of 1.0. To promote exploration, entropy regularization is applied, and hybrid rewards are normalized to further stabilize training. We curate the SafeSQL benchmark by constructing diverse set of manually designed seed programs covering common database query patterns and known security pitfalls. These seed programs are further evolved with GPT 4.15 using code mutation and transformation strategies inspired by [Luo et al., 2024] , producing comprehensive benchmark that captures wide range of realistic and challenging SQL generation scenarios. 3https://github.com/volcengine/verl 4https://huggingface.co/ 5https://openai.com/index/gpt-4-1/"
        }
    ],
    "affiliations": [
        "CausalFlow Inc.",
        "Microsoft Research",
        "University of California, San Diego"
    ]
}