{
    "paper_title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning",
    "authors": [
        "Minggui He",
        "Yilun Liu",
        "Shimin Tao",
        "Yuanchang Luo",
        "Hongyong Zeng",
        "Chang Su",
        "Li Zhang",
        "Hongxia Ma",
        "Daimeng Wei",
        "Weibin Meng",
        "Hao Yang",
        "Boxing Chen",
        "Osamu Yoshie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT."
        },
        {
            "title": "Start",
            "content": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning Minggui He1, Yilun Liu1(cid:66) , Shimin Tao1, Yuanchang Luo1, Hongyong Zeng1, Chang Su1, Li Zhang1, Hongxia Ma1, Daimeng Wei1, Weibin Meng1, Hao Yang1, Boxing Chen2, Osamu Yoshie3 1 Huawei, China 2 Huawei Canada, Canada 3 Waseda University, Japan heminggui@huawei.com, liuyilun3@huawei.com"
        },
        {
            "title": "Abstract",
            "content": "Despite recent breakthroughs in reasoningenhanced large language models (LLMs) like DeepSeek-R1, incorporating inferencetime reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-ofthoughts (CoTs), is yet underexplored. Existing methods either design fixed CoT tailored for specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoningbased translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like contextaware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KLconstrained rewards. Experimental results indicate steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs) have brought prominent paradigms of O1-like (Jaech et al., 2024) and R1-like models (DeepSeek-AI, 2025), which leverage inferencetime scaling to amplify chain-of-thought (CoT) reasoning (Wei et al., 2022) for tasks like mathemat- (cid:66) Corresponding author. (liuyilun3@huawei.com) ical proofs and code generation, and employ reinforcement learning (RL) to incentivize autonomous reasoning behaviors without supervised fine-tuning (SFT) (Shao et al., 2024). For instance, DeepSeekR1-Zero (DeepSeek-AI, 2025) demonstrated that pure RL training could unlock self-verification and reflection capabilities, achieving state-of-the-art performance on multiple reasoning benchmarks. These models exemplify how extended CoT reasoning through scaled inference can tackle complex problem-solving. However, the application of extended CoT reasoning to machine translation (MT)a task demanding both linguistic fidelity and contextual reasoningremains underexplored. While LLMs have shown promise in MT, prior work primarily focuses on direct fine-tuning on parallel corpus (Kocmi et al., 2024) or retrievalaugmented methods (Wang et al., 2024b). Recently, Wang et al. (2024a) proposed DRT and introduced long CoT to enhance Chinese-English literary translation, revealing that extended reasoning steps improve handling of culturally nuanced phrases and metaphors. Yet, this approach narrowly targets literature, overlooking the broader spectrum of translation scenarios where professional translators rigorously apply structured reasoningsuch as decomposing idioms, resolving contextual conflicts, and adopting techniques (e.g., back translations) to refine outputs (Ordudari, 2007; Nida, 1964; Brislin and Freimanis, 2001). Nida (1964) also states that technical translator should make thorough study of the source language text before making attempts translate it, suggesting deep thinking process in human translation. Current MT systems lack mechanisms to reflect these human-like, complex CoT processes, limiting their adaptability to diverse translation challenges. Moreover, existing approaches for reasoningbased translation utilize purely synthesized CoTs distilled from LLMs (e.g., DRT (Wang et al., 2024a)) or design single fixed procedure (e.g., 5 2 0 2 7 2 ] . [ 1 5 3 7 9 1 . 2 0 5 2 : r TASTE (Wang et al., 2024c)), which may be suboptimal and unaligned with the hybrid reasoning strategies employed by human experts. Also, their reliance on pre-defined CoTs restricts adaptability to novel translation domains and tasks where high-quality CoT data is scarce. And the pure SFT strategy is easy to cause catastrophic forgetting (Li et al., 2024; Huang et al., 2024; Thompson et al., 2019), leading to damage in general abilities other than MT. To address these, we propose R1-Translator (R1-T1), framework that fully incentivizes reasoning-based translation through three innovations: (1) Reasoning for general MT. Beyond literature translation, we extended reasoning-based MT to general MT and diverse tasks such as documentlevel translation, idiom translation, and domain translation (e.g., legal, medical), while expanding language support to six languages, including lowresource pairs. This aligns with our core assumption that reasoning is beneficial for general MT. (2) Human-aligned and complex translation CoTs. Professional translators often interleave complex reasoning procedure such as lexical disambiguation, context-aware paraphrasing, and iterative self-correction. With the help of language experts, we formalized these into six distinct CoT templates mimicking common thinking patterns of human translators and curated training dataset with hybrid translation CoT trajectories. Unlike single-type CoT in existing approaches, our approach mirrors the multi-layered reasoning observed in human workflows, enhancing translation robustness across domains. (3) Self-evolving translation CoTs and antiforgetting MT adaption. Inspired by R1s RLdriven self-evolution (DeepSeek-AI, 2025), we designed RL process with specially designed rewards for MT, enabling LLMs to autonomously discover novel CoT trajectories for unseen translation tasks. For example, when translating technical manuals without pre-existing CoT data, the model explores reasoning paths via RL rewards for terminological accuracy and readability, bypassing dependency on curated SFT data. Another advantage of this RL-based adaption approach over SFT is its anti-forgetting ability due to KL-control (i.e., control on KL-divergence) (Korbak et al., 2022), resulting in more robust MT adaption without losing general abilities. To realize this vision, we introduce the concept of translation reasoning learning, training series of R1-T1 models through two complementary strategies: (1) fine-tuning on hybrid CoTs curated with professional translators, and (2) RLbased exploration to self-discover optimal reasoning paths. Our contributions are threefold: We demonstrate the benefits of reasoning (i.e., long CoTs) for general MT, achieving superior translation performance in six languages and multiple domains. We design novel methodology for constructing collection of CoT trajectories combining six distinct CoT templates that reflect human translators multi-stage reasoning, validated on challenging tasks like idiom translation and cross-domain adaptation. We pioneer general-purpose, R1-like MT framework that autonomously evolves translation CoTs and enabling anti-forgetting MT adaption via RL. In addition, we open-source R1-T1s datasets and code, facilitating future research into reasoningdriven MT systems1."
        },
        {
            "title": "2.1 Reasoning-based LLMs",
            "content": "The pioneering advancements in reasoning-based LLMs, such as OpenAIs O1 (Jaech et al., 2024) and DeepSeek-R1 (DeepSeek-AI, 2025), have excelled in many tasks and attracted significant research attentions. While earlier explorations focus on using inference-time reasoning for solving complex tasks such as math and coding (Qin et al., 2024; Zhang et al., 2024), there is trending belief towards utilizing reasoning-based LLMs for general AI tasks. Zhao et al. (2024) expanded reasoning-based LLMs to open-ended text generation, generalizing them to broader domains where clear standards and quantified rewards are absent. Shen et al. (2025) propose VLM-R1, stable and generalizable R1-style LLM for vision-language tasks. In addition, reasoning-based LLMs are utilized for financial tasks (Chu et al., 2025) and adapted to multilingual reasoning (Ko et al., 2025). Compared with existing endeavors on reasoningbased LLMs, our work focuses on introducing long CoTs into the field of general MT, thereby developing reasoning-based LLM for MT. 1Codes and data will be available at www.coming-soon. com. 2.2 LLMs for Machine Translation Since the appearance of ChatGPT (Ouyang et al., 2022), endeavors on applying LLMs for MT continuously emerge. Early studies focus on directly prompting LLMs to translate via prompt strategies (Jiao et al., 2023, 2024; Peng et al., 2023), or fine-tune open-source LLMs using parallel corpus (Xu et al., 2024a; Wu et al., 2024; Zhang et al., 2023). Techniques are further proposed to enhance the performance of LLMs for the tasks of MT, such as continuous pre-training (Boughorbel et al., 2024; Fujii et al., 2024), mixture-of-expert modules (Xu et al., 2024b; Zhu et al., 2025) and multi-task training (Wang et al., 2024c; Ul Hassan et al., 2024). Recently, MT with CoT methodology draws attentions of researchers. Feng et al. (2024) introduced an API-based self-correcting framework for LLMs, where LLMs autonomously call external evaluation models and refine translation hypothesis based on the evaluation result. Wang et al. (2024c) designed multi-task training phase and multistage inference phase for MT, guiding the LLM to first conduct translation task, then evaluation task and revision task. DRT (Wang et al., 2024a) merged this procedure into inference-time CoT, utilized multi-agent mechanism to synthesize such long CoTs and examined performance in EnglishChinese literature translation. Compared with existing MT approaches, our R1T1 incorporates six types of human-aligned CoTs for translation, possesses the ability to self-evolve its CoTs via RL, and is designed for general MT tasks."
        },
        {
            "title": "3 Methodology",
            "content": "To incentivize reasoning capabilities of LLM-based translators, in Section 3.1.1, we begin by preparing seed dataset of parallel corpus that is necessary for general translation abilities across various domains, sub-tasks and languages. In Section 3.1.2, we introduce our translation CoT templates that integrates six strategies often used in human expert translation experience. These complex CoTs serve as mean to inject prior reasoning knowledge into the LLM, enabling the model to better align with human thinking processes. In Section 3.1.3, we discuss how the CoT templates are incorporated into the seed dataset of parallel corpus, so that the model can learn from instantiated reasoning process with real translation examples. In Section 3.2, we discuss the implementation of translation reasoning learning, including self-evolving mechanism to further enhance the models understanding of reasoning-based MT, ultimately improving performance on general MT tasks. 3.1 Construction of MT Reasoning Dataset 3.1.1 Collection of Parallel Corpus as Seed Dataset Our seed dataset comprises 2k parallel translation pairs carefully sampled from diverse range of real-world open-source MT datasets2345, ensuring essential coverage in terms of token length distribution, domain knowledge, and translation languages. For the length of each translation pair, we consider pairs of both sentence-level and paragraph-level, covering the majority of common translation scenarios. As illustrated in Fig. 1, the token length distribution of our seed dataset primarily ranges from 10 to 1200 tokens, consistent with several authoritative MT benchmarks(Bañón et al., 2020; Goyal et al., 2022). For domain diversity, we select wide variety of content types, including news6, literature, and specialized texts that require domain-specific terminology. This ensures that our dataset addresses various translation challenges across different fields. For language coverage in the training set, we focus on six major languagesRussian (ru), French (fr), German (de), Japanese (ja), Chinese (zh), and English (en)chosen for their large speaker populations, the linguistic similarities and cross-linguistic relationships between them (Bojar et al., 2018). From the open-source datasets described above, 20 translation directions within the six languages are distinguished. For each direction, 100 parallel pairs are randomly sampled, leading to total of 2k parallel pairs."
        },
        {
            "title": "Templates",
            "content": "Reasoning strategies in Human Translation In the pursuit of mimicking human reasoning process in manual translation, we propose framework that incorporate reasoning process into the plain MT seed dataset. As shown in Fig. ??, our framework 2https://machinetranslate.org/wmt 3https://www.un.org/dgacm/zh/content/uncorpus/ 4https://huggingface.co/datasets/joefox/ newstest-2017-2019-ru_zh/tree/main 5https://www.jizhi-dataset.top/index/category/ detail/15 6https://huggingface.co/datasets/joefox/ newstest-2017-2019-ru_zh/tree/main Almanna, 2015), enabling the translation of idiomatic expressions, cultural nuances, and domainspecific terms with higher fidelity. (5) Translation Explanation: Human translators often explain why they select one translation option over another, especially when multiple interpretations are possible (Bühler, 2002). This explanation can help identify potential issues in the translation process and ensure that the translated text aligns more closely with the intended meaning of the source. (6) Structural Transformation: Structural transformation refers to the step-by-step adjustments made in the sentence structure when translating between languages with different syntactic norms. For example, when translating from English to languages like Japanese or German, human may need to restructure the sentence to ensure it adheres to the grammatical conventions of the target language. It not only accurate but also syntactically appropriate for the target language. Construction of CoT Templates with human strategies By, we observe that, when conducting translation, human translators typically prioritize on extracting key information, such as important keywords and phrases from the source text. This ensures that the primary translation subject is clearly identified. Then, multiple translation strategies can be actively employed considering different target languages, domains, and complexity levels. This hierarchical approach of information extraction followed by strategy application is key to human-like reasoning in translation. To enable LLMs to replicate this process, we construct two reasoning modules when building translation CoT templates: a. Information Extraction Reasoning: In the first reasoning step, the model extracts the main translation subject and key linguistic elements from the source text. This serves to identify the core meaning of the text, which is critical for accurate translation. The extraction process can be seen as analogous to the human translators practice of identifying key terms and understanding the context before translating. b. Translation Strategy Reasoning: In the next step, the model employs set of predefined translation strategies depending on the identified translation subject and context. The choice of strategywhether hierarchical translation, back translation, or another approachdepends on the characteristics of the source text (e.g., syntactic comFigure 1: The token length distribution of our seed dataset comprising 2k parallel translation pairs. first employs six reasoning strategies frequently utilized by human translators as guidance, then abstracts them into two basic reasoning modules. The goal is to convert these human reasoning strategies into systematic and generalized CoT templates for translation. The six strategies are outlined below: (1) Hierarchical Translation: It reflects the human expert practice of breaking down complex sentences or passages into smaller, more manageable components. By first identifying the key elements of the source text, the model can then translate each segment with clearer understanding of the overall structure and meaning. This process helps prevent errors that may arise from translating without considering the broader context. (2) Triangulation Translation: This method (also called relay translation (Ringmar, 2012)) uses an intermediate (pivot) language to translate between linguistically or culturally distant languages (Guyot, 2018). When direct translation is challenging, the source text is first translated into the pivot language and then into the target language. This helps bridge gaps in grammar, vocabulary, or cultural context, ensuring more accurate and nuanced final translation. It emphasizes the role of intermediary steps in achieving high-quality translations, especially for complex or culturally rich texts. (3) Back Translation: process where translated sentence is re-translated into the original language to identify inconsistencies or errors (Brislin and Freimanis, 2001). This helps refine the translation by detecting possible flaws and improving fluency. (4) Context-aware Translation: This strategy is useful since it focuses on understanding the broader context (e.g., paragraph or document) in which phrase or sentence appears (Cabezas-García, 2023; plexity, domain specificity). This module aims to guide the LLM through series of decision points, allowing it to adaptively apply the most appropriate translation strategy at each stage. By combining the two modules with the six strategies, we manually created 18 translation CoT templates, displayed in Appendix ??. All templates are carefully calibrated by language experts in case of inaccuracy or redundancy. 3.1.3 Incorporating CoTs into Parallel Corpus With the calibrated template, we use GPT4o (Achiam et al., 2023) to generate instantiated translation CoTs for each sample within our seed dataset, leading to total of 2k parallel pairs with translation CoTs. Each sample contain source sentence Wsrc and target sentence Wtgt, along with reasoning trajectory revealing how the target translation reference is obtained step-by-step from the source sentence. Specifically, during the construction of reasoning trajectory, we employed multi-step refinement process from multi-agent review to mitigate hallucinations in the data (the used prompt is provided in Appendix ??). This iterative process can be represented as: (s) : Wsrc, Wtgt t0, f0, s0 tm, fm, sm. (1) In Eq. (1), ti represents the translation at step i, and fi is the confidence score. Redundant translations, where the output does not change between steps, are discarded, resulting in refined reasoning trajectory."
        },
        {
            "title": "3.2 Translation Reasoning Learning",
            "content": "Our reasoning learning method involves three primary components, each addressing critical aspect of enhancing translation capabilities in language models:"
        },
        {
            "title": "3.2.1 SFT with Reasoning Dataset\nThis section outlines how to utilize our CoT dataset\nto fine-tune the model directly on reasoning-based\ntranslation tasks. This fine-tuning process estab-\nlishes a foundational layer of reasoning capability\nthat is critical for the subsequent phases of learn-\ning. By training the model to handle structured\nreasoning tasks, we enable it to internalize essen-\ntial translation strategies that will be refined in later\nstages.",
            "content": "3.2.2 Self-Evolving with Reinforcement Learning This part introduces self-evolving mechanism that allows the model to progressively refine its reasoning skills over time through reinforcement learning. This iterative process facilitates the continuous enhancement of the models translation performance, ensuring that it not only learns from its immediate outputs but also adapts based on long-term feedback. Reward Modeling To effectively guide the models reasoning and translation quality, we have devised reward modeling system that incorporates two distinct types of rewards: Format Reward and Answer Reward. These rewards are designed to align the models output with the desired reasoning format and translation quality, allowing for targeted refinements based on specific performance criteria. (1) Format Reward: This reward encourages the model to adhere to structured response format. We employ regular expression extraction to ensure the model places its reasoning process within the <think></think> tags and provides its final translation inside the <answer></answer> tags. The format reward score (Sf ormat) is computed as follows: Sformat = (cid:40) 1 0 if format is correct if format is incorrect (2) This design ensures that the models output is both readable and logically structured, crucial for subsequent reasoning steps. (2) Answer Reward: The answer reward evaluates the translation quality in the models output. To objectively assess the translation results, we adopt COMET (Rei et al., 2020) as the evaluation metric, which compares the models translation against reference translation. However, we encountered several challenges with directly using COMET scores: When the COMET score is lower than the format reward, the model (or actor) may output an incorrect format to avoid receiving the lowest possible score, leading to suboptimal behavior. The continuous nature of COMETs reward signal may cause instability in gradient updates, resulting in slower convergence and potential performance degradation during training. To address these issues, the answer score is computed as follows: R(x) = (cid:40) 0 round(x, 3) if 0 if > 0 (3) This discretization helps mitigate instability and encourages more controlled behavior from the model. Reinforcement Learning Strategy For the reinforcement learning phase, we adopt modified version of the REINFORCE++ algorithm (Hu, 2025), which has been shown to be effective in optimizing models with complex, continuous reward structures. The core idea of REINFORCE++ is to refine the models reasoning over multiple iterations, gradually improving its ability to generate highquality translations. The following equation describes our adaptation of the REINFORCE++ algorithm: helps minimize potential negative impacts from insufficient base model capabilities. The dataset was randomly split into training set and validation set at ratio of 9:1. In RL implementation, we utilized the modified REINFORCE++ framework (Hu, 2025), which has proven effective with low training costs. The training configuration consists of 3 epochs, learning rate of 3 107, batch size of 8, and 16 rollouts. For SFT with CoTs, we used 2 epochs of full parameter fine-tuning with learning rate of 1 104. 4.1.2 Evaluation Dataset We employed the Flores-101 test set (Goyal et al., 2021), which is well-suited for evaluating multilingual MT models due to its extensive coverage of diverse language pairs and domains. The selection of this dataset allows us to measure the robustness and generalization of our approach across different linguistic contexts. In Table 1, we summarize the token coverage between the evaluation datasets and our training sets across six languages. This helps to confirm the integrity of our experimental setup and the validity of the evaluation results. θt = α (cid:16) R(xt) ˆR (cid:17) log (xtθ), (4) Dataset ru fr de ja zh en Flores101 0.142 0.137 0. 0.113 0.11 0.0972 where α represents the learning rate, R(xt) is the reward signal, and ˆR is the baseline reward estimate."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we provide detailed overview of the experimental setup used to evaluate the effectiveness of our proposed approach. We begin by presenting the implementation details, followed by the evaluation dataset, and then move on to the core evaluation on multilingual machine translation (MT) performance. Finally, we discuss our evaluation of the models ability to preserve general capabilities during training, and analyze the self-evolution of Chain of Thoughts (CoT) in the reinforcement learning (RL) process."
        },
        {
            "title": "4.1 Experiment Setting",
            "content": "4.1."
        },
        {
            "title": "Implement Details",
            "content": "we selected Qwen2.5-7B-Instruct as backbone model due to its superior multilingual performance among open-source models with fewer than 10 billion parameters (Yang et al., 2024). This choice Table 1: similarity evaluation between test set and our training set."
        },
        {
            "title": "4.2 Evaluation on Multilingual MT",
            "content": "Our experiment aims to assess the multilingual MT performance of our model. Specifically, we investigate how well the model handles multiple languages from training, including both seen and unseen languages, and compare it against relevant baselines."
        },
        {
            "title": "4.2.1 Setup",
            "content": "Metric we employ COMETScore (Rei et al., 2022)7 to measure quality of translation hypotheses given references. COMETScore has been widely adopted due to its strong correlation with human judgment, providing reliable metric for evaluating machine translation performance. It reflects the translation accuracy of the model, with higher score indicating better translation quality. 7https://huggingface.co/Unbabel/ wmt20-comet-da Models Q2.5a-7b-base Q2.5-7b-Insb DS-R1-Dc DS-R1-DMd xx2en en2xx zh2xx xx2zh avg zh ja ru fr de zh ja ru fr de en ja ru fr de en ja ru fr de - General-purpose LLMs 0.700 0.659 0.649 0.800 0.747 0.624 0.435 0.538 0.759 0.559 0.624 0.583 0.525 0.565 0.527 0.700 0.531 0.489 0.555 0.475 0.602 0.673 0.641 0.556 0.736 0.604 0.622 0.452 0.343 0.495 0.453 0.622 0.550 0.465 0.531 0.473 0.673 0.450 0.088 0.152 0.280 0.493 0.572 0.377 0.441 0.684 0.577 0.452 -1.020 -0.851 0.033 -0.454 0.452 0.009 0.199 0.290 0.206 0.572 -0.621 -0.903 -0.222 -0.580 0.011 0.572 0.321 0.437 0.689 0.577 0.504 -0.662 -0.508 0.077 -0.299 0.504 -0.014 0.266 0.292 0.193 0.572 -0.474 -0.744 -0.279 -0.538 0.074 Q2.5-7b-SFT (w/o CoT) 0.702 0.654 0.650 0.797 0.749 0.645 0.588 0.621 0.742 0.559 0.645 0.585 0.525 0.590 0.565 0.702 0.573 0.562 0.539 0.461 0.623 0.667 0.615 0.613 0.774 0.715 0.564 0.419 0.418 0.636 0.444 0.564 0.533 0.463 0.514 0.482 0.667 0.420 0.309 0.418 0.275 0.525 Q2.5-7b-SFT (CoT) SFT LLMs R1-T 0.704 0.643 0.646 0.793 0.743 0.664 0.611 0.592 0.745 0.570 0.664 0.602 0.542 0.599 0.575 0.704 0.595 0.513 0.552 0.464 0.626 RL Table 2: R1-T1s performance on Trained languages tested in Flores-101. xx2en (or xx2zh) means the translation direction is from language xx to English (or Chinese). Q2.5 means Qwen2.5. Ins means Instruction. DS-R1-D means DeepSeek-R1-Distill-Qwen-7B. DS-R1-DM means DeepSeek-R1-Distill-Qwen-7B-Multilingual. Models Q2.5a-7b-base Q2.5-7b-Insb DS-R1-Dc DS-R1-DMd xx2en en2xx zh2xx xx2zh th nl vi tr cs th nl vi tr cs th nl vi tr cs th nl vi tr cs avg - General-purpose LLMs 0.668 0.482 0.715 0.441 0.701 0.438 0.672 0.619 0.381 0.513 0.515 0.501 0.578 0.368 0.521 0.355 0.409 0.589 0.256 0.410 0.507 0.566 0.580 0.536 0.391 0.601 0.148 0.026 0.226 0.185 0.333 0.434 0.418 0.456 0.301 0.470 0.012 -0.046 -0.065 -0.237 0.075 0.270 0.143 0.408 0.217 0.156 0.323 -1.222 -0.777 -0.925 -0.812 -0.900 -0.194 -0.016 -0.160 -0.295 -0.049 -0.986 -0.786 -0.805 -0.915 -0.979 -0.429 0.161 0.415 0.181 0.023 0.331 -0.930 -0.593 -0.538 -0.715 -0.767 -0.478 -0.072 -0.467 -0.445 0.063 -0.868 -0.739 -0.691 -0.901 -0.840 -0.394 Q2.5-7b-SFT (w/o CoT) 0.660 0.680 0.711 0.657 0.710 0.388 0.478 0.607 0.432 0.458 0.539 0.529 0.558 0.494 0.553 0.282 0.367 0.549 0.171 0.337 0.508 0.625 0.642 0.664 0.610 0.668 0.178 0.271 0.327 0.043 0.146 0.475 0.434 0.516 0.404 0.462 0.104 0.155 0.244 -0.093 0.037 0.345 Q2.5-7b-SFT (CoT) SFT LLMs R1-T 0.659 0.679 0.713 0.680 0.707 0.367 0.460 0.573 0.513 0.461 0.558 0.528 0.594 0.503 0.558 0.289 0.380 0.526 0.280 0.390 0.521 RL Table 3: R1-T1s performance on Unseen languages in Flores-101. xx2en (or xx2zh) means the translation direction is from language xx to English (or Chinese). Q2.5 means Qwen2.5. Ins means Instruction. DS-R1-D means DeepSeek-R1-Distill-Qwen-7B. DS-R1-DM means DeepSeek-R1-Distill-Qwen-7B-Multilingual. Baselines We compare our model against set of baseline models, including two models from the Qwen series (Yang et al., 2024): Qwen2.5-7b-base, Qwen2.5-7B-Instruct, and two distilled models of DeepSeek-R1 (DeepSeek-AI, 2025): DeepSeek-R1Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-7BMultilingual 8. These baselines are chosen because they represent diverse range of approaches to multilingual MT, including general-purpose LLMs, instruction-tuned LLMs, and multilingual distilled models. The inclusion of these baselines allows for comprehensive comparison across different configurations and architectures, ensuring that our models performance is evaluated in the context of current state-of-the-art techniques."
        },
        {
            "title": "4.2.2 Result of Trained Languages\nAs shown in Table 2, the performance of our model\non trained languages demonstrates strong results\nacross all evaluation metrics. The model outper-\nforms general-purpose LLMs and multilingual dis-\ntilled models in terms of translation quality, partic-\nularly when compared to the baseline models like\nDeepSeek-R1-Distill-Qwen-7B.",
            "content": "8https://huggingface.co/lightblue/ DeepSeek-R1-Distill-Qwen-7B-Multilingual"
        },
        {
            "title": "4.3 CoT Self-evolution Analysis",
            "content": "To provide more intuitive evaluation of the CoT self-evolution mechanism, we conduct visualization for the translation of complex sentence from Chinese to English, with and without the integration of self-evolving CoTs in Figure ??. In both instances, the translated sentence correctly conveys the meaning, but the version utilizing self-evolution provides more nuanced translations. The description without CoTs includes generic phrasing, such as \"the meeting was held,\" whereas the self-evolved CoT version includes additional context, such as \"the critical meeting was held to address urgent concerns.\" Through this analysis, it becomes evident that the self-evolving CoT improves the models ability to adapt over time, refining its translation strategies. The self-evolving CoT allows the model to generate more diverse and accurate translations by incorporating expert-level reasoning strategies and incorporating feedback from prior outputs. This demonstrates that CoT self-evolution is effective in enhancing the models reliability and its capacity to provide richer, context-aware translations across languages."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Ali Almanna. 2015. Contextualizing translation theories: Aspects of ArabicEnglish interlingual communication. Cambridge Scholars Publishing. Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, et al. 2020. Paracrawl: Web-scale acquisition of parallel corpora. Association for Computational Linguistics (ACL). Ondˇrej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. 2018. Findings of the 2018 conference on machine translation (WMT18). In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 272303, Belgium, Brussels. Association for Computational Linguistics. Sabri Boughorbel, Md Rizwan Parvez, and Majd Hawasly. 2024. Improving language models trained on translated data with continual pre-training and dictionary learning analysis. In Proceedings of The Second Arabic Natural Language Processing Conference, pages 7388. Richard Brislin and Carolina Freimanis. 2001. Back-translation. An Encyclopaedia of Translation: Chinese-English, English-Chinese, 22. Axel Bühler. 2002. Translation as interpretation. Translation studies: Perspectives on an emerging discipline, pages 5674. Melania Cabezas-García. 2023. The use of context in multiword-term translation. Perspectives, 31(2):365 382. Xu Chu, Zhijie Tan, Hanlin Xue, Guanyu Wang, Tong Mo, and Weiping Li. 2025. Domaino1s: Guiding llm reasoning for explainable answers in high-stakes domains. arXiv preprint arXiv:2501.14431. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. In arXiv preprint arXiv:2501.12948. Zhaopeng Feng, Yan Zhang, Hao Li, Wenqiang Liu, Jun Lang, Yang Feng, Jian Wu, and Zuozhu Liu. 2024. Improving llm-based machine translation with systematic self-correction. arXiv e-prints, pages arXiv 2402. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjan Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2021. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538. Alexandra Guyot. 2018. Translating mircea eliades\" ivan\" from romanian to english: triangular approach using the french translation. Jian Hu. 2025. Reinforce++: simple and efficient approach for aligning large language models. ArXiv, abs/2501.03262. Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao, and Jinsong Su. 2024. Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14161428. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. arXiv preprint 2024. Openai o1 system card. arXiv:2412.16720. Hui Jiao, Bei Peng, Lu Zong, Xiaojun Zhang, and Xinwei Li. 2024. Gradable chatgpt translation evaluation. Procesamiento del Lenguaje Natural, 72:7385. Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt good arXiv preprint translator? arXiv:2301.08745. preliminary study. Hyunwoo Ko, Guijin Son, and Dasol Choi. 2025. Understand, solve and translate: Bridging the multilingual mathematical reasoning gap. arXiv preprint arXiv:2501.02448. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, et al. 2024. Findings of the wmt24 general machine translation shared task: The llm era is here but mt is not solved yet. In Proceedings of the Ninth Conference on Machine Translation, pages 146. Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio Yokota, and Naoaki Okazaki. 2024. Continual pre-training for cross-lingual llm adaptation: Enhancing japanese language capabilities. In First Conference on Language Modeling. Tomasz Korbak, Hady Elsahar, Germán Kruszewski, and Marc Dymetman. 2022. On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting. Advances in Neural Information Processing Systems, 35:1620316220. Hongyu Li, Liang Ding, Meng Fang, and Dacheng Tao. 2024. Revisiting catastrophic forgetting in large language model tuning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 42974308. neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 20622068. Eugene Albert Nida. 1964. Toward science of translating: with special reference to principles and procedures involved in Bible translating. Brill Archive. Mahmoud Ordudari. 2007. Translation procedures, strategies and methods. Translation journal, 11(3):8. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao. 2023. Towards making the most of chatgpt for machine translation. arXiv preprint arXiv:2303.13780. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702, Online. Association for Computational Linguistics. Martin Ringmar. 2012. Relay translation. Handbook of translation studies, 3:141144. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Haozhan Shen, Zilun Zhang, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. 2025. Vlm-r1: stable and generalizable r1-style large vision-language model. https://github.com/om-ai-lab/VLM-R1. Accessed: 2025-02-15. Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. 2019. Overcoming catastrophic forgetting during domain adaptation of Muhammad Naeem Ul Hassan, Zhengtao Yu, Jian Wang, Ying Li, Shengxiang Gao, Shuwan Yang, and Cunli Mao. 2024. Lkmt: Linguistics knowledgedriven multi-task neural machine translation for urdu and english. Computers, Materials & Continua, 81(1). Jiaan Wang, Fandong Meng, Yunlong Liang, and Jie Zhou. 2024a. Drt-o1: Optimized deep reasoning translation via long chain-of-thought. arXiv preprint arXiv:2412.17498. Jiaan Wang, Fandong Meng, Yingxue Zhang, and Jie Zhou. 2024b. Retrieval-augmented machine translation with unstructured knowledge. arXiv preprint arXiv:2412.04342. Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, and Min Zhang. 2024c. TasTe: Teaching large language models to translate through selfreflection. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 61446158, Bangkok, Thailand. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems (NeurIPS). Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Foster, and Gholamreza Haffari. 2024. Adapting large language models for document-level machine translation. arXiv preprint arXiv:2401.06468. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024a. paradigm shift in machine translation: Boosting translation performance of large language models. In The Twelfth International Conference on Learning Representations. Haoran Xu, Kenton Murray, Philipp Koehn, Hieu Hoang, Akiko Eriguchi, and Huda Khayrallah. 2024b. X-alma: Plug & play modules and adaptive rejection for quality translation at scale. arXiv preprint arXiv:2410.03115. Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, and Zekun Wang. 2024. Qwen2.5 technical report. ArXiv, abs/2412.15115. Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp Koehn. 2023. Machine translation with large language models: Prompting, few-shot learning, and fine-tuning with qlora. In Proceedings of the Eighth Conference on Machine Translation, pages 468481. Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. 2024. arXiv o1-coder: an o1 replication for coding. preprint arXiv:2412.00154. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405. Shaolin Zhu, Leiyu Pan, Dong Jian, and Deyi Xiong. 2025. Overcoming language barriers via machine translation with sparse mixture-of-experts fusion of large language models. Information Processing & Management, 62(3):104078."
        }
    ],
    "affiliations": [
        "Huawei Canada, Canada",
        "Huawei, China",
        "Waseda University, Japan"
    ]
}