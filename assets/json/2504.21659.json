{
    "paper_title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization",
    "authors": [
        "Haotian Luo",
        "Haiying He",
        "Yibo Wang",
        "Jinluan Yang",
        "Rui Liu",
        "Naiqiang Tan",
        "Xiaochun Cao",
        "Dacheng Tao",
        "Li Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 9 5 6 1 2 . 4 0 5 2 : r AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization Haotian Luo1, Haiying He2, Yibo Wang3, Jinluan Yang4, Rui Liu5 Naiqiang Tan5, Xiaochun Cao1, Dacheng Tao6, Li Shen1 1 Sun Yat-sen University; 2 China Agricultural University; 3 Tsinghua University; 4 Zhejiang University; 5 Didichuxing Co. Ltd; 6 Nanyang Technological University"
        },
        {
            "title": "Abstract",
            "content": "Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvementor even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose novel two-stage framework for adaptive and efficient reasoning. First, we construct hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR"
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs), such as OpenAIs O1[1] and Deepseeks R1[2], feature the adoption of detailed and complex reasoning processes (Long-CoT) analogous to human deliberation on complex problems. This reasoning paradigm demonstrably enhances the models problem-solving capabilities and yields promising results. However, the enhanced reasoning capability derived from Long-CoT often comes at substantial inference overhead. Generating lengthy sequences of intermediate reasoning steps incurs significant computational costs, increases latency, and consumes considerable resources[35]. This efficiency bottleneck poses critical challenge for deploying these powerful reasoning models in real-world, resource-constrained applications or interactive scenarios where responsiveness is paramount. Existing approaches to improving reasoning efficiency often focus on optimizing within the LongCoT distribution. Techniques such as O1-Pruner [6], Training and Overthinking compress reasoning paths with various methods but they are inherently limited because they still choose long-thought distribution as the starting point. They primarily address redundancy within potentially unnecessarily long path, rather than focusing whether such long path was needed in the first place. This leaves *Equal contribution Corresponding Author: Li Shen (shenli6@mail.sysu.edu.cn) Preprint. Under review. unexplored the potential efficiency gains from fundamentally choosing different, more concise reasoning strategy altogether when appropriate. CoT-Valve[7] allows models to produce both extended and concise responses simultaneously; however, it lacks mechanisms for adaptively selecting the optimal reasoning path contingent on the characteristics of the input problem, which brings degraded performance. Our investigation (presented in Section 3) about the benefit of Long-CoT reasoning reveals crucial insight: the utility of long, elaborate reasoning chains is highly problem-dependent. While complex problems genuinely benefit from detailed, step-by-step derivations, many other problems can be solved accurately and more efficiently with shorter, more direct reasoning paths. In fact, for simpler problems, forcing Long-CoT process might not only be wasteful but can sometimes even introduce errors or degrade performance. This observation strongly motivates the need for adaptive reasoning strategies systems that can tailor the depth and style of their reasoning process to the specific demands of the input problem. Inspired by this, we propose novel two-stage framework designed to enable efficient and adaptive reasoning. Our core idea is to move beyond optimizing single reasoning style and instead equip the model with the flexibility to choose between different reasoning approaches. First, we construct hybrid reasoning model by integrating the capabilities of both Long-CoT and Short-CoT models. This hybrid model serves as reasoning model capable of generating both reasoning styles. Second, we introduce specialized training methodology called Bi-Level Adaptive Reasoning Optimization. This training process operates on two levels: (i) Group-Level Preference: It guides the hybrid model to implicitly discern the overall complexity of given problem and select the most appropriate reasoning style (e.g., Long-CoT for complex tasks, Short-CoT for simpler ones). (ii) Instance-Level Preference: Within the chosen style group (long or short), it further refines the models output by encouraging it to prefer reasoning paths that are not only correct but also concise. By training the model to make intelligent choices at both the style-selection level and the instancegeneration level, our framework aims to dynamically allocate computational resources where they are most needed. As demonstrated through our experiments, this approach leads to significant reductions in inference costs compared to standard Long-CoT models and other efficiency-focused baselines, while maintaining strong reasoning performance. Specifically, in MATH[8], the reasoning length decreases by 58% while maintaining accuracy, while in GSM8K[9], the length decreases by 74% alongside an improvement in accuracy. This work underscores the promise of adaptive strategies as key direction for optimizing the trade-off between reasoning quality and computational efficiency of large reasoning models. Our contributions can be summarized as follows: We conduct an empirical analysis investigating the benefits of long Chain-of-Thought (CoT) reasoning relative to shorter CoT approaches, identifying the conditions under which extended reasoning paths offer tangible advantages. We propose using Adaptive Hybrid Reasoning Model to enhance inference efficiency, accompanied by novel training pipeline (AdaR1). Comprehensive experiments demonstrate that our proposed method achieves excellent performance, significantly improving efficiency while maintaining high accuracy. We perform further analyses on the resulting Adaptive Hybrid Reasoning Model to gain deeper insights into its characteristics and operational behavior. And we will release the model weights of the Adaptive Hybrid Reasoning Model to the public to encourage further research and application by the community."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Model Merging Model merging[10] is an emerging technique that combines the parameters of multiple trained models into single one without requiring access to the original training data. Unlike ensemble learning[11], which aggregates outputs at inference time, model merging operates at the parameter level, resulting in more efficient and compact model. Recent researchers have proposed various merging methods, including parameter interpolation[12] and alignment-based strategies[13], and 2 explored their applications in large language models, multimodal language models and many subfields in machine learning. Beyond simple linear merging, which averages the model parameters, researchers have proposed more advanced methods such as DARE[14], TIES-Merging[15], and AdaMerging[16]. DARE reduces delta parameter redundancy by dropping and rescaling parameters before merging. TIES-Merging focus on reducing interference from redundant parameters and sign disagreements by trimming, electing parameter signs, and performing disjoint merges. AdaMerging adaptively adjusts task or layer weights via entropy minimization on unlabeled test samples to enhance merged model performance without original training data. Different from traditional model merging, which integrates parameters from multiple source models to consolidate learned capabilities, our work try to empower single model to adaptively select an appropriate reasoning mode (Long-CoT or Short-CoT) for each problem instance, primarily targeting the optimization of computational efficiency in reasoning rather than multi-task performance. 2.2 Alignment LLM alignment[17] research aims to ensure that large language models generate responses that meet human expectations, enabling us to utilize these powerful language models in reliable and responsible way. Reinforcement Learning from Human Feedback (RLHF)[18] is one of the mainstream methods, which optimizes the model based on human feedback, but its high computational cost has led researchers to explore more efficient alignment techniques, such as Direct Preference Optimization (DPO)[19], which simplify the alignment process by directly optimizing the LLM policy based on preference data. other approaches such as Reinforcement Learning from AI Feedback (RLAIF)[20] utilize AI-generated feedback to reduce costs and enhance alignment efficiency. These diverse techniques reflect the dynamic evolution of the field, with the aim of improving both the effectiveness and efficiency of LLM alignment. Although we use preference learning, our method diverges from the objectives of LLM alignment. While alignment techniques focus on shaping model outputs and behavior to conform with human preferences, values, and safety criteria, our research optimizes the internal reasoning generation processspecifically, the Chain-of-Thought. The aim is to enhance problem-solving efficiency and accuracy rather than aligning model behavior with human judgment."
        },
        {
            "title": "3 Motivation",
            "content": "3.1 Problem Setup Chain-of-Thought (CoT) prompting has emerged as powerful technique for enhancing the reasoning capabilities of large language models. Within the CoT paradigm, distinction can be made between Long-CoT, which involves generating detailed and extensive thinking steps, and Short-CoT, which directly generate solving steps. 3.2 When Do We Need Long-CoT? Simply applying Long-CoT uniformly across all problems leads to unnecessary overhead on instances where its detailed steps are not required or do not yield accuracy gains. Therefore, we empirically investigate the necessity of Long-CoT and analyze the relationship between problem characteristics and Long-CoT efficacy in this subsection. We design an experiment to analyze the accuracy gains of Long-CoT over Short-CoT across different types of problems. Specifically, we use DeepSeek-R1Distill-Qwen-7B as the Long-CoT model. For the Short-CoT model, we fine-tune the DeepSeek-R1Distill-Qwen-7B using 2,000 Short-CoT samples generated by Qwen2.5-Math-7B-Instruct[21]. We do not directly use Qwen2.5-Math-7B-Instruct as the Short-CoT model, since its training template differs substantially from that of the DeepSeek-Distill series, which could introduce unpredictable effect in the subsequent model merging and sampling process. We construct mixed dataset (called MixMathematics) which comprises training examples from AIME[22], MATH, and GSM8K. Further datasets details can be found in the Section 5.1. We selected 2,500 samples and generated 12 responses for each question. To refine our analysis and focus on solvable instances, we first removed samples where both the long and short CoT accuracies were zero, as such cases typically indicate problems exceeding the models current Figure 1: The proportion of gain in the data (left) and the relationship between CoT length and accuracy improvement (right), Long-CoT reasoning improves accuracy on difficult problems but has little effect or harms performance on easy ones. Method Overthinking[4] kimi-1.5[23] O1-Pruner Naive Merge CoT-Valve AdaR1(Ours) Optimization Scope Limited Limited Limited Broad Broad Broad Performance (Accuracy) Slightly Dropped Slightly Dropped Not Dropped (mostly) Dropped Dropped Slightly Dropped Table 1: Comparison of Different Methods. The term \"limited\" refers to an optimization scope where the model operates within the distribution of long-CoT, inherently limiting its capacity for efficiency. In contrast, \"broader\" denotes an optimization approach that encompasses both long-CoT and short-CoT, allowing the model to achieve efficiency by generating shorter responses without thinking. \"slightly dropped\" indicates reduction in accuracy of less than 3%. Conversely, the designation \"dropped\" signifies more substantial performance degradation, corresponding to decrease in accuracy exceeding 3%. capabilities. Subsequently, we calculated the accuracy for both the long and short CoT samples and then determined the accuracy gain (i.e., the accuracy of the Long-CoT minus the accuracy of the Short-CoT). Our analysis proceeded in two phases. First, as depicted in the left portion of Figure 1, we quantified the proportion of samples falling into three categories based on the accuracy gain: gain > 0, gain = 0, and gain < 0. Notably, we observed that for nearly half of the samples, employing Long-CoT did not yield any improvement, and in small fraction of cases, it even led to decrease in performance. To further investigate this phenomenon, we categorized the samples based on the average length of their Long-CoT reasoning, as length is often correlated with problem difficulty. The right portion of Figure 1 reveals that the benefit derived from Long-CoT becomes more pronounced for problems with greater length (and thus, presumably higher difficulty). In the shortest length interval, Long-CoT offered negligible gains, whereas in the longest length interval, it resulted in substantial improvement in accuracy. Prior methods  (Table 1)  , such as Overthinking [4], kimi-1.5 [23], and O1-Pruner, typically operate within limited optimization scope but generally maintain performance stability or incur only slight drop, with O1-Pruner notably achieving no performance decrease. In contrast, methods designed for broad optimization scope, including Model Merge and CoT-Valve, did not consider how to tackle easy and different problems, rendering the model incapable of determining its reasoning depth according to the inherent difficulty of the task. Thus they frequently result in significant performance degradation. In nutshell, methods with restricted optimization can generally preserve performance but lose the chance to utilize shorter CoT. However, approaches capable of utilize broader CoT distribution have struggled to maintain accuracy due to their inability to adapt adequate reasoning depth to problem complexity. The finding mentioned in last section motivates us to address the efficiency challenge of Long-CoT models from novel perspective: enabling the reasoning model to adaptively select an appropriate 4 reasoning mode (long or short CoT) for different problems, and then generate correct and concise CoT in the determined mode. Our proposed method (AdaR1) differentiates itself by successfully achieving broad optimization scope while incurring only marginal performance decrement. This demonstrates more favorable trade-off between efficiency and accuracy compared to existing broad-scope optimization techniques."
        },
        {
            "title": "4 Bi-Level Adaptive Reasoning Optimization",
            "content": "4.1 Problem Setup We consider LLM parameterized by θ and denoted as πθ. In the context of math problem solving, the LLM accepts sequence = [x1, . . . , xn], commonly termed as the problem, and then generate corresponding solution = [y1, . . . , ym]. Hence, the solution is construed as sample drawn from the conditional probability distribution πθ(x). The conditional probability distribution πθ(yx) can be decomposed as follows: πθ(yx) = (cid:89) j=1 πθ(yjx, y<j). (1) We consider two LLMs: one trained to generate long, reflective Chain-of-Thought (CoT) reasoning (Long-CoT model, denoted as θL) and the other trained for short and concise reasoning paths (ShortCoT model, denoted as θS). These two models are typically fine-tuned with different CoT and demonstrate distinct reasoning patterns. 4.2 Method Overview Our method consists of two stages, shown in Figure 2. First, we merge Long-CoT model and Short-CoT model to obtain unified reasoning model capable of generating both types of reasoning paths. This allows exploration over broader CoT distribution. In the second stage, we apply Bi-Level Preference Training: for group-level preference, the model learns to choose between long and short reasoning group based on the input; for instance-level preference, it learns to compress the reasoning path to improve efficiency within the chosen group determined by group-level preference. Figure 2: Pipeline of AdaR1. At Stage I, we fused the models to obtain πθH . In Stage II, we sample from both long and short models and then elicit the group-level and instance-level preference. After this, we optimize πθH at both group and instance level to obtain hybrid adaptive reasoning model. 4.3 Stage I: Long-and-Short Reasoning Merge To enable flexible reasoning behaviors within single model, we first perform model merging with long and short models. We adopt simple yet effective strategy of linearly merging their parameters. Given two models with parameters θL and θS, we compute the merged model as: θH = αθL + (1 α)θS, (2) 5 where α [0, 1] is merging coefficient that balances the contribution from each model. The resulting hybrid reasoning model, πθH , inherits the capacity to generate both long and short CoT depending on the input. This merged model expands the diversity of the CoT distribution it can produce, laying the foundation for adaptive reasoning. By combining the strengths of both reasoning styles, it enables the model to potentially match different problem types with suitable reasoning strategies, which is key to improving efficiency in the next stage. 4.4 Stage II: Bi-Level Preference Training In this stage, we introduce Bi-Level Preference Training strategy to fine-tune the model toward efficient reasoning. The core idea is to train the model to: (1) select the appropriate reasoning style (long or short) for each problem (group-level preference) and (2) further compress the reasoning within the determined chosen group (instance-level preference). Group Labels. We define group label to denote the reasoning style of response group. Let gL denote the long reasoning group and gS denote the short reasoning group. For given input problem x, generated resposne (solution) belongs to one of the two groups. We use {yi}g=gL to denote the set of Long-CoT responses generated by the Long-CoT model θL, and {yj}g=gS for the corresponding short responses from the Short-CoT model θS. Group-Level Preference. For each math problem in the dataset D, we sample solutions from }K both the long and short reasoning models. Let {yL j=1 be the respective sample sets. We define the approximated accuracy expectation for each group as: i=1 and {yS }K ˆE[C L(x)] = 1 (cid:88) i=1 1[Correct(yL )], ˆE[C S(x)] = 1 (cid:88) j=1 1[Correct(yS )], (3) where 1[] is the indicator function. Then we introduce preference margin threshold ϵ > 0. The group-level preference for is then determined as: (cid:40) gL gS if gS gL if ˆE[C L(x)] ˆE[C S(x)] > ϵ, ˆE[C L(x)] ˆE[C S(x)] ϵ. Given the group-level preference for an input x, we form training pairs from the Cartesian product of the two groups. For example, if gL gS x, we construct the preference pairs as: Pgroup(x) = (cid:8)(yL , yS ) (cid:12) (cid:12) [1, K], [1, K](cid:9) . (4) From this set of pairs, we randomly sample subset contain M1 pairs to construct DPO training tuples (x, yw, yl), where yw is the preferred (chosen) response and yl is the less preferred (rejected). For all D, we perform group-level preference assignment by comparing the sampled long and short responses as described above. These tuples are then aggregated into new dataset Dgroup = {(x, yw, yl)}, which serves as supervision for optimizing the DPO objective at the group level. Instance-Level Preference. Once the preferred group {gL, gS} is determined for given x, we further construct instance-level preferences within that group to encourage more concise reasoning. We compare response pairs (ya, yb) such that both belong to the same group (e.g., ya, yb {yL }), and prefer the shortest correct response. For dispreferred samples, we select M2 longest responses. Formally, for each with preferred group g, we first identify the subset of correct responses {yi}correct {yi}g=g . Among these, we select the shortest correct response as the preferred instance: yw = arg min y{yi}correct y. To construct instance-level preference pairs, we then select the M2 longest responses from the entire group {yi}g=g . Denote these as {ylj }M2 j=1. This yields dataset of instance-level training tuples: Dinstance = (cid:110) (x, yw, yl) (cid:12) (cid:12) (cid:12) yw = arg miny{yi}correct g=g y, yl arg max(M2) (cid:111) y{yi}g=g 6 Bench Model 7B Models Long(R1-distill) Short Merge DPO O1-Pruner CoT-Valve AdaR1(Ours) 1.5B Models Long(R1-distill) Short Merge DPO O1-Pruner CoT-Valve AdaR1(Ours) AIME25 MATH500 GSM8K Olympiad Minerva Avg.(%) 38.3 (11005) 10.0 (957) 21.7 (9079) 35.8 (9976) 40.0 (9353) 22.5 (5024) 35.8 (8426) 23.3 (12307) 9.0 (1098) 20.8 (9226) 20.8 (10224) 23.3 (9496) 14.2 (7744) 23.0 (9516) 90.2 (3534) 78.6 (591) 79.4 (916) 89.4 (2334) 92.4 (2212) 78.6 (747) 90.2 (1468) 81.0 (4416) 69.4 (740) 71.8 (1740) 81.4 (3055) 82.6 (2782) 69.6 (1299) 80.8 (2455) 88.9 (1014) 89.5 (272) 88.4 (236) 86.0 (360) 89.4 (377) 87.9 (235) 90.3 (260) 80.9 (1481) 78.2 (269) 74.2 (251) 74.8 (374) 84.6 (726) 76.3 (205) 79.2 (341) 54.4 (7492) 39.4 (910) 41.2 (3743) 55.2 (5309) 55.3 (5295) 39.6 (2313) 52.4 (4889) 41.6 (7687) 30.7 (1373) 28.6 (3767) 42.8 (6319) 44.7 (5658) 28.7 (3169) 42.1 (5802) 35.7 (4533) 28.6 (579) 25.7 (1734) 35.6 (3281) 35.3 (3259) 29.4 (629) 34.1 (1647) 26.1 (5789) 22.4 (725) 20.0 (1399) 24.3 (3905) 28.3 (3964) 19.5 (867) 23.5 (3021) - - 19.97% (-84.57%) 18.63% (-56.02%) -3.56% (-33.26%) +2.48% (-34.53%) 18.41% (-73.06%) -1.65% (-50.93%) - - 26.34% (-85.15%) 10.12% (-59.10%) 5.93% (-34.57%) +2.18% (-33.75%) 19.61% (-67.52%) -1.21% (-43.28%) Table 2: Accuracy (shown above) and length (shown below) of models and methods on different benchmarks. Avg represents the change in length and accuracy compared to the Long model (+ for increase, - for decrease). These instance-level preferences encourage the model not only to reason correctly, but also to do so concisely within the preferred reasoning style. We sample such intra-group pairs and use them as additional training data for DPO to encourage the model to favor more concise reasoning within each group. Objective. Given collelcted preference datasets Dgroup and Dinstance sampled from which contains preference pairs (x, yw, yl). With parameter β controlling the deviation from the reference model pref, DPO optimize the model by: E(x,yw,yl)DgroupDinstance max πθH (cid:20) (cid:18) log σ β log πθH (yw x) πθref (yw x) β log (cid:19)(cid:21) πθH (yl x) πθref (yl x)"
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Setup Long-CoT Models. The long thought models we chosen for our experiment are DeepSeek-R1-DistillQwen-7B and DeepSeek-R1-Distill-Qwen-1.5B, which have demonstrated excellent performance on most math problem-solving tasks. For both models, we utilize full-parameter fine-tuning. Short CoT Models. Since model merging requires Shot-CoT models, we face two issues with existing Shot-CoT models: (1) they often employ templates that differ from those used in LongCoT models; (2) they tend to exhibit substantial parameter deviations from the base model, which introduces instability during the merging process[16, 24]. To address these challenges, we fine-tune the Long-CoT models using small number of short CoT examples to obtain the corresponding Shot-CoT models. This approach ensures consistency in template usage and maintains closer parameter proximity between the two models. Dataset. Following s1[25] and Light-R1[26], we construct mixed training dataset to ensure coverage across mathematical problems of varying difficulty levels. Specifically, we combine GSM8K, MATH, and AIME datasets in ratio of 1:3:1, resulting in total of 2,500 diverse math problems. Evaluation. We use the GSM8K test set, the MATH test set, and AIME25 as in-distribution evaluation data, while Olympiad[27] and Minerva[28] are employed as out-of-distribution test sets. For evaluation metrics, we consider both accuracy and sequence length. Additionally, we report the average accuracy degrade rate and the average length reduction rate across all test sets. 5.2 Competitive Methods DPO. DPO are widely used baselines in reasoning optimization area. Follwoing the setting of [23, 4], we choose shortest sample as chosen samples and longest sample as rejected sample. CoT-Valve. CoT-Valve enables dynamic control of Chain-of-Thought length using single model by identifying and leveraging controllable direction in the models parameter space, reducing inference cost by compressing reasoning paths based on task difficulty. O1-Pruner. O1-Pruner is method designed to reduce reasoning overhead while maintaining model accuracy. It begins by establishing baseline through pre-sampling, and then applies reinforcement learning-based finetuning to guide the model toward generating more concise reasoning under accuracy-preserving constraints. 5.3 Main Results We can be seen from the Table 2 that: the Short and Merge models achieve the most significant length reduction compared to the Long Model. However, this efficiency gain is accompanied by notable degradation in accuracy, exceeding 10 percentage points. Among the models that do not suffer significant accuracy degradation, our method achieves the best length reduction performance, reaching 50.93% for the 7B model and 43.28% for the 1.5B model. Compared to DPO, our approach demonstrates both more substantial length reduction and significantly less accuracy degradation. While O1-Pruner maintains high accuracy, its length reduction effect is considerably weaker than that of our method. 5.4 Ablation Study To understand the contribution of each component of our proposed framework, we conduct an ablation study. We evaluate several variants of our model on subset of the benchmarks (AIME25, MATH500, GSM8K). Table 3 presents the results. As shown, the Merge model achieves significant average length reduction of 56.10% compared to the baseline, but this comes at substantial accuracy degradation of 12.83%. Next, we investigate the effect of Supervised Fine-Tuning (using the chosen sample in our group level preference dataset) on the merged model. SFT helps recover significant portion of the lost accuracy, bringing the average degradation down to 3.82%. However, its average length reduction is less pronounced (31.86%) compared to the Merge model without further training. 8 Bench Model Long(R1-distill) Merge Merge + SFT Merge + group level Merge + bi level AIME25 MATH500 GSM8K Avg.(%) 38.3 (11005) 21.7 (9079) 35.8 (11222) 30.8 (9049) 35.8 (8426) 90.2 (3534) 79.4 (916) 84.6 (2314) 87.8 (1565) 90.2 (1468) 88.9 (1014) 88.4 (236) 88.7 (375) 91.6 (359) 90.3 (260) - - 12.83% (-56.10%) -3.82% (-31.86%) -3.31% (-46.03%) -0.51% (-52.08%) Table 3: Ablation study of each component on several benchmarks, showing that the Merge + bi-level achieves the best trade-off, with 52.08% average length reduction and minimal 0.51% accuracy degradation compared to others. Introducing the group-level preference training after merging (Merge + group level) yields better results than SFT. It achieves higher average length reduction (46.03%) and slightly better accuracy recovery, with only 3.31% average degradation relative to the baseline. This indicates that training the model to select the appropriate reasoning style is effective in balancing efficiency and accuracy. Finally, the full proposed method, incorporating both group and instance-level preference training (Merge + bi level), demonstrates the most favorable trade-off. It achieves an impressive average length reduction of 52.08% while recovering accuracy to within 0.51% of the baseline, significantly outperforming all other ablated variants in terms of accuracy retention while maintaining substantial efficiency gains. This result highlights the complementary benefits of the bi-level training approach: the group level guides the model towards suitable reasoning styles, and the instance level further refines the chosen style by favoring concise and correct responses, leading to highly efficient and accurate hybrid reasoning model."
        },
        {
            "title": "6 Evaluation",
            "content": "In this section, we analyze the reasoning behavior of the obtained Hybrid Adaptive Reasoning Model (AdaR1) to gain deeper insights into its characteristics. 6.1 Thinking Ratio Study To investigate the thinking characteristics of different models, we propose the \"Thinking Ratio\" metric. This metric is designed to detect whether response constitutes deep thinking (Long-CoT) sample. Long-CoT responses typically include unique keywords (e.g., wait, recheck, hold on). By detecting the presence of these keywords in response, we can effectively determine if it is deep thinking sample. This detection method is more generalizable than relying solely on response length. We considered subset of the models from the main experiments and collected their responses on the Math Testset. Using the method described above, we analyzed the proportion of deep thinking samples for each model. Furthermore, for each category (thinking/non-thinking samples), we also calculated their accuracy. The results are shown in Figure 3. The baseline Long-CoT model predominantly employs deep thinking (0.98), yielding high accuracy. In contrast, the Naive Merge model drastically shifts towards non-thinking responses (0.94) but suffers significant accuracy degradation on both thinking (0.68) and non-thinking (0.81) paths. DPO shows moderate shift to non-thinking (0.34) while preserving accuracy. Our AdaR1 model achieves more significant shift towards non-thinking (0.72) than DPO, yet crucially maintains high accuracy for these dominant non-thinking responses (0.96), unlike the Naive Merge. This demonstrates AdaR1s effective adaptation, utilizing efficient shorter paths without substantial accuracy loss. 9 Figure 3: The proportion and accuracy of thinking and non-thinking in different methods, AdaR1 can achieve good balance and accuracy between thinking and non-thinking. 6.2 Adaptive Reasoning Study This section evaluates the adaptive reasoning capability of AdaR1(7B) models on mathematical tasks using the MATH dataset. The problems within the MATH dataset are categorized into five distinct difficulty levels (Level 1 to Level 5), making it suitable for our study. We analyze the AdaR1 models thinking ratio (mentioned in last subsection) and average correctness across these five difficulty levels. As illustrated in the left part of Figure 4, the AdaR1 model demonstrates clear trend in the utilization of thinking (Long-CoT) based on problem difficulty. clear trend is observed where the proportion of responses exhibiting long-CoT significantly increases with task difficulty. Specifically, Level 1 tasks show very low thinking ratio, which progressively increases through intermediate levels, reaching its highest ratio for Level 5 tasks. This suggests that the AdaR1 model adaptively engages in thinking more frequently when confronted with more complex problems. Regarding average correctness, as depicted in the right panel of Figure 4, the AdaR1 model generally performs well across most difficulty levels. Its accuracy is comparable to that expected from pure Long-CoT model (Deepseek-R1-Qwen-7B-Distill) and significantly surpasses that of Short-CoT model, particularly across levels 3 to 5. This outcome aligns with our expectation presented in Section 3, suggesting that the model has learned to adaptively employ Long-CoT when beneficial, thereby achieving favorable trade-off between computational efficiency and performance. Figure 4: The ratio of thinking and non-thinking CoTs of AdaR1-7B on different MATH levels (left) and the accuracy on different MATH levels of different models (right). As the difficulty increases, AdaR1 is able to think more on harder problems and maintain higher accuracy."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we demonstrate through empirical analysis that the benefits of Long-CoT reasoning vary significantly depending on the problem. Motivated by this, we propose novel two-stage training framework for adaptive reasoning. Experiments show that model trained with our method can reason adaptively to different problems. And our method significantly reduces inference costs while preserving performance, highlighting the promise of adaptive strategies for optimizing reasoning efficiency in large language models."
        },
        {
            "title": "References",
            "content": "[1] OpenAI."
        },
        {
            "title": "Learning",
            "content": "to reason with llms. https://openai.com/index/ learning-to-reason-with-llms/, 2024. [Accessed 19-09-2024]. [2] DeepSeek-AI team. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [3] Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations, 2024. [4] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2025. [5] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: survey on efficient reasoning for large language models, 2025. [6] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025. [7] Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Length-compressible chain-of-thought tuning, 2025. [8] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. [10] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities, 2024. [11] Yichong Huang, Xiaocheng Feng, Baohang Li, Yang Xiang, Hui Wang, Bing Qin, and Ting Liu. Ensemble learning for heterogeneous large language models with deep parallel collaboration, 2024. [12] Yue Zhou, Yi Chang, and Yuan Wu. Mixup model merge: Enhancing model merging performance through randomized linear interpolation, 2025. [13] Rishabh Bhardwaj, Do Duc Anh, and Soujanya Poria. Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic, 2024. [14] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch, 2024. [15] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models, 2023. 11 [16] Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng Tao. Adamerging: Adaptive model merging for multi-task learning, 2024. [17] Zhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran Ramnath, Sougata Chaudhuri, Shubham Mehrotra, Zixu, Zhu, Xiang-Bo Mao, Sitaram Asur, Na, and Cheng. comprehensive survey of llm alignment techniques: Rlhf, rlaif, ppo, dpo and more, 2024. [18] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. [19] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. [20] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback, 2024. [21] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [22] MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. [23] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms, 2025. [24] Yuyan Zhou, Liang Song, Bingning Wang, and Weipeng Chen. Metagpt: Merging large language models using model exclusive task arithmetic, 2024. [25] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [26] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025. [27] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. 12 [28] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022."
        },
        {
            "title": "A Training Details",
            "content": "For both models, we selected 2,500 problems from the MixMathematics as training data. For each problem, we sample 12 times. From each set of solutions, we randomly selected 2 solutions for training. After computing the rewards, we normalized the reward values. Both models are trained with 8 * A800-80G GPUs. The other hyperparameters used in the training process are presented in the table below. Table 4: Hyperparameters for the Deepseek-Distill-1.5B and Deepseek-Distill-7B. Hyperparameter Deepseek-Distill-1.5B Deepseek-Distill-7B. cutoff_len batch_size learning_rate num_train_epochs lr_scheduler_type M1 M2 beta 4096 32 5.0e-7 2.0 constant 4 2 0.1 4096 32 5.0e-7 2.0 constant 4 2 0."
        },
        {
            "title": "B Further Evalution of Different Methods",
            "content": "We further evaluate the performance and efficiency of different methods (AdaR1, DPO, O1-Pruner) across varying levels of problem difficulty, as illustrated in Figure 5 and Figure 6. Figure Figure 5 presents the accuracy ratio of each method relative to baseline model across different difficulty levels within the MATH dataset. The results indicate that while performance trends may vary, our proposed AdaR1 method demonstrates strong robustness. Specifically, as the inherent difficulty of the mathematical problems increases, AdaR1 is able to consistently maintain high accuracy ratio. Figure 6 show the ratio of average tokens consumed by each method to solve problems across the same difficulty spectrum. As expected, solving more difficult problems generally requires more reasoning steps and thus more tokens. However, Figure 2 reveals that AdaR1 exhibits favorable token efficiency. Critically, when faced with increasing problem difficulty, AdaR1 manages to solve these complex problems while utilizing relatively fewer tokens compared to other evaluated methods, showcasing its ability to achieve efficient reasoning even for demanding tasks. Collectively, these figures highlight AdaR1s ability to strike beneficial balance between accuracy and efficiency. It not only maintains high performance on challenging problems (Figure 5) but also does so in computationally efficient manner, particularly evident in its lower token usage for difficult instances (Figure 6), addressing limitations observed in prior methods."
        },
        {
            "title": "C Case Study",
            "content": "14 Figure 5: The ratio of accuracy at different MATH levels on different models. As the difficulty increases, AdaR1 is able to maintain high accuracy. Figure 6: The ratio of average tokens on different models. As the difficulty increases, AdaR1 is able to use relatively fewer tokens to solve difficult problems. 15 Question: Convert the point (0, 3) in rectangular coordinates to polar coordinates. Enter your answer in the form (r, θ), where > 0 and 0 θ < 2π. Lets think step by step and output the final answer within boxed{}. Ground Truth: (3, π 2 ) AdaR1: To convert the rectangular coordinate (0, 3) to polar coordinates, we use the formulas: = (cid:112)x2 + y2 and θ = arctan (cid:0) 02 + 32 = 9 = 3 - Since 2 . Thus, the polar coordinates are (3, π the point lies on the positive y-axis, the angle θ is π 2 ). (cid:1). For the point (0, 3): - = Final Answer: (3, π ) R1: Okay, so need to convert the rectangular coordinate (0, 3) to polar coordinates. Hmm, polar coordinates are represented as (r, θ), where is the radius (distance from the origin) and θ is the angle made with the positive x-axis. Let me recall the formulas to convert from rectangular to polar coordinates. First, remember that the radius can be found using the Pythagorean theorem. Since in rectangular coordinates, the point is (x, y), the distance from the origin is = (cid:112)x2 + y2. So 02 + 32 which is in this case, is 0 and is 3. Plugging that into the formula, should be 0 + 9 = 9 = 3. Okay, that seems straightforward. 2 ) is 0 and sin( π 2 ? No, because 3π 2 ) and should be sin θ = 3 sin( π 2 , positive y-axis; at θ = π, negative x-axis; at θ = 3π Now, the angle θ. know that θ is calculated using the arctangent function because tan θ = . But wait, in this case, is 0. Hmm, arctangent of 0 . But dividing by zero is undefined. need to think about what angle θ corresponds to when is 0. In rectangular coordinates, when is 0, the point lies on the y-axis. Since is positive here (y = 3), the point is on the positive y-axis. So, in terms of polar coordinates, the angle θ should be the angle that points directly upwards along the y-axis. think thats π 2 radians, which is 90 degrees. Because starting from the positive x-axis, if we rotate π 2 radians counterclockwise, we end up pointing straight up along the positive y-axis. Let me double-check that. If θ is π 2 , then converting back to rectangular coordinates, should be cos θ = 3 cos( π 2 ). cos( π 2 ) is 1, so that gives = 0 and = 3, which is exactly the point we started with. So that makes sense. Wait, but could θ also be 3π 2 is pointing downward along the negative y-axis, which would give us = 3, not 3. So θ has to be in the range where the point is on the positive y-axis, which is π 2 . Just to make sure, lets think about the unit circle. At θ = 0, were on the positive x-axis; at θ = π 2 , negative y-axis. So yeah, θ = π is correct for the positive y-axis. So, putting it all together, is 3 and θ is π Let me just write that out to make sure didnt make any mistakes. Wait, another thought: sometimes people use different ranges for θ, like from π to π. But in this case, the problem specifies that θ should be between 0 and 2π, so π 2 is within that range. So, dont need to adjust θ or anything. Just to recap: the point is (0, 3) in rectangular, so = 0, = 3. Calculating r: Calculating θ: since = 0 and is positive, θ is π think thats solid. Let me visualize it too. If plot (0, 3) on the rectangular coordinate system, its on the y-axis, three units up. In polar coordinates, thats radius of 3 and an angle of π 2 from the positive x-axis. Yep, that lines up perfectly. Just to add another check, let me compute the sine and cosine with θ = π = 3 0 = 0, which matches. sin( π no discrepancies there. So, think Im confident that the polar coordinates are (3, π reasoning, and all the checks seem to confirm that this is the correct answer. 2 ) is 0, so 2 ) is 1, so = 3 1 = 3, which also matches. Perfect, 2 . Therefore, the polar coordinates should be (3, π 2 ). dont see any mistakes in my 2 ) is the polar coordinate. 2 . So, (3, π 02 + 32 = 3. 2 . cos( π 2 ). 2 Final Answer: The polar coordinates are (3, π 2 ) . Figure 7: AdaR1 can infer answers to simple problems with small amount of CoT, whereas the R1 model requires long CoT. 16 Question: Define = (cid:80) k=1 (cid:80) 1 1 k3 . Find way to write (j+k)3 in terms of and q. Lets think step by step and output the fiand = (cid:80) k=1 (cid:80) 1 k2 k=1 j=1 nal answer within boxed{}. Ground Truth: 1 1 1 1 1 1 j= k=1 k=1 n=2 n=2 n=2 n= n=2 n=2 n1 (cid:80) (cid:80) n2 (cid:80)n1 j=1 (cid:0) 1 n2 1 n3 n2 = 1 n3 = 1 (j+k)3 = (cid:80) (cid:1). Yes, because n1 n2 (cid:80) n=2 13 = 1. Therefore, the original ... k3 . And the expression we need to evaluate is (cid:80) k3 . And the expression we need to evaluate is (cid:80) n3 (n-1) times. So, the double sum becomes: (cid:80) n3 = (cid:80) n3 = AdaR1: Okay, so have this problem where need to express double sum in terms of two known series, and q. Let me first write down whats given. We have: = (cid:80) 1 k2 and k=1 = (cid:80) (j+k)3 . Hmm, so k=1 this is double sum over and starting from 1 to infinity, and each term is 1 divided by (j + k) cubed. need to find way to write this in terms of and q. Let me think about how to approach this. Maybe can change variables to simplify the expression. If let = + k, then for each fixed n, can range from 1 to n-1, and would be - j. So, for each n, the number of terms where + = is - 1. So, maybe can rewrite the double sum as single sum over n, and for each n, sum over from 1 to n-1. Let me write that down: (cid:80) 1 n3 . Yes, that seems right. Because when + = j=1 n, can go from 1 to n-1, and for each j, is determined as - j. So, the inner sum is just adding 1 n1 n3 . Simplify that expression: (cid:80) n3 1 n3 = 1 n3 . So now, the sum n=2 can be split into two separate sums: (cid:80) 1 n3 . But notice that these sums are just and minus their first term. Lets write that: The first sum is: (cid:80) 12 = 1. Similarly, the second sum is: (cid:80) [OMITTED] Thus, the original double sum becomes: (p 1) (q 1) = 1 + 1 = q. Therefore, the final answer is: . R1: Okay, so have this problem where need to express double sum in terms of two known series, and q. Let me first write down whats given. We have: = (cid:80) 1 k2 and k=1 = (cid:80) (j+k)3 . Hmm, so this k=1 is double infinite sum where both and start from 1 and go to infinity, and each term is 1 over (j + k) cubed. need to find way to write this in terms of and q. Let me think about how to approach this. Maybe can find substitution or change variables to simplify the expression. Since the denominator is (j + k)3, maybe if let = + k, then can rewrite the sum in terms of and another variable. Lets try that. Let me set = + k. Then, for each fixed n, can range from 1 to - 1, because would be - j, which also needs to be at least 1. So, for each n, goes from 1 to - 1, and = - j. So, substituting, the double sum becomes: (cid:80) n3 . Wait, that seems promising. Because now for each n, we have term 1 n3 multiplied by the number of js, which is (n - 1). So, the inner sum is just n1 n3 . Therefore, (cid:1). Which is: (cid:0) 1 the entire expression becomes: (cid:80) n2 1 n3 (cid:80) 1 n3 = q. So, subtracting the first term from both series: (cid:80) 1 n3 = 1. Therefore, substituting back into the expression: (p 1) (q 1) = 1 + 1 = q. Wait, so the entire double sum simplifies to - q? Let me verify my steps to make sure didnt make mistake. 1. started by changing variables, setting = + k. Then for each n, goes from 1 to - 1, so = - also goes from 1 to - 1. That seems correct. 2. Then, rewrote the double sum as single sum over n, with each term being n1 n3 . That makes sense because for each n, there are (n - 1) terms contributing 1 n3 . Thats valid step because n1 n2 1 n3 1 n3 . 4. Then, separated the sums into two infinite series starting from = 2, which is correct because the original sum starts from = 2. 5. Finally, expressed the sums in terms of and by subtracting the n=1 term from each series. Thats correct because the original and start from n=1. So, putting it all together, the double sum equals - q. That seems neat.But wait, let me double-check. Maybe can compute the original double sum numerically for few terms and see if it approximates to - q. Let me compute and first. We know that... [OMITTED] Therefore, the final answer is: n3 each. 3. Then, split the fraction into 1 n3 = n3 . Simplify that: (cid:80) n2 = and (cid:80) n2 = 1 and (cid:80) n3 . But know that: (cid:80) n2 (cid:80) n3 = n2 1 (cid:80)n1 j= (cid:80) n1 n=2 n=2 n=2 n= n=2 n=2 n=1 n=2 n=1 k= j=1 1 1 1 1 1 Figure 8: AdaR1 can infer answers to difficult problems with relatively few long CoT, whereas the R1 model requires longer CoT."
        }
    ],
    "affiliations": [
        "China Agricultural University",
        "Didichuxing Co. Ltd",
        "Nanyang Technological University",
        "Sun Yat-sen University",
        "Tsinghua University",
        "Zhejiang University"
    ]
}