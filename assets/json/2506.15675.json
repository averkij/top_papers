{
    "paper_title": "Sekai: A Video Dataset towards World Exploration",
    "authors": [
        "Zhen Li",
        "Chuanhao Li",
        "Xiaofeng Mao",
        "Shaoheng Lin",
        "Ming Li",
        "Shitian Zhao",
        "Zhaopan Xu",
        "Xinyue Li",
        "Yukang Feng",
        "Jianwen Sun",
        "Zizhen Li",
        "Fanrui Zhang",
        "Jiaxin Ai",
        "Zhixiang Wang",
        "Yuwei Wu",
        "Tong He",
        "Jiangmiao Pang",
        "Yu Qiao",
        "Yunde Jia",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 5 7 6 5 1 . 6 0 5 2 : r Sekai: Video Dataset towards World Exploration Zhen Li1,2,4* Chuanhao Li1*, Xiaofeng Mao1, Shaoheng Lin1, Ming Li1, Shitian Zhao1, Zhaopan Xu1, Xinyue Li1, Yukang Feng3, Jianwen Sun3, Zizhen Li3, Fanrui Zhang3, Jiaxin Ai3, Zhixiang Wang5, Yuwei Wu2,4, Tong He1, Jiangmiao Pang1, Yu Qiao1, Yunde Jia4, Kaipeng Zhang1,3 1Shanghai AI Laboratory 2Beijing Institute of Technology 3Shanghai Innovation Institute 4Shenzhen MSU-BIT University 5The University of Tokyo https://lixsp11.github.io/sekai-project/"
        },
        {
            "title": "Abstract",
            "content": "Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and lack of annotations about exploration and the world. In this paper, we introduce SEKAI (meaning world in Japanese), high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use subset to train an interactive video world exploration model, named YUME (meaning dream in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications. We are looking for collaboration and self-motivated interns interested in interactive world generation. Contact: zhangkaipeng@pjlab.org.cn"
        },
        {
            "title": "Introduction",
            "content": "Explore. Dream. Discover. Mark Twain World exploration and interaction form the foundation of humankinds odyssey, which are practical scenarios for world generation models [1]. These models aim to adhere to the world laws (real world or games) while facilitating unrestricted exploration and interaction within environments. In this paper, we focus on the first act of world generationworld exploration, which aims to use image, text, or video to construct dynamic and realistic world for interactive and unrestricted exploration. Recent advancements in video generation [2, 3, 4, 5, 6] have been remarkable, making it promising approach for world generation through video generation. Meanwhile, camera-controlled video generation [7, 8, 9] is suitable way for world exploration, since camera trajectories can be converted This work was done during the internship at Shanghai AI Laboratory. *Equal contribution. Corresponding authors: wuyuwei@bit.edu.cn; lichuanhao@pjlab.org.cn; zhangkaipeng@pjlab.org.cn Project leader. Preprint. Under review. Figure 1: Sekai is collected from Youtube and video game s. It consists of walking and drone-view egocentric videos with recorded audio. We provide rich annotations of camera trajectories, location, crowd density, scene, weather, time of day, and captions. by keyboard and mouse inputs. However, generating long and realistic videos with precise camera control remains significant challenge. Data is critical challenge. Existing video generation datasets [10, 11, 12] are not well-suited for world exploration as they suffer from limitations: limited locations, short duration, static scenes, and lack of annotations about exploration (e.g., camera trajectories) and world annotations (e.g., location, weather and scene). SEKAI („Åõ„Åã„ÅÑ, meaning world in Japanese), high-quality In this paper, we introduce egocentric worldwide video dataset for world exploration (see Figure 1 and Figure 2). Most videos contain audio for an immersive world generation. It also benefits other applications, such as video understanding, navigation, and video-audio co-generation. Sekai-Real comprises over 5000 hours of videos collected from YouTube with high-quality annotations. Sekai-Game comprises videos from realistic video game s, Lushfoil Photography Sim, with ground-truth annotations. It has five distinct features: (1) High-quality and diverse video. All videos are recorded in 720p at 30 FPS, featuring diverse weather conditions, various times, and dynamic scenes. (2) Worldwide location. Videos are captured across 101 countries and regions, featuring over 750 cities with diverse cultures, activities, architectures, and landscapes. (3) Walking and drone view. Beyond the walking videos (e.g., citywalk and hiking), Seikai contains drone view (FPV and UAV) videos for unrestricted world exploration. (4) Long duration. All walking videos are at least 60 seconds long, ensuring real-world, long-term world exploration. (5) Rich annotations. All videos are annotated with location, scene, weather, crowd density, captions, and camera trajectories. YouTube videos annotations are of high quality, while annotations from the game are considered ground truth. To construct the Sekai dataset, we develop curation pipeline (see Section 3) for Sekai-Real (YouTube videos) and Sekai-Game (video game videos). (1) For Sekai-Real , we first manually search and download high-quality walking and drone videos. Then we introduce pre-processing pipeline to obtain video clips by shot detection, video transcoding, and quality evaluation. After that, we develop an annotation framework to annotate location, scene type, weather, crowd density, captions, and camera trajectories. Considering the large amount of data and practical usage, we further introduce video sampling module to sample the best-of-the-best videos according to the computational resources for training the video generation model. (2) For the Sekai-Game s, we first play Lushfoil Photography Sim and record videos. Then we use the same pre-processing pipeline to obtain video clips. For the annotation, we develop toolbox to record ground-truth annotations while playing. We do dataset statistics analysis to show the scale and diversity of the dataset. And, we validate the accuracy of annotations for YouTube videos. Besides, we use subset of the Sekai-Real to train an interactive video world exploration model, named YUME („ÇÜ„ÇÅ, meaning dream in Japanese). The main contribution of this work is the dataset, Sekai. It is high-quality long-form video dataset for worldwide exploration by walking or drone. We annotate diverse labels, including captions, location, scene, weather, crowd density, time-of-day, and camera trajectories for each video. Experiments demonstrate the effectiveness and the quality of the dataset. We believe Sekai will benefit the areas of video generation and world generation, and motivate valuable applications. We provide an introduction video on the project page. Figure 2: An overview of the Sekai dataset. Sekai-Real is collected from YouTube with high-quality annotations, while Sekai-Game is collected from game with ground-truth annotations."
        },
        {
            "title": "2.1 World Generation Model",
            "content": "Recent years have seen growing interest in video generation [2, 6, 13, 14, 15, 16, 17], 3D scene generation [18, 19, 20, 21, 22, 23], and 4D generation [24, 25, 26, 27, 28], with significant advancements opening up new possibilities in the development of world generation models [29, 30, 31, 32, 33]. In the realm of video generation, text-to-video generation [14, 15] has played pivotal role, achieving high-fidelity results, while image-to-video generation [16, 17, 34] has also seen notable advancements. Sora [29] further underscores the significance of video generation in the context of world generation models. Among 3D scene generation methods, techniques [20, 21, 22, 18] utilize depth estimation models [35, 36, 37] to extend 2D scenes into 3D representations. 4D scene generation [24, 25, 26] further introduces dynamics, focusing on the evolution of objects or scenes over time [27] and dynamic interactions [28]. This paper primarily focuses on interactive video generation for world exploration, aiming to construct dynamic and realistic world using image, text, or video for unrestricted exploration."
        },
        {
            "title": "2.2 Video Generation Dataset",
            "content": "The continuous development of annotated datasets has played pivotal role in shaping the landscape of artificial intelligence-generated content, offering both insights and challenges for accurate model assessment. Existing video generation datasets can be categorized as specific-scenario and 3 Figure 3: The dataset curation pipeline. * indicates we annotate trajectories for partial data. open-scenario. Typical specific-scenario datasets including UCF-101 [38], Taichi-HD [39], SkyTimelapse [40], FaceForensics++ [41], ChronoMagic [42] and Celebv-HQ [43]. These datasets have limited amount of data (with total duration of less than 800 hours), limited individual video duration (with an average length of less than 20 seconds), and generally lack annotation information (only few datasets, such as ChromoMagic, provide caption annotations). Open-scenario datasets [44, 45, 46, 47] have somewhat alleviated issues with data scale and annotation information. For example, OpenSoraPlan-V1.0 [46] includes videos with total duration of 274 hours, each accompanied by detailed captions. Similarly, the recently introduced OpenVid-1M dataset [10] comprises videos totaling 2100 hours, with long captions provided for each video. However, the average duration of individual videos still does not exceed 25 seconds, and they only provide caption annotations. MiraData [12] consists of longer videos with an average length of 72.1 seconds. It is still not long enough for the world exploration, and exploration annotations (e.g., camera poses or keyboard and mouse inputs) and world annotations (e.g., location, time and weather) are missing. By contrast, the proposed Sekai dataset focuses on egocentric world exploration, which covers walking and drone view videos across diverse locations and scenes with long video duration (1 to 39 minutes, average is 2 minutes) and rich exploratory and world annotation."
        },
        {
            "title": "3 Dataset Curation",
            "content": "The overall process of curating the Sekai dataset includes four major parts: video collection, preprocessing, annotation, and sampling, seeing Figure 3 for an illustration."
        },
        {
            "title": "3.1 Video Collection",
            "content": "In the collection stage, we collect over 8623 hours of YouTube videos and over 40 hours of game videos from Lushfoil Photography Sim. YouTube. We manually collect high-quality video URLs from popular YouTubers and extend them by searching additional videos using related keywords (e.g., walk, drone, HDR, and 4K). In total, we collect 10471 hours of walking videos (with stereo audio) and 628 hours of drone (FPV or UAV) videos. All videos were released over the past three years, with 30-minute to 12-hour duration. They are at least 1080P with 30 to 60 FPS. We download the 1080P version with the highest Mbps for further video processing and annotation. Due to network issues and some videos are broken, there are 8409 hours of walking videos and 214 hours of drone videos after downloading. Video Game. Beyond real-world data, we collect additional data from the video game since its ground-truth annotations are accessible (e.g., location, weather, and camera trajectory). Lushfoil 4 Photography Sim is video game that allows walking or using first-person drone to explore realworld landscapes. It is built by Unreal Engine 5 and showcases the games locations in stunning visual fidelity, making it an excellent source for collecting realistic synthetic data. We use OBS Studio to record 40 hours videos at 1080P 30FPS (8 to 12 Mbps) with diverse locations and weather. Scaling the amount of data is low-cost."
        },
        {
            "title": "3.2 Video Pre-processing",
            "content": "For YouTube videos, we trim two minutes from the start and end of each original video to remove the opening and ending. Then we do the following steps and obtain 6620 hours (Sekai-Real) and 60 hours (Sekai-Game) of video clips for YouTube and the game, respectively. Shot Boundary Detection. YouTube videos are often cut and stitched, and video games commonly feature teleportation pointsboth of which contribute to discontinuous shot segments in one video. Thus, following Cosmos [30], we employ TransNetV2 [48] with threshold of 0.4 for shot boundary detection. However, the original implementation runs slowly. We refactored the codebase for GPU acceleration, which is five times faster than the original version. In particular, we use the PyNVideoCodec library for video decoding and employ the CVCUDA library to offload frame operations such as color space conversion and histogram computation to the GPU. We trim five seconds from the start and end of each shot. After shot detection, the duration of video clips is from 1 to 5.88 hours. Clip Extraction and Transcoding. Considering practical processing, we split each shot into multiple one-minute clips (shorter than one minute will be discarded). In model training, we can stitch contiguous clips according to the computation resources. We re-encode each video clip using the PyNVideoCodec library to standardize the diverse codec configurations in the raw videos, targeting 720p at 30fps in H.265 MP4 format with bitrate of 4 Mbps. Evaluation of the transcoded video clips across diverse scenes yields PSNR values above 35, indicating no perceptible visual degradation. We think the world exploration should contain realistic sound. Thus, we keep the audio of walking videos. We trim the audio tracks based on the timestamps of the video clips, re-encode them into AAC format using FFmpeg at 48kHz, and mux each audio clip with its corresponding video clip. Luminance Filtering. Overly dark or bright videos are not suitable for model training. We apply simple filter based on the luma channel in YUV color space, and remove video clips with more than 15 consecutive frames of extremely high or low average brightness. Especially, this step is necessary for video game data, as the engine often employs simplified lighting and camera systems. In this step, we filter out 300 hours of videos. Quality Filtering. We use COVER [49], comprehensive video quality evaluator to filter low-quality video clips according to the technical quality metric. Technical quality evaluates issues such as image clarity, transmission distortion, and transcoding artifacts. The lowest-scoring 10% of video clips are removed after filtering. Subtitle Filtering. Some videos contain hardcoded subtitles, which are artificial texts embedded in the video frames. These subtitles compromise the videos fidelity to the real world and may introduce misleading patterns during model training. To mitigate this, we apply VideoSubFinder to detect hardcoded subtitles on the bottom one-third of the video frames. clip is flagged if it contains any subtitle that remains visible for more than 0.75 seconds, in order to reduce false positives. All flagged clips are removed, resulting in the exclusion of approximately 5% of the video clips. Camera Trajectory Filtering. For Sekai-Real, we employ state-of-the-art structure from motion (SfM) model to extract camera trajectories. However, some trajectories exhibit implausible or counterintuitive motions, so we heuristically filter out abnormal cases using the following rules. Specifically, we exclude video clips if they satisfy either of the following: (1) Multiple abrupt trajectory reversals (i.e., directional changes exceeding 150 degrees) within 10-second window. (2) camera viewpoint shift greater than 60 degrees between two consecutive frames. (3) camera position displacement greater than 5 times the average displacement of the 30 consecutive frames containing these two frames. This filtering phase is only conducted for partial data annotated with camera trajectories."
        },
        {
            "title": "3.3 Video Annotation of Sekai-Real",
            "content": "We annotate video clips using large vision-language models. 5 Location. Utilizing the Google YouTube Data API, we fetch the title and description of each video. Since most videos contain multiple chapters filmed at different locations with timeline-based descriptions, we employ GPT-4o [50] to extract formatted location for each chapter with the ISO 3166 country/region code attached for subsequent processing. We use the interval tree to efficiently match each video clip to its corresponding chapter based on the timestamp, thereby retrieving the location information. Video clips that cannot be uniquely matched to chapter are discarded, which accounts for approximately 8% of the total clips. Category and Caption. We adopt two-stage strategy to annotate each video with category and caption. In the first stage, the video is classified along four orthogonal dimensions: scene type, weather, time of day, and crowd density, each with mutually exclusive labels. The model selects the most suitable label for each and abstains when uncertain. In the second stage, we carefully design prompts that incorporate the predicted category labels, location information, and video frames to generate detailed, time-ordered descriptions of actions and scenes for each video clip. Practically, we extract one frame every two seconds from each video clip and use 72B version of Qwen2.5-VL [51] to annotate them. We deploy vLLM [52] inference services with Nginx [53] for load balance. The final caption length averages over 176 words per video clip. Camera Trajectories. We experimented with various camera trajectory annotation methods of different types, including the visual odometry method DPVO [54], the deep visual SLAM framework MegaSaM [35], and carefully designed 3D transformer VGGT [55] that outputs 3D quantities. Through empirical experiments and comparisons, we chose MegaSaM as the baseline annotation method and made adjustments to optimize annotation accuracy and efficiency. First, we replaced the monocular depth estimation model Depth Anything [56] used in MegaSaM with Video Depth Anything [36], which performs better in terms of temporal consistency. We also tried modifying the metric depth estimation model in MegaSaM by replacing UniDepth [37] with DepthPro [57]. Additionally, we optimized the official implementation of MegaSaM to support cross-machine, multiGPU parallel inference, significantly improving annotation efficiency. We annotate over 600 hours of videos (after the sampling)."
        },
        {
            "title": "3.4 Video Annotation of Sekai-Game",
            "content": "We developed concise yet comprehensive toolchain based on the open-source tools RE-UE4SS and OBS Studio to capture ground-truth annotations from video games. RE-UE4SS is powerful script system for Unreal Engine, enabling access and modification of the UE object system with minimal overhead at runtime. Based on its Lua Scripting API, we develop practical tools for video collection and annotation, including the standardization of camera system configuration, real-time camera pose capture, GUI hiding, ensuring the collection of clean data with aligned annotations. The location and category are obtained from the description of the game map, and the prompt used for captioning is tightly modified to better suit the video game context. For camera trajectories, the captured camera poses are further calibrated to compensate for delays and interpolated to synchronize with the video frames."
        },
        {
            "title": "3.5 Video Sampling",
            "content": "Given the prohibitive cost of training on the full Sekai-Real, we propose strategy to sample the best of the best clips with the highest quality and diversity. The number is related to the computational budget for further video generation model training. In this paper, we sample 400 hours of the best of the best videos as Sekai-Real-HQ."
        },
        {
            "title": "3.5.1 Quality Sampling",
            "content": "We sample the highest-quality clips according to two aspects: aesthetic quality and semantic quality. Aesthetic quality reflects the visual harmony among different elements in the video. Semantic quality assesses the semantic completeness and consistency of the content. We use COVER [49] to obtain two quality scores and sum them for each video clip. We sample ùõºùëûùë¢ùëéùëôùëñùë° ùë¶ = 0.7 proportion of video clips with the highest scores."
        },
        {
            "title": "3.5.2 Diversity Sampling",
            "content": "We balance the videos using the following modules one by one. And for Sekai-Real-HQ, the sampling ratio ùõºùëêùëúùëõùë°ùëíùëõùë° , ùõºùëôùëúùëê, ùõºùëêùëéùë°ùëí, ùõºùëêùëéùëöùëíùëü ùëé are equal to 70%, 60%, 60%, and 75%, respectivelty. Content Diversity. Given the vast volume of video clips, the presence of similar video clips is inevitable. We use InternVideo2 [58] to extract embeddings for each video clip, and apply mini batch K-Means [59] to cluster the embeddings of each countryregion. Subsequently, in each cluster, we use the scores in quality sampling to rank the samples. Then we iteratively sample video clip and remove its most similar one until 1 ùõºùëêùëúùëõùë°ùëíùëõùë° proportion of video clips have been removed. Location Diversity. We denote the number of cities as ùëÅùëê. For each city, we count the number of video clips as ùëÅ. Given sampling ratio ùõºùëôùëúùëê, we sort the cities in ascending order based on their ùëÅ. For each city in this order, we sample approximately ùëÅ ùõºùëôùëúùëê/ùëÅ videos from each city. If it is larger than the corresponding ùëÅ, we sample all video clips for this city and redistribute the shortfall proportionally across the remaining cities by updating ùõºùëôùëúùëê. Category Diversity. To ensure broad coverage across semantic categories, we perform inverseprobability weighted sampling based on four independent categories: weather, scene, time of day, and crowd density. For each category, we compute the frequency of each label and assign sampling probabilities inversely proportional to their frequencies. Assuming independence among categories, the sampling probability for video is initialized as the product of its label probabilities across the four categories. These probabilities are then normalized to sum to 1. We perform non-replacement sampling according to these probabilities until ùõºùëêùëéùë°ùëí proportion of video clips have been sampled. Camera Trajectory Diversity. We perform trajectory-aware sampling by the following steps. First, for the remaining videos, we calculate direction vector (from the start to end of the trajectory) and the overall jitter, defined as the Euclidean norm of positional variance computed every 30 frames. Next, direction vectors are discretized into bins mapped onto sphere, and jitter values are also discretized into bins. Then, joint grouping is formed based on the direction and jitter bins. Finally, we do average sampling in each joint group according to the sampling ratio ùõºùëêùëéùëöùëíùëü ùëé."
        },
        {
            "title": "4 Dataset Statistics",
            "content": "For the Sekai-Game collection, we have considered the data balance issue while playing the game. Figure 4 provides an overview of the statistical information of the Sekai-Real dataset. Overall, the SekaiReal dataset includes 101 countries and regions, with the video durations exhibiting distinct long-tail distribution. The top eight countries (such as the Japan, the United States, and the United Kingdom) account for approximately 60% of the total duration. From various perspectives, we categorize the Sekai-Real dataset into four weather types, four scene types, four time-of-day categories, and five levels of crowd density. Specifically, the majority of the videos were outdoors. For outdoor videos, the weather conditions are predominantly sunny and cloudy, and the dataset also includes unique weather situations, such as rain and snow, increasing the diversity of the dataset. Regarding the time of filming, most videos were shot during the daytime, followed by nighttime footage. This provides mix of well-lit scenes under natural light and darker scenes with artificial lighting, offering diverse challenges for model learning. In terms of crowd density, the distribution is relatively uniform, ranging from the sparsely populated forests of Finland to the bustling streets of Japan. This stepwise distribution covers variety of crowd density scenarios, providing diverse training settings for the model, such as curriculum learning and evaluation of video generation models under varying levels of crowd density. Figure 4: Statistical information on five dimensions of the Sekai-Real dataset. The statistics of Sekai-Real and Sekai-Real-HQ across multiple dimensions are shown in Figure 5. Sekai-Real-HQ is the best of the best subset of Sekai-Real, and with more balanced data distribution. 7 (a) Overall Video Quality Score (b) Location Distribution (c) Caption Tokens Distribution (d) Camera Trajectory Jitter Distribution Figure 5: Statistics of the proposed Sekai-Real and Sekai-Real-HQ dataset. Seeing Figure 5 (a), Sekai-Real demonstrates strong overall video quality scores, with more than half of the videos scoring above 0.9, while Sekai-Real-HQ exhibits higher mean video quality score and lower variance to address the long-tail distribution issue. Figure 5 (b) shows the distribution of Sekai-Reals locations. Both Sekai-Real and Sekai-Real-HQ cover wide range of countries globally. Sekai-Real-HQ demonstrates more balanced distribution, which is more effective in mitigating potential bias during model training. Figure 5 (c) shows the distribution of caption token counts in Sekai-Real and OpenVid-1M. It can be observed that the average number of caption tokens in Sekai-Real exceeds 200, which is significantly higher than OpenVid-1Ms average of around 130. Figure 5 (d) shows the distribution of camera trajectory jitter values before and after applying Camera Trajectory Diversity. We can observe that the distribution for Sekai-Real-HQ is smoother compared to that of the Subset of Sekai-Real (500 hours, before applying Camera Trajectory Diversity). This indicates that Sekai-Real-HQ achieves better diversity and more uniform distribution."
        },
        {
            "title": "5 YUME Model",
            "content": "Figure 6: Examples of videos generated by YUME using keyboard and mouse control. We train an interactive world exploration model named YUME („ÇÜ„ÇÅ, meaning dream in Japanese) using subset of the Sekai-Real-HQ. Specifically, it receives an image and allows unrestricted exploitation using keyboard and mouse control from users. We show some examples in Figure 6."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce new video dataset, named Sekai, for video generation-based world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from 101 countries and over 750 cities. We develop an efficient and effective toolchain to pre-process and annotate videos. For each video, we annotate location, scene type, weather, crowd density, captions, and camera trajectories. Besides, we introduce video sampling module to sample the best of the best videos according to the model training budget. Experiments demonstrate the effectiveness and the quality of the dataset. We believe Sekai will benefit the area of video world generation and raise valuable applications."
        },
        {
            "title": "7 Limitation",
            "content": "Insufficient training. Due to the limited computational resources, we use only small proportion of Sekai-Real-HQ in model training. Insufficient camera trajectory annotation. Due to our limited computational resources, for SekaiReal, we annotate camera trajectories only for partial data."
        },
        {
            "title": "References",
            "content": "[1] Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983, 2025. [2] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. [3] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [4] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying Liang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, Leijun Cheng, Limin Zhang, Minghao Li, Ruijie Zhang, Silan Hu, Shijie Huang, Xiaokang Wang, Yuanheng Zhao, Yuqi Wang, Ziang Wei, and Yang You. Open-sora 2.0: Training commercial-level video generation model in $200k. arXiv preprint arXiv:2503.09642, 2025. [5] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [6] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [7] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for video diffusion models. In The Thirteenth International Conference on Learning Representations. [8] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David Lindell. Sg-i2v: Self-guided trajectory control in image-to-video generation. arXiv preprint arXiv:2411.04989, 2024. [9] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. [10] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. [11] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 9 [12] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. Advances in Neural Information Processing Systems, 37:4895548970, 2024. [13] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [14] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73107320, 2024. [15] Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. arXiv preprint arXiv:2405.18750, 2024. [16] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2024. [17] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. [18] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Freeman, and Jiajun Wu. Wonderworld: Interactive 3d scene generation from single image. arXiv preprint arXiv:2406.09394, 2024. [19] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: Going from anywhere to everywhere. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. [20] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. [21] Paul Engstler, Andrea Vedaldi, Iro Laina, and Christian Rupprecht. Invisible stitch: Generating smooth 3d scenes with depth inpainting. arXiv preprint arXiv:2404.19758, 2024. [22] Lukas H√∂llein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie√üner. Text2room: Extracting In Proceedings of the IEEE/CVF International textured 3d meshes from 2d text-to-image models. Conference on Computer Vision, pages 79097920, 2023. [23] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. Text2nerf: Text-driven 3d scene generation with neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 30(12):7749 7762, 2024. [24] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79968006, 2024. [25] Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: Trajectory-conditioned text-to-4d generation. In European Conference on Computer Vision, pages 5372. Springer, 2024. [26] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multiview video diffusion model for 4d generation. Advances in Neural Information Processing Systems, 37:1527215295, 2024. [27] Dejia Xu, Hanwen Liang, Neel Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos Plataniotis, and Zhangyang Wang. Comp4d: Llm-guided compositional 4d scene generation. arXiv preprint arXiv:2403.16993, 2024. [28] Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Zedong Gao, Eric Li, Yang Liu, and Yahui Zhou. Matrix-game: Interactive world foundation model. arXiv, 2025. [29] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1:8, 2024. 10 [30] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [31] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. [32] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. [33] Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, et al. Pandora: Towards general world model with natural language actions and video states. arXiv preprint arXiv:2406.09455, 2024. [34] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [35] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [36] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. arXiv preprint arXiv:2501.12375, 2025. [37] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1010610116, 2024. [38] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [39] Aliaksandr Siarohin, St√©phane Lathuili√®re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019. [40] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 23642373, 2018. [41] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie√üner. Faceforensics++: Learning to detect manipulated facial images. In Proceedings of the IEEE/CVF international conference on computer vision, pages 111, 2019. [42] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Rui-Jie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. Advances in Neural Information Processing Systems, 37:2123621270, 2024. [43] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq: large-scale video facial attributes dataset. In European conference on computer vision, pages 650667. Springer, 2022. [44] Max Bain, Arsha Nagrani, G√ºl Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. [45] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. [46] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 11 [47] Xueyang Wang, Xiya Zhang, Yinheng Zhu, Yuchen Guo, Xiaoyun Yuan, Liuyu Xiang, Zerun Wang, Guiguang Ding, David Brady, Qionghai Dai, et al. Panda: gigapixel-level human-centric video dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 32683278, 2020. [48] Tom√°s Soucek and Jakub Lokoc. Transnet v2: An effective deep network architecture for fast shot transition detection. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1121811221, 2024. [49] Chenlong He, Qi Zheng, Ruoxi Zhu, Xiaoyang Zeng, Yibo Fan, and Zhengzhong Tu. Cover: comprehensive video quality evaluator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 57995809, June 2024. [50] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [51] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [52] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [53] Will Reese. Nginx: the high-performance web server and reverse proxy. Linux Journal, 2008(173):2, 2008. [54] Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. Advances in Neural Information Processing Systems, 36:3903339051, 2023. [55] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [56] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [57] Aleksei Bochkovskii, Ama√ÉG, Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. [58] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pages 396416. Springer, 2024. [59] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "Shenzhen MSU-BIT University",
        "The University of Tokyo"
    ]
}