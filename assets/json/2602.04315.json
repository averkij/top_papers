{
    "paper_title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
    "authors": [
        "Guoqing Ma",
        "Siheng Wang",
        "Zeyu Zhang",
        "Shan Yu",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA."
        },
        {
            "title": "Start",
            "content": "GeneralVLA: Generalizable VisionLanguageAction Models with Knowledge-Guided Trajectory Planning Guoqing Ma1 Siheng Wang2 Zeyu Zhang2 Shan Yu1 Hao Tang2 1CASIA 2Peking University Equal contribution. Project lead. Corresponding author: bjdxtanghao@gmail.com. 6 2 0 2 4 ] . [ 1 5 1 3 4 0 . 2 0 6 2 : r AbstractLarge foundation models have shown strong openworld generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable VisionLanguageAction Models with Knowledge-Guided Trajectory Planning), hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-theart methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA. I. INTRODUCTION Developing robot manipulation policies that generalize in zero-shot manner remains long-standing challenge. Foundation models can be viewed as compressed world models, through which humans distill and transmit accumulated knowledge and experience about the physical world. Motivated by the strong generalization capabilities exhibited by large vision-language models (VLMs), recent work has explored whether this paradigm can be directly extended to robot manipulation. line of prior studies [77, 31, 5] proposes openworld vision-language-action models (VLAs) by finetuning pretrained VLMs to output robot actions in an end-to-end manner. We refer to these approaches as monolithic VLA models. Such models critically depend on large-scale robotics datasets that couple on-robot sensory observations, including visual inputs and proprioceptive states, with corresponding action trajectories. Fig. 1: Overview of GeneralVLA, VLAs and earlier imitation learning methods. GeneralVLAs hierarchical design results in better generalization. It enables 3D trajectory planning framework that fully exploits the prior knowledge of foundation models. However, current monolithic VLA models are incapable of achieving off-domain zero-shot generalization at level comparable to that of VLMs and LLMs in their research domains [24, 74, 69]. Moreover, even after fine-tuning, VLMs tend to underperform compared to other models. Because they cannot provide fine-grained coordinates, unlike task-specific models(eg,. SAM [56], YOLO [29], Deformable-DETR [76].), although they possess an understanding capability. In addition, planning long-horizon manipulation trajectories in 3D space without demonstrations remains challenging due to the gap between high-level semantic reasoning and continuous geometric constraints [1, 38], as well as the inability of existing systems to accumulate and reuse experience across tasks [21]. Besides, monolithic VLA models have yet to demonstrate emergent generalization comparable to VLMs and LLMs in other domains of study and are are constrained by their inference frequency to achieve dexterous and dynamic manipulation tasks [77, 31]. To address the above challenges, we propose hierarchical architecture for VLA, GeneralVLA (Generalizable VisionLanguageAction Models with Knowledge-Guided Trajectory Planning), which can complete robotic manipulation tasks while generating rich robotic data in zero-shot manner. Specifically, the high-level ASM (Affordance Segmentation Module) leverages highly generalized vision-language-model (VLM) and the segment anything model (SAM) for task understanding and identifying the affordance positions of key objects. In addition, By converting these affordances into 3D representations via depth map, the mid-level KnowledgeGuided Trajectory Planning carries out task understanding, skill knowledge, and trajectory planning to generate 3D path that outlines the desired trajectory for the robots endeffector. Furthermore, in order to fully utilize the experience of multitasking execution, we have designed knowledge bank that enables it to summarize and store common skills. We term the combination of ASM and Knowledge-Guided Trajectory Planning the Hierarchical World Model. Both ASM and Knowledge-Guided Trajectory Planning are foundation models infused with massive world priors and are dedicated to trajectory planning like classic world models. The intermediate 3D path planning then serves as guidance for the low-level, 3D-aware control policy capable of precise manipulation. The low-level policy is executed by estimating precise grasp poses. We design HGM to sharpen grasping accuracy. The hierarchical architecture has multiple benefits. ASM integrates the generalization capabilities of VLM and SAM, enabling more precise multi-object affordance localization. The hierarchical design presented in GeneralVLA also offers additional advantages through the decoupling of ASM finetuning, LLM planning and low-level action prediction. Specifically, while the higher-level ASM is predicting semantically meaningful 2D affordance point from RGB camera inputs, it does not need to sacrifice visual understanding to preserve long horizon planning capability. The mid-level KnowledgeGuided Trajectory Planning is used for long horizon planning. By leveraging the strong textual generalization, it enables spatial reasoning, pose estimation, and trajectory planning. The lower-level policy models can additionally operate from rich 3D and proprioceptive information. In doing so, GeneralVLA inherits the general visual perception ability of VLM and SAM, semantic reasoning benefits of LLMs and the 3D spatial awareness benefits of 3D policy models [13, 28]. Our contributions are as follows: We propose zero-shot 3D trajectory planning framework that solves robotic manipulation tasks while generating rich robotic data. This paper achieves it via hierarchical VLA that fully exploits the prior knowledge of foundation models. We propose an ASM (Affordance Segmentation Module) that leverages VLM and SAM foundations with iterative refinement for accurate affordance segmentation. Additionally, knowledge bank is introduced in KnowledgeGuided Trajectory Planning to capture cross-task skills, thereby improving success rates of manipulation tasks. Experiments show that our method achieves high zeroshot accuracy on diverse manipulation tasks, outperforming state-of-the-art methods such as VoxPoser. Moreover, benchmarking experiments demonstrate that the data generated by GeneralVLA is high-quality and scalable. II. RELATED WORK LLMs and VLMs for robotics. Early attempts in leveraging LLMs or VLMs for robotics are through pretrained language [58] and visual [57, 54, 49, 45] representations. However, these are insufficient for complex semantic reasoning and generalization to the new scene in zero-shot manner. Recent research has focused on directly leveraging open world reasoning and generalization capability of LLMs and VLMs, by prompting or fine-tuning them to, e.g., generate plans [39, 10, 20, 41, 58, 23] or construct value [22] and reward functions [33, 59, 70, 46, 66]. However, LLM planners are limited to high-level macro planning, with success rates heavily dependent on the control primitives [39]. Besides, VLM-based methods (e.g., VoxPoser [22]) can only produce longsimple value functions and struggle with complex, horizon tasks. Our work simultaneously leverages VLM and LLM, enabling long-horizon planning in 3D space while incorporating precise motion control. Monolithic VLA models as language-conditioned robot policies. Monolithic VLA models have been proposed to produce robot actions given task descriptions and image observations directly [6, 26, 77, 12, 31, 55]. Monolithic VLA models are often constructed from VLMs [42, 3, 9, 40], and are trained on large-scale on-robot data [6, 52, 30] to predict actions as text or special tokens. However, due to the lack of coverage in existing robotics datasets, they must be finetuned in-domain on expensive on-robot data. Their action frequency is also constrained by inference frequency, limiting their capability to achieve dexterous and dynamic tasks. The most relevant monolithic VLA model to our work is LLARVA [51], which predicts end-effector trajectories in addition to robot actions. However, LLARVA only uses trajectory prediction as an auxiliary task to improve the action prediction of monolithic VLA model. In contrast, our work takes hierarchical approach that can preserve semantic reasoning when fine-tuning the ASM, while avoiding any trade-off between stronger reasoning and visual understanding. It enables us to fully exploit the prior knowledge of foundation models, leverage abundant and inexpensive off-domain data, and achieve zero-shot task completion. VLMs representations. Point-based predictions: common intermediate prediction interface has been keypoint affordances [60, 62, 50, 73, 32]. Keypoint affordances can be obtained through the use of openvocabulary detectors [48], iterative prompting of VLMs [50], or fine-tuning detectors to identify certain parts of an object semantically [62]. As opposed to these, our work plans 3D paths in 3D space, leveraging the powerful reasoning capabilities of LLM. for predicting intermediate Trajectory-based predictions: The idea of using trajectorybased task specifications to condition low-level policies was proposed in RT-trajectory [15], largely from the perspective the emergence of of flexible task specification. Similarly, track-any-point (TAP) models [8, 65] has enabled policies conditioned on object trajectories [71, 68, 4] or points sampled from fixed grid in the image [67]. Our work performs trajectory planning in 3D space and enables obstacle avoidance and global planning, achieving zero-shot completion. Leveraging simulation data for training robot policies. There has been extensive work on leveraging simulation for Fig. 2: Inference workflow of of GeneralVLA. (a) The high-level ASM is called to generate the 2D points and corresponding semantic information. (b) The mid-level Knowledge-Guided Trajectory Planning carries out task understanding, 3D reasoning and planning to produce 3D path indicating the desired robot end-effector trajectory. (c) The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy enhanced by HGM for precise manipulation. robot learning. Simulation data is popular in reinforcement learning (RL), as RL on real robotic systems is often impractical due to high sample complexity and safety concerns [35, 18, 64]. Recently, simulation has been also exploited to directly generate [11] or bootstrap [47] large-scale datasets for imitation learning, to reduce the amount of expensive robot teleoperation data needed. Our work harnesses the inherent capabilities of VLM, SAM and LLM as world model for planning, thereby generating simulation data. This simulation data is massive, low-cost, and of high quality with high success rate. III. THE PROPOSED GENERALVLA A. GeneralVLAs Framework In this work, we examine how VLA models can leverage prior knowledge of foundation models and demonstrate crossdomain transfer capabilities, as opposed to relying purely on expensive observation-language action data collected on robot. GeneralVLA is hierarchical VLA model designed for this purpose, exhibiting generalizable and robust manipulation. It consists of three interconnected models  (Fig. 2)  : first, higher-level ASM produces point affordance guidance (detailed in Sec. III-B); second, mid-level 3DAgent serves as general knowledge world model to produce 3D path guidance (detailed in Sec. III-C); and third, low-level policy that produces actions conditioned on 3D paths (detailed in Sec. III-D). B. Affordance Segmentation Module ASM is used to perceive the current scene, understand the affordance of objects and obstacles, and mark them in the form of points (Fig. 2a). This facilitates understanding and planning by the following 3DAgent. Traditional VLM models already possess certain capabilities for recognizing the 2D positions of objects [73, 63], but their ability to locate the precise positions of objects and their affordances is still relatively weak, failing to meet the demands of high-precision operations. To address this, we incorporate SAMs segmentation priors and design refinement process. To achieve finer-grained segmentation results (e.g., identifying graspable regions of robotic arm in an image), we first leverage the reasoning capabilities of multimodal large language model (MLLM) to guide the segmentation process of SAM [34], as illustrated in Fig 3. When the LLM is required to produce binary segmentation mask, the generated output sequence includes special <SEG> token. We extract the hidden representation hseg from the last layer of the LLM corresponding to this <SEG> token, and project it through an MLP to obtain the segmentation embedding hseg. The overall process can be summarized as: hseg = γ(hseg), = Fenc(ximg), ˆM = Fdec(hseg, ). (1) where the ˆM denotes the final segmentation mask, and the enc and dec refer to the SAMs Encoder and Decoder. However, single-pass segmentation may suffer from over-segmentation or under-segmentation, which degrades the accuracy of the extracted 2D coordinates. Such errors are further propagated to the reconstructed 3D geometry, resulting in discrepancies between the planned and actual 3D trajectories, and ultimately leading to execution failures of the robotic manipulator. To address this issue, following SegAgent [75], we propose Fig. 3: Detailed framework of ASM and 3DAgent. (a) Given the input image and task text as query, the multimodal LLM (e.g., LLaVA [36]) generates text output. The last-layer embedding for the <SEG> token is then decoded into the segmentation mask via the decoder. We use LoRA [19] for efficient fine-tuning. The choice of vision backbone can be flexible (e.g., SAM3 [7]). an iterative segmentation refinement mechanism. Specifically, after the Interaction Segmentation Tool produces an initial segmentation, multimodal large language model (MLLM) evaluates the result and provides two types of feedback points: positive points, indicating regions that are correctly segmented, and negative points, indicating regions with segmentation errors. These points are then used to guide the Interaction Segmentation Tool in refining its prediction in the next iteration. This interaction process is repeated for up to iterations. When no negative points are generated, the segmentation is considered sufficiently accurate, and the corresponding result is taken as the final output. C. Knowledge-Guided Trajectory Planning Due to the insufficient capability of existing visual foundation models in understanding and planning in 3D scenes, we leverage the textual information of 3D points and utilize the powerful text generalization ability of LLMs to solve the 3D path planning problem. To fully utilize the experience of multitasking execution, we have designed knowledge bank that enables it to summarize and store common skills. We refer to this process as 3DAgent (Fig. 2b). The information from ASM includes 2D point information and object semantics. We use depth information to project the 2D points into the depth field to obtain 3D information, thereby acquiring 3D point information along with object semantics. The task instruction, 3D point information, and object semantics are then input into the LLM. The LLM can understand the task, comprehend the scene, and plan the trajectory of the robotic arm. The trajectory includes states for the gripper being closed and open. motion trajectory can consist of multiple stages, enabling longhorizon planning and avoiding the discrete sub-tasks issues in traditional planning method, as well as the problems arising from stitching discrete sub-tasks [39]. Additionally, the motion trajectory can satisfy obstacle avoidance requirements. In our experiments, when the number of points per object exceeds 3, the LLM can effectively understand the spatial pose of the object. 3DAgent is equipped with KnowledgeBank. The integration proceeds in three steps: (i) knowledge retrieval, (ii) knowledge construction, and (iii) knowledge consolidation, as shown in Fig. 2b. During knowledge retrieval, the agent queries KnowledgeBank with the current query context to identify the top-k relevant experiences and their corresponding knowledge items using embedding-based similarity search. Retrieved items are injected into the agents system instruction, ensuring that the manipulation is grounded in useful past experiences. When the current query task is completed, we will perform knowledge construction to extract new knowledge items. The first step is to obtain proxy signals for the correctness of completed trajectories: we adopt an LLM-as-a-judge [14] to label outcomes as success or failure, given the query and trajectory, without access to any ground-truth. Based on these signals, we apply different extraction strategies: successful experiences contribute validated manipulation strategies, while failed ones supply counterfactual signals and pitfalls that help sharpen guardrails. In practice, we extract multiple Fig. 4: Example GeneralVLA rollouts demonstrate its strong performance in multi-object, multi-stage scenes, achieved by leveraging ASMs segmentation capability, 3DAgents spatial reasoning ability, and the robust execution of the low-level 3D policy. knowledge items for each trajectory/experience (Fig. 3b). Finally, knowledge consolidation incorporates these items into KnowledgeBank with simple addition operation, maintaining an evolving repository of knowledge items. Together, these steps form closed-loop process: the agent leverages past experiences, constructs new knowledge from current tasks, and continually updates its knowledge, enabling sustained evolution in test-time learning scenarios. D. Path Guided Low-level policy The planned 3D Path is macroscopic coarse trajectory. When grasping an object at grasp point in 3D space, precise grasp pose estimation is required to ensure robust grasping of different objects. To enhance the precision of robotic arm operations, we designed multimodal hybrid grasping module (HGM). The 3D point information can locate the 3D spatial range of the object. After determining the 3D spatial range, RGB color and depth are fused, and point cloud data is obtained using the inverse projection method. Once the object point cloud data is determined, we use the grasp prediction model [72] to estimate the grasp pose. Additionally, since graspnet provides multiple candidate grasp points, we filter out those that would cause collisions and select the grasp pose whose grasp center is closest to the objects center point. This approach allows us to obtain the optimal task-specific grasp pose, all leveraging the capabilities of the foundation model. Due to cropping the point cloud using 3D spatial range, the speed of grasp pose estimation is improved. After the action is generated, motion planner can be used to move the robot to the desired pose, as detailed in Fig. 2c. IV. EXPERIMENTS Our experiments are designed to address two questions: 1) Can GeneralVLA accurately solve diverse set of tasks in zero-shot manner? 2) Can data generated from GeneralVLA be used to train robust policy? Implementation details. We use our ASM to detect and extract object information. To estimate object positions more accurately and enable the language model to estimate the objects 3D pose, each object is sampled with at least three points. We use Deepseek R1 for reasoning and planning on pure text-based 3D points. To ensure zero-shot execution within reasonable budget, we limit the number of 3D points in the 3D path to 20. This also ensures the stability of the planning process and avoids excessively long planning. Full prompts are included in the Appendix. To theoretically validate the feasibility of the GeneralVLA framework without overly increasing system complexity, we used only the front viewpoint, which typically offers the best perspective. For better reasoning by the VLM, we use resolution of 256 256. A. Zero-shot Performance in Simulation We empirically study the zero-shot capability of GeneralVLA in solving 14 diverse tasks in simulation, covering wide range of task configurations and action primitives for both prehensile and non-prehensile tasks. Our simulation experiments are reported to ensure reproducibility and provide benchmark for future methods. TABLE I: Task-averaged success rate % for zero-shot evaluation. GeneralVLA outperformed other baselines in 10 out of 14 simulation tasks from RLBench [25]. w/o PA indicates that only the pre-tuned VLM is used for localization without ASM. Each task was evaluated over 3 seeds to obtain the task-averaged success rate and standard deviations. Method Put block Play jenga Open jar Close box Open box Pickup cup Push block VoxPoser [22] CAP [39] Scaling-up [17] GeneralVLA (Ours) GeneralVLA w/o PA 70.702.31 84.0016.00 77.336.11 93.333.06 72.007.21 0.000.00 0.000.00 0.000.00 84.6711.02 60.679. 0.000.00 0.000.00 78.6711.55 84.003.46 67.3312.06 0.000.00 0.000.00 0.000.00 52.0011.14 28.677.57 0.000.00 0.000.00 0.000.00 35.3312.86 8.674.16 26.7014.00 14.674.62 9.332.26 88.671.15 74.0010.58 25.338.33 8.004.00 5.336.11 22.6714.05 12.008.72 Method Take umbrella Sort mustard Open wine Lamp on Put knife Pick & lift Insert block VoxPoser [22] CAP [39] Scaling-up [17] GeneralVLA (Ours) GeneralVLA w/o PA 33.338.33 4.004.00 6.672.31 67.3314.05 46.0012.00 96.006.93 0.000.00 41.3312.86 76.0015.62 56.008.72 8.004.00 0.000.00 33.3320.13 47.6611.37 22.0011.14 57.3012.22 64.006.93 60.008.00 74.6711.37 57.3315.53 92.004.00 14.678.33 24.000.00 60.6719.22 44.004.00 96.000.00 100.000.00 100.000.00 92.006.00 64.6712. 0.000.00 0.000.00 0.000.00 32.676.43 13.334.16 [17] Environment and tasks. The simulation setup involves Franka Panda robot with parallel gripper, using CoppeliaSim and PyRep for interfacing. Four RGB-D cameras capture input observations in front of tabletop environment. RLBench [25] is used as the task benchmark, with 14 sampled tasks that cover various object category, object position variations and task horizons. The robots actions are represented as waypoints, with trajectories computed and executed via motion planner [61]. Baselines. We compare against three state-of-the-art zeroshot data generation approaches: Code-as-Policies (CAP) [39], Scaling-up-Distilling-Down (Scaling-up) and VoxPoser [22]. CAP uses language models to generate programs that call hand-crafted primitive actions, while VoxPoser predicts waypoints via 3D voxel map of value functions. Scaling-up leverages an LLM with 6 DoF exploration primitives to generate robotic data for policy distillation. We provided CAP and Scaling-up with ground truth simulation states and object models, and VoxPoser with segmented object point clouds, which inherently disadvantages GeneralVLA in comparison. Results: GeneralVLA can generate successful trajectories for all 14 tasks, while Scaling-up, VoxPoser, and CAP cover only 10, 9, and 7 tasks, respectively (Tab. I). GeneralVLA outperforms the baselines in 10 out of the 14 tasks. The three lowest-performing tasks for GeneralVLA are non-prehensile or complex fine-grained manipulation tasks that require more precise visual 3D pose estimation or dynamic adjustment during execution. VoxPoser fails in tasks that require moving the arm beyond 4-DoF. We further tested the multi-stage execution capability  (Fig. 4)  . The results show that GeneralVLA can generalize to multi-object, multi-stage tasks. B. Behavior cloning with demonstrations from GeneralVLA GeneralVLA are computationally expensive with the present to fully leverage of ASM and 3DAgent. Besides, the experience from previous successful plans due to the replanning from scratch for each time. It holds the potential to generate useful training data. Therefore, we record the it fail trajectories generated by zero-shot in Sec. IV-A and distill them into policy network to improve the execution efficiency of the robotic arm. This also fully utilizes the experience from successful plans to further reduce the error rate. We also compare performance against model trained on humangenerated demonstrations across the 12 tasks. We use the data to train behavior cloning policies. TABLE III: Quantitative comparisons on object reference (RoboRefIt). The metric is percentage of predicted points within the target mask. Method Reference Accuracy GPT-4o [53] OpenAI 24 15.31.3 LLaVA-NeXT [43] LLaVA 24 20.00.9 Qwen2.5-VL [2] Qwen 25 24.10.9 SpatialLLM [44] CVPR 25 21.30.9 ASM Ours 63.41.4 TABLE Ablation data composition. on IV: the Data Accuracy No VQA 52.12.8 No LVIS 32.12.2 No Pixel 48.21.1 No Sim 51.91.7 No Robo 56.21.8 All 63.41.4 Data generation details. We generate 10 successful demonstrations per task. We use the systems success condition to filter for successful demonstrations. Each of the demonstrations consists of language instruction, RGB-D frames for the trajectory, and waypoints represented as 6 DoF gripper poses and states. For the tasks where the baselines were unable to generate any successful demonstrations, we patched the missing training data with RLBench system-generated demonstrations. Training and evaluation protocol. We train model using the generated demonstrations: the RVT-2 model [13], transformer-based robotic manipulation behavior cloning model that expects tokenized voxel grids and language instructions as inputs and predicts discretized voxel grid 6 DoF poses and gripper states. For all the generated training datasets, we train multi-task RVT-2 policy with batch size of 4 for 30k iterations on single RTX A40. To ensure consistent evaluation, we generate one set of testing environments with RLBench. We evaluate the last checkpoint from each of the TABLE II: Behavior Cloning with different generated data. The behavior cloning policy trained on the data generated by GeneralVLA provides the best performance on 10 out of 12 tasks compared to the other autonomous data generation baselines (excluding RLBench). We report the Success Rate % for behaviour cloning policies trained with data generated from VoxPoser [22] and Code as Policies [39] in comparison. Note that the RLBench [25] baseline uses human expert demonstrations and is considered an upper bound for behavior cloning. Data Models Put block Play jenga Open jar Close box Open box Pickup cup VoxPoser [22] CAP [39] Scaling-up [17] GeneralVLA (Ours) RLBench [25] RVT-2 [13] RVT-2 [13] RVT-2 [13] RVT-2 [13] RVT-2 [13] 2.672.31 6.672.31 22.6715.14 86.673.06 20.0018.33 - - - 82.679.45 81.339.24 - - 5.336.11 21.6713.05 58.6745.49 - - - 54.0012.00 68.0024.98 - - - 32.6714.19 14.676. 4.004.00 14.6712.86 14.672.31 56.005.29 54.6723.09 Data Models Take umbrella Sort mustard Open wine Lamp on Put knife Pick & lift VoxPoser [22] CAP [39] Scaling-up [17] GeneralVLA (Ours) RLBench [25] RVT-2 [13] RVT-2 [13] RVT-2 [13] RVT-2 [13] RVT-2 [13] 4.004.00 13.3310.06 4.004.00 87.335.03 58.6750. 0.000.00 - 0.000.00 60.677.57 53.3334.02 1.332.31 - 81.3312.86 81.338.33 86.6712.86 5.334.62 8.0016.00 76.004.00 88.674.16 84.0013.86 1.332.31 9.336.11 5.332.31 8.004.00 30.6710.07 5.671.64 46.672.31 53.3310.06 47.334.16 62.679.24 Fig. 5: GeneralVLA is an open-vocabulary robot demonstration generation system. We show zero-shot demonstrations for 4 tasks in the real world. trained policies. Each policy is evaluated for 50 episodes across each task using 3 different seeds. We measure the success rate based on the simulation-defined success condition. Results: Policies trained using GeneralVLA data perform similarly to policies trained using hand-scripted demonstrations for RVT-2 (Tab. II). Training on either GeneralVLA or handscripted demonstrations results in performance difference of just 2.7% on average across all tasks. Furthermore, models trained on data from the baselines exhibit statistically lower performance (VoxPoser, Scaling-up, and CAP). One of the main factors potentially contributing to the performance differences could be that GeneralVLA generates diverse expert trajectories that are preferable to humans. We also observe that the policy trained on GeneralVLA data achieves lower standard deviation of 6.24, on average, across all tasks, compared to the zero-shot performance standard deviation of 11.02. This suggests the benefits of training on generated data instead of relying solely on zero-shot deployment. C. Real-world experiments Environment and tasks. We conduct zero-shot tests on an Agilex-2.0 Piper manipulator equipped with parallel gripper. We use top-facing Intel RealSense L515 LiDAR RGB-D camera. We selected 4 representative real-world tasks: move spray bottle, open drawer, open jar, sort object, all conditioned on language instructions. Each task was evaluated over 10 episodes with varying object poses across 3 trials. TABLE V: We present comparison of success rates for task completion in zero-shot manner (Code as Policies [39] and GeneralVLA) Method Move spray bottle Open drawer Open jar Sort object CAP (0-shot) Robopoint (0-shot) GeneralVLA (0-shot) 6.67 0.00 63.33.00 0.00 0.00 36.67 36.67 20.00 50.00 70.00 63.33 76.67 Results: GeneralVLA is able to generate successful demonstrations for each of the 4 real world tasks. Taking Move spray bottle as an example, GeneralVLA can calculate the height of the bottle using 3DLLM and provide suitable placement position, while Robopoint lacks trajectory planning. In the Open draw task, 3DLLM can determine the orientation of the drawer, whereas CAP does not have pre-designed handcrafted primitive actions for opening drawers. V. ABLATION STUDY A. Point Location Accuracy of ASM In Tab. III, we report the average prediction accuracy for ASM and the baselines, along with the standard deviation computed from 3 different runs. The accuracy is calculated as the percentage of predicted points within the ground truth target mask. We can see that ASM achieves significantly higher accuracy than all baselines, demonstrating the power of ASM in spatial reasoning and precise target generation. In Tab. IV, we evaluated the importance of each data component on the RoboRefIt benchmark. Each data component VQA on real images, object detection from LVIS, object reference on real and synthetic images significantly contributes to overall accuracy. This highlights the value of general problem formulation that incorporates diverse data sources. Among them, LVIS provides precise semantic information and location information, playing an important role. B. Information Required by 3DAgent In the 3DAgent planning process, we designed multiple components, including 3D point information input, with each TABLE VII: Ablation Study on the HGM Module. Method Play jenga Take umbrella HGM w/o rgb HGM w/o 3D point HGM w/o filter-C HGM w/o filter-N HGM 56.674.53 0.000.00 58.005.52 76.337.24 84.6711. 32.3314.03 0.000.00 53.0012.41 54.6714.00 67.3314.05 Fig. 6: The multi-view robustness of ASM. This assist 3DAgent in reasoning about the direction for pulling out the block. TABLE VI: Ablation Study on 3DAgent for Trajectory Planning. Method Take umbrella Put block 3DAgent-2D 3DAgent-1point 3DAgent w/o obstacle 3DAgent 4.004.00 24.004.00 23.337.57 67.3314.05 19.333.06 82.007.21 91.334.16 93.333.06 object having no fewer than 3 points and incorporating target object and obstacle information. We conducted ablation experiments on these components (Tab. VI). Among them, 3DAgent-2D performs trajectory reasoning using only 2D point information and projects the inferred trajectory into 3D space for execution after reasoning. 3DAgent-1point includes only 1 3D point per object. In this case, 3DAgent cannot determine the orientation of objects or obstacles through multiple points. 3DAgent w/o obstacle only recognizes the position of the operated object, failing to achieve obstacle avoidance or determine obstacle orientation. For example, it cannot judge the orientation of an umbrella bag, thus failing to pull out the umbrella in the correct direction. The results show that the settings of 3D point information input, with no fewer than 3 points per object, and the inclusion of both target object and obstacle information are necessary, improving the success rate of trajectories planned by 3DAgent. C. Ablation Study of the HGM Module The HGM module integrates multi-modal input information, including RGB images, depth maps, and 3D point ranges. It also designs collision filter-out mechanism and nearestselection mechanism. The specific process is further organized in the appendix. We conducted ablation experiments on the above designs (Tab. VII). Among them, HGM w/o rgb does not include RGB information and only uses depth maps to estimate grasping poses. HGM w/o 3D point does not use the 3D points and their semantic information estimated by ASM, making HGM unable to determine the category of the grasped object. HGM w/o filter-C does not perform collision detection on multiple grasping poses. In this case, the estimated grasping pose may collide with surrounding obstacles or embed into the object, affecting grasping accuracy. HGM w/o filter-N does not select the grasping pose closest to the objects center point but Fig. 7: Scaling experiment. Scaling effect of model performance with increasing training demonstrations. randomly selects grasping poses that meet other conditions. The results indicate that multi-modal information from RGB images, depth maps, and 3D point ranges is necessary for task completion, and collision detection as well as optimal pose selection can also improve the algorithms success rate. D. Date scaling For the effective further development of GeneralVLA, it is crucial that the collected data supports the scaling of robotics tasks. We conducted an ablation study to evaluate the quality of the GeneralVLA-generated data for scaling. Our scaling experiments demonstrate that generating more training data via GeneralVLA improves RVT-2 policy performance  (Fig. 7)  . The data from our approach show better rate of change with slope of 0.539 for linear fit, compared to 0.178 for RLBenchgenerated data. VI. CONCLUSION In this work, we introduced GeneralVLA, hierarchical vision-language-action framework that enables zero-shot robotic manipulation and high-quality data generation without requiring real-world robotic data. Our key innovation lies in the three-tier architecture that effectively leverages foundation models world knowledge: ASM for precise affordance perception, 3DAgent for spatial reasoning and trajectory planning, and HGM for fine-grained manipulation. It performs well in scenarios requiring 3D reasoning and also features obstacle avoidance capabilities. Given that current VLMs exhibit limitations in precise spatial pose estimation, our work utilizes them only for 2D point estimation. Future work could enhance VLMs spatial perception capability by incorporating 3D pose estimation. We believe GeneralVLA establishes promising direction for leveraging foundation models in robotics, demonstrating that hierarchical decomposition combined with world knowledge can overcome data scarcity challenges while achieving robust generalization."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, et al. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. doi: 10.48550/ARXIV.2502.13923. URL https://doi.org/10. 48550/arXiv.2502.13923. [4] Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, and Shubham Tulsiani. Track2act: Predicting point tracks from internet videos enables diverse zeroshot robot manipulation. CoRR, abs/2405.01527, 2024. doi: 10.48550/ARXIV.2405.01527. URL https://doi.org/ 10.48550/arXiv.2405.01527. [5] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, et al. π0: vision-language-action flow model for general robot control. CoRR, abs/2410.24164, 2024. doi: 10.48550/ARXIV.2410.24164. URL https://doi.org/ 10.48550/arXiv.2410.24164. [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, et al. RT1: robotics transformer for real-world control at scale. In Kostas E. Bekris, Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, editors, Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, 2023. doi: 10.15607/RSS.2023.XIX.025. URL https://doi.org/ 10.15607/RSS.2023.XIX.025. [7] Nicolas Carion, Laura Gustafson, Yuan-Ting Hu, Shoubhik Debnath, Ronghang Hu, Didac Suris, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, Andrew Huang, et al. Sam 3: Segment anything with concepts. arXiv preprint arXiv:2511.16719, 2025. [8] Carl Doersch, Yi Yang, Mel Vecerık, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. TAPIR: tracking any point with per-frame initialization and temporal refinement. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 1002710038. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00923. URL https://doi.org/10.1109/ICCV51070.2023.00923. [9] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 84698488. PMLR, 2023. URL https: //proceedings.mlr.press/v202/driess23a.html. [10] Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, and Ranjay Krishna. Manipulate-anything: Automating real-world robots using vision-language models. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Conference on Robot Learning, 6-9 November 2024, Munich, Germany, volume 270 of Proceedings of Machine Learning Research, pages 53265350. PMLR, 2024. URL https: //proceedings.mlr.press/v270/duan25a.html. [11] Adam Fishman, Adithyavairavan Murali, Clemens Eppner, Bryan Peele, Byron Boots, and Dieter Fox. Motion policy networks. In Karen Liu, Dana Kulic, and Jeffrey Ichnowski, editors, Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 967977. PMLR, 2022. URL https:// proceedings.mlr.press/v205/fishman23a.html. [12] Dibya Ghosh, Homer Rich Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Quan Vuong, Ted Xiao, Pannag R. Sanketi, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In Dana Kulic, Gentiane Venture, Kostas E. Bekris, and Enrique Coronado, editors, Robotics: Science and Systems XX, Delft, The Netherlands, July 15-19, 2024, doi: 10.15607/RSS.2024.XX.090. URL https: 2024. //doi.org/10.15607/RSS.2024.XX.090. [13] Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, and Dieter Fox. RVT-2: learning precise manipulation from few demonstrations. In Dana Kulic, Gentiane Venture, Kostas E. Bekris, and Enrique Coronado, editors, Robotics: Science and Systems XX, Delft, The Netherlands, July 15-19, 2024, 2024. doi: 10.15607/RSS. 2024.XX.055. URL https://doi.org/10.15607/RSS.2024. XX.055. [14] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. The Innovation, 2024. [15] Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, et al. Rttrajectory: Robotic task generalization via hindsight traIn The Twelfth International Conferjectory sketches. ence on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=F1TKzG8LJO. for large"
        },
        {
            "title": "Pattern",
            "content": "vocabulary doi: 10.1109/CVPR.2019.00550. [16] Agrim Gupta, Piotr Dollar, and Ross B. Girshick. instance LVIS: dataset In IEEE Conference on Computer segmentation. Vision 2019, Recognition, CVPR and Long Beach, CA, USA, June 16-20, 2019, pages IEEE, 53565364. Computer Vision Foundation / 2019. URL http://openaccess.thecvf.com/content CVPR 2019/ html/Gupta LVIS Dataset for Large Vocabulary Instance Segmentation CVPR 2019 paper.html. [17] Huy Ha, Pete Florence, and Shuran Song. Scaling up and distilling down: Language-guided robot skill acquisition. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 69 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 3766 3777. PMLR, 2023. URL https://proceedings.mlr.press/ v229/ha23a.html. [18] Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, and Yashraj Narang. Dextreme: Transfer of agile in-hand manipulation from simIn IEEE International Conference ulation to reality. on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pages 59775984. IEEE, 2023. doi: 10.1109/ICRA48891.2023.10160216. URL https: //doi.org/10.1109/ICRA48891.2023.10160216. [19] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Lora: Low-rank adaptation of large Weizhu Chen. language models, 2021. URL https://arxiv.org/abs/2106. 09685. [20] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In Karen Liu, Dana Kulic, and Jeffrey Ichnowski, editors, Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 17691782. PMLR, 2022. URL https: //proceedings.mlr.press/v205/huang23c.html. [21] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022. [22] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 540562. PMLR, 2023. URL https://proceedings.mlr. press/v229/huang23b.html. [23] Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, et al. Do as can, not as say: Grounding language in robotic affordances. In Karen Liu, Dana Kulic, and Jeffrey Ichnowski, editors, Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 287318. PMLR, 2022. URL https://proceedings.mlr.press/v205/ichter23a. html. [24] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, π0. 5: vision-language-action model with et al. open-world generalization, 2025. URL https://arxiv. org/abs/2504.16054, 1(2):3. [25] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. Rlbench: The robot learning benchIEEE Robotics Aumark & learning environment. tom. Lett., 5(2):30193026, 2020. doi: 10.1109/LRA. 2020.2974707. URL https://doi.org/10.1109/LRA.2020. 2974707. [26] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. VIMA: general Internarobot manipulation with multimodal prompts. tional Conference on Machine Learning, 2023. [27] Kushal Kafle and Christopher Kanan. An analysis of In IEEE Intervisual question answering algorithms. national Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 19831991. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017. 217. URL https://doi.org/10.1109/ICCV.2017.217. [28] Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina 3d diffuser actor: Policy diffusion with Fragkiadaki. 3d scene representations. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Conference on Robot Learning, 6-9 November 2024, Munich, Germany, volume 270 of Proceedings of Machine Learning Research, pages 19491974. PMLR, 2024. URL https: //proceedings.mlr.press/v270/ke25a.html. [29] Rahima Khanam and Muhammad Hussain. Yolov11: An overview of the key architectural enhancements. arXiv preprint arXiv:2410.17725, 2024. [30] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, et al. DROID: large-scale in-the-wild robot manipulation dataset. In Dana Kulic, Gentiane Venture, Kostas E. Bekris, and Enrique Coronado, editors, Robotics: Science and Systems XX, Delft, The Netherlands, July 15-19, 2024, 2024. doi: 10.15607/RSS.2024.XX.120. URL https://doi.org/10.15607/RSS.2024.XX.120. [31] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Paul Foster, Pannag R. Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Conference on Robot Learning, 6-9 November 2024, Munich, Germany, volume 270 of Proceedings of Machine Learning Research, pages 26792713. PMLR, 2024. URL https://proceedings.mlr.press/v270/ kim25c.html. [32] Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas J. Guibas, He Wang, and Yue Wang. RAM: retrieval-based affordance transfer for generalizable zero-shot robotic manipulation. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Conference on Robot Learning, 6-9 November 2024, Munich, Germany, volume 270 of Proceedings of Machine Learning Research, pages 547565. PMLR, 2024. URL https://proceedings.mlr.press/v270/kuang25a.html. [33] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview. net/forum?id=10uNUgI5Kl. [34] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segarXiv preprint mentation via large language model. arXiv:2308.00692, 2023. [35] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Sci. Robotics, 5(47): 5986, 2020. doi: 10.1126/SCIROBOTICS.ABC5986. URL https://doi.org/10.1126/scirobotics.abc5986. [36] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-nextinterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [37] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Oier Mees, Karl Pertsch, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Conference on Robot Learning, 6-9 November 2024, Munich, Germany, volume 270 of Proceedings of Machine Learning Research, pages 3705 3728. PMLR, 2024. URL https://proceedings.mlr.press/ v270/li25c.html. [38] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022. [39] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for emIn IEEE International Conference on bodied control. Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pages 94939500. IEEE, 2023. doi: 10.1109/ICRA48891.2023.10160591. URL https: //doi.org/10.1109/ICRA48891.2023.10160591. [40] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. VILA: on pre-training for visual language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2667926689. IEEE, 2024. doi: 10.1109/CVPR52733.2024.02520. URL https://doi.org/10.1109/CVPR52733.2024.02520. [41] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: from natAuural language instructions to feasible plans. ton. Robots, 47(8):13451365, 2023. doi: 10.1007/ URL https://doi.org/10.1007/ S10514-023-10131-7. s10514-023-10131-7. Visual [42] Haotian Liu, Chunyuan Li, Qingyang Wu, instruction tuning. and Yong Jae Lee. In Alice Oh, Tristan Naumann, Amir Globerson, and Sergey Levine, Kate Saenko, Moritz Hardt, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/paper/2023/hash/ 6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference. html. [43] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, january 2024. URL https://llava-vl. github. io/blog/2024-01-30llava-next, 1(8), 2024. [44] Wufei Ma, Luoxin Ye, Celso de Melo, Alan Yuille, and Jieneng Chen. Spatialllm: compound 3d-informed design towards spatially-intelligent large multimodal In Proceedings of the Computer Vision and models. Pattern Recognition Conference, pages 1724917260, 2025. [45] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. VIP: towards universal visual reward and representation via value-implicit pre-training. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=YJ7o2wetJ2. [46] Yecheng Jason Ma, William Liang, Guanzhi Wang, DeAn Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Humanlevel reward design via coding large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview. net/forum?id=IEduRUO55F. [47] Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox. Mimicgen: data generation system for scalable robot learning using human demonstrations. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 69 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 1820 1864. PMLR, 2023. URL https://proceedings.mlr.press/ v229/mandlekar23a.html. [48] Matthias Minderer, Alexey A. Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. Simple open-vocabulary object detection. In Shai Avidan, Gabriel J. Brostow, Moustapha Cisse, Giovanni Maria Farinella, and Tal Hassner, editors, Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part X, volume 13670 of Lecture Notes in Computer Science, pages 728755. Springer, doi: 10.1007/978-3-031-20080-9 42. URL 2022. https://doi.org/10.1007/978-3-031-20080-9 42. [49] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3M: universal visual In Karen Liu, representation for robot manipulation. Dana Kulic, and Jeffrey Ichnowski, editors, Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 892909. PMLR, 2022. URL https://proceedings.mlr.press/v205/nair23a. html. [50] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, et al. PIVOT: iterative visual prompting elicits actionable knowledge for vlms. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=051jaf8MQy. [51] Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei Herzig. LLARVA: vision-action instruction tuning In Pulkit Agrawal, Oliver enhances robot Kroemer, and Wolfram Burgard, editors, Conference on Robot Learning, 6-9 November 2024, Munich, Germany, volume 270 of Proceedings of Machine Learning Relearning. search, pages 33333355. PMLR, 2024. URL https: //proceedings.mlr.press/v270/niu25a.html. [52] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, et al. Open x-embodiment: Robotic learning datasets and RT-X models : Open xembodiment collaboration. In IEEE International Conference on Robotics and Automation, ICRA 2024, Yokohama, Japan, May 13-17, 2024, pages 68926903. IEEE, 2024. doi: 10.1109/ICRA57147.2024.10611477. URL https://doi.org/10.1109/ICRA57147.2024.10611477. [53] OpenAI. Hello gpt-4o. 2024. URL https://openai.com/ index/hello-gpt-4o. [54] Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurprising effectiveness of pre-trained vision models for control. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 1735917371. PMLR, 2022. URL https://proceedings. mlr.press/v162/parisi22a.html. [55] Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik. Robot learning with sensorimotor pre-training. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 683693. PMLR, 2023. URL https: //proceedings.mlr.press/v229/radosavovic23a.html. [56] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. learning. [57] Rutav M. Shah and Vikash Kumar. RRL: resnet as representation for reinforcement In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 9465 9476. PMLR, 2021. URL http://proceedings.mlr.press/ v139/shah21a.html. [58] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Genertask plans using large language ating situated robot models. In IEEE International Conference on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pages 1152311530. IEEE, 2023. doi: 10.1109/ICRA48891.2023.10161317. URL https://doi. org/10.1109/ICRA48891.2023.10161317. [59] Sumedh Sontakke, Jesse Zhang, Sebastien M. R. Arnold, Karl Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn, and Laurent Itti. Roboclip: One demonstration is enough to learn robot policies. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/paper/2023/hash/ ae54ce310476218f26dd48c1626d5187-Abstract-Conference. html. [60] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Chelsea Finn, and Karol Hausman. Open-world object manipulation using In Jie Tan, Marc pre-trained vision-language models. Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 33973417. PMLR, 2023. URL https://proceedings.mlr.press/v229/stone23a.html. [61] Ioan Alexandru Sucan, Mark Moll, and Lydia E. Kavraki. IEEE Robotics The open motion planning library. Autom. Mag., 19(4):7282, 2012. doi: 10.1109/MRA. 2012.2205651. URL https://doi.org/10.1109/MRA.2012. 2205651. [62] Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, and Jeannette Bohg. KITE: keypoint-conditioned policies for semantic manipulation. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 10061021. PMLR, 2023. URL https://proceedings.mlr.press/v229/sundaresan23a.html. [63] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini arXiv robotics: Bringing ai into the physical world. preprint arXiv:2503.20020, 2025. [64] Marcel Torne Villasevil, Anthony Simeonov, Zechu Li, April Chan, Tao Chen, Abhishek Gupta, and Pulkit Agrawal. Reconciling reality through simulation: real-to-sim-to-real approach for robust manipulation. In Dana Kulic, Gentiane Venture, Kostas E. Bekris, and Enrique Coronado, editors, Robotics: Science and Systems XX, Delft, The Netherlands, July 15-19, 2024, 2024. doi: 10.15607/RSS.2024.XX.015. URL https://doi.org/ 10.15607/RSS.2024.XX.015. [65] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 1973819749. doi: 10.1109/ ICCV51070.2023.01813. URL https://doi.org/10.1109/ ICCV51070.2023.01813. IEEE, 2023. [66] Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David Held, and Zackory Erickson. RLVLM-F: reinforcement learning from vision language foundation model feedback. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=YSoMmNWZZx. [67] Chuan Wen, Xingyu Lin, John Ian Reyes So, Kai Chen, Qi Dou, Yang Gao, and Pieter Abbeel. Anypoint trajectory modeling for policy learning. In Dana Kulic, Gentiane Venture, Kostas E. Bekris, and Enrique Coronado, editors, Robotics: Science and Systems XX, Delft, The Netherlands, July 15-19, 2024, 2024. doi: 10.15607/RSS.2024.XX.092. URL https://doi.org/10. 15607/RSS.2024.XX.092. [68] Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein, Manuela Veloso, and Shuran Song. Flow as the cross-domain manipulation interface. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Conference on Robot Learning, 6-9 November 2024, Munich, Germany, volume 270 of Proceedings of Machine Learning Research, pages 24752499. PMLR, 2024. URL https://proceedings.mlr.press/v270/xu25a.html. [69] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [70] Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montserrat Gonzalez Arenas, HaoTien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, Yuval Tassa, and Fei Xia. Language to rewards for robotic skill synthesis. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 374404. PMLR, 2023. URL https: //proceedings.mlr.press/v229/yu23a.html. [71] Chengbo Yuan, Chuan Wen, Tong Zhang, and Yang Gao. General flow as foundation affordance for scalable robot learning. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Conference on Robot Learning, 6-9 November 2024, Munich, Germany, volume 270 of Proceedings of Machine Learning Research, pages 1541 1566. PMLR, 2024. URL https://proceedings.mlr.press/ v270/yuan25a.html. [72] Wentao Yuan, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. M2T2: multi-task masked transformer for object-centric pick and place. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 36193630. PMLR, 2023. URL https://proceedings.mlr.press/v229/yuan23a.html. [73] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: visionlanguage model for spatial affordance prediction in robotics. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Conference on Robot Learning, 6-9 November 2024, Munich, Germany, volume 270 of Proceedings of Machine Learning Research, pages 4005 4020. PMLR, 2024. URL https://proceedings.mlr.press/ v270/yuan25c.html. [74] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chainof-thought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17021713, 2025. [75] Muzhi Zhu, Yuzhuo Tian, Hao Chen, Chunluan Zhou, Qingpei Guo, Yang Liu, Ming Yang, and Chunhua Shen. Segagent: Exploring pixel understanding capabilities in mllms by imitating human annotator trajectories. arXiv preprint arXiv:2503.08625, 2025. URL https://arxiv.org/ abs/2503.08625. [76] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable arXiv transformers for end-to-end object detection. preprint arXiv:2010.04159, 2020. [77] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. RT-2: vision-languageaction models transfer web knowledge to robotic control. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Conference on Robot Learning, CoRL 2023, 69 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pages 2165 2183. PMLR, 2023. URL https://proceedings.mlr.press/ v229/zitkovich23a.html."
        },
        {
            "title": "APPENDIX",
            "content": "VII. VLM FINETUNING DATASET DETAILS Our dataset for fine-tuning consists of 5 different sources. Pixel Point Pred Data. Our point prediction dataset comes from Robopoint [73]. 347k samples in our point prediction dataset contain labels given as set of unordered points, such as po = [(0.25, 0.11), (0.22, 0.19), (0.53, 0.23)], or bounding boxes in [(cx, cy, w, h)] style. Most answers are represented as list of 2D points corresponding to locations on the image. LVIS is constructed from [16]. Unlike [73], which focuses on bounding boxes, we pay more attention to the semantic regions of objects. We randomly sample 2D points from segmentation masks and attach corresponding semantic information to them. This teaches the model how to ground language to image regions. Robot Data. Using robot data allows us to ensure that the VLM can reason about objects and robot gripper positions. We use existing online robot datasets that are not from the deployment environment to enable this VLM capability. We source 100k points from the Open X-Embodiment dataset [52], consisting of Jaco Play arm (a different embodiment from the test robot) performing manipulation tasks. We additionally generate dataset of simulated robotics tasks from SIMPLER [37]. The point positions and categories are resolved via proprioception, camera parameters, and task descriptions. VQA data. The VQA data is mix of 667K conversations from [27] where the model is asked to answer questions in natural language based on the input image, such as What is the person feeding the cat?, We keep this data as is because these VQA queries are likely to benefit VLMs semantic reasoning and visual generalization capabilities This ensures the model can reason in language. We formulate different data sources into the same format and co-train with all of them. VIII. IMPLEMENTATION AND ARCHITECTURE DETAILS Dataflow of the GeneralVLA algorithm. To provide more comprehensive illustration of the algorithms execution details, we present the dataflow diagram in Fig. 8. The algorithm takes depth map and an RGB image as input and produces the final end-effector motion trajectory of the robotic arm. Specifically, the RGB image is processed by ASM to detect task-relevant objects and generate their 2D pixel coordinates (2D points). The depth map and RGB image are fused to construct full 3D mesh of the scene. The 2D points are projected into 3D space using the camera intrinsic parameters, yielding the corresponding 3D coordinates of the objects (3D points). These 3D points are then used in two parallel branches. On the one hand, the 3D points are fed to 3DAgent, which, according to the task description, plans sequence of waypoints required to complete the task, resulting in 3D path. On the other hand, to grasp the task-relevant object, the corresponding 3D points are used to crop the full 3D mesh, producing localized point cloud (3D cloud) that contains only the points in the vicinity of the target object. This 3D cloud is then passed to graspnet for grasp pose estimation. The candidate grasps are further filtered using the selection rules implemented in HGM, yielding the optimal 6D grasp pose (6Dpose). Finally, the 6D pose and the 3D path are combined to form the complete motion trajectory of the robotic arms end-effector. The entire pipeline operates in zero-shot manner and generates trajectories with high success rates. Spatial Affordance Prediction. We formulate the problem of spatial affordance prediction as predicting set of target point coordinates {object : (x0, y0), (x1, y1), ..., (xn, yn)} in image space that satisfy the relations indicated by language prompt. To further improve prediction accuracy, we perform multiple rounds of recognition on the initially detected points. After each detection yields coordinate, we crop quarter of the original image centered on that point and use this smaller region as the new input for the next recognition step. Each point is recognized three times in total. This formulation has several advantages. First, compared to fuzzy language actions such as place the apple in the drawer, which requires additional detection of apple and drawer before execution, point prediction is much more precise and places greater emphasis on affordance. Most VLMs are trained to predict bounding boxes. However, bounding boxes often include lot of undesirable clutter due to camera perspective and are not as specific as point outputs. Second, our formulation is general enough to enable various robotic tasks. For example, the predicted points can be interpreted as contact points for grasping, region proposals for placement or waypoints for navigation. This not only allows the model to perform multiple tasks but also means it can be trained with multi-task data. Instruction Fine-tuning. Specifically, we follow the instruction tuning pipeline in Liu et al. [42]. As shown in Fig. 2A, the model consists of an image encoder, MLP projector, language tokenizer and transformer vision language model. The image encoder processes the image into set of tokens which are then projected by 2-layer MLP into the same tokens are space as the language tokens. The multimodal concatenated and passed through the language transformer. All modules are initialized with pre-trained weights. The projector and the transformer weights are allowed to update while the vision encoder and tokenizer weights are frozen. The model is autoregressive and the objective is to predict the response tokens and special token delineating the boundary between instruction and response. Action Generation Module. We generate each action using object-centric approach. We utilize M2T2 [72], NVIDIAs foundational grasp prediction model, for pick and place actions. For 6-DoF grasping, we input single 3D point cloud from RGB-D camera. The model outputs set of grasp proposals on any graspable objects, providing 6-DoF grasp candidates (3-DoF rotation and 3-DoF translation) and default gripper close states. For placement actions, M2T2 outputs set of 6-DoF placement poses, indicating where the endeffector should be before executing drop action based on trajectory plan. The network ensures the object is stably positioned without collisions. We also set default values for mask threshold and object threshold to control the number Fig. 8: Dataflow diagram of GeneralVLA. The inputs of GeneralVLA are depth map and an RGB image, while the output is the motion trajectory of the robotic arms end-effector. methods, along with their RLBench variations and success conditions. We have made some modifications to the original tasks to enhance the detection rate by Code-As-Policies and VoxPoser. A. put block Filename: put block.py Task: Pick up the green block and place it on the red mat. Success Metric: The success condition on the red mat detects the target green block. B. close box Filename: close box.py Task: Close the box. Success Metric: The revolute joint of the specified handle is at least 60 off from the starting position. C. open box Filename: open box.py Task: Open the box. Success Metric: The revolute joint of the specified handle is at least 60 off from the starting position. D. play jenga Filename: play jenga.py Task: Pull out the green jenga block. Success Metric: The green jenga block is out of its pre-defined location. E. open jar Filename: open jar.py Task: Uncap the green jar. Success Metric: The green jar is out of its pre-defined capped location. F. pickup cup Filename: Filename: pickup cup.py Task: Pick up the red cup. Success Metric: Lift up the red cup above the pre-defined location. G. take umbrella Filename: take umbrella out of stand.py Task: Pick up the umbrella out of the umbrella stand. Success Metric: Lift up the umbrella out of the umbrella stand. Fig. 9: Simulation scene setup. of proposed grasp candidates. As illustrated in the main text, the 3D point information can locate the 3D spatial range of the object. After determining the 3D spatial range, the scope of candidate grasp poses are determined. We also select the grasp pose whose grasp center is closest to the objects center point. This approach allows us to obtain the most optimal task-specific grasp pose, placement pose, and pre-action pose for manipulation tasks, all leveraging the capabilities of the foundation model. After the action is generated, motion planner can be used to move the end-effector to the desired pose as detailed in Fig. 2C. IX. EXPERIMENTS DETAILS Simulation Setup. All the simulated experiments use fourcamera setup as illustrated in Fig. 9. The cameras are positioned at the front, left shoulder, wrist, and right shoulder. All cameras are static, except for the wrist camera, which is mounted on the end effector. We did not modify the default camera poses from the original RLBench [25]. These poses maximize coverage of the entire table, and we use 256 256 resolution for better input to the VLMs. Task Details. We describe in detail each of the 12 tasks for simulation evaluation, both for trained policies and zero-shot N. pick & lift Filename: pick and lift.py Task: Pick up the red cube. Success Metric: The red cube is lifted up. X. PROMPTS ASM Prompt. We list the prompt for zero-shot manipulation and evaluation in Fig. 2. We condition the model on an image and the prompt, except when training on affordance prediction data where we used the given prompts from the dataset. We ask the model to output the position of each object related with task. Each object will contains more than 3 points to describe its position and spacial pose. 3DAgent Prompt. We list the prompt for zero-shot manipulation and evaluation in Fig. 2. We condition the model on the prompt, which contains the 3D position with the name of related objects. Note that we ask the model to output gripper changes as separate language tokens, i.e., Open Gripper/Close Gripper, as opposed to as dot as shown in simplified depictions like Fig. 4. as ASM Prompt In the image, please describe the related object in task described in questquest/quest. Provide list of points denoting the affordance position of related objects. list of enFormat your answer closed by ans and /ans tags. For example: <ans>[[cube, (0.25, 0.21), (0.22, 0.23), (0.23, 0.24)], ...]</ans> The tuple denotes the x, location of the object in the image. Each object contains more than 3 points. The coordinates should be floats ranging between 0 and 1, indicating the relative locations of the points in the image, with (0,0) at the bottom-left corner. tuples Fig. 10: Task execution in four real-world scenarios, with each column representing one task. From left to right, the images illustrate the execution process of the four tasks listed in Tab. H. sort mustard Filename: sort mustard.py Task: Pick up the yellow mustard bottle, and place it into the red container. Success Metric: The yellow mustard bottle inside red container. I. open wine Filename: open wine.py Task: Uncap the wine bottle. Success Metric: The wine bottle cap is out of its original position. J. lamp on Filename: lamp on.py Task: Turn on the lamp. Success Metric: The lamp light up. K. put knife Filename: put knife on chopping board.py Task: Pick up the knife and place it onto the chopping board. Success Metric: Knife on chopping board. L. push block Filename: push block to target.py Task: Push the red block down towards the green target. Success Metric: The red block fails within the green target. M. insert block Filename: insert block.py Task: Push the green block into the jenga tower. Success Metric: The green block inserted in. Fig. 11: Performance Distribution of GeneralVLA in play jenga task. B. Failure Analysis Our analysis in Fig. 11 reveals distinct failure tendencies in space reasoning and manipulation task. In the Play Jenga task, GeneralVLA successfully completes the task in 82% of cases. This demonstrates that ASM accurately identifies object positions, 3DAgent correctly infers the boxs 3D pose, and HGM successfully executes grasping. However, most failures occur during the action execution phase. Precise object manipulation remains challenging problem; despite our carefully designed grasping module, grasping failures are still possible. ASM exhibits low failure rate, thanks to our targeted improvements in its object position recognition capability. 3DAgent, leveraging its strong semantic reasoning ability, produces few failure cases. in are the scene in the described command coordinates of objects 3DAgent Prompt execute Please questquest/quest. The <ans>[[cube, (0.25, 0.21, 0.11), (0.22, 0.23, 0.10), (0.23, 0.24, , 0.11)], ...]</ans> Provide sequence of points denoting the trajectory of robot gripper to achieve the goal. Format your answer as list of tuples enclosed by ans and /ans tags. For example: <ans>[(0.25, 0.32, 0.10), (0.32, 0.17, 0.10), <action>CloseGripper</action>, (0.13, 0.24, 0.10), <action>OpenGripper</action>, (0.74, 0.21, 0.20), <action>Grasp</action>, ...]</ans> The tuple denotes the x, and location of the end effector of the gripper in the space. The action tags indicate the gripper action. The coordinates should be floats ranging between 0 and 1, indicating the relative locations of the points in the space. The points on the trajectory should not exceed 20. XI. FAILURE ANALYSIS A. Different Failure Modes Affordance Prediction Failures - Failure to understand the scene: ASM may err in task understanding, failing to extract task-relevant object information or lacking sufficient visual-semantic perception to provide correct object categories. - Incorrect trajectory prediction: The ASM may fail to predict the precise 2D points of target objects. For example, mislabeling boxs position onto the tabletop can lead to incorrect 3D pose estimation, or failing to mark small block due to its size. Trajectory Prediction Failures - Failure to understand the language goal: Although the 3DAgent demonstrates strong capabilities in handling diverse text question, it can misunderstand the goal and make inaccurate predictions. - 3D ambiguity: Insufficient 3D points may prevent accurate box pose estimation, or limited reasoning may fail to infer the 3D direction for extracting block. - Incorrect trajectory prediction: 3DAgent may fail to avoid obstacles, resulting in planned trajectories that collide with objects. Action Execution Failures - Incorrect object interaction: The low-level action model is not explicitly constrained to strictly follow the predicted trajectory. As result, it may deviate, interacting with the wrong object and causing task failures. - Trajectory Adherence Failures in The policy may fail during execution. For example, grasping tasks, an incorrect grasp angle can cause the object to slip, resulting in failed grasp."
        }
    ],
    "affiliations": [
        "CASIA",
        "Peking University"
    ]
}