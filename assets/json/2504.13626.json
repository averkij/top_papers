{
    "paper_title": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models",
    "authors": [
        "Yule Liu",
        "Jingyi Zheng",
        "Zhen Sun",
        "Zifan Peng",
        "Wenhan Dong",
        "Zeyang Sha",
        "Shiwen Cui",
        "Weiqiang Wang",
        "Xinlei He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities in multiple tasks. However, LRMs typically suffer from \"overthinking\" problems, where models generate significantly redundant reasoning steps while bringing limited performance gains. Existing work relies on fine-tuning to mitigate overthinking, which requires additional data, unconventional training setups, risky safety misalignment, and poor generalization. Through empirical analysis, we reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token ($\\texttt{<think>}$ and $\\texttt{</think>)}$ can effectively manipulate the model to generate fewer thoughts. Building on these insights, we propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass unnecessary intermediate steps and reduce computational costs significantly. We conduct extensive experiments to validate the utility and efficiency of ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, ThoughtMani keeps the original performance and reduces output token counts by approximately 30%, with little overhead from the CoT generator. Furthermore, we find that ThoughtMani enhances safety alignment by an average of 10%. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani provides an effective way to construct more efficient and accessible LRMs for real-world applications."
        },
        {
            "title": "Start",
            "content": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models Yule Liu1 Jingyi Zheng1 Zhen Sun1 Zifan Peng1 Wenhan Dong 1 Zeyang Sha 2 Shiwen Cui 2 Weiqiang Wang 2 Xinlei He1* 1Hong Kong University of Science and Technology (Guangzhou) 2Ant Group 5 2 0 A 8 1 ] . [ 1 6 2 6 3 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "in large reasoning models Recent advancements (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities in multiple tasks. However, LRMs typically suffer from overthinking problems, where models generate significantly redundant reasoning steps while bringing limited performance gains. Existing work relies on finetuning to mitigate overthinking, which requires additional data, unconventional training setups, risky safety misalignment, and poor generalization. Through empirical analysis, we reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token (<think> and </think>) can effectively manipulate the model to generate fewer thoughts. Building on these insights, we propose simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass unnecessary intermediate steps and reduce computational costs significantly. We conduct extensive experiments to validate the utility and efficiency of ThoughtMani. For instance, when applied to QwQ32B on the LiveBench/Code dataset, ThoughtMani keeps the original performance and reduces output token counts by approximately 30%, with little overhead from the CoT generator. Furthermore, we find that ThoughtMani enhances safety alignment by an average of 10%. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani provides an effective way to construct more efficient and accessible LRMs for real-world applications. *Corresponding author (xinleihe@hkust-gz.edu.cn). in large reasoning models Recent advancements (LRMs) have demonstrated the great potential of incorporating long-thinking processes in enhancing reasoning capabilities for complex tasks [24, 33]. By leveraging reinforcement learning (RL), LRMs are trained to generate step-by-step chain-of-thought (CoT) reasoning, breaking down problems into smaller components and performing multiple checks before arriving at final response [22, 25, 28]. Models like DeepSeekR1 [4] and QwQ [30] exemplify the effectiveness of this method, showcasing substantial improvements in reasoning accuracy. Despite the improved performance, scaling up CoT often requires exponentially larger computational resources [27]. Models like QwQ typically consume 510 times more tokens to reach conclusions compared to standard approaches. Previous studies introduce overthinking to describe the phenomenon that unnecessary steps may lead to inefficiencies, particularly in simple questions [1]. This inefficiency not only undermines the utility of LRMs in time-sensitive scenarios but also leads to additional computational resource consumption, further degrading overall system performance. Existing work has explored fine-tuning-based techniques to mitigate inefficiencies related to overthinking [1, 27]. They rely on constructing datasets that consist of different reasoning compression patterns, either skipping less critical tokens [32] or introducing task arithmetic [20] to manipulate the parameter. However, these fine-tuning-based methods often require additional data collection, leading to increased costs. Additionally, fine-tuning may introduce safety misalignment [8, 19, 35] 1 To solve the problem, we delve into the inherent characteristics of when an LRM enters and exits its thinking state. Specifically, we examine how LRMs behave when presented with human-provided thoughts framed within explicit thinking tokens (<think> and </think>) during the prompt phase. Through empirical analysis, we uncover the following distinct patterns in the behavior of LRMs trained via different methods. For RL-based LRMs, these models continue generating thoughts until they perceive that sufficient reasoning has been conducted, irrespective of encountering the closing </think> token. For Distillation-based LRMs, they terminate the reasoning process immediately upon encountering </think> token, regardless of the quantity or quality of thoughts generated. Given these insights, we propose ThoughtMani, training-free method to reduce the computational cost generated due to the overthinking problem. By providing reasoning process generated by smaller nonreasoning model (CoT generator), e.g., Qwen-2.5-7binstruct [34], and inserting it between thinking tokens, the reasoning model, e.g., QwQ, can directly extract sufficient information from the provided thoughts, thus bypassing unnecessary intermediate steps. Since the CoT generators normally cost much less computational resources than the reasoning models, ThoughtMani can significantly reduce the inference cost. We comprehensively evaluate ten different compression techniques (replicated three times each) across three LRMs on four diverse datasets. Additionally, we perform ablation studies to analyze the key factors in the proposed method, ensuring thorough validation of its effectiveness and robustness. For instance, when using Qwen-2.5-7B-Instruct as the CoT generator for QwQ-32B on the GSM-8k dataset, ThoughtMani reduces the output token count by approximately 40% (from 1,791 to 1,075 tokens), with an average additional cost of only 52 tokens from the CoT generator. Additionally, we investigate the safety performance of ThoughtMani and find that it provides an average safety gain of approximately 10% in most cases, whereas other fine-tuning-based methods exhibit safety drop of 7%. These results demonstrate that our method not only significantly reduces computational overhead but also maintains the reasoning accuracy and enhances the safety alignment of LRMs. Our contribution can be summarized as follows: We reveal unique pattern in the behavior of LRMs when external thoughts are given, which sheds light on LRMs characteristics. Specifically, we uncover how RL-based and distillation-based LRMs differ in 2 their handling of provided CoT, offering insights into their reasoning mechanisms and decision-making processes. We propose training-free inference pipeline, ThoughtMani, to reduce redundant reasoning tokens. By leveraging smaller CoT generators and strategically inserting thoughts within the reasoning process, our approach achieves significant computational savings without compromising performance or requiring additional training resources. Extensive experiments on three models and four datasets validate the superiority of ThoughtMani in terms of utility, efficiency, and safety. Our results demonstrate consistent improvements across diverse datasets and tasks, highlighting its practical applicability and robustness in real-world scenarios."
        },
        {
            "title": "2.1 Large Reasoning Model",
            "content": "By scaling up training data size and model size, large language models (LLMs) have developed powerful language understanding and generation capabilities [36], such as GPT-4o [11] and DeepSeekv3 [5], enabling rapid and coherent responses to user inputs. However, these models perform poorly when facing complex reasoning and logical analysis tasks [33, 36], falling far short of human cognitive levels. To address this issue, recent studies focus on improving the capability of language models by utilizing more inferencetime computation instead of simply scaling model parameters [26]. This line of research has already outcome many powerful LRMs such as DeepSeek-R1 [4], OpenAI-o1/o3 [22, 23], and QwQ [30], which shift from fast, intuitive processing to structured, step-bystep reasoning. Deeper reasoning capabilities enable these LRMs to make remarkable improvements when tackling challenging tasks like advanced mathematics and logical reasoning [29]."
        },
        {
            "title": "2.2 Chain-of-Thought Compression",
            "content": "Despite the improved capabilities, introducing intermediate CoTs brings additional token overhead. To enable efficient inference without performance degradation, one line of research is to shorten the length of CoT while maintaining its effectiveness. For traditional LLMs, lots of efforts have been put into reducing redundant steps [7, 17, 20] or skipping less-important tokens [10, 32] Another line is to represent the CoT using latent space compression [2, 6], which utilize embeddings instead of tokens to serve as the CoT. With the development of LRMs, the extensive inherent thinking process not only improves the reasoing capability, but also bring computation inefficiency [1]. Existing work mainly relies on fine-tuning to control model behaviors [20, 32]. However, these fine-tuning require additional data and is prohibitively expensive for larger models like [4] and introduce unexpected safety misalignment [19, 35]."
        },
        {
            "title": "3 Thought Manipulation",
            "content": "In this section, we investigate when an LRM enters and exits its thinking state. key observation is that all thoughts generated by LRMs are consistently framed within designated thinking tokens, specifically <think> and </think>. This consistent pattern intuitively suggests that inserting pre-generated CoT between explicit thinking tokens may allow the model to effectively utilize this external reasoning, thereby reducing the necessity to internally generate intermediate reasoning steps. To validate this hypothesis, we first randomly collect 100 samples each from the GSM-8k and MATH-500 datasets, which are widely used datasets for benchmarking the models reasoning ability. We then select several Qwen series modincluding Qwen-Max, Qwen-Plus, Qwenels [34], 2.5-7B-Instruct, and Qwen-2.5-3B-Instruct, to generate high-level ideas for solving the problems. The generation process is guided by the provided prompt in Appendix A, which focuses solely on high-level reasoning steps without delving into detailed calculations or producing final answers. By employing CoTs across models of different scales, we obtain CoTs with different levels of quality. Next, we insert generated thoughts, which are enclosed within <think> and </think> tokens, at the end of standardized inference template, together with the users prompts. The template is then used to invoke LRMs such as QwQ and Deepseek-Distillation-Qwen2.5-32b(14b)-instruct. This placement allows us to observe how effectively the LRMs leverage the provided CoTs to streamline their reasoning process and reduce unnecessary intermediate steps. By analyzing the outputs of these models, we try to figure out whether the externally provided CoTs in the prompt can help LRMs reduce redundant reasoning. Specifically, we count the number of occurrences of the </think> in the generated response to provide insights into how often the model starts to rethink. An example of rethinking is provided in Appendix A. Table 1: Number of occurrences of </think> in the response using different models to generate CoT, which indicates the frequency of rethinking. The dataset is sampled from the original one. Dataset Model CoT Template standard 3b max plus 7b GSM-8k MATHDistill-qwen-14b Distill-qwen-32b QwQ-32B Distill-qwen-14b Distill-qwen-32b QwQ-32B 0 0 89 3 0 171 0 0 59 3 0 0 0 38 0 0 94 0 0 44 0"
        },
        {
            "title": "Inference Template",
            "content": "<im_start> User: [Question] <im_end> <im_start> Assistant: <im_end> <think> [Generated Thought] </think> The results are shown in Table 1, and we conduct additional experiments in Appendix to show that an insert like that is more optimal approach to manipulate the thought than other templates. For RL-based LRMs, even when CoT is provided between the <think> and </think> tokens, the model still generates its own thoughts in many cases. We observe that providing higher-quality (e.g., Qwen-max) CoTs can effectively reduce the presence of rethinking. Additionally, given CoT generated by Qwen-2.5-7b as an example, on the Math-500 dataset, the average difficulty level (identified by the level data of the dataset) of the problem with/without rethinking is 3.58/2.96 out of 5. This suggests that while RL-based LRMs rely on their internal judgment of sufficient reasoning, they can be influenced by the quality of external thoughts. In contrast, distillation-based LRMs show different behavior. These models hardly generate additional thoughts beyond the provided CoT and start the final response when encountering the </think> token. This indicates that distillation-based LRMs may not truly understand the concept of reasoning or thinking. Instead, their behavior is primarily driven by patternfollowing skills learned during supervised fine-tuning. Based on these observations, the findings can be summarized as follows: For RL-based LRMs, these models continue generating thoughts until they internally perceive that sufficient reasoning has been conducted, regardless of whether the closing </think> token is encountered. The quality of the provided CoT can influence the extent of additional reasoning. The LRMs rethink the harder problems more frequently. For distillation-based LRMs, these models terminate their reasoning process immediately upon encountering the </think> token, irrespective of the quantity or quality of thoughts provided. This behavior reflects reliance on pattern-matching rather than an understanding of the reasoning process. Based on these observations, we further propose our method, ThoughtMani, to leverage the identified behaviors and improve reasoning efficiency in LRMs."
        },
        {
            "title": "4 Pipeline of ThoughtMani",
            "content": "In this section, we design an inference pipeline that innovatively involves small model to generate CoT and concatenates it at the end of the inference template. The following prompt guides the CoT generation. CoT Generation - ThoughtMani If you are teacher, you are listing the important key points for solving the problem and no calculation details should be included. You are not allowed to produce any final answer. Add <STOP> when the key points are finished. You may provide **only very high-level ideas** for solving the problem, no calculation details should be included. If you feel that you cannot solve it, output <STOP> and return. Compared to the previous CoT generation approach, the key difference is that we prompt the model to produce stopping identifier when encountering highly complex problems. This strategy aims to fully leverage the reasoning capabilities of LRMs for challenging scenarios while remaining efficient for simpler questions. if the generated CoT only contains STOP, we drop it and use the original inference template, which aims to prevent the incorrect CoTs from misleading the LRMs. The detailed pipeline is shown in Algorithm 1. Specifically,"
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Datasets. To evaluate the effectiveness of the proposed CoT-reduced reasoning process, we select four different datasets, covering reasoning ability in both math and coding. For reasoning, we select three widely used math datasets, including AIME-2024 [21], GSM-8k [3] Algorithm 1: ThoughtMani Pipeline Input: dataset = {q1, q2, . . . , qn} containing problems, CoT generator model G, reasoning model Output: Final responses {r1, r2, . . . , rn} for each problem in D. TMani <im_start> User : [Question] <im_end> <im_start> Assistant : <im_end> <think> [CoT] </think> ; TOri <im_start> User : [Question] <im_end> <im_start> Assistant : <im_end> <think> ; for each problem qi do Ci G(qi) // Generate CoTs if Ci = <STOP> then Ti TOri[Question qi] // Format"
        },
        {
            "title": "TOri with qi",
            "content": "else Ti TMani[Question qi, CoT Ci] // Format TMani with qi and Ci ri M(Ti) // Obtain final response from Append ri to the output set {r1, r2, . . . , rn} ; return {r1, r2, . . . , rn} and MATH-500 [15]. For coding, we select the coding category from LiveBench [31]. To evaluate the safety of the model response, we select the WildJailbreak [12] as the target dataset, which transforms harmful queries with randomly sampled in-the-wild jailbreak tactics includingGCG [37], AutoDAN [18], DeepInception [14], etc. It contains over 2,000 adversarial jailbreak prompts. Metrics. We quantify the performance from three perspectives, i.e., utility, efficiency, and safety. For utility, we extract answers via string matching for the AIME, GSM-8k, and MATH-500 datasets. Regarding the coding dataset, we follow the official guidance and report the pass@1 metric on private test cases. For efficiency, we compute the generated tokens from the reasoning model and the additional tokens produced by the CoT generators. The total cost of generation is evaluated as the sum of these two components. Since the CoTs are generated by smaller models, the cost of producing these additional tokens is significantly lower compared to the computational expense of the larger reasoning model. For safety, we utilize widely used safety mod4 erator, Llama-Guard-3-8B [9], to evaluate the safety of model output. If the response is unsafe, the moderator will output unsafe, followed by the reason; otherwise, the moderator will output safe. Models. Regarding CoT generators, we consider Qwen-series [34], including Qwen-Max, Qwen-Plus, Qwen-2.5-7B-Instruct, and Qwen-2.5-3B-Instruct. Since we need to accurately manipulate the inference template in the stated approach, we only consider local open-source LRMs. Regarding RL-based LRMs, we select QwQ-32B [30], which is derived by RL from Qwen-2.5-32B-Instruct. Regarding distillation-based LRMs, we select Deepseek-Distillation-Qwen-2.514B-Instruct and its 32B version, which distills the CoT generated from DeepSeek-R1 on Qwen series [4]. Specifically, we use 4-bit AWQ [16] quantized version of these models to save GPU memory and utilize the vLLM [13] framework for efficient inference. Regarding the decoding algorithm, we follow their official guideline1 and use greedy decoding to generate the outputs, where temperature is set to 0.7 and top-p is set to 0.95. In the efficiency and utility experiment, the max output token number of the AIME-2024 dataset is set to 30,000 due to the problem complexity, while we set the max output token number to 20,000 for the remaining datasets. Baselines. We take the following methods as our baselines: Empty Thought works like ThoughtMani while placing empty CoT within the thinking tokens. Truncation directly terminates the thinking process by interrupting the generation when predefined thinking budget is met and inserting </think> token to output the answer. Specifically, we cut 50% length of the original thinking process. Prompt Reduction [7] provides specific instructions like Lets quickly conclude the answer without showing step-by-step reasoning. to reduce the thinking process. Tokenskip [32] first constructs dataset where less important tokens are pruned and fine-tunes the models on the compressed dataset to enable the model to selectively skip the redundant tokens in inference. For GSM-8k and MATH-500, we first fine-tune the model on the training set and evaluate the performance on the test set, where the training dataset has 7,453 and 7,500 problems, respectively. Since 1https://huggingface.co/Qwen/QwQ-32B AIME-2024 and Code have no official training set, we transfer the tuned model from MATH, which includes more challenging problems. CoT-Valve [20] utilizes interpolation of the LLMs and LRMs parameters to collect CoTs of varying lengths, followed by progressively fine-tuning the LRMs to compress the length of CoT. We fine-tune the model on their officially provided dataset, i.e., MixChain-Z-GSM8K (6,863 samples), MixChainZ-PRM12K (12,000 samples), and select the best model for comparison. Specifically, we choose CoTValve+P as the fine-tuning pattern."
        },
        {
            "title": "5.2 Efficiency and Utility Performance",
            "content": "The main results of our experiments are shown in Table 2. EmptyThought can effectively reduce the tokens on the distillation-based models at the cost of performance, while showing limited effects on RL-based models. Prompt Reduction and Truncation can decrease token counts to some extent, but the reduction varies unpredictably, and the associated performance drop can be substantial. For Tokenskip, the performance of indomain cases, i.e., GSM-8k and MATH-500, is competitive in both utility and efficiency, while showing limited ability to transfer to other datasets. For CoTVavle, the reproduced performance shows increased utility while the compression ability is usually. Generally, ThoughtMani shows competitive performance. For the RL-based model (QwQ), ThoughtMani with four different CoT generators reduces the response length by 1%, 18%, 26%, and 37% with 1.5%, 2.8%, 0.8%, and 7.2% performance drop for the average on four different datasets. For the distillation-based models, ThoughtMani with four different CoT generators reduces the response length by 2%, 45%, 82%, and 86% with relatively higher 4.5%, 11.5%, 20.4%, and 18.2% performance drop for the average on four different datasets. Since smaller CoT generators may refuse to provide CoT in many hard cases and return empty thoughts, this makes the average CoT length relatively short. Larger CoT Generators Are Not Better. Across various experiments, we observe that using stronger CoT generators, such as Qwen-Max, can negatively impact the performance of our ThoughtMani inference framework. Larger models generate more specific and detailed CoT processes. While these detailed thoughts may appear helpful, they often contain hallucinations or reasoning paths that are misaligned with the LRMs expectations, leading to suboptimal performance. 5 Table 2: Efficiency and Utility Results: Utility is reported by Accuracy and Pass@1 for different datasets. Efficiency is reported by the number of generated tokens. Full represents inference with vanilla settings. Empty represents inference with EmptyThought. Prompt represents inference with Prompt Reduction. For ThoughtMani, we additionally reported the number of generated CoT tokens, which represents the additional cost. Method Full Empty Prompt Truncation TokenSkip CoT-Valve ThoughtMani3b ThoughtMani7b ThoughtManiPlus ThoughtManiMax Full Empty Prompt Truncation TokenSkip CoT-Valve ThoughtMani3b ThoughtMani7b ThoughtManiPlus ThoughtManiMax Full Empty Prompt Truncation TokenSkip CoT-Valve ThoughtMani3b ThoughtMani7b ThoughtManiPlus ThoughtManiMax AIME-2024 GSM-8k MATH-500 Livebench/Coding Acc Tokens CoT Acc Tokens CoT Acc Tokens CoT Pass@k Tokens CoT 70.0 40.0 43.3 36.7 50.0 74.4 70.0 70.0 75.6 60.0 68.9 43.3 50.0 30.0 40.0 63.3 62.2 54.3 20.1 21. 31.1 30.0 33.3 26.7 30.0 15.0 19.9 24.4 16.6 18.8 13661 12085 10897 12508 11172 14199 14329 13101 11400 9607 9915 9032 8808 4638 3455 10359 10210 7985 2076 8273 8215 8803 5204 8503 10967 8649 7952 2209 1838 - 11 77 209 568 - 11 77 209 - 11 77 209 568 QwQ-32B 95.3 95.1 93.1 95.7 94.4 95.5 95.3 94.0 93.5 93.9 1791 1552 665 1624 536 1725 1075 961 759 - 7 52 79 132 88.5 80.4 82.2 81.0 86.8 89.2 86.1 86.0 86.7 85.6 4537 4321 3190 4938 3225 4077 3526 2792 2335 - 22 56 141 209 Deepseek-Distillation-Qwen-2.5-32B-Instruct 88.3 89.7 89.6 88.8 89.4 88.8 88.3 86.8 87.5 88. 439 223 370 267 423 478 415 292 263 267 - 7 52 79 132 84.0 69.4 78.2 75.8 76.6 82.1 82.6 79.4 68.3 67. 2973 609 2167 1760 1567 2856 2526 2170 554 562 - 22 56 141 209 Deepseek-Distillation-Qwen-2.5-14B-Instruct 87.6 75.2 88.3 84.2 89.3 86. 86.4 85.7 88.1 89.6 756 216 516 214 314 681 691 356 272 281 - 7 52 79 132 65.3 63.8 65.2 62.6 73.2 62. 65.2 69.2 65.4 64.6 2392 796 1904 1627 1356 2190 2080 1742 600 595 - 22 56 141 209 66.7 64.3 63.5 57.8 65.9 74. 65.6 62.2 64.1 60.9 60.2 43.2 57.3 54.7 49.5 60.2 59.1 41.7 45.8 44.5 54.7 33.6 54.9 46.9 0.0 56.2 53.9 38.5 39.6 37.0 6840 5865 6518 4128 4269 6842 4409 4461 4209 6777 737 5882 10103 6084 6012 6557 528 528 465 6871 657 6312 9245 10750 6042 6670 588 625 523 - 2 120 137 183 - 2 120 137 183 - 2 120 137 183 In contrast, as discussed in Section 4, we prompt the CoT generator to return an identifier (e.g., <STOP>) when the problem is too complex to provide meaningful insights. Due to their relatively limited reasoning capabilities, smaller models tend to reject generating detailed thoughts and leave the heavy burden to the LRM. Generally, we find Qwen-2.5-7b-Instruct is the optimal or suboptimal solution in nearly all scenarios. RL-based LRMs Benefits More. Our findings indicate that RL-based LRMs benefit more significantly from external CoTs compared to distillation-based LRMs. This is because RL-based LRMs are trained to dynamically evaluate the sufficiency of their reasoning process with rewards. Given the observation in Section 3, the RL-based model has the capability to rethink, thus dynamically deciding when to accept the external CoT and when to rethink to support and revise the provided insufficient information. As result, RL-based LRMs are less sensitive to the quality of provided while preserving the accuracy and utility. On the other hand, distillation-based LRMs, which rely more heavily on pattern-matching during train6 Table 3: Safety Results: Safety is reported based on the judgment from Llama-Guard-3-8B, which assigns scores within range of 100. Higher values indicate safer responses. QwQ Qwen-32B-distill Qwen-14B-distill Full Empty Prompt Truncation TokenSkip CoT-Valve ThoughtMani-3b ThoughtMani-7b ThoughtMani-Plus ThoughtMani-Max 66.33 95.56 79.35 95.79 73.30 64.40 70.41 66.47 77.92 74.03 65.48 98.18 66.86 71.67 60.07 34.67 64.98 60.68 74.34 71. 64.80 95.97 72.70 73.85 61.40 72.26 65.66 65.11 75.07 73.48 ing, tend to terminate their reasoning process immediately upon encountering the </think> token, regardless of the quality or completeness of the provided CoT. This rigidity limits their ability to fully utilize external thoughts, resulting in less pronounced gains compared to RL-based models."
        },
        {
            "title": "5.3 Safety Performance",
            "content": "Despite the improved efficiency, it remains unclear whether the efficiency-oriented methods will influence the safety performance. To fill this gap, we evaluate the robustness of ThoughtMani, along with other baselines, against jailbreak prompts. We generate the CoT and response using the pipeline stated in Section 4 and set the model length to 10,000. The experiment compares the performance across different baselines and ThoughtMani with different CoT generators. The results are shown in Table 3. Benefit from the CoT generated by well-aligned non-reasoning models, we find that ThoughtMani improves the model safety by 10% on average, while the fine-tuning-based methods show an average 7% safety drop. Additionally, one interesting observation is that placing empty thoughts or truncating half of the generated thoughts can effectively improve safety performance. However, the mechanism remains to be further discussed. These findings uncover an important yet largely ignored aspect of the current study."
        },
        {
            "title": "5.4 Performance under Different Token Bud-",
            "content": "gets For more challenging datasets, such as AIME and Code, the inference process of RL-based LRMs typically demands significant number of tokens to achieve high-quality reasoning. To systematically study the impact of token budgets on performance, we adjust the max_model_length parameter during inference and evaluate the corresponding outcomes. Specifically, for the AIME dataset, we set the maximum token length to 10,000, 20,000, and 30,000, while for the Code dataset, we use 10,000, 15,000, and 20,000. The results are presented in Figure 1. Generally, as the max_model_length Figure 1: Relation between Model Performance and Token Budgets: Bar plot represents the token consumption and line plot represents the model utility (Accuracy or Pass@1) increases, the accuracy improves while maintaining inference efficiency. An interesting observation is that the performance gap between the baseline and ThoughtMani is more pronounced when the token budget is limited. One possible explanation is that the externally generated CoTs, especially for very hard problems, may include hallucinations or insufficient information. In such cases, the LRM (e.g., QwQ) compensates by generating additional thoughts to correct or supplement the provided CoTs, showcasing one limitation of ThoughtMani. In contrast, for simpler datasets like GSM-8k, where most questions can be resolved within fewer than 1,000 tokens, this performance gap does not exist. These findings underscore the importance of balancing token budgets with problem complexity. While ThoughtMani demonstrates benefits in reducing computational overhead, it shows effectiveness more evidently in scenarios where the token budget is sufficient."
        },
        {
            "title": "5.5 Dataset-specific CoTs",
            "content": "For the Code dataset, the task involves coding rather than mathematical reasoning, and we evaluate the im7 Table 4: Performance of Utilizing CoTs generated from Different Prompts The results are reported using Pass@1 and number of generated tokens. Normal represents using original COTs, while Specific represents using the task-specific CoTs. QwQ-32B Deepseek-Distillation-Qwen-2.5-32B-Instruct Deepseek-Distillation-Qwen-2.5-14B-Instruct Method Normal Specific Normal Specific Normal Specific Pass@1 Tokens Pass@1 Tokens Pass@1 Tokens Pass@1 Tokens Pass@1 Tokens Pass@1 Tokens ThoughtMani3b ThoughtMani7b ThoughtManiPlus ThoughtManiMax 65.6 62.3 64.1 60.9 6842 4461 4209 64.1 61.7 61.9 62. 7009 4485 4408 4181 59.1 41. 45.8 44.5 6557 528 528 55.7 42.4 45.3 41.1 6163 539 577 53.9 38.5 39.8 36. 6670 588 625 523 54.4 35. 41.9 38.8 6535 582 536 when model stops thinking or how to detect whether model is actively engaged in reasoning. Understanding these dynamics could lead to more precise control over the reasoning process, enabling better alignment with desired outcomes and further optimizing computational efficiency."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we propose ThoughtMani, simple yet efficient inference pipeline, and reveal an important characteristic of LRM behaviors. Through extensive experiments, we demonstrate that ThoughtMani can significantly reduce computational costs while keeping the utility. By providing practical solution to improve efficiency without compromising utility or safety, ThoughtMani makes LRMs more accessible for realworld, resource-constrained applications. Implications. Our findings highlight significant implications for LRMs. By understanding the distinct behaviors of RL-trained and distillation-trained LRMs, we can make better-informed deployment decisions. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani offers practical solution to reduce computational costs while maintaining accuracy, making LRMs more efficient and accessible for real-world, resource-constrained scenarios. pact of using task-specific system prompts for CoT generators. In the common setting, we utilize the general prompt described in Section 4, which is designed to generate high-level reasoning steps applicable to wide range of tasks. For the code generation task, we modify the prompt slightly to emphasize the coding scenario (Appendix B), while still adhering to the principle of providing only high-level ideas without delving into implementation details. Comparison of the generated CoTs is shown in Appendix B. The results, shown in Table 4, reveal no significant performance difference between the two settings. This finding highlights the one-for-all property of our approach: the general CoT generation framework is robust enough to handle diverse tasks without requiring taskIn other words, ThoughtMani specific adjustments. demonstrates strong adaptability across domains, eliminating the need for choosing different CoT templates for different types of problems."
        },
        {
            "title": "6 Discussion",
            "content": "Dataset and Models. Despite extensive experiments having been conducted, we mainly cover the performance of ThoughtMani on mathematical and code reasoning tasks, leaving other important aspects such as instruction-following and function-calling untested. Additionally, since we cannot manipulate the inference template of API calling, we only employ ThoughtMani for mainstream local reasoning models. Further studies on different tasks and APIs are encouraged. Manipulation for Other Tasks. In this paper, we propose ThoughtMani primarily for achieving efficient inference. However, the insights gained from our study open up several other meaningful research directions. One direct application is to insert malicious or misleading thoughts into the reasoning process to manipulate model behavior. Another promising line of research is to investigate the inherent mechanisms that govern"
        },
        {
            "title": "References",
            "content": "[1] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. 1, 3 [2] Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning arXiv preprint through dense representations. arXiv:2412.13171, 2024. 2 [3] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 4 [4] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. 1, 2, 3, 5 [5] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. [6] Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to arXiv preprint internalize cot step by step. arXiv:2405.14838, 2024. 2 [7] Mengru Ding, Hanmeng Liu, Zhizhang Fu, Jian Song, Wenbo Xie, and Yue Zhang. Break the chain: Large language models can be shortcut reasoners. arXiv preprint arXiv:2406.06580, 2024. 2, 5 [8] Yichen Gong, Delong Ran, Xinlei He, Tianshuo Cong, Anyu Wang, and Xiaoyun Wang. Safety misalignment against large language models. 1 [9] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 5 [10] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. arXiv Token-budget-aware llm reasoning. preprint arXiv:2412.18547, 2024. 2 [11] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex BakerWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. Gpt-4o system card. CoRR, abs/2410.21276, 2024. 2 [12] Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, and Nouha Dziri. Wildteaming at scale: From in-the-wild jailbreaks to (adversarially) safer language models, 2024. 4 [13] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 5 [14] Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo Han. Deepinception: Hypnotize large language model to be jailbreaker. arXiv preprint arXiv:2311.03191, 2023. 4 [15] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [16] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration, 2024. 5 [17] Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Can language models learn to skip steps? arXiv preprint arXiv:2411.01855, 2024. 2 [18] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023. 4 [19] Yule Liu, Zhen Sun, Xinlei He, and Xinyi Huang. Quantized delta weight is safety keeper. arXiv preprint arXiv:2411.19530, 2024. 1, 3 [20] Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: LengtharXiv compressible chain-of-thought preprint arXiv:2502.09601, 2025. 1, 2, 3, 5 tuning. [21] Maxwell-Jia. AIME 2024 Dataset. https://hu ggingface.co/datasets/Maxwell-Jia/AIME _2024, 2024. 4 [22] OpenAI. Introducing openai o1. https://open ai.com/o1/, 2025. Accessed: 01-April-2025. 1, 2 [23] OpenAI. Openai o3-mini. https://openai.c om/index/openai-o3-mini/, 2025. Accessed: 01-April-2025. 2 [24] Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. Reasoning with large language models, survey. arXiv preprint arXiv:2407.11511, 2024. 1 [25] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 1 [26] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. 2 [27] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: survey on efficient reasonarXiv preprint ing for large language models. arXiv:2503.16419, 2025. 1 [28] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 1 [29] M.-A-P. Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Jun Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling LLM evaluation across 285 graduate disciplines. CoRR, abs/2502.14739, 2025. 2 [30] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. 1, 2, 5 [31] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sid11 dartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 2024. 4 [32] Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. 1, 2, 3, 5 [33] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. 1, 2 [34] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 2, 3, 5 [35] Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. Jailbreak attacks and defenses against large language models: survey. arXiv preprint arXiv:2407.04295, 2024. 1, [36] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. survey of large language models. CoRR, abs/2303.18223, 2023. 2 [37] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023."
        },
        {
            "title": "A Supplement for Thought Manipulation",
            "content": "Prompt for CoT Generation. The prompt for generating the CoT is shown as follows: CoT Generation - Standard If you are teacher, you are listing the important key points for solving the problem and no calculation details should be included. You are not allowed to produce any final answer. Add <STOP> when the key points are finished. You may provide **only very high-level ideas** for solving the problem, no calculation details should be included. Ablation Study for Optimal Manipulation. Additionally, we conduct an ablation study to show the effectiveness of placing CoTs, enclosed by <think> and </think>, at the end of the chat template. Specifically, we design the following two cases and represent them using chat templates, where the dashed line represents the start position for model generation. For the first case, we consider placing the CoTs enclosed by thinking tokens within the template, i.e., before <im_end>. The experiment results are shown in Table 5. Inference Template - Case 1 (within chat) <im_start> User: [Question] <im_end> <im_start> Assistant: <think> [Generated Thought] </think> <im_end> Table 5: Number of occurrences of </think> in the response using different models to generate CoT, which indicates the frequency of rethinking. The dataset is sampled from the original one. Dataset Model CoT Template within chat plus 3b max 7b GSM-8k MATH-500 Distill-qwen-14b Distill-qwen-32b QwQ-32B Distill-qwen-14b Distill-qwen-32b QwQ-32B 86 0 98 164 9 193 88 0 97 160 1 183 85 0 86 172 1 86 0 89 155 0 163 For the second case, we consider directly placing the CoTs enclosed after the template, i.e., after <im_end> but without thinking tokens. The experiment results are shown in Table 6. Inference Template - Case 2 (no thinking token) <im_start> User: [Question] <im_end> <im_start> Assistant: <im_end> [Generated Thought] Table 6: Number of occurrences of </think> in the response using different models to generate CoT, which indicates the frequency of rethinking. The dataset is sampled from the original one. Dataset Model CoT Template no thinking plus 3b max 7b GSM-8k MATH-500 Distill-qwen-14b Distill-qwen-32b QwQ-32B Distill-qwen-14b Distill-qwen-32b QwQ-32B 86 0 97 165 12 87 0 81 161 2 193 91 0 83 172 0 180 81 0 55 163 0 Rethinking Example. We provide an example for rethinking as follows, which is referred to as regenerating the thinking process, although external thoughts have been provided."
        },
        {
            "title": "A Rethinking Example",
            "content": "<im_start> User: tower is made out of 4 blue blocks, twice as many yellow blocks, and an If there are 32 unknown number of red blocks. blocks in the tower in total, how many red blocks are there? <im_end> <im_start> Assistant: <im_end> <think> [External CoT] - Identify the number of blue blocks, which is given as 4. -Determine the number. ... </think> [Inner Thought] Okay, lets see here. The problem is about ... ... **Final Answer**The number of red blocks is boxed{20}. [Final Response] The tower is made up of 4 blue blocks, ...,Thus, the number of red blocks is ( boxed{20}).\", Supplement for Dataset-specific Prompt Prompt for Code Reasoning. The prompt for generating code-specific CoT is shown as follows:"
        },
        {
            "title": "CoT Generation",
            "content": "If you are teacher, you are listing the important key points for writing code. You are not allowed to output any Final Code. Add <STOP> when the key points are finished. Note that you may provide **only very high-level ideas** for solving the problem and writing code. If you feel that you cannot solve it, output <STOP> and return. Difference in Generated CoTs. We provide two examples to demonstrate the difference between the CoTs generated by general and code-specific prompts. The code-specific prompts generally provide more details and reminders for the code-writing task."
        },
        {
            "title": "Example of General CoT",
            "content": "To solve this problem, follow these key points: newline 1. Understand that the goal is to remove elements smaller than until all remaining elements are greater than or equal to k. 2. Count how many elements in the array are less than k. 3. The count of such elements will be the minimum number of operations required. <STOP> Example of Code-specific CoT Initialize counter to zero, which will keep 1. track of the number of operations. 2. Iterate through the array nums and count how many elements are less than k. 3. Each element less than represents one operation needed to remove it. 4. Return the total count as the minimum number of operations required. 5. If all elements are already greater than or equal to k, return 0 operations. <STOP>"
        }
    ],
    "affiliations": [
        "Ant Group",
        "Hong Kong University of Science and Technology (Guangzhou)"
    ]
}