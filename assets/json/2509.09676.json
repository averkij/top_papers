{
    "paper_title": "SpatialVID: A Large-Scale Video Dataset with Spatial Annotations",
    "authors": [
        "Jiahao Wang",
        "Yufeng Yuan",
        "Rujie Zheng",
        "Youtian Lin",
        "Jian Gao",
        "Lin-Zhuo Chen",
        "Yajie Bao",
        "Yi Zhang",
        "Chang Zeng",
        "Yanxi Zhou",
        "Xiaoxiao Long",
        "Hao Zhu",
        "Zhaoxiang Zhang",
        "Xun Cao",
        "Yao Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect \\textbf{SpatialVID}, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 7 6 9 0 . 9 0 5 2 : r SPATIALVID: LARGE-SCALE VIDEO DATASET WITH SPATIAL ANNOTATIONS Jiahao Wang1 Yufeng Yuan1 Rujie Zheng1 Youtian Lin1 Jian Gao1 Lin-Zhuo Chen1 Yajie Bao1 Yi Zhang1 Chang Zeng1 Yanxi Zhou1 Xiaoxiao Long1 Hao Zhu1 Zhaoxiang Zhang2 Xun Cao1 Yao Yao1 1 Nanjing University 2 Institute of Automation, Chinese Academy of Science https://nju-3dv.github.io/projects/SpatialVID Figure 1. We introduce SpatialVID, large-scale video dataset with explicit spatial annotations including camera poses, depth maps, structured captions and serialized motion instructions. The dataset consists of 7,089 hours of real-world dynamic scenes. An exemplar of our SpatialVID is shown on the right."
        },
        {
            "title": "ABSTRACT",
            "content": "Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of largescale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect SpatialVID, dataset consists of large corpus of in-thewild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVIDs data statistics reveals richness and diversity that directly foster improved model generalization and performance, establishing it as key asset for the video and 3D vision research community. Equal contribution. Corresponding author."
        },
        {
            "title": "INTRODUCTION",
            "content": "Perceiving, reasoning about, and interacting with the 3D world are crucial to artificial general intelligence. Recent advances has developed intrinsic 3D models that generate consistent, navigable environments from simple prompts, supporting both reconstruction and future simulation (Ball et al., 2025). These representations naturally align with the dual objectives of recovering the current physical world and imagining plausible future worlds. Recent advances in 3D scene understanding increasingly rely on large-scale, data-driven models that learn efficient representations of geometry and motion. With the rise of neural networks, 3D reconstruction has shifted from optimization-based methods to data-driven approaches. Early work such as Structure-from-Motion (SfM) (Schonberger and Frahm, 2016) and neural Multi-View Stereo (Yao et al., 2018) laid the foundation for geometric scene understanding. Subsequently, large-scale neural models have recently emerged: the LRM series (Hong et al., 2023; Zhang et al., 2024b; Wei et al., 2024) learns to reconstruct high-quality 3D objects from single images or text prompts; LGM (Tang et al., 2024) extends this to multi-view Gaussian-based generation; GSLRM (Wei et al., 2024) leverages neural radiance fields for improved generalization from minimal inputs;DUSt3R series (Wang et al., 2024a; Cabon et al., 2025)have demonstrated robust multi-view matching capabilities; VGGT (Wang et al., 2025) directly predicts key 3D scene attributes such as camera poses and point clouds in feed-forward manner; and LVSM (Jin et al., 2025) enables novel view synthesis through fully data-driven transformer pipeline. Collectively, these models exemplify shift toward scalable, data-driven reconstruction and synthesis with reduced reliance on explicit geometry. Collecting large-scale 3D data remains challenging due to high acquisition costs and the strong dependence on accurate 3D annotation pipelines (Yao et al., 2020; Deitke et al., 2023; Roberts et al., 2021). This bottleneck stands in sharp contrast to the progress of large language models, where abundant and easily accessible text data has proven crucial for scaling model capabilities (Kaplan et al., 2020). In comparison, videos are widely available on the internet and inherently encode rich spatial and geometric cues. Leveraging such data at scale holds significant potential to advance high-fidelity 3D reconstruction and overcome the limitations imposed by scarce 3D ground-truth datasets. While recent advances in video generation have made remarkable progress, current datasets and models still face two critical limitations: (1) weak semantic annotations that fail to capture nuanced scene structure and camera motion, and (2) insufficient spatial metadata, such as camera intrinsics and extrinsics, that ground visual contents to the underlying 3D world. Addressing these gaps is essential for transforming video generation into an effective world-modeling tool. Beyond reconstruction, video generation has emerged as foundational capability for building world models, serving as simulators that represent and predict the physical world. Progress in this area has been impressive: UNet-based diffusion models such as Stable Video Diffusion (SVD) (Blattmann et al., 2023), DiT-based architectures including Sora (OpenAI, 2024), HunyuanVideo (Kong et al., 2024), and CogVideoX (Yang et al., 2024c), as well as emerging autoregressive approaches (Bardes et al., 2024), have enabled the generation of high-fidelity video content. However, converting these models into reliable world simulators requires understanding and controlling the underlying 3D structure. This has motivated controllable video generation, from DragNUWA (Yin et al., 2023) for object-level motion control to CameraCtrl (He et al., 2024) and MotionCtrl (Wang et al., 2024b) for explicit camera trajectory guidance. Beyond camera motion, recent work incorporates point clouds and other 3D signals into video generation pipelines to enhance spatial awareness (Yu et al., 2024). These capabilities represent crucial steps toward physically grounded video simulation and align with recent world models such as Cosmos Predictor (Agarwal et al., 2025), HunyuanWorld 1.0 (Team et al., 2025), and Genie3 (Ball et al., 2025). clear gap exists in current data, which is divided into two distinct categories. This fundamental division hinders the development of spatiotemporally coherent world simulators, underscoring an urgent need for dataset that can bridge scene reconstruction and world simulation. On one hand, massive video datasets offer scale and semantic diversity but lack explicit 3D information. While efforts like MotionSight (Du et al., 2025) infer spatial cues from 2D videos, they lack direct geometric ground truth, forcing models to learn spatial relationships implicitly from pixels, often leading to physically inconsistent outputs. As result, models trained on video datasets must learn spatial relationships implicitly from 2D pixel patterns, widely recognized difficult task that often leads to outputs with physically inconsistent geometry and dynamic behaviors. On the other hand, spatial 2 Figure 2. The data flow of video filtering. Raw videos are first pre-filtered to exclude content with quality defects, incorrect dimensions, or irrelevant titles. The remaining videos are segmented into clips, which are then ranked via hierarchical scoring strategy integrating aesthetics metrics, luminance, OCR, and motion values. High-scoring clips undergo dual annotation pipeline to capture both spatial structure and semantic information, yielding the final SpatialVID dataset. This pipeline is also employed to curate high-quality subset (SpatialVID-hq) with more balanced category distribution. datasets like CO3D (Reizenstein et al., 2021) and realestate10k (Zhou et al., 2018) provide precise camera parameters and geometric ground truth but are limited in scale, diversity, and dynamic richness. Many of these are object-centric (Reizenstein et al., 2021) or based on synthetic data like Tartanair (Wang et al., 2020), failing to capture the complexity of real-world scenes. This separation between datasets rich in semantics (without geometry) and those with geometric fidelity (without semantics) stifles the development of unified models for spatial intelligence. To bridge the gap between dynamic videos and spatial understanding, we introduce SpatialVID  (Fig. 1)  , large-scale multimodal dataset that connects raw pixels with the physical world. Our curation pipeline begins with over 21,000 hours of raw internet video, manually screened to ensure diversity and motion richness. hierarchical filtering process  (Fig. 2)  distills the raw videos into 7,089-hour core dataset of high-quality 720P clips with high-quality camera motion. From this core, we construct SpatialVID-HQ, 1,146-hour balanced subset optimized for robust model training and evaluation. To our knowledge, SpatialVID is the largest dataset of dynamic videos with explicit geometric annotations and makes three primary contributions: Manually Screened Videos with Camera Motions: SpatialVID is built from massive internet video collection of more than 21,000 hours, which are manually selected for rich scene motion. This motion-first curation, followed by our processing pipeline, yields diverse set of high-quality clips well-suited for training spatially aware models. Comprehensive Geometric Annotations: For each clip in the SpatialVID dataaset, we provide camera poses and depth maps generated via an adjusted pipeline (Li et al., 2024). These annotations supply explicit 3D grounding and motion dynamics, filling key gap in existing video datasets. Spatially-Aware Captions and Motion Instructions: We generate structured captions that integrate scene descriptions, camera motion details, and hierarchical semantic attributes such as weather, lighting, and time of day, providing rich text-video alignment. In addition, motion instructions are derived from camera trajectories, offering precise supervision for navigation-related model training."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Scene Reconstruction Early methods in scene reconstruction have evolved primarily along two main trajectories: Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM). SfM methods, such as the widely used COLMAP (Schonberger and Frahm, 2016), excel at generating 3 highly accurate reconstructions from unordered images through global optimization. In contrast, SLAM systems (Mur-Artal et al., 2015) focus on real-time, incremental map building and camera tracking. Despite their foundational success, these traditional approaches rely on handcrafted features, limiting their robustness in challenging scenarios, such as texture-less regions, thus motivating the shift toward learnable methods. To address the limitations of heuristic techniques, the field has moved toward learnable Multi-View Stereo (MVS). Early works like MVSNet (Yao et al., 2018) significantly improved reconstruction density by using deep cost volumes, though they typically assume known camera information and map multi-view images to depth maps. Recent advancements have introduced powerful feedforward models, with Transformer-based architectures marking significant milestone. Models such as DUSt3R (Wang et al., 2024a) and MASt3R (Leroy et al., 2024) have demonstrated robust multi-view matching capabilities. Further developments, including MUSt3R (Cabon et al., 2025), Align3R (Lu et al., 2025), and Fast3R (Yang et al., 2025b), extend DUSt3R to more complex and generalized scenarios. More recently, holistic, end-to-end frameworks like VGGT (Wang et al., 2025) have emerged, directly inferring diverse 3D structures from images, signaling clear trend toward comprehensive, data-driven pipelines that move beyond simple depth or point cloud estimation. Reconstructing dynamic scenes remains formidable challenge. Building on the success of foundational models, several approaches have been proposed to handle moving objects in dynamic environments. For instance, models like Easi3R (Chen et al., 2025), MonST3R (Zhang et al., 2024a), and CUT3R (Wang* et al., 2025) extend the capabilities of DUSt3R to incorporate dynamic elements. In parallel, dense dynamic reconstruction methods have explored various strategies. CasuaL-SAM (Zhang et al., 2022) integrates optical flow for motion handling, while others, like MegaSaM (Li et al., 2024), leverage robust SLAM backbones such as DROID-SLAM (Teed and Deng, 2021) for dense tracking and mapping in dynamic environments. Given its demonstrated robustness in unconstrained, in-the-wild videos, we utilize an enhanced version of MegaSaM in our work to generate the initial geometric annotations for our dataset. World Simulator World simulator, fundamental aspect of spatial intelligence, involves simulating, perceiving, and interactively understanding dynamic environments. In this context, video generation technologies have become central. The field has rapidly evolved, beginning with breakthroughs in UNet-based video diffusion models, such as Stable Video Diffusion (SVD) (Blattmann et al., 2023). More recently, DiT-based architectures have made significant strides, with models like Sora (OpenAI, 2024), HunyuanVideo (Kong et al., 2024), and CogVideoX (Yang et al., 2024c) pushing the boundaries of fidelity and scalability in both model capacity and video duration. In parallel, autoregressive approaches (Bardes et al., 2024) remain prominent for generating high-quality video content. For these generative models to serve as effective world simulators, precise control over the underlying 3D structure is essential. Recent advances, such as DragNUWA (Yin et al., 2023), enable fine-grained manipulation of object movements within synthesized scenes, while CameraCtrl (He et al., 2024) and Table 1. Comparisons with previous datasets with spatial information. SpatialVID is million-level, dynamic and open-scenario high-quality video dataset with rich annotated geometric and semantic information. In Geometry Info. column, C. denotes camera, D. denotes depth or points cloud. Dataset Domain Real/Syn. Dyn./Sta. Geometry Info. Caption # Video Clips # Frames BlendedMVS (Yao et al., 2020) Multi-Cam Video (Bai et al., 2025) PointOdyssey (Zheng et al., 2023) Camerabench (Lin et al., 2025) ScanNet (Dai et al., 2017) MVImgNet (Yu et al., 2023) CO3Dv2 (Reizenstein et al., 2021) DL3DV (Ling et al., 2024) WebVi3D (Ma et al., 2025) Dynpose100k (Rockwell et al., 2025) CamVid-30K (Zhao et al., 2024) Stereo4d (Jin et al., 2024) RealEstate10K (Zhou et al., 2018) Waymo (Sun et al., 2020) Map-free (Arnold et al., 2022) Princeton365 (Kayan et al., 2025) SpatialVID SpatialVID-HQ Open Open Walk Open Indoor Object-Centric Object-Centric Open Open Open Open Fisheye Indoor Drive Object-Centric open Open Open Syn. Syn. Syn. Real&Syn. Real Real Real Real Real Real Real Real Real Real Real Real Real Real Sta. Dyn. Dyn. Dyn. Sta. Sta. Sta. Sta. Sta. Dyn. Dyn. Dyn. Sta. Dyn. Sta. Dyn. Dyn. Dyn. 4 C. D. C. C. D. N/A C. D. C. C. N/A N/A C. C. C. D. C. C. C. C. D. C. D. C. D. Label Label N/A Label, Short N/A Label Label Label - Short N/A Short N/A N/A N/A N/A 113 (Scenes) 136K 159 3,381 1500 (Scenes) 219,188 19K 10,510 15.99M 100,131 30K 110K 80K 1150 655 (Scenes) 365 Structured Caption Structured Caption 2.71M 0.37M 17K 11.02M 200K - 2.50M 6.50M 1.50M 512M 320M 6.81M - 10M 10M 230K 560K 1.19M 127.60M 20.63M MotionCtrl (Wang et al., 2024b) offer explicit guidance over camera trajectories, crucial for simulating dynamic exploration perspectives. Incorporating 3D data into video generation brings us closer to comprehensive world models. Approaches like ViewCrafter (Yu et al., 2024) integrate point clouds, enhancing spatial awareness and geometric modeling for more coherent video generation. These controlled video generation methods directly contribute to the development of world models such as Cosmos Predictor (Agarwal et al., 2025), HunyuanWorld 1.0 (Team et al., 2025), and Genie3 (Ball et al., 2025), which focus on predicting spatiotemporal dynamics and enabling interactive exploration in complex environments. This integration of 3D geometry and temporal dynamics is key to advancing world models from generating isolated video sequences to enabling interactive simulations of virtual environments Datasets with Spatial Annotations High-quality spatial datasets are essential for advancing world reconstruction and spatial exploration. Ideally, they integrate 3D geometry, dynamic information, and rich semantic context, overcoming the limitations of single-modal datasets. Current efforts to construct such datasets can be broadly categorized into three approaches. The first is synthetic data, exemplified by datasets like Multi-Cam Video (Bai et al., 2025), virtualKITTI (Cabon et al., 2020), and BlendedMVS (Yao et al., 2020), which offer ground-truth camera poses and geometric data. However, their creation requires substantial engineering effort, limiting their scale. The second approach involves real-world datasets that rely on semi-automatic annotation, such as CO3DV2 (Reizenstein et al., 2021) and RealEstate10K (Zhou et al., 2018), which utilize SfM or SLAM methods for efficiency. While effective for static scenes, this method faces limitations, including sparse camera trajectories, reduced robustness in dynamic environments, and computational overhead that restricts scalability. Finally, the third approach leverages large video corpora to mine semantic information. Datasets like CamVid-30K (Zhao et al., 2024) and DynPose100K (Rockwell et al., 2025) refine video datasets (e.g., Panda70M (Chen et al., 2024), OpenVid (Nan et al., 2024)) to extract camera motion data. However, these datasets often lack rich semantic details and dynamic content. Recent efforts aim to integrate both spatial geometry and semantics to enhance multimodal spatial reasoning. For instance, CameraBench (Lin et al., 2025) and VLM4D (Zhou et al., 2025) focus on this integration, and the recent Sekai (Li et al., 2025c) dataset, which is concurrent work to ours, provides 600 hours of diverse videos with camera annotations to bridge this gap. However, still limited by the geometric annotation, motion instruction, or the scale of the dataset. As illustrated in Table 1, SpatialVID collects real-world dynamic scenes with rich geometric and semantic annotations, surpassing current datasets in both scale and diversity."
        },
        {
            "title": "3 SPATIALVID CURATION",
            "content": "As illustrated in Fig. 3, the SpatialVID data processing pipeline is organized into three core stages: filtering, annotation, and sampling, each of which is elaborated in the following subsections. Sec. 3.1 describes the video collection procedure, the unification of encoding formats, and the initial segmentation of clips using scenedetect. Sec. 3.2 introduces the multi-dimensional filtering strategy based on aesthetic quality, motion intensity, OCR-based text interference, and luminance, with the goal of retaining clips that contain diverse and meaningful motion. Sec. 3.3 explains the annotation of geometric camera motion, including the generation of camera poses and depth maps that serve as spatial priors for each clip. Sec. 3.4 details the creation of motion instructions, and Sec. 3.5 describes the annotation of structured captions, which integrate scene descriptions, camera motion details, and other semantic labels such as weather, lighting, and time of day. Representative examples of motion instruction annotations are shown in Fig. 7, while additional results illustrating geometric and semantic annotations are also presented in Fig. 14. 3.1 DATA COLLECTION AND PREPROCESSING 3.1.1 SOURCE DATA While several efforts have focused on filtering existing high-quality video datasets, such as Panda70M (Chen et al., 2024) and MiraData (Ju et al., 2024), our analysis indicates that these resources remain insufficient for our purposes. Their limitations stem primarily from restricted camera perspectives and lack of diverse motion types. For instance, when we processed the validation split of Panda70M through our pipeline, only about 10% of the clips met our quality criteria, yielding 5 Figure 3. Overview of the curation pipeline. The curation pipeline consists of three stages: filtering, annotation, and sampling. It begins with raw web videos manually collected for camera motion, yielding high-quality dynamic clips with geometric and semantic annotations. In the filtering stage, raw videos are preprocessed and filtered hierarchically, producing 3.4M clips. The annotation stage performs geometric and semantic labeling while deriving motion instructions from camera poses, resulting in base dataset of 2.7M clips. Finally, in the sampling stage, clips are balanced by motion characteristics and categories to generate high-quality (HQ) subset with evenly distributed classes, ensuring comprehensive coverage for downstream tasks. too little data to scale up our dataset effectively. detailed quantitative comparison is provided in Sec. 4.3. To overcome these constraints, we turned to YouTube, platform that offers vast and heterogeneous collection of high-resolution videos. In the initial collection phase, we queried the platform using motion-relevant keywords such as walk, tour, and drone, which are indicative of the smooth and varied camera trajectories needed for 3D reconstruction. Each candidate video was subsequently subjected to rigorous manual screening process to ensure its suitability for robust camera pose estimation and reconstruction within the MegaSaM pipeline. Several exclusion criteria were enforced to preserve data quality. Videos dominated by dynamic foreground objects, such as pedestrians or vehicles, were excluded to avoid unstable feature tracking. Footage from static viewpoints or containing only zoom transformations was discarded to guarantee sufficient parallax for reliable 3D geometry inference. Videos with substantial motion blur, poor illumination, or strong wide-angle distortion were removed, as these factors significantly degrade feature detection and matching. Finally, clips with severe occlusions or intrusive graphical overlays, including logos and subtitles, were rejected to prevent reconstruction artifacts. Through this meticulous curation procedure, we obtained dataset with stable camera motion, rich parallax, and high-quality textures, making it well suited for the demanding requirements of 3D reconstruction. In total, we collected 33,443 videos from YouTube, amounting to 21,789.07 hours of raw footage. The resulting dataset exhibits broad distribution across motion types, camera trajectories, and scene categories, as illustrated in Fig. 4. 3.1.2 DATA PREPROCESSING The collected long-form videos are segmented into clips of 3 to 15 seconds using the PySceneDetect library (Castellano). key challenge is handling aesthetic transitions, such as fades, which introduce subtle inter-frame differences that standard detection algorithms often miss. To overcome this, we modify the librarys sensitivity thresholds and replaced the default adjacent-frame analysis with an interval-based, multi-frame comparison approach. These modifications improves both segmentation accuracy and processing speed. The raw video collection exhibits substantial variability in technical specifications, including encoding formats and resolutions. To establish consistent baseline for our pipeline, we standardize the dataset by transcoding all segmented clips into uniform H.265-encoded MP4 format at resolution of 19201080. This normalization step ensures both technical consistency and compatibility across all subsequent processing stages. After preprocessing, we obtain more than 7 million video clips. 6 Figure 4. Statistics of pre-filtered videos (aka. the Clips in the Fig. 2). The left panel shows the quantity distribution of raw videos, while the right panel presents the duration distribution. These charts illustrate the variety of shooting contexts, including indoor (house tour) and outdoor (walking, train, drone, etc.) scenarios, demonstrating broad coverage of different shooting carriers and environments. (a) Aesthetics Filtering (b) Luminance Filtering (c) OCR Filtering (d) Motion Filtering Figure 5. Video filtering strategies. Videos are filtered based on various quality criteria (Aesthetics, Luminance, OCR and Motion). The number in the bottom-right corner of each clip represents its score for the corresponding quality filter. Clips with green boxes are retained, while those with red boxes are discarded due to scores below the threshold."
        },
        {
            "title": "3.2 VIDEO QUALITY FILTERING",
            "content": "We filter videos using key metrics, including aesthetic quality, motion intensity, optical character recognition (OCR) result, and luminous intensity, as illustrated in Fig. 5. This pre-filtering step ensures the retention of clips with richer motion information, improving the success rate of camera pose estimation. Given the large volume of videos in our dataset, it is crucial to filter out low-quality samples to ensure their suitability for training and evaluation. The following contents describe the specific evaluation criteria used. Aesthetic Filtering. To quantitatively assess visual appeal, we use CLIP+MLP aesthetic score predictor (Schuhmann). The model assigns score from 0 to 10, with higher values indicating better quality. For each video clip, the score is averaged across the first, middle, and last frames. Clips with an average score below 4.0 are considered insufficiently appealing and discarded (Fig. 5a). Luminance Filtering. Luminance is calculated for the first, middle, and last frames using the standard formula = 0.2126R + 0.7152G + 0.0722B, where R, G, and are the respective channel values. Clips with average luminance outside the range [20, 140], either too dark or too bright, are excluded, ensuring that only videos with proper exposure are retained (Fig. 5b). Optical Character Recognition (OCR). For text detection, we use the latest release of PaddleOCR (Authors, 2020), which offers high accuracy and robust multilingual support. We processe the first, middle, and last frames of each clip to detect text regions, computing the ratio of text area to frame size. Clips where the text area exceeded 30% are removed, as these are considered informational rather than visual (Fig. 5c). Motion Filtering. We use lightweight VMAF (Zhi et al., 2016), integrated FFmpeg with the valid motion score ranging from 2.0 to 14.0 (Fig. 5d). 3.3 GEOMETRY INFORMATION ANNOTATION We empirically select our camera estimator. As shown in Fig. 6, MegaSaM outperforms DROIDSLAM (Teed and Deng, 2021), COLMAP (Schönberger and Frahm, 2016), and Fast3R (Yang et al., 2025b) in terms of accuracy. Besides, compared to MonST3R (Zhang et al., 2024a), MegaSaM requires significantly less time to achieve similar accuracy. VGGT (Wang et al., 2025), while excelling in inference speed, struggles in scenarios with limited number of feature points, as demonstrated by the last two examples in the figure. Considering both time consumption and prediction accuracy, weve chosen MegaSaM as our default method. However, our annotation pipeline is poised to improve with the integration of future camera estimators that offer enhanced performance (e.g., ViPE (Huang et al., 2025)). Specificly, MegaSaM (Li et al., 2024) is video camera estimation system for precise, rapid, and robust estimation of camera parameters and depth maps from monocular videos of dynamic scenes. It builds on previous deep visual SLAM systems, such as DROID-SLAM (Teed and Deng, 2021), by incorporating key enhancements. Notably, MegaSaM improves initialization by utilizing monocular depth priors from advanced models like Depth Anything (Yang et al., 2024a) and UniDepth (Piccinelli et al., 2024). Additionally, it incorporates motion probability maps and an uncertainty-aware global bundle adjustment (BA) mechanism. These innovations allow MegaSaM to effectively handle dynamic content and unconstrained camera motion. Empirical results from both synthetic and realworld benchmarks demonstrate that MegaSaM outperforms its predecessors in terms of accuracy, robustness, and computational efficiency. Despite its advanced capabilities, MegaSaM has certain limitations. It struggles in extreme cases, such as when moving objects dominate the field of view or when camera and object motions are collinear. Additionally, it is not designed to handle variable focal lengths or significant radial distortion. Its performance is also constrained by its reliance on external monocular depth models, which may be insufficient in challenging scenarios. To address these limitations, we upgrade the depth estimation component of our pipeline by replacing the original depth model with the advanced UniDepth v2 (Piccinelli et al., 2025) and Depth Anything v2 (Yang et al., 2024b) models. This enhancement improves both the accuracy and robustness of the depth estimation, resulting in more reliable pose annotations. 8 Figure 6. Comparison of MegaSaM with other SLAM/3D reconstruction methods. We visualize the trajectories predicted by six representative methods. The color order ROYGBV corresponds to the progression from the initial to the final time step. MegaSaM achieves great balance between accuracy and speed, making it the preferred choice for our dataset annotation. To validate the physical plausibility of camera trajectories reconstructed by MegaSaM, we introduce method based on the concept of acceleration. This approach detects abrupt, non-physical fluctuations in acceleration, which signal tracking errors. Additionally, we propose three novel evaluation metrics to quantitatively assess camera motion: Move Distance (MoveDist). This metric quantifies the total path length of the cameras trajectory, calculated by summing the Euclidean distances between consecutive camera positions. Rotation angle (RotAngle). This metric measures the cumulative rotational change in the cameras viewing direction, computed as the sum of absolute angular differences in orientation between consecutive frames. Trajectory turns (TrajTurns). This metric estimates the number of significant turns in the cameras trajectory, serving as an indicator of trajectory complexity. It is computed by first establishing reference line between the start and end points of the trajectory. For each intermediate point, the angle relative to the reference line is calculated, and the total number of local extrema in the resulting sequence of angles corresponds to the number of turns. We observe that the systems motion probability maps lack sufficient precision for effective dynamic object masking, resulting in inaccurate segmentation. To address this limitation, we integrate the SAM2 model (Ravi et al., 2024) into our pipeline to refine motion cues and achieve more robust segmentation of moving objects. An adaptive thresholding mechanism, calibrated to the motion probability distribution, is first applied to generate initial masks. Contour detection is then performed to reduce redundant segmentation in overlapping regions; for each contour, four evenly spaced anchor points are sampled along the perimeter and used as prompts for the SAM2 model. Leveraging the resulting enhanced segmentation, we introduce the dynamic ratio metric, which quantifies the proportion of each frame occupied by dynamic regions and serves as key criterion for filtering and selecting clips based on their motion content. 3.4 MOTION INSTRUCTION DECOMPOSITION Given that motion instructions are essential for training models such as Hunyuan-GameCraft (Li et al., 2025b), our dataset explicitly incorporates them to facilitate controllable and semantically meaningful motion learning. To generate these motion instructions, we begin by processing the output sequences of camera poses estimated from each clip. The relative translations and rotations between consecutive frames serve as the basis for instruction derivation, as they directly encode camera motion dynamics. To ensure reliability, we apply temporal smoothing filters to the raw camera 9 Figure 7. Examples of motion instructions. Keyboard-style icons denote camera motion. The cluster on the left corresponds to translations: and indicate forward and backward movement, and indicate left and right movement, and , indicate vertical movement. The cluster on the right corresponds to rotations: arrows represent pitch (, ) and yaw (<, >), while circular symbols represent roll (, ). poses, effectively suppressing jitter and measurement noise that could otherwise result in spurious motion labels. Next, we employ thresholding mechanism to identify segments with perceptible motion: only when the magnitude of relative pose change between adjacent frames surpasses predefined thresholds do we generate an instruction. This step prevents the generation of trivial or redundant instructions in near-static scenarios. Finally, to maximize clarity and consistency, we map these motion signals to controlled vocabulary of cinematographic terms (Deguzman), such as dolly in (forward translation), pan left (horizontal rotation to the left), and truck right (lateral translation to the right), as exemplified in Fig. 7. This decomposition not only standardizes the representation of camera motion but also makes it interpretable and readily usable for downstream model training, where high-level motion semantics are crucial. 3.5 SEMANTIC INFORMATION ANNOTATION Multimodal models have made substantial progress in bridging vision and language, adopting diverse strategies for video caption generation. VATEX (Wang et al., 2019) relies on manual annotation to deliver precise captions but suffers from limited scalability. More recent datasets, such as VidGen (Tan et al., 2024), Panda70M (Chen et al., 2024), and OpenVid (Nan et al., 2024), leverage multimodal large language models (MLLMs) for automated caption generation, achieving balance between efficiency and semantic relevance. Complex pipelines like MiraData (Ju et al., 2024) and Koala36M (Wang et al., 2025) produce structured captions to improve alignment between visual content and text, while OpenHumanVid (Li et al., 2025a) introduces voting mechanisms and LLM-based reformatting to further enhance accuracy across multiple structured outputs. Despite this progress, vision-language models (VLLMs) such as Gemini (Team et al., 2023) remain limited in capturing spatial information, often omitting or misrepresenting geometric details, as illustrated by the original camera description in Fig. 9. Recent studies, including CameraBench (Lin et al., 2025) and VLM4D (Zhou et al., 2025), emphasize the importance of spatial reasoning and present early efforts to address this challenge. Motivated by these observations, we propose structured caption generation framework that integrates VLLMs with large language models (LLMs) to strengthen spatiotemporal reasoning and improve the spatial consistency of captions within our dataset. Several recent approaches have explored encoding positional and geometric information into LLMs to enhance spatial comprehension. For instance, 3D LLM-Mem (Hu et al., 2025) incorporates depth maps and camera parameters, significantly improving spatial understanding. Inspired by these efforts, we introduce novel captioning pipeline that integrates camera pose information into structured caption generation, as illustrated in Fig. 8. Our pipeline consists of two stages: visual parsing and language refinement. In the first stage, Gemini-2.0-flash processes frames sampled at 1 fps to produce an initial camera motion description and detailed scene description. These outputs, combined with accurate camera poses, are passed 10 Figure 8. Structured caption generation. The pipeline consists of vision description and spatial enhancement. Firstly,VLM generates initial camera motion and scene descriptions using its visual capabilities.Secondly,leveraging camera poses as geometric priors, the LLM refines and expands these descriptions into structured captions. Figure 9. Effect of spatial enhancement. In this example, the camera is actually panning to the left. The VLM initially misidentified the motion as right during the vision description stage. After applying spatial enhancement, which integrates camera motion as geometric prior, the LLM correctly inferred the direction as left. Figure 10. Distribution of camera motion directions. The donut charts show the distribution of camera motion directions for the SpatialVID (left) and HQ SpatialVID (right) datasets. The original SpatialVID dataset exhibits wide range of motion patterns. In contrast, the HQ SpatialVID dataset features more balanced distribution, addressing the overrepresentation of any single motion direction. to Qwen3-30B-A3B (Yang et al., 2025a), which refines the camera description for greater accuracy, summarizes the scene into concise abstract, and composes an immersive shot-level narrative that jointly characterizes scene content and camera motion. The effect of this spatial enhancement, which corrects errors in motion direction by incorporating camera pose as geometric prior, is illustrated in Fig. 9. Beyond textual refinement, Qwen also produces structured semantic annotations specifying scene type, lighting condition, weather, crowd density, and time of day, as well as Motion Trends tags that capture camera dynamics such as pans, dollies, rotations, or steady movements (the distribution of Motion Trends for SpatialVID and its high-quality subset SpatialVID-HQ is shown in Fig. 10). This pipeline yields multi-level textual representation that accurately reflects the spatial, temporal, and motion characteristics of each video clip. To further enrich the semantic layer, we apply an additional LLM pass to extract multi-dimensional attributes, including weather, time of day, crowd density, brightness, and scene type. The scene type attribute is hierarchically organized to enable fine-grained classification. The distribution of scene tags across the dataset is depicted in Fig. 12a, where sector widths correspond to category prevalence."
        },
        {
            "title": "4 DATASET ANALYSIS",
            "content": "4.1 DATA SAMPLING Our primary objective is to curate clips that are both high quality and diverse. To achieve this, we adopt two-step strategy. First, we raise the thresholds of key video quality metrics to improve the 11 (a) Motion caption length distribution (b) Scene caption length distribution Figure 11. Statistical analysis of the caption data. Fig. (a) and Fig. (b) show the length distributions for motion and scene captions, respectively, comparing the original captions to our enhanced versions. significant increase in caption length is evident for both types after enhancement. (a) Scene tags distribution (b) World cloud Figure 12. (a) Distribution of scene tags. The sunburst chart shows the distribution of categorical tags across five primary attributes: weather, time of day, crowd density, lighting, and scene type. The scene type attribute is hierarchical, with sub-categories for more detailed classification. The width of each sector reflects the prevalence of the corresponding tag in the dataset. (b) Word cloud. word cloud shaped into the SpatialVID logo, generated from the enhanced captions. The size of each word corresponds to its frequency in the corpus. Key terms such as motion, forward, left, and right emphasize the datasets focus on describing camera movement and spatial dynamics. overall standard of the dataset. Second, we balance the distribution of semantic tags and camera trajectory statistics (e.g., Arc Nums) to preserve diversity across content types. This refined sampling procedure yields over one thousand hours of high-quality clips, forming our curated spatial video dataset, SpatialVID-HQ. 4.2 SEMANTIC ANALYSIS Our semantic analysis confirms that SpatialVID delivers versatile, multi-level caption suite tailored for diverse research needs. To enhance camera motion descriptions, we distilled the original outputs into the compact OptCamMotion format, isolating pure kinematic instructions to provide clean signal for motion control tasks. This refinement reduces the mean caption length from 62.5 to 50.3 (Fig. 11a), resulting in more precise and machine-friendly motion supervision. Scene descriptions are organized to balance brevity and richness through dual-level strategy. The SceneSummary offers succinct overview with mean length of 28.6 (Fig. 11b), supporting applications that require high-level semantic context. In contrast, the ShotImmersion caption integrates scene semantics with camera dynamics, forming comprehensive narrative with mean length of 89.7 that facilitates deeper contextual reasoning. Qualitative analysis further validates the 12 (a) Aesthetics Distribution (b) Luminance Distribution (c) Motion Distribution (d) RotAngle Distribution (e) MoveDist Distribution (f) TrajTurns Distribution Figure 13. Dataset quality comparison. comparative analysis of our SpatialVID, its high-quality subset (SpatialVID-HQ), and the Panda70M-test set processed with our identical pipeline, visualized via histograms and Kernel Density Estimation (KDE) curves. KDE curves illustrate the continuous distribution patterns of each dataset, facilitating intuitive comparison of their value distribution shapes. The results demonstrate that SpatialVID-HQ exhibits markedly superior quality across all metrics, validating the effectiveness of our manual collection, filtering and sampling methodology. datasets breadth and spatial emphasis: the sunburst chart in Fig. 12a demonstrates broad coverage of attributes such as weather, time of day, and environment type, while the word cloud in Fig. 12b highlights motion-centric terms including motion, forward, left, and glides, underscoring the datasets strong focus on spatial dynamics across all caption formats. 4.3 COMPARISON WITH PANDA-70M Panda-70M is large-scale video dataset designed to support research on vision-language and video understanding. However, it suffers from several quality issues since many videos are static, flicker-prone, or low-resolution, and captions tend to be short and underspecified. To highlight the advantages of our dataset, we conduct systematic comparison between SpatialVID and Panda-70M, as illustrated in Fig. 13. In terms of video-quality metrics including Aesthetics (Fig. 13a), Luminance (Fig. 13b), and Motion (Fig. 13c), both SpatialVID and its high-quality subset SpatialVID-HQ show more compact distributions, indicating greater consistency and higher average quality. Regarding camera motion statistics, Panda-70M is dominated by static videos, as seen in the distributions of camera rotation (Fig. 13d) and translation distance (Fig. 13e). In contrast, SpatialVID achieves balanced and realistic distribution of camera movements. Finally, Fig. 13f shows the distribution of trajectory turns (Arc Nums): over 80% of Panda-70M videos cannot be reconstructed by MegaSaM due to insufficient motion, whereas SpatialVID-HQ deliberately increases the proportion of clips with curved or turning trajectories, providing richer and more diverse motion profile."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduce SpatialVID, large-scale video dataset consisting of diverse real-world scenes accompanied with both semantically and geometrically rich annotations. We propose procedural data curation pipeline to gain 2.7 million video clips meticulously annotated with perframe hierarchical camera motions and depth, based on collection of 21k hours of in-the-wild videos. Thanks to this dense geometric prior, we augment each vanilla scene description with more accurate and structured motion-centric observations, surpassing generic VLM-based captioning 13 Figure 14. Sample videos of SpatialVID. SpatialVID collects diverse set of dynamic videos with geometry and structured semantic information. strategies in terms of 3D awareness. These diverse 3D-aware labels enable SpatialVID to convey hierarchical knowledge towards the spatiotemporal properties of the collected real-world data, which is to various extents ignored by current datasets in both fields of 3D reconstruction and video generation. By introducing stronger 3D inductive biases, as inherent in human assumptions towards our physical world, we aim to advance future studies in physically-grounded camera-controlled video generation, dynamic scene synthesis and embodied agents. Furthermore, by unifying explicit 3D motion controls with textual semantics at scale, our SpatialVID lays foundation for future works capable of simulating complex interactions within real-world scenario."
        },
        {
            "title": "REFERENCES",
            "content": "Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. 2, 5 Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo Garcia-Hernando, Áron Monszpart, Victor Adrian Prisacariu, Daniyar Turmukhambetov, and Eric Brachmann. Map-free visual relocalization: Metric pose relative to single image. In ECCV, 2022. 4 PaddlePaddle Authors. Paddleocr, awesome multilingual ocr toolkits based on paddlepaddle. https: //github.com/PaddlePaddle/PaddleOCR, 2020. 8 Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. 4, 5 Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, and et al. Genie 3: new frontier for world models. 2025. URL https://deepmind.google/discover/blog/ genie-3-a-new-frontier-for-world-models/. 2, Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471, 2024. 2, 4 Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 4 Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 5 Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela Csurka, Boris Chidlovskii, Jerome Revaud, and Vincent Leroy. Must3r: Multi-view network for stereo 3d reconstruction. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 10501060, 2025. 2, Brandon Castellano. PySceneDetect. 6 Pyscenedetect. https://github.com/Breakthrough/ Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479, 2024. 5, 10 Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. arXiv preprint arXiv:2503.24391, 2025. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. 4 Kyle Deguzman. Types of Definitive different-types-of-camera-movements-in-film/. guide. camera movements film explained: https://www.studiobinder.com/blog/ in Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 2 Yipeng Du, Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Xiang Li, Jian Yang, Zhenheng Yang, and Ying Tai. Motionsight: Boosting fine-grained motion understanding in multimodal llms. arXiv preprint arXiv:2506.01674, 2025. 2 Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 2, 4 Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 2 Wenbo Hu, Yining Hong, Yanjun Wang, Leison Gao, Zibu Wei, Xingcheng Yao, Nanyun Peng, Yonatan Bitton, Idan Szpektor, and Kai-Wei Chang. 3dllm-mem: Long-term spatial-temporal memory for embodied 3d large language model. arXiv preprint arXiv:2505.22657, 2025. 10 Jiahui Huang, Qunjie Zhou, Hesam Rabeti, Aleksandr Korovko, Huan Ling, Xuanchi Ren, Tianchang Shen, Jun Gao, Dmitry Slepichev, Chen-Hsuan Lin, et al. Vipe: Video pose engine for 3d geometric perception. arXiv preprint arXiv:2508.10934, 2025. 8 Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. Lvsm: large view synthesis model with minimal 3d inductive bias. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=QQBPWtvtcn. 2 Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, and Aleksander Holynski. Stereo4d: Learning how things move in 3d from internet stereo videos. arXiv preprint arXiv:2412.09621, 2024. Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions, 2024. URL https://arxiv.org/abs/2407.06358. 5, 10 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 2 Karhan Kayan, Stamatis Alexandropoulos, Rishabh Jain, Yiming Zuo, Erich Liang, and Jia Deng. Princeton365: diverse dataset with accurate camera pose, 2025. URL https://arxiv.org/ abs/2506.09035. 4 Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 4 Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, et al. Openhumanvid: large-scale high-quality dataset for enhancing human-centric video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77527762, 2025a. 10 Jiaqi Li, Junshu Tang, Zhiyong Xu, Longhuang Wu, Yuan Zhou, Shuai Shao, Tianbao Yu, Zhiguo Cao, and Qinglin Lu. Hunyuan-gamecraft: High-dynamic interactive game video generation with hybrid history condition, 2025b. URL https://arxiv.org/abs/2506.17201. 9 Zhen Li, Chuanhao Li, Xiaofeng Mao, Shaoheng Lin, Ming Li, Shitian Zhao, Zhaopan Xu, Xinyue Li, Yukang Feng, Jianwen Sun, et al. Sekai: video dataset towards world exploration. arXiv preprint arXiv:2506.15675, 2025c. 5 Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In arxiv, 2024. 3, 4, 8 Zhiqiu Lin, Siyuan Cen, Daniel Jiang, Jay Karhade, Hewei Wang, Chancharik Mitra, Tiffany Ling, Yuhan Huang, Sifan Liu, Mingyu Chen, et al. Towards understanding camera motions in any video. arXiv preprint arXiv:2504.15376, 2025. 4, 5, 10 Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 4 Jiahao Lu, Tianyu Huang, Peng Li, Zhiyang Dou, Cheng Lin, Zhiming Cui, Zhen Dong, Sai-Kit Yeung, Wenping Wang, and Yuan Liu. Align3r: Aligned monocular depth estimation for dynamic In Proceedings of the Computer Vision and Pattern Recognition Conference, pages videos. 2282022830, 2025. 4 Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 20162029, 2025. 4 Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. Orb-slam: versatile and accurate monocular slam system. IEEE transactions on robotics, 31(5):11471163, 2015. 4 Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. 5, OpenAI. Video generation models as world simulators. https://openai.com/index/ video-generation-models-as-world-simulators/, 2024. Accessed: 2024-02-15. 2, 4 Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and In Proceedings of the Fisher Yu. UniDepth: Universal monocular metric depth estimation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 8 Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool. UniDepthV2: Universal monocular metric depth estimation made simpler, 2025. URL https://arxiv.org/abs/2502.20110. 8 Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714. 9 Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1090110911, 2021. 3, 4, Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. 2 Chris Rockwell, Joseph Tung, Tsung-Yi Lin, Ming-Yu Liu, David Fouhey, and Chen-Hsuan Lin. Dynamic camera poses and where to find them. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1244412455, 2025. 4, 5 Johannes L. Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 2, 3 Johannes Lutz Schönberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 8 Christoph Schuhmann. aesthetic predictor. https://github.com/christophschuhmann/ improved-aesthetic-predictor. 8 17 Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. 4 Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, and Hao Li. Vidgen-1m: large-scale dataset for text-to-video generation. arXiv preprint arXiv:2408.02629, 2024. 10 Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 10 HunyuanWorld Team, Zhenwei Wang, Yuhao Liu, Junta Wu, Zixiao Gu, Haoyuan Wang, Xuhui Zuo, Tianyu Huang, Wenhuan Li, Sheng Zhang, et al. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv preprint arXiv:2507.21809, 2025. 2, 5 Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:1655816569, 2021. 4, 8 Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 2, 4, 8 Qianqian Wang*, Yifei Zhang*, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, 2025. 4 Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84288437, 2025. 10 Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024a. 2, 4 Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 49094916. IEEE, 2020. 3 Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 45814591, 2019. 10 Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024b. 2, Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for high-quality meshes. arXiv preprint arXiv:2404.12385, 2024. 2 An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. 11 18 Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2192421935, 2025b. 4, 8 Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024a. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024b. 8 Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024c. 2, 4 Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European conference on computer vision (ECCV), pages 767783, 2018. 2, 4 Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: large-scale dataset for generalized multi-view stereo networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17901799, 2020. 2, 4, 5 Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 2, Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, TienTsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 2, 5 Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Tianyou Liang, Guanying Chen, Shuguang Cui, and Xiaoguang Han. Mvimgnet: large-scale dataset of multi-view images. In CVPR, 2023. 4 Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arxiv:2410.03825, 2024a. 4, 8 Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, pages 119. Springer, 2024b. 2 Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William Freeman. Structure and motion from casual videos. In European Conference on Computer Vision, pages 2037. Springer, 2022. 4 Yuyang Zhao, Chung-Ching Lin, Kevin Lin, Zhiwen Yan, Linjie Li, Zhengyuan Yang, Jianfeng Wang, Gim Hee Lee, and Lijuan Wang. Genxd: Generating any 3d and 4d scenes. arXiv preprint arXiv:2411.02319, 2024. 4, Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wetzstein, and Leonidas J. Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In ICCV, 2023. 4 Li Zhi, Aaron Anne, Katsavounidis Ioannis, Moorthy Anush, and Manohara Megha. Toward practical perceptual video quality metric. https://netflixtechblog.com/ toward-a-practical-perceptual-video-quality-metric-653f208b9652, 2016. 8 Shijie Zhou, Alexander Vilesov, Xuehai He, Ziyu Wan, Shuwang Zhang, Aditya Nagachandra, Di Chang, Dongdong Chen, Eric Xin Wang, and Achuta Kadambi. Vlm4d: Towards spatiotemporal awareness in vision language models. In Proceedings of the IEEE/CVF international conference on computer vision, 2025. 5, 10 19 Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. 3, 4,"
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Science",
        "Nanjing University"
    ]
}