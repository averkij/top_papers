{
    "paper_title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports",
    "authors": [
        "Kevin Wu",
        "Eric Wu",
        "Rahul Thapa",
        "Kevin Wei",
        "Angela Zhang",
        "Arvind Suresh",
        "Jacqueline J. Tao",
        "Min Woo Sun",
        "Alejandro Lozano",
        "James Zou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Doctors and patients alike increasingly use Large Language Models (LLMs) to diagnose clinical cases. However, unlike domains such as math or coding, where correctness can be objectively defined by the final answer, medical diagnosis requires both the outcome and the reasoning process to be accurate. Currently, widely used medical benchmarks like MedQA and MMLU assess only accuracy in the final answer, overlooking the quality and faithfulness of the clinical reasoning process. To address this limitation, we introduce MedCaseReasoning, the first open-access dataset for evaluating LLMs on their ability to align with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic question-and-answer cases, each paired with detailed reasoning statements derived from open-access medical case reports. We evaluate state-of-the-art reasoning LLMs on MedCaseReasoning and find significant shortcomings in their diagnoses and reasoning: for instance, the top-performing open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy and mentions only 64% of the clinician reasoning statements (recall). However, we demonstrate that fine-tuning LLMs on the reasoning traces derived from MedCaseReasoning significantly improves diagnostic accuracy and clinical reasoning recall by an average relative gain of 29% and 41%, respectively. The open-source dataset, code, and models are available at https://github.com/kevinwu23/Stanford-MedCaseReasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 3 3 7 1 1 . 5 0 5 2 : r MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports Kevin Wu1 Eric Wu1 Rahul Thapa Kevin Wei2 Angela Zhang3 Arvind Suresh3 Jacqueline J. Tao3 Min Woo Sun1 Alejandro Lozano1 James Zou1 1Stanford University 2University of Southern California 3University of California, San Francisco"
        },
        {
            "title": "Abstract",
            "content": "Doctors and patients alike increasingly use Large Language Models (LLMs) to diagnose clinical cases. However, unlike domains such as math or coding, where correctness can be objectively defined by the final answer, medical diagnosis requires both the outcome and the reasoning process to be accurate. Currently, widely used medical benchmarks like MedQA and MMLU assess only accuracy in the final answer, overlooking the quality and faithfulness of the clinical reasoning process. To address this limitation, we introduce MedCaseReasoning, the first open-access dataset for evaluating LLMs on their ability to align with clinicianauthored diagnostic reasoning. The dataset includes 14,489 diagnostic questionand-answer cases, each paired with detailed reasoning statements derived from open-access medical case reports. We evaluate state-of-the-art reasoning LLMs on MedCaseReasoning and find significant shortcomings in their diagnoses and reasoning: for instance, the top-performing open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy and mentions only 64% of the clinician reasoning statements (recall). However, we demonstrate that fine-tuning LLMs on the reasoning traces derived from MedCaseReasoning significantly improves diagnostic accuracy and clinical reasoning recall by an average relative gain of 29% and 41%, respectively. The open-source dataset, code, and models are available at https://github.com/kevinwu23/Stanford-MedCaseReasoning."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are increasingly deployed in medicine (AMA, 2025), where they show promise in performing complex tasks like clinical reasoning (Goh et al., 2024) and disease diagnosis (McDuff et al., 2025). While there are number of benchmark datasets designed to assess the diagnostic capabilities of medical LLMs (e.g. MedQA (Jin et al., 2020), MMLU (Hendrycks et al., 2020), MultiMedQA (Singhal et al., 2023), and MedXpertQA (Zuo et al., 2025)), these datasets all share the same limitation in that they only assess the correctness of the models final answer. Unlike domains like mathematics (Ahn et al., 2024) or coding (Ding et al., 2024), where the reasoning process is secondary to the correct answer, medical diagnoses require both the final diagnosis and the thought process to be defensible. In clinical practice, physicians need to articulate coherent rationales, which is important for ensuring informed consent, meeting the standard of care, facilitating documentation and billing, and enabling clinical collaboration (Patel et al., 2005). Mistakes or deficiencies in reasoning, even if the correct diagnosis is given, can risk case mismanagement. Highlighting this Equal contribution. Preprint. Figure 1: schematic of the MedCaseReasoning case processing pipeline. A: From the initial set of 98,994 PMC Open Subset Case Reports, we select 28,313 appropriate candidate cases, which are converted to QA format and then filtered for quality. The resulting 14,489 cases form the MedCaseReasoning dataset, with 897 test cases and 13,092 training cases. full example is available in Table 8. B: For each test case, the case presentation is posed to an LLM to reason and then answer with diagnosis. The clinician-authored diagnostic reasoning is compared with the LLMgenerated reasoning to produce the reasoning recall score, and the case diagnosis is compared with the predicted diagnosis to produce diagnostic accuracy. C: The original case report text is converted into three sections: the case presentation that contains the relevant and sufficient patient information for making diagnosis; the diagnostic reasoning that includes the diagnostic decision-making by the case author (in enumerated statements); and the final diagnosis. gap, recent study found that even frontier models like GPT-4 could produce the correct diagnosis for incorrect reasons in up to third of clinical scenarios (Jin et al., 2024). Thus, evaluating and improving model capabilities to reason and answer correctly is crucial for LLMs to gain clinical viability and trustworthiness. In this work, we propose MedCaseReasoning, diagnostic reasoning dataset with two complementary goals: (1) an evaluation benchmark to assess how well LLM reasoning aligns with clinicianauthored diagnostic reasoning, and (2) training dataset for improving diagnostic reasoning in LLMs from reasoning traces. This clinician-validated dataset is created from publicly available case reports published in PubMedCentral (PubMed Central, 2025). We perform filtering and clinician validation from an initial set of 98,994 case reports to produce corpus of 14,489 diagnostic QA, with high-quality test subset of 897 cases. First, we observe that MedCaseReasoning is meaningfully difficult and unsaturated benchmark, with the top commercial model, OpenAI o3, achieving only 65% on 10-shot diagnostic accuracy. Second, we find that even top-performing reasoning models are substantially limited in their ability to reason like clinicians, where the best open-source model, DeepSeek-R1, only recalls around half of the reasons provided in the case report. Third, we find that training on MedCaseReasoning can significantly improve the diagnostic and reasoning capabilities of LLMs: both diagnostic accuracy and clinician reasoning alignment increased after fine-tuning three top medical open-source LLMs, with an average relative gain of 29% and 41%. respectively. Importantly, after training on MedCaseReasoning, their performance on NEJM CPC, held-out test dataset, also improved, showcasing the value of MedCaseReasoning in improving diagnostic generalizability. Main Contributions: 2 We release MedCaseReasoning, an open-access dataset of 14K+ clinical diagnostic cases from 800+ medical journals and 30+ specialties with accompanying clinician-authored reasoning. To produce MedCaseReasoning, we describe scalable, multi-step, clinician-validated pipeline for converting raw case reports into high-quality QA-format diagnostic cases. We evaluate state-of-the-art reasoning LLMs models on MedCaseReasoning, which reveals limitations in medical diagnostic reasoning. We show training on dense reasoning traces from MedCaseReasoning improves the diagnostic accuracy and reasoning recall of open-source LLMs. Our work is the first medical benchmark dataset that explicitly evaluates the reasoning accompanying diagnosis by comparing it to real-world case reports written by clinicians. These cases contain relevant differential diagnoses along with case presentations that present sufficient detail for an accurate diagnosis. Our findings underscore the significant challenges of aligning the reasoning of current top-performing LLMs with established clinical practice and provide dataset for improving this alignment. Related Works While commonly used medical benchmark datasets like MedQA (Jin et al., 2020), MMLU (Hendrycks et al., 2020), and MedXpertQA (Zuo et al., 2025) include diagnostic reasoning questions, the clinical cases presented are typically shorter, textbook-style cases intended to evaluate students and test medical knowledge, not real-world clinical cases that physicians may encounter. Several datasets construct clinical vignettes from real-world sources, such as PubMedQA (Jin et al., 2019), which generated QA examples based on PubMed articles, and MedAlign (Fleming et al., 2023), which generated clinical vignettes from electronic health records. By comparison, MedCaseReasoning is derived directly from clinician-authored case reports, which are valuable tools for advancing clinical practice (Nayak, 2010). The New England Journal of Medicine Clinicopathologic Conferences (NEJM CPC) is collection of case reports sourced from Massachusetts General Hospital in Boston, MA (McDuff et al., 2025). MedCaseReasoning improves upon this dataset in several ways. First, the collection of case reports from PubMedCentral represents globally diverse set of clinician and patient backgrounds, whereas NEJM CPC represents the medical practices of physicians from single hospital system on specific population. Second, the NEJM CPC dataset consists of only 302 test cases, while MedCaseReasoning is comprised of over 14K cases, allowing for more in-depth analysis as well as model fine-tuning. Third, MedCaseReasoning is derived from open-access articles on PubMedCentral, whereas NEJM CPC is available only under license. Several studies (Goh et al. (2025) and Strong et al. (2023)) have employed grading rubrics applied to curated clinical vignettes, but are restricted to handful of cases, not openly accessible, and focus on human-AI comparison rather than establishing benchmarks for standalone LLM reasoning performance. Studies by Kanjee et al. (2023), McDuff et al. (2025), and Gemini (2023) leveraged cases from NEJM CPC to evaluate model capabilities, but primarily focusing on generating differential diagnosis (DDx) lists, not full reasoning traces. Recent models like HuaTuoGPT-o1 (Chen et al., 2024) and MedReason (Wu et al., 2025) involve supervised fine-tuning (SFT) on diagnostic reasoning traces, but the traces are distilled from larger proprietary models (e.g., GPT-4) rather than being grounded in reasoning authored by clinicians based on real patient cases. One study by (Savage et al., 2024) conducted manual clinician evaluations of reasoning traces from GPT-3.5 and GPT-4 to identify logical inconsistencies, but such methods are labor-intensive and inherently limited in scale, making them infeasible for large-scale benchmarking. Collectively, prior work highlights gap in large-scale, open-access benchmarks grounded in realworld, clinician-authored reasoning that evaluate the complete diagnostic thought process beyond just final answer accuracy."
        },
        {
            "title": "2 Methods",
            "content": "2.1 Data curation Case reports are typically written to highlight rare or complex disease. core tradeoff exists between the novelty and usefulness of case report models should learn from difficult cases, but 3 Figure 2: (Left) Evaluation of LLMs on the MedCaseReasoning test dataset (N = 897), along with diagnostic accuracy and reasoning recall percentage. (Right) Model performance on the MedCaseReasoning test set and NEJM case studies (N = 302), showing strong correlation between the two benchmarks. In both plots, accuracy is computed with 10-shot accuracy; circle size encodes the average length of the model-generated reasoning trace. LLaMA-3.1-8B and Qwen-2.5-7B are both Instruct variants (omitted for brevity). Dataset MedQA (Jin et al., 2020) MedXpertQA (Zuo et al., 2025) NEJM CPC (McDuff et al., 2025) MedCaseReasoning Open Access Reasoning Question Type Question Source 12,723 1,050 302 14,489 Clinical vignettes USMLE Clinical vignettes Licensing exams Real diagnostic cases NEJM CPC Real diagnostic cases PMC Case Reports Table 1: Comparison of clinical diagnosis benchmark datasets. MedCaseReasoning is the only and largest open-access dataset with clinician reasoning. Existing datasets do not contain expert reasoning. should be able to practically deduce the diagnosis from the presented information. To this end, we produced LLM-based pipeline with the goal of maximizing case novelty and usefulness. As expert verification on each step is prohibitively expensive, we perform our clinician validation only on the outputs of the last step. 1. Data Provenance: We started with the PMC Open Subset pulled from Jan 1st, 2005-April 27th, 2025. The query was filtered for free-access articles with article type case report, which yielded 98,994 case reports. The articles text was extracted from the main body in the XML file, along with the PMCID, publication date, title, journal name, and article link. 2. Candidate Selection: Next, we excluded case reports without the word differential (ie. discussion on differential diagnoses), which reduced the set to 28,313 case reports. These initial candidate case reports were converted into diagnostic QA with o4-mini, strong reasoning model that can be run at scale. The full prompt is available in the Appendix (Prompt 1) and instructs models to identify the core conflict within the case report and provide an information cutoff that would ensure the presented information does not give away the final diagnosis. In parallel, each candidate case report was scored according to the following criteria: 1) thoroughness of case presentation, 2) presence of explicit differential 4 Reasoning LLaMA-3.1-8B-Instruct (Base) Trace The patient has history of Wilson disease , genetic disorder that leads to copper accumulation in the body. However, the brain MRI shows no evidence of pathological copper deposition , and laboratory tests show normal copper metabolism suggesting the current condition is not related to Wilson disease . The symptoms . . . are consistent with psychotic disorder. The history of Wilson disease and orthotopic liver transplantation may have led to post-liver-transplant psychosis . . . Given the patients symptoms and history, post-liver-transplant psychosis is the most likely diagnosis. LLaMA-3.1-8B-Instruct (SFT on MedCaseReasoning) We first considered acute graft rejection due to non-adherence with tacrolimus and mycophenolate, but this was ruled out because tacrolimus trough levels were low. Next, tacrolimus neurotoxicity was excluded for the same reason. We also considered copper overload from Wilson disease , but MRI showed no pathological copper deposition . Delirium was excluded; finally, we evaluated substance-induced psychosis but found no other medical condition or substance use disorder . With alternatives excluded, schizophrenia was suspected. Answer Coverage Post-liver-transplant psychosis 1/3 reasons recalled Schizophrenia 3/3 reasons recalled Groundtruth (from MedCaseReasoning) 1. Wilson-disease recurrence no abnormalities, including pathological copper accumulation . . . no evidence of abnormal copper metabolism . . . psychosis due to recurrence of Wilson disease was ruled out. 2. Tacrolimus-induced psychosis Tacrolimus-induced psychosis was ruled out because the trough tacrolimus concentration had not increased before onset of psychosis, and psychosis persisted after the trough value decreased. 3. Substance-related or other medical causes No evidence of other medical conditions or substance use disorder that could cause psychotic symptoms was found. Schizophrenia Groundtruth Table 2: An example of comparing reasoning traces against the case report reasoning. The reasoning from the case report is extracted using an LLM as an enumerated list. Then, each element of the list is referenced against the models reasoning trace, which gives us the reasoning recall for given trace. diagnosis, 3) dependence on integrative clinical reasoning, 4) transparency of diagnostic reasoning process, and 5) presence of stated final diagnosis. These criteria were developed by clinicians, and o4-mini was used to produce the actual scores. Criteria 1, 3, and 4 are given out of 1-5 points, whereas criteria 2 and 5 are yes/no responses. We filtered out cases where 1) thoroughness of case presentation was either 1 or 2 (seriously deficient or major gaps), and also removed cases that either did not discuss 2+ plausible alternatives or did not state final diagnosis. The full prompt is available in the Appendix (Prompt 2). After filtering, there remained 19,428 total case reports. 3. Quality Filter: Next, to avoid model blind spots, we checked each of the generated case reports using separate LLM (gemini-2.5-pro), which evaluated the generated case reports faithfulness to the source article and the plausibility of each case report. The prompt is also available in the Appendix (Prompt 3). We removed cases that had any flags, leaving final total of 14,489 cases. We created an initial testing subset of 897 cases where the transparency and integrative diagnostic reasoning scores were at least 4 or 5. The case reports used in MedCaseReasoning span over 800 different medical journals, with the top 10 shown in Table 5. Additionally, the diagnostic case prompts from MedCaseReasoning are significantly longer and more detailed than the ones from MedQA (displayed in Figure 3). Furthermore, case report publication dates are heavily enriched towards years after 2020, with over 16% after Jan 1 2024 (Figure 4). Additionally, our pipeline can be regularly updated, incorporating new case reports with low marginal cost. To validate the case prompts, diagnostic reasoning, and final diagnosis extracted from each case report, team of four board-certified physicians reviewed total of 100 randomly selected cases. For each case, they responded to three questions related to the presence of hallucinations, faithfulness, and reasonableness of each case. The full questions can be found in the Appendix  (Table 5)  . 2.2 Evaluating Diagnostic Accuracy in Models We evaluate each models diagnostic correctness using LLM-as-a-judge, in accordance to previous literature (e.g., McDuff et al. (2025); Wu et al. (2025)). We adopt the same prompt used in McDuff et al. (2025), (Prompt 7), which has been validated to have high concordance with human raters, and use gpt-4o-mini as the judge model for its speed and accuracy. Differential diagnoses typically contain between five to ten candidate diagnoses which are followedup in clinical setting. Also following evaluation schema from McDuff et al. (2025), each model is evaluated total of 10 times (temperature of 0.8 and top-p of 0.95), and the N-shot performance is recorded. 5 As an external validation to the MedCaseReasoning Test Set, we also evaluate models on case reports from NEJM Clinicopathologic Conferences. In particular, we use subset of 302 cases from previous works (McDuff et al., 2025; Kanjee et al., 2023; Gemini, 2023) as gold-standard dataset of complex diagnostic cases. We manually extracted each case report with personal license and fed the case presentation portion as the case prompt without any additional modifications. As NEJM CPC is not an open-access publication, we are not able to open-source this validation set. Additionally, no NEJM case reports were included in MedCaseReasoning or used in training any models. 2.3 Evaluating Reasoning Traces in Models While some case studies include comprehensive reasoning steps, it is not guaranteed that case reports include an exhaustive set of reasoning steps. As such, we focus our study on evaluating how well models recall clinician-produced reasons (as opposed to evaluating precision). We define this metric as the Reasoning Recall. (Note: We were not able to evaluate reasoning recall for OpenAI o3 and other frontier models where the full reasoning traces are not available via API call. While they can produce reasoning tokens when prompted (i.e. with think tags), the comprehensive reasoning that is used to inform diagnosis is not accessible.) Definition 1 (Reasoning Recall). Let be the total number of cases. For each case i, let Ri = {groundtruth reasons from case report}, Ti = {reasons from model reasoning trace}. For case i, the recall rate is The Reasoning Recall is the average of these rates: ci = Ri Ti Ri . RR = 1 (cid:88) i=1 ci = 1 (cid:88) i=1 Ri Ti Ri . For example, reasoning recall of 50% indicates that on average, models reasoning trace will cover half of the reasons provided by the case report. We evaluate the reasoning trace associated with its best-of-10 response, where we use the trace from randomly selected correct response (or incorrect if there are no correct responses). The evaluation is implemented using an LLM-as-a-judge, where o4-mini is instructed to return JSON with decision on whether each of the groundtruth reasons are found in the reasoning trace. We validate this step with annotations from board-certified physician. The prompt used is found in the Appendix (Prompt 5). 2.4 Supervised Fine-Tuning on Case Report Reasoning In addition to evaluating model reasoning traces, we explore whether fine-tuning on extracted reasons directly can improve model performance on diagnostic accuracy and reasoning recall. One technical challenge is that the extracted diagnostic reasoning is formatted as an enumerated list of summary points and quotations, rather than cohesive reasoning trace. Naturally, we can use LLMs to stitch the list of points into reasoning traces without adding new information. We provide the prompt used to perform this step in the Appendix (Prompt 4). The stitching process may introduce biases if stronger model introduces its own priors into the reasoning traces. To control for this, we ensure that the model that is being fine-tuned is also the model that stitches the reasoning traces together. An example of stitched reasoning trace can be found in the Appendix (Prompt 5). We perform supervised fine-tuning (SFT) on two popular open-sourced models: Qwen-2.5-7B-Instruct and LLaMA-3.1-8B-Instruct. Additionally, to test the marginal value of our reasoning dataset, we include MedReason-8B, model based on LLaMA-3.1-8B-Instruct that was previously fine-tuned on synthetic medical reasoning dataset. We updated each model with full-weight fine-tuning for three epochs on 8 NVIDIA H100 GPUs with learning rate of 2e-5 and batch size of 256. All other hyperparameters are default according to the verl Python package implementation of SFT (version v0.3.0.rc0). 6 MedCaseReasoning Test Set Base Models Reasoning Recall 1-shot Acc. 5-shot Acc. 10-shot Acc. N/A OpenAI o3 0.6420.616-0.667 DeepSeek R1 0.5900.560-0.619 QwQ-32B MedReason-8B 0.4070.383-0.431 LLaMA-3.1-8B-Instruct 0.4510.428-0.475 0.4950.440-0.551 m1-7b-23k 0.3240.301-0.347 Qwen-2.5-7B 0.4700.440-0.500 0.3200.291-0.349 0.2720.245-0.302 0.2480.224-0.275 0.1610.138-0.184 0.1550.133-0.177 0.1740.152-0. 0.6090.579-0.639 0.4470.417-0.478 0.3710.341-0.400 0.3310.303-0.363 0.2810.252-0.311 0.2380.211-0.264 0.2520.223-0.279 0.6450.618-0.675 0.4800.450-0.510 0.3980.368-0.428 0.3820.353-0.412 0.3320.304-0.360 0.2910.262-0.321 0.2870.259-0.316 Fine-Tuned Models Reasoning Recall 1-shot Acc. 5-shot Acc. 10-shot Acc. MedReason-8B (SFT) LLaMA-3.1-8B-Instruct (SFT) Qwen-2.5-7B (SFT) 0.5220.498-0.545 0.3030.276-0.331 0.4300.400-0.459 0.5010.470-0. 0.4850.460-0.510 0.4860.461-0.510 0.2780.249-0.307 0.2490.221-0.278 0.4110.378-0.443 0.3630.335-0.394 0.4790.448-0.509 0.4250.394-0.455 Table 3: Performance of models on MedCaseReasonings test set (N=897). Each model is evaluated on reasoning coverage and diagnostic accuracy (broken down into 1, 5, and 10-shot). Reasoning coverage is not available for OpenAI o3 as the reasoning traces are not provided via API. We additionally perform supervised fine-tuning (SFT) on three open-source models on the training split of MedCaseReasoning and find significant improvements in both reasoning coverage and diagnostic accuracy."
        },
        {
            "title": "3 Results",
            "content": "3.1 Validity of Generated Case Prompts, Diagnostic Reasonings, and Final Diagnoses We performed human validation of the model-generated case prompts, diagnostic reasons, and final diagnoses on the MedCaseReasoning dataset. Our study included four U.S. board-certified physicians reviewing total of 100 cases. Each physician reported spending an average of 10-20 minutes per case. We found high degree of agreement that the generated cases were free of hallucinations in the case prompt or diagnostic reasoning (98%). We also report that 92% of final diagnoses were reported to have been faithful to the article and reasonably inferrable from the details of the case prompt. Finally, 93% of diagnostic reasoning steps were faithful to the case report and clinically relevant. The results are found in the Appendix  (Table 5)  . 3.2 Diagnostic Accuracy We evaluate the following combination of frontier reasoning models and popular open-sourced models: OpenAI o3, DeepSeek R1, QwQ-32B, MedReason-8B, m1-7b-23k, LLaMA-3.1-8B-Instruct, and Qwen-2.5-7B-Instruct. The models are each evaluated on 1-shot, 5-shot, and 10-shot accuracy, listed in Table 3 and Table 6. Overall, we found that diagnostic reasoning remains difficult task for top-performing LLMs. OpenAI o3 performs significantly better than the rest of the models, with 64.5% 10-shot accuracy on the MedCaseReasoning test set, whereas DeepSeek R1 achieves 48.0%. In comparison, OpenAI o3 and DeepSeek R1 achieve similar scores of 62.3% and 43.7% 10-shot accuracy on NEJM CPC, signaling the validity of MedCaseReasoning as an open-access alternative evaluation set for diagnostic cases. We find that supervised fine-tuning significantly improves model performance across both test sets. For example, on the MedCaseReasoning test set, MedReason-8B improves by 31% 10-shot accuracy, outperforming DeepSeek R1. Of note, this model also improves by 18% on NEJM CPC and outperforms QwQ-32B. This provides evidence of the generalizability of MedCaseReasoning 7 training data, as NEJM CPC consists of out-of-distribution, hand-crafted diagnostic cases that are often much longer in length. 3.3 Validity of Reasoning Recall Metric We validate our LLM-as-a-judge for determining reasoning recall with verification from boardcertified physician. The physician was given N=33 cases and was asked to verify the LLM judges decisions on total of 89 pairs of groundtruth reasons and model thinking traces. The cases were randomly sampled across all evaluated models. For example, for given case report that contained three reasoning steps, the physician was asked to cross-check each step against the entire model reasoning trace to see if it was considered. The physician found 84/89 (94.4, 95% CI of 84.8%-100%) of pairs of reasons and thinking traces to be correctly assessed by the model, and total of 31/33 (93.9, 95% CI of 84.8%-100%) cases to be completely accurately assessed by the model. 3.4 Evaluation of Reasoning LLMs We evaluate six of seven models that provide reasoning traces on reasoning recall and find that the top model, DeepSeek R1, covers 64.2% of the reasoning steps provided in case reports. We also found that MedReason-8B, model explicitly trained on synthetically generated medical reasoning traces, did not have substantially higher reasoning recall, and even had lower recall than LLaMA-3.1-8B-Instructs base model. However, after SFT on MedCaseReasoning, each of the base models improved significantly on reasoning recall. For example, MedReason-8B improved by 28% and Qwen-2.5-7B-Instruct improved by 50%. Common types of case report reasoning not found within the base model reasoning traces were either missing candidate diagnoses or exclusionary symptoms for common diagnoses. For example, one case report focused on case of Adult-onset Stills disease (AOSD), which presented very similarly to Rocky Mountain Spotted Fever (RMSF). The model (m1-7b-23k) made diagnosis for RMSF, missing reasoning about the patient presentation from the case report: We feel that our patients symptoms were not due to RMSF, because the rash was salmon colored and worsened with the fever spikes.. We found significant correlation between model performance and reasoning recall (Pearson r=0.710, p=0.0485), indicating the value of measuring reasoning steps as proxy for model performance itself. Furthermore, we also observe significant correlation between the length of the models reasoning trace and the reasoning recall (r=0.790, p=0.0196)."
        },
        {
            "title": "4 Discussion",
            "content": "Our work introduces MedCaseReasoning, the first open-access dataset designed to evaluate the alignment of LLMs with clinician-authored diagnostic reasoning. MedCaseReasoning addresses critical gap in current medical benchmarks: the assessment of diagnostic accuracy without scrutinizing the underlying reasoning process. We report two key findings: first, even top-performing LLMs exhibit deficiencies in aligning with clinician reasoning, achieving maximum recall of 64.2%. Second, fine-tuning models on the reasoning traces derived from MedCaseReasoning significantly improves both their recall with clinician reasoning and their final diagnostic accuracy. These findings underscore the value of MedCaseReasoning in both evaluating and enhancing the clinical diagnostic capabilities of LLMs. SFT with the MedCaseReasoning training dataset demonstrates that smaller models, such as Llama 3.1 8B and Qwen 2.5 7B, can achieve diagnostic accuracy comparable to or exceeding that of larger models like Qwen1.5-32B and DeepSeek-R1 after SFT. While prior studies (Chen et al. (2024); Wu et al. (2025)) have explored learning reasoning from synthetic traces generated by more powerful models, our research is the first to demonstrate the efficacy of training directly on clinician-written diagnostic reasoning. Compared to established benchmarks like MedQA, where leading models such as GPT-4o have already achieved over 90% accuracy, diagnostic performance on MedCaseReasoning currently peaks at 64.5%. This suggests MedCaseReasoning presents more challenging task, focusing on the nuanced alignment with expert reasoning. This characteristic is shared with complex diagnostic case report datasets like the NEJM CPC; indeed, we observe strong correlation in diagnostic performance 8 between MedCaseReasoning and NEJM CPC (Figure 2). However, MedCaseReasoning offers distinct advantages: it is open-access, unlike the license-restricted NEJM CPC, and provides substantially larger corpus, with nearly 14,489 examples (including 13,092-example training set), compared to NEJM CPCs 302 test cases. Additionally, the MedCaseReasoning case curation pipeline is also extensible to other case reports, so the dataset is able to be extended as more reports become available, allowing the dataset to be updated to reflect current medical guidelines. Our study presents several limitations. First, some case reports may lack sufficient detail for definitive diagnosis or present trivial cases. The QA conversion process can also introduce variability, where case details may be inadvertently left out or hallucinated. While we implemented clinicianvalidated filtering pipeline for the test set to ensure reasoning statements are grounded in the case presentation, some intractable cases (eg. ones where the diagnosis cannot be made without certain information) or trivial cases (eg. ones where the diagnosis is given away in the prompt) still persist. Second, MedCaseReasoning captures the case presentation at single timestep before asking for final diagnosis. It does not reflect the iterative, multi-stage nature of real-world clinical diagnosis, which involves refining differential diagnoses based on evolving information from tests, imaging, and treatment responses. Third, our reasoning recall metric only captures clinical reasoning provided within case reports. Diagnostic reasoning is inherently subjective, and while our extensive training corpus aims to encompass diverse diagnostic standards, the alignment metric should be interpreted as adherence to observed clinical reasoning patterns across diverse range of clinicians rather than an absolute, single gold standard. Diagnosing rare and complex diseases has broad societal implications on patient health, patient-doctor interactions, and the trustworthiness of LLMs. Our study aims to shed light on key factor mediating all three of these factors, namely diagnostic reasoning."
        },
        {
            "title": "References",
            "content": "Ahn, J., Verma, R., Lou, R., Liu, D., Zhang, R., and Yin, W. Large language models for mathematical reasoning: Progresses and challenges. arXiv, 2024. doi: 10.48550/arxiv.2402.00157. AMA. AMA: Physician enthusiasm grows for health care AI undefined american medical association, 2025. URL https://www.ama-assn.org/press-center/ama-press-releases/ ama-physician-enthusiasm-grows-health-care-ai. Chen, J., Cai, Z., Ji, K., Wang, X., Liu, W., Wang, R., Hou, J., and Wang, B. HuatuoGPT-o1, towards medical complex reasoning with LLMs. arXiv [cs.CL], December 2024. Ding, H., Fan, Z., Guehring, I., Gupta, G., Ha, W., Huan, J., Liu, L., Omidvar-Tehrani, B., Wang, S., and Zhou, H. Reasoning and planning with large language models in code development. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, volume 35, pp. 64806490, New York, NY, USA, August 2024. ACM. Fleming, S. L., Lozano, A., Haberkorn, W. J., Jindal, J. A., Reis, E. P., Thapa, R., Blankemeier, L., Genkins, J. Z., Steinberg, E., Nayak, A., Patel, B. S., Chiang, C.-C., Callahan, A., Huo, Z., Gatidis, S., Adams, S. J., Fayanju, O., Shah, S. J., Savage, T., Goh, E., Chaudhari, A. S., Aghaeepour, N., Sharp, C., Pfeffer, M. A., Liang, P., Chen, J. H., Morse, K. E., Brunskill, E. P., Fries, J. A., and Shah, N. H. MedAlign: clinician-generated dataset for instruction following with electronic medical records. arXiv [cs.CL], August 2023. Gemini. Gemini: family of highly capable multimodal models. arXiv [cs.CL], December 2023. Goh, E., Gallo, R., Hom, J., Strong, E., Weng, Y., Kerman, H., Cool, J. A., Kanjee, Z., Parsons, A. S., Ahuja, N., Horvitz, E., Yang, D., Milstein, A., Olson, A. P. J., Rodman, A., and Chen, J. H. Large language model influence on diagnostic reasoning. JAMA Network Open, 7(10):e2440969, 2024. doi: 10.1001/jamanetworkopen.2024.40969. Goh, E., Gallo, R. J., Strong, E., Weng, Y., Kerman, H., Freed, J. A., Cool, J. A., Kanjee, Z., Lane, K. P., Parsons, A. S., Ahuja, N., Horvitz, E., Yang, D., Milstein, A., Olson, A. P. J., Hom, J., Chen, J. H., and Rodman, A. GPT-4 assistance for improvement of physician performance on patient care tasks: randomized controlled trial. Nature Medicine, 31(4):12331238, 2025. ISSN 1078-8956. doi: 10.1038/s41591-024-03456-y. 9 Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv, 2020. doi: 10.48550/arxiv.2009.03300. Jin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and Szolovits, P. What disease does this patient have? large-scale open domain question answering dataset from medical exams. arXiv [cs.CL], 2020. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. PubMedQA: dataset for biomedical research question answering. arXiv [cs.CL], September 2019. Jin, Q., Chen, F., Zhou, Y., Xu, Z., Cheung, J. M., Chen, R., Summers, R. M., Rousseau, J. F., Ni, P., Landsman, M. J., Baxter, S. L., AlAref, S. J., Li, Y., Chen, A., Brejt, J. A., Chiang, M. F., Peng, Y., and Lu, Z. Hidden flaws behind expert-level accuracy of multimodal GPT-4 vision in medicine. NPJ Digit. Med., 7(1):190, July 2024. Kanjee, Z., Crowe, B., and Rodman, A. Accuracy of generative artificial intelligence model in complex diagnostic challenge. Jama, 330(1):7880, 2023. McDuff, D., Schaekermann, M., Tu, T., Palepu, A., Wang, A., Garrison, J., Singhal, K., Sharma, Y., Azizi, S., Kulkarni, K., et al. Towards accurate differential diagnosis with large language models. Nature, pp. 17, 2025. Nayak, B. K. The significance of case reports in biomedical publication. Indian J. Ophthalmol., 58 (5):363364, September 2010. Patel, V. L., Arocha, J. F., and Zhang, J. Thinking and reasoning in medicine. The Cambridge handbook of thinking and reasoning, 14:727750, 2005. PubMed Central. PMC open access subset. https://pmc.ncbi.nlm.nih.gov/tools/ openftlist/, 2025. Accessed: 2025-5-15. Savage, T., Nayak, A., Gallo, R., Rangan, E., and Chen, J. H. Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine. npj Digital Medicine, 7(1):20, 2024. doi: 10.1038/s41746-024-01010-1. Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tanwani, A., ColeLewis, H., Pfohl, S., Payne, P., Seneviratne, M., Gamble, P., Kelly, C., Babiker, A., Scharli, N., Chowdhery, A., Mansfield, P., Demner-Fushman, D., Aguera Arcas, B., Webster, D., Corrado, G. S., Matias, Y., Chou, K., Gottweis, J., Tomasev, N., Liu, Y., Rajkomar, A., Barral, J., Semturs, C., Karthikesalingam, A., and Natarajan, V. Large language models encode clinical knowledge. Nature, 620(7972):172180, August 2023. Strong, E., DiGiammarino, A., Weng, Y., Kumar, A., Hosamani, P., Hom, J., and Chen, J. H. Chatbot vs medical student performance on free-response clinical reasoning examinations. JAMA Intern. Med., 183(9):10281030, 2023. Wu, J., Deng, W., Li, X., Liu, S., Mi, T., Peng, Y., Xu, Z., Liu, Y., Cho, H., Choi, C.-I., Cao, Y., Ren, H., Li, X., Li, X., and Zhou, Y. MedReason: Eliciting factual medical reasoning steps in LLMs via knowledge graphs. arXiv [cs.CL], April 2025. Zuo, Y., Qu, S., Li, Y., Chen, Z., Zhu, X., Hua, E., Zhang, K., Ding, N., and Zhou, B. MedXpertQA: Benchmarking expert-level medical reasoning and understanding. arXiv [cs.AI], January 2025."
        },
        {
            "title": "5 Appendix",
            "content": "Question Q1: There are no hallucinated details in the case prompt or diagnostic reasoning. Physician Agreement 98.0%95.0100.0% Q2: The final diagnosis is faithful to the article and reasonably inferrable from the details in the case prompt. Q3: The diagnostic reasoning steps are faithful to the case report and are clinically relevant. 92.0%86.097.0% 93.0%88.098.0% Table 4: Physician agreement rates with 95% confidence intervals for diagnostic prompt evaluation, with = 100 total case reports reviewed. We find that diagnostic cases generated from case reports contain low rates of hallucinations and are faithful to the original reports. Journal Name Journal of Medical Case Reports Clinical Case Reports International Journal of Surgery Case Reports Radiology Case Reports Journal of Surgical Case Reports JAAD Case Reports SAGE Open Medical Case Reports Journal of Orthopaedic Case Reports Case Reports in Medicine European Heart Journal: Case Reports Prevalence (%) 8.20% 7.57% 6.81% 6.36% 2.88% 2.47% 2.16% 2.10% 1.87% 1.60% Table 5: Top 10 medical case report journals by prevalence in the PMC Open Access Subset dataset. In total, there are 813 unique journals in MedCaseReasoning, spanning 30+ different medical specialties across multiple countries. 11 Figure 3: Comparison of length of questions from MedQA vs. MedCaseReasoning. Diagnostic case prompts are, on average, 2.5x longer in MedCaseReasoning and contain real patient information vs. synthetic case vignettes. Figure 4: Distribution of dates of publication for PMC case reports used in MedCaseReasoning. Cases are largely from recent dates (after 2020), and over 500 cases are after Jan 1 2025. 12 NEJM Clinicopathologic Conferences (CPC) - Generalization Base Models 1-shot Acc. 5-shot Acc. 10-shot Acc. OpenAI o3 DeepSeek R1 QwQ-32B MedReason-8B LLaMA-3.1-8B-Instruct m1-7b-23k Qwen-2.5-7B 0.4300.377-0.490 0.5790.526-0.639 0.6230.569-0.675 0.2720.225-0.318 0.4070.354-0.467 0.4370.384-0.497 0.1820.136-0.225 0.2810.232-0.334 0.3410.288-0.397 0.1690.126-0.215 0.2620.212-0.308 0.2950.245-0.348 0.0700.043-0.099 0.1420.103-0.182 0.2120.165-0.255 0.0990.066-0.136 0.1720.136-0.215 0.2090.162-0.252 0.0700.043-0.099 0.1090.076-0.142 0.1230.086-0.159 Fine-Tuned Models 1-shot Acc. 5-shot Acc. 10-shot Acc. 0.1790.136-0.225 0.2780.228-0.328 0.3480.295-0.404 MedReason-8B (SFT) LLaMA-3.1-8B-Instruct (SFT) 0.1690.126-0.212 0.2550.205-0.305 0.3080.255-0.358 0.1290.093-0.166 0.2020.159-0.248 0.2520.205-0.305 Qwen-2.5-7B (SFT) Table 6: To validate the generalization of our dataset, we evaluate models on NEJM Clinicopathologic Conference (CPC) cases (N=302), previously used in McDuff et al. (2025). We find that models fine-tuned on MedCaseReasoning also significantly improve on NEJM cases, which are considered gold-standard in diagnostic case reports. Diagnostic Reasoning 1. Benign soft-tissue tumors (lipoma or fibroma) were considered but deemed unlikely because Lipomas and fibromas typically present as solid masses and are rarely associated with infection. 2. canal of Nuck cyst was considered but excluded because The canal of Nuck represents cystic lesion extending into the inguinal region, which was ruled out based on imaging. 3. Bartholins gland abscess was supported by pelvic imaging showing contrast-enhancing fluid collection at the gland sites; therefore, diagnosis of bilateral Bartholins gland abscesses was made. Stitched Reasoning Lipomas and fibromas typically present as solid masses and are rarely associated with infection, making benign soft-tissue tumors such as lipomas or fibromas unlikely. The canal of Nuck represents cystic lesion extending into the inguinal region, which was ruled out based on imaging. Pelvic imaging showed contrast-enhancing fluid collection at the gland sites, supporting the diagnosis; therefore, diagnosis of bilateral Bartholins gland abscesses was made. Table 7: The model being fine-tuned (LLaMA-3.1-8B-Instruct) is used to convert the enumerated diagnostic reasoning into cohesive reasoning trace, as shown above. The model is asked to stitch the reasoning without introducing any additional information. 13 Type Content Article Title Journal Article Link Case Prompt Sebaceous carcinoma of the breast predominantly characterized by intraductal growth: case report Surgical Case Reports https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7040145/ 47-year-old Japanese woman with no breast symptoms was referred after screening mammography revealed clustered pleomorphic calcifications in the left breast. Her mother had bilateral breast and ovarian cancer. On examination, there was no palpable mass or nipple discharge. Serum tumor markers (CEA, CA15-3, NCC-ST-439, BCA225) were within normal limits. Breast ultrasound showed 13127 mm irregular, hypoechoic mass with clear margins and internal high-echogenic foci in the left breast. MRI demonstrated 141112 mm lesion with early arterial enhancement in the same region. PET-CT revealed focal uptake (SUVmax 3.54) in the left breast without abnormal uptake elsewhere. Core-needle biopsy of the mass showed ductal carcinoma in situ, nuclear grade 2, negative for ER, PgR, and HER2, with Ki-67 labeling index of 32.4%. Given her family history, hereditary breast and ovarian cancer syndrome was considered, but the patient declined genetic testing. The patient elected to undergo total left mastectomy with sentinel lymph node biopsy. Diagnostic Reasoning 1. Consideration of invasive ductal carcinoma SC is characterized by lobular forms or nests of tumor cells that exhibit sebaceous differentiation, which distinguishes SC from invasive ductal carcinoma (IDC). 2. Consideration of glycogen-rich clear cell carcinoma Glycogen-rich clear cell carcinomas typically become periodic acid-Schiff (PAS)-positive and therefore can be distinguished from SC cells because they do not produce glycogen. Final Diagnosis Sebaceous carcinoma Table 8: single example from MedCaseReasoning of case prompt, extract diagnostic reasoning, and final diagnosis. 14 Prompt 1: Converting Case Reports to Diagnostic QA with Reasoning You are an expert clinicianeducator. Your job is to transform published case report into teaching diagnostic case that medical students can work through step-by-step. In terms of style, think of NEJMs Clinicopathologic Conferences (Case Records of the Mass General Hospital) as template. RULES (Read CarefullyNo Exceptions) 1. Source Fidelity Extract facts only from the supplied case report. Do NOT invent, embellish, or smooth out missing data. Paraphrase narrative prose into concise bullets where helpful, but never add new facts. 2. Structure the Teaching Case in Three Phases Case Presentation Diagnostic Reasoning Final Diagnosis 3. Use the XML Tags Exactly as Shown <think> . . . </think> your hidden analytic notes (not visible to students). <case prompt> . . . </case prompt> the information given to students before they generate differential. <diagnostic reasoning> . . . </diagnostic reasoning> numbered bullet reasons, each built as full sentence followed by direct quote. <final diagnosis> . . . </final diagnosis> single disease/entity name only, nothing more. 4. What Goes Inside <think> 1. Key points What makes this case non-trivial or pedagogically interesting? This should guide where the breakpoint should be. 2. Ideal breakpoint What details of the case presentation should you include and exclude so that students have enough data to reason, but no spoilers? 3. Authors analytic distinctions How did they separate the final diagnosis from look-alikes and other conditions? 5. What Goes Inside <case prompt> Present only the facts known before working differential was made: chief complaint, HPI, vitals, physical exam, and early investigations. Do not include references to Figure 1, Table 1, etc. directly. Summarize any imaging findings from what is given in the text. Present the case in the order presented in the case report (e.g., physical labs before imaging, etc.) Omit any wording that directly states or hints at the final diagnosis. Present this as closely as possible to the style in which the case report is written. 6. What Goes Inside <diagnostic reasoning> Numbered list (1., 2., 3., . . . ). Include every alternative diagnosis mentioned in the article, along with reasons why it was considered and excluded, if provided. Each entry: concise summary of reason [direct quote from article]. You can use ellipses (. . . ) to shorten the quote if there are irrelevant details. Quotes must justify why one possibility rose or fell in likelihood. 7. What Goes Inside <final diagnosis> Single disease/entity name (e.g., sarcoidosis). No adjectives, punctuation, or explanatory text. OUTPUT TEMPLATE (copy exactly) <think> 1. [Core tension] 2. [Best breakpoint of case report, what to include and what to exclude] 3. [Key analytic distinctions between competing diagnoses (taken from case report)] </think> <case_prompt> [Your case presentation text, faithful to the report and stopping at the breakpoint] </case_prompt> <diagnostic_reasoning> 1. Summarized reasoning Direct quote from article...\" 2. Summarized reasoning Direct quote from article...\" 3. ... </diagnostic_reasoning> <final_diagnosis> DiseaseName </final_diagnosis> SUPPLIED CASE REPORT <case_report> {case_report} </case_report> 15 Prompt 2: Grading the Quality of Case Reports You are an expert medical educator tasked with evaluating case reports for their diagnostic-reasoning value. The goal is to find case reports that have similar style to diagnostic teaching cases from medical textbooks. CASE REPORT EVALUATION RUBRIC >> HOW TO USE 1. Read the entire case once without scoring. 2. Re-read, taking notes. 3. Inside <think>...</think>, write the reasoning that leads you to each score. 4. Output only the five XML tags shown after the rubricnothing else. (1{5 points) Look for: HPI, past history, meds, allergies, vitals, focused exam, labs, imaging, hospital course, outcome. +-------------------------------------------------------------+ 1. THOROUGHNESS OF CASE PRESENTATION 1 { Seriously deficient (identifiers only; no vitals) 2 { Major gaps (HPI + vitals OR exam, not both). 3 { Adequate (present but sketchy details). 4 { Very good (complete data, clear timeline). 5 { Exemplary (serial data & course, high quality). +-------------------------------------------------------------+ 2. EXPLICIT DIFFERENTIAL DIAGNOSIS (Yes / No) >=2 plausible alternatives? If yes \"Yes\"; else \"No\". +-------------------------------------------------------------+ 3. DEPENDENCE ON INTEGRATIVE CLINICAL REASONING (1{5) Measures need to combine >=2 data points (hx, labs, etc) +-------------------------------------------------------------+ 4. TRANSPARENCY OF DIAGNOSTIC REASONING PROCESS (1{5) +-------------------------------------------------------------+ 5. STATED FINAL DIAGNOSIS Is diagnosis clearly named? Yes \"Yes\"; else \"No\". +-------------------------------------------------------------+ 1 { Trivial: lone clue gives answer. 2 { Minimal: one dominant clue. 3 { Moderate: must merge TWO findings. 4 { High: THREE+ clues; requires synthesis. 5 { Outstanding: stepwise, complex reasoning. 1 { None (no rationale). 2 { Superficial (lists w/o \"why\"). 3 { Adequate (brief pivots). 4 { Detailed (stepwise, probabilities). 5 { Model (structured, addresses pitfalls). (Yes / No) Additional Rules: 1. If the given article is not actually case report, output NA for all scores. 2. Think through whether the final diagnosis can be reasonably deduced from the case presentation when determining the educational value in #3. OUTPUT TEMPLATE (leave tags exactly as written) <think> ...your internal reasoning for each item... </think> <case_presentation_score>[1-5]</case_presentation_score> <differential_diagnosis_score>[Yes/No]</differential_diagnosis_score> <integrative_reasoning_score>[1-5]</integrative_reasoning_score> <transparency_score>[1-5]</transparency_score> <final_diagnosis_score>[Yes/No]</final_diagnosis_score> SUPPLIED CASE REPORT <case_report> {case_report} </case_report> 16 Prompt 3: Detecting Hallucinations and Inconsistencies in Generated Case Prompts You are the Editor. Your job is to audit draft teaching case that was produced from published case report. You must confirm strict compliance with all instructions, detect hallucinations, and ensure pedagogic quality. YOUR INPUTS 1. The original draft you must audit appears between <generated case> . . . </generated case>. 2. The source article appears between <case report> . . . </case report>. Here is the original guidelines the draft was produced in accordance with: {convert case report prompt} CHECKLIST FAIL ANY ITEM RAISE FLAG A. Source Fidelity Every fact in each section is traceable to the source article. No invented details or embellishments. B. Case Presentation Quality All facts from the case prompt are present in the source article. Contains only information known before the clinicians formed differential. Does not reveal the final diagnosis (there should be room for at least some inference). Provides sufficient data (HPI, vitals, exam initial tests) for clinicians to formulate reasonable differential and get the correct final diagnosis. C. Diagnostic Reasoning Section Each numbered entry starts with summary of the reasoning plus direct quote from the article. Quotes are verbatim or use ellipses (. . . ) without changing meaning. Paraphrased quotes are okay, as long as they retain the original meaning. Rationales reference only information that already appears in <case prompt> (not based on new findings, confirmatory tests, or data withheld from students). D. Final Diagnosis Tag Final diagnosis is reasonably deducible from the case-presentation facts. i.e., the final diagnosis should not depend entirely on some test, imaging, or lab result not given in the case presentation. E. No Hallucinations Anywhere Every datum, quote, or diagnosis is found in the case report. HOW TO REPORT YOUR FINDINGS Output only the two XML blocks below. 1. <flags> . . . </flags> If an item fails, add line FLAG: [short descriptor]. Use one line per failed item, drawn from this controlled vocabulary: CASE PROMPT HALLUCINATION, FINAL DIAGNOSIS IN CASE PROMPT, INSUFFICIENT INFO FOR DIAGNOSIS, DIAGNOSTIC REASONING HALLUCINATION, OTHER. If no issues, write NONE. 2. <editor comments> . . . </editor comments> Briefly justify each flag (one sentence each). If no flags, you may omit or leave empty. Example when problems exist: <flags> FLAG: SOURCE_FIDELITY FLAG: REASONING_EXTRA_INFO </flags> <editor_comments> { SOURCE_FIDELITY: Mentions family history of SLE,\" not present in article. { REASONING_EXTRA_INFO: Rationale cites biopsy result that is not included in the case_prompt. </editor_comments> Example when everything passes: <flags> NONE </flags> <editor_comments></editor_comments> INPUT BLOCKS TO REVIEW Here is the reference case report: <case_report> {case_report} </case_report> Here is the diagnostic case generated by the model: <case_prompt> {generated_case_prompt} </case_prompt> <diagnostic_reasoning> {generated_diagnostic_reasoning} </diagnostic_reasoning> <final_diagnosis> {generated_final_diagnosis} </final_diagnosis> 17 Prompt 4: Stitching Reasoning Trace From Enumerated Diagnostic Reasoning Your job is to convert list of diagnostic reasoning points into cohesive diagnostic reasoning narrative. INPUTS * CASE_PROMPT: {case_prompt} * REASONING_POINTS (in the order they were generated): {reasoning_points} * FINAL_DIAGNOSIS: {final_diagnosis} TASK Write single, cohesive reasoning trace by stitching together all the REASONING_POINTS. The REASONING_POINTS are given as an enumerated list, where each point is brief summary of particular reasoning point, followed by quote from the full case report that supports the reasoning point. This should be written from the perspective of reasoning trace that an LLM chatbot would write in response to case presentation. Do not use the past-tense nature of REASONING_POINTS, instead make them present-tense and third-person as you are considering each potential diagnosis. OUTPUT Only use the reasoning from the REASONING_POINTS to come up with the final diagnosis. Use ALL of the REASONING_POINTS in producing the reasoning trace. Do not include your own reasoning over the case, only use the reasoning points provided. Try to incorporate as much of the quotes as possible into the reasoning trace, using word-for-word copies when possible (dont actually put quotation marks in the reasoning trace). You may rephrase the reasoning points, but only for style and tone, not for substance. No headings, no bullets, no numbered lists - just continuous explanatory narrative. Place this between the tags <stitched_reasoning> and </stitched_reasoning> Prompt 5: Grading Reasoning Recall You are an experienced medical expert tasked with comparing diagnostic reasoning statements that support given diagnosis for given patient case. Your goal is to find supporting statements in the predicted diagnostic reasons that match the groundtruth diagnostic reasons. For each of the statements in Groundtruth Diagnostic Reasons, you need to find the statement or statements in the Predicted Diagnostic Reasons that state the equivalent justification for the diagnosis. For instance, if the groundtruth diagnostic reason is \"The patient has fever\", and the predicted diagnostic reason is \"The patient has fever due to viral infection\", then this is match. If the groundtruth diagnostic reason is \"The patient has fever\", and the predicted diagnostic reason is \"The patient has sore throat\", then this is not match. Instructions: 1. Analyze each statement in Groundtruth Diagnostic Reasons. 2. For each statement in Groundtruth Diagnostic Reasons, find any matching statements in Predicted Diagnostic Reasons. 3. Create JSON object with the following structure: - The main key should be \"matching_dict\" - Each key within \"matching_dict\" should be number representing statement from Groundtruth Diagnostic Reasons - The value for each key should be list of matching statements from Predicted Diagnostic Reasons - If there are no matches for statement, use an empty array Before providing your final output, wrap your analysis inside <diagnostic_comparison> tags: 1. List all statements from Groundtruth Diagnostic Reasons and Predicted Diagnostic Reasons. 2. For each statement in Groundtruth Diagnostic Reasons, consider potential matches from Predicted Diagnostic Reasons: - List pros and cons for each potential match - Its OK for this section to be quite long 3. Summarize your final matching decisions 4. In the JSON output, only include the statements that are in the Predicted Diagnostic Reasons. 5. In the JSON output, the statements should appear exactly as they are in the Predicted Diagnostic Reasons, verbatim, letter for letter. Do not modify the statements in any way, such as rewording them, adding punctuation, quotes, etc. Wrap your JSON output in json tags. Example of the required JSON structure: json {{ \"matching_dict\": {{ \"1\": [], \"2\": [\"Matching statement 1\", \"Matching statement 2\"], \"3\": [\"Matching statement 3\"] }} }} 18 Prompt 6: Diagnostic Question Template Read the following case presentation and give the most likely diagnosis. First, provide your internal reasoning for the diagnosis within the tags <think> ... </think>. Then, output the final diagnosis (just the name of the disease/entity) within the tags <answer> ... </answer>. ---------------------------------------- CASE PRESENTATION ---------------------------------------- {case_presentation} ---------------------------------------- OUTPUT TEMPLATE ---------------------------------------- <think> ...your internal reasoning for the diagnosis... </think> <answer> ...the name of the disease/entity... </answer> \"\"\" a_prompt = \"\"\"<think> {reasoning} </think> <answer> {answer} </answer> Prompt 7: Diagnostic Accuracy LLM-as-a-judge Is our predicted diagnosis correct (y/n)? Predicted diagnosis: {predicted_diagnosis}, True diagnosis: {actual_diagnosis} Answer [y/n]."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly state the papers main contributions and scope. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, we discuss limitations at length in our discussion section. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate Limitations section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [NA] Justification: There are no theoretical results in this paper. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We include the data, prompts, and experimental recipes required to reproduce the main results of our paper. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? 21 Answer: [Yes] Justification: Yes, we have included open access to the dataset. Additionally, we provide the code required to reproduce the original case report set, and prompts required to generate and filter the curated dataset. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, the training and test details are described in the Methods section, along with how they were chosen. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We include 95% confidence intervals with our experimental results. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer Yes if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) 22 The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We describe the type of compute used for producing the SFT experiments. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In our discussion section, we describe the potential impact of incorrect and erroneous reasoning models in medicine. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. 23 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Our dataset is based in open-access data and does not have high risk of license misuse. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Our dataset is derived from open-access on PubMedCentral which allow for free and open use. The outputs derived from the outputs of OpenAI models are cleared for research purposes according to their terms fo use. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. 24 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Yes, we describe the dataset in detail in the paper, as well as in the accompanying Github repository. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not crowdsource or research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: All data is publicly released. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] Justification: We describe usage of LLMs extensively in the paper, and run human validations on the LLM outputs. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "Stanford University",
        "University of California, San Francisco",
        "University of Southern California"
    ]
}