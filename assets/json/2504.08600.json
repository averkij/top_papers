{
    "paper_title": "SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning",
    "authors": [
        "Peixian Ma",
        "Xialie Zhuang",
        "Chengjin Xu",
        "Xuhui Jiang",
        "Ran Chen",
        "Jian Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 0 0 6 8 0 . 4 0 5 2 : r SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning Peixian Ma12, Xialie Zhuang13, Chengjin Xu14, Xuhui Jiang14, Ran Chen1, Jian Guo1 1IDEA Research, International Digital Economy Academy 2The Hong Kong University of Science and Technology (Guangzhou) 3University of Chinese Academy of Sciences 4DataArc Tech Ltd. {mapeixian, zhuangxialie, xuchengjin, jiangxuhui, chenran, guojian} @idea.edu.com"
        },
        {
            "title": "Abstract",
            "content": "Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model."
        },
        {
            "title": "Introduction",
            "content": "Natural Language to SQL (NL2SQL, or Text2SQL) converts natural language questions (NL) into structured SQL statements, simplifying database interaction without requiring database expertise [1, 2]. Recent advancements in NL2SQL have significantly enhanced the level of human-computer interaction within database query applications and contribute to wide range of data science analysis tasks [3, 4]. Current NL2SQL models mainly focus on optimizing workflows and their components, such as schema linking [5, 6], content retrieval [7], generation correction [812]. Despite these advancements, improving the NL2SQL systems inference performance in complex database scenarios remains considerable challenge. As shown in Figure 1, schema complexity may lead to generation errors in processing multi-table joins and nested queries, and it is difficult for individually trained models to think and process complex semantics. Currently, significant portion of NL2SQL research is devoted to training open-source large language models (LLMs) by supervised fine-tuning (SFT) [1315] to achieve accuracy at smaller model scale compared to approaches using closed-source LLMs (e.g., GPT-4, GPT-4o) [8, 10, 16]. However, SFT relies on the database schemas structure and the training data scale. This may lead to the existing models instability Corresponding author. Preprint. Under review. Figure 1: Demonstration of our work. Previous work on NL2SQL primarily relies on supervised fine-tuning to enable the model to learn how to generate SQL. However, in the case of complex database schema or ambiguous semantics, the fine-tuned model may struggle to produce SQL that does not align with the users intentions, as it depends on fixed generation strategy and previous data. By introducing reinforcement learning algorithms, the model can receive intuitive feedback from the database during the training process. This feedback encourages the model to independently explore various SQL generation reasoning approaches, ultimately enhancing the accuracy of its output. in domain adaptation and generalization in new database environments. Additionally, the lack of interpretability of NL2SQL reasoning logic limits the application of the model in high-risk fields, such as finance and healthcare. Recently, reinforcement learning (RL) has shown great potential in training the reasoning ability of LLMs in recent research. Compared with supervised fine-tuning, reinforcement learning can dynamically adjust the decision-making strategy of the LLMs through interaction with the environment, thereby achieving superior performance in complex reasoning tasks [17]. RL-based methods have proven effective in enhancing model reasoning and generalization capabilities in financial reasoning [18], search engines [19] and mathematical reasoning [20, 21]. Based on the above inspiration, we proposed SQL-R1, NL2SQL reasoning model trained by the reinforcement learning algorithm. Figure 1 demonstrates the overview of our work. In the following sections, we will focus on answering the following critical questions: Q1: Can we design specific reinforcement learning algorithm for the NL2SQL task and successfully train NL2SQL reasoning model? In contrast to SFT, RL algorithms prioritize the direct optimization of NL2SQL reasoning, specifically by generating SQL queries that accurately reflect the users query intent. The design of effective feedback mechanisms for reinforcement learning presents substantial challenge in developing NL2SQL reasoning models. Appropriately structured rewards within the reinforcement learning framework can significantly enhance its performance. Q2: For the RL-based NL2SQL reasoning model, do we need to perform specific form of cold start on it? For the existing base model, an effective cold start can strengthen the models instructionfollowing ability and activate its NL2SQL generation ability, thus promoting it to generate higherquality SQL queries in reinforcement learning exploration. Designing the form of cold start will also be significant challenge. Q3: Can we deploy sustainable data engineering for training robust and efficient NL2SQL reasoning models? RL training relies on high-quality training data, while current NL2SQL tasks lack large amount of real data for training. How to develop the data support for NL2SQL inference model based on the existing data engineering technology is an important challenge to solve the model training, improve the robustness and generalization of the model. Above all, the contribution of this work are as follows: 2 Explicit NL2SQL Reasoning Model: We propose SQL-R1, NL2SQL reasoning model trained on few NL2SQL data (e.g., 5K) currently, which can achieve 88.6% and 66.6% accuracy on the leading benchmark Spider-Test and BIRD respectively, and can output detailed explicit reasoning process. Training Strategy for NL2SQL Reasoning Model: We explore the impact of cold-start training on the performance of SQL-R1 and construct training strategy based on SFT and RL, which can effectively improve the NL2SQL inference performance of the model."
        },
        {
            "title": "2 SQL-R1",
            "content": "2.1 Overview This section mainly introduces two forms of training NL2SQL models via RL algorithms: direct reinforcement training and reinforcement training via cold start after training. Among them, cold start refers to using specific data to train the base model by SFT first so that it has particular ability to think and follow instructions. In addition, due to the limited real data, we use the latest synthetic data to support the above training process. Section 2.2 will introduce our current data engineering solution, Section 2.3 will introduce the SFT algorithm and the RL algorithm designed for NL2SQL. 2.2 Data Preparation 2.2.1 Source Currently, we utilize the SynSQL-2.5M [22] dataset as primary data source, which is the first million-scale synthetic NL2SQL dataset, encompassing over 2.5 million diverse and high-quality data samples. Each sample consists of quadruple comprising database, natural language question, an SQL query, and chain-of-thought (CoT) solution. The dataset features more than 16,000 synthetic databases across various domains, thereby ensuring extensive coverage of realistic scenarios. SynSQL-2.5M includes wide range of SQL complexity levels, from simple single-table queries to intricate multi-table joins, functions, and common table expressions. 2.2.2 Preprocessing SFT Dataset In this study, we investigated the impact of the cold start condition in SFT on RL training. Currently, we utilized dataset comprising 200,000 samples drawn from the SynSQL-2.5M for the SFT training, whose sample size is uniform across different difficulty levels, with each level comprising 50000 samples. For clarity, we will refer to this subset as SynSQL-200K in subsequent sections. It is essential to highlight that the query results obtained from the all SQL ground truth are exclusively are non-null values. For each sample = (x, t, y) in the SFT dataset , represents the NL, while represents the reasoning process enclosed in <think>...</think> tags and denotes the SQL enclosed in <answer>...</answer> tags. RL Dataset The current NL2SQL base model has demonstrated strong capability in generating simple to moderate SQL queries. However, it exhibits limitations when tasked with the creation of more sophisticated SQL queries. Consequently, employing dataset comprised of more challenging samples during the training process may prove beneficial in addressing these deficiencies and enhancing the models overall performance in generating complex SQL. We randomly sampled 5K NL-SQL pairs from SynSQL-2.5M, whose complexity are Complex. For each NL-SQL pair = (x, y) in the RL dataset , represents the NL, while denotes the SQL candidate generated by the model. The aim of reinforcement learning is to enhance the accuracy of the answers and ensure that they adhere to the expected format. The RL Dataset is introduced as SynSQL-Complex-5K in the next section. Notably, The input of the RL dataset does not include the CoT data of the original SynSQL-2.5M. 3 2.3 Training 2.3.1 Supervised Fine-Tuning In this study, we conduct SFT on the Qwen2.5-Coder-7B-Instruct model to enhance the models capacity for instruction adherence and generation within the NL2SQL domain. We investigate two distinct strategies for SFT cold start training. The first one employs raw instructions focusing exclusively on SQL generation. We leverage the existing OmniSQL-7B [22] checkpoint for the reference. The second strategy utilizes full fine-tuning and reasoning generation instructions promoting the development of compliant thought processes alongside final answers. 2.3.2 Reinforcement Training In the reinforcement learning phase, we employ the Group Relative Policy Optimization (GRPO) algorithm to enhance our training protocol, which obviates the need for the value model, operates with less memory requirements, and facilitates clear definition of reward targets, rendering it an optimal choice for the effective optimization of the NL2SQL policy model [23]. For each natural language question aligned with its corresponding database schema, the policy model generates set of SQL candidates {o1, o2..., oG} from the old policy πold, which are meticulously evaluated using composite reward function that assigns specific reward scores. By concentrating on the relative performance of the SQL candidates within the group, GRPO adeptly calculates the rewards for each output, thereby guiding the policy update in accordance with our established objectives. JGRPO(θ) =E vP (V),{oi}G (cid:34) i=1πθold (Ov) (cid:0)min (cid:0)rratio Ai, clip (cid:0)rratio , 1 ϵ, 1 + ϵ(cid:1) Ai (cid:1) βDKL(πθπref)(cid:1) (cid:35) , (1) 1 (cid:88) i=1 = πθ(oiV ) where rratio πold(oiV ) represents the importance sampling ratio that quantifies the relative likelihood of generating output oi under the new policy πθ compared to πold; Ai represents the grouprelative advantage for each output; the clipping operator, hyperparameter ϵ and β control the update step and divergence regularization; πref represents the reference policy. 2.3.3 Reward Function Design When training NL2SQL reward model using reinforcement learning, we utilize progressive feedback mechanism that consists of four types of rewards: Format Reward, Execution Reward, Result Reward, and Length Reward. This layered approach enhances the models learning by providing detailed feedback at various stages. Format Reward We encourage the model to enclose the NL2SQL reasoning process within <think>...</think> tags and to present the final answer enclosed within <answer>...</answer> tags. To achieve more standardized format for SQL queries, it is essential that all SQL statements be contained within sql... tags; failure to do so will result in their format as erroneous. The structure of the format reward function is delineated as follows: Sf = (cid:26)1, 1, if format is correct if format is incorrect Execution Reward Execution rewards are designed to evaluate the syntactic correctness of SQL candidates, preventing the model from generating messy, unexecutable responses. When the SQL candidate fails to execute correctly, the model will not receive all subsequent rewards. In addition, we limit the execution time to prevent the model from tending to generate too complex SQL for high rewards. 4 Se = 2, 0, 2, if SQL candidate is executable if format is incorrect if SQL candidate is not executable Result Reward The accuracy of query results is an important criterion in the evaluation of SQL candidates. We prioritize the Result Reward as key component of the reward mechanism, aimed at motivating the model to generate SQL candidates that aligns with the real intention of the user. To evaluate the correctness of SQL candidate query results and the associated feedback, we utilize Execution Accuracy (EX). In cases of incorrect results, we impose stringent penalties to guide the model in its subsequent reasoning. Sr = 3, 0, 3, if query result is correct if format is incorrect or SQL candidate is not executable if query result is incorrect Length Reward We apply length reward mechanism to incentivize the model to produce more comprehensive reasoning process. It is divided into two components: The first component allocates half of the reward based on the proportional relationship between the total length of the answer and the maximum length of response; The second component computes the remaining half of the reward based on the ratio of the SQL candidate length within the <answer>, which aims to mitigate the occurrence of superfluous explanations in the response. When the response exceeds the maximum length, penalized feedback is given to the model. Sl = 0.5 Stl + Sal, 0.5 + Sal, 0, if query result is correct and lenresponse <= MAX LENGTH if query result is correct and lenresponse > MAX LENGTH other cases where Stl = (lenthink + lenanswer) / MAX LENGTH and Sal = lensql / lenanswer. 2.4 SQL Candidates Selection In order to select the most appropriate SQL in the inference process, the model generates several SQL candidates and their thought processes for problem. We execute all SQL candidates and select the SQL with the highest score as the final answer based on self-consistency voting. Notably, the reasoning response of SQL-R1 comprises an observable process of thinking and interpreting, making the results easier for the user to understand."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Setup Evaluation Benchmark We evaluated the proposed SQL-R1 and related NL2SQL models on two benchmarks, Spider [24] and BIRD [25]. Spider comprises 10,181 questions paired with 5,693 complex SQL queries from 200 databases and 138 domains. BIRD comprises 12,751 NL2SQL pairs encompassing 95 databases from 37 specialized domains. Metric For fair comparisons, we follow the standard evaluation metric in previous works. We use Execution Accuracy (EX) the evaluation metric on Spider and BIRD benchmark. EX serves to estimate the proportion of questions that produce consistent outcomes for both the given query and its corresponding basic fact query across all query requests. Implementation Settings Currently, SQL-R1 is mainly built on Qwen2.5-Coder-7B-Instruct. For the SFT training, we set the learning rate as 5e-5; batch size as 1. For the RL training, we set the learning rate as 3e-7, rollout of actor model as 8; max response length as 2048. For inference, we set the count of SQL candidates as 8 and the temperature as 0.8. 5 Table 1: Execution accuracy of different NL2SQL methods on Spider and BIRD benchmark. Candidate Selection Spider (Dev) Spider (Test) BIRD (Dev) NL2SQL Method Base Model CodeS [14] CHESS [7] CHASE-SQL [28] Alpha-SQL [29] SQL-o1 [30] OmniSQL [22] CodeS-15B Deepseek-Coder-33B Gemimi-1.5-Pro Qwen2.5-Coder-7B Qwen2.5-Coder-7B Qwen2.5-Coder-7B C3-SQL [10] DIN-SQL [8] DAIL-SQL [16] MAC-SQL [9] SuperSQL [31] MCTS-SQL [32] MCTS-SQL [32] OpenSearch-SQL [33] GPT-3.5-Turbo GPT-4 GPT-4 GPT-4 GPT-4 GPT-4o-mini GPT-4o GPT-4o - - - Self-Consistency Self-Consistency Self-Consistency Self-Consistency - Self-Consistency Self-Consistency Self-Consistency - - Self-Consistency SQL-R1 (Ours) Qwen2.5-Coder-7B Self-Consistency 84.9 - - 84.0 84.7 81.6 82.0 82.8 83.6 86.8 84.0 86.2 88.7 - 87. 79.4 87.2 87.6 - 85.1 88.9 82.3 85.3 86.2 82.8 87.0 83.7 86.6 87.1 88.7 57.0 61.5 73.0 66.8 66.7 66.1 - - 54.8 59.4 58.5 63.2 69.4 69.3 66. For all NL2SQL data samples in the dataset, we first convert them into suitable input-output sequence pairs for training. Specifically, the input sequence includes natural language questions and related database schemas. Inspired by previous research [7, 22, 26, 27], the database schema was formatted as CREATE TABLE statement with supplementary annotations including column attribute descriptions and representative values. Currently, annotations of representative values will not be added during the training phase for the time being to enhance the models exploration ability during the reinforcement learning phase. Environment All experiments conducted in this study are performed on server operating under the Ubuntu 20.04 Linux distribution. This server is equipped with Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60 GHz CPU, and is complemented by 512 GB of system memory. The environment for training open-source LLMs comprises configuration of 8 GPUs, each possessing 80 GB of memory and delivering performance capacity of 312 TFLOPS when utilizing BF16 precision. 3.2 Main Results Performance on Main Benchmarks The results presented in Table 1 highlight the remarkable performance of SQL-R1. It managed to attain the best execution accuracy of 87.6% in the Spider development dataset, 88.7% in the test dataset, and 66.6% in the BIRD development dataset. Compared to the NL2SQL solution that employs the Qwen2.5-Coder-7B, our proposed model demonstrates comparable performance levels. Notably, SQL-R1 exhibits superior performance relative to numerous NL2SQL solutions that rely on closed-source models (e.g. GPT-4 and GPT-4o). Insights of Performance and Model Scale Trade-offs As illustrated in Figure 2, we investigated the relationship between performance and model size utilizing on BIRD development dataset. The comparative analysis encompasses various model types, including NL2SQL models, reasoning LLMs and general LLMs. For GPT-4, GPT-4o, GPT-4o-mini and Gemni-1.5-Pro, we refer to the parametric description of [22, 29, 34]. The findings demonstrate that when utilizing the 7B model as foundational model, SQL-R1 attains accuracy levels that surpass those of larger-scale models, particularly. This underscores the efficacy of the proposed model in optimizing the performance of NL2SQL while ensuring that cost efficiency is maintained. Case Study To explore the impact of RL training on the actual NL2SQL reasoning, we select some examples for analysis on the BIRD development dataset. As illustrated in Figure 3 to Figure 8, the model demonstrates enhanced reasoning capabilities following RL training. In the handling of more challenging samples, the model exhibits discernible top-down cognitive strategy in the reasoning 6 Table 2: Execution accuracy of models with different cold start strategy. Spider (Test) Reasoning Instruction Spider (Dev) SFT Data Model Qwen2.5-Coder-7B Qwen2.5-Coder-7B OmniSQL-7B [22] (cid:37) SynSQL-200K SynSQL-2.5M SQL-R1 + Qwen2.5-Coder-7B SQL-R1 + Qwen2.5-Coder-7B SynSQL-200K SynSQL-2.5M SQL-R1 + OmniSQL-7B [22] (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) 77.4 82.7 81.6 84.5 84.7 87.6 79.4 83.3 88.9 86.1 86.4 88.7 Table 3: Ablation Study of Reward Components on BIRD-Dev dataset. Reward Function Accuracy (%) Qwen2.5-Coder-7B Sf + Se + Sr + Sl - w/o Sf (Format Score) - w/o Se (Execution Score) - w/o Sr (Result Score) - w/o Sl (Length Score) 58.2 63.1 60.4 ( 2.7) 60.7 ( 2.4) 62.4 ( 0.7) 61.0 ( 2.1) BIRD (Dev) 58.2 57.0 66.1 63.1 59.2 66.6 of generating SQL queries. This observation substantiates that reinforcement learning can further improve the reasoning ability of the model in NL2SQL tasks. Above all, for Q1, SQL-R1 achieves superior performance on Spider and BIRD benchmarks, demonstrating the effectiveness of reinforcement learning in optimizing NL2SQL reasoning and outperforming models based on closed-source LLMs. This confirms the feasibility of designing an RL-based algorithm for NL2SQL tasks. 3.3 Analysis of SFT Cold Start In this experiment, we adopt three different baselines for comparison, namely the initial model that has not been pretrained, the model that uses the original format for SFT (OmniSQL-7B), and the model that uses the reinforced instruction format for SFT. As demonstrated in Table 2, coldstart training significantly enhances the performance of RL-based NL2SQL reasoning models. We observed improved instruction-following capabilities and SQL generation quality during the RL phase by implementing two forms of cold-start strategies. For Q2, these result confirms the necessity of cold-start training to optimize the models exploration and reasoning performance in complex database scenarios. At present, we are still conducting further experiments, and relevant results will continue to be updated in subsequent versions. For Q3, we currently employ small-scale subset of the SynSQL-2.5M dataset for the purpose of reinforcement training. This methodology demonstrates substantial advancements within the models, thereby showcasing the viability of utilizing synthetic data engineering to augment model inference capabilities in the NL2SQL domain. Subsequent publications will outline further strategies pertaining to synthetic data engineering. 3.4 Ablation Study of Reward Components We conducted an ablation experiment on the BIRD development dataset to verify the proposed reinforcement learning reward mechanisms effectiveness. The experiment sequentially removed individual reward components from the comprehensive reward function while maintaining the parameter settings established in Section 3.1. As presented in Table 3, the result demonstrates that omitting any reward component from the original reward function adversely impacts inference performance. This underscores the critical importance of execution feedback and result reward in the model training process. 7 Figure 2: Performance and model scale on the BIRD-Dev dataset."
        },
        {
            "title": "4 Limitations",
            "content": "At present, this study still has the following limitations: Supported Database Dialect The current research only supports training and evaluation on the SQLite dialect, and no further evaluation has been done on more database dialects (e.g., Snowflake, DuckDB)."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose SQL-R1, novel NL2SQL reasoning model trained via reinforcement learning (RL), addressing key challenges in semantic understanding, reasoning, and generalization for complex database scenarios. By integrating dynamic reward mechanisms, cold start strategies, and sustainable data engineering, SQL-R1 achieves state-of-the-art performance on benchmark datasets (88.6% accuracy on Spider-Test and 66.6% on BIRD) while generating interpretable reasoning traces. Our study demonstrates the effectiveness of RL in enhancing model generalization and reducing domain adaptation costs, providing transparency for high-risk applications. Future work will focus on improving model interpretability, expanding multi-table joint capabilities, and exploring synthetic data generation to support scalable training. This research advances the practical usability of NL2SQL systems by bridging the gap between reasoning capability and real-world applicability."
        },
        {
            "title": "References",
            "content": "[1] Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuyu Luo, Yuxin Zhang, Ju Fan, Guoliang Li, and Nan Tang. survey of NL2SQL with large language models: Where are we, and where are we going? CoRR, abs/2408.05109, 2024. [2] Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, and Xiao Huang. Next-generation database interfaces: survey of llm-based text-to-sql. arXiv preprint arXiv:2406.08426, 2024. [3] Yuyu Luo, Nan Tang, Guoliang Li, Wenbo Li, Tianyu Zhao, and Xiang Yu. Deepeye: data science system for monitoring and exploring COVID-19 data. IEEE Data Eng. Bull., 43(2):121132, 2020. [4] Yuyu Luo, Xuedi Qin, Chengliang Chai, Nan Tang, Guoliang Li, and Wenbo Li. Steerable self-driving data visualization. IEEE Trans. Knowl. Data Eng., 34(1):475490, 2022. [5] Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In AAAI, 2023. [6] Zhenbiao Cao, Yuanlei Zheng, Zhihao Fan, Xiaojin Zhang, Wei Chen, and Xiang Bai. Rsl-sql: Robust schema linking in text-to-sql generation. arXiv preprint arXiv:2411.00073, 2024. [7] Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, and Amin Saberi. Chess: Contextual harnessing for efficient sql synthesis. arXiv preprint arXiv:2405.16755, 2024. [8] Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. arXiv preprint arXiv:2304.11015, 2023. [9] Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Linzheng Chai, Zhao Yan, QianWen Zhang, Di Yin, Xing Sun, and Zhoujun Li. Mac-sql: multi-agent collaborative framework for text-to-sql, 2024. [10] Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023. [11] Peixian Ma, Boyan Li, Runzhi Jiang, Ju Fan, Nan Tang, and Yuyu Luo. plug-and-play natural language rewriter for natural language to sql. arXiv preprint arXiv:2412.17068, 2024. [12] Xinyu Liu, Shuyu Shen, Boyan Li, Nan Tang, and Yuyu Luo. Nl2sql-bugs: benchmark for detecting semantic errors in nl2sql translation. arXiv preprint arXiv:2503.11984, 2025. [13] Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. arXiv preprint arXiv:2402.01117, 2024. [14] Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):128, 2024. [15] Han Fu, Chang Liu, Bin Wu, Feifei Li, Jian Tan, and Jianling Sun. Catsql: Towards real world natural language to sql applications. Proceedings of the VLDB Endowment, 16(6):15341547, 2023. [16] Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: benchmark evaluation. CoRR, abs/2308.15363, 2023. [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [18] Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, et al. Fin-r1: large language model for financial reasoning through reinforcement learning. arXiv preprint arXiv:2503.16252, 2025. [19] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [20] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [21] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. 9 [22] Haoyang Li, Shang Wu, Xiaokang Zhang, Xinmei Huang, Jing Zhang, Fuxin Jiang, Shuai Wang, Tieying Zhang, Jianjun Chen, Rui Shi, et al. Omnisql: Synthesizing high-quality text-to-sql data at scale. arXiv preprint arXiv:2503.02240, 2025. [23] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [24] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: large-scale humanlabeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018. Association for Computational Linguistics. [25] Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. Can llm already serve as database interface? big bench for large-scale database grounded text-to-sqls, 2023. [26] Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, and Chang Zhou. Synthesizing text-to-sql data from weak and strong llms. arXiv preprint arXiv:2408.03256, 2024. [27] Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. Evaluating the text-to-sql capabilities of large language models. arXiv preprint arXiv:2204.00498, 2022. [28] Mohammadreza Pourreza, Hailong Li, Ruoxi Sun, Yeounoh Chung, Shayan Talaei, Gaurav Tarlok Kakkar, Yu Gan, Amin Saberi, Fatma Ozcan, and Sercan Arik. Chase-sql: Multi-path reasoning and preference optimized candidate selection in text-to-sql. arXiv preprint arXiv:2410.01943, 2024. [29] Masahiro Matsui, Takuto Sugisaki, Kensaku Okada, and Noboru Koshizuka. Alphasql: Open source software tool for automatic dependency resolution, parallelization and validation for sql and data. In 2022 IEEE 38th International Conference on Data Engineering Workshops (ICDEW), pages 3845. IEEE, 2022. [30] Shuai Lyu, Haoran Luo, Zhonghong Ou, Yifan Zhu, Xiaoran Shang, Yang Qin, and Meina Song. Sql-o1: self-reward heuristic dynamic search method for text-to-sql. arXiv preprint arXiv:2502.11741, 2025. [31] Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to sql: are we fully ready? arXiv preprint arXiv:2406.01265, 2024. [32] Shuozhi Yuan, Liming Chen, Miaomiao Yuan, Jin Zhao, Haoran Peng, and Wenming Guo. Mcts-sql: An effective framework for text-to-sql with monte carlo tree search. arXiv preprint arXiv:2501.16607, 2025. [33] Xiangjin Xie, Guangwei Xu, Lingyan Zhao, and Ruijie Guo. Opensearch-sql: Enhancing text-to-sql with dynamic few-shot and consistency alignment. arXiv preprint arXiv:2502.14913, 2025. [34] Asma Ben Abacha, Wen-wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, and Thomas Lin. Medec: benchmark for medical error detection and correction in clinical notes. arXiv preprint arXiv:2412.19260, 2024."
        },
        {
            "title": "A Detailed Case Study",
            "content": "Example for NL2SQL Reasoning - No RL Training - Category: Challenge Question: Tally the toxicology element of the 4th atom of each molecule that was carcinogenic. Lets break down the question and the database schema step by step to generate the appropriate SQL query. 1. **Identify the Relevant Tables and Columns:** - We need to identify the molecules that are carcinogenic (label = +). - We need to find the 4th atom of each molecule. - We need to determine the element of the 4th atom. - We need to tally the toxicology element. 2. **Filter Molecules by Carcinogenicity:** - We will use the molecule table to filter molecules where label = +. 3. **Find the 4th Atom of Each Molecule:** - We will use the atom table and the connected table to find the 4th atom. The 4th atoms atom_id will be the 7th character of the atom_id column (since atom_id follows the pattern TR000_1). 4. **Determine the Element of the 4th Atom:** - We will use the atom table to get the element of the 4th atom. 5. **Tally the Toxicology Element:** - We will use the element column from the atom table to tally the toxicology element. 6. **Construct the SQL Query:** - We will join the molecule, atom, connected, and atom tables to get the required information. Here is the SQL query that addresses the question: sql SELECT a.element FROM molecule JOIN atom ON m.molecule_id = a.molecule_id JOIN connected ON a.atom_id = c.atom_id WHERE m.label = + AND a.atom_id LIKE TR000_4 GROUP BY a.element; (cid:37) Explanation: - SELECT a.element: Selects the element of the 4th atom. - FROM molecule m: Joins with the molecule table to filter molecules by their label. - JOIN atom ON m.molecule_id = a.molecule_id: Joins with the atom table to get the atoms of each molecule. - JOIN connected ON a.atom_id = c.atom_id: Joins with the connected table to find the connections between atoms. - WHERE m.label = + AND a.atom_id LIKE TR000_4: Filters the atoms to only include the 4th atom (atom_id starting with TR000_4). - GROUP BY a.element: Groups the results by the element to tally the number of each element. This query will provide the tally of the toxicology element of the 4th atom of each carcinogenic molecule. Figure 3: Example for NL2SQL Reasoning - No RL Training - Challenge Sample 11 Example for NL2SQL Reasoning - RL Training - Category: Challenge Question: Tally the toxicology element of the 4th atom of each molecule that was carcinogenic. <think> To answer the given question, we need to follow these steps: 1. **Identify the Molecules that are Carcinogenic**: According to the question, molecules that are carcinogenic have label of + in the molecule table. 2. **Identify the 4th Atom of Each Molecule**: The 4th atom is identified by the atom_id where the substring of atom_id starting from the 7th character and having length of 1 is 4. 3. **Retrieve the Element of the 4th Atom**: Once we have identified the 4th atom, we need to retrieve the element associated with it from the atom table. 4. **Tally the Toxicology Element**: This step is not explicitly clear from the question, but it seems to imply that we need to count or list the elements of the 4th atoms of the carcinogenic molecules. Lets break down the steps into SQL queries: ### Step 1: Identify the Molecules that are Carcinogenic sql SELECT molecule_id FROM molecule WHERE label = +; ### Step 2: Identify the 4th Atom of Each Molecule To find the 4th atom, we need to filter the atom_id where the substring starting from the 7th character and having length of 1 is 4. We can use the SUBSTR function for this. sql SELECT atom_id FROM atom WHERE SUBSTR(atom_id, 7, 1) = 4; ### Step 3: Retrieve the Element of the 4th Atom We need to join the atom table with the connected table to find the 4th atom and then retrieve its element. sql SELECT atom.element FROM atom JOIN connected ON atom.atom_id = connected.atom_id WHERE SUBSTR(atom.atom_id, 7, 1) = 4; ### Step 4: Tally the Toxicology Element. To tally the toxicology element, we need to count or list the unique elements of the 4th atoms. We can use the GROUP BY clause to achieve this. sql SELECT atom.element FROM atom JOIN connected ON atom.atom_id = connected.atom_id WHERE SUBSTR(atom.atom_id, 7, 1) = 4 GROUP BY atom.element; ### Combining All Steps: We need to combine all the steps into single query that filters the carcinogenic molecules, identifies the 4th atom, retrieves its element, and tallies the unique elements. sql SELECT atom.element FROM atom JOIN connected ON atom.atom_id = connected.atom_id JOIN molecule ON atom.molecule_id = molecule.molecule_id WHERE SUBSTR(atom.atom_id, 7, 1) = 4 AND molecule.label = + GROUP BY atom.element; </think> <answer> sql SELECT atom.element FROM atom JOIN connected ON atom.atom_id = connected.atom_id JOIN molecule ON atom.molecule_id = molecule.molecule_id WHERE SUBSTR(atom.atom_id, 7, 1) = 4 AND molecule.label = + GROUP BY atom.element; </answer> Figure 4: Example for NL2SQL Reasoning - RL Training - Challenge Sample 12 Example for NL2SQL Reasoning - No RL Training - Category: Moderate Question: What is the display name of the user who last edited the post \"Examples for teaching: Correlation does not mean causation\" To determine the display name of the user who last edited the post with the title Ëxamples for teaching: Correlation does not mean causation, we need to follow these steps: 1. **Identify the relevant table**: The information about posts, including their titles and the user who last edited them, is stored in the posts table. 2. **Locate the correct post by title**: We need to filter the posts to find the one with the title Ëxamples for teaching: Correlation does not mean causation. 3. **Retrieve the last editors display name**: Once we have identified the post, we can access the LastEditorDisplayName column to get the display name of the user who last edited the post. Here is the SQL query that accomplishes this: sql SELECT LastEditorDisplayName FROM posts WHERE Title = Examples for teaching: (cid:37) This query selects the LastEditorDisplayName from the posts table where the Title matches the specified title. Correlation does not mean causation; Figure 5: Example for NL2SQL Reasoning - No RL Training - Moderate Sample Example for NL2SQL Reasoning - RL Training - Category: Moderate Question: What is the display name of the user who last edited the post \"Examples for teaching: Correlation does not mean causation\" <think> To answer this question, we need to find the display name of the user who last edited specific post. Here are the steps to derive the SQL query: 1. **Identify the Post**: We start by identifying the post with the title Ëxamples for teaching: Correlation does not mean causation. This information is available in the posts table under the Title column. 2. **Find the Last Editor**: The question asks for the user who last edited the post, which is referred to by LastEditorUserId in the posts table. This LastEditorUserId is foreign key referencing the Id column in the users table. 3. **Get the Display Name**: Once we have the LastEditorUserId, we need to find the corresponding display name of the user. This information is available in the users table under the DisplayName column. 4. **Formulate the Query**: - **Select the Required Column**: We need to select the DisplayName from the users table. - **Join the Tables**: We need to join the posts table (aliased as T1) with the users table (aliased as T2) to link the post to the user who last edited it. This is done using the LastEditorUserId in the posts table and the Id in the users table. - **Filter the Post**: We need to filter the posts to find the one with the title \"Examples for teaching: Correlation does not mean causation\". 5. **Construct the SQL Query**: - **SELECT**: We select the DisplayName from the users table. - **FROM**: We start from the posts table (aliased as T1). - **INNER JOIN**: We join the users table (aliased as T2) on the condition that T1.LastEditorUserId equals T2.Id. - **WHERE**: We filter the results to find the post with the title \"Examples for teaching: Correlation does not mean causation\". Accordingly, the final SQL query is: sql SELECT T2.DisplayName FROM posts AS T1 INNER JOIN users AS T2 ON T1.LastEditorUserId = T2.Id WHERE T1.Title = Examples for teaching: Correlation does not mean causation; </think> <answer> sql SELECT T2.DisplayName FROM posts AS T1 INNER JOIN users AS T2 ON T1.LastEditorUserId = T2.Id WHERE T1.Title = Examples for teaching: Correlation does not mean causation; </answer> Figure 6: Example for NL2SQL Reasoning - No RL Training - Moderate Sample 14 Example for NL2SQL Reasoning - RL Training - Category: Simple Question: How many female patients born in 1964 were admitted to the hospital? List them by ID. To answer this question, we need to generate SQL query that will filter the patients based on the given conditions and then count the number of patients who meet these conditions. Here are the steps to derive the SQL query: 1. **Identify the Relevant Table**: The information about patients, including their sex, birth year, and hospital admission status, is stored in the Patient table. 2. **Filter Conditions**: - The patient must be female, which is represented by SEX = F. - The patient must be born in 1964, which can be determined using the Birthday column and the strftime function to extract the year. - The patient must have been admitted to the hospital, which is represented by Admission = +. 3. **Count the Number of Patients**: Since the question asks for the number of patients, we will use the COUNT function to count the number of patient IDs that meet the specified conditions. 4. **Select the Patient IDs**: The question also asks to list the patients by ID, so we will include the ID column in the SELECT clause. Here is the SQL query that satisfies all these requirements: sql SELECT COUNT(ID) AS NumberOfPatients, ID FROM Patient WHERE SEX = AND strftime(%Y, Birthday) = 1964 AND Admission = +; (cid:37) This query will return the number of female patients born in 1964 who were admitted to the hospital and list their IDs Figure 7: Example for NL2SQL Reasoning - No RL Training - Simple Sample 15 Example for NL2SQL Reasoning - RL Training - Category: Simple Question: How many female patients born in 1964 were admitted to the hospital? List them by ID. <think> To generate the SQL query for the given question, we need to follow these steps: 1. **Identify the Relevant Table**: The question is about patients, so we need to focus on the Patient table. 2. **Determine the Conditions**: - femalerefers to SEX = F. - born in 1964refers to YEAR(Birthday) = 1964. However, SQLite does not have built-in YEAR function, so we need to use the strftime function to extract the year from the Birthday column. - ädmitted to the hospitalrefers to Admission = +. 3. **Select the Required Information**: The question asks for the IDs of the patients who meet the specified conditions. Therefore, we need to select the ID column from the Patient table. 4. **Formulate the SQL Query**: - Use the SELECT statement to specify that we want to retrieve the ID column. - Use the FROM clause to specify the Patient table. - Use the WHERE clause to apply the conditions: - SEX = for female patients. - strftime(%Y, Birthday) = 1964 to filter patients born in 1964. - Admission = + to filter patients who were admitted to the hospital. 5. **Construct the Final Query**: - Start with the SELECT statement to choose the ID column. - Use the FROM clause to specify the Patient table. - Use the WHERE clause to include the conditions: SEX = F, strftime(%Y, Birthday) = 1964, and Admission = +. Accordingly, the final SQL query is: sql SELECT ID FROM Patient WHERE SEX = AND strftime(%Y, Birthday) = 1964 AND Admission = +; </think> <answer> sql SELECT ID FROM Patient WHERE SEX = AND strftime(%Y, Birthday) = 1964 AND Admission = +; </answer> Figure 8: Example for NL2SQL Reasoning - RL Training - Simple Sample"
        },
        {
            "title": "B Prompt Templates for Training",
            "content": "Prompt Template for RL Training You are helpful SQL expert assistant. The assistant first thinks about how to write the SQL query by analyzing the question, database schema and external knowledge, then provides the final SQL query. The reasoning process and SQL query are enclosed within <think> </think> and <answer> </answer> tags respectively. The answer must contain the SQL query within sql... tags. Database Schema: {schema} External Knowledge: {external_knowledge} For example: <think> To translate the given natural language question into an executable SQLite query, we need to follow these detailed steps: 1. **Identify Key Elements**: The question queries for code snippets that are both complicated (complexity score > 5) and public (is_public = 1). We need to retrieve their descriptions and complexity scores. 2. **Focus on Relevant Tables**: The code_snippets table contains the necessary fields (description, complexity, is_public). 3. **Construct the Query**: We should select the required fields (description and complexity) from the code_snippets table. We also apply the conditions specified in the question to filter the results. 4. **Ordering**: The reference solution includes an ORDER BY clause to sort results by complexity in descending order, which is reasonable way to present the data to highlight the most complex snippets first. 5. **Final Query Construction**: Putting all this together into SQL query. </think> <answer> Heres how the query can be written: sql SELECT description, complexity FROM code_snippets WHERE complexity > 5 AND is_public = 1 ORDER BY complexity DESC; This query retrieves the descriptions and complexity scores of code snippets that are both complicated (complexity > 5) and publicly available (is_public = 1), sorted by complexity in descending order. This solution is straightforward and precisely matches the requirements of the question. It avoids unnecessary complexities, such as joining or selecting columns not relevant to the query itself. </answer> Question: {question} Figure 9: Prompt Template for RL Training 17 Prompt Template for SFT Training The user asks question about database, and the Assistant helps convert it to SQL.The assistant first thinks about how to write the SQL query by analyzing the question, database schema and external knowledge, then provides the final SQL query. The reasoning process and SQL query are enclosed within <think> </think> and <answer> </answer> tags respectively. The answer must contain the SQL query within sql tags. Database Schema: {schema} External Knowledge: {external_knowledge} User: {question} Figure 10: Prompt Template for SFT Training"
        }
    ],
    "affiliations": [
        "DataArc Tech Ltd.",
        "IDEA Research, International Digital Economy Academy",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "University of Chinese Academy of Sciences"
    ]
}