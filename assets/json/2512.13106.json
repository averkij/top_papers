{
    "paper_title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
    "authors": [
        "Shenzhi Yang",
        "Guangcheng Zhu",
        "Xing Zheng",
        "Yingfan MA",
        "Zhongqi Chen",
        "Bowen Song",
        "Weiqiang Wang",
        "Junbo Zhao",
        "Gang Chen",
        "Haobo Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 6 0 1 3 1 . 2 1 5 2 : r TRAPO: Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning Shenzhi Yang1,, Guangcheng Zhu1,, Xing Zheng2, Yingfan MA2, Zhongqi Chen2, Bowen Song2,, Weiqiang Wang2, Junbo Zhao1, Gang Chen1, Haobo Wang1, 1Zhejiang University 2Ant Group Equal contribution., Corresponding authors. Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the models internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate novel semi-supervised RLVR paradigm that utilizes small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm TRAPO that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TRAPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TRAPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TRAPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO."
        },
        {
            "title": "Introduction",
            "content": "The reinforcement learning with verifiable rewards (RLVR), pioneered by DeepSeek-R1 (Guo et al., 2025), has significantly advanced the development of large reasoning models (LRMs). In typical RLVR (Shao et al., 2024; Liu et al., 2025; Yu et al., 2025; Zheng et al., 2025), questions from training corpus are fed into an LRM, which then generates multiple reasoning paths (rollouts) per input. Rewards are computed based on verifiable rules: most commonly, whether the final answer in response matches the ground-truth label. By leveraging such an answer-verifiable structure, RLVR enables reward assignment through group-based advantage estimation, guiding the model to explore reasoning paths that lead to the correct final answer. However, when scaling to large corpora, the reliance of this reward paradigm on gold-standard labels incurs 1 Figure 1 Performance overview. We use six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro) for evaluation, and Qwen2.5-Math-7B for training. (Left) TRAPO surpasses fully supervised RLVR (45K samples) using just 10% (4K) annotated data. (Right) TRAPO scaling law: performance improves consistently with increasing sample sizes and varying annotation ratios. We only show the changes with sample size at 25% annotation rate in the figure; for other specific results, please see Table 12. prohibitively high annotation costs, making it difficult to generalize to specialized domains where groundtruth answers are scarce or expensive to obtain, such as medicine and finance (Wang et al., 2024b). To address this challenge, recent work has explored unsupervised RLVR methods (Zhang et al., 2025a; Zhao et al., 2025; Agarwal et al., 2025; Li et al., 2025a; Zuo et al., 2025; Zhang et al., 2025a) that aim to eliminate dependence on external supervision directly. These approaches are grounded in the observation that LRMs have already internalized substantial knowledge during pretraining (Ye et al., 2025); thus, the goal shifts from learning factual correctness to eliciting latent reasoning capabilities through self-guided exploration. In this framework, rewards are computed based on intrinsic signals such as self-certainty (Zhao et al., 2025), entropy (Agarwal et al., 2025), or majority voting (Zuo et al., 2025), to encourage high-confidence and consistent outputs. Despite their promise, these unsupervised methods often fail to capture valid reasoning patterns and tend to reinforce incorrect consensus, leading to severe performance degradation in late training. This drawback can be attributed to the absence of external ground truth: the reward signal becomes self-reinforcing and prone to reinforcing systematic biases, leading to degenerate feedback loop. Analogous to human learning, unsupervised RLVR resembles student solving problems based solely on current beliefs, treating the most confident answer as the ground truth. When incorrect, repeated reinforcement of the same reasoning path entrenches errors, leading to failure on both the current and related tasks. To break this vicious cycle, humans typically learn from few well-solved examples with verified solutions to establish correct conceptual foundation, then generalize via analogical reasoning. Therefore, we hypothesize that LRMs possess similar property: small number of verifiable labeled samples can enable LRMs to generalize patterns from larger amounts of unlabeled corpora. Inspired by this process, we propose Semi-supervised RLVR (SS-RLVR) paradigm that takes advantage of small set of labeled examples to anchor the reward signal, guiding the model toward reliable reasoning patterns and allowing more robust self-improvement. Figure 2 Comparison between different RLVR training paradigms. 2 Although promising in principle, our experiments show that simply combining supervised and unsupervised RLVR algorithms delivers only marginal benefits. For example, when combined with 3K entropy-based unlabeled RLVR training, the 1K supervised baseline only improves 0.6% accuracy. We argue that such failure stems from the neglect of internal links between labeled and unlabeled sets. In other words, only those reasoning patterns that are verified on labeled instances should be incorporated into RL training, and labeled data should be used as role models (Tarvainen and Valpola, 2017) to guide robust learning on unlabeled instances, as shown in Figure 2. Based on this key insight, we propose TRAPO (Trajectory-based Policy Optimization), which measures the similarity between unlabeled and labeled samples in terms of their pass rate trajectories and uses this alignment as criterion to select unlabeled samples with reliable pseudo-supervision for training. Experimental results demonstrate that TRAPO, trained with only 1K labeled and 3K unlabeled samples, achieves 4.3% improvement in in-domain performance over the strongest unsupervised baseline (trained on 45K unlabeled samples), 2.6% over the best naive semi-supervised method, and 3.2% over the supervised baseline (trained on 1K labeled samples). Notably, with 4K labeled and 12K unlabeled samples, TRAPO surpasses the fully supervised model trained on all 45K labeled samples across all benchmarks, using only 10% of the labeled data (see Figure 1, left). The scaling law for TRAPO (Figure 1, right) further demonstrates that with increased data and labeling ratio (e.g, 25%), TraPO achieves or approaches fully supervised performance without extra labels. These results strongly demonstrate TRAPOs ability to balance data efficiency and learning effectiveness."
        },
        {
            "title": "2 Related Work",
            "content": "Semi-supervised Learning leverages both labeled and unlabeled data to improve model performance, typically by exploiting data structure (Chapelle et al., 2009; Rasmus et al., 2015) or consistency assumptions (Laine and Aila, 2016; Berthelot et al., 2019; Xie et al., 2020; Sohn et al., 2020). In traditional classification tasks, outputs are drawn from shared discrete label space, enabling effective label propagation via feature similarity. However, in RLVR, each input has an instance-specific solution space, where correct outputs vary significantly across examples. This makes direct alignment of unlabeled samples with labeled ones through standard similarity-based methods impractical, posing key challenge in bridging labeled and unlabeled data for RLVR. Thus, in this paper, we turn from what the model learns to how it learns and employ the pass rate change trajectory as medium to bridge the gap. Unsupervised RLVR is built upon supervised RLVR, which has proven effective for aligning reasoning models in domains with executable or exact feedback, such as math and code (Hu et al., 2025; Guo et al., 2025; Shao et al., 2024), using deterministic, rule-based reward verifiers (Jaech et al., 2024). However, its reliance on outcome supervision limits applicability to tasks lacking clear ground truth. Recent work explores unsupervised RLVR, which uses intrinsic, self-generated signals to enable reward-free training. Methods include self-rewarding via judgment prompting (Wu et al., 2024; Yuan et al., 2024; Xiong et al., 2025) or ensemble heads (Wang et al., 2024c; Zhou et al., 2025), though often costly for online use. More scalable approaches leverage lightweight signalssuch as entropy (Agarwal et al., 2025), self-confidence (Li et al., 2025a), or majority voting (Zuo et al., 2025)to guide online policy updates (Zhang et al., 2025a; Zhao et al., 2025). However, purely unsupervised training risks model collapse due to biased or noisy signals reinforcing incorrect behaviors (Zhang et al., 2025c,b). Our work builds on this line by introducing new semi-supervised framework that anchors learning with labeled data to correct intrinsic signals, improving stability and generalization. Reasoning Data Selection is critical step in training LRMs, which can be broadly categorized into external and internal approaches. External methods rely on auxiliary resources such as human annotations (Li et al., 2022), knowledge bases (Nguyen et al., 2024), or proxy models (He et al., 2025a) to evaluate correctness and 3 Figure 3 TRAPO is semi-supervised RLVR training framework to dynamically select reliable unlabeled samples throughout the training process based on pass rate trajectory matching. confidence, but suffer from limited applicability due to dependency on external resources (Bi et al., 2025). In contrast, internal methods leverage model-internal signals, such as output probabilities (Plaut et al., 2024), semantic entropy (Kuhn et al., 2023), hidden representations (Wang et al., 2024a), or reward changes (Li et al., 2025b) to estimate data quality in label-free manner. Nevertheless, such metrics do not reflect the fundamental characteristics of data that are most beneficial for model learning. In this work, we go beyond superficial indicators by probing the intrinsic learning dynamics of the data, thereby identifying unlabeled instances that genuinely contribute to effective and robust model training."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present our semi-supervised reinforcement learning paradigm, which uses limited labeled data to guide reliable policy learning on large-scale unlabeled data. In Section 3.1, we discuss the limitations of supervised and unsupervised RLVR, and highlight the motivation for semi-supervised RLVR. In Section 3.2, we explore the bridge between labeled and unlabeled data, propose trajectory-based method to select reliable rewards and provide theoretical analysis on generalization."
        },
        {
            "title": "3.1 Semi-supervised Reinforcement Learning with Verifiable Rewards",
            "content": "In traditional RLVR, we assume access to large labeled dataset Dl = {(qi, yi)}Nl Supervised RLVR. i=1, where each sample consists of question qi and its corresponding verifiable ground-truth answer yi. For each question qi, we input it into policy model πθ to generate candidate outputs, denoted as {τ j=1. Given the ground-truth answer yi as supervision, we assign rewards to the generated responses based on whether they derive the correct answer. Specifically, we define binary reward function that evaluates the : final extracted answer from each output τ }G R(τ , yi) = I(τ , yi) = (cid:40) 1 0 if aj = yi, otherwise. (1) = extract(τ Here, aj ) denotes the answer extracted from the generated response τ , such as the content within boxed delimiters (e.g., boxed{}). With the ground-truth answers yi serving as explicit guidance signals, this Supervised RLVR paradigm reinforces only the responses that yield the correct answers; the policy model πθ is gradually steered toward discovering valid and consistent reasoning paths, thereby enabling stable and scalable policy optimization. Unsupervised RLVR. Although supervised RLVR has achieved great success, its reliance on golden answers yi incurs high annotation costs. To address this, the community has explored unsupervised RLVR techniques that rely solely on unlabeled data Du = {qi}Nu i=1. Under this setting, the absence of golden answers necessitates the use of proxy rewards Ru(τ , yi) based on the models confidence or consensus conf(). widely adopted method is majority voting, where the reward is defined as: ) that estimate R(τ Ru(τ = MAJ(a1 ) = conf(πθ(τ qi)) = I(aj where MAJ() denotes the pseudo-label obtained by majority answer among rollouts. This approach effectively treats the most frequently generated answer as the pseudo-label, providing form of self-supervised signal. Beyond majority voting, Zhao et al. (2025) use self-certainty, Agarwal et al. (2025) use token-level or sequence-level entropy as proxy for confidence, and compute rewards accordingly. Fundamentally, these methods are based on key assumption: higher confidence implies greater probability of producing the correct answer, and thus the higher the reward it should receive. , , aG , a2 )) (2) However, this assumption breaks down when the proxy reward diverges from actual correctness. Take the majority voting as an example, if the majority answer is not the correct answer, i.e., MAJ(a1 ) = yi, then the incorrect responses are reinforced. This creates dangerous feedback loop: the policy becomes more confident in the wrong answer, leading to even stronger wrong consensus in subsequent iterations. Over time, the model converges to state where it confidently produces incorrect outputs. , , aG Semi-supervised RLVR. To break this vicious loop induced by the absence of grounded feedback, we hypothesize that we must introduce labeled examples to anchor the reward to ground truth. Formally, we adopt hybrid reward function that computes rewards differently for labeled and unlabeled data: Rsemi(τ ) = (cid:40) R(τ Ru(τ , yi), ), if (qi, yi) Dl, if qi Du. (3) Here, labeled data are used to compute rewards under supervision from the ground-truth labels yi, while unlabeled data can adopt any self-consistency-based reward we have stated previously. Since the reward R(τ , yi) of labeled data is independent of the models consensus, this training paradigm introduces crucial distinction between correctness (alignment with ground truth) and self-consistency (internal agreement among outputs), thereby preventing the policy from reinforcing incorrect but internally consistent outputs. The design of our Semi-supervised RLVR (SS-RLVR) framework stems from the inherent trade-off between data efficiency and learning effectiveness. Compared to unsupervised variants, SS-RLVR effectively guides robust learning on unlabeled instances by using labeled data as reliable anchor. In contrast to fully supervised approaches, it significantly reduces the need for costly annotationour experiments show that SS-RLVR achieves performance close to supervised learning using only 10% of the labeled data. In practice, this trade-off not only directly reduces the annotation burden, but also enables high-quality data synthesis within iterative refinement pipelines, thereby improving data quality over time. This makes SS-RLVR particularly attractive for domains where labeled data is scarce or expensive to obtain, such as medicine and finance."
        },
        {
            "title": "3.2 Progressive Trajectory Guidance for Bridging Labeled and Unlabeled Data",
            "content": "Despite its promise, we show that trivial baseline that simply combines supervised and unsupervised RLVR algorithms delivers only marginal benefits. For example, when supplemented with 3K entropy-based unlabeled RLVR training, the 1K supervised baseline achieves merely 0.6% accuracy improvement. This suggests that such naive strategy remains constrained by the internal signals of LRMs and suffers from the internal ungrounded reasoning patterns. Thus, SS-RLVR must move beyond shallow integration and instead uncover the deeper intrinsic relationships between labeled and unlabeled data. However, large language models in RLVR tasks differ fundamentally from traditional semi-supervised settings. The semantic independence between samples makes it impractical to establish relationships among their answers through embeddings. Thus, the key is to exploit those reasoning patterns in unlabeled data that can be externally validated by labeled examples. To achieve this goal, it is required to identify shared, meaningful signal that transcends the heterogeneity of solution spaces and reliably reflects the models ability to transfer knowledge from labeled to unlabeled data. In this work, we propose TRAPO (Trajectory-based Policy Optimization), which leverages the learning dynamics of LRMs across training steps as proxy to connect labeled and unlabeled data, as shown in Figure 3. Specifically, at each step t, TRAPO computes the pass rate for each training point. We then identify those unlabeled samples whose pass rate trajectories closely align with those of labeled samples as reliable data, which means that their reasoning patterns can be externally validated by the labeled set. In other words, we hypothesize that when an unlabeled sample is well-learned, its pass rate trajectory should exhibit trends consistent with those observed in labeled data. Naturally, since pass rates cannot be directly computed for unlabeled data, we introduce pseudopass rate approximation to serve as proxy. Formally, for question at epoch t, the (pseudo) pass rate is defined as the fraction of generated responses that satisfy the expected answer criteria: (t) = (cid:40) 1 1 i= i=1 I(a I(a (t) ), (t) = (t) = y), Du, Dl, (4) Then, we define the pass rate trajectory of question as the sequence of its pass rates across training epochs: (cid:104) (t) = (1) (2) , (t) , . . . , (cid:105) [0, 1]t, (5) (0) = [ ] and updated iteratively via concatenation: , where denotes initialized as sequence concatenation. We maintain reliable pass rate database Dreliable, initialized with all labeled sample trajectories: D(0) reliable = {Tl Dl} . Reliably pseudo-labeled trajectories from unlabeled data selected in subsequent steps are added to update this database. The average trajectory of this database, TDreliable T, serves as trusted reference for assessing the reliability of unlabeled samples based on trajectory alignment. Then we compute trajectory-based cosine similarity (TCS) as: (t) reliable = 1 Dreliable (t) = (t) (t1) TCS(T (t) , (t) reliable) = ˆT (t) ˆT (t) reliable = j= (j) ˆP ˆP (j) reliable (6) (j) = where ˆP (j) i=1(P and the reliable database, respectively. and ˆP (j) reliable = (i) )2 (cid:113) (cid:113) (j) reliable (i) i=1( reliable)2 are the normalized pass rate of the unlabeled sample To select the reliable trajectories, we combine two criteria: the top-p of unlabeled samples with highest trajectory similarity to the labeled data, and any sample whose similarity exceeds threshold Γ. M(u) = (u top-p (TCS(Tu, Treliable))) (TCS(Tu, Treliable) Γ) (7) 6 With this selection mask in hand, we now integrate it into the training process to ensure only reliably improving samples influence model updates. To ensure stability, we employ warm-up phase using only labeled data for updates, while accumulating unlabeled trajectories. After warm-up, we apply the mask to include only reliable unlabeled samples: L(θ) = labeled GRPO (θ) + unlabeled GRPO (θ). where denotes the dot product of vectors. Here, JGRPO is the GRPO objective (Shao et al., 2024): JGRPO(θ) = 1 i=1 τi i=1 τi l= CLIP(γi,l(θ), Ai, ϵ) β DKL[πθπref] (8) (9) where γi,l(θ) = πθ(τi,lq, τi,<l)/πθold(τi,lq, τi,<l) is the importance sampling term, and CLIP(γ, A, ϵ) = min[r A, clip(γ; 1 ϵ, 1 + ϵ) A] is the clipped surrogate objective. In summary, we propose leveraging the evolution of correctness during training (pass rate trajectories) as reliable signal for evaluating unlabeled samples. By measuring the similarity between the pass rate trajectory of an unlabeled instance and the average trajectory derived from labeled data, we identify samples whose learning dynamics align closely with those observed under trusted supervision. To validate the effectiveness of TRAPO in selecting high-quality unlabeled samples and grounding unsupervised learning within stable feedback framework, we provide theoretical analysis of its generalization error bound: Theorem 3.1 (Trajectory-Consistent Generalization). (Informal) Let the generalization error of policy (t) θ be the expected risk on the true distribution. Assuming Ly is the label space diameter, under the π TRAPO framework, with probability at least 1 δ, this error is bounded by:"
        },
        {
            "title": "RDl",
            "content": "(π (t) θ ) + λ + α qDu (cid:104) 1 TCS(cid:0)T (t) , (t) reliable (cid:1)(cid:105) (cid:32) + Ly 1 C(t) + (cid:33) (cid:114) ln(2n/δ) 2G (10) where RDl and Du, and C(t) is the average voting confidence across samples based on votes. (t) θ ) is the empirical risk on Dl, λ = λ + λd 0 bounds the domain shift between Dl (π Theorem 3.1 highlights the role of trajectory consistency as regularizer in semi-supervised policy learning. Specifically, the term encourages unlabeled samples to follow learning dynamics similar to those of labeled data, effectively anchoring the optimization path. The dependence on C(t) reflects the models self-confidence during training, with lower confidence leading to looser bound, thus promoting cautious updates. The formal theorem and its proof are presented in Appendix A.13. 1 TCS(cid:0)T (t) reliable (t) , qDu (cid:1)(cid:105) (cid:104)"
        },
        {
            "title": "4 Experiment",
            "content": "This section reports the main experimental results. Appendix D.1 compares more fully supervised baselines; D.2 further validates TraPO on more models; D.3 shows that TraPO is plug-and-play; D.4 evaluates TraPO on the DeepMath dataset (He et al., 2025b); D.5 compares TraPO with other selection strategies; D.6 confirms TraPOs stability; D.7 analyzed the training cost of TraPO; D.8 analyzed different ways of utilizing reliable passrate databases."
        },
        {
            "title": "4.1 Setup",
            "content": "Dataset and Benchmarks. We follow prior work Yan et al. (2025) and use the widely used math reasoning dataset OpenR1-Math-220k (Face, 2025) for training. For evaluation, we focus on six in-distribution (ID) 7 Table 1 Overall performance based on Qwen2.5-Math-7B under three different training paradigms. Bold and underline indicate the best and second-best results, respectively. Model In-Distribution Performance Out-of-Distribution Performance AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Original Models Qwen-Base Qwen-Instruct 11.5/4.9 12.5/10. 31.3 48.5 43.6 80.4 7.4 32. 15.6 41.0 19.0 18.2 37.6 70. TTRL Self-certainty Token-level Entropy Sentence-level Entropy Unsupervised Methods Trained on 45K Samples w/o Any Labels 14.1/12. 16.9/10.2 15.0/9.9 11.4/10.7 51.5 51.7 50. 42.1 76.6 77.6 75.2 68.0 33. 34.9 36.8 32.7 40.3 38.8 38. 30.5 38.2 80.5 38.3 72.9 37. 75.6 32.6 79.4 11.1 24.7 35. 30.8 33.3 32.3 Semi-supervised Methods Trained on 1K Labeled Samples & 3K Unlabeled Samples Fully Supervised w/ 1K Labels 14.2/13. TTRL Self-certainty Token-level Entropy Sentence-level Entropy TRAPO (ours) 14.9/10. 16.5/11.4 18.2/11.9 15.4/11.5 17.9/13.8 Fully Supervised w/ 4K Labels 19.6/14. 52.6 55.3 55.6 53.4 54.9 58. 57.9 80.2 77.8 79.8 80.2 79. 81.4 80.6 34.9 33.1 35.3 34. 36.0 38.2 39.3 40.9 43.6 41. 41.9 41.2 45.5 46.5 39.4 76. 39.2 72.6 40.0 64.8 40.0 72. 39.7 79.4 42.6 83.7 43.1 82. 36.4 35.4 30.3 32.3 33.8 37. 39.9 TRAPO Trained on 4K Labeled Samples & 12K Unlabeled Samples TRAPO (ours) 24.3/17.1 60.0 Fully Supervised w/ 45K Labels 25.1/15.3 62.0 84.6 84.4 39.3 39. 48.3 45.6 84.6 43.9 46.8 45. 82.3 40.4 16.9 34.1 41.3 41. 40.9 42.7 43.6 42.7 41.6 44. 44.5 46.8 48.2 50.7 49.3 15. 43.0 52.4 48.4 49.9 51.5 52. 50.2 45.6 49.7 52.6 56.1 56. 59.7 57.3 math reasoning benchmarks: AIME 2024, AIME 2025, AMC (Li et al., 2024), Minerva (Lewkowycz et al., 2022), OlympiadBench (He et al., 2024), and MATH-500 (Hendrycks et al., 2021). We report avg@32 on AIME 2024/2025 and AMC (due to small test sets) and pass@1 on the others. For out-of-distribution (OOD) generalization, we evaluate on ARC-c (Clark et al., 2018), GPQA-diamond (Rein et al., 2024) (GPQA), and MMLU-Pro (Wang et al., 2024b), covering open-domain reasoning, graduate-level science, and academic reasoning. All evaluations use temperature sampling with = 0.6. Baseline Methods. We evaluate supervised, unsupervised, and semi-supervised RLVR methods across varying data scales. For supervised training, we apply GRPO on 1K, 4K, and 45K labeled samples. In the unsupervised setting, we remove ground-truth labels from the full 45K dataset and evaluate four approaches: (1) TTRL (Zuo et al., 2025), which uses majority-voted outputs as pseudo-labels; (2) Self-Certainty (Zhao et al., 2025), which maximizes KL divergence to encourage confident predictions; (3) Token-Level Entropy (Agarwal et al., 2025), which minimizes token-level entropy for consistency; and (4) Sentence-Level Entropy (Agarwal et al., 2025), which maximizes sentence likelihood. For semi-supervised training, we use 1K labeled and 3K unlabeled samples, applying GRPO on the labeled subset and each unsupervised method on the unlabeled subset to form hybrid baselines. We further evaluate stronger setting with 4K labeled and 12K unlabeled samples to assess performance under higher label efficiency. In Appendix D.1, we compare with more supervised baselines (Zeng et al., 2025b; Hu et al., 2025; Cui et al., 2025; Liu et al., 2025). 8 Table 2 Performance of different training paradigms with 1K labeled math (ID) samples and 1K unlabeled non-math (OOD) samples. Bold and underline indicate the best and second-best results, respectively. Model In-Distribution Performance Out-of-Distribution Performance AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Original Model Qwen-Base Qwen-Instruct 11.5/4.9 12.5/10.2 31.3 48.5 43. 80.4 7.4 32.7 15.6 41.0 19. 18.2 37.6 70.3 11.1 24.7 Unsupervised Methods Trained on 1K Unlabeled ID Samples & 1K Unlabeled OOD Samples TTRL Self-certainty Token-level Entropy Sentence-level Entropy 13.3/9.4 18.5/9. 14.6/13.3 16.4/11.5 48.2 53.4 46.8 51. 72.2 79.6 77.6 74.0 27.6 33. 27.9 33.5 34.8 40.4 40.1 37. 34.3 76.7 39.2 76.7 36.7 74. 37.4 74.5 33.8 37.9 36.4 34. Semi-supervised Methods Trained on 1K Labeled ID Samples & 1K Unlabeled OOD Samples TTRL Self-certainty Token-level Entropy Sentence-level Entropy TRAPO (ours) 16.4/13.6 16.0/10.9 17.7/11.0 15.7/10.0 18.5/15.7 Fully Supervised w/ 2K Labels 17.3/12.4 49.9 53.0 51.7 51.4 53. 56.8 66.9 78.4 77.0 77.4 80. 81.4 26.5 34.2 33.1 34.9 33. 38.6 37.8 39.0 41.0 37.5 44. 44.8 35.2 62.0 38.6 77.1 38. 76.5 37.8 41.0 75.1 83.6 41. 82.0 31.8 32.8 30.8 31.3 38. 38.9 16.9 34.1 36.2 45.6 35. 43.3 43.5 45.7 44.7 44.3 48. 52.4 15.4 43.0 48.9 53.4 48. 50.9 45.8 51.9 50.7 50.2 56. 57.8 Figure 4 Left: Average performance changes on labeled and unlabeled data. Center: Unlabeled data performance vs. trajectory matching score using true training dynamics on unlabeled data. Right: Unlabeled data performance vs. trajectory matching score using pseudo training dynamics on unlabeled data."
        },
        {
            "title": "4.2 Experimental Results",
            "content": "TRAPO achieves SOTA performance. Our main results are summarized in Table 1. First, TRAPO significantly outperforms all fully unsupervised baselines using only 1K labeled samples (with 3K unlabeled). Compared to the best unsupervised method trained on the full 45K unlabeled set, TRAPO achieves gains of 4.3% in ID and 3.7% in OOD accuracy, demonstrating that even minimal labeled data can lead to substantial improvements when effectively integrated. Second, TRAPO outperforms naive semi-supervised approaches that treat labeled and unlabeled data independently, improving the strongest such baseline by 2.6% (ID) and 3.5% (OOD), which underscores the importance of using labels to actively guide the learning from unlabeled examples. Finally, TRAPO surpasses the fully supervised model trained on the same 1K labels by 3.2% (ID) and 4.0% (OOD). It matches the performance of fully supervised model trained on 4K labels while using only 25% of the labeled data. Notably, when trained with 4K labeled and 12K unlabeled samples, TRAPO 9 Figure 5 Sensitivity Analysis. The left three plots show sensitivity analyses of top-p, Γ, and warmup epochs (Tables 9, 10, and 11 in the Appendix). The right two plots compare performance for different ratios of selected and available unlabeled samples (3K σM). See tables 14 and 15 in the Appendix for details. achieves 45.6 ID and 59.7 OOD accuracy, exceeding the fully supervised model trained on all 45K labels by 0.1% (ID) and 2.4% (OOD), despite using only 10% of the total labels. This remarkable performance highlights TRAPOs superior data efficiency and generalization capability. TRAPO succeeds with OOD unlabeled data. To investigate whether labeled data can guide learning on out-of-domain (OOD) unlabeled data, we evaluate semi-supervised setup with 1K labeled samples from the mathematics domain (ID) and 1K unlabeled samples from non-mathematical domains (OOD). This cross-domain setting is challenging due to the limited transfer of reasoning patterns across domains. As shown in Table 2, naive semi-supervised methods fail to benefit from labeled data well. For instance, self-certainty drops by 0.6% on ID and 1.5% on OOD, indicating that naive integration of labeled and unlabeled data harms learning under domain shift. In contrast, TRAPO achieves significant improvements, outperforming the best unsupervised baseline by 1.8% on ID and 3.5% on OOD. It also closely matches the fully supervised model with 2K labels, trailing by only 0.9% on both metrics. The substantial gain in OOD performance demonstrates that TRAPO enables robust cross-domain generalization, highlighting its strong ability to transfer reasoning knowledge even under domain discrepancy. Effectiveness of trajectory matching. To evaluate whether trajectory matching identifies reliable unlabeled examples, we analyze the link between trajectory similarity and performance. As shown in the middle plot of Figure 4, samples with dynamics more aligned to labeled data achieve much higher performance. The top 10% of samples outperform the bottom 10% by over 40%, confirming that alignment correlates with reliability. In practice, we use pseudo-labels from voting to estimate unlabeled sample dynamics. The right plot of Figure 4 shows that matching pseudo dynamics to true labeled dynamics still yields strong positive correlation with final performance. This validates the robustness and practical utility of TRAPO. Sensitivity analysis. We systematically analyze the impact of top-p, Γ, and warm-up length with the Qwen2.5-7B model using 1K labeled and 3K unlabeled samples (left three plots in Figure 5). For top-p, larger values lead to noisy early-stage predictions and unreliable pseudo-labels, degrading overall performance. For Γ, setting it too low admits too many low-quality unlabeled samples, while setting it too high is overly conservative, leading to underutilization; both extremes harm the model. Short warm-up lengths lead to unstable pseudo-labeling, but performance stabilizes as the warm-up lengthens. With different selection ratios and varying proportions (σM) of available unlabeled samples, TraPO outperforms random selection and strong token-level entropy baseline (the right two plots in Figure 5). We find that TraPO achieves optimal Figure 6 Performance comparison on Llama-3.1-8B. 10 results using the top 30% of unlabeled samples, benefiting from high pseudo-label accuracy, whereas adding more unlabeled samples increases noise and reduces gains. These experiments highlight the critical role of intelligent denoising and selection strategies. Experiments with other LLMs. Besides Qwen, we also compare the training effectiveness of the three paradigms using the Llama-3.1-8B-Instruct model. The model performance during training is shown in Figure 6, and detailed results are presented in Table 5. Here, our semi-supervised TRAPO method exhibits similar trend to supervised training and maintains consistent improvement. In contrast, unsupervised training leads to rapid performance collapse within tens of training steps. This underscores the critical importance of effective pseudo-supervision selection via trajectory matching in stabilizing the training process."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present the first exploration of semi-supervised learning in the RLVR setting. We introduce novel paradigm that leverages small set of labeled data to guide robust self-improvement on unlabeled data. We propose TRAPO (Trajectory based Policy Optimization), method that enables reliable pseudosupervision by aligning the learning dynamics of labeled and unlabeled samples through trajectory similarity in pass rate progression. Results show TRAPO significantly outperforms various baselines using only fraction of labeled data, achieving an exceptional balance between efficiency and effectiveness."
        },
        {
            "title": "References",
            "content": "Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025. Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, Russ Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. Advances in neural information processing systems, 32, 2019. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. theory of learning from different domains. Machine learning, 79(1):151175, 2010. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019. Jinhe Bi, Danqi Yan, Yifan Wang, Wenke Huang, Haokun Chen, Guancheng Wan, Mang Ye, Xun Xiao, Hinrich Schuetze, Volker Tresp, et al. Cot-kinetics: theoretical modeling assessing lrm reasoning process. arXiv preprint arXiv:2505.13408, 2025. Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92100, 1998. Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542542, 2009. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Miroslav Dudík, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. arXiv preprint arXiv:1103.4601, 2011. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. https://github.com/huggingface/ open-r1. Chelsea Finn, Tianhe Yu, Justin Fu, Pieter Abbeel, and Sergey Levine. Generalizing skills with semi-supervised reinforcement learning. arXiv preprint arXiv:1612.00429, 2016. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, 2024. Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, et al. Can large language models detect errors in long chain-of-thought reasoning? arXiv preprint arXiv:2502.19361, 2025a. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025b. Marzi Heidari, Hanping Zhang, and Yuhong Guo. Reinforcement learning guided semi-supervised learning. Advances in Neural Information Processing Systems, 37:136990137009, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. https://arxiv.org/abs/2503.24290. Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy evaluation in markov decision processes. Journal of Machine Learning Research, 21(167):163, 2020. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023. Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q. Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. https://huggingface.co/datasets/Numinamath, 2024. Hugging Face repository, 13:9. Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan Oseledets. Confidence is all you need: Few-shot rl fine-tuning of language models. arXiv preprint arXiv:2506.06395, 2025a. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling. arXiv preprint arXiv:2502.11886, 2025b. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making large language models better reasoners with step-aware verifier. arXiv preprint arXiv:2206.02336, 2022. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Minh-Vuong Nguyen, Linhao Luo, Fatemeh Shiri, Dinh Phung, Yuan-Fang Li, Thuy-Trang Vu, and Gholamreza Haffari. Direct evaluation of chain-of-thought in multi-hop reasoning with knowledge graphs. arXiv preprint arXiv:2402.11199, 2024. Benjamin Plaut, Nguyen Khanh, and Tu Trinh. Probabilities of chat llms are miscalibrated but still predict correctness on multiple-choice q&a. arXiv preprint arXiv:2402.13213, 2024. Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised learning with ladder networks. Advances in neural information processing systems, 28, 2015. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. https://openreview.net/forum?id=Ti67584b98. 12 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. https: //arxiv.org/abs/2402.03300. Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596608, 2020. Amarnag Subramanya and Jeff Bilmes. Semi-supervised learning with measure propagation. Journal of Machine Learning Research, 12(11), 2011. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semisupervised deep learning results. Advances in neural information processing systems, 30, 2017. Meta Team. The llama 3 herd of models, 2024. https://arxiv.org/abs/2407.21783. Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International conference on machine learning, pages 21392148. PMLR, 2016. Yiming Wang, Pei Zhang, Baosong Yang, Derek Wong, and Rui Wang. Latent space chain-of-embedding enables output-free llm self-evaluation. arXiv preprint arXiv:2410.13640, 2024a. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024b. Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, and Huaxiu Yao. Cream: Consistency regularized self-rewarding language models. arXiv preprint arXiv:2410.12735, 2024c. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024. Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. Advances in neural information processing systems, 33:62566268, 2020. Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. Self-rewarding correction for mathematical reasoning. arXiv preprint arXiv:2502.19613, 2025. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. https://arxiv.org/abs/2409.12122. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 3, 2024. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025a. https://arxiv.org/abs/2503.18892. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025b. Notion Blog. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025a. Yanzhi Zhang, Zhaoxi Zhang, Haoxiang Guan, Yilin Cheng, Yitong Duan, Chen Wang, Yue Wang, Shuxin Zheng, and Jiyan He. No free lunch: Rethinking internal feedback for llm reasoning. arXiv preprint arXiv:2506.17219, 2025b. 13 Zizhuo Zhang, Jianing Zhu, Xinmu Ge, Zihua Zhao, Zhanke Zhou, Xuan Li, Xiao Feng, Jiangchao Yao, and Bo Han. Co-reward: Selfsupervised reinforcement learning for large language model reasoning via contrastive agreement. arXiv preprint arXiv:2508.00410, 2025c. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Xin Zhou, Yiwen Guo, Ruotian Ma, Tao Gui, Qi Zhang, and Xuanjing Huang. Self-consistency of the internal reward models improves self-rewarding language models. arXiv preprint arXiv:2502.08922, 2025. Zhengyuan Zhou, Susan Athey, and Stefan Wager. Offline multi-action policy learning: Generalization and optimization. Operations Research, 71(1):148183, 2023. Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. geometric analysis of neural collapse with unconstrained features. Advances in Neural Information Processing Systems, 34:2982029834, 2021. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "Notation",
            "content": "J ri {0, 1} Jpref N+, p+, ˆAi,l ri,l(θ) clip(, 1 ε) kq) log πt(τ Θ((q, τ), (q, τ)) Θ++ > 0, Θ > Orthogonal gradients (t) traj(q, q) sign( log πt) = +1 dHH(Dl, Du) Table 3 Table of Notations and Descriptions Optimization and Reward Setup"
        },
        {
            "title": "Description",
            "content": "Group Relative Policy Optimization (GRPO): policy update via response grouping and relative advantage. Binary reward: 1 for correct, 0 for incorrect response. Equivalent preference optimization objective under binary rewards. Empirical accuracy: fraction of correct responses in batch. Expected number of correct and incorrect responses: N+ = pN, = (1 p)N. Group-specific weights: p+ = 1p ˆAi,l = rip Advantage estimator: p(1p) . , = p(1p) . p(1p) Probability ratio between current and old policy for token generation. Clipping function to stabilize policy updates. after update. Generalization and NTK Analysis Change in log-probability of response τ Response-level NTK: θ log π(τq), θ log π(τq). Gradient alignment: correct-correct and error-error responses align. Correct and incorrect response gradients are orthogonal. Trajectory divergence: 1 cos between response pass rate. Positive generalization: similar questions benefit from training."
        },
        {
            "title": "Convergence and Risk Bounds",
            "content": "Domain discrepancy: maximum distinguishability under H. dHH αE[Dtraj] + λd Trajectory divergence bounds domain shift. (t) θ ) RDu (π R(t) TC C(t) Generalization risk on target domain. Dynamic trajectory consistency risk: αE[D Average confidence (e.g., pass rate) at iteration t. Expected target risk, used in convergence analysis. (t) traj] + Ly(1 C(t)). Ut = E[RDu (π Ut+1 Ut ηtξt + βt Monotonic convergence inequality under consistent learning. (t) θ )] Residual term: includes Dtraj, C, and η2 M2. βt"
        },
        {
            "title": "A Theoretical Proof",
            "content": "In this section, we provide proofs for the generalization error bound and convergence of the proposed semi-supervised framework TRAPO. A.1 Notion We provide the notions used in the proof in Table 3. 15 A.2 GRPO as Preference Optimization We begin by formally establishing that GRPO performs preference optimization between correct and incorrect responses when the reward is binary. Lemma A.1 (GRPO as Preference Optimization). When the reward is binary (ri {0, 1}), the expected GRPO loss for question reduces to weighted preference optimization objective: Jpref = p+ N+ i=1 min (cid:32) πθ(τ+ πθold(τ+ q) q) (cid:33) , 1 + ε N j=1 max where: (cid:32) πθ(τ πθold(τ q) q) (cid:33) , 1 ε , (11) = 1 N+ = pN, = (1 p)N are the expected number of correct and incorrect responses in batch of i=1 1[ri(q) = 1] is the empirical correctness rate for q, samples, p+ = 1p p(1p) , = p(1p) are the group-specific weights. Proof. The standard GRPO loss for batch of responses {τi}N i=1 is: = i=1 τi l=1 min (cid:0)ri,l(θ) ˆAi,l, ˆAi,l clip(ri,l(θ), 1 ε, 1 + ε)(cid:1) , where ri,l(θ) = πθ (τi,l q,τi,<l ) πθold (τi,l q,τi,<l ) is the probability ratio at token l, and ˆAi,l is the advantage estimator. For binary rewards, ri(q) = ri,l = 1 if the response τi is correct, and 0 otherwise. The advantage ˆAi,l is defined as: ˆAi,l = ri ˆµ ˆσ , where ˆµ = is the empirical mean reward (correctness rate), and ˆσ = (cid:112) deviation. p(1 p) is the empirical standard Thus, the advantage simplifies to: ˆAi,l = 1p p(1p) p(1p) = p+ if ri = 1 (correct), = if ri = 0 (incorrect). Now, consider the term in the loss: min (cid:0)ri,l(θ) ˆAi,l, ˆAi,l clip(ri,l(θ), 1 ε, 1 + ε)(cid:1) . We analyze this based on the sign of ˆAi,l: ˆAi,l > 0 (ri = 1, correct response) Case 1: In this case, the min function simplifies to: ˆAi,l min (ri,l(θ), 1 + ε) = p+ min (cid:18) πθ(τi,l q, τi,<l) πθold(τi,l q, τi,<l) (cid:19) . , 1 + ε 16 Summing over all tokens in the response τ+ we have (in the limit of small learning rate or by ignoring token normalization): , and noting that τ+ l=1 log πθ(τi,lq, τi,<l) = log πθ(τ+ q), τ+ l=1 min() p+ min (cid:32) πθ(τ+ πθold(τ+ q) q) (cid:33) , 1 + ε . ˆAi,l < 0 (ri = 0, incorrect response) Case 2: Here, ˆAi,l = p, and the min function becomes: min (cid:0)pri,l(θ), clip(ri,l(θ), 1 ε, 1 + ε)(cid:1) = max (ri,l(θ), 1 ε) , because min(a, b) = max(a, b). Summing over tokens: τ l=1 min() max (cid:32) πθ(τ πθold(τ q) q) (cid:33) , 1 ε . Taking the expectation over the response batch {τi}N N+ = pN correct and = (1 p)N incorrect responses on average, we obtain the expected loss: i=1 πθold(q), and using the fact that there are E[J ] = p+ N+ i=1 min (cid:32) πθ(τ+ πθold(τ+ q) q) (cid:33) , 1 + ε N j=1 max (cid:32) πθ(τ πθold(τ q) q) (cid:33) , 1 ε . This is exactly the preference optimization objective in 11. This completes the proof of A.1. A.3 Gradient Dynamics and NTK Alignment We now analyze how training on question affects the models behavior on another question q, leveraging the NTK framework. A.3.1 Change in Log-Probability We start by deriving the change in the log-probability of generating response τ update on question q. kq) be the Proposition A.2 (Gradient Update Effect). Let log πt(τ change in log-probability after one GRPO update on q. Under the assumption that the parameter update θt+1 θt is small and given by the SGD update on q, we have: to question after GRPO kq) = log πt+1(τ kq) log πt(τ log πt(τ q) = log πt(τ q), p+ (cid:42) N+ i=1 log πt(τ+ q) (cid:43) log πt(τ q) . (12) j=1 Proof. Using first-order Taylor expansion of log πθ(τ kq) = log πt(τ log πt+1(τ kq) + θ log πt(τ (cid:68) kq) around θt: kq), θt+1 θt(cid:69) + O(θt+1 θt2). The parameter update θt+1 θt is proportional to the negative gradient of the GRPO loss on q. From A.1, the loss gradient is: θJq = p+ (cid:34) (cid:32) θ min N+ i= πθ(τ+ πθold(τ+ q) q) (cid:33)(cid:35) , 1 + ε (cid:34) θ max j=1 17 (cid:32) πθ(τ πθold(τ q) q) (cid:33)(cid:35) , 1 ε . In the \"nearly online\" setting of GRPO, where responses are resampled at each iteration, we assume πθ πθold, so the ratios are close to 1. In this case, the min and max operators are inactive (i.e., the clipping does not bind), and we have: (cid:34) θ min (cid:34) θ max (cid:32) πθ(τ+ πθold(τ+ (cid:32) πθ(τ πθold(τ q) q) q) q) (cid:33)(cid:35) , 1 + ε θ log πθ(τ+ q), (cid:33)(cid:35) , 1 ε θ log πθ(τ q). Thus, the update direction is: (cid:32) θt+1 θt η p+ N+ i=1 θ log πt(τ+ q) (cid:33) θ log πt(τ q) , j=1 where η is the learning rate. Substituting into the Taylor expansion and dropping higher-order terms, we get: log πt(τ kq) η log πt(τ kq), p+ (cid:42) N+ i=1 log πt(τ+ q) (cid:43) log πt(τ q) . j=1 The learning rate η is positive scalar. Since we are interested in the sign of the change (increase or decrease), we can absorb η into the expression and consider the inner product as the primary determinant of the sign. For notational simplicity and consistency with the original text, we present the update direction without η, leading to 12. This completes the proof of A.2. To analyze the sign of log πt(τ assumption. kq), we introduce the response-level NTK and state the gradient alignment Definition A.3 (Response-level NTK). The response-level Neural Tangent Kernel (NTK) between two response-generation events (q, τ) and (q, τ) is defined as: Θ(cid:0)(q, τ), (q, τ)(cid:1) := (cid:10)θ log πθ(τ q), θ log πθ(τ q)(cid:11) . Under the NTK regime for sufficiently wide neural networks, Θ converges to deterministic limit and remains approximately constant during training (Jacot et al., 2018; Arora et al., 2019). Assumption A.4 (Gradient Alignment). Let q, be two questions from the same task family , with indicating semantic similarity. Then, in the infinite-width limit, the following asymptotic properties hold: (i) (Correct-Correct Alignment) For all correct responses τ+ lim width (cid:10)θ log πθ(τ+ q), θ log πθ(τ+ R+(q), τ+ R+(q): kk,ii > 0. q)(cid:11) = Θ++ (ii) (Incorrect-Incorrect Alignment) For all incorrect responses τ R(q), τ R(q): (cid:68) lim width θ log πθ(τ q), θ log πθ(τ q) (cid:69) = Θ kk,jj > 0. (iii) (Correct-Incorrect Orthogonality) For all τ+ R+(q), τ {τ+ R(q), τ (cid:69) , τ }: lim width lim width (cid:68) θ log πθ(τ+ q), θ log πθ(τ q) = 0, (cid:10)θ log πθ(τ q), θ log πθ(τ+ q)(cid:11) = 0. Remark A.5. This assumption is motivated by the structure of the NTK. For semantically similar inputs and valid (correct) outputs, the corresponding feature representations activate overlapping sets of neurons, leading to positive kernel values. Conversely, correct and incorrect responses represent conflicting patterns, and their gradient directions become nearly orthogonal in overparameterized models (Zhu et al., 2021). A.3.2 Main Generalization Result With the NTK alignment assumption in place, we can now prove that training on improves performance on similar q. Proposition A.6 (Generalization through Gradient Alignment). Let and be two questions that are similar in structure and difficulty, denoted q, belonging to shared task family . Let τ be response to q. Under A.4 and the GRPO update rule, the sign of the change in log-probability log πt(τ q) is determined as follows in the infinite-width limit: sign (cid:0) log πt(τ q)(cid:1) = (cid:40) +1 if τ 1 if τ is correct response to q, is an incorrect response to q. Proof. We substitute 12 and analyze the two cases separately. Case 1: τ is correct response (τ = τ+ ) log πt(τ+ q) = p+ N+ i=1 (cid:10)θ log πt(τ+ q), θ log πt(τ+ q)(cid:11) (cid:68) j=1 θ log πt(τ+ q), θ log πt(τ (cid:69) . q) (13) By A.4(i), each inner product in the first sum is strictly positive in the infinite-width limit. Since p+ > 0, the entire first term is positive. By A.4(iii), each inner product in the second sum is zero. Thus, the second term vanishes. Therefore, log πt(τ+ q) > 0, meaning the log-probability of the correct response τ+ Case 2: τ is an incorrect response (τ = τ ) increases. log πt(τ q) = p+ N+ i=1 (cid:10)θ log πt(τ q), θ log πt(τ+ q)(cid:11) (cid:68) j=1 θ log πt(τ q), θ log πt(τ (cid:69) . q) (14) 19 By A.4(iii), each inner product in the first sum is zero. By A.4(ii), each inner product in the second sum is strictly positive. Since > 0, the sum is positive, but it is preceded by negative sign, making the entire second term negative. Therefore, log πt(τ q) < 0, meaning the log-probability of the incorrect response τ decreases. Combining both cases proves A.6. This shows that GRPO implicitly pushes the model in direction that generalizes to similar tasks by reinforcing correct responses and suppressing incorrect ones. Corollary A.7. In the NTK regime, GRPO encourages an inductive bias towards solutions that lie in directions of high kernel alignment across correct responses within task family. This promotes generalization even with sparse supervision. A.4 Unifying Trajectory Divergence and Domain Discrepancy We now establish formal connection between the trajectory-level dynamics in our method and classical domain adaptation theory. While our theoretical analysis begins with gradient alignment in parameter space, the practical metric we usetrajectory divergenceis measured in the space of confidence dynamics. We first define gradient-based notion of coherence, then show it implies similarity in pass rate evolution. Definition A.8 (Gradient Coherence). For questions and q, the gradient coherence at step is: (t) grad(q, q) := τπθt τπθt (q) (q) (cid:2)cos (cid:0)θ log πθt (τq), θ log πθt (τq)(cid:1)(cid:3) , (15) where cos (a, b) = a,b ab . High coherence indicates similar optimization directions. Definition A.9 (Trajectory Divergence). Let (1) (t) = (P (2) , , . . . , (t) ) Rt be the trajectory vector of (s) question q, where is its pass rate at round s. The trajectory divergence between and at step is: (t) traj(q, q) := 1 (t) , (t) (t) (t) . (16) This measures the angular dissimilarity between their confidence evolution paths. We now establish the key link: gradient coherence implies low trajectory divergence. Lemma A.10 (From Gradient Coherence to Trajectory Coherence). Suppose the policy πθ is trained under small learning rates and lies in region where the NTK is approximately constant. If for all and for questions q, q, we have (s) grad(q, q) 1 ϵs, then there exists constant > 0 such that: (t) traj(q, q) (cid:33)2 ηsϵs . (cid:32) s=1 Proof (Sketch). Under NTK linearity, the change in log-probability is log πs(τq) ηsθ log πθs (τq), θs. High gradient coherence implies that the relative improvement for correct responses is similar across and q. (s) Since the pass rate (s) coherent log-prob updates lead to similar is an empirical estimate of the models confidence in generating correct responses, evolutions. By vector concentration and Lipschitz continuity of 20 the cosine similarity, the Euclidean distance (cid:16) (t) (t) 2 2 (cid:17) . The full proof is in A.7. (t) (t) 2 = (cid:0)t s=1 ηsϵs (cid:1), which implies (t) traj(q, q) = We now state the main result, bounding domain discrepancy via trajectory divergence. Proposition A.11 (Trajectory Divergence as Proxy for Domain Discrepancy). The HH-divergence between Dl and Du is bounded by the expected pass-rate trajectory divergence: (cid:105) (cid:104) dHH(Dl, Du) α D (t) traj(q, q) + λd, (17) qDl qDu where α > 0 depends on model smoothness and training dynamics, and λd 0 is an irreducible baseline discrepancy. Proof. The HH-divergence is: dHH(Dl, Du) = sup h,hH (cid:12) (cid:12) (cid:12) (cid:12) Pr qDl (h(q) = h(q)) Pr qDu (h(q) = h(q)) (cid:12) (cid:12) (cid:12) (cid:12) . In our setting, hypotheses are induced by the policy πθ. The ability of to distinguish Dl from Du depends on the discrepancy in their induced gradient fields: (t) = qDl (cid:2)θJq(θt)(cid:3) , (t) = qDu (cid:104) θJq (θt) (cid:105) . Let (t) = (t) (t) . Standard domain adaptation theory gives: dHH(Dl, Du) sup (t) + λd, for some > 0. Now, (t) is captured by (t) traj(q, q). is small when the gradient fields are aligned across domains. From Definition A.8, this alignment (t) grad) implies low (t) grad(q, q). Applying Lemma A.10, high gradient coherence (low 1 (t) traj(q, q) is small on average, it indicates that the confidence evolution is coherent across Conversely, if domains, which (by contrapositive of Lemma A.10) implies that gradient coherence must be high, hence (t) is small. Therefore, E[D constants yields the result. traj] serves as an upper bound proxy for (t) , and thus for dHH. Setting α to absorb the (t) Corollary A.12. Low pass-rate trajectory divergence Dtraj implies low domain discrepancy, enabling effective transfer without explicit adversarial or feature-level alignment. 21 A.5 Main Theorem: Generalization Bound Theorem A.13 (Trajectory-Consistent Generalization Bound). (Formal) Let δ (0, 1) be confidence parameter. Suppose the loss function : R0 is Ly-Lipschitz in its second argument and bounded, (t) i.e., L(, ) B. Let π θ be model trained under the TRAPO framework at round t. Then, with probability at least 1 δ over the sampling of labeled and unlabeled data, the expected risk of π (t) θ on the target distribution Du satisfies: RDu (π (t) θ ) ˆRDl (cid:114) (t) (π θ ) + (cid:32) + Ly 1 C(t) + ln(4/δ) 2m (cid:114) + α (cid:33) ln(2n/δ) 2G (cid:104) (cid:105) (t) traj(q) qDu + λ, where: ˆRDl (π (t) θ ) is the empirical risk on labeled source samples; (t) traj(q) = 1 reliable trajectory; (t) , (t) T (t) reliable (t) reliable is the cosine divergence between the trajectory of and the average C(t) = 1 j=1 (t) , with (t) = 1 I(a (t) j,i = (t) ) the voting confidence for unlabeled sample j; i=1 λ = λ + λd 0 absorbs the irreducible domain shift and best-in-class error. Moreover, define the Dynamic Trajectory Consistency Risk: R(t) TC := α [D (t) traj(q)] + Ly (cid:32) 1 C(t) + (cid:114) ln(2n/δ) 2G (cid:33) . If the Consistent Trajectory Learning Condition holds: q [D (t) traj(q)] = 0 and lim C(t) = 1, lim then R(t) TC 0, and RDu (π (t) θ ) ˆRDl (π (t) θ ) + λ, implying asymptotic generalization to the target domain. Proof. We start from the standard domain adaptation risk decomposition (Ben-David et al., 2010): where λ = infhH (cid:0)RDl (t) θ ) RDl RDu (π (h) + RDu (h)(cid:1). (π (t) θ ) + dHH(Dl, Du) + λ, (18) Step 1: Bounding the source risk RDl lemma) for bounded losses B, with probability at least 1 δ/2: (π (t) θ ). Using standard concentration inequality (e.g., Hoeffdings"
        },
        {
            "title": "RDl",
            "content": "(π (t) θ ) ˆRDl (π (t) θ ) + (cid:114) ln(4/δ) 2m . Step 2: Bounding the domain discrepancy dHH. Under the NTK alignment assumption, trajectory consistency controls gradient field divergence. From the trajectory-proxy proposition A.11, we have: dHH(Dl, Du) α qDu (cid:104) (cid:105) (t) traj(q) + λd, 22 (t) traj(q) measures the cosine divergence between the gradient trajectory of and the average reliable where trajectory (t) reliable over source or high-confidence samples. Step 3: Pseudo-labeling error. Let y(t) be the pseudo-label for via majority voting. The error in using y(t) instead of true is bounded by: (cid:12) (cid:12) (cid:12)RDu (π (t) θ ) [L(π (t) θ (q), y(t))] (cid:12) (cid:12) Ly P(y (cid:12) true = y(t)). For unlabeled samples, let estimates . Then: = P(a (t) = ytrue,j). The observed confidence (t) = 1 G i=1 I(a (t) j,i = (t) ) P( (t) = ytrue,j) 1 (t) + (t) . By Hoeffdings inequality and union bound over = 1, . . . , n, with probability at least 1 δ/2: Averaging over j, we get: (t) (cid:114) ln(2n/δ) 2G , j. P(y true = y(t)) 1 C(t) + (cid:114) ln(2n/δ) 2G . Step 4: Union bound. Combining Steps 13 with union bound (total probability 1 δ), and absorbing λd into λ = λ + λd, we obtain the desired bound. Finally, under the Consistent Trajectory Learning Condition, both yielding asymptotic generalization. (t) traj 0 and C(t) 1, so R(t) TC 0, A.6 Main Theorem: Convergence Analysis (t) Theorem A.14 (Monotonic Convergence under Consistent Trajectory Learning). Let Ut = θ ) denote the expected target risk at training round t. Under the Consistent Trajectory Learning Condition (A.13), and assuming: RDu (π (cid:104) (cid:105) 1. Stochastic Gradient Descent (SGD) with learning rate ηt > 0, 2. NTK stability: θπ (t) θ (x) is bounded for all x, 3. Lipschitz smoothness of π (t) θ , 4. Sufficient ensemble size such that (cid:113) ln(2n/δ) 2G ϵ, then the expected risk sequence {Ut} t=1 satisfies: Ut+1 Ut ηtξt + βt, where: (cid:104) ξt = θ ˆRDl (π (t) θ )2(cid:105) 0 measures the expected gradient magnitude on source data, βt = α (t) traj + Ly C(t) + η2 M2 aggregates the residual dynamics, with: (t) traj = C(t) = (cid:104) (cid:104) (t+1) traj (q) C(t+1) C(t)(cid:105) , (t) traj(q) (cid:105) , and > 0 bounds the gradient variance. t=1 ηt = and t=1 η2 Moreover, if (t) traj 0, C(t) 0 for all T0, then: < , and (cid:104) θ ˆRDl lim (t) θ )2(cid:105) (π = 0, and lim sup Ut ˆRDl ( ) + λ, where is stationary point of the source risk. Proof. We analyze the expected change in target risk: Ut+1 Ut = (cid:104) RDu ( ft+1) RDu (π (cid:105) . (t) θ ) Using the smoothness of π have: (t) θ and the update θt+1 = θt ηtgt, where gt is the stochastic gradient, we RDu ( ft+1) RDu (π (t) θ ) ηtθ RDu (π (t) θ ), gt + Taking expectation over the stochastic gradient and data sampling: 2 gt2. η2 Ut+1 Ut ηtE (cid:104) θ RDu (π (t) θ )2(cid:105) + 2 η2 (cid:104) gt2(cid:105) . Now, from A.13, we know: Thus, the gradient θ RDu (π (cid:104) Now, observe that: RDu (π (t) θ ) ˆRDl (π (t) θ ) is aligned with θ ˆRDl θ )2(cid:105) (t) (cid:104) θ RDu (π (t) θ ) + R(t) (π (t) TC + const. θ ) and θR(t) TC. Specifically: (cid:13) θ )2(cid:105) (cid:13)θR(t) (cid:13) (cid:13) (cid:13) (cid:13) . (π TC (t) θ ˆRDl (cid:13) (cid:13)θR(t) (cid:13) TC (cid:13) (cid:13) (cid:13) α (cid:12) (cid:12) (cid:12) (cid:12) dt E[D (t) traj] (cid:12) (cid:12) (cid:12) (cid:12) + Ly (cid:12) (cid:12) (cid:12) (cid:12) dt C(t) (cid:12) (cid:12) (cid:12) (cid:12) α (t) traj + Ly C(t), in discrete time. Under the assumption that trajectory divergence is decreasing (D (C(t) 0), the residual βt captures the rate of improvement in transferability. Furthermore, E[gt2] M2 under NTK stability and bounded loss. (t) traj 0) and confidence is increasing Thus, we obtain: Ut+1 Ut ηtξt + βt, 24 with ξt = E[θ ˆRDl Now, summing over t: (π (t) θ )2], βt = α (t) traj + Ly C(t) + η2 M2. t=1 ηtξt U1 lim inf Ut + t= βt. (t) traj 0 and C(t) 0, then βt η2 If ηt = , we must have ξt 0, i.e., M2 eventually, and η2 < implies ηtξt < . Since (cid:104) θ ˆRDl (π (t) θ )2(cid:105) = 0. lim Finally, from A.13, since R(t) TC 0, we get: lim sup Ut ˆRDl ( ) + λ, where is stationary point. This completes the proof. A.7 Addition Proofs We provide the full proof of Lemma A.10, which connects gradient coherence in parameter space to trajectory coherence in the space of confidence dynamics. Lemma A.15 (Restatement of Lemma A.10). Suppose the policy πθ is trained under small learning rates {ηs}t s=1, and lies in region where the Neural Tangent Kernel (NTK) is approximately constant. If for all (s) and for questions q, q, the gradient coherence satisfies grad(q, q) 1 ϵs, then there exists constant > 0 such that: (t) traj(q, q) (cid:33)2 ηsϵs . (cid:32) s=1 Proof. We proceed in three steps: (1) bound the difference in log-probability updates under gradient coherence; (2) relate log-prob changes to pass rate evolution; (3) bound the cosine distance between trajectory vectors. Step 1: Gradient coherence implies coherent log-prob updates. Under the NTK regime, the model evolves via kernel gradient descent, and the change in log-probability after update is approximately linear in the gradient: log πs(τq) := log πθs (τq) log πθs1 (τq) ηs1θ log πθs1 (τq), θs1. and τ Let τ generating correct responses evolves. be the correct responses for and q. We are interested in how the models confidence in"
        },
        {
            "title": "Let g",
            "content": "(s) = θ log πθs (τ q) and (s) = θ log πθs (τ q). By Definition A.8, we have: (s) , (s) (s) (s) g 1 ϵs. 25 This implies (by standard vector inequality): (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) g (s) (s) (s) (s) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) g 2ϵs. Assume the gradient norms are bounded: (s) G, (s) G. Then: (s) (s) 2ϵs + (s) (s) . For simplicity, assume gradient magnitudes evolve similarly (or absorb into constants), so: (s) (s) G ϵs. Now, the parameter update is θs = ηsθJs, which is weighted sum of gradients over the batch. If and are both in the batch or their gradients are representative, then: log πs(τ q) log πs(τ q) ηsg (s) (s) θs/ηs ηsG ϵs M, where bounds the update direction. Thus: log πs(τ q) log πs(τ q) ηsC1 ϵs. Summing over = 1 to t, the total difference in log-prob evolution is: log πθt (τ q) log πθt (τ q) C1 ηs ϵs. s=1 (s) Step 2: Log-prob coherence implies pass rate coherence. The pass rate is defined as: (s) = 1 k= 1 [ fθs (q; ξk) passes] , (s) where ξk represents stochasticity (e.g., dropout, sampling). is an empirical estimate of Pr(correctq, θs). Assume the mapping from log πθs (τ gradients). Then: (s) q) to E[P ] is L-Lipschitz (holds for softmax policies under bounded (s) E[P ] E[P (s) ] log πθs (τ q) log πθs (τ q) LC1 ηr ϵr. r=1 By concentration (e.g., Hoeffdings inequality), with high probability: (s) (s) LC1 ηr r=1 ϵr + νs, where νs = O(1/ G) is sampling error. For large N, νs is negligible. 26 Step 3: Trajectory vector proximity implies low divergence. Let Then: (1) (t) = (P , . . . , (t) ), (1) (t) = (P , . . . , (t) ). (t) (t) 2 2 = s=1 (s) (s) 2 s=1 (cid:32) ϵr ηr (cid:33) . LC1 r=1 Using the inequality (s r=1 ar)2 and assuming ηr, ϵr small, we get: (t) (t) 2 2 C2 (cid:33)2 ϵs ηs C2 (cid:32) s=1 (cid:33) (cid:32) s=1 ηs (cid:33) ηsϵs , r=1 a2 (cid:32) s=1 but more conservatively, if ηsϵs summable, then: (t) (t) 2 = (cid:33) ηsϵ1/2 . (cid:32) s= Now, the cosine distance: (t) traj(q, q) = 1 (t) , (t) (t) (t) = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 2 (t) (t) (t) (t) T 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) + O(T (t) (t) 2). If the trajectories are bounded away from zero (i.e., not all zeros), then: (t) traj(q, q) (t) (t) 2 2 ϵs ηs (cid:33)2 . (cid:32) s=1 To match the lemma statement, we can weaken coherence gap. In either case, there exists constant > 0 such that: ϵs to ϵs under ϵs (0, 1), or redefine ϵs as the squared (t) traj(q, q) (cid:33)2 ηsϵs , (cid:32) s= which completes the proof."
        },
        {
            "title": "B Discussion and Limitations",
            "content": "First, our results demonstrate that semi-supervised training using 4K labeled data combined with 16K unlabeled data outperforms fully supervised training on 45K labeled data. This encouraging finding aligns with the insight proposed by Li et al. (2025b) in the context of RLVR training: thorough training (i.e., more training epochs) on smaller curated datasets can yield better performance than training with larger datasets for fewer epochs. Our work further extends this observation by showing that unlabeled data, when carefully selected using guidance from labeled data training, can effectively enhance the models reasoning capabilities, thus amplifying the benefits of semi-supervised RLVR. In addition, due to computational constraints, our evaluation is currently limited to models under the 7B parameter scale. Exploring the applicability and scalability of this semi-supervised paradigm to larger language models (e.g., 13B or beyond) remains an important direction for future research, as larger models may benefit even more from effective utilization of unlabeled data. 27 One key observation from our experiments comparing Qwen2.5-Math-7B and LLaMA-3.1-8B is that the more effective supervised training with labeled data is on model, the better the labeled data guides the selection and utilization of unlabeled data. Conversely, if RLVR training with labeled data yields only marginal gains, its impact on unlabeled data filtering is also limited. Therefore, we recommend applying TRAPO primarily in settings where the labeled data and model are well aligned. Finally, we believe it is promising direction, similar to active learning, to investigate what types of labeled examples most effectively guide unlabeled data training, and we plan to explore this in future work."
        },
        {
            "title": "C Experiment Details",
            "content": "C.1 Detailed Setup Implementation Details. Following Dr.GRPO (Liu et al., 2025), we disable length and standard error normalization in the GRPO loss (Eq. 9) for all experiments. By default, we use Qwen2.5-Math-7B (Yang et al., 2024), following prior work Cui et al. (2025); Zeng et al. (2025b); Liu et al. (2025). Besides, we remove the KL regularization by setting β = 0 and set the entropy coefficient to 0.01. Our rollout batch size is 64, with 8 rollouts per prompt, and update batch size 64. Rollouts are generated with temperature sampling (T = 1.0). We use Math-Verify 1 as the reward function, without format or length bonuses. For unlabeled data selection in the training with 1K labeled ID samples and 1K unlabeled OOD samples, we set the top-p threshold to 0.1, the threshold Γ to 0.4, and the warmup stage consists of 10 epochs. For unlabeled data selection in the training with 1K labeled ID samples and 3K unlabeled ID samples, we set the top-p threshold to 0.1, the threshold Γ to 0.4, and the warmup stage consists of 8 epochs. For unlabeled data selection in the training with 4K labeled ID samples and 12K unlabeled ID samples, we set the top-p threshold to 0.1, the threshold Γ to 0.4, and the warmup stage consists of 8 epochs. In addition, given that experiments are performed across different data scales, the samples used in non-full-data scenarios are random sampled from the original dataset without. Training. In addition to Qwen2.5-Math-7B, we extend TRAPO to DeepSeek-R1-Distill-Qwen-1.5B (Guo et al., 2025) and LLaMA-3.1-8B-Instruct (Team, 2024). To ensure fairness, we maintain 8 samples per prompt for all RL-trained models. The learning rate is constantly set as 1e-6. For all training, we follow Yan et al. (2025) and use the same validation set to select the best checkpoint. All the experiments were run with an 8 NVIDIA H200 with 141GB memory. Our implementation is based on LUFFY2 and veRL3, which use vLLM4 as the rollout generators. We are thankful for these open-source repositories. Qwen2.5-Series Models. Since the context length of Qwen2.5-Math-base is 4096 and the generation length of off-policy samples could be lengthy, we change the rope theta from 10000 to 40000 and extend the window size to 16384. For all Qwen2.5-Series models, we use the same dataset as described in Sec. 4. DeepSeek-R1-Distill-Qwen-1.5B. DeepSeek-R1-Distill-Qwen-1.5B is compact, 1.5-billion-parameter language model distilled from the high-performing DeepSeek-R1 series (Guo et al., 2025). Built on the Qwen architecture, it combines strong reasoning capabilities with high efficiency, offering excellent performance in math and logic tasks despite its small size. For DeepSeek-R1-Distill-Qwen-1.5B, we use the same dataset as described in Sec. 4. 1https://github.com/huggingface/Math-Verify 2https://github.com/ElliottYan/LUFFY 3https://github.com/volcengine/verl 4https://github.com/vllm-project/vllm 28 Llama-3.1-8B. For Llama3.1-8B, we follow Simple-RL-Zoo Zeng et al. (2025a) and use simplified prompt, and we do not ask the model to generate <think>n </think>n tokens. C.2 System Prompt All our trained models, except LLaMA-3.1-8B, share the same system prompt for training and inference: Your task is to follow systematic, thorough reasoning process before providing the final solution. This involves analyzing, summarizing, exploring, reassessing, and refining your thought process through multiple iterations. Structure your response into two sections: Thought and Solution. In the Thought section, present your reasoning using the format: <think>n thoughts </think>n. Each thought should include detailed analysis, brainstorming, verification, and refinement of ideas. After </think>n in the Solution section, provide the final, logical, and accurate answer, clearly derived from the exploration in the Thought section. If applicable, include the answer in boxed{} for closed-form results like multiple choices or mathematical solutions. User: This is the problem: {QUESTION} Assistant: <think> For LLaMA-3.1-8B, we do not use the above system prompt as we find the model cannot follow such an instruction. Thus, we use simplified version that only includes the CoT prompt and do not include <think> token. User: {QUESTION} Answer: Lets think step by step. C.3 Baseline Description Unsupervised Baselines: TTRL (Zuo et al., 2025): treating the majority-voted output as the pseudo-label and training with GRPO. Self-Certainty (Zhao et al., 2025): maximizing the KL divergence between the models rollout token probabilities and uniform distribution to encourage confident predictions. Token-Level Entropy (Agarwal et al., 2025): minimizing the entropy of individual output tokens during rollout to promote consistency. Sentence-Level Entropy (Agarwal et al., 2025): maximizing the overall sentence probability of the generated output to favor high-likelihood sequences. Supervised Baselines: Simple-RL (Zeng et al., 2025b): training from Qwen2.5-Math-7B using rule-based reward. Oat-Zero (Liu et al., 2025): training from Qwen2.5-Math-7B and rule-based reward, proposing to remove the standard deviation in GRPO advantage computation and token-level normalization in policy loss computation. PRIME-Zero (Cui et al., 2025): using policy rollouts and outcome labels through implict process rewards. OpenReasonerZero (Cui et al., 2025): recent open-source implementation of RLVR methods. Fully Supervised (Yan et al., 2025): trained on-policy RL within the RLVR paradigm using Dr.GRPO (Liu et al., 2025) with the same reward and data. Table 4 Comparison with other fully supervised training methods. Bold and underline indicate the best and second-best results, respectively. Model In-Distribution Performance Out-of-Distribution Performance AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Qwen-Base (Yang et al., 2024) 11.5/4.9 Qwen-Instruct (Yang et al., 2024) 12.5/10.2 31.3 48.5 43. 80.4 7.4 32.7 15.6 41.0 19. 18.2 37.6 70.3 11.1 24.7 Fully Supervised Methods Trained on 45K Samples w/ All Labels SimpleRL-Zero (Zeng et al., 2025b) 27.0/6.8 OpenReasoner-Zero (Hu et al., 2025) 16.5/15.0 PRIME-Zero (Cui et al., 2025) Oat-Zero (Liu et al., 2025) On-Policy RL (Yan et al., 2025) 17.0/12.8 33.4/11.9 25.1/15.3 54.9 52. 54.0 61.2 62.0 76.0 82.4 81. 78.0 84.4 25.0 33.1 39.0 34. 39.3 34.7 47.1 40.3 43.4 46. 37.4 30.2 41.0 66.2 40.7 73. 43.7 70.1 45.5 82.3 23.2 29. 18.2 23.7 40.4 TRAPO Trained w/ 4K Labeled Samples & 12K Unlabeled Samples 16.9 34. 34.5 58.7 32.7 41.7 49.3 15. 43.0 29.3 51.6 41.4 45.2 57. TRAPO (ours) 24.3/17.1 60.0 84.6 39.3 48. 45.6 84.6 43.9 50.7 59."
        },
        {
            "title": "D More Experiments",
            "content": "D.1 Comparison with More Supervised RLVR Baselines In Table 4, we compare our method with additional fully supervised RLVR baselines, all of which are trained on the complete 45K labeled dataset, with results taken directly from Yan et al. (2025). The results show that our model, trained with only 4K labeled and 12K unlabeled samples, achieves performance that surpasses all baselines trained on the full 45K labeled data. For instance, our TRAPO method outperforms the outstanding Oat-Zero baseline by 1.9% in in-distribution performance and by significant 14.5% in out-of-distribution performance. This further underscores the effectiveness and value of our proposed TRAPO. D.2 Extend TraPO to More Models We further investigate whether our proposed semi-supervised paradigm, TRAPO, generalizes to small models, instruction-tuned models, and weak models. To this end, we conduct experiments on DeepSeek-R1-DistillQwen-1.5B (representing small models) and LLaMA-3.1-8B-Instruct (representing instruction-tuned and relatively weaker models), under unsupervised, semi-supervised, and fully supervised training settings. The experimental setup follows that of Table 2. As shown in Table 5 and Table 6, TRAPO consistently outperforms the unsupervised baseline (TTRL) by significant margin and approaches (or even surpasses) the performance of the fully supervised baseline on both models. Specifically, on DeepSeek-R1-Distill-Qwen-1.5B, TRAPO improves over TTRL by 2.0% in in-distribution performance and 9.5% in out-of-distribution performance. On LLaMA-3.1-8B-Instruct, it exceeds TTRL by 1.2% in ID performance and 0.9% in OOD performance. Notably, TRAPO even outperforms the fully supervised baseline by 0.5% in ID performance. These results strongly demonstrate the robustness, adaptability, and broad applicability of our method across diverse model scales and architectures. D.3 TraPO Is Universal Component We demonstrate that TRAPO serves as universal and modular component, whose pass rate trajectory-based sample selection mechanism can be readily integrated into various semi-supervised baselines to identify reliable unsupervised reward signals. As shown in Figure 7, we apply this selection strategy to three 30 Table 5 Overall performance on nine competition-level benchmark performance on LLaMA-3.1-8B-Instruct (Team, 2024). Model In-Distribution Performance Out-of-Distribution Performance AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Original Model Original Model 5.1/0.4 18.6 44.6 19. 14.1 17.1 24.2 0.5 38.6 21. Unsupervised Methods Trained on 1K Unlabeled ID Samples & 1K Unlabeled OOD Samples TTRL Self-certainty Token-level Entropy Sentence-level Entropy 6.1/0. 6.9/1.2 5.3/0.1 7.2/0.2 21.8 20.3 19. 20.9 46.6 45.5 43.5 46.4 25. 23.7 22.7 24.7 16.7 17.1 16. 16.5 19.5 11.0 19.1 13.3 18. 10.5 19.3 11.7 0.0 0.0 0. 0.0 Semi-supervised Methods Trained on 1K Labeled ID Samples & 1K Unlabeled OOD Samples TTRL Self-certainty Token-level Entropy Sentence-level Entropy TRAPO (ours) 7.1/0.1 6.6/0.6 6.4/0.1 7.5/0.1 9.9/0. Fully Supervised w/ 2K Labels 6.9/1.6 20.5 20.7 20.5 21. 21.5 22.2 46.4 46.4 44.6 46. 48.0 52.2 24.6 23.2 23.3 25. 26.1 21.0 17.3 16.3 16.4 16. 18.7 17.5 19.3 11.5 19.0 12. 18.6 11.3 19.6 20.7 12.3 12. 20.2 10.4 0.0 0.0 0.0 0. 0.0 0.0 41.8 39.5 38.7 41. 40.9 40.3 41.6 41.9 43.4 47. 17.6 17.6 16.4 17.7 17.5 17. 17.6 18.1 18.5 19.3 Table 6 Overall performance on nine competition-level benchmark performance on DeepSeek-R1-Distill-Qwen-1.5B (Guo et al., 2025). Model Original Model Unsupervised (TTRL) Semi-supervised (TRAPO) Supervised AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. 21.0/20.3 26.1/21.7 27.9/22.6 28.5/22.5 51. 57.0 61.9 64.1 76.6 80.6 82.2 84.6 26.5 28.7 32.0 37.1 36. 42.7 45.3 47.0 38.8 42.8 45.3 47.3 3.7 25.7 34.4 57.3 0. 0.0 0.0 0.0 11.0 31.9 33.5 38.9 4.9 19.2 22.6 32.1 representative baselines: Sentence-level Entropy, Token-level Entropy, and TTRL. Compared to the naive semi-supervised counterparts that simply combine supervised and unsupervised objectives, augmenting these methods with our sample selection framework consistently yields performance gains across multiple benchmarks. This further validates the extensibility and plug-and-play nature of our approach, indicating that the core principle of TRAPOdynamically identifying high-quality unlabeled samples via learning trajectoriesis broadly applicable and complementary to diverse semi-supervised paradigms. D.4 Run TraPO on DeepMath To further verify TraPOs broad applicability, we run it on DeepMath (He et al., 2025b), recently released dataset for mathematical reasoning. We randomly select 2K samples as labeled data and 8K samples as unlabeled data. We compare results from unsupervised, naive semi-supervised, and fully supervised methods. As shown in Table 8, our method, TraPO, outperforms all unsupervised methods and naive supervised methods. Specifically, on the ID test set, TraPO achieves 1.5% improvement over the best naive semisupervised method combined with TTRL, and is only 1.2% behind fully supervised training. Notably, on the OOD test set, TraPO even surpasses fully supervised training by 2.4%, highlighting that TraPO is not only label-efficient but also delivers outstanding performance. 31 (a) Sequence Entropy Method (b) Token Entropy Method (c) TTRL Method Figure 7 Different unsupervised methods combined with our trajectory-based filtering approach can improve performance, compared to naive semi-supervised method that directly combines supervised and unsupervised approaches. The experimental setup follows Table 2. D.5 Different Selection Strategies Under fixed selection ratio (30%), we compare TraPO with other possible selection strategies, including simple random selection, sentence-level entropy-based selection (where lower entropy indicates more reliable pseudo-labels for the corresponding rollouts), and self-certainty (where higher self-certainty suggests more reliable pseudo-labels for the corresponding rollout). The experimental results in Table 13 show that with fixed selection ratio of 30%, all other methods are significantly inferior to our selection method, TraPO, on both the ID and OOD test sets. D.6 Stability of TraPO We seek to verify whether TraPO is sufficiently stable and insensitive to sample order. To this end, we ran TraPO three times with data randomly shuffled. Across these three trials (Qwen-2.5-7B, 1K labeled, 3K unlabeled), both the results and the selected samples were nearly identical, confirming TraPOs robustness (see table 16). D.7 Training Cost Analysis of TraPO We analyze the practical training cost of TraPO from both theoretical and empirical perspectives. Time Complexity. Each labeled or unlabeled sample is rolled out times per epoch, in line with standard RLVR practices. Let represent the total number of training epochs, NL, NU the number of labeled and unlabeled samples, Csim the computational cost of cosine similarity computation over short vectors, and Cgen the computational cost of single rollout. The only additional operation is cosine similarity computation Csim over short vectors, which is negligible compared to the cost of rollout generation Cgen, i.e, Csim << Cgen. The time complexity of fully supervised training (using = NL + NU labeled samples) is: TSup = O(T Cgen) (19) TraPO has the same complexity: TTraPO = O(cid:0)T (NL + NU) Cgen (cid:1) + O(cid:0)T (NL + NU) Csim (cid:1) O(T Cgen) (20) Therefore, TRAPO and fully supervised RLVR share identical time complexity, both dominated by forward sampling and GRPO updates. 32 Empirical Training Cost. In our experiments, TraPO, supervised RLVR, and unsupervised RLVR are trained under identical conditions: same number of epochs, batch sizes, and hardware configuration (8H200 GPUs). Notably, TraPO reaches its best checkpoint at nearly the same training step as the supervised baseline, indicating no significant overhead in convergence speed. Table 7 summarizes the wall-clock training times across different data scales, demonstrating that TraPO incurs no substantial additional training cost compared to supervised RLVR. Table 7 Wall-clock training time (reported as GPU-hours GPUs) across data regimes. Data Size Unsupervised Supervised Semi-Supervised (TraPO) 4k 8k 45k 7 8 13 8 11 8 25 8 39 8 57 8 26 8 38 8 55 D.8 Different Ways of Utilizing Reliable Passrate Databases One may also consider other variants, such as not using the average pass rate trajectory and instead selecting, from the unlabeled samples, those whose pass rate trajectory is most similar to the trajectory of any labeled sample for inclusion in training. However, this approach can lead to unstable selection because, among the unlabeled samples, problems that are too difficult, too easy, or of moderate difficulty can all exhibit relatively similar pass-rate trajectories among the labeled samples. As result, the selection is ineffective  (Table 17)  ."
        },
        {
            "title": "E More Related Work",
            "content": "Semi-supervised Reinforcement Learning. Semi-supervised learning has been widely studied in supervised settings, where labeled and unlabeled data are combined to improve model performance under limited annotation budgets (Blum and Mitchell, 1998; Chapelle et al., 2009; Subramanya and Bilmes, 2011; Rasmus et al., 2015; Laine and Aila, 2016; Tarvainen and Valpola, 2017; Berthelot et al., 2019; Xie et al., 2020; Sohn et al., 2020; Heidari et al., 2024). In reinforcement learning, early work explored combining reward-based learning with self-supervised signals or pseudo-rewards derived from environment dynamics or intrinsic motivation (Dudík et al., 2011; Finn et al., 2016; Thomas and Brunskill, 2016; Kallus and Uehara, 2020; Zhou et al., 2023). These methods typically treat supervised and unsupervised signals independently, for instance by summing reward and consistency objectives, or by pre-training on unlabeled data before fine-tuning on labeled trajectories. However, such semi-supervised RL approaches are ill-suited for large language model (LLM) training under verifiable rewards (RLVR). In RLVR, the policy is optimized using feedback signals derived from answer verification (e.g., correctness of final outputs), rather than explicit action-level rewards. Unsupervised methods in this space rely on internal consistency, such as low token entropy (Agarwal et al., 2025), high self-certainty (Zhao et al., 2025), or majority voting (Zuo et al., 2025), to construct pseudo-rewards. While these signals can guide exploration, they often reinforce incorrect or degenerate reasoning patterns in the absence of external supervision, leading to model collapse (Zhang et al., 2025c). Our work departs from prior approaches by introducing guidance mechanism: the labeled data are not merely used to provide an additional reward signal, but to actively steer the selection and utilization of unlabeled samples. Specifically, we observe that reliable reasoning trajectories on unlabeled data exhibit learning dynamics similar to those on labeled data. By measuring trajectory similarity in the reward model space, TRAPO identifies high-quality unlabeled samples whose reasoning patterns are consistent with verified 33 Algorithm 1 TRAPO: Trajectory-based Policy Optimization Require: Labeled data Dl, Unlabeled data Du, Warm-up epochs Twarm, Threshold Γ, Top-p fraction Ensure: Policy πθ Initialize: Pass rate trajectories Tq [ ] for all 1: Reliable database Dreliable {Tl Dl} 2: for each training epoch do 3: Generate responses for Dl Du using πθ (t) Compute (pseudo) pass rates (t1) (t) Update trajectories: if > Twarm then for all questions (t) Compute average reliable trajectory for Du do Compute similarity: TCSu = cos end for Select reliable unlabeled samples: (t) reliable (cid:16) ˆT (t) , ˆT (t) reliable (cid:17) Ureliable = top-p(TCS) {u TCSu Γ}"
        },
        {
            "title": "Add their trajectories to Dreliable",
            "content": "end if Compute loss: L(θ) = labeled GRPO + uUreliable unlabeled GRPO,u 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: Update πθ using θL(θ) 15: 16: end for ones. This ensures that unsupervised signals are only leveraged when they align with externally validated behavior, preventing the amplification of spurious patterns. This paradigm shift from independent combination to supervised guidance addresses key limitation of traditional methods. In high-dimensional open-ended generation tasks, such as reasoning with LLMs, consistency alone is insufficient for correctness. Without supervision to anchor the learning process, models easily overfit to superficial patterns or self-reinforced errors. TRAPO resolves this by using minimal labeled data as north star enabling stable and practical learning from large amounts of unlabeled data. As we show empirically, this leads to superior performance and data efficiency, surpassing both fully supervised baselines trained on orders of magnitude more labels and unsupervised methods that fail to generalize."
        },
        {
            "title": "F Pseudo Code",
            "content": "We provide the pseudo code 1. 34 Table 8 Overall performance based on Qwen2.5-Math-7B under three different training paradigms using DeepMath dataset (He et al., 2025b). Bold and underline indicate the best and second-best results, respectively. Model In-Distribution Performance Out-of-Distribution Performance AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. TTRL Self-certainty Token-level Entropy Sentence-level Entropy Unsupervised Methods Trained on 8K Samples w/o Any Labels 11.6/8.4 11.9/10.2 13.5/9.3 13.6/9.6 50.2 45. 43.2 50.1 74.8 74.4 71.4 75. 37.1 36.4 36.0 36.8 38.7 37. 35.0 37.0 36.8 74.7 35.9 75. 34.7 75.9 37.1 72.1 30.3 23. 32.8 28.8 Semi-supervised Methods Trained on 2K Labeled Samples & 6K Unlabeled Samples TTRL Self-certainty Token-level Entropy Sentence-level Entropy TRAPO (ours) 14.1/13.0 12.8/8.3 13.8/10.9 9.6/9. 13.8/13.6 48.8 45.2 48.6 45.6 51. Fully Supervised w/ 8K Labels 16.0/12.1 52.9 77.8 71.6 74. 73.8 79.8 78.8 32.4 29.4 33. 32.4 33.8 36.8 37.0 32.0 34. 34.5 40.0 37.2 33.2 77.4 77. 35.8 77.0 34.3 38.7 76.9 77. 27.2 28.3 30.8 28.3 35.4 42. 39.9 77.0 29.3 39.8 36.7 39. 36.9 40.1 42.9 37.2 39.8 43. 42.7 48.3 45.4 49.3 45.9 48. 49.5 48.3 48.3 52.1 49.7 Table 9 Overall performance on nine competition-level benchmarks for Qwen-2.5-7B under different top-p settings, with fixed Γ (0.5) and fixed warmup length (5). Training was performed with 1K labeled and 3K unlabeled samples. Model AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Qwen-Base Qwen-Instruct 11.5/4.9 12.5/10.2 31.3 48.5 43.6 80. 7.4 32.7 15.6 41.0 19.0 37.6 18.2 70.3 11.1 24.7 16.9 34. 15.4 43.0 TRAPO 17.9/13.8 58.7 81.4 TRAPO 16.6/15.7 56.0 82.6 TRAPO 15.9/9.5 52. 79.0 TRAPO 14.9/10.8 53.4 81.8 TRAPO 14.9/10.7 55.3 77.8 45.5 42.6 83. 37.9 46.8 56.1 44.0 41.7 79. 34.3 46.7 53.6 39.9 38.5 73. 32.7 45.6 50.5 41.8 39.6 75. 36.9 43.8 52.0 43.6 39.2 72. 35.4 42.7 50.2 top-p = 0.1 38.2 top-p = 0.3 35.6 top-p = 0.5 34. top-p = 0.7 34.9 top-p = 1.0 33.1 35 Table 10 Overall performance across nine competition-level benchmarks for Qwen-2.5-7B under varying Γ values, with fixed top-p (0.1) and warmup length (5). Training was conducted with 1K labeled samples and 3K unlabeled samples. Model AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Qwen-Base Qwen-Instruct 11.5/4.9 12.5/10.2 31.3 48.5 43.6 80.4 7.4 32.7 15.6 41. 19.0 37.6 18.2 70.3 11.1 24.7 16.9 34.1 15.4 43.0 Γ = 0. TRAPO 15.7/10.9 52.6 81.1 34.5 41. 39.4 74.0 37.1 43.2 51.4 TRAPO 16.5/12.9 56.8 81.9 37.6 45.9 41. 81.9 38.1 46.3 55.4 Γ = 0.3 TRAPO 17.9/13.8 58.7 81.4 38.2 45.5 42. 83.7 37.9 46.8 56.1 TRAPO 14.3/12. 53.9 79.2 35.1 42.6 39.6 80. 35.6 43.7 53.3 Γ = 0.7 Γ = 0.5 TRAPO 14.9/13.3 53.9 79.7 34.7 42.1 39. 81.3 35.9 43.4 53.5 Γ = 1.0 Table 11 Overall performance across nine competition-level benchmarks for Qwen-2.5-7B under varying warmup lengths, with fixed top-p (0.1) and fixed Γ (0.5). Training conducted with 1K labeled and 3K unlabeled samples. Model AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Qwen-Base Qwen-Instruct 11.5/4.9 12.5/10.2 31.3 48.5 43.6 80. 7.4 32.7 15.6 41.0 19.0 37.6 18.2 70.3 11.1 24.7 16.9 34. 15.4 43.0 TRAPO 16.1/12.0 54.9 77.8 TRAPO 17.4/13.6 57.5 80.2 TRAPO 17.9/13.8 58. 81.4 TRAPO 18.2/14.1 59.3 82.0 TRAPO 17.6/13.5 58.1 80.9 warm-up length = 2 40.2 34.0 warm-up length = 3 43. 37.1 warm-up length = 5 45.5 38.2 warm-up length = 8 46.1 38.8 warm-up length = 12 44. 37.7 39.2 78.5 33.2 41.5 51. 41.6 81.9 36.2 44.8 54.3 42. 83.7 37.9 46.8 56.1 43.1 84. 38.4 47.3 56.6 42.1 83.1 37. 46.1 55.6 36 Table 12 Overall performance of Qwen2.5-Math-7B under different training sample sizes and annotation ratios. Model In-Distribution Performance Out-of-Distribution Performance AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Qwen-Base Qwen-Instruct 11.5/4.9 12.5/10. 31.3 48.5 43.6 80.4 7.4 32. 15.6 41.0 19.0 18.2 37.6 70. Original Models TRAPO w/ 1K Samples TRAPO w/ 2K Samples TRAPO w/ 4K Samples TRAPO w/ 16K Samples TRAPO Trained on Varying Sample Sizes (12.5% Labeled) 13.5/10.1 15.0/11.6 16.1/12.9 21.3/16.1 52.3 53. 56.8 60.9 80.7 81.2 82.3 84. 39.4 38.9 36.7 38.2 42.2 44. 45.4 43.3 39.7 75.2 40.7 82. 41.7 82.1 44.1 82.6 TRAPO Trained on Varying Sample Sizes (25% Labeled) TRAPO w/ 1K Samples TRAPO w/ 2K Samples TRAPO w/ 4K Samples 17.1/12.8 18.1/14.3 17.9/13.8 53. 55.4 58.7 TRAPO w/ 16K Samples 24.3/17.1 60.0 79. 81.6 81.4 84.6 39.3 33.1 38. 39.3 41.5 43.4 45.5 48.3 40. 72.7 41.0 82.6 42.6 83.7 45. 84.6 TRAPO Trained on Varying Sample Sizes (50% Labeled) TRAPO w/ 1K Samples TRAPO w/ 2K Samples TRAPO w/ 4K Samples TRAPO w/ 16K Samples 14.3/10.9 16.2/13.1 17.3/15.7 24.4/18.3 51.7 54. 59.2 61.5 Fully Supervised w/ 45K Labels 25.1/15.3 62.0 81. 82.3 83.9 84.1 84.4 34.2 37. 39.4 40.8 39.3 42.1 45.7 47. 46.3 39.1 78.3 41.5 81.5 43. 83.7 45.9 84.2 46.8 45.5 82. 40.4 11.1 24.7 24.1 28.7 33. 39.5 30.3 39.4 37.9 43.9 30. 34.2 36.8 43.7 16.9 34.1 43. 45.2 46.7 46.2 42.4 45.0 46. 50.7 45.2 46.6 46.6 49.7 49. 15.4 43.0 47.6 52.1 54.2 56. 48.5 55.7 56.1 59.7 51.2 54. 55.7 59.2 57.3 Table 13 Qwen-2.5-7B results on nine competition-level benchmarks using 1K labeled and 3K unlabeled samples (30% reliable data selected by Sentence-level Entropy, Self-certainty, and TraPO) Model Qwen-Base Qwen-Instruct Random Sentence-level Entropy Self-certainty TRAPO AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. 15.6 41.0 41.8 42.4 43.5 44.6 19.0 37.6 39.7 40.2 40.5 41. 18.2 70.3 80.8 81.8 80.4 83.2 11.1 24.7 35.8 35.4 35.8 37.4 16.9 34.1 43.2 43.7 42.9 45. 15.4 43.0 53.3 53.6 53.0 55.5 11.5/4.9 12.5/10.2 15.8/12.3 16.3/12.5 15.8/13.3 16.7/13.7 31.3 48.5 53.5 54.6 52.9 57. 43.6 80.4 79.8 80.2 80.7 81.0 7.4 32.7 34.8 35.3 36.6 37.3 37 Table 14 Overall performance on nine competition-level benchmarks for Qwen-2.5-7B using random selection or TraPO. Training was conducted with 1K labeled samples and 3K unlabeled samples. Model AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Qwen-Base Qwen-Instruct 11.5/4.9 12.5/10.2 No Selection 14.2/13. Random TRAPO Random TRAPO Random TRAPO Random TRAPO 14.9/13.3 15.8/13.5 15.8/12.3 16.7/13. 14.5/12.8 15.1/13.6 14.6/13.0 14.9/13.5 All Selection 14.9/10.7 31.3 48.5 52. 53.9 55.0 53.5 57.1 51.5 54.2 52.4 53.8 55.3 43.6 80. 80.2 79.7 80.3 79.8 81.0 77.2 80.5 78.5 79.9 77. 7.4 32.7 34.9 15.6 41.0 40.9 10% Selected 34.7 35.8 42.1 43. 30% Selected 34.8 37.3 41.8 44.6 50% Selected 31.5 35.2 40.0 42.5 70% Selected 34.0 34.9 40.8 41. 33.1 43.6 19.0 37.6 39.4 39.8 40.7 39.7 41. 37.9 40.1 38.4 39.8 39.2 18.2 70.3 76.2 80.3 81. 80.8 83.2 77.8 82.0 79.2 81.0 72.6 11.1 24.7 36. 34.9 35.8 35.8 37.4 34.8 36.2 35.2 35.4 35.4 16.9 34. 43.6 42.4 43.5 43.2 45.9 41.8 43.8 42.5 42.8 42. 15.4 43.0 52.1 52.5 53.6 53.3 55.5 51.5 54.0 52.3 53. 50.2 Table 15 Overall performance across nine competition-level benchmarks for Qwen-2.5-7B with varying ratios (σM) of unlabeled samples. Training uses 1K labeled samples and 3K unlabeled samples. AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Model Qwen-Base Qwen-Instruct σM = 0. 11.5/4.9 12.5/10.2 14.2/13.5 Token-level Entropy TRAPO 16.7/13.6 14.6/13.6 Token-level Entropy TRAPO 15.0/12.4 16.8/13. Token-level Entropy TRAPO 16.2/13.4 17.4/12.9 31.3 48.5 52.6 54.6 55.4 51.6 56. 52.1 57.2 Token-level Entropy TRAPO 18.2/11.9 17.9/13.8 53.4 58.7 43.6 80.4 80. 81.4 79.8 79.8 80.5 79.0 80.8 80.2 81.4 7.4 32.7 34. σM = 0.25 34.3 35.7 σM = 0.50 32.7 38.9 σM = 0.75 33.8 37.5 σM = 1.00 34.6 38.2 15.6 41.0 40. 41.3 42.1 39.9 43.6 39.1 44.3 41.9 45.5 19.0 37.6 39. 40.4 40.2 38.6 41.6 38.9 41.7 40.0 42.6 18.2 70.3 76. 79.6 81.9 77.3 82.8 77.6 82.5 72.9 83.7 11.1 24.7 36. 35.9 35.4 34.8 36.6 29.8 37.2 32.3 37.9 16.9 34.1 43. 44.6 44.0 42.9 44.9 41.3 45.9 44.0 46.8 15.4 43.0 52. 53.4 53.8 51.7 54.8 49.6 55.2 49.7 56.1 Table 16 Overall performance across nine competition-level benchmarks for Qwen-2.5-7B, averaged over three runs. Training was performed with 1K labeled samples and 3K unlabeled samples. Model AIME 24/25 AMC MATH-500 Minerva Olympiad Qwen-Base Qwen-Instruct 11.5/4.9 12.5/10.2 31.3 48. 43.6 80.4 7.4 32.7 15.6 41.0 Avg. 19.0 37.6 ARC-c GPQA MMLU-Pro 18.2 70.3 11.1 24.7 16.9 34.1 Avg. 15.4 43. TRAPO 18.2 0.3 / 13.6 0.2 59.3 0.5 81.9 0.4 37.9 0.4 45.8 0. 42.8 0.4 83.9 0.6 37.8 0.6 47.5 0.5 56.4 0.5 Table 17 Qwen-2.5-7B results on nine competition-level benchmarks using 1K labeled and 3K unlabeled samples, with average trajectory matching or maximum trajectory matching. Model AIME 24/25 AMC MATH-500 Minerva Olympiad Avg. ARC-c GPQA MMLU-Pro Avg. Qwen-Base Qwen-Instruct TRAPO-MAX TRAPO-MEAN 11.5/4.9 12.5/10. 16.3/9.9 17.9/13.8 31.3 48.5 52.7 58.7 43.6 80.4 80.8 81.4 7.4 32. 35.6 38.2 15.6 41.0 41.3 45.5 19.0 37.6 39.4 42.6 18.2 70. 81.6 83.7 11.1 24.7 33.2 37.9 16.9 34.1 42.6 46.8 15.4 43. 52.5 56."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Zhejiang University"
    ]
}