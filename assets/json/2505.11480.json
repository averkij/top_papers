{
    "paper_title": "Improving Assembly Code Performance with Large Language Models via Reinforcement Learning",
    "authors": [
        "Anjiang Wei",
        "Tarun Suresh",
        "Huanmi Tan",
        "Yinglun Xu",
        "Gagandeep Singh",
        "Ke Wang",
        "Alex Aiken"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 0 8 4 1 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Improving Assembly Code Performance with Large\nLanguage Models via Reinforcement Learning",
            "content": "Anjiang Wei1 Tarun Suresh2 Huanmi Tan3 Yinglun Xu2 Gagandeep Singh2 Ke Wang4 Alex Aiken1 1Stanford University 2University of Illinois Urbana-Champaign 3Carnegie Mellon University 4Visa Research {anjiang,aiken}@cs.stanford.edu"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated strong performance across wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industrystandard compiler gcc -O3. To support this study, we introduce benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47 over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models (LLMs) have achieved state-of-the-art solutions across wide range of programming tasks [15]. However, their potential for program optimization remains underexplored. Generating highly optimized code is critical in performance-sensitive domains, and prior work has investigated the use of LLMs to optimize C++ and Python programs [68]. In this work, we aim to utilize LLMs to improve the performance of assembly code, extending their capabilities beyond optimization for high-level languages. Assembly code optimization is traditionally the responsibility of compilers. While modern compilers apply series of rule-based transformations to improve performance, such design introduces the classic phase ordering problem [9], where the order of optimizations can substantially affect the performance of the generated code. Due to the inherent complexity of the optimization task, especially the vast space of possible transformation sequences, compilers face fundamental challenges in converging to optimal code, often leaving significant performance on the table [10]. An alternative approach is superoptimization, which searches the space of all programs that are functionally equivalent to the compilers output, aiming to identify the most performant variant. In principle, this strategy may yield optimal code. However, the search space grows exponentially with program size, making exhaustive exploration computationally infeasible in practice. Furthermore, prior work on superoptimization [11, 12] has primarily targeted loop-free, straight-line code, where it is more tractable to formally verify that the optimized code is semantically equivalent to the original. As result, these approaches are not directly applicable to most real-world programs with loops. Preprint. Under review. Figure 1: Overview of the assembly code optimization task. Given program and its baseline assembly from gcc -O3, an LLM is fine-tuned with Proximal Policy Optimization (PPO) to generate improved assembly. The reward function reflects correctness and performance based on test execution. In this work, we explore using large language models to optimize the performance of assembly code. Compared with high-level languages such as Python or C++, assembly code operates closer to the hardware, offering fine-grained control over execution and enabling optimizations that are difficult to express or realize in higher-level code. However, this setting poses several challenges. Assembly code is relatively rare and may be underrepresented in pretraining corpora [13], making it harder for LLMs to reason effectively about their behavior. Furthermore, industry compilers such as GCC have been extensively tuned by performance engineers over decades. Achieving additional speedups beyond gcc -O3 (the compilers highest optimization level) is technically challenging task. To address the challenges of low-level code optimization, we apply reinforcement learning to enhance the ability of LLMs to optimize assembly code. As shown in Figure 1, we use Proximal Policy Optimization (PPO) to train an LLM using reward function that considers both correctness and performance. Correctness is evaluated based on whether the generated code passes program-specific test cases, and performance is measured by its speedup relative to the baseline produced by gcc -O3. To support this setting, we construct new dataset of 8,072 assembly programs derived from real-world competitive programming submissions. Each instance includes input-output test cases and baseline assembly code generated by the compiler at the highest optimization level, which serves as the starting point for further optimization. We evaluate our approach on the proposed benchmark and find that reinforcement learning substantially improves the ability of LLMs to optimize assembly code. Starting from the base model Qwen2.5-Coder-7B-Instruct, which achieves modest 1.10x speedup over the gcc -O3 baseline, our PPO-trained model reaches 1.47 average speedup and improves both compile and test pass rates to 96.0%. It achieves the strongest performance across all evaluation metrics, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. Ablation studies show that reward functions emphasizing final speedup, rather than intermediate correctness signals, lead to more effective training. In summary, our contributions are as follows: We introduce the task of optimizing assembly code performance using large language models, aiming for fine-grained performance improvements beyond what traditional compiler optimizations can achieve. We construct dataset of real-world programs paired with the corresponding assembly code generated by the gcc -O3 baseline. Using this dataset, we explore improving LLMs on this task through reinforcement learning, applying Proximal Policy Optimization (PPO). We evaluate 21 LLMs on the proposed benchmark and show that our training substantially improves performance: Qwen2.5-Coder-7B-PPO achieves the highest compile and test pass rates, as well as the best average speedup (1.47) over the gcc -O3 baseline, outperforming all other models (including Claude-3.7-sonnet) across all evaluation metrics."
        },
        {
            "title": "2 Related Work",
            "content": "Large Language Models for Code. Benchmarks for evaluating large language models (LLMs) on code generation from natural language specifications have received increasing attention. Notable examples include HumanEval [1], MBPP [2], APPS [3], and more recent efforts [1417]. In parallel, many models have been developed to enhance code generation capabilities, such as Codex [1], AlphaCode [18], CodeGen [19], InCoder [20], StarCoder [21], DeepSeek-Coder [22], Code Llama [23], and others [24, 25]. Beyond code generation, LLMs have been applied to real-world software engineering tasks including automated program repair [26, 27], software testing [28, 29], bug localization [30], and transpilation [31, 32]. SWE-bench [4] integrates these tasks into benchmark for resolving real GitHub issues. Building on this, SWE-agent [5] and subsequent works [33, 34] employ an agent-based framework that leverages LLMs to improve the issue resolution process. Recent work has also explored LLMs for improving program performance. CodeRosetta [35] targets automatic parallelization, such as translating C++ to CUDA. Other efforts focus on optimizing Python code for efficiency [7, 8] or enabling self-adaptation [36], and improving C++ performance [6]. Of particular relevance are approaches to low-level code optimization [37, 38]. The LLM Compiler foundation models [39, 40] are primarily designed for code size reduction and binary disassembly, whereas our work focuses on optimizing assembly code for performance. LLM-Vectorizer [41] offers formally verified solution for auto-vectorization, specific compiler pass. In contrast, our work does not restrict the optimization type and uses test-case validation. Learning-Based Code Optimization. The space of code optimization is vast, and many approaches have leveraged machine learning to improve program performance. classic challenge in compilers is the phase-ordering problem, where performance depends heavily on the sequence of optimization passes. AutoPhase [42] uses deep reinforcement learning to tackle this, while Coreset [43] employs graph neural networks (GNNs) to guide optimization decisions. Modern compilers apply extensive rewrite rules but offer no guarantee of optimality. Superoptimization seeks the most efficient program among all semantically equivalent variants of the compiler output. Traditional methods use stochastic search, such as Markov Chain Monte Carlo [11], with follow-up work improving scalability [44, 12] and extending to broader domains [45, 46]. These rely on formal verification for correctness, restricting them to small, loop-free programs. In contrast, our approach uses test-based validation, enabling optimization of general programs with loops. With the rise of deep learning, substantial attention has turned to optimizing GPU kernel code. AutoTVM [47] pioneered statistical cost modelbased search for CUDA code optimization, followed by methods such as Ansor [48], AMOS [49], and other recent systems [5052]. More recently, using LLMs as code optimizers has gained popularity [6, 53, 37], with growing attention to reinforcement learning approaches that guide LLMs through reward-based feedback [54, 34]. CodeRL [55] incorporates unit test-based rewards within an actor-critic framework [56], while PPOCoder [57] extends this with Proximal Policy Optimization (PPO) [58], along with other variants [59]. Subsequent efforts have adapted RL-based techniques [60] to additional low-resource programming languages, including Verilog [61]. To the best of our knowledge, our work is the first to apply reinforcement learning to optimize assembly code using LLMs. Assembly code offers fine-grained control and potential for significant performance gains, but it remains underexplored due to limited training data and the complexity of low-level semantics."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Task Definition Let be program written in high-level language such as C. modern compiler like gcc can compile into an x86-64 assembly program = gcc(C), which can then be further assembled into an executable binary. The assembly program serves as an intermediate representation that exposes 3 low-level optimization opportunities, making it suitable for aggressive performance improvement. We assume the semantics-preserving nature of the compilation process, i.e., , so that the (cid:75) (cid:75) behavior of the assembly program is identical to that of the source program C. In theory, the goal is to produce program that is functionally equivalent to across the entire input space , i.e., (x) = (x) for all . Since verifying this property is undecidable in general, we approximate equivalence using finite test set = {(xi, yi)}n i=1, where each inputoutput pair (xi, yi) captures the expected behavior of C. We say that an assembly program is valid if it can be successfully assembled and linked into an executable binary. Let valid(P ) {True, False} denote this property. We define the set of correct programs as: = P (cid:74) (cid:74) S(P ) = {P valid(P ) (xi, yi) , (xi) = yi} . Performance and Speedup. Let t(P ) denote the execution time of on the test set , and let t(P ) be the corresponding execution time for . The speedup of relative to is defined as: Speedup(P ) = (cid:40) t(P ) t(P ) 1 if S(P ) and t(P ) < t(P ), otherwise. Optimization Objective. The objective is to generate candidate program that maximizes Speedup(P ). Only programs in S(P ) are eligible for speedup; any candidate that fails to compile into binary or produces incorrect outputs is assigned default speedup of 1. This reflects practical fallback: when the generated program is invalid, the system can revert to the baseline , compiled with gcc -O3, which defines the 1 reference performance. Although S(P ) captures the correctness criteria, we do not restrict the LLM to generate only valid programs. Instead, the model produces arbitrary assembly code, and correctness is verified post hoc via compilation and test execution. We train an LLM using reinforcement learning (see Section 3.3) to generate candidates that both satisfy correctness and achieve performance improvements. 3.2 Dataset Construction We construct our dataset using programs from CodeNet [62], large-scale corpus of competitive programming submissions. Each dataset instance is tuple (C, P, ), where is the original source code, = gcc_O3(C) is the corresponding x86-64 assembly generated by compiling with gcc at the -O3 optimization level, and = {(xi, yi)}n i=1 is the test set. Since not all CodeNet problems include test inputs, we adopt those provided by prior work [18] to define xi, but discard their output labels. Instead, we regenerate each output yi by executing the original submission on input xi, as many CodeNet programs are not accepted solutions, and even accepted ones do not reliably pass all test cases. Given the scale of CodeNet, which contains over 8 million and C++ submissions, we sample subset for this study. To focus on performance-critical cases, we sample programs that exhibit the highest relative speedup from gcc -O0 (no optimization) to gcc -O3 (maximum optimization). Such strategy serves two purposes: (1) it favors programs with complex logic that lead to suboptimal performance under -O0 and can be effectively optimized by -O3, and (2) it creates more challenging setting by starting from code that has already benefited from aggressive compiler optimizations. If an LLM can generate code that further improves upon gcc -O3, it suggests that the model can outperform the compilers expert solution. The final dataset consists of 7,872 training programs and 200 held-out evaluation programs, with additional statistics provided in Section 4. 3.3 Reinforcement Learning We conceptualize our task as standard contextual multi-armed bandit problem [63], defined by context space S, an action space A, and reward function : R. Each context represents problem instance, comprising the source program C, its baseline assembly , and the associated test cases . An action corresponds to generating candidate assembly program . The reward function r(s, a) evaluates the quality of the generated program based on correctness and performance. We will describe different designs of the reward function later. policy π : (A) 4 maps context to probability distribution over actions and samples an action stochastically. Given distribution µ over problem instances, the expected performance of policy π under reward function is expressed as Esµ,aπ(s) [r(s, a)]. The objective is to find policy that maximizes this expected reward. (cid:104) (cid:16) Optimization with PPO. We train the policy using Proximal Policy Optimization (PPO) [58], first-order policy-gradient algorithm that stabilizes training by constraining each policy update to remain close to the previous one. Specifically, PPO maximizes clipped surrogate objective of the form Es,a , where ρ(θ) = πθ(a s)/πθold(a s) is the probability ratio between the current and previous policy, ˆA is the estimated advantage of action in state s, and ϵ is clipping coefficient that limits the policy update to small trust region. We use critic model to estimate ˆA, and compute rewards based on the correctness and execution time of the generated program, eliminating the need for separate reward model. ρ(θ) ˆA, clip(ρ(θ), 1 ϵ, 1 + ϵ) ˆA min (cid:17)(cid:105) Reward Function Design. As defined in our contextual bandit setup, the reward function : assigns scalar score to each (context, action) pair. Each context consists of the source program C, the baseline assembly , and test set = {(xi, yi)}n i=1. An action corresponds to generation procedure that produces candidate assembly program = gen(a). We define two auxiliary metrics for computing reward: pass(s, a) = 1 (cid:88) (x,y)T 1[ (x) = y], speedup(s, a) = t(P )/t( ), which respectively denote the fraction of test cases passed and the speedup of the generated program relative to the baseline . We evaluate two reward function variants: 1. Correctness-Guided Speedup (CGS): r(s, a) = 1, pass(s, a), if fails to compile, if some tests fail, 1 + α speedup(s, a), if all tests pass. 2. Speedup-Only (SO): r(s, a) = (cid:40)0, if fails to compile or any test fails, speedup(s, a), otherwise. In CGS, the constant α controls the relative importance of speedup once full correctness is achieved (i.e., all test cases pass). The CGS reward provides dense signal by assigning intermediate credit for successful compilation and partially correct outputs, guiding the policy even when the final objective is not yet met. In contrast, SO defines more direct and sparse objective, assigning nonzero reward only to programs that are both correct and performant, thereby rewarding only the terminal goal of achieving speedup."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Dataset. We describe our dataset construction approach in Section 3.2. Each instance consists of source program C, the corresponding gcc -O3 compiled assembly , and set of test cases for correctness evaluation. The final dataset contains 7,872 training programs and 200 evaluation programs, with average program lengths and test case counts summarized in Table 1. Split # Prog. Avg. Tests Avg. LOC Assembly Training Evaluation 7,872 200 8.86 8.92 22.3 21.9 130.3 133.3 Table 1: Dataset statistics across training and evaluation splits. LOC = lines of code. 5 Prompts. For each instance, we construct prompt that includes the original program along with the generated assembly using gcc -O3. All test cases are withheld from the model. The model is instructed to generate only the optimized x86-64 assembly code. We show the prompt template in Appendix A.2. Metrics. We evaluate each model using both correctness and performance metrics. Compile pass is the percentage of problems for which the generated assembly compiles to binary executable successfully, while test pass is the percentage of problems where the compiled code passes all test cases. For given problem, any single failed test case is considered failure for the test pass metric. Both metrics are computed across the entire validation set. For performance, we measure the relative speedup over the gcc -O3 baseline. As defined in Section 3.1, we assign default speedup of 1 to any candidate that fails to compile, fails any test case, or is slower than the baseline. This reflects the practical setting where system can fall back to the gcc -O3 output, resulting in no performance gain. We report the 25th, 50th (median), and 75th percentiles of speedup to capture distributional behavior, along with the average speedup over the entire evaluation set. Models. We evaluate 21 state-of-the-art language models spanning diverse range of architectures. Our benchmark includes frontier proprietary models such as gpt-4o [64], o4-mini, gemini-2.0-flash-001 [65], and claude-3.7-sonnet, as well as open-source families such as Llama [66], DeepSeek [67], and Qwen [25]. In addition, we include models distilled from DeepSeekR1 [68] based on Qwen and Llama. Finally, we evaluate recent compiler foundation models [39, 40] that are pre-trained on assembly code, building upon Code Llama and designed specifically for compiler-related tasks. All open-source models are instruction-tuned. Performance Measurement. To ensure an accurate performance evaluation, we use hyperfine [69], benchmarking tool that reduces measurement noise by performing warmup runs followed by repeated timed executions. For each programs execution, we discard the first three runs and report the average runtime over the next ten runs. Implementation. We implement our customized reinforcement learning reward functions within the VERL framework [70], which enables fine-tuning of LLMs using Proximal Policy Optimization (PPO). As part of this setup, we build task-specific environment that handles program compilation, test execution, and runtime measurement, as detailed in Section 3.3. This environment provides the model with direct scalar feedback based on both functional correctness and execution performance. Training Configurations. Among all select Qwen2.5-Coder-7B-Instruct for training due to its strongest correctness results and substantial room for performance improvement, while intentionally avoiding compiler-specific foundation models to preserve generality. Training is performed on single node with four A100 GPUs. Full hyperparameter settings are provided in Appendix A.1. (see Table 2), we evaluated models"
        },
        {
            "title": "5 Results",
            "content": "5.1 Evaluation of Different Models Table 2 presents results across evaluated models. Most models struggle to generate performant assembly: the majority yield only 1.00 speedup, with low compile and test pass rates. Among baseline models, claude-3.7-sonnet and DeepSeek-V3 perform best, achieving test pass rates above 40% and average speedups of 1.22 and 1.21, respectively. Notably, some models such as DeepSeek-R1 fail to generate any valid assembly, and o4-mini achieves only 4.5% test pass. These results underscore the difficulty of the task and motivate the need for task-specific approach. Compiler foundation models (prefixed with llm-compiler-) are pretrained on assembly code and compiler intermediate representations. Among them, llm-compiler-13b demonstrates strong performance in both correctness and speedup. In contrast, the fine-tuned variants (-ftd) perform poorly, likely because they are adapted for tasks such as disassembling x86-64 and ARM assembly into LLVM-IR, rather than optimizing assembly code for execution performance. 6 Model DS-R1-Distill-Qwen-1.5B DeepSeek-R1 DS-R1-Distill-Llama-70B DS-R1-Distill-Qwen-14B gpt-4o-mini Llama-4-Maverick-17B Llama-3.2-11B gpt-4o Llama-4-Scout-17B o4-mini gemini-2.0-flash-001 Qwen2.5-72B Llama-3.2-90B Qwen2.5-Coder-7B DeepSeek-V3 claude-3.7-sonnet llm-compiler-7b-ftd llm-compiler-13b-ftd llm-compiler-7b llm-compiler-13b Compile Pass Test Pass Speedup Percentiles 50th 25th 75th Average Speedup 0.0% 1.00 0.0% 0.0% 1.00 0.0% 0.0% 1.00 5.5% 0.5% 1.00 11.5% 1.0% 1.00 44.5% 77.5% 7.0% 1.00 84.0% 21.0% 1.00 5.0% 1.00 81.0% 5.5% 1.00 68.5% 4.5% 1.00 25.0% 4.0% 1.00 57.5% 59.5% 7.5% 1.00 82.5% 15.0% 1.00 79.0% 61.0% 1.00 94.0% 43.0% 1.00 94.5% 58.5% 1.00 2.0% 1.00 2.0% 2.0% 1.00 2.5% 55.0% 54.0% 1.00 60.5% 59.5% 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.10 1.00 1.00 1.00 1.27 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.40 1.45 1.00 1.00 1.00 1.63 1.00 1.00 1.00 1.00 1.00 1.02 1.02 1.02 1.02 1.02 1.03 1.03 1.05 1.10 1.21 1.22 1.00 1.01 1.09 1.34 1.47 Qwen2.5-Coder-7B-PPO (Ours) 96.0% 96.0% 1.21 1.42 1.66 Table 2: Comparison of LLMs on our assembly optimization benchmark. We report compilation success rate, test pass rate, and average speedup over the gcc -O3 baseline. All open-source models are instruction-tuned. We evaluate general-purpose foundational models, compiler-specific foundation models, and our PPO-trained model, which improves average speedup from 1.10 to 1.47. We select Qwen2.5-Coder-7B-Instruct for RL training due to its strong compile pass rate (79.0%) and highest test pass rate (61.0%) among models. After PPO fine-tuning, it achieves 96.0% on both metrics and increases average speedup from 1.10 to 1.47. Notably, it is the only model to exhibit meaningful speedup even at the 25th percentile, and it outperforms all other models across all evaluation metrics, including correctness, average speedup, and speedup percentiles. 5.2 Ablation Study of Reward Function Design We evaluate two reward designs for RL: Correctness-Guided Speedup (CGS) and Speedup-Only (SO). CGS penalizes compilation failures, rewards partial correctness, and scales final reward by speedup once all tests pass. SO uses speedup as the sole reward, but only when all tests pass. Method Compile Pass Test Pass Avg. Speedup Base Model RL w/ CGS RL w/ SO 79.0% 95.5% 96.0% 61.0% 94.5% 96.0% 1.10x 1.38 1.47 As shown in Table 3, both variants achieve high compile pass rates and test pass rates, but SO yields better performance. Removing intermediate shaping appears to help the model focus on terminal objectives. We also tried varying the CGS scaling factor α (5 or 10) and found that it has negligible effect. Table 3: Ablation study comparing reward function variants. CGS provides intermediate reward shaping, while SO uses sparse and terminal signal. These results indicate that sparse, terminal rewards (SO) are more effective in this setting. Since the base model already reaches 61.0% test pass, correctness is not the bottleneck; optimizing directly for speedup offers stronger training signal. 7 Method Avg. Speedup Test Pass Compile Pass w/ O3 w/o O3 w/ O3 w/o O3 w/ O3 w/o O3 94.0% 25.5% 43.0% 4.5% 1.21 DeepSeek-V3 94.5% 53.0% 58.5% 16.0% 1.22 claude-3.7-sonnet 55.0% 12.0% 54.0% 0.0% 1.09 llm-compiler-7b 60.5% 2.0% 59.5% 0.5% 1.34 llm-comiler-13b Qwen2.5-Coder-7B 79.0% 0.0% 61.0% 0.0% 1.10 Qwen2.5-Coder-7B-PPO 96.0% 0.0% 96.0% 0.0% 1.47 1.02 1.07 1.00 1.00 1.00 1.00 Table 4: Ablation study on the impact of including gcc -O3 baseline assembly in the prompt. Each metric is reported with and without access to the baseline assembly generated by the compiler. 5.3 Can LLMs Directly Compile Programs without Baseline Assembly? In our main evaluation, we always include the baseline assembly generated by gcc -O3 in the prompt. While this baseline offers strong starting point for further optimization, it may also bias the model toward replicating patterns from the compilers output. In this subsection, we investigate more challenging setup: Can large language models directly compile code into performant assembly without relying on the compiler-generated baseline? We compare two settings in evaluation: (1) providing both the source and the gcc -O3 assembly (default, w/ O3 in Table 4), and (2) providing only the source (w/o O3). For each model, we report compile success rate, test pass rate, and average speedup. Table 4 shows that removing the baseline assembly leads to severe degradation. For instance, Qwen2.5-Coder-7B-PPO drops from 96.0% correctness and 1.47 speedup to 0.0% and 1.00, respectively. Even strong models like Claude-3.7-sonnet suffer substantial declines. These results suggest that direct compilation from to optimized assembly remains challenging for current LLMs. The compiler output provides reliable reference for LLMs. This supports our framework design: using gcc -O3 as an effective starting point for reinforcement learning. While future work may explore direct generation from C, we expect it to be substantially more challenging due to the absence of compiler guidance. 5.4 Case Study Figure 2 presents representative example where large language models (LLMs), including gpt4o and claude-3.7-sonnet, discover an optimization that outperforms state-of-the-art compiler. The original function computes the population count (i.e., the number of set bits) by repeatedly shifting the input and accumulating its least significant bit. The assembly code produced by gcc -O3 preserves this loop structure, relying on explicit bitwise operations and conditional branches to compute the result. In contrast, the LLM generates significantly more concise and efficient implementation that replaces the entire loop with single popcnt instruction. This instruction, supported by modern x86-64 architectures, performs the same computation in one operation, thereby reducing both instruction count and runtime overhead. Such transformation is beyond the reach of gcc -O3, which applies predetermined sequence of rule-based optimization passes and does not conduct semantic-level rewrites of this kind. In this case, the language model is able to synthesize functionally equivalent code that exploits hardware-level instructions not utilized by the compiler. This demonstrates the potential of language models to optimize assembly by exploring broader space of semantics-preserving program transformations."
        },
        {
            "title": "6 Discussion",
            "content": "Limitations. key limitation of our approach is the absence of formal correctness guarantees. Although we validate generated programs using input-output test cases, such testing is inherently incomplete and may overlook edge cases. Consequently, unlike fully verified systems such as Stoke [11], LLM-generated assembly may produce wrong results or exhibit undefined behavior. This"
        },
        {
            "title": "C Code",
            "content": "GCC -O3 Output"
        },
        {
            "title": "LLM Generated",
            "content": ".L0: popcnt % rdi , % rax retq int ( unsigned long ) { int res = 0 ; while ( > 0 ) { res += & 1 ; > >= 1 ; } return res ; } .L0: xorl % eax , % eax testq % rdi , % rdi je .L2 .L1: movq % rdi , % rdx andl $0x1 , % edx addq % rdx , % rax shrq $0x1 , % rdi jne .L1 retq .L2: retq Figure 2: Case study comparing the code, baseline assembly produced by gcc -O3, and optimized assembly generated by the LLM. The model successfully replaces the loop with the specialized hardware instruction popcnt, resulting in significantly more concise implementation. limitation reflects broader challenge in programming languages: verifying the semantic equivalence between two arbitrary programs is undecidable in the general case. Another limitation lies in the inherent randomness of performance measurement on real hardware. Although we mitigate noise through repeated measurements, low-level hardware fluctuations can still introduce variability into speedup estimates. Such nondeterminism is difficult to eliminate entirely in real-world settings. While prior work has adopted simulator-based evaluation [6], simulators may fail to faithfully capture the actual hardware performance. Finally, the observed performance gains may not generalize across machines. The model may implicitly learn and exploit hardware-specific characteristics like cache size. As result, model trained on one machine may not retain its effectiveness when deployed on different machine. Future Work. While we use Proximal Policy Optimization (PPO), future work may explore alternative reinforcement learning algorithms such as GRPO [71]. Expanding to larger and more diverse datasets, particularly those involving performance-critical code beyond competitive programming, would make the setting more realistic and applicable. Combining reinforcement learning with supervised fine-tuning may also be beneficial, although it remains unclear whether training on gcc -O3 outputs would provide additional gains. Another direction is to incorporate an interactive refinement loop, where the model iteratively updates its output using feedback from errors or performance measurements. Finally, extending our approach from x86-64 to other architectures such as MIPS, ARM, or GPU programming could broaden its applicability and impact."
        },
        {
            "title": "7 Conclusion",
            "content": "We explore the use of large language models (LLMs) for optimizing assembly code, setting where fine-grained control over execution enables performance improvements that are difficult to express in high-level languages. While traditional compilers rely on fixed rule-based transformations, they face fundamental limitations due to the complexity of the optimization space. To address this, we apply reinforcement learning to fine-tune LLMs with Proximal Policy Optimization (PPO), using reward function based on correctness and speedup over the gcc -O3 baseline. To support this effort, we introduce benchmark of 8,072 real-world programs with compiler-generated baseline assembly and test cases. Our resulting model, Qwen2.5-Coder-7B-PPO, achieves the highest compile and test pass rates (96.0%) and the best average speedup (1.47), outperforming all 20 other models evaluated across all metrics. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance."
        },
        {
            "title": "References",
            "content": "[1] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., Evaluating large language models trained on code, arXiv preprint arXiv:2107.03374, 2021. [2] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le et al., Program synthesis with large language models, arXiv preprint arXiv:2108.07732, 2021. [3] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song et al., Measuring coding challenge competence with apps, arXiv preprint arXiv:2105.09938, 2021. [4] C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan, Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [5] J. Yang, C. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press, Swe-agent: Agent-computer interfaces enable automated software engineering, Advances in Neural Information Processing Systems, vol. 37, pp. 50 52850 652, 2024. [6] A. Shypula, A. Madaan, Y. Zeng, U. Alon, J. Gardner, M. Hashemi, G. Neubig, P. Ranganathan, O. Bastani, and A. Yazdanbakhsh, Learning performance-improving code edits, arXiv preprint arXiv:2302.07867, 2023. [7] M. Du, A. T. Luu, B. Ji, Q. Liu, and S.-K. Ng, Mercury: code efficiency benchmark for code large language models, arXiv preprint arXiv:2402.07844, 2024. [8] J. Liu, S. Xie, J. Wang, Y. Wei, Y. Ding, and L. Zhang, Evaluating language models for efficient code generation, arXiv preprint arXiv:2408.06450, 2024. [9] T. I. R. Center, F. Allen, J. W. Transformations. https://books.google.com/books?id=oeXaZwEACAAJ and J. Cocke, Catalogue of Optimizing IBM Thomas J. Watson Research Center, 1971. [Online]. Available: [10] T. Theodoridis, M. Rigger, and Z. Su, Finding missed optimizations through the lens of dead code elimination, in Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2022, pp. 697709. [11] E. Schkufza, R. Sharma, and A. Aiken, Stochastic superoptimization, ACM SIGARCH Computer Architecture News, vol. 41, no. 1, pp. 305316, 2013. [12] R. Bunel, A. Desmaison, M. P. Kumar, P. H. Torr, and P. Kohli, Learning to superoptimize programs, arXiv preprint arXiv:1611.01787, 2016. [13] A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi, A. Tang, D. Pykhtar, J. Liu, Y. Wei et al., Starcoder 2 and the stack v2: The next generation, arXiv preprint arXiv:2402.19173, 2024. [14] J. Liu, C. S. Xia, Y. Wang, and L. Zhang, Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation, in Advances in Neural Information Processing Systems, 2023. [15] J. Li, G. Li, X. Zhang, Y. Zhao, Y. Dong, Z. Jin, B. Li, F. Huang, and Y. Li, Evocodebench: An evolving code generation benchmark with domain-specific evaluations, Advances in Neural Information Processing Systems, vol. 37, pp. 57 61957 641, 2024. [16] C. S. Xia, Y. Deng, and L. Zhang, Top leaderboard ranking= top coding proficiency, always? evoeval: Evolving coding benchmarks via llm, arXiv preprint arXiv:2403.19114, 2024. [17] T. Y. Zhuo, M. C. Vu, J. Chim, H. Hu, W. Yu, R. Widyasari, I. N. B. Yusuf, H. Zhan, J. He, I. Paul et al., Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions, arXiv preprint arXiv:2406.15877, 2024. 10 [18] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al., Competition-level code generation with alphacode, Science, vol. 378, no. 6624, pp. 10921097, 2022. [19] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, Codegen: An open large language model for code with multi-turn program synthesis, arXiv preprint arXiv:2203.13474, 2022. [20] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettlemoyer, and M. Lewis, Incoder: generative model for code infilling and synthesis, arXiv preprint arXiv:2204.05999, 2022. [21] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim et al., Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. [22] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li et al., Deepseek-coder: When the large language model meets programmingthe rise of code intelligence, arXiv preprint arXiv:2401.14196, 2024. [23] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez et al., Code llama: Open foundation models for code, arXiv preprint arXiv:2308.12950, 2023. [24] Y. Wei, Z. Wang, J. Liu, Y. Ding, and L. Zhang, Magicoder: Empowering code generation with oss-instruct, arXiv preprint arXiv:2312.02120, 2023. [25] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu et al., Qwen2. 5-coder technical report, arXiv preprint arXiv:2409.12186, 2024. [26] C. S. Xia and L. Zhang, Less training, more repairing please: revisiting automated program repair via zero-shot learning, in Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2022, pp. 959971. [27] C. S. Xia, Y. Wei, and L. Zhang, Automated program repair in the era of large pre-trained language models, in 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023, pp. 14821494. [28] C. S. Xia, M. Paltenghi, J. Le Tian, M. Pradel, and L. Zhang, Universal fuzzing via large language models, CoRR, 2023. [29] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, Large language models are edge-case generators: Crafting unusual programs for fuzzing deep learning libraries, in Proceedings of the 46th IEEE/ACM international conference on software engineering, 2024, pp. 113. [30] A. Z. Yang, C. Le Goues, R. Martins, and V. Hellendoorn, Large language models for test-free fault localization, in Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, 2024, pp. 112. [31] Z. Yang, F. Liu, Z. Yu, J. W. Keung, J. Li, S. Liu, Y. Hong, X. Ma, Z. Jin, and G. Li, Exploring and unleashing the power of large language models in automated code translation, Proceedings of the ACM on Software Engineering, vol. 1, no. FSE, pp. 15851608, 2024. [32] S. Bhatia, J. Qiu, N. Hasabnis, S. Seshia, and A. Cheung, Verified code transpilation with llms, Advances in Neural Information Processing Systems, vol. 37, pp. 41 39441 424, 2024. [33] C. S. Xia, Y. Deng, S. Dunn, and L. Zhang, Agentless: Demystifying llm-based software engineering agents, arXiv preprint arXiv:2407.01489, 2024. [34] Y. Wei, O. Duchenne, J. Copet, Q. Carbonneaux, L. Zhang, D. Fried, G. Synnaeve, R. Singh, and S. I. Wang, Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution, arXiv preprint arXiv:2502.18449, 2025. [35] A. TehraniJamsaz, A. Bhattacharjee, L. Chen, N. K. Ahmed, A. Yazdanbakhsh, and A. Jannesari, Coderosetta: Pushing the boundaries of unsupervised code translation for parallel programming, arXiv preprint arXiv:2410.20527, 2024. [36] D. Huang, J. Dai, H. Weng, P. Wu, Y. Qing, H. Cui, Z. Guo, and J. Zhang, Effilearner: Enhancing efficiency of generated code via self-optimization, Advances in Neural Information Processing Systems, vol. 37, pp. 84 48284 522, 2024. [37] A. Wei, A. Nie, T. S. Teixeira, R. Yadav, W. Lee, K. Wang, and A. Aiken, Improving parallel program performance through dsl-driven code generation with llm optimizers, arXiv preprint arXiv:2410.15625, 2024. [38] A. Ouyang, S. Guo, S. Arora, A. L. Zhang, W. Hu, C. Ré, and A. Mirhoseini, Kernelbench: Can llms write efficient gpu kernels? arXiv preprint arXiv:2502.10517, 2025. [39] C. Cummins, V. Seeker, D. Grubisic, B. Roziere, J. Gehring, G. Synnaeve, and H. Leather, Meta large language model compiler: Foundation models of compiler optimization, arXiv preprint arXiv:2407.02524, 2024. [40] , Llm compiler: Foundation language models for compiler optimization, in Proceedings of the 34th ACM SIGPLAN International Conference on Compiler Construction, 2025, pp. 141153. [41] J. Taneja, A. Laird, C. Yan, M. Musuvathi, and S. K. Lahiri, Llm-vectorizer: Llm-based verified loop vectorizer, in Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization, 2025, pp. 137149. [42] A. Haj-Ali, Q. J. Huang, J. Xiang, W. Moses, K. Asanovic, J. Wawrzynek, and I. Stoica, Autophase: Juggling hls phase orderings in random forests with deep reinforcement learning, Proceedings of machine learning and systems, vol. 2, pp. 7081, 2020. [43] Y. Liang, K. Stone, A. Shameli, C. Cummins, M. Elhoushi, J. Guo, B. Steiner, X. Yang, P. Xie, H. J. Leather et al., Learning compiler pass orders using coreset and normalized value prediction, in International Conference on Machine Learning. PMLR, 2023, pp. 20 74620 762. [44] P. M. Phothilimthana, A. Thakur, R. Bodik, and D. Dhurjati, Scaling up superoptimization, in Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems, 2016, pp. 297310. [45] R. Sharma, E. Schkufza, B. Churchill, and A. Aiken, Conditionally correct superoptimization, ACM SIGPLAN Notices, vol. 50, no. 10, pp. 147162, 2015. [46] B. Churchill, R. Sharma, J. Bastien, and A. Aiken, Sound loop superoptimization for google native client, ACM SIGPLAN Notices, vol. 52, no. 4, pp. 313326, 2017. [47] T. Chen, L. Zheng, E. Yan, Z. Jiang, T. Moreau, L. Ceze, C. Guestrin, and A. Krishnamurthy, Learning to optimize tensor programs, Advances in Neural Information Processing Systems, vol. 31, 2018. [48] L. Zheng, C. Jia, M. Sun, Z. Wu, C. H. Yu, A. Haj-Ali, Y. Wang, J. Yang, D. Zhuo, K. Sen et al., Ansor: Generating {High-Performance} tensor programs for deep learning, in 14th USENIX symposium on operating systems design and implementation (OSDI 20), 2020, pp. 863879. [49] S. Zheng, R. Chen, A. Wei, Y. Jin, Q. Han, L. Lu, B. Wu, X. Li, S. Yan, and Y. Liang, Amos: enabling automatic mapping for tensor computations on spatial accelerators with hardware abstraction, in Proceedings of the 49th Annual International Symposium on Computer Architecture, 2022, pp. 874887. [50] J. Shao, X. Zhou, S. Feng, B. Hou, R. Lai, H. Jin, W. Lin, M. Masuda, C. H. Yu, and T. Chen, Tensor program optimization with probabilistic programs, Advances in Neural Information Processing Systems, vol. 35, pp. 35 78335 796, 2022. 12 [51] Y. Zhao, H. Sharif, V. Adve, and S. Misailovic, Felix: Optimizing tensor programs with gradient descent, in Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, 2024, pp. 367381. [52] M. Wu, X. Cheng, S. Liu, C. Shi, J. Ji, K. Ao, P. Velliengiri, X. Miao, O. Padon, and Z. Jia, Mirage: multi-level superoptimizer for tensor programs, arXiv preprint arXiv:2405.05751, 2024. [53] D. Grubisic, C. Cummins, V. Seeker, and H. Leather, Compiler generated feedback for large language models, arXiv preprint arXiv:2403.14714, 2024. [54] S. Dou, Y. Liu, H. Jia, L. Xiong, E. Zhou, W. Shen, J. Shan, C. Huang, X. Wang, X. Fan et al., Stepcoder: Improve code generation with reinforcement learning from compiler feedback, arXiv preprint arXiv:2402.01391, 2024. [55] H. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi, Coderl: Mastering code generation through pretrained models and deep reinforcement learning, Advances in Neural Information Processing Systems, vol. 35, pp. 21 31421 328, 2022. [56] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, Policy gradient methods for reinforcement learning with function approximation, Advances in neural information processing systems, vol. 12, 1999. [57] P. Shojaee, A. Jain, S. Tipirneni, and C. K. Reddy, Execution-based code generation using deep reinforcement learning, arXiv preprint arXiv:2301.13816, 2023. [58] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. [59] J. Liu, Y. Zhu, K. Xiao, Q. Fu, X. Han, W. Yang, and D. Ye, Rltf: Reinforcement learning from unit test feedback, arXiv preprint arXiv:2307.04349, 2023. [60] J. Liu, T. Nguyen, M. Shang, H. Ding, X. Li, Y. Yu, V. Kumar, and Z. Wang, Learning code preference via synthetic evolution, arXiv preprint arXiv:2410.03837, 2024. [61] N. Wang, B. Yao, J. Zhou, X. Wang, Z. Jiang, and N. Guan, Large language model for verilog generation with code-structure-guided reinforcement learning, 2025. [Online]. Available: https://arxiv.org/abs/2407. [62] R. Puri, D. S. Kung, G. Janssen, W. Zhang, G. Domeniconi, V. Zolotov, J. Dolby, J. Chen, M. Choudhury, L. Decker et al., Codenet: large-scale ai for code dataset for learning diversity of coding tasks, arXiv preprint arXiv:2105.12655, 2021. [63] T. Lu, D. Pál, and M. Pál, Contextual multi-armed bandits, in Proceedings of the Thirteenth JMLR Workshop and international conference on Artificial Intelligence and Statistics. Conference Proceedings, 2010, pp. 485492. [64] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [65] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [66] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [67] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., Deepseek-v3 technical report, arXiv preprint arXiv:2412.19437, 2024. [68] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. 13 [69] Hyperfine, 2025. [Online]. Available: https://github.com/sharkdp/hyperfine [70] G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu, Hybridflow: flexible and efficient rlhf framework, arXiv preprint arXiv:2409.19256, 2024. [71] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Training Configurations Component Setting Base model Actors learning rate Critics learning rate Batch size Epoch Max prompt length Max response length Gradient checkpointing Rollout temperature Hardware Qwen2.5-Coder-7B-Instruct 1e-6 1e-5 16 1 2000 tokens 2000 tokens Enabled (both actor and critic) 0.5 4 A100 GPUs Table A1: Key training configurations for VERL PPO fine-tuning. A.2 Prompt Template Prompt Template Given the following code and assembly code, your task is to generate highly optimized x86-64 assembly code. Code: <C code here> Assembly Code: <baseline assembly code here produced by gcc -O3> Only output the optimized assembly code. Do not write any comments in the assembly code. assembly tags. Optimized Assembly Code: Do not include any other text. Wrap the assembly code in"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Stanford University",
        "University of Illinois Urbana-Champaign",
        "Visa Research"
    ]
}