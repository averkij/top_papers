{
    "paper_title": "Efficient Distillation of Classifier-Free Guidance using Adapters",
    "authors": [
        "Cristian Perez Jensen",
        "Seyedmorteza Sadat"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While classifier-free guidance (CFG) is essential for conditional diffusion models, it doubles the number of neural function evaluations (NFEs) per inference step. To mitigate this inefficiency, we introduce adapter guidance distillation (AGD), a novel approach that simulates CFG in a single forward pass. AGD leverages lightweight adapters to approximate CFG, effectively doubling the sampling speed while maintaining or even improving sample quality. Unlike prior guidance distillation methods that tune the entire model, AGD keeps the base model frozen and only trains minimal additional parameters ($\\sim$2%) to significantly reduce the resource requirement of the distillation phase. Additionally, this approach preserves the original model weights and enables the adapters to be seamlessly combined with other checkpoints derived from the same base model. We also address a key mismatch between training and inference in existing guidance distillation methods by training on CFG-guided trajectories instead of standard diffusion trajectories. Through extensive experiments, we show that AGD achieves comparable or superior FID to CFG across multiple architectures with only half the NFEs. Notably, our method enables the distillation of large models ($\\sim$2.6B parameters) on a single consumer GPU with 24 GB of VRAM, making it more accessible than previous approaches that require multiple high-end GPUs. We will publicly release the implementation of our method."
        },
        {
            "title": "Start",
            "content": "Efficient Distillation of Classifier-Free Guidance using Adapters Cristian Perez Jensen Seyedmorteza Sadat ETH Zürich {cjense, ssadat}@ethz.ch 5 2 0 2 M 0 1 ] . [ 1 4 7 2 7 0 . 3 0 5 2 : r Stable Diffusion XL Stable Diffusion 2. Diffusion Transformer Figure 1. Generated samples using adapter guidance distillation (AGD) applied to various models. By efficiently baking classifier-free guidance (CFG) into the base diffusion model, AGD generates high-quality samples with only single forward pass per inference step, which results in doubling the sampling speed compared to standard CFG."
        },
        {
            "title": "Abstract",
            "content": "While classifier-free guidance (CFG) is essential for conditional diffusion models, it doubles the number of neural function evaluations (NFEs) per inference step. To mitigate this inefficiency, we introduce adapter guidance distillation (AGD), novel approach that simulates CFG in single forward pass. AGD leverages lightweight adapters to ap- *These authors contributed equally to this work proximate CFG, effectively doubling the sampling speed while maintaining or even improving sample quality. Unlike prior guidance distillation methods that tune the entire model, AGD keeps the base model frozen and only trains minimal additional parameters (2%) to significantly reduce the resource requirement of the distillation phase. Additionally, this approach preserves the original model weights and enables the adapters to be seamlessly combined with other checkpoints derived from the same base model. We 1 also address key mismatch between training and inference in existing guidance distillation methods by training on CFG-guided trajectories instead of standard diffusion trajectories. Through extensive experiments, we show that AGD achieves comparable or superior FID to CFG across multiple architectures with only half the NFEs. Notably, our method enables the distillation of large models (2.6B parameters) on single consumer GPU with 24 GB of VRAM, making it more accessible than previous approaches that require multiple high-end GPUs. We will publicly release the implementation of our method. 1. Introduction Score-based diffusion models [9, 43, 46] are family of generative models that learn the data distribution by reversing forward process that progressively corrupts the data until it becomes indistinguishable from pure Gaussian noise. Theoretically, running the reverse diffusion process should enable accurate sampling from the data distribution, assuming access to the ground truth score function. However, in practice, unguided sampling from diffusion models often produces low-quality images that fail to align well with the given input condition due to optimization errors [15]. Accordingly, classifier-free guidance (CFG) [8] has become crucial technique in modern conditional diffusion models to enhance both generation quality and alignment to conditioning signalsthough this comes at the expense of reduced sample diversity [8, 35]. CFG is an inference method that enhances generation quality by leveraging the difference between conditional and unconditional model predictions at each inference step. This difference serves as an update direction to improve both quality and alignment with the target condition. However, CFG requires two forward passes per inference step, resulting in twice the sampling cost compared to unguided sampling. This increased cost introduces significant computational overhead, especially when sampling from large-scale diffusion models or employing these pre-trained models for tasks such as score distillation [31]. In this paper, we aim to double the sampling speed of CFG by training small set of adapters to integrate the CFG behavior directly into the model. Our method, called adapter guidance distillation (AGD), learns to replicate the CFG-guided output at each inference step using single forward pass while preserving the original diffusion model weights. These lightweight adapters add only 15% more parameters to the base model and introduce negligible latency overhead during inference. Since the base model remains frozen during training, and only the adapter parameters are updated, AGD is computationally efficient and can be trained on single consumer GPU with 24 GB of VRAM, even for large models like Stable Diffusion XL (SDXL). Furthermore, AGD allows the trained adapters to be seamlessly integrated with other checkpoints originating from the same base model, such as IP-adapters [49]. We demonstrate that our approach maintains or improves generation quality compared to standard CFG and outperforms existing methods such as guidance distillation (GD) [25], all while significantly reducing resource requirements during training. Moreover, we identify and address mismatch between training and inference trajectories in prior guidance distillation methods. We argue that effective guidance distillation requires training on CFG-guided trajectories computed by running the sampling process with CFG, as these differ significantly from standard diffusion trajectories obtained by adding noise to the training data. Furthermore, training on guided trajectories eliminates the need to load teacher model during distillation, thus reducing memory requirements when training AGD. Another advantage is that the distillation can be performed entirely on the synthetic data generated by the teacher model without needing any real dataset in advance. Our experiments demonstrate that training on CFG-guided trajectories enhances performance compared with training on diffusion trajectories. Figure 2 gives an overview of different components in AGD. In summary, our primary contributions are as follows: We introduce AGD, an efficient method for simulating CFG in single forward pass by training lightweight adapters alongside frozen base diffusion model, eliminating the need to fine-tune the entire model. We propose training AGD on CFG-guided trajectories instead of diffusion trajectories, reducing the mismatch between training and inference and improving performance. We demonstrate the resource efficiency of AGD by successfully distilling SDXL (2.6B parameters) on single RTX 4090 GPU with 24 GB of VRAM. Through extensive experiments, we show that AGD matches or surpasses CFG in performance across various models such as Diffusion Transformer and Stable Diffusion while doubling the sampling speed compared to CFG. 2. Related work Diffusion models [9, 4346] have emerged as leading approach for generative modeling across various domains, including images [3, 32, 33, 38], text [1, 10, 20], audio [6], and molecular generation [11]. Since the introduction of DDPM [9], significant progress has been made in multiple aspects, such as refining network architectures [5, 12, 16, 29], developing more efficient sampling methods [14, 22, 24, 40, 44], and introducing novel training strategies [14, 16, 27, 33, 39, 46]. Nevertheless, various guidance techniques [5, 8, 15, 37] have remained essential for enhancing generation quality and the alignment between conditioning inputs and generated outputs [28], though they lead to increased sampling time [8] and reduced diversity [19, 35]. 2 xt xt"
        },
        {
            "title": "Base model",
            "content": "ω ϵ xt, t, c, ω Base + adapter ˆϵ ϵ xt, t, c, ω Base + adapter ϵ xt, t, xt, t, (a) Trajectory collection (Section 4.1). (b) Training on collected trajectories (Section 4.2). (c) Inference with adapters. Figure 2. High-level overview of AGD components. (a) Instead of training on diffusion trajectories, we first run the sampling process with classifier-free guidance (CFG) and use the resulting guided trajectories (i.e., intermediate predictions at each time step t) as our training dataset. (b) We then introduce small adapters to the base model and train them to replicate the CFG-guided predictions from (a) while keeping the base model frozen. (c) During inference, the base model combined with the trained adapter produces guided predictions in single forward pass, effectively doubling the sampling speed compared to CFG. Several works have recently explored modifying the weight schedule of CFG by applying guidance only at certain sampling steps [2, 19, 48], primarily to balance diversity and quality in generation. However, these methods still require two neural function evaluations (NFEs) for most steps and therefore cannot fully double the inference speed. Additionally, our approach is orthogonal to these methods, as the distilled model can be used for the steps where CFG is applied in the above works. Alternatively, Meng et al. [25] introduced guidance distillation (GD), which fine-tunes diffusion model to generate guided predictions in single forward pass. However, fully fine-tuning the base model is often inefficient and unstable for large models, overwrites the original model weights, and demands high-end GPUs with substantial memory for training. To address these limitations, we propose more efficient distillation method using adapters. Moreover, Meng et al. [25] trained their model on standard diffusion trajectories, which we show to be less effective than training the distillation process on CFG-guided trajectories. Finally, adapters [13] have emerged as parameterefficient solution for fine-tuning large-scale diffusion models, mainly for integrating image conditions into pre-trained textto-image models [26, 49]. In contrast, we leverage adapters to inject guided predictions directly into the models forward pass. Notably, we demonstrate that adapters not only require significantly fewer training resources but also slightly outperform full fine-tuning. 3. Background In this section, we provide concise overview of diffusion models. Consider data point pdata and noise ϵ (0, I), and let the forward diffusion process be defined as xt = + σ(t)ϵ, where noise is gradually introduced over time [0, ]. The function σ(t) serves as the noise schedule, determining the extent of perturbation at each step, with σ(0) = 0 and σ(T ) = σmax. As shown by Karras et al. [14], this process can be described by the following ordinary differential equation (ODE): dxt = σ(t)σ(t) xt log pt(xt)dt, (1) where pt is the marginal distribution of the noisy data at time step t, transitioning from the original data distribution p0 = pdata to Gaussian prior pT = (0, σ2 maxI). Assuming access to the time-dependent score function xt log pt(xt), one can solve this ODE in reverse, i.e., from = to = 0, to generate new samples from pdata. The unknown score function xt log pt(xt) is typically learned using neural denoiser Dθ(xt, t), which is trained to recover clean samples from their noisy counterparts xt. Additionally, conditional generation can be achieved by extending the denoiser to Dθ(xt, t, c), where represents auxiliary conditioning information, such as class labels or text. Training Following [9], the denoiser Dθ(xt, t) is commonly parameterized as Dθ(xt, t) = xt σ(t)ϵθ(xt, t), (2) and is trained by predicting the added noise ϵ in xt, that is by solving θ argmin θ Ex,ϵ,t (cid:2)ϵθ(xt, t) ϵ2(cid:3). (3) After training, the score function can be approximated via xt log pt(xt) Dθ(xt, t) xt σ(t)2 = ϵθ(xt, t) σ(t) . (4) Classifier-free guidance CFG is an inference technique aimed at improving the quality of generated samples by blending the outputs of conditional and an unconditional model [8]. Specifically, CFG adjusts the denoisers output at each sampling step according to ϵθ(xt, t, c, ω) = ωϵθ(xt, t, c) (ω 1)ϵθ(xt, t, ), (5) 3 Algorithm 1 Trajectory collection for AGD. Require: Set of conditions Require: Guidance scale range [ωmin, ωmax] Require: Pre-trained diffusion model ϵθ 1: Ω = 2: for do 3: 4: ω Unif([ωmin, ωmax]) Run the reverse diffusion process from Equation (1) using ϵθ with ω and in steps 5: Cache (xt, t, c, ω) and the CFG-guided prediction ϵθ(xt, t, c, ω) at each sampling step: Ω Ω {(xt, t, c, ω, ϵθ(xt, t, c, ω))}N t=1 6: end for Algorithm 2 Adapter training for AGD. Require: Trajectory dataset Ω from Algorithm 1 Require: Model with adapters ϵ[θ,ψ] Require: Loss function ℓ 1: while not converged do 2: 3: 4: 5: end while (xt, t, c, ω, ϵ) Unif(Ω) = ℓ(ϵ, ϵ[θ,ψ](xt, t, c, ω)) Update ψ with gradient step on where ω = 1 corresponds to unguided sampling, and = represents the unconditional prediction. The unconditional model ϵθ(xt, t, ) is typically trained by randomly replacing the conditioning input with = during training. Alternatively, dedicated denoiser can be trained separately to approximate the unconditional score [16]. CFG is known to significantly improve generation quality, though it comes at the cost of doubling the sampling time [8]. 4. Adapter guidance distillation We now introduce our method, adapter guidance distillation (AGD), for doubling the sampling speed of CFG. As shown in Figure 2, AGD consists of two main components: (1) training on CFG-guided trajectories instead of standard diffusion trajectories, and (2) training lightweight adapters to distill CFG instead of fine-tuning the full model. Below, we discuss each component in detail, with Algorithms 1 and 2 also providing the training details of AGD. (a) Diffusion process with mixture of Gaussians as the data distribution. (b) Standard diffusion trajectory densities for each class obtained by adding noise to the data according to the forward diffusion process. (c) CFG-guided trajectory densities obtained by running the diffusion reverse process with classifier-free guidance. Figure 3. One-dimensional illustration of the mismatch between standard diffusion trajectories used for training in existing guidance distillation methods and the actual guided trajectories followed during inference. The CFG trajectories in (c) occupy regions in space distinct from the standard diffusion trajectories in (b). Training directly on CFG-guided trajectories ensures that the adapters focus on the regions primarily encountered during sampling with CFG. enhances guidance distillation by exposing the model to regions in space that the guided reverse process will follow. To bridge the gap between training and inference, we thus train AGD directly on CFG-guided trajectories. We generate guided trajectories as outlined in Algorithm 1, which are then used to train AGD. Since these trajectories can be cached, the teacher model does not need to be loaded during training, freeing up VRAM. Moreover, because this method only makes use of samples of the teacher model, it does not require an external dataset to train. Additionally, the trajectory dataset only needs to be collected once, enabling efficient hyperparameter tuning for the adapters. 4.1. Training on CFG-guided trajectories 4.2. Efficient distillation with adapters Prior guidance distillation methods are trained on standard diffusion trajectories, where noise is added to the training data, and the CFG prediction is matched at each inference step [25]. However, since CFG modifies the reverse process of diffusion models, guided trajectories differ significantly from standard diffusion trajectories, as shown in Figure 3. We argue that training directly on CFG-guided trajectories For more efficient training, AGD only uses small learnable modules, or adapters [13], to replicate the effect of CFG. Unlike tuning the whole diffusion network as in GD [25], we freeze the original model weights, ensuring that the base model is still available after training. This also allows us to use the learned adapters with other checkpoints that are obtained from the same base model, such as IP-adapters [49]. 4 The details of the adapters used in AGD are given below. Adapter formulation Let fθ denote an intermediate layer in the network with RLd being its upstream input. Further, fθ receives the time step and the condition embedding as input. An adapter gψ with parameters ψ is layer that combines fθ with encodings of the guidance scale ω, and the input conditions (t, c) via residual connection: f[θ,ψ](Z, ω, t, c) = fθ(Z, t, c) + gψ(Z, ω, t, c). (6) This architecture is illustrated in Figure 4. During training, the model weights θ are kept frozen and only the adapter parameters ψ are optimized to match the CFG step based on the trajectory dataset, as introduced in Section 4.1, i.e., ψ argmin ψ E(cid:2)ℓ(ϵ[θ,ψ](xt, t, c, ω), ϵθ(xt, t, c, ω))(cid:3), (7) Trainable ψ adapter adapter adapter where ϵθ(xt, t, c, ω) denotes CFG step with guidance scale ω, ϵ[θ,ψ](xt, t, c, ω) is the output of the model with the adapters, and ℓ is the loss function. ω ϵω [θ,ψ] head + block + block + block xt, t, Frozen θ Adapter architecture We mainly experiment with two adapter architectures: (1) cross-attention adapters, and (2) offset adapters. Let = [c1, . . . , cC] represent the matrix containing all conditioning embeddings (e.g., guidance scale, prompt embeddings, etc.), linearly projected to the same dimensionality via learned projection. Akin to IP-adapter [49], the cross-attention adapter formulates gψ as gψ(Z, ω, t, c) = Softmax (cid:18) QK (cid:19) V, (8) where = ZWq, = CWk, and = CWv. The offset adapter formulates gψ as Figure 4. Visual illustration of the trainable adapters alongside the frozen base model. The adapters are typically integrated with attention layers (either self-attention or cross-attention), and their outputs are added to those of the frozen attention blocks. Efficiency Since the adapters introduce only 15% additional parameters relative to the underlying model, their computational overhead remains negligible during both training and inference. Furthermore, unlike CFG, which requires two forward passes per diffusion step, our approach performs only one, effectively halving the NFEs. Consequently, our method achieves twice the speed of CFG when generating samples from pre-trained diffusion models. gψ(Z, ω, t, c) = MLP (cid:32) (cid:88) (cid:33) ci i=1 (9) 5. Experiments We found that offset adapters perform better for simpler models like DiT, whereas cross-attention adapters are more effective for text-to-image models. Several ablations on the adapter design space are provided in Appendix B. Implementation details We embed the guidance scale ω via Fourier feature encoder [47] followed by multilayer perceptron (MLP). We also extract the text or class embeddings from the base model (e.g., CLIP embeddings) and linearly project them into the same dimensionality as the guidance scale embedding. In DiT [29], we place an adapter in each transformer block after the self-attention mechanism. For text-to-image models such as Stable Diffusion 2.1 (SD2.1) [33] and SDXL [30], we place the adapters in conjunction with the cross-attention layers, since the text prompt is only used in these blocks. Setup We evaluate AGD on class-conditional generation using Diffusion Transformer (DiT) [29], and text-to-image generation using Stable Diffusion 2.1 (SD2.1) [33] and Stable Diffusion XL (SDXL) [30]. All experiments are conducted on single RTX 4090 GPU (24 GB of VRAM). Training is performed using the Adam optimizer [17] without weight decay, where the learning rate follows linear warmup to 1 104 over the first 10% of steps, after which it decays via cosine annealing schedule [23]. For training adapters on DiT, trajectories are sampled with guidance scales ω Unif([1, 6]), with four trajectories per class label of ImageNet [4]. For text-to-image models, we randomly select 500 captions from the COCO-2017 training set [21], generating single trajectory per caption with guidance scales ω Unif([1, 12]). Please refer to Appendix for additional details regarding the experiments."
        },
        {
            "title": "CFG",
            "content": "CFG AGD (Ours) (a) Diffusion Transformer (256256) AGD (Ours) (b) Stable Diffusion 2.1 (768768) AGD (Ours) (c) Stable Diffusion XL (10241024) Figure 5. Qualitative comparison between AGD and CFG. AGD produces samples with comparable quality to CFG while achieving twice the inference speed by requiring only single forward pass through the model. Additionally, AGD samples maintain structural similarity to CFG but often have better visual coherence. Evaluation metrics We mainly use Fréchet inception distance (FID) [7] to measure the quality and diversity of generated images, since it closely aligns with human perception. Given FIDs sensitivity to implementation details, we evaluate all models under identical conditions to ensure consistency. Additionally, we report precision as measure of generation quality and recall as an indicator of diversity [18]. 5.1. Qualitative results We evaluate the qualitative performance of AGD and CFG in Figure 5, generating samples using the same random seeds for both methods. Our results indicate that AGD produces images structurally similar to CFG while being more visually appealing across multiple models and resolutions. Thus, AGD retains the quality benefits of CFG while achieving twice the sampling speed per image. 5.2. Quantitative results The quantitative evaluation of AGD and CFG is shown in Table 1. We observe that AGD achieves metrics comparable to CFG, with both methods significantly outperforming the unguided sampling baseline. This confirms that AGD enhances generation quality similarly to CFG while requiring only half the NFEs. Notably, AGD even slightly outperforms CFG for the DiT model. 5.3. Comparing AGD with guidance distillation We next compare our method to guidance distillation (GD) [25], which fine-tunes the entire diffusion model to replicate guided outputs. We train AGD and GD under the same training setup using DiT as the base diffusion model for class-conditional ImageNet generation. Table 2 shows that AGD outperforms GD in FID while having significantly less 6 Table 1. Quantitative comparison between AGD and CFG. AGD outperforms CFG in class-conditional generation using DiT and performs similarly for text-to-image models (SD2.1 and SDXL). Model Guidance FID Precision Recall"
        },
        {
            "title": "DiT",
            "content": "SD2."
        },
        {
            "title": "SDXL",
            "content": "Unguided CFG [8] AGD (Ours) Unguided CFG [8] AGD (Ours) Unguided CFG [8] AGD (Ours) 12.57 5.30 5.03 49.94 20.94 21.09 60.30 22.82 22. 0.67 0.83 0.80 0.39 0.67 0.66 0.35 0.66 0.67 0.74 0.66 0.68 0.63 0.55 0.55 0.54 0.52 0. Table 2. Comparing AGD and GD [25] using DiT under the same training setup. AGD slightly outperforms GD while only training the adapters instead of tuning the full model. Method Params FID Precision Recall GD [25] AGD (ours) 676 16 5.66 5. 0.80 0.80 0.67 0.68 Table 3. Importance of training on guided trajectories. AGD performs best when trained on CFG-guided trajectories instead of the standard diffusion trajectories used in [25]. Guidance method FID Precision Recall CFG [8] AGD (Diffusion) AGD (Trajectory) 5.30 5.54 5.03 0.83 0.80 0.80 0.66 0.68 0.68 trainable parameters. Thus, we conclude that GD can be made significantly more efficient by keeping the base model frozen and only training the adapters. Moreover, Figure 6 shows that GD completely fails when used with guidance scales outside the domain seen during fine-tuning. In contrast, AGD remains robust to this issue, demonstrating better generalization across guidance scales. GD [25] AGD (Ours) 4 = ω 0 1 = ω Figure 6. Comparison of AGD with guidance distillation (GD) [25] for unseen guidance scales. While GD fails completely for out-ofdomain guidance scales, AGD continues to generate meaningful images. The models in this experiment were trained for ω [1, 6]."
        },
        {
            "title": "CFG",
            "content": "AGD (Ours) Figure 7. Using AGD with IP-adapter [49] and ControlNet [50] for SDXL. AGD can be integrated with other checkpoints derived from the same base model, achieving the benefits of both modules. 5.5. Training efficiency Table 4 compares the training speed and VRAM usage of AGD and GD [25]. We note that for larger networks like SDXL, AGD can successfully distill the model using consumer GPU with 24 GB VRAM, whereas GD encounters out-of-memory issues. Even when VRAM is not constraint, each training step of AGD remains significantly more efficient than GD (4.5 faster for DiT). 5.4. Importance of training on guided trajectories 5.6. Combining AGD with other adapters To validate our claim that training on guidance trajectories is beneficial, we compare AGD trained on standard diffusion trajectories with AGD trained on guidance trajectories. As shown in Table 3, training on guidance trajectories yields substantial improvement over training on standard diffusion trajectories. Hence, we conclude that bridging the train-inference gap by aligning these trajectories enhances performance, as it focuses training on regions of the space that are important for CFG. Figure 7 shows samples from combining AGD with IPadapter [49] and ControlNet [50]. As can be seen, AGD continues to produce high-quality samples when other adapters or networks are applied to the same model, while achieving the goals of both modules. 5.7. AGD vs guidance scale Figure 8 shows how the performance of AGD varies as we increase the guidance scale ω. We observe that the FID 7 10"
        },
        {
            "title": "Recall",
            "content": "0.9 0.8 0.7 0.7 0.6 0. 1.5 2 ω 2.5 3 1 1. 2 ω 2.5 0.4 1 3 1. 2.5 3 2 ω CFG AGD (Ours) Figure 8. The performance of DiT with AGD as we increase the guidance scale. Compared to CFG, AGD offers better trade-off between precision and recall resulting in better FID for most guidance scales. Table 4. Comparing the memory requirements and training speed of AGD and GD. AGD enables the distillation of large models on an RTX 4090 with 24 GB of VRAM while also being significantly faster at each training iteration. Model Method VRAM (GB) Speed (it/s) DiT SD2. SDXL GD [25] AGD (Ours) GD [25] AGD (Ours) GD [25] AGD (Ours) 17.67 16.79 0.52 2. Out-of-memory 23.83 2.05 Out-of-memory 22.77 3. curve for CFG is more peaked, whereas the curve for AGD is relatively flatter, making it less sensitive to the exact guidance value at inference for good performance. Additionally, we note that AGD have more favorable trade-off between precision and recall compared to CFG, resulting in better FID scores for most guidance scales. 5.8. Changing the scheduler Next, we show that AGD is not sensitive to the exact choice of scheduler used for generating guided trajectories. Figure 9 presents samples from the DDPM algorithm [9], where the adapters were trained on DDIM trajectories [44]. AGD is able to produce high-quality images even when different scheduler is used during inference. Moreover, Table 5 shows that the FID scores remain comparable for both schedulers. 6. Conclusion This paper introduced adapter guidance distillation (AGD), an efficient approach to achieving the benefits of classifierfree guidance at half the sampling cost. By training lightweight adapters to estimate guided outputs and training on CFG-guided trajectories, we address both the computa-"
        },
        {
            "title": "DDPM",
            "content": "Figure 9. Samples of SD2.1 generated with AGD using the DDPM sampler, with adapters trained on DDIM trajectories. AGD successfully generates high-quality samples when applied with different schedulers at inference. Table 5. Effect of using different diffusion sampler at inference. The adapter in this case was trained on DDIM trajectories, but other sampling methods such as DDPM can be used at inference. Sampler FID Precision Recall DDIM [44] 21.09 22.15 DDPM [9] 0.66 0. 0.55 0.51 tional overhead and the train-inference mismatch of prior guidance distillation methods. Through extensive experiments, we showed that AGD matches or surpasses CFG performance, remains robust to previously unseen guidance scales, and can be trained on single consumer GPU even for large models such as SDXL. Thus, we believe that AGD offers an efficient and flexible alternative to prior guidance distillation methods while eliminating the sampling overhead of classifier-free guidance. Future research directions could explore integrating AGD with enhanced guidance algorithms [15, 19, 36] and leveraging adapters for other distillation techniques, e.g., adversarial distillation [41, 42], to further reduce the sampling time of diffusion models."
        },
        {
            "title": "References",
            "content": "[1] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. 2 [2] Angela Castillo, Jonas Kohler, Juan Pérez, Juan Pablo Pérez, Albert Pumarola, Bernard Ghanem, Pablo Arbeláez, and Ali Thabet. Adaptive guidance: Training-free acceleration of conditional diffusion models. arXiv preprint arXiv:2312.12487, 2023. 3 [3] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. 2 [4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 5 [5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2, 12 [6] Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. In Forty-first International Conference on Machine Learning, 2024. [7] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [8] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 3, 4, 7 [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3, 8 [10] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in neural information processing systems, 34:1245412465, 2021. 2 [11] Emiel Hoogeboom, Vıctor Garcia Satorras, Clément Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning, pages 88678887. PMLR, 2022. 2 [12] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. CoRR, abs/2301.11093, 2023. 2 [13] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer In International conference on machine learning for nlp. learning, pages 27902799. PMLR, 2019. 3, 4, [14] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 2, 3 [15] Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024. 2, 8 [16] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. 2, 4 [17] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5 [18] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 6 [19] Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. 2, 3, [20] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35:43284343, 2022. 2 [21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 5 [22] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. 2 [23] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 5 [24] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ODE solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. 2 [25] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 2, 3, 4, 6, 7, 8 [26] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 42964304, 2024. [27] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In Proceedings denoising diffusion probabilistic models. of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pages 81628171. PMLR, 2021. 2 9 Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. 2 [41] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 8 [42] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pages 87103. Springer, 2025. 8 [43] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. [44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 2, 8 [45] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [46] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [47] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:75377547, 2020. 5, 11 [48] Xi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule CANI, Victoria Fernandez Abrevaya, David Picard, and Vicky Kalogeiton. Analysis of classifier-free guidance weight schedulers. Transactions on Machine Learning Research, 2024. 3 [49] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3, 4, 5, [50] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 7 [28] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, pages 16784 16804. PMLR, 2022. 2 [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 2, 5 [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 5 [31] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, 2023. 2 [32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2): 3, 2022. [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 5, 11 [34] Negar Rostamzadeh, Emily Denton, and Linda Petrini. Ethics and creativity in computer vision. CoRR, abs/2112.03111, 2021. 11 [35] Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann M. Weber. CADS: Unleashing the diversity of diffusion models through condition-annealed sampling. In The Twelfth International Conference on Learning Representations, 2024. 2 [36] Seyedmorteza Sadat, Otmar Hilliges, and Romann Weber. Eliminating oversaturation and artifacts of high guidance scales in diffusion models. arXiv preprint arXiv:2410.02416, 2024. 8 [37] Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann M. Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. 2 [38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [39] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 2 [40] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In The Tenth International 10 A. Broader impact Table 6. Ablation on adapter architecture using DiT. Our method accelerates guided sampling in diffusion models, broadening accessibility to large-scale text-to-image or classconditional generative systems. This can reduce energy consumption and computational barriers to use AI-generated content for various creative applications. However, while advancements in AI-generated content have the potential to improve efficiency and stimulate creativity, it is essential to consider the associated ethical implications. For more indepth exploration of ethics and creativity in computer vision, we refer readers to Rostamzadeh et al. [34] B. Ablation studies This section presents our ablation studies. Unless otherwise specified, all experiments are conducted using the DiT model for class-conditional generation. We use FID as the primary metric to determine the adapter configuration used in the main experiments. Adapter architecture We first examine various design choices for the adapter architecture gψ(Z, ω, t, c). Let = [c1, . . . , cC] represent the matrix containing all conditioning embeddings. The cross-attention and offset adapter architectures are formalized in Equations (8) and (9) respectively. We further experimented with gating architecture defined as gψ(Z, ω, t, c) = (cid:16) σ( Zv) MLP( Z) (cid:17) W, (10) i=1 ci (cid:104) zj, (cid:80)C (cid:105) where zj = , σ is the sigmoid function, and : RT RT RT scales each d-dimensional vector independently. Lastly, we also considered positional encoding adapter architecture given by gψ(Z, ω, t, c) = MLP(c), (11) (cid:104) ej, (cid:80)C (cid:105) i=1 ci where cj = and ej encodes the j-th attention time step. Specifically, ej is computed by Fourier feature encoder [47], followed by an MLP. The performance of these architectures using the DiT model are given in Table 6. Note that for the DiT model, the offset architecture works the best. However, as shown in Table 7, the crossattention adapter works better for text-to-image models such as Stable Diffusion [33]. Hence, we used the offset architecture for class-conditional generation, and the cross-attention adapter for more complex text-to-image models. We also experimented with using dropout in the offset MLPs for further regularization but found that the model performs best without using any dropout (see Table 8). Architecture FID Precision Recall Cross-attention Offset Positional encoding Gating 5.49 5.03 5.25 5.54 0.83 0.80 0.81 0. 0.65 0.68 0.66 0.66 Table 7. Ablation on the adapter architecture for SD2.1. Architecture FID Precision Recall Cross-attention 21.09 22.05 Offset 0.66 0. 0.55 0.54 Table 8. Ablation on the dropout rate using DiT. Dropout FID Precision Recall 0% 10% 20% 50% 5.22 5.27 5.39 5.69 0.80 0.82 0.83 0. 0.67 0.67 0.66 0.63 Table 9. Ablation on the hidden dimensionality of the adapters. Dim. Params FID Precision Recall 64 128 256 512 0.8% 2.5% 6.1% 17.2% 5.33 5.03 5.22 5. 0.81 0.80 0.80 0.81 0.67 0.68 0.67 0.67 Table 10. Ablation on the initialization type of the adapter layers. Init. scheme FID Precision Recall Zero Xavier 5.24 5. 0.81 0.80 0.68 0.68 that increasing the hidden dimension initially improves FID but eventually leads to degradation, likely due to overfitting. Therefore, we recommend designing adapters with fewer than 5% additional parameters w.r.t. the base model. Adapter initialization While adapters are typically initialized with zero values such that ϵ[θ,ψ] = ϵθ at initialization [13], Table 10 shows that Xavier initialization yields better results for guidance distillation. Therefore, we recommend avoiding zero initialization of the adapters for AGD. Dimensionality of the adapter We now examine the impact of adapter dimensionality in Table 9. Our results show Training loss functions We also explored various loss functions for training AGD. Specifically, we experimented 11 24,GB of VRAM. For all quantitative experiments, we set the guidance scale to the value that achieved the best FID for each method. The AGD implementation will be publicly released to support further research on guidance distillation. The FID scores for class-conditional models were computed using 10k generated samples and the entire ImageNet validation set. For text-to-image models, we used the full COCO-2017 validation set as the real data. All metrics were computed using the ADM evaluation code base [5] to ensure fairness across experiments. Table 11. Effect of using different loss functions for distillation. Loss Weight λ(t) FID ℓ1 ℓ2 ℓ2 ℓ2 1 1 σ(t) 2 1 cos (ϵθ, ϵθ) 1 9.91 5.03 5.30 6.64 with ℓ1(x, y) = y1 and weighted ℓ2(x, y) = λ(t)x y2 2, where λ(t) is weighting function depending on the time step. As shown in Table 11, the simple ℓ2 loss with λ(t) = 1 performs best. C. Details of the evaluation samples DiT The samples used guidance scale 4. SD2.1 The samples used guidance scale 10. From left to right, the prompts used in Figure 5b were: 1. cat on the flower. 2. close-up of blooming flower. 3. quiet beach at sunset with gentle waves. 4. calm lake reflecting the blue sky. SDXL The samples used guidance scale 12. Further, the prompts used in Figure 5c were: 1. modern reinterpretation of classical Renaissance painting, where futuristic elements and digital motifs merge with traditional portraiture. 2. fantastical scene of celestial garden floating in space, featuring luminous, otherworldly flora against backdrop of swirling galaxies. 3. hyper-realistic digital painting of futuristic metropolis at sunset, with neon lights reflecting off rain-soaked streets and towering holograms. 4. cozy winter scene of remote mountain village, with softly glowing windows, snow-covered rooftops, and star-filled night sky. D. Additional visual samples Figures 10 to 21 provides more uncurated samples of AGD and CFG on various models used in the paper. The samples are best viewed when zoomed in. E. Additional implementation details Section 4.2 provides the main implementation details. The DiT model was trained with batch size of 64 for 5000 gradient steps, the SD2.1 model with batch size of 8 for 5000 gradient steps, and the SDXL model with batch size of 1 for 20000 gradient steps. These settings were selected based on the maximum batch size that fit within"
        },
        {
            "title": "CFG",
            "content": "AGD (Ours) Figure 10. Uncurated samples using DiT. Class label: Space shuttle (812), guidance scale: 2. CFG AGD (Ours) Figure 11. Uncurated samples using DiT. Class label: Golden retriever (207), guidance scale: 3. CFG AGD (Ours) Figure 12. Uncurated samples using DiT. Class lable: Macaw (88), guidance scale: 3."
        },
        {
            "title": "CFG",
            "content": "AGD (Ours) Figure 13. Uncurated samples using DiT. Class label: Arctic fox (279), guidance scale: 4. CFG AGD (Ours) Figure 14. Uncurated samples using DiT. Class lable: Red panda (387), guidance scale: 5. CFG AGD (Ours) Figure 15. Uncurated samples using DiT. Class label: Dog sled (537), guidance scale: 6."
        },
        {
            "title": "CFG",
            "content": "AGD (Ours) Figure 16. Uncurated samples using SD2.1. Prompt: close up of clear vase with flowers., guidance scale: 10. CFG AGD (Ours) Figure 17. Uncurated samples using SD2.1. Prompt: set of plush toy teddy bears sitting in sled., guidance scale: 10. CFG AGD (Ours) Figure 18. Uncurated samples using SD2.1. Prompt: People flying kites in park on windy day., guidance scale: 10."
        },
        {
            "title": "CFG",
            "content": "AGD (Ours) Figure 19. Uncurated samples using SDXL. Prompt: Two stuffed animals posed together in black and white. guidance scale: 12. CFG AGD (Ours) Figure 20. Uncurated samples using SDXL Prompt: capybara made of lego sitting in realistic, natural field., guidance scale: 12. CFG AGD (Ours) Figure 21. Uncurated samples using SDXL Prompt: close-up of fire spitting dragon, cinematic shot., guidance scale: 12."
        }
    ],
    "affiliations": [
        "ETH Zürich"
    ]
}