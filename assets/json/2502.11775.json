{
    "paper_title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
    "authors": [
        "Guangzhi Sun",
        "Yudong Yang",
        "Jimin Zhuang",
        "Changli Tang",
        "Yixuan Li",
        "Wei Li",
        "Zejun MA",
        "Chao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding.This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop a reasoning-intensive dataset featuring challenging audio-visual questions with step-by-step solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient step-level reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. video-SALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONN-o1 zero-shot synthetic video detection capabilities."
        },
        {
            "title": "Start",
            "content": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Guangzhi Sun 1 2 Yudong Yang 2 3 Jimin Zhuang 2 3 Changli Tang 2 3 Yixuan Li 2 3 Wei Li 2 Zejun MA 2 Chao Zhang 3 5 2 0 2 7 1 ] . [ 1 5 7 7 1 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While recent advancements in reasoning optimization have significantly enhanced the capabilities of large language models (LLMs), existing efforts to improve reasoning have been limited to solving mathematical problems and focusing on visual graphical inputs, neglecting broader applications in general video understanding. This paper proposes video-SALMONN-o1, the first open-source reasoning-enhanced audiovisual LLM designed for general video understanding tasks. To enhance its reasoning abilities, we develop reasoning-intensive dataset featuring challenging audio-visual questions with step-bystep solutions. We also propose process direct preference optimization (pDPO), which leverages contrastive step selection to achieve efficient steplevel reward modelling tailored for multimodal inputs. Additionally, we introduce RivaBench, the first reasoning-intensive video understanding benchmark, featuring over 4,000 high-quality, expert-curated question-answer pairs across scenarios such as standup comedy, academic presentations, and synthetic video detection. videoSALMONN-o1 achieves 3-8% accuracy improvements over the LLaVA-OneVision baseline across different video reasoning benchmarks. Besides, pDPO achieves 6-8% improvements compared to the supervised fine-tuning model on RivaBench. Enhanced reasoning enables video-SALMONNo1 zero-shot synthetic video detection capabilities. Demo Page: https://github.com/ BriansIDP/video-SALMONN-o1 1. Introduction The recent advancements in optimizing the reasoning process have further boosted text-based large language models (LLMs) (OpenAI, 2024; DeepSeek Team, 2024; Qwen 1Univeristy of Cambridge 2ByteDance 3Tsinghua university. Correspondence to: Chao Zhang <cz277@tsinghua.edu.cn>. Preprint. 1 Team, 2024b; Zhao et al., 2024; Yuan et al., 2024) performance in answering complex logical questions, such as math problems (Yang et al., 2024; Wang et al., 2024b; Sun et al., 2024d; Ying et al., 2024) and coding tasks (Zhang et al., 2024e). These methods usually first split the solution into multiple simpler steps to form reasoning path ending with the final solution, as demonstrated in chain-of-thought (CoT) (Wei et al., 2022). Advanced training approaches have been developed such as the outcome reward model (ORM) (Cobbe et al., 2021; Yu et al., 2024; Zhang et al., 2024b) that optimizes the entire reasoning path based on the final solution, and the process reward model (PRM) (Uesato et al., 2022; Lightman et al., 2023; Luo et al., 2024; Zhang et al., 2024a) that optimizes each reasoning step based on how likely each step would lead to correct answer. In addition to text-based questions, reasoning also plays an indispensable role in understanding the physical world, such as comprehending concepts in an academic presentation, interpreting complex interactions among people or even detecting artificial anomalies. Thus, improving reasoning ability is also critical for multimodal LLMs (Tang et al., 2024c;b; Sun et al., 2024b; Cheng et al., 2024; Zhang et al., 2024d; Lin et al., 2024; Team et al., 2024; Wang et al., 2024a; Tang et al., 2024a) that process audio and visual inputs in addition to text, as the interactions among multiple modalities can largely increase the difficulty of the task. To this end, investigations have been performed on optimizing the reasoning process with multimodal inputs (Du et al., 2024), and on particularly visual LLMs (Qwen Team, 2024a; Xu et al., 2024; Du et al., 2025). However, current research on enhancing reasoning capabilities for multimodal LLMs has predominantly focused on solving mathematical problems and image inputs. This overlooks the importance of reasoning in general video understanding and the interactions among audio, visual and text modalities, largely limiting their scopes of applications. This paper proposes video-SALMONN-o1, the first opensource reasoning-enhanced audio-visual LLM with improved reasoning abilities in general video understanding tasks. The audio-visual reasoning capability of videoSALMONN-o1 is first enhanced by creating new dataset with challenging questions and step-by-step solutions for supervised fine-tuning (SFT), and then further boosted by the video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model proposed variant of direct preference optimization (DPO), process DPO (pDPO) (Rafailov et al., 2024; Zhang et al., 2024c). pDPO achieves step-level pairwise reward modelling via an efficient contrastive step selection approach tailored for multimodal inputs. While being more effective than the standard PRMs in general video understanding, pDPO and the step selection make audio-visual reasoning more efficient without the need for an external reward model or two-pass re-ranking pipeline. To evaluate the performance on multimodal reasoning for general video understanding, we propose the first reasoningintensive video with audio understanding benchmark (RivaBench). RivaBench primarily focuses on three representative scenarios, including standup comedy, academic presentation and synthetic video detection. In particular, RivaBench contains over 4k high-quality question-answer pairs that are carefully crafted by human experts (e.g. medical doctors). Our key contributions are summarized as follows: We propose video-SALMONN-o1, the first open-source reasoning-enhanced audio-visual LLM for general video understanding tasks. video-SALMONN-o1 is the first to explore RL-based reasoning optimization for general video understanding. The proposed pDPO method with efficient contrastive step selection further enhances reasoning abilities. We propose RivaBench, the first general video understanding benchmark focusing on challenging audio-visual reasoning scenarios with human expert annotations. video-SALMONN-o1 consistently outperforms the strong LLaVA-OneVision visual baseline on VideoMME, NExTQA and RivaBench, with 3-8% absolute accuracy improvements. The pDPO training achieved 6-8% improvements on RivaBench over the SFT model. Moreover, video-SALMONN-o1 is also the first open-source model that showed zero-shot synthetic video detection ability. 2. Related Work 2.1. CoT Reasoning CoT reasoning is one of the remarkable abilities of LLMs when solving difficult and complex problems. Earlier investigations employed prompt tuning and various search algorithms such as the Monte-Carlo tree search during inference time (Hao et al., 2023; Snell et al., 2024; Feng et al., 2024; Yao et al., 2023; Goyal et al., 2024). Later on, training stage approaches using reinforcement learning (RL) were developed to further and more radically boost the reasoning capabilities of LLMs. PRMs which estimate the value function of each reasoning step have emerged as one of the most prevalent approaches in reasoning optimization tasks (Uesato et al., 2022; Lightman et al., 2023; Luo et al., 2024; Zhang et al., 2024a; Li et al., 2023b). However, constructing step-level annotations for PRM training can be expensive and difficult to scale up. As mitigation, Wang et al. (2024b) and Luo et al. (2024) proposed automatic step annotation using rollout, which approximated the expected correctness of each step by sampling multiple paths till the end with the same prefix solution. In particular, Luo et al. (2024) treats the first wrong step as the critical step to perform rollout which was found by binary search. 2.2. Reasoning in Multimodal LLMs Researchers have been investigating optimizing CoT reasoning for multimodal LLMs to tackle increasingly challenging tasks. Most of them focus on extracting graphical or text information from an image and solving mathematical tasks based on the extracted information. Specifically, LLaVA-CoT (Xu et al., 2024) investigated better sampling and search algorithms to find better reasoning path for math questions with image inputs. Virgo, on the other hand, explores the fine-tuning data organization and transferability of text-based reasoning tasks to image-based reasoning tasks (Du et al., 2025). Recently, MAmmoTH-VL (Guo et al., 2024) built large-scale multimodal instructiontuning dataset that can improve the question-answering performance on diverse modalities including video. Different from these works, video-SALMONN-o1 particularly focuses on general video understanding scenarios, where different parts of the audio-visual information are constantly referred to during the reasoning process. 2.3. Benchmarks for Audio-visual LLMs The fast-paced development of multimodal LLMs has boosted the creation of more challenging video understanding benchmarks. Benchmark focus evolves from video description and perception abilities (Li et al., 2021; Alamri et al., 2019; Chen et al., 2023a; Li et al., 2022; Chen et al., 2023b; Ning et al., 2023; Mangalam et al., 2023; Yun et al., 2021; Sun et al., 2024a), to video reasoning abilities such as inference about temporal and causal relations (Xiao et al., 2021; Li et al., 2024b; 2023a; Fu et al., 2024; Liu et al., 2024a; Fang et al., 2024). In particular, NExT-QA (Xiao et al., 2021) focuses on causal relation reasoning such as why certain action is performed, and Video-MME (Fu et al., 2024) contains questions that require the combination of both audio and visual information to perform reasoning. Our proposed RivaBench has more challenging questions that require longer thinking steps, broader world knowledge and tighter combination of audio-visual information. 3. video-SALMONN-o1 3.1. Model Structure We adopt the same model structure as video-SALMONN 2, as shown in Fig. 1. As video-SALMONN 2 (Tang et al., 2 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model when video is given, and always directly generate the final answer. To re-obtain the reasoning ability during the SFT stage, we create set of more challenging questionanswering pairs based on the same training set videos using proprietary LLMs, and the pipeline is shown in Fig. 2. Figure 1. video-SALMONN-o1 model structure. The input video is processed by the visual and audio branches, generating encodings from the visual and audio frame sequences respectively. Two encoding streams are combined in an interleaved fashion to synchronize across time before sending to LLM. 2024a), the model is built based on pre-trained visual LLM by adding the audio encoder branch. The input video and audio streams are processed separately by the audio encoder and visual encoder and are then separately mapped to the dimension of the LLM input via individual modality aligners. To combine the audio and visual encodings, the interleaved synchronization module is employed as illustrated in Fig. 1. The groups of encodings per visual frame are equally spaced across time, and the audio encodings corresponding to the time between two visual frames t1 and t2 are inserted between the two groups of visual encodings. The process is summarized as in Eqn. (1): HAV = Concat(. . . , HV t1:t2, HV t2, . . . ) where HA represent groups of audio and visual encodings, and and are the number of encodings in each group. t1, HA Rn and HV Rm (1) multi-stage SFT pipeline with the cross-entropy loss on reference response is adopted to train video-SALMONN-o1 before optimizing the reasoning process with RL. Starting from the pre-trained visual model, the audio aligner is trained from scratch keeping other parts of the model frozen. Then, using paired audio-video data, the modality aligners and the low-rank adaptation (LoRA) module (Hu et al., 2022) are trained with other parts frozen. Figure 2. Acquisition pipeline of reasoning-intensive SFT data. The question, answer and reasoning paths are generated by Gemini1.5-pro taking the video with paired audio as inputs. GPT4o is employed for quality checks to ensure the QA-pair and the reasoning steps are valid and require logical thinking. For each video with paired audio, we use Gemini-1.5-pro to generate question-answer pair with the reasoning steps. Then, to avoid bias in Gemini models and ensure the quality of the questions and reasoning steps, quality check stage is employed using GPT-4o. Questions with poor quality will be discarded and new question-answer pair will be generated again. In addition to the newly created question, we augment the original training set by generating reasoning paths with Gemini-1.5-pro and checking by GPT-4o following the pipeline to avoid network learning two distinct mechanisms for reasoning and direct answer. This turned out to be important to yield competitive reasoning performance from SFT in our empirical study. 4. Training to Enhance Reasoning Abilities 4.1. Preliminary { s1, s2, ..., sK} The reasoning process refers to the LLM generating the sequence, where is the question, is the answer and sk are reasoning steps that logically connect the question to the final answer A. By treating this as Markov decision process (MDP) and the LLM as the policy model, PRM is to provide feedback for each step sk that guides the LLM in making accurate reasoning by optimising the policy to maximise the reward. 3.2. Reasoning-intensive SFT Data We empirically discovered that video understanding models (Cheng et al., 2024; Zhang et al., 2024d; Lin et al., 2024) generally lose the ability to perform step-by-step reasoning Following Wang et al. (2024b), the PRM is to estimate the expected answer correctness, psk , of prefix solution s1:k} . { The expected correctness score can be approximated with Monte Carlo sampling of multiple paths from the prefix 3 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Figure 3. Illustration of the contrastive step selection (top) and pairwise rollout (bottom) to construct per-step expected correctness score for pDPO. Contrastive step selection: Top 2 steps, s2 and s5 are selected in this example, and for s2, an alternative step, 2, is sampled to form the preference pair. Pairwise rollout: Three rollouts are shown for each step and s2 and 2 are step pairs with the same prefix solution. The answer correctness is checked using GPT-4o by comparing it against the reference answer. solution to an answer An as shown in Eqn (2). function for each step of interest can be written as psk 1 (cid:88)N n=1 1(An = Aref) (2) r(sk) = β log πθ(sks<k, HAV) πref(sks<k, HAV) + β log Z(s<k, HAV) (4) where Aref is the reference answer and An is one sampled sk+1,n, sk+2,n, . . . , sK(n),n} answer. The sampled path { that leads to An is referred to as rollout. The PRM training loss is then shown as LPRM = (cid:88)K k=1 psk log rsk + (1 psk ) log(1 rsk ) (3) [0, 1] is the PRM prediction which can be where rsk derived from the LLM output at the last token of each step ). with fully connected layer with sigmoid function σ( 4.2. Process DPO where πθ, πref, β and Z( ) are the LLM policy, reference policy, parameter controlling the deviation from πref, and the partition function as in Rafailov et al. (2024) respectively. Z(s<k, HAV) = (cid:80) β r(sk)). For each step, an alternative step sk is generated, and pairwise rollout is performed for both steps as shown in Fig. 3. The probability of sk being better than sk is then defined using the Bradley-Terry model as s<k, HAV) exp( 1 πref(sk sk p(sk sk) = σ(r(sk) r(sk)). Then, the pDPO loss can be written as: (cid:104) = αk log p(sk k) + (1 αk) log p(s sk) (5) (cid:105) (6) As pointed out by Zhang et al. (2024a), predicting an absolute score fails to exploit the instruction-following capabilities of LLMs as well as influenced by ambiguities in score standards. Both problems are more severe in audio-visual LLMs. Therefore, we propose pDPO for video-SALMONNo1, which is pairwise preference modelling approach by training the model to select the better reasoning path rather than giving absolute scores to the paths. Different from the pairwise preference reward model (PPRM) in (Zhang et al., 2024a) that leverages the partial ordering of entire reasoning paths, pDPO models the preference for specific reasoning step given the same prefix solution. Specifically, the reward where αk = 1(psk > psk ). Alternatively, αk = σ((psk psk )/µ) can be used as soft labels for DPO to accommodate the estimation noise introduced by the limited number of rollouts in psk , where µ is the calibration hyper-parameter determining how much we believe the process annotations. As result, pDPO retains the advantages of PPRM while offering finer modelling granularity at each step. In practice, pDPO is integrated with PPRM to construct complete reasoning paths, enhancing overall performance. While PPRM enables full-solution-level preference training, ensuring the generation of entire solutions, pDPO complements it by providing fine-grained, step-level preference guidance. 4 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Table 1. RivaBench basic statistics. The duration is given by mean standard deviation. The SynthDec split contains 100 synthetic videos and 100 real videos that human annotators search to have similar content as synthetic videos. MCQ stands for multiplechoice questions. Video sources are all from YouTube. Attribute Academic StandUp SynthDec Num. of QA Duration (s) Format 2,128 1,912 47.2 66.1 43.2 15.1 5-way MCQ 5-way MCQ 200 8.13.2 Yes/No 4.3. Contrastive Step Selection While rollouts allow automatic process annotation, the computational cost can be high when the numbers of rollouts and steps grow. However, in pDPO, certain steps are more error-prone and hence more valuable to be optimized than others. For general video understanding, by examining held-out validation set for reasoning paths with wrong answers, we found that over 70% of the reasoning errors occur at steps where the model misinterprets or hallucinates the video content. Therefore, we choose to particularly focus the pDPO on optimizing those steps. To locate those steps, we quantify the susceptibility of each reasoning step to the input video by applying tiny perturbation to the input video and measuring the length-normalized per-token KL divergence. Specifically, as shown in the top part of Fig. 3, for each step sk we compute the lengthnormalized KL-divergence by dsk = 1 sk (cid:88) (cid:16) DKL sk yi (yi y<i, HAV) (yi (cid:17) y<i, HAV) , ) computes the KL-divergence between the where DKL( output distributions with the original inputs HAV and perturbed inputs HAV. higher dsk indicates that the reasoning step sk is more susceptible to small input change, and this high susceptibility is likely to yield more diverged subsequent steps. We select the top steps with the highest dsk to perform pairwise rollout. While this selection biases pDPO training towards video-dependent errors, the other text-based logic errors can be accommodated by PPRM with entire reasoning paths. 5. Audio-visual Reasoning Benchmark The RivaBench is proposed to extend the scope of complex video understanding with three new reasoning-intensive application scenarios, including academic presentation (Academic), stand-up comedy (StandUp) and synthetic video detection (SynthDec). The statistics of videos for each scenario partition are shown in Table 1. The Academic partition is based on the M3AV (Chen et al., 5 2024) test set containing recordings of conference or lecture presentations spanning five different domains. Human experts with mathematical, engineering and medical backgrounds are recruited to provide questions, answers and detailed explanations based on the video clips. Example annotations are shown in Figs. 9 and 10 in Appendix C. While humour in videos has been explored from descriptive perspective (Hyun et al., 2024; Liu et al., 2024b; Xie et al., 2024), the StandUp partition of RivaBench explores from an audio-visual reasoning perspective. Specifically, instead of prompting the model to list all funny elements in the video, we particularly focus on understanding why certain punchline is interesting and task the human annotators to set questions that require reasoning about the comedians gestures, facial expression and speech content. Human annotators provide questions, answers and explanations (with automatically generated confusing choices), as shown in Figs. 7 and 8 in Appendix B. This paper proposes the SynthDec partition for synthetic video detection, which has great potential since video generation models are becoming increasingly powerful. This task requires LLM to classify whether given video clip is real or synthetic by finding clues in the video such as motions violating physics rules or objects being distorted. Videos are generated using the Hunyuan-large model (Sun et al., 2024c) (see examples in Figs. 11 and 12) This is challenging task that requires both logical reasoning and accurate perception of video content. The SynthDec partition can also serve as the performance indicator for reward models used to train video generators in the future. 6. Experimental Setup 6.1. Model and Training Specifications video-SALMONN-o1 is built based on the SigLIP (Zhai et al., 2023) visual encoder and Qwen 2 with 7B parameters backbone LLM. Two linear layers with GELU activation function are used (Hendrycks & Gimpel, 2016) as the visual aligner. The model processes videos at 2-frame-per-second rate with maximum of 60 frames. The Whisper-Large-v3 encoder (Radford et al., 2023) is used as the audio encoder, and the window-level Q-Former (Tang et al., 2024c) with window length of 0.2 seconds is used as the audio aligner, producing 150 audio tokens for every 30 seconds. We set LoRA hyper-parameters = 64 and α = 256 for the backbone LLM for both SFT and pDPO. During training, the visual encoder and aligner, audio encoder, and LLM remain frozen. SFT is performed on 16 A100 GPUs for 48 hours and pDPO is trained with A100 GPUs for 24 hours. Prompts used for reasoning 8 are shown in Appendix E. The code, SFT data, pDPO data and model checkpoints will be released. video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model 7. Results 7.1. Main Results The main results on VideoMME, NExT-QA and the RivaBench are shown in Table 2. No subtitles are given to any of the models under test for VideoMME. As performance references, we include GPT-4o (checkpoint at 2024-08-06) and Gemini-1.5-pro, with their results on VideoMME as reported in Fu et al. (2024). When testing GPT-4o with videos, each video is split into images at frame rate of 2 fps with maximum of 30 frames due to token limitation, and the sequence of images is sent as the input. For open-source models, we compare video-SALMONN-o1 to LLaVA-OneVision (Li et al., 2024a) (same visual encoder and LLM backbone), together with video-SALMONN (Sun et al., 2024b) and Video-LLaMA 2 (Cheng et al., 2024) as the two most recent audio-visual LLMs. Proprietary LLM performance on RivaBench: For the two proprietary LLMs, GPT-4o underperforms Gemini-1.5pro on StandUp and Academic test sets due to the lack of audio information. This indicates that RivaBench provides challenging questions that require more audio-visual joint understanding compared to VideoMME. On the SynthDec set, since only the visual part is synthesized, GPT-4o demonstrated stronger ability. Moreover, by performing reasoning with GPT-4o and Gemini-1.5-pro, larger improvements are found on StandUp and Academic test sets than VideoMME and NExT-QA, indicating the necessity of reasoning on RivaBench. Open-source LLM performance comparison: Audiovisual SFT on video-SALMONN-o1 already yields better performance than LLaVA-OneVision on VideoMME due to the ability to comprehend speech and audio information, whereas no obvious improvements are found on the other benchmarks. The main improvements on other benchmarks come from pDPO, which achieved 4.1%, 8.1% and 5.8% absolute accuracy improvements on NExT-QA, StandUp and Academic test sets respectively compared to the SFT model. Larger improvements are found on the RivaBench with 6-8% absolute accuracy improvements obtained compared to LLaVA-OneVision, and video-SALMONN-o1 even performs better on the StandUp test set than Gemini-1.5-pro without reasoning. Besides, compared to other audio-visual LLMs, video-SALMONN-o1 exhibits better interpretability of the model output, and the cause of mistakes can be located by analyzing the reasoning process. Zero-shot synthetic video detection: video-SALMONNo1 achieves zero-shot synthetic video detection ability while other open-source models output real all the time, which also benefit from better explanation with examples of anomalies in synthesized videos in the prompt. However, even for the videos where the motions obviously violate Figure 4. Distributions of the numbers of reasoning steps in SFT data. Left: Distribution of the entire SFT data. Right: Distribution on the reasoning-intensive subset of SFT data. Due to the difficulty of the reasoning-intensive subset, more reasoning steps are required in general for samples in this set. 6.2. Data Following Tang et al. (2024a), the audio modality alignment stage employs LibriSpeech-960h (Panayotov et al., 2015) ASR data and AudioCaps (Kim et al., 2019) audio caption data to train the audio aligner. During the audio-visual SFT stage, 13k videos with rich audio information are selected with high-quality audio-visual captions. Around 150k normal question-answer (QA) pairs are directly generated using GPT-4o by providing detailed audio-visual captions, and an additional subset of 30k reasoning-intensive SFT QA pairs are generated with the proposed data generation pipeline. Each QA, regardless of the difficulty, is associated with reasoning steps, and the distributions of the numbers of reasoning steps for the QA pairs used for SFT are shown in Fig. 4. Both captions and QA pairs are used for SFT. The reasoning-intensive subset is used to collect the data for pDPO training by sampling 10 paths for each QA. The QA pairs where the SFT model generates incorrect solutions are retained to perform rollouts and others that only contain correct solutions are discarded. For complete solutions, instead of directly comparing the paths (Zhang et al., 2024a), we compare each pair of solutions against the reference answer using GPT-4o and choose the one closer to the reference as the preferred solution. For intermediate steps, we choose the top 3 steps based on contrastive step selection, and 6 roll100k outs are performed for each chosen step. As result, pairs of complete solutions from 5k video clips are selected, and an extra 100k pairs of step-level partial solution pairs from these complete solutions are used for pDPO. Besides RivaBench, video-SALMONN-o1 is also evaluated on Video-MME (Fu et al., 2024) and NExT-QA (Xiao et al., 2021) benchmarks with challenging reasoning questions where the former is an audio-visual task and the latter focuses on visual information only. For consistency, paired audios are also provided for NExT-QA videos if they exist. Note that the synthetic video detection task is never seen in model training, and hence is zero-shot emergent ability. 6 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Table 2. Main results of video-SALMONN-o1 compared against other visual (V) and audio-visual (A+V) LLMs. SFT refers to the model after SFT with reasoning data and pDPO refers to the model obtained after training with pDPO based on the same SFT model. F1-score (Precision/Recall) is reported for SynthDec and accuracy is reported for others. Results with are directly taken from the corresponding papers. video-SALMONN-o1 performs reasoning during inference and other open-source models give answers directly."
        },
        {
            "title": "Model",
            "content": "Modality VideoMME NExT-QA"
        },
        {
            "title": "StandUp Academic",
            "content": "SynthDec (P/R) Proprietary models Gemini-1.5-pro (Team et al., 2024) Gemini-1.5-pro+reasoning GPT-4o (OpenAI Team, 2024) GPT-4o+reasoning Open-source baselines LLaVA-OneVision (Li et al., 2024a) video-SALMONN (Sun et al., 2024b) Video-LLaMA 2.1 (Cheng et al., 2024) video-SALMONN-o1 (ours, SFT) video-SALMONN-o1 (ours, pDPO) A+V A+V A+V A+V A+V A+V 75.0% 75.1% 71.9% 72.1% 58.2% 43.3% 54.9% 62.9% 65.6% 79.2% 79.5% 81.7% 81.9% 79.4% 49.2% 75.6% 78.2% 82.3% 75.8% 81.8% 63.3% 69.6% 67.2% 47.8% 53.7% 68.6% 76.7% 67.1% 23.6% (55%/15%) 69.5% 40.0% (49%/34%) 60.0% 34.1%(90%/21%) 61.0% 25.8%(53%/17%) 45.8% 33.6% 34.3% 0.0%(97%/0%) 0.0%(100%/0%) 0.0%(99%/0%) 5.8%(97%/5%) 42.5% 48.3% 17.8%(87%/13%) Table 3. Effect of different parts of the audio-visual SFT data on VideoMME, Academic and StandUp test sets. Underscore for second-best results. w/o reasoning-intensive part means removing the reasoning-intensive SFT data, and w/o any reasoning always directly outputting answers during SFT. Reasoning-intensive part only always performs reasoning for QA. Training Data Inference Reasoning VideoMME NExT-QA Academic StandUp Full SFT data Full SFT data w/o any reasoning w/o reasoning-intensive part w/o reasoning-intensive part Reasoning-intensive part only Full SFT data + pDPO 63.7% 62.9% 63.2% 62.7% 61.6% 58.8% 65.6% 80.7% 78.2% 81.0% 78.9% 76.6% 75.2% 82.3% 45.2% 42.5% 44.1% 44.7% 42.3% 40.1% 48.3% 72.3% 68.6% 71.1% 71.5% 67.5% 63.5% 76.7% physics rules, current state-of-the-art video LLMs still fail to detect most of the time. In addition, two qualitative examples are shown in Figs. 16 and 17 in Appendix G, where LlaVA-OneVison (and also other audio-visual models) are unable to provide the reasoning steps and the final answer is completely biased to Real. On the other hand, video-SALMONN-o1 can look for distortions in the video as part of its reasoning process, leading to the correct identification of synthesized videos. 7.2. Effect of SFT Data The audio-visual SFT data is crucial for video-SALMONNo1 to gain the initial audio-visual reasoning ability, and the effect of different data partitions is shown in Table 3. Direct answer outperforms reasoning after SFT: Directly outputting short answer or an option has been the dominating output mode for audio-visual LLMs on general video understanding, major difference to math questions. Comparing row 2 to row 1 in Table 3, when using all the SFT data including the reasoning-intensive part, the model after SFT is still better at directly generating the answer than performing reasoning. This is due to the exposure bias in teacher forcing which has much higher impact on the reasoning paths as they are much longer sequences. By learning on its own samples, pDPO mitigates this exposure bias and achieves consistently better performance than the SFT model. Next, comparing row 1 to row 3 and row 4 in Table 3, when directly outputting the answer during inference, incorporating reasoning steps in SFT does not always yield an improvement on videoMME and NExT-QA, despite being slightly helpful on RivaBench. The reasoning-intensive part is important: When excluding the reasoning-intensive part, there is clear degradation in model performance with reasoning during inference, showing the importance of this part of data to enable bet7 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Table 4. Effect of different reward modelling methods on VideoMME, NExT-QA, the StandUp and Academic split of RivaBench. Major@20 and RM@20 are evaluated following Zhang et al. (2024a), where Major@20 refers to the accuracy under majority voting with 20 sampled paths, and RM@20 is the best-of-n with 20 samples. Samples are all generated from the model after SFT. pDPO with full paths only uses preference pairs of complete reasoning paths."
        },
        {
            "title": "Inference",
            "content": "VideoMME NExT-QA StandUp Academic SFT SFT SFT + ORM SFT + PRM SFT + pDPO 1-best Major@20 RM@20 RM@20 1-best 62.9% 63.5% 62.7% 63.5% 65.6% 78.2% 81.5% 78.5% 79.3% 82.3% 68.6% 73.5% 69.0% 72.1% 76.7% 42.5% 45.3% 42.6% 43.9% 48.3% ter reasoning performance. However, when only using the reasoning-intensive part for SFT, the model struggles to acquire the fundamental audio-visual perception abilities, yielding sub-optimal performance. 7.3. Effect of pDPO Training We then analyse different reward modelling techniques for the model performance in Table 4. In addition to pairwise preference models, we include ORM and PRM as proposed in Lightman et al. (2023) as follows: ORM: projection layer is added to LLM output states and projects the last output state to scalar. which is then passed through sigmoid activation function to predict 1 if the final answer is correct, and 0 otherwise. PRM: projection layer is added to LLM output states and projects the state at the end of each step to scalar with sigmoid to predict 1(psk > 0). The score of each solution is the lowest score among all steps (Wang et al., 2024b). Both ORM and PRM are initialized with video-SALMONNo1 after the SFT stage. Best-of-n is used for ORM and PRM where 20 sampled solutions are generated from the SFT model and the top one with the highest score is selected. Moreover, majority voting among the 20 samples is used as baseline which is consistently marginally better than the 1-best solution across all test sets. While ORM showed mixed results compared to the 1-best solution from the SFT model, PRM showed consistent but marginal improvements and is on par with majority voting. The training loss of PRM and ORM only dropped about 5%, which reflects the difficulty of learning raw scores for general video understanding tasks. Last, comparing the models above against the pDPO model, the use of pairwise preference models is much more effective compared to predicting the raw score, showing the difficulty of direct raw score modelling in general video QAs. Qualitative examples comparing answers between SFT and pDPO are provided in Figs. 13 to 15 in Appendix F. Effect of Contrastive Step Selection. To analyze the effect Figure 5. Comparison between different top steps selected for pDPO. Pairs of full solution paths are always used in addition to pairs of intermediate steps. of the number of steps selected for pairwise training, we conducted experiments without intermediate pairs of steps and with all intermediate pairs, in addition to using the top three steps from the contrastive step selection. The comparisons are given in Fig. 5. Using intermediate steps in pDPO achieved further consistently improves model performance compared to only using the full solutions, especially on questions that require frequent reference to the video or audio information at intermediate reasoning steps. case study qualitatively showing the effect of contrastive step selection is included in Appendix H. 8. Conclusions We propose video-SALMONN-o1, the first open-source audio-visual LLM with enhanced reasoning abilities. videoSALMONN-o1 is the first to explore reasoning process optimization for general video understanding and proposes the pDPO method with an efficient contrastive step selection algorithm. To further evaluate the reasoning abilities of audio-visual LLMs, the RivaBench is introduced with innovative and challenging tasks and over 4000 high-quality human expert annotations. video-SALMONN-o1 consistently outperforms the strong LLaVA-OneVision baseline with 3-8% absolute accuracy improvements. pDPO training consistently outperformed the SFT model. Moreover, videoSALMONN-o1 showed zero-shot synthetic video detection abilities as result of the enhanced reasoning abilities. 8 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model"
        },
        {
            "title": "Impact Statement",
            "content": "By enhancing reasoning abilities in general video understanding, video-SALMONN-o1 provides more transparent and interpretable interface that is compatible with general videos to access and explain model responses and behaviours. This is indispensable to ensure the reliability of LLMs when applied to different video understanding scenarios and will be largely beneficial for pinpointing the specific causes or errors when the model generates dubious or toxic contents, thus enhancing AI safety. The approaches in this paper do not give rise to any additional potential biases beyond the ones directly inherited from the pre-trained model checkpoints used. The audio encoder and visual encoder might work worse for people from particular demographics. The framework also inherits biases from all the LLMs used in this paper. To mitigate potential biases, we clearly describe the nature of each dataset and provide clear and adequate references to all the resources we used for video-SALMONN-o1. The ability of video-SALMONN-o1 to understand speech in videos could lead to potential technology abuses like surveillance and eavesdropping. To counter this, weve consulted with legal experts to establish clear usage guidelines, reducing risks and addressing concerns, highlighting our dedication to responsible research sharing."
        },
        {
            "title": "References",
            "content": "Alamri, H., Cartillier, V., Das, A., Wang, J., Cherian, A., Essa, I., Batra, D., Marks, T. K., Hori, C., and Anderson, In Proc. CVPR, P. Audio visual scene-aware dialog. 2019. Chen, S., He, X., Guo, L., Zhu, X., Wang, W., Tang, J., and Liu, J. VALOR: Vision-audio-language omniperception pretraining model and dataset. arXiv preprint arXiv:2304.08345, 2023a. Chen, S., Li, H., Wang, Q., Zhao, Z., Sun, M., Zhu, X., and Liu, J. Vast: vision-audio-subtitle-text omni-modality foundation model and dataset. 2023b. Chen, Z., Liu, H., Yu, W., Sun, G., Liu, H., Wu, J., Zhang, C., Wang, Y., and Wang, Y. M3AV: multimodal, multigenre, and multipurpose audio-visual academic lecture dataset. In Proc. ACL, 2024. Cheng, Z., Leng, S., Zhang, H., Xin, Y., Li, X., Chen, G., Zhu, Y., Zhang, W., Luo, Z., Zhao, D., and Bing, L. VideoLLaMA 2: Advancing spatial-temporal modeling and audio understanding in Video-LLMs. arXiv preprint arXiv:2406.07476, 2024. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv:2110.14168, 2021. DeepSeek Team. Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power, 2024. Du, Y., Ma, Z., Yang, Y., Deng, K., Chen, X., Yang, B., Xiang, Y., Liu, M., and Qin, B. CoT-ST: Enhancing LLM-based speech translation with multimodal chain-ofthought. arXiv:2409.19510, 2024. Du, Y., Liu, Z., Li, Y., Zhao, W. X., Huo, Y., Wang, B., Chen, W., Liu, Z., Wang, Z., and Wen, J.-R. Virgo: preliminary exploration on reproducing o1-like mllm. arXiv:2501.01904, 2025. Fang, X., Mao, K., Duan, H., Zhao, X., Li, Y., Lin, D., and Chen, K. MMBench-Video: long-form multishot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. Feng, X., Wan, Z., Wen, M., Wen, Y., Zhang, W., and Wang, J. Alphazero-like tree-search can guide large language model decoding and training. In Proc. ICML, 2024. Fu, C., Dai, Y., Luo, Y., Li, L., Ren, S., Zhang, R., Wang, Z., Zhou, C., Shen, Y., Zhang, M., et al. Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal LLMs in video analysis. arXiv preprint arXiv:2405.21075, 2024. Goyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar, S., and Nagarajan, V. Think before you speak: Training language models with pause tokens. In Proc. ICLR, 2024. Guo, J., Zheng, T., Bai, Y., Li, B., Wang, Y., Zhu, K., Li, Y., Neubig, G., Chen, W., and Yue, X. MAmmoTH-VL: Eliciting multimodal reasoning with instruction tuning at scale. arXiv:2412.05237, 2024. Hao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z., and Hu, Z. Reasoning with language model is planning with world model. In Proc. EMNLP, 2023. Hendrycks, D. and Gimpel, K. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In Proc. ICLR, 2022. Hyun, L., Sung-Bin, K., Han, S., Yu, Y., and Oh, T.-H. SMILE: Multimodal dataset for understanding laughter in video with language models. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proc. NAACL Findings, 2024. Kim, C. D., Kim, B., Lee, H., and Kim, G. AudioCaps: Generating captions for audios in the wild. In Proc. NAACLHLT, 2019. 9 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., and Li, C. LLaVAOneVision: Easy visual task transfer. arXiv:2408.03326, 2024a. Ning, M., Zhu, B., Xie, Y., Lin, B., Cui, J., Yuan, L., Chen, D., and Yuan, L. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. Li, G., Wei, Y., Tian, Y., Xu, C., Wen, J.-R., and Hu, D. Learning to answer questions in dynamic audio-visual scenarios. In Proc. CVPR, 2022. Li, K., Wang, Y., He, Y., Li, Y., Wang, Y., Liu, Y., Wang, Z., Xu, J., Chen, G., Luo, P., et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proc. CVPR, 2024b. Li, L., Lei, J., Gan, Z., Yu, L., Chen, Y.-C., Pillai, R., Cheng, Y., Zhou, L., Wang, X. E., Wang, W. Y., et al. Value: multi-task benchmark for video-and-language understanding evaluation. arXiv preprint arXiv:2106.04632, 2021. Li, S., Li, L., Ren, S., Liu, Y., Liu, Y., Gao, R., Sun, X., and Hou, L. Vitatecs: diagnostic dataset for temporal concept understanding of video-language models. arXiv preprint arXiv:2311.17404, 2023a. Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., and Chen, W. Making language models better reasoners with step-aware verifier. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proc. ACL, 2023b. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv:2305.20050, 2023. Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., and Yuan, L. Video-LLaVA: Learning united visual representation by alignment before projection. In Proc. CVPR, 2024. Liu, Y., Li, S., Liu, Y., Wang, Y., Ren, S., Li, L., Chen, S., Sun, X., and Hou, L. Tempcompass: Do video LLMs really understand videos? arXiv preprint arXiv:2403.00476, 2024a. Liu, Z.-S., Courant, R., and Kalogeiton, V. FunnyNet-W: Multimodal learning of funny moments in videos in the wild. arXiv:2401.04210, 2024b. Luo, L., Liu, Y., Liu, R., Phatale, S., Guo, M., Lara, H., Li, Y., Shu, L., Zhu, Y., Meng, L., Sun, J., and Rastogi, A. Improve mathematical reasoning in language models by automated process supervision. arXiv:2406.06592, 2024. Mangalam, K., Akshulakov, R., and Malik, J. Egoschema: diagnostic benchmark for very long-form video language understanding. In Proc. NeurIPS, 2023. OpenAI. Learning to reason with large language models, 2024. OpenAI Team. Gpt-4o system card. arXiv:2410.21276, 2024. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: An ASR corpus based on public domain audio books. In Proc. ICASSP, 2015. Qwen Team. To see the world with wisdom, 2024a. Qwen Team. QwQ: Reflect deeply on the boundaries of the unknown, 2024b. Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust Speech Recognition via Large-scale Weak Supervision. In Proc. ICML, 2023. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct Preference Optimization: Your language model is secretly reward model. In Proc. NeurIPS, 2024. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. arXiv:2408.03314, 2024. Sun, G., Manakul, P., Liusie, A., Pipatanakul, K., Zhang, C., Woodland, P., and Gales, M. CrossCheckGPT: Universal hallucination ranking for multimodal foundation models. arXiv preprint arXiv:2405.13684, 2024a. Sun, G., Yu, W., Tang, C., Chen, X., Tan, T., Li, W., Lu, L., MA, Z., Wang, Y., and Zhang, C. video-SALMONN: Speech-enhanced audio-visual large language models. In Proc. ICML, 2024b. Sun, X., Chen, Y., Huang, Y., et al. Hunyuan-Large: An open-source moe model with 52 billion activated parameters by tencent. arXiv preprint arXiv:2411.02265, 2024c. Sun, Z., Yu, L., Shen, Y., Liu, W., Yang, Y., Welleck, S., and Gan, C. Easy-to-hard generalization: Scalable alignment beyond human supervision. In Proc. NeurIPS, 2024d. Tang, C., Li, Y., Yang, Y., Zhuang, J., Sun, G., Li, W., Ma, Z., and Zhang, C. Enhancing multimodal LLM for detailed and accurate video captioning using multi-round preference optimization. arXiv:2410.06682, 2024a. Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., MA, Z., and Zhang, C. SALMONN: Towards generic hearing abilities for large language models. In Proc. ICLR, 2024b. 10 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Yu, F., Gao, A., and Wang, B. OVM, outcome-supervised value models for planning in mathematical reasoning. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proc. NAACL Findings, 2024. Yuan, L., Cui, G., Wang, H., Ding, N., Wang, X., Deng, J., Shan, B., Chen, H., Xie, R., Lin, Y., Liu, Z., Zhou, B., Peng, H., Liu, Z., and Sun, M. Advancing LLM reasoning generalists with preference trees. arXiv:2404.02078, 2024. Yun, H., Yu, Y., Yang, W., Lee, K., and Kim, G. PanoAVQA: Grounded audio-visual question answering on 360deg videos. In Proc. ICCV, 2021. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. arXiv preprint arXiv:2303.15343, 2023. Zhang, D., Wu, J., Lei, J., Che, T., Li, J., Xie, T., Huang, X., Zhang, S., Pavone, M., Li, Y., Ouyang, W., and Zhou, D. LLaMA-Berry: Pairwise optimization for o1-like Olympiad-level mathematical reasoning. arXiv:2410.02884, 2024a. Zhang, L., Hosseini, A., Bansal, H., Kazemi, M., Kumar, A., and Agarwal, R. Generative verifiers: Reward modeling as next-token prediction. arXiv:2408.15240, 2024b. Zhang, R., Gui, L., Sun, Z., Feng, Y., Xu, K., Zhang, Y., Fu, D., Li, C., Hauptmann, A., Bisk, Y., and Yang, Y. Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward. arXiv preprint arXiv:2404.01258, 2024c. Zhang, Y., Wu, J., Li, W., Li, B., Ma, Z., Liu, Z., and Li, C. Video instruction tuning with synthetic data. arXiv:2410.02713, 2024d. Zhang, Y., Wu, S., Yang, Y., Shu, J., Xiao, J., Kong, C., and Sang, J. o1-Coder: An o1 replication for coding. arXiv:2412.00154, 2024e. Zhao, Y., Yin, H., Zeng, B., Wang, H., Shi, T., Lyu, C., Wang, L., Luo, W., and Zhang, K. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv:2411.14405, 2024. Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C. Extending large language models for speech and audio captioning. In Proc. ICASSP, 2024c. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2024. Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with processand outcomebased feedback. arXiv:2211.14275, 2022. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and Lin, J. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv:2409.12191, 2024a. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proc. ACL, 2024b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. In Proc. NeurIPS, 2022. Xiao, J., Shang, X., Yao, A., and Chua, T.-S. Next-QA: Next phase of question-answering to explaining temporal actions. In Proc. CVPR, 2021. Xie, B., Zhang, S., Zhou, Z., Li, B., Zhang, Y., Hessel, J., Yang, J., and Liu, Z. FunQA: Towards surprising video comprehension. arXiv:2306.14899, 2024. Xu, G., Jin, P., Li, H., Song, Y., Sun, L., and Yuan, L. LLaVA-CoT: Let vision language models reason step-bystep. arXiv:2411.10440, 2024. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. arXiv:2409.12122, 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of Thoughts: Deliberate problem solving with large language models. In Proc. NeurIPS, 2023. Ying, H., Zhang, S., Li, L., Zhou, Z., Shao, Y., Fei, Z., Ma, Y., Hong, J., Liu, K., Wang, Z., Wang, Y., Wu, Z., Li, S., Zhou, F., Liu, H., Zhang, S., Zhang, W., Yan, H., Qiu, X., Wang, J., Chen, K., and Lin, D. InternLM-Math: Open math large language models toward verifiable reasoning. arXiv:2402.06332, 2024. 11 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model A. Reasoning SFT Data Example Figure 6. Example of reasoning SFT data B. StandUp Data Examples Two examples of the StandUp part of RivaBench are shown in Fig. 7 and 8 respectively. Figure 7. Example of StandUp part of the RivaBench. 12 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Figure 8. Example of StandUp part of the RivaBench. C. Academic Data Examples Two examples of the Academic part of RivaBench are shown in Fig. 9 and 10 respectively. Figure 9. Example of Academic part of the RivaBench. 13 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Figure 10. Example of Academic part of the RivaBench. D. Synthetic Video Detection Data Examples Two synthetic video examples in the SynthDec partition of RivaBench are shown in Fig. 11 and 12 respectively. Figure 11. Example video clip of the SynthDec part of RivaBench. Figure 12. Example video clip of the SynthDec part of RivaBench. E. Prompt Templates Prompt templates for video-SALMONN-o1 are shown in Table 5. When trained with the reasoning prompt template, the same template is used during inference. Changing templates may cause small perturbations in performance. 14 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Table 5. Prompt used for different types of tasks."
        },
        {
            "title": "Prompt content",
            "content": "Direct answer <VIDEO>Select the best answer to the following question based on the video. Respond with only the letter of the correct option."
        },
        {
            "title": "SynthDec",
            "content": "Question } { Choose from: A. <VIDEO> Question: Question } Option { , B, } Option { ... } , B, } Option { Option { { Choose from: A. Answer the question step by step. Output each thinking step. Mark the end of each step with <end of step> token. An AI-generated video contains unnatural distorted things, such as distorted hands or faces. Is the given video AI generated? Answer YES or NO. Answer step by step and output each step clearly. ... } F. Case Studies: Solution with Reasoning Examples Figure 13. Example video and solutions from the StandUp test set. 15 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Figure 14. Example video and solutions from videoMME test set. Figure 15. Example video and solutions from videoMME test set. video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model G. Case Studies: Zero-shot Synthetic Video Detection Figure 16. Example output from video-SALMONN-o1, GPT-4o and Gemini-1.5-pro for synthetic video detection. 17 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model Figure 17. Example output from video-SALMONN-o1, GPT-4o and Gemini-1.5-pro for synthetic video detection. 18 video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model H. Examples of Contrastive Step Selection Process Figure 18. Example of the contrastive step selection process where two sampled paths are shown and the scores dsk are given for each reasoning steps. The 3rd step in the first solution is wrong due to visual hallucination, and as result, very high score is assigned to that step and that step will be used to perform rollout."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Tsinghua university",
        "Univeristy of Cambridge"
    ]
}