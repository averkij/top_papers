{
    "paper_title": "Benchmarking LLMs for Political Science: A United Nations Perspective",
    "authors": [
        "Yueqing Liang",
        "Liangwei Yang",
        "Chen Wang",
        "Congying Xia",
        "Rui Meng",
        "Xiongxiao Xu",
        "Haoran Wang",
        "Ali Payani",
        "Kai Shu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved significant advances in natural language processing, yet their potential for high-stake political decision-making remains largely unexplored. This paper addresses the gap by focusing on the application of LLMs to the United Nations (UN) decision-making process, where the stakes are particularly high and political decisions can have far-reaching consequences. We introduce a novel dataset comprising publicly available UN Security Council (UNSC) records from 1994 to 2024, including draft resolutions, voting records, and diplomatic speeches. Using this dataset, we propose the United Nations Benchmark (UNBench), the first comprehensive benchmark designed to evaluate LLMs across four interconnected political science tasks: co-penholder judgment, representative voting simulation, draft adoption prediction, and representative statement generation. These tasks span the three stages of the UN decision-making process--drafting, voting, and discussing--and aim to assess LLMs' ability to understand and simulate political dynamics. Our experimental analysis demonstrates the potential and challenges of applying LLMs in this domain, providing insights into their strengths and limitations in political science. This work contributes to the growing intersection of AI and political science, opening new avenues for research and practical applications in global governance. The UNBench Repository can be accessed at: https://github.com/yueqingliang1/UNBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 2 2 1 4 1 . 2 0 5 2 : r Benchmarking LLMs for Political Science: United Nations Perspective Yueqing Liang1, Liangwei Yang2, Chen Wang3, Congying Xia4, Rui Meng2, Xiongxiao Xu1, Haoran Wang6, Ali Payani5, Kai Shu6 1Illinois Institute of Technology 2Salesforce 3University of Illinois at Chicago 4Meta 5Cisco 6Emory University {yliang40, xxu85}@hawk.iit.edu {liangwei.yang, ruimeng}@salesforce.com cwang266@uic.edu, congyingxia@meta.com, apayani@cisco.com {haorang.wang, kai.shu}@emory.edu"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved significant advances in natural language processing, yet their potential for high-stake political decision-making remains largely unexplored. This paper addresses the gap by focusing on the application of LLMs to the United Nations (UN) decision-making process, where the stakes are particularly high and political decisions can have far-reaching consequences. We introduce novel dataset comprising publicly available UN Security Council (UNSC) records from 1994 to 2024, including draft resolutions, voting records, and diplomatic speeches. Using this dataset, we propose the United Nations Benchmark (UNBench), the first comprehensive benchmark designed to evaluate LLMs across four interconnected political science tasks: co-penholder judgment, representative voting simulation, draft adoption prediction, and representative statement generation. These tasks span the three stages of the UN decision-making processdrafting, voting, and discussingand aim to assess LLMs ability to understand and simulate political dynamics. Our experimental analysis demonstrates the potential and challenges of applying LLMs in this domain, providing insights into their strengths and limitations in political science. This work contributes to the growing intersection of AI and political science, opening new avenues for research and practical applications in global governance. The UNBench can be accessed at: https://github. com/yueqingliang1/UNBench."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) such as GPT4 (OpenAI, 2023), Llama (Dubey et al., 2024), and DeepSeek (Liu et al., 2024) have achieved unprecedented proficiency in language tasks and are increasingly under development tailored for different domains (Cheng et al., 2023). Yet, their adaptation to high-stakes political decision-making 1 remains underexploredparticularly in scenarios where model outputs could influence real-world governance. Political science demands capabilities beyond semantic understanding: predicting coalition dynamics, interpreting ambiguous diplomatic language, and navigating the tension between national interests and global norms. These challenges unfold within the United Nations (UN), where single draft resolution, once adopted, becomes binding international law under Chapter VII of the UN Charter, with cascading impacts on global security, trade, and human rights (e.g., Resolution 1973s no-fly zone over Libya in 2011). Studying the application of LLMs in political science represents both technical frontier and critical societal challenge. The scope of UN resolutions is extensive, extending far beyond political statements. Adopted resolutions can authorize military interventions (such as Resolution 678 in 1990), impose sanctions that cripple national economies (e.g., Resolution 1718 on North Korea in 2006), or redefine global priorities (e.g., Resolution 2341 on critical infrastructure protection in 2017). By analyzing the open-access data from draft resolutions and meeting records, we can explore how current LLMs understand the critical issues facing the international community and assess their ability to interpret bilateral and multilateral relations. This extends LLMs application toward political science, enhancing the analysis of international policies and diplomacy. Currently, there is no dedicated benchmark designed specifically for LLM applications in political science. Existing benchmarks (e.g., MMLU (Hendrycks et al., 2021b,a), BIGBench (Suzgun et al., 2022)) with related tasks remain fragmented and their designs may not adequately reflect LLMs understanding of political science. Such fragmented evaluation overlooks the interconnected nature of real-world political decision-making, particularly in high-stake multilateral settings like the UN. Political science enmary, our work makes the following contributions: Systematically curated and processed United Nations data from 1994 to 2024, including draft resolutions, voting records, and diplomatic speeches, to provide new and comprehensive dataset that facilitates LLM applications in political science. Introduce the first comprehensive benchmark in political science, designed to assess LLM performance across various real-world tasks in different stages of the UN process. Conduct extensive experimental analysis on the dataset and benchmark, demonstrating the effectiveness and limitations of current LLMs in handling complex political tasks."
        },
        {
            "title": "2.1 LLMs in Political Science",
            "content": "Existing benchmark datasets have played critical role in advancing Large Language Models (LLMs) in political science tasks. Datasets such as OpinionQA (Santurkar et al., 2023), PerSenT (Bastan et al., 2020), and GermEval-2017 (Wojatzki et al., 2017) evaluate LLMs on classifying sentiments or identifying topics within political texts. These benchmarks primarily emphasize static text understanding. BillSum (Kornilova and Eidelman, 2019) and CaseLaw (Shu et al., 2024) specialize in summarization or analysis of legislative documents. Datasets like PolitiFact (Shu et al., 2020), GossipCop (Grover et al., 2022), and Weibo (Jin et al., 2017) focus on detecting misinformation. Election prediction and voting behavior datasets, such as U.S. Senate Returns 2020 (Data and Lab, 2022b) and State Precinct-Level Returns 2018 (Data and Lab, 2022a) evaluate models capabilities in statistical pattern recognition and forecasting. These tasks often rely on structured data and focus on predictive performance related to electoral outcomes. While these benchmarks have advanced singletask evaluation, they isolate tasks and fail to capture the interconnectedness of real-world political scenarios. They also often focus on static or structured data, neglecting the dynamic nature of high-stakes political decision-making."
        },
        {
            "title": "2.2 United Nations-Related Datasets",
            "content": "The United Nations has long been focal institution for global politics, attracting extensive study in Figure 1: Three Key Stages of the United Nations Decision-Making Process compasses complex, inter-connected tasks (Li et al., 2024). Figure 1 shows the stages of each UN draft resolution. It consists of three stages. (1) Drafting: The creation of the resolutions text, involving the collaboration among member states. (2) Voting: The process in which the resolution is formally approved or rejected by voting from the UN members. (3) Discussing: Each member states the rationality of the voting. Different tasks occur in different stages and also inter-connected with each other. To fill the benchmark gap in political science, we introduce the United Nations Benchmark (UNBench), the first comprehensive benchmark to evaluate LLMs ability across four distinct yet interconnected political science tasks of different stages: (a) Co-Penholder Judgement: Given anonymized draft content, identify optimal co-author nations, simulating coalition-building strategies in multilateral diplomacy. (b) Representatives Voting Simulation: Instruct an LLM to act as national agent (e.g., \"As the U.S. representative\") and output voting decisions ([In favour, Against, etc.]), testing contextual understanding of national interests. (c) Draft Adoption Prediction: Input draft resolution to predict its passage probability, requiring analysis of historical voting patterns and geopolitical alignments. (d) Representative Statement Generation: Generate country-specific speeches justifying voting positions, evaluating persuasive language generation under political constraints. The tasks are designed from different UN stages, varying across predictive and generative tasks. Our benchmark is built from publicly available UNSC official records (1994-2024), comprising draft resolutions, voting records, and diplomatic speeches which are extracted from meeting records. In sum2 political science (Bailey et al., 2017; Voeten, 2013). While various publicly available datasets shed partial light on UN processes, they typically focus on limited aspects of the organization. Harvard Dataverse UN Voting Dataset (Voeten et al., 2009) compiles pairwise country voting statistics, offering quantitative insights but lacking textual data such as draft resolutions or debate transcripts. UNSCR.com (i Redondo and Llovera, 2025) collects mainly adopted Security Council resolutions, providing topic labels and related resolutions but minimal coverage of draft content. UNSCdeb8 (Kohlenberg et al., 2019) includes verbatim debate transcripts from 2010 to 2017, capturing real-time deliberation but omitting links to draft texts or voting outcomes. Lastly, the UN Parallel Corpus (United Nations, 2025) supplies multilingual final resolutions and meeting records (19942014), yet lacks draft-stage materials and detailed metadata to trace evolving negotiations. Despite aiding research on voting patterns or resolution content, these datasets do not capture the entire draft-to-adoption pipeline, missing out on the high-stakes negotiations and coalition-building that characterize UN decision-making. Moreover, none have been transformed into benchmark specifically for LLM evaluation, limiting their utility for advanced NLP tasks. Our dataset addresses these shortcomings by compiling draft resolutions, verbatim debates, and voting records from 19942024, thereby offering the first UN LLM benchmark to encompass the full trajectory from initial drafting to final resolution adoption."
        },
        {
            "title": "3 The United Nations Benchmark",
            "content": "We build UNBench from United Nations Security Council resolutions, which undergoes three distinct stages (Drafting, Voting, Discussing) before being either adopted or rejected. From this process, we extract four tasks designed to evaluate different capabilities of LLMs in political science. In this section, we systematically introduce each stage and its corresponding tasks, followed by detailed explanation of our data collection and dataset construction process."
        },
        {
            "title": "3.1 Benchmark Notation Definition",
            "content": "To formalize our benchmark, we introduce the following notations: = {r1, r2, . . . , rN }: The set of all draft resolutions. Each resolution ri contains proposed actions, mandates, and contextual details (e.g., sponsoring countries). = {c1, c2, . . . , cM }: The set of all UN members, encompassing both permanent and non-permanent members. For each resolution ri R, Ccandidate(ri) denotes the candidate co-penholder countriesthose likely to sponsor or support ri during the drafting process. V(ri) = {vi,1, vi,2, . . . , vi,15}: The votes of the 15 Security Council members on ri. is one of {[In Favour], [Against], [Abstention]}. Each vote vi,j Result(ri) {ADOPTED, NOTADOPTED}: The decision on ri. It is NOTADOPTED if it fails to secure the necessary majority or is vetoed by permanent member. S(ri, cj): The official statement (speech) of country cj regarding ri, delivered during the discussion stage. This typically includes the countrys rationale, policy concerns, and diplomatic stance."
        },
        {
            "title": "3.2 Stage 1: Drafting",
            "content": "Drafting is the initial phase in the lifecycle of resolution. Typically, one or more countriesoften referred to as penholderstake the lead in preparing draft text, outlining the resolutions objectives, scope, and operative clauses. The draft is then refined through closed-door consultations, circulated informally among Council members. unique feature of drafting is the practice of copenholdership, wherein multiple countries jointly sponsor or own the resolution from its inception. We design task focusing on identifying the most suitable co-penholder. Task 1: Co-Penholder Judgement. Formally, let ri be draft resolution authored by country ca. We sample Ccandidate(ri) as set of copenholders candidates, each representing different country. The LLM is prompted to assume the role of the author country ca, given the text of ri, and asked to choose exactly one co-penholder from the set Ccandidate(ri). In practice, we vary the number of candidates from 2 to 5, making this task multi-choice setup with single correct answer. 3 Figure 2: The proposed UNBench. It consists of 4 tasks extracted from different stages of UN draft. Co-penholdership reflects shared strategic interests, diplomatic partnerships, or specialized expertise on the issue at hand. Identifying suitable co-penholder requires the model to: Comprehend Contextual Information: Understand the resolutions key themes (e.g., conflict prevention, sanctions regime, peacekeeping mandates), and recognize which policy domains (e.g., human rights, climate, nuclear disarmament) are relevant. This tests the models ability to integrate textual comprehension of policy content with broader geopolitical and diplomatic reasoning. Infer Diplomatic Alignments: Analyze historical or implied alignments and identify country pairings likely to co-sponsor resolution. This evaluates whether the model can correlate textual cues with knowledge of past collaborations or alliances, and navigate multi-choice questions where the differences between candidate countries may be subtle or context-dependent. Reason About Multilateral Cooperation: Weigh factors such as candidate countrys veto power (if permanent), geopolitical priorities, and regional interests to recommend co-penholder that maximizes the resolutions likelihood of success. Hence, Task 1 offers focused measure of the models capacity to perform political and textual reasoning in controlled, multi-choice format, laying the groundwork for subsequent stages involving voting and post-vote deliberation."
        },
        {
            "title": "3.3 Stage 2: Voting",
            "content": "In the second stage of the resolution lifecycle, each of the 15 Council members casts vote to determine whether draft resolution is adopted or rejected. Permanent members wield veto power, meaning single Against vote from any of the five permanent countries can block the resolution, regardless of overall support. Non-permanent members, on the other hand, primarily influence the outcome through collective consensus and persuasive negotiation. Based on this voting mechanism, we define two tasks that capture different facets of decision-making at this stage. Task 2: Representatives Voting Simulation. Formally, for draft resolution ri, let V(ri) = {vi,1, . . . , vi,15} denote the votes cast by each of the 15 Council members. In this task, the LLM is given the content of ri and prompted to assume the role of specific representative cj (where cj C) to determine how that country would vote on ri. Each vote vi,j must be one of {[In Favour], [Against], [Abstention]}. 4 Objective and Challenges. Effective voting simulation requires the model to: Comprehend the Resolution: Interpret the text of ri in light of its subject matter (e.g., conflict prevention, sanctions). Incorporate National Interests: Weigh representative countrys known priorities and geopolitical alignments (e.g., historical alliances, regional blocs). Account for Veto Power: Recognize whether cj is permanent member with veto ability. These dimensions test not only the models reading comprehension but also its aptitude for political and strategic reasoning, reflecting real-world complexities in UNSC negotiations. Task 3: Draft Adoption Prediction. Once the votes V(ri) are cast, the resolution is adopted if it secures the necessary majority (i.e., at least nine [In Favour] votes) and no permanent member exercises veto. In this task, the LLM receives the text of ri and predict the final outcome, denoted Result(ri) {ADOPTED, NOTADOPTED}. Unlike Task 2, which focuses on individual country votes, this task tests whether the model can account for the collective dynamics of all 15 Council members. Key factors include but are not limited to overall council sentiment, potential veto threats, historical precedents, etc. Accurate adoption prediction thus demands higher-level inference about the distribution of possible votes, the interplay of veto power, and the delicate balancing of geopolitical interests. Together, Tasks 2 and 3 provide complementary perspectives on an LLMs capacity to model real-world decision-making under complex international relations."
        },
        {
            "title": "3.4 Stage 3: Discussing",
            "content": "Once the voting concludes, each member typically delivers statement clarifying the vote and articulating national positions or broader policy perspectives. These statements reveal the rationale behind each countrys stancewhether In Favour, Against, or Abstention. Since these statements are given in an open discussion format, countries may engage in debates, directly addressing or countering the arguments made by other members. This final discussion phase can shape diplomatic narratives surrounding the resolutions implications and signal future policy directions. We design task that evaluates an LLMs ability to generate representative statements aligned with national interests, voting behavior, and diplomatic discourse norms. Task 4: Representative Statement Generation Formally, for draft resolution ri, let S(ri, cj) denote the official statement made by country cj. In this task, the LLM receives the text of ri alongside contextual details, including the outcome of the vote, each countrys voting decision, and any prior statements made in the discussion (if available, in the order they were delivered). The model is then asked to generate the statement that cj would deliver. This statement should reflect: National Interests and Policies: How does cjs geopolitical position shape its response to the resolution (e.g., security concerns, regional dynamics)? Vote Justification: If cj voted [In Favour], [Against], or [Abstention], the statement should provide coherent rationale for the decision. Diplomatic Tone and Style: UNSC discourse follows formal, measured tone. The model should generate text that aligns with the conventions of diplomatic statements. By prompting the model to produce countryspecific statements, Task 4 evaluates higher-level language generation skills in multi-faceted political context. The ability to incorporate historical alliances, policy priorities, and rhetorical conventions into coherent and persuasive statements indicates an advanced understanding of both textual composition and global political dynamics."
        },
        {
            "title": "4 Potential Applications",
            "content": "The UNBench offers significant value to both LLM researchers and stakeholders in international governance, enabling practical applications and advancing research in geopolitical AI. Below, we outline potential use cases for each group. For LLM Researchers: The UNBench provides rich testbed for advancing research in LLMs, particularly in the context of geopolitical reasoning and time-series analysis: (1) Geopolitical Reasoning: The tasks in the benchmark span wide range of capabilities, from alliance identification (Task 1) 5 to issue-specific voting prediction (Task 2), offering researchers comprehensive framework for evaluating and improving LLMs understanding of international relations. (2) Temporal Analysis: With data spanning 30 years, the benchmark enables time-series tasks such as predicting trends in diplomatic behavior, forecasting shifts in international alliances, or analyzing the impact of historical events (e.g., the end of the Cold War) on UNSC dynamics. For instance, researchers could use the dataset to predict how emerging global issues (e.g., climate change) will influence future (3) Fine-Grained Prediction: The resolutions. benchmarks focus on multi-choice and generative tasks challenges researchers to develop models that balance precision and creativity. For example, improving ROUGE scores in Task 4 could lead to breakthroughs in generating protocol-compliant diplomatic text. (4) Bias and Fairness Analysis: The dataset provides an opportunity to study and mitigate biases in LLMs geopolitical reasoning, ensuring that models do not perpetuate stereotypes or oversimplify complex international dynamics. By addressing these research challenges, the UNBench can drive advancements in LLMs ability, contributing to more reliable and ethical AI systems for international governance. For UN Stakeholders: The ability to predict and analyze UNSC decision-making using LLMs also has implications for diplomats, policymakers, and international organizations: (1) Draft Adoption Forecasting: By predicting whether draft resolution will be adopted (Task 3), stakeholders can proactively adjust negotiation strategies, allocate resources more effectively, and build coalitions to maximize the likelihood of success. (2) Voting Behavior Simulation: Simulating country-specific voting behavior (Task 2) allows stakeholders to anticipate the positions of key nations, identify potential allies or opponents, and tailor diplomatic outreach accordingly. This could be particularly useful for smaller nations or NGOs seeking to navigate complex geopolitical landscapes. These applications demonstrate how the UNBench can serve as decision-support tool, enabling stakeholders to navigate the complexities of international governance with greater foresight."
        },
        {
            "title": "5 Experiments",
            "content": "Having introduced the design of UNBench, we now turn to an empirical evaluation of various models Task Statistic Task 1 Task 2 Task Task 4 # Drafts # Unique Draft Authors Avg. # Authors per Draft # Total Instances # Drafts # Total Instances # [In Favour] # [Against] # [Abstention] # Drafts # [Adopted] # [NotAdopted] # Meetings (Drafts) # Statements # Countries Avg. # Tokens per Statement Table 1: Statistics for our UNBench. Value 1,300 209 7 355,126 1,162 17,430 17,020 16 391 1,978 1,880 98 1,752 7,394 204 450 In this section, we first present on our dataset. summary statistics of UNBench, then detail the experimental setups and metrics used to assess model performance across Tasks 14."
        },
        {
            "title": "5.1 Dataset Statistics",
            "content": "Our UNBench covers broad range of draft resolutions, voting records, and meeting transcripts, providing diverse scenarios for evaluating multiple LLM capabilities. As shown in Table 1, Task 1 features 1,300 draft resolutions with total of 355,126 instancesreflecting multi-choice setup where each instance corresponds to an author country selecting co-penholder. Task 2 contains 17,430 instances of individual votes for each country that participating in the voting, while Task 3 comprises 1,978 draft resolutions with both [Adopted] and [NotAdopted] labels. Finally, Task 4 includes 7,394 statements from 1,752 UNSC meetings, testing the ability to generate coherent speeches that align with national stances. Due to space constraints, we provide the detailed construction process of UNBench in Appendix A.3."
        },
        {
            "title": "5.2 Experimental Setup",
            "content": "Models. Tasks 1, 2, and 3 are classificationoriented. We compare two traditional text classification models (BERT (Devlin, 2018) and DeBERTa (He et al., 2020)) against 6 task 1 task 2 task 3 task Model (1/2) (1/5) 0.011 BERT DeBERTa 0.010 Llama-3.2-1B 0.581 Llama-3.2-3B 0.578 Llama-3.1-8B 0.665 0.563 Mistral-7B 0.726 GPT-4o 0.642 Qwen2.5-7B 0.695 DeepSeek-V3 0.010 0.011 0.269 0.297 0.379 0.281 0.464 0.293 0.422 Bal. ACC 0.537 0.500 0.546 0.597 0.530 0.426 0.823 0.699 0.724 PR AUC 0.396 0.527 0.185 0.385 0.168 0.268 0.696 0.375 0.655 Bal. ACC 0.333 0.333 0.320 0.597 0.357 0.529 0.677 0.578 0.668 Mac. 0.328 0.328 0.326 0.402 0.359 0.140 0.363 0.241 0.351 ROUGE Cosine Sim. / / 0.329 0.290 0.355 0.575 0.619 0.623 0.623 / / 0.033 0.041 0.039 0.194 0.199 0.201 0.207 Table 2: Our UNBench contains four tasks. For each task, we choose two metrics to show. (1/k) means choosing 1 from choices, Bal. ACC is balance accuracy, PR AUC is precision-recall AUC. The best results for each metric are highlighted in bold, while the second-best results are underlined. More results could be found in Appendix B. instruction-tuned LLMs: Llama-3.2several 1B-Instruct (Dubey et al., 2024), Llama-3.23B-Instruct, Llama-3.1-8B-Instruct, Mistral-7BInstruct (Jiang et al., 2023), DeepSeek-V3 (Liu et al., 2024), Qwen2.5-7B-Instruct (Yang et al., 2024), and GPT-4o via the Azure API. BERT and DeBERTa are fine-tuned for three epochs with learning rate of 5 105. Llama models run on an 8A6000 GPU server, while Mistral and DeepSeek are accessed through the TogetherAI platform. Since Task 4 (representatives statement generation) is inherently generative task, we only evaluate LLMs on it, setting temperature of 0.0 for consistent comparisons and adjusting maximum output lengths per task. Settings. For each classification-oriented task, we employ time-based train/test split. Specifically, we reserve half of the samples from the less frequent labels (according to chronological order) as the test set, ensuring the training set remains balanced and temporally earlier. This protocol simulates real-world scenarios where future events must be predicted from past data. Metrics. Task 1 is multi-choice question with ranging from 2 to 5. We calculate accuracy by checking whether the model identifies the single valid co-penholder. Tasks 2 and 3 are classification problems (multi-class and binary, respectively), so we report metrics robust to class imbalance, including F1-score, balanced accuracy (Bal. ACC), and PR AUC. Task 4 is evaluated using textgeneration metrics (ROUGE) and semantic similarFigure 3: Models performance in Task 1 by varying the number of choices. ity (Sentence-BERT (Reimers, 2019)) to measure how closely the generated statements match ground truth in style and content. For brevity, we present only two primary metrics per task in Table 2, with detailed breakdowns available in Appendix 7."
        },
        {
            "title": "5.3 Task 1: Co-Penholder Judgement",
            "content": "This task evaluates LLMs ability to identify strategic geopolitical alliances by selecting copenholders for UNSC draft resolutions, requiring nuanced understanding of international relations and procedural norms. GPT-4o (0.726) and DeepSeek-V3 (0.695) dominate, demonstrating superior contextual reasoning and geopolitical knowledge. Smaller LLMs (e.g., Llama-3.2-1B: 0.581) lag significantly, while traditional models (BERT: 0.011) fail entirely, underscoring the necessity of LLM-scale architectures for complex political inference. In addition, we vary the number of candidate choices (25) to test models robustness 7 under increasing complexity. As shown in Figure 3, all models exhibit declining accuracy as choices increase, with GPT-4o maintaining dominance across all levels. The widening performance gaps as choices increase highlight the divergent capacities of LLMs to resolve ambiguity, validating that large, modern LLMs excel at synthesizing latent political knowledge, while smaller or traditional models lack the representational capacity for such nuanced reasoning. 5.4 Task 2: Representatives Voting Simulation Focused on simulating country-specific voting behavior, this task tests models ability to infer issuespecific voting patterns by analyzing how nations prioritize resolution content and contextual geopolitical dynamics. Unlike Task 1, which evaluates proactive alliance-building, Task 2 emphasizes reactive decision-making based on the interplay of national interests, ideological alignment, and external pressures. GPT-4o achieves the highest performance (0.823 Bal. ACC, 0.696 PR AUC), demonstrating strong ability to model nuanced trade-offs. DeepSeek-V3 (0.724 Bal. ACC, 0.655 PR AUC) and Llama-3.2-3B (0.597 Bal. ACC) show moderate success but struggle with ambiguous cases, while Mistral-7B (0.426 Bal. ACC) performs poorly, reflecting its inability to systematically weigh competing factors. The results highlight LLMs potential to simulate diplomatic behavior but reveal significant variance in their capacity to reason about issue-specific voting dynamics. country-specific rhetoric and protocol. Qwen2.57B and DeepSeek-V3 tie for semantic fidelity (0.623 Cosine), demonstrating strong alignment with the intended meaning and tone of diplomatic statements. DeepSeek-V3 also leads in lexical overlap (0.207 ROUGE), suggesting better adherence to precise terminological requirements. Mistral-7B achieves high Cosine similarity (0.575) but modest ROUGE (0.194), indicating strength in paraphrasing and conceptual alignment rather than verbatim replication. All models underperform in ROUGE, exposing limitations in precise terminological alignmenta critical requirement for diplomatic drafting. This highlights the unresolved challenge of balancing creativity and protocol adherence in LLM-generated diplomatic text, particularly in capturing the formal and nuanced language of international diplomacy. Cross-Task Summary. Each task targets distinct facet of UNSC decision-making. Task 1 primarily tests textual and geopolitical reasoning in multi-choice format, Task 2 and Task 3 emphasize political prediction capabilities (from simulating individual votes to forecasting final outcomes), and Task 4 stresses diplomatic language generation, requiring alignment with formal protocols and country-specific rhetoric. Performance gaps across tasks and models highlight both the promise and complexity of applying LLMs to real-world international governance, reinforcing the need for dedicated benchmarks as UNBench."
        },
        {
            "title": "6 Conclusions",
            "content": "Whereas Task 2 centers on individual votes, Task 3 measures document-level outcome prediction, requiring holistic reasoning about all 15 Council members to predict whether resolution is eventually [Adopted] or [NotAdopted]. GPT-4o shows the best Bal. ACC (0.677) and competitive macroF1 (0.363), while Llama-3.2-3B surpasses others in macro-F1 (0.402) but has lower Bal. ACC (0.597). The divergence between Bal. ACC and F1 metrics reveals the challenge of modeling adoption mechanics, where understanding Council-wide political dynamics, potential veto threats, and support coalitions play roles."
        },
        {
            "title": "Generation",
            "content": "This task evaluates LLMs ability to generate stylesensitive diplomatic statements that align with This paper introduces UNBench, the first comprehensive benchmark for evaluating LLMs capabilities in political science through UN Security Council records (1994-2024). By designing four interconnected tasks spanning the complete UN resolution lifecycleco-penholder judgement, representatives voting simulation, draft adoption prediction, and representative statement generationwe provide more authentic framework for assessing LLMs understanding of complex diplomatic dynamics. Our work not only addresses the current gap in LLM evaluation frameworks but also establishes foundation for future research at the intersection of artificial intelligence and international relations, demonstrating how LLMs could potentially assist in analyzing global governance processes. More applications and detailed analyses can be found in Appendix 7."
        },
        {
            "title": "7 Limitations",
            "content": "Despite UNBenchs contributions, several limitations should be noted. Our dataset is restricted to UN Security Council records from 1994-2024, which may not fully represent broader international relations dynamics. The benchmark currently focuses on English-language documents, potentially missing nuances in diplomatic communications in other UN languages. Additionally, as political landscapes evolve rapidly, historical patterns in our training data may not accurately reflect current diplomatic dynamics. Finally, there exists potential data contamination since LLMs may have been pre-trained on publicly available UN documents. The original UN Security Council records are publicly available, and their interpretation and official authority remain with the United Nations. In this paper, generative AI tools (ChatGPT, Grammarly) are used to fix grammar bugs and typos. As benchmark for political science, we do not foresee any potential risks that need to be disclosed."
        },
        {
            "title": "References",
            "content": "Michael Bailey, Anton Strezhnev, and Erik Voeten. 2017. Estimating dynamic state preferences from united nations voting data. Journal of Conflict Resolution, 61(2):430456. Mohaddeseh Bastan, Mahnaz Koupaee, Youngseo Son, Richard Sicoli, and Niranjan Balasubramanian. 2020. Authors sentiment prediction. arXiv preprint arXiv:2011.06128. Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023. Adapting large language models via reading comprehension. In The Twelfth International Conference on Learning Representations. MIT Election Data and Science Lab. 2022a. State Precinct-Level Returns 2018. MIT Election Data and Science Lab. 2022b. U.S. Senate Precinct-Level Returns 2020. Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Karish Grover, SM Angara, Md Shad Akhtar, and Tanmoy Chakraborty. 2022. Public wisdom matters! discourse-aware hyperbolic fourier co-attention for social text classification. Advances in Neural Information Processing Systems, 35:94179431. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced arXiv preprint bert with disentangled attention. arXiv:2006.03654. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021a. Aligning ai with shared human values. Proceedings of the International Conference on Learning Representations (ICLR). Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Marc Comes Redondo and Guillem Comes Llovera. 2025. United nations security council resolutions. Accessed: 2025-02-15. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Zhiwei Jin, Juan Cao, Han Guo, Yongdong Zhang, and Jiebo Luo. 2017. Multimodal fusion with recurrent neural networks for rumor detection on microblogs. In Proceedings of the 25th ACM international conference on Multimedia, pages 795816. Paul J. Kohlenberg, Nadine Godehardt, Stephen Aris, Fred Sündermann, Aglaya Snetkov, and Juliet Fall. 2019. Introducing unscdeb8 (beta). database for corpus-driven research on the united nations security council. Anastassia Kornilova and Vlad Eidelman. 2019. Billsum: corpus for automatic summarization of us legislation. arXiv preprint arXiv:1910.00523. Lincan Li, Jiaqi Li, Catherine Chen, Fred Gui, Hongjia Yang, Chenxiao Yu, Zhengguang Wang, Jianing Cai, Junlong Aaron Zhou, Bolin Shen, et al. 2024. Political-llm: Large language models in political science. arXiv preprint arXiv:2412.06864. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. LlamaIndex. 2024. Llamaparse. Open-source document parsing tool. OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Reimers. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. 9 Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. 2023. Whose opinions do language models reflect? In International Conference on Machine Learning, pages 2997130004. PMLR. Dong Shu, Haoran Zhao, Xukun Liu, David Demeter, Mengnan Du, and Yongfeng Zhang. 2024. Lawllm: Law large language model for the us legal system. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 48824889. Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu. 2020. Fakenewsnet: data repository with news content, social context, and spatiotemporal information for studying fake news on social media. Big data, 8(3):171188. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. United Nations. 2025. Un parallel corpus. Accessed: 2025-02-15. United Nations Security Council. 2024. United nations security council official digital library. Accessed: 14 Feb. 2025. Erik Voeten. 2013. Data and analyses of voting in the united nations: General assembly. Routledge handbook of international organization, pages 5466. Erik Voeten, Anton Strezhnev, and Michael Bailey. 2009. United Nations General Assembly Voting Data. Michael Wojatzki, Eugen Ruppert, Sarah Holschneider, Torsten Zesch, and Chris Biemann. 2017. Germeval 2017: Shared task on aspect-based sentiment in social media customer feedback. Proceedings of the GermEval, pages 112. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115."
        },
        {
            "title": "A Data Analysis",
            "content": "This section introduces our data analysis on the collected datasets from United Nation. A.1 Subjects Analysis A.1.1 Author-subject Co-occurrence Analysis The author-subjects co-occurrence matrix is shown in Figure 4, which reveals distinct patterns in UN Security Council draft resolution authorship. Permanent members of the Security Councilparticularly the United States, United Kingdom, and Francedemonstrate high engagement across most subject areas, with notably strong involvement in peacekeeping operations and humanitarian assistance. This pattern underscores their important role in global governance while reflecting Western powers emphasis on human rights and humanitarian interventions. From thematic perspective, peacekeeping operations, humanitarian assistance, and human rights in armed conflicts emerge as the most prominent subjects across member states, forming an interconnected core of Security Council priorities. This pattern suggests holistic approach to global security challenges, where military peacekeeping efforts are consistently coupled with humanitarian considerations. The strong co-occurrence between sanctions-related topics and peacekeeping operations indicates that the Council frequently employs dual strategy of enforcement and intervention. Notably, the emergence of terrorism and counter-terrorism as significant themes reflects the evolving nature of global security threats. The data also reveals that technical cooperation and peacebuilding subjects often appear alongside humanitarian assistance, suggesting long-term approach to crisis resolution that extends beyond immediate security concerns. A.1.2 Controversial Subjects in UN The Top-30 subjects that at least one country did It reveals not vote Yes is shown in Figure 5. that enforcement-related topicsparticularly sanctions, peacekeeping operations, and humanitarian assistancegenerate the most disagreement in Security Council voting. The high frequency of nonaffirmative votes on these subjects suggests persistent tensions between international intervention and national sovereignty. Additionally, the presence of international criminal courts and human rights matters among frequently contested subjects highlights the ongoing challenges in balancing international 10 Figure 4: Author-Subjects Relationships. This figure shows the co-occurrence matrix of the top 15 authors and subjects. Each cell represents the number of times an author has written about topic. The darker the cell, the more the author has written about the topic. Figure 5: This figure shows the top 30 subjects that at least one country did not vote Yes on. 11 justice with state sovereignty concerns. A.1.3 Trend in UN Resolutions Figure 7: Distribution of the duration of each subject. We can observe that most subjects last for 1 to 5 years, while few last for more than 30 years. immediate crises that do not necessarily remain central once the situation is resolved. This pattern underscores the dynamic balance between the UNs long-standing priorities and its flexibility in addressing emerging global issues as they arise. Our benchmark collects all resolutions from this period, making it both challenging and comprehensive, capturing the full scope of the UNs evolving focus on global governance. A.2 National-Wised Analysis A.2.1 Voting Frequency Figure 8: The top 30 countries that participated in voting the most. Figure 8 shows the vote frequency of Top-30 countries. It reveals clear dominance by the five permanent members of the UN Security Council (UNSC), namely China, France, Russia, the 12 Figure 6: This figure shows the top 10 subjects per 5year period from 1994 to 2024. The trends in UN resolution topics are also changing over time. The Top-10 subjects per 5year period from 1994 to 2024 are shown in Figure 6. Besides, we also show the distribution of the duration of each subject in Figure 7. Most subjects last for 1 to 5 years, while few last for more than 30 years. The two figures reveal two key patterns in the United Nations focus on global issues. First, certain topics have consistently appeared over the years, indicating the UNs ongoing attention to these issues. Topics such as international peace and security, human rights, and conflict resolution have remained at the forefront of UN resolutions, reflecting the organizations continuous efforts to address global stability, protect human rights, and resolve conflicts. These persistent topics suggest sustained, long-term commitment to addressing the most pressing and enduring global challenges. On the other hand, there are topics that have emerged briefly and faded over time, often in response to specific events or crises. For example, resolutions related to regional conflicts or emergency sanctions have been intermittently, typically tied to shortlived geopolitical developments such as military interventions or economic sanctions. These topics highlight the UNs responsive nature, focusing on Author Rejection Count RUSSIAN United States United States CHINA United Kingdom RUSSIAN RUSSIAN France CHINA France CHINA United Kingdom RUSSIAN Germany CHINA Germany CHINA Japan CHINA Italy 68 52 47 45 39 39 22 21 15 13 Figure 9: Distribution of the number of votes each country participated United Kingdom, and the United States. These countries consistently hold the highest number of votes, reflecting their influential roles in shaping international decisions and maintaining global security. In addition to the permanent members, other non-permanent members such as Japan, Brazil, Argentina, Germany, and Nigeria also appear prominently on the list, highlighting their significant involvement in global governance. Their high voting frequencies may reflect their strategic interests, regional influence, and active participation in international diplomacy. The presence of these countries, along with the permanent members, underscores the UN Security Councils complex decision-making process, where both major powers and key regional players contribute to shaping resolutions and global policies. The distribution of the number of votes each country participated in, as shown in Figure 9, reveals that most countries have participated in fewer than 250 votes. This suggests that the majority of countries engage in voting on limited number of resolutions, likely reflecting their geopolitical priorities and areas of influence within the UN. While some countries may focus on specific regional or issue-based resolutions, others may be more passive in their participation, contributing to fewer votes overall. The relatively small number of countries with more than 250 votes highlights the most active players in UN decision-making, likely including key international powers and nations with significant stakes in global governance. This distribution underscores the varied levels of involvement in the UNs voting process, with certain countries playing consistently active role while others engage more selectively. Table 3: This table shows the top 10 pairs of authors and countries that vote not Yes the most. The Rejection means receiving either No or Abstention vote. The Count column represents the number of times the authors draft was not voted Yes by the country. A.2.2 Country Relationships revealed within"
        },
        {
            "title": "UN Resolutions",
            "content": "Table 3 presents the top 10 pairs of authors and countries that most frequently voted \"No\" or abstained on draft resolutions. The data reveals that the United States has the highest number of rejections, with Russia rejecting 68 times and China 52 times. This indicates consistent divide between the U.S. and these two powers, likely reflecting ongoing geopolitical tensions. Similarly, the United Kingdom and France also show high rejection rates from both Russia and China, suggesting shared stance among Western powers in opposition to certain resolutions proposed by these countries. On the other hand, countries like Germany, Japan, and Italy appear less frequently in the table, with rejections ranging from 13 to 22 times. These smaller states seem to align more often with the major powers but still demonstrate some differences, particularly with China and Russia. Overall, the table highlights significant diplomatic rifts between the Western powers and Russia/China, with frequent rejections indicating key areas of contention in international relations. Table 4 and Table 5 provide insights into the voting patterns of country pairs within the United Nations, highlighting both strong collaboration and significant divergence in voting behaviors. Table 4 shows that certain country pairs, such as France and the United Kingdom (1,153 joint \"Yes\" votes), United Kingdom and the United States (1,147 joint \"Yes\" votes), and France and the United States (1,142 joint \"Yes\" votes), consistently align on many resolutions, reflecting their close diplomatic 13 Country Pair (FRANCE, UK) (UK, US) (FRANCE, US) (CHINA, UK) (CHINA, FRANCE) (CHINA, US) (RUSSIAN, UK) (FRANCE, RUSSIAN) (RUSSIAN, US) (CHINA, RUSSIAN) Count 1,153 1,147 1,142 1,068 1,064 1,058 1,035 1,031 1,024 1,013 Country Pair Count (CHINA, RUSSIAN) (ALGERIA, CHINA) (ALGERIA, RUSSIAN) (RUSSIAN, SOUTH AFRICA) (GABON, RUSSIAN) (CHINA, GABON) (RUSSIAN, VENEZUELA (KENYA, RUSSIAN) (EGYPT, RUSSIAN) (CHINA, INDIA) 69 8 7 6 6 6 6 5 5 5 Table 4: This table shows the top 10 pairs of countries that voted Yes together the most. US and UK are the abbreviations for UNITED STATES and UNITED KINGDOM. The Count column represents the number of times the two countries voted Yes together. Table 5: This table shows the top 10 pairs of countries that did not vote Yes together the most. The Count column represents the number of times the two countries did not vote Yes together. and strategic cooperation. In contrast, China and the Western powers also exhibit frequent collaboration, with the China-United Kingdom, ChinaFrance, and China-United States pairs each voting together over 1,000 times, indicating areas of common interest despite occasional political differences. On the other hand, Table 5 reveals country pairs that most often did not vote \"Yes\" together. The China-Russia pair stands out with 69 instances of disagreement, indicating that the two countries share some geopolitical interests. Other pairs, such as Algeria-China (8 times) and Russia-South Africa (6 times), also display divergent voting patterns, reflecting how national and regional interests can influence voting behavior at the UN. These tables underscore the complex dynamics of international diplomacy, where countries may cooperate on certain issues while diverging on others based on their specific interests and priorities. A.3 Dataset Construction Our dataset is constructed from United Nations Security Council (UNSC) meeting records, draft resolutions, and voting histories spanning the years 1994 to 2024. The resulting corpus not only includes the textual content of each draft resolution but also contextual metadata such as voting outcomes, sponsoring nations, meeting transcripts, and the temporal sequence of events. The overarching goal of this work is to provide unified and extensive collection of the decisionmaking process, thereby enabling evaluation of multiple LLM capabilities in single benchmark. To achieve this, we collect multi-perspective data from the official website and digital library(United Nations Security Council, 2024), which archive draft resolutions, voting records, and meeting minutes. Below, we highlight key challenges and our corresponding strategies in constructing in the different stages of our benchmark construction. Data Collection. In the data collection stage, we have three challenges: (1) Fragmented Records. Draft resolutions, voting logs, and meeting transcripts reside in separate sections of the UN database. We utilize shared identifiers (e.g., resolution numbers, meeting record IDs) to link these sources. As illustrated schematically in Figure 2, we first retrieve all draft resolutions, then query corresponding voting records by resolution ID (when applicable), and finally map meeting transcripts via the meeting record ID. (2) Missing or Incomplete Metadata. Despite the UNs comprehensive record-keeping, certain entries contain missing fields (e.g., sponsor lists), inconsistent data formats, or broken links. We mitigate these issues by cross-referencing multiple UN repositories, manually curating ambiguous entries, and applying standardized naming conventions for country references. (3) Historical Document Diversity. The official document formats and website structures vary considerably across decades, complicating automated crawling and parsing. We address this by implementing adaptive web-scraping scripts that detect layout differences and by performing iterative quality checks to ensure data consistency. Data Conversion. UN documents are primarily stored in PDF format, making direct ingestion by current LLMs infeasible. We therefore extract and 14 2 choices 3 choices 4 choices 5 choices ROUGE Jaccard TF-IDF SentBERT Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B GPT-4o DeepSeek-V3 Mistral-7B Qwen2.5-7B 0.581 0.578 0.665 0.726 0.695 0.563 0.642 0.394 0.393 0.507 0.613 0.555 0.407 0.478 0.312 0.328 0.408 0.511 0.443 0.335 0. 0.269 0.297 0.379 0.464 0.422 0.281 0.293 Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B GPT-4o DeepSeek-V3 Mistral-7B Qwen2.5-7B 0.0328 0.0407 0.0394 0.1985 0.2069 0.1935 0.2008 0.0304 0.0341 0.0363 0.1837 0.1876 0.1688 0.1761 0.3666 0.4287 0.4021 0.7958 0.8012 0.7522 0.7842 0.3293 0.2902 0.3553 0.6188 0.6225 0.5750 0. Table 6: Comprehensive results for Task 1. Table 7: Comprehensive results for Task 4. Similarity of IT-IDF and SentBert are calculated by cosine similarity. convert the content into plain text. Early attempts using generic Python PDF libraries yielded mixed accuracy due to the unstructured, domain-specific nature of political documents. We applied LLMbased parser (LlamaParse(LlamaIndex, 2024)) to handle complex formatting (e.g., multi-column layouts, footnotes, multilingual text). Data Processing. (1) Labeling Adopted vs. Unadopted Drafts. Some drafts never become official resolutions (i.e., unadopted), lacking formal resolution ID. We thus inspect the official notes in each drafts record and cross-verify with the final resolution index to categorize them correctly. (2) Country Name Normalization. Different records refer to the same country with variations (e.g., United Kingdom vs. Kingdom). We employ standardized dictionary to unify references to the same country entity across all entries. (3) Metadata Alignment. For each draft ri, we compile relevant informationauthor/sponsor countries, date, issue category, voting breakdown, and meeting transcriptsinto structured format compatible with modern NLP frameworks. Through these steps, UNBench incorporates the entire lifecycle of each UNSC draft resolution, from initial sponsorship and negotiations to final votes and discussions, ensuring comprehensive coverage for our benchmark tasks."
        },
        {
            "title": "B Detailed Results of UNBench",
            "content": "In this section, we present the detailed results of the UNBench benchmark, evaluating multiple models across four distinct tasks. The tables provide comprehensive performance metrics for each model on various tasks, including accuracy, precision, recall, AUC, F1 score, and other relevant evaluation metrics. Task 1  (Table 6)  evaluates the models performance across multiple-choice tasks with varying numbers of choices (2 to 5). The results indicate that GPT-4o outperforms the other models across all choice levels, particularly excelling in the 2choice and 3-choice tasks, where it maintains the highest scores in terms of accuracy. Models like Llama-3.1-8B and DeepSeek-V3 also show competitive results, especially for more complex tasks (4 and 5 choices), though they trail behind GPT-4o. Task 2  (Table 8)  presents set of metrics evaluating model performance on binary classification tasks. Here, Qwen2.5-7B leads with the highest accuracy (0.935) and AUC (0.719), indicating its strong ability to differentiate between classes. However, GPT-4o shows superior performance in other metrics such as F1 (0.686) and G-Mean (0.807), making it the most balanced model for this task. DeepSeek-V3 also performs strongly across multiple metrics, especially in precision (0.828), suggesting it excels in tasks where false positives need to be minimized. Task 3  (Table 9)  focuses on multi-class classification tasks, where GPT-4o again stands out with the highest accuracy (0.968) and balanced performance across other metrics like recall, F1, and G-Mean. DeepSeek-V3 shows strong performance in precision (0.828) and recall (0.453), which may indicate more specialized capability in identifying specific class instances. Task 4  (Table 7)  assesses the models ability to generate meaningful representations and comparisons between text using various similarity measures like ROUGE, Jaccard, and cosine similarity. DeepSeek-V3 performs best across multiple metrics, particularly in cosine similarity (0.8012 with TF-IDF and 0.6225 with SentBERT), demonstrating its strength in textual similarity and comparison tasks. GPT-4o also shows strong performance, particularly with cosine similarity (0.7958 with TFIDF and 0.6188 with SentBERT). Overall, the results demonstrate the competitive nature of current models, with GPT-4o leading in several tasks due to its balanced performance across various metrics. However, other models like 15 Accuracy AUC Bal. ACC Precision Recall F1 PR_AUC MCC G-Mean Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B GPT-4o DeepSeek-V3 Mistral-7B Qwen2.5-7B 0.898 0.523 0.917 0.922 0.931 0.557 0.935 0.497 0.597 0.532 0.731 0.720 0.593 0. 0.320 0.597 0.357 0.677 0.668 0.426 0.699 0.332 0.520 0.360 0.400 0.464 0.345 0.373 0.320 0.597 0.357 0.677 0.668 0.426 0.699 0.326 0.402 0.359 0.363 0.351 0.268 0.375 0.334 0.956 0.338 0.343 0.343 0.341 0.344 0.006 0.087 0.079 0.162 0.151 0.100 0. 0.464 0.597 0.502 0.729 0.718 0.569 0.719 Table 8: Comprehensive results for Task 2. Accuracy AUC Bal. ACC Precision Recall F1 PR_AUC MCC G-Mean Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B GPT-4o DeepSeek-V3 Mistral-7B Qwen2.5-7B 0.815 0.523 0.935 0.968 0.966 0.867 0. 0.546 0.597 0.530 0.823 0.724 0.529 0.578 0.546 0.597 0.530 0.823 0.724 0.529 0.578 0.083 0.073 0.211 0.714 0.828 0.084 0.250 0.245 0.679 0.076 0.660 0.453 0.151 0.189 0.124 0.132 0.111 0.686 0.585 0.108 0.215 0.185 0.385 0.168 0.696 0.655 0.140 0. 0.057 0.087 0.098 0.670 0.597 0.044 0.179 0.456 0.591 0.273 0.807 0.671 0.370 0.427 Table 9: Comprehensive results for Task 3. DeepSeek-V3 and Qwen2.5-7B also show strong results in specific areas, such as precision and text similarity. These findings highlight the strengths and limitations of each model, offering valuable insights for selecting the most suitable model for specific tasks within the UNBench framework."
        }
    ],
    "affiliations": [
        "Cisco",
        "Emory University",
        "Illinois Institute of Technology",
        "Meta",
        "Salesforce",
        "University of Illinois at Chicago"
    ]
}