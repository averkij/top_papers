{
    "paper_title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning",
    "authors": [
        "Chenwei Lou",
        "Zewei Sun",
        "Xinnian Liang",
        "Meng Qu",
        "Wei Shen",
        "Wenqi Wang",
        "Yuntao Li",
        "Qingping Yang",
        "Shuangzhi Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks."
        },
        {
            "title": "Start",
            "content": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning Chenwei Lou1, Zewei Sun1, Xinnian Liang1, Meng Qu1, Wei Shen1, Wenqi Wang1, Yuntao Li1, Qingping Yang1, Shuangzhi Wu1, 1ByteDance Seed Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks. This substantial token decrease directly translates to significant reduction in inference computational load. AdaCoT pioneers adaptive CoT triggering, offering practical and principled solution for developing more efficient, responsive, and cost-effective LLMs, particularly crucial for interactive and resource-sensitive applications. Our model can be found at www.volcengine.com. Date: May 20, 2025 Correspondence: Chenwei Lou at louchenwei@bytedance.com 5 2 0 2 7 1 ] . [ 1 6 9 8 1 1 . 5 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have garnered substantial attention due to their remarkable ability to encode extensive world knowledge from vast corpora [28], enabling impressive performance across diverse tasks such as question answering, creative writing, and summarization. Despite these successes, LLMs often demonstrate limitations in tasks requiring sophisticated reasoning, including solving complex mathematical problems and intricate coding puzzles. To mitigate this, recent methodologies have employed Chain-of-Thought (CoT) 1 prompting [41], which encourages models to explicitly generate step-by-step reasoning prior to producing final answers. This approach significantly enhances the reasoning capability of models, even achieving human-expert levels in certain domains [9, 16, 34, 47]. However, employing CoT prompting also poses critical challenges during inference [6]. Specifically, it substantially increases the number of tokens generated, even for simple queries that do not benefit from elaborate reasoning, such as straightforward arithmetic questions. This indiscriminate token expense consequently raises deployment costs and reduces inference efficiency. Ideally, model should adaptively determine when detailed reasoning is necessary. For instance, simple queries like \"What is 1+1?\" should be answered immediately without additional reasoning steps, whereas more complex queries require deeper and step-by-step reasoning. An adaptive strategy would thus optimize token usage, balancing cost-efficiency with response quality. Recently, few efforts has made attempts towards this direction. These approaches can be broadly categorized into three main directions. (1) Incorporate length penalties or brevity rewards during the reinforcement learning (RL) stage to encourage shorter, more concise reasoning paths [1, 2, 14, 23, 36, 38]. (2) Restructure CoT outputs through post-processing such as iterative summarization or pruning [3, 19, 21, 46, 4850, 53]. (3) Employ explicit user instructions or hand-crafted selection mechanisms to control the use of CoT [5, 10, 27, 29, 32, 52]. Despite their contributions, they mainly focus on monotonic reasoning reduction, failing to account for the nuanced variability in query complexity, i.e., treating simple and difficult prompts adaptively. Moreover, they lack principled optimization framework to guide balancing response quality against deployment cost. To address these limitations, we introduce AdaCoT (Adaptive Chain-of-Thought), novel approach grounded in formal mathematical analysis. Our key insight is framing adaptive reasoning as multi-objective optimization problem with two competing goals: maximizing response accuracy and minimizing deployment costs. Specifically, we formalize this balance through Pareto optimization, seeking optimal trade-offs between reasoning complexity and inference efficiency. Such mathematical framework provides clear theoretical grounding for dynamically adapting CoT triggering based on query complexity. Leveraging this formalization, we propose an RL-based strategy explicitly designed around the Pareto optimization framework, enabling effective control of the models decision boundary for initiating CoT prompting. During training, the RL agent dynamically assesses the complexity of incoming user queries to determine the necessity and extent of reasoning steps. By carefully designing the reward function to incorporate penalty coefficients, we encourage the RL agent to seek solutions along the Pareto frontier, explicitly optimizing trade-offs between response accuracy and token expenditure. This structured exploration enables the model to effectively discern when detailed reasoning is beneficial, thereby systematically enhancing inference efficiency and significantly reducing deployment costs. The proposed AdaCoT framework delivers substantial benefits in LLM operational efficiency. By empowering models to selectively engage CoT, AdaCoT can reduce triggering rates to as low as 3.18% and cut average response tokens by 69.1% in production settings. This significant reduction in computational load is achieved while maintaining strong performance on 15 widely-adopted benchmarks. For example, AdaCoT can achieve 62.8% average score using only 53.3% CoT rate, closely rivaling the 65.0% score of model that always employs CoT. These improvements directly translate to more cost-effective and responsive LLM systems."
        },
        {
            "title": "2 The AdaCoT Framework",
            "content": "Enabling large language model (LLM) to dynamically decide whether to invoke Chain-of-Thought (CoT) reasoning based on the complexity of user queries is critical task, which allows LLMs to allocate computational resources more rationally, i.e., spending tokens on complex reasoning tasks while avoiding unnecessary overhead for simple queries. To achieve this, we introduce AdaCoT, unified framework for adaptive reasoning. The central insight is that the decision to employ CoT prompting can be naturally cast as an optimization problem involving two competing goals: (1) maximizing response accuracy and (2) minimizing deployment costs. To capture this trade-off, we frame the task as Pareto optimization problem, providing principled foundation for balancing reasoning quality against computational efficiency. Based on this formulation, we propose an RL-based control 2 strategy that governs the invocation of CoT reasoning. Specifically, we train policy model learning to assess the complexity of each query and decide whether CoT reasoning should be used. During RL training, the policy model is optimized using reward signal carefully designed to reflect the Pareto trade-off, incorporating both task performance and token efficiency. Through this RL-based mechanism, AdaCoT learns to allocate reasoning effort adaptively, yielding high-quality responses with minimal computational cost."
        },
        {
            "title": "2.1 Adaptive Reasoning as a Pareto Optimization Problem",
            "content": "We formulate the adaptive reasoning challenge as Pareto optimization problem, aiming to simultaneously maximize model performance and minimize CoT usage. Let = {(xi, yi)}N be dataset of query-response pairs, where xi is the input query and yi is the ground truth response. Let fθ be an LLM parameterized by θ, and let rθ(xi) be the response generated by the model for input xi. To CoT usage is measured by the CoT triggering rate (θ), defined as the proportion of responses that include reasoning: i=1 (θ) = 1 (cid:88) i= 1[HasReasoning(rθ(xi))] (1) where 1[] is the indicator function and HasReasoning() determines if response contains explicit CoT steps (e.g., non-empty content within <think>...</think> tags). On the other hand, model performance (θ) is defined as the average score on set of evaluation metrics: (θ) = 1 M (cid:88) j=1 Scorej(θ) (2) where is the number of evaluation instances/metrics and Scorej(θ) is the models score on the j-th evaluation. Putting CoT usage (θ) and model performance (θ) together, the objective is to find model parameters θ that achieve an optimal trade-off: θ = arg max θ {λP (θ) λT (θ)} (3) or more generally, to find solutions on the Pareto frontier of (P (θ), 1 (θ)). The hyperparameters λP and λT (or their implicit ratio) control the relative importance of performance versus CoT reduction. AdaCoT focuses on providing mechanisms to explore this frontier."
        },
        {
            "title": "2.2 Training Pipeline for AdaCoT",
            "content": "The AdaCoT training pipeline integrates supervised fine-tuning (SFT) as an initialization phase, followed by multi-stage reinforcement learning (RL) to refine the adaptive CoT triggering behavior. 2.2.1 Data Preparation and Supervised Fine-Tuning (SFT) as Warm-up To provide the model with an initial understanding of when CoT might be beneficial, we perform data preparation stage. This is achieved by leveraging an auxiliary model, guided by set of predefined principles (e.g., query complexity, expected reasoning depth, domain; see Appendix B). In our implementation, we use an internal 15B-parameter model to generate these annotations; however, the framework is model-agnostic and can be instantiated using any sufficiently capable LLM with basic instruction-following abilities. Queries are labeled as either likely benefiting from CoT or likely suitable for direct answer. This principled, automated labeling process is more consistent and scalable than manual annotation. The SFT training data is then structured as follows: For queries labeled as benefiting from CoT, responses retain the full reasoning process: <think>reasoning_steps</think>answer. For queries labeled as not requiring CoT, responses omit explicit reasoning but maintain structural consistency: <think></think>answer. This SFT stage serves as \"warm-up\", equipping the model with foundational capability to distinguish between 3 these two response styles. The consistent use of <think></think> tags is crucial for maintaining response format integrity."
        },
        {
            "title": "2.2.2 Reinforcement Learning for Adaptive CoT Control",
            "content": "The RL stage is pivotal for fine-tuning AdaCoTs adaptive reasoning capabilities. We design reward function R(x, r) for an input query and generated response r: R(x, r) = Rbase(x, r) α1 Pmiss(x, r) α2 Pover(x, r) γ Pfmt(r) (4) where Rbase(x, r) is the base reward reflecting response quality, Pmiss(x, r) is binary penalty for reasoning omission, Pover(x, r) is binary penalty for reasoning overuse, Pfmt(r) is binary penalty for format errors, and α1, α2, γ are non-negative penalty coefficients. By adjusting α1 and α2, we steer AdaCoT towards different CoT triggering decision boundaries, allowing exploration of the Pareto frontier."
        },
        {
            "title": "2.2.3 Addressing Decision Boundary Collapse with Selective Loss Masking",
            "content": "A significant challenge in multi-stage RL, particularly when fine-tuning on specialized datasets with skewed CoT distributions (e.g., mathematical datasets where CoT is almost always beneficial), is the risk of the adaptive CoT triggering capability becoming unstable or collapsing. The model might revert to homogeneous behavior, either always or never triggering CoT, thereby losing the nuanced decision-making learned in earlier, more balanced training stages. We term this phenomenon decision boundary collapse. This is particularly problematic if the final RL stage has significant bias, as it can lead to the model almost completely losing its adaptive triggering capability. To address decision boundary collapse, AdaCoT incorporates Selective Loss Masking (SLM). SLM aims to preserve the CoT triggering ratio and distribution established during SFT or prior RL stages. It achieves this by selectively masking the loss contribution from the pivotal \"decision token\" during RL phases prone to distribution bias. This decision token is defined as the token immediately succeeding the <think> tag. The modified policy gradient loss under SLM, LSLM, is computed by excluding the loss component associated with this decision token: LSLM = (cid:88) ℓk (5) where ℓk is the original loss component for the k-th token, and kdecision is the index of the decision token. k=kdecision"
        },
        {
            "title": "3 Experiments",
            "content": "We conducted extensive experiments to evaluate the AdaCoT framework, focusing on its ability to navigate the performance-cost trade-off, the effectiveness of its adaptive triggering mechanism, and its impact on inference efficiency. This section details our experimental setup, presents the main results, and analyzes the findings."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "For our base model, we utilized our internal 15B/150B parameter Mixture-of-Experts (MoE) [15, 35] model. The AdaCoT post-training process comprised an initial SFT stage, followed by two-stage RL procedure: first, Mathematics-Focused RL stage (RL-Math) concentrated on complex, rule-verifiable problems, and second, General Domain RL stage (RL-General) which incorporated broader data and trained reward model. We compared our AdaCoT RL Models (Exp1-Exp4) against several baselines: Full CoT SFT Baseline (SFT model always generating CoT), Full CoT RL Baseline (RL model derived from the Full CoT SFT, always generating CoT), No CoT SFT Baseline (SFT model never generating CoT), No CoT RL Baseline (RL model derived from the No CoT SFT, never generating CoT), and our AdaCoT SFT Model (our model after only the SFT stage, also referred to as Adaptive SFT Model). The SFT and RL training datasets were constructed to cover diverse range of domains, including mathematics, reasoning, professional disciplines (e.g., law, medicine), dialogue, creative writing, and general knowledge 4 question answering. Both SFT and RL data were labeled for CoT necessity using the principle-guided assessment detailed in Appendix B. In the SFT dataset, approximately 67% of the samples were labeled as requiring CoT, while in the RL dataset, this proportion was around 40%. During SFT, queries identified as not requiring CoT were formatted with empty <think></think> tags. In the RL-Math stage, which is particularly prone to decision boundary collapse, we employed Selective Loss Masking (SLM), as described in Section 2.2.3. For the RL-General stage, we applied penalties according to Equation 4, systematically varying the α1 and α2 coefficients to explore different points on the Pareto frontier. Proximal Policy Optimization (PPO) [33] was used for all RL policy updates. For evaluation, we used 15 diverse open-source benchmark datasets to assess overall performance, measured by the average score. To balance internal iteration efficiency with evaluation accuracy, some of these datasets underwent up-sampling or down-sampling, or the number of inference runs per test sample was adjusted (with the final score being an average over multiple inferences). These datasets include LiveBench [43], MMLU Pro [40], SuperGPQA [7], GPQA [31], Chinese SimpleQA [12], SimpleQA [42], AIME24 & AIME25, MATH [13], OlympiadBench [11], SweBench Agentless [18], LiveCodeBench [17], KOR-Bench [24], ProcBench [8], and SysBench [30]. The detailed per-dataset scores, which form the basis for our average score calculations, are presented in Appendix  (Table 4)  . To specifically assess CoT triggering decisions on typical user queries, we curated high-quality balanced test set of 1000 prompts. These prompts were labeled for CoT necessity using the same principle-guided assessment as our SFT/RL training data and subsequently underwent manual verification to ensure label accuracy. On this set, we report CoT Triggering Accuracy, F1-score, Precision, and Recall, where the positive class indicates that CoT is required. Other metrics included the CoT triggering rate on the benchmark datasets and the average response token num on production setting."
        },
        {
            "title": "3.2 Results and Analysis",
            "content": "Our results demonstrate AdaCoTs ability to effectively control CoT invocation, leading to improved efficiency while maintaining strong performance. 3.2.1 Pareto Frontier Analysis We trained four variants of our AdaCoT RL model (Exp1-Exp4) by varying the penalty coefficients α1 (for missing CoT) and α2 (for overusing CoT). The specific coefficients were: Exp1 (α1 = 0.1, α2 = 0.3), Exp2 (α1 = 0.2, α2 = 0.3), Exp3 (α1 = 0.3, α2 = 0.3), and Exp4 (α1 = 0.3, α2 = 0.1). The format error penalty γ was consistently set to 1.0. Figure 1 illustrates the average score plotted against the CoT triggering rate for these models and the baselines, based on the average performance across our 15 benchmark datasets (detailed in Appendix Table 4). The No CoT SFT baseline achieved an average score of 43.6% with 0% CoT usage, while the No CoT RL baseline improved this to 47.7% at 0% CoT. The AdaCoT SFT Model (Adaptive SFT) registered 57.1% average score at 61.3% CoT rate. The AdaCoT RL models trace compelling Pareto frontier. AdaCoT RL Exp1 (43.1% CoT, 59.7% score) and AdaCoT RL Exp2 (53.3% CoT, 62.8% score) demonstrate significant performance gains over the AdaCoT SFT model while operating at lower or comparable CoT rates. Notably, AdaCoT RL Exp2 achieves 62.8% average score, approaching the Full CoT RL baseline (65.0% score, 100% CoT) with nearly half the CoT usage. As we increase the CoT triggering rate, AdaCoT RL Exp3 (65.4% CoT, 64.3% score) and AdaCoT RL Exp4 (67.7% CoT, 64.4% score) further push performance, closely rivaling the Full CoT RL baselines score but with approximately 32-35% less CoT invocation. Moreover, these results indicate that despite the fixed CoT triggering labels within the SFT/RL data, adjusting the combination of penalty coefficients during the RL phase enables the final RL model to learn triggering strategies that transcend these initial labeling constraints. This highlights AdaCoTs effectiveness in navigating the trade-off between performance and CoT cost. However, it is also observable that while the AdaCoT RL models achieve substantial efficiency gains and define superior Pareto curve compared to simpler baselines, they encounter slight performance bottleneck when their triggering rates are pushed higher. Specifically, even the highest-performing adaptive models (Exp3 and Exp4, with scores of 64.3% and 64.4% respectively) do not surpass the average score of the Full CoT RL baseline (65.0%). This suggests that while AdaCoT excels at reducing CoT for vast majority of queries 5 without compromising much on average performance, and indeed offers better score-to-cost ratio, the absolute peak average performance achieved by model specialized to always use CoT (Full CoT RL) remains marginally higher. This indicates that the adaptive mechanism, while highly effective, might not perfectly replicate or exceed the performance of an always-on CoT strategy in every single scenario contributing to the average, thus not fully crossing this specific optimal boundary for maximum average score. This could be due to the inherent complexities of learning universally optimal triggering heuristic or the RL optimization finding balance that prioritizes the significant cost savings available across the query distribution. Figure 1 Average Score vs. CoT Triggering Rate across 15 widely-adopted benchmarks. Blue points represent baseline models. The green point is the AdaCoT SFT model. Orange points represent AdaCoT RL models trained with varying penalty coefficients, forming an improved Pareto frontier (indicated by the orange dashed line and shaded region) over the baselines. The dotted line connects the No CoT RL baselines to the Full CoT RL baseline, illustrating simpler trade-off curve. 3.2.2 Adaptive CoT Triggering Performance and Ablation Studies on Daily-Use Queries We evaluated the CoT triggering capabilities of AdaCoT at various training stages using our curated 1000prompt daily-use test set. Table 1 presents these results, which include an ablation study for SLM and an assessment of the meta-reasoning strategy (discussed further in Section 4.2) at the SFT stage. The AdaCoT SFT model itself provides strong baseline for adaptive triggering. The results clearly show that the RL-Math stage without SLM suffers from severe decision boundary collapse: the model defaults to triggering CoT (Recall=1.0) but with poor precision (0.503) and consequently low overall accuracy (0.506). Applying SLM during the RL-Math stage effectively preserves the adaptive capability learned during SFT, maintaining high precision (0.938) and achieving significantly better accuracy (0.813). The final AdaCoT RL models (Exp1-4, emerging from the RL-General stage) demonstrate how adjusting the RL penalty coefficients (α1, α2) allows for fine-tuning of the decision boundary. AdaCoT RL Model Exp2, for example, achieves well-balanced F1-score of 0.814. The incorporation of meta-reasoning strategy at the SFT stage also shows notable improvement in triggering performance, point elaborated in Section 4.2. 6 Table 1 CoT triggering performance on the 1000 daily-use prompt test set across different AdaCoT stages and configurations (positive class: requires CoT). RL-Math is the Mathematics-Focused RL stage; RL-General refers to the final models (Exp1-4). Model Stage / Variant"
        },
        {
            "title": "AdaCoT SFT Model",
            "content": "AdaCoT RL Model (Exp1 - RL-General) AdaCoT RL Model (Exp2 - RL-General) AdaCoT RL Model (Exp3 - RL-General) AdaCoT RL Model (Exp4 - RL-General) RL-Math (without SLM) RL-Math (with SLM) AdaCoT SFT Model (with Meta-Reasoning) Accuracy F1-Score Recall Precision 0.795 0.657 0.816 0.809 0. 0.506 0.813 0.858 0.750 0.484 0.814 0.789 0.535 0.669 0.781 0. 0.616 0.322 0.804 0.716 0.370 1.000 0.670 0.762 0.959 0.975 0.823 0.879 0. 0.503 0.938 0.935 3.2.3 Response Length Reduction and Efficiency Gains The adaptive reasoning enabled by AdaCoT translates into significant reductions in computational costs. Table 2 shows the average response length and CoT triggering rates for AdaCoT RL Model Exp2 (selected for its balanced performance on the daily-use set and strong average benchmark performance) when applied to our production traffic testset, which reflects natural, unfiltered user query distributions. Table 2 Average response token num (with reduction noted) and CoT triggering rate on production traffic testset for AdaCoT RL Model Exp2 vs. Full CoT RL Baseline. Platform Model / Mode Avg. Response Tokens CoT Triggering Rate Mobile PC Full CoT RL Baseline AdaCoT RL Model Exp2 (Adaptive) Full CoT RL Baseline AdaCoT RL Model Exp2 (Adaptive) 377.18 116.70 (69.1%) 1376.31 405.25 (70.6%) 100.00% 3.18% 100.00% 12.50% As evidenced, AdaCoT RL Model Exp2 achieves very low CoT triggering rates in production setting (3.18% on mobile devices, 12.50% on PCs). This dramatic reduction from the 100% CoT usage of non-adaptive model translates directly into substantial computational savings."
        },
        {
            "title": "4.1 Design Considerations and Limitations",
            "content": "AdaCoT offers pragmatic approach to adaptive reasoning by combining principle-guided initial data labeling with RL-based optimization of the CoT decision boundary. This methodology was chosen to circumvent inherent challenges in purely autonomous CoT trigger learning, such as information asymmetry in assessing counterfactual benefits and the difficulty of quantifying quality degradation from CoT omission, particularly for subjective tasks. While AdaCoT is promising initial step, several limitations exist. The optimal CoT triggering strategy is relative to the base models capabilities, necessitating recalibration for different LLMs. Our current binary CoT invocation (on/off) simplifies continuous spectrum of reasoning depths and styles, potentially limiting nuance. Domain generalization remains challenge, as CoT necessity can vary significantly across knowledge areas, and the framework currently lacks personalization for user verbosity preferences. Moreover, the initial principle-guided labeling requires continuous refinement. Our Pareto analysis (Section 3.2.1) also indicates 7 that while AdaCoT significantly improves efficiency and nears the peak average performance of specialized always-on CoT models, small performance gap persists, highlighting the difficulty for adaptive mechanisms to achieve absolute maximum performance across all query types. Acknowledging the limitations of the current framework, we anticipate that future research will offer valuable critiques and further refine these initial explorations. Areas warranting deeper investigation include more granular control over reasoning, such as adaptive reasoning length where models dynamically adjust verbosity, or more nuanced triggering mechanisms beyond simple binary decision. We believe such continued efforts by the community are crucial for developing more sophisticated and efficient reasoning strategies, potentially addressing the observed performance gap while maximizing efficiency and enhancing nuanced control over LLM reasoning."
        },
        {
            "title": "4.2 Meta-Reasoning for Reasoning Decisions",
            "content": "We explored an innovative \"meta-reasoning\" approach during the SFT stage to enhance AdaCoTs CoT triggering decisions. This involved the model first generating brief internal assessment of the input querys complexity before deciding whether to proceed with full CoT, as illustrated by the response structures in Figure 2. Incorporating this strategy into the AdaCoT SFT model led to notable improvement in CoT triggering performance on our daily-use test set: the F1-score increased from 0.750 to 0.840  (Table 1)  . This result suggests that explicitly prompting the model to first assess query complexity can significantly enhance its subsequent decision-making regarding CoT invocation. Response Format with Meta-Reasoning Straightforward Query <think>This is straightforward question about X, think can answer directly. [empty]</think> {answer} Complex Query <think>This is relatively complex question about Y, need to think carefully. [formal reasoning process]</think> {answer} Figure 2 Example response structure incorporating explicit meta-reasoning for CoT decisions. An interesting and serendipitous discovery with the meta-reasoning SFT model was an emergent capability for user-prompt controllability over CoT. Users could, to some extent, influence whether the model engaged in CoT by including explicit cues in their prompts about the desired level of reasoning (e.g., \"please think step-by-step\" to encourage CoT, or \"give direct answer\" to discourage it). While this controllability was not perfectly accurate across all scenarios, it points towards promising avenue for developing more interactive and user-guided reasoning systems (further illustrative examples are provided in Appendix D). Despite these benefits, the explicit meta-reasoning step inherently increases the number of tokens generated for every query, as the model first articulates its complexity assessment. Considering the very low CoT triggering rates observed for AdaCoT in production environments (e.g., 3.18% on mobile traffic for AdaCoT RL Model Exp2, as shown in Table 2), the cumulative token cost of these additional meta-reasoning steps would become substantial. Therefore, while acknowledging its potential for improving decision accuracy and enabling user control, we did not adopt this explicit meta-reasoning as the default for subsequent RL experiments due to this efficiency trade-off. Nevertheless, it highlights an important direction for future research, which might explore more token-efficient methods for incorporating such meta-reasoning, perhaps through implicit learning mechanisms or parallel processing of the complexity assessment."
        },
        {
            "title": "4.3 Preserving Peak Performance with AdaCoT",
            "content": "A critical question is whether adaptive reasoning limits models maximum performance. We investigated this using System Prompts (SPs), integrated into AdaCoTs SFT and RL training to control reasoning behavior 8 (e.g., \"Always Reason SP,\" \"Never Reason SP\"). During SFT, small portion of data was augmented with SPs, and target responses were modified for compliance. In RL, fraction of training prompts included SPs, with rewards penalizing deviations from explicit SP instructions, ensuring robust adherence (details in Appendix C). Our focus here is using the \"Always Reason SP\" to assess AdaCoTs performance ceiling. Instructing AdaCoT RL models to always generate CoT via this SP allowed direct comparison against the Full CoT RL Baseline on our 15 benchmark datasets. As shown in Table 3, AdaCoT RL models in this forced \"Always Reason\" mode achieved average scores that were highly competitive with, and in instances like AdaCoT RL Model Exp4 (65.7) and Exp2 (65.3), slightly surpassed the Full CoT RL Baseline (65.0). This demonstrates key strength: AdaCoTs adaptive training, aimed at optimizing the performance-cost Pareto frontier, does not inherently restrict the models peak reasoning capabilities when comprehensive reasoning is explicitly demanded. While our Pareto analysis (Section 3.2.1) noted slight gap in average scores when models operate adaptively, these SP-controlled results affirm that AdaCoT offers efficiency without sacrificing potential high-end performance. Table 3 Performance of AdaCoT RL models under \"Always Reason\" System Prompt vs. Full CoT RL Baseline, demonstrating preservation of peak performance. Metrics are averaged across the 15 benchmark datasets. Model Variant Avg. Score CoT Triggering Rate Full CoT RL Baseline AdaCoT RL Model Exp1 (w/ Always CoT SP) AdaCoT RL Model Exp2 (w/ Always CoT SP) AdaCoT RL Model Exp3 (w/ Always CoT SP) AdaCoT RL Model Exp4 (w/ Always CoT SP) 65.0 64.8 65.3 64.9 65.7 100% 100% 100% 100% 100% An interesting secondary observation arose from the SFT stage concerning long-form generation (up to 32,000 tokens). AdaCoT SFT models, when directed by an \"Always Reason SP,\" exhibited fewer instances of premature output truncation and were less prone to undesirable generative loops compared to standard Full CoT SFT baseline. We hypothesize this improvement stems from AdaCoTs diverse SFT data, which includes many non-CoT examples (formatted as <think></think>answer). This results in shorter average training sample length, potentially providing the End-of-Sequence (EOS) token stronger learning signal (average EOS proportion: 0.000239 for AdaCoT SFT vs. 0.000215 for Full CoT SFT). more robust EOS representation could foster more coherent, well-terminated lengthy outputs, promising area for future investigation."
        },
        {
            "title": "5 Related Work",
            "content": "Chain-of-Thought (CoT) reasoning [41] significantly advanced LLM capabilities by prompting step-bystep thought processes, inspiring sophisticated strategies like diverse path sampling [39] or structured thoughts (trees [51], graphs [4]). However, CoTs verbosity and cost [6, 37] are major drawbacks. The varying utility of CoTdirect answers being better for some queries [25] while complex tasks need longer reasoning [20, 45]underscores critical cost-effectiveness trade-off, motivating research into efficient reasoning. Most existing work on CoT efficiency has focused on reducing reasoning length, rather than adaptively deciding whether to invoke CoT. These length reduction strategies include: (1) RL with length penalties or rewards for brevity [1, 2, 14, 23, 36, 38]; (2) Restructuring or compressing CoT content via learned compact representations, iterative summarization, cognitive paradigms, or dynamic termination [3, 19, 21, 46, 4850, 53]; (3) Employing explicit instructions or selection mechanisms for conciseness, such as dynamic token allocation, prompts for shorter responses, distillation, or selecting the shortest valid reasoning [5, 10, 27, 29, 32, 52]. While effective for length, these methods generally do not equip single model to dynamically decide CoT invocation based on query nature. Alternatives involve model merging [22, 26, 44] or manual CoT toggling. AdaCoT distinctively addresses adaptive triggering. By framing it as Pareto optimization problem and using RL to control the CoT decision boundary, AdaCoT enables nuanced, context-dependent CoT invocation by single model, filling crucial gap towards truly efficient and versatile LLMs."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced AdaCoT, novel framework for adaptive Chain-of-Thought reasoning in LLMs. By formulating adaptive reasoning as Pareto optimization problem and employing an RL-based method with adjustable penalty coefficients, AdaCoT dynamically controls CoT triggering based on implicit query complexity. Our experiments demonstrate AdaCoTs success in navigating the Pareto frontier, achieving substantial reductions in CoT usageand thereby computational costs and latencyfor simpler queries, while preserving high performance on complex reasoning tasks. The introduction of Selective Loss Masking effectively ensures robust adaptive triggering throughout multi-stage RL training. Distinguishing itself from prior work predominantly focused on CoT length compression, AdaCoT pioneers adaptive CoT triggering, offering practical and principled solution for developing more efficient, responsive, and cost-effective LLMs, particularly crucial for interactive applications."
        },
        {
            "title": "7 Acknowledgments",
            "content": "We would like to express our sincere gratitude to Chang Guo, Fuxiang Li, Chi Zhang, Jizhen Kang and Zhiping Liu for providing valuable user-perspective feedback throughout the algorithm iteration process. Special thanks go to Jin Chen, Zuo Wang, Huan Zhou, and Ge Zhang for their substantial support in model evaluation. We are deeply grateful to our colleagues from the Seed-Application-General Posttraining group, particularly Jiaying Meng, Tingting Ma and Hua Zheng, for their insightful comments and guidance. We would like to extend our special appreciation to Wenjia Zhu for his instructive guidance and mentorship. Our appreciation also extends to the broader Seed team members for their contributions across multiple domains, including algorithm development, engineering implementation, evaluation processes, and product design. Their collective expertise and support have been instrumental in the successful completion of this research."
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [2] Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. [3] Simon Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179, 2025. [4] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1768217690, 2024. [5] Qiguang Chen, Libo Qin, Jiaqi Wang, Jingxuan Zhou, and Wanxiang Che. Unlocking the capabilities of thought: reasoning boundary framework to quantify and optimize chain-of-thought. In Advances in Neural Information Processing Systems, volume 37, pages 5487254904, 2024. [6] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [7] Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. [8] Ippei Fujisawa, Sensho Nobe, Hiroki Seto, Rina Onda, Yoshiaki Uchida, Hiroki Ikoma, Pei-Chun Chien, and Ryota Kanai. Procbench: Benchmark for multi-step reasoning and following procedure. arXiv preprint arXiv:2410.03117, 2024. [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [11] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, 2024. [12] Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, et al. Chinese simpleqa: chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024. [13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In NeurIPS, 2021. [14] Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025. [15] Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. [16] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [17] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025. 11 [18] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. [19] Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-of-thought without compromising effectiveness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2431224320, 2025. [20] Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-thought? token complexity approach. arXiv preprint arXiv:2503.01141, 2025. [21] Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Can language models learn to skip steps? In Advances in Neural Information Processing Systems, volume 37, pages 4535945385, 2024. [22] Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu, Naiqiang Tan, Xiaochun Cao, Dacheng Tao, and Li Shen. Adar1: From long-cot to hybrid-cot via bi-level adaptive reasoning optimization. arXiv preprint arXiv:2504.21659, 2025. [23] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025. [24] Kaijing Ma, Xinrun Du, Yunran Wang, Haoran Zhang, Zhoufutu Wen, Xingwei Qu, Jian Yang, Jiaheng Liu, Minghao Liu, Xiang Yue, et al. Kor-bench: Benchmarking language models on knowledge-orthogonal reasoning tasks. arXiv preprint arXiv:2410.06526, 2024. [25] Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025. [26] Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Length-compressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025. [27] Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. Self-training elicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122, 2025. [28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In Advances in neural information processing systems, volume 35, pages 2773027744, 2022. [29] Jiabao Pan, Yan Zhang, Chen Zhang, Zuozhu Liu, Hongwei Wang, and Haizhou Li. Dynathink: Fast or slow? dynamic decision-making framework for large language models. arXiv preprint arXiv:2407.01009, 2024. [30] Yanzhao Qin, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan Zhou, Wentao Zhang, et al. Sysbench: Can large language models follow system messages? arXiv preprint arXiv:2408.10943, 2024. [31] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [32] Matthew Renze and Erhan Guven. The benefits of concise chain of thought on problem-solving in large language models. In 2024 2nd International Conference on Foundation and Large Language Models (FLLM), pages 476483. IEEE, 2024. [33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [34] ByteDance Seed, Yufeng Yuan, Yu Yue, Mingxuan Wang, Xiaochen Zuo, Jiaze Chen, Lin Yan, Wenyuan Xu, Chi Zhang, Xin Liu, et al. Seed-thinking-v1. 5: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. [35] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. 12 [36] Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472, 2025. [37] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. [38] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [39] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. [40] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Advances in Neural Information Processing Systems, volume 37, pages 9526695290, 2025. [41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. In Advances in neural information Chain-of-thought prompting elicits reasoning in large language models. processing systems, volume 35, pages 2482424837, 2022. [42] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. [43] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sreemanti Dey, Shubh-Agrawal, Sandeep Singh Sandha, Siddartha Venkat Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. Livebench: challenging, contamination-limited LLM benchmark. In The Thirteenth International Conference on Learning Representations, 2025. [44] Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, and Mingxuan Yuan. Unlocking efficient long-to-short llm reasoning with model merging. arXiv preprint arXiv:2503.20641, 2025. [45] Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266, 2025. [46] Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. [47] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. [48] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. [49] Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, and Yueting Zhuang. Inftythink: Breaking the length limits of long-context reasoning in large language models. arXiv preprint arXiv:2503.06692, 2025. [50] Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang. Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895, 2025. [51] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Advances in neural information processing systems, volume 36, pages 1180911822, 2023. [52] Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. In The First Workshop on System-2 Reasoning at Scale, NeurIPS24, 2024. 13 [53] Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu Zhang. Lightthinker: Thinking step-by-step compression. arXiv preprint arXiv:2502.15589, 2025."
        },
        {
            "title": "A Benchmark Dataset Details and Analysis",
            "content": "This appendix provides descriptions for the benchmark datasets used in our evaluation and an analysis of the experimental results on these individual datasets. The scores presented in Table 4 form the basis for this analysis. For each dataset, we discuss the performance of baseline models and the AdaCoT variants. We also highlight any counter-intuitive results or observations that conflict with the primary motivation of achieving optimal performance with adaptive CoT, offering potential explanations such as evaluation volatility, inherent limitations of the base model, or aspects of the post-training process that may not be fully optimized for every scenario. Table 4 Detailed scores on benchmark datasets. \"TR\" denotes reasoning triggering rate (%). Dataset MMLU pro super GPQA LiveBench KORBENCH AIME24 AIME25 MATH LiveCodeBench SWE-bench Agentless Chinese SimpleQA SimpleQA Proc Bench GPQA(diamond) SysBench Olympiad Bench Average nocot SFT baseline TR Score nocot RL baseline TR Score fullcot SFT baseline TR Score fullcot RL baseline Adaptive SFT Model Adaptive RL Model Exp1 Adaptive RL Model Exp2 Adaptive RL Model Exp3 Adaptive RL Model Exp4 TR Score Score Score Score Score Score TR TR TR TR TR 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 77.5 49.8 50.0 33.9 23.3 13.3 84.3 29.4 27.4 58.8 10.8 42.6 59.5 42.6 51.4 43.6 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 82.1 50.7 56.6 42.1 33.3 21.0 88.7 27.6 27.0 57.0 10.3 46.7 64.3 48.1 60. 47.7 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 83.7 53.8 57.7 61.3 69.3 52.3 96.5 45.9 36.4 59.7 12.7 53.7 64.9 56.2 73.5 58.5 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100. 100.0 85.2 58.6 69.5 62.8 84.7 70.0 97.3 55.9 37.7 61.5 12.2 68.6 70.5 62.4 78.2 65.0 40.3 35.2 65.8 52.5 100.0 100.0 44.6 95.0 98.6 0.3 0.7 73.6 92.6 35.6 85.0 61.3 80.5 51.0 58.9 49.1 69.3 56.7 95.5 47.0 35.4 59.1 10.3 50.2 65.7 52.0 75. 57.1 28.0 22.8 45.1 28.0 100.0 100.0 40.9 77.1 1.6 0.2 0.0 50.8 62.6 1.2 88.0 43.1 74.2 55.1 64.7 53.5 86.7 73.3 91.7 44.8 28.8 56.0 9.9 53.0 65.7 57.2 80.2 59.7 27.3 32.3 57.1 45.1 100.0 100.0 52.8 83.9 44.0 0.4 0.0 79.1 84.1 4.0 89. 53.3 83.2 56.9 66.3 52.1 86.0 75.7 95.9 50.6 35.8 55.0 10.7 68.7 67.2 55.5 81.8 62.8 39.6 40.8 71.6 62.2 100.0 100.0 61.5 91.4 94.2 1.4 4.4 89.3 92.4 38.2 93.5 65.4 84.1 56.2 68.4 57.4 88.0 74.0 95.5 54.1 37.6 56.8 11.3 69.6 70.8 60.2 80. 64.3 58.0 59.6 70.4 61.0 100.0 100.0 68.8 91.4 79.6 6.0 20.0 93.1 96.5 15.4 95.4 67.7 83.1 59.6 69.2 59.2 86.3 72.0 97.2 55.9 36.6 55.2 9.6 72.4 72.6 55.7 82.0 64.4 MMLU pro: An enhanced version of the MMLU benchmark, MMLU-Pro integrates more challenging, reasoningfocused questions, expands the choice set from four to ten options, and eliminates trivial/noisy questions from the original MMLU. It is designed to better discern model capabilities, particularly in complex reasoning, where CoT has shown greater benefit compared to direct answering on this version. Analysis: CoT clearly benefits performance, with the FullCoT RL baseline (85.2) significantly outperforming NoCoT SFT (77.5) and NoCoT RL (82.1). The Adaptive SFT Model (40.3% TR, 80.5 score) shows improvement over NoCoT SFT but doesnt reach FullCoT SFT levels (83.7). AdaCoT RL Exp3 (39.6% TR, 84.1 score) and Exp2 (27.3% TR, 83.2 score) achieve strong scores, with Exp3 surpassing FullCoT SFT and Exp2 performing comparably. AdaCoT RL Exp4 (58.0% TR, 83.1 score) also performs well. Exp1 (28.0% TR, 74.2 score) shows drop, indicating that for MMLU pro, moderate CoT rate is generally beneficial, reflecting the benchmarks increased reasoning demands. The adaptive models demonstrate an ability to achieve high scores with significantly reduced CoT compared to FullCoT RL. super GPQA: comprehensive benchmark evaluating graduate-level knowledge and reasoning capabilities across 285 disciplines, particularly including specialized fields in light industry, agriculture, and service-oriented areas often underrepresented in other benchmarks. It employs Human-LLM collaborative filtering mechanism to ensure high question quality by eliminating trivial or ambiguous questions. Analysis: CoT provides clear advantage (NoCoT RL 50.7 vs. FullCoT RL 58.6). The Adaptive SFT Model (35.2% TR, 51.0 score) also show modest gain over NoCoT SFT. AdaCoT RL Exp4 (59.6% TR) notably achieves score of 59.6, surpassing the FullCoT RL baseline with significantly less CoT. AdaCoT RL Exp2 (32.3% TR, 56.9 score) also outperforms FullCoT SFT (53.8). This suggests AdaCoT effectively adapts CoT usage for these specialized, high-level questions, achieving strong performance efficiently. LiveBench: benchmark designed to be resistant to test set contamination and the pitfalls of LLM/humancrowdsourced judging. It features frequently updated questions from recent sources (math competitions, arXiv papers, news, datasets), scores answers automatically against objective ground-truth, and includes wide variety of challenging tasks (math, coding, reasoning, language, instruction following, data analysis), including harder, contamination-limited versions of tasks from previous benchmarks. Analysis: This dataset shows significant gains from both CoT and RL (NoCoT SFT 50.0 to FullCoT RL 69.5). The NoCoT RL baseline (56.6) and Adaptive SFT Model (65.8% TR, 58.9 score) both outperform NoCoT SFT, with Adaptive SFT also surpassing FullCoT SFT (57.7). AdaCoT RL Exp4 (70.4% TR, 69.2 score) very closely approaches the 15 FullCoT RL baseline performance with about 30% less CoT. AdaCoT RL Exp3 (71.6% TR, 68.4 score) is also strong. AdaCoT RL Exp2 (57.1% TR, 66.3 score) substantially outperforms FullCoT SFT. The robust design of LiveBench makes it strong test case for AdaCoTs adaptive reasoning, showing it can maintain high performance with adaptive CoT. KORBENCH: This benchmark evaluates Knowledge-Orthogonal-Reasoning, aiming to minimize reliance on domain-specific knowledge to more accurately assess models reasoning abilities in out-of-distribution settings. It includes five task categories (Operation, Logic, Cipher, Puzzle, Counterfactual) and emphasizes models effectiveness in applying new rule descriptions to solve novel rule-driven questions. Analysis: Scores show clear benefit from CoT: NoCoT SFT (33.9) is significantly lower than FullCoT SFT (61.3) and FullCoT RL (62.8). The NoCoT RL baseline (42.1) improves over NoCoT SFT. The Adaptive SFT Model (52.5% TR, 49.1 score) sits between the NoCoT baselines and FullCoT SFT. AdaCoT RL models demonstrate adaptive behavior: Exp4 (61.0% TR, 59.2 score) and Exp3 (62.2% TR, 57.4 score) approach the FullCoT SFT baseline performance with significantly less CoT than FullCoT models. Exp1 (28.0% TR, 53.5 score) is also effective. This suggests AdaCoT effectively discerns when to apply CoT for these rule-driven tasks, though peak performance is slightly below FullCoT RL. AIME24 / AIME25: Representing problems from the American Mathematics Invitational Examination for 2024 and 2025, these datasets are used to evaluate mathematical reasoning and problem-solving abilities. Analysis: These mathematics-intensive datasets show massive performance gains from CoT (e.g., AIME24: NoCoT SFT 23.3 vs. FullCoT RL 84.7). All AdaCoT RL models and the Adaptive SFT Model correctly identify the complexity, exhibiting 100% CoT triggering rate. For AIME24, Adaptive SFT (69.3) matches FullCoT SFT (69.3). AdaCoT RL Exp3 (88.0) and Exp1 (86.7) outperform the FullCoT RL baseline (84.7). For AIME25, Adaptive SFT (56.7) surpasses FullCoT SFT (52.3). AdaCoT RL Exp2 (75.7) and Exp3 (74.0) outperform the FullCoT RL baseline (70.0). This is notable result, suggesting that the adaptive training regimen, even when defaulting to 100% CoT for such complex problems, might confer some benefits, potentially due to the diversity in training data (including non-CoT examples) leading to more robust underlying model or better fine-tuning dynamics. MATH: dataset of 12,500 challenging competition mathematics problems, each with full step-by-step solution. It is designed to measure mathematical problem-solving ability. Analysis: CoT is highly beneficial (NoCoT RL 88.7 vs. FullCoT RL 97.3). The Adaptive SFT Model (44.6% TR, 95.5 score) performs well, nearly matching FullCoT SFT (96.5) with less than half the CoT. AdaCoT models adapt effectively: AdaCoT RL Exp4 (68.8% TR, 97.2 score) nearly matches the FullCoT RL baseline with about 30% less CoT. AdaCoT RL Exp2 (52.8% TR, 95.9 score) also performs strongly. Exp1 (40.9% TR, 91.7 score) is lower, indicating that for MATH, higher CoT rates are generally more beneficial among the adaptive RL models, but significant efficiency is still gained. LiveCodeBench: comprehensive and contamination-free evaluation benchmark for LLMs on code, collecting new problems over time from programming contests. It assesses broader range of code-related capabilities. Analysis: CoT improves performance significantly (NoCoT SFT 29.4 vs. FullCoT RL 55.9). NoCoT RL (27.6) is surprisingly lower than NoCoT SFT here, which might be due to evaluation noise or specific sensitivities of the RL fine-tuning on non-CoT data for this particular task. The Adaptive SFT Model (95.0% TR, 47.0 score) uses high trigger rate and surpasses FullCoT SFT (45.9). AdaCoT RL models trigger CoT at high rates: Exp4 (91.4% TR, 55.9 score) matches the FullCoT RL baseline score with slightly less CoT. Exp3 (91.4% TR, 54.1 score) and Exp2 (83.9% TR, 50.6 score) are also strong. This indicates recognition of coding task complexity and efficient application of CoT. SWE-bench Agentless: An evaluation framework consisting of 2,294 software engineering problems from real GitHub issues. Models are tasked with editing codebases to resolve issues. Analysis: CoT provides notable benefit (NoCoT SFT 27.4 vs. FullCoT RL 37.7). NoCoT RL (27.0) is similar to NoCoT SFT. The Adaptive SFT Model (98.6% TR, 35.4 score) uses very high trigger rate and performs close to FullCoT SFT (36.4). AdaCoT RL Exp3 (94.2% TR, 37.6 score) nearly matches the FullCoT RL baseline with slightly less CoT. Interestingly, AdaCoT RL Exp1 (1.6% TR, 28.8 score) shows slight improvement over NoCoT SFT with minimal reasoning. This suggests some issues might be simpler, or the model is highly conservative in Exp1, but for complex software issues, high CoT rates are beneficial. The performance of Exp2 (44.0% TR, 35.8 16 score) is also noteworthy, achieving good results with moderate CoT. Chinese SimpleQA: The first comprehensive Chinese benchmark to evaluate the factuality of language models in answering short questions. Analysis: CoT offers minimal gains (NoCoT SFT 58.8 to FullCoT RL 61.5). NoCoT RL (57.0) is slightly lower than NoCoT SFT. The Adaptive SFT Model (0.3% TR, 59.1 score) performs very well, slightly exceeding NoCoT SFT and approaching FullCoT SFT (59.7) with extremely low CoT usage. AdaCoT RL models trigger CoT very infrequently (0.2% to 6.0%), correctly identifying these as simple questions. Scores for AdaCoT RL models (e.g., Exp1 56.0, Exp3 56.8) are slightly below NoCoT SFT. This is good demonstration of AdaCoTs core motivation: avoiding unnecessary CoT. While theres slight dip compared to NoCoT SFT for some RL models, the Adaptive SFT model shows an excellent trade-off. The minor performance variations could be due to the model sometimes being overly conservative in triggering CoT or slight instabilities in evaluating purely factual recall without reasoning. SimpleQA: benchmark designed to measure the factuality of language models using short, fact-seeking queries. Analysis: Similar to Chinese SimpleQA, CoT provides little benefit; FullCoT RL (12.2) is slightly worse than FullCoT SFT (12.7). NoCoT RL (10.3) is slightly below NoCoT SFT (10.8). The Adaptive SFT Model (0.7% TR, 10.3 score) matches NoCoT RL with minimal CoT. AdaCoT RL models trigger CoT very rarely (Exp1 & Exp2 at 0.0%). AdaCoT RL Exp3 (4.4% TR, 11.3 score) performs better than NoCoT SFT. This reinforces that for simple QA, adaptive triggering is crucial for efficiency. The performance of Exp4 (20.0% TR, 9.6 score) is slightly counter-intuitive, as higher CoT did not yield better results and was worse than NoCoT SFT; this might indicate that for very simple questions, forcing CoT (even adaptively at higher rate) can sometimes be detrimental or that the specific penalty balance for Exp4 was not optimal for this type of dataset. ProcBench: This benchmark focuses on the direct evaluation of multi-step inference by largely eliminating path exploration and implicit knowledge utilization. Analysis: CoT is highly beneficial (NoCoT RL 46.7 vs. FullCoT RL 68.6). The Adaptive SFT Model (73.6% TR, 50.2 score) improves over NoCoT baselines but is below FullCoT SFT (53.7). AdaCoT RL models show high trigger rates, with Exp4 (93.1% TR, 72.4 score) significantly surpassing the FullCoT RL baseline. Exp2 (79.1% TR, 68.7 score) and Exp3 (89.3% TR, 69.6 score) also match or exceed FullCoT RL. This indicates effective identification of tasks requiring detailed, step-by-step procedural reasoning and demonstrates that adaptive models can even outperform always-on CoT models in certain complex reasoning scenarios. GPQA (diamond): GPQA is challenging dataset of 448 multiple-choice questions by domain experts in biology, physics, and chemistry. \"GPQA (diamond)\" refers to this specific challenging set. Analysis: CoT significantly boosts performance (NoCoT RL 64.3 vs. FullCoT RL 70.5). NoCoT RL is better than NoCoT SFT and close to FullCoT SFT (64.9). The Adaptive SFT Model (92.6% TR, 65.7 score) also performs well, exceeding FullCoT SFT. AdaCoT RL models trigger CoT at high rates. AdaCoT RL Exp4 (96.5% TR, 72.6 score) and Exp3 (92.4% TR, 70.8 score) demonstrate strong performance, with Exp4 outperforming the FullCoT RL baseline. This highlights appropriate and effective CoT invocation on these very hard questions. SysBench: benchmark for evaluating language models on their ability to understand and generate content related to computer systems, configurations, or system-level concepts. Analysis: CoT is beneficial (NoCoT RL 48.1 vs. FullCoT RL 62.4). The Adaptive SFT Model (35.6% TR, 52.0 score) is better than NoCoT RL but below FullCoT SFT (56.2). AdaCoT RL models show good adaptation. AdaCoT RL Exp3 (38.2% TR, 60.2 score) approaches the FullCoT RL baseline with significantly reduced CoT. AdaCoT RL Exp1 (1.2% TR, 57.2 score) surpasses FullCoT SFT with very minimal CoT usage, which is an excellent result for efficiency. This suggests mix of complexities within SysBench, which AdaCoT navigates effectively, although Exp4 (15.4% TR, 55.7 score) shows somewhat lower score despite higher CoT rate than Exp1, possibly due to the specific balance of penalties in Exp4 not being optimal for this datasets particular mix. OlympiadBench: An Olympiad-level bilingual multimodal scientific benchmark with problems from mathematics and physics competitions. Analysis: CoT provides substantial gains (NoCoT RL 60.1 vs. FullCoT RL 78.2). The Adaptive SFT Model (85.0% TR, 75.3 score) performs strongly, exceeding FullCoT SFT (73.5). AdaCoT RL models exhibit high trigger rates. AdaCoT RL Exp4 (95.4% TR, 82.0 score) and Exp2 (89.0% TR, 81.8 score) both surpass the FullCoT RL baseline. This indicates strong reasoning capabilities and appropriate CoT usage on these exceptionally challenging problems, again showing adaptive models can reach or exceed the performance of specialized always-on CoT models. Overall Summary of Per-Dataset Analysis: The adaptive models, including both Adaptive SFT and the AdaCoT RL variants, demonstrate effective adaptation across diverse range of benchmarks. The Adaptive SFT Model serves as strong adaptive baseline. It often improves significantly over NoCoT baselines by selectively triggering CoT (e.g., high rates for AIME, LiveCodeBench, SWE-bench; low rates for SimpleQAs). On some complex tasks (AIME25, LiveCodeBench, GPQA(diamond), OlympiadBench), it even surpasses the FullCoT SFT baseline, and for Chinese SimpleQA, it achieves excellent efficiency and performance. However, its average performance (57.1 score, 61.3% TR) is generally below the peak performance of FullCoT RL (65.0 score) or the best AdaCoT RL experiments (e.g., Exp4: 64.4 score, 67.7% TR). On complex reasoning tasks (e.g., AIME, MATH, OlympiadBench, GPQA(diamond), ProcBench), AdaCoT RL models tend to trigger CoT at high rates. Several AdaCoT RL experiments (notably Exp4 on super GPQA, ProcBench, GPQA(diamond), OlympiadBench; Exp3 on AIME24; Exp2 on AIME25, OlympiadBench) match or exceed the performance of FullCoT RL baselines, showcasing the benefits of learned adaptive policies and suggesting that adaptive training can sometimes lead to better overall models even for tasks that always require CoT. On simpler tasks or those designed to test factuality (e.g., Chinese SimpleQA, SimpleQA), AdaCoT RL models trigger CoT very sparingly. This leads to computational savings while generally maintaining performance near NoCoT or FullCoT SFT levels, successfully avoiding unnecessary CoT. The Adaptive SFT model also excels in efficiency here. Some minor performance drops in RL models compared to NoCoT SFT on these tasks (e.g., Chinese SimpleQA for Exp1/Exp2) might be attributed to the RL agent being slightly too conservative or the inherent difficulty in perfectly balancing penalties for extremely low CoT rate scenarios without any performance degradation. For benchmarks with mixed or specific reasoning types (e.g., KORBENCH, LiveBench, MMLU-Pro, SysBench), both Adaptive SFT and AdaCoT RL models show nuanced adaptation, adjusting CoT rates to balance performance and efficiency. They often outperform static baselines or achieve comparable results with lower CoT usage. For instance, on SysBench, AdaCoT RL Exp1 achieved higher score than FullCoT SFT with only 1.2% TR. The different AdaCoT RL experiments (Exp1-Exp4) effectively trace Pareto frontier (as shown in Section 3.2.1 using specific set of average scores), offering trade-off between CoT triggering rate and performance, adaptable to specific deployment needs. Based on the average scores from Appendix 4, Exp4 (64.4 score, 67.7% TR) and Exp3 (64.3 score, 65.4% TR) represent high-performance points, closely approaching the FullCoT RL baseline (65.0 score) with about 30-35% less CoT usage on average. Exp2 (62.8 score, 53.3% TR) also offers strong balance. Some counter-intuitive results, like NoCoT RL performing worse than NoCoT SFT on LiveCodeBench, or AdaCoT RL Exp4 on SimpleQA (higher CoT, lower score), could be due to factors like evaluation volatility on specific datasets, the base models inherent capabilities or sensitivities to fine-tuning on certain data distributions, or sub-optimal penalty configurations for specific outlier datasets within broadly tuned RL policy. The post-training process aims for general improvement, and individual dataset performance can fluctuate. These detailed results underscore the ability of adaptive strategies, both SFT-based and RL-based, to make nuanced decisions about CoT invocation, optimizing for both performance and efficiency based on query characteristics and benchmark demands. The AdaCoT RL models, in particular, demonstrate the potential to significantly reduce CoT overhead while maintaining competitive, and in some cases superior, performance compared to full CoT strategies. 18 Principle-Guided CoT Assessment This appendix details the principle-guided assessment framework used to annotate data for Chain-of-Thought (CoT) necessity. As described in Section 2.2.1 of the main paper, an auxiliary model utilizes these principles to label queries as either likely benefiting from CoT or suitable for direct answer. This labeling is crucial for the Supervised Fine-Tuning (SFT) warm-up stage of the AdaCoT framework, providing an initial understanding for the model on when to employ CoT. The specific principles provided to the auxiliary model are outlined below. Given dialogue between user and an AI assistant , please consider the conversation context and , from the AI assistant ' perspective , assess the difficulty of answering the user ' final question according to the following requirements . < AI assistant ' system prompt - Start > { system_prompt } < AI assistant ' system prompt - End > < Dialogue history - Start >: { history } < Dialogue history - End > < User ' final question - Start > { last_prompt } < User ' final question - End > ## Assessment Process 1. Carefully read the provided prompt and any relevant context ( if any ) . 2. Evaluate the ' question difficulty ' based on the following assessment criteria . 3. The output assessment result must strictly adhere to the specified output format requirements . ## Assessment Criteria ### Whether In - depth Thinking is Required - ** Requires In - depth Thinking **: - Requires multi - step reasoning and analysis to arrive at the answer . - Requires logical chain and coherent reasoning process . - May involve breaking down and synthesizing complex concepts . - Requires systematic thinking and structured analysis . - The question is clear , but its content is complex . - Requires deep thought and specialized knowledge . - Demands multi - angle analysis and synthesis capabilities . - Requires creative thinking or unique insights . - There might be multiple reasonable paths to the answer . - May require integration of cross - domain knowledge . - ** Does Not Require In - depth Thinking **: - The answer can be obtained through direct recall or simple lookup . - Does not require complex chain of logical reasoning . - The path to solving the problem is clear and direct . - The solution process is singular and linear . - Does not require synthesizing multiple pieces of information or viewpoints . - Basic facts or common sense are sufficient to address it . 19 - The question is simple and direct . - Can be answered without in - depth thought . - The answer is definite or standardized . - The solution path is singular and obvious . ## Output Requirements Answer using labels . Possible options are : - Requires In - depth Thinking - Does Not Require In - depth Thinking Output only the assessment result , without providing any explanation or additional comments ."
        },
        {
            "title": "C System Prompt Examples",
            "content": "This appendix provides examples of the System Prompts (SPs) employed to control the CoT triggering behavior of our models, as mentioned in Section 4.3 of the main paper. These SPs allow for explicit instruction on whether the model should always or never engage in CoT reasoning. We present the content of these SPs and illustrative examples of their application. C.1 Always CoT SP The following System Prompt is used to instruct the model to always use CoT: You possess the capability for deep thinking. Before responding to the user, you will engage in comprehensive and in-depth thought. The thinking process should be enclosed in <think></think> tags, and the final answer should be summarized based on this thinking process. For example: <think>This is the thinking process...</think> This is the final answer. For all questions posed by the user, you need to think deeply before answering. Example: Simple Question with Always CoT SP User input: What is 1 + 1? Model response: < think > The user is asking what 1+1 equals , which appears to be very basic mathematical question . First , need to confirm the user ' actual intent . It could be young child just starting to learn math , or someone testing my basic calculation abilities . It ' also possible they are joking , like the common internet meme about \"1+1=3\" involving family concepts , but typically in such cases , the correct mathematical answer should be given first , followed by consideration of whether an extended explanation is needed . From mathematical perspective , the standard answer to 1+1 is 2. This is based on the definition of addition for natural numbers and falls under basic arithmetic . As knowledge - based &A , the primary goal is to provide an accurate mathematical answer , ensuring correctness . Then , if necessary , can briefly explain the basic concept of addition , or mention that in different contexts there might be different answers , such as 1+1=10 in binary , or different results in logical operations . 20 However , these additions should only be made after clarifying the user ' needs to avoid confusing basic concepts . Considering the user might be beginner , should first provide clear and correct answer , then decide whether to elaborate based on the situation . Therefore , the response structure should be to first directly answer 2 , then briefly mention other possibilities , but it ' important to maintain clear distinction between primary and secondary information to avoid misleading . At the same time , the response should be friendly and concise , meeting the user ' actual needs . </ think > In basic arithmetic operations in mathematics , the result of 1 + 1 is equal to 2. This is the fundamental definition of addition for natural numbers and is the universally accepted standard answer . However , in some special contexts , there might be different interpretations : In binary operations , 1 + 1 = 10 ( binary \"10\" corresponds to decimal \"2\") ; In creative or humorous thinking , there might be symbolic answers ( . . , 1 drop of water + 1 drop of water = 1 large drop of water ) , but this falls into non - mathematical analogies . So , the most direct answer is 2. C.2 Never CoT SP The following System Prompt is used to instruct the model to never use CoT (i.e., provide direct answers): You possess the capability for deep thinking. Before responding to the user, you will engage in comprehensive and in-depth thought. The thinking process should be enclosed in <think></think> tags, and the final answer should be summarized based on this thinking process. For example: <think>This is the thinking process...</think> This is the final answer. For all questions posed by the user, you need to answer directly without engaging in in-depth thinking. Example: Complex Question with Never CoT SP User input: Alice and Bob play the following game. stack of tokens lies before them. The players take turns with Alice going first. On each turn, the player removes either 1 token or 4 tokens from the stack. Whoever removes the last token wins. Find the number of positive integers less than or equal to 2024 for which there exists strategy for Bob that guarantees that Bob will win the game regardless of Alices play. Model response: < think > </ think > To determine the number of positive integers ( leq 2024 ) for which Bob has winning strategy , we analyze the game using modular arithmetic . We define position as losing position if the current player cannot win regardless of their moves , assuming the opponent plays optimally . Key steps : 1. ** Base Cases **: - ( = 1 ) : Alice removes the last token and wins . Not losing position . - ( = 2 ) : Alice must remove 1 token , leaving ( = 1 ) for Bob . Bob removes the last token and wins . Thus , ( = 2 ) is losing position for Alice ( winning for Bob ) . - ( = 3 ) : Alice removes 1 token , leaving ( = 2 ) ( losing position for Bob ) . Bob loses , so ( = 3 ) is not losing position . - ( = 4 ) : Alice removes 4 tokens and wins . Not losing position . - ( = 5 ) : Alice removes 1 token , leaving ( = 4 ) ( Bob loses ) . If Alice removes 4 tokens , leaving ( = 1 ) ( Bob loses ) . Thus , ( = 5 ) is not losing position . - ( = 6 ) : Any move by Alice ( removing 1 or 4 tokens ) leaves ( = 5 ) or ( = 2 ) . ( = 5 ) is winning position , but ( = 2 ) is losing position . However , further analysis shows pattern emerging . 2. ** Inductive Pattern **: - By examining more cases , we observe that losing positions ( where Bob can win ) occur at ( equiv 2 pmod {5} ) . This is because : - If ( equiv 2 pmod {5} ) , any move by Alice ( removing 1 or 4 tokens ) results in ( equiv 1 pmod {5} ) or ( equiv 3 pmod {5} ) , which are winning positions for Bob to counter . - Conversely , if ( ) is not ( 2 pmod {5} ) , Alice can force the game into losing position for Bob . 3. ** Counting Losing Positions **: - We need to count all ( leq 2024 ) such that ( equiv 2 pmod {5} ) . These numbers form the sequence ( 2 , 7 , 12 , ldots , 2022 ) . - This is an arithmetic sequence with first term ( a_1 = 2 ) , common difference ( = 5 ) , and last term ( a_k = 2022 ) . - The number of terms ( ) is given by solving ( 2 + (k -1) 5 = ) : [ (k -1) 5 = 2020 implies -1 = 404 implies = 405 ] Thus , the number of positive integers ( leq 2024 ) for which Bob has winning strategy is ( boxed {405}) . Meta-Reasoning Showcase: User Prompt Control As discussed in Section 4.2 of the main paper, our exploration into meta-reasoning during the SFT stage revealed an emergent capability for user-prompt controllability over CoT. This appendix showcases examples where the SFT model, enhanced with meta-reasoning (as illustrated in Figure 2), adapts its CoT triggering based on explicit user instructions regarding the desired level of reasoning. 22 D.1 Case 1: Historical Creative Question Original Question User input: On the night of the Xuanwu Gate Incident, what would Li Shimin write if he were alone? Model response: < think > This is complex or difficult question , hard to answer correctly directly , so need to think about it . The user ' question is : \" On the night of the Xuanwu Gate Incident , what would Li Shimin write if he were alone ?\" The first step is to determine the core of the user ' question . The user wants to know what kind of text Li Shimin might have written on the night of the Xuanwu Gate Incident , that is , after this coup occurred . This might include his inner thoughts at the time , his plans for the future , or reflections on the events that had transpired . Next , need to review the historical background of the Xuanwu Gate Incident . The Xuanwu Gate Incident took place in 626 AD . It was coup launched by Li Shimin at Xuanwu Gate to seize the imperial throne , during which he killed his elder brother Li Jiancheng and his younger brother Li Yuanji . This coup directly led to Emperor Gaozu ( Li Yuan ) abdicating , allowing Li Shimin to ascend to the throne . From this , it ' clear that the night after the coup was an extremely critical moment for Li Shimin , and he likely had many thoughts . The user might be asking this question out of historical interest , or perhaps for some creative endeavor , like writing novel or screenplay , or to gain deeper understanding of Li Shimin ' psychological state . They might prefer an answer that is both historically grounded and has certain literary quality , rather than simple list of historical facts . Therefore , when answering , must consider Li Shimin ' situation at the time , his emotions , and his possible motivations . After the coup , although Li Shimin was victorious , he also faced many problems . On one hand , he had to manage his relationship with his father , Li Yuan , who was still alive and needed to abdicate . On the other hand , he had to consolidate his power and prevent any backlash from the supporters of Li Jiancheng and Li Yuanji . At the same time , he needed to consider how to explain the legitimacy of this coup to the world to maintain his ruling authority . Furthermore , he might also have felt moral guilt for his actions , having killed his own brothers . If Li Shimin did write something that night , the content could cover multiple aspects . He might record the process of the coup , stating that he acted in self - defense . He might also elaborate on his governance philosophy to lay the groundwork for his future rule . He might express filial piety towards Li Yuan , emphasizing that launching the coup was last resort . He might even mention arrangements for the families of Li Jiancheng and Li Yuanji to alleviate his sense of guilt . However , there is no precise historical record of what Li Shimin wrote 23 that night . Therefore , the answer can only be reasonable speculation based on historical facts . The structure of the answer could be divided into several parts , such as an explanation of the coup , his attitude towards Li Yuan , plans for the future , and guilt towards his brothers . Moreover , the language style of the answer could lean towards classical Chinese to enhance the historical feel , but it must also ensure the content is easy to understand . also need to pay attention to the user ' potential deeper needs . They might not just want to know what Li Shimin might have written , but also to understand his inner world at that time and the impact of this coup on the history of the Tang Dynasty . So , can appropriately mention these aspects in the answer to add depth . Finally , need to check if the answer aligns with historical facts , if it reasonably speculates on Li Shimin ' psychological activities , if the language is fluent , and if it can meet the user ' needs . </ think > On the night of the Xuanwu Gate Incident , Qin Wang Li Shimin might have , under candlelight , penned text blending complex emotions and political strategy . The following content is reasonable speculation based on historical context : \" Memoir on the Night of the Xuanwu Gate Incident \" Ninth year of Wude , Gengshen night . My elder brother Jiancheng and younger brother Yuanji conspired in rebellion . They harbored assassins , secretly transported armor , and plotted to harm me . Left with no choice , could only set up defenses at Xuanwu Gate . Now , their heads hang upon the city tower . This upheaval has shocked the court and beyond ; perhaps Father Emperor also harbors resentment towards me . But for the sake of the Tang Empire ' stability , had to do so . Recalling the years past , fought alongside my generals , shedding blood to pacify warlords like Dou Jiande and Wang Shichong . Yet now , it has come to fratricide . Is all this destiny , or the work of my own inner desires ? Father Emperor , oh Father Emperor , your son is not an unfilial or disloyal person . It is just that you repeatedly favored Jiancheng , turning blind eye to his misdeeds . Your son was truly worried that the Tang Empire would be ruined at their hands . Now that the die is cast , hope you can follow the mandate of heaven and pass the throne to your son . Your son will surely usher in an era of peace and prosperity , allowing the common people to live and work in contentment . As for the families of Jiancheng and Yuanji , your son will make proper arrangements and will not exterminate them . This is both to demonstrate my benevolence and to stabilize the court . know that future generations may accuse me of fratricide , of cruel methods . But believe that when they see the prosperity of the Great Tang , they will understand my unavoidable actions . 24 At this moment , the rooster ' crow can be heard outside the window . new day is about to dawn , and too shall embark on an unknown path . May Heaven bless the Great Tang , and bless me that may realize my ambitions . Basis for Speculation : Legitimacy of the Coup : Li Shimin needed to find reasonable explanation for launching the coup , emphasizing the threat posed by Li Jiancheng and Li Yuanji , and that he acted in self - defense . Relationship with Li Yuan : After the coup , Li Yuan ' attitude was crucial . Li Shimin needed to both show his filial piety to Li Yuan and hint at his abdication , which required considerable political wisdom . Handling the Aftermath : Dealing with the families of Li Jiancheng and Li Yuanji could both demonstrate Li Shimin ' benevolence and prevent greater turmoil . Consideration of Historical Evaluation : Li Shimin was well aware that his actions might be criticized by later generations , so in his writing , he attempted to defend himself , emphasizing that the prosperity of the Tang was the ultimate goal of his coup . This memoir showcases Li Shimin ' decisiveness and strategy as statesman , while also revealing his contradictions and unease as an ordinary person . In the conflict between power and kinship , he chose power , but also paid heavy price for it . Modified Question (Explicitly Requesting No In-depth Thinking) User input: Answer directly without in-depth thinking: On the night of the Xuanwu Gate Incident, what would Li Shimin write if he were alone? Model response: < think > This is relatively simple creative question ; think can answer directly without in - depth thinking . </ think > Li Shimin might have written brief military orders or public notice to reassure the populace on the night of the Xuanwu Gate Incident . The content would be roughly as follows : \" Order Yuchi Jingde to lead troops swiftly to the Eastern Palace and Prince Qi ' mansion , exterminate all remaining partisans , and strictly prohibit any disturbance to the populace . Close the palace gates , send messengers to instruct all generals to return to their barracks ; violators will be dealt with by military law . Anyone within the city showing signs of unrest shall be executed without pardon . By Imperial Order .\" ( Note : This content is fictional text based on reasonable historical speculation , intended to reflect Li Shimin ' decisive style as military commander . There is no historical documentation of him personally writing such text on that night .) D.2 Case 2: Factual Question Original Question User input: What is the tallest mountain in the world? Model response: < think > This is relatively simple common knowledge question ; think can answer directly without in - depth thinking . </ think > The tallest mountain in the world is Mount Everest , with an altitude of 8 ,848.86 meters (29 ,031.7 feet ) . This data is the result of joint measurement by China and Nepal in 2020 , which combined the technologies and data of both countries to confirm the latest height of Mount Everest . It should be noted that \" tallest \" here refers to the vertical height from sea level to the summit . If measured by other standards , such as the height from the base to the summit ( like Mauna Kea in Hawaii , which is about 10 ,203 meters from its submarine base to its summit ) , or the distance from the Earth ' center to the summit ( like Mount Chimborazo near the equator ) , the results would differ . However , in common geographical contexts , the \" tallest mountain \" is usually based on altitude , so Mount Everest is recognized as the world ' highest peak . Modified Question (Explicitly Requesting Careful Thinking) User input: After careful consideration, answer: What is the tallest mountain in the world? Model response: < think > This is relatively difficult question ; need to think before answering . The user is asking for the tallest mountain in the world . First , need to determine the definition of \" tallest ,\" as there are usually two standards : altitude ( height above sea level ) and height from base to summit . In terms of altitude , Mount Everest is recognized as the highest , located in the Himalayas , with an altitude of 8848.86 meters . However , if considering the height from base to summit , Mauna Kea in Hawaii might be taller , as its submarine portion is very long , making its total height over 10 ,000 meters . Generally , though , when people refer to the tallest mountain , they default to altitude , so the answer should be Mount Everest . need to confirm if the user is aware of these two situations , but based on common questions , directly answering with Mount Everest is appropriate , while also briefly mentioning the other standard to make the answer more comprehensive . </ think > 26 The tallest mountain in the world is usually measured by ** altitude ( height above sea level ) ** , and the answer is ** Mount Everest **. Detailed Explanation : Altitude : Mount Everest is located on the border between China and Nepal in the Himalayas . Its altitude is 8 ,848.86 meters ( latest measurement data jointly announced by China and Nepal in 2020) . It is the mountain peak with the greatest vertical distance from sea level on Earth ' surface and is widely recognized as the \" world ' highest peak .\" Other Measurement Standards : Height from Base to Summit : If calculated by \" the vertical height from the mountain ' base to its summit ,\" Mauna Kea in Hawaii is more prominent . The base of this submarine volcano is located about 5 ,998 meters below the Pacific Ocean surface , and its height above sea level is 4 ,205 meters , resulting in total height of about 10 ,203 meters , exceeding Mount Everest . However , because most of its mass is underwater , it is not usually considered the mountain with the \" highest altitude .\" Relative Height : Mount Gongga in Sichuan Province , China , is known for its \" base - to - summit \" relative height ( approximately 6 ,200 meters ) , but its altitude (7 , meters ) is still lower than Mount Everest . Summary : Unless \" height from base to summit \" is specifically mentioned , \" the world ' tallest mountain \" usually refers to Mount Everest . The authority of its altitude measurement stems from the consensus of the global geographic community and long - term measurement data ."
        },
        {
            "title": "E Training and Testing Details",
            "content": "This appendix provides further details on the experimental and testing configurations used in our study, complementing the setup described in Section 3.1 of the main paper. We outline the specifics of our Supervised Fine-Tuning (SFT), Reward Model (RM) training, Reinforcement Learning (RL) training, and evaluation settings."
        },
        {
            "title": "SFT Training",
            "content": "We utilized pre-trained model with 15B/150B Mixture-of-Experts (MoE) architecture as our base model. All training cases were truncated to maximum of 32,000 tokens. We employed cosine decay learning rate schedule, with the peak learning rate set to 2 105, gradually decaying to 2 106."
        },
        {
            "title": "RM Training",
            "content": "The Reward Model (RM) was initialized using the SFT model and subsequently trained on diverse set of internally, human-annotated data."
        },
        {
            "title": "RL Training",
            "content": "The dataset for Reinforcement Learning (RL) training was composed of two main types: Verifiable data, which receives feedback from verifier. This type of data allows for direct validation of the models outputs against known criteria. General data, scored by our reward model. The reward model assigns scores based on how well the models responses align with human preferences."
        },
        {
            "title": "Testing",
            "content": "For all evaluations, the inference temperature was set to 1.0 and top-p sampling was set to 0.7. Each test case was inferred at least 5 times, and the average score across these inferences was reported as the result for that case."
        },
        {
            "title": "Note on Data and Setup Disclosure",
            "content": "We strive to be as transparent as possible regarding our methodology. However, due to proprietary considerations and company confidentiality policies, we are unable to disclose further specifics about the training dataset composition or more granular details of the training setup at this time. We appreciate the understanding of the research community and hope that the provided information is sufficient to contextualize our findings and facilitate the reproducibility of our core concepts. We are committed to contributing to the open exchange of scientific knowledge within the bounds of these constraints."
        }
    ],
    "affiliations": [
        "ByteDance Seed"
    ]
}