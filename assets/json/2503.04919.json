{
    "paper_title": "FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement",
    "authors": [
        "Ian Huang",
        "Yanan Bao",
        "Karen Truong",
        "Howard Zhou",
        "Cordelia Schmid",
        "Leonidas Guibas",
        "Alireza Fathi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scene generation with 3D assets presents a complex challenge, requiring both high-level semantic understanding and low-level geometric reasoning. While Multimodal Large Language Models (MLLMs) excel at semantic tasks, their application to 3D scene generation is hindered by their limited grounding on 3D geometry. In this paper, we investigate how to best work with MLLMs in an object placement task. Towards this goal, we introduce a novel framework, FirePlace, that applies existing MLLMs in (1) 3D geometric reasoning and the extraction of relevant geometric details from the 3D scene, (2) constructing and solving geometric constraints on the extracted low-level geometry, and (3) pruning for final placements that conform to common sense. By combining geometric reasoning with real-world understanding of MLLMs, our method can propose object placements that satisfy both geometric constraints as well as high-level semantic common-sense considerations. Our experiments show that these capabilities allow our method to place objects more effectively in complex scenes with intricate geometry, surpassing the quality of prior work."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 9 1 9 4 0 . 3 0 5 2 : r FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement Ian Huang1,2* Yanan Bao2 Karen Truong2 Howard Zhou2 Cordelia Schmid2 Leonidas Guibas1,2 Alireza Fathi2 1Stanford University 2Google DeepMind Figure 1. FirePlace enables multi-modal large language models (MLLMs)to place new 3D objects into complex, preexisting 3D scenes, given (1) 3D scene, (2) 3D object, and (3) language prompt. It uses combination of MLLM common sense and low-level geometry constraints through the process described in this paper. Object placements generated by FirePlace are shown in Red."
        },
        {
            "title": "Abstract",
            "content": "Scene generation with 3D assets presents complex challenge, requiring both high-level semantic understanding and low-level geometric reasoning. While Multimodal Large Language Models (MLLMs) excel at semantic tasks, their application to 3D scene generation is hindered by their limited grounding on 3D geometry. In this paper, we investi1Work done during an internship at DeepMind. Correspondence to: ianhuang@cs.stanford.edu. gate how to best work with MLLMs in an object placement task. Towards this goal, we introduce novel framework, FirePlace, that applies existing MLLMs in (1) 3D geometric reasoning and the extraction of relevant geometric details from the 3D scene, (2) constructing and solving geometric constraints on the extracted low-level geometry, and (3) pruning for final placements that conform to common sense. By combining geometric reasoning with real-world understanding of MLLMs, our method can propose object placements that satisfy both geometric constraints as well as high-level semantic common-sense considerations. Our 1 experiments show that these capabilities allow our method to place objects more effectively in complex scenes with intricate geometry, surpassing the quality of prior work. 1. Introduction Generating scenes with preexisting 3D assets is crucial for diverse applications like architecture, game development, and virtual reality. While constructing scene can be seen as an iterative process of object placement, doing so even for single object requires complex understanding of both high-level considerations for why an object should be placed at particular location, as well as low-level understanding of the geometry of that object and its environment. Constructing such scenes requires placing objects in way that is both physically feasible and contextually appropriate. This involves understanding high-level concepts like object relationships and aesthetics, as well as low-level geometric constraints. Multi-modal Large Language Models (MLLMs) [33, 43] offer promising approach due to their real-world knowledge and common-sense reasoning capabilities [29, 58], which have been leveraged for tasks such as graphical editing [10, 14, 19, 20, 24, 47, 52], scene understanding [3], and question answering [25, 53]. However, applying them to object placement leads to many failure cases, for the reason that MLLMs struggle with precise 3D spatial reasoning and fine-grained geometry [9, 55]. Existing efforts often resort to training or fine-tuning these models on extensive 3D datasets to enhance their spatial capabilities [5, 17, 21, 27, 30, 59]. This paper introduces FirePlace, novel approach for placing new 3D objects into existing scenes using off-theshelf MLLMs, guided by natural language instructions. More concretely, our method takes as input Universal Scene Descriptor (USD) of 3D scene (including 3D assets within the scene), camera angle capturing the scene, new object mesh to be placed, and language instruction describing the desired placement. FirePlace then outputs an edited USD file, representing the scene with the object inserted. Our system iteratively translates abstract constraint descriptions (e.g., book should be on the shelf) into lowerlevel grounded 3D constraints (i.e. Which part of the book should be related to which part of the shelf, and in what way?). To accurately determine the parts of the scene that are relevant to the placement task, FirePlace introduces way for the MLLM to interrogate, visualize, and reason about relevant 3D information from the scene, by giving it access to range of 3D processing tools. FirePlace is able to combine the 3D processing capabilities of such tools with the common-sense reasoning capabilities of MLLMs to produce placements of objects that are feasible (i.e., satisfy geometric constraints) and plausible (i.e., satisfy commonsense reasoning about aesthetics, function and accessibility of object placements). Figure 1 shows some object placements generated by FirePlace. While recent works have introduced systems that generate object placements in scenes from scratch [1, 10, 18, 34, 42, 46, 47, 51, 52], we note that they would perform poorly on the task of object placement in complex preexisting scenes since they lack the following three capabilities: Reasoning with Fine-grained 3D Geometry. Prior works in scene-generation propose methods that act upon bounding boxes of objects [1, 18, 20, 34, 51, 52], attending to coarse physical relationships between objects to be synthesized. However, such representations fail when the key surfaces for the placement task lie within the bounding box, as with shelves in Figure 1. For example, placing book on shelf requires reasoning about the specific geometric features of the shelfs surface, such as its top plane and its depth, rather than simply its overall bounding box. To overcome the limitations of bounding-box-based approaches, FirePlace reasons with fine-grained 3D geometry from an explicit 3D scene representation by extracting, visualizing, and reasoning about object surfaces. This allows it to generate scenes that are more realistic and physically plausible. Reasoning about object instances. Existing scene generation methods lack the ability to reference specific object instances (e.g. which chair, which wall). These methods rely on pre-defined assumptions about wall layouts [1, 10, 51, 52] and assume either that the specific instance choice is irrelevant or predetermined by construction [1, 18, 52]. This assumption fails when inserting objects into complex scenes, where the correct instance choice may be context-dependent. For example, hanging picture in room with multiple walls requires understanding the specific context to identify the intended wall. To tackle this, FirePlace enables MLLMs to precisely reference object instances through visual selection. Common-sense placement. Existing methods [10, 18, 34, 46, 51, 52] often neglect the final visual result and common-sense considerations such as aesthetics, functionality, and accessibility during object placement. FirePlace addresses this limitation by leveraging the knowledge embedded in MLLMs, selecting among potential placements based on wider range of criteria, including aesthetic appeal, functional appropriateness, and accessibility considerations. This results in more realistic and plausible scene configurations that go beyond mere geometric feasibility. In order to enable MLLMs to reason about fine-grained 3D geometry and object instances, the MLLM must select among many discrete choices (of objects, and of surfaces) to choose the right ones for the placement task. However, MLLMs become substantially more prone to error as the set of options gets larger. We introduce visual selection 2 method that uses inference-compute scaling [2, 39] to mitigate this, called Batched Visual Selection, whereby choosing single surface/object among set of options is broken down into multi-round decision process, and each round limits the number of options that the MLLM has to look at. To summarize, our contributions are: 1. 3D reasoning framework that enables an off-the-shelf MLLM to translate high-level understanding of object placement requests into geometrically grounded 3D constraint functions while also adhering to common sense. 2. Batched Visual Selection: method that enhances the reliability of MLLMs in complex visual selection tasks by increasing computational resources during inference. 3. Experiments: Our results show that FirePlace surpasses existing LLM-based methods in generating realistic and plausible object placements within complex 3D scenes. Human evaluations confirm that FirePlace produces placements that are both physically feasible and aligned with common-sense expectations. Our investigation demonstrates the nuanced design choices and trade-offs needed when using MLLMs for downstream 3D tasks that require 3D understanding capabilities to be externally provided. 2. Related Works 3D scene generation and object placement. Significant efforts have been made towards collecting 3D scene datasets [4, 13, 36, 38, 40, 41], enabling the community to train and develop systems that generate and/or position elements within indoor scenes [11, 32, 34, 37, 4446, 51]. While they demonstrate that object placement rules can be distilled from scene databases, these were not designed to handle open vocabularies of objects, and even less so to take into account the level of common sense reasoning that underlies human decisions to place objects where they are placed within our living environments. FirePlace introduces method to leverage the knowledge of MLLMs to do this in training-free manner. While other works [6 8, 12, 15, 16, 22, 23, 26, 28, 31, 35, 50, 54, 56, 57] use 2D image priors to generate scenes and objects, they often have issues preserving object identity and physical plausibility of the final object arrangement. In contrast, FirePlace works with an explicit 3D scene representation, where explicit geometric constraints are enforced. Foundation models for 3D graphics. More recent works [14, 19, 20, 24, 47] have demonstrated the potential of involving large pretrained models for different stages of the 3D graphical design process. While they demonstrate capabilities in editing materials [20], texture [19], and controlling animation [14], they struggle with tasks that require complex spatial reasoning, like object placement. Existing works like [10] have attempted to position objects in scene by directly using LLMs through predicting the position and orientation of objects as LLM outputs. More recent works [1, 18, 42, 52] have demonstrated the benefit of using LLMs to predict constraints instead, before using solver to solve for final object placements. However, despite being able to create large-scale scenes, they represent each object using bounding boxes, making it impossible to express finegrained constraints between parts of objects, leading to constraints that can only explain placements of box-like objects (as opposed to putting book on shelf, or stuffed toy on chair with backrest and armrests). This design choice is understandable, since parts of objects become increasingly hard for LLMs to reason about. FirePlace introduces way to overcome this limitation. 3. Method As input, FirePlace is given (1) 3D scene = {o1, o2...} defined as collection of objects in world frame (along with camera pose that captures the scene), (2) transformable 3D object OT to be inserted, and (3) text instruction that describes the desired placement of the object. FirePlace returns single transformation matrix , for the final placement of the object (OT ) in the world frame. We desire = {T (OT )} that best matches the description l. FirePlace can be seen as executing visual chain-ofthought process that breaks down the visual reasoning task of object placement into individual steps. FirePlace starts by leveraging MLLMs world knowledge to generate constraint sketches, which is representation that references list of constraint functions, and textually annotates the surfaces that these constraints should act upon. FirePlace then performs three 3D reasoning stages of (1) resolving the surface references (e.g. the seat of the chair) to the 3D surfaces of the objects, (2) estimating continuous parameters, if applicable, of the constraint functions chosen in the constraint sketch, and (3) using constraint solver to produce candidate object placements that satisfy the constraints. FirePlace finally prunes the set of candidate object placements based on their renderings, selecting ones that adhere the most to common-sense reasoning of aesthetics, functionality and accessibility. Figure 2 shows the outcome of these stages in action. 3.1. Constraint Outline Generation [Stage 1] Given text l, constraint library (described in Section 3.4), and the rendering HR(D) of the scene (for some rendering function HR), the MLLM is prompted to produce list of triplets (f, tA, tT ), where references the specific constraint that should be followed, and tA and tT are text descriptions describing the relevant surfaces participating in this constraint. tA corresponds to the surface of the anchor object (e.g. the top surface of the white cabinet) and tT to that of the transformable object (e.g. the bottom of the 3 Figure 2. FirePlace pipeline. [Stage 1] FirePlace first generates set of constraint outlines, describing in text from the applicable constraints and the corresponding interacting surfaces. [Stages 2-4] FirePlace then selects the anchor object using Batched Visual Selection on instance segmentation masks. It extracts the surfaces that best match the constraint outline, and then uses constraint solver to produce feasible layouts. [Stage 5] Finally, it uses an MLLM to select subset of placements that adhere to common sense principles. TV screen). Generating constraint outlines can be accomplished simply by appropriately prompting the MLLM and parsing its results, which we will show in the Supp. Mat.. An example of this can be seen in Stage 1 of Figure 2. 3.2. 3D Reasoning for Feasibility [Stages 2-4] In the 3D reasoning stage, the objective is to form placement candidates {Ti} that satisfy geometric constraints expressed by the constraint outline. In order to do so, FirePlace must resolve the references of tA and tT down to the level of 3D surfaces, for every triplet within outline G. The method is described in Algorithm 1. For each triplet, the method starts by running visual selection process among the objects within the scene = {o1, o2...} for the object, OA, that best matches the description tA. FirePlace renders the segmentation masks of every object visible in the camera viewpoint, HS(oi), and tasks the MLLM with pointing to the object by naming the color of the associated segmentation mask. This enables FirePlace to reason about object instances. See Stage 2 of Figure 2. Then, FirePlace renders HR(OA), and tasks the MLLM in DirExtr(.) to extract the normal facing direction for the surfaces of OA that best correspond with tA. For instance, if tA mentions the seat of the chair, DirExtr() will return up. The same is done for OT and tT . FirePlace then uses geometric processing algorithms (described further in the Supp. Mat.) to extract sets of surfaces from both OT and OA based on their face-normals and their alignment with the surface directions extracted previously. This gives us set of candidate interaction surfaces {sA 1 ...} for OA and {sT 1 ...} for OT . Each surface is expressed as planar convex hull in 3D, circumscribing the faces that match. Its expected that within complex 3D asset, there will Algorithm 1 3D Reasoning of FirePlace 1: procedure CONSTRUCT CONSTRAINTS( 3D Scene D, Target Object OT , Constraint outline ) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: Constraints for (f, tA, tT ) in do OA (D, tA) dA DirExtr(OA, tA) dT DirExtr(OT , tT ) 1 , ...} SurfExtr(OA, dA) {sA 1 , ...} SurfExtr(OT , dT ) {sT ({sA sA 1 , ...}, tA) sT ({sT 1 , ...}, tT ) π ContinuousParam(f ) {(f, sA Choose anchor object OA surface normal OT surface normal OA surfaces OT surfaces Choose OA surface Choose OT surface Guess params , π)} Add to constraints , sT end for {T 1 ...} arg minT (cid:80) (f,sA,sT ,π)R (sA, (sT ), π) Solve for feasible set of transformations 1 , ...} return {T 15: 16: end procedure be many candidate interaction surfaces for any particular direction. FirePlace therefore uses visual selection process once again to choose the best match among them (sT and sA ) corresponding to tT and tA. FirePlace renders the interaction surfaces {sA 1 ...}, overlaid on top of OT and OA, and renders these surfaces in different colors. It then tasks the MLLM to point to the best plane by naming its color. This allows FirePlace to express constraints in terms of fine-grained geometry. See Stage 3 of Figure 2. 1 ...} and {sT Finally, the MLLM is prompted to estimate continuous arguments π (e.g. such as distances for distance constraints), completing the last piece of fully 3D-grounded 4 , (sT constraint function (Supp. Mat. shows the prompts used). This function can then be evaluated for some transformation as (sA ), π). After accumulating this for every outline in G, FirePlace minimizes the sum of the constraint functions with respect to using simulated annealing, as done in [36], and returns the set of candidate placements that successfully minimize the sum of the constraint functions below scalar threshold. See Stage 4 of Figure 2. 3.3. Plausibility Pruning [Stage 5] As done in [20], we use MLLM-based pruning of the final results to remove placements that are geometrically feasible but implausible based on considerations such as aesthetics, functionality, and accessibility. This is done by running visual-selection on the renderings HR(D{T (OT )}) through tasking the MLLM to find the most plausible placements. This stage guides the MLLM to first generate considerations for what good placements would look like before picking between pairs of renderings shown. By the end of this stage, the selected placement adheres more to common sense. See Stage 5 of Figure 2. 3.4. Constraint Library We provide FirePlace with small constraint library composed of binary constraint functions. They take in two interaction surfaces (from the anchor and target objects) and output loss, which is minimal (i.e. 0) when the constraint is met. We provide FirePlace with the following constraint functions: (1) Parallel(p1, p2): p1 and (2) CloseTo(p1, p2, dist): p2 are parallel. between p1 and p2 is the maximum distance dist. the minimum distance between p1 and p2 is dist. (4) InfrontPlane(p1, p2): p2 is on the front-facing side of p1 (in the direction of its surface normal). (5) Contact(p1, p2): p1 and p2 are in contact. (6) NoOverhang(p1, p2): p1s projection onto p2 is entirely contained within p2. This is useful for expressing that an object placed on another should not overhang the edge of the top surface it sits upon. (3) FarFrom(p1, p2, dist): While this library is small, we find that this set of planar constraint functions can, in combinations, communicate most, if not all, of the constraint functions in prior works like [18, 52]. We provide more details in the Supp. Mat.. 3.5. Batched Visual Selection FirePlace uses visual selection pervasively throughout its 3D reasoning and plausibility pruning. However, selecting among many visual options is challenge for MLLMs. Similar to findings in [48], we found that overwhelming the MLLM with visual options heavily impacts its ability to choose the correct object or interaction surfaces. Algorithm 2 Batched Visual Selection 1: procedure V(candidates {c1, c2, ...ck}, objective t, 2: 3: 4: 5: 6: 7: 8: batch size m) [c1, c2, ...ck] if < then List of candidates return Best match among according to end if while > 1 do If multiple candidates exist Shuffle before batching ({Cim+1...Cm(i+1)}, t, m) Break into m-sized batches then recursively call self. Shuffle(C) (cid:83)C/m i=0 end while return 9: 10: 11: end procedure Inspired by the use of inference compute scaling to overcome inherent limitations in MLLMs [2, 20, 39], we propose Batched Visual Selection (outlined in Algorithm 2), method that recursively breaks down the set of options into batches of size m. The method shows at most options to the MLLM at time, merges the ones that get selected within each batch, and repeats this process until single option remains. In the context of visual-selection for anchor objects, candidate objects within certain batch are masked out from the scene rendering in different colors. The MLLM is provided list of these color names alongside the image, and selects options by referencing color names. Similarly, candidate interaction surfaces within mask are visualized on top of the original object mesh in different colors, and the MLLM is prompted to choose the most relevant interaction surface in similar fashion. 4. Experiments 4.1. Dataset, Metrics & Baselines To demonstrate FirePlaces ability to perform object placement in complicated scenes, we conduct our experiments on 50 photorealistic 3D scenes with fixed camera viewpoints designed by human experts. To create placement tasks, we select one object for each placement task (transformable object OT ) and caption its placement in the scene using language annotation l. We then remove the object from the scene and save the modified scene as the initial scene D. Doing so for number of objects in each of the 50 scenes creates an evaluation dataset of 266 placement tasks. More details on these 3D scenes and tasks can be found in the Supp. Mat.. We use five metrics to evaluate our placements: 1. Min L2 Error: L2 error of the closest match to the groundtruth translation among multiple tries per placement task, generated by each method. 5 Figure 3. Qualitative samples of object placements (shown in red masks) within 3D scenes based on language instructions. FirePlace can place diverse objects in variety of settings, and produce geometrically feasible and semantically plausible object placements. 2. Mean L2 Error: L2 error in the groundtruth translation and the predicted translation of the object, averaged across multiple tries per placement task. 3. Energy Score: the proportion of constraint functions generated by the constraint generator that outputs LOW energy at the groundtruth placements ( < 0.01). 4. Plausibility Score: Inspired by the latest works that use MLLMs for evaluation of 3D generative models [49], the plausibility score is generated by Gemini, and ranges from 1 (poor) to 4 (great). We provide it concrete rubric for evaluation of every sample, which is shown in detail in the Supp. Mat.. Section 4.3 also shows great alignment between the plausibility score and human preferences in our user preference study. 5. Visibility Score (in range [0,1]) is the rate which transformable objects are observable within the generated placement renderings after placement. We compare against Holodeck[52] and LayoutGPT[10], works that use LLMs for the generation of constraints of object bounding boxes [52] or 3D position of object bounding boxes [10]. For both methods, we provide list of Geminigenerated captions for the objects within the scene (generated based on per-object renderings) and ask their constraints/positions to be generated in the context of preexistMetric LayoutGPT Holodeck Ours Min L2 error cm () Mean L2 error cm () Visibility score () Plausibility score () Energy score () 89.01 132.96 0.79 2.14 N/A 113.17 137.60 0.63 2.13 0.22 48.39 69.89 0.88 2.95 0.42 Table 1. Comparison with Holodeck[52] and LayoutGPT[10]. LayoutGPT does not use constraints, so Energy Score is N/A. FirePlace outperforms both baselines in all of the 5 metrics. See Figure 4 and Figure 5 for visual samples. ing objects in the scene. Our full prompts for both methods are shown in the Supp. Mat.. For Holodeck, we only use the Constraint-based Layout Design Module [52], the module relevant to placement. We choose to compare with both methods because both leverage the common-sense reasoning of large pretrained foundation models, but are lacking in the 3 capabilities enabled by FirePlace in different ways. For our method and in all baselines, Gemini-1.5 Pro [43] is used. 6 Figure 4. Comparisons against Holodeck. Holodeck fails to put the collection of books onto the shelf (due to its bounding box representation), and produces many implausible placements due to incorrect selection of anchor objects using the caption-based selection method. 4.2. Qualitative Examples & Baseline Comparisons Like Figure 1, Figure 3 shows the breadth of objects that can be placed into variety of existing 3D scenes using FirePlace. This demonstrates FirePlaces capabilities in producing object placements compatible with visual context and lower-level geometry like the surfaces of bookshelves. Figure 2 shows the intermittent steps of placing animal figurines on the window seat. Note that for the figurines to be put onto the window seat, the 3D reasoning stage must both select the window seat from the many segmentation masks in the image and extract the surface that corresponds to placeable parts of the window seat. Despite the fact that cushions and plant vases are attached to the window seat mesh, FirePlace extracts and chooses the placeable surface. Additionally, note that among the feasible placements, few are aesthetic/accessible positions. Plausibility pruning is able to select more centered placements among these as the final output. Table 1 shows comparison of our method against Holodeck and LayoutGPT across the 266 placement tasks. All methods are given language descriptions of the desired placement. FirePlace achieves close to half the L2 losses of baselines, and additionally scores higher on visibility, plausibility, and energy. Qualitatively comparing outputs (Figure 4 and Figure 5) shows that our method is able to produce higher quality placements. 4.3. Human Evaluations We perform user preference studies (30 participants, 2075 side-by-side comparisons to LayoutGPT and Holodeck) on randomly chosen outputs of FirePlace, LayoutGPT, and Holodeck. Human participants are shown masked renderings of two placements (presented as option and in random order) and the original language prompt. Participants Physics Semantics Com. Sense Win vs. Holodeck Tie vs. Holodeck 60.27% 65.70% 19.69% 20.77% Win vs. LayoutGPT 76.02% 76.10% 11.39% 11.47% Tie vs. LayoutGPT 62.08% 19.81% 76.42% 11.22% Table 2. User preference study shows that FirePlace placements are preferred over both baselines along all 3 axes of comparisons. are tasked with annotating which of two placements (or tie) is (1) more physically plausible (e.g. objects are not floating in space), (2) more semantically aligned with the input language prompt, and (3) more plausible according to common sense with considerations for aesthetics, functionality, and accessibility within living spaces. We show the results in Table 2 for the win-rate of outputs from our method compared to baselines for questions (1), (2) and (3) under physics, semantics and com. sense respectively. We surpass the performance of prior works quite notably, and predominantly tie in human preferences when our method does not clearly produce better result, as judged by our human annotators. These human annotations also show high correlation with plausibility. Among the cases when clear winner is decided by human raters, plausibility metrics and human judgment agree 89.82% of the time. 4.4. Ablation Studies FirePlace introduces many design choices that we validate through ablations. The results are shown in Table 3. Use of geometric constraints. To what extent can the placement problem be solved by asking the MLLM to pick the best placement among random guesses of placement? The Constraints row in Table 3 demonstrates the consequences of doing this FirePlace generates 100 random 7 Figure 5. Comparisons against LayoutGPT. LayoutGPT produces implausible object placements with intersections, showing that LLMs often fail to accurately estimate object positions and should be guided by constraints, as done in FirePlace. Ablation L2 () Vis() Plaus.() Energy () 62.52 Ours Constraints 124.11 Vis. Select. 67.61 Geometry 63.44 Com. Sense 66.08 Vis. Scale. 83.95 0.87 0.24 0.82 0.79 0.87 0. 2.94 1.95 2.85 2.89 2.60 2.55 0.34 N/A 0.25 0.44 0.34 0.19 Table 3. Ablations of design decisions in FirePlace. L2, Vis, Plaus, and Energy are short for mean L2 error, visibility, plausibility, and energy score, respectively. placements, renders them all, and asks the VLM to choose among them. The result has substantially worse L2 error, visibility (notably, placed objects are only visible 24% of the time), and plausibility scores, motivating the need for method that uses fine-grained geometric constraints to guide the common-sense selection. Visual selection of anchor objects. What happens when we remove the visual selection process from FirePlace, and use caption-based selection (as done for the baselines) instead? The results are shown in the Vis. Select. row in Table 3. Removing the ability to visually select object instances for the anchor objects heavily relies on robust captioning for the assets, leading to drops in performance across the board. Moreover, the generated constraints match groundtruth placements less well, as shown by the substantial decrease in energy score. Fine-grained geometry. We replace interaction surfaces extracted in Algorithm 1 with the bounding box surfaces facing that direction in row Geometry of Table 3. Interestingly, we find that the energy score becomes higher, likely due to the fact that among the fewer constraints predicted by FirePlace, many are explainable using bounding box constraints. However, this leads to lower expressivity of the constraints, evidenced by the dip in plausibility and higher chance of the object being placed out of view, shown by the lower visibility score. Qualitatively, this leads to the observation that objects are often unable to be put onto shelves or flat surfaces that are circumscribed by bounding boxes substantially larger than it or at different elevation. Plausibility pruning. Row Com. Sense in Table 3 shows the consequences of simply using the geometrically feasible placement solutions as final solutions, without plausibility pruning. This causes large dip in plausibility, leaving visibility and energy scores largely unchanged. Inference compute scaling for Batched Visual Selection. We gradually increase the batch size (from size 3 in our system), to the point where the MLLM must select among up to 100 options at time. The consequence of doing this is shown in the Vis. Scale. row in Table 3. Anchor objects and interaction surfaces chosen become significantly less accurate, shown by the substantially lower energy score, In fact, removing inferenceplausibility, and L2 errors. compute scaling for Batched Visual Selection lowers performance along three of the four metrics below that of ablating low-level geometry, plausibility pruning, and removing visual-selection. This shows the importance of Batched Visual Selection to FirePlace. More in the Supp. Mat.. 5. Conclusion & Discussion FirePlace is novel approach to address the task of 3D object placement by integrating the geometric reasoning capabilities of 3D processing tools with the common-sense reasoning of MLLMs. By leveraging inference-compute scaling for the visual selection task through Batched Visual Selection, FirePlace translates language instructions into grounded 3D constraints, producing object placements that 8 are not only geometrically feasible but also semantically plausible when considering factors like aesthetics, functionality, and accessibility. FirePlace highlights the potential for MLLMs in 3D environments to solve spatial reasoning tasks when complemented by external geometric reasoning tools."
        },
        {
            "title": "Acknowledgements",
            "content": "Wed like to acknowledge the SpatialVerse team at Manycore Tech Inc. for their provision of the 3D scene assets used in this project. Finally, wed like to thank Kyle Genova and Tom Funkhouser for their feedback on the paper draft."
        },
        {
            "title": "References",
            "content": "[1] Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie. Open-universe indoor scene generation using llm program synthesis and uncurated object databases. arXiv preprint arXiv:2403.09675, 2024. 2, 3 [2] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. 3, 5 [3] Shivam Chandhok. Scenegpt: language model for 3d arXiv preprint arXiv:2408.06926, scene understanding. 2024. 2 [4] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning arXiv preprint from rgb-d data in indoor environments. arXiv:1709.06158, 2017. 3 [5] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. SpatialVLM: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. [6] Shen Chen, Jiale Zhou, Zhongyu Jiang, Tianfang Zhang, Zongkai Wu, Jenq-Neng Hwang, and Lei Li. Scalinggaussian: Enhancing 3d content creation with generative gaussian splatting. arXiv preprint arXiv:2407.19035, 2024. 3 [7] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Scenedreamer: Unbounded 3d scene generation from 2d image collections. IEEE transactions on pattern analysis and machine intelligence, 2023. [8] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free genarXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. 3 [9] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2179521806, 2024. 2 [10] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 6 [11] Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan. Example-based synthesis of 3d object arrangements. ACM Transactions on Graphics (TOG), 31(6):111, 2012. 3 [12] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. Advances in Neural Information Processing Systems, 36, 2024. [13] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts In Proceedings of the IEEE/CVF Internaand semantics. tional Conference on Computer Vision, pages 1093310942, 2021. 3 [14] Purvi Goel, Kuan-Chieh Wang, Karen Liu, and Kayvon Iterative motion editing with natural language. Fatahalian. arXiv preprint arXiv:2312.11538, 2023. 2, 3 [15] Paul Henderson, Melonie de Almeida, Daniela Ivanova, Sampling 3d gaussian scenes arXiv preprint and Titas Anciukeviˇcius. in seconds with latent diffusion models. arXiv:2406.13099, 2024. 3 [16] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d In Proceedings of meshes from 2d text-to-image models. the IEEE/CVF International Conference on Computer Vision, pages 79097920, 2023. [17] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023. 2 [18] Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David Ross, Cordelia Schmid, and Alireza Fathi. Scenecraft: An llm agent for synthesizing 3d scenes as blender code. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 5 [19] Ian Huang, Vrishab Krishna, Omoruyi Atekha, and Leonidas Guibas. Aladdin: Zero-shot hallucination of stylized 3d arXiv preprint assets from abstract scene descriptions. arXiv:2306.06212, 2023. 2, 3 [20] Ian Huang, Guandao Yang, and Leonidas Guibas. Blenderalchemy: Editing 3d graphics with vision-language models. arXiv preprint arXiv:2404.17672, 2024. 2, 3, 5 [21] Kuan-Chih Huang, Xiangtai Li, Lu Qi, Shuicheng Yan, and Ming-Hsuan Yang. Reason3d: Searching and reasoning 3d segmentation via large language model. arXiv preprint arXiv:2405.17427, 2024. 2 [22] Yukun Huang, Jianan Wang, Ailing Zeng, Zheng-Jun Zha, Lei Zhang, and Xihui Liu. Dreamwaltz-g: Expressive 3d gaussian avatars from skeleton-guided 2d diffusion. arXiv preprint arXiv:2409.17145, 2024. [23] Xiaoliang Ju, Zhaoyang Huang, Yijin Li, Guofeng Zhang, Yu Qiao, and Hongsheng Li. Diffindscene: Diffusion-based 9 high-quality 3d indoor scene generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 45264535, 2024. 3 [24] Peter Kulits, Haiwen Feng, Weiyang Liu, Victoria Abrevaya, and Michael Black. Re-thinking inverse graphics with large language models. arXiv preprint arXiv:2404.15228, 2024. 2, 3 [25] Jusung Lee, Sungguk Cha, Younghyun Lee, and Cheoljong Yang. Visual question answering instruction: Unlocking multimodal large language model to domain-specific visual multitasks. arXiv preprint arXiv:2402.08360, 2024. 2 [26] Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, and Sung-Eui Yoon. Semcity: Semantic scene generation with triplane diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2833728347, 2024. 3 [27] Mingsheng Li, Xin Chen, Chi Zhang, Sijin Chen, Hongyuan Zhu, Fukun Yin, Gang Yu, and Tao Chen. M3dbench: Lets instruct large models with multi-modal 3d prompts. arXiv preprint arXiv:2312.10763, 2023. [28] Pengzhi Li, Chengshuai Tang, Qinxuan Huang, and Zhiheng Li. Art3d: 3d gaussian splatting for text-guided artistic scenes generation. arXiv preprint arXiv:2405.10508, 2024. 3 [29] Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson dAutume, Phil Blunsom, and Aida Nematzadeh. systematic investigation of commonsense arXiv preprint knowledge in large language models. arXiv:2111.00607, 2021. 2 [30] Zeju Li, Chao Zhang, Xiaoyan Wang, Ruilong Ren, Yifan Xu, Ruifei Ma, Xiangde Liu, and Rong Wei. 3dmit: 3d multi-modal instruction tuning for scene understanding. In 2024 IEEE International Conference on Multimedia and Expo Workshops (ICMEW), pages 15. IEEE, 2024. 2 [31] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from sinIn Proceedings of the IEEE/CVF International gle image. Conference on Computer Vision, pages 1445814467, 2021. 3 [32] Rui Ma, Akshay Gadi Patil, Matthew Fisher, Manyi Li, Soren Pirk, Binh-Son Hua, Sai-Kit Yeung, Xin Tong, Leonidas Guibas, and Hao Zhang. Language-driven synthesis of 3d scenes from scene databases. ACM Transactions on Graphics (TOG), 37(6):116, 2018. 3 [33] OpenAI. Gpt-4v(ision) system card. OpenAI, 2023. 2 [34] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregressive transformers for indoor scene synthesis. Advances in Neural Information Processing Systems, 34:1201312026, 2021. 2, 3 [35] Ryan Po and Gordon Wetzstein. Compositional 3d scene generation using locally conditioned diffusion. In 2024 International Conference on 3D Vision (3DV), pages 651663. IEEE, 2024. [36] Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, et al. Infinigen indoors: Photorealistic indoor scenes using procedural generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21783 21794, 2024. 3, 5 [37] Daniel Ritchie, Kai Wang, and Yu-an Lin. Fast and flexible indoor scene synthesis via deep convolutional generaIn Proceedings of the IEEE/CVF Conference tive models. on Computer Vision and Pattern Recognition, pages 6182 6190, 2019. 3 [38] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In International Conference on Computer Vision (ICCV) 2021, 2021. 3 [39] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. 3, 5 [40] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. Sun rgb-d: rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567576, 2015. [41] Shuran Song, Fisher Yu, Andy Zeng, Angel Chang, Manolis Savva, and Thomas Funkhouser. Semantic scene comIn Proceedings of the pletion from single depth image. IEEE conference on computer vision and pattern recognition, pages 17461754, 2017. 3 [42] Hou In Ivan Tam, Hou In Derek Pun, Austin Wang, Angel Chang, and Manolis Savva. Scenemotifcoder: Example-driven visual program learning for generating 3d arXiv preprint arXiv:2408.02211, object arrangements. 2024. 2, 3 [43] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 6 [44] Kai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel Chang, and Daniel Ritchie. Planit: Planning and instantiating indoor scenes with relation graph and spatial prior networks. ACM Transactions on Graphics (TOG), 38(4):1 15, 2019. 3 [45] Xinpeng Wang, Chandan Yeshwanth, and Matthias Nießner. Sceneformer: Indoor scene generation with transformers. In 2021 International Conference on 3D Vision (3DV), pages 106115. IEEE, 2021. [46] Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, and Leonidas Guibas. Lego-net: Learning regular rearrangements of objects in rooms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19037 19047, 2023. 2, [47] Zehao Wen, Zichen Liu, Srinath Sridhar, and Rao Fu. Anyhome: Open-vocabulary generation of structured and textured 3d homes. arXiv preprint arXiv:2312.06644, 2023. 2, 3 10 [48] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms, 2023b. URL https://arxiv. org/abs/2312.14135. 5 [49] Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein. Gpt-4v (ision) is human-aligned evaluator for text-to-3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2222722238, 2024. 6, 2 [50] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. Citydreamer: Compositional generative model of unbounded In Proceedings of the IEEE/CVF Conference 3d cities. on Computer Vision and Pattern Recognition, pages 9666 9675, 2024. 3 [51] Yixuan Yang, Junru Lu, Zixiang Zhao, Zhen Luo, James JQ Yu, Victor Sanchez, and Feng Zheng. Llplace: The 3d indoor scene layout generation and editing via large language model. arXiv preprint arXiv:2406.03866, 2024. 2, 3 [52] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al. Holodeck: Language guided generation of 3d embodied ai environments. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2024), pages 2025. IEEE/CVF, 2024. 2, 3, 5, [53] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023. 2 [54] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67966807, 2024. 3 [55] Yang You, Mikaela Angelina Uy, Jiaqi Han, Rahul Thomas, Haotong Zhang, Suya You, and Leonidas Guibas. Img2cad: Reverse engineering 3d cad models from images through arXiv preprint vlm-assisted conditional arXiv:2408.01437, 2024. 2 factorization. [56] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: In Proceedings of Going from anywhere to everywhere. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. 3 [57] Xuening Yuan, Hongyu Yang, Yueming Zhao, and Di Huang. Dreamscape: 3d scene creation via gaussian splatting joint arXiv preprint arXiv:2404.09227, correlation modeling. 2024. [58] Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. Advances in Neural Information Processing Systems, 36, 2024. 2 [59] Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, and Xihui Liu. Empowering 3d visual grounding with reasoning capabilities. arXiv e-prints, pages arXiv2407, 2024. 2 11 FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement"
        },
        {
            "title": "Supplementary Material",
            "content": "In Section 6, we discuss the limitations and potential improvements to FirePlace, and comment on societal impact in Section 7. We elaborate on the designs of metrics (specifically the energy and plausibility scores introduced in the main paper) in further detail in Section 8. We discuss the prompts and algorithms used in constraint outline (Section 9), anchor object extraction (Section 10), interaction surface extraction (Section 11) and continuous parameter estimation (Section 12). We also share the implementations of our constraint functions in Section 13. In Section 14, we explain how we adapted Holodeck and LayoutGPT for the object placement task, and discuss the prompts used. Further details on the evaluation dataset is provided in Section 15. In Section 16, we show qualitative consequences of ablations done in the main paper, and also do additional experiments on the performance effects of scaling down inference compute used in Batched Visual Selection, further demonstrating the benefit of scaling inference compute for the visual selection task. In Section 17, we show the performance of FirePlace on image inputs that depict single example of an object placement, where FirePlace is tasked to generate similar placements. Finally, in Section 18, we show superior performance of FirePlace over baselines (for both text and image inputs), based on comparisons done by an MLLM. 6. System Limitations Since our method uses MLLMs in every step of the placement generation process (with the exception of Step 4 in Figure 2), latency is limitation of our approach. Our system currently takes on the range of 30 seconds to 2 minutes per object placement, depending on the number of prior objects within the scene (with more anchor object candidates, Batched Visual Selection must select among more options, creating more calls to the MLLM) and the number of surfaces that get extracted. Additionally, much of the computation time was used for rendering object placements. For our experiments, we only used CPUs for FirePlace, so it is likely possible to be sped up using GPU rendering. Figure 6 shows some qualitative examples of common failure cases for FirePlace. They often come from intersections between the placed objects and preexisting objects within the environment (which can be addressed using intersection constraints), or failure to generate comprehensive sets of constraints (leading to under-constrained Figure 6. Common failure modes. On the left, the placement of the object overlaps with preexisting objects, due to the constraint library not including constraint to minimize intersections. In the middle, the placement of the chair was not constrained beyond contact to the ground, but additional constraints should have been generated (such as parallelism between the backs of the masked chair and the adjacent chair). On the right, the plausibility pruning step failed to remove implausible placements in the event of underconstrained placements (the bottom of the books are in contact with the table, but is overhanging), leading to placement result that features the book floating over the edge of the table. Figure 7. When surface extraction is done in canonical space, but the object is rotated in world space. failure case where stack of books is placed under shelf because contact constraints were enforced for upward-pointing (in the canonical space) surface of the shelf that has been rotated by the artist in its final position in the world frame. placements), or failure to prune the set of generated placements, often when the item being placed is small compared to the rest of the scene. In other cases, FirePlace may choose the incorrect object for the anchor object or the incorrect surfaces of objects, limitation inherited from existing MLLMs. As MLLMs improve, we expect these issues to be mitigated. Our surface extraction method extracts surfaces of anchor objects in canonical space. This assumes that surfaces of anchor objects that are pointing, say, upwards in the canonical space are also pointing upwards in the world 1 space once the anchor object is transformed. In rare cases within our dataset, this is not valid assumption, the resultant optimized placements reflect this. An example can be seen in Figure 7. 7. Societal Impact We do not foresee any substantial negative impacts of our work, beyond the inheritance of potential bias that may already be inside the MLLMs that currently exist. We anticipate that ongoing and future efforts in reducing MLLM bias will mitigate this. 8. Design of Metrics Evaluating object placement is tricky. On the one hand, we have groundtruth placement created by human artists with which we can compare generated placements, but such groundtruth accounts for only one of many possible object placements (e.g. consider placing cup on table). To mitigate this, energy and plausibility scores guard against the precision and recall of the constraint functions that are generated. Consider the simple example of placing cup on top of table. Our system creates constraint functions that should be minimally valued at the groundtruth positions. This is what the energy score communicates the proportion of constraint functions constructed that are minimal (< 0.01) when evaluated on the groundtruth placement generated by the human artist. If energy score is 1, this means that all the constraint energy functions generated for placing the cup (e.g. parallel constraints between bottom of cup and the top of the table) are correct according to the one groundtruth example. The energy score can be low when the placement is over-constrained, or when the constraint functions have low precision. On the other hand, what if the constraints are underconstrained? The plausibility score seeks to measure this. Assume that the parallel constraint mentioned above was the only one generated. As you may imagine, many solutions satisfying this constraint would render the object floating in parallel to the table, but not necessarily in contact or within the tabletops perimeter. This would score poorly on plausibility by the MLLM, since many final renderings will display placements that are not physically realistic or disagreeing with the input text prompt (see Figure 8 for examples). In this case, the plausibility score will indicate that the placement is under-constrained, or that the constraint functions have low recall. The prompts (and rubric) used to evaluate the plausibility score are given in Figure 21. Prior works like [49] use MLLMs for evaluation, and have shown through human evaluations that MLLM evaluations are aligned with human preferences. In this paper, we observe the same for the plausibility score. As shown in Figure 8, plausibility scores capture the extent to which placements are physically feasible and semantically plausible. Additionally, as noted in the main paper, when using plausibility scores to decide the better placement between two samples, they agree with humans 89.92% of the time, when using Gemini 1.5 Pro. See Section 4.3 for more. 9. Constraint Outline Generation We use the prompt shown in Figure 23 to query the MLLM to generate constraint outlines. As part of the prompt, we also provide the rendering of the input 3D scene ( [Source Layout Rendering] ), the input text prompt ( [Placement text prompt] ) and doc string describing the constraint functions within our constraint function library ( [Constraint library doc string] ). The contents of this doc string is shown in Figure 22. 10. Extraction of Anchor Objects We use Batched Visual Selection to select for object instances that match language descriptions of the anchor objects from the constraint outline. Beyond the procedure outlined in Algorithm 2, we use the prompts shown in Figure 24 to select among every batch. The result of doing so is shown in Figure 9. Their image segmentation masks are derived from the USD rendering process of the input 3D scene. 11. Extraction of Surfaces To execute the surface extraction steps in Algorithm 1, we will first describe how DirExtr() works, then elaborate on the geometric processing underlying SurfExtr(). In DirExtr() the MLLM is prompted to generate surface normals that match the language descriptions of the surfaces that best match the constraint outline descriptions of surfaces that participate in the constraint. For instance, if the transformable object should be sitting on the seat of the chair, the surface that should be extracted from the chair should be pointing upwards (i.e. the seat). This is done through the prompt shown in Figure 25. Note that in our experiments, we provide the MLLM with the 6 major directions (left, right, front, back, up, down) that surfaces can point, but our method can also work with more directions by updating the prompt accordingly. After this is done for the anchor and transformable objects, geometric processing algorithms are then called to extract the surfaces that point in these directions. To this end, we first filter the faces of the object mesh to subset that have face normals within some threshold level of cosine similarity with the unit vectors corresponding to the extraction direction generated in the previous step. For the faces that have similar face normals to the extraction direction, we project the center of these faces along the desired 2 Figure 8. Examples of plausibility scores for different placements. The text prompts are shown on the left, and objects placed are shown on the right. plausibility score of 4 is the maximum, and 1 is the minimum. Refer to Figure 21 for the definitions of these scores. surface normal, then cluster them using DBSCAN. This allows us to find different sets of faces that lie along similar level along the desired surface normal. Faces in each set are then projected to the same level before convex hull is fitted onto that group, which extracts flat convex hull tightly circumscribing each set. Each of these planar convex hulls is an interaction surface candidate, and has surface normal equal to the extraction direction. This process leads to many candidate interaction surfaces and, depending on the level of geometric complexity of assets, may be prohibitively cumbersome/expensive/difficult to filter down to one using Batched Visual Selection. As such, we merge interaction surfaces that are close to each other within certain distance threshold by keeping the interaction surfaces that have larger area. Additionally, we recognize that many constraints can be adequately expressed using bounding box constraints, so we append the bounding box surface aligned with the extraction direction to the set of candidate interaction surfaces. The process of choosing the correct interaction surfaces for the anchor and transformable objects is very similar to that of choosing the anchor object (See Section 10). We render the surfaces overlaid on top of the original object mesh using matplotlib, taking care to show no greater than 3 candidates for every batch. The selection is done according to the prompt shown in Figure 26. Figure 15, Figure 16, Figure 17, Figure 18, Figure 19, Figure 20 show examples of the constraints constructed by FirePlace for different placement tasks using the surfaces extracted by this approach. 12. Parameter estimation Once interaction surfaces of the anchor and transformable objects are extracted, we prompt the MLLM with renderings of the scene and matplotlib renderings of the interaction surfaces, according to the prompt shown in Figure 27. documentation of the meaning of the continuous parameters of each constraint function is also provided. In our setting, only two of the constraint functions have continuous parameters (CloseTo and FarFrom); namely, maxthen evaluates the dot product between each vector difference and the surface normal of p1, n1. The return value of this function is min(0, max({ vij vij n1}ij)) for all vector differences vij between the ith vertex in p1 and the jth vertex in p2. 4. Contact(p1, p2) is InFrontPlane(p1,p2)+InFrontPlane(p2,p1) which is minimal when p1 and p2 are either in contact with each other or coplanar. 5. NoOverhang(p1, p2) is more involved. Let p1 be the set of vertices of p1 (with vertices and 3 coordinates, p1 RN 3) and p2 be that of p2. Let n2 be the normal vector of p2. We then first project p1 onto p2, p1p2 = p1 ((p1 o) n2)n2 for some arbitrary vertex from p2. Then, we sample 1000 points from the region bound by p1p2, which we call R1000. We can then calculate whether is contained within the bounds of p2. The final output value of this function is defined as 1 1 1000 1000 (cid:88) i=1 Iinside(qi) where Iinside is an indicator function indicating whether qi lies on the inside of p2. The function is minimal when the projection of p1 onto p2 is entirely contained within p2 (i.e. i, Iinside(qi) = 1). 14. Prompts and Constraint Functions for"
        },
        {
            "title": "Holodeck and LayoutGPT",
            "content": "For the Holodeck and LayoutGPT baselines in our experiments, we use the prompts shown in Figure 28 and Figure 29 for Holodeck, and Figure 30 for LayoutGPT. Note that we use the prompts found in the implementations released on github 1, modified only to provide additional information about the objects that already exist within the scene. To do this, we use Gemini to caption object renderings of assets found within the scene, and provide these captions to both methods via the prompts. For LayoutGPT, we provide the caption of each object alongside their bounding box information (length, width, height, left, top, depth, orientation) as part of the prompt. These can be derived from their local-to-global transformation matrices, as well as the length, width, height of their bounding boxes in canonical space. 1Official Holodeck prompts and official LayoutGPT prompts Figure 9. The batched visual selection process for scene with only few items. Here, the MLLM is tasked to find the anchor object corresponding to the white cabinet from constraint outline generated. Each batch shows 3 options rendered in different colors (for batch size = 3), and the MLLM chooses object instances that best match the description by indicating the color of the mask in each round. The chosen instances across each batch are merged and the process is repeated until only one object instance is chosen. This is done using the prompt shown in Figure 24. imum/minimum distances between the two interaction surfaces. 13. Constraint functions The constraint functions outlined in Section 3.4 are implemented as binary functions that evaluate to 0 when geometric relationship is satisfied between two interaction surfaces, p1 and p2. Below, we describe the implementations of each: 1. Parallel(p1, p2) returns min(1 nT 1 n2, 1 nT 1 n2) where n1 is the surface normal of p1 and n2 that of p2. This function is minimal when either the surface normals are aligned, or pointing in parallel but opposite directions. 2. CloseTo(p1, p2, dist) returns max(d(p1, p2) dist, 0) for some scaling constant (which we set to 0.1) and distance function between two interaction surfaces. This function is minimal when d(p1, p2) is smaller than dist. 3. InFrontPlane(p1, p2) first finds the pairwise vector differences between the vertices of p1 and p2, 4 IDs these captions (e.g. alongside indicated Holodeck, curtain) (e.g. For obare provided brown object-34), ject by [Descriptions and labels of preexisting objects in the scene] in Figure 29. Holodeck can then reference the object IDs in the construction of the bounding box constraints. For the bounding box constraints used by Holodeck, we implement bounding box constraints as binary constraint functions, similar to those in Section 3.4, with the big difference they operate on bounding boxes instead of being that interaction surfaces. The Holodeck baseline has access library composed of the to bounding box constraint (1) FaceTo (that the front face following constraints: of the object bounding box faces the center of another bound box), (2) near (that objects are closer than 150 cm from each other and further than 50 cm away) (3) far (object are further than 150 cm away from each other) (4) infront, (5) sideof, (6) leftof, (7) rightof, (8) behind, (9) ontop, (10) centeraligned_front, (11) centeraligned_side. These are all bounding box constraints originally used in the Holodeck impleFor comparisons between Holodeck and mentation. FirePlace, we use the same constraint solver (with the same parameters) to solve constraints created by both methods. As mentioned in the paper, experiments on FirePlace, LayoutGPT, Holodeck all use the same MLLM (Gemini 1.5Pro) for fair comparison. 15. Evaluation Dataset All scenes used for evaluation of our method and the baselines are in Universal Scene Descriptor (USD) format, which contains meshes of all objects, architectural elements and photorealistic materials. When choosing the transformable objects for each placement task, our goal was to choose objects for which the correctness of its final position depends on the successful identification of anchor objects and the relevant constraints. As such, the transformable objects selected for our evaluation placement tasks are often furniture pieces (chairs, tables, refrigerators ...etc) and decorative items (books, picture-frames, wall art...etc). Figure 10 shows the distribution of the number of possible anchor objects in each of the placement tasks composed of architectural elements and furniture/household objects. This motivates the need for Batched Visual Selection to make the visual selection task easier by breaking down the decision process into multiple stages. 16. Ablations 16.1. Qualitative Examples of Ablations The ablations tested in Table 3 have qualitative consequences on the placements that get generated. Figure 11 and Figure 12 show this for two placement tasks. Note that Figure 10. Distributions of the number of objects (furniture and architectural elements) within the placement tasks used for evaluation. in both cases, the transformable object must be placed into shelf-like object, and that its crucial to have access to the low-level geometry, which bounding box representation do not provide ( Geometry). In both cases, purely using the MLLM to choose among randomly generated placements leads to suboptimal placements, oftentimes creating final placements that feature the object floating in air (e.g. 5 Figure 11. An example of the effects of ablations in Table 3 on the placements. In this example, FirePlace is tasked to place the coat into the closet. Figure 13. Increasing the batch size within Batched Visual Selection leads to lower performance on placement tasks. Metric LayoutGPT Holodeck Ours Min L2 error cm () Mean l2 error cm () Visibility score () Plausibility score () Energy score () 126.19 166.11 0.69 2.31 91.25 136.51 0.59 1.99 0.17 43.69 68.84 0.88 2.92 0.38 Table 4. Comparison with Baseline for image inputs Figure 12. An example of the effects of ablations in Table 3 on the placements. In this example, FirePlace is tasked to place the bottle on the cabinet. Constraints in Figure 12 shows the bottle floating slightly above the ground). We can also see that plausibility pruning tends to get rid of implausible overlaps that may happen when placing the object according to raw geometric constraints in both Figure 11 and Figure 12, removing plausibility pruning ( Com. Sense) leads to final placements that overlap with assets already in the scene. Finally, removing the ability to visually select anchor objects (as opposed to selecting anchor objects based on text annotations) and removing the ability to scale inference compute for Batched Visual Selection both lead to incorrect placements, due to the wrong anchor object/surfaces being selected for the constraints. 16.2. Inference Compute Scaling for Batched Visual"
        },
        {
            "title": "Selection",
            "content": "Figure 13 displays the trends on the performance metrics as we increase the batch size (lower the level of inference compute) used by Batched Visual Selection. This means that for the selection process of anchor objects and interaction surfaces, an MLLM must choose among larger sets of options at time. We can observe downward trend in plausibility and energy scores (due to incorrectly selected object instances and and interaction surfaces), and also an upward trend in both mean and minimum L2 errors, suggesting that the resultant placements become further away from the groundtruth as MLLMs are prompted to choose among more and more visual options at time. Our default settings uses batch size of 3, and for Figure 13, we increase the batch size to 6, 20, 50 and 100. 6 Figure 14. Qualitative results of object placement when FirePlace is given image inputs of placement examples. Note how generated placements follow the semantics of object placements shown in the input image to varying degrees, but can vary in their final positions. 17. Performance on Image Inputs Since FirePlace uses an MLLM for constraint outline generation, an additional input modality that we can demonstrate besides language annotations is an image example. Given an image showing one possible placement of the object, FirePlace can generate variations of placements that are semantically similar in the sense that output placements capture the underlying constraints and placement considerations of the image example. Table 4 shows that our method also outperforms the baselines across the metrics on this task. Figure 14 shows object placements generated from our system when given different placement examples. For this experiment, the prompts of our method, Holodeck, and LayoutGPT are changed accordingly to insert the image example instead of text prompt, as done for the experiments in the main paper. derings (image and B) generated by our method and the two baselines. The objective (text input or image input) is also provided to the MLLM. The prompt used is shown below: Between Image and Image B, which is better match to the objective, in terms of its placement of the object masked in red? Describe the scene and describe the object masked in red (what is it? Where should the object be according to the objective?), then respond with first or second in json: json { \"final_answer\": \"A\"/\"B\" 18. Using MLLMs to Compare FirePlace to"
        },
        {
            "title": "Baselines",
            "content": "} In addition to the human evaluations that indicate FirePlaces superior performance, we can also use MLLMs to do pairwise comparisons, by giving it shuffled pairs of renThe results generated with Gemini 1.5Pro for text inputs is shown in Table 6 and the result for image inputs (See Section 17) is shown in Table 5, showing FirePlaces superior 7 vs. LayoutGPT vs. Holodeck"
        },
        {
            "title": "LayoutGPT wins\nHolodeck wins\nOurs wins",
            "content": "- 0.44 0.72 0.56 - 0.72 Table 5. Win-rate comparison between our method and LayoutGPT and Holodeck according to Gemini 1.5Pro as judge, for placement tasks with image input. vs. LayoutGPT vs. Holodeck"
        },
        {
            "title": "LayoutGPT wins\nHolodeck wins\nOurs wins",
            "content": "- 0.46 0.70 0.54 - 0.72 Table 6. Win-rate comparison between our method and LayoutGPT and Holodeck according to Gemini 1.5Pro as judge, for placement tasks with text input. performance over both baselines in both task settings. 8 Figure 15. The constraints and interaction surfaces generated for the task of mounting TV. For clarification, the contact constraint is enforced between the back of the TV and the wall (visualized as plane). Note that there are multiple wall meshes within this scene (for instance, see Figure 9 blue in Batch 3 and cyan in Batch 2 are both alternatives), and that Batched Visual Selection chooses the correct one Figure 16. To place the small table into the scene, the constraints generated first identifies Contact constraint between the bottom of the table and the floor, then uses various CloseTo constraints to capture its rough position in the room - the final position generated is close to all 3 surfaces chosen. 9 Figure 17. Similar to Figure 17, FirePlace enforces contact constraint with the floor, then uses CloseTo constraints to restrict plausible placements. Figure 18. Placing piano into the room. FirePlace successfully discerns that the left and the right of the piano must be close to different things, and that the right side should be closer to the left of the curtains, and the left should be closer to the bed. 10 Figure 19. Placing the lamp in the living room is done by locating near-by objects and enforcing CloseTo constraints. Figure 20. The desk is L-shaped, meaning that in order to insert place the desktop, it must correctly extract the top of the desk, and not just use the top face of the desks bounding-box. We see here that it enforces Contact and NoOverhang constraints to the table, guides the placement according to other CloseTo constraints. 11 The desired placement of the object masked in red can be described by: [Placement text prompt] The predicted placement of the object is shown in the predicted image below: [Masked rendering of predicted final placement, with placed object in semi-transparent red mask.] Please grade the predicted placement of the predicted placement of the object masked in red on scale of 1-4, where: 1 = either the target object is not observed at all within the scene, or the placement is physically implausible (e.g. the object is floating in the air or intersecting with other objects) 2 = The object is observed within the scene, but its placement differs substantially from the placement observed in the reference image. 3 = The object is observed within the scene, and its placement is sensible (e.g . the target object is NOT floating in air or intersecting with other objects), and the placement of the target object is different from the reference image, but not in substantial ways. 4 = totally sensible placement of the target object, and captures both valid physics (e.g. the target object is NOT floating in air or intersecting with other objects) as well as considerations for function, accessibility, and aesthetics. Please reason about what is in Describe the scene and describe the object masked in red (what is it? Where should the object be according to the objective?), then respond with the final score in json: json { \"final_answer\": 1/2/3/4 } Figure 21. Prompt for extracting plausibility scores. * Contact (surface1, surface2) Enforces that surface1 and surface2 are in contact with each other. * FarFrom (surface1, surface2, const) This enforcs that surface1 and surface2 are at AT LEAST some distance away from eachother specified by const, float in CENTIMETERS in the lifesize 3D scene. * CloseTo (surface1, surface2, const) This enforcs that surface1 and surface2 are AT MOST some distance away from eachother specified by const, float in CENTIMETERS in the life-size 3D scene. * Parallel (surface1, surface2) This enforces that surface1 and surface2 are parallel. * Above (surface1, surface2) This enforces that surface1 is ABOVE surface2 (note the order.) This does NOT ensure that the birdseye view of object1 and object2 overlap. * NoOverhang (surface1, surface2) This enforces that surface1s vertical projection is entirely contained in surface2s projection. This is good when youd like one surface to be entirely contained within another, for instance for physical stability. Also, if you ever want to ensure contact with floor, make sure to use NoOverhang as well. Figure 22. Prompt for constraint documentation [Source layout rendering] The following text describes the target object that we would like to place in the image above: [Placement text prompt] In each of the reference images below, target object is masked in red. Even though this may be different from the target object just mentioned and even though the scene may be different, the placements of the object shares underlying patterns and structure across the scenes. You are given library of the following functions: above: [Constraint library doc string] For the {item_to_focus_on}, return json that has list. Each element of the list specifies: 1) in the field called constraints: the constraint function (e.g. Parallel) 2) in the field called surface1 string describing surface1 -- if its surface on the target object (or collection of objects), refer to it as the target object. (e.g. bottom of the target object) 3) in the field called surface2 string describing surface2 -- if its surface on the target object (or collection of objects), refer to it as the target object. (e.g. top of the table in the corner of the room) Return json marked by json at the very beginning, with list of ALL of the constraints that is relevant to the {item_to_focus_on}. NOTE: since none of the objects are floating in the scene, ALWAYS start off the list with CONTACT constraint. What is the target object in contact with? Which surface of the target object is in contact with something else? NOTE: specify distance relations between objects using the constraints CloseTo and FarFrom, by spcifying which surfaces on the target object and the surounding objects are relevant to these distance relations. For instance, if the target object is potted plant at the corner of the room, you would use the CloseTo distance relation with respect to one side (left or right) of the plant, the surface of the correct wall, and another CloseTo distance relation between the back side of the plant and the other wall forming the corner. Alternatively, if the target object is something mounted on the wall above something else, it would make sense to specify FarFrom constraint between the bottom surface of the target object and the surface of the object underneath. Note that in both cases, you must specify threshold distance in Centimeters. Use the assumption that the scene is real-life sized in 3D. You are allowed to return an empty list if nothing is relevant to the description of the desired physical relation above. Figure 23. Prompt for constraint outline generation 14 The following text describes the target object to be placed into the scene: [Placement text prompt] The target object is related to another anchor object within the scene by the physical relation: [Constraint outline] The surface of that anchor object in the physical relation can be described by: [constraint outline] We refer to this object as the anchor object. In the following image, want you to find the anchor object among the segmentation masks show in different colors. What is the color of segmentation mask of the the anchor object in the following image?[ concat color names /none] [Insert masked rendering of anchor objects] want you to reason about it, then output json, like: json {final_answer: \" concat color names /none\" } respond with none if none of the segmentation masks in the image match the anchor masked object. Figure 24. Prompt for extracting anchor objects 15 The following image shows our \"anchor\" object in semi-transparent red mask. [insert masked rendering of anchor object] Heres 3D plot of the [ANCHOR/TARGET] object in its canonical space. In this plot: The direction upwards is described by the positive direction. (down is negative) the direction right is described by the positive direction. (left is negative) the direction forwards id described by the negative direction. (backwards is positive) [Insert 3D plot of ANCHOR/TARGET object ] And the following text describes the target object: [Placement text prompt] Youve decided the following must be true: [Constraint outline] Think about the [ANCHOR/TARGET] object. Which object does that correspond to in the above? Think about the surface where the interaction between the two objects is happening. Which way is it pointing on the [ANCHOR/TARGET] object? Id like you now to tell me how you would extract the relevant surfaces from our [ANCHOR/TARGET] object relevant for the placements described above. The definition of relevant surface is one that is involved in physical constraints. If something is supposed to be to the RIGHT of this object, then it makes sense that surface pointing to the right is relevant, since that surface can be usd to judge whether the object is truly to the right of the object. If the anchor object were table, and the target object is to be put onto the table then the relevant surface of interaction for the ANCHOR OBJECT is pointing UPWARDS. The surface of interaction for the TARGET OBJECT is pointing downwards. The surface you extract does NOT NEED TO BE in physical contact with the other object. For instance, if distance is to be maintained between two objects, think which surface is most relevant in that distance calculation. want you to reason about it, then output json to specify the direction of the surface in the [ANCHOR/TARGET] object relevant to the interaction, like : json {final_answer: up/down/left/right/front/back} You cannot return the word none. Figure 25. Prompt for extracting object surface directions. 16 The following image shows our \"anchor\" object in semi-transparent red mask. [Insert masked rendering of ANCHOR object] Heres 3D plot of the [ANCHOR/TARGET] object in its canonical space. In this plot: The direction upwards is described by the positive direction. (down is negative) the direction right is described by the positive direction. (left is negative) the direction forwards id described by the negative direction. (backwards is positive) [Insert 3D plot of ANCHOR/TARGET object] And the following text describes the target object: [Placement text prompt] Youve decided the following must be true: [Constraint outline] Think about the [ANCHOR/TARGET] object. Which object does that correspond to in the above? Think about the surface where the interaction between the two objects is happening. Which surface ( [LIST OF COLORS] ) is relevant to the interaction? want you to reason about it, then output json to specify the the surface in the [ANCHOR/TARGET] object relevant to the interaction, like: json {final_answer: \" [LIST OF COLORS] /none\" } Figure 26. Prompt for choosing object surfaces based on the color of the surface in the visualization. 17 The following image shows our \"anchor\" object in semi-transparent red mask. [Insert masked rendering of ANCHOR object] Heres 3D plot of the [ANCHOR/TARGET] object in its canonical space. In this plot: The direction upwards is described by the positive direction. (down is negative) the direction right is described by the positive direction. (left is negative) the direction forwards id described by the negative direction. (backwards is positive) [Insert 3D plot of ANCHOR/TARGET object] And the following text describes the target object: [Placement text prompt] Youve decided the following must be true: [constraint outline] In order to enforce this constraint, theres few parameters you must specify. Use your visual judgment to determine the best values for the following parameters: [Arg documentation string for each constraint function] For the target object, you chose the surface visualized below: [Colorized visualization of the target object and the interaction surface chosen] For the anchor object, you chose the surface visualized below: [Colorized visualization of the anchor object and the interaction surface chosen] What approximation of the parameters make the most sense, given the surfaces that youve chosen, and the constraint youve chosen to enforce? Return json with each value specified in the json. Begin the json with json. Figure 27. Prompt for estimating continuous parameters 18 You are an experienced room designer. Please help me arrange objects in the room by assigning constraints to each object. Here are the constraints and their definitions: 1. distance constraint: 1) near, object: near to the other object, but with some distance, 50cm < distance < 150cm. 2) far, object: far away from the other object, distance >= 150cm. 2. position constraint: 1) in front of, object: in front of another object. 2) around, object: around another object, usually used for chairs. 3) side of, object: on the side (left or right) of another object. 4) left of, object: to the left of another object. 5) right of, object: to the right of another object. 6) behind of, object: behind another object. 7) in front of, object: in front of another object. 8) ontop of, object: on top of another object. 3. direction constraint: 1) face to, object: facing another object. 4. alignment constraint: 1) center aligned top, object: align the center of the object with the center of the TOP of another object. 2) center aligned front, object: align the center of the object with the center of the FRONT of another object. 3) center aligned side, object: align the center of the object with the center of the SIDE of another object. Figure 28. Prompt for Holodeck (Part 1) For part 2, see Figure 29. 19 For each object, you can select various numbers of constraints and any combinations of them and the output format must be: object constraint 1 constraint 2 ... For example: coffee table-0 near, sofa-0 in front of, sofa-0 center aligned front, sofa-0 face to, sofatv stand-0 far, coffee table-0 in front of, coffee table-0 center aligned front, coffee table-0 face to, coffee table-0 desk-0 far, tv stand-0 chair-0 in front of, desk-0 near, desk-0 center aligned front, desk-0 face to, desk-0 floot lamp-0 near, chair-0 side of, chair-0 Here are some guidelines for you: 1. The objects of the *same type* are usually *aligned*. 2. When handling chairs, you should use the around position constraint. Chairs must be placed near to the table/desk and face to the table/desk. In the above examples, \"coffe table-0\", \"sofa-0\", \"tv stand-0\", \"desk-0\", \" chair-0\", \"floot lamp-0\" are all object IDS in the scene. In reality, the object IDs look more like \"object-0\", \"object-1\", ... i.e. object-1 in front of, object-2 near, object-2 face to, objectHere is list of preexisting objects in the scene, in the format [object_id]: [object_description] [Descriptions and labels of preexisting objects in the scene] Here is the object that want to place in the room (object id of it is objecttarget): [Description of transformable object] Please first use natural language to explain your high-level design strategy, and then follow the desired format *strictly* (do not add any additional text at the beginning or end) to provide the constraints for the object. Figure 29. Prompt for Holodeck (Part 2) For Part 1, see Figure 28. 20 Instruction: synthesize the 3D layout of an indoor scene. The generated 3D layout should follow the CSS style, where each line starts with the furniture category and is followed by the 3D size, orientation and absolute position. Formally, each line should follow the template: FURNITURE {length: ?cm; width: ?cm; height: ?cm; left: ?cm; top: ?cm; depth: ? cm; orientation: ? degrees;} All values are in cm but the orientation angle is in degrees. Here are the info of other objects within the scene: [List of objects inside the scene, colon separated from their width, height, left, top, depth and orientation information.] Generate line for an object described by the following: [Description of transformable object] Figure 30. Prompt for LayoutGPT"
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Stanford University"
    ]
}