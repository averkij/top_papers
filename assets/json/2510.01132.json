{
    "paper_title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
    "authors": [
        "Ruiyi Wang",
        "Prithviraj Ammanabrolu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro"
        },
        {
            "title": "Start",
            "content": "Preprint. Under Review. PRACTITIONERS GUIDE TO MULTI-TURN AGENTIC REINFORCEMENT LEARNING Ruiyi Wang University of California, San Diego {ruiyiwang}@ucsd.edu Prithviraj Ammanabrolu University of California, San Diego, NVIDIA {prithvi}@ucsd.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "We study what actually works and what doesnt for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillarsenvironment, reward, and policyand empirically derive recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agents policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given fixed budget. We distill these findings into training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro 5 2 0 2 1 ] . [ 1 2 3 1 1 0 . 0 1 5 2 : r Figure 1: Illustration of multi-turn agentic RL and the key research questions. 1 Preprint. Under Review."
        },
        {
            "title": "INTRODUCTION",
            "content": "Training LLMs as autonomous agents to navigate open-ended environments presents unique challenges: planning across extended horizons, making multi-turn sequential decisions, and optimizing for multi-turn rewards. The transition from static single-turn problem-solving to dynamic multistep reasoning is essential for agentic benchmarks such as interactive text and embodied simulations (TextWorld (Cˆote et al., 2018), ALFWorld (Shridhar et al., 2021), etc.), real-world software programming (OSWorld (Xie et al., 2024), SWE-gym (Pan et al., 2025), etc.), and abstract reasoning in novel situations (ARC-AGI (Chollet et al., 2025)). However, existing multi-turn RL implementations vary widely: some refer to tool-augmented single queries as multi-turn (Zeng et al., 2025), while many rely on model-based assumptions (Wang et al., 2025). This fragmentation has led to incomparable results across papers and confusion about what constitutes true multi-turn learning versus pseudo-multi-turn adaptations of single-turn methods. This paper aims to facilitate research efforts on the open research question: What factors are practically important in making multi-turn RL for LLM agent learning work. Motivated by the lack of standardization of multi-turn RL approaches, we systematically decompose the design space into three interdependent pillarsenvironment, reward, and policyand empirically derive recipe for training LLM agents in situated textual domains (Figure 1). We evaluate our approach on TextWorld and ALFWorld for embodied reasoning, and SWE-gym for real-world programming, revealing critical insights for each pillar. For the environment, we investigate how performance scales with environment complexity and task diversity, and how agents generalize across different environments and tasks. For the policy, we investigate how model priors affect continual multi-turn RL training and analyze the interplay between multi-turn imitation learning (SFT) and multi-turn RL. We further compare biased (PPO, GRPO) and unbiased (RLOO) policy gradient RL algorithms to isolate benefits from algorithmic heuristics. For the reward, we experiment with varying densities of per-turn rewards to understand their impact on training. The experiments show that our recipe works across textual reasoning, situated embodied reasoning, as well as software engineering tasks. The key findings from our analysis are: (1) Multi-turn RL performance scales with environment complexity across spatial, object, and solution dimensions; (2) Agents trained on simpler environments show promising generalization to complex ones; (3) Multitask training enhances multi-turn RL performance; (4) Model priors from minimal demonstrations accelerate convergence, but RL remains essential for generalization; (5) An optimal SFT:RL ratio exists that balances task accuracy and generalization under fixed budgets; (6) Both biased and unbiased algorithms achieve stable learning, validating that gains stem from our multi-turn formulation rather than algorithmic heuristics; (7) Dense turn-level rewards accelerate multi-turn RL training compared to sparse rewards, but require algorithm-specific tuning. These insights yield concrete multi-turn RL recipe that guides co-design across all three pillars. We demonstrate that multi-turn RL with LLMs is not simply an extension of single-turn optimization but requires fundamental rethinking across environment, policy, and reward. To facilitate future research, we will release our multi-turn agentic RL framework built on veRL (Sheng et al., 2025), including the training scripts and model weights of the experiments in this paper. This work provides both theoretical insights and practical guidelines for developing agentic AI systems capable of operating effectively in real-world interactive environments."
        },
        {
            "title": "2 RELATED WORK",
            "content": "While single-turn RL methods for LLMs including PPO (Schulman et al., 2017), RLOO (Ahmadian et al., 2024), GRPO (Shao et al., 2024), and DAPO (Yu et al., 2025) have been extensively optimized for immediate response quality, adapting them to multi-turn agentic scenarios remains non-trivial. These methods assume rewards directly follow individual actions, but multi-turn environments only reveal outcomes after extended interaction sequences, breaking the action-reward coupling that single-turn methods rely upon. Existing efforts on multi-turn RL have made limited progress on these challenges. Some approaches construct multi-turn scenarios by interleaving tool-use or reasoning steps for single-turn QA pairs (Zeng et al., 2025; Dong et al., 2025). Others who work on true interactive environments either rely on sparse terminal rewards without turn-level learning signals (Wang et al., 2025), or assign turn-level advantages uniformly across sequence to2 Preprint. Under Review. kens without fine-grained credit assignment (Zhou et al., 2025). More importantly, there lacks comprehensive understanding of how the three fundamental pillars of RL environment, policy, and reward jointly determine performance in multi-turn interactive environments. This paper provides systematic analysis on how the fundamental pillars of RL impact multi-turn RL training, respectively, and concludes insights on how to practically train multi-turn RL in different interactive agentic environments. Throughout the paper, we dedicate related works in individual sections."
        },
        {
            "title": "3 MULTI-TURN AGENTIC REINFORCEMENT LEARNING",
            "content": "We formulate multi-turn agentic tasks as Partially Observable Markov Decision Process (POMDP) problem, defined as tuple (S, A, , R, Ω, O, γ). Taking the Textworld task (Cˆote et al., 2018) as an example, an agent takes the action at (go south) sampled from the action space and receives text observation ot (You are in front of garden) from the observation space Ω. ot is partial description of the true state st in the hidden state space which contains the complete state world model. We assume that the state transition function : is deterministic. Upon taking an action, the agent also receives scalar reward rt = R(st, at). The agents objective is to learn policy that maximizes the expected discounted sum of rewards E[(cid:80) We denote the trajectory history consisting of task prompt u, action and state sequences by ht = (u, s0, a0, s1, a1, , st)1. An LLM agent with policy πθ samples an action sequence at πθ(ht) based on the trajectory history. at is token sequence in natural language: (a1 ), with each token ai ). Agentic environments execute language commands only upon completion, naturally defining the reward structure at the command boundaries, marked by <eos> tokens. Therefore, we assign scalar reward rt at aeos , and the reward for each action generated as πθ(ht, a<i , ..., ant γt rt]. , aeos , t token is formulated as: ri = (cid:26)rt 0 = <eos> if ai otherwise . We make sure only action tokens contribute to the loss by masking out all state tokens. Here is concrete example: the input to the LLM during the rollout stage using chat template is: <im_start>user Your task is: {task prompt}. state: {state 0} your action:<im_end> <im_start>assistant {action 0}<im_end> ... <im_start>user state: {state t} your action:<im_end> <im_start>assistant The LLM of policy πθ generates the output {action t}<im end>. The environment handles state transition and reward computation: next state, reward, done = env.step(state, action). The reward for each turn is assigned to the <im end> token. The action and next state are then appended to the chat history under the template."
        },
        {
            "title": "4 BACKGROUND AND EXPERIMENTAL SETUP",
            "content": "Our experiments systematically investigate how three fundamental pillars of RL impact multi-turn agentic RL performance. For Environment (5), we examine how scaling environment complexity affects learning across spatial, object, and solution dimensions (5.1), evaluate whether agents trained on simpler environments generalize to complex ones (5.2), and analyze how task diversity impacts training and generalization (5.3). For Policy (6), we analyze how the model priors from demonstration data influence RL convergence and identify optimal ratios of SFT-to-RL data ratios under budget constraints (6.1). We compare biased algorithms (PPO, GRPO) against unbiased algorithms (RLOO) to isolate benefits of algorithmic design versus our multi-turn formulation (6.2). For Reward (7), we investigate how reward density, the frequency of feedback signals during trajectories, affects training and final performance (7.1). These experiments demonstrate that multi-turn RL requires careful co-design across all components rather than naive extensions of single-turn methods. 1We substitute observation for state for simplicity. The agent has no access to the true state of the game. 3 Preprint. Under Review. Tasks and Environments. We evaluate on three situated textual benchmarks: TextWorld (Cˆote et al., 2018) and ALFWorld (Shridhar et al., 2021) for language grounding in situated embodied reasoning, and SWE-Gym (Pan et al., 2025) for real-world software engineering. We extend veRL (Sheng et al., 2025)s efficient RL training infrastructure, integrating these benchmark environments through standard step and reset interfaces. Critically, unlike traditional RL settings that provide both observations and admissible action lists (reducing the problem to action selection), our agents must generate executable natural language commands from environment observations alone, without action hintstesting their ability to discover valid actions through exploration. The details of tasks and environments are provided in Appendix B. TextWorld: text adventure game environment where agents navigate rooms, manipulate objects, and solve quests via natural language. We procedurally generate tasks with controlled complexity across three dimensions: world size (w), number of objects (o), and quest length (q). For example, w2-o3-q4 denotes 2 rooms, 3 objects, and 4-step quest. Each task is uses unique seeds for diversity. ALFWorld: An embodied household environment requiring multi-step task completion through text interaction. We use the text-only variant spanning six task categories, training on the train split and evaluating on the valid unseen split. SWE-Gym: Real-world programming tasks including bug fixes and feature implementation. We focus on 5 representative task types: getmoto, pydantic, mypy, pandas, and iterative dvc, randomly sampling 90 training and 25 evaluation instances. Training and Evaluation. We use Qwen2.5-1.5B-Instruct, Qwen2.5-7B-Instruct2, and Qwen38B3 as base models (abbreviated as Qwen-1.5B, Qwen-7B, and Qwen-8B), training with PPO (Schulman et al., 2017), GRPO (Shao et al., 2024), and RLOO (Ahmadian et al., 2024) algorithms. Maximum iteration steps and token limits scale with task complexity. During rollout generation, we use temperature 0.7 to balance exploration-exploitation. We evaluate agents on heldout test sets, reporting task success rate as our primary metricthe percentage of episodes where agents achieve objectives within exploration budgets. For SWE-Gym, we report test suite passing ratios. Full training details and hyperparameter tuning analysis appear in Appendices and A."
        },
        {
            "title": "5 ENVIRONMENT",
            "content": "The environment fundamentally determines the challenges agents must overcome. While single-turn tasks primarily measure reasoning difficulty, multi-turn environments introduce dimensions such as spatial navigation, object manipulation, and extended planning horizons. We investigate three core research questions for practical multi-turn deployment: (1) How does environment complexity affect multi-turn RL training efficiency? This determines exploration budgets and model size requirements for tasks with varied environment complexities. (2) Can agents trained on simpler environments generalize to complex ones? This addresses the key consideration to scaling up agentic systems. (3) How does task diversity impact training and generalization? We not only show our multi-turn RL recipe that works for TextWorld generalizes to ALFWorld and SWE-Gym, but also reveal whether agents learn generalizable skills or memorize task-specific behaviors. Our experiments demonstrate that multi-turn RL enables effective knowledge transfer across environments and diverse tasks. Setup. We design two experimental conditions to vary environment complexity and task diversity. For environment complexity, starting from the base configuration w2-o3-q4, we create controlled variations: w8-o3-q4 (increased spatial complexity), w2-o12-q4 (increased object complexity), w8o12-q4 (increased both dimensions), and w4-o6-q8 (proportional scaling across all dimensions). For task diversity, we construct mixtures of increasing heterogeneity for training. In ALFWorld, we test single type (pick & place only), 4 types (heat, cool, cook, examine) and all 6 types (adding pick two & place). In SWE-Gym, we compare single type (getmoto only) against all 5 types (getmoto, pydantic, mypy, pandas, and iterative dvc). All mixtures have the same data size to ensure fair comparison, with evaluation on both single-type and mixed-types held-out test sets. We train Qwen1.5B, Qwen-7B, and Qwen-8B models using PPO and GRPO with consistent hyperparameters. Full task specifications and training details appear in Appendices B.1 and C.1. 2https://huggingface.co/Qwen/Qwen2.5 3https://huggingface.co/Qwen/Qwen3-8B 4 Preprint. Under Review."
        },
        {
            "title": "5.1 HOW DOES MULTI-TURN RL PERFORMANCE SCALE WITH ENVIRONMENT COMPLEXITY?",
            "content": "Table 1 reveals that base models struggle dramatically as environment complexity increases, with performance dropping from 17% to just 3% when both spatial and object dimensions are scaled. Critically, multi-turn RL improvements diminish with increasing complexitywhile PPO achieves 88% improvement on the base environment, this drops to 51% on the most complex setting. Notably, object complexity proves more challenging than spatial complexity, suggesting that manipulating and tracking multiple objects across turns presents fundamentally harder challenges than spatial exploration in situated textual domains. When proportionally scaling all environment parameters, Table 2 shows that doubling all dimensions creates dramatically harder problems beyond linear scaling. The base Qwen-1.5B models performance collapses from 15% to 1%, indicating exponential search space expansion. While multi-turn PPO on Qwen-1.5B achieves substantial gains (58%), the final 59% success rate falls well short of the 80% achieved on w2-o3-q4. Performance also scales with model sizethe 7B model reaches 72% success on w4-o6-q8, demonstrating that larger models better handle the increased state spaces of complex environments. Notably, even the 1.5B model shows strong learning potential with 65% and 58% gains on w2-o3-q4 and w4-o6-q8 respectively. Table 1: Multi-turn PPO performance on TextWorld tasks with varying complexity dimensions. Base environment: w2-o3-q4. Maximum steps: 12. Tasks w/ varying env complexity Qwen-1.5B Qwen-1.5B (PPO) w2-o3-q4 (base env) w8-o3-q4 (4x rooms) w2-o12-q4 (4x objects) w8-o12-q4 (4x objects & rooms) 0.17 0.07 0.08 0. 0.880.71 0.680.61 0.540.46 0.510.48 Table 2: Multi-turn PPO performance comparison across model sizes on proportionally scaled TextWorld environments. Maximum steps: 12 (w2-o3-q4) and 24 (w4-o6-q8). Tasks Qwen-1.5B Qwen-1.5B (PPO) Qwen-7B Qwen-7B (PPO) w2-o3-q4 w4-o6-q8 0.15 0. 0.80.65 0.590.58 0.65 0.28 0.980.33 0.720.44 Table 3: Multi-turn PPO performance on TextWorld w2-o3-q4 tasks with different exploration sizes. #Exploration steps Qwen-1.5B Qwen-1.5B (PPO) 6 (1.5 optimal) 8 (2 optimal) 12 (3 optimal) 16 (4 optimal) 0.05 0.09 0.15 0. 0.550.5 0.730.64 0.80.65 0.880.71 Lastly, we investigate how the exploration budget (the maximum steps agents can take during rollout) affects learning. For w2-o3-q4 tasks with 4-step optimal solutions, we vary maximum steps from 6 to 16. Table 3 shows that performance saturates beyond 8 exploration steps. Constraining agents to 6 steps (1.5 optimal) limits PPO to 55% success, while 8 steps (2 optimal) yields 73% success. Further increases to 12 and 16 steps produces only marginal gains. These results indicate that while insufficient exploration severely limits learning, budgets beyond 2 optimal provide negligible benefits for TextWorld tasks. 5.2 HOW DOES MULTI-TURN RL GENERALIZE TO ENVIRONMENTS WITH DIFFERENT COMPLEXITIES? To investigate whether multi-turn RL learns transferable skills, we evaluate cross-environment generalization. We train single-task models on w2-o3-q4, w8-o3-q4, and w2-o12-q4, plus mixedcomplexity model on equal proportions of w2-o12-q4 and w8-o3-q4, maintaining constant total RL Preprint. Under Review. data across conditions. Table 4 reveals that agents trained on simpler environments achieve substantial generalization to more complex ones, evidenced by the model trained on w2-o3-q4 that improves performance across all higher-complexity environments. Transfer is particularly strong from w8-o3-q4 (increased spatial complexity), which achieves the largest average improvements across targetsnotably improving w8-o12-q4 by 48%, matching the 48% gain from training solely on w8-o12-q4. These results demonstrate that multi-turn RL acquires reusable skills like spatial exploration and object manipulation that transfer across complexity levels. Table 4: Cross-environment generalization with multi-turn PPO on TextWorld tasks. All models trained with 5000 episodes/epoch (mixed-task: 2500 episodes each). Tasks w2-o12-q4 w8-o3-q4 w8-o12-q4 w4-o6-q8 w2-o3-q4 w8-o3-q4 w2-o12-q4 w2-o12-q4 + w8-o3-q 0.40.32 0.50.42 0.540.46 0.410.33 0.510.44 0.680.61 0.270.2 0.520.45 0.270.24 0.510.48 0.270.24 0.340.31 0.120.11 0.210.2 0.130.12 0.170.16 5.3 HOW DOES MULTI-TURN RL GENERALIZE TO DIFFERENT TYPES OF TASKS WITHIN DOMAIN? In 5.1 and 5.2, we examined how agents handle varying environment complexities within single task types. Here we address broader question: does our multi-turn RL recipe works for complex situated environments like ALFWorld and real-world scenarios like SWE-Gym? Moreover, can agents trained on task subsets generalize to full task distributions? We investigate how training on diverse task types affects generalization across two domains. In ALFWorld, different tasks require distinct physical skillscleaning involves finding and placing objects, while heating requires operating appliances in specific sequences. In SWE-gym, tasks span diverse software engineering challenges from fixing pydantic issues to resolving pandas problems. Table 5 and 6 show that multi-turn RL successfully applies to challenging ALFworld and SWEGym environments. Remarkably, agents trained on single task types achieve decent generalization to unseen task types, with 12% (ALFWorld) and 7% (SWE-Gym) improvements across all task types from single-type training alone. This indicates that multi-turn RL learns transferable skills even from limited task exposure. Surprisingly, multi-task training benefits seemingly unrelated tasksagents trained on clean, heat, cool, and cook mixtures outperform single pick & place specialists by 19% on the single task itself and 21% on all-type evaluation. The results demonstrate that multi-turn RL develops generalizable skills that transfer across diverse objectives. Table 5: Multi-turn PPO performance on ALFWorld unseen tasks, trained on various task mixtures. Task mixture for training Tested on single type (PPO) Tested on all types (PPO) Single type of tasks Mixture of 4 types of tasks Mixture of 6 types of tasks (all) 0.630.19 0.820.38 0.760.32 0.590.12 0.80.33 0.740.27 Table 6: Multi-turn GRPO performance on SWE-gym tasks, trained on various task mixtures. More comments on the use of GRPO algorithm are presented in 6.2 Task mixture for training Tested on single type (GRPO) Tested on all types (GRPO) Single type of tasks (getmoto) Mixture of 5 types of tasks (all) 0.280.19 0.370.28 0.110.07 0.220."
        },
        {
            "title": "6 POLICY",
            "content": "The choice of RL optimization algorithm and model initialization critically determines multi-turn RL performance. First, we examine the role of demonstration data. In practice, access to human 6 Preprint. Under Review. demonstration data for supervised fine-tuning (SFT) may be limited. We ask: (1) What prior model policy enables effective multi-turn RL? This addresses whether expensive human demonstrations (2) Given fixed data budget, what are necessary if agents can learn effectively from scratch. is the optimal ratio of SFT to RL data? This determines how to best allocate limited resources between demonstration collection and online learning. Second, we investigate whether RL optimization choices significantly impact multi-turn RL training. We compare heuristic policy gradient methods (PPO (Schulman et al., 2017), GRPO (Shao et al., 2024)) against unbiased methods (RLOO (Ahmadian et al., 2024)) to isolate the contributions of our multi-turn formulation from specific optimization heuristicsessential for making rigorous claims about algorithmic improvements, as demonstrated in recent work on the pitfalls of heuristic-dependent results (Oertell et al., 2025). Setup. TextWorld provides gold solutions for each procedurally generated game, which we use as demonstration data for SFT, representing human multi-turn trajectories. The SFT data follows turnbased chat format, the default template used in most instruction-following scenarios. We generate SFT data with different random seeds than RL data to prevent leakage. We train Qwen-1.5B, Qwen7B, and Qwen-8B models using PPO, GRPO, and RLOO with consistent hyperparameters across experiments. Full task specifications and training details appear in Appendices B.2 and C.2. Multi-turn PPO Formulation. For optimization algorithms with advantage estimation, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), we adopt token-level credit assignment. We compute token-level values and apply to TD error as δi t) where is the history up to and including token ai hi t. Then we estimate the advantage for each token using GAE: ˆAi l=0 (γλ)lδi+l , where is the horizon (number of tokens until episode ends). Even though only <eos> tokens receive rewards, all preceding tokens get non-zero advantages through value bootstrapping. Therefore, the Clipped Surrogate Objective for all tokens in the trajectory can be written as: + γV (hi+1 = (cid:80)Li ) (hi = ri LCLIP (θ) = Eτ πθ (cid:34) (cid:88) nt+1 (cid:88) (cid:16) min t=0 i=1 t(θ) ˆAi ri t, clip(ri t(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:35) (cid:17) where the probability ratio for each token is: ri t(θ) = πθ(ai πθold (ai tht, a<i ) tht, a<i ) . 6.1 HOW DOES PRIOR MODEL POLICY INFLUENCE MULTI-TURN RL TRAINING? We distinguish between model priors (initial policies from SFT) and continual training (subsequent multi-turn online RL). This two-stage approach mirrors real-world deployment where agents learn from demonstrations before online learning. Training SFT priors on golden solutions from w2o3-q4 TextWorld reveals that multi-turn RL with good imitation priors achieves comparable performance with dramatically fewer RL episodes. Table 7 shows that an SFT prior trained on 60 demonstrations plus 400 RL episodes achieves 85% success on w2-o3-q4, nearly matching the 88% performance of pure RL training with 5000 episodes. This represents significant reduction in RL training data while maintaining competitive performance. To determine optimal resource allocation, we analyze performance under fixed budget of 1000 cost units, assuming SFT data costs 10 more than RL episodes (reflecting higher human annotation effort). Pure SFT (100 demonstrations) achieves excellent in-domain performance (95% on w2-o3-q4) but limited generalization (55% on w4-o6-q8). The optimal configuration uses 60 demonstration data with 400 RL episodes, achieving 85% on w2-o3-q4 and 59% on w4-o6-q8, balancing task-specific performance with generalization. This demonstrates that while SFT provides crucial behavioral priors, RL training remains essential for robustnessparticularly important given that real-world demonstrations are typically noisy. Finally, we investigate cross-domain priors by training SFT models on ALFWorld demonstrations (3553 samples) then applying PPO to TextWorld, and vice versa (3000 TextWorld demonstrations to ALFWorld). We find that cross-domain priors cause rapid policy collapse during multi-turn RL, likely because demonstration biases conflict with the target environments action-outcome relationships, destabilizing the policy. Preprint. Under Review. Table 7: Performance across SFT/RL data allocations under fixed budget. Models trained on w2-o3q4, evaluated on w2-o3-q4 and w4-o6-q8. SFT followed by multi-turn PPO. Results from previous experiments included for comparison. #SFT data #RL data SFT SFT (test on w4-o6-q8) / 0 20 40 60 80 100 5000 1000 800 600 400 200 0 0.17 (base) 0.01 (base) / 0.59 0.75 0.71 0.94 0.95 / 0.15 0.51 0.53 0.29 0.55 SFT+PPO SFT+PPO (test on w4-o6-q8) 0.88 0.12 0.54 0.62 0.72 0.85 0.95 / 0.11 0.15 0.44 0.59 0.35 /"
        },
        {
            "title": "6.2 HOW DO RL ALGORITHMS IMPACT MULTI-TURN RL TRAINING?",
            "content": "Understanding whether performance gains stem from our multi-turn formulation or specific algorithmic choices is crucial for establishing the generalizability of our approach. We compare PPO (heuristic policy gradient method with value function bootstrapping) against RLOO (unbiased policy gradient estimator) to isolate the contributions of our token-level credit assignment from algorithmic design decisions (Oertell et al., 2025). Both PPO and RLOO achieve substantial improvements over base models, demonstrating that performance gains stem from our multi-turn formulation rather than PPO-specific heuristics. Table 8 shows that PPO achieves 88% success on w2-o3-q4 versus RLOOs 51%. This gap widens on w4-o6-q8, where PPO reaches 59% success while RLOO fails completely (0%) for the 1.5B model. PPO consistently outperforms RLOO in multi-turn settings, with performance gaps increasing for complex environments. Model size also affects the gap: at 7B parameters, both algorithms perform similarly on simple tasks (97% vs 98%), but PPO maintains advantages on complex tasks (72% vs 47%). These results confirm that the performance gains are not due to PPO heuristics, evidenced by RLOOs consistent improvements across tasks. Table 8: Comparison of PPO and RLOO on multi-turn TextWorld tasks across model sizes. Task / Model Base model RLOO PPO w2-o3-q4 / Qwen-1.5B 0.15 w4-o6-q8 / Qwen-1.5B 0.01 w2-o3-q4 / Qwen-7B w4-o6-q8 / Qwen-7B 0.65 0.28 0.510.36 0.0 0.970.32 0.470.21 0.880.73 0.590.58 0.980.33 0.720. We also examine GRPO, another heuristic-based method, implemented in 5.3 for SWE-Gym tasks. While GRPO does not naturally apply to multi-turn training, SWE-Gyms heavyweight environment makes GRPO computationally advantageous, nullifying advantages of other methods given the lack of dense reward functions for SWE-Gym. Therefore, we draw parallels between PPO and GRPO as biased algorithms only under sparse reward settings (only final rewards given per trajectory). The improvements from GRPO in Table 6, together with PPOs results in Table 8, demonstrate the effectiveness of biased methods in multi-turn RL."
        },
        {
            "title": "7 REWARD",
            "content": "Multi-turn environments typically provide sparse feedback upon task completion, creating challenges across extended sequences that can lead to slow convergence or training instability. However, some environments offer dense reward signals with partial rewards at solution milestones. We investigate how reward density, the frequency of feedback signals in trajectories, affects multi-turn RL performance and learning efficiency. 8 Preprint. Under Review. Setup. We experiment with different reward densities on TextWorld tasks using PPO and RLOO. Our reward density configurations leverage TextWorlds built-in reward functions: (1) sparse rewards, provided only upon task completion, and (2) dense rewards, provided at intermediate steps. We quantify reward density as the average number of steps between reward signals and larger values indicate sparser rewards. We evaluate the tw-simple tasks4 from TextWorld, training on 3,000 episodes with different random seeds. Full task specifications and training details appear in Appendices B.3 and C.3. Table 9: Performance across reward density schemes on tw-simple tasks (Qwen-7B). Parentheses show reward density as steps per reward. Reward density Qwen-7B (PPO) Qwen-7B (RLOO) Sparse (10.22) Dense 1 (2.89) Dense 2 (1.17) 0.410.12 0.290.0 0.580.29 0.350.06 0.550.26 0.550.26 7.1 WHAT REWARD SIGNALS ARE NEEDED FOR MULTI-TURN RL TRAINING? Dense rewards significantly improve multi-turn RL performance, with optimal density varying by algorithm. Table 9 shows differential effects on PPO and RLOO. PPO benefits most from the densest rewards (Dense 2), achieving 58% success versus 41% with sparse rewards. RLOO shows robust performance across dense schemes, achieving 55% success with both Dense 1 and Dense 2 configurations, suggesting its unbiased gradient estimates are less sensitive to reward density. The key insight is that reward density should align with the optimization algorithm. While dense rewards enable faster convergenceevidenced by substantial gains across both algorithmstheir effectiveness depends on signal quality. Poorly designed intermediate rewards can provide misleading guidance that impairs learning."
        },
        {
            "title": "8 THE RECIPE AND CONCLUSION",
            "content": "Our systematic investigation across environment, policy, and reward pillars yields practical recipe for multi-turn agentic RL. Environment: Start training on simpler environments as agents may develop transferable skills that generalize to complex settings. Object complexity proves more challenging than spatial complexity, suggesting that curriculum design should prioritize object manipulation skills. While single-task training provides decent cross-task generalization, mixed-task training yields superior robustness. Policy: Good imitation learning priors dramatically reduce RL sample complexity while maintaining comparable performance. An optimal balance exists between demonstration and RL data that maximizes both task-specific accuracy and generalization. Biased algorithms (PPO, GRPO) outperform unbiased alternatives (RLOO) in multi-turn settings, with performance gaps widening in complex environments. Reward: Dense rewards significantly improve multi-turn RL performance, with optimal density varying by algorithmPPO benefits most from dense feedback while RLOO shows robustness across reward schemes. Through systematic hyperparameter tuning, we identify recipe that works across TextWorld, ALFWorld, and SWE-Gym, providing starting point for practitioners. This work establishes that multiturn RL requires fundamental rethinking beyond single-turn optimization. By releasing our framework and experimental artifacts, we aim to accelerate research in agentic AI systems capable of extended real-world interaction, providing both the empirical foundation and practical guidelines for developing truly autonomous agents. 4https://textworld.readthedocs.io/en/stable/textworld.challenges. simple.html 9 Preprint. Under Review."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We are committed to ensuring the reproducibility of our experimental results. To this end, we provide comprehensive details about our experimental setup and will release all necessary resources for replication: Code and Framework: We will open-source our complete multi-turn RL framework built on veRL, including environment integrations for TextWorld, ALFWorld, and SWE-Gym. All training scripts, evaluation pipelines, and data generation procedures will be included in the repository upon publication. Experimental Details: Full hyperparameters for all experiments are provided in Appendices, including learning rates, KL penalties, batch sizes, exploration budgets, and temperature settings for each environment and model configuration. We specify exact model versions (Qwen2.5-1.5BInstruct, Qwen2.5-7B-Instruct, Qwen3-8B), training epochs, and convergence criteria. Task selection procedures, including random seeds for procedural generation in TextWorld and specific task splits for ALFWorld and SWE-Gym, are also documented. Data and Models: We detail our data generation process, including the creation of SFT demonstrations from TextWorld gold solutions and the sampling strategy for SWE-Gym tasks. Model weights for key experimental configurations will be released. For environments requiring specific versions, we provide exact package versions and installation instructions. Computational Requirements: All experiments were conducted on NVIDIA H100 GPUs. We report approximate training times and computational resources required for each experimental configuration, enabling researchers to estimate reproduction costs. The framework supports distributed training for efficient replication at scale. Preprint. Under Review."
        },
        {
            "title": "REFERENCES",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024. URL https://arxiv.org/abs/2402.14740. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report, 2025. URL https://arxiv.org/abs/2412.04604. Marc-Alexandre Cˆote, Akos Kadar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: learning environment for text-based games. In Workshop on Computer Games, pp. 4175. Springer, 2018. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. Agentic reinforced policy optimization, 2025. URL https://arxiv.org/ abs/2507.19849. Owen Oertell, Wenhao Zhan, Gokul Swamy, Zhiwei Steven Wu, Kiante Brantley, Jason Lee, and Wen Sun. Heuristics considered harmful: Rl with random rewards should not URL https://fuchsia-arch-d8e.notion.site/ make llms Heuristics-Considered-Harmful-RL-With-Random-Rewards-Should-Not-Make-LLMs-Reason-21ba29497c4180ca86ffce303f01923d. reason, 2025. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. In Proceedings of the 42nd International Conference on Machine Learning (ICML 2025), 2025. URL https://arxiv. org/abs/2412.21139. arXiv:2412.21139, accepted at ICML 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, pp. 12791297. ACM, doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10.1145/ March 2025. 3689031.3696075. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cˆote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning, 2021. URL https://arxiv.org/abs/2010.03768. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning, 2025. URL https://arxiv.org/abs/ 2504.20073. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024. URL https://arxiv. org/abs/2404.07972. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao 11 Preprint. Under Review. Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Siliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, and Mingyi Hong. Reinforcing multi-turn reasoning in llm agents via turn-level credit assignment, 2025. URL https://arxiv.org/abs/2505.11821. Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks, 2025. URL https://arxiv.org/abs/2503.15478. 12 Preprint. Under Review. HYPERPARAMETER TUNING FOR MULTI-TURN RL To establish robust training recipe for multi-turn RL, we conducted systematic hyperparameter sweeps training Qwen-1.5B on TextWorld w2-o3-q4 tasks using PPO. Following the setup in 4, we train on 5,000 episodes and evaluate on 100 held-out episodes. Each experiment runs for 30 epochs (approximately 575 steps) to assess early training stability, measuring task success rate throughout training. Figure 2 reveals substantial performance variation across hyperparameter configurations. Higher KL coefficients (> 0.001) yield more stable training curves. Comparing configurations shows that gamma values below 1.0 impair performance (blue vs. pink curves), while rollout temperature critically affects outcomesoptimal performance occurs between 0.7 and 1.0 (comparing light green, pink, and dark green curves). Higher learning rates for both actor and critic networks improve training efficiency and final performance (brown vs. light green curves). Table 10 confirms the optimal configuration: KL coefficient of 0.01, temperature of 0.7, actor learning rate of 1e-6, critic learning rate of 1e-5, and gamma of 1.0. These settings provide both training stability and strong final performance, forming the basis for our experiments across all benchmarks. Figure 2: Training curves for Qwen-1.5B on TextWorld w2-o3-q4 with varying hyperparameters. Parameters shown: KL coefficient (kl coef), rollout temperature (t), actor/critic learning rates, and discount factor (gam). 13 Preprint. Under Review. Table 10: Success rates at epoch 30 on TextWorld w2-o3-q4 test set, sorted by performance. Only top-performing configurations shown; excluded configurations performed substantially worse."
        },
        {
            "title": "Ranking",
            "content": "kl coef temperature actor lr critic lr gamma win rate 1 2 3 4 5 6 7 0.01 0.01 0.01 0.01 0.005 0.01 0.001 0.001 0.7 0.8 0.7 1.0 0.7 0.7 1.0 0.7 1e-6 5e-7 5e-7 5e-7 5e-7 5e-7 5e-7 5e-7 1e-5 5e-6 5e-6 5e-6 5e-6 5e-6 5e-6 5e-6 1.0 1.0 1.0 1.0 1.0 0.99 1.0 1.0 0.9 0.86 0.78 0.71 0.66 0.66 0.57 0."
        },
        {
            "title": "B DETAILS OF TASKS AND ENVIRONMENTS",
            "content": "We show an example of the TextWorld w2-o3-q4 task in Figure 3. The ALFWorld tasks are built on TextWorld and they are similar in format, but the tasks in ALFWorld rely more on agents embodied understanding. An example from ALFWorld heat & place task is presented in Figure 4. We extract SWE-Gym tasks from their huggingface repository (https://huggingface.co/ datasets/SWE-Gym/SWE-Gym. An example from SWE-Gym getmoto task is presented in Figure 5. B.1 ADDITIONAL TASK DETAILS FOR 5 For environment complexity, we procedurally generated 5000 RL episodes for w2-o3-q4, w8-o3-q4, w2-o12-q4, and w4-o6-q8 respectively using TextWorlds tw-make, using random seeds ranging from 30001 to 35000. For task complexity using ALFWorld, we make sure the number of data for each training mixture is the same, which is 1000. Here is brief data statistics: (1) single-type tasks contain 1000 pick & place tasks, randomly sampled from 3553 train data; (2) mixed-type (4 types) contain 250 heat, cool, cook, and examine tasks respectively; (3) mixed-type (all types), randomly sampled 1000 any type of tasks from 3553 train data. For task complexity using SWE-Gym, we make sure the number of data for each training mixture is the same, which is 90. Here is brief data statistics: (1) single-type tasks contain 90 getmoto tasks, randomly sampled from the SWE-Gym huggingface repo; (2) mixed-type (5 types) contain 18 getmoto, pydantic, mypy, pandas, and interactive dvc tasks respectively. B.2 ADDITIONAL TASK DETAILS FOR 6 We collect 0/20/40/60/80/100 SFT data from the gold trajectories from TextWorld games. The seeds range from 40001 to 40101, different from the RL data. B.3 ADDITIONAL TASK DETAILS FOR 7 The tasks are procedurally generated using TextWorlds built-in tw-simple function, which has dense reward calculator for each step. We utilize TextWorlds built-in reward function to assign rewards per step. We collect 3000 episodes with sparse, dense 1, and dense 2 density levels, respectively."
        },
        {
            "title": "C DETAILS OF TRAINING AND EVALUATION",
            "content": "We train Qwen-1.5B, Qwen-7B, and Qwen-8B on 8 NVIDIA H100 GPUs. 14 Preprint. Under Review. Figure 3: Textworld w2-o3-q4 task example. The text in gray are the prompts. The bold text is the objective. The text in blue are the observations and the text in orange are the actions. 15 Preprint. Under Review. Figure 4: ALFWorld heat & place task example. The text in gray are the prompts. The bold text is the objective. The text in blue are the observations and the text in orange are the actions. We only provide part of the task trajectory because it is very long and is similar to the TextWorld example. 16 Preprint. Under Review. Figure 5: SWE-Gym getmoto task example. The text in gray are the prompts. The bold text is the objective. The text in blue are the observations and the text in orange are the actions. 17 Preprint. Under Review. For PPO, by default, we use an actor learning rate of 5e-7, critic learning rate of 5e-6, clip ratio of 0.2, discount factor γ of 1.0, KL penalty coefficient of 0.001, batch size of 256, and zero entropy regularization for both Qwen-1.5B and Qwen-7B models. For GRPO, by default, we use an actor learning rate of 1e-6, KL loss coefficient of 0.001, GRPO sampling number of 4, and batch size of 16 for Qwen3-8B model. For RLOO, by default, we use an actor learning rate of 1e-6, KL penalty coefficient of 0.001, and batch size of 256 for both Qwen-1.5B and Qwen-7B models. For SFT, by default, we use learning rate of 1e-6. C.1 ADDITIONAL TRAINING DETAILS FOR 5 For TextWorld experiments, we train 150 epochs and evaluate on 100 held-out test sets (with seeds 1 to 100) at epoch 150. For ALFWorld experiments, given that the base model has zero accuracy on the tasks, we do one epoch of SFT on 300 data, with different seeds from RL data. After the SFT initialization, we train 90 epochs and evaluate on 100 held-out test sets at epoch 90. For SWE-Gym experiments, we train 15 epochs and evaluate on 25 held-out test sets at epoch 15. C.2 ADDITIONAL TRAINING DETAILS FOR 6 For SFT, we do exactly one epoch, ensuring there is only one pass on the demonstration data to avoid overfitting. We train RL on top of the SFT for another 100 epochs. For RLOO and PPO comparison, we train 150 epochs for each, similar to Appendix C.1. C.3 ADDITIONAL TRAINING DETAILS FOR For PPO and RLOO training for dense rewards, we train 150 epochs for each and test on 100 heldout data at epoch 150."
        },
        {
            "title": "D USE OF LLMS",
            "content": "We used large language models (specifically Claude) exclusively for text polishing and grammar checking during the preparation of this manuscript. The LLM was used to improve clarity, fix grammatical errors, and enhance the conciseness of our writing. All experimental design, analysis, interpretations, and scientific conclusions are entirely our own original work. No LLMs were used for generating experimental results, creating figures or tables, or producing any technical content. The core research ideas, methodology, and findings presented in this paper are the product of human authorship."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "University of California, San Diego"
    ]
}