{
    "paper_title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
    "authors": [
        "Yong Xien Chng",
        "Tao Hu",
        "Wenwen Tong",
        "Xueheng Li",
        "Jiandong Chen",
        "Haojia Yu",
        "Jiefan Lu",
        "Hewei Guo",
        "Hanming Deng",
        "Chengjun Xie",
        "Gao Huang",
        "Dahua Lin",
        "Lewei Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 0 3 3 4 2 . 2 1 5 2 : r SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning Yong Xien Chng,1,2, Tao Hu,1,3, Wenwen Tong,,1, Xueheng Li1,3, Jiandong Chen1, Haojia Yu1, Jiefan Lu1, Hewei Guo1, Hanming Deng1, Chengjun Xie3, Gao Huang2, Dahua Lin1, Lewei Lu(cid:0),1 Equal Contribution, Project Lead, (cid:0) Corresponding Author 1SenseTime Research, 2Tsinghua University, 3University of Science and Technology of China"
        },
        {
            "title": "Abstract",
            "content": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the models ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNovaMARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets. Date: January 1, 2026 Codebase: https://github.com/OpenSenseNova/SenseNova-MARS Model: https://huggingface.co/sensenova/SenseNova-MARS-8B"
        },
        {
            "title": "Introduction",
            "content": "Vision-Language Models (VLMs) have significantly advanced the development of artificial general intelligence with impressive capabilities across wide range of visual understanding tasks [2, 5, 7, 17, 42]. To tackle more complex real-world challenges, recent efforts have equipped VLMs with reasoning capabilities and external tools, such as search engines for knowledge-intensive tasks or cropping tools for fine-grained visual analysis [1, 14, 15, 44]. However, these systems remain constrained by text-centric reasoning chains and isolated tool calls, lacking the dynamic integration of 1 Figure 1 Overall performance of SenseNova-MARS-8B compares to other models across six benchmarks. All other models are evaluated under agentic workflow. SenseNova-MARS-8B demonstrates exceptional performance on the search-oriented benchmarks such as MMSearch [19], HR-MMSearch and FVQA [44], surpassing leading proprietary models such as Gemini-3-Flash [12] and GPT-5 [32]. For the high-resolution perception benchmark such as V* Bench [45] and HR-Bench [43], SenseNova-MARS-8B also outperforms existing open-source models, including DeepEyesV2 [15] and Mini o3 [22]. multimodal reasoning and tool use. As illustrated in Fig. 2, real-world visual understanding demands agentic models that can interleave planning, reasoning, and multi-tool execution into cohesive, adaptive, and multi-step process. Therefore, it is critical to develop agentic models with robust tool invocation and human-like multi-step reasoning capabilities to solve challenging visual tasks. The potential of end-to-end reinforcement learning (RL) with Group Relative Policy Optimization (GRPO) to enhance the reasoning capabilities has been highlighted by models such as DeepSeek-R1 [14], establishing RL as the mainstream approach for developing reasoning models. Existing search-based agents [8, 20, 25] have utilized end-to-end RL to enhance model performance through multi-turn reasoning with text search tools, demonstrating substantial performance gains over Retrieval-Augmented Generation (RAG) methods. MMSearch-R1 [44] further integrates image search with text search tools, enabling VLM to interact with the real-world environment to tackle knowledge-intensive and info-seeking visual tasks. However, it remains inadequate for high-resolution perception tasks as shown in Fig. 2, which requires level of detailed visual reasoning and fine-grained analysis that cannot be achieved with image and text search tools alone. Recent advances such as OpenAI-o3 [31] introduced the Thinking with images paradigm, which interleaves image and text reasoning by employing the image crop tool to perform fine-grained analysis of complex visual scenes [39]. Subsequent work, including Pixel Reasoner [37] and DeepEyes [54], further demonstrates that pixel-space visual reasoning capabilities can be incentivized by RL, enabling VLMs to learn how to interact with image manipulation tools and proactively gather necessary visual information. These approaches are typically restricted to either search-based or image manipulation tools, which hinders their ability to handle dynamic real-world visual tasks requiring both. This limitation necessitates the creation of search-oriented agentic VLMs that can leverage tools like image crop to handle complex visual scenes, driven by end-to-end RL. To this end, we propose SenseNova-MARS, novel Multimodal Agentic Reasoning and Search framework that leverages RL to integrate image search, text search, and image crop tools into dynamic, multi-turn reasoning process. Specifically, SenseNova-MARS adaptively interacts with the tool sets through reasoning and planning, learning when and how to invoke the search and image crop tools during the iterative reasoning process. We systematically investigate how 2 Figure 2 Reasoning trajectory of SenseNova-MARS. SenseNova-MARS tackles the challenging visual task by leveraging an integrated suite of text search, image search, and image crop tools within the reasoning process. to build this agentic VLM based on Qwen2.5-VL-7B [2] and Qwen3-VL-8B [1], named SenseNova-MARS-7B and SenseNova-MARS-8B, respectively. Our approach focuses on three key aspects: training data construction, two-stage training pipeline including the cold-start stage and RL, and the design of RL algorithm. We first curate the high-quality cold-start and RL training data through synthesis pipeline with rigorous quality verification. The dataset spans diverse visual tasks, including knowledge-intensive, high-resolution perception, and search-oriented tasks, which necessitate the use of the search tool, image crop tool, or combination of both. During the initial cold-start supervised fine-tuning (SFT) stage, the model learns basic tool-usage patterns from minimal dataset of approximately 3,000 samples. This compact dataset is crucial as it establishes foundation for the subsequent RL stage, particularly in learning to use the tools effectively. To advance the agentic VLMs capabilities in multi-tool collaboration and reasoning during the RL stage, we propose Batch-Normalized Group Sequence Policy Optimization (BN-GSPO), an extension of the standard GSPO algorithm. Experiments indicate that BN-GSPO can improve training stability for multi-tool rollout trajectories and yield significant performance gains. To further evaluate visual search agents for complex scenes, we introduce HR-MMSearch, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. SenseNovaMARS demonstrates superior performance on HR-MMSearch, far beyond the search-only or crop-only models, and is comparable with proprietary models like Gemini-2.5-Flash [7], Gemini-3-Flash [12], and GPT-5 [32]. Furthermore, SenseNova-MARS-8B achieves state-of-the-art (SOTA) results across diverse open-source search-oriented and perception benchmarks as shown in Fig. 1. It outperforms MMSearch-R1 [44] and DeepEyesV2 [15] on search-oriented 3 benchmarks and attains leading score of 67.84 in MMSearch [19] and 41.64 in HR-MMSearch. For the high-resolution perception benchmark, SenseNova-MARS-8B achieves 92.2 on V* Bench [45] and 83.1 on HR-Bench 4K [43], exceeding specialized models like DeepEyes [54], Pixel-Reasoner [37] and Mini o3 [22]. To sum up, our contributions are threefold: (1) We propose SenseNova-MARS, the first end-to-end agentic highresolution vision-language model developed by RL, with the capability of performing image search, text search, and image crop tools within the reasoning process. (2) We introduce HR-MMSearch, the first benchmark specifically designed for high-resolution, search-oriented, and knowledge-intensive tasks, enabling comprehensive evaluation of agentic reasoning and multi-tool invocation capabilities in VLMs. (3) We propose BN-GSPO, stable and highperforming RL algorithm for training agentic VLMs in tool invocation. Extensive experiments on diverse open-source benchmarks with SOTA performance demonstrate that SenseNova-MARS exhibits strong reasoning and robust tool invocation, validating the effectiveness of our approach."
        },
        {
            "title": "2.1 Search-based Agentic VLMs",
            "content": "To mitigate the restriction of static knowledge bases, large language models have evolved from static retrieval mechanisms to dynamic, tool-augmented reasoning. Early RAG systems such as Dense Passage Retrieval [21] and RetrievalAugmented Generation [23] rely on static document corpora for text grounding. Later, search-augmented systems such as WebGPT [29], Toolformer [33], and SAIL [28] allow models to access live web information and reason over up-to-date content. In parallel, multimodal models including REVEAL [16], RagVL [4], and VisRAG [49] show that combining textual and visual retrieval improves knowledge-intensive understanding. However, these RAG-based methods follow the fixed workflow and often result in excessive retrieval, which is suboptimal in practice. Recently, text-based agentic search models such as Search R1 [20] and Search o1 [25] integrate search tools into the chain of thought to enhance performance on knowledge-intensive tasks. In the multimodal domain, agentic search models such as MMSearch-R1 [44] train VLMs to trigger image and text searches dynamically, while WebWatcher [13] extends this approach through synthetic trajectories that improve generalization. Existing agentic VLM research has mainly focused on holistic image understanding that gathers broad contextual information while overlooking the analysis of fine-grained image regions. This limitation reduces their ability to answer region-specific questions and to perform tasks that require precise visual grounding."
        },
        {
            "title": "2.2 Thinking with Images",
            "content": "The Thinking with images paradigm, introduced by OpenAI-o3 [31], has spurred the development of agentic visual reasoning VLMs that can interleave image and text reasoning with iterative visual analysis [38, 39]. DeepEyes [54] provides an open-source implementation, demonstrating that end-to-end RL can incentivize models to adopt this behavior, significantly improving performance on fine-grained visual tasks. However, subsequent works reveal that pure RL training is insufficient for complex, multi-turn interaction [22, 37, 51, 55]. Pixel Reasoner [37] identifies critical \"learning trap\" where models bypass nascent visual tools. To address this, it proposes two-phase approach: cold-start phase to first establish foundational tool use, followed by curiosity-driven RL phase to incentivize pixel space exploration. Similarly, Mini o3 [22] observes that pure RL cannot generate the deep trajectories for hard visual searches and therefore adopted two-stage training method to activate multi-turn capabilities. Despite these advances, current methods remain fundamentally knowledge-limited, as their toolsets focus solely on perceptual image operations and are inherently ill-equipped for tasks that demand open-web access or external knowledge."
        },
        {
            "title": "2.3 Multi-tool Agentic VLMs",
            "content": "Recent advancements in VLMs [2, 17, 42, 46], particularly the integration of RL paradigms, have catalyzed the rapid evolution of sophisticated agentic systems [9, 18, 40]. These models demonstrate impressive capabilities in complex problem-solving through hierarchical planning, reasoning, and the strategic invocation of diverse external tools (e.g., image processing, code execution, web search). Notably, recent studies [10, 26, 36] have highlighted the crucial role of RL in enabling agents to develop adaptive multi-tool utilization strategies, optimizing both tool-calling policies and reasoning trajectories through interaction with the environment and feedback from execution outcomes. Visual-ARFT [27] introduces framework that empowers agents to perform multi-tool collaboration by integrating 4 Figure 3 The illustration of SenseNova-MARS RL training pipeline. SenseNova-MARS adaptively invokes the image search, text search and image crop tools in the multi-turn reasoning process to obtain the final answer. The policy VLM is optimized by the BN-GSPO algorithm, driven by the format reward and answer reward. web information retrieval with autonomous code execution for visual reasoning. Similarly, DeepMMSearch-R1 [30] proposes VLM-based agent equipped with advanced tool integration for web search, orchestrating textual queries alongside the image search mechanism that leverages intermediate cropping to concentrate on relevant visual entities. However, existing methods struggle to effectively coordinate the acquisition of critical visual information with external knowledge retrieval, limiting their capability for collaborative tool invocation and coherent reasoning."
        },
        {
            "title": "3 Method",
            "content": "In this section, we investigate how to build an agentic search-reasoning model with fine-grained visual analysis capabilities for complex search-oriented and knowledge-intensive tasks. Our investigation focuses on the two-stage training strategy, data construction, and benchmark evaluation. We first present the formal task formulation in Sec. 3.1. We then describe the two-stage training approach in Sec. 3.2, which consists of cold-start SFT and RL. The data collection pipeline for the SFT and RL stages is detailed in Sec. 3.3. Finally, in Sec. 3.4, we introduce HR-MMSearch, our newly constructed benchmark designed to rigorously evaluate the fine-grained visual analysis and search-reasoning capabilities of VLMs."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "We build upon the MMSearch-R1 [44] problem setting for interactive and search-augmented multimodal reasoning in real web environment, and extend it in two main ways. First, we expand the available toolset beyond text and image search by adding the image crop tool, which allows the agent to zoom into specific regions of an image. Second, we focus on multimodal search tasks that involve high-resolution images, where important information can only be revealed through selective zooming. The left panel of Fig. 3 presents an overview of this setting. It illustrates how the agent interacts with the environment through reasoning and tool use, and how the task, observation space, and action space are structured within this process. 5 Task and Objective. The task begins with natural language query and an initial image I0. At each turn, the agent may invoke tool or produce final answer when it deems the information sufficient. The process ends once the final answer is given. If no valid answer is produced within turns, the result is considered incorrect. Observation Space. At turn t, the model observes the full interaction history Tt, which records all text and image elements in order, including the initial prompt, the query, prior reasoning steps, tool calls, and their outputs. Each tool call yields compact observation ot, in text, image, or both, which is then appended to Tt. For instance, calling the image crop tool adds the resulting cropped image. The framework is agnostic to the specific form of these observations, requiring only that each provides self-contained information for reasoning. Action Space. At each turn, the model first generates reasoning step, then selects one of four actions: 1. Perform text-based web search using text search. 2. Execute reverse image search using image search. 3. Zoom into specific image region using image crop. 4. Produce the final answer. The output, such as search results or cropped image, is added to Tt, forming an evolving trajectory. Each turn includes reasoning step and one valid action. If either is missing, the trajectory is invalid."
        },
        {
            "title": "3.2 Reinforcement Learning with Cold-start",
            "content": "We follow two-stage training recipe. In the first stage, we perform cold-start SFT to bootstrap the models basic ability to learn multi-tool invocations. In the subsequent RL stage, we employ the proposed Batch-normalized GSPO (BN-GSPO) algorithm to further refine the tool invocation and reasoning capabilities. Cold-start SFT. The cold-start stage involves SFT on small, meticulously curated dataset DSFT of multi-turn interaction trajectories. We formulate the cold-start process as follows: LSFT = (cid:88) log πθ(yi xi), (xi,yi)DSFT (1) where DSFT is the cold-start dataset, xi denotes the user query, yi is the target reasoning trajectory, and πθ represents the model parameterized by θ. BN-GSPO for RL. We build on GSPO [52] to train agents capable of multimodal search and visual reasoning via external tools. In this setting, each agent output is sequence that may include text, tool calls, and observations, with supervision provided only at the sequence level through an external reward model. This necessitates sequencelevel optimization, for which GSPO is natural starting point. However, standard GSPO is sensitive to the diverse trajectory structures and reward magnitudes that arise in multimodal and tool-augmented tasks. Different prompts or tool interactions within the same batch can result in varying outcome lengths, reward scales, and difficulty levels, which may bias the learning signal and destabilize training. To address this issue, we propose BN-GSPO, which applies two-stage normalization to the advantage estimates. This stabilizes optimization across heterogeneous prompts and preserves consistent learning signals within and across groups. Given question-answer pair (xb, zb) DRL, we sample responses {yb,g}G g=1 πθold ( xb) and define the length-normalized sequence importance ratio: sb,g(θ) = (cid:18) πθ(yb,g xb) πθold(yb,g xb) (cid:19)1/yb,g = exp 1 yb,g yb,g (cid:88) t=1 log πθ(yb,g,t xb, yb,g,<t) πθold(yb,g,t xb, yb,g,<t) Let rb,g denote the scalar sequence-level reward from the external reward model for (xb, yb,g). We first apply GSPOs group-level standardization to compute the group-normalized rewards: rb,g mean (cid:0){rb,g}G g=1 (cid:17) Ab,g = (2) (cid:16) (cid:1) , std {rb,g}G g=1 6 Figure 4 Cold-start data generation pipeline. It consists of data mining, trajectory synthesis and quality verification. and then normalize these values across the entire optimizer minibatch: Ab,g = Ab,g mean(cid:0){ Ab,g}bB, gG std(cid:0){ Ab,g}bB, gG (cid:1) (cid:1) , (3) where denotes the current minibatch and = {1, . . . , G} is the set of group indices. This second step helps correct inconsistent scales and variances across different prompts within the same batch. With the normalized advantages, we apply the following clipped sequence-level objective J(θ): Exb,{yb,g} (cid:34) 1 (cid:88) g=1 min(sb,g(θ) Ab,g, clipϵhigh ϵlow (sb,g(θ)) Ab,g) (cid:35) β DKL(πθ πref). (4) This clipped objective stabilizes RL training by preventing excessively large policy updates. Similarly, the KL term helps prevent overfitting by applying small penalty to deviations from frozen reference policy πref. Reward Modeling. To optimize the learning process in BN-GSPO, we formulate the sequence-level reward as combination of final answer accuracy and structural format compliance. Specifically, for trajectory τ , the total sequence-level reward R(τ ) is given by R(τ ) = Racc(τ ) + Rf ormat(τ ). The accuracy reward Racc(τ ) measures the semantic agreement between the predicted answer and the ground truth, which is evaluated using an LLM-as-a-judge. The format reward Rf ormat(τ ) guarantees strict compliance with the interaction protocol. Under this protocol, each non-final turn must consist of reasoning trace and single tool call. The final turn, in contrast, must contain the reasoning trace and the answer. Additionally, the protocol requires all content to be enclosed within special tags and all tool calls to conform to specified JSON schema. Full details are provided in the Appendix."
        },
        {
            "title": "3.3 Training Dataset Collection",
            "content": "For cold-start SFT, we design structured three-phase data pipeline, as illustrated in Fig. 4. The pipeline begins with data filtering, where we merge the FVQA train set [44], the Pixel-Reasoner warm-start corpus [37], and curated expert-annotated multimodal QA pairs to construct the raw data pool. We then filter this pool using 8 rollouts from Qwen2.5-VL-7B-Instruct, marking sample as hard if the model answers it correctly one time or fewer. For these hard QA samples, we prompt Gemini-2.5-Flash to synthesize complete solution trajectories through tool invocations. Finally, GPT-4o is used to verify format compliance, logical coherence, and answer plausibility. Only the validated results are retained, yielding 3,000 high-quality samples. For RL, we use FVQA-train [44] together with VisualProbe-train and DeepEyes-4K-train [22], which provides robust mix of factual question answering and high-resolution visual analysis tasks."
        },
        {
            "title": "3.4 HR-MMSearch Benchmark Construction",
            "content": "Existing benchmarks, such as FVQA [44] or MMSearch [19], typically utilize standard HD or lower-resolution images and focus on holistic scene understanding, which leaves critical gap in evaluating an agents detailed visual understanding capabilities. Thus, we introduce HR-MMSearch as shown in Fig. 5, benchmark for the fine-grained perception and search-reasoning capabilities of VLM agents. This dataset consists of 305 4K-resolution images curated from 8 diverse, high-impact domains, covering areas such as Sports, Leisure & Culture, and Science & Technology. 7 Figure 5 Statistics of our proposed HR-MMSearch benchmark. HR-MMSearch is characterized by the high-resolution images and knowledge-intensive question, covering areas such as Sports, Leisure&Culture, Science&Technology, Business&Finance, Games, and Academic Research. To prevent data leakage from the VLMs pre-trained knowledge, all images are sourced exclusively from recent 2025 events. For each image, we manually craft knowledge-intensive, search-oriented questions that focus on key visual subject, such as small or inconspicuous object or text occupying less than 5% of the image area."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "Model and Training. SenseNova-MARS-7B, which is based on Qwen2.5-VL-7B-Instruct [2], is trained using the two-stage pipeline. This process involves SFT followed by RL, which we implement using the LLaMA-Factory [53] and veRL [35] frameworks, respectively. During the SFT stage, we fine-tune only the language model while keeping the vision encoder and multi-modal projector frozen. The training uses learning rate of 1 105 for 3 epochs. The SFT stage bootstraps the models ability to follow the interaction protocol and utilize the multi-toolset. The subsequent RL stage then teaches the model to use these tools more effectively and efficiently. For RL stage, we use global batch size of 128, learning rate of 1 106, and KL coefficient β of 1 104. Additionally, to encourage better exploration, we follow DAPO [48] by using the Clip-Higher strategy with ϵlow = 0.2 and ϵhigh = 0.28. During this RL phase, single training trajectory allows the agent to interact for maximum of = 10 turns. This means the agent can iteratively use tools and refine its plan for up to 10 steps before producing final answer for that trajectory. In each turn, the agent can generate up to 8, 192 tokens, with cumulative limit of 32, 768 tokens for the entire trajectory. Compared with SenseNova-MARS-7B, SenseNova-MARS-8B is developed based on Qwen3-VL-8B-Instruct [1], and only the RL stage is employed, leveraging the strong tool-usage capability of the base model. Detailed training hyperparameters are available in the Appendix. Training Rewards. To guide the RL process, we employ two-component reward that jointly promotes answer correctness and format compliance. Each component is binary score determined by GPT-4o acting as the LLM-as-aJudge. Specifically, the accuracy reward Racc(τ ) = 1.0 and format reward Rf ormat(τ ) = 0.5 are conferred when their respective criteria are fully satisfied, and are 0.0 otherwise. Multimodal Tools. SenseNova-MARS can invoke following three multimodal tools using JSON-style arguments: text search: This tool enables the agent to query the web for textual information. It accepts single text query as its argument and is powered by the Serper Search API. Following MMSearch-R1 [44], to avoid overwhelming the context length of the model, the top five results are first summarized by Qwen3-32B [47] before being returned to the agent. 8 Type Model Average MMSearch HR-MMSearch FVQA-test InfoSeek SimpleVQA LiveVQA MAT-Search Open-source Qwen2.5-VL-7B-Instruct [2] Qwen2.5-VL-32B-Instruct [2] Qwen3-VL-8B-Instruct [1] Proprietary GPT-4o-mini [17] Gemini-2.5-Flash [7] GPT-4o [17] GPT-5 [32] Gemini-3-Flash [12] Open-source Qwen2.5-VL-7B-Instruct [2] Qwen2.5-VL-32B-Instruct [2] Qwen3-VL-8B-Instruct [1] Proprietary Open-source GPT-4o-mini [17] Gemini-2.5-Flash [7] GPT-4o [17] GPT-5 [32] Gemini-3-Flash [12] Visual-ARFT [27] DeepMMSearch-R1 [30] MMSearch-R1 [44] DeepEyesV2 [15] SenseNova-MARS-8B 27.70 32.01 29.24 33.08 40.87 42.38 50.24 53.68 35.50 53.45 51.52 45.65 58.05 55.09 60.12 61.26 40.13 52.49 64.20 Direct Answer 0.58 3.93 12.13 1.31 7.54 13.11 22.62 21.97 7.60 11.70 11.70 15.79 21.64 23.39 35.09 57.31 Agentic Model (zero-shot) 32.16 49.71 47. 38.60 59.06 49.12 52.63 62.57 34.50 53.80 63.70 67.84 19.34 33.44 27.87 26.23 40.00 30.16 38.36 41.64 Agentic Model 24.92 20.33 41. 26.28 30.50 24.22 36.83 43.78 48.00 54.39 56.50 36.00 52.22 53.61 50.00 61.72 66.34 62.61 64.89 41.72 58.40 60.60 67.11 31.95 36.65 23. 35.95 44.10 52.90 61.70 63.57 28.80 50.10 46.15 42.35 53.70 59.55 70.58 67.92 37.95 47.51 55.10 51.10 70.19 47.88 48.57 42.94 44.42 55.48 51.73 54.15 54. 42.35 65.15 62.29 50.84 68.81 63.67 55.95 61.10 42.45 55.87 57.40 59.40 61.70 19.63 21.40 23.18 24.63 31.57 28.18 44.39 38.90 22.52 42.17 39. 31.54 47.75 40.09 56.02 48.06 25.40 48.40 56.22 60.00 71.33 67.33 72.66 82.00 79.33 79.33 82.67 67.33 81.33 84.00 80.00 75.33 76.67 84.67 82. 74.00 74.00 84.67 Table 1 Performance on search-oriented benchmarks under Direct Answer and Agentic Model workflows. image search: This tool performs reverse image search to retrieve images that are visually similar or contextually related to given input. It takes an image index as its argument and is powered by the Serper Image Search API. To minimize financial cost and reduce latency during RL training, the top five image search titles and thumbnails for all prompts in the training dataset are pre-fetched and cached in advance. image crop: This tool allows the agent to crop previously seen image, allowing focused fine-grained analysis of specific region of interest. It requires two arguments: normalized coordinates of bounding box [0.0, 1.0] and an index referencing the target image. Evaluation Benchmarks. To evaluate these capabilities, we test the model on two corresponding categories of benchmarks. For agentic search, we use benchmarks including FVQA-test [44], InfoSeek [3], MMSearch [19], SimpleVQA [6], LiveVQA [11], MAT-Search [27], and our newly constructed HR-MMSearch benchmark. For visual reasoning, we test fine-grained understanding capability using V* Bench [45], HR-Bench [43] and MME Realworld [50]. Full details on all these datasets are provided in the Appendix. Baselines. We evaluate our model against several strong baselines. These include proprietary models, such as GPT-4o [17], GPT-5 [32], Gemini-2.5-Flash [7] and Gemini-3-Flash [12], as well as open-source agentic models like MMSearchR1 [44], DeepMMSearch-R1 [30] and DeepEyesV2 [15]. All models are tested under the following two workflow settings: Direct Answer: The model produces an answer directly without using external tools. Agentic Model: The model is provided with all three tools and autonomously decides how to use these tools in the rollout reasoning process. Details for these workflows are provided in the Appendix. Evaluation Metrics. For agentic search tasks, the primary metric is Pass@1, assessed by GPT-4o judge that scores the models final answer against ground-truth results. For visual understanding benchmarks, we report Avg@8 Exact Match on V* Bench and HR-Bench, and Pass@1 Exact Match on the large-scale MME-RealWorld benchmark. The evaluation prompt is provided in the Appendix. Model V* Bench HR-Bench 4K HR-Bench 8K MME RealWorld Avg. GPT-4o [17] LLaVA-onevison [24] Qwen2.5-VL-7B-Instruct [2] Qwen2.5-VL-32B-Instruct [2] Qwen3-VL-8B-Instruct [1] SEAL [45] Monet [41] Pixel-Reasoner [37] DeepEyes [54] Thyme [51] DeepEyesV2 [15] Mini o3 [22] SenseNova-MARS-8B Direct Answer 65.0 63.0 65.5 69.3 78. Agentic Model - 71.0 72.6 73.2 77.0 77.9 77.5 83.1 67.5 75.4 75.3 80.6 86.4 74.8 83.3 84.3 83.3 82.2 81.8 88.2 92.2 59.6 59.8 62.1 63.6 74.6 - 68.0 66.1 69.5 72.0 73.8 73.3 78. Table 2 Performance on visual understanding benchmarks. 62.8 57.4 56.8 59.1 61.9 - - 64.4 64.1 64.8 64.9 65.5 67.9 63.7 63.9 64.9 68.2 75.5 - - 71.9 72.5 74.0 74.6 76.1 80."
        },
        {
            "title": "4.2 Main Results",
            "content": "Search-oriented Evaluation. As shown in Tab. 1, SenseNova-MARS-8B establishes new SOTA among opensource agentic models under 8B parameters and closed-source proprietary models. Specifically, SenseNova-MARS-8B attains an average performance gain of 12.68 points compared with Qwen3-VL-8B. Additionally, SenseNova-MARS outperforms search-orientSearch-oriented Evaluationed agentic models such as MMSearch-R1 [44], DeepMMSearch-R1 [30], and DeepEyesV2 [15], surpassing MMSearch-R1 by an average of 11.71 points. Compared with proprietary closed-source models, SenseNova-MARS-8B surpasses GPT-5 [32], Gemini-2.5-Flash [7], and Gemini-3-Flash [12] by significant margin, outperforming Gemini-3-Flash by an average of 2.94 points. These results highlight the efficacy of our unified agentic search and visual reasoning RL framework. Fine-grained Visual Understanding. As shown in Tab. 2, SenseNova-MARS-8B demonstrates superior fine-grained perception capabilities on high-resolution benchmarks, validating the effectiveness of our approach in detailed, pixelspace analysis. SenseNova-MARS-8B achieves leading scores of 92.2 on V* Bench [45], 83.1 on HR-Bench 4k [43], 78.4 on HR-Bench-8k [43], and 67.9 on MME-RealWorld [50], outperforming all existing tool-based models such as Pixel Reasoner [37], DeepEyes [54], and Mini o3 [22]. Compared with Qwen3-VL-8B, SenseNova-MARS-8B attains an average performance gain of 4.9 points, demonstrating the effectiveness of our proposed RL algorithm."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "Effectiveness of Proposed BN-GSPO. We evaluate the effectiveness of our proposed BN-GSPO based on SenseNovaMARS-7B by comparing it against the commonly used RL algorithms GRPO and GSPO in Tab. 3. To isolate the influence of SFT, this comparison uses pure RL setup where all models are initialized from Qwen2.5-VL-7B-Instruct [2] without cold-start. As shown, BN-GSPO achieves the best overall performance across all benchmarks and demonstrates stable, balanced improvements. In contrast, while GRPO and GSPO improve on some tasks, they fail to perform well across both search and perception. This indicates that batch normalization effectively mitigates reward scale variance, leading to more consistent and robust multi-tool RL training. 10 Figure 6 Distribution of tool calls across different benchmarks for Qwen3-VL-8B and SenseNova-MARS-8B. Method MMSearch V* Bench HR-Bench 4K GRPO [34] GSPO [52] BN-GSPO 50.88 53.80 56.72 67.54 53.93 79.05 61.38 44.50 69.12 Table 3 Effectiveness of Proposed BN-GSPO for SenseNova-MARS-7B. Search Perception MMSearch HR-MMSearch V* Bench SenseNova-MARS-7B-SFT 53. 54.97 54.09 59.06 29.80 36.80 33.11 38.52 82.20 82.72 85.24 83.84 Table 4 Impact of different data distribution on RL performance for SenseNova-MARS-7B. Impact of different data distributions on RL performance. We evaluate the impact of RL data distributions for SenseNova-MARS-7B in Tab. 4. Training the model only on specialized data, such as fine-grained perception datasets, makes the agent overspecialize. This boosts V* Bench performance to 85.24 but causes major drop on search-oriented tasks compared to the SFT baseline. Using our full hybrid dataset, which includes both search and perception tasks, gives the best results by wide margin on search-oriented metrics. These findings show that hybrid training data is essential. It provides the signals the agent needs to learn unified, multi-tool policy that avoids overspecialization and balances external knowledge retrieval with fine-grained visual analysis. Analysis of tool use behavior. Figure 6 illustrates the adaptive tool-use behavior of SenseNova-MARS-8B. On the knowledge-intensive MMSearch benchmark, SenseNova-MARS primarily relies on image and text search tools to acquire external information, with minimal dependence on cropping-based perception. In contrast, on the more challenging HR-MMSearch, which requires both high-resolution perception and complex reasoning, SenseNovaMARS demonstrates more balanced tool usage, indicating effective integration of localized visual cues and external knowledge. Overall, compared to Qwen3-VL-8B, SenseNova-MARS-8B shows stronger adaptability across diverse tasks by dynamically selecting the most effective tools for each task setting."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose SenseNova-MARS, novel multimodal agentic reasoning and search framework that can actively employ the image search, text search and image crop tools within the multi-turn reasoning process for searchdriven and fine-grained visual tasks. To empower this capability, we introduce the BN-GSPO algorithm to enhance the reasoning robustness and tool-use proficiency. Furthermore, we construct the HR-MMSearch Benchmark with high-resolution images and knowledge-intensive questions to evaluate the performance of VLMs. Extensive experiments demonstrate that SenseNova-MARS achieves superior performance across diverse benchmarks, showcasing efficient tool invocation and robust reasoning capabilities."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, et al. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. [3] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. Can pre-trained vision and language models answer visual information-seeking questions? arXiv:2302.11713, 2023. [4] Zhanpeng Chen, Chengjin Xu, Yiyan Qi, and Jian Guo. Mllm is strong reranker: Advancing multimodal retrieval-augmented generation via knowledge-enhanced reranking and noise-injected training. arXiv:2407.21439, 2024. [5] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 2024. [6] Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. In ICCV, 2025. [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv:2507.06261, 2025. [8] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv:2505.16410, 2025. [9] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv:2507.19849, 2025. [10] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv:2504.11536, 2025. [11] Mingyang Fu, Yuyang Peng, Benlin Liu, Yao Wan, and Dongping Chen. Livevqa: Live visual knowledge seeking. arXiv:2504.05288, 2025. [12] Gemini. Gemini-3-flash. https://blog.google/products/gemini/gemini-3-flash/, 2025. [13] Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, et al. Webwatcher: Breaking new frontier of vision-language deep research agent. arXiv:2508.05748, 2025. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv:2501.12948, 2025. [15] Jack Hong, Chenxiao Zhao, ChengLin Zhu, Weiheng Lu, Guohai Xu, and Xing Yu. Deepeyesv2: Toward agentic multimodal model. arXiv:2511.05271, 2025. [16] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory. In CVPR, 2023. [17] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv:2410.21276, 2024. [18] Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, et al. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv:2509.01055, 2025. [19] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, et al. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv:2409.12959, 2024. [20] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv:2503.09516, 2025. [21] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP, 2020. [22] Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning patterns and interaction turns for visual search. arXiv:2509.07969, 2025. 13 [23] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. In NeurIPS, 2020. [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv:2408.03326, 2024. [25] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv:2501.05366, 2025. [26] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv:2503.23383, 2025. [27] Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual agentic reinforcement fine-tuning. arXiv:2505.14246, 2025. [28] Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, and James Glass. Sail: Search-augmented instruction learning. arXiv:2305.15225, 2023. [29] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv:2112.09332, 2021. [30] Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, and Zhe Gan. Deepmmsearch-r1: Empowering multimodal llms in multimodal web search. arXiv:2510.12801, 2025. [31] OpenAI. Thinking with images. https://openai.com/index/thinking-with-images/, 2025. [32] OpenAI. Gpt-5. https://openai.com/gpt-5, 2025. [33] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In NeurIPS, 2023. [34] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv:2402.03300, 2024. [35] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In EuroSys, 2025. [36] Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv:2505.01441, 2025. [37] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv:2505.15966, 2025. [38] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv:2505.08617, 2025. [39] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv:2506.23918, 2025. [40] Chenyu Wang, Weixin Luo, Sixun Dong, Xiaohua Xuan, Zhengxin Li, Lin Ma, and Shenghua Gao. Mllm-tool: multimodal large language model for tool agent learning. In WACV, 2025. [41] Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, and Yisen Wang. Monet: Reasoning in latent visual space beyond images and language. arXiv preprint arXiv:2511.21395, 2025. [42] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv:2508.18265, 2025. [43] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In AAAI, 2025. [44] Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing lmms to search. arXiv:2506.20670, 2025. 14 [45] Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In CVPR, 2024. [46] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2.5-omni technical report. arXiv:2503.20215, 2025. [47] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. [48] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv:2503.14476, 2025. [49] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, et al. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv:2410.10594, 2024. [50] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv:2408.13257, 2024. [51] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv:2508.11630, 2025. [52] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv:2507.18071, 2025. [53] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv:2403.13372, 2024. [54] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv:2505.14362, 2025. [55] Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, and Ranjay Krishna. Reinforced visual perception with tools. arXiv:2509.01656, 2025."
        },
        {
            "title": "A Additional Details on Training Data",
            "content": "In this section, we provide more details on the training data used for both the cold-start supervised fine-tuning (SFT) phase and the reinforcement learning (RL) phase. A.1 Cold-Start SFT Data The Cold-start SFT dataset is constructed to equip the model with foundational capabilities in tool usage and reasoning. As visualized in the main text, we apply rigorous filtering and synthesis pipeline to three primary data sources: FVQA [44]: Starting from the original training set of 4,849 samples, we identify \"hard\" samples where the base model frequently failed. After trajectory synthesis and validation, we retain 1,115 high-quality trajectories for SFT. Pixel-Reasoner Corpus [37]: We leverage the warm-start corpus containing 7.85k samples. Through our filtering process, we select approximately 2,000 samples that best demonstrate pixel-level reasoning capabilities. Curated Expert Data: To specifically enhance the models proficiency in complex visual scene with multi-step tool invocations, we manually construct the training subset comprising 200 intricate reasoning trajectories. This process yields total of approximately 3,315 high-quality samples for the cold-start SFT phase. A.2 Reinforcement Learning Data For the RL phase, we utilize larger and more diverse dataset to generalize the models reasoning and tool-use policies. The RL training set comprises: FVQA (Remaining): The subset of the FVQA training set not used for SFT, consisting of 3,695 samples. DeepEyes-4K [22]: We utilize 4,000 samples from the DeepEyes-4K training set to reinforce high-resolution visual analysis. Visual-Probe [22]: We include the complete training set of 5,729 samples to support broad visual reasoning tasks."
        },
        {
            "title": "B Additional Details on Evaluation Data",
            "content": "In this section, we provide more details on the evaluation data used for agentic search and visual reasoning task. B.1 Agentic Search For agentic search, we mainly rely on our proposed HR-MMSearch, along with MMSearch [19], FVQA-test [44], Infoseek [3], SimpleVQA [6], LiveVQA [11] and MAT-Search [27]: HR-MMSearch. Existing benchmarks, such as FVQA-test [44] or MMSearch [19], typically rely on standard HD or lower-resolution images to test holistic scene understanding. However, this approach leaves critical gap in evaluating an agents ability to understand fine visual details. To fill this gap, we introduce HR-MMSearch, benchmark designed to assess the fine-grained perception and search-reasoning capabilities of vision-language model (VLM) agents. As shown in Fig. 7, we construct the dataset through four-phase pipeline: large-scale image crawling, filtering, human annotation, and rigorous quality checking. We begin by crawling candidate images exclusively from three reputable international news outletsReuters, the Associated Press (AP), and CNBC. By focusing on timely news photographs, we ensure that the images depict recent events unlikely to appear in existing VLM pre-training data. This design choice reduces the chance of models relying on memorized knowledge and instead encourages genuine use of external tools. After collection, we apply strict filtering to retain only 4K-resolution images from 2025, minimizing pre-training leakage risks while providing rich, fine-grained visual detail. During the human annotation phase, three annotators (all holding bachelors degrees) independently assign each image to one of eight high-impact domains: Sports, Entertainment & Culture, Science & Technology, Business & Finance, Games, Academic Research, Geography & Travel, and Others. They then manually craft 16 Figure 7 Overview of the proposed HR-MMSearch dataset. This figure details the methodology used to construct the dataset and presents the resulting distribution of categories and difficulties. The colorbar uses gradation of dark, medium, and light to denote the number of total, hard, and easy samples, respectively. knowledge-intensive questions that target key visual subjects, especially small objects or text regions that occupy less than 5% of the image. Each question is designed so that solving it requires at least one of three multimodal tools: image search, text search, or image crop. Afterwards, three additional experts, each holding at least masters degree, carefully cross-verify the resulting 305 imagequestion pairs to confirm the labels, assess quality, and ensure answer correctness. To define difficulty levels, we adopt pass@8 evaluation protocol in which the agent generates eight independent rollouts for each question using the available tools. question is considered solved if at least one rollout yields the correct answer. Using this agentic setup, we run Qwen2.5-VL-7B-Instruct as representative agent to approximate question difficulty. Based on its performance, we categorize the 188 questions for which the model fails all eight rollouts as Hard. These failures typically arise on questions that require more complex reasoning or interaction, often involving three or more tool calls; notably, 17 of these questions require coordinated use of all three tools. The remaining 117 questions are classified as Easy, as the model produces at least one correct rollout. These easier questions generally require only one or two tool calls. As shown in the bottom-right panel of Fig. 7, the difficulty distribution is roughly consistent across all eight categories, with around 60% Hard samples and 40% Easy samples in each. Overall, HR-MMSearch offers challenging and diverse benchmark for evaluating the capabilities of tool-augmented VLM agents in agentic search and fine-grained visual reasoning. MMSearch. We use MMSearch [19] to test whether models can retrieve up-to-date information or reason about obscure facts. The full dataset contains 300 manually collected examples across 14 subdomains, split into News and Knowledge sections. The News section covers events starting from August 2024 to avoid overlap with training data, while the Knowledge section focuses on rare facts that often challenge advanced models. Similar to MMSearch-R1 [44], we only use the 171 questions that include images and exclude text-only queries to focus on real-world information seeking with visual grounding. FVQA-test. We use the FVQA-test set [44] to ensure our evaluation spans both visual and textual domains. This benchmark includes 1,800 high-quality examples from three sources. The first 600 come from FVQA-auto-vc and are verified for accuracy and separated from training data. Another 600 are taken from the InfoSeek Human Split, 17 with manually corrected answers to fix missing public labels. The final 600 were created by human annotators specifically for this benchmark to expand its coverage. InfoSeek. We evaluate real-world knowledge retrieval using the InfoSeek benchmark [3]. Its creators generated questions by converting Wikidata triples into natural-language questions using human-designed templates. These templates were developed for 300 relations and include placeholders for units and entity types to improve clarity. They removed unanswerable questions and balanced the dataset across entities to ensure quality. From the test split, we sample 2,000 instances to capture diverse set of factual queries. SimpleVQA. SimpleVQA [6] focuses on factual accuracy and real-world usefulness. It combines two types of examples: imagequestion pairs from post-2023 VQA datasets and new pairs produced by experts using internet search results. All examples pass strict filters for difficulty and quality to ensure they test objective information. From the full benchmark, we use the 1,013 English examples to evaluate factual reasoning without language-related noise. LiveVQA. To measure performance on fast-changing news, we include LiveVQA [11]. This dataset draws content from major international outlets such as CNN and the BBC and spans 14 categories, including science and sports. It contains 3,602 pairs generated with GPT-4o, ranging from simple visual checks to complex questions requiring reasoning over accompanying text. This range allows us to test how well models combine visual and textual information in dynamic news environments. MAT-Search. We include MAT-Search [27] to evaluate agentic search and multimodal multi-hop reasoning. This benchmark contains 150 high-quality examples, each manually crafted and verified by human annotators. The questions vary in difficulty and require different depths of reasoning, with more complex queries involving additional inference steps and factual knowledge. These examples are designed to test models ability to handle composite problems, retrieve relevant external information, and use tools effectively, providing focused evaluation of agentic multimodal reasoning. Figure 8 Overview of our Text Search Pipeline. The pipeline utilizes separate retrieval modes for training and inference. Local retrieval from Wikipedia knowledge base is used during RL training to avoid the prohibitive cost of live web searches, while live web search (via Serper API) is used exclusively during inference. Crucially, the retrieved passages from both separate modes are uniformly processed by Qwen3-32B summarizer before being passed to the main model. B.2 Visual Reasoning For visual reasoning, we mainly evaluate our models on V* Bench [45], HR-Bench [43] and MME-RealWorld [50]: V* Bench. V*-Bench [45] is designed to evaluate the detailed visual grounding and collaborative reasoning capabilities of VLMs, specifically addressing the need for visual search mechanisms. Sourced from the highresolution SA-1B dataset, it features images with an average resolution of 2246 1582 that are visually crowded or contain small details. For this study, we utilize all 191 images from the benchmark. The dataset includes human-annotated tasks focused on attribute recognition and spatial relationship reasoning, which are deliberately crafted to be unsolvable without precise visual processing and the ability to focus on specific, often obscure, visual elements. HR-Bench. HR-Bench [43] serves as specialized benchmark for assessing model performance on ultra-highresolution inputs, challenging VLMs to overcome the information loss typically associated with image resizing. It evaluates perception capabilities across fine-grained single-instance and cross-instance tasks. To rigorously test 18 the models scalability and detail preservation, we employ two distinct splits from this dataset: the 4K resolution split and the 8K resolution split, containing 800 images each. This setup allows for focused evaluation of the models stability and accuracy when processing inputs with extreme pixel counts. MME-RealWorld. MME-RealWorld [50] is large-scale, manually annotated benchmark targeting real-world scenarios that are perceptually challenging even for humans. It covers 43 subtasks across five key domains: OCR in the wild, remote sensing, diagrams and tables, video monitoring, and autonomous driving. The images feature high resolutions (average 2000 1500) and complex, clutter-heavy scenes requiring zooming and multi-step reasoning. In our experiments, we utilize 23,599 QA samples to evaluate the models robustness in handling diverse, high-difficulty visual perception tasks in practical applications."
        },
        {
            "title": "C Additional Implementation Details",
            "content": "In this section, we provide more details on the reward model and search pipeline, the differences between the direct answer, RAG, and agentic workflows, and the evaluation metrics used throughout our training and evaluation processes. C.1 Reward model During training for RL, we utilize Qwen2.5-VL-72B-Instruct as the LLM-as-a-Judge for all experiments. The judges response is generated using greedy setting with temperature of 0.0. The full prompt is given in Fig. 15. C.2 Comparison between Direct Answer, RAG and Agentic Workflows As described in Section 4.1 of the main paper, we evaluate our models against other baselines using three separate workflows, specifically Direct Answer, RAG, and Agentic Workflows. These workflows are distinguished primarily by the prompts used during inference, which control the tools available to the models. These different prompts are given in Fig. 16, Fig. 17 and Fig. 18. C.3 Text search pipeline The structure of our text search pipeline is illustrated in Fig. 8. Most aspects of the text-search setup are shared between training and inference. The main exception is that the text search method uses local retrieval during training and live web search during inference. During RL training, we avoid the prohibitive cost of live web searches by using locally hosted Wikipedia knowledge base built from the 20250901 dump file (enwiki-20250901-pages-articles.xml.bz2). Retrieval is performed with the E5-retriever [20], and the total number of returned passages is fixed at 5. Unlike prior work, such as MMSearch-R1 [44], we do not use the Jina API to extract clean, LLM-friendly text. Instead, to mitigate bot detection, our system uses Playwright to fetch the HTML content. To keep the pipeline simple and efficient, we skip JavaScript rendering and parse the static HTML directly using Pythons BeautifulSoup library. In contrast, during the inference phase, the text-search tool sends its queries through the Serper Search API. Crucially, in both the training and inference phases, the top 5 retrieved passages are first summarized individually by Qwen3-32B [47]. Following the individual summaries, final, holistic summary of all 5 passages is then generated. This shared two-stage summarization process ensures that the model learns the core tool-use behaviors on data formatted identically to what it will encounter in live setting. The complete prompts used for both page and final summarization are identical and are provided in Fig. 19. This training design has clear benefits. It is fast and lightweight. The trade-off is that the system cannot read webpages that depend on JavaScript. While supporting these pages could improve performance, this limitation is acceptable because the model still learns the core tool-use behaviors. We observe that these behaviors transfer reliably to inference. Notably, this transfer succeeds even though the model never encounters real Serper Search outputs during its training. C.4 Evaluation Metrics We run all evaluations with sampling temperature of 0.0 and choose different metrics based on the type of output in each domain. For agentic search tasks, we use an LLM-as-a-Judge setup because the answers are open-ended and require flexible semantic evaluation. In this setup, GPT-4o is used to score the Pass@1 accuracy by comparing the final response with the ground truth. For visual reasoning benchmarks, such as V* Bench [45], HR-Bench [43], and MME-RealWorld [50], we use Exact Match [22] because these datasets mainly contain closed-ended multiple-choice questions that require an exact string match with the correct option. To reduce variance caused by the sampling temperature in these strict visual tasks, we report Avg@8 accuracy for Bench and HR-Bench by averaging the Exact Match score across eight independent attempts for each question. For the large-scale MME-RealWorld benchmark, which has less variance, we report Pass@1 accuracy."
        },
        {
            "title": "D Additional Exprimental Analysis",
            "content": "In this section, we provide more detailed analysis for the tool use behavior based on SenseNova-MARS-7B. As shown in Fig. 9, The base model Qwen2.5-VL-7B shows an extreme bias. It relies almost exclusively on text search and ignores the image crop tool, making it ineffective for fine-grained perception. In contrast, SenseNova-MARS-7B shows strong multi tool ability that adapts well to task demands. SenseNova-MARS-7B illustrates its adaptability by identifying and executing the optimal strategy for each benchmark. For the fine-grained perception tasks in V* Bench, it relies almost entirely on the image crop tool. In the search-oriented MMSearch benchmark, on the other hand, it uses only search tools. In contrast, for the more complex HR-MMSearch, it adopts hybrid tool-use strategy. After the cold start, the models tool-use behavior tends to be redundant. However, RL training progressively streamlines this process, reducing the average tool calls from 4 to 2 as shown in Fig. 9. This demonstrates that our RL method successfully eliminates superfluous actions, improving both the agents efficacy and efficiency. Figure 9 Analysis of tool use behavior. Top: Distribution of tool calls across different benchmarks. Bottom Left: The tool use number in different benchmarks. Bottom Right: Evolution of tool call frequency in the RL training process, indicating that SenseNova-MARS learns more efficient tool invocation strategies."
        },
        {
            "title": "E Case Study",
            "content": "We present more SenseNova-MARS inference cases in Fig. 10, Fig. 11 and Fig. 12."
        },
        {
            "title": "F Limitations",
            "content": "Despite the strong performance of SenseNova-MARS, our error analysis reveals some limitations: Vulnerability to Retrieval Noise. As seen in Fig. 13, SenseNova-MARS occasionally fails to distinguish between semantically similar but distinct attributes (e.g., conflating based in with born in) within retrieved snippets. 20 This suggests that the current reasoning module lacks sufficient robustness against distractor information in open-world search results, leading to hallucinated reasoning paths. Ineffective Tool Usage. In some scenarios requiring fine-grained visual extraction as shown in Fig. 14, SenseNovaMARSmay fail to ground specific visual entities (e.g., CHED Regional Office 1) into the search query. Instead, it resorts to generic terms (e.g., in this region), resulting in the retrieval of irrelevant global statistics. This indicates gap in cross-modal alignment during the tool parameter generation phase. Figure 10 Case study 1 of SenseNova-MARS. 21 Figure 11 Case study 2 of SenseNova-MARS. 22 Figure 12 Case study 3 of SenseNova-MARS. 23 Figure 13 Case study 4 of SenseNova-MARS. Figure 14 Case study 5 of SenseNova-MARS. 24 System Message You are an AI assistant tasked with evaluating the correctness of model responses based on an image, question, and ground truth answer. Your judgment should follow these principles: If the model response is more specific form of the ground truth answer, it is correct. If the model response includes all key information but adds minor details, it is correct as long as the extra details are factually correct. If the model response contradicts, modifies, or omits critical parts of the answer, it is incorrect. 1. Consider the image, question, and ground truth answer holistically before evaluating the models response. 2. Your decision should be strictly Yes or No, based on whether the models response is factually accurate and aligns with the ground truth answer. 3. 4. 5. 6. For numerical values, ensure correctness even when presented in different units. 7. For names, check for first and last name correctness. If the middle name is extra but correct, consider it correct. 8. For yes/no questions, the response must exactly match \"Yes\" or \"No\" to be correct. 9. If the judgment can be made based solely on the text, you may choose to ignore the input image, as some images may be unfamiliar to you and could affect your judgment. Refer to the image only when necessary to minimize misjudgment. If there are multiple candidate answers, you can also evaluate the models response against all of them. If the response aligns with at least one candidate according to the rules above, it should be considered correct. 10. 11. For multiple choice questions (A, B, C, D), be more lenient. If the model provides the correct letter choice, even with additional text or formatting, consider it correct. If the models answer contains the correct choice letter (A, B, C, or D) anywhere in the response, and its clear this is the intended answer, mark it as correct. Ignore formatting issues like extra parentheses, brackets, or minor text variations as long as the core answer is correct. 12. 13. Your output must be in the following format: <judge>Yes/No</judge> <reason>Explanation of why the answer is correct or incorrect.</reason> Prompt # Prompt: Image, Question, and Model Response Evaluation Question: {question} Ground Truth Answer: {ground_truth_answer} Model Response: {model_response} Evaluation Instructions Evaluate whether the Model Response is correct based on the Image, Question and Ground Truth Answer. Follow the predefined judgment rules and provide clear Yes/No answer along with justification. Output Format <judge>Yes/No</judge> <reason>Detailed reasoning following the evaluation principles.</reason> Figure 15 Full prompt used for Qwen2.5-VL-72B-Instruct as the LLM-as-a-Judge. 25 System Message #Role You are step-by-step reasoning assistant. Given question, your task is to solve the problem **one substep at time**. ## Guiding Principles At each turn, you must **either**: 1. Issue **one specific tool** enclosed in <tool_call> </tool_call> tags, 2. Or provide the **final answer** enclosed in <answer> </answer> tags. All outputs **must begin with thought** enclosed in <think> </think> tags, explaining your current reasoning and what to do next. ## Output Format (strict): Always start with <think>. Do not output the previous reasoning chain. Then, depending on the case, output one of the following: 1. If reasoning continues: <think> Your current reasoning and next plan </think> <tool_call> One precise, tool call to assist your reasoning </tool_call> 2. If ready to conclude: <think> Summarize all reasoning and derive the answer </think> <answer> Final answer </answer> # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags: <tools> {\"type\": \"function\", \"function\": {\"name\": \"web_search\", \"description\": \"Search the web for information you dont have or to verify facts.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"Query to search the web\"}}, \"required\": [\"query\"]}}} {\"type\": \"function\", \"function\": {\"name\": \"crop_image\", \"description\": \"Crop the image based on the bounding box coordinates to zoom in on specific regions for detailed analysis.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"bbox\": { \"type\": \"array\", \"items\": { \"type\": \"number\" }, \"minItems\": 4, \"maxItems\": 4, \"description\": \"Normalized bounding box [x1, y1, x2, y2], where 0.0 <= x1 < x2 <= 1.0 and 0.0 <= y1 < y2 <= 1.0. (x1,y1) is top-left corner, (x2,y2) is bottom-right corner.\" }, \"image_index\": { \"type\": \"integer\", \"minimum\": 1, \"description\": \"Index of the image to crop: 1 for original input image, 2 for first cropped image, 3 for second cropped image, etc.\" }}, \"required\": [\"bbox\", \"image_index\"]}}} {\"type\": \"function\", \"function\": {\"name\": \"image_search\", \"description\": \"Reverse search the current image to get more information. This function does not accept any text queries or arguments.\", \"parameters\": {\"type\": \"object\", \"properties\": {}}}} </tools> For each function call, return json object with function name and arguments within <tool_call></tool_call> XML tags: <tool_call> {\"name\": <function-name>, \"arguments\": <args-json-object>} </tool_call> Prompt # Prompt: {image} {question} Figure 16 Full prompt used during training and inference for the Agentic Workflow. System Message You are helpful assistant. Prompt # Prompt: {image} {question} Figure 17 Full prompt used for the Direct Answer workflow. 26 System Message #Role You are step-by-step reasoning assistant. Given question, your task is to solve the problem **one substep at time**. ## Guiding Principles At each turn, you must **either**: 1. Issue **one specific tool** enclosed in <tool_call> </tool_call> tags, 2. Or provide the **final answer** enclosed in <answer> </answer> tags. All outputs **must begin with thought** enclosed in <think> </think> tags, explaining your current reasoning and what to do next. ## Output Format (strict): Always start with <think>. Do not output the previous reasoning chain. Then, depending on the case, output one of the following: 1. If reasoning continues: <think> Your current reasoning and next plan </think> <tool_call> One precise, tool call to assist your reasoning </tool_call> 2. If ready to conclude: <think> Summarize all reasoning and derive the answer </think> <answer> Final answer </answer> # Tools You may call one or more functions to assist with the user query. You are provided with function signatures within <tools></tools> XML tags: <tools> {\"type\": \"function\", \"function\": {\"name\": \"web_search\", \"description\": \"Search the web for information you dont have or to verify facts.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"Query to search the web\"}}, \"required\": [\"query\"]}}} {\"type\": \"function\", \"function\": {\"name\": \"crop_image\", \"description\": \"Crop the image based on the bounding box coordinates to zoom in on specific regions for detailed analysis.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"bbox\": { \"type\": \"array\", \"items\": { \"type\": \"number\" }, \"minItems\": 4, \"maxItems\": 4, \"description\": \"Normalized bounding box [x1, y1, x2, y2], where 0.0 <= x1 < x2 <= 1.0 and 0.0 <= y1 < y2 <= 1.0. (x1,y1) is top-left corner, (x2,y2) is bottom-right corner.\" }, \"image_index\": { \"type\": \"integer\", \"minimum\": 1, \"description\": \"Index of the image to crop: 1 for original input image, 2 for first cropped image, 3 for second cropped image, etc.\" }}, \"required\": [\"bbox\", \"image_index\"]}}} </tools> For each function call, return json object with function name and arguments within <tool_call></tool_call> XML tags: <tool_call> {\"name\": <function-name>, \"arguments\": <args-json-object>} </tool_call> Prompt # Prompt: {image} {question} To help you answer the question, here are reverse image search results for the given image. Reverse image search results: {image_search_results} System Message Figure 18 Full prompt used for the RAG Workflow. You are helpful assistant. Your task is to summarize the main content of the given web page in no more than five sentences. Your summary should cover the overall key points of the page, not just parts related to the users question. If any part of the content is helpful for answering the users question, be sure to include it clearly in the summary. Do not ignore relevant information, but also make sure the general structure and main ideas of the page are preserved. Your summary should be concise, factual, and informative. Prompt # Prompt: Webpage Content (first 30000 characters) is: {content} Question: {question} Figure 19 Full prompt used by Qwen3-32B to perform page summary and final summary."
        }
    ],
    "affiliations": [
        "SenseTime Research",
        "Tsinghua University",
        "University of Science and Technology of China"
    ]
}