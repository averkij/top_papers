{
    "paper_title": "Curia: A Multi-Modal Foundation Model for Radiology",
    "authors": [
        "Corentin Dancette",
        "Julien Khlaut",
        "Antoine Saporta",
        "Helene Philippe",
        "Elodie Ferreres",
        "Baptiste Callard",
        "Théo Danielou",
        "Léo Alberge",
        "Léo Machado",
        "Daniel Tordjman",
        "Julie Dupuis",
        "Korentin Le Floch",
        "Jean Du Terrail",
        "Mariam Moshiri",
        "Laurent Dercle",
        "Tom Boeken",
        "Jules Gregory",
        "Maxime Ronot",
        "François Legou",
        "Pascal Roux",
        "Marc Sapoval",
        "Pierre Manceron",
        "Paul Hérent"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base model's weights at https://huggingface.co/raidium/curia."
        },
        {
            "title": "Start",
            "content": "Curia: Multi-Modal Foundation Model for Radiology Corentin Dancette1*, Julien Khlaut1,2,3, Antoine Saporta1, Helene Philippe1,3,6, Elodie Ferreres1, Baptiste Callard1, Theo Danielou1, Leo Alberge1, Leo Machado1,6, Daniel Tordjman1, Julie Dupuis1, Korentin Le Floch1,2,3,4, Jean Du Terrail7, Mariam Moshiri8, Laurent Dercle9, Tom Boeken2,3,4, Jules Gregory6, Maxime Ronot6, Francois Legou10, Pascal Roux10, Marc Sapoval2,3,5, Pierre Manceron1, Paul Herent1 1*Raidium, 27 rue du faubourg Saint-Jacques, Paris, 75014, France. 2Department of Vascular and Oncological Interventional Radiology, Hˆopital Europeen Georges Pompidou, AP-HP, Paris, France. 3Faculte de Sante, Universite Paris-Cite, Paris, France. 4HEKA, INRIA, Paris, France. 5PARCC 970, INSERM, Paris, France. 6Department of Radiology, FHU MOSAIC, Beaujon Hospital, APHP.Nord, Clichy, France. 7.omics, Paris, France. 8Department of Radiology and Radiological Science, Medical University of South Carolina, Charleston, SC, USA. 9Department of Radiology, Columbia University Irving Medical Center, New York, NY, 10032, USA. 10Centre Cardiologique du Nord, Saint-Denis, 93200, France. *Corresponding author(s). E-mail(s): corentin.dancette@raidium.eu; These authors contributed equally to this work. Abstract AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, foundation model trained on the entire cross-sectional imaging output of major hospital over several yearswhich to our knowledge is the largest such corpus of real-world dataencompassing 150,000 exams (130 TB). On newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base models weights at https://huggingface.co/raidium/curia. Keywords: foundation model, computed tomography, CT, magnetic resonance imaging, MRI, deep learning 5 2 0 2 8 ] . [ 1 0 3 8 6 0 . 9 0 5 2 : r a"
        },
        {
            "title": "1 Introduction",
            "content": "Radiology is at the center of many medical specialties, which rely on radiologists interpretation of images from various modalities, including CT, MRI, ultrasound, and X-ray [1]. The analysis of these images is crucial for detecting and characterizing medical conditions, quantifying disease progression, and monitoring treatment efficacy across broad spectrum of diseases. AI has the potential to enhance radiology workflows and improve radiologists efficiency, particularly for labor-intensive tasks such as image segmentation, or specialized and/or complex tasks which are prone to inter-reader variability [2, 3]. To date, the dominant paradigm in radiological AI development has involved training specialized models for individual tasks such as segmentation, abnormality detection (e.g., tumor detection), or pathology classification. However, this one-task, one-model approach is exceptionally resource-intensive, as it necessitates the curation and manual annotation of large, taskspecific datasets for each modality and clinical application [4, 5]. It is potentially one of the bottlenecks in moving AI radiology models into the clinical workflow. Foundation models (FM) represent significant paradigm shift in the field of AI. More specifically, in 1 Fig. 1: Overview of Curia. Curia is radiological foundation model for CT and MRI images, trained with self-supervised learning with the DINOv2 algorithm on 200M images, based on the vision transformer architecture. (a) Pre-training methodology and statistics - All reported numbers correspond to the number of 2D images. PACS = Picture Archiving and Communication System. (b) List of tasks and pathology areas evaluated in the benchmark. We evaluated Curia on classification, regression, survival prediction, and segmentation tasks, and also explore generalization in few-shot and crossmodal settings. (c) Method for supervised evaluations: image-level prediction, object-level prediction, and volume-level prediction. (d) Radar plot of Curia-Ls performance against MedImageInsight and BioMedCLIP. Metrics are detailed in Fig. 2. To provide robust estimates, we report the mean performance over 1000 bootstrap samples for each task. (e) Performance comparison of Curia-L against resident radiologists. We report the mean performance with 95% confidence intervals, calculated over 1000 bootstrap samples, along with the statistical significance using paired bootstrap hypothesis test. the domain of natural images, self-supervised models such as DINOv2 [6] and MAE [7] have demonstrated the effectiveness of this approach, often reaching performances of supervised models. Leveraging large-scale unlabeled datasets, these models learn fine-grained semantic features that can be effectively transferred to downstream tasks using simple, lightweight classifiers with minimal or no fine-tuning. Adapting these methods to medical imaging is promising solution to tackle the plethora of radiological use cases across multiple image modalities. By assisting radiologists in detecting and characterizing diseases, 2 Our analysis is first structured to assess the models generalization capabilities on anatomical tasks, focusing on few-shot learning and cross-modality performance. We then evaluate its performance on our benchmark of medical tasks. We also conduct an indepth study in oncology, which demonstrates that the model can help predict risks associated to tumors and their related survival rates, and we study the attention maps of the prediction heads, giving interpretability to the models predictions. Those results highlight the potential of Curia to accelerate the development of robust, versatile, and data-efficient AI tools to enhance patient care, ultimately equipping the community with powerful, novel models that deliver tangible clinical impact."
        },
        {
            "title": "2 Results",
            "content": "A key aspect of FMs is their ability to adapt to multitude of downstream tasks with minimal task-specific fine-tuning. After the initial pre-training of Curia using the DINOv2 framework, our primary evaluation protocol consisted of training lightweight classifier on the features extracted from the frozen model backbone. More details about the methodology can be found in the Method Section 4. We compared Curia against two other FMs: MedImageInsight [15], an open-source visual embedding model by Microsoft, trained on multi-modal medical data from various domains (radiology, histology, pathology, dermatology, ophthalmology), for total of 3.8M images. It is based on the DaViT [16] architecture, and contains 360M parameters. BiomedCLIP [17], ViT-B model trained with contrastive learning on 15M (image, text) pairs extracted from PubMed. We present summary of the main results from our benchmark in Fig. 1d. More details about the benchmark can be found in the Benchmark Section 4.5. We also present examples of the whole benchmark in Fig. 2."
        },
        {
            "title": "Benchmark",
            "content": "To comprehensively evaluate Curia, we first established its proficiency in organ recognition across various body regions. We then investigated its data efficiency by evaluating our model in few-shot setting on these tasks. key focus of our study was to assess Curias cross-modality generalization. By training on CT scans and evaluating on MRI scans, we investigated whether the model could capture fundamental, modality-agnostic features. To further demonstrate its capabilities, we examined its performance on registration, task that inherently requires deep understanding of spatial anatomy. We finally evaluated Curia on prompted organ segmentation. these models may help improve patient outcomes and streamline clinical workflows. Ultimately, integrating FMs into radiology offers path toward enhanced diagnostic precision, innovative research, and personalized precision medicine [8, 9]. Previous research on FMs in radiology includes models such as BiomedCLIP [10], BiomedParse [11], and MedImageInsight [12]. These models have been trained on medium-scale datasets (e.g., 15M images for BiomedCLIP), larger than typical supervised training datasets, but considerably smaller than those used for FMs in natural language or vision (e.g., 120M images for DINOv2). More critically, their training sets are often heterogeneous mixtures of biomedical images from various medical specialties (including ophthalmologic imaging, pathology, radiology, endoscopy, and dermatology). Because these datasets typically aggregate specialized collections, they can introduce biases that constrain the models generalizability to novel scenarios and do not encompass the broad range of tasks radiologist performs in its daily activities. Adding to this challenge, the absence of unified benchmark has prevented rigorous comparison of these existing FMs [13]. In this article, we apply self-supervised learning to large-scale dataset of routine clinical cross-sectional imaging. Specifically, we pre-train vision transformer models (ViT-B and ViT-L [14]) on more than 200 million CT and MRI images (130 TB of data from 150K exams, see Fig. 1a.) using the DINOv2 [6] algorithm. Moreover, to assess the general performance of radiological FMs, we introduce comprehensive benchmark, CuriaBench, comprising 19 distinct radiological tasks (Fig. 1b.) that span both CT and MRI modalities and cover most anatomical regions. This benchmark encompasses broad spectrum of clinical cases that radiologist encounters, including disorders related to aging (e.g., Alzheimers disease, degenerative spine condition), emergencies (e.g., Anterior Cruciate Ligament (ACL) tears, abdominal trauma, brain hemorrhage), infectious diseases (e.g., lung infections), and oncological conditions with survival predictors (e.g., renal malignancy). It contains classification, regression, survival prediction and segmentation tasks, and allows us to explore generalization in few-shot and cross-modal settings. We evaluate Curia without any fine-tuning, and only train lightweight prediction heads with the models features (Fig. 1c.). Our model, which we have named Curia, sets new standard in radiological image interpretation. We present the result of Curia-B and Curia-L based on the ViT-B (for Base) and ViT-L (for Large) architectures [14]. Evaluation on our benchmark shows that Curia is highly adaptable across numerous tasks, performs strongly in few-shot learning scenarios, and demonstrates emergent cross-modal generalization capabilities Curia learns similar features for the same structures across modalities. Furthermore, the model consistently and significantly outperforms existing foundation models, such as BiomedCLIP and MedImageInsight (Fig. 1d.). Notably, Curia delivers performance comparable to, or even exceeding, the accuracy of resident radiologists on the benchmark tasks (Fig. 1e.). 3 Fig. 2: List of downstream tasks considered in the CuriaBench benchmark. For each task, we report the modality (CT/MRI), the type (2D/3D), the metric (Accuracy, AUC, Balanced Accuracy, r2), the number of classes for classification tasks, and the sizes of the training, validation, and test sets. The registration task and the prompted segmentation task are showcased in Fig. 3. Curia obtains excellent performance in anatomy classification We evaluated models on organ classification in both CT and MRI images, based on our CT Organ Recognition and MRI Organ Recognition benchmarks. Curia-L outperformed other FMs in organ classification on CT scans, achieving near-perfect accuracy score of 98.40% (Fig. 3a), outperforming both MedImageInsight, which achieved 88.19% (P < 0.001) and BiomedCLIP, which achieved 84.95% (P < 0.001). On MRI data, Curia-L obtained an accuracy of 89.11% 4 Fig. 3: Performance on anatomical tasks (a) Comparison of foundation models on the CuriaBench anatomical subset. Metrics are defined in Fig. 2. The error bars represent the 95% confidence interval derived from 1000 bootstrap samples. (b) Performance on Imaging Registration on three datasets: XCAT, Learn2Reg Brain, Learn2Reg Abdomen. (c) Crossmodality generalization on organ classification. We report the gap between the two modalities for each model. (d) Data Efficiency: performance of FMs and model from scratch with varying number of labeled samples on the Anatomy - CT task. All are models trained on the full dataset. (e) First two lines: Principal Component Analysis (PCA) visualization of feature maps from Curia, MedImageInsight, BiomedCLIP, and DINOv2 on CT and an MRI image. Last line: Image registration results using image features. displacement field was computed, and used to project the moving image to match the fixed image. We display the projected image for Curia, MedImageInsight, BioMedCLIP and DINOv2. We also show in red the positions of kidneys from the fixed image for reference. and also surpassed the other models: MedImageInsight with 63.18% (P < 0.001) and BiomedCLIP with 63.22% (P < 0.001). Curia can predict the age from brain MRIs From T1-weighted MRI image of healthy patients brain, the model was tasked with predicting the patients age. Our model Curia-L achieved an age 5 prediction RMSE of 6.15 years with an r2 score of 75.54 (Fig. 3a). In comparison, BiomedCLIP had an RMSE of 7.18 years and an r2 score of 69.41 (P < 0.001), whereas MedImageInsight obtained 6.72 years with an r2 score of 72.46 (P = 0.004). Curia is more efficient in the low-data regime than other FMs We investigated the few-shot learning capabilities of FMs using the CT Organ Recognition benchmark. Experiments were conducted with varying sample sizes per class, ranging from 1 to 40 images per class. In addition, we represent the performance of each model trained using all available data from the CT Organ Recognition benchmark with dashed line. The results, presented in Fig. 3d, indicate that Curias performance is near its maximum accuracy with small number of training samples. MedImageInsight exhibited lower performance compared to Curia across various sample sizes, with its final accuracy being 10.2 percentage points lower than Curias, and the performance gap was more pronounced at 20 samples per class, resulting in an approximately 20-point difference between the two models. model trained from scratch required 10 examples per class to reach more than 80% accuracy. Curia displays emerging properties of cross-modal generalization We evaluated Curias ability to generalize across different imaging modalities by training linear classification head for anatomy recognition on CT scans and evaluating it on MRI scans, and vice versa, based on the Cross-Modality Organ Recognition benchmark. We also performed the same experiment for MedImageInsight, BiomedCLIP, and ViT-L trained from scratch, and present the results in Fig. 3c. On CT to MRI, Curia demonstrated cross-modal generalization capability, exhibiting balanced accuracy decrease of 9.17 percentage points when evaluated on the out-of-distribution MRI dataset. Notably, Curia achieved higher accuracy on MRI in this zero-shot setting than other foundation models when trained directly on MRI. In contrast, other FMs showed larger drops in performance, ranging from 35.51 to 71.76 percentage points. As anticipated by the absence of pre-training, ViT-L model trained from scratch suffered the most pronounced performance degradation, highlighting its limited ability to generalize to the target modality. On MRI to CT, although other models exhibited more moderate performance dropsranging from 7.14 to 17.31the key finding was that Curia maintained virtually identical performance between the in-distribution MRI dataset and out-of-distribution CT dataset. Remarkably, its accuracy even improved slightly by 0.61 percentage points, underscoring its robust generalization ability across image modalities. Curia allows better volume registration across modalities than other FMs The results in Fig. 3b, complemented by the per-organ Dice Similarity Coefficient (DSC) values in Table D6 for XCAT and Table D4 for Learn2Reg Abdomen, demonstrate that Curia consistently outperformed or matched the performance of other models across all image registration tasks. For XCAT CT-to-CT registration, Curia-B and Curia-L achieved mean DSC of 81.30% and 80.12%, respectively, fairly close to 6 BiomedCLIPs performance on the task with 81.74%. All three models excelled in liver (94.26%, 93.37%, and 94.65% for Curia-B, Curia-L, and BiomedCLIP, respectively) and spleen (87.18%, 87.30%, and 90.22% for Curia-B, Curia-L, and BiomedCLIP, respectively). In XCAT MR-to-MR registration, Curia led with mean DSC of 86.10% and 84.25% for Curia-B and Curia-L, respectively, achieving the best scores across all organs. Similarly to organ classification, we explored crossmodal capabilities of Curia for image registration. For XCAT CT-to-MR registration, Curia achieved the highest mean DSC (64.03% and 65.25% for Curia-B and Curia-L, respectively) and outperformed others on all organ-specific metrics, with notable liver DSC of 86.12% for Curia-B and 85.34% for Curia-L. Although MedImageInsight and BiomedCLIP performed well in certain areas, they were less consistent, particularly in cross-modality tasks. On Learn2Reg benchmarks, Curia consistently outperformed other models. More specifically, on Learn2Reg Abdomen MRI/CT, Curia-B and Curia-L achieved mean DSC scores of 85.1% and 83.84%, respectively, greatly surpassing the performance of both MedImageInsight and BiomedCLIP, achieving 77.83% and 74.52%, respectively. On Learn2Reg Brain, the difference between model performance is less pronounced, but Curia-B and Curia-L still led with mean DSC scores of 77.68% and 77.96%, respectively, compared to MedImageInsights 75.91% and BiomedCLIPs 76.29%. Furthermore, we experimented with DINOv2 Large as baseline for image registration. Despite not being it achieved respectable trained on medical performance across benchmarks, at times even outperforming MedImageInsight and BiomedCLIP or matching Curias performance. images, Finally, it is also worth noting that Curia maintained competitive results across benchmarks on smoothness metric measured by the standard deviation values of the log of the Jacobian determinant (stdLogJ) [18]. To further investigate the behavior of the different FMs on image registration, we performed PCA visualizations of their extracted feature maps. Specifically, we projected the high-dimensional features onto 2D space to qualitatively assess the semantic alignment between modalities. These visualizations were generated on an MRI image and its corresponding registered CT images. The results, shown in Fig. 3e, reveal distinct structural patterns in the feature embeddings, offering insights into how well each model captures anatomical consistency across modalities. Interestingly, while MedImageInsight and BiomedCLIP delineated anatomical regions to some extent, their projections predominantly exhibited one or two dominant colors, indicating limited variation across principal components. This suggests lower diversity in their feature representations. In contrast, Curias feature maps displayed broader range of colors corresponding to different anatomical structures. This increased visual complexity reflects richer and more discriminative embedding space, aligning with Curias stronger quantitative performance in registration tasks. Fig. 4: Performance of Curia, SAM and RadSAM on the Prompted Segmentation benchmark. (a) Performance with bounding box prompt. For each structure, we report the average Dice Score over all samples. (b) Performance with point prompt. For each structure, we report the average Dice Score over all samples. (c) Visualization of the segmentation maps predicted by Curia-B and Curia-L with the SAM decoder. We compare the results with SAM and RadSAM on the same image and prompt. Top row: with bounding box prompt; Bottom row: with point prompt. prompts, respectively, highlighting the importance of designing segmentation methods specifically for the medical domain. In Supplementary Table C3, we also report the DSC scores per organ. These results demonstrate the quality of Curias features, which has not been pre-trained for segmentation but attained results comparable or better than RadSAM. Qualitative results are presented in Fig. 4c., showing the predictions of each model. Curia-L successfully segmented all disconnected components of the liver using only single point prompt. RadSAM and CuriaB were accurate on the main regions but failed to capture one component. With bounding box prompt, all models produced similar segmentation masks, except for the original SAM model, which again failed to generate accurate segmentation, as in the example with the point prompt. Curia can be adapted for prompted segmentation, matching the performance of specialized models We compared Curia in the prompted segmentation framework [19] for radiological images, similar to available models such as MedSAM [20] or RadSAM [21]. We conducted the same evaluation protocol as used in RadSAM, employing both point and bounding box prompts on the Prompted Organ Segmentation benchmark. We replaced the original SAM vision encoder with our Curia backbone and performed two-stage fine-tuning process. We finally evaluated our approach against SAM and RadSAM (Fig.4a.,4b.). Our performance results were on par with RadSAM. Using bounding box and point prompts, the RadSAM model achieved DSC scores of 91.08% and 85.27%, respectively. In comparison, our Curia-L model obtained DSC scores of 91.49% and 87.49% while Curia-B got 91.13% and 87.87%. We also evaluated the original SAM model, which was pre-trained on approximately 1 billion masks from 11M non-medical images. It achieved significantly lower DSC scores of 70.16% and 33.04% with bounding box and point 7 Fig. 5: Performance of Curia on Oncology-related tasks (a) Results of CuriaBench Oncology subset Tumor anatomical site, kidney tumor and lung nodule malignancy, and renal malignancy survival. We compared Curia against BioMedCLIP and MedImageInsight, as well as Harvard Onco-FM [22] for lung nodule malignancy. The 95% CI of the scores obtained with 1000 bootstrap samples is shown by error bars, except for Harvard-RT for which we report the score from the original publication. (b) Performance (AUC) of Curia, BioMedCLIP, MedImageInsight, and ViT-B without any pretraining, with varying number of examples in the training set of the Kidney lesion malignancy task. The error bars represent the variance over 5 runs for each point. (c) ROC curves for Lung nodule and Kidney lesion malignancy tasks. All models were evaluated 5 times, and we aggregated the predictions using the pooling method [23, 24] to display the final ROC curve. (d) KaplanMeier curves for groups, stratified by local stage (T1/T2 vs T3/T4), and by models risk prediction. The error bars represent the 95% CI of the estimates."
        },
        {
            "title": "2.2 Evaluation on Oncology Benchmark",
            "content": "Curia outperforms generalist and specialized FMs in oncological tasks To investigate Curias performance in oncology, we evaluated the task of finding the tumors localisation (Fig. 5a). Curia-L achieved balanced accuracy of 91.74%, while BiomedCLIP and MedImageInsight attained balanced accuracy of 86.39% (P < 0.001 ) and 88.91% (P = 0.041), respectively. We then evaluated our models on two tumor classification tasks for kidney lesion and lung nodules where the aim is to predict the malingnancy of the tumor. 8 We report the aggregate scores in Fig. 5a and we also show the ROC curves in Fig. 5c. On kidney lesion malignancy classification, Curia-B achieved an average AUROC of 74.41 and Curia-L 80.29. This result surpassed the score achieved by other foundational models, with BiomedCLIP attaining 62.95 (Curia-B < 0.001, Curia-L < 0.001) and MedImageInsight achieving 67.81 (Curia-B = 0.177, Curia-L = 0.025). We display the ROC curve in Extended Fig. A1. Regarding lung nodule malignancy, Curia-B obtained an average AUROC of 94.98, and CuriaL 92.45 while MedImageInsight and BiomedCLIP obtained comparable scores of 92.18 (Curia-B = 0.482, Curia-L = 0.993) and 88.72 (Curia-B = 0.126, Curia-L = 0.876), respectively. Additionally, we also compared to the original paper by Pai et al. [25], using their dataset split. Their Onco-FM obtains an AUROC of 94.40 with full fine-tuning, and 88.23 with feature-based approach. Curia outperformed these results, without any fine-tuning of the base model. We display the ROC curve in Extended Fig. A1. Finally, we investigated the low-data regime, as illustrated in Fig. 5b. Similar to the anatomical tasks, we evaluated the FMs on the Kidney lesion malignancy task using varying numbers of training examples, and compared their performance to FMs fine-tuned on the full dataset (dashed lines). While we see high variances with low number of examples(5-25), Curia-B and Curia-Ls performance increased greatly with additional examples, outperforming the other models by large margin, when trained with more than 50 examples per class. The model trained from scratch did not learn to classify kidney lesions at all with small number of examples, obtaining an AUC of around 0.5. Curia helps predict survival rate in cancer patients To probe the capacity of our FM for complex clinical reasoning, we tackled the challenging problem of oncologic prognosis, focusing on the prediction of survival time at baseline, using cox regression model [26, 27] on the models features. On cohort of 183 patients with renal malignancies from TCIA [28], resident radiologists annotated the lesion positions with pixel-level masks. We then trained the cox regression model [27] to predict the survival time utilizing the concordance index (c-index) as readout. We benchmarked the image-based survival predictor against conventional tumor staging using the T-stage of the TNM classification, also known as the local stage. Tumors were categorized into low-stage (T1T2) and high-stage (T3T4) groups according to their locoregional spread. Curia-B and Curia-L achieved c-index of 0.71 and 0.63, respectively, for survival prediction. Notably, Curia-B substantially outperformed both BiomedCLIP (c-index: 0.64, Curia-B = 0.035, Curia-L = 0.79) and MedImageInsight (c-index: 0.63, Curia-B = 0.003, Curia-L = 0.99) on the benchmark. The local stage alone obtained c-index of 0.66. The plot of the two Kaplan-Meier curves is shown in Fig. 5d."
        },
        {
            "title": "Curia achieves leading performance in\nmusculoskeletal disease assessment",
            "content": "The model accurately classified the severity of degenerative disease of the lumbar spine. It was able to assess three types of conditions: foraminal narrowing, subarticular stenosis, and spinal canal stenosis, defined through three severity levels (Mild, Moderate, or Severe). Curia-L obtained AUROC scores of 86.21 for foraminal narrowing, 87.46 for subarticular stenosis, and 93.73 for spinal canal stenosis. As shown in Fig. 6a. Compared to other FMs, Curia-L obtained comparable or better performance on foraminal narrowing 84.45 for BiomedCLIP (P = 0.07) and 86.32 for MedImageInsight (P = 0.87) and equivalent performance on spinal cord stenosis 92.33 for BiomedCLIP (P = 0.344) and 92.98 for MedImageInsight (P = 0.243). Notably, Curia-L outperformed both models on subarticular stenosis: BiomedCLIP achieved 83.92 (P = 0.02) and MedImageInsight achieved 85.61 (P < 0.001). We also studied the performance of Curia on ACL tear benchmark in knee MRI. By showing cropped region of interest around the ligament of interest, Curia-L obtained an AUROC of 87.34, whereas BiomedCLIP and MedImageInsight achieved significantly lower scores with 81.97 (P = 0.004) and 78.39 (P = 0.013), respectively (Fig. 6a)."
        },
        {
            "title": "Benchmark",
            "content": "Curia delivers competitive results in emergency medicine Fig. 6c shows the performance of Curia on multiple emergency-related medical tasks. First, the model was able to detect the presence or absence of intracranial hemorrhage on head CT examinations. Curia-L reached an AUROC of 93.54 on the test set. In comparison, MedImageInsight and BiomedCLIP achieved lower AUROC scores of 90.11 (P < 0.001) and 87.77 (P = 0.015), respectively. Curia accurately detected myocardial infarction in 2D cardiac MRI images. From squared mask around the myocardium, Curia-L obtained an AUROC of 89.16. MedImageInsight obtained higher score of 94.08 (P = 0.104), while BiomedCLIP was significantly lower at 71.39 (P < 0.001). Curia was also able to detect signs of active intra-abdominal bleeding on abdominal CT images. Curia-L achieved an AUROC of 87.10, while MedImageInsight and BiomedCLIP obtained AUROCs of 93.14 (P < 0.001) and 79.14 (P < 0.001), respectively. Finally, Curia could detect signs of past strokes on brain T1-weighted MR images. Curia-L obtained an AUROC of 89.78, while MedImageInsight and BiomedCLIP obtained 88.62 (P = 0.001) and 85.72 (P < 0.001), respectively. We display the ROC curves in Extended Fig. A1 for those three tasks. 9 Fig. 6: Performance of Curia, BioMedCLIP and MedImageInsight on four subsets of CuriaBench. Metrics are described in Fig. 2. The metrics reported are the average across 1000 bootstrap samples for 5 runs, and we show the 95% confidence intervals. (a) Musculoskeletal (MSK) conditions (b) Infectious conditions (c) Emergency-related conditions (d) Neurodegenerative conditions"
        },
        {
            "title": "Benchmark",
            "content": "0.936, Curia-L = 0.325), respectively, as shown in Fig. 6d. Curia is competitive on neurodegenerative disease We evaluated Curia on Alzheimers disease prediction on brain MRI images. On the full MRI volume, Curia-B and Curia-L obtained an AUROC of 87.83 and 87.74, respectively, whereas BiomedCLIP and MedImageInsight obtained scores of 88.19 (Curia-B = 0.003, Curia-L = 0.005) and 87.66 (Curia-B ="
        },
        {
            "title": "2.6 Evaluation on Infectious Benchmark",
            "content": "Curia achieves superior accuracy in pulmonary infection detection compared to previous FMs We evaluated Curia on pulmonary infections with our dedicated benchmark, which contained images of patients diagnosed as COVID-19 positive, non COVID pneumonia positive, or negative. Curia-L obtained 10 balanced accuracy of 93.40%, outperforming BiomedCLIP that obtained 89.25% (P < 0.001) and MedImageInsight with 89.97% (P < 0.001) as shown in Fig. 6b."
        },
        {
            "title": "Curia outperforms radiology residents on most\ntasks",
            "content": "Fig. 1e compares the performance of Curia against the average scores of four final-year radiology residents across fourteen different medical imaging tasks. More details on the radiologist evaluation method is given in the Methods Section 4.4. The results show that Curia obtained performance comparable to, and often higher than, the radiologists predictions. Overall, the data indicate that Curia was reliable across wide range of medical imaging tasks, suggesting that it could be valuable tool to support clinical diagnosis and enhance diagnostic consistency."
        },
        {
            "title": "2.8 Interpretability of Curia’s",
            "content": "predictions To qualitatively asses the focus and interpretability of the feature maps of FMs, we acquired the attention maps for Curia, BiomedCLIP, and MedImageInsight on the intracranial hemorrhage classification task. These maps, shown in Fig. 7a, represent the cross-attention weights of the best-performing classifiers, each trained with single query vector. Negative instances yielded more diffuse attention patterns, aligning with the premise that no singular region is indicative of negative finding. It was also observed that BiomedCLIP, which demonstrated the lowest classification accuracy, generated the most widespread attention maps, often encompassing areas beyond the anatomical boundaries of the brain. To further acknowledge the robustness and generalization of Curias feature representations, we performed patch-level keypoint matching using Curia-B between the features of source and target 2D image as shown in Fig. 7b. Keypoints are randomly sampled from the source image and matched to the most similar patches in the target image based on cosine similarity scores. We used three datasets from the image registration benchmark: OASIS [29], Learn2Reg CT-Abdomen and Learn2Reg-MR-CT [18]. We conducted this experiment under different setups: using MRI as the source and CT as the target from the same patient, as well as using source and target images from different patients but within the same modality. The results demonstrate Curia-Bs ability to perform cross-modality and interpatient feature transfer, highlighting its capacity to understand relationships between anatomically similar regions across different imaging modalities."
        },
        {
            "title": "2.9 Scaling curves",
            "content": "Extended Fig. A2 presents series of experiments with varying dataset sizes and training durations for subset of our benchmark tasks on the ViT-B and ViT-L architectures. These results highlight that both dataset size and training duration are important factors for downstream performance."
        },
        {
            "title": "3 Discussion",
            "content": "In this article, we introduced Curia, multi-modality FM for radiology, with comprehensive benchmark of 19 tasks to evaluate its capabilities. Our results demonstrate that by pre-training on large-scale dataset of over 200 million unlabeled CT and MRI images, Curia established new standard in radiological image interpretation, consistently outperforming existing models. major contribution of this study is the demonstration that self-supervised applied to large, unlabeled dataset can produce model with robust generalization capabilities. Unlike previous models trained on smaller, more specialized, and often heterogeneous biomedical datasets, Curias training on large body of routine clinical images has resulted in deep, transferable understanding of complex anatomy and pathology. This is evidenced by its superior performance on wide array of tasks spanning different anatomical regions (abdomen, brain, chest) and medical specialincluding oncology, musculoskeletal conditions, ties, and emergency imaging. One of the most significant findings is Curias emergent property of cross-modal generalization. The model, despite being trained on CT and MRI data without explicit pairing, can generalize features from one modality to another. For instance, when trained for organ recognition on CT images, it demonstrates strong ability to perform the same tasks on MRI, significantly outperforming other models, which suffer from substantial performance drops. This suggests that Curia has learned modality-agnostic representations of anatomical structures, critical step toward creating truly universal radiological AI. This capability is further highlighted in registration tasks, where Curia excels at CT-to-CT, MR-to-MR, and even the more challenging cross-modality CT-to-MR alignments, maintaining high accuracy and plausible deformations. Our experiments also show the data efficiency of the FM paradigm. Curia displays strong few-shot learning performance, achieving high accuracy on anatomical classification tasks with small number of labeled examples. This is crucial advantage in the medical imaging domain, where large, expertly annotated datasets are notoriously difficult and costly to produce. In oncological imaging, we demonstrate that the model can automatically and efficiently characterize lesions, labeling them as benign or malignant, which could aid physicians. Additionally, the model exhibits strong performance on risk assessment, surpassing the score used in clinical practice to predict survival for kidney cancer, paving the way for complex FM-derived predictive biomarkers in oncology. Although our findings are encouraging, this study has certain limitations. First, while the pre-training dataset was large and diverse in content, the data Fig. 7: (a) Visualization of the attention maps. Images displayed are windowed to brain standard viewing parameters (level=40, width=80) with varying blue-to-red colormap corresponding to increasing attention scores. The attention maps were computed between the final patches of each model and single learnable query vector, highlighting the areas used in the decision-making process. (b) Visualization of keypoint matching. The first row illustrates keypoint matching between two MRI images from different patients in the OASIS dataset. The second row presents matches between two CT images from different patients in the Learn2Reg CT-Abdomen dataset. The third and fourth rows demonstrate cross-modality matching between MRI (source) and CT (target) images from the same patient in the Learn2Reg MR-CT dataset. source is from single center, which may introduce institutional biases affecting generalizability, such as site-specific imaging protocols, reliance on specific vendor for the imaging equipment, or specificities of the local patient population. However, the impact of this limitation is somewhat mitigated through utilization of multi-center evaluation benchmark, which could support the claim of generalizability of Curia. Second, Curia is fundamentally 2D model, processing volumetric CT and MRI data on image-by-image basis. While this approach is computationally efficient, it necessitates the addition of specialized prediction heads to aggregate 2D features for 3D tasks such as volumetric segmentation or registration. native 3D FM could potentially offer improved performance on tasks that require deeper characterization of volumetric images. Lastly, while our benchmark includes 19 well-defined radiological tasks across CT and MRI, it does not yet cover the entire spectrum of imaging practice which also include ultrasound, X-ray imaging, and nuclear medicine imaging, some of which are cornerstones of global diagnostic workflow. Expanding the benchmark to include additional imaging modalities and clinical tasks will be essential to further assess and extend the universality of models like Curia. Finally, translating this significant technical achievement into practical clinical asset is complex endeavor where AI excellence is not the only requirement. Integration into 12 hospital IT systems (PACS), adherence to strict regulatory standards, and acceptance within established physician workflows are other hurdles that need to be addressed to truly translate FM research into clinical use. In conclusion, Curia represents significant advancement in the application of FMs to radiology. By leveraging large-scale, self-supervised pre-training, it achieves leading performance across diverse set of clinical tasks, demonstrates impressive data efficiency, and exhibits powerful cross-modal generalization. This study provides robust foundation and standardized benchmark for future research in the field, paving the way for the development of more powerful, versatile, and data-efficient AI tools that can enhance diagnostic accuracy and assist clinical workflows. Looking forward, the evolution of Curia will likely center on incorporating rich, multimodal data from electronic health records and textual reports. Such an approach promises to unlock deeper level of contextual understanding, significantly boosting generalization and enabling conversational interactions where clinicians can interact with the model using natural language."
        },
        {
            "title": "4.1 Pre-training recipe",
            "content": "Pre-training dataset curation We partnered with private hospital to create dataset from routine cross sectional clinical examinations from 2019 to 2022. All exams were completely anonymized (all identifying metadata was removed, and defacing was applied on exams encompassing the patients head). The original dataset contains 130TB of data, totaling 228M DICOM files (164M CT and 64M MR DICOM files). To ensure high-quality data, only 3D CT and MR exams with at least 5 images were kept, and low-quality localizer or scout sequences were removed. For our study on scaling curves, we created sub-versions of our dataset of different sizes: 30K, 200K, 2M, 20M and 200M images. Large-Scale Pre-training Preprocessing All images were resized using bilinear interpolation to fixed 512x512 dimension, and then normalized using z-score standardization. Input images are divided into 16x16 patches like shown in Fig. 8. For CT images, we sampled all images in the axial axis. For MRI, we sampled images following the acquisition axis. For BiomedCLIP and MedImageInsight, we followed the pre-processing, and automatically applied windowing adapted to the task when possible (Curia was not trained with windowing, all images were processed with the same normalization). Architecture and training We used standard Vision Transformer [14] models for the architecture of Curia. We adapted the DINOv2 codebase [6] for medical imaging. We trained two variants of this model: ViT-B, resulting in Curia-B, which contains 86M parameters, and ViT-L, resulting in Curia-L, which contains 300M parameters. We trained the ViT using the self-supervised learning objective from DINOv2 [6]. It is combination of multiple losses: an imagelevel objective (aligning representations of class tokens between teacher and student network), patchlevel objective (based on masking random patches), and multiple regularization losses. We used DINOv2 default augmentations, except for the image rotations and color jittering, following [30] advocating for using only cropping for self-supervised learning. We trained our model on smaller datasets to find optimal learning rates and hyperparameters (learning rates and transforms). We report the final parameters in Table B1. Our final models were trained on 475,000 steps on distributed cluster of 16 A100 GPUs for the ViT-B, and 32 A100 for the ViT-L. Curia-B is trained on 20M images, while Curia-L on the full dataset of around 200M images. The training time was approximately 5 days for the largest model."
        },
        {
            "title": "4.2.1 Adapting the model for downstream",
            "content": "tasks To adapt the model for downstream tasks, we trained classification, regression, and survival heads on top of 13 our FM, without fine-tuning the ViT weights. This allowed us to have lightweight and fast adaptation for downstream tasks. All heads, unless otherwise specified, were trained using stochastic gradient descent and cosine scheduler, with grid search of 10 learning rates. The best head was chosen on held-out validation set and evaluated on the test set to obtain the final results. For each task, we report the chosen head in Supplementary Table B2. Image-level classification tasks We evaluated multiple types of heads for classification tasks shown in Fig. 9. 1. Classification from the class token: Similarly to DINOv2 [6], we trained linear layer on top of the CLS token of the ViT. This is suitable for image-level classification tasks, but may fail if the task requires identifying fine-grained details in the image. 2. Classification from patch tokens: we pooled all the class tokens together using an average or max pooling, then trained linear layer, similarly to the previous method. 3. Attention-based pooling: we added single crossattention layer with learned query to aggregate the patch tokens. The model could then learn to use specific parts of the image if necessary. Mask-level classification tasks Some tasks involve classifying zone in the image for example, identifying specific organ. Instead of cropping the image around the organ and feeding this crop to the vision transformer, we input the whole image. We then apply similar methods to the image-level classification tasks: apply an average pooling of all mask tokens and perform linear classification, or use cross-attention block followed by linear classifier. We show in Fig. 9b the two methods. Handling 3D volumes tasks with 2D image model For downstream tasks with 3D volumes, (e.g. lung nodule malignancy, or ACL tear), we forwarded all the images of the volume separately as detailed in Fig. 1c. We were then able to apply similar methods to the 2D case: in the volume-level classification (without mask) setup, we either pooled the patch tokens or the class tokens, and performed linear classification, or trained cross-attention layer to perform the pooling. In the mask-level classification setup, similarly, we performed an average pooling on the mask, or trained an attention head on the mask patches. Adapting the model for regression downstream tasks We trained regression head on top of class of mask tokens to perform regression tasks. We trained linear heads and multi-layer perception (MLP) heads, both with the mean squared error (MSE) loss. The MLP Fig. 8: The Vision Transformer architecture. The image is tokenized into 16x16 patches, then converted into tokens and fed into the vision transformer. An additional class token is added, to perform image-level pre-training tasks. (a) (b) Fig. 9: Classification methods from the output of the vision transformer. (a) Image-level classification methods. We either used the class token, or pooled the patch tokens using an average or cross-attention with learned query. (b) Mask-level classification methods: we pooled the mask tokens, using either an average or crossattention with learned query, and then performed linear classification. head was made of three layers, and the ReLU activation function and batch normalization were used between each layer. Adapting the model for survival prediction To perform survival prediction, we added linear head on top of the models and used the torchsurv package [27] to compute the negative partial log likelihood loss from the Cox survival framework. This loss took into account the censoring that happens in longitudinal data. The main metric used to perform model evaluation and selection was the c-index. Additionally, to separate the test samples into two groups, we chose threshold that maximized the log-rank test statistic on held-out validation set. Adapting the model for registration tasks Zero-shot registration can be achieved using Curia or, equivalently, any model that can encode both global image information (class tokens) and patches (patch tokens). We followed the methodology described in [31]. In their approach, class the DINO-Reg paper tokens are first used at the image level to perform rigid registration by estimating pairwise differences between images. Subsequently, deformable registration is achieved by using patch tokens as features in an optimization problem. This two-step adaptation enables the evaluation of the quality of both global (class tokens) and local (patch tokens) features. In order to obtain fine-grained deformation fields for Curia, and BiomedCLIP models, images were upsampled to 1024x1024 and 896x896, respectively, leading to 64x64 and 56x56 deformation fields, respectively. For MedImageInsight, this led to poor results (probably due to its different vision encoder architecture), and therefore, we chose the best configuration, which was to keep the models input dimensions but extract features after the first block leading to 60x60 deformation field. Adapting the model for prompted segmentation We followed the promptable segmentation paradigm established by recent works like SAM [19] and RadSAM [21]. The architecture comprises three core components: (1) vision encoder, (2) prompt encoder, 14 and (3) mask decoder. For the vision encoder, we replaced the original SAM encoder with our Curia, which was initialized with its pre-trained weights. The prompt encoder and mask decoder architectures were adopted and initialized directly from SAM. We employed two-stage training procedure to effectively integrate the components. In the first stage, we froze the weights of the Curia vision encoder and trained only the prompt encoder and mask decoder. This allowed the model to learn the prompt-decoding mechanism without altering the powerful base features of Curia. In the second stage, we unfroze the vision encoder and performed an end-to-end fine-tuning of the entire model, allowing all parameters to adapt to the target segmentation task. In the first stage, we trained Curia-B and Curia-L for 6 epochs with global batch size of 384 and learning rate of 103. In the second stage, we trained Curia-B for 8 epochs and Curia-L for 6 epochs with global batch size of 384 and learning rate of 7.5e5. All trainings were performed on 4 nodes with 4 80GB NVIDIA A100 GPUs."
        },
        {
            "title": "4.3 Statistical Analysis",
            "content": "For all supervised training experiments, we performed non-parametric bootstrapping with 1,000 samples to report 95% confidence intervals. Specifically, to better evaluate each model performance across training runs, we applied bootstrapping to the mean performance over 5 runs. We report bootstrapping metrics on each benchmark in Appendix E.1. For statistical significance, we performed two-sided paired bootstrap test with 1000 samples on the mean performance across 5 training runs for each model pair, in order to estimate the p-value. We report statistical significance results in Supplementary Table E.2."
        },
        {
            "title": "4.4 Radiologist Evaluation",
            "content": "We created tool to evaluate radiologists on the benchmark tasks. For each task, the radiologists had the possibility to visualize training set examples along with their labels, change the windowing, and scroll through the images if the exam is in 3D. The training set label distribution was displayed. They were then asked to annotate subset of the testing set, on which score was calculated. When comparing with Curia in Fig. 1e., we computed the metrics on the same subset for each task. Our cohort of evaluators consisted in four resident radiologists in Paris-area hospitals."
        },
        {
            "title": "4.5 The CuriaBench Benchmark",
            "content": "This section describes CuriaBench, benchmark consisting in 19 downstream tasks we used to evaluate Curia and other FMs. Fig. 2 presents example images for each task, detailing their modality, the number of images in the training, validation, and test sets, and the performance metric used in the benchmark."
        },
        {
            "title": "4.5.1 Anatomical Benchmark",
            "content": "CT Organ Recognition To create CT Organ Recognition, we used the Total Segmentator (TS) [32] dataset to create benchmark for organ classification on CT scans. The task consisted of predicting the organ class based on the image and mask of the organ. We merged some classes from TS together, such as individual ribs and vertebrae, due to the difficulty of distinguishing them in 2D images. We sampled one image from TS for each 3D volume and annotated organ pair, weighted by the number of mask pixels on each image. The final dataset contained 54 organ labels. We used part of the training set to build held-out testing set. The training, validation and test sets contained 23,096, 1200 and 1554 samples, each containing one image-mask-target triplet."
        },
        {
            "title": "MRI Organ Recognition",
            "content": "We used the same process with Total Segmentator MRI [33] to create the MRI Organ Recognition task. It contained 56 classes. The final dataset contains 14,197 training samples, 1559 validation samples and 1259 test samples. Cross-Modality Organ Recognition For Cross-Modality Organ Recognition experiments, we follow similar procedure to construct both CT and MRI benchmarks, but restrict the label space to the 41 anatomical classes shared across both modalities. This ensures consistent evaluation of generalization performance between modalities. The final datasets contained 54,394 training samples, 2,718 validation samples and 4,996 test samples for CT, and 13,470 training samples, 1,412 validation and 1,412 test samples for MRI. Neuroimaging Age Estimation Neuroimaging Age Estimation is prediction task formulated as regression problem using the IXI dataset [34]. The dataset comprised approximately 600 MR images collected from normal, healthy individuals, with mean age of 48 years (16). For this task, we partitioned the dataset into 393 volumes for training, 80 for validation, and 94 for testing. Subsequently, we extracted 20% of the brains axial images from the T1weighted MR images and trained the model on these images to predict the patients age. Image Registration the patch level. FMs Registration tasks evaluate the models fine-grained representations at should ensure patch-level feature consistency across patients (anatomical registration), time (longitudinal tracking), and modalities (cross-modality alignment) enabling respectively population analysis/generalization, disease progression tracking, and multimodal support. Three registration tasks were used to evaluate the proposed model and comparison to existing models: two tasks from the Learn2Reg challenge [18] and one synthetic multi-modal task. These tasks were evaluated in zero-shot manner, meaning that no additional training or fine-tuning of the model was performed. Learn2Reg Abdomen MRI/CT - Intra-Patient Registration. The Learn2Reg Abdomen MRI/CT task included 8 pairs of corresponding MRI and CT images from the same patients, sourced from the TCIA 15 database [28]. Data were resampled to an isotropic resolution of 2 mm, with dimensions standardized to 192160192. Ground truth segmentations of the liver, spleen, and left and right kidneys were provided to evaluate registration performance. The evaluation was based on two metrics: the Dice Similarity Coefficient (DSC) to assess overlap accuracy and the standard deviation of the logarithm of the Jacobian determinant (SDlogJ), which evaluates the smoothness and plausibility of the displacement field. Learn2Reg Brain - Inter-Patient Registration. The Learn2Reg Brain task focused on whole-brain MRI registration, using data from the Open Access Series of Imaging Studies (OASIS). total of 20 pairs of inter-patient T1-weighted MRI scans were selected to evaluate the models capability to align brain structures across subjects. Anatomical segmentation labels for 35 brain structures were provided for evaluation. The data were preprocessed, including skull stripping, and resampled to an isotropic resolution of 1 mm with dimensions standardized to 160192224. This task emphasizes the models ability to capture fine-grained structural information within the brain. Registration performance was measured using the DSC and SDlogJ metrics. XCAT - Synthetic Multimodal Abdominal Image Registration. This task used dataset generated synthetically by [35] based on XCAT phantom data. CycleGAN model was trained to map between the XCAT phantom and real image domains, producing synthetic T1-weighted MRI and CT images. The dataset comprised 56 inter-patient image pairs in both inhaled and exhaled states. The data were preprocessed to an isotropic resolution of 2 mm and standardized to dimensions of 192160192. The exhaled phase served as the fixed reference, with the task requiring the registration of inhaled to exhaled phases. For cross-modality evaluation, the MRI images were used as the fixed reference. Ground truth segmentations for the liver, spleen, and kidneys were generated using the TotalSegmentator tool [32] on both T1weighted and CT images. The evaluation relied on DSC to quantify overlap and SDlogJ to assess deformation plausibility. Prompted Organ Segmentation For Prompted Organ Segmentation, we constructed our benchmark following the evaluation protocol established by RadSAM [21]. We utilized subset of the AMOS dataset [36], which contained only CT images, for both training and evaluation. The final dataset comprised 300 CT scans with pixel-level annotations, covering 15 abdominal organs: spleen, kidneys (left and right), adrenal glands (left and right), gallbladder, esophagus, liver, stomach, aorta, postcava, pancreas, bladder, duodenum, and prostate/uterus. Evaluation was conducted by synthetically generating prompts in the form of bounding boxes and points for each 2D image. The bounding boxes were derived by perturbing the ground-truth boxes, introducing random offsets ranging from 5 to +20 pixels on each side. For the points, random location was selected from within the ground truth mask, avoiding 2 pixels along its contour. The idea was to imitate prompts made by an operator."
        },
        {
            "title": "Lung Nodule Malignancy",
            "content": "We employed the LUNA16 dataset [37] with the specific split proposed by Harvard Onco-FM [25] to build the Lung Nodule Malignancy benchmark. This dataset comprised images of benign or suspicious pulmonary nodules. For our binary classification task, we utilized 3D Region Of Interest (ROI) around each lesion. Notably, all ROIs had the same size, which simplified the analysis. The resulting datasets had 338 training samples, 169 validation samples, and 170 test samples."
        },
        {
            "title": "Kidney Lesion Malignancy",
            "content": "We used the KITS23 dataset [38] to create the Kidney Lesion Malignancy benchmark. The objective of the task was classifying kidney lesions with two classes: solid tumors and cysts. We kept the mask annotations for the downstream task. We randomly sampled one image per sample where the tumor or cyst mask was not empty. The sampling method ensured an even distribution across mask sizes. Finally, to balance the dataset, we ensured there were as many images for cysts and tumors. The resulting dataset had 324 training samples, 66 validation samples, and 144 test samples. Tumor Localisation We used the DeepLesion dataset [39] to establish Tumor Localisation, benchmark for classifying the anatomical location of tumors. Specifically, given 2D CT image and region of interest (ROI) surrounding tumor, the model was tasked with predicting the anatomical region type of the tumor (e.g., classifying if the tumor was located in the abdomen, bone, kidney, liver, lung, mediastinum, pelvis, or soft tissue). The ROIs corresponded to the bounding boxes provided in the original dataset. For this benchmark, one image per lesion was sampled from the dataset, excluding instances where the anatomical region was unspecified. The resulting dataset comprised 2,610 training samples, 1,220 validation samples, and 1,221 test samples, all with corresponding ROIs. Kidney Cancer Survival To build the Kidney Cancer Survival benchmark, we assembled kidney-cancer cohort of 183 patients by extracting the molecularclinical records of TCGA [40] from the imaging collections of TCIA [28]. The joint resource provided contrast-enhanced CT scans, TNM staging and overall survival. Data were retrieved with tcia utils Python client1 and converted from DICOM to NIfTI via dcm2niix 2. We retained only those cases that carried either days to death or 1TCIA Notebooks TCGA Clinical.ipynb 2https://github.com/rordenlab/dcm2niix 16 days to last follow up metadata. Tumor volumes were semi-automatically delineated with segmentation FM [41]; two radiology residents (> 2 years of oncology experience) then manually corrected the masks. The polished masks were fed to the FM whose classification head was replaced by Cox layer to predict time-to-event, following our survival framework implementation. We benchmarked the image-based survival predictor against conventional anatomical staging using the stage of the TNM classification, called local stage."
        },
        {
            "title": "Degenerative Lumbar Spine",
            "content": "The Degenerative Lumbar Spine classification benchmarks were based on the dataset from the RSNA 2024 Lumbar Spine Degenerative Classification Challenge [42]. This dataset consisted of distinguishing between five lumbar spine degenerative conditions which occur at intervertebral disc levels and are visible on specific MRI sequences: Left and Right Foraminal Space Narrowing, visible on sagittal T1WI. Left and Right Subarticular Stenosis, visible on axial T2WI. Spinal Canal Stenosis, visible on sagittal T2WI and STIR. The dataset provided severity scores that could take the values Normal, Moderate, or Severe for each imaging study in the dataset and each combination of medical condition and inter-vertebral disc level. The location of the anatomical sites where the conditions could occur were also available for every patient regardless of the presence of medical condition, given as the coordinate of 3D point on the corresponding MRI sequence. We constructed three benchmarks from this dataset. Foraminal Narrowing. For each sagittal T1WI sequence, we selected the images on the sagittal axis based on the location of the anatomical sites given in the dataset. The objective of the benchmark was to predict the severity of foraminal narrowing with one of three values Normal, Moderate, or Severe. The benchmark dataset consisted in 31,468 training samples, 3930 validation samples, and 3960 test samples. Subarticular Stenosis. For each axial T2WI sequence, we selected the images on the axial axis based on the location of the anatomical sites given in the dataset. The objective of the benchmark was to predict the severity of subarticular stenosis with one of three values Normal, Moderate, or Severe. The benchmark dataset consisted in 29,956 training samples, 3798 validation samples, and 3766 test samples. was to predict the severity of spinal canal stenosis with one of three values Normal, Moderate, or Severe. The benchmark dataset consisted in 15,622 training samples, 1938 validation samples, and 1946 test samples. Anterior Cruciate Ligament (ACL) Tear The ACL Tear task [43] involved classifying MRI images of both knees to detect the presence or absence of an Anterior Cruciate Ligament (ACL) tear. Each knee volume was classified as one of the following: absence, injury, or complete rupture of the ACL. To prepare this dataset for training, we selected 3D box region within each volume where the ACL should be visible. We then evaluated the performance of models on the injury and complete Rupture classes using the ROC Area Under the Curve (AUC) metric. The resulting datasets had 733 training samples, 92 validation samples, and 92 test samples. Notably, the sets were imbalanced; for instance, the training set had significantly larger number of absences (554 samples) compared to injury (133 samples) and complete rupture (48 samples)."
        },
        {
            "title": "4.5.4 Emergency Benchmark",
            "content": "Myocardial Infarction Myocardial Infarction was classification task consisting in, given cardiac MRI and square mask around the myocardium, detecting if signs of infarction are visible or not. The EMIDEC dataset [44] provided 3D segmentations of the myocardium, cardiac cavity, infarction and no-reflow regions. The square mask input was computed from the myocardium segmentation. Abdominal Trauma The Abdominal Trauma task consisted of predicting the presence of active contrast extravasation on axial CT images. The dataset from the RSNA 2023 Abdominal Trauma Detection Challenge [45] provided the information of every image index that showed active extravasation. For the sampling of the 2D axial images, every image with active extravasation was added to the dataset. To ensure the balance of the dataset, an equal amount of images were randomly chosen from the remaining images without active contrast extravasation. Intracranial Hemorrhage The Intracranial Hemorrhage task consisted of predicting whether hemorrhage was present in given cranial 2D CT image regardless of its type (e.g., subdural, epidural, intraparenchymal). An equal number of positive and negative images (25 000 in total) were sampled for the training set from the original dataset [46]. The validation and test sets (5000 images in each set) were sampled randomly from the original dataset without balancing. Spinal Canal Stenosis. For each sagittal T2WI and STIR sequence, we selected the images on the sagittal axis based on the location of the anatomical sites given in the dataset. The objective of the benchmark Stroke The task was originally proposed by the ATLAS R2.0 dataset (Anatomical Tracings of Lesions After 17 access to the EuroHPC supercomputer LEONARDO, hosted by CINECA (Italy) and the LEONARDO consortium through an EuroHPC AI Access call. Additionally, this work was granted access to the HPC resources of IDRIS under the allocation 2025A0171015718 made by GENCI. We thank Denis Habip Gatenyo and Jordan Mukadi for their help on the human evaluation. Stroke) [47] involving segmentation of brain lesions in patients who have experienced stroke. For Stroke, we simplified this task into classification problem, to determine whether an axial image contained brain lesions resulting from stroke. The dataset included 655 T1-weighted MRI exams, which we split into 459 exams for training, 98 exams for validation, and 98 exams for testing. We extracted the axial images containing the brain and used 30% of these images to create the dataset."
        },
        {
            "title": "4.5.5 Neurodegenerative Benchmark",
            "content": "Alzheimers Disease This task was based on the Oasis-1 dataset [29], which contained brain MRIs from patients with varying levels of dementia. The Clinical Dementia Rating (CDR) scale categorizes patients as: non-demented, very mild dementia, mild dementia, or moderate dementia. For Alzheimers Disease benchmark, we simplified the classification problem to binary decision: either nondementia or one of the other three dementia categories. We used the entire brain MRI volume for the classification pipeline rather than extracting specific features or regions of interest. The resulting datasets were imbalanced, with 348 training samples, 44 validation samples, and 44 test samples. The significant disparity across classes is worth noting."
        },
        {
            "title": "4.5.6 Infectious Benchmark",
            "content": "Pulmonary Infections For the Pulmonary Infections benchmark, we utilized the COVIDx CT dataset [48], which comprises chest CT scans of patients diagnosed as COVID-19 positive, non-COVID pneumonia positive, or negative (healthy). We created stratified, sub-sampled version of the COVIDx CT dataset, maintaining the same training and evaluation splits. The resulting dataset represented 10% of the original, consisting of 35,748 training samples, 3,367 validation samples, and 3,374 test samples. The training dataset was imbalanced, with significantly more COVID-19 positive cases compared to the other classes. However, this imbalance was mitigated in the validation and test sets which were more balanced."
        },
        {
            "title": "4.6 Computing software and hardware",
            "content": "We used python for all experiments, with the Pytorch library, and the DINOv2 codebase [6] that we adapted for medical images. We leveraged public HPC clusters to pre-train our model. For the ViT-B architecture, we used 4 nodes with 4 80GB NVIDIA A100 GPUs for 125 hours. We used DistributedDataParallel to train models with multi-GPU multi-node setting. All downstream experiments were done on single NVIDIA 4090 GPU. We used HuggingFace to load other FMs: BiomedCLIP (https://huggingface. co/microsoft/BiomedCLIP-PubMedBERT 256-vit base patch16 224, (https://huggingface.co/lion-ai/MedImageInsights). and MedImageInsight the Acknowledgements. We EuroHPC Joint Undertaking for awarding this project acknowledge"
        },
        {
            "title": "References",
            "content": "[1] Baloch, M., Ali, K. & Nawaz, D. Illuminating insights: The role of radiology in diagnosing medical conditions (2023). [2] Litjens, G. et al. survey on deep learning in medical image analysis. CoRR abs/1702.05747 (2017). URL http://arxiv.org/abs/1702.05747. [3] Nair, A. et al. Enhancing radiologist productivity with artificial intelligence in magnetic resonance imaging (mri): narrative review. Diagnostics 15 (2025). URL https://www.mdpi.com/2075-4418/ 15/9/1146. [4] Prevedello, L. M. et al. Challenges related to artificial intelligence research in medical imaging and the importance of image analysis competitions. Radiology: Artificial Intelligence 1, e180031 (2019). [5] Bian, Y., Li, J., Ye, C., Jia, X. & Yang, Q. Artificial intelligence in medical imaging: From taskspecific models to large-scale foundation models. Chinese Medical Journal 138, 651663 (2025). [6] Oquab, M. et al. Dinov2: Learning robust visual features without supervision (2023). [7] He, K. et al. Masked autoencoders are scalable vision learners (2022). [8] Paschali, M. et al. Foundation models in radiology: What, how, why, and why not. Radiology 314, e240597 (2025). [9] Lesaunier, A. et al. Artificial intelligence in interventional radiology: Current concepts and future trends. Diagnostic and Interventional Imaging 106, 510 (2025). [10] Zhang, S. et al. BioMedCLIP: Multimodal Biomedical Foundation Model Trained from Fifteen Million ImageText Pairs. NEJM AI 2, AIoa2400640 (2025). [11] Zhao, T. et al. foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. Nature Methods 111 (2024). URL https://www.nature.com/ articles/s41592-024-02499-w. Publisher: Nature Publishing Group. [12] Codella, N. C. F. et al. MedImageInsight: An Open-Source Embedding Model for General Domain Medical Imaging (2024). 2410.06542. [13] Mahmood, F. benchmarking crisis in biomedNature Medicine 31, ical machine learning. 10601060 (2025). [15] Codella, N. C. F. et al. MedImageInsight: An Open-Source Embedding Model for General Domain Medical Imaging (2024). URL http: //arxiv.org/abs/2410.06542. ArXiv:2410.06542 [eess]. [16] Ding, M. et al. Davit: Dual attention vision transformers (2022). [17] Zhang, S. et al. BioMedCLIP: Multimodal Biomedical Foundation Model Trained from Fifteen Million ImageText Pairs. NEJM AI 2, AIoa2400640 (2025). URL https://ai.nejm. org/doi/10.1056/AIoa2400640. Publisher: Massachusetts Medical Society. [18] Hering, A. et al. Learn2reg: comprehensive multitask medical image registration challenge, dataset and evaluation in the era of deep learning. IEEE Transactions on Medical Imaging 42, 697712 (2022). [19] Kirillov, A. et al. Segment anything (2023). URL https://arxiv.org/abs/2304.02643. 2304.02643. [20] Ma, J. et al. Segment anything in medical images. Nature Communications 15 (2024). URL http: //dx.doi.org/10.1038/s41467-024-44824-z. [21] Khlaut, J. et al. Radsam: Segmenting 3d radiological images with 2d promptable model (2025). [22] Pai, S. et al. Foundation model for cancer imaging biomarkers. Nature Machine Intelligence 6, 354367 (2024). URL https://www.nature.com/ articles/s42256-024-00807-9. Publisher: Nature Publishing Group. [23] Swets, J. Evaluation of diagnostic systems (Elsevier, 2012). [24] Hogan, J. & Adams, N. M. On averaging roc curves. Transactions on Machine Learning Research (2023). [25] Pai, S. et al. Foundation model for cancer imaging biomarkers. Nature machine intelligence 6, 354 367 (2024). [26] Cox, D. R. Regression models and life-tables. Journal of the Royal Statistical Society: Series (Methodological) 34, 187202 (1972). [27] Monod, M. et al. torchsurv: lightweight packarXiv preprint age for deep survival analysis. arXiv:2404.10761 (2024). [28] Clark, K. et al. The cancer imaging archive (tcia): maintaining and operating public information repository. Journal of digital imaging 26, 1045 1057 (2013). [14] Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR abs/2010.11929 (2020). URL https://arxiv.org/abs/2010.11929. [29] Marcus, D. S. et al. Open access series of imaging studies (oasis): cross-sectional mri data in young, middle aged, nondemented, and demented older adults. Journal of cognitive neuroscience 19, 19 [42] Richards, et al. T. spine 2024 classificalumbar tion. https://kaggle.com/competitions/ rsna-2024-lumbar-spine-degenerative-classification (2024). Kaggle. degenerative"
        },
        {
            "title": "Rsna",
            "content": "[43] ˇStajduhar, I., Mamula, M., Miletic, D. & Uenal, G. Semi-automated detection of anterior cruciate ligament injury from mri. Computer methods and programs in biomedicine 140, 151164 (2017). [44] Lalande, A. et al. Emidec: database usable for the automatic evaluation of myocardial infarction from delayed-enhancement cardiac mri. Data 5 (2020). [45] Colak, E. et al. Rsna 2023 abdominal trauma https://kaggle.com/competitions/ (2023). detection. rsna-2023-abdominal-trauma-detection Kaggle. [46] Flanders, A. E. et al. Construction of machine learning dataset through collaboration: the rsna 2019 brain ct hemorrhage challenge. Radiology: Artificial Intelligence 2, e190211 (2020). [47] Liew, S.-L. et al. Anatomical tracings of lesions after stroke (atlas) r2.0. URL https://fcon 1000. projects.nitrc.org/indi/retro/atlas.html. [48] Gunraj, H., Sabri, A., Koff, D. & Wong, A. Covid-net ct-2: Enhanced deep neural networks for detection of covid-19 from chest ct images through bigger, more diverse learning. Frontiers in Medicine 8, 729287 (2022). URL https://www.frontiersin.org/articles/10.3389/ fmed.2021.729287. 14981507 (2007). [30] Moutakanni, T., Oquab, M., Szafraniec, M., Vakalopoulou, M. & Bojanowski, P. You dont need domain-specific data augmentations when scaling self-supervised learning. Advances in Neural Information Processing Systems 37, 116106 116125 (2024). [31] Song, X., Xu, X. & Yan, P. Dino-reg: General purpose image encoder for training-free multi-modal deformable medical image registration (2024). [32] Wasserthal, J. et al. Totalsegmentator: Robust segmentation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence 5 (2023). URL http://dx.doi.org/10.1148/ryai.230024. [33] DAntonoli, T. A. et al. Totalsegmentator mri: Sequence-independent segmentation of 59 anatomical structures in mr images. arXiv preprint arXiv:2405.19492 (2024). [34] Ixi dataset. URL https://brain-development.org/ ixi-dataset. [35] Bauer, D. F. et al. Generation of annotated multimodal ground truth datasets for abdominal medical image registration. International journal of computer assisted radiology and surgery 16, 12771285 (2021). [36] Ji, Y. et al. Amos: large-scale abdominal multiorgan benchmark for versatile medical image segmentation. Advances in neural information processing systems 35, 3672236732 (2022). [37] Setio, A. A. A. et al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the luna16 challenge. Medical image analysis 42, 113 (2017). [38] Heller, N. et al. The kits21 challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase ct (2023). 2307. 01984. [39] Yan, K., Wang, X., Lu, L. & Summers, R. M. Deeplesion: automated mining of large-scale lesion annotations and universal lesion detection with Journal of medical imaging 5, deep learning. 036501036501 (2018). [40] The Cancer Genome Atlas Research Network. Comprehensive genomic characterization defines human glioblastoma genes and core pathways. Nature 455, 10611068 (2008). URL https://doi. org/10.1038/nature07385. [41] Machado, L. et al. promptable ct foundation model for solid tumor evaluation. npj Precision Oncology 9, 121 (2025). URL https://doi.org/10. 1038/s41698-025-00903-y."
        },
        {
            "title": "Appendix A Extended Data",
            "content": "We display in Fig. A2 results on our benchmark for multiple model sizes, dataset sizes and number of training steps. We used dataset sizes of 30K, 200K, 2M, 20M, and 200M. The number of training steps followed the dataset sizes: for given global batch size b, we trained the model 30K/b 200K/b, 2M/b, 20M/b, and 200M/b steps: this means model trained for 30K/b steps would have seen 30K images during its training. We skipped the setups where the number of steps would result in not seeing the full dataset (e.g. 2M dataset with 30K/b steps). We observe that increasing one of those three parameter, fixing the two others, leads to an increase in performance. The number of training steps in particularly crucial: even on small dataset, of 40K images, ViT-B can reach an error rate of under 20% 21 Fig. A1: Receiver Operating Characteristic (ROC) curves for all binary classification tasks in our benchmark. The curves were computed on 5 runs for each models, and were aggregated using the pooling method [23, 24]. All predictions from the 5 runs were concatenated into single ensemble of predictions, that was used to plot the ROC curve. 22 Fig. A2: Scaling curves for two architectures: ViT-B and ViT-L. We train the two models with various dataset sizes (30K, 200K, 2M, 20M, 200M), for different number of training steps, and show all results in the top 5 plots. The average is computed on all our 19 benchmarks. On the bottom, we plot, for each dataset size, the best performing model."
        },
        {
            "title": "Appendix B Model Parameters",
            "content": "B.1 Pre-training hyperparameters The Table B1 presents the main hyperparameters used in the pre-training. B.2 Head setups We provide in Table B2 the different configurations used for each downstream tasks (see Fig. 9)."
        },
        {
            "title": "Segmentation",
            "content": "Table C3 presents the detailed results of the prompted segmentation, reported per organ."
        },
        {
            "title": "Appendix D Detailed Results",
            "content": "D.1 Registration This section showcases the results on the image registration benchmark. More specifically, Table D4 shows the results on Learn2Reg Abdomen, Table D5 the results on Learn2Reg Brain, and Table D6 the results on XCAT. D.2 Cross-Modality Generalization The results in Table D7 showcase the generalization capability of FMs in cross-modality context. Models were fine-tuned on the organ recognition task using either CT or MRI data, and evaluated on the other modalityi.e., MRI or CT, respectively."
        },
        {
            "title": "Benchmark",
            "content": "E.1 Bootstrapping Scores In this section, we display the detailed scores for each benchmark obtained through non-parametric bootstrapping. Specifically, we report the average value of the main metric along with the corresponding 95% confidence intervals. E.2 Statistical Significance In this section, we present the results of statistical significance tests performed using paired bootstrap approach across five training runs. Specifically, for each pair of models and for each benchmark, we report the computed p-value as well as the 95% confidence interval (in %) of the main metric, derived from the paired bootstrap distribution. 24 Table B1: Main hyperparameters used in the pre-training for Curia-B and Curia-L."
        },
        {
            "title": "Models",
            "content": "Curia-B Curia-L"
        },
        {
            "title": "Projection\nHeads",
            "content": "Optimizer Lr Scheduler Weight decay start value Weight decay end value Total batch size Number of iterations Patch size Resolution Register tokens Embedding dimension Layers Heads MLP ratio MLP activation Heads prototypes DINO head bottleneck dim 384 iBOT head bottleneck dim 256 25,000 AdamW Cosine 0.04 0.2 512 475,000 16 512 0 768 12 12 4.0 SwiGLU fused 131072 Augmentation Global crop scale Local crop scale Global crop number Local crop number Global crop size Local crop size GPUs Precision Hardware [0.32, 1.0] [0.05, 0.32] 2 8 512 224 16xA100 80 GB 16xA100 80 GB FP FP16 25,000 AdamW Cosine 0.04 0.2 256 475,000 16 512 0 1024 24 16 4.0 SwiGLU fused 131072 384 256 [0.32, 1.0] [0.05, 0.32] 2 8 512 224 Table B2: Head setup for all tasks for Curia models. We report whether features were aggregated at the image level or within segmentation masks (see Fig. 9). Additionally, we specify whether CLS tokens or patch tokens were used. Benchmark Level Type CLS Tokens Patch Tokens Category Anatomy Oncology CT Organ Recognition MRI Organ Recognition Neuroimaging Brain Estimation Lung Nodule Malignancy Kidney Lesion Malignancy Tumor Localisation Musculoskeletal Renal Malignancy Survival Foraminal Narrowing Spinal Cord Stenosis Subarticular Stenosis ACL Tear Myocardial Infarction Abdominal Trauma Intracranial Hemorrhage Stroke Alzheimers Disease Pulmonary Infections Emergency Degenerative Infectious 25 mask mask image mask mask mask mask mask mask mask mask mask image image image image image linear linear linear attention linear linear linear linear linear linear attention linear linear linear attention attention linear Table C3: Results on the AMOS Dataset for Prompted Organ Segmentation. For each model, we report the mean Dice Similarity Coefficient (DSC) computed per organ. Both bounding box (Bbox) and point-based prompts are considered in the evaluation."
        },
        {
            "title": "Aorta\nBladder\nDuodenum\nEsophagus\nGall Bladder\nLeft Adrenal Gland\nLeft Kidney\nLiver\nPancreas\nPostcava\nProstate Uterus\nRight Adrenal Gland\nRight Kidney\nSpleen\nStomach",
            "content": "All"
        },
        {
            "title": "RadSAM",
            "content": "Curia-B Curia-L Bbox Point Bbox Point Bbox Point Bbox Point 78.31 76.08 57.43 59.44 76.50 49.03 87.91 81.59 63.12 71.56 70.73 33.47 85.40 86.52 76.11 70.16 65.68 30.98 18.07 6.60 26.97 6.29 80.70 50.24 26.12 8.74 17.65 1.48 71.01 54.54 30. 33.04 95.82 93.29 85.97 88.28 91.51 81.49 96.51 97.03 85.92 91.47 91.56 79.25 96.49 97.12 94.64 91.08 94.97 90.50 70.04 83.47 80.22 73.61 95.84 94.50 75.03 86.38 84.31 72.51 95.52 95.74 86.08 85.27 95.96 92.85 85.82 87.95 90.89 81.08 96.56 97.56 87.77 91.45 91.10 80.27 96.44 97.01 94. 91.13 95.23 90.17 74.78 85.67 83.45 78.08 96.51 95.96 82.45 87.87 88.13 77.75 95.75 95.53 90.50 96.11 93.47 86.39 88.49 91.08 81.64 96.75 97.77 87.97 91.76 91.71 80.20 96.63 97.38 95.10 87.87 91.49 95.68 91.32 76.99 83.67 77.84 76.31 96.66 96.64 84.49 86.01 89.17 72.47 95.91 96.56 92. 87.49 Table D4: Learn2Reg Abdomen MRI/CT Registration Results. For each model, the metrics reported are, in order, the mean Dice Similiary Coefficient (DSC) score (in %), the DSC scores on liver, spleen, right kidney, and left kidney (in%), and the standard deviation of the log-Jacobian determinant. Model Mean Liver Spleen Kidney Kidney stdLogJ Curia-B Curia-L MedImageInsight BiomedCLIP DINOv2 Large 85.1 83.84 77.83 74.52 79.83 87.96 86.11 75.54 83.99 84.03 84.22 84.27 74.38 74.07 74.55 82.76 81.55 80.24 71.66 80.51 85.46 83.41 81.16 68.36 80. 0.1039 0.3317 0.3439 0.1317 0.1173 Table D5: Learn2Reg Brain Registration Results. For each model, the metrics reported are, the mean Dice Similiary Coefficient (DSC) score (in %), and the standard deviation of the log-Jacobian determinant. Model Mean stdLogJ Curia-B Curia-L MedImageInsight BiomedCLIP DINOv2 Large 77.68 77.96 75.91 76.29 68.06 0.0519 0.0938 0.0843 0.1333 0.1572 26 Table D6: XCAT - Multimodal Abdominal Medical Image Registration Results. For each model, the metrics reported are, in order, the mean Dice Similiary Coefficient (DSC) score (in %), the DSC scores on liver, spleen, right kidney, and left kidney (in%), and the standard deviation of the log-Jacobian determinant."
        },
        {
            "title": "Model",
            "content": "Mean Liver Spleen Kidney Kidney stdLogJ Curia-B (CT CT) Curia-L (CT CT) MedImageInsight (CT CT) BiomedCLIP (CT CT) DINOv2 Large (CT CT) Curia-B (MR MR) Curia-L (MR MR) MedImageInsight (MR MR) BiomedCLIP (MR MR) DINOv2 Large (MR MR) Curia-B (CT MR) Curia-L (CT MR) MedImageInsight (CT MR) BiomedCLIP (CT MR) DINOv2 Large (CT MR) 81.30 80.12 76.25 81.74 79.60 86.10 84.25 76.55 82.96 85.81 64.03 65.25 56.99 52.32 64.71 94.26 93.37 91.92 94.65 93.22 94.47 92.81 88.02 94.25 93.99 86.12 85.34 78.44 81.69 86. 87.18 87.30 80.63 90.22 86.14 84.66 82.12 70.51 80.12 86.13 70.09 71.92 65.31 60.02 71.90 58.51 56.46 52.12 57.52 56.88 82.91 81.53 73.25 79.52 82.89 43.85 44.41 35.85 30.53 44. 85.27 83.35 80.33 84.55 82.17 82.38 80.52 74.40 77.96 80.24 56.06 59.33 48.37 37.05 55.96 0.0369 0.0817 0.0292 0.0603 0.0615 0.0371 0.0722 0.0281 0.0646 0.0619 0.0633 0.1070 0.0468 0.2233 0. Table D7: Cross-modality generalization results of the Organ Recognition task on CT and MRI. The metrics reported are the balanced accuracies (in %) on the 41 common classes between CT and MRI data in the benchmark. Model In-Distribution CT Out-of-Distribution MRI In-Distribution MRI Out-of-Distribution CT CT MRI MRI CT Curia-L BiomedCLIP MedImageInsight ViT-L 97.44 85.62 88.59 84.29 88.27 42.53 53.08 12.53 95.79 72.30 70.66 31. 96.40 54.99 63.52 18.04 Table E8: Comparison of Models for Benchmark: CT Organ Recognition. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution (all metrics in %). Model Accuracy Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 98.10 98.40 88.19 84. 97.58 97.93 87.00 83.75 98.55 98.83 89.33 86.07 Table E9: Comparison of Models for Benchmark: MRI Organ Recognition. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. Model Accuracy Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 82.27 89.11 63.18 63.22 80.22 87.59 60.62 60.78 84.16 90.69 65.66 65.56 27 Table E10: Comparison of Models for Benchmark: Neuroimaging Brain Estimation. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution."
        },
        {
            "title": "Model",
            "content": "r2 Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 75.80 75.54 72.46 69.41 73.35 72.94 69.69 66.35 78.17 77.66 75.17 71.85 Table E11: Comparison of Models for Benchmark: Lung Nodule Malignancy. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. * indicates results reported from the original publication. While all other models only train linear head, Harvard OncoFM finetuned also updates the pretrained encoder weights on the downstream task."
        },
        {
            "title": "Model",
            "content": "AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP Harvard OncoFM* Harvard OncoFM finetuned* 94.98 92.45 92.18 88.72 88.23 94.40 92.05 87.96 88.19 83.99 - - 97.41 96.26 95.70 92.82 - - Table E12: Comparison of Models for Benchmark: Kidney Lesion Malignancy. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. Model AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 74.41 80.29 67.81 62.95 66.08 72.16 59.62 54.62 82.10 87.83 76.20 71. Table E13: Comparison of Models for Benchmark: Tumor Localisation. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. Model Accuracy Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 91.87 91.74 88.91 86.39 89.76 89.56 85.99 83. 93.75 93.84 91.40 88.95 Table E14: Comparison of Models for Benchmark: Renal Malignancy Survival. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. Model c-index Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 71.12 62.81 62.79 63. 65.38 56.43 56.03 57.02 76.86 69.52 69.24 70.15 28 Table E15: Comparison of Models for Benchmark: Foraminal Narrowing. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution."
        },
        {
            "title": "Model",
            "content": "AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 86.16 86.21 86.32 84.45 84.71 84.76 84.64 82.93 87.69 87.62 87.75 86.13 Table E16: Comparison of Models for Benchmark: Spinal Canal Stenosis. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution."
        },
        {
            "title": "Model",
            "content": "AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 94.65 93.73 92.98 92.33 93.22 92.38 91.31 90.61 95.94 95.00 94.58 93.91 Table E17: Comparison of Models for Benchmark: Subarticular Stenosis. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. Model AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 87.81 87.46 85.61 83.92 89.09 86.14 84.26 82.53 86.57 88.72 87.00 85. Table E18: Comparison of Models for Benchmark: ACL Tear. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. Model AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 85.03 87.34 78.39 81.97 77.91 80.89 68.62 75. 91.80 92.97 86.52 88.32 Table E19: Comparison of Models for Benchmark: Myocardial Infarction. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. Model AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 84.55 89.16 94.08 71. 75.92 81.40 87.87 58.85 91.98 95.65 98.80 82.44 29 Table E20: Comparison of Models for Benchmark: Abdominal Trauma. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution."
        },
        {
            "title": "Model",
            "content": "AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 82.63 87.10 93.14 79.14 83.98 85.75 92.27 77.58 81.31 88.28 93.99 80.60 Table E21: Comparison of Models for Benchmark: Intracranial Hemorrhage. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution."
        },
        {
            "title": "Model",
            "content": "AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 93.69 93.54 90.11 87.77 92.77 92.69 88.94 86.57 94.56 94.40 91.19 88.92 Table E22: Comparison of Models for Benchmark: Stroke. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. Model AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 89.93 89.78 88.62 85.72 89.02 88.79 87.47 84.52 90.83 90.67 89.66 86. Table E23: Comparison of Models for Benchmark: Alzheimers Disease. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. Model AUC Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 87.83 84.90 87.66 88.19 77.70 74.51 75.78 77. 95.33 93.78 96.38 96.45 Table E24: Comparison of Models for Benchmark: Pulmonary Infections. Each models performance was evaluated over 5 training runs using non-parametric bootstrapping with 1,000 resamples. The table reports the average accuracy score along with the corresponding 95% confidence intervals computed across the bootstrapped distribution. Model Balanced Accuracy Score Lower 95% CI Upper 95% CI Curia-B Curia-L MedImageInsight BiomedCLIP 91.49 93.40 89.97 89. 90.54 92.61 88.83 88.22 92.43 94.18 91.02 90.26 30 Table E25: Statistical Comparison Curia-B vs MedImageInsight. For each benchmark and category, we report the p-value from paired bootstrap test (1,000 resamples across 5 training runs) assessing whether the performance difference between the two models is statistically significant. The table also includes the 95% confidence interval (in %) of the main metric from the paired bootstrap distribution."
        },
        {
            "title": "Benchmark",
            "content": "p-value Lower 95% CI Upper 95% CI"
        },
        {
            "title": "Emergency",
            "content": "Musculoskeletal Foraminal Narrowing Spinal Cord Stenosis Subarticular Stenosis ACL Tear Myocardial Infarction Abdominal Trauma Intracranial Hemorrhage Stroke Alzheimers Disease Pulmonary Infections Degenerative Infectious < 0.001 < 0.001 0.006 0.126 0.177 0.027 0.003 0.812 0.011 < 0.001 0.093 0.017 < 0.001 < 0.001 0.001 0.936 0.007 8.84 16.74 1.20 -0.01 -0.02 0.49 2.96 -0.01 0.00 1.41 -0.79 -0.17 -0.12 0.03 0.01 -6.22 0.40 10.94 21.48 5.80 0.06 0.16 5.80 13.64 0.01 0.03 3.02 15.99 -0.03 -0.09 0.04 0.02 6.60 2.61 Table E26: Statistical Comparison Curia-L vs MedImageInsight. For each benchmark and category, we report the p-value from paired bootstrap test (1,000 resamples across 5 training runs) assessing whether the performance difference between the two models is statistically significant. The table also includes the 95% confidence interval (in %) of the main metric from the paired bootstrap distribution. Benchmark p-value Lower 95% CI Upper 95% CI Category Anatomy Oncology CT Organ Recognition MRI Organ Recognition Neuroimaging Brain Estimation Lung Nodule Malignancy Kidney Lesion Malignancy Tumor Localisation Renal Malignancy Survival Emergency Musculoskeletal Foraminal Narrowing Spinal Cord Stenosis Subarticular Stenosis ACL Tear Myocardial Infarction Abdominal Trauma Intracranial Hemorrhage Stroke Alzheimers Disease Pulmonary Infections Degenerative Infectious 31 < 0.001 < 0.001 0.004 0.876 0.025 0.041 0.99 0.87 0.243 < 0.001 0.013 0.104 < 0.001 < 0.001 0.001 0.325 < 0. 9.23 23.49 0.81 -0.04 0.02 0.17 -4.47 -0.01 -0.00 0.01 2.04 -0.12 -0.07 0.03 0.01 -7.88 3.141 11.26 28.48 5.33 0.05 0.22 5.59 4.52 0.01 0.02 0.03 15.85 0.01 -0.05 0.04 0.02 2.63 5.257 Table E27: Statistical Comparison Curia-B vs BiomedCLIP. For each benchmark and category, we report the p-value from paired bootstrap test (1,000 resamples across 5 training runs) assessing whether the performance difference between the two models is statistically significant. The table also includes the 95% confidence interval (in %) of the main metric from the paired bootstrap distribution."
        },
        {
            "title": "Benchmark",
            "content": "p-value Lower 95% CI Upper 95% CI"
        },
        {
            "title": "Oncology",
            "content": "< 0.001 CT Organ Recognition < 0.001 MRI Organ Recognition Neuroimaging Brain Estimation < 0.001 Lung Nodule Malignancy Kidney Lesion Malignancy Tumor Localisation Renal Malignancy Survival"
        },
        {
            "title": "Emergency",
            "content": "Musculoskeletal Foraminal Narrowing Spinal Cord Stenosis Subarticular Stenosis ACL Tear Myocardial Infarction Abdominal Trauma Intracranial Hemorrhage Stroke Alzheimers Disease Pulmonary Infections Degenerative Infectious 11.98 16.90 0.05 -0.03 3.81 1.21 0.51 0.03 -7.97 0.01 0.01 2.83 0.02 -0.00 0.03 0.53 2.86 14.20 21.19 0.07 0.07 9.11 3.34 14.08 0.11 7.66 0.03 0.25 8.06 0.05 0.26 0.05 2.87 4.94 0.482 < 0.001 < 0.001 0.035 0.002 0.907 < 0.001 0.051 < 0.001 < 0.001 0.053 < 0.001 0.003 < 0.001 Table E28: Statistical Comparison Curia-L vs BiomedCLIP. For each benchmark and category, we report the p-value from paired bootstrap test (1,000 resamples across 5 training runs) assessing whether the performance difference between the two models is statistically significant. The table also includes the 95% confidence interval (in %) of the main metric from the paired bootstrap distribution. Benchmark p-value Lower 95% CI Upper 95% CI Category Anatomy Oncology < 0.001 CT Organ Recognition MRI Organ Recognition < 0.001 Neuroimaging Brain Estimation < 0.001 Lung Nodule Malignancy Kidney Lesion Malignancy Tumor Localisation Renal Malignancy Survival Emergency Musculoskeletal Foraminal Narrowing Spinal Cord Stenosis Subarticular Stenosis ACL Tear Myocardial Infarction Abdominal Trauma Intracranial Hemorrhage Stroke Alzheimers Disease Pulmonary Infections Degenerative Infectious 32 12.37 23.62 0.05 -0.06 3.48 3.14 -7.87 -0.00 -10.10 0.00 0.05 2.68 0.06 0.04 0.03 0.50 0. 14.57 28.07 0.07 0.06 8.75 5.26 6.36 0.08 3.56 0.02 0.28 7.89 0.10 0.31 0.05 2.94 0.05 0.993 < 0.001 < 0.001 0.79 0.07 0.344 0.02 0.004 < 0.001 < 0.001 0.015 < 0.001 0.005 < 0."
        }
    ],
    "affiliations": [
        ".omics, Paris, France",
        "Centre Cardiologique du Nord, Saint-Denis, 93200, France",
        "Department of Radiology and Radiological Science, Medical University of South Carolina, Charleston, SC, USA",
        "Department of Radiology, Columbia University Irving Medical Center, New York, NY, 10032, USA",
        "Department of Radiology, FHU MOSAIC, Beaujon Hospital, APHP.Nord, Clichy, France",
        "Department of Vascular and Oncological Interventional Radiology, Hˆopital Europeen Georges Pompidou, AP-HP, Paris, France",
        "Faculte de Sante, Universite Paris-Cite, Paris, France",
        "HEKA, INRIA, Paris, France",
        "PARCC 970, INSERM, Paris, France",
        "Raidium, 27 rue du faubourg Saint-Jacques, Paris, 75014, France"
    ]
}