{
    "paper_title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing",
    "authors": [
        "Xijie Huang",
        "Chengming Xu",
        "Donghao Luo",
        "Xiaobin Hu",
        "Peng Tang",
        "Xu Peng",
        "Jiangning Zhang",
        "Chengjie Wang",
        "Yanwei Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors."
        },
        {
            "title": "Start",
            "content": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing Xijie Huang1*, Chengming Xu2*, Donghao Luo2, Xiaobin Hu2, Peng Tang2, Xu Peng2, Jiangning Zhang2 Chengjie Wang2, Yanwei Fu1 2Tencent YouTu Lab 1FDU, ffp-300k.github.io 6 2 0 2 ] . [ 2 0 2 7 1 0 . 1 0 6 2 : r Figure 1. (a) Results from our framework and Aleph [1], commercial video editing model, with zoom-ins highlighting the main subject. While generally capable, Aleph may fail to follow the original motion (top Change task) or present limited visual quality (bottom Stylization task), reflecting the capacity limits of current models. In comparison, based on first frame edited by Qwen-Edit [31], our framework achieves temporally consistent and visually realistic results on both tasks. For clarity, only Change and Stylization results are shown here; please see the supplementary material for more examples. (b) Overall comparison between our proposed FFP-300K and previous video editing datasets. Each axis represents key dataset aspect, including total frames for scale, resolution level, supported edit types, completeness of paired sourcetarget data, content diversity across visual content and orientation types, and visual quality of generated target videos, providing an overall assessment of dataset scale, diversity, and consistency. Our FFP-300K is well suited for FFP-based video editing with higher-quality data. (c) Overall comparison between our framework and previous video editing methods, in which ours is generally better among all metrics."
        },
        {
            "title": "Abstract",
            "content": "First-Frame Propagation (FFP) offers promising paradigm for controllable video editing, but existing methods are hampered by reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining firstframe appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ self-distillation strategy where an identity propagation task acts as powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors. 1. Introduction High-fidelity video editing is pivotal task with applications spanning professional film production, interactive entertainment, and the surge of user-generated content. An ideal model must provide users with precise control over edits while ensuring realism and temporal coherence. Current diffusion-based methods [13, 21, 26, 35] largely follow two paradigms. The Instruction-based approaches [5, 25], while powerful for images, face compounded difficulty in the video domain. model must simultaneously interpret users textual intent and apply it coherently across temporal sequence, dual challenge that often yields results that lag behind the fidelity of their image-based counIn contrast, the First-Frame Propagation (FFP) terparts. paradigm [15, 16, 19] offers more pragmatic and powerful alternative by strategically decoupling the editing process. It allows users to leverage the sophisticated and mature ecosystem of image editing toolsfrom professional software to advanced generative modelsto perfect single frame with high precision. This approach alleviates the models burden of semantic interpretation, transforming the complex task of text-to-video editing into more constrained and well-defined problem: robust temporal propagation. However, this elegant promise of control is undermined by current models reliance on cumbersome run-time guidance, such as per-video LoRA fine-tuning [20] or auxiliary inputs like depth maps [15], which incur high computational costs and limit generalization. This reliance on guidance is not flaw in the FFP paradigm itself, but symptom of inadequate training data. Lacking long, high-resolution, and diverse examples, models fail to learn robust temporal priors and are forced to use external guidance as crutch. This data gap manifests in key limitations: (1) Insufficient Length and Resolution: Datasets like Senorita-2M [41] and InsViE [32] feature short, low-resolution clips, hindering the learning of long-range motion and fine details. (2) Limited Task Diversity: Many datasets focus on narrow tasks like inpainting (VPData [3]) or fail to distinguish between local and global edits. (3) Inconsistent Temporal Alignment: Hybrid datasets like VIVID-10M [9] mix images and videos, disrupting the learning of continuous motion priors. To overcome these fundamental limitations, we introduce synergistic solution comprising new dataset and novel framework. First, we present FFP-300K, largescale dataset engineered to directly address the aforementioned data challenges, which is constructed with twotrack synthesis pipeline. This pipeline leverages motionaware generative prior learned by VACE [11] as its backbone to ensure temporal stability, employing mask-based manipulation for precise local edits and depth-guided conditioning for geometry-aware global stylization. This structured approach ensures task diversity and high fidelity. Benefited from the modularized pipeline, our dataset can be easily scaled up to provide sufficient generalization ability, which contains about 290,441 original/edited video pairs at 720p resolution and length of 81 frames, providing rich and diverse foundation for training the next generation of video editing models. Building upon FFP-300K, we then advance the FFP paradigm by proposing new framework dubbed FreeProp, aiming to tackle the core challenge of balance between referencing the first frame for appearance and referencing the source video for motion with two key contributions. For architectural level, we design an Adaptive Spatio-Temporal RoPE (AST-RoPE) that creates content-aware geometry for the model. It learns from the source video to dynamically remap the spatio-temporal geometry, effectively disentangling the two references: it reduces the positional distance to the first frame to anchor appearance, while simultaneously rescaling the temporal axis to match the source videos motion. As for the objective level, we introduce self-distillation strategy, in which the virtually created identity propagation task acts as powerful regularizer, ensuring that the relational structure between the edited first frame and all subsequent frames follows stable trajectory. This prevents semantic drift and ensures the edits influence remains potent throughout the video. We comprehensively evaluate our framework on the EditVerseBench [12] benchmark, demonstrating superior performance against recent models including both academic ones such as EditVerse [12] and commercial ones such as Aleph [1] in both visual fidelity and temporal coherence. Our main contributions are: We introduce FFP-300K, large-scale dataset for FFPbased video editing, and the principled two-track generation pipeline used for its creation, addressing key limitations in prior data. We propose the novel Adaptive Spatio-Temporal RoPE (AST-RoPE) which disentangles appearance and motion. We introduce powerful self-distillation strategy, which is crucial for maintaining the temporal stability and visual integrity required for guidance-free generation. 2. Related Work Instruction-Based Video Editing Models. Instructionbased methods edit videos by interpreting natural language prompts. This paradigm is broadly divided into inversionbased and inversion-free approaches. Inversion-based models like VideoSwap [7] and VideoDirector [30] first map source video into latent noise space for editing. While this can yield precise results, the inversion process introduces significant computational overhead, limiting practical application. To circumvent this, inversion-free models are trained on large-scale datasets to generalize across diverse editing instructions. For instance, InsV2V [5] adapts image-to-image translation principles to video, while LucyEdit [25] and EditVerse [12] introduce architectures to better integrate textual and visual conditioning. However, due to the intrinsic difficulty of this task, current instructionbased methods fall far behind their image counterparts. FFP-Based Video Editing Models. The FFP paradigm offers more controllable alternative by decomposing video editing into two steps: user-driven first-frame modification and automated temporal propagation. Early methods like AnyV2V [14] and Videoshop [6] demonstrated the potential of this approach but struggled with complex motion. Subsequent works sought to improve temporal coherence but introduced significant dependencies. For example, I2VEdit [19] requires costly per-video fine-tuning, rendering it unscalable. Others, like StableV2V [15] and GenProp [16], rely on auxiliary guidance such as depth maps, optical flow, or predicted masks to preserve structure. Such reliance on external guidance complicates the pipeline and limits model generality due to dependence on auxiliary input quality. Our approach, by contrast, enables fully guidance-free propagation, i.e. conditioning solely on the source video and edited first frame to achieve temporally coherent and controllable results. Video Editing Datasets. The capabilities of video editing models are fundamentally shaped by the data they are trained on. Several large-scale datasets have been introduced to advance the field. Datasets like EffiVED [40] and VPLM [38] pioneered synthetic data generation for instruction-based tasks, while Senorita-2M [41], VIVID10M [9], VPData [3], and InsViE [32] significantly increased the scale and diversity of available data for objectlevel editing. Others such as IVEBench [4] mainly focus on evaluation. However, existing datasets limit robust FFP model development with low-resolution, short clips and unclear distinctions between local and global edits. This forces models to rely on brittle, short-range priors, requiring the external guidance our method eliminates. Our FFP300K dataset overcomes these issues with high-resolution (720p), long-form (81-frame) videos and separate tracks for local and global editing, establishing standardized training set for generalizable FFP models. 3. Scalable FFP Data Construction Pipeline To address the need for large-scale, high-fidelity dataset for FFP research, we construct FFP-300K. Our data generation framework is two-track modular pipeline designed to produce semantically aligned video editing pairs at 720p resolution. Unlike unified pipelines, our framework operates via two independent and specialized branches to maximize quality for distinct editing categories: (1) Local Editing: Built upon the Koala-36M [28], this track focuses on fine-grained, object-level operations such as swapping and removal. (2) Global Stylization: Derived from the Omni-Style [29], this track emphasizes full-scene stylization. Each branch employs tailored process of perception, captioning, and synthesis, culminating in standardized dataset that supports both instruction-based and First-Frame Propagation (FFP) video editing frameworks. 3.1. Local Editing The local editing branch generates precise object-level modifications. The process integrates large vision-language models (VLMs) for reasoning, advanced segmentation models for spatial localization, and powerful video inpainting model for synthesis. Automated Editing Pipeline. For each source video from Koala-36M, we first use Qwen2.5-VL-72B-Instruct [27] to analyze the first frame and identify primary editable objects. Subsequently, Grounded-SAM2 [23] performs instance segmentation to produce frame-wise mask videos, providing precise spatial constraints. These masks, along with task-specific captions, guide the video inpainting model VACE [11] to synthesize the edit. For Swap tasks, the original caption is used to guide VACE in replacing the masked object while preserving the background context. For Removal tasks, we prompt Qwen2.5-VL to generate modified caption that explicitly describes the scene without the target object (e.g., street with bench instead of street with person sitting on bench). This caption then guides VACE to remove the object and plausibly reconstruct the background. Refining Edits with Mask and Bounding Box Strategies. To optimize visual consistency, we discovered that the nature of the spatial conditioning is critical. We employ mask erosion strategy to preserve only the boundary regions of the target mask, encouraging VACE to better leverage its internal priors for coherent inpainting. Furthermore, we experimented with two complementary conditioning schemes: providing VACE with only the eroded mask Figure 2. Overview of our Data Construction Pipeline. Our pipeline has two parallel tracks. Left: The local editing track performs object Swap and Removal. For swapping, we use target objects and captions from the source video to generate edits with erosion masks, followed by quality filtering step. For removal, captions are constructed and paired with bounding-box masks to generate the edited videos. Notably, filtered samples are used to refine our VACE [11] model, which then regenerates the entire removal subset for higher quality (Sec. 3.1). Right: The global stylization track first generates source videos from images using Wan-I2V. It then combines these source videos, style reference images, and corresponding depth videos to produce high-fidelity stylized results (Sec. 3.2). (without-bbox) versus providing both the mask and the objects bounding box (with-bbox). Our empirical analysis revealed clear task-specific preference: Swap tasks benefit from the without-bbox approach, as the lack of hard spatial constraint prevents artifacting and yields more semantically natural object integration. Removal tasks are more successful with the with-bbox configuration, which provides strong spatial prior that ensures complete object erasure and consistent background reconstruction. This insight informs our quality control process, where we generate both high-quality variants. 3.2. Global Stylization The global stylization branch transforms the entire visual appearance of scene. Built upon the diverse Omni-Style dataset [29], this track uses two-stage process to ensure both semantic coherence and high stylistic fidelity. Stage 1: Source Video Generation. We first use Qwen2.5VL to analyze each artistic image from Omni-Style and generate cinematic video caption describing its scene, atmosphere, and tone. This caption is then used to prompt the Wan2.1-14B-I2V [26] to synthesize source video, ensuring the generated motion and content are semantically aligned with the reference style image. Stage 2: Stylized Video Generation. Next, Qwen2.5-VL generates detailed style caption by observing both the reference style image and the synthesized source video. This caption, which describes color palettes and textures, guides VACE in the stylization process. To preserve geometric structure, we provide VACE with depth maps extracted by Video Depth Anything [34]. This combination of semantic guidance (style caption), structural guidance (depth), and appearance reference (style image) allows VACE to generate the final, temporally coherent stylized video. 3.3. Quality Control and Curation multi-stage filtering and verification process is applied to ensure the final datasets quality and semantic integrity. Iterative Refinement for Removal Tasks. The removal subset underwent particularly rigorous curation loop to maximize precision. First, Qwen2.5-VL automatically screens all generated videos for the removal task to filter out low-fidelity pairs, resulting in an initial set of nearly 40,000 candidates. This was followed by manual verification, yielding 14,389 high-quality samples. We then used this curated set to fine-tune the VACE model, significantly enhancing its removal capabilities. Finally, this improved VACE model was used to regenerate the entire removal subset, achieving cleaner background restoration. and reference image, providing strong prior for learning motion from the conditioning video while inheriting appearance from the conditioning image. This design is naturally suited for FFP, but was not originally designed for our specific task, presenting two key limitations: 1. Its conditioning videos are low-level signals (e.g., depth maps), not the full RGB videos required in our case. 2. Its reference images are often spatially unaligned, whereas in FFP, ˆv is aligned in most regions. Therefore, task-specific adaptations are necessary. Formally, given and ˆv, we first extract their corresponding VAE latents: zsrc RF W and ˆz RH C, where denotes feature channels. The first-frame latent ˆz is then padded with zeros along the temporal dimension and concatenated with the noisy latent z, the source latent zsrc, and binary mask RF W (indicating the first frame) along the channel dimension. The resulting composite latent is fed into the DiT backbone for velocity prediction. By fine-tuning this model on our FFP-300K dataset using flow matching objective, it effectively adapts its pretrained motion prior to the specific requirements of FFPbased video editing. 4.2. Adaptive Spatio-Temporal RoPE The self-attention mechanism in Diffusion Transformer (DiT) relies on Rotary Position Embeddings (RoPE) [24] to understand spatio-temporal relationships. However, standard RoPE imposes static coordinate system that is illsuited for FFP. Its uniform temporal progression is agnostic to the source videos intrinsic motion, and its fixed spatial distances hinder the propagation of the edited first frame, which must serve as global content anchor. To overcome this, we introduce Adaptive SpatioTemporal RoPE (AST-RoPE), mechanism that endows the DiT with the ability to dynamically adapt its understanding of space and time based on the source videos content. Instead of static grid, AST-RoPE learns to modulate the perceived positions of tokens, guiding self-attention to generate motion and appearance that is faithful to the source. This is achieved by predicting content-aware scaling coefficients that separately adjust the RoPE for specialized spatial and temporal self-attention heads. Source-Aware Scaling Coefficient Prediction. Inspired by the observation of head specialization in DiTs [18, 33], we classify the attention heads in each layer into static set of Spatial Heads (HS) and Temporal Heads (HT ). For each video, lightweight transformer module followed by twohead MLP predicts spatial scaling factor αS and temporal scaling factor αT directly from the source latent zsrc. This allows the model to infer high-level properties from the source video. For instance, predicting smaller temporal scaling factor for video with rapid motion. We apply these coefficients distinctly to each head set. Figure 3. Overview of training paradigm. Left: The source video and edited frame are encoded. The source latent informs our AST-RoPE module for adaptive spatio-temporal scaling. Right: The target video is processed identically to extract latent DiT embedding, which is used to align the generation process. Final Verification and Statistics. All generated videos (swap, removal, and stylization) undergo final semantic verification by Qwen2.5-VL to ensure precise correspondence between the edit instruction and the visual transformation. After filtering and deduplication, our final FFP300K dataset comprises 290,441 high-quality video pairs (source/edited video). This includes 143,913 stylization, 40,000 removal, and 106,528 swap/modification tasks. All videos are standardized to 720p resolution and 81 frames, making FFP-300K robust and large-scale resource for advancing video editing research. 4. Methodology Our proposed FFP framework is designed to intrinsically handle temporal consistency, removing the need for explicit run-time conditions. The model is built upon powerful conditional video model, which we adapt for the FFP task. Two core innovations, i.e. an adaptive positional encoding scheme and self-distillation training objective, are introduced to resolve the core tension between appearance propagation and motion fidelity. 4.1. Preliminary Problem Formulation. Our goal is the First-Frame Propagation (FFP) task: given source video RF HW 3, where F, H, denotes number of frames, height and width respectively, and an edited first frame ˆv RHW 3, we aim to generate target video ˆV that preserves the motion of while propagating the edit from ˆv. Adapting Fun-Control for FFP. Our method is built upon Fun-Control, powerful conditional video generation model derived from Wan 2.1 [26]. Fun-Control is designed to aggregate conditioning information from both video For spatial heads (HS), to enhance the first frames influence, we use αS to modulate its perceived positional distance. Specifically, the first item of temporal indice if offset from 0 to αS . By learning to predict αS < 1, we reduce the effective distance between the first frame and all other frames, especially the ending onees. This biases self-attention to assign higher scores between tokens in the edited first frame and those in subsequent frames, ensuring its content is robustly propagated. For temporal heads (HT ), we use αT to rescale the temporal axis for all frames. The original temporal indices [0, 1, . . . , 1] are transformed to [0, αT , . . . , αT (F 1)]. This operation effectively stretches or compresses the temporal manifold. For source video with rapid motion, the model can learn smaller αT , reducing the perceived temporal distance between frames and encouraging the temporal heads to model more intense motion. 4.3. Self-Distillation with Identity Propagation To enforce precise motion dynamics and first-frame reference, which standard flow matching fails to sufficiently constrain, we introduce self-distillation paradigm. Our key insight is that the models own internal processing of the source video provides the ideal alignment target. We implement this via parallel identity propagation task, where the teacher task is to reconstruct the ground-truth target video ˆV from itself, i.e. conditioned on the ˆV and its first frame ˆv. This identity mapping forces its internal latents to perfectly encode the desired spatio-temporal dynamics. We then use distillation losses to align the standard student FFP tasks representations with this idealized teacher representation, ensuring faithful motion preservation. Inter-Frame Relational Distillation. To ensure global motion patterns are preserved, we distill the frame-to-frame similarity structure, inspired by VideoREPA [39]. Given latent representation zl RF W from the l-th DiT block of the FFP task, and the corresponding latent ˆzl from the identity propagation task, we first downsample them spatially by factor of KS to focus on motion over appearance. Let the resulting latents be zl ds, with = (H )/K 2 spatial tokens. Based on these two latents, the motion alignment can be calculate as: ds and ˆzl = Gram(zl ˆG = Gram(ˆzl ds) ds) RF F (1) (2) Lmotion = 1 (F 1) (cid:88) (cid:88) i,j=1 i=j Gi,:,j.: ˆGi,:,j.: (3) where Gram denotes gram matrix along the channel dimension. Lmotion minimizes the distance between the interframe relationships of the FFP latent and the identity propagation latent, which are indicative of motion, remain consistent with the source videos dynamics. First-Frame Consistency Loss. While motion alignment captures global structure, we need focused mechanism to ensure the edit from the first frame propagates its influence consistently. We propose novel loss based on Maximum Mean Discrepancy (MMD) to align the evolution of tokenwise relationships with respect to the first frame. i)T RN , where zl For given frame i, we compute the token-wise similarity matrix between the first frame and frame i: Si = RN are the (downzl 1, zl 1(zl sampled and reshaped) latents for the respective frames. Each of the rows of Si is feature vector describing how token in the first frame relates to all tokens in frame i. We treat this set of row vectors as an empirical distribution Pi over an -dimensional relation space. We then use MMD with RBF kernel k(, ) to measure the divergence between the relational distribution of frame and that of the first frame (an identity relation), yielding temporal drift score di = MMD2(P1, Pi), along with the identity propagation counterpart ˆdi, which are constrained to ensure similar evolution with each other: LMMD = (cid:88) i=2 di ˆdi (4) This loss regulates that the propagation of the first-frame edit follows natural dynamic trajectory, as learned from the idealized identity task, preventing the edits influence from fading or becoming distorted over time. Overall Training Objective. Our final training objective combines the standard flow matching loss LFM with our two proposed distillation objectives: = LFM + λmotionLmotion + λMMDLMMD, (5) where λmotion and λMMD are hyperparameters. Unlike methods that distill from external, generalist models [39], our self-referential guidance is uniquely suited to FFP, as it distills from teacher that has perfect knowledge of the source videos specific motion, ensuring edits are propagated without corrupting its essential temporal character. 5. Experiments and Results 5.1. Experiment Setup Implementation Details. We finetune Fun-Control using LoRA [8] for 2 epoches with rank is set to 128. AdamW [17] is utilized for training with learning rate of 2 104 and cosine decay. λmotion and λMMD are set to 5 and 1. For fair comparison with previous methods, for the main experiments we train two variants of our model with 81-frame videos and 33-frame videos. For ablation study, the one trained with 81-frame videos is engaged in. Benchmark and Metrics. For evaluation, we adopt EditVerseBench [12], comprehensive benchmark for video Type Method Resolution Frames CLIP DINO Temporal Consistency Text Alignment Frame Video Video Quality VLM Evaluation Pick Score VLM Score Training-free TokenFlow [22] STDF [36] 640336 576320 Instruction-based FFP-based InsV2V [5] LucyEdit [25] EditVerse [12] Aleph [1] VACE [11] Senorita [41] Senorita [41] Ours-33f Ours-81f 384384 832480 624352 1280 832480 864448 864448 1280720 1280720 48 24 32 81 64 64 61 33 33 33 81 0.987 0.965 0.972 0.985 0.986 0. 0.990 0.981 0.989 0.991 0.991 0.989 0.964 0.969 0.984 0.986 0.984 0.989 0.982 0.987 0.990 0.991 26.779 26.422 25.923 26.398 27.776 28. 27.169 27.243 27.754 28.293 28.316 24.244 23.768 23.092 23.491 25.293 24.837 24.188 24.404 24.657 25.398 25.925 20.058 19.817 19.611 19.611 20.132 20. 20.095 19.786 19.913 20.419 20.405 5.067 4.911 5.252 5.678 7.104 7.154 6.072 6.991 7.341 7.631 7.600 Table 1. Quantitative comparison. We compared three types of video editing methods on EditVerseBench. The best results are highlighted in bold, and the second-best results are underlined. As shown, our 33f and 81f variants achieve the best performance across all automated evaluation metrics, establishing state-of-the-art results on EditVerseBench. Senorita refers to using Qwen-Edit [31] to edit the first frame. editing that covers 20 diverse editing categories. Since our method focuses on FFP-based video editing, we further filter the benchmark to 125 videos with stable temporal structures that can be evaluated under propagation setting. Then Qwen-Edit [31] is leveraged to generate the edited first frame for these videos. We follow the six metrics defined in EditVerseBench: VLM editing quality, PickScore, Frame score, Video score, CLIP textimage alignment, and DINO-based temporal consistency. To better assess longsequence propagation, we extend the VLM evaluation from 2 frames to 10 sampled frames. Different from the original setup that uses GPT-4o [10] as the evaluation model, we replace it with Qwen2.5-VL-72B-Instruct [2] to ensure full reproducibility and consistency across evaluation runs. Competitors. We directly adopt the baseline models provided in EditVerseBench[12], including Token-Flow [22], STDF [36], InsV2V [5], Lucy-Edit [25], Senorita-2M [41], and Aleph [1], to ensure fair and consistent comparison. 5.2. Quantitative Comparison In Tab. 1 we present the quantitative comparison between the competitors and two variants of our method. For fair comparison, we test Senorita with the same edited first frames as ours. Our method, in both 33-frame (Ours33f) and 81-frame (Ours-81f) configurations, consistently outperforms all competing approaches across the board. Specifically, Ours-81f achieves the highest scores in temporal consistency (0.991 CLIP score, 0.991 DINO score) and video-level text alignment (25.925), showcasing its exceptional ability to maintain coherence over longer sequences. Furthermore, Ours-33f obtains the top scores in perceptual quality (20.419 Pick Score) and semantic correctness (7.631 VLM Score), indicating superior alignment with user intent. Notably, our model surpasses not only other FFPbased methods like VACE [11] but also strong instructionbased models, including the commercially used Aleph [1]. This highlights the effectiveness of our approach in achieving superior balance of temporal stability, edit fidelity, and overall visual quality. 5.3. Qualitative Comparison We further provide qualitative results to visually demonstrate the advantages of our framework. As illustrated in Fig. 4, previous instruction-based methods such as Aleph and EditVerse mainly suffer from the problem of unsuitable edited first frame, such as the wrong position of starfish in Fig. 4(a), and failure to preserve the content in the original videos. Moreover, it is noteworthy that videos generated by EditVerse also has the flickering problem, which cannot be fully presented with static frames but will be shown in the supplementary material. On the other hand, Senorita, as FFP-based method, is limited with the video quality, showing mosaic in the bottom of each frame. In contrast, our method produces results with not only longer duration, but also significantly better general quality, accurately preserving object structure and scene layout while maintaining global temporal consistency. This qualitative superiority verifies that the combination of our proposed techniques for consistency modeling and curated high-fidelity dataset enables robust editing propagation across diverse and challenging real-world scenarios. 5.4. User Study To further evaluate perceptual quality and editing accuracy, we conducted user study where participants rated videos on 15 scale based on: (1) Editing Accuracy (EA): instruction adherence and semantic consistency, (2) Motion Accuracy (MA): motion fidelity to the source video, and (3) Video Quality (VQ): temporal smoothness and realism. With 15 participants each assessing 8 random videos from EditVerseBench, our method achieved the highest mean scores in all criteria (Tab. 2), demonstrating user preference for its precise alignment and stable dynamics, consistent with quantitative results. Figure 4. Qualitative comparison. We Choose top three method in quantitative comparison to compare with our visual results across four representative video editing tasks. Red boxes highlight the unreasonable generated contents. The gray placeholder denotes these methods cannot generate such long videos. Our method generally enjoys better editing fidelity, temporal consistency and visual quality. Methods EA MA VQ EditVerse [12] Senorita-2M [41] Aleph [1] Ours 4.063 3.563 3.412 4. 3.792 3.208 3.271 4.333 3.354 2.354 3.459 4.146 Table 2. User study preference regarding editing accuracy (EA), motion accuracy (MA) and video quality (VQ). Our method is consistently preferred. Method Temporal Consistency CLIP DINO Text Alignment Frame Video Video Quality VLM Evaluation Pick Score VLM Score Baseline +AST-RoPE Full 0.986 0.989 0.991 0.984 0.988 0. 27.420 28.178 28.316 24.960 25.817 25.925 20.010 20.354 20.405 7.210 7.542 7.600 Table 3. Quantitative results for ablation variants of our model. 5.5. Ablation Study To validate the efficacy of each component in our framework, we conduct ablation study on three variants trained with 81-frame videos: the original WanFun model fine-tuned on our dataset without any modification, (2) +AST-RoPE: applying our spatialtemporal RoPE adaptation to enhance attention modules, (3) Full: integrating both RoPE adaptation and our proposed self-distillation (1) Baseline: strategy. Results are summarized in Tab. 3. Thanks to our proposed dataset, the baseline model can already achieve strong performance. Based on that, the RoPE adaptation and self-distillation can further enhance the quality in terms of both visual quality and text alignment, indicating the effectiveness of the proposed method. 6. Conclusion We addressed core limitation in First-Frame Propagation (FFP) video editing: foundational data gap that necessitates complicated run-time guidance which results in limited generalization ability, for which our solution covers two main aspects. First, we introduce FFP-300K, largescale dataset with high-quality and diverse videos. Second, our model leverages novel Adaptive Spatio-Temporal RoPE (AST-RoPE) and self-distillation to strengthen the first-frame reference and source motion preservation. This dual approach achieves state-of-the-art fidelity and temporal coherence. By tackling both data and model, we make high-fidelity, controllable video editing practical reality."
        },
        {
            "title": "References",
            "content": "[1] Introducing runway aleph. 1, 2, 7, 8 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 7, 11 [3] Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Anylength video inpainting and editing with plug-and-play context control. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. 2, 3 [4] Yinan Chen, Jiangning Zhang, Teng Hu, Yuxiang Zeng, Zhucun Xue, Qingdong He, Chengjie Wang, Yong Liu, Xiaobin Hu, and Shuicheng Yan. Ivebench: Modern benchmark suite for instruction-guided video editing assessment. arXiv preprint arXiv:2510.11647, 2025. 3 [5] Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent videoIn The Twelfth to-video transfer using synthetic dataset. International Conference on Learning Representations, 2024. 2, 3, 7 [6] Xiang Fan, Anand Bhattad, and Ranjay Krishna. Videoshop: Localized semantic video editing with noise-extrapolated diffusion inversion. In European Conference on Computer Vision, pages 232250. Springer, 2024. 3 [7] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactive semantic point correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7621 7630, 2024. [8] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 6 [9] Jiahao Hu, Tianxiong Zhong, Xuebo Wang, Boyuan Jiang, Xingye Tian, Fei Yang, Pengfei Wan, and Di Zhang. Vivid10m: dataset and baseline for versatile and interactive video local editing, 2025. 2, 3 [10] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 7 [11] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 2, 3, 4, 7 [12] Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin, Soo Ye Kim, and Qiang Xu. Editverse: Unifying image and video editing and generation with in-context learning, 2025. 2, 3, 6, 7, 8 [13] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. [14] Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. Anyv2v: plug-and-play framework for any videotovideo editing tasks. arXiv preprint arXiv:2403.14468, 2(3): 5, 2024. 3, 14 [15] Chang Liu, Rui Li, Kaidong Zhang, Yunwei Lan, and Dong Liu. Stablev2v: Stablizing shape consistency in video-tovideo editing. arXiv preprint arXiv:2411.11045, 2024. 2, 3 [16] Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, In Soo Ye Kim, et al. Generative video propagation. Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1771217722, 2025. 2, 3 [17] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [18] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. [19] Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. I2vedit: First-frame-guided video editing via image-to-video diffusion models. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2, 3 [20] Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. I2vedit: First-frame-guided video editing via image-to-video diffusion models. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2 [21] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4195 4205, 2023. 2 [22] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25452555, 2025. 7 [23] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 3 [24] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [25] Decart AI Team. Lucy-edit: Open-weight text-guided video editing. Technical report, Lucy-Edit Team, 2024. Accessed: 2025-10-28. 2, 3, 7, 14 Conference on Computer Vision and Pattern Recognition, pages 84668476, 2024. 7 [37] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing, 2025. 13 [38] Jaehong Yoon, Shoubin Yu, and Mohit Bansal. Raccoon: Versatile instructional video editing with auto-generated narratives. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 27960 27996, 2025. 3 [39] Xiangdong Zhang, Jiaqi Liao, Shaofeng Zhang, Fanqing Meng, Xiangpeng Wan, Junchi Yan, and Yu Cheng. Videorepa: Learning physics for video generation through relational alignment with foundation models. arXiv preprint arXiv:2505.23656, 2025. 6 [40] Zhenghao Zhang, Zuozhuo Dai, Long Qin, and Weizhi Wang. Effived:efficient video editing via text-instruction diffusion models, 2024. [41] Bojia Zi, Penghui Ruan, Marco Chen, Xianbiao Qi, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, and Kam-Fai Wong. Senorita-2m: high-quality instructionbased dataset for general video editing by video specialists, 2025. 2, 3, 7, 8, 14 [26] Wan Team. Wan: Open and advanced large-scale video generative models. 2025. 2, 4, 5 [27] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 [28] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84288437, 2025. 3 [29] Ye Wang, Ruiqi Liu, Jiang Lin, Fei Liu, Zili Yi, Yilin Wang, and Rui Ma. Omnistyle: Filtering high quality style transfer data at scale. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 78477856, 2025. 3, 4 [30] Yukun Wang, Longguang Wang, Zhiyuan Ma, Qibin Hu, Kai Xu, and Yulan Guo. Videodirector: Precise video editing via text-to-video models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2589 2598, 2025. [31] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 1, 7 [32] Yuhui Wu, Liyi Chen, Ruibin Li, Shihao Wang, Chenxi Xie, and Lei Zhang. Insvie-1m: Effective instruction-based video editing with elaborate dataset construction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1669216701, 2025. 2, 3 [33] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 5 [34] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. 4 [35] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. 2 [36] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In Proceedings of the IEEE/CVF 7. Additional Information about FFP-300K The video frames shown in our figures have been packaged and uploaded. Please refer to the accompanying zip file for details. 7.1. Dataset Construction Prompts used. Our data construction pipeline follows two-track modular pipeline and we use Qwen2.5-VL-72BInstruct [2] to produce the prompts for both local editing and global stylization. 7.1.1. Local Editing To identify the primary editable objects in each video, we use prompt that analyzes the first frame. Object Identification. You are given single video frame. main editable object in this frame. Rules: Identify the Output THREE lowercase category word only (e.g., person, car, dog, ball, cup, bottle, phone, bag, plant, flower, sign, tableware). Do not describe background or actions. If several candidates exist, choose the smallest salient object that humans often edit/remove (e.g., ball before person in sports; cup before hands on table). Output strictly in JSON: {Object: Category} The original caption is constructed to preserve the scene context and serve as reference when replacing the masked object for swap tasks. Original Caption. You are given short video. Write ONE concise caption in present tense (1830 words) describing only the stable scene elements (location, background, persistent subject categories). Rules: Describe what across the clip. remains visually consistent Use generic categories for moving actors (e.g., person, three red cups on white table, yellow taxi by street). Avoid counts unless they are constant; avoid names, brands, emotions, camera terms. No negations (e.g., no/without). Keep it objective, concrete, and free of text/letters. Output strictly in JSON: Caption: The video shows ... The removal caption describes the scene without the target object identified earlier. Removal Caption. You are given an original video caption and target object to remove. Rewrite the caption so it naturally describes the scene as if that object never existed. Rules: Output ONE fluent sentence in present tense, 3560 words. Start with: The video shows ... Do NOT mention the removed object, any pronouns referring to it, or actions tied to it (e.g., holding, touching, pouring). Do NOT use negations like no/without. Do NOT invent new objects or text/letters that the original background did not imply. Keep only concrete, persistent background elements (location, surfaces, vehicles, trees, buildings, sky, lighting, colors, furniture). Original Caption: Input: Removal:Object Output strictly in JSON: {Remove Caption: The video shows ... } Original Caption, For swap tasks, this prompt determines whether the edited video should be classified as swap or modification. Task Discrimination. You are given two short videos: Video A: Source Video Video B: Generated after editing Task: Compare and by examining the first, middle, and last frames. Decide: swap: an object in is replaced by different object in B. The region still contains an object, but its identity changes. modification: the same object remains in B, but its attributes (shape, color, texture, style, size, letters, patterns, or fine details) are edited without replacing it with different object. If the objects identity clearly changes, classify as swap. If only attributes or features change while the object stays the same, classify as modification. Output strictly in JSON: {Task: Swap} or {Task: Modification} 7.1.2. Global Stylization For global stylization, cinematic caption is first constructed to summarize the scene and atmosphere of the input Figure 5. Word cloud of edited objects of the local editing subst of FFP-300K. artistic image for source video generation. Image-to-Video Caption. You are an image-tovideo prompt generator. Analyze the input image {image} and output only one cinematic video prompt. Rules: Provide concise scene description (environment, atmosphere, subjects). Do not add new motions to the subjects; keep them static as in the image. Focus on cinematic camera work: wide shots, dolly-in, pans, or close-ups. You may suggest smooth transitions or scene framing, but no new actions. Limit the output to 34 sentences. Output: only the final video prompt. For stylized video generation, the following prompt produces detailed style description based on both the reference style image and the source video. Style Caption. Apply style transfer using the reference image, but keep the output cinematic and natural. Rules: Style Control: Use soft, balanced colors with reduced saturation, no overexposure. Subject Preservation: Preserve the subjects natural tones and details (do not oversaturate). Lighting & Texture: Maintain subtle textures, soft lighting, and film-like atmosphere. Constraints: Avoid harsh highlights, neon effects, or unnatural color shifts. Figure 6. Scene distribution of the local editing subset of FFP300K 7.2. Dataset Analysis Distribution of edited objects. We visualize the objects selected for local editing in FFP-300K in Fig. 5. The word cloud highlights substantial diversity in the edited-object space of the local-editing subset of FFP-300K: while people and hand-held items (e.g., person, microphone, guitar, phone) are prominent, there is wide spread of categories spanning furniture and electronics (table, chair, laptop), animals and nature (horse, tree, bird), vehicles and buildings, and many everyday objects. This long-tailed, semantically rich distribution indicates the dataset supports broad range of local-editing scenarios, from fine-grained human-centric manipulations to structurally complex scene elements. Consequently, models trained on FFP-300K are exposed to varied object types and contexts, which helps foster robustness and generalization across diverse editing tasks. Distribution of video content. To demonstrate the content diversity of FFP-300K, for each source video adopted, we extract 5 frames and ask Qwen2.5-VL to classify them into 15 predefined scenes, of which the distribution is shown in Fig. 6. The scene distribution of the local-editing subset is strongly skewed toward few dominant contextsIndoor Activities (12,710 videos, 29.2%), Performance & Entertainment (8,225, 19.0%) and Urban Scenes (6,904, 15.9%)while the remaining categories (e.g., Outdoor Activities, Sports, Nature, Animals, Transport, Technology, Medical, etc.) form long tail with individual shares typically below 8%. This composition provides dense coverage of common indoor and urban editing scenarios that are crucial for real-world applications, while still retaining broad scene diversity for generalization. 7.3. Visualization of FFP-300K To illustrate the visual results of FFP-300K, we provide representative examples from the two tracks of our data construction pipeline. Both tracks maintain spatial coherence and temporal consistency across all frames, enabling the model to learn strong motion priors through the first-frame propagation paradigm and supporting reliable video editing. Local Editing. The local editing track constructs objectlevel samples using remove and swap manipulations. These samples are generated by editing specific target objects in the source video while keeping the surrounding scene unchanged, forming paired sequences that cover diverse object categories and scene contexts. As shown in Fig. 7, these examples reflect the broad coverage of fine-grained object manipulations and varied local-editing scenarios present in FFP-300K. Global Stylization. The global stylization track generates full-scene style-transfer samples by applying the appearance of reference image to the entire source video. Each source video is paired with multiple reference images, producing multiple stylized sequences that span wide range of aesthetic styles. As illustrated in Fig. 8, these samples expand the appearance diversity of the dataset and represent the full-scene stylization capabilities captured in FFP300K. 8. Additional Method Details 8.1. Attention Head Classification Heuristic Our proposed AST-RoPE requires pre-classification for each self-attention head. While previous methods such as SparseVidGen and Follow-your-motion utilize samplespecific classification, we find that the category of each attention head is generally sample-agnostic, which is intuitively reasonable that each head learns fixed prior knowledge. Therefore we design simple classification strategy as follows. Grid-based Partitioning of the Attention Map. For given self-attention head and an input video with frames, each of resolution , the total number of tokens is = . The attention map is matrix RN . We conceptually partition this large matrix into grid of smaller sub-matrices. Each submatrix Aij represents the attention from all tokens in the source latent frame to all tokens in the target latent frame j. Quantifying Attention Density. We measure the activity within each grid by calculating its attention density. The attention density ρij for grid Aij is defined as the proportion of its elements that are non-zero. In practice, due to the softmax function, all attention scores are positive. We therefore define density as the proportion of attention scores exceeding small threshold ϵ (e.g., ϵ = 106) to filter out negligible floating-point values. ρij ="
        },
        {
            "title": "1\nH × W × H × W",
            "content": "HW (cid:88) HW (cid:88) u=1 v=1 where I() is the indicator function. I(Aij[u, v] > ϵ) (6) The Classification Rule. Our heuristic compares the strongest temporal signal against the weakest spatial signal. Let Ddiag = {ρii [1, ]} be the set of densities for all diagonal (spatial) grids, and Dnon-diag = {ρij i, [1, ], = j} be the set for all non-diagonal (temporal) grids. An attention head is classified as temporal if its maximum non-diagonal attention density is greater than its minimum diagonal attention density. Otherwise, it is classified as spatial. (cid:40) Head Type = Temporal Spatial if max(Dnon-diag) > min(Ddiag) otherwise (7) The intuition is that for head to be genuinely temporal, its cross-frame attention must be meaningful and stronger than its most diffuse, weakest intra-frame attention. head that only pays weak, noisy attention across frames but strong attention within frames will be correctly classified as spatial. Final Classification via Majority Voting. The behavior of an attention head can be content-dependent. To obtain stable and generalizable classification, we do not rely on single video sample. Instead, we apply the classification process described above to set of 10 diverse video samples randomly drawn from our validation set. The final, definitive classification for each attention head is determined by majority vote on the outcomes from these 10 samples. This aggregation ensures that the assigned role reflects the heads typical behavior rather than an artifact of specific input. 9. Additional Experiment Results 9.1. Experiments on UNICBench As supplement to the experiment in the main paper, we further conduct experiments on UNICBench [37], which is filtered by us with the same principle as for EditVerseBench to delete samples that are not suitable for FFP. The whole test set contains 128 videos, covering tasks of add, delete, change and stylization. We adopt UNIC [37], Type AnyV2V LucyEdit Senorita UNIC Ours Temporal Consistency CLIP DINO Text Alignment Frame Video Video Quality VLM Evaluation Pick Score VLM Score 0.941 0.978 0.985 0.980 0.986 0.92 0.977 0.981 0.973 0.982 23.597 22.171 24.197 24.267 24.879 20.138 18.036 20.273 20.116 20.733 19.864 19.612 19.950 19.182 19. 4.132 5.065 6.648 5.203 6.672 Table 4. Quantitative comparison. We compared three types of video editing methods on UNICBench. The best results are highlighted in bold. AnyV2V [14], LucyEdit [25] and Senorita [41] as baseline methods, among which the results of UNIC and AnyV2V are provided by UNIC, and results of the other two methods are produced by us. We adopt the same metrics as EditVerseBench, which are presented in Tab. 4. Our method receives the best performance in terms of all metrics. The qualitative comparison is shown in Fig. 12, which further demonstrates that our method is not only more accurate for editing but also visually better. 9.2. More Results on EditVerseBench As complement to the visual examples in the main paper, we provide additional visualization results on EditVerseBench to offer broader view of the editing results produced by our method. Among these results is full-task visualization that shows all four main editing tasksadd, remove, change, and stylizationtogether with the correIn addition, sponding source video, as shown in Fig. 9. we include two orientation-specific visualizations: one for landscape orientation, as presented in Fig. 10, and one for portrait orientation, as illustrated in Fig. 11. Each visualization compares the edited videos with its corresponding source video and serves as supplementary demonstration of our methods editing results under different video orientations. 9.3. More Results on UNICBench We provide additional visualization results on UNICBench to present the editing results of our method together with the source video and UNIC under the FFP-based video editing paradigm, as shown in Fig. 13. This example offers direct visual comparison of the editing results produced by our method and UNIC. We also include mixed visualization that incorporates cases for which UNIC does not provide FFP-based video editing outputs, as presented in Fig. ??. In these cases, we present the instruction-based outputs from UNIC alongside our FFP-Based results to provide broader visual reference across the different video editing types. Figure 7. The visualization of local editing track in FFP-300K. Figure 8. The visualization of global stylization track in FFP-300K. Figure 9. More results of local editing and global stylization tasks on EditVerseBench. Figure 10. More visual results in the landscape orientation on EditVerseBench. Figure 11. More visual results in the portrait orientation on EditVerseBench. Figure 12. Qualitative Comparison. We choose top three methods in quantitative comparison to compare with our-33f visual results across local editing and global stylization tasks. Figure 13. Visualization results of our method and UNIC on UNICBench for FFP-based video editing."
        }
    ],
    "affiliations": [
        "FDU",
        "Tencent YouTu Lab"
    ]
}