{
    "paper_title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing",
    "authors": [
        "Zinan Guo",
        "Pengze Zhang",
        "Yanze Wu",
        "Chong Mou",
        "Songtao Zhao",
        "Qian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 3 2 8 2 0 . 5 0 5 2 : r MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing Zinan Guo Pengze Zhang Yanze Wu Chong Mou Bytedance Intelligent Creation Songtao Zhao Qian He Figure 1: Breaking the data barrier, MUSAR enables remarkable multi-subject customization from solely single-subject dataset, demonstrating scalable generalization as the number of subjects grows."
        },
        {
            "title": "Abstract",
            "content": "Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting Corresponding author Preprint. Under review. the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset."
        },
        {
            "title": "Introduction",
            "content": "Customized text-to-image (T2I) generation is the process of creating images from text prompts with additional user-specific inputs, such as personal identity, subject, or style, to produce results tailored to individual needs. It is widely applied in areas such as personalized content creation, virtual try-on, creative design, and marketing. To circumvent the need for test-time fine-tuning [9, 34, 15] on each user input, developing tuning-free approaches for image customization has emerged as central focus of current research. For T2I diffusion models that utilize UNet as the backbone [7, 33, 30], mainstream image customization methods [46, 43, 5, 48, 12, 42, 25] typically involve using an encoder (e.g., CLIP [31]) to extract features from the reference image, which are then injected into the modelalongside the textat the cross-attention layers of the UNet. Despite their progress, these methods still face challenges such as insufficient fidelitydue to information loss during feature extractionand the need to meticulously design task-specific adapters for each application. With the emergence of diffusion transformers (DiT) [29, 8, 21], new paradigm for controllable generation has recently gained traction [40, 3, 26, 22]. These methods employ DiTs pre-trained VAE to extract features from the reference image, which are concatenated with the text modality and the noisy image modality along the sequence dimension. The integrated sequence is then processed by the multi-modal self-attention block (i.e., MMDiT [8] block) in the DiT, enabling information interaction and conditional fusion. Owing to its simple and unified design that can be applied across various tasks [22], as well as the substantial reduction in information loss achieved through the use of VAE, this class of methods exhibits clear advantages over previous approaches. However, despite achieving notable improvements in single-subject customization [40], extending these methods to multi-subject scenarios remains significant challenge. Firstly, the vast majority of previous works [25, 48, 42] on multi-subject customization are tailored for UNet architecture and cannot be directly transferred to the DiT model. Secondly, the few existing multi-subject customization methods [6] built on the unified DiT framework [6] heavily rely on large-scale multi-subject paired datasets, which are often difficult to construct or collect. Lastly, in the absence of strong model priors (e.g., fine-tuned from video diffusion model [6]), distinguishing the features of different reference images and avoiding attribute entanglement remain non-trivial challenges. In this study, we propose MUSAR to address two key challenges in multi-subject text-to-image (T2I) generation: heavy reliance on large-scale multi-subject datasets and attribute entanglement across subjects. First, we discover that diptych-based trainingconstructed from single-subject data effectively handles multi-subject generation while mitigating data scarcity. However, naive diptych training leads to mode collapse due to the inherent bias in learning diptych pairs. To resolve this, we introduce de-biased diptych learning, incorporating two strategies, i.e., static attention routing and dual-branch Lora mechanisms, to alleviate the systematic bias introduced by diptych data. Second, we observe that in T2I generation, distinct subjects (e.g., \"Einstein and Newton shaking hands\") are typically rendered without identity confusion by state-of-the-art models [30, 21], implying that each noisy image token can be mapped to its corresponding subject in the prompt. Leveraging this insight, we propose dynamic attention routing mechanism, constraining each noisy token to attend only to reference tokens of its associated subject during self-attention, significantly alleviating attribute entanglement. Thanks to our carefully designed framework, our MUSAR achieves robust generalization to multi-subject generation tasks while requiring only single-image training data (Figure 1), even outperforming existing approaches that rely on large-scale multi-subject datasets. 2 We summarize the contributions as follows. (1) We circumvent the difficulty of acquiring high-quality multi-subject data by training solely on diptych data constructed from concatenated single-subject samples, and further mitigate potential diptych-induced biases through static attention routing and dual-branch LoRA mechanism. (2) We propose dynamic attention routing mechanism that adaptively aligns image regions with their corresponding condition subjects, effectively preventing cross-subject entanglement while maintaining scalability for increasing reference subjects. (3) Experimental results demonstrate that our method enables flexible and coherent multi-subject interactions while maintaining high fidelity using only single-object datasets."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Diffusion models Diffusion models [37, 14, 39, 7, 38, 19, 23] have emerged as cornerstone of generative tasks, particularly in text-to-image synthesis. Early UNet-based designs [33, 30] integrated text conditioning via cross-attention within convolutional backbones. These works laid the groundwork for diffusion models, enabling wide range of downstream tasks such as customized text-to-image generation [47, 28, 46], image inpainting [24, 45], image-to-image translation [36, 49], and image editing [1, 27]. Recent transformer-based models such as the diffusion transformer (DiT) [29, 8, 21] represent significant advance by incorporating full attention to simultaneously model both intraimage and text-image interactions. This architectural has proven to be highly efficient and has brought substantial enhancements in downstream tasks, as evidenced by [17, 16, 2, 40]. However, it unavoidably leads to feature entanglement when dealing with multi-condition inputs. This entanglement presents formidable challenges for applications that require fine-grained control, such as multi-subject customization. 2.2 Subject Customization Initial research on subject customization mainly employed in UNet-based diffusion models, which can be broadly classified into two paradigms: test-time fine-tuning and fine-tuning-free paradigms. The test-time fine-tuning methods, exemplified by works like [41, 35, 20, 13, 11, 10], typically involve refining textual embeddings or adjusting model parameters to achieve subject-specific adaptation. Although effective, these approaches require computationally expensive optimization for each new subject, significantly limiting their practical applicability. To overcome this constraint, researchers developed tuning-free alternatives [46, 43, 48, 5, 12, 42, 25, 18] that employ external encoders to represent target subjects, subsequently injecting these representations into pre-trained models via lightweight adapter modules. Some extensions of these methods [25, 48, 42] have demonstrated potential for multi-subject generation tasks. Nevertheless, their performance remains constrained by inherent limitations of the U-Net based diffusion model and the absence of specialized designs for handling multiple subjects. [7, 33, 30] has significantly transformed the The emergence of diffusion transformers (DiTs) paradigm of subject customization. Unlike traditional U-Net-based approaches, most DiT-based methods [40, 3, 26, 22] adopt unified conditioning strategy that jointly processes text embeddings, latent tokens, and VAE-encoded condition subjects. This unified design enables for more effective interaction via full attention mechanisms, demonstrating superior performance in single-subject generation tasks. Nevertheless, this architecture faces inherent limitations in multi-object generation, as the global attention mechanism induces feature interference between objects, resulting in attribute entanglement and identity degradation. Recent work [6] has attempted to address this by constructing video-derived paired training data. However, they not only require massive amounts of carefully aligned paired data, but lacks solutions to prevent attribute entanglement. Consequently, achieving robust multi-subject customization within DiT frameworks remains an open research challenge."
        },
        {
            "title": "3 Methods",
            "content": "Our MUSAR framework addresses multi-subject customization through two key components. Firstly, to overcome the scarcity of multi-subject datasets, we propose De-biased Diptych Learning strategy. It simultaneously enhances multi-object preservation through diptych data construction, and reduces 3 Figure 2: sample of diptych learning. We pairing existing single-subject data, creating multicondition and prompts inputs, and diptych targets for effective multi-subject learning. learning bias via our Static Attention Routing and Dual-branch LoRA techniques. Secondly, we propose Dynamic Attention Routing to address the critical issue of subject entanglement. It dynamically establishes bijective mappings between image regions and condition subjects, effectively eliminating cross-object interference through selective attention masking of non-corresponding conditions. The following sections provide detailed descriptions of these core components. 3.1 Preliminary The Diffusion Transformer (DiT) represents state-of-the-art framework for image generation by jointly processing noisy image tokens Rnd and prompt tokens Rmd through unified multi-modal attention mechanism, where corresponds to the latent dimension, while and correspond to the sequence lengths of text and image tokens respectively. FLUX.1, as DiT implementation, employs specialized architecture combining adaptive layer normalization modules with Multi-Modal Attention (MMA) blocks. Within this framework, both image and text tokens are linearly projected into query (Q), key (K), and value (V ) representations, enabling cross-modal attention across all tokens: MMA([T ; X]) = softmax (cid:18) QK (cid:19) + V, (1) where [T ; X] denotes the concatenation of noise image and prompt tokens. R(l)(l)(l = + n) serves as the attention flow matrix that regulates cross-modal interactions. Each entry Mi,j controls the attention strength between token pairs, where Mi,j = 0 permits full cross-attention between image token and j, while Mi,j = completely blocks their interaction. In FLUX.1, is initialized as zero matrix, enabling unconstrained bidirectional attention across all image-text tokens. 3.2 De-biased Diptych Learning 3.2.1 Diptych Learning Models trained on single-subject datasets are struggling in multi-subject customization. Meanwhile, the collection of multi-subject datasets encounters formidable obstacles in terms of both data construction and annotation. To bridge this critical gap, as shown in Figure 2, we present simple yet effective framework to construct multi-subject datasets based on existing single-subject datasets. For data construction, we randomly select pairs of distinct subjects from single-subject datasets as conditional references, then concatenate their target images as the diptych target image. For prompt engineering, conditional prompts are derived directly from original single-subject descriptions, with the addition of the \"another\" modifier to disambiguate identical subject types; target image prompts are generated using structured two-column template, where conditional prompts are adaptively inserted into corresponding left/right column descriptions. This paradigm successfully emulates authentic multi-subject scenarios while preserving crucial visual-textual relationships. With the constructed diptych data, we extract representation for each condition i: condition image tokens CI Rnd extracted by VAE, and condition prompt tokens CT Rmd by text encoder. Then these tokens are directly concatenated with the text tokens and the noisy image tokens to form the composite input [CI 1; CT 1; ...; CI c; CT c; ; X] R(cl+l) for the DiT module, where = + denotes the combined token length of text and image of conditions i, and is the number Figure 3: Two strategies to mitigate learning diptych biases. (a) Static Attention Routing: routing mechanism that prevents prompt-condition contamination and inter-condition interactions. (b) Dualbranch LoRA: specific LoRA pathways are selectively activated based on input condition types. of condition subjects. This formulation naturally extends the attention flow matrix (Eq. 2) to dimensionsR(cl+l)(cl+l), enabling the model to learn coherent multi-subject preservation on the target diptych images. It is worth noting that is set to 2 during training due to the diptych data, though the model supports arbitrary numbers of conditions at inference. 3.2.2 Diptych Biases Mitigation Although the straightforward diptych data construction effectively enables multi-subject learning, it inadvertently introduces two distinct diptych-induced biases: (1) prompt bias, where the use of diptych templates in prompt input corrupts the pre-trained models text-to-image generation priors, and (2) layout bias, where the uniform diptych arrangement causes the model to develop strong inherent preference for this specific spatial pattern. To mitigate these biases, we introduce two key strategies: static attention routing and dual-branch LoRA. Static Attention Routing. To effectively address diptych bias induced by input prompts, we propose static feature routing mechanism that operates across multiple conditions (Figure 3(a)), comprising two key components: (1) Prompt-Condition Decoupling. The attention flow between the prompt and all conditional features is disabled, i.e., Mcl:cl+m,0:cl = M0:cl,cl:cl+m = . This effectively blocks the transmission of prompt-induced bias to the conditional learning (2) Interpathway, ensuring the model focuses on preserving individual object characteristics. Condition Isolation. We further enforce strict separation between different conditional inputs, i.e., Mil:(i+1)l,jl:(j+1)l = Mjl:(j+1)l,il:(i+1)l = , where 0 i, 1 and = j. This design minimizes cross-condition interference while enhancing feature discriminability, significantly improving the models multi-subject generation robustness. Dual-branch LoRA. To effectively mitigate diptych bias while learning the multi-subject generation, we propose specialized LoRA [15] optimization strategy that differentially processes each input (Figure 3 (b)): (1) For prompt input, we intentionally freeze the corresponding weights without LoRA fine-tuning, since the exhibiting inherent template bias that could reinforce two-column image priors. (2) For noise image and conditional inputs, we design dual-branch LoRA, incorporating low-rank noisy image LoRA to suppress layout bias learning from the target image, and high-rank subject LoRA to efficiently learn multi-subject preservation. This asymmetric rank design creates balanced feature learning that simultaneously suppresses spatial overfitting while enhancing-subject representation learning. 3.3 Dynamic Attention Routing Although diptych learning and static attention routing reduce inter-subject feature entanglement, significant entanglement remains for semantically similar subjects (e.g., subjects of the same category). We believe that this phenomenon originates from the attention flow between noise tokens and condition tokens. Such flow enables the noise tokens to indiscriminately attend to and blend features across multiple conditions, leading to spatial superposition of conflicting object features. straightforward 5 Figure 4: Dynamic Attention Routing enforces bijective mapping between noise tokens and condition subjects, effectively mitigating multi-subject feature entanglement. solution [18] is to predict subject-specific masks at inference time, enforcing the spatial separation of feature injection through constrained attention regions. However, these approaches present two fundamental challenges: (1) the mask predicted from intermediate text-to-image timestep often shows significant shape discrepancies with target subjects, leading to irreversible information loss, and (2) they typically require extensive tuning of optimal timestep and specific network blocks for mask prediction, severely hindering cross-architecture generalization. To resolve these compounded challenges, we propose Dynamic Attention Routing that adaptively determines optimal injection subject targets for each noise token. The architecture of Dynamic Attention Routing is illustrated in Figure 4. The routing process begins with computing similarity matrix Rnm between noise tokens and prompt tokens: = softmax (cid:18) QX (cid:19) , (2) where QX Rnd and KT Rmd denote the projected query and key matrices derived from noise tokens and prompt tokens respectively. As discussed in the introduction, modern text-to-image models can correctly map noisy image tokens to their corresponding textual subjects. Building on this insight, we establish condition-level associations by computing noise-condition affinity scores, obtained through averaging across each conditions relevant tokens. Let pk denotes the starting index and lk specifies the length of the token subsequence Tpk:pk+lk corresponding to the k-th condition in the prompt tokens. The affinity score between noise token and condition can be expressed as: i,k = 1 lk lk1 (cid:88) z=0 Si,pk+z, {0, . . . , 1}, (3) yielding the noise-condition affinity matrix Rnc. Based on this affinity measure, we assign each noise token to its maximally relevant condition by taking the argmax function over S, while simultaneously masking attention to all other competing conditions: Mi,j = 0 if (cid:4) if (cid:4) (cid:5) = arg max k{0,...,c1} (cid:5) = arg max k{0,...,c1} (cid:17) (cid:16) i,k (cid:17) , for (cid:16) i,k (cid:26)c + < + 0 < . (4) In this way, as shown in the right part of the Figure 4, the proposed dynamic attention routing enforces strict bijective mappings between noise tokens and condition subjects, effectively eliminating feature entanglement. It is worth noting that while the Dynamic Attention Routing assigns condition for all all noise tokens - including background, the inherently low correlation between background and condition tokens renders this mandatory selection negligible. We further visualize the noise-conditional affinity matrix in Figure 5 to verify the effectiveness of the proposed dynamic attention routing. It can be seen that (1) can successfully establish clear 6 Figure 5: Visualization of the noise-condition affinity score in Dynamic Attention Routing. Each row displays dynamic routing probabilities per condition, demonstrating how adaptive attention selectively focuses on different conditions throughout the denoising process. relation between noise and condition tokens throughout the diffusion process, even at large timesteps (t 1); (2) each noise token adaptively switches its focus condition across timesteps, enabling more natural multi-object interactions (e.g., perspective relationships between sunglasses and cartoon characters)."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details We build our model based on FLUX.1-dev [21] and fine-tune it with Dual-branch LoRA. Specifically, we use LoRA rank of 128 for subject LoRA, and 4 for the noisy image LoRA. We construct our training set from Subject200K [40], retaining only samples with the maximum quality rating (score = 5), resulting in 111,761 high-fidelity single-subject paired samples. The training process is divided into three stages, progressing from easy to hard. The initial stage (20,000 iterations) establishes fundamental capabilities through exclusive single-subject training, developing robust subject-specific adaptation. Building upon this foundation, the second stage (10,000 iterations) implements strategic mixed regime combining 80% randomly paired diptych data with 20% single-subject samples, cultivating essential cross-subject discriminative abilities for effective multi-subject customization. This enables the model to develop robust cross-subject discriminative capabilities essential for handling multi-subject customization. The final stage (10,000 iterations) replaces random pairings with same-category diptych constructions, compelling the model to master fine-grained intra-class distinctions while mitigating attribute entanglement. All experiments were conducted on 8 NVIDIA A100 GPUs, with batch size of 8, learning rate of 1e-5, and training resolution of 512 512. We evaluate our methods performance across both single-subject and multi-subject customization tasks. For single-subject evaluation, we employ the complete set of 750 test samples from the DreamBench dataset [34]. For multi-subject scenarios, we construct test cases by pairing subjects from DreamBench to create 60 unique pairs, along with 20 composed triplets, resulting in comprehensive set of 80 multi-subject test samples. Following previous works, we measured the models quantitative performance through image and text fidelity. For image fidelity, we used the CLIP [32] and DINO [4] models to calculate the cosine similarity between the generated images and the reference images, referred to as CLIP-I and DINO, respectively. To evaluate multi-subject generation, we extended CLIP-I and DINO by computing the average similarity between each generated image and all corresponding reference images. For text fidelity, we used the CLIP model to calculate the cosine similarity between the generated images and the text prompts, which is known as CLIP-T. To ensure statistical reliability, each test sample was generated with four different random seeds. 4.2 Qualitative Comparison We conduct comprehensive qualitative comparison with state-of-the-art multi-subject customization methods, including Omnigen [44] and MS-Diffusion [42], as shown in Figure 6. It is worth to note that while all comparison methods are trained on carefully curated multi-subject datasets, our MURSAR achieves superior performance in subject consistency, attribute disentanglement, and visual 7 Figure 6: Qualitative comparison with several state-of-the-art methods on multi-subject customization. denotes the method training on multi-subject dataset. fidelity using only single-subject training data. Specifically, for the case of two subjects (row 1-4), our method generates naturally coordinated compositions, in contrast to baseline methods that often produce artificial copy-paste artifacts or physically implausible interactions. For instance, as shown in Figure 6 our method accurately renders the lighting interaction between the lantern and figurine (first row, left), demonstrates plausible sitting postures (second row, right), and correctly positions the scarf around the robots neck (third row, right). Despite being trained exclusively on two-subject data, MUSAR shows remarkable generalization to multi-object scenarios (row 5), surpassing all competing methods. This capability stems from our novel Dynamic Attention Routing mechanism, which intelligently allocates optimal conditions to each spatial regions, effectively mitigating multi-subject interference. Furthermore, we conducted qualitative comparisons with single-subject methods, including Omnigen [44] and MS-Diffusion [42], OminiControl and DSD [3]. The results demonstrate that while our method primarily focuses on multi-subject preservation, it simultaneously achieves state-of-the-art performance in single-subject customization . 4.3 Quantitative Comparison As demonstrated in Table 1, our method outperforms existing approaches in both single-subject and multi-subject customization tasks. As one can see, MUSAR achieves the highest scores across four metrics compared to all baseline methods. Particularly noteworthy is MUSARs performance in multi-subject scenarios: despite being trained solely on single-subject dataset, it surpasses the method (OmniGen [44], MS-DIFFUSION [42]) using specialized multi-subject data in visual fidelity metrics (DINO and CLIP-I) while maintaining comparable text alignment (CLIP-T). These results clearly demonstrate MUSARs superior performance in preserving subject identity and attributes across different customization scenarios. 8 Figure 7: Qualitative comparison on single-subject customization. Table 1: Quantitative comparisons of single-subject and multi-subject customization on DreamBench. denotes the method training on multi-subject dataset. The best results are shown in bold. CLIP-I DINO Single-subject Customization CLIP-T DINO 31.07 0.720 31.06 0.752 31.01 0.765 31.91 0.735 0.774 30. 0.804 0.811 0.820 0.819 0.833 Multi-subject Customization CLIP-T 33.17 34.20 33.90 CLIP-I 0.716 0.711 0.720 0.691 0.678 0.704 Mothod OminiControl [40] DSD [3] OmniGen [44] MS-DIFFUSION [42] MUSAR (Ours) 4.4 Ablation Study We conduct comprehensive ablation experiments to analyze the impact of each model component. The experimental setup involves removing the following elements from our full model. w/o Diptych data. This model is training without the constructed diptych data and relies exclusively on single-subject data for training. w/o Diptych Biases Mitigation. This model removes the Diptych Biases Mitigation: Static Attention Routing module and Dual-branch LoRA, enabling prompt-condition and inter-condition interactions, uniformly use single set of LoRA to fine-tune all parameters. w/o Dynamic Attention Routing. This model removes the Dynamic Attention Routing module, enabling each region to simultaneously attend to multiple subjects. Full Model. De-biased diptych learning and Dynamic Attention Routing are applied in this model. Figure 8 presents the qualitative results of our ablation study. For models without diptych learning (column 1), the generated samples exhibit poor multi-subject consistency. This conclusively demonstrates the importance of diptych learning for preserving multi-subject characteristic. For models without static attention flow (column 2), the model tend to learn biases from the diptych data prompts during training, leading to generate diptych results during inference, and causes the loss of elements included in the prompt. For example, the case in row 2, the model erroneously merges multiple subjects in the generated output, failing to properly respond to the discrete objects specified in the text prompt. For models without dynamic attention flow (column 3), the model exhibits significant cross-subject confusion, manifesting in erroneous attribute entanglement between distinct subjects - 9 Figure 8: Qualitative comparison of the ablation study. as exemplified in row 2 by the toy incorrectly adopting the robotic dogs color. Thanks to our carefully designed de-biased diptych learning and dynamic attention routing mechanism, our proposed MUSAR demonstrates the remarkable capability of learning complex multi-object customization solely from single-subject datasets."
        },
        {
            "title": "5 Conclusion",
            "content": "We present MUSAR, novel framework for multi-subject customization that learns effectively from single-subject data. To address data limitations, we propose debiased diptych learning, which synthesizes diptych training pairs from individual subject images while correcting systemic bias through static attention routing and dual-branch LoRA adaptation. For cross-subject entanglement, we develop dynamic attention routing that employs spatial gating to guide image regions to associate with their corresponding subjects. Quantitative and qualitative results comprehensively demonstrate MUSARs superiority over state-of-the-art methods across image fidelity, subject consistency, and interaction naturalness, while requiring only single-subject training dataset."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1820818218, June 2022. [2] Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, and Daniel Cohen-Or. Stable flow: Vital layers for training-free image editing, 2024. [3] Shengqu Cai, Eric Chan, Yunzhi Zhang, Leonidas Guibas, Jiajun Wu, and Gordon Wetzstein. Diffusion self-distillation for zero-shot customized image generation. arXiv preprint arXiv:2411.18616, 2024. [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers, 2021. [5] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In CVPR, pages 65936602, 2024. [6] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. arXiv preprint arXiv:2412.07774, 2024. [7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In ICLR, 2023. [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2023. [11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Trans. Graph., 42(4), July 2023. [12] Zinan Guo, Yanze Wu, Chen Zhuowei, Peng Zhang, Qian He, et al. Pulid: Pure and lightning id customization via contrastive alignment. 2024. [13] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 73237334, October 2023. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 68406851, 2020. [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. [16] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Huanzhang Dou, Yupeng Shi, Yutong Feng, Chen Liang, Yu Liu, and Jingren Zhou. Group diffusion transformers are unsupervised multitask learners. arXiv preprint arxiv:2410.15027, 2024. [17] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arxiv:2410.23775, 2024. [18] Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom: Narrowing real text word for real-time open-domain text-to-image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 74767485, June 2024. [19] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Advances in Neural Information Processing Systems, volume 35, pages 2656526577. Curran Associates, Inc., 2022. [20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multiconcept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19311941, June 2023. [21] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [22] Duong Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all. arXiv preprint arXiv:2411.16318, 2024. [23] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [24] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1146111471, June 2022. [25] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In SIGGRAPH Asia, 2024. [26] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instruction-based image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 11 [27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. [28] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 42964304, 2024. [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. [35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2250022510, June 2023. [36] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, 2022. [37] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, volume 37, pages 22562265, 0709 Jul 2015. [38] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 3221132252, 2329 Jul 2023. [39] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data In Advances in Neural Information Processing Systems, volume 32. Curran distribution. Associates, Inc., 2019. [40] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv:2411.15098, 2024. [41] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. [42] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multisubject zero-shot image personalization with layout guidance. In ICLR, 2025. [43] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In ICCV, 2023. [44] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [45] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2242822437, June 2023. 12 [46] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv:2308.06721, 2023. [47] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. [48] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In CVPR, 2024. [49] Min Zhao, Fan Bao, Chongxuan LI, and Jun Zhu. Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations. In Advances in Neural Information Processing Systems, volume 35, pages 36093623. Curran Associates, Inc., 2022."
        }
    ],
    "affiliations": [
        "Bytedance Intelligent Creation"
    ]
}