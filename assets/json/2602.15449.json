{
    "paper_title": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models",
    "authors": [
        "Chansung Park",
        "Juyong Jiang",
        "Fan Wang",
        "Sayak Paul",
        "Jiasi Shen",
        "Jing Tang",
        "Jianguo Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT."
        },
        {
            "title": "Start",
            "content": "TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models Chansung Park1* Juyong Jiang2 3* Fan Wang2* Sayak Paul4 Jiasi Shen3 Jing Tang2 3 1Electronics and Telecommunications Research Institute 2The Hong Kong University of Science and Technology (Guangzhou) 3The Hong Kong University of Science and Technology 4Hugging Face 5Ant Group {deep.diver.csp,csjuyongjiang,csfanwang,spsayakpaul}@gmail.com sjs@cse.ust.hk, jingtang@ust.hk, lijg.zero@antgroup.com Jianguo Li5 6 2 0 2 7 1 ] . [ 1 9 4 4 5 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Testdriven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, four-tier test suite (basic, intermediate, complex, edge), providing controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to models inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under hard-first curriculum. TAROT provides reproducible method that adaptively tailors curriculum design to models capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT. *Equal contributors: Juyong Jiang, Chansung Park, and Fan Wang. Listing order is random. Corresponding authors. 1 Large Language Models (LLMs) are driving significant changes in software engineering, with automated code generation emerging as pivotal application (Du et al., 2024; Jiang et al., 2024; Xu and Zhu, 2022; Park et al., 2025; Wang et al., 2024). Foundational models exhibit strong capacity to translate natural language specifications into functional code, promising significant enhancements in developer productivity (Weber et al., 2024; Li et al., 2025). Nevertheless, generating solutions that are not only correct but also algorithmically sophisticated and robust remains an open challenge (Zhuo et al., 2025b,a). Advancing this frontier calls for enhancing the reasoning and problem-solving capacities of these models. Curriculum Learning (CL), which structures training data by difficulty (Bengio et al., 2009), offers promising pathway for improving these capabilities and training efficiency (Zhang et al., 2025; Naïr et al., 2024). In the context of code generation, however, existing CL applications predominantly operate at the inter-problem level, sequencing tasks by coarse difficulty metrics (Naïr et al., 2024; Khant et al., 2025). This curriculum neglects the nuanced, intra-problem difficulty gradient inherent in software verification. In practice, developers often employ incremental methodologies like Test-Driven Development (TDD) (Beck, 2003), refining solution against progressively more challenging test cases to ensure its robustness. Yet, this natural axis for curriculum design remains largely untapped in LLM training. Furthermore, reliance on problem-level sequencing often leads to flat reward landscapes when integrated with Reinforcement Fine-tuning (RFT), dampening the learning signal. This oversight of heterogeneous test-case difficulty results in imbalanced reward signals and consequently biased gradient updates during training, hindering the models ability to acquire robust, sophisticated reasoning skills. Recent work has begun to move toward more dynamic curriculum learning paradigms for LLMs, where task difficulty is progressively increased during training (Xu et al., 2024; Cheng et al., 2025). However, these methods predominantly define difficulty based on the intrinsic properties of the data or task structure. For instance, curricula are often structured using automated metrics like cyclomatic complexity (Naïr et al., 2024), or by decomposing problem into fixed sequence of simpler subtasks (Dou et al., 2024b). This prevailing focus on the data, rather than the learner, overlooks the crucial variable of the models own evolving and multi-faceted capability. curriculum tailored to an early-stage model may cause learning stagnation for more advanced one, while curriculum designed for experts can overwhelm less-capable model and hinder its convergence. more holistic and effective learning approach, therefore, should consider not just the intrinsic properties of the data, but also the evolving capabilities of the learner itself, leading to capability-adaptive framework. To address these limitations, we introduce TAROT, novel framework for Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning. TAROT decouples curriculum progression from raw rewards, enabling capabilityconditioned evaluation and principled selection from portfolio of curriculum policies rather than relying on the incidental composition of test-case difficulty. This design fosters stable optimization and efficient competency acquisition. Our proposed frameworks core novelty is twofold. First, TAROT formalizes an intra-problem difficulty gradient through test-driven curriculum. We construct the TAROT dataset, where each programming problem is augmented with tiered test suite comprising basic, intermediate, complex, and edge cases. This structure defines difficulty as spectrum of functional correctness and allows curriculum progression through differential emphasis on test tiers during training. This engineered gradient provides structured and informative signal to mitigate the flat-reward issue observed in reinforcement learning (RL) for code generation. Second, TAROT enables capability-adaptive curriculum design. Leveraging the tiered test structure, TAROT instantiates portfolio of curriculum policies that differ in tier allocation, sequencing, and reward weighting. This setup supports capabilityconditioned evaluation and principled curriculum selection for models with varying effective capabilities, influenced by model scale and specialization. Extensive evaluations on state-of-the-art LLMs across coding benchmarks demonstrate that TAROT consistently outperforms the baselines, efficiently enhancing both functional correctness and robustness of the synthesized code. Our empirical investigation further reveals that the optimal curriculum is capability-dependent for RFT in code generation. Specifically, less-capable models benefit from basic-to-complex progression, whereas more-capable models learn more effectively from complex tiers. Our main contributions are as follows: We propose capability-adaptive curriculum framework, TAROT, which addresses the imbalanced reward issue in RFT for code generation. It integrates an intra-problem, test-driven dataset featuring four-tiered test suites with multi-dimensional curriculum portfolio. By providing granular difficulty landscape for capability-conditioned curriculum design and evaluation, TAROT enables stable and efficient optimization. We conduct extensive experiments on stateof-the-art LLMs across well-known coding benchmarks. Experimental results show that TAROT consistently improves model performance and training efficiency compared to strong baselines. Through empirical analysis, we uncover the capability-dependent optimal curriculum. These findings provide guidance for future work on test suite design and automated curriculum selection."
        },
        {
            "title": "2.1 Curriculum Learning for Code",
            "content": "Curriculum Learning (CL) is training strategy inspired by human cognition that presents data to model in structured order, typically from simple to complex examples (Bengio et al., 2009; Wang et al., 2021). This method has been shown to accelerate convergence and improve generalization by guiding the optimization process toward better solutions (Ryu et al., 2025; Xi et al., 2024). In the context of LLMs, curricula have been implemented in various ways, such as using teacher model to progressively generate more complex instructions, as 2 exemplified by the Evol-Instruct method (Xu et al., 2024), or by fine-tuning on small set of meticulously curated, high-quality examples as demonstrated by LIMA (Zhou et al., 2023). For code generation, where task complexity varies, CL is promising but challenging area. While many code datasets rely on manual difficulty labels, recent research has focused on more systematic approaches. notable example is the use of automatic difficulty metrics, combining measures like cyclomatic complexity and Halstead difficulty, to sort problems into multi-stage curriculum (Naïr et al., 2024). Training with this structured approach yielded significant gains, demonstrating the value of CL in the code domain. Other methods, like StepCoder, create an implicit curriculum by breaking complex problem into sequence of simpler code-completion subtasks (Dou et al., 2024b). These efforts show clear trend towards leveraging curricula to organize the training process for code generation. Unlike these methods, TAROT introduces tiered test suite to construct capabilityadaptive curriculum for RL."
        },
        {
            "title": "2.2 Reinforcement Learning for Code LLMs",
            "content": "Reinforcement Learning (RL) is widely used for aligning LLMs with desired behaviors, including RLHF (Ouyang et al., 2022), DPO (Rafailov et al., 2023), PPO (Schulman et al., 2017), GRPO (Shao et al., 2024), and GSPO (Zheng et al., 2025). For code generation, RL typically optimizes functional correctness using unit test outcomes as rewards. Despite its effectiveness, this paradigm suffers from two key limitations: reward sparsity and reward flatness. Reward sparsity stems from the lack of informative feedback when model fails task entirely, whereas reward flatness occurs when the signal fails to differentiate among levels of problem difficulty (Parashar et al., 2025). These can lead to imbalanced gradients and suboptimal learning dynamics. Recent work addresses these issues from different angles. Process Reward Models alleviate reward sparsity by providing dense, line-level feedback, guiding the model even when the final code is incorrect (Dai et al., 2025). Another line of works combine RL with curriculum design to address these issues. For instance, StepCoder decomposes long tasks into curriculum of easier subtasks and trains the model step-by-step (Dou et al., 2024a), while Self-Evolving Curriculum adaptively adjusts curriculum selection, formulated as multi-armed bandit problem, according to the models evolving capability (Chen et al., 2025). Our TAROT framework addresses reward flatness by making the reward signal curriculum-aware. Instead of treating all successes equally, we modulate the reward based on the difficulty of the solved test tier. By integrating this tiered scheme directly into stable policy optimization algorithm, TAROT provides more nuanced learning gradient that encourages the model to master harder problems. Integrating structured curriculum directly into the RL reward mechanism is novel contribution that complements other recent innovations in the field."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we elaborate on the proposed TAROT framework, which enhances the code generation capability by training LLMs on test cases of varying difficulty levels, where curriculum progression and reward weights are adaptively conditioned on the models baseline capability."
        },
        {
            "title": "3.1 Dataset",
            "content": "A coding problem is formally defined as tuple consisting of problem statement S, reference solution R, and test suite : = (S, R, ) (1) In this structure, the problem statement outlines the task, the reference solution provides correct implementation, and the test suite serves for correctness verification without imposing tiered difficulty structure. From software engineering perspective, development is commonly test-driven. It begins with simple tests and progressively incorporates more complex and edge cases, with implementations refactored along the way to improve both correctness and design. This staged expansion of tests naturally aligns with the intuition behind CL. However, typical coding problem test suites are not crafted with this incremental pedagogy in mind. They are primarily designed for summative verification, with highly variable size and difficulty, offering little support for CL. To address this gap, we introduce the TAROT dataset DTAROT, constructed according to the procedure depicted in Figure 1 (a). Each problem is augmented with tiered test suite spanning four predefined difficulty levels = {B, I, C, E} (basic, intermediate, complex, and edge), without modify3 Figure 1: Overview of TAROT framework. (top) Build four-tier test suite (basic/intermediate/complex/edge) per problem using frontier LLMs and verify them against the reference solution. (bottom) Reinforcement fine-tuning under capability-conditioned, reward-decoupled curriculum. Less capable models perform best with basic complex, whereas more capable models perform best with complex basic. ing the original statement or the reference solution: DTAROT = (cid:8) (cid:0)Si, Ri, { Ti,l }lL = {B, I, C, E}, (cid:1) (cid:9)N i=1, Ti = (cid:91) lL Ti,l, (2) (3) (4) s.t. [N ], L, Ti,l : Pass(Ri, t). (5) The full suite for each problem is the union of its per-level subsets (Equation (4)) and every test case is validated against the reference solution to ensure data quality (Equation (5)). Any curriculum order (e.g., basiccomplex) is imposed only during training and is not part of the dataset definition."
        },
        {
            "title": "3.2 Training Mechanism",
            "content": "The training mechanism of TAROT decouples the curriculum from raw test scores. It achieves this by utilizing two pre-defined components: (1) curriculum allocator that defines fixed proportion of training focus for each difficulty tier L, and (2) tailored reward weights that prioritize tiers by placing greater value where the learning signal is most beneficial. During the training loop, the model generates candidate solutions for given problem, which are evaluated against the tiered test cases. The resulting pass/fail outcomes are used to calculate and accumulate tier-weighted return. Both the curriculum allocation and reward weights are specified prior to training based on the models effective capability, composite of instruction following fidelity and baseline coding proficiency. By conditioning these design choices on capability, TAROT concentrates the training signal on model-specific zone of optimal difficulty, resulting in fixed yet highly customized training schedule. Specifically, models with lower baseline capability receive larger share of basic and intermediate cases, whereas stronger models focus on complex and edge cases to extend their performance frontier. The reward weights follow the same principle, ensuring that successes on capability-appropriate tiers contribute more to the final objective. Formally, in the RL setting, for each problem Pi and difficulty level L, we define the tier-level success of policy π as the average pass rate over the corresponding test set: ri,l(π) = 1 Ti,l (cid:88) tTi,l 1{Pass(π, t)}, (6) where Pass(π, t) indicates that the solution produced by π satisfies test case t. Given curriculum allocation α = (αl)lL and reward weights = (wl)lL, the TAROT return 4 for Pi is defined as:"
        },
        {
            "title": "RTAROT",
            "content": "(cid:0)Pi, π; α, w(cid:1) = (cid:88) lL αl wl ri,l(π), (cid:88) lL αl = 1, wl 0. (7) Here, αl is the curriculum allocation that specifies the share of training updates to tier l, and wl is the reward weight that controls the contribution of tier ls success to the overall return, reflecting its relevance given the models baseline capability. Based on this tier-aggregated return, training is formulated as the maximization of the expected TAROT return over problems: (cid:104)"
        },
        {
            "title": "RTAROT",
            "content": "JTAROT(θ) = EPiDTAROT (cid:0)Pi, πθ; α, w(cid:1) (cid:105) . (8) By decoupling training effort allocation α from success valuation w, TAROT enables capabilityadaptive curriculum that concentrates optimization on the most informative difficulty tiers for given model, rather than enforcing fixed, modelagnostic learning path."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "We construct the TAROT dataset based on 15k Python coding interview problems1 with validated basic/intermediate/complex/edge test suites. As illustrated in Table 1, we design curriculum policies along two axes: allocation order and reward weighting. For allocation, we explore Forward (basicedge), Reversed (edgebasic), and Static schedules. Transitions for staged curricula occur at 0.2, 0.4, and 0.6 of the total epoch. For weighting, we define three templates: Uniform (0.25 for all tiers), B/I Weighted (emphasizing the basic and intermediate tiers), and C/Edge Weighted (emphasizing the complex and edge tiers). We first validate the empirical integrity of the TAROT datasets tiering by analyzing its structure using quantitative and qualitative metrics. Subsequently, we evaluate TAROT training mechanism including Qwen2.5-Instruct, on diverse LLMs, Qwen2.5-Coder-Instruct (1.5B, 3B, 7B) (Qwen et al., 2025; Hui et al., 2024), Gemma-2-IT (2B, 9B) (Team et al., 2024), and Qwen3-4BInstruct-2507 (Yang et al., 2025) on well-known 1https://huggingface.co/datasets/open-r1/ verifiable-coding-problems-python 5 benchmarks including HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), HumanEval+, MBPP+ (Liu et al., 2024), LiveCodeBench v5 (Jain et al., 2024), CodeForces (Penedo et al., 2025), and CruxEval (Gu et al., 2024). This selection allows us to assess the frameworks effectiveness across wide spectrum of model scales, architectures, coding specializations, and performance tiers. All models are fine-tuned using GRPO (Shao et al., 2024). Full implementation details are presented in Appendix B."
        },
        {
            "title": "4.2 Main Results",
            "content": "The structure analysis of the TAROT dataset is illustrated in Figure 2. The three KDE plots demonstrate clear progression: as the tiers advance from basic to complex, the distributions for input length, token diversity, and character transitions all exhibit consistent rightward shift, signifying systematic increase in structural complexity. Furthermore, the qualitative bar chart reveals functional separation between the two hardest tiers: test cases designed to probe complexity peak in the complex tier, while those targeting boundary checks are predominantly concentrated in the edge tier. These results demonstrate that our four-level taxonomy not only stratifies overall difficulty but also effectively separates different types of challenge, establishing robust foundation for subsequent experiments. TAROTs training performances across various models on coding benchmarks are shown in Figure 3. Our approach consistently improves the pass@1 score over the base checkpoints across all benchmarks and model sizes, indicating its robustness and efficiency. These results reveal nuanced relationship between model scale, specialization, and optimal curriculum design. The model scale influences the optimal curriculum design, which is exemplified by the trend shown in Qwen2.5-Instruct models. The largest model (7B) performs best with complexfocused strategies, while the smallest (1.5B) benefits from conventional basic-focused approach. Model specialization further modulates the effective curriculum preference. As illustrated by the Qwen2.5-Coder models, the mid-scale Qwen2.5Coder-3B model displays learning preference akin to the much larger Instruct-7B model despite its smaller scale. It achieves its peak HumanEval score using the same complex-focused strategy and outperforms its general-purpose 3B counterpart. This suggests that models prior specialization Figure 2: Quantitative and qualitative validation of the TAROT dataset. The KDE plots show the distribution of structural complexity, where the x-axis represents the metrics magnitude. Token Diversity (unique/total tokens) and Transitions (character class changes) serve as proxies for lexical and syntactic complexity, respectively. The systematic rightward shift confirms increasing difficulty across tiers. GPT-4o validation on the right confirms that complex tiers target algorithmic complexity, while edge tiers focus on boundary conditions. Table 1: Overview of the experimental schedules for curriculum learning. Each strategy varies in reward distribution and the sequence of difficulties presented to the model. The abbreviations B, I, C, and correspond to basic, intermediate, complex, and edge difficulty tiers, respectively. For staged curricula, transitions occur at 0.2, 0.4, and 0.6 of the total epoch."
        },
        {
            "title": "Strategy",
            "content": "Reward Weights (B, I, C, E) Curriculum Schedule Progression Forward (Uniform) (0.25, 0.25, 0.25, 0.25) (B,I) (B,I,C) All Forward (B & Weighted) (0.35, 0.35, 0.15, 0.15) (B,I) (B,I,C) All Forward (C & Weighted) (0.15, 0.15, 0.35, 0.35) (B,I) (B,I,C) All Reversed (C & Weighted) (0.15, 0.15, 0.35, 0.35) (C,E) (C,E,I) All"
        },
        {
            "title": "Edge Only",
            "content": "(1.0, 0.0, 0.0, 0.0) (0.0, 0.0, 1.0, 0.0) (0.0, 0.0, 0.0, 1.0)"
        },
        {
            "title": "Static",
            "content": "enhances its effective capability, making it more critical determinant of the ideal learning path than parameter count alone. We further evaluate TAROT on the more recent Qwen3-4B-Instruct-2507 to generalize these findings. As detailed in Table 2, this model, fine-tuned with the optimal curriculum strategy C/E Weighted, consistently outperforms the base model across all benchmarks, yielding gains ranging from +2.12 to +4.26 percentage points. Notably, these improvements are achieved on an already strong performance baseline, indicating that curriculum learning is effective even for eliciting further gains from highly capable models. In addition, these results align with our prior observations on the Qwen2.5 models. They suggest that preference for curricula is primarily driven by models effective capability rather than parameter count alone, and that stronger models benefit most from training regimes that prioritize challenging examples. We attribute this divergence to the Zone of Optimal Difficulty. For more-capable models, the basic tier is too trivial to provide informative learning signal, whereas the complex tier supplies the necessary high-entropy signal needed for improvement. Conversely, less-capable models facing complex tiers initially suffer from sparse rewards, leading to training collapse. Therefore, effective curriculum design must be calibrated to model capability: basic curricula for less-capable models to ensure training stability, while complex curricula for morecapable or code-specialized models to maximize gradient efficiency. 4.3 In-depth Analysis Out-Of-Distribution Benchmarks To assess the robustness of TAROT beyond the training distribution, we evaluate it on OOD benchmarks including CodeForces, LiveCodeBench v5, and CruxEval. Results are presented in Figure 4. We observe that TAROT consistently outperforms baselines; however, the optimal curriculum strategy is highly task-dependent rather than universal. For instance, Qwen2.5-7B achieved peak performance 6 Figure 3: Experimental results for Qwen2.5-Instruct and Qwen2.5-Coder-Instruct on HumanEval, HumanEval+, MBPP, and MBPP+. Scores are pass@1. Numbers above bars indicate gains in percentage points relative to each models base checkpoint. Labels inside bars indicate the best performing curriculum strategy. Table 2: Performance comparison of curriculum strategies for Qwen3-4B-Instruct-2507. The best-performing C/E Weighted strategy is compared against the base model, with improvements shown in percentage points."
        },
        {
            "title": "HumanEval",
            "content": "HumanEval+"
        },
        {
            "title": "MBPP",
            "content": "MBPP+ Base C/E Weighted 89.02% 91.46% (+2.44pp) 78.66% 82.92% (+4.26pp) 52.60% 55.20% (+2.60pp) 56.61% 58.73% (+2.12pp) with the Basic Only curriculum on LiveCodeBench v5, whereas the C/E Weighted strategy proved most effective for CruxEval and CodeForces. This divergence stems from varying skill alignments between coding interview-style training data and downstream tasks. Consequently, effective curriculum selection must account for the target domains computational structure, necessitating future research into task-specific intra-problem test design and adaptive policy selection. Comparison with Standard Reward Schemes To verify the effectiveness of TAROTs capabilityadaptive curriculum strategy, we compare our framework against standard reward strategies commonly used in RL for code generation. These baselines utilize the full four-tier test suite throughout training but do not incorporate curriculum scheduling. Specifically, we consider two standard RL baselines: Avg-reward, which computes rewards as the average pass rate across the four tiers (R [0, 1]), and Pass@All, strict functional correctness setting that assigns reward of 1 only when all four tiers pass and 0 otherwise (R {0, 1}). Results presented in Table 3 show that TAROT consistently outperforms the standard reward schemes across all benchmarks, indicating that its performance gains arise from its capabilityadaptive curriculum strategy. Architectural Generalization We apply TAROT to the Gemma2 models, with results summarized in Table 4. Consistent with our findings on Qwen models, the optimal curriculum is governed by models effective capability rather than its parameter count alone. For the larger Gemma2-9B-IT, Complex Only curriculum offered no decisive advantage, while simpler strategies like Basic Only often perform better on key benchmarks. This principle is even more pronounced for the smaller Gemma2-2B-IT. As shown in Appendix F, most curricula are actively harmful, leading to performance collapse from sparse reward signal. In contrast, Basic Only strategy focuses on fundamentals yields substantial improvements. Together, these results indicate that for less-capable models, fundamentals-first curriculum is essential for successful fine-tuning, whereas unstructured approaches can be severely detrimental. For completeness, we report the full per-strategy and per-curriculum results for Qwen2.5-Instruct, Qwen2.5-Coder-Instruct, and Qwen3-4B-Instruct models across HumanEval, MBPP, and the OOD benchmarks in Appendix G, Table 7 and 8. We furFigure 4: Experimental results for Qwen2.5-Instruct and Qwen2.5-Coder-Instruct models on CodeForces, LiveCodeBench v5 (LCBv5), and CruxEval. Scores are the overall accuracy across easy, medium, and hard problems. Numbers above bars indicate gains in percentage points relative to each models base checkpoint. Labels inside bars indicate the best performing curriculum strategy. Table 3: Performance comparison between TAROT and standard RL Baselines. TAROT outperforms conventional reward schemes that use the same test cases but lack curriculum scheduling. Highest scores are highlighted in bold."
        },
        {
            "title": "Strategy",
            "content": "HumanEval HumanEval+ MBPP MBPP+ CodeForces LCBv"
        },
        {
            "title": "CruxEval",
            "content": "Qwen2.5-1.5B-Instruct Qwen2.5-7B-Instruct Avg-reward Pass@All TAROT (Best) Avg-reward Pass@All TAROT (Best) 59.15% 60.98% 60.98% 83.75% 81.10% 84.15% 54.27% 56.10% 55.49% 76.22% 73.78% 77.44% 49.20% 57.93% 44.60% 53.43% 51.80% 58.20% 66.00% 69.84% 63.00% 68.52% 69.00% 70.63% 2.72% 2.72% 4.49% 3.70% 38.75/32.87% 3.94% 34.62/31.75% 5.26% 40.20/33.75% 11.95% 56.37/58.50% 11.41% 9.49% 14.81% 55.62/56.63% 12.95% 19.12% 57.88/59.38% Table 4: Performance comparison for Gemma2-9B-IT across key curriculum strategies. Highest scores on each benchmark are highlighted in bold."
        },
        {
            "title": "Strategy",
            "content": "HumanEval HumanEval+ MBPP MBPP+ CodeForces LCBv"
        },
        {
            "title": "Base\nUniform\nBasic Only\nComplex Only",
            "content": "60.37% 65.85% 63.41% 65.85% 54.88% 57.93% 56.10% 60.37% 59.60% 65.08% 59.20% 64.55% 60.40% 65.08% 58.60% 64.55% 8.61% 10.82% 9.49% 9.93% 11.83% 45.63/47.63% 13.62% 51.63/47.13% 14.70% 51.00/48.00% 12.54% 48.25/48.00% ther analyze the hyperparameter sensitivity (temperature, GRPO β) and the inference-time maximum token limit in Appendix and E, and investigate the training dynamics and reward correlations in Appendix H."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced TAROT, test-driven and capabilityadaptive framework for curriculum RFT in code generation. It moves beyond the conventional onesize-fits-all approach by constructing four-tier, intra-problem test suite that allows curriculum design to be tailored to models effective capability. Extensive experiments validate TAROTs effectiveness in consistently improving model performance in code generation over strong baselines. In addition, we find that the optimal curriculum depends on models effective capability rather than size alone. Specifically, we show that less-capable models benefit most from basic-focused progression, while more-capable models excel with curricula that prioritize complex-focused challenges. These results and findings lay the groundwork for future research into automated and task-specific curriculum policies in code generation."
        },
        {
            "title": "Limitations",
            "content": "A primary limitation of our framework lies in its dependence on the TAROT dataset where the fourtier test suite is synthetically generated by frontier LLMs. Despite the rigorous verification process, potential biases or latent coverage gaps in the generator could propagate to the policy model and constrain the diversity of the learning signal. Additionally, the current study is restricted to Python coding tasks, and the generalization of this tiered reinforcement fine-tuning approach to multilingual or low-resource programming languages remains to be verified. Finally, while our capability-adaptive mechanism selects curriculum policies from predefined portfolio based on static baseline assessments, we leave the exploration of continuous curriculum spaces for dynamic schedule optimization during training to future research."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models. Preprint, arXiv:2108.07732. K. Beck. 2003. Test-driven Development: By Example. Addison-Wesley signature series. Addison-Wesley. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning - ICML 09, pages 18, Montreal, Quebec, Canada. ACM Press. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating large language models trained on code. Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, and Ehsan Kamalloo. 2025. Selfevolving curriculum for llm reasoning. Preprint, arXiv:2505.14970. Yang Cheng, Zilai Wang, Weiyu Ma, Wenhui Zhu, Yue Deng, and Jian Zhao. 2025. Evocurr: Self-evolving curriculum with behavior code generation for complex decision-making. Preprint, arXiv:2508.09586. Ning Dai, Zheng Wu, Renjie Zheng, Ziyun Wei, Wenlei Shi, Xing Jin, Guanlin Liu, Chen Dun, Liang Huang, and Lin Yan. 2025. Process supervision-guided policy optimization for code generation. Preprint, arXiv:2410.17621. Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Wei Shen, Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan, and 1 others. 2024a. Stepcoder: Improve code generation with reinforcement learning from compiler feedback. arXiv preprint arXiv:2402.01391. Shihan Dou, Yan Liu, Haoxiang Jia, Enyu Zhou, Limao Xiong, Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan, Zhiheng Xi, Yuhao Zhou, Tao Ji, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024b. StepCoder: Improving code generation with reinforcement learning from compiler feedback. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 45714585, Bangkok, Thailand. Association for Computational Linguistics. Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. 2024. Evaluating large language models in class-level code generation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE 24, New York, NY, USA. Association for Computing Machinery. Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang. 2024. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065. Hugging Face. 2025. Open r1: fully open reproduction of deepseek-r1. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, and 5 others. 2024. Qwen2.5-coder technical report. Preprint, arXiv:2409.12186. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. Preprint, arXiv:2403.07974. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024. survey on large language models for code generation. ACM Transactions on Software Engineering and Methodology. Kyi Shin Khant, Hong Yi Lin, and Patanamon Thongtanunam. 2025. Should code models learn pedagogically? preliminary evaluation of curriculum learning for real-world software engineering tasks. In 2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR), pages 249254. 9 Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimization: your language model is secretly reward model. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. Shangyu Li, Juyong Jiang, Tiancheng Zhao, and Jiasi Shen. 2025. Osvbench: Benchmarking llms on specification generation tasks for operating system verification. arXiv preprint arXiv:2504.20964. Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. 2024. Evaluating language models for efficient code generation. In First Conference on Language Modeling. Marwa Naïr, Kamel Yamani, Lynda Lhadj, and Riyadh Baghdadi. 2024. Curriculum learning for small code language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pages 390401, Bangkok, Thailand. Association for Computational Linguistics. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, and 1 others. 2025. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning. arXiv preprint arXiv:2506.06632. Chansung Park, Juyong Jiang, Fan Wang, Sayak Paul, and Jing Tang. 2025. Llamaduo: Llmops pipeline for seamless migration from service llms to small-scale local llms. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3319433215. Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. 2025. Codeforces. https://huggingface. co/datasets/open-r1/codeforces. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Negin Raoof, Etash Kumar Guha, Ryan Marten, Jean Mercat, Eric Frankel, Sedrick Keh, Hritik Bansal, Georgios Smyrnis, Marianna Nezhurina, Trung Vu, Zayne Rea Sprague, Mike Merrill, Liangyu Chen, Caroline Choi, Zaid Khan, Sachin Grover, Benjamin Feuer, Ashima Suvarna, Shiye Su, and 27 others. 2025. Evalchemy. Kanghyun Ryu, Qiayuan Liao, Zhongyu Li, Payam Delgosha, Koushil Sreenath, and Negar Mehr. 2025. Curricullm: Automatic task curricula design for learning complex robot skills using large language models. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 44704477. IEEE. John Schulman, Filip Wolski, Prafulla Dhariwal, ProxiAlec Radford, and Oleg Klimov. 2017. mal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, and 179 others. 2024. Gemma 2: Improving open language models at practical size. Preprint, arXiv:2408.00118. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. 2020. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl. Fan Wang, Juyong Jiang, Chansung Park, Sunghun Kim, and Jing Tang. 2024. Kasa: Knowledge-aware singular-value adaptation of large language models. arXiv preprint arXiv:2412.06071. Xin Wang, Yudong Chen, and Wenwu Zhu. 2021. IEEE transacA survey on curriculum learning. tions on pattern analysis and machine intelligence, 44(9):45554576. Thomas Weber, Maximilian Brandmaier, Albrecht Schmidt, and Sven Mayer. 2024. Significant productivity gains through programming with large language models. Proc. ACM Hum.-Comput. Interact., 8(EICS). 10 others. 2025a. Bigcodearena: Unveiling more reliable human preferences in code generation via execution. arXiv preprint arXiv:2510.08697. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, and 14 others. 2025b. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. Preprint, arXiv:2406.15877. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, and 1 others. 2024. Training large language models for reasoning through reverse curriculum reinforcement learning. arXiv preprint arXiv:2402.05808. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations. Yichen Xu and Yanqiao Zhu. 2022. survey on pretrained language models for neural code intelligence. arXiv preprint arXiv:2212.10079. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Enci Zhang, Xingang Yan, Wei Lin, Tianxiang Zhang, and Lu Qianchun. 2025. Learning like humans: Advancing llm reasoning capabilities via adaptive difficulty curriculum learning and expert-guided selfreformulation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 66306644. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. 2025. Group sequence policy optimization. Preprint, arXiv:2507.18071. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: less is more for alignment. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. Terry Yue Zhuo, Xiaolong Jin, Hange Liu, Juyong Jiang, Tianyang Liu, Chen Gong, Bhupesh Bishnoi, Vaisakhi Mishra, Marek Suppa, Noah Ziems, and"
        },
        {
            "title": "A Test Case Generation Prompts",
            "content": "To ensure the consistent generation of high-quality, four-tiered test cases, we designed detailed prompt template. This template, listed in Table 5, guides the language model to act as an expert software engineer and produce test cases that adhere to our specific difficulty criteria."
        },
        {
            "title": "B Implementation Details",
            "content": "TAROT Dataset Our experiments utilize the TAROT dataset, which is constructed by augmenting approximately 15k Python problems from the verifiable-coding-problems-python dataset2. For each problem, we employ OpenAIs most powerful o3 and o4 models3 with the highest reasoning effort to generate four-tiered test suite spanning basic, intermediate, complex, and edge cases. The prompts used for the generation are detailed in Table 5. To ensure high quality, every generated test case is validated using the reference solution, and any problem with even one failing tier is discarded. This rigorous curation process yields final dataset consisting of 60k tiered test suites (15k problems 4 tiers). Samples of the generated tiered test cases can be found in Appendix J. Model Selection To validate our proposed framework, we select diverse models to investigate four key research questions: (1) the effect of model scale, to test whether the optimal curriculum is capability-dependent, using three Qwen2.5 models (1.5B, 3B, 7B) (Qwen et al., 2025); (2) the impact of specialization, to determine if TAROT can further enhance models already proficient in coding, using their code-specialized counterparts (Hui et al., 2024); (3) architectural generalizability, to test if our findings apply beyond single model family, by incorporating two instruction-tuned Gemma2 models (2B, 9B) (Team et al., 2024); and (4) pushing performance frontiers, to assess if our framework can improve even state-of-the-art models with strong baselines, by fine-tuning the recent Qwen3-4B-Instruct-2507 (Yang et al., 2025). For all models, we use their instruction-tuned variants to ensure foundational code-generation capability, prerequisite for effective RL-based fine-tuning. Training Details We fine-tune all selected models for single epoch using the TAROT framework. 2https://huggingface.co/datasets/open-r1/ verifiable-coding-problems-python 3https://platform.openai.com/docs/models For policy optimization, we employ GRPO (Shao et al., 2024). All models are trained using the AdamW optimizer with constant learning rate of 1 106. We set the global batch size to 8, reducing it to 4 for larger models (Qwen2.5-7B-Instruct, Qwen2.5-Coder-7B-Instruct, and Gemma2-9B-IT) to accommodate memory constraints. The maximum input and completion token lengths are set to 1,024 and 4,096, respectively. For GRPO-specific settings, we generate 8 candidate completions per prompt to estimate the policy advantage, with the core hyperparameter β set to 0.01 in our main experiments. The GRPO hyperparameter β controls the strength of the Kullback-Leibler (KL) divergence regularization term, which penalizes the policy for deviating too far from the original base models behavior. The training temperature, in turn, manages the exploration-exploitation tradeoff; higher values encourage the model to sample wider variety of solutions (exploration), while lower values cause it to refine high-probability ones (exploitation). To identify the optimal settings for the crucial hyperparameters, we provide an ablation study on the GRPO β value (0.1, 0.05, 0.01) and the sampling temperature (1.0, 0.7, 0.5) in Appendix D. All fine-tuning experiments are conducted on server with 8 NVIDIA A100 (80 GB) GPUs, running CUDA 12.4 and PyTorch 2.6. Our implementation is based on open-source libraries including Transformers (Wolf et al., 2020), TRL (von Werra et al., 2020), vLLM (Kwon et al., 2023), and Open-R1 (Hugging Face, 2025). Evaluation Metrics We evaluate the efficacy of TAROT on diverse code generation benchmarks. For functional correctness, we measure the pass@1 metric on HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), HumanEval+, and MBPP+ (Liu et al., 2024). To assess competitive problem-solving skills, we use the overall accuracy on LiveCodeBench v5 (Jain et al., 2024) and CodeForces (Penedo et al., 2025), averaged across their difficulty tiers. Finally, the models code reasoning capability is evaluated using the input and output prediction accuracy on CruxEval (Gu et al., 2024). The detailed generation parameters and execution environment are described in Appendix C."
        },
        {
            "title": "Environment",
            "content": "The entire evaluation pipeline is managed by the EvalChemy framework (Raoof et al., 2025). We 12 follow the benchmark-specific generation configurations predefined within the framework, such as temperature, top-p, prompt formatting, and stopping criteria, to ensure consistency with established evaluation protocols. By default, the maximum completion tokens for each benchmark adhered to its standard setting; however, for an ablation study on generation length (Appendix E), we systematically increase this limit to 4,096, 8,192, and 16,384 tokens to observe performance trends. All code generation for evaluation is conducted by serving the fine-tuned models via the vLLM framework (Kwon et al., 2023) on servers equipped with 4 NVIDIA A100 (80 GB) GPUs, using batch size of 64. The resulting code is executed in secure, sandboxed Python 3.11 environment, where strict 10-second timeout is enforced for each test case to prevent infinite loops and manage evaluation time."
        },
        {
            "title": "D Hyperparameter Sensitivity Analysis",
            "content": "This section provides ablation studies on two key training hyperparameters to analyze their impact on final benchmark performance: the GRPO regularization coefficient β and the sampling temperature during training. Impact of GRPOs β The hyperparameter β in GRPO controls the strength of the Kullback-Leibler (KL) divergence regularization, which prevents the fine-tuned policy from deviating excessively from the original base model. The results of varying β are shown in Figure 5. Performance sensitivity to β is not uniform across benchmarks. For functionsynthesis tasks like HumanEval and HumanEval+, small β of 0.01, which allows for greater policy exploration, yields the best results. Conversely, benchmarks like MBPP and CodeForces appear to benefit from slightly stronger regularization (β = 0.05). This variance suggests that the optimal regularization strength is task-dependent. We selected β = 0.01 for our main experiments as it proved most effective on our primary evaluation benchmarks. Impact of Training Temperature The sampling temperature manages the exploration-exploitation trade-off during training. The results, presented in Figure 6, indicate that higher temperature of 1.0, which encourages greater exploration of diverse solutions, is optimal for HumanEval and HumanEval+. However, other benchmarks show different trends; MBPP, for example, peaks at more conservative temperature of 0.7. This highlights that the optimal degree of exploration is also taskspecific, and suggests that task-adaptive temperature scheduling could be potential area for future work."
        },
        {
            "title": "Tokens at Inference Time",
            "content": "We analyzed the impact of the maximum completion token limit during inference on the fine-tuned Qwen3-4B model, with results presented in Figure 7. The findings reveal clear, benchmarkdependent dichotomy. On function-completion tasks like HumanEval and HumanEval+, performance generally degrades as the token limit increases beyond 4,096. In stark contrast, benchmarks like MBPP and MBPP+ benefit from larger generation space, with optimal results often found at 8,192 or 16,384 tokens. This divergence suggests that for tasks requiring concise solutions, such as those in HumanEval, larger token limit may encourage verbose and error-prone code. Conversely, the nature of MBPP problems may necessitate longer generation process to fully develop the correct logic. This analysis underscores critical point for standardized evaluation: the ideal setting for maximum completion tokens is highly contingent on the characteristics of the target benchmark. Additional Results on Gemma2-2B-IT This appendix provides the full curriculum comparison for Gemma2-2B-IT as in Table 6. Unlike larger or stronger models, Gemma2-2B-IT exhibits curriculum fragility: most curricula depress performance, consistent with the observation in the main text that sparse reward signals can cause collapse for less-capable models. In contrast, Basic Onlya fundamentals-first scheduleyields the most reliable gains among the tested strategies. These results reinforce our capability-dependent view of curriculum design: for weaker models, emphasizing simpler tiers is prerequisite for successful fine-tuning, whereas complex-focused or mixed curricula can be harmful. Full Benchmark Tables (Qwen2.5 & Qwen3-4B) We report the complete benchmark results for all curriculum strategies on Qwen2.5 family 13 Figure 5: Performance sensitivity to the GRPO hyperparameter β. The plots show the final pass@1 or accuracy scores on various benchmarks as β is varied. The optimal value is task-dependent; for instance, HumanEval and HumanEval+ benefit from smaller β (0.01) that allows greater policy exploration, whereas MBPP and CodeForces achieve peak performance with larger β (0.05) that enforces stronger regularization. Figure 6: Performance sensitivity to the sampling temperature during training. The plots illustrate the final benchmark scores for different training temperatures. higher temperature of 1.0, which encourages greater exploration, is optimal for benchmarks like HumanEval and HumanEval+. In contrast, other benchmarks such as MBPP show preference for more moderate temperature of 0.7, highlighting that the ideal exploration-exploitation balance is task-specific. (1.5B/3B/7B, including Coder variants) and Qwen3-4B-Instruct-2507. These tables expand the main figures by listing pass@1 on HumanEval, HumanEval+, MBPP, MBPP+, and average accuracy of CodeForces, LiveCodeBench v5, and CruxEval for every strategy in Table 8 and 7. Consistent with the main text, the C/E Weighted strategy tends to be the top performer for the morecapable Qwen3-4B model, improving over the base across all four code-function benchmarks. The full per-strategy breakdowns here allow exact comparison across OOD benchmarks as well."
        },
        {
            "title": "H Training Dynamics Analysis",
            "content": "The Limits of the Reward Signal. Figure 8 (a) shows that the training reward increases stably and is clearly separated by model capacity, indicating stable optimization process. Note that initial rewards are relatively low even for capable models; this is due to strict output formatting requirements and execution timeouts enforced by the sandbox, which the models quickly adapt to during the early stages of fine-tuning. This pattern suggests that the policy learns the training distribution well and that stronger models achieve higher reward levels under the same curriculum. However, the reward observed during training does not reliably anticipate downstream benchmark outcomes. As shown in Figure 8 (c), the final reward has only weak Pearson correlation coefficient with benchmark scores, which means that runs with similar rewards can still deliver very different levels of task performance. 14 Figure 7: Performance sensitivity to the maximum completion token limit at inference time for Qwen3-4B-Instruct2507 fine-tuned on various curriculum strategies. The results reveal clear, benchmark-dependent dichotomy. For function-completion tasks like HumanEval and HumanEval+, performance tends to degrade as the token limit increases beyond 4,096, suggesting that larger generation space may encourage verbose, error-prone solutions. Conversely, for benchmarks like MBPP and MBPP+, larger token limit is generally beneficial, indicating that their problem structures may require more extensive code to solve correctly. and intentional progression in difficulty and scope, which is cornerstone of our framework. The tiers are generally designed to validate different aspects of solution. Basic tiers focus on the core logic of problem with simple, straightforward inputs. Following this, intermediate and complex tiers introduce greater difficulty through larger inputs, more intricate scenarios, or patterns requiring more sophisticated algorithmic reasoning. Finally, edge tiers are designed to test for robustness by probing boundary conditions, constraints, and performance-intensive cases such as large numbers or long strings. This tiered structure exemplifies the intra-problem difficulty gradient that forms the basis of our capability-adaptive curriculum. Conciseness as Proxy for Advanced Reasoning. different perspective comes from analyzing completion length. Figure 8 (b) shows that models with greater capability tend to produce shorter solutions as training progresses, and this tendency becomes more pronounced for stronger configurations. Importantly, Figure 8 (d) indicates that mean completion length exhibits stronger negative correlation with benchmark scores than the reward does, implying that conciseness aligns better with final solution quality. Shorter programs are more likely to capture the essential reasoning steps without unnecessary detours, whereas longer outputs often reflect uncertainty or inefficient search. These observations support using solution conciseness as practical secondary proxy for advanced reasoning quality, complementing the reward based perspective and providing more informative early indicator of downstream performance."
        },
        {
            "title": "I The Use of AI Assistants",
            "content": "AI Assistants (e.g., LLMs) are employed solely for polishing writing."
        },
        {
            "title": "J Sample Tiered Test Cases",
            "content": "Table 918 present concrete examples of the fourtiered test cases generated for several problems in the TAROT dataset. These samples illustrate clear 15 Figure 8: Training dynamics vs. downstream performance. (a) and (b) show the reward and the mean completion length curves during reinforcement fine-tuning, and the annotations mark the curriculum strategy with the best average downstream performance. (c) and (d) show the Pearson correlation coefficient of the final rewards vs. benchmark scores and the mean completion length vs. benchmark scores, respectively. Light, semi-transparent lines represent alternative curriculum strategies, while the solid, annotated lines correspond to the best-performing strategy for each model. Some trajectories terminate earlier than others because different model sizes utilize varying batch sizes and gradient accumulation steps under fixed total compute budget. 16 Table 5: The prompt template used to generate tiered test suite given coding problem. The problem statement and the default test case from the original source are injected into {problem_statement} and {baseline_test_case} placeholders, respectively. You are an expert software engineer with extensive experience in designing comprehensive unit tests. Your task is to generate four distinct unit tests for given code implementation based solely on the provided problem statement. Treat this as black-box testing exercisefocus exclusively on the inputs and expected outputs without assuming any details about the internal implementation. Important: baseline test case will be provided separately. Each test case you generate must be more challenging than the baseline test case. Please generate four unit tests with the following guidelines: 1. Basic Complexity Test (label as \"basic\"): Use simple, straightforward inputs. Validate the core behavior under normal conditions. Focus on the happy path scenario. This should be the least challenging test case relative to the others. 2. Medium Complexity Test (label as \"intermediate\"): Include moderately complex inputs with some edge conditions. Test with mixed data types or larger inputs. Incorporate common edge cases and boundary values. Ensure this test is more challenging than the basic test. 3. High Complexity Test (label as \"complex\"): Use complex, nested, or structured inputs. Validate advanced functionality and complex logic paths. Stress test the implementation with challenging scenarios. This test should be more intricate than both the basic and intermediate tests. 4. Edge Case Test (label as \"edge\"): Use extreme boundary conditions and special cases. Validate behavior with empty, null, or invalid inputs. Focus on error handling and exception scenarios. This should be the most challenging test case among the four. For each test case, follow the JSON format provided in the example below (include only the input and expected output): { \" language \": \" python \", \" test_cases \": [ { } \" input \": \"4 n4 n0001 n1000 n0011 n0111 n3 n010 n101 n0 n2 n00000 n00001 n4 n01 n001 n0001 n00001 n\", \" output \": \"1 n3 -1 n0 n2 n1 2 \", \" type \": \" stdin_stdout \", \" label \": \" basic \", \" reason \": \" This test represents simple , straightforward input conditions .\" ] } Remember: Do not assume any knowledge about the internal code; base your tests purely on the input-output behavior described in the problem statement. Ensure that each of your test cases is incrementally more challenging than the baseline test case provided. Problem Statement: {problem_statement} Baseline Test Case: {baseline_test_case} 17 Table 6: Performance comparison for Gemma2-2B-IT across all curriculum strategies. Scores are colored and bolded based on their deviation from the Base strategy (blue for higher, red for lower)."
        },
        {
            "title": "Strategy",
            "content": "HumanEval HumanEval+ MBPP MBPP+ CodeForces LCBv"
        },
        {
            "title": "CruxEval",
            "content": "Base Uniform B/I Weighted C/E Weighted C/E Weighted (Rev) Basic Only Edge Only Complex Only 42.07% 39.02% 35.98% 41.46% 40.86% 44.51% 39.63% 42.07% 34.76% 31.09% 32.32% 34.15% 35.37% 37.20% 35.37% 36.59% 41.20% 47.09% 33.80% 39.95% 35.60% 42.06% 39.20% 48.41% 40.20% 44.44% 38.60% 46.83% 38.00% 46.03% 37.00% 45.77% 2.21% 0.22% 0.22% 0.22% 0.44% 1.77% 0.22% 2.21% 4.30% 37.50/26.88% 3.58% 33.00/26.63% 4.30% 38.63/27.25% 3.94% 36.63/26.75% 3.94% 35.63/26.75% 3.94% 39.88/27.88% 4.30% 39.00/28.13% 2.87% 35.63/27.55% Table 7: Comprehensive performance evaluation of all curriculum strategies on Qwen3-4B-Instruct-2507. The highest score on each benchmark is highlighted in bold. The performance of Qwen3-Coder-30B-A3B-Instruct is included to enable comparison against leading code-specialized model."
        },
        {
            "title": "Model Strategy",
            "content": "HumanEval HumanEval+ MBPP MBPP+ CodeForces LCBv"
        },
        {
            "title": "CruxEval",
            "content": "Qwen3-Coder-30B-A3B-Instruct"
        },
        {
            "title": "Base",
            "content": "94.51% 86.59% 73.80% 75.13% 29.65% 37.63% 81.75/79.25% Qwen3-4B-InstructBase Uniform B/I Weighted C/E Weighted C/E Weighted (Rev) Basic Only Edge Only Complex Only 89.02% 88.41% 89.63% 91.46% 89.63% 89.63% 89.63% 90.85% 78.66% 80.09% 81.09% 82.92% 80.48% 79.87% 79.88% 80.48% 52.60% 56.61% 35.30% 53.70% 28.00% 52.38% 55.20% 58.73% 36.20% 35.98% 39.80% 56.34% 47.20% 56.61% 28.60% 51.85% 33.63% 31.86% 33.04% 31.79% 34.66% 33.11% 31.86% 30.61% 32.02% 78.25/77.75% 31.96% 79.37/75.75% 33.81% 79.50/75.38% 31.54% 81.12/75.25% 31.66% 79.50/76.00% 31.90% 78.50/75.00% 30.59% 80.25/74.00% 31.30% 80.37/76.37% 18 Table 8: Comprehensive performance evaluation of all curriculum strategies across Qwen2.5-Instruct and Qwen2.5Coder-Instruct models (1.5B, 3B, 7B). For each model size, the highest score on each benchmark is highlighted in bold."
        },
        {
            "title": "Model Strategy",
            "content": "HumanEval HumanEval+ MBPP MBPP+ CodeForces LCBv"
        },
        {
            "title": "CruxEval",
            "content": "Qwen2.5-7B-Instruct Base Uniform B/I Weighted C/E Weighted C/E Weighted (Rev) Basic Only Edge Only Complex Only Qwen2.5-3B-Instruct Base Uniform B/I Weighted C/E Weighted C/E Weighted (Rev) Basic Only Edge Only Complex Only Qwen2.5-1.5B-Instruct Base Uniform B/I Weighted C/E Weighted C/E Weighted (Rev) Basic Only Edge Only Complex Only Qwen2.5-Coder-7B-Instruct Base Uniform B/I Weighted C/E Weighted C/E Weighted (Rev) Basic Only Edge Only Complex Only Qwen2.5-Coder-3B-Instruct Base Uniform B/I Weighted C/E Weighted C/E Weighted (Rev) Basic Only Edge Only Complex Only Qwen2.5-Coder-1.5B-Instruct Base Uniform B/I Weighted C/E Weighted C/E Weighted (Rev) Basic Only Edge Only Complex Only 82.93% 82.93% 78.05% 79.27% 84.15% 82.32% 83.54% 84.15% 69.51% 71.34% 69.51% 69.51% 70.12% 66.46% 71.34% 67.68% 58.54% 60.98% 59.15% 60.98% 56.71% 57.32% 55.49% 59.76% 85.98% 85.76% 84.76% 87.80% 88.41% 85.98% 79.02% 87.80% 79.27% 81.10% 81.71% 79.88% 82.32% 80.49% 79.27% 78.05% 68.29% 71.34% 72.56% 71.34% 72.56% 70.12% 72.56% 71.34% 8.54% 12.36% 11.56% 10.89% 8.24% 12.29% 11.11% 12.95% 4.34% 6.92% 7.21% 7.21% 6.92% 6.33% 6.11% 6.84% 2.65% 3.68% 3.83% 3.61% 4.49% 4.05% 3.75% 3.46% 10.89% 13.98% 13.32% 14.94% 13.98% 14.86% 12.14% 14.35% 3.90% 7.21% 6.70% 8.02% 8.17% 7.95% 7.21% 7.65% 2.06% 4.56% 4.19% 3.46% 3.38% 4.49% 4.56% 3.31% 14.10% 57.75/58.00% 14.93% 57.88/59.38% 15.89% 57.25/59.25% 15.77% 57.13/55.50% 15.41% 57.88/56.38% 19.12% 55.63/57.50% 17.08% 56.13/57.75% 17.80% 57.25/56.38% 5.02% 38.75/44.63% 8.72% 42.00/42.50% 9.44% 42.38/46.75% 7.17% 43.75/44.50% 8.00% 43.63/42.50% 6.09% 40.50/44.13% 7.05% 43.13/42.63% 6.33% 41.25/42.88% 5.02% 38.63/30.88% 5.26% 37.13/33.75% 4.54% 36.00/29.75% 5.02% 34.75/32.38% 4.90% 34.00/31.75% 4.66% 40.25/33.00% 4.42% 35.50/31.50% 4.54% 36.13/33.38% 13.86% 66.38/66.13% 17.68% 66.50/66.38% 17.44% 68.38/65.88% 19.24% 66.25/67.13% 19.12% 68.63/65.00% 19.47% 67.50/66.38% 19.12% 68.75/66.00% 18.16% 67.75/66.50% 9.80% 53.38/53.75% 10.75% 54.00/54.75% 9.80% 54.25/53.38% 10.51% 56.75/53.50% 10.75% 52.63/55.13% 13.14% 55.88/55.88% 10.63% 53.75/53.25% 10.63% 53.13/55.25% 3.46% 44.38/36.38% 4.42% 44.75/36.38% 4.66% 45.13/35.75% 4.42% 45.13/38.00% 4.18% 45.25/37.00% 4.66% 43.25/36.00% 5.02% 44.86/35.63% 4.54% 43.75/37.36% 75.61% 73.78% 76.83% 73.78% 77.44% 75.61% 76.22% 75.61% 61.59% 63.41% 62.20% 62.80% 62.80% 59.15% 64.02% 60.37% 54.88% 54.88% 54.27% 55.49% 52.44% 53.05% 50.61% 54.88% 79.27% 79.27% 78.66% 82.32% 81.10% 79.88% 81.07% 80.49% 75.00% 76.83% 78.05% 76.83% 77.44% 76.22% 75.00% 73.78% 63.41% 65.24% 64.02% 66.65% 64.20% 64.02% 67.10% 66.46% 63.20% 67.46% 67.40% 67.46% 67.60% 69.58% 66.20% 70.37% 69.00% 70.11% 66.20% 68.52% 67.60% 70.63% 69.00% 69.05% 58.40% 64.81% 59.40% 63.49% 59.00% 63.49% 56.60% 63.76% 57.00% 63.49% 59.40% 64.02% 58.20% 62.70% 59.00% 64.81% 46.80% 52.91% 50.00% 57.14% 51.80% 57.94% 49.40% 56.08% 50.40% 58.20% 50.60% 58.20% 50.20% 56.08% 51.80% 55.29% 75.60% 69.05% 77.20% 72.49% 77.60% 71.96% 76.20% 70.90% 75.00% 71.42% 76.20% 71.96% 77.20% 71.96% 76.60% 70.90% 62.20% 66.93% 62.00% 67.20% 61.40% 66.93% 61.00% 67.46% 62.00% 68.52% 62.80% 66.67% 62.60% 66.14% 63.00% 67.72% 52.60% 63.49% 52.80% 62.96% 55.80% 62.70% 54.60% 62.96% 54.20% 62.96% 54.00% 64.76% 53.60% 62.17% 53.20% 63.49% 19 Table 9: sample from TAROT dataset comprising 4-tiered test cases: basic, intermediate, complex, and edge. The Reason column details the rationale for each tier assignment. Solve the following coding problem using the programming language python: You are given two positive integer numbers and b. Permute (change order) of the digits of to construct maximal number not exceeding b. No number in input and/or output can start with the digit 0. It is allowed to leave as it is. Input The first line contains integer (1 1018). The second line contains integer (1 1018). Numbers dont have leading zeroes. It is guaranteed that answer exists. Output Print the maximum possible number that is permutation of digits of and is not greater than b. The answer cant have any leading zeroes. It is guaranteed that the answer exists. The number in the output should have exactly the same length as number a. It should be permutation of digits of a. Examples Input 123 222 Output Input 3921 10000 Output 9321 Input 4940 5000 Output 4940 The input will be given via stdin and the output should be printed to stdout by your code. Now solve the problem by providing the code."
        },
        {
            "title": "Reason",
            "content": "21 12 12 3051"
        },
        {
            "title": "Complex",
            "content": "98761230 98765000 98763210 simple 2-digit case where swapping the digits yields the only valid permutation bound, illustrating the happy path. 4-digit case including zero, requiring the algorithm to match the upper bound exactly with permutation of the digits. An 8-digit case where matching the bound fails at later position, forcing backtracking and maximal tail fill."
        },
        {
            "title": "Edge",
            "content": "111222333444555666 1000000000000000000 666555444333222111 An extreme boundary case with an 18-digit input and longer 19-digit bound, where any valid permutation fits, so the result is the digits sorted in descending order. Table 10: sample from TAROT dataset comprising 4-tiered test cases: basic, intermediate, complex, and edge. The Reason column details the rationale for each tier assignment. Solve the following coding problem using the programming language python: Winter is here at the North and the White Walkers are close. John Snow has an army consisting of soldiers. While the rest of the world is fighting for the Iron Throne, he is going to get ready for the attack of the White Walkers. He has created method to know how strong his army is. Let the i-th soldiers strength be ai. For some k, we call the indices i1, i2, . . . , ik clan if i1 < i2 < < ik and gcd(ai1 , ai2 , . . . , aik ) > 1. The strength of that clan is defined as gcd(ai1 , ai2 , . . . , aik ). The strength of the army is defined by the sum of the strengths of all possible clans. Your task is to find the strength of his army. As the number may be very large, you have to print it modulo 1000000007 (109 + 7). Greatest common divisor (gcd) of sequence of integers is the maximum possible integer so that each element of the sequence is divisible by it. Input The first line contains integer (1 200000) the size of the army. The second line contains integers a1, a2, ..., an (1 ai 1000000) denoting the strengths of his soldiers. Output Print one integer the strength of John Snows army modulo 1000000007 (109 + 7). Examples Input 3 3 3 Output 12 Input 4 2 3 4 6 Output 39 Note In the first sample the clans are {1}, {2}, {1, 2} so the answer will be 1 3 + 1 3 + 2 3 ="
        },
        {
            "title": "The input will be stdin and you should print your solution to stdout",
            "content": "Now solve the problem and return the code."
        },
        {
            "title": "Complex",
            "content": "4 2 3 5 7 17 6 2 4 8 3 9 6 119 7 2 2 2 2 2 2"
        },
        {
            "title": "Edge",
            "content": "5 1 1 1 1 1 0 All strengths are prime, so only single-soldier clans contribute. Mix of primes and composites yields clans of various sizes and gcds. Uniform strengths where every nonempty subset is valid clan (gcd=2). All strengths are 1, so no clan has gcd>1; result is zero. 21 Table 11: sample from TAROT dataset comprising 4-tiered test cases: basic, intermediate, complex, and edge. The Reason column details the rationale for each tier assignment. Solve the following coding problem using the programming language python: permutation is sequence of length integers from 1 to n, in which all the numbers occur exactly once. For example, [1], [3, 5, 2, 1, 4], [1, 3, 2] permutations, and [2, 3, 2], [4, 3, 1], [0] no. Polycarp was recently gifted permutation a[1 . . . n] of length n. Polycarp likes trees more than permutations, so he wants to transform permutation into rooted binary tree. He transforms an array of different integers into tree as follows: The maximum element of the array becomes the root of the tree; All elements to the left of the maximum form left subtree (which is built according to the same rules but applied to the left part of the array), but if there are no elements to the left of the maximum, then the root has no left child; All elements to the right of the maximum form right subtree (which is built according to the same rules but applied to the right side of the array), but if there are no elements to the right of the maximum, then the root has no right child. For example, if he builds tree by permutation = [3, 5, 2, 1, 4], then the root will be the element a2 = 5, and the left subtree will be the tree that will be built for the subarray a[1 . . . 1] = [3], and the right one for the subarray a[3 . . . 5] = [2, 1, 4]. As result, the following tree will be built: <image> The tree corresponding to the permutation a=[3, 5, 2, 1, 4]. Another example: let the permutation be a=[1, 3, 2, 7, 5, 6, 4]. In this case, the tree looks like this: <image> The tree corresponding to the permutation a=[1, 3, 2, 7, 5, 6, 4]. Let us denote by dv the depth of the vertex av, that is, the number of edges on the path from the root to the vertex numbered av. Note that the root depth is zero. Given the permutation a, for each vertex, find the value of dv. Input The first line contains one integer (1 100) the number of test cases. Then test cases follow. The first line of each test case contains an integer (1 100) the length of the permutation. This is followed by numbers a1, a2, . . . , an permutation a. Output For each test case, output values d1, d2, . . . , dn. Example Input 3 5 3 5 2 1 4 1 1 4 4 3 1 2 Output 1 0 2 3 1 0 0 1 3 2 The input will be stdin and you should print your solution to stdout Now solve the problem and return the code."
        },
        {
            "title": "Input",
            "content": "1 3 1"
        },
        {
            "title": "Output",
            "content": "2 1 0 2 4 2 1 4 3 5 5 4 3 2 1 1 2 0 1 0 1 2"
        },
        {
            "title": "Reason",
            "content": "Simple ascending permutation forming left-skewed tree under normal conditions. Includes mixed permutation and strictly decreasing permutation to test right-skewed tree and boundary values. 1 10 3 8 2 5 10 9 1 7 4 6 1 15 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 2 1 3 2 0 1 3 2 4 3 Complex permutation of length 10 to test multiple recursion levels and both left and right subtrees. 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 Maximum ascending chain of length 15 to test deep recursion and large boundary condition. 22 Table 12: sample from TAROT dataset comprising 4-tiered test cases: basic, intermediate, complex, and edge. The Reason column details the rationale for each tier assignment. Solve the following coding problem using the programming language python: PrincessMarriage Marriage of princess English text is not available in this practice contest. brave princess in poor country, knowing that gambling payouts are determined by the parimutuel method, felt more familiar with gambling and was convinced of her victory in gambling. As result, he spent more money than ever before and lost enough to lose all the taxes paid by the people. The King, who took this situation seriously, decided to marry the princess to the neighboring country. By doing this, thought that would like the princess to reflect on her daily activities and at the same time deepen her friendship with neighboring countries and receive financial assistance. The princess and the prince of the neighboring country liked each other, and the kings of both countries agreed on political marriage. The princess triumphantly went to the neighboring country with little money in her hand. On the other hand, the motive for the princess to marry is to pursue the unilateral interests of the king, and the aide of the prince of the neighboring country who thinks that it is not pleasant shoots countless thugs along the way to make the princess dead. It was. The path the princess will take has already been decided. There are total of post stations on the path of the princess. For convenience, the departure and arrival points are also set as post stations, and each post station is called S1, S2, . . . SL. The princess shall be in S1 first, visit the post station in ascending order (S2, S3 . . . in that order), and finally go to SL. At the post station, you can pay money to hire an escort, and as long as you have the money, you can contract for as long as you like to protect the princess. The cost of hiring an escort is 1 gold per distance. Note that the princess can also partially protect the section through which she passes. The distance between Si and Si + 1 is given by Di, and the expected value of the number of times thug is attacked per distance between Si and Si + 1 is given by Pi. Find the expected number of thugs to reach your destination when the princess has budget of and hires an escort to minimize the expected number of thugs. Input The input consists of multiple datasets. Each dataset has the following format. > > D1 P1 > D2 P2 > ... > DN PN Two integers are given in the first row of each dataset, representing the number of intervals (1 10, 000 and the budget (0 1, 000, 000, 000 of the princess difference, respectively. The next lines show information about the path the princess takes. Each line contains two integers, and the i-th line is the expected value of the interval Di (1 Di 10, 000) and the number of attacks when moving one unit distance between them Pi (0 10) ). The end of the input is represented by data set with = 0 and = 0. Do not output the calculation result for this data set. Output For each dataset, output the expected number of times the princess will be attacked by thugs to your destination. Sample Input 2 8 5 6 4 5 3 1 5 10 5 10 5 10 0 0 Output for the Sample Input Five The input will be given via stdin and the output should be printed to stdout by your code."
        },
        {
            "title": "Input",
            "content": "4 7 3 2 4 1 1 5 2 2"
        },
        {
            "title": "Output",
            "content": "3 6 12 5 3 2 0 7 2 3 3 4 1 6 2 0 0 22 10 30 10 1 5 5 8 5 6 3 12 2 4 5 7 3 9 4 3 0 11 2 0"
        },
        {
            "title": "Edge",
            "content": "3 0 5 2 10 4 7 3 4 100 5 2 10 4 7 3 8 0 2 1000 100 0 200 0 0 0 71"
        },
        {
            "title": "Reason",
            "content": "Simple scenario with multiple segments and straightforward positive Pi values; tests basic greedy coverage under limited budget. Moderate number of segments including Pi=0, ensuring segments with no attacks are ignored and budget partially covers higher-Pi segments. Larger set of segments with ties in Pi values and varied distances, requiring correct sorting and partial coverage among equal-Pi segments. Edge conditions including zero budget, budget exceeding total distance, and segments with Pi=0 to verify no-protection and full-protection behaviors. 23 Table 13: sample from TAROT dataset comprising 4-tiered test cases: basic, intermediate, complex, and edge. The Reason column details the rationale for each tier assignment. Solve the following coding problem using the programming language python: Valera loves his garden, where fruit trees grow. This year he will enjoy great harvest! On the i-th tree bi fruit grow, they will ripen on day number ai. Unfortunately, the fruit on the tree get withered, so they can only be collected on day ai and day ai + 1 (all fruits that are not collected in these two days, become unfit to eat). Valera is not very fast, but there are some positive points. Valera is ready to work every day. In one day, Valera can collect no more than fruits. The fruits may be either from the same tree, or from different ones. What is the maximum amount of fruit Valera can collect for all time, if he operates optimally well? Input The first line contains two space-separated integers and (1 n, 3000) the number of fruit trees in the garden and the number of fruits that Valera can collect in day. Next lines contain the description of trees in the garden. The i-th line contains two space-separated integers ai and bi (1 ai, bi 3000) the day the fruits ripen on the i-th tree and the number of fruits on the i-th tree. Output Print single integer the maximum number of fruit that Valera can collect. Examples Input 2 3 1 5 2 3 Output 8 Input 5 10 3 20 2 20 1 20 4 20 5 20 Output 60 Note In the first sample, in order to obtain the optimal answer, you should act as follows. On the first day collect 3 fruits from the 1-st tree. On the second day collect 1 fruit from the 2-nd tree and 2 fruits from the 1-st tree. On the third day collect the remaining fruits from the 2-nd tree. In the second sample, you can only collect 60 fruits, the remaining fruit will simply wither. The input will be stdin and you should print your solution to stdout Now solve the problem and return the code."
        },
        {
            "title": "Reason",
            "content": "2 5 1 3 3 4 7 3 1 1 2 2 2 3 2 4 5 5 1 4 2 6 2 3 5 10 6 2 No overlapping ripening days and capacity exceeds daily fruits; collect all fruits on their ripening days. Capacity is only 1 per day with overlapping two-day windows; requires optimal scheduling across consecutive days. Multiple trees ripen on the same days, gaps between ripening days, and moderate capacity to stress multi-day planning."
        },
        {
            "title": "Edge",
            "content": "2 1000 2999 1500 3000 2500 3000 Ripening on the maximum allowed days (2999 and 3000) tests boundary handling and two-day collection windows at the end of the range. 24 Table 14: sample from TAROT dataset comprising 4-tiered test cases: basic, intermediate, complex, and edge. The Reason column details the rationale for each tier assignment. Solve the following coding problem using the programming language python: Bessie the cow has just intercepted text that Farmer John sent to Burger Queen! However, Bessie is sure that there is secret message hidden inside. The text is string of lowercase Latin letters. She considers string as hidden in string if exists as subsequence of whose indices form an arithmetic progression. For example, the string aab is hidden in string aaabb because it occurs at indices 1, 3, and 5, which form an arithmetic progression with common difference of 2. Bessie thinks that any hidden string that occurs the most times is the secret message. Two occurrences of subsequence of are distinct if the sets of indices are different. Help her find the number of occurrences of the secret message! For example, in the string aaabb, is hidden 3 times, is hidden 2 times, ab is hidden 6 times, aa is hidden 3 times, bb is hidden 1 time, aab is hidden 2 times, aaa is hidden 1 time, abb is hidden 1 time, aaab is hidden 1 time, aabb is hidden 1 time, and aaabb is hidden 1 time. The number of occurrences of the secret message is 6. Input The first line contains string of lowercase Latin letters (1 105) the text that Bessie intercepted. Output Output single integer the number of occurrences of the secret message. Examples Input aaabb Output"
        },
        {
            "title": "Input\nusaco",
            "content": "Output"
        },
        {
            "title": "Input\nlol",
            "content": "Output 2 Note In the first example, these are all the hidden strings and their indice sets: occurs at (1), (2), (3) occurs at (4), (5) ab occurs at (1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5) aa occurs at (1, 2), (1, 3), (2, 3) bb occurs at (4, 5) aab occurs at (1, 3, 5), (2, 3, 4) aaa occurs at (1, 2, 3) abb occurs at (3, 4, 5) aaab occurs at (1, 2, 3, 4) aabb occurs at (2, 3, 4, 5) aaabb occurs at (1, 2, 3, 4, 5) Note that all the sets of indices are arithmetic progressions. In the second example, no hidden string occurs more than once. In the third example, the hidden string is the letter l."
        },
        {
            "title": "The input will be stdin and you should print your solution to stdout",
            "content": "Now solve the problem and return the code."
        },
        {
            "title": "Reason",
            "content": "abab 3 abacaba"
        },
        {
            "title": "Complex",
            "content": "abababab 10 simple alternating pattern to validate basic subsequence counting. Mixed letters and repeating patterns to test moderately complex subsequences. Longer alternating pattern to stress test counting of many arithmetic-progression subsequences."
        },
        {
            "title": "Edge",
            "content": "z 1 Minimal input length boundary case. Table 15: sample from TAROT dataset comprising 4-tiered test cases: basic, intermediate, complex, and edge. The Reason column details the rationale for each tier assignment. Solve the following coding problem using the programming language python: The ZCO scholarship contest offers scholarships to first time ZCO participants. You are participating in it for the first time. So you want to know the number of participants wholl get the scholarship. You know that the maximum number of scholarships offered is and there are total of participants numbered from 1 to . Out of these, you know the set of people (denoted by X) who you know, had participated in previous year ZCOs and hence, they shall not get the scholarship. Further, as the world isnt free from plagiarism, so is the case with the scholarship contest. And from your secret sources, you also know the set of people (denoted by set ) who were involved in plagiarism and therefore arent eligible for scholarship either. Find out the number of participants who shall get the scholarship. PS: Dont ask how so many scholarships are being offered when you see the constraints on R. You never questioned it when in mathematics classes, some person bought 80 watermelons twice just to compare them and save 1. Input: - The first line will contain single integer, , the number of testcases. Then the testcases follow. - The first line of each test case contains four integers; , R, and denoting the number of participants, maximum number of scholarships offered, number of old participants, and the number of participants involved in plagiarism, respectively. - The second line of each test case contains space separated integers x1, x2 . . . xX denoting the indices of people who participated in previous years. If is empty, this line is skipped and no empty line is in the input. - The third line of each test case contains space separated integers y1, y2 . . . yY denoting the indices of people who are involved in plagiarism. If is empty, this line is skipped and no empty line is in input. Output: For each testcase, print single integer in new line, denoting the number of participants who shall get the scholarship. Constraints - 1 1000 - 1 1015 - 0 1015 - 0 X, min(N, 2 105) - 1 xi, yi - All xi are distinct - All yi are distinct - Sum of over all test cases does not exceed 5 105 - Sum of over all test cases does not exceed 5 105 Subtasks - 20 points : 1 103, and the sum of over all test cases does not exceed 3 103 - 30 points : 1 2 105, and the sum of over all test cases does not exceed 5 105 - 50 points: Original constraints Sample Input: 3 5 3 0 1 4 10 2 4 6 3 1 7 6 4 3 1 5 9 7 10 4 4 6 3 1 7 6 4 3 1 5 9 7 Sample Output: 3 2 3 EXPLANATION: - In the first testcase, only participant 4 is involved in plagiarism, and thus not eligible for the scholarship. No user has participated in previous years, and so no empty line is there in the sample. All participants except participant 4 are eligible for the scholarship, but only three of them get it because = 3. - Both second and third testcases are the same, except for R. In both samples, only participants 2, 8 and 10 are eligible for scholarships. - In the second testcase, since the maximum number of scholarships is 2, only 2 participants get scholarships. - In the third testcase, all three eligible participants get scholarships. The input will be stdin and you should print your solution to stdout"
        },
        {
            "title": "Basic",
            "content": "1 4 2 1 1"
        },
        {
            "title": "Output",
            "content": "2 2 7 4 3 2 2 4 5 4 6 5 1 0 2 3 5 3 1 3 1000000000000 1000000000000 2 3 1 1000000000000 500000000000 1 999999999999 20 15 5 5 1 2 3 4 5 4 5 6 7 8 50 100 3 2 10 20 30 30 40 999999999996"
        },
        {
            "title": "Reason",
            "content": "Basic test with single test case, non-empty and sets without overlap, validating core functionality. Medium complexity with multiple test cases, overlapping and in the first, and an empty set in the second. High complexity with very large and values, moderate and sizes, and multiple test cases to stress-test the implementation. 26 2 1000000000000000 0 0 0 5 10 3 3 1 2 3 3 4 5 0 Edge case with maximum boundary values and zero scholarships in the first, and and covering all participants in the second, testing empty sets and full exclusion. Table 16: sample from TAROT dataset comprising 4-tiered test cases: basic, intermediate, complex, and edge. The Reason column details the rationale for each tier assignment. Solve the following coding problem using the programming language python: Polycarp has recently got himself new job. He now earns so much that his old wallet cant even store all the money he has. Berland bills somehow come in lots of different sizes. However, all of them are shaped as rectangles (possibly squares). All wallets are also produced in form of rectangles (possibly squares). bill fits into some wallet if either and or and w. Bills can overlap with each other in wallet and an infinite amount of bills can fit into wallet. That implies that all the bills Polycarp currently have fit into wallet if every single one of them fits into it independently of the others. Now you are asked to perform the queries of two types: + Polycarp earns bill of size y; ? Polycarp wants to check if all the bills he has earned to this moment fit into wallet of size w. It is guaranteed that there is at least one query of type 1 before the first query of type 2 and that there is at least one query of type 2 in the input data. For each query of type 2 print \"YES\" if all the bills he has earned to this moment fit into wallet of given size. Print \"NO\" otherwise. Input The first line contains single integer (2 5 105) the number of queries. Each of the next lines contains query of one of these two types: + (1 x, 109) Polycarp earns bill of size y; ? (1 h, 109) Polycarp wants to check if all the bills he has earned to this moment fit into wallet of size w. It is guaranteed that there is at least one query of type 1 before the first query of type 2 and that there is at least one query of type 2 in the input data. Output For each query of type 2 print \"YES\" if all the bills he has earned to this moment fit into wallet of given size. Print \"NO\" otherwise. Example Input 9 + 3 2 + 2 3 ? 1 20 ? 3 3 ? 2 3 + 1 5 ? 10 10 ? 1 5 + 1 1 Output NO YES YES YES NO Note The queries of type 2 of the example: Neither bill fits; Both bills fit (just checking that you got that bills can overlap); Both bills fit (both bills are actually the same); All bills fit (too much of free space in wallet is not problem); Only bill 1 5 fit (all the others dont, thus its \"NO\"). The input will be stdin and you should print your solution to stdout"
        },
        {
            "title": "Input",
            "content": "4 + 4 5 ? 5 4 ? 4 4 ?"
        },
        {
            "title": "Output",
            "content": "YES NO YES 7 + 2 7 + 3 3 + 7 2 ? 3 7 ? 4 6 + 10 1 ? 10 5 YES NO YES 13 + 5 5 + 6 4 + 9 1 ? 5 5 ? 9 1 ? 4 9 + 2 8 ? 8 9 + 7 7 ? 7 7 ? 8 7 ? 10 8 ? 6 6 NO NO NO YES NO NO YES NO 8 + 1000000000 1 + 1 1000000000 + 500000000 500000000 ? 1000000000 1000000000 ? 999999999 1000000000 ? 1000000000 499999999 + 1000000000 1000000000 ? 1000000000 YES YES NO YES"
        },
        {
            "title": "Reason",
            "content": "Single bill with queries testing orientation and size validation under straightforward conditions. Multiple bills including duplicates and interleaved adds and queries testing correct global dimension tracking. Complex interleaving of many adds and queries with varying dimensions to stress test global maximum computations. Extreme boundary values testing maximum limits and strict comparison edge where one dimension is just below requirement. 27 Table 17: sample from TAROT dataset comprising 4-tiered test cases: basic, intermediate, complex, and edge. The Reason column details the rationale for each tier assignment. Solve the following coding problem using the programming language python: Valera had an undirected connected graph without self-loops and multiple edges consisting of vertices. The graph had an interesting property: there were at most edges adjacent to each of its vertices. For convenience, we will assume that the graph vertices were indexed by integers from 1 to n. One day Valera counted the shortest distances from one of the graph vertices to all other ones and wrote them out in array d. Thus, element d[i] of the array shows the shortest distance from the vertex Valera chose to vertex number i. Then something irreparable terrible happened. Valera lost the initial graph. However, he still has the array d. Help him restore the lost graph. Input The first line contains two space-separated integers and (1 105). Number shows the number of vertices in the original graph. Number shows that at most edges were adjacent to each vertex in the original graph. The second line contains space-separated integers d[1], d[2], ..., d[n] (0 d[i] < n). Number d[i] shows the shortest distance from the vertex Valera chose to the vertex number i. Output If Valera made mistake in his notes and the required graph doesnt exist, print in the first line number -1. Otherwise, in the first line print integer (0 106) the number of edges in the found graph. bi n; ai = bi), denoting the edge that In each of the next lines print two space-separated integers ai and bi (1 ai, connects vertices with numbers ai and bi. The graph shouldnt contain self-loops and multiple edges. If there are multiple possible answers, print any of them. Examples Input 3 2 0 1 1 Output 3 1 2 1 3 3 2 Input 4 2 2 0 1 3 Output 3 1 3 1 4 2 3 Input 3 1 0 0 0 Output - The input will be given via stdin and the output should be printed to stdout by your code."
        },
        {
            "title": "Output",
            "content": "4 2 0 1 1 2 3 1 2 1 3 2 4 7 3 0 1 2 2 1 2 3 10 3 0 1 1 1 2 2 2 2 2 3 6 1 2 1 5 2 3 2 4 5 6 3 7 9 1 2 1 3 1 4 2 5 2 6 3 7 3 8 4 9"
        },
        {
            "title": "Edge",
            "content": "5 3 0 2 2 3 3 -"
        },
        {
            "title": "Reason",
            "content": "Simple BFS tree with one level-2 vertex. Moderately sized tree with branching and various depths. Larger tree with multiple branches and depth-3 leaf. No vertices at distance 1, invalid distance sequence 28 Table 18: sample from TAROT dataset comprising 4-tiered test cases: basic, intermediate, complex, and edge. The Reason column details the rationale for each tier assignment. Solve the following coding problem using the programming language python: The game of Berland poker is played with deck of cards, of which are jokers. players play this game (n is divisible by k). At the beginning of the game, each player takes cards from the deck (so each card is taken by exactly one player). The player who has the maximum number of jokers is the winner, and he gets the number of points equal to y, where is the number of jokers in the winners hand, and is the maximum number of jokers among all other players. If there are two or more players with maximum number of jokers, all of them are winners and they get 0 points. Here are some examples: = 8, = 3, = 2. If one player gets 3 jokers and 1 plain card, and another player gets 0 jokers and 4 plain cards, then the first player is the winner and gets 3 0 = 3 points; = 4, = 2, = 4. Two players get plain cards, and the other two players get jokers, so both of them are winners and get 0 points; = 9, = 6, = 3. If the first player gets 3 jokers, the second player gets 1 joker and 2 plain cards, and the third player gets 2 jokers and 1 plain card, then the first player is the winner, and he gets 3 2 = 1 point; = 42, = 0, = 7. Since there are no jokers, everyone gets 0 jokers, everyone is winner, and everyone gets 0 points. Given n, and k, calculate the maximum number of points player can get for winning the game. Input The first line of the input contains one integer (1 500) the number of test cases. Then the test cases follow. Each test case contains three integers n, and (2 50, 0 n, 2 n, is divisors of n). Output For each test case, print one integer the maximum number of points player can get for winning the game. Example Input 4 8 3 2 4 2 4 9 6 3 42 0 7 Output 3 0 1 Note Test cases of the example are described in the statement."
        },
        {
            "title": "The input will be stdin and you should print your solution to stdout",
            "content": "Now solve the problem and return the code."
        },
        {
            "title": "Basic",
            "content": "3 9 2 3 12 5 4 6"
        },
        {
            "title": "Output",
            "content": "2"
        },
        {
            "title": "Complex",
            "content": "5 20 10 5 15 3 5 10 10 2 14 7 7 18 17 3 2 3 0 1 0 7 50 25 25 49 49 7 48 20 6 30 0 5 32 16 4 45 23 9 28 14 7 1 0 5 0 5"
        },
        {
            "title": "Edge",
            "content": "6 2 0 2 2 2 2 50 0 25 50 50 50 50 25 5 50 1 2 0 0 0 0"
        },
        {
            "title": "Reason",
            "content": "Simple small cases covering scenarios where jokers are fewer than, equal to, or exceed the per-player limit. Moderate-sized inputs, testing exact division of jokers, no jokers, and tied maximum distributions. Varied larger values including big decks, testing heavy distributions and zero-joker scenarios. Extreme boundary conditions with minimal and maximal n, k, and values to test edge handling."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Electronics and Telecommunications Research Institute",
        "Hugging Face",
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}