{
    "paper_title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research",
    "authors": [
        "Guijin Son",
        "Jiwoo Hong",
        "Honglu Fan",
        "Heejeong Nam",
        "Hyunwoo Ko",
        "Seungwon Lim",
        "Jinyeop Song",
        "Jinha Choi",
        "Gonçalo Paulo",
        "Youngjae Yu",
        "Stella Biderman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the \\textbf{academic verification of scientific manuscripts}. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 5 5 8 1 1 . 5 0 5 2 : r When AI Co-Scientists Fail: SPOTa Benchmark for Automated Verification of Scientific Research Guijin Son1,2 Hyunwoo Ko Jiwoo Hong3 Honglu Fan2 Heejeong Nam4 Jinha Choi5 Seungwon Lim5 Jinyeop Song6 Gonçalo Paulo2 Youngjae Yu5 Stella Biderman2 OneLineAI1 EleutherAI2 Yonsei University MIT6 KAIST AI3 Boeing Korea4 spthsrbwls123@yonsei.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore complementary application: using LLMs as verifiers to automate the academic verification of scientific manuscripts. To that end, we introduce SPOT, dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1% recall or 6.1% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification. https://huggingface.co/datasets/amphora/SPOT-MetaData"
        },
        {
            "title": "Introduction",
            "content": "From simple next-token predictors [1, 2], large language models (LLMs) have evolved to exhibit graduate-level STEM proficiency [35], generate hypotheses [6, 7], synthesize literature [8], and draft manuscripts [9]. Such advances have driven interest in their deployment as AI Co-Scientists [10, 11], proving to be viable options in the \"generative\" role of scientific research. They have rediscovered established findings [12] and generated novel hypotheses worthy of investigation across diverse fields [1315]. However, despite their widespread usage as generators in the forward pass of scientific research, their utility in the backward pass of academic verification or as verifiers remains underexplored, blind spot in which most systems lean on LLM judges [16] without validation on their credibility in reviewing scientific research. Prior research on factual verification has primarily focused on everyday knowledge tasks [1719], reference-based claim checking [20, 21], or computerscience disciplines alone [2224]. This limits the potential applicability of the proposed benchmarks as evaluation tools for verification systems in AI-driven science research. In this paper, we introduce SPOT (Scientific Paper Error Detection), complex multi-modal academic error verification benchmark, comprising 83 up-to-date manuscripts spanning ten scientific fields Preprint. Figure 1: Overview of SPOT. Green indicates benchmark construction process, from seed collection through validation to normalization; blue indicates evaluation, where LLM outputs are compared to ground-truth errors and classified as true positives, false positives, or false negatives. with multiple human-annotated errors. Given large-scale multi-modal inputs with 12,000 text tokens and 18 images on average, multi-modal LLMs (MLLMs) are tasked with generatively identifying more than one error with varying difficulties in single paper: e.g., , factual inconsistencies, figure duplications, and mathematical errors. We only select papers published from 2024, minimizing the potential contamination with parametric knowledge during evaluation [25]. It should be noted that, whereas prior evaluation suites focus on sentence-level fact checks of everyday knowledge [26, 27] or on reproducing noisy peer-review feedback [28, 29], SPOT extends verification to the full complexity of frontier-level scientific research. This paper is mainly divided into three parts. 1. SPOT Benchmark Design Principles (Section 2): We detail our efforts of multiple automated filtering, author verifications, and human annotations, highlighting our commitment to include confirmed, noncontroversial errors across diverse scientific subdomains. 2. Model Evaluation and Analysis (Section 3) We present evaluation results, demonstrating that even the state-of-the-art models struggle on SPOT. Specifically, OpenAIs o3 [30] and Llama-4Maverick [31] achieved 18.4% and 0.9% at pass@1 to name few. Furthermore, model confidence approaches zero when repeated over eight independent trials, questioning their reliability. We also observe that proprietary reasoning models suffer in detecting figure-related errors, highlighting shortcomings in their multi-modal capabilities. Such results cast serious concerns, revealing significant gap between current AI capabilities and the demands of rigorous scientific verification. 3. Expert-led Case Studies (Section 4) We present expert-led case studies in mathematics and materials science, analyzing model outputs to diagnose their failures. Our observations show that models struggle with long-tail knowledge likely absent in web data and extremely long contexts. We also note that, without fully spelled-out derivations, models fail to understand some calculations and overlook domain-specific conventions, making student-like errors."
        },
        {
            "title": "2 SPOT: Automating Error Detection in Scientific Research",
            "content": "In this section, we introduce detailed overview of SPOT, complex multi-modal academic verification benchmark with cross-validated scientific manuscripts. We ensure credibility in the error annotations through cross-validation process between human experts in each field and proprietary language models (Section 2.1). Spanning over ten different fields and six error types (Section 2.2), we introduce evaluation protocols mainly based on precision, recall, and pass@K (Section 2.3)."
        },
        {
            "title": "2.1 Data Curation",
            "content": "Stage 1 - Seed Collection We source our seed manuscripts from two major repositories: (1) WITHDRARXIV [32] and (2) PubPeer1. First, we extract entries annotated as factual/methodological/other 1https://pubpeer.com/ 2 critical errors from WITHDRARXIV, dataset of 14,000 papers and their associated retraction comments. Second, we crawl PubPeer, an anonymous post-publication peer review website, where users flag methodological flaws, image manipulations, and other scientific concerns. Following [33], we query initial searches using alphabets, extract high-frequency keywords from the returned paper titles, re-query using those keywords, and scrape each papers metadata (title, authors, venue) alongside the entire comments. We briefly attempted to include medRxiv and bioRxiv, but having retrieved only 1 and 13 papers, respectively, we dropped them due to the low yield. Stage 2 - Automated Filtering We apply two GPT-4o [34]2 filtering passes. The first retains commentmanuscript pairs that unambiguously pinpoint specific section, figure, equation, or table, reducing our pool to 1,855 WITHDRARXIV and 25,378 PubPeer samples. The second pass removes reports that require external artifacts (e.g., duplicated images across papers or errors detectable only via external datasets or code). Finally, to avoid overlap with GPT-4os training cutoff , we filter for papers published after 2024, yielding 58 WITHDRARXIV and 215 PubPeer samples. Stage 3 - Error Validation by Original Authors For remaining manuscripts, we only retain those the original authors directly confirmed. Specifically, we only retain PubPeer comments followed by an explicit author response acknowledging the mistake and treat WITHDRARXIV self-retractions as definitive evidence of critical error. In all cases where the author themselves admits the problem, we take this acknowledgment as confirmation of genuine error. While some errors may appear to be evident, we do not include any error with explicit acknowledgment from the original authors, as many of the work cover ungoing areas of research, which remain unsettled in the scientific discourse. Stage 4 - Sanity Check from Human Annotators We further apply two-stage human validation with mutually exclusive annotators. First, with part of the authors as human annotators, we manually validate if remaining flagged issues fulfill three conditions: (1) self-contained, (2) identifiable, and (3) explicitly acknowledged by the original authors. For those which satisfy the conditions, We retrieve the archived PDF to verify that the error remains visible, then document concise description of the problem, quote the authors acknowledgement verbatim, and assign both an error category and severity ratingproxied by the form of the authors response (erratum versus retraction). Afterwards, the second group conducted comprehensive audit of all annotations to ensure consistent application of these standards. The final SPOT benchmark comprises 83 manuscripts with 91 annotated errors. Although modest in size, our dataset aligns with recent trends toward compact, high-quality benchmarks: MT-Bench (80 items) [16], GPQA-D (198 items) [4], AIME 2024/2025 (30 items each) [35], USAMO 2025 (6 items) [36] and PaperBench (20 items) [37]. Stage 5 - Normalization We normalize final manuscripts in PDF format into text and image sets to best evaluate the comprehension of the target models. While prior benchmarks in manuscript error detection [24] and AI-assisted science [38] have relied on raw PDFs or text-only inputs, this approach offloads document understanding to OCR and parsing modules rather than the LLM itself, thereby conflating upstream parser failures with downstream model errors. Instead, we process all the documents for usage. We first employ Llama-Parse3 to convert each PDF into Markdown and capture high-fidelity screenshots of every figure, table, and equation. In pilot experiments, OCR failures, particularly in mathematical expressions, led downstream models to misinterpret formatting artifacts as errors. To address this, we introduce refinement stage. For each page, the initial OCR text and screenshots (one full-page image plus isolated equations and paragraphs, roughly eight images per page) are sent to GPT-4.1 for correction. Finally, we conduct manual audit of all processed pages to ensure that every flagged error remains visible and accurately represented in the OCR output."
        },
        {
            "title": "2.2 Benchmark Statistics",
            "content": "Error Types We derive the six categories in Table 1 inductively from our annotations rather than setting priori. As we review each error, we group similar cases. This is to capture the true distribution of errors existing in manuscripts. During this process, figure-duplication instances initially overwhelmed the dataset, so we filtered based on severity and paper category to prevent single type from dominating. 2Using version gpt-4o-2024-08-06 3https://www.llamaindex.ai/llamaparse 3 Table 1: Overview of SPOT. Left: High-level statistics83 manuscripts, 91 errors from 47 paper sources; tokens per manuscript (mean std., range) and images per manuscript (mean std., range). All token counts were computed using the GPT-4o [39] tokenizer from tiktoken [40]. Right: Six error categories with concise descriptions and instance counts in parentheses. Benchmark Statistics Category Descriptions General Total Manuscripts: 83 Total Errors: 91 Total Paper Sources: 47 Tokens Avg (std): 12, 8877,421 Max / Min: 46, 441/1, 207 Images Avg (std): 17.520.1 Max / Min : 80/0 Error severity Errata / Retract: 59/32 Equation / proof (37) Incorrect mathematical derivations Figure duplication (27) Reused or manipulated images Data inconsistency (18) Mismatched values between text, tables, and figures Statistical Reporting (4) Misused statistical values or inappropriate tests Reagent Identity (3) Mislabeled or incorrect materials Experiment Setup (2) Missing controls or misreported protocols Paper Subjects We present general statistics in Table 1. We classify each paper into ten research domains: Mathematics, Physics, Biology, Chemistry, Materials Science, Medicine, Environmental Science, Engineering, Computer Science, and Multidisciplinary, based on its journal venue or arXiv subject. In Figure 2, we observe clear domain patterns: mathematics, computer science, and physics papers skew toward equation/proof flaws; biology toward figure-duplication. 76 manuscripts out of 83 contain single error, six contain two, and one paper has the maximum of three annotated errors. We proxy error severity by the authors post-publication response: 59 errors were addressed via errata, while 32 led to full retractions. Retractions are concentrated mostly in equation/proof cases. Manuscripts span 1k46k tokens and include 080 figures, creating long context, multimodal, and figure-rich benchmark far exceeding the scale and complexity of existing error-detection datasets. Although longer papers tend to include more figures, the relationship is weak (Pearsons = 0.19), highlighting diverse presentation styles across fields. Figure 2: Distribution of annotated errors by research domain and error type."
        },
        {
            "title": "2.3 Evaluation protocol",
            "content": "We provide the full paper as interleaved text and image data, followed by the prompt to return every error with each errors location (section, figure, equation, or table), accompanied by description. The output is prompted to be structured JSON format (see Appendix for an example). Evaluation Metric We mainly evaluate verification performance through precision, recall, and pass@K. predicted error is counted as true positive (TP) only when the models reported location matches benchmark annotation and an LLM confirms they indicate the same error4. All other predictions, including those at non-annotated locations or those matching an annotated location but with different description, are considered false positives (FP), and any benchmark annotation the model fails to predict is false negative (FN). We treat the error annotations included in SPOT as exhaustive: any model-reported error not matching an annotation is counted as false positive. Although models could, in principle, flag genuine errors outside our annotations, through case studies later in this paper, we notice such cases are highly unlikely. To summarize model performance, we 4GPT-4.1 is used to compare predicted error descriptions against benchmark annotations as similarity check [41]; the LLM does not evaluate the errors correctness or severity. 4 Table 2: Performance of ten models on the SPOT dataset. The Think column denotes the use of test-time scaling. Precision, Recall, pass@1 and pass@4 (all in %) are reported as mean and standard deviation (in parentheses) over eight independent trials. The highest value in each column is bolded, and the second-highest is underlined. Detailed evaluation results are available in Appendix G. Models Think Precision (%) Recall (%) pass@1 (%) pass@4 (%) o3 (2025-04-16) GPT-4.1 (2025-04-14) Gemini-2.5-Pro (preview-03-25) Gemini-2.0-Flash-Lite (001) Claude-3.7-Sonnet (20250219:Think) Claude-3.7-Sonnet (20250219) Qwen2.5-VL-72B-Instruct Qwen2.5-VL-32B-Instruct Llama-4-Maverick Llama-4-Scout 6.11.3 2.80.8 3.11.7 1.00.8 3.01.3 3.21.5 0.61.2 1.92.0 2.02.6 0.81. 21.14.4 6.01.6 10.15.6 1.61.3 6.02.4 5.82.7 0.40.7 1.91.7 0.91.2 1.92.3 18.42.1 6.61.7 7.83.8 1.51.0 5.51.7 4.51.9 0.40.6 2.01.5 0.91.0 1.82.0 37.81.8 17.81.5 25.94.0 6.01.5 18.62.8 14.11.6 1.71.0 5.61.6 3.31.2 7.23. report Precision and Recall: Precision = TP TP + FP , Recall = TP TP + FN . (1) Precision quantifies the proportion of the models flagged errors that match benchmark annotations, penalizing unexpected predictions, and is most appropriate when false positives impose significant review overhead or undermine confidence in the tools outputs. Recall quantifies the proportion of annotated errors the model successfully identifies, penalizing missed detections. In practice, users concerned about model hallucinations or the impact of unannotated flags should focus on Precision. In contrast, those seeking comprehensive error coverage, or who doubt the exhaustiveness of our annotations, should emphasize Recall. Following [42] and [43], to capture how error detection improves with multiple attempts, for runs per paper, we define pass@K = 1 i=1 Gi (cid:80)N (cid:88) (cid:88) (cid:104) 1 i=1 gGi {1, . . . , K} : pi[s] (cid:105) , (2) where Gi is the set of annotated errors in paper and pi[s] the set predicted in the s-th run. With 83 papers and 91 total errors we generate = 8 independent runs per paper. For each pass@K we draw runs without replacement from the eight, repeat this resampling = 1000 times, and report the mean and standard deviation of the resulting bootstrap distribution for {1, 4}."
        },
        {
            "title": "3 Main Results and Analysis",
            "content": "In the following sections, we evaluate six proprietary models: OpenAI o3 [30], GPT-4.1 [44], Google Gemini 2.5 Pro [45], Gemini 2.0 Flash Lite [46], Anthropic Claude 3.7 Sonnet:Thinking [47], and Claude 3.7 Sonnet and four open models: Qwen 2.5-VL-72B/32B-Instruct [48], and Llama 4 Maverick/Scout [31]. We select the most capable models per family and observe that these models already score near zero in SPOT. Accordingly, as smaller models are unlikely to perform any better, we do not include them in our evaluations. All models are accessed via APIs, and each call is retried up to three times; those that still fail or are cut off due to length limits are marked incorrect."
        },
        {
            "title": "3.1 Main Results",
            "content": "Table 2 compares ten multi-modal LLMs on SPOT. o3 achieves the highest scores, with 6.1% 1.3 precision, 21.1% 4.4 recall, and 37.8% pass@4. It is followed by Gemini-2.5-Pro (3.1%, 10.1%, 25.9%), Claude-3.7-Sonnet:Thinking (3.0%, 6.0%, 18.6%), and GPT-4.1 (2.8%, 6.0%, 17.8%). The lighter proprietary variants, Gemini-2.0-Flash-Lite and the non-Thinking Claude-3.7-Sonnet, score marginally above zero. Surprisingly, open-source models such as Qwen2.5-VL-72B-Instruct 5 Figure 4: Category-specific performance and calibration of six LLMs on SPOT. Left: Kernel density estimates of each models reported confidence; all six models predominantly express very low confidence. Right: Scatter plot of mean reported confidence (see Appendix for further details) versus pass@4 for each model (color), broken down by error type (shape). The dashed diagonal marks perfect calibration. and Llama-4-Maverick, which match proprietary models on existing multi-modal benchmarks like MMMU [49] or MathVista [50], perform far worse on SPOT. As shown in Figure 3, (1) the performance gap between o3 and Llama-4-Maverick is widest on SPOT (ours) ( = 20.2 pp), and (2) SPOT is the only benchmark where Llama-4-Mavericks score collapses to near zero (0.9 %). While neither proprietary nor open-source models fully satisfy the requirements of practical deployments of error-detecting AI systems, open-source models lag far behind in domain-specific rigor and robust error-detection capabilities essential for scientific applications. New Challenging Benchmark for STEM. Figure 3 illustrates the performance of o3 on six benchmarks: MathVista [50], MMLU-Pro [51], GPQA Diamond [4], MMMU [49], HLE [52] and SPOT (recall). o3 exceeds 80% on the first four benchmarks, demonstrating robust general reasoning and code understanding. However, performance drops to roughly 20% on HLE, curated set of frontier, research-level academic questions, and remains similarly low on SPOT (21.1%). This drop in performance underscores the difficulty of spotting errors in lengthy scientific text and figures. Figure 3: Performance of o3 and Llama-4Maverick across six challenging STEM benchmarks. The short red horizontal lines mark the gap = o3 Llama-4-Maverick for each benchmark. Reasoning Models Excel at Equations but Falter on Figures The right panel of Figure 4 presents the performance of six models across each category. In the Equation/Proof category, o3 leads with 62.6% (pass@4), followed by Gemini-2.5-Pro at 36.4%, while all other models remain below 5%, underscoring o3s superior mathematical reasoning. Surprisingly, GPT-4.1 achieves 44.4% in the Figure Duplication category, outperforming Claude-3.7-Sonnet Thinking (33.3%), o3 (0%), and Gemini-2.5-Pro (0%), revealing weakness in figure analysis in reasoning models."
        },
        {
            "title": "3.2 Unreliability of Miscalibrated Models.",
            "content": "Alongside pass@4, calibration [53, 54] indicates how much we should trust models predictions. In error detection, where false positives can incur substantial time and labor, knowing when to trust model is crucial. For each error category, we assess calibration by comparing the models actual performance, measured as its average pass@4 rate, with its self-estimated confidence. For details on how the confidence is derived see Appendix C. 6 Table 3: Multi-modality ablation for 13 models: recall and pass@4 (in %) are reported as mean (std) over eight independent trials. The left panel shows each models performance with multi-modal inputs; the right panel shows performance on the text-only subset of SPOT (48 figure-independent instances), including additional unimodal LLMs (DeepSeek-R1, DeepSeek-V3, Qwen3-235B-A22B). The highest value in each column is bolded, and the second-highest is underlined. Detailed evaluation results are available in Appendix G. Multi-Modal Text-Only Models Think Recall (%) pass@4 (%) Recall (%) pass@4 (%) o3 (2025-04-16) GPT-4.1 (2025-04-14) Gemini-2.5-Pro (preview-03-25) Gemini-2.0-Flash-Lite (001) Claude-3.7-Sonnet (20250219:Think) Claude-3.7-Sonnet (20250219) DeepSeek-R1 DeepSeek-V3 (0324) Qwen3-235B-A22B Qwen2.5-VL-72B-Instruct Qwen2.5-VL-32B-Instruct Llama-4-Maverick Llama-4-Scout 34.67.1 0.50.9 13.78.6 0.40.9 2.92.3 1.92. 0.00.0 0.40.8 0.50.9 0.40.8 61.12.9 2.01.4 34.86.1 2.11.3 8.51.7 4.81.5 0.00.0 1.70.9 2.11.4 2.01.4 25.77.1 8.42.5 6.93.1 1.91.4 5.02.3 5.83. 14.83.8 1.91.1 15.46.2 4.72.2 1.11.5 0.81.0 1.62.1 56.24.2 19.82.7 17.02.8 8.01.8 17.03.1 15.22.8 38.63.3 6.72.1 38.23.1 11.22.5 3.01.2 3.51.5 5.92.5 However, Figure 4 (right) shows that confidence correlates only weakly with pass@4, and the left panel reveals that most models report very low confidence, clustering near zero. Across 498 modelinstance evaluations (83 instances six models), we observe only two cases (both from o3) of full confidence, highlighting the widespread difficulty of reliably detecting errors in scientific manuscripts. These findings demonstrate substantial variability across categories and reaffirm that current LLMs remain unreliable for scientific error detection. 3.3 Impact of Multi-Modality in Detecting Scientific Errors To isolate the impact of visual inputs, we create text-only subset of SPOT by removing all instances from the figure-duplication and any data-inconsistency category that necessitate figures for comprehension. This yields 48 instances in which errors can be detected using text alone. Table 3 compares model performance on the selected instances under multimodal and text-only conditions. The left panel reports each models accuracy on these 48 cases with figures included (extracted from the runs of Table 2); the right panel shows performance after stripping out all figures. In the text-only setting, we add three unimodal LLMs: DeepSeek-R1 [55], DeepSeek-V3 [56], and Qwen3-235B-A22B [57]. We observe two key findings. First, most models improve in recall and pass@4 when removing images, suggesting that figures usually act as distractors rather than helpful context. The exceptions are o3 and Gemini-2.5-Pro, which see modest drop without visual inputs. This indicates that they have been leveraging figures to understand the paper rather than treating them as mere auxiliary signals. Second, the divide between proprietary and open models is vast in the multi-modal setting, proprietary systems maintain substantial recall (e.g., o3 at 34.6 %, Gemini-2.5-Pro at 13.7 %) and pass@4, whereas open-source models collapse to near zero. Under text-only conditions, while proprietary systems still lead, the performance margin is comparably smaller."
        },
        {
            "title": "4 Case Study",
            "content": "We select two withdrawn manuscripts, each from mathematics and materials science, for qualitative review. domain expert evaluated each paper, either researcher with relevant publications or PhD-trained postdoc in the field. Reviewers are provided the LLM-flagged errors from o3 and Gemini 2.5 Pro alongside the official withdrawal notices. They are asked to verify whether the model has missed any benchmarked errors (i.e., true positives mislabeled as false negatives). Moreover, they are required to assess each flagged issue that falls outside our annotations to determine if any 7 presumed false positives correspond to valid flaws. We consulted the original authors to verify the disputed issues whenever reviewer remained uncertain."
        },
        {
            "title": "4.1 Mathematics",
            "content": "Petersen and Tommasi [58] studies the configuration spaces of points in algebraic varieties with multiplicative decomposition, and discusses some applications such as the cohomology of moduli stacks of hyperelliptic curves. It was withdrawn because of gap that lies in the core arguments of Theorem 1.8 and Theorem 1.13 which invalidates the bulk of the paper. Both o3 and Gemini-2.5-Pro exclusively flag issues in Section 3. Ironically, this is the only part of the manuscript not affected by the actual mathematical gap. o3 criticizes the calculation of k(M1,1, Vℓ) in Section 3.3, claiming that the use of Eichler-Shimura isomorphism is wrong without giving too much details. But there is no mistake in this part and the claim is hallucinated. It is worth noting that applying Eichler-Shimura isomorphism in similar context as the paper is exceedingly rare in internet data. The formula may also appear differently than how the Eichler-Shimura isomorphism is presented in common internet sources, whose equivalence requires genuine understanding of multiple subjects, such as variation of Hodge structures and the theory of modular forms. EichlerShimura is quoted as 1(M1,1, Vℓ) = Sℓ+2 Q(1 ℓ), but the Tate (Eisenstein) summand should be Q(1), independent of ℓ. The extra ℓ is mis-twist. Figure 5: o3s feedback on [58]. On the other hand, Gemini-2.5-Pro tackles Theorem 3.8 as shown in Figure 6. Despite doing reasonable calculations, the model makes false claim on 0(A1,1, (2, 2))(1), resembling mistakes students could make after recollecting wrong fact. The stated result 1(M2, (2, 2)) = Q(3) contradicts the Gysin-sequence derivation (Sec. 3.1), which yields 1(A2, (2, 2)) = 0, together with 0(A1,1, (2, 2))(1) = Q(3) (Sec. 3.5) and the non-vanishing/isomorphism of the Gysin map : 0(A1,1, (2, 2))(1) 2(A2), thereby implying 1(M2, (2, 2)) = 0. Figure 6: Gemini-2.5-Pros feedback on [58]."
        },
        {
            "title": "4.2 Material science",
            "content": "In our materials science case study, we select [59], which uses TfOH to synthesize covalent organic frameworks (COFs). However, it mislabels several samples in its figures. These errors are easier to spot than those in the mathematics papers and would be obvious to any attentive reader. Optimal TfOH concentration is stated as 0.0060.016 M, yet all optimisation and standard syntheses use 0.2 TfOH. Internal inconsistency invalidates the universality of the claimed optimum. Figure 7: o3s feedback on [59]. Errors pointed out by o3 exemplify failures in both long-range context comprehension and multi-hop reasoning. For instance, in Figure 7, o3 does not realize that the optimal concentration value reported by the authors (0.0060.016 M) is the concentration of the final mixture, while the the second value (0.2M) is the concentration of the acid before being added to the final mixture. This misunderstanding likely arises because the optimal concentration in the final mixture is mentioned only once, and the 5Due to space constraints, we show only excerpts of model responses and one case study per domain; for the complete results, see Appendix D. 8 explicit calculation is not shown throughout the manuscript. As result, o3, having seen references only to the concentration before mixture, fails to infer the relationship between the two values. (A) There is contradiction in the indexing of PXRD peaks for TAPPy-TFPPy-COF (Figure 6H). The peaks are initially assigned to facets including (020): TAPPy-TFPPy-COF displayed peaks [...] (B) The BET surface area for the scaled-up TFPPy-PDA-COF is reported as 1606 cm2 g1. The correct unit is m2 g1. This unit error misrepresents the surface area by factor of 10,000, constituting fundamental data-reporting mistake. Figure 8: Two of Gemini 2.5 Pros feedback on [59]. In (A) of Figure 8, Gemini 2.5 Pro seems to make \"reading\" mistake, attributing the second facet pair to TAPPy-TFPPy-COF when it in fact describes TAPPy-BPTC-COF. Notably, however, in (B), it notices potential error in the units, where certain compound was assigned surface area 10000x smaller than all the other compounds in the same family. Because the authors do not mention this extreme property of this material, we suspect that this is real typo. While not severe, this error is the only instance in which we observe an LLM identifying an unannotated but genuine error."
        },
        {
            "title": "5 Related Works",
            "content": "AI Co-Scientists Recent breakthroughs have pushed LLMs to PhD-level performance on STEM benchmarks [4], driving efforts to embed them as generators of the scientific forward pass, encompassing hypothesis generation [6], experimental planning [38], and manuscript drafting [9]. Such systems, or AI Co-Scientists [11, 15], employ agent-based pipelines that mirror the stages of scientific research. However, concurrent works often omit rigorous backward pass or verification and instead rely on LLM judges [16]. Yet, prior studies demonstrate that LLM judges may fail on complex tasks [60], allowing factual and methodological errors to remain undetected. This compromises the reliability of AI-driven research. It should be noted that verifiability has long been central to scaling AI progress: self-supervised learning employs next-token prediction as provable training objective [61]; instruction-tuning leverages LLM-generated instructionresponse pairs as reliable signals [62]; and reinforcement learning uses verifiable rewards [55] for alignment. Likewise, we posit that robust scientific verification must underpin reliable LLM-driven scientific research. Automating Scientific Verification Two research strands, fact verification and automated peer review generation, may appear related to SPOT, but each has critical limitations. Prior fact verification benchmarks [26, 27] concentrate on claims at the sentence level and rely on text inputs only to assess consistency with reference documents. Automated peer review systems draw almost entirely on computer science publications [63, 23, 24], restricting their disciplinary coverage. These approaches measure success by matching past reviews via metrics such as ROUGE [64] rather than detecting errors. They also overlook the inherent noise in peer review reports [65, 66] and seldom apply adequate quality control or validate ground truth. Our work sets apart from previous efforts by applying expert and automated validation to distill only genuine mistakes into SPOT, additionally, we package full, multimodal papers into models at inference mirroring real-world academic verifications."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce SPOT, multimodal error-detection benchmark that captures the full complexity of frontier-level scientific research. Each instance averages 12,000 text tokens and 18 images, posing significant challenge for current large language models: OpenAIs o3 and Googles Gemini 2.5 Pro achieve pass@1 scores of only 18.4 % and 7.3 %, respectively. Our expert-led case studies further show that these models fall short in long-tail domain knowledge and implicit multi-step calculations. Together with the rise of interest in AI Co-Scientists, these results highlight the need for further research in robust verification systems to ensure reliability in AI-driven research workflows."
        },
        {
            "title": "References",
            "content": "[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [3] Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, et al. R-bench: Graduate-level multi-disciplinary benchmarks for llm & mllm complex reasoning evaluation. arXiv preprint arXiv:2505.02018, 2025. [4] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [5] Kaiyue Feng, Yilun Zhao, Yixin Liu, Tianyu Yang, Chen Zhao, John Sous, and Arman Cohan. Physics: Benchmarking foundation models on university-level physics problem solving. arXiv preprint arXiv:2503.21821, 2025. [6] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024. [7] Yang Jeong Park, Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, and Ju Li. Can chatgpt be used to generate scientific hypotheses? Journal of Materiomics, 10(3):578584, 2024. [8] Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, et al. Pasa: An llm agent for comprehensive academic paper search. arXiv preprint arXiv:2501.10120, 2025. [9] Rishab Jain and Aditya Jain. Generative ai in writing research papers: new type of algorithmic bias and uncertainty in scholarly work. In Intelligent Systems Conference, pages 656669. Springer, 2024. [10] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864, 2025. [11] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. [12] José Penadés, Juraj Gottweis, Lingchen He, Jonasz Patkowski, Alexander Shurick, WeiHung Weng, Tao Tu, Anil Palepu, Artiom Myaskovsky, Annalisa Pawlosky, et al. Ai mirrors experimental science to uncover novel mechanism of gene transfer crucial to bacterial evolution. bioRxiv, pages 202502, 2025. [13] Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew White, and Philippe Schwaller. Augmenting large language models with chemistry tools. Nature Machine Intelligence, 6(5):525535, 2024. [14] Haining Pan, Nayantara Mudur, William Taranto, Maria Tikhanovskaya, Subhashini Venugopalan, Yasaman Bahri, Michael Brenner, and Eun-Ah Kim. Quantum many-body physics calculations with large language models. Communications Physics, 8(1):49, 2025. [15] DeepMind. advanced ing alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/, 2025. Accessed: 2025-05-15. gemini-powered designhttps://deepmind.google/discover/blog/ coding agent for Alphaevolve: algorithms. [16] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 10 [17] Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. Tabfact: large-scale dataset for table-based fact verification. arXiv preprint arXiv:1909.02164, 2019. [18] Giannis Bekoulis, Christina Papagiannopoulou, and Nikos Deligiannis. review on fact extraction and verification. ACM Computing Surveys (CSUR), 55(1):135, 2021. [19] Hanzhi Zhang, Sumera Anjum, Heng Fan, Weijian Zheng, Yan Huang, and Yunhe Feng. Polyfever: multilingual fact verification benchmark for hallucination detection in large language models. arXiv preprint arXiv:2503.16541, 2025. [20] Raúl Ortega and José Manuel Gómez-Pérez. Sciclaims: An end-to-end generative system for biomedical claim analysis. arXiv preprint arXiv:2503.18526, 2025. [21] Sujit Kumar, Anshul Sharma, Siddharth Hemant Khincha, Gargi Shroff, Sanasam Ranbir Singh, and Rahul Mishra. Sciclaimhunt: large dataset for evidence-based scientific claim verification. arXiv preprint arXiv:2502.10003, 2025. [22] Zachary Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, and Arvind Narayanan. Corebench: Fostering the credibility of published research through computational reproducibility agent benchmark. arXiv preprint arXiv:2409.11363, 2024. [23] Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. Nlpeer: unified resource for the computational study of peer review. arXiv preprint arXiv:2211.06651, 2022. [24] Tim Baumgärtner, Ted Briscoe, and Iryna Gurevych. Peerqa: scientific question answering dataset from peer reviews. arXiv preprint arXiv:2502.13668, 2025. [25] Irina Bejan, Artem Sokolov, and Katja Filippova. Make every example count: On the stability and utility of self-influence for learning from noisy NLP datasets. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1010710121, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.625. URL https:// aclanthology.org/2023.emnlp-main.625/. [26] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: large-scale dataset for fact extraction and VERification. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://aclanthology.org/N18-1074/. [27] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 75347550, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.609. URL https://aclanthology.org/2020.emnlp-main.609/. [28] Jialiang Lin, Jiaxin Song, Zhangping Zhou, Yidong Chen, and Xiaodong Shi. Moprd: multidisciplinary open peer review dataset. Neural Computing and Applications, 35(34): 2419124206, 2023. [29] Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, and Juho Kim. Automatically evaluating the paper reviewing capability of large language models. arXiv preprint arXiv:2502.17086, 2025. [30] OpenAI. Openai o3 and o4-mini system card. https://openai.com/index/ o3-o4-mini-system-card/, April 2025. Accessed: 2025-05-12. [31] Meta AI. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai.meta.com/blog/llama-4-multimodal-intelligence/, April 2025. Accessed: 2025-05-12. 11 [32] Delip Rao, Jonathan Young, Thomas Dietterich, and Chris Callison-Burch. Withdrarxiv: large-scale dataset for retraction study. arXiv preprint arXiv:2412.03775, 2024. [33] José Luis Ortega. Classification and analysis of pubpeer comments: How web journal club is used. Journal of the Association for Information Science and Technology, 73(5):655670, 2022. [34] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, 12 Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. [35] MAA. American Invitational Mathematics Examination AIME. In American Invitational Mathematics Examination AIME 2024, February 2024, February 2024. URL https://maa.org/ math-competitions/american-invitational-mathematics-examination-aime. [36] Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovic, Nikola Jovanovic, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934, 2025. [37] Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. [38] Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code generation from scientific papers in machine learning. arXiv preprint arXiv:2504.17192, 2025. [39] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [40] OpenAI. tiktoken: fast bpe tokeniser for use with openais models. https://github.com/ openai/tiktoken, 2025. GitHub repository; version 0.9.0 (Feb. 14, 2025); accessed May 12, 2025. [41] Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and Yang You. Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures. arXiv preprint arXiv:2406.06565, 2024. [42] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy Liang. Spoc: Search-based pseudocode to code. Advances in Neural Information Processing Systems, 32, 2019. [43] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [44] OpenAI. GPT-4.1. https://openai.com/index/gpt-4-1/, 2025. Accessed: May 15, 2025. [45] Google Cloud. Gemini 2.5 pro. https://cloud.google.com/vertex-ai/ generative-ai/docs/models/gemini/2-5-pro, 2025. Accessed: 2025-05-12. [46] Google Cloud. Gemini 2.0 Flash Lite. https://cloud.google.com/vertex-ai/ generative-ai/docs/models/gemini/2-0-flash-lite, 2025. Accessed: May 15, 2025. [47] Anthropic. Claude 3.7 Sonnet System Card. https://assets.anthropic.com/ m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf, 2025. Accessed: May 15, 2025. [48] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [49] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [50] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [51] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [52] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [53] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 13211330. PMLR, 2017. [54] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your models uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32, 2019. [55] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [56] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [57] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [58] Dan Petersen and Orsola Tommasi. Multiplicative chow-künneth decomposition and homology splitting of configuration spaces, 2024. URL https://arxiv.org/abs/2401.06455. [59] Xingyao Ye, Ruoyang Liu, Xinyu Mu, Shanshan Tao, Hao Yang, Xuejiao Gao, Shuo-Wang Yang, and Donglin Jiang. Superacid in situ protected synthesis of covalent organic frameworks. Journal of the American Chemical Society, 2025. [60] Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, and Seunghyeok Hong. Llm-as-a-judge & reward model: What they can and cannot do. arXiv preprint arXiv:2409.11239, 2024. [61] Yacine Jernite, Samuel Bowman, and David Sontag. Discourse-based objectives for fast unsupervised sentence representation learning. arXiv preprint arXiv:1705.00557, 2017. [62] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. 14 [63] Zhaolin Gao, Kianté Brantley, and Thorsten Joachims. Reviewer2: Optimizing review generation through prompt generation. arXiv preprint arXiv:2402.10886, 2024. [64] Qi Zeng, Mankeerat Sidhu, Ansel Blume, Hou Pong Chan, Lu Wang, and Heng Ji. Scientific opinion summarization: Paper meta-review generation dataset, methods, and evaluation. In Artificial Intelligence for Research and Democracy: First International Workshop, AI4Research 2024, and 4th International Workshop, DemocrAI 2024, Held in Conjunction with IJCAI 2024, Jeju, South Korea, August 5, 2024, Proceedings, page 20. Springer Nature, 2024. [65] Corinna Cortes and Neil Lawrence. Inconsistency in conference peer review: Revisiting the 2014 neurips experiment. arXiv preprint arXiv:2109.09774, 2021. [66] Tomas Bonavia and Juan Marin-Garcia. noise audit of the peer review of scientific article: wpom journal case study. WPOM-Working Papers on Operations Management, 14(2): 137166, 2023. [67] Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, et al. Michelangelo: Long context evaluations beyond haystacks via latent structure queries. arXiv preprint arXiv:2409.12640, 2024. [68] Andy Jones. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113, 2021. [69] Guijin Son, Jiwoo Hong, Hyunwoo Ko, and James Thorne. Linguistic generalizability of test-time scaling in mathematical reasoning. arXiv preprint arXiv:2502.17407, 2025. [70] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [71] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [72] Kabir Ahuja, Melanie Sclar, and Yulia Tsvetkov. Finding flawed fictions: Evaluating complex reasoning in language models via plot hole detection. arXiv preprint arXiv:2504.11900, 2025. [73] Jeehoon Park, Junyeong Park, and Philsang Yoo. Algebraic description of complex conjugation on cohomology of smooth projective hypersurface, 2024. URL https://arxiv.org/abs/ 2402.14546. [74] Reem Altuijri, Atta, Abdeltwab, and MM Abdelhamied. Impacts of low energy argon beam on enhancing the surface wettability and electrical performance of ca/pani films. ECS Journal of Solid State Science and Technology, 13(4):043017, 2024. [75] Michael Simpson. The scientific case against net zero: Falsifying the greenhouse gas hypothesis. Journal of Sustainable Development, 17(6):137157, 2024. doi: 10.5539/jsd.v17n6p137. URL https://ideas.repec.org/a/ibn/jsd123/v17y2024i6p137.html."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The abstract and introduction is written to summarize our work. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Appendix A. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] 16 Justification: This paper discusses novel benchmark without theoretical results. We provide how our confidence metric is derived in Appendix C. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide details on the parsing process in Section 2, and further details on the prompts and generation configurations in Appendix F. The benchmark and codes are also provided through supplementary materials. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code 17 Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The experiments are fully reproducible using the provided code. Minor variations may arise from proprietary model availability or inherent stochasticity, but these do not affect our overall conclusions. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We do not train our own models, but prompts and generation configurations for inference are provided in Appendix F. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Table 2, Table 3 all provide means and standard deviation of multiple independent trials and bootstrapping. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. 18 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Our experiments are conducted entirely through APIs. We use the official provider for each model if available; if not, we use OpenRouter6. Total API expenditures amount to approximately $5,000. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We dont see negative societal impact in this work. Guidelines: 6https://openrouter.ai/ 19 The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We do not train custom models. Our sharing policy for the SPOT dataset varies with the copyright status of each original paper; see Appendix for details. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All assets used in the paper are mentioned through footnotes or references. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. 20 For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: See Section 2 and Appendix E. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: An image of the annotation platform is available in Appendix E. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. 21 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [No] Justification: LLM services such as ChatGPT are used to draft and edit the manuscript and to generate code for visualizations. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        },
        {
            "title": "A Limitations",
            "content": "Benchmark Coverage By prioritizing copyright compliance(Appendix E), contamination prevention, and annotation accuracy, SPOT remains relatively modest in size. We leave the expansion of this effort to create larger, more diverse benchmarks that span additional scientific disciplines and error categories to future works. Annotation Validity and Evaluation Protocol All errors in SPOT are verified by explicit author acknowledgments or retraction notices, but the complexity of scientific manuscripts means some true errors may be unannotated. Our case studies reveal that false negatives can arise from the following cases: 1. The authors note contains an error location that does not sufficiently cover all the affected results. 2. There exist smaller errors unrelated to the main technical error in the preprint. Conversely, false positives may occur when: 1. An LLM correctly points out theorem that contains an error, but the content in the LLMs response is still irrelevant. We therefore recommend secondary expert review, particularly for domains with complex logical dependencies or deep specialization, to validate and refine model-flagged errors."
        },
        {
            "title": "B Additional Analysis",
            "content": "B.1 Impact of Context Length in Detecting Scientific Errors In Table 2, we evaluate each model on complete manuscripts, which can span up to 140,000 characters and 90 figures. However, LLMs still struggle to synthesize long-context information [67]; to isolate the effect of context length on error detection, we extract the page containing the ground-truth error and rerun the same detection prompt on this shorter segment. Comparing the full-paper and segment-only settings decouples long-context processing from core error-detection ability. For this ablation, we conduct experiments on subset of 36 instances, excluding the Equation/Proof categorymathematical papers often rely on global notation and prior results, making single sections insufficientand we omit any errors that span multiple sections. Figure 9: Impact of context length on error detection. Each bar shows = (segment-only - fullpaper) for precision and recall across five models (o3, Gemini 2.5 Pro, Claude 3.7-Sonnet:Thinking, Qwen 2.5-VL-72B-Instruct, Llama-4-Maverick). Figure 9 plots = (segment-only full-paper) for precision and recall. Gemini-2.5-Pro leads with gains of +4.7 precision and +13.5 recall. o3 follows at +3.7/+8.5, then Claude-3.7-Sonnet at +2.2/+2.1, and Llama-4-Maverick at +1.6/+2.2showing that long-context processing often masks their true error-detection performance. o3s smaller gains reflect the removal of Equation/Proof cases, its original strength. Qwen2.5-VL-72B-Instruct shows almost no change (0.4 precision, 0.1 recall), indicating fundamental limit in its error-detection capability rather than context-length issue. B.2 Impact of Test-Time Scaling in Detecting Scientific Errors Test-time scaling involves adjusting the inference budget [68], such as the depth of reasoning or number of solution paths explored [69], to boost model performance on complex tasks. This approach is widely adopted in STEM and reasoning benchmarks [70], where allocating more computational 23 effort to inference has been shown to yield higher performance. We use OpenAIs o4-mini series [30] for our experiments and vary the reasoning effort parameter across low, medium, and high settings.7 In Figure 10, we see that o4-minis error-detection performance increases almost linearly with higher reasoning effort, demonstrating that scaling computation at test time effectively boosts accuracy. This finding is consistent with how specialized Thinking modes (e.g., Claude 3.7-Sonnet:Thinking VSClaude 3.7-Sonnet) and reasoning-trained models (DeepSeek-R1 vs. DeepSeek-V3) deliver similar boosts, also in line with recent error-detection literature [72]. Estimating Confidence for pass@K Given papers with ground-truth error sets G1, . . . , GN , we define the pass@K metric as Figure 10: Performance of o4-mini with varying reasoning effort. Performance is reported from three independent trials. pass@K = 1 i=1 Gi (cid:80)N (cid:88) (cid:88) i= gGi (cid:104) 1 (cid:105) {1, . . . , K} : pi[s] , (3) where pi[s] denotes the set of errors predicted in the sth run for paper i. This captures the fraction of all ground-truth errors detected in at least one of independent attempts. C.1 Unbiased Per-Error Confidence To assign each ground-truth error Gi confidence score, we perform independent runs (here = 8) and let ci,g be the number of runs in which is detected. The probability that all fresh attempts miss is (cid:1) , (cid:0)nci,g (cid:1) (cid:0) (4) So one minus this quantity is the probability of 1 success [43]. Hence the unbiased estimator for the pass@K probability of error is ˆpi,g = 1 (cid:1) . (cid:0)nci,g (cid:1) (cid:0) C.2 Aggregating Confidence and Calibration We then aggregate these per-error confidences into an overall self-estimated confidence: Confidence = 1 i=1 Gi (cid:80)N (cid:88) (cid:88) i=1 gGi ˆpi,g. (5) (6) 7While recent work has demonstrated similar budget controlling strategies for open models [71], the fullsize MLLMs (Llama-4-Maverick totaling 402B parameters) were too large to host for multi-thousand-token generations."
        },
        {
            "title": "D Case Study",
            "content": "D.1 Mathematics We selected [58] and [73], both in the area of algebraic geometry in pure mathematics. [58] studies the configuration spaces of points in algebraic varieties with multiplicative decomposition, and discusses some applications such as the cohomology of moduli stacks of hyperelliptic curves. It was withdrawn because of gap that lies in the core arguments of Theorem 1.8 and Theorem 1.13. On the other hand, [73] studies projective hypersurfaces with real coefficients and presents an explicit description of the complex conjugation of singular cohomology in terms of algebraic de Rham cohomology. The authors withdrawal note on arXiv suggests that hidden assumption on Hodge numbers was accidentally introduced in the early set-up of section 2.2, thus making Theorem 2.3 only valid under the additional assumption. We run our evaluation using OpenAI o3 and Gemini 2.5 Pro over the two pre-prints, once for each combination. Section 2.3, proof of Theorem 2.3: They set C(cid:0)φC (Uk), φC (Uk)(cid:1) = (cid:90) φC (Uk) φC (Uk) and claim it equals the nonzero quantity Uk,γ; however, for even n, φC (Uk) has odd degree 1, so φC (Uk) φC (Uk) = 0 identically, making the equality impossible and invalidating Lemma 2.5 and Theorem 2.3. Example 2.4: Using the erroneous formula without complex conjugation yields µ = 0, which would force φC ([1]) = 0, contradicting that [1] represents the holomorphic 1-form. This example exposes the same flaw as in the main proof. Theorem 2.3 (implicit assumption): Formula (2.10) divides by Uk,γ, but the proof provides no argument that Uk,γ = 0. In fact, for odd-degree forms Uk,γ vanishes, so the construction of fji(x) is undefined. Figure 11: o3s initial feedback on [73]. Section 2.5: Corollary 2.7 incorrectly describes the R-span of the algebraic representatives for the real prim(X, R). The basis elements given, [ωji ] and [ primitive cohomology n1 1 ωji ], correspond under φC to ℜ(ωk) and ℜ(ωk). The correct basis for the real cohomology H(X, R) should correspond to ℜ(ωk) and ℑ(ωk), where ωk = φC (Uk). This makes the explicit description of (φ prim(X, R)(cid:1) scientifically invalid. )(cid:0)H n1 Example 2.8: This example illustrates Corollary 2.7 for elliptic curves and thus inherits the same 1 R[. . . ] describes an incorrect subspace related only to the flaw. The stated R-span R[. . . ] + real part of the basis forms, not the full real cohomology 1(X, R). Figure 12: Gemini-2.5-Pros initial feedback on [73]. Since we have already analyzed the LLM responses for [58] in Section 4, here we only describe some additional analysis on [73]. Our first attempt at running our pipeline introduced many OCR errors, such as missing conjugations in an integral. Both o3 and Gemini-2.5-Pro reacted to different OCR errors and identified them as mistakes (Figures 11 and 12). After an improvement (inclusion of GPT-4.1 mentioned in Section 2), surprisingly, o3 claimed that there are no errors in [73]. As to Gemini 2.5 Pro, it proposed several potential mistakes (Figure 13). First, it argued that in the statement of Theorem 2.3, the existence of polynomial gji such that rfji ,gji = 0\" was never justified. Although this may be fair writing feedback, this does not constitute an error. Aside from another OCR problem where the LLM saw an incorrect mathematical equation, another argument it made was that in Example 2.8, the basis of the cohomology on the right-hand side was unconventional 25 and [ω] + (i[ω]) was better. This was made out of context, and the LLM might have forgotten that this example was an application of Corollary 2.7. Section 2.2, Theorem 2.3: The theorem requires the existence of polynomial gji such that (hence fji / Jac(G)). However, the paper never justifies that the chosen basis elements rfji,gji = 0 Uk = yifji (relative to Jac(S)) can always be represented by polynomials fji lying outside Jac(G). This hidden assumption is crucial for formula (2.10) to be well-defined and for the theorem to hold. Section 2.2, Example 2.4: The text claims the constant µ is proportional to (cid:90) (cid:0) (cid:90) ω(cid:1) (cid:0) (cid:90) ω(cid:1) (cid:0) (cid:90) ω(cid:1) (cid:0) ω(cid:1). γ γ2 γ2 γ1 But by Definition 2.2 and the pairing C(ω, ω), it should be (cid:90) (cid:0) (cid:90) ω(cid:1) (cid:0) (cid:90) ω(cid:1) (cid:0) (cid:90) ω(cid:1) (cid:0) ω(cid:1), γ1 γ2 γ γ1 the presented formula omits the complex-conjugate terms, indicating calculation error. Section 2.4: For odd n, the paper defines Uk,γ = X . But since Uk,γ comes from the pairing C(ϕC (Uk), ϕC (Uk)) and is complex, the correct expression uses the conjugate transpose: Uk,γ = XT , so using is mathematically incorrect. Section 2.5, Example 2.8: The basis for the real cohomology is written as 1 R(cid:2)(1 + µ yd(x))/2(cid:3), R(cid:2)(1 + µ yd(x))/2(cid:3) + which is unconventional and obscures the structure. clearer description is [ω] + (cid:0)i[ω](cid:1), explicitly showing the real subspace spanned by [ω] and its i-multiple. Figure 13: Gemini-2.5-Pros regenerated feedback on [73]. 26 D.2 Material science We select [59] and [74], two recent materials-science studies. [59] investigates the synthesis of covalent organic frameworks (COFs), class of porous materials with diverse applications, using novel superacid-catalyzed process that enables the fabrication of new types of COF architectures. Several errors were pointed out about this article, mostly due to mislabeling COFs either in the text or in the figures, but none of them impact the conclusions of the article. [74] proposes method to increase the hydrophobicity and electrical properties of certain types of films by incidence with low-energy argon beams. This work had wrong caption, claiming to have used different beam intensity on certain panel when that panel was at different magnification. Both these errors are easier to spot than the ones found in the mathematics articles. While some of the errors in [59] require some domain knowledge in COFs, most of the other errors can be spotted by paying close attention to the text and figures. Section 2.2, Model Reactions (Figures 1CG): Quantitative yields of individual species are derived from single UV absorbance at 240 nm without extinction-coefficient calibration or spectral deconvolution, even though all species overlap strongly in this region. The reported 92 %, 82 %, etc. compositions are therefore not experimentally supported. Section 2.2 / Figure 1H: Assumes acetals are unaffected by NaHCO3 neutralization; no control provided. If acetals hydrolyze under basic conditions, the calculated protected fractions are invalid, undermining the mechanistic interpretation. Section 2.3: Conclusion that barriers < 20 kcal mol1 imply full reversibility at room temperature ignores solvent and entropic contributions; 19 kcal mol1 barrier can correspond to multi-hour timescales. The stated kinetic implication is not justified by the DFT data. Abstract & Section 2.5: Optimal TfOH concentration is stated as 0.0060.016 M, yet all optimization and standard syntheses use 0.2 TfOH. Internal inconsistency invalidates the universality of the claimed optimum. Section 2.4 / Figure 2: Higher PXRD peak counts are used as evidence of greater crystallinity, but raw intensity is instrumentand packing-dependent. Without normalization or an internal standard, counts cannot be compared between samples, so the claim of superior crystallinity is unsupported. Section 2.7, nine new COFs claim: Authors conclude AcOH cannot give these COFs after testing only single AcOH condition per monomer pair. Lack of systematic screening means the impossibility claim is unsubstantiated. Section 2.11, Photocatalysis: Performance deemed exceptional based solely on DPBF bleaching time, with no quantum yield, turnover number, or photon-normalized rate. Comparisons to literature therefore lack common metric, invalidating the superiority claim. Figure 14: o3s regenerated feedback on [59]. Most of the errors OpenAIs o3 pointed out on [59] were on the experimental setup and the conclusions that the authors derived from them. Although the specific points raised by o3 seem to be plausible mistakes, all of them either stem from misunderstandings by o3 or from ignoring information. For example in Figure 14, the claim that the UV-derived yields in Section 2.2 (Figures 1CG) lack support is hallucination: the authors validate those compositions with 1H NMR spectroscopy, which produces results similar to the UV measurements, so it is incorrect to argue that the yields are unsupported. In Section 2.3, o3 contends that barriers below 20 kcal mol1 cannot guarantee full reversibility at room temperature. While entropic factors do affect reaction rates, barriers under 20 kcal mol1 are generally regarded as low enough for reversibility, and multi-hour timescales still allow dynamic equilibrium. This error likely stems from o3s unfamiliarity with practical conventions in kinetic chemistry. Finally, for photocatalysis performance (Section 2.11), o3 questions the exceptional activity because no quantum yields or turnover numbers are provided. However, o3 does not have access to the supplementary information and thus is hallucinating that no DPBF bleaching comparisons exist, Table S10 demonstrates that these COFs outperform those in the literature. Gemini 2.5 Pro also misunderstand the authors, in it feedback regarding Section 2.7 (Figure 4C) is related to the fact that the authors claim that certain reagent is not suitable for the production of 27 Section 2.3 and Figure 1I (related to Section 2.2(2)): The DFT-calculated relative energy of the acetal product P1 (+7.4 kcal/mol compared to benzaldehyde) indicates it is thermodynamically highly unfavorable, directly contradicting the experimental 81 % yield reported in Section 2.2(2), which would require negative (approx. 0.85 kcal/mol). This discrepancy undermines the validity of the DFT calculations as an explanation for the experimentally observed acetal protection. Section 2.5(1) and Figures 3A, 3C, 3E: The text states that low-boiling point solvents (MeOH, EtOH, n-propanol) are unsuitable for reactions at 120 due to their boiling points being below this temperature. However, Figure 3 A,C,E present PXRD data for COFs synthesized in these solvents, presumably at 120 C. The manuscript fails to specify whether these reactions were performed in sealed vessels. If not, solvent evaporation would lead to uncontrolled and non-reproducible reaction conditions, invalidating the results obtained with these specific solvents. Section 2.6: The BET surface area for the scaled-up TFPPy-PDA-COF is reported as 1606 cm2 g1. The correct unit is m2/g. This unit error misrepresents the surface area value by factor of 10,000 and is fundamental error in data reporting. Abstract; Introductory paragraph; Section 2.7; Figure 4C: The paper claims nine new iminelinked frameworks, but Figure 4C highlights ten COFs (#5, #9, #13, #14, #15, #18, #23, #24, #25, #28) as newly synthesized or inaccessible by traditional methods, creating numerical inconsistency. Unnumbered paragraph on PXRD for Figure 6H: There is contradiction in the indexing of PXRD peaks for TAPPy-TFPPy-COF (Figure 6H). The peaks are initially assigned to facets including (020): TAPPy-TFPPy-COF displayed peaks at 5.00, 6.90, 10.08, 15.24, and 23.22...assigned to the (110), (020), (220), (330), and (001) facets.... subsequent concluding sentence for this group of COFs states related peaks were attributed to the (100), (110), (220), (330), and (001) facets.... The differing indices for the second peak ((020) vs. (100)) for the same data create ambiguity and undermine the reliability of the structural characterization from PXRD for this COF. Figure 15: Gemini-2.5-Pros regenerated feedback on [59]. COF and they show how it does not produce such good results, because of this Gemini claims that the reagent can actually be used because the authors used it in figure. Contrary to o3, it does find real mistake by noticing that one of the characterizations of certain COF contained the wrong unit, making the measurement 10000 times smaller. Figure 16, are feedbacks of o3 on [74]. Here, it seems like many of o3s flagged issues arise from misinterpretation rather than true errors. Its EDX objection ignores that the authors corroborate composition changes with complementary measurements and that small weight-percent shifts demand statistical treatment. Regarding contact angle, o3 uses diiodomethane data instead of water, miscomputing Wa. The supposed OwensWendt inconsistency stems from misunderstanding that two liquids yield unique solution per fluence, so the reported parallel trends do not imply mathematical error. Finally, the potential barrier formula comes from literature conventions, and Figure 11 presents related quantity rather than misapplying the sign. In Figure 17 we observe multiple errors where Gemini-2.5-Pro fails to identify visual inputs correctly; for instance, it claims that Tables and II referenced in the discussion of Figures 79 are missing, even though those tables are present, indicating it has failed to identify the context. Additionally, in Figure 10, it misreads log-scaled plot as linear and thereby hallucinates values. D.3 Environmental Science We selected [75] in environmental science, the paper conductsa comprehensive evaluation of human contributions to atmospheric CO2. In data-driven analysis, [75] argues that human-induced CO2 emissions are negligible compared to natural sources, thereby questioning the validity of the Greenhouse Gas Hypothesis. However, the original version of the paper contains miscalculation in Section 3.1.2, Measurement of Infrared Absorption of the Earths Atmosphere. Specifically, the author incorrectly applies Equation (3) from the IPCC: = 5.35 ln by using the base-10 logarithm (log10) instead of the natural logarithm (ln), leading to erroneous numerical values. Regarding the miscalculation, in Figure 18, o3 correctly locates the target error: log10 instead of the natural log, reducing by 2.3 and yielding spurious warming of 0.640.81 for CO2 doubling. (cid:16) Ct C0 (cid:17) 28 Methods SRIM simulation / Figure 2: Authors discuss ion penetration up to 1 µm for 4 keV Ar+, whereas SRIM and the plotted axis (01000 Å) show stopping range below 20 nm. Conclusions about bulk (µm-scale) modification rest on an impossible depth estimate. Results EDX (Figures 3 & 4): Claim that irradiation changed elemental composition is based on C, O, shifts under 2 wt %, well inside normal EDX error; no statistics provided, so the asserted chemical change is unsupported. Results Contact angle / Work of adhesion (Eq. [1], Figures 7 & 8): Reported Wa values do not satisfy Wa = γℓ(1 + cos θ); for example, θ = 62.1 for water gives 106 mJ m2, but the table lists 74.2 mJ m2. All subsequent surface-energy conclusions use these erroneous numbers. Results Surface free energy (Figure 9): OwensWendt requires two liquids to solve for γd and γp . With only two liquids the solution at each fluence is unique, yet the authors report independent trends for both components, indicating mathematical inconsistency. Results Electrical conductivity (Figure 10, text): Two mutually contradictory pristine conductivities are given (3.8 107 cm1 vs. 1.1 108 cm1). The inconsistency invalidates the reported enhancements due to irradiation. Results Potential barrier calculation (Figure 11, Eq. [4]): Formula Wm = 4kBT /m sets an upper magnitude of 0.104 eV at 300 K, yet values up to 1.07 eV are reported and the sign is ignored; the dielectric analysis is therefore mathematically impossible. Figure 16: o3s regenerated feedback on [74]. Correct use of ln gives 1.51.9 C, highlighting mismatch between the scale of results and the correct calculation basis. However, its subsequent claim that this error invalidates the assertion of negligible future warming seems overstated. As the authors acknowledge, projected temperature increases remain modest. In short, the magnitude of these miscalculations is insufficient to overturn the papers broader argument about limited warming. On the other hand, Gemini2.5 fails to point out the specific error."
        },
        {
            "title": "E Additional Details on SPOT",
            "content": "License & Copyright SPOT comprises 83 manuscripts published across 28 venues (including arXiv). Of these, 62 (74.7 %) are openly accessible under CC license; we publicly share our fully preprocessed versions via the Hugging Face Hub. The remaining 21 (25.3 %) are paywalled, so we do not redistribute them directly. Organizations with institutional access to Springer Nature or Elsevier can apply our preprocessing pipeline to generate their own versions. Date To minimize contamination against parametric knowledge [25], we aim to include only papers published from 2024 onward. As Figure 19 shows, the bulk of our corpus dates to 2024, with ten papers from 2025 and three that originally appeared before 2023. Those three early manuscripts passed our automated filters because revisions were submitted after 2024; we retained them since their first error notices appeared in March 2024, minimizing any chance that models were exposed to the original withdrawal details during training. Annotation We use human annotators in Section 2 during the benchmark creation process. Details on the annotator guideline are available in Figure 20, sample image of the platform in Figure 21."
        },
        {
            "title": "F Additional Details on Evaluation",
            "content": "Evaluation consists of two phases. In the first phase, the target LLM is prompted to identify potential errors in each paper using our Generation Prompt. In the second phase, we employ GPT-4.1 to align and compare the models candidates against the ground-truth annotations with our Evaluation Prompt. In the remainder of this section, we specify details on generation configurations and present the full text of both prompts. 29 Figure 2a: The text states SRIM simulation data is shown for depths of 0 to 1 µm, but the x-axis of Figure 2a (Target Depth) only extends to 1000 Å (0.1 µm). This misrepresents the range of simulation data presented visually. Results and Discussion Contact Angle, Work of Adhesion, Surface Free Energy (Figures 7, 8, 9): Table (containing contact angle and work of adhesion data) and Table II (containing surface free energy data) are referenced but not provided. This prevents independent verification of the data plotted in Figures 79 and the calculations derived from them, undermining the scientific validity of these results. Results and Discussion Work of Adhesion (Figure 8): The work of adhesion Wa values reported in the text (and presumably plotted in Figure 8) are inconsistent with calculations based on contact angle data from Figure 7 and standard liquid surface tension values (e.g., water at 4 1014 ions/cm2: reported Wa = 74.2 mJ/m2, calculated Wa 106.4 mJ/m2) . The liquid surface tension values ( γℓ) used for calculations are not stated, and the discrepancy suggests significant errors in calculation or data, invalidating the reported Wa values. Results and Discussion Figure 10 and related text: AC conductivity σac values at 50 Hz reported in the text for irradiated samples differ significantly (e.g. for 4 1014 ionscm2: text 3.8 107 cm1 vs. Fig. 10 data 3.74 108 cm1) in summary paragraph preceding the electrical characterization. This contradiction undermines the baseline characterization of the material. Results and Discussion Electrical Properties: The AC conductivity σac for unirradiated (pristine) CA/PANI at 50 Hz is reported inconsistently: as 1.1 108 cm1 in the Figure 10 discussion (matching the figure) and as 3.8 107 cm1 in preceding summary paragraph. Results and Discussion Figure 11 and Equation [4]: The exponent for calculating Wm is derived from narrow low-frequency range in Figure 11 (ln ω 5.86.5), which is not justified and may not be representative of the model used. The validity of conclusions based on Wm is therefore questionable Results and Discussion Equation [4]: The formula Wm = 4kBT used for calculating the maximum potential barrier is unconventional for hopping conduction models, and its scientific basis or appropriateness for this system is not established via the provided citation [40] or derivation. Using an potentially incorrect or unsubstantiated formula invalidates the calculated Wm values. Results and Discussion Calculation of Wm: The text states that the estimated Wm decreased from 0.116 eV. . . to 1.03 eV. However, the authors reported values (0.116, 0.11, 1.07, 1.03) eV show non-monotonic behavior (slight decrease, then large increase, then slight decrease). This is misrepresentation of their calculated results. Figure 17: Gemini-2.5-Pros regenerated feedback on [74]. F.1 Generation Configurations For each model, we adopt the providers recommended parameters when available; otherwise, we use sampling temperature of 0.6, top-p of 0.95, repetition penalty of 1.0, and enforce minimum of 8 and maximum of 8192 tokens. 30 Section 3.1.1 / Table 1: Temperature predictions obtained from = ρR are tautological because ρ = RT by definition; using measured , , and ρ always reproduces the measured . This provides no evidence against the greenhouse effect yet is used as the cornerstone of the papers conclusions. Section 3.1.2 (Equations 34 and Figure 1): The forcing equation is applied with log10 instead of the natural log, reducing by 2.3 and yielding spurious warming of 0.640.81 for CO2 doubling. Correct use of ln gives 1.51.9 C, invalidating the claim that future warming is negligible. Section 3.1.2 (Climate-sensitivity discussion): Derived ECS 0.5 ignores water-vapour, lapserate, and cloud feedbacks without justification; hence the stated upper bound on warming lacks the physical components known to amplify CO2 forcing. Section 3.1.4 (Residence time of CO2): Paper conflates 35 yr molecular turnover time with the multi-century adjustment time of concentration perturbation, leading to the unsupported claim that anthropogenic CO2 quickly disappears and is climatically irrelevant. Section 3.1.4 (Anthropogenic share argument): Using the fact that anthropogenic emissions are 4% of gross annual flux to assert they cannot raise atmospheric CO2 ignores that natural sources and sinks are nearly balanced; failure to perform mass-balance analysis renders the conclusion scientifically invalid. Section 3.1 (General claim of no greenhouse effect): Assertion that adiabatic compression or thermal conductivity alone explains the 33 surfacespace temperature difference disregards radiativetransfer energy balance and provides no quantitative mechanism, contradicting well-established physics without offering an alternative that reproduces observed fluxes. Figure 18: o3s regenerated feedback on [75]. Figure 19: Publication dates against first error-notice dates for the 83 manuscripts. Each point denotes one paper; blue markers note papers published in 2024, while red markers are those otherwise. 31 lightweight Streamlit app for labeling errors discussed on PubPeer or papers withdrawn from arXiv. Contributors review randomly selected papers, answer guided questions, and append their work to annotations.csv."
        },
        {
            "title": "Getting Started\nPrerequisites",
            "content": "Python 3.8 Streamlit pandas Installation # 1 Clone the repo git clone https://github.com/guijinSON/ai4s_r2.git cd ai4s_r2 # 2 Install dependencies pip install streamlit pandas Dataset retracted_machine_filtered_final.csv ships with the repositoryno additional download required. Usage streamlit run streamlit_sample.py 1. Shuffle Sample loads random paper. 2. Complete the six annotation questions in the right panel. 3. Click Save Annotation to append to annotations.csv. 4. Repeat until 35 rows are completed. 5. Click Submit to send annotations.csv to the maintainer. Figure 20: Guideline provided to annotators. F.2 Prompts"
        },
        {
            "title": "Generation Prompt",
            "content": "You are scientific-rigor auditor. You will receive the parsed contents of research paper. Your job is to identify only those errors or flaws that directly undermine the scientific validity of the papers methods, analyses, or conclusions. Your sole focus is identifying flaws, such as errors in experimental design, data integrity, calculations, statistical inference, or reproducibility, that directly call into question the validity of specific claim, paragraph, or the paper. Do not report issues purely presentational, rhetorical, stylistic, or related to citation practices. After youve done detailed walkthrough of the paper, output exactly in this formatno extra keys or commentary: <analysis> {detailed walk-through of how you checked each section/figure and why you flagged (or did not flag) any flaw} </analysis> <response> { \"has_error\": <true false>, \"errors\": [ \"location\": \"Section 2.1\", \"description\": \"Claim that all are is ...\" { }, { } ] } </response> flaw is. \"location\": \"Figure 3\", \"description\": \"X-Axis labeled Time (s) but units ...\" // ...more entries... - Do not include other keys or prose outside these two tagged blocks. - Do not report stylistic or citation issues. - Be as precise as possible about where (section ID or figure/table) and what the scientific - Each description of the errors must be rooted in scientific rationale explaining why they are wrong (not how they could be improved). Begin your analysis now. Figure 21: Example image of annotation platform."
        },
        {
            "title": "Evaluation Prompt",
            "content": "You are an expert LLM-as-a-Judge. You will receive JSON object with two arrays: 1. \"annotations\": the ground-truth errors (each has \"location\" and \"description\"). 2. \"predictions\": the models reported errors (same format). Task 1. Compare each prediction against each annotation. 2. match occurs only when both \"location\" and \"description\" are identical. 3. Your output should be generated in the following format: <analysis> Analysis and comparison of each prediction and annotation. </analysis> <response> { \"matches\": [ { \"location\": the location of the matched object, which should be based on the annotated location, \"description\": your explanation on why you think it is match. }, { \"location\": ... , \"description\": ... }, ] } </response> Be rigorous in considering matches; the location may be slightly differently named, but the description must match overall."
        },
        {
            "title": "G Detailed Results",
            "content": "1. In Table 4 to 13 we present detailed results of each model from Table 2. 2. In Table 14 to 26 we present detailed results of the text-only evaluation from Table 3. Table 4: Mean and standard deviation of pass@K for o3 (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluations results for Table 2. Error Category Paper Category Category pass@ pass@2 pass@4 Category pass@1 pass@2 pass@ Data Inconsistency Equation / proof Experiment setup Figure duplication Reagent identity Statistical reporting 13.18.3 33.63.8 0.00.0 0.00.0 22.025.1 45.717.2 19.47.0 51.66.1 0.00.0 0.00.0 40.725.5 62.817.9 25.74.3 Biology 67.53.7 Chemistry 0.00.0 Computer Science 0.00.0 Engineering 62.710.8 Environmental Science 88.412.5 Materials Science Mathematics Medicine Multidisciplinary Physics 5.15.8 0.00.0 21.011.5 0.00.0 5.112.0 14.26.0 34.35.3 0.00.0 20.00.0 33.720. 9.45.9 0.00.0 36.112.1 0.00.0 10.715.6 16.70.0 53.36.0 0.00.0 20.00.0 56.317.6 14.52.5 0.00.0 55.99.5 0.00.0 22.115.8 16.70.0 67.62.5 0.00.0 20.00.0 79.07.1 Table 5: Mean and standard deviation of pass@K for GPT-4.1 (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluations results for Table 2. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@1 pass@ pass@4 Data Inconsistency Equation / proof Experiment setup Figure duplication Reagent identity Statistical reporting 6.45.6 0.41.0 0.00.0 16.34.9 4.511.4 0.00.0 11.46.6 0.81.3 0.00.0 27.65.7 8.614.6 0.00.0 19.26.1 Biology 1.51.5 Chemistry 0.00.0 Computer Science 41.15.6 Engineering 16.516.7 Environmental Science 0.00.0 Materials Science Mathematics Medicine Multidisciplinary Physics 21.46.5 2.15.5 0.00.0 12.921.9 0.00.0 14.69.8 0.00.0 5.916.1 12.78.4 0.00.0 34.87.5 4.07.1 0.00.0 23.124.9 0.00.0 26.811.3 0.00.0 11.320.9 22.98.7 0.00.0 48.97.3 8.28.3 0.00.0 39.320.5 0.00.0 41.59.4 0.00.0 25.225.0 36.47.4 0.00.0 Table 6: Mean and standard deviation of pass@K for Gemini-2.5-Pro (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluations results for Table 2. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@ pass@2 pass@4 2.03.4 8.68.3 8.27.1 0.00.0 8.314.4 3.97.1 11.38.5 0.00.0 11.25.9 14.614.5 3.94.4 15.59.5 16.79.7 0.00.0 15.616.6 7.58.3 21.19.7 0.00.0 20.37.7 25.714.9 7.65.1 25.98.7 31.711.5 0.00.0 26.413.6 13.06.9 38.08.6 0.00.0 32.88.8 39.712.8 Data Inconsistency Equation / proof Experiment setup Figure duplication Reagent identity Statistical reporting 12.67.0 11.87.0 0.00.0 1.52.6 0.00.0 15.224.3 22.37.6 21.97.9 0.00.0 2.83.4 0.00.0 31.631.5 36.56.9 Biology 38.47.3 Chemistry 0.00.0 5.53.9 0.00.0 57.732.0 Materials Science Computer Science Engineering Environmental Science Mathematics Medicine Multidisciplinary Physics Table 7: Mean and standard deviation of pass@K for Gemini-2.0-Flash-Lite-001 (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluations results for Table 2. Error Category Paper Category Category pass@1 pass@ pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Figure duplication Reagent identity Statistical reporting 1.83.1 0.00.0 0.00.0 3.22.9 3.910.7 0.00.0 3.54.0 0.00.0 0.00.0 6.33.7 8.714.7 0.00.0 7.04.7 Biology 0.00.0 Chemistry 0.00.0 Computer Science 12.94.2 Engineering 16.916.7 Environmental Science 0.00.0 Materials Science Mathematics Medicine Multidisciplinary Physics 3.73.8 2.15.5 0.00.0 0.00.0 0.00.0 2.15.5 0.00.0 3.18.2 3.74.8 0.00.0 7.64.9 4.27.2 0.00.0 0.00.0 0.00.0 4.17.2 0.00.0 6.611.0 7.26.3 0.00. 15.45.7 8.18.3 0.00.0 0.00.0 0.00.0 8.38.3 0.00.0 12.612.5 14.97.4 0.00.0 Table 8: Mean and standard deviation of pass@K for Claude-3.7-Sonnet:Thinking (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluations results for Table 2. Error Category Paper Category Category pass@ pass@2 pass@4 Category pass@1 pass@2 pass@ Data Inconsistency Equation / proof Experiment setup Figure duplication Reagent identity Statistical reporting 8.04.2 0.81.3 0.00.0 10.63.5 3.910.8 3.18.3 15.75.8 1.61.8 0.00.0 19.74.9 7.513.9 6.110.7 29.36.9 Biology 3.02.0 Chemistry 0.00.0 Computer Science 34.96.0 Engineering 16.816.7 Environmental Science 12.512.5 Materials Science Mathematics Medicine Multidisciplinary Physics 13.47.3 2.15.5 0.00.0 6.216.5 16.423.2 10.411.4 0.61.6 6.210.8 10.010.0 0.00. 23.99.9 4.17.2 0.00.0 12.021.4 33.129.8 20.814.3 1.32.2 12.114.2 19.213.2 0.00.0 41.210.6 8.38.3 0.00.0 24.825.0 59.829.3 38.414.5 2.52.5 24.916.4 35.814.9 0.00.0 Table 9: Mean and standard deviation of pass@K for Claude-3.7-Sonnet (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluations results for Table 2. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@1 pass@ pass@4 Data Inconsistency Equation / proof Experiment setup Figure duplication Reagent identity Statistical reporting 7.06.2 0.41.0 0.00.0 8.84.8 8.614.6 0.00.0 13.67.7 0.71.3 0.00.0 15.54.8 15.416.6 0.00.0 25.48.1 Biology 1.51.5 Chemistry 0.00.0 Computer Science 25.14.0 Engineering 26.113.8 Environmental Science 0.00.0 Materials Science Mathematics Medicine Multidisciplinary Physics 35 10.65.3 4.27.2 0.00.0 0.00.0 3.910.8 8.28.3 0.00.0 3.48.5 13.811.2 0.00.0 17.85.5 8.29.3 0.00.0 0.00.0 7.513.9 15.910.2 0.00.0 6.410.9 25.412.7 0.00.0 27.35.0 16.410.9 0.00.0 0.00.0 16.816.7 30.111.2 0.00.0 12.812.5 42.610.9 0.00.0 Table 10: Mean and standard deviation of pass@K for Qwen2.5-VL-72B-instruct (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluations results for Table 2. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Figure duplication Reagent identity Statistical reporting 0.41.7 0.00.0 0.00.0 1.01.6 0.00.0 0.00.0 0.92.4 0.00.0 0.00.0 1.92.2 0.00.0 0.00. 1.83.1 Biology 0.00.0 Chemistry 0.00.0 Computer Science 3.93.0 0.00.0 0.00.0 Materials Science Engineering Environmental Science Mathematics Medicine Multidisciplinary Physics 1.53.1 0.93.9 0.00.0 3.112.1 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 3.04.1 2.15.5 0.00.0 6.616.9 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 6.25.7 4.37.3 0.00.0 12.821.8 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00. Table 11: Mean and standard deviation of pass@K for Qwen2.5-VL-32B-instruct (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluations results for Table 2. Error Category Paper Category Category pass@1 pass@ pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Figure duplication Reagent identity Statistical reporting 0.92.4 0.00.0 0.00.0 4.74.5 8.014.3 0.00.0 1.83.1 0.00.0 0.00.0 7.94.9 16.116.7 0.00.0 3.53.6 Biology 0.00.0 Chemistry 0.00.0 Computer Science 12.24.8 Engineering 26.613.4 Environmental Science 0.00.0 Materials Science Mathematics Medicine Multidisciplinary Physics 9.78.5 6.411.6 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 16.48.3 12.014.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00. 25.58.1 21.113.3 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 Table 12: Mean and standard deviation of pass@K for Llama-4-Maverick (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluations results for Table 2. Error Category Paper Category Category pass@ pass@2 pass@4 Category pass@1 pass@2 pass@ Data Inconsistency Equation / proof Experiment setup Figure duplication Reagent identity Statistical reporting 0.82.3 0.00.0 0.00.0 1.92.7 4.111.0 0.00.0 1.93.1 0.00.0 0.00.0 3.63.2 8.014.2 0.00.0 3.63.6 Biology 0.00.0 Chemistry 0.00.0 Computer Science 6.63.1 Engineering 16.516.7 Environmental Science 0.00.0 Materials Science Mathematics Medicine Multidisciplinary Physics 2.95.4 2.25.7 0.00.0 0.00.0 0.00.0 4.27.2 0.00.0 0.00.0 0.00.0 0.00.0 5.46.4 4.37.3 0.00.0 0.00.0 0.00.0 8.79.7 0.00.0 0.00.0 0.00.0 0.00.0 9.96.1 8.58.3 0.00.0 0.00.0 0.00.0 16.711.1 0.00.0 0.00.0 0.00.0 0.00.0 Table 13: Mean and standard deviation of pass@K for Llama-4-Scout (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluations results for Table 2. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@ pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Figure duplication Reagent identity Statistical reporting 2.63.4 0.00.0 0.00.0 3.64.9 3.910.7 3.18.3 4.94.1 0.00.0 0.00.0 7.06.4 8.714.7 6.110.7 9.24.2 Biology 0.00.0 Chemistry 0.00.0 Computer Science 14.07.6 Engineering 16.916.7 Environmental Science 12.512.5 Materials Science Mathematics Medicine Multidisciplinary Physics 6.56.0 6.18.0 0.00.0 6.216.5 0.00.0 0.00.0 0.00.0 3.18.3 1.23.3 0.00.0 12.87.9 11.59.5 0.00.0 12.221.5 0.00.0 0.00.0 0.00.0 6.110.7 2.44.3 0.00.0 25.39.6 21.49.7 0.00.0 24.925.0 0.00.0 0.00.0 0.00.0 12.512.5 5.05.0 0.00.0 Table 14: Mean and standard deviation of pass@K for o3 (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 14.65.5 24.64.9 6.717.0 8.314.4 24.817.8 25.28.3 41.35.5 12.921.9 17.119.0 44.118. 38.09.7 Biology 62.85.7 Computer Science 24.825.0 Environmental Science 33.021.7 Materials Science 66.112.4 Mathematics Medicine Multidisciplinary Physics 6.216.5 24.07.8 12.021.4 0.00.0 21.85.5 12.433.0 33.30.0 27.414.4 13.122.0 42.88.9 24.125.0 0.00.0 37.77.9 25.143.4 41.714.5 46.014.5 25.125.0 66.87.5 40.020.0 0.00.0 58.98.4 48.750.0 49.616.7 67.610.5 Table 15: Mean and standard deviation of pass@K for GPT-4.1 (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@1 pass@2 pass@ Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 8.614.6 6.03.4 0.00.0 8.314.4 15.612. 15.716.6 10.83.6 0.00.0 16.719.1 22.37.8 26.113.8 Biology 18.13.1 Computer Science 0.00.0 33.221.9 Materials Science 25.00.0 Mathematics Environmental Science Medicine Multidisciplinary Physics 6.216.5 4.14.2 19.635.6 0.00.0 4.33.9 12.433.0 45.916.2 0.00.0 13.122.0 7.95.2 36.442.2 0.00.0 8.04.3 24.042.7 71.317.4 0.00. 25.125.0 14.85.4 64.640.1 0.00.0 13.53.8 49.550.0 92.214.1 0.00.0 Table 16: Mean and standard deviation of pass@K for Gemini-2.5-Pro (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@ pass@2 pass@4 Category pass@1 pass@2 pass@ Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 4.17.2 7.64.0 0.00.0 8.314.4 12.112.5 7.78.3 11.63.9 0.00.0 15.416.6 19.610.3 13.06.9 Biology 16.23.8 Computer Science 0.00.0 26.113.8 Materials Science 24.72.8 Mathematics Environmental Science Medicine Multidisciplinary Physics 37 0.00.0 4.25.9 12.921.9 0.00.0 5.02.5 24.843.2 49.416.7 0.00.0 0.00.0 8.27.5 23.525.0 0.00.0 7.12.5 46.349.9 73.620.2 0.00.0 0.00.0 14.88.5 38.621.0 0.00.0 9.02.0 78.241.3 92.314.1 0.00.0 Table 17: Mean and standard deviation of pass@K for Gemini-2.0-Flash-Lite-001 (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 2.15.5 1.11.5 0.00.0 12.516.1 3.48.5 4.07.1 2.11.7 0.00.0 25.720.9 6.510. Biology Computer Science Environmental Science 8.28.3 3.91.8 0.00.0 50.124.2 Materials Science 12.412.5 Mathematics Medicine Multidisciplinary Physics 12.921.9 2.13.6 0.00.0 0.00.0 0.61.6 11.732.2 8.614.6 0.00.0 25.428.6 3.84.2 0.00.0 0.00.0 1.12.1 26.244.0 16.619.3 0.00.0 49.932.9 6.53.5 0.00.0 0.00.0 2.52.5 50.750.0 33.022.2 0.00. Table 18: Mean and standard deviation of pass@K for Claude-3.7-Sonnet:Thinking (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@1 pass@ pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 6.211.6 4.63.7 0.00.0 0.00.0 18.420.6 12.114.0 8.44.6 0.00.0 0.00.0 31.219.9 21.113.3 Biology 15.14.9 Computer Science 0.00.0 0.00.0 Materials Science 44.311.6 Mathematics Environmental Science Medicine Multidisciplinary Physics 0.00.0 7.26.5 12.421.6 6.216.5 4.53.9 0.00.0 8.014.3 4.27. 0.00.0 12.47.8 23.825.0 12.621.7 8.04.3 0.00.0 15.816.7 7.78.3 0.00.0 20.78.9 39.120.7 24.325.0 13.63.7 0.00.0 26.413.5 13.06.9 Table 19: Mean and standard deviation of pass@K for Claude-3.7-Sonnet (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@1 pass@ pass@4 Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 6.38.1 4.92.1 0.00.0 4.111.0 3.48.5 11.610.1 8.93.0 0.00.0 8.014.2 6.510.9 21.510.0 Biology 15.23.4 Computer Science 0.00.0 16.516.7 Materials Science 12.412.5 Mathematics Environmental Science Medicine Multidisciplinary Physics 6.216.5 6.35.5 18.424.1 0.00.0 3.84.2 0.00.0 8.914.8 0.00.0 12.021.4 11.86.9 34.429.3 0.00.0 6.44.2 0.00.0 17.218.9 0.00.0 24.825.0 21.37.0 63.429.3 0.00.0 9.93.3 0.00.0 33.521.2 0.00.0 Table 20: Mean and standard deviation of pass@K for DeepSeek-R1 (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 12.111.7 16.15.5 0.00.0 4.911.8 28.315.7 20.511.5 27.86.0 0.00.0 9.515.1 43.319. 30.26.5 Biology 41.54.3 Computer Science 0.00.0 19.316.5 Materials Science 61.318.1 Mathematics Environmental Science Medicine Multidisciplinary Physics 38 7.417.8 13.09.7 15.723.2 0.00.0 15.94.1 0.00.0 38.420.8 16.318.0 14.322.6 23.810.3 26.924.9 0.00.0 26.95.2 0.00.0 55.915.6 28.117. 28.924.7 39.17.7 41.918.5 0.00.0 39.75.0 0.00.0 65.85.3 42.29.7 Table 21: Mean and standard deviation of pass@K for DeepSeek-V3-0324 (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@ pass@2 pass@4 Category pass@1 pass@2 pass@ Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 7.08.2 1.31.5 0.00.0 4.711.6 0.00.0 12.17.5 2.62.0 0.00.0 9.515.1 0.00.0 16.22.7 Biology 5.22.1 0.00.0 19.316.5 Materials Science 0.00.0 Mathematics Computer Science Environmental Science Medicine Multidisciplinary Physics 7.117.5 1.12.8 29.324.6 0.00.0 0.71.7 0.00.0 0.00.0 0.00.0 14.222.6 2.23.7 51.427.0 0.00.0 1.42.3 0.00.0 0.00.0 0.00.0 28.924.7 4.94.1 76.824.9 0.00.0 2.92.5 0.00.0 0.00.0 0.00.0 Table 22: Mean and standard deviation of pass@K for Qwen3-235A-22B (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@ pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 8.48.3 17.16.9 0.00.0 5.712.5 25.920.6 15.89.5 28.16.1 0.00.0 11.615.9 44.418.9 26.78.2 Biology 40.73.3 Computer Science 0.00.0 22.315.7 Materials Science 64.812.3 Mathematics Environmental Science Medicine Multidisciplinary Physics 0.00.0 17.95.7 7.618.0 0.00.0 12.76.3 17.037.6 45.224.4 16.69.7 0.00.0 29.97.4 16.023.3 0.00.0 20.66.3 34.847.7 74.625.0 28.712.8 0.00.0 43.25.6 33.223.6 0.00.0 31.24.9 66.947.1 98.27.5 43.210.2 Table 23: Mean and standard deviation of pass@K for Qwen2.5-VL-72B-Instruct (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@1 pass@2 pass@ Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 18.910.4 0.41.0 0.00.0 15.525.4 6.911. 31.09.1 0.81.3 0.00.0 27.728.8 13.312.5 42.28.3 Biology 1.81.5 0.00.0 47.224.4 Materials Science 21.29.0 Mathematics Computer Science Environmental Science Medicine Multidisciplinary Physics 23.238.1 0.00.0 34.434.5 8.218.6 0.00.0 0.00.0 22.915.5 0.00.0 41.643.3 0.00.0 55.431.7 15.223.0 0.00.0 0.00.0 41.317.9 0.00. 70.836.6 0.00.0 79.624.6 28.124.8 0.00.0 0.00.0 60.712.8 0.00.0 Table 24: Mean and standard deviation of pass@K for Qwen2.5-VL-32B-Instruct (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@ pass@2 pass@4 Category pass@1 pass@2 pass@ Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 4.67.4 0.00.0 0.00.0 4.711.6 0.00.0 8.98.3 0.00.0 0.00.0 9.515.1 0.00.0 14.16.0 Biology 0.00.0 0.00.0 19.316.5 Materials Science 0.00.0 Mathematics Computer Science Environmental Science Medicine Multidisciplinary Physics 39 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 14.234.9 9.114.9 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 28.545.2 17.716.6 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 0.00.0 57.849.4 28.311.9 0.00.0 Table 25: Mean and standard deviation of pass@K for Llama-4-Maverick (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@1 pass@2 pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 0.00.0 0.81.3 0.00.0 4.511.4 0.00.0 0.00.0 1.71.8 0.00.0 9.314.9 0.00. Biology Computer Science Environmental Science 0.00.0 3.52.0 0.00.0 18.916.5 Materials Science 0.00.0 Mathematics Medicine Multidisciplinary Physics 6.717.0 1.12.8 0.00.0 0.00.0 0.00.0 0.00.0 4.711.6 0.00.0 13.922.4 2.23.7 0.00.0 0.00.0 0.00.0 0.00.0 9.715.2 0.00.0 28.324.8 4.94.1 0.00.0 0.00.0 0.00.0 0.00.0 18.616.6 0.00. Table 26: Mean and standard deviation of pass@K for Llama-4-Scout (K {1, 2, 4}) by error category (left) and paper category (right). Detailed evaluation results for text-only evaluation of Table 3. Error Category Paper Category Category pass@1 pass@ pass@4 Category pass@1 pass@2 pass@4 Data Inconsistency Equation / proof Experiment setup Reagent identity Statistical reporting 6.98.2 0.81.3 0.00.0 4.311.2 0.00.0 13.19.7 1.51.5 0.00.0 8.714.6 0.00.0 23.99.2 Biology 2.61.0 0.00.0 19.716.4 Materials Science 0.00.0 Mathematics Computer Science Environmental Science Medicine Multidisciplinary Physics 6.516.8 2.33.7 14.122.5 6.516.8 0.00.0 0.00.0 0.00.0 0.00. 13.122.0 4.14.2 26.225.0 13.122.0 0.00.0 0.00.0 0.00.0 0.00.0 29.624.6 7.22.8 42.218.2 29.624.6 0.00.0 0.00.0 0.00.0 0.00."
        }
    ],
    "affiliations": [
        "Boeing Korea",
        "EleutherAI",
        "KAIST",
        "MIT",
        "OneLineAI",
        "Yonsei University"
    ]
}