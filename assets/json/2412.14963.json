{
    "paper_title": "IDOL: Instant Photorealistic 3D Human Creation from a Single Image",
    "authors": [
        "Yiyu Zhuang",
        "Jiaxi Lv",
        "Hao Wen",
        "Qing Shuai",
        "Ailing Zeng",
        "Hao Zhu",
        "Shifeng Chen",
        "Yujiu Yang",
        "Xun Cao",
        "Wei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K diverse, photorealistic sets of human images. Each set contains 24-view frames in specific human poses, generated using a pose-controllable image-to-multi-view model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space from a given human image. This model is trained to disentangle human pose, body shape, clothing geometry, and texture. The estimated Gaussians can be animated without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the ability to efficiently reconstruct photorealistic humans at 1K resolution from a single input image using a single GPU instantly. Additionally, it seamlessly supports various applications, as well as shape and texture editing tasks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 3 6 9 4 1 . 2 1 4 2 : r IDOL: Instant Photorealistic 3D Human Creation from Single Image Yiyu Zhuang1,4 Jiaxi Lv2,4 Hao Wen3,4 Qing Shuai4 Ailing Zeng4 Hao Zhu1 Shifeng Chen2,5 Yujiu Yang3 Xun Cao1 Wei Liu4 1 Nanjing University, 2 Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences 3 Tsinghua University, 4 Tencent, 5 Shenzhen University of Advanced Technology https://yiyuzhuang.github.io/IDOL/ Figure 1. This work introduces (a) IDOL, feed-forward, single-image human reconstruction framework that is fast, high-fidelity, and generalizable; (b) Utilizing the proposed Large Generated Human Multi-View Dataset consisting of 100K multi-view subjects, our method exhibits exceptional generalizability in handling diverse human shapes, cross-domain data, severe viewpoints, and occlusions; (c) With uniform structured representation, the avatars can be directly animatable and easily editable."
        },
        {
            "title": "Abstract",
            "content": "Creating high-fidelity, animatable 3D full-body avatar from single image is challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce large-scale HUmancentric GEnerated dataset, HuGe100K, consisting of 100K diverse, photorealistic sets of human images. Each set contains 24-view frames in specific human poses, generated using pose-controllable image-to-multi-view model. Work done during the internship at Tencent by Yiyu Zhuang, Jiaxi Lv, and Wenhao. Equal contributions Corresponding authors Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop scalable feedforward transformer model to predict 3D human Gaussian representation in uniform space from given human image. This model is trained to disentangle human pose, body shape, clothing geometry, and texture. The estimated Gaussians can be animated without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the ability to efficiently reconstruct photorealistic humans at 1K resolution from single input image using single GPU instantly. Additionally, it seamlessly supports various applications, as well as shape and texture editing tasks. 1. Introduction Reconstructing 3D clothed avatars from single image is crucial in user-friendly virtual reality, gaming, and 3D content creation. This task involves mapping 2D images to 3D models, highly ill-posed and challenging problem due to the complexity of clothing and the diversity of human poses. Learning-based methods trained on publicly available 3D human models [14, 20, 21, 59, 74] have improved reconstruction quality, but their generalization and quality remain limited [49, 50, 60, 61, 67, 73, 75]. Techniques that integrate parametric body estimation [26, 60, 75], loop optimization [60, 61, 73], and diffusion model priors [2, 33, 57] enhance performance but often result in slower and fragile reconstructions. Recently, large generic image-to-3D reconstruction and generation models [17, 32, 54, 55, 58, 62], leveraging large-scale datasets [11, 12, 40] or pre-trained diffusion models [3, 47, 48], boost capabilities but still struggle with real-life human reconstruction due to the scarcity of photorealistic 3D human data. Back to the first principle, we rethink existing data, models, and representations. First, current data acquisition is limited by the photographing or scanning of individual subjects, making them inherently unscalable. For instance, the largest publicly available human dataset, MVHumanNet [59], contains only thousands of subjects and clothing variations, far from the dataset scale required to achieve robust model generalization across diverse input images. Second, in the realm of animatable reconstruction models, existing approaches predominantly rely on either reconstructand-rig methods that necessitate manual post-processing or entangle human parametric models (e.g., SMPL(-X) [39, 43]) for optimization. However, disentangling human pose and body shape, clothed geometry, and texture could simplify the learning process, avoid error accumulation in parameter estimation, and improve efficiency. Furthermore, when focusing solely on human reconstruction, the incorporation of multi-view image generation or refinement models warrants careful consideration [45, 57, 62]. While these models can introduce finer details, they may also lead to inconsistencies across different views. Third, the community of 3D vision develops various representations tailored to application-specific requirements and technological advancements [28, 41]. For human-centric applications, ideal representations should be well-structured and expressive to facilitate easy rigging and editing [23, 35], as well as capable of high-resolution and fast rendering [28] to enhance both functionality and realism. In this work, we introduce scalable pipeline for training simple yet efficient feed-forward model for instant photorealistic human reconstruction. We present HuGe100K, large-scale dataset comprising over 2.4M high-resolution (896640) multi-view images of 100K diverse subjects. To facilitate comprehensive 3D human reconstruction, we develop scalable data creation pipeline that integrates synthetic and real-world data, ensuring wide range of attributes such as age, shape, clothing, race, and gender. Building upon this dataset, we introduce novel feedforward transformer model IDOL that efficiently predicts 3D human avatars with photorealistic textures and accurate geometry. Our approach leverages pretrained encoder [29] and transformer-based backbone for feature extraction and fusion, enabling instant reconstruction (within 1 second on an A100 GPU) without relying on generative models [73] for refinement. By integrating uniform representation for the 3D human, our model achieves enhanced texture completion and ensures that the reconstructed humans are naturally animatable. Extensive evaluations demonstrate that our method excels in diverse and challenging scenarios, offering superior consistency and quality compared to existing techniques. Additionally, the efficient representation of our model enables seamless applications in animation, editing, and other downstream tasks. Our contributions can be summarized as follows: We rethink single-view 3D human reconstruction from the perspectives of data, model, and representation. We introduce scalable pipeline for training simple yet efficient feed-forward model, named IDOL, for instant photorealistic human reconstruction. We develop data generation pipeline and present HuGe100K, large-scale multi-view human dataset featuring diverse attributes, high-fidelity, high-resolution appearances, and well-aligned SMPL-X model. Our comprehensive study demonstrates that leveraging large-scale generated training data significantly enhances model performance and generalizability, paving the way for further scaling up 3D human reconstruction models. 2. Related Work 2.1. 3D Human Datasets High-precision 3D human models typically rely on scanning or multi-view camera acquisition systems. Scan-based datasets, such as THuman2.0 [64], Twindom [21], and 2K2K [14], provide high-fidelity 3D geometries but are limited by small number of subjects, simple standing poses, and non-human artifacts (e.g., 2K2K). In contrast, multiview acquisition systems [4, 8, 9, 27, 34, 37, 46, 59, 66] facilitate the collection of larger datasets with more flexible actions. However, these datasets often face challenges such as biased indoor lighting, fixed viewpoints, and limited scale. Large-scale synthetic and real datasets for generic objects, including Objaverse and MVImgNet [11, 12, 65], address open-world reconstruction but lack specificity for human models. To overcome these limitations, the proposed HuGe100K significantly scales up dataset size, increasing subject diversity by over 100 times compared to previous datasets. It emphasizes diversity in pose, shape, viewpoint, and clothing, paving the way for large-scale model training. We compare existing datasets with ours in Tab. 1. Type Dataset #Frames c 3 a e - u THuman [74] THuman2.1 [64] 2K2K [14] X-Avatar [52] ZJU-MoCap [46] Neural Actor [37] HUMBI [66] AIST++ [34] HuMMan [4] GeneBody [8] ActorsHQ [27] DNA-Rendering [9] MVHumanNet [59] IDs 200 2500 2050 20 SMPL(-X) - - - 35.5K 180K 250K 26M 10.1M 10 8 772 30 60M 1000 50 8 500 645.1M 4500 2.95M 40K 67.5M Ours HuGe100K > 2.4M 100K Table 1. Comparisons of related datasets. HuGe100K is largescale generated multi-view human dataset with 100K diverse high-fidelity humans. 2.2. Single-Image Human Reconstruction For clothed 3D reconstruction methods, implicit representation methods such as PIFU [49], PIFU-HD [50], ARCH [16, 26], and PaMIR [75] have been widely adopted. To enhance reconstruction quality and generalizability, loop optimization techniques integrate implicit representations with explicit or hybrid human priors for improved robustness [2, 60, 61, 71]. Similarly, GTA [72] and SIFU [73] entangle SMPL priors and side-view conditioned features to enhance feature representation. However, these approaches rely heavily on the accuracy of SMPL estimates and are computationally expensive, typically requiring several minutes for inference. Moreover, the resulting 3D representations are not drivable. Recent advancements focus on recovering animatable 3D humans from single images [10, 24, 25]. Some methods benefit from the pre-trained diffusion models or large reconstruction models [45, 57]. Nevertheless, all methods are constrained by the limitations of training datasets, resulting in suboptimal texture detail and limited generalizability. The latest work, E3Gen [69], combines UV maps and Gaussian splatting [28] to directly generate Gaussian attribute maps in UV space from images, which does not support arbitrary image inputs. To generate consistent human images or videos given single image, recent diffusion video models animate static image with pose-controllable video conditions[6, 6, 22, 56, 56, 63, 77]. Specifically, Champ [77] incorporate several rendered maps obtained from SMPL sequences, alongside skeleton-based motion guidance, to enrich the conditions to the latent diffusion model. However, they all fail to generate 360-degree video and animate precise expressive SMPLX-based humans due to insufficient training data. 3. Dataset Creation Drawing inspiration from the latest advancements in generalizable large models, our core insight is to develop large-scale reconstruction model with exceptional generalization capabilities. The crux of this endeavor lies in creating comprehensive and high-quality digital human dataset. In this section, we introduce the creation of large-scale human-centric generated dataset comprising over 2.4M high-resolution (896 640) multi-view images from more than 100K diverse subjects. As shown in Fig. 2, the data generation pipeline consists of two stages. Firstly, diverse text prompts are generated by large language models incorporating various human-centric attributes, and high-quality images are synthesized using text-to-image generation models (Sec. 3.1). Secondly, we train multi-view video generation model, MVChamp, conditioned on rendered full-body motions. With this, large-scale multi-view images dataset is established by feeding both the synthesized and captured images (Sec. 3.2). Lastly, the statistics and characteristics of the HuGe100K are demonstrated in Sec. 3.3. 3.1. Image Collection and Generation Given the limited amount of existing datasets [4, 46, 59, 64] and legal concerns regarding portrait and copyright issues, we propose leveraging an image generation model to construct large-scale, diverse, and high-fidelity dataset of full-body human images. Using the latest text-to-image model Flux [18], we design descriptive prompts based on the required attributes for human figures, ensuring diversity across area, clothing, body shape, age, and gender shown in Fig. 2(i). To avoid long-tail attribute distributions and ensure uniform coverage of traits, we apply uniform sampling in attribute selection for the image generation process. This approach generated over 100K images in total. However, due to issues of visual non-compliance and high similarity among some images, we conducted manual filtering and retained final set of 90K high-quality images. Additionally, we combine 90K synthetic images and 10K real-life images from DeepFashion [38]. 3.2. Multi-view Image Animation and Generation Image-based animating models [6, 22, 63, 70, 77] struggle to achieve generation of 360-consistent and diverse human videos conditioned on pose sequences (e.g., 2D poses and 3D SMPL(-X) conditions). To address this limitation, we re-train state-of-the-art video generative model, Champ [77], to obtain multi-view consistent generative model, MVChamp. Firstly, We collect curated dataset of over 100K single-person videos with various motions Figure 2. Pipeline for constructing our HuGe100K. Diverse attribute combinations from GPT-4 templates create text prompts, generating synthetic images via FLUX, combined with real images from DeepFashion. SMPL-X fitting produces multi-view pose sequences with 360-degree rotations and diverse animatable motions. MVChamp then converts these sequences into multi-view images, ensuring 3D consistency in the dataset. (e.g., dancing), including approximately 20K videos involving the action of turning around, to fine-tune the model and enhance its generalizability for diverse inputs and motions. Secondly, to further improve 3D consistency[13], we utilize scanned models from THuman 2.1 to generate uniformly distributed views via Blender. From these, we select subset of 24 views that are evenly spaced around the full 360 rotation to fine-tune MVChamps temporal layers using diffusion loss. Lastly, to support high-quality whole-body animation (i.e., body and hands), we introduce precise SMPLX estimation using NLF [51] for body shape and pose estimation. Additionally, we employ HaMeR [44] for hand estimation, rendering the corresponding hand template into depth maps as additional control signals. After these training processes, MVChamp generates multi-view images conditioned on SMPL-X parametric model. The pose condition is set to an A-Pose 80% of the time and random pose from dance videos (via SMPLer-X [5]) 20% of the time, balancing pose variability. To address discrepancies between the first and last frames, we propose Temporal Shift Denoising Strategy: during the denoising steps, we shift latent inputs and pose signals temporally, moving the last frame to the first. This improves continuity between frames without additional inference costs. Finally, FaceFusion [19] is utilized to enhance facial details. 3.3. Data Statistics and Characteristics Tab. 1 compares our dataset HuGe100K to other 3D human datasets regarding scale, diversity, and consistency. With over 100K human identities and 20K poses, HuGe100K offers balanced diversity across dimensions such as area, clothing, body shape, age, and gender (see Fig. 1). Each Figure 3. paired example from the proposed HuGe100K Dataset. For each reference image, we generate set of multiview images using an estimated shape and specific pose. The figure shows the pose is well-aligned. sample includes reference image, SMPL-X estimates, 24 uniformly sampled multi-view images, camera parameters, and SMPL-X data (see Fig. 3). 4. Large Human Reconstruction Model In this section, we present the large-scale human reconstruction model, named IDOL. The overview pipeline is shown in Fig. 4. In Sec. 4.1, we describe the animatable human representation. Sec. 4.2 details the network architecture of IDOL, while Sec. 4.3 explains how we trained the model in an end-to-end manner using the multi-view image dataset. 4.1. Animatable Human Representation Similarly to previous work [23, 31, 69], IDOL leverages 3D Gaussian Splatting [28] in conjunction with SMPL-X for 3D human representation, aiming to address the challenges of real-time rendering and accurate animation of human avatars. Specifically, each Gaussian primitive Gk is characterized by: Gk = {µk, αk, rk, sk, ck} , where µk is the 3D position of the Gaussian, αk is opacity, rk is the rotation, sk is the scale, and ck is the color. Figure 4. The architecture of IDOL, full-differentiable transformer-based framework for reconstructing animatable 3D human from single image. The model integrates high-resolution (1024 1024) encoder [29] and fuses image tokens with learnable UV tokens through the UV-Alignment Transformer. UV Decoder predicts Gaussian attribute maps as intermediate representations, capturing the humans geometry and appearance in structured 2D UV space defined by the SMPL-X model. These maps, in conjunction with the SMPL-X model, represent 3D human avatar in canonical space, which can be animated using linear blend skinning (LBS). The model is optimized using multi-view images with diverse poses and identities, learning to disentangle pose, appearance, and shape. 3D Gaussian Human. Directly predicting all 3D Gaussian primitives is computationally intensive. Instead, IDOL leverages the predefined 2D UV space of the SMPL-X model to transform the 3D representation task into more manageable 2D problem. Initially, IDOL predicts Gaussian attribute maps that encode the offset values {δµk , δrk, δsk}, as well as the color ck and opacity αk for each Gaussian primitive Gk. Similarly to E3Gen [69], the position µk, scale sk, and rotation rk of each Gaussian primitive Gk is modeled relative to its SMPL-X vertex as follows: µk = ˆµk + δµk , sk = ˆsk δsk , rk = ˆrk δrk , where ˆµk, ˆsk, and ˆrk are the initial values based on the SMPL-X model. The color ck is assigned using RGB values, and the opacity αk is set to 1, indicating full opacity. This unified UV space significantly reduces computational complexity and fully exploits the geometric and semantic priors provided by the SMPL-X model. By modeling Gaussian primitives relative to SMPL-X vertices, IDOL ensures semantic consistency across corresponding body parts of diverse avatars, thereby enhancing the models generalization capability across various reference images. Animation and Rendering. Given target pose, we calculate the transformation of each human joint using predefined kinematic relationships. The transformation of each Gaussian primitive is performed using forward skinning technique based on LBS. Specifically, the position of each Gaussian primitive Gk is transformed as follows: µ = (cid:80)nb i=1 wiBiµk. Additionally, the rotation matrix Rk is updated by: i=1 wiBi, and Rk is the rotation matrix representation of the rotation angle rk. Here, nb is the number of joints, Bi is the transformation matrix for each joint, and wi represents the skinning weights, indicating the influence of each joints motion on the Gaussian primitives position µk. Rk, where Tk = (cid:80)nb = T1:3,1:3 To estimate the skinning weights for each Gk, we compute the skinning weight field for body parts using predefined low-resolution volume[7]. For the smaller regions, such as the hands and face, which exhibit minimal topology changes, their skinning weights are calculated by interpolating from the SMPL-X template using barycentric coordinates. This approach supports modeling large-scale topology changes among different identities, including substantial changes introduced by clothing. In addition, it helps stabilize the convergence for areas like the fingers and face, which experience fewer topological changes compared to the SMPL-X template. 4.2. Network Structure 4, IDOL is full-differentiable As illustrated in Fig. transformer-based framework for reconstructing animatable 3D human from single image. High-resolution Image Encoder. Higher resolution of the input image results in high-quality reconstruction. However, previous works suffer from low-resolution ViTbased encoders, such as DINOv2 [42], which support the 448 448 resolution. To fully leverage the resolution of the HuGe100K dataset, we further adopt high-resolution human foundation model, Sapiens [29], to encode the 1024 1024 resolution images into patch-wise feature tokens, fori=1 RdE , where denotes the mulated as: = {fi}n i-th image patch, and dE is the channel length of each token. The Sapiens model is pretrained on 300 million in-the-wild human images using the Masked Autoencoder (MAE) framework [15], enabling it to excel in preserving fine-grained details and capturing diverse human poses and appearances, making it highly effective for high-resolution human image feature extraction. UV-Alignment Transformer. To map irregular and diverse reference images onto regular UV feature maps, UV-Alignment Transformer is employed to align learnable spatial-positional UV tokens Q0 with reference image features F. Specifically, the UV-Alignment Transformer concatenates from Spaiens with Q0, and aggregates and refines features through transformer blocks, producing an enhanced representation QD. Each transformer block utilizes self-attention mechanisms, enabling the model to capture complex relationships among the input tokens and impute missing parts using correlated tokens. UV Decoder. The UV tokens QD are reshaped and decoded into Gaussian attribute maps, capturing the humans geometry and appearance within the structured 2D UV space as shown in Sec. 4.1. To preserve and enhance fine details in the decoded Gaussian attribute maps, Convolutional Neural Network is employed for spatial up-sampling. 4.3. Training Objectives IDOL reconstructs 3D human by predicting Gaussian attribute UV maps in single forward pass, offering significant advantages in inference speed and enabling end-to-end training with multi-view images. For each sample, we select front view as the reference image Iref, along with random set of generated multi-view images {Igt,i}N i=1 and their corresponding SMPL-X parameters and camera for supervision. It takes Iref as input to generate the 3D human, using differentiable rendering to produce the multi-view images {Ipred,i}N i=1. The loss function is defined as follows: = (cid:88) (cid:16) i=1 Igt,i Ipred,i2 + λLvgg(Igt,i, Ipred,i) (cid:17) , (1) where λ controls the balance between the mean square error loss and the perceptual loss Lvgg. 5. Experiments 5.1. Experiments on Dataset Creation Fig. 5 (left) presents the qualitative results of the ablation study on our data generation method. Specifically, Fig. 5 (a) and Fig. 5 (b) demonstrate how hand refinement and face swapping effectively enhance the quality of the generated hands and faces. Fig. 5 (c) highlights the importance of fine-tuning the 3D dataset Thuman 2.1 for improving the 3D consistency of MVChamp, while Fig. 5 (d) illustrates that the Temporal Shift Denoising strategy, which involves cycling the first and last frames, improves the consistency between the first and last frames. Fig. 5 (right) compares our MVChamps generation quality against previous human image animation models (e.g., Champ, MimicMotion [70]) and general multi-view video generation model (e.g., SV3D [55]). The issues of these methods are highlighted in the figure. Champ struggles to generate plausible hands and heads in multi-view scenarios. MimicMotion not only fails to produce realistic shoes but also has difficulty preserving the identity, especially for the anime image. SV3D produces low-quality multi-view human generation due to the lack of 3D prior knowledge of the human body. In contrast, our model generates consistent, high-quality results across views. 5.2. Comparison of Reconstruction Model Implementation Details. For the encoder, we employed the pre-trained Sapiens-1B model[29] and kept its weights frozen during training. We then defined transformer-based framework with parameter size of 0.5 billion to perform feature fusion. Additionally, we densify the SMPL-X vertex set by sampling approximately 200,000 vertices to map attributes and represent the 3D human model effectively. Dataset and Metrics. We train IDOL on dataset consisting of generated multi-view images from HuGe100K and rendered images from THuman 2.1 [74]. For THuman 2.1, we render 72 view images at resolution of 896 640 for each scan, with rendering views uniformly distributed. To evaluate performance quantitatively, we reserve the last 50 cases from both HuGe100K and THuman 2.1 as the testing set. We use the most frontal image as the reference image and the remaining images as ground-truth data for evaluation. The ground-truth camera parameters and SMPL-X parameters are provided for all methods. The renderings of the reconstructed model are then compared to their corresponding ground truth images with several metrics, including Mean Squared Error (MSE), Learned Perceptual Image Patch Similarity (LPIPS) [68], Peak Signal-to-Noise Ratio (PSNR). More details on implementation can be found in the supplementary material. Baselines. We compare IDOL with three baseline categories in the Single-Image Human Reconstruction task. The first category comprises methods based on loop optimization or pixel-alignment modules, including GTA [72] and SIFU [73]. These methods focus on refining human reconstruction through iterative optimization or predicting 3D geometry from pixel-aligned features. The second category encompasses large-scale generic reconstruction networks, represented by LGM [54]. These models are known for their ability to handle large datasets and their scalability, offering advantages in terms of fast reference speeds and large output resolutions. The third category involves optimization-based 3D generation methods using score distillation sampling (SDS), exemplified by DreamGaussian [53]. These methods leverage priors from 2D diffusion models to distill 3D objects. Figure 5. Qualitative results of our MVChamp ablation study (left) and comparison experiment (right). Figure 6. Comparisons on (a) the upper: novel-view synthesis given single image, and (b) the lower: our animated results. DreamGaussian accelerates convergence by progressively densifying 3D Gaussians, significantly reducing the reconstruction time. However, it still takes approximately two minutes to reconstruct single object. Quantitative comparison. As shown in Tab 2, our method outperforms all baselines in all metrics. We attribute this superior performance to the large-scale dataset HuGe100K and the design of our large-scale reconstruction model IDOL, which allows for more effective training and improved synthesis of appearance results. Despite this, we note that SIFU and GTA report lower metric than what we expected in our test settings. While we provide accurate ground-truth SMPL-X parameters and camera settings, the ideal orthographic projection required by the pixel-alignment modules in SIFU and GTA is not wellsuited to our perspective projection model, where the camera focus ranges from 35 to 80. This mismatch leads to Method MSE PSNR LPIPS SIFU [73] GTA [72] Ours-w/o HuGe100K Ours-full 0.042 0.041 0.017 0.008 14.204 14.282 19.225 21.673 1.612 1.629 1.326 1.138 Table 2. Evaluation of Comparison and Ablation Experiments. misalignment between the rendered images and the ground truth, adversely affecting their performance metrics. In fact, many real-life photographs are taken with medium to short focal lengths and cannot be approximated using orthographic projection, which is less noticeable drawback of SIFU and GTA. Additionally, SIFU and GTA, trained with images at resolution of 512 512, struggle to synthesize detailed textures, particularly in the invisible areas of the reference images. This limitation is primarily due to the lack of comprehensiveness and diversity in their training datasets, which restricts their performance in generating invisible aspects of the images. Qualitative comparison. We perform qualitative evaluation on an in-the-wild dataset with methods from [53, 54, 72, 73], with the results presented in Fig. 6. Our evaluation includes subject in complex outfit featuring baseball cap and textured clothing, demonstrating our methods ability to synthesize intricate textures and handle loose outfits across different views. Additional tests include out-ofdomain cartoon data and large-angle side views and assessing model adaptability and viewpoint handling. IDOL consistently outperforms the baselines, which struggle with detail reproduction and texture consistency under these various conditions. 5.3. Ablation Study on Reconstruction Model We assess the impact of proposed components by removing them individually from IDOL. As shown in Fig. 7, replacing the Sapiens [29] encoder with DINO v2 [42] (w/o Sapiens) reduces detail quality, resulting in less realistic textures and folds. Excluding the HuGe100K dataset (w/o HuGe100K) causes significant distortion, including blurred details and color bleeding. The complete model, with all components, produces the most realistic and detailed results. These findings highlight the critical roles of both the Sapiens encoder and the HuGe100K dataset in achieving high-quality avatar generation. 5.4. Downstream Applications IDOL reconstructs an avatar by combining Gaussian attribute maps with the SMPL-X model, enabling users to modify the avatars appearance by editing UV texture maps and to control body shape by adjusting SMPL-X shape paFigure 7. Qualitative Results of Ablation Study of IDOL. Figure 8. Controllable Avatar Editing: (a) texture editing; (b) body shape editing. rameters. Fig. 8(a) illustrates the effect of texture editing on clothing patterns, while Fig. 8(b) demonstrates shape editing to adjust the avatars body size. This approach provides high degree of controllability over both the avatars appearance and body shape. 6. Conclusions and Limitations In conclusion, our work has made significant strides in creating an animatable 3D human from single image. We introduced scalable pipeline for training simple yet efficient feed-forward model, incorporating dataset generation framework, large-scale dataset HuGe100K, and scalable reconstruction transformer model, IDOL. This model efficiently reconstructs photorealistic 3D humans in under second and demonstrates versatility across various applications. This model can efficiently reconstruct photorealistic humans in less than second and is versatile enough to support various applications. However, there are limitations. Due to the constraints of the video model used, we can only synthesize singleframe images from fixed viewpoints. Future work could consider generating longer motion sequences. Additionally, our dataset lacks the generation of data involving multiple people interacting or people interacting with objects. Our method also struggles with handling half-body inputs. Future efforts could focus on enhancing data generation strategies to improve model performance."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d In SIGhuman digitization with shape-guided diffusion. GRAPH Asia 2023 Conference Papers, 2023. 2, 3, 4, 8 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [4] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, et al. Humman: Multi-modal 4d human dataset for verIn European Conference on satile sensing and modeling. Computer Vision, pages 557577. Springer, 2022. 2, 3 [5] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Wang Yanjun, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, et al. Smpler-x: Scaling up expressive human pose and shape estimation. Advances in Neural Information Processing Systems, 36, 2024. 4, 3 [6] Di Chang, Yichun Shi, Quankai Gao, Hongyi Xu, Jessica Fu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. In Forty-first International Conference on Machine Learning, 2023. 3 [7] Xu Chen, Tianjian Jiang, Jie Song, Max Rietmann, Andreas Geiger, Michael Black, and Otmar Hilliges. Fast-snarf: IEEE Transacfast deformer for articulated neural fields. tions on Pattern Analysis and Machine Intelligence, 45(10): 1179611809, 2023. 5 [8] Wei Cheng, Su Xu, Jingtan Piao, Chen Qian, Wayne Wu, Kwan-Yee Lin, and Hongsheng Li. Generalizable neural performer: Learning robust radiance fields for human novel view synthesis. arXiv preprint arXiv:2204.11798, 2022. 2, 3 [9] Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu Chen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, et al. Dna-rendering: diverse neural actor repository for high-fidelity human-centric rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1998219993, 2023. 2, [10] Enric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard Gabriel Bazavan, Andrei Zanfir, and Cristian Sminchisescu. Structured 3d features for reconstructing controllable avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16954 16964, 2023. 3 [11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2 [12] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. 2 [13] Junlin Han, Filippos Kokkinos, and Philip Torr. Vfusion3d: Learning scalable 3d generative models from video diffusion models. In European Conference on Computer Vision, pages 333350. Springer, 2025. 4 [14] Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang, Young-Jae Park, and Hae-Gon Jeon. High-fidelity 3d human digitization from single 2k resolution images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1286912879, 2023. 2, 3 [15] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. [16] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. Arch++: Animation-ready clothed human reconstruction revisited. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1104611056, 2021. 3 [17] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 2 [18] https://github.com/black-forest labs/flux. Flux latent rectified flow transformers, 2024. 3, 1, 5 [19] https://github.com/facefusion/facefusion. Facefusion, 2024. [20] https://renderpeople.com/3d people. Renderpeople dataset, 2015. 2 [21] https://web.twindom.com/. Twindom dataset, 2020. 2 [22] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 3 [23] Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, and Liqiang Nie. Gaussianavatar: Towards realistic human avatar modeling In Profrom single video via animatable 3d gaussians. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 634644, 2024. 2, 4 [24] Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, and Ziwei Liu. Sherf: Generalizable human nerf from In Proceedings of the IEEE/CVF Internaa single image. tional Conference on Computer Vision (ICCV), 2023. 3 [25] Yangyi Huang, Hongwei Yi, Weiyang Liu, Haofan Wang, Boxi Wu, Wenxiao Wang, Binbin Lin, Debing Zhang, and Deng Cai. One-shot implicit animatable avatars with modelIn IEEE Conference on Computer Vision based priors. (ICCV), 2023. 3 [26] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. Arch: Animatable reconstruction of clothed humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30933102, 2020. 2, [27] Mustafa Isık, Martin Runz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, and Matthias Nießner. Humanrf: High-fidelity neural radiance fields for humans in motion. ACM Transactions on Graphics (TOG), 42(4):112, 2023. 2, 3 [28] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3, 4 [29] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision modIn European Conference on Computer Vision, pages els. 206228. Springer, 2025. 2, 5, 6, 8, 3, 4 [30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 3 [31] Tobias Kirschstein, Simon Giebenhain, Jiapeng Tang, Markos Georgopoulos, and Matthias Nießner. Gghead: Fast and generalizable 3d gaussian heads. arXiv preprint arXiv:2406.09377, 2024. [32] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. In International Conference on Learning Representations (ICLR), 2024. 2 [33] Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, et al. Pshuman: Photorealistic single-view human reconstruction using cross-scale diffusion. arXiv preprint arXiv:2409.10141, 2024. 2 [34] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13401 13412, 2021. 2, 3 [35] Zhe Li, Zerong Zheng, Lizhen Wang, and Yebin Liu. Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1971119722, 2024. 2 [36] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 36: 2526825280, 2023. 3 [37] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor: Neural free-view synthesis of human actors with pose control. ACM transactions on graphics (TOG), 40(6):116, 2021. 2, 3 [38] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition In Proceedings of the and retrieval with rich annotations. IEEE conference on computer vision and pattern recognition, pages 10961104, 2016. 3, 1, 7 [39] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. ACM Transactions on Graphics, 34(6), 2015. 2 [40] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems, 36, 2024. 2 [41] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [42] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 5, [43] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Expressive body capture: 3d hands, Michael Black. In Proceedings of face, and body from single image. the IEEE/CVF conference on computer vision and pattern recognition, pages 1097510985, 2019. 2 [44] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3d with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98269836, 2024. 4, 1 [45] Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu. Charactergen: Efficient 3d character generation from single images with multi-view pose canonicalization. ACM Transactions on Graphics (TOG), 2024. 2, 3 [46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes In Proceedfor novel view synthesis of dynamic humans. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90549063, 2021. 2, 3 [47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2 [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. [49] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 23042314, 2019. 2, 3 [50] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for In Proceedings of high-resolution 3d human digitization. the IEEE/CVF conference on computer vision and pattern recognition, pages 8493, 2020. 2, 3 [51] Istvan Sarandi and Gerard Pons-Moll. Neural localizer fields for continuous 3d human pose and shape estimation. arXiv preprint arXiv:2407.07532, 2024. 4 [52] Kaiyue Shen, Chen Guo, Manuel Kaufmann, Juan Jose Zarate, Julien Valentin, Jie Song, and Otmar Hilliges. XIn Proceedings of the avatar: Expressive human avatars. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1691116921, 2023. 3 [53] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 6, 8 [54] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2025. 2, 6, [55] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2025. 2, 6 [56] Qilin Wang, Zhengkai Jiang, Chengming Xu, Jiangning Zhang, Yabiao Wang, Xinyi Zhang, Yun Cao, Weijian Cao, Chengjie Wang, and Yanwei Fu. Vividpose: Advancing stable video diffusion for realistic human image animation. arXiv preprint arXiv:2405.18156, 2024. 3 [57] Zhenzhen Weng, Jingyuan Liu, Hao Tan, Zhan Xu, Yang Zhou, Serena Yeung-Levy, and Jimei Yang. Template-free single-view 3d human digitalization with diffusion-guided lrm. arXiv preprint arXiv:2401.12175, 2024. 2, 3, 4, 8, 9 [58] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. arXiv preprint arXiv:2405.20343, 2024. 2 [59] Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, et al. Mvhumannet: largescale dataset of multi-view daily dressing human captures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1980119811, 2024. 2, 3 [60] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael Icon: Implicit clothed humans obtained from norBlack. In 2022 IEEE/CVF Conference on Computer Vimals. sion and Pattern Recognition (CVPR), pages 1328613296. IEEE, 2022. 2, 3 In Proceedings of the Optimized via Normal integration. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, [62] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2 [63] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human imIn Proceedings of age animation using diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. 3 [64] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57465756, 2021. 2, 3 [65] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: large-scale dataset of multi-view images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91509161, 2023. 2 [66] Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth Venkatesh, Jaesik Park, Jihun Yu, and Hyun Soo Park. Humbi: large multiview dataset of human body expressions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29903000, 2020. 2, 3 [67] Jingbo Zhang, Xiaoyu Li, Qi Zhang, Yanpei Cao, Ying Shan, and Jing Liao. Humanref: Single image to 3d human genIn Proceedings of eration via reference-guided diffusion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18441854, 2024. 2 [68] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [69] Weitian Zhang, Yichao Yan, Yunhui Liu, Xingdong Sheng, and Xiaokang Yang. E3gen: Efficient, expressive and editable avatars generation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 68606869, 2024. 3, 4, 5 [70] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation arXiv preprint with confidence-aware pose guidance. arXiv:2406.19680, 2024. 3, [71] Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen, and Yi Yang. Global-correlated 3d-decoupling transformer for clothed avatar reconstruction. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 3 [61] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. ECON: Explicit Clothed humans [72] Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen, and Yi Yang. Global-correlated 3d-decoupling transformer for clothed avatar reconstruction. Advances in Neural Information Processing Systems, 36, 2024. 3, 6, 8 [73] Zechuan Zhang, Zongxin Yang, and Yi Yang. Sifu: Side-view conditioned implicit function for real-world usIn Proceedings of the able clothed human reconstruction. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99369947, 2024. 2, 3, 6, [74] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and Yebin Liu. Deephuman: 3d human reconstruction from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 77397749, 2019. 2, 3, 6, 1 [75] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction. IEEE transactions on pattern analysis and machine intelligence, 44(6): 31703184, 2021. 2, 3 [76] Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and Chen Change Loy. Propainter: Improving propagation and In Proceedings of the transformer for video inpainting. IEEE/CVF International Conference on Computer Vision, pages 1047710486, 2023. 3 [77] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision (ECCV), 2024. 3, 1 IDOL: Instant Photorealistic 3D Human Creation from Single Image"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we provide additional details and visualizations to support the claims made in our main paper. Sec. provides further details on the HuGe100K dataset, important statistics, and the methodology to enhance the 3D consistency and diversity of multi-view images. Sec. describes the training procedure and setup for our proposed method, IDOL. Sec. presents additional experimental results, including comparison tables and results from the user study. including visualizations, A. More Details of HuGe100K Data This section provides more detailed explanation of the HuGe100K dataset generation process, along with additional visualizations. Sec A.1 describes our approach to improving the 3D consistency of MVChamp during training, while Sec A.2 presents the prompt template and attribute set used to generate reference images, as well as the generation process with MVChamp. Sec. A.3 demonstrates more visualization of the dataset. A.1. Improving 3D Consistency of Image Animation Champ [77] is one of the state-of-the-art Human Image Animation models, enhanced by multiple conditions rendered from DWPose and SMPL. We employ two-stage training process to enhance the 3D consistency of the Champ model for human multi-view synthetic, referred to as MVChamp. Fine-tuning Champ on Large-scale Human Videos with Whole-body Conditions To enable Champ to learn more human 3D prior knowledge, we curate dataset of approximately 100K dance videos for fine-tuning, of which around 20K explicitly contain human turning motions. Full parameter training of MVChamp on such large dance dataset effectively enhances its understanding of human 3D prior knowledge. Additionally, we employ HaMeR [44], stateof-the-art model for 3D hand reconstruction, to specifically reconstruct hand poses from images. These reconstructed hand poses are rendered into depth maps and used as an additional pose control signal for precise whole-body reconstruction and animation. Fine-Tuning Temporal Blocks on 3D Human Dataset We use the open-source scanned dataset THuman 2.1 [74], to produce 24 uniformly sampled rendered in Blender, views along the horizontal dimension to fine-tune the temporal layers of MVChamp using standard diffusion loss. Improving Temporal Consistency from the First to Last Frames Although the MVChamp model generates highly continuous multi-view images between adjacent frames, significant discrepancies remain between the first and last views, even though these two views are continuous in content. This issue likely arises from the models emphasis during training on ensuring continuity between adjacent frames while neglecting the larger temporal gap between the first and last frames. Thus, we propose the Temporal Shift Denoising Strategy to address this issue. During each denoising step, we shift the current latent inputs and pose condition signals along the temporal axis, moving the latent inputs and pose condition of the last frame to the first frame. This strategy ensures that each frame can access contextual information during most of the denoising steps, effectively eliminating discrepancies between the first and last frames at the same inference cost. A.2. Generating Balanced and Diverse Images Balanced, diverse, high-quality, high-resolution, and fullbody images are scarce in existing human-centric datasets, and they are challenging to collect on the Internet due to copyright and portrait rights issues. Therefore, we mix the real-life images and generate photorealistic images to obtain the large-scale quantity and high-quality images. Specifically, we extract approximately 10,000 real-life images from the open-source dataset DeepFashion [38] and use Flux [18], state-of-the-art text-to-image model, to generate balanced and diverse human reference images. We ensure balance and diversity across five dimensions during image generation: area, clothing, body shape, age and gender. Each dimension value is randomly selected from large set of options generated by GPT-4 [1], with prompt templates as follows: Front view, full-body pose of {age} old {body shape} {area} {gender} wearing {clothing} and visible hands. He/She stands against white background, evenly lit. Ultimately, we collect total of 100,000 balanced and diverse full-body human reference images. For each dimension, the possible options are as follows: 1. Area: United States, Canada, Mexico, Guatemala, Cuba, Brazil, Argentina, Colombia, Chile, Peru, United Kingdom, Germany, France, Italy, Spain, Netherlands, Belgium, Switzerland, Poland, Sweden, Nigeria, Egypt, South Africa, Kenya, Morocco, Ghana, Tanzania, Ethiopia, Uganda, Algeria, Saudi Arabia, Iran, Turkey, Israel, United Arab Emirates, Qatar, Kuwait, Jordan, Oman, Lebanon, Kazakhstan, Uzbekistan, Turkmenistan, Kyrgyzstan, Tajikistan, India, Pakistan, Bangladesh, Sri Lanka, Nepal, Bhutan, China, Figure 9. The visualization of the reenactment. Japan, South Korea, Mongolia, North Korea, Indonesia, Thailand, Vietnam, Malaysia, Philippines, Singapore, Myanmar, Cambodia, Laos, Brunei, Australia, New Zealand, Papua New Guinea, Fiji, Solomon Islands, Jamaica, Haiti, Dominican Republic, Puerto Rico, Trinidad and Tobago, Panama, Costa Rica, Nicaragua, Honduras, El Salvador, Belize, etc. 2. Clothing: T-shirts, Jeans, Casual pants, Dresses, Shorts, Tank tops, Sweaters, Cardigans, Jumpsuits, Hoodies, Suits, Business shirts, Formal skirts, Dress pants, Blazers, Tie, Waistcoats, Formal shoes, Briefcases, Leather belts, Sport shirts, Fitness clothes, Sports shoes, Tracksuits, Gym shorts, Leggings, Swimwear, Cycling gear, Compression wear, Evening gowns, Tuxedos, Long dresses, Tailcoats, Cocktail dresses, Party wear, Ceremonial suits, Ball gowns, Dress shoes, Fine jewelry, Hiking clothes, Waterproof jackets, Thermal wear, Camping gear, Fishing vests, Hunting apparel, Snowboarding pants, Rain boots, Cotton shirts, Linen dresses, Chiffon blouses, Sandals, Sunglasses, Short sleeves, Beachwear, Crop tops, Wool coats, Thick cotton sweaters, Fur jackets, Beanies, Boots, Gloves, Scarves, Thermal Insulated boots, Hanfu, Kimono, Sari, African tribal dresses, Scottish kilts, Bavarian lederhosen, Moroccan kaftans, Hawaiian shirts, Russian ushankas, Streetwear, Avant-garde designs, Fusion wear, Boho chic, Minimalist styles, High fashion, Urban outfits, Eco-friendly clothing, Techwear, Nurse uniforms, Firefighter gear, Construction vests, Police uniforms, Military boots, Lab coats, Coveralls, Military uniforms, Academic gowns, Judicial robes, Clerical vestments, Diplomatic suits, Regalia, etc. leggings, Padded parkas, 3. Body shape: Slight, Lean, Petite, Athletic, Fit, Average, Built, Buff, Bodybuilder, Full-figured, Stocky, Large. 4. Age: 2030 years, 3040 years, 4050 years, 5060 years, 6070 years, 7080 years, 8090 years. 5. Gender: Female and male. A.3. Additional Visualization Fig. 11 shows the diversity of reference images generated using our prompt template and attribute set. Fig.12 and Fig. 13 illustrate the multi-view images under diverse poses generated by our MVChamp. A.4. Application: Human Video Reenactment The goal of this application is to replace person in reference video with new identity while preserving the background and pose. Given reference image that provides the target identity, and reference video that provides the pose and background of the original person, the task is to seamlessly swap the person in the video while maintaining the integrity of the scene. We visualize the results in Fig. 9. To achieve this, we follow multi-step process: Identity Reconstruction: The IDOL model is used to reconstruct an animatable 3D human from the reference image. This model generates highly detailed and realistic representation of the target identity, allowing us to manipulate the avatar to match various poses. Background Inpainting: The video inpainting process restores the regions of the video frame where the original person has been replaced, ensuring seamless background. It involves detecting and tracking the target area using segmentation method, which is initialized and refined by the widely used zero-shot segmentation model, Segment Anything Model (SAM)[30]. Once the target area is segmented and tracked, the remaining regions are completed using the video inpainting method, ProPainter[76], ensuring the background is seamlessly restored with no traces of the replaced identity. Pose Animation: The target pose is extracted from the reference video [5, 36], and the reconstructed human model is animated to match this pose. The IDOL model provides precise control over the 3D humans pose, including fine details such as finger movements, allowing it to adapt dynamically to the reference videos actions. After animating the 3D human, we render it into the target view and seamlessly blend it with the background. Utilizing IDOL, our process offers an efficient and highquality solution for identity replacement in videos, providing greater stability and lower computational cost compared to 2D-based approaches [22, 77]. This opens up new possibilities for digital content creation and interactive media applications. A.5. Representation Comparisons To further illustrate the differences between our method and previous approaches, we provide comparison in Fig. 10. Below, we explain the key differences: Comparison to PIFU: PIFu predicts the 3D human shape directly from given image without leveraging parametric model prior. While effective for simple cases, it often lacks robustness and precision, particularly when handling challenging poses or incomplete observations [60]. Comparison to GTA/SIFU: GTA and SIFU utilize loop optimization [60, 61] to align the reconstructed output with SMPL models. While this alignment step is crucial for pixel-aligned operations [49], it introduces several significant drawbacks: - High computational cost: Loop optimization requires multiple iterations, adding several minutes of processing time. Additionally, it depends on the estimation of intermediate representations such as masks, normals, and skeletons. - Error accumulation: Misalignments during optimization can accumulate over iterations, degrading the quality of the final 3D human reconstruction. Our Approach: In contrast, our method adopts direct and efficient pipeline: We extract image features using large-scale encoder [29], which captures rich and detailed visual information. We then predict the 3D human shape and appearance in uniform space, directly providing the 3D human reconstruction along with the estimated SMPLX parameters. By decoupling feature extraction from SMPL-X-based 3D prediction, our approach avoids the error accumulation inherent in optimization-based methods. When pose information is unnecessary, our method relies primarily on body shape estimation, reducing the dependency on precise pose alignment. Furthermore, our method supports direct animation and editing (e.g., shape and texture), unlocking additional applications and expanding its potential value in digital content creation. Figure 10. Visualization of different approaches for 3D human reconstruction. Unlike PIFu, which directly predicts the 3D human without parametric prior, and GTA/SIFU, which relies on computationally expensive loop optimization for SMPL alignment, our IDOL method leverages SMPL-X as prior. This enables more robust and accurate reconstruction while avoiding the pitfalls of error accumulation. Furthermore, our method supports direct animation and editing, enabling additional applications in digital content creation. B. More Details of IDOL In this section, we describe the training setup and methodology for our proposed method, IDOL. B.1. Implement Details Our models are trained on cluster of 32 NVIDIA H100 GPUs for approximately 1 day, with batch size of 32. The optimization is performed using the Adam optimizer with learning rate of 5e 4. warm-up schedule of 3, 000 steps is employed to stabilize training in the initial stages. The training loss function is weighted combination of VGG perceptual loss and Mean Squared Error, balanced with 1 : 1 ratio. This loss formulation ensures both perceptual quality and pixel-wise accuracy. Sapiens [29] to extract and tokenize human features from the input image. UV-Alignment Transformer. The neck module employs hierarchical design inspired by recent advancements in vision transformer architectures [29], featuring decoder embedding layer with width of 1536 and 16 transformer layers. Each transformer encoder layer consists of the following components: 1. layer normalization operation for input stabilization, enhancing training dynamics, and preventing gradient instability. 2. multi-head self-attention mechanism that maps inputs into query, key, and value representations, followed by linear projection layer to integrate attention outputs. This process is regularized through dropout for improved generalization and further normalized to ensure consistent feature scales. 3. feed-forward network (FFN) composed of two dense layers with GeLU activation function applied between them. The FFN architecture is complemented by intermediate normalization layers to enhance stability and improve optimization convergence. UV Decoder. The decoder begins by reshaping tokens into 2D feature map of 64 64 resolution. It employs hierarchical upsampling and convolutional strategy to progressively refine and synthesize outputs. The upsampling mechanism uses transposed convolutional layers to increase spatial resolution, with each stage incorporating normalization and non-linear activation for stable feature transformations. Specifically: 1. Upsampling Blocks: The decoder incorporates multiple transposed convolutional layers, which double the spatial resolution at each stage. Instance normalization and SiLU activations provide stable scaling and enable nonlinear feature transformations. 2. Convolution Block: Three convolutional layers with output channels {128, 128, 32} further process the features, applying instance normalization and activation functions to improve feature quality and representation. Head Module. Following [69], we construct two distinct convolutional networks for decoding geometry and color separately. These networks progressively process feature channels, transitioning from an initial channel size of 32 to the target parameters δµk , δsk , δrk for geometry and ck for color. B.2. Network Architecture C. Experiment The proposed network consists of multi-stage structure designed for high-dimensional feature extraction and reconstruction tasks. The primary components include the pretrained encoder, UV-Alignment Transformer, and UV decoder. For the encoder, we utilize the large-scale model In this section, we present additional experimental results, including comparison tables and the user study. We show additional visual comparisons in Fig. 14 and Fig. 15. We compare with the reported results by Weng et al. [57] and AlBahar et al. [2]. Figure 11. Visualization of diverse images generated by Flux [18]. Figure 12. Visualization of examples from HuGe100K, where the images are generated by Flux and used to generate multi-view images. Figure 13. Visualization of examples from HuGe100K, where the images are derived from the DeepFashion [38] dataset and used to generate multi-view images. Figure 14. More visualization for comparison in the in-the-wild cases. We compare with the reported results by HumanLRM[57]. Figure 15. More visualization for comparison in the in-the-wild cases. We compare with the reported results by HumanSGD[2]. User Study. We conducted user study with 20 participants via evaluating 50 cases. Participants ranked results based on face, clothing, back-view consistency, and the overall quality. The aggregated results are presented in Tab. 3, showing the superiority of our method."
        },
        {
            "title": "Overall",
            "content": "0% 2.27% 0% GTA SIFU 4.55% HumanLRM 45.45% 43.18% 36.36% 45.45% Ours 52.28% 50.0% 59.09% 2.27% 4.55% 0% 4.55% 50% Table 3. The user study. We evaluated IDOL on selected cases reported by HumanLRM[57], designed to highlight their strengths. Despite the selection for HumanLRM, our method achieves slightly superior performance, demonstrating greater robustness and effectiveness under comparable conditions."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
        "Shenzhen University of Advanced Technology",
        "Tencent",
        "Tsinghua University"
    ]
}