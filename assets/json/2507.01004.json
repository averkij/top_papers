{
    "paper_title": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention",
    "authors": [
        "Yuhong Chou",
        "Zehao Liu",
        "Ruijie Zhu",
        "Xinyi Wan",
        "Tianjian Li",
        "Congying Chu",
        "Qian Liu",
        "Jibin Wu",
        "Zejun Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths."
        },
        {
            "title": "Start",
            "content": "ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention Yuhong Chou1 , Ruijie Zhu3, Xinyi Wan4, Tianjian Li2, Congying Chu5, , Zehao Liu1 , Jibin Wu1 1The Hong Kong Polytechnic University, 2TikTok, 3UC Santa Cruz 4National University of Singapore, 5Institute of Automation, Chinese Academy of Sciences Qian Liu2 , Zejun Ma Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training model with 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on single device. At the heart of ZeCO lies All-Scan, new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 devices with an 8M sequence length, ZeCO achieves 60% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths. Date: July 1, 2025 Correspondence: Jibin Wu (jibin.wu@polyu.edu.hk), Qian Liu (qian.liu@tiktok.com) 5 2 0 2 ] . [ 2 4 0 0 1 0 . 7 0 5 2 : r Figure 1: (Left) ZeCO demonstrates near-linear scaling efficiency when scaling sequence length proportionally with device count, approaching the theoretical upper bound. (Right) ZeCO substantially outperforms SOTA SP methods across three performance metrics: communication time, runtime extra cost, and per-GPU throughput. Metrics obtained using 256 GPUs (8M sequences) for communication / throughput and 128 GPUs (4M sequences) for runtime extra cost. Equal contribution, <yuhong.chou@connect.polyu.hk> <zliu.polyu@gmail.com> Corresponding author ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention 1. Introduction Long-context capabilities are becoming increasingly critical for Large Language Models (LLMs), powering advancements in document-level reasoning, multimodal understanding, and retrieval-augmented generation where extensive context is paramount (OpenAI et al. 2024; Touvron, Lavril, et al. 2023; Touvron, Martin, et al. 2023; Grattafiori, Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Vaughan, et al. 2024a; Gemma Team, Mesnard, et al. 2024; Gemma Team, Riviere, et al. 2024). The trajectory from models like GPT3.5 (4K context) (Brown et al. 2020) to Gemini 1.5 Pro (Gemini Team et al. 2024) (1M context) highlights this trend. However, pre-training models on such vast sequence lengths presents significant computational and communication challenges. Standard self-attention mechanisms exhibit quadratic complexity (O(L2)) with respect to sequence length L. Scaling from 4k to 128k tokens, for instance, inflates the attention FLOPs by over 1000. The prohibitive computational cost of training on ultra-long sequences restricts the long-context pretraining to specialized adaptation phase (i.e., mid-training) (Abdin et al. 2024), rather than allowing for extensive training with long sequences from scratch (Grattafiori, Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Vaughan, et al. 2024b; Qwen et al. 2025; Gu and Dao 2024; Gemma Team, Kamath, et al. 2025; A. Liu, Feng, Bin Wang, et al. 2024; A. Liu, Feng, Xue, et al. 2024). Linear attention models (Katharopoulos et al. 2020) offer an algorithmic solution by replacing the O(L2) softmax attention with operations linear in sequence length, typically O(Ld2) where is the hidden dimension. The efficiency is achieved by compressing the Key-Value (KV) cache, which would otherwise grow with sequence length, into fixed-size hidden state representation. This method effectively eliminates the computational bottleneck of quadratic complexity and balances the per-token computation across the sequence, enabling efficient processing of ultra-long sequences. While linear attention provides these algorithmic advantages, Sequence Parallelism (SP), essential for distributing such computationally intensive workloads, paradoxically becomes bottleneck that impedes efficient scaling across multiple devices. Existing approaches (Weigao Sun, Qin, et al. 2025; Weigao Sun, Lan, et al. 2025; A. Li et al. 2025)) are often hampered by issues such as serial execution dependencies or, more significantly, substantial communication overheads that scale poorly. Consequently, despite the inherent efficiency of linear attention, the practical implementation of SP, particularly its communication burden, has become the primary impediment to achieving high throughput and true scalability for long-context training. In this paper, we introduce ZeCO, novel sequence parallelism strategy designed to overcome the limitations of current methods, particularly for linear attention. ZeCO achieves optimal scalability through fundamental redesign of its communication algorithm and communication-computation scheduling. At the heart of ZeCO lies All-Scan, new collective operator executing pipelined receive-scan-send pattern across devices. This new communication primitive achieves the theoretically optimal communication volume for SP. Crucially, empirical results confirm that All-Scan consistently delivers the lowest latency among all existing SP communication techniques. To further minimize communication overhead, ZeCO intelligently schedules All-Scan to overlap with local device computation, thereby enabling parallel use of both communication and computation resources. Moreover, ZeCO meticulously optimizes the auxiliary computations inherent in SP, effectively reducing their associated I/O and computational overheads to negligible level. In summary, our main contributions can be summarized as follows. 2 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention 1. We introduce ZeCO, novel sequence parallelism method for linear attention models. ZeCO reformulates sequence parallelism by leveraging our All-Scan collective communication, which employs pipelined communication to achieve the theoretically minimum communication volume. This integrated approach enables efficient overlap of communication and computation, incurring minimal extra computational and I/O overhead. 2. We theoretically prove the optimality of ZeCO. The time cost analysis of different sequence parallelism strategies demonstrates that ZeCO constitutes the minimum required cost, establishing its efficiency. 3. Comprehensive multi-level experiments (collective communication, operator, and model) demonstrate the significant performance gains of ZeCO. As shown in Figure 1, the All-Scan collective achieves up to 3.9 communication speedup, the fastest existing sequence parallelism method, while the ZeCO sequence parallel operator delivers up to 9.3 overall speedup. At the model level, ZeCO boosts throughput by over 60% and demonstrates near-linear scalability from 8 to 256 devices, even with context lengths up to 8M tokens. 2. Background & Related Work In this section, we firstly provide brief background on Gated Linear Attention (GLA), general linear attention operator that encompasses family of linear attention mechanisms (Qin, Weixuan Sun, et al. 2022; Y. Sun et al. 2023; Qin, S. Yang, et al. 2024; Dao and Gu 2024; S. Yang, Kautz, and Hatamizadeh 2024; S. Yang, Bailin Wang, Y. Zhang, et al. 2024). Then, we introduce the existing sequence parallelism methods. We use bold upper-case letters (e.g., Q) to denote matrices, And the same alphabet to represent rows of matrix, such that Qt refers to the t-th row of Q. Unless otherwise specified, denotes the number of devices, denotes the sequence length per device, denotes the hidden dimension, denotes the number of attention heads, and denotes the chunk length. 2.1. Recurrent and Chunk-wise Form of General Linear Attention The linear attention mechanism uses the kernel trick to remove the softmax computation in full attention and exchange the calculation order to reduce the attention computational complexity from quadratic to linear (Katharopoulos et al. 2020; Choromanski et al. 2022). There are many ways to implement this mechanism. Sevaral works (S. Yang, Bailin Wang, Shen, et al. 2024; Qin, S. Yang, et al. 2024; Chou et al. 2024) has summarized unified form of (diagonal decay) linear model. In this paper, we use gated linear attention (GLA) operator (S. Yang, Bailin Wang, Shen, et al. 2024), one of the generalization forms of linear models, to demonstrate our algorithm. The attention state is updated recurrently as: St = (α 1) St1 + V t, Ot = QtSt, (1) (0, 1) dk is decay factor.To enable efficient parallelism during training, the sequence is partitioned where α into chunks of length C, and the recurrence is reformulated in chunkwise manner. Let chunk include . Let S[i] Rdd be the chunk-level hidden state tokens from iC to (i + 1)C 1, with decay vectors αiC+j after processing chunks, i.e., S[i] , j=1 αiC+j iC+j = b(i+1)C . The chunk-level GLA state and Token-wise scaling: Γ biC+j . GLA define, Cumulative decay for the chunk: γ[i] iC+j = biC+j biC , where bt = = SiC , Λ = s=1 αs output are calculated as: S[i] = (γ [i]1) S[i1] + (K[i] Γ [i]) [i], (2) 3 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention The output of each chunk should be computed as: O[i] = (Q[i] Λ [i]) S[i1] (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) inter [i] Λ + [((Q[i] [i]) (K[i] ) M] [i] , (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) intra [i] Γ [i]) (3) the inter-chunk recurrently updates the global state, while the intra-chunk term handles mask attention computation on the diagonal. 2.2. Sequence Parallelism for Linear Attention Models LASP1 (Weigao Sun, Qin, et al. 2025) adopts chunkwise parallelization strategy by dividing the input sequence into multiple contiguous chunks and evenly distributing them across devices. Each device serially computes the output of each device based on linear attention formula. For communication, each device receives the state from the previous block and updates it before passing it to the next device. Although this avoids redundant communication volume, it enforces strict serial order to be executed across devices, causing the total computation time to grow linearly with the number of devices, which severely limits parallel efficiency and throughput. LASP2 (Weigao Sun, Lan, et al. 2025; A. Li et al. 2025) follows similar chunkwise computation structure, but replaces the serial state passing with All-Gather communication. Each device must first collect the local states from all other devices and subsequently perform an identical scan operation on the same data. This enables devices to do computation parallelism, but introduces substantial communication overhead. The total communication volume grows linearly with the number of devices, as each device must collect state tensors from all others. 2.3. Sequence Parallelism for Full Attention The sequence parallelism in Megatron-LM (Shoeybi et al. 2020) (also known as Context Parallelism) and Ring Attention (H. Liu, Zaharia, and Abbeel 2023; Brandon et al. 2023) achieve global synchronization of KV blocks in either an all-at-once or pipelined manner, based on All Gather and P2P communication respectively, and have been widely adopted (Grattafiori, Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Vaughan, et al. 2024b; Weigao Sun, Lan, et al. 2025). Ulysses (Jacobs et al. 2023)distributes the computation of different self-attention heads across devices, which is easy to implement but incompatible with tensor parallelism (TP) and limited by the number of heads. Ring Self Attention (S. Li et al. 2021) was the earliest method to propose full attention sequence parallelism, but it does not leverage the I/Oefficient optimizations of self-attention (Dao, Fu, et al. 2022; Dao 2023; Shah et al. 2024; Rabe and Staats 2021), which limits its applicability. Nevertheless, sequence parallelism for full attention is fundamentally constrained by the self-attention algorithm itself: even disregarding communication, the computational cost becomes prohibitively expensive for ultra-long sequences due to the algorithms inherent complexity. 3. Method 3.1. ZeCO Sequence Parallel Methods and Communication Requirements Let each device be assigned sequence of length L, which is partitioned into = L/C. After projection of input RLd, partitioned into = L/C, we get non-overlapping chunks Q[n], K[n], G[n] for N. We use Si, PL to denote the Global states, and S[n], denote local chunk states on each device. 4 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention Figure 2: Illustration of ZeCO. ZeCO highlights its strengths in three dimensions: (1) Parallel Scalability: achieving efficiency comparable to DP (sub-figure: Algorithm Comparison); (2) Operator-Level Computation: enabling overlap of communication and local computation for maximal resource utilization (sub-figure: ZeCO Computation); and (3) Communication Pattern: utilizing customized pipelined All-Scan Communication pattern to substantially reduce inter-device synchronization delays. (sub-figure: All-Scan Communication) Local State Computation. According to Equation (2). Within each device, we sequentially compute the local states starting from the initial state S[0] = 0: S[n] = (γ [n]1) S[n1] + [n]V [n], for = 1, . . . , N. (4) Compared to GLA, we additionally maintain the cumulative decay vector γ[n] plicative decay from the first to the n-th chunk: , which saves the total multiγ[n] = i= γ[i], for = 1, . . . , N. (5) At the end of this local recurrence, we obtain list of local chunk states {S[0], S[1] . . . S[N]} Global State Update. The recurrence for the global state is defined as Equation (1). To obtain the series of from device 1. Then we global states of the device p, the device must get the last global state S(p1)L 5 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention can update the global State of the current device by applying: S(p1)L+nC = ( γ [n]1) S(p1)L + S[n]. (6) to its global state S(p1)L+nC is independent across chunks, we can first update S[N] of device p, and send it to enable the global state update in + 1 device. We give proof of the above global update computation in Appendix A.1. Since updating each local state to the last global S[n] state SpL To fulfill this communication requirement, we propose the All-Scan Collective Communication operator in Section 3.2. All-Scan Communication is overlapped with the local computations that do not depend on communication; in practice, we compute the diagonal attention scores simultaneously as shown in Figure 2. All-Scan allows ZeCO to parallelize both inter-device communication and intra-device computation. In implementation, ZeCO rearranges the standard form of GLA with minimal extra computation and I/O cost, so as to achieve efficient sequential parallel training Algorithm 1. 3.2. All-Scan Collective Communication To convert local chunk states in each device into globally consistent values, each device requires the final state S(p1)L from its predecessor. It presents dependency chain between devices, which will cause communication latency related to the number of devices. To address this efficiently, we propose an All-Scan Collective Communication strategy to receive, update, and send. Specifically, All-Scan splits large state tensors into smaller segments that can be sequentially transmitted and processed. Pipelined State Scan. Rather than receive the full state S(p1)L into contiguous blocks to send from device 1: , we partition it along the dk dimension S(p1)L = [S(1) (p1)L, S(2) (p1)L, . . . , S(K) (p1)L] , S(k) (p1)L dk dv. (7) Correspondingly, the decay factor is split into aligned segments γ transmitted pipelined from 1 to p, and immediately applies the update and send: R1 dk (j) [N] . Each block of state is S(k) pL = ( γ (k) [N] 1) S(k) (p1)L + S(k) [N] for = 0, ..., (8) in All-Scan as soon as it This design enables device + 1 to begin updating its last global state S(p+1)L receives the first block of SpL , as shown in Algorithm 2. As Communication Primitive, All-Scan could run independently with other CUDA stream, achieves fine-grained communication-computation overlap, maximizing device utilization and throughput in long-context training. 3.3. Optimality Analysis We now formally establish that the sequence parallelism strategy using the All-Scan collective communication algorithm could achieve theoretical optimality for linear attention SP. We identify two necessary and sufficient conditions for optimality and prove that ZeCO satisfies both: 1. Zero Communication Overhead: Each device transmits and receives only the minimal essential size of information (data). No redundant communication. ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention Algorithm 1 Forward pass for ZeCO with All-Scan comunication 1: Note: The Highlighted part represents the SP adaptation of GLA Algorithm by ZeCO algorithm, and the lower cost of the red part represents the lower extra cost of SP. 2: Input: Q, K, RLdk , RLdv, = [α1...αL] RLdk , chunk size C, num_device P, device_rank blocks {Q[1]...Q[N]}, {K[1]...K[N]}, {G[1]...G[N]} of size dk each. Divide {0, 1, . . . , 1} 3: Divide Q, K, into = into blocks {V [1]...V [N]} of size dv each. dv, γ = 1 Rdk on SRAM from HBM to SRAM. [n] RCdk and K[n] = K[n] Γ [n], γ = γ γ[n] [n]1) + [n]V [n] . 4: Initialize = 0 Rdk 5: Write γ, to HBM as γ[0], S[0] 6: for 0, do 7: . 8: 9: 10: Load K[n], G[n], [n] Rdk , Γ On chip, compute γ[n] Write γ to HBM as γ[n] . On chip, compute = (γ . Write to HBM as S[n] 11: 12: end for 13: In parallel do: 14: parallel stream 1: 15: S(p1)L, SpL All-Scan(S[N], γ[N]) 16: parallel stream 2: 17: parfor 1, do 18: 19: 20: 21: Load Q[n], K[n], G[n] RCdk from HBM to SRAM. On chip, construct causal mask RCC On chip, compute Λ On chip, compute = ( Q[n] Write as P[i] [n], RCdk , Q[n] [n]) RCC = Q[n] to HBM. Λ [n], K[n] = K[n]/Λ [n] 22: 23: end parfor 24: stream barrier 25: for 1, do 26: 27: 28: 29: 30: Load Q[n], G[n], [n], S(p1)L, S[n], γ[n1], from HBM to SRAM. On chip, compute Λ [n] On chip, compute Q[n] On chip, compute On chip, compute O[n] = to HBM. Store O[n] Λ = Q[n] [n] = Q[n](S[n1] intra inter + [n-1]1) S(p1)L), inter [n] +( γ 31: 32: end for 33: return = {O[1]...O[N]}, = {S(p1)L, S[1]...S[N], SpL}. intra = PV [n] RCdv 2. Optimal Extra Cost: Communication is overlapped with other computations as much as possible to minimize idle computational resources. Furthermore, the additional computation and I/O overhead introduced by SP is reduced to minimum. 7 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention send_rank = + 1, recv rank = 1 for device p, start = 0, last = compute send_rank = 1, recv_rank = + 1 for device p, start = 1, last = 0 Algorithm 2 All Scan Algorithm 1: Input: num_device P, device_rank [P], Local State Slocal, factor γ, the direction tag DIR 2: if DIR == FWD then 3: 4: else if DIR == BWD then 5: 6: end if 7: Initialize the send state as Ssend, 8: if is not 0 then 9: 10: end if 11: Slice Srecv, Ssend, Slocal, γ alone the first dimension in blocks. 12: for 0, 1 do if is start then 13: Receive state from recv_rank as Srecv , Single Direction Communication 14: 15: 16: 17: 18: 19: Send Slocal to recv_rank else if is not 0 then (k) send = Sk Sk local + ( γ Send Slocal to recv_rank [N]1) Sk recv else if is last then send = Sk Sk local + ( γ end if 20: 21: end for 22: return Srecv, Ssend (k) [N]1) Sk recv dv denote the accumulated state tensor. According to linear Zero Communication Overhead Let Rdk attention output Equation (3), this state represents the minimal information that must be communicated between chunks. For sequence distributed across devices, any SP algorithm must communicate at least the state information (p) across device boundaries. Let ZeCO denote the communication volume of the p-th device in ZeCO (All-Scan) Each device sends last global state to the next device exactly once, resulting in: (p) ZeCO = = dk dv. (9) This represents the theoretical lower bound. However, existing approaches such as LASP-2 rely on all-gather operations, receive local states from all the other 1 devices, resulting in communication volume of (P 1) dk Which grows linearly with P. As shown in Figure 3, ZeCO achieves the minimal communication volume possible for the SP scenario. , which increases with the number of devices. dv Optimal SP strategy Let TP devices with sequence parallelism, and let T1 length PL on single device. Let TP SP(L) denote the total runtime for processing sequence of length using SP(PL) represent the runtime for processing total sequence of ideal-SP(PL) denote the runtime under ideal conditions, assuming perfect 8 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention parallelism with zero additional overhead. For an ideal SP, the following properties should be satisfied: TP ideal-sp(PL) = T1 ideal-SP(L) = T1 ideal-SP(PL) . (10) The implication of Equation (10) is that, in the ideal case, the throughput of sequence parallelism should scale linearly with the number of devices (i.e., the processing time is inversely proportional to the throughput). In practice, however, sequence parallelism introduces additional overhead. Therefore, we next analyze the latency under practical scenarios. In practice, the prerequisite communication latency, computation, and I/O for transferring and synchronizing data between devices introduce additional latency. For ZeCO (and other sequence parallelism), the relation becomes ideal sequence parallelism cost + extra cost, which can be formularized as follow: SP(PL) = TP TP ideal-SP(PL) + Textra_comp&I/O + (TAll_Scan Toverlaped_comp). (11) For ZeCO, the first two components are independent of communication. The last two represent communication latency, accounting for the portion of the All-Scan operator that cannot be overlapped by local diagonal attention (we suppose the worst-case scenario). This can also be viewed as the gap relative to the ideal SP. From Equation (11), we can see that TP ideal-SP(PL) and Toverlaped_comp are inherent to the algorithm. Therefore, the key question in SP is to what extent the additional time cost of linear attention, namely Textra_comp&I/O and TAll_Scan, can be reduced. The following presents how ZeCO successfully achieves the optimal by giving the analysis of the optimality of All-Scan communication. And proof extra_computation&I/O is negligible system cost. dv, and transmit these blocks in pipelined fashion In All-scan, we partition into blocks, each of size as shown in Figure 2. By partitioning the state into blocks and updating them in pipeline, the effective communication latency could be computed as: dk TAll_Scan = τ(dk dv) + (P 1)τ(dk dv) , (12) where τ() represents the time required to communicate tensor of the given size. Equation12 shows the two components of the cost TAll_Scan. The first term represents the overhead that can be parallelized by the pipelined approach, which is necessary and corresponds to the minimum communication requirement. The second term accounts for the overhead at the boundaries. As increases, the boundary overhead decrease, and the degree of overlap improves. Consequently, when becomes sufficiently large, the boundary overhead approaches zero. Thus, ZeCO with All-Scan achieves the minimal time cost of communication. in lines 9 and 26 are vector, constituting only 1 dv The term Textra_comp&I/O consists of two parts: small number of additional floating-point operations, and HBM load and store operations for few auxiliary tensors. In Algorithm 1, the load and store operations for γ[n] of the state tensor. The required additional state overhead. For sequence length of 8192 and chunk size of 64 can be reused times, incurring just 1 (N = 128), which is comparable to typical dv , the added overhead is less than 1%. The cost of element-wise multiplications is negligible. Hence, Textra_comp&I/O can be safely ignored in practice. It proved that the time of ZeCO Equation (11) should be: TP ZeCO(PL) = T1 T1 ideal-SP(L) Toverlaped_comp + τ(dk ideal-SP(L) Toverlaped_comp + τ(dk dv) + ϵ dv), (13) (14) 9 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention Figure 3: ZeCO has the lowest communication time while satisfying the lowest communication volume. The left two figures show the theoretical values of the algorithm calculation speed and communication volume, and the right figure shows the actual communication time. where ϵ represents negligible computation and I/O cost. In contrast, existing methods like LASP have strictly serial dependency across devices, resulting in (We assume that the Textra_comp&I/O term in other methods can also be optimized to negligible level. Even so, these methods remain suboptimal. ): TP LASP(PL) = (T1 ideal-SP(L) + τ(dk dv)) > TP ZeCO(PL). (15) While LASP-2 improves on LASP with parallel computation, but suffers from higher communication cost: TP LASP-2(PL) = T1 ideal-SP(L) + τ(dk dv) > TP ZeCO(PL). (16) Thus, ZeCO achieves zero communication overhead and an optimal SP strategy with minimum extra cost. This optimality translates directly to superior performance. In Figure 3, we show the theoretical values of communication cost and computational overhead, and the actual values of communication time for different SP algorithms. We also present unified communication and runtime analysis of existing SP algorithm in Appendix A.2. 4. Experiments We evaluate the efficiency and scalability of the proposed ZeCO SP Algorithm and All-Scan Communication Operator on 1B-GLA models. Our assessment focuses on two aspects: (1) Communication speed of different Collective Communication Operators; (2) The Algorithm-level and model-level scalability under increasing GPU count. All experiments are conducted on GPU cluster equipped with 256H100 80GB GPUs. Model is trained in Lingua (Videau et al. 2024), PyTorch-based distributed training framework. To ensure fair comparison with baseline sequence parallelism (SP) methods such as LASP1 (Weigao Sun, Qin, et al. 2025) and 10 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention Figure 4: Scalability evaluation of LASP on SP operator runtime(top half) and Scalability evaluation of LASP on Throughput(bottom half). In the comparison test of 16k and 32k sequence length per GPU, ZeCO algorithm shows the same stable time as the DP algorithm. In both 16k and 32k, ZeCO exhibits linear scaling curve of throughput growth approach to DP, while the other methods degenerate. LASP2 (Weigao Sun, Lan, et al. 2025), we adapt the chunk-wise gated linear attention operator from the Flash Linear Attention (S. Yang and Y. Zhang 2024) repository for our implementation. The complete experimental setup and data are provided in the Appendix A.3. 4.1. Communication Speed In this experiment, we evaluate the communication Runtime of different communication operators under their own communication workload sufficient for correct training. Experiments are conducted with from 8 to 256 GPUs, and each GPU is assigned 8K sequence length. We warm up each communication kernel for 5 rounds, then report the average over 50 runs. More details of communication workloads and protocol differences are discussed in Appendix A.2. As shown in Figure 3, memory-out occurred in the experiments of 128 GPUs and 256 GPUs All-gather (Megatron). For other methods, it should be noted that, for presentation purposes, the upper half of the Y-axis represents the rendering results after taking the log scale. All-Scan significantly outperforms other methods in different scales of clusters. Notably, on 256 GPUs, All-Gather (LASP2) is 4 slower than All-Scan. 4.2. SP Algorithm Runtime and Model Throughput Next, we evaluate both micro-level (algorithm) and macro-level (model training) performance for Linear Attention SP methods, including LASP1, LASP2, and ZeCO. 11 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention SP Algorithm Runtime We measure the forward and backward pass time of each SP operator under the same setting (L = 16K or 32K, = 16) and compare it against the ideal case of DP operator. The time of the DP operator serves as the theoretical lower bound. Figure 4 demonstrates that in the 128 GPUs experiment (2M and 4M sequence length), ZeCO is only 3 ms slower than the theoretical lower bound for single forward and backward pass, which satisfies our analysis in Section 3.3 and demonstrates the optimality of our algorithm. Model Throughput We experimented with 1B-GLA models with different sequence parallel methods to test the training throughput, under the same setting (L = 16K or 32K, = 16) and compare it against the ideal case of Model that uses DP. The throughput of the GLA model uses DP in training, serves as the theoretical upper bound. For the result shown in Figure 4, as the number of GPUs increases, ZeCO achieves linear increase in total throughput, which meets the original intention of sequence parallelism, while other methods experience serious degradation. 5. Conclusion and Future Works In this work, we propose ZeCO sequence parallelism for linear attention, achieving SOTA for both theoretical and empirical results. More importantly, our method fully unleashes the algorithmic efficiency of linear models and, for the first time, enables near-linear throughput scaling for sequence parallelism. At the system level, our approach introduces the novel All-Scan collective communication primitive, which not only underpins the efficiency of ZeCO but also provides foundational innovation for advancing distributed computing in the linear model community. In the future, we plan to pursue three main directions. First, we will further improve the algorithmic implementation of the All-Scan collective communication primitive. For example, tree-like implementation. Second, we aim to generalize the sequence parallelism algorithm for linear attention beyond diagonal decay, extending it to support various forms, including matrix transform structures. Third, we will investigate efficient parallel topologies for sequence parallelism in large-scale models. 12 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention"
        },
        {
            "title": "References",
            "content": "Abdin, Marah et al. (2024). Phi-4 Technical Report. In: CoRR abs/2412.08905. doi: 10.48550/ARXIV. 2412.08905. arXiv: 2412.08905. url: https://doi.org/10.48550/arXiv.2412.08905. Brandon, William et al. (2023). Striped attention: Faster ring attention for causal transformers. In: arXiv preprint arXiv:2311.09431. Brown, Tom et al. (2020). Language models are few-shot learners. In: Advances in neural information processing systems 33, pp. 18771901. Choromanski, Krzysztof et al. (2022). Rethinking Attention with Performers. arXiv: 2009.14794 [cs.LG]. ur l: https://arxiv.org/abs/2009.14794. Chou, Yuhong et al. (2024). MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map. arXiv: 2411.10741 [cs.LG]. url: https://arxiv.org/abs/2411.10741. Dao, Tri (2023). Flashattention-2: Faster attention with better parallelism and work partitioning. In: arXiv preprint arXiv:2307.08691. Dao, Tri, Dan Fu, et al. (2022). Flashattention: Fast and memory-efficient exact attention with io-awareness. In: Advances in neural information processing systems 35, pp. 1634416359. Dao, Tri and Albert Gu (2024). Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In: arXiv preprint arXiv:2405.21060. Grattafiori, Aaron, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. (2024a). The llama 3 herd of models. In: arXiv preprint arXiv:2407.21783. Grattafiori, Aaron, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. (2024b). The Llama 3 Herd of Models. arXiv: 2407.21783 [cs.AI]. url: https://arxiv.org/abs/2407.21783. Gu, Albert and Tri Dao (2024). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv: 2312.00752 [cs.LG]. url: https://arxiv.org/abs/2312.00752. Jacobs, Sam Ade et al. (2023). DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. arXiv: 2309.14509 [cs.LG]. l: https://arxiv.org/abs/2309. 14509. Katharopoulos, Angelos et al. (2020). Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. arXiv: 2006.16236 [cs.LG]. url: https://arxiv.org/abs/2006.16236. Li, Aonian et al. (2025). Minimax-01: Scaling foundation models with lightning attention. In: arXiv preprint arXiv:2501.08313. Li, Shenggui et al. (2021). Sequence parallelism: Long sequence training from system perspective. In: arXiv preprint arXiv:2105.13120. Liu, Aixin, Bei Feng, Bin Wang, et al. (2024). Deepseek-v2: strong, economical, and efficient mixture-ofexperts language model. In: arXiv preprint arXiv:2405.04434. Liu, Aixin, Bei Feng, Bing Xue, et al. (2024). Deepseek-v3 technical report. In: arXiv preprint arXiv:2412.19437. Liu, Hao, Matei Zaharia, and Pieter Abbeel (2023). Ring attention with blockwise transformers for nearinfinite context. In: arXiv preprint arXiv:2310.01889. OpenAI et al. (2024). GPT-4 Technical Report. arXiv: 2303.08774 [cs.CL]. url: https://arxiv.org/ abs/2303.08774. Qin, Zhen, Weixuan Sun, et al. (2022). cosformer: Rethinking softmax in attention. In: arXiv preprint arXiv:2202.08791. Qin, Zhen, Songlin Yang, et al. (2024). Hgrn2: Gated linear rnns with state expansion. In: arXiv preprint arXiv:2404.07904. 13 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention Qwen et al. (2025). Qwen2.5 Technical Report. arXiv: 2412.15115 [cs.CL]. url: https://arxiv.org/ Rabe, Markus and Charles Staats (2021). Self-attention does not need O(n2) memory. In: arXiv preprint abs/2412.15115. arXiv:2112.05682. Shah, Jay et al. (2024). Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In: Advances in Neural Information Processing Systems 37, pp. 6865868685. Shoeybi, Mohammad et al. (2020). Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv: 1909.08053 [cs.CL]. url: https://arxiv.org/abs/1909.08053. Sun, Weigao, Disen Lan, et al. (2025). LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid. arXiv: 2502.07563 [cs.LG]. url: https://arxiv.org/abs/2502.07563. Sun, Weigao, Zhen Qin, et al. (2025). Linear Attention Sequence Parallelism. arXiv: 2404.02882 [cs.LG]. url: https://arxiv.org/abs/2404.02882. Sun, Yutao et al. (2023). Retentive network: successor to transformer for large language models. In: arXiv preprint arXiv:2307.08621. Team, Gemini et al. (2024). Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv: 2403.05530 [cs.CL]. url: https://arxiv.org/abs/2403.05530. Team, Gemma, Aishwarya Kamath, et al. (2025). Gemma 3 technical report. In: arXiv preprint arXiv:2503.19786. Team, Gemma, Thomas Mesnard, et al. (2024). Gemma: Open models based on gemini research and technology. In: arXiv preprint arXiv:2403.08295. Team, Gemma, Morgane Riviere, et al. (2024). Gemma 2: Improving open language models at practical size. In: arXiv preprint arXiv:2408.00118. Touvron, Hugo, Thibaut Lavril, et al. (2023). LLaMA: Open and Efficient Foundation Language Models. In: arXiv preprint arXiv:2302.13971. Touvron, Hugo, Louis Martin, et al. (2023). Llama 2: Open foundation and fine-tuned chat models. In: arXiv preprint arXiv:2307.09288. Videau, Mathurin et al. (2024). Meta Lingua: minimal PyTorch LLM training library. l: https:// github.com/facebookresearch/lingua. Yang, Songlin, Jan Kautz, and Ali Hatamizadeh (2024). Gated Delta Networks: Improving Mamba2 with Delta Rule. In: arXiv preprint arXiv:2412.06464. Yang, Songlin, Bailin Wang, Yikang Shen, et al. (2024). Gated Linear Attention Transformers with HardwareEfficient Training. arXiv: 2312.06635 [cs.LG]. url: https://arxiv.org/abs/2312.06635. Yang, Songlin, Bailin Wang, Yu Zhang, et al. (2024). Parallelizing linear transformers with the delta rule over sequence length. In: arXiv preprint arXiv:2406.06484. Yang, Songlin and Yu Zhang (Jan. 2024). FLA: Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism. url: https://github.com/fla-org/flash-linear-attention. 14 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention A. Appendix A.1. Global Chunk Update Proof We prove the correctness of the global correction formula used in Equation (6) of the main text, which expresses the global state at position (p 1)L + nC as: S(p1)L+nC = ( γ[n]1) S(p1)L + S[n]. (17) We begin from the chunkwise recurrence of the Gated Linear Attention (GLA) state update within each device. For any chunk n, the recurrence is: [n]1) S[n1] with initial state S[0] = 0. Unfolding the recurrence, we obtain the closed-form expression of the final local state S[n] S[n] = (γ + (18) : [n]V [n], S[n] = i=1 j=i+1 [j]1 γ ( [i]V [i]) + j=1 [j]1 γ S[0]. (19) Equation 19 represents the result of local computation, it captures the final local state obtained by starting from zero initial state and considering only the local contribution within the current chunk n. The key observation is that the second term (n vanishes due to the initial condition S[0] = 0, making completely determined by local information. We now demonstrate the linear decomposition property of S[n] global state updates. The key insight is that when non-zero initial state exists, the final global state can be decomposed into two independent linear contributions: attenuated propagation of global computation and the current chunks local contribution. Now suppose we instead perform the same recurrence starting from non-zero initial state S(p1)L , which is the final global state of the previous device. The updated state at global index (p 1)L + nC becomes: [j]) S[0] j=1 γ S(p1)L+nC = γ i=1 [j]1 j=i+1 = ( γ[n]1) S(p1)L + S[n]. ( [i]V [i]) + j=1 [j] γ (0 + S(p1)L) (20) (21) This linear property allows local computation S[n] multiplying the incoming global state S(p1)L global state. stores only the residual contribution from chunk n, and precisely reconstructs the full by the cumulative decay γ[n] A.2. Unified Analysis of Sequence Parallel Methods In this section, we use multi-head attention with head = 32, dk = dv = = e. We present unified analysis of several representative sequence parallel (SP) methods across both full attention and linear attention models. Specifically, we compare them from the following three perspectives: Communication Volume: The total amount of data transferred per device during SP execution. Computation Cost: The total computation time to process sequence of length PL in parallel. Additional Computation Overhead: The extra operations introduced due to SP-specific logic. ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention Full Attention Models. Ulysses: Uses All-to-All communication to exchange Q, K, V, and Output tensors. Communication volume is 4LD per device. Due to full attentions quadratic complexity, the computation cost is L2DP. Megatron CP: Utilizes All-Gather to collect and from all devices. Communication volume is 2PLD. Computation cost is the same as Ulysses, L2DP. In linear attention, since each device processes sequence of length L, the Linear Attention Models. inherent computation per device is LDe (here we ignore the additional lower-order terms introduced by the chunk-wise algorithm), which is independent of P. LASP-1: Employs serial P2P communication. Each device transmits single state tensor RHee, with communication volume De. However, the devices execute sequentially, resulting in an equivalent time overhead (including both communication and computation) as if each device performed times the workload. LASP-2: Uses All-Gather to collect all intermediate state tensors across devices. Each device processes all global states, leading to PDe communication volume and additional computation cost of log(P)De + NDe for sum reduction and state updates. ZeCO (Ours): Implements pipelined communication via All-Scan. Each device sends/receives only one state S, with communication volume De. It additionally maintains cumulative decay vectors γ and updates intermediate states using global recurrence. So, here is an extra computation cost NDe + Nd. In conclusion, for sequence parallelism with full attention, both the computation cost and the number of device parameters are strongly dependent on the number of devices P, which becomes major efficiency bottleneck. In the case of linear attention, although both the communication and computation costs of LASP-1 and LASP-2 scale with P, the computation cost constitutes only small fraction of the total overhead. As result, the communication costs dependence on becomes the primary bottleneck. In contrast, our ZeCO algorithm achieves both communication and computation costs that are independent of the number of devices P. Table 1: Comparison of Sequence Parallel Methods: Communication Volume and Computation Cost (For LASP-1, we consider the sequential execution order)"
        },
        {
            "title": "Communication Volume Computation Cost",
            "content": "Ulysses (Full) Megatron CP (Full) LASP-1 (Linear) LASP-2 (Linear) ZeCO (Ours, Linear) De 4LD 2PLD PDe PDe L2DP L2DP PLDe LDe + log(P)De + NDe LDe + NDe + Nd A.3. Experimental Setting and Supplementary Data In experiment Section 4.1, is 32, the tensor size of each chunk of segmentation is 16384, the hidden dimension is 4096, and sequence length per device is 8192. The experimental setup with 5 rounds of warm-up and 50 rounds of experiment was averaged, see in Table 2. ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention In experiment Section 4.2, In the experiment of algorithm run time, we test the GLA-attention algorithm equipped with different SP methods, record the time of 1 iteration of FWD and BWD. is 16, the tensor size of each chunk of segmentation is 16384, the hidden dimension is 2048, and sequence length per device is 16384 and 32768. The experimental setup with 5 rounds of warm-up and reported the average of 50 rounds of experiment, see in Table 3, Table 4. In the experiment of Model throughput, we test the GLA-1B Model equipped with different SP methods, and record the throughput in the training stage. is 32, the tensor size of each chunk of segmentation is 16384, the number of model layers is 20, the hidden dimension is 2048, and the sequence length per device is 16384 and 32768. The experimental setup with 5 rounds of warm-up reported the average of 100 steps of the experiment, see in Table 5, Table 6. Table 2: Communication Runtime GPU Number All Gather Linear 8 16 32 64 128 0.37488 0.65769 1.61594 2.54305 4.35 8.51388 Table 3: Algorithm Runtime (16k sequence length) GPU Number 8 16 32 64 128 LASP1 22.59ms 28.20ms 39.03ms 61.18ms 113.71ms LASP2 19.39ms 24.48ms 25.72ms 29.17ms 35.72ms Table 4: Algorithm Runtime (32k sequence length)"
        },
        {
            "title": "GPU Number",
            "content": "8 16 32 64 128 LASP1 41.64ms 52.14ms 72.97ms 119.79ms 217.20ms LASP2 27.57ms 30.80ms 33.08ms 39.44ms 42.50ms Method All-Scan 0.22578 0.29686 0.44775 0.73899 1.27166 2.16454 Method"
        },
        {
            "title": "ZeCO",
            "content": "7.32ms 7.45ms 7.65ms 8.04ms 9.88ms"
        },
        {
            "title": "ZeCO",
            "content": "12.12ms 12.41ms 12.98ms 13.56ms 15.06ms All Reduce 0.1375 0.22803 0.31073 0.41486 0.50084 0.60405 GLA (baseline) 6.39ms 6.47ms 6.44ms 6.12ms 6.55ms GLA (baseline) 11.76ms 11.74ms 11.74ms 11.79ms 11.74ms 17 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention Table 5: GLA Model SP Throughput/GPU (tokens/sec) on 1B-16k Method GPU Number 8 16 32 64 128 256 LASP1 26428 22244 17813 12596 - - LASP2 37812 31415 29802 27166 22386 15196 Table 6: GLA Model SP Throughput/GPU (tokens/sec) on 1B-32k Method"
        },
        {
            "title": "GPU Number",
            "content": "8 16 32 64 128 256 LASP1 27014 23302 18129 12268 - - LASP2 42946 41190 39669 37485 33327 25402 ZeCO 44497 42328 40463 39832"
        },
        {
            "title": "ZeCO",
            "content": "47369 46209 45091 44468 43278 40967 GLA (baseline) 47594 46786 46214 45744 44847 42838 GLA (baseline) 49633 49058 47980 48230 47848 46588 A.4. Backward pass for ZeCO with All-Scan comunication In the backward propagation of the ZeCO algorithm, most of the process is similar to the forward propagation. denotes the decay factor for the reverse cumulative It is important to note the difference in notation here: γ[n] product. Furthermore, in the official implementation of gated linear attention, S[n] needs to be recomputed during the backward pass. However, since the global initial state has already been obtained during the forward pass, there is no need for all-scan communication when recomputing S[n] . 18 ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention Algorithm 3 Backward pass for ZeCO with All-Scan comunication Input: Q, K, RLdk , , dO RLdv, chunk size C, num_device P, device_rank {0, 1, . . . , 1} Initialize dS = 0 Rdk 1: for to 0 do 2: RCdk , dO[n] RCdv from HBM to SRAM dv on SRAM and Q[n] = Q[n] G[n], γ = γ γ[n] [n] [n]1) dS + [n]dO[n] with S[0] = S(p1)L , = {0, 1, 2, . . . , 1} 3: 4: 5: Load G[n] RCdk , Q[n] On chip, compute γ[n], Γ Store γ in HBM as γ[n] On chip, compute dS = (γ Store dS in HBM as dS[n] 6: 7: end for 8: In parallel do: 9: parallel stream 1: 10: dS(p1)L, dSpL All-Scan(dS[0], γ[0]) 11: parallel stream 2: 12: Load S(p1)L 13: On chip, recompute S[n] 14: Store {S[n], {0, 1, 2, . . . , 1}} 15: for 1 to in parallel do 16: from HBM to SRAM from HBM to SRAM dv, from HBM to SRAM Load Q[n], K[n], G[n], [n], dO[n] Load , Rdk On chip, construct causal mask RBB [n] RCdk On chip, compute Λ [n], Γ Λ On chip, compute Q[n] [n], K[n] = K[n] = Q[n] [n]) RCC On chip, compute P[n] = ( Q[n] On chip, compute dP[n] = (dO[n]V [n]) On chip, compute K[n] = [n]dP On chip, compute dK[n] = K[n]/Λ On chip, compute Q[n] = dP K[n] On chip, compute dQ[n] = Q[n] in HBM. Store P[n], dQ[n], dK[n] Λ [n] [n] Γ [n] 27: 28: end for 29: stream barrier 30: for 1 to in parallel do 31: [n], Γ Load P[n], dQ[n], dK[n], dO[n], Q[n], K[n], G[n], γ[n-1], dSpL, S[n1], from HBM to SRAM [n] RCdk On chip, compute Λ On chip, compute K[n] = K[n] Γ [n] On chip, compute K[n] = [n](dS [n1] + K[n] On chip, compute dK[n] = dK[n] On chip, compute Q[n] [n1] On chip, compute dQ[n] + Q[n] On chip, compute dV [n] = Store dK[n], dV [n] = dO[n]S = dQ[n] [n]dO[n] Λ [n] + K[n](dS [n1] [n-1]1) dS [n-1]1) dS +( γ Γ in HBM +( γ pL) pL) [n] 39: 40: end for 41: Let dQ = {dQ[1], . . . , dQ[N]}, dK = {dK[1], . . . , dK[N]}, dV = {dV [1], . . . , dV [N]} 42: Compute dA = dQ dK, dG = revcum(dA) 43: return dQ, dK, dV , dG 19 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 32: 33: 34: 35: 36: 37: 38:"
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "National University of Singapore",
        "The Hong Kong Polytechnic University",
        "TikTok",
        "UC Santa Cruz"
    ]
}