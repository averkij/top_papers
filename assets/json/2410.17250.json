{
    "paper_title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
    "authors": [
        "Shota Onohara",
        "Atsuyuki Miyai",
        "Yuki Imajuku",
        "Kazuki Egashira",
        "Jeonghun Baek",
        "Xiang Yue",
        "Graham Neubig",
        "Kiyoharu Aizawa"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accelerating research on Large Multimodal Models (LMMs) in non-English languages is crucial for enhancing user experiences across broader populations. In this paper, we introduce JMMMU (Japanese MMMU), the first large-scale Japanese benchmark designed to evaluate LMMs on expert-level tasks based on the Japanese cultural context. To facilitate comprehensive culture-aware evaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA) subset, where the culture-independent subjects (e.g., Math) are selected and translated into Japanese, enabling one-to-one comparison with its English counterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly crafted subjects that reflect Japanese cultural context. Using the CA subset, we observe performance drop in many LMMs when evaluated in Japanese, which is purely attributable to language variation. Using the CS subset, we reveal their inadequate Japanese cultural understanding. Further, by combining both subsets, we identify that some LMMs perform well on the CA subset but not on the CS subset, exposing a shallow understanding of the Japanese language that lacks depth in cultural understanding. We hope this work will not only help advance LMM performance in Japanese but also serve as a guideline to create high-standard, culturally diverse benchmarks for multilingual LMM development. The project page is https://mmmu-japanese-benchmark.github.io/JMMMU/."
        },
        {
            "title": "Start",
            "content": "JMMMU: Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation Shota Onohara* Atsuyuki Miyai* Yuki Imajuku* Kazuki Egashira* Jeonghun Baek* Xiang Yue Graham Neubig Kiyoharu Aizawa The University of Tokyo Carnegie Mellon University 4 2 0 2 2 2 ] . [ 1 0 5 2 7 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "culture-aware Accelerating research on Large Multimodal Models (LMMs) in non-English languages is crucial for enhancing user experiences across broader populations. In this paper, we introduce JMMMU (Japanese MMMU), the first large-scale Japanese benchmark designed to evaluate LMMs on expert-level tasks based on the Japanese cultural context. To facilitate comprehensive evaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA) subset, where the culture-independent subjects (e.g., Math) are selected and translated into Japanese, enabling one-to-one comparison with its English counterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly crafted subjects that reflect Japanese cultural context. Using the CA subset, we observe performance drop in many LMMs when evaluated in Japanese, which is purely attributable to language variation. Using the CS subset, we reveal their inadequate Japanese cultural understanding. Further, by combining both subsets, we identify that some LMMs perform well on the CA subset but not on the CS subset, exposing shallow understanding of the Japanese language that lacks depth in cultural understanding. We hope this work will not only help advance LMM performance in Japanese but also serve as guideline to create high-standard, culturally diverse benchmarks for multilingual LMM development. The project page https://mmmu-japanese-benchmark. is github.io/JMMMU/."
        },
        {
            "title": "Introduction",
            "content": "In recent years, large language models (LLMs) have revolutionized the field of language processing (Chen et al., 2023a; vic, 2023; Touvron et al., 2023; Wei et al., 2023). Building on the success of LLMs, large multimodal models (LMMs) have *Equal contribution. 1 demonstrated remarkable performance across tasks ranging from common sense reasoning to domainspecific, expert-level challenges (Antol et al., 2015; Liu et al., 2023a, 2024c; Yue et al., 2024). As their capabilities grow, the need for robust criteria to evaluate LMMs has become increasingly important, highlighting the role of comprehensive benchmarks in assessing the full scope of their abilities. However, current benchmarks focus primarily on performance in English (Liu et al., 2024c; Yue et al., 2024; Li et al., 2024b; Liu et al., 2023b; Yu et al., 2024; Fu et al., 2024), with less emphasis on evaluation in other languages. Given that LMMs are widely used across diverse languages, it is imperative to evaluate their performance beyond English. Additionally, such multilingual evaluations should actively involve contributions from diverse communities, ensuring that the associated cultural contexts are appropriately considered. In this paper, we introduce JMMMU (Japanese MMMU), the first benchmark designed to evaluate LMMs on extensive, multi-disciplinary tasks in Japanese that require college-level subject knowledge, deliberate reasoning, and cultural understanding. The overview of JMMMU is shown in Figure 1. JMMMU draws inspiration from the wellestablished MMMU (Yue et al., 2024) and expands existing culture-aware Japanese benchmarks (Inoue et al., 2024b; SakanaAI, 2024c) by over 10 times, with 1,320 questions using 1,118 images, covering diverse range of subjects. JMMMU offers two key subsets: (i) CultureAgnostic (CA) Subset: We extracted and translated the culture-agnostic components from MMMU. This subset allows for direct comparison of the performance gaps between English and Japanese that are purely attributable to language variations. (ii) Culture-Specific (CS) Subset: We carefully crafted brand-new questions that align with the Japanese cultural context. With CS subset, developers can assess capabilities specifically Figure 1: Overview of the JMMMU dataset. JMMMU includes 720 culture-agnostic (translation-based) questions and 600 culture-specific (newly created) questions, totaling 1,320 questions, thus expanding the existing cultureaware Japanese benchmark (Inoue et al., 2024b) by over 10 times. JMMMU serves as diagnostic tool for assessing both Japanese cultural understanding and culture-agnostic language understanding capability. tailored to Japanese culture. Together, JMMMU serves as diagnostic tool for model developers, providing valuable feedback for future improvements. Evaluating 15 open-source LMMs and three advanced proprietary LMMs on JMMMU, our key findings are summarized as follows: Overall performance is up to 58.6%, leaving great room for improvement in the utility of the Japanese context. The CA subset reveals that most models perform worse when asked in Japanese than in English (up to 8.6%), even when the question asks exactly the same content. This appleto-apple comparison clearly indicates that the utility in non-English languages is falling behind in current LMMs. The CS subset reveals that models trained on Japanese datasets perform the best among open-source models, suggesting that such finetuning effectively contributes to incorporating Japanese cultural knowledge into the models. Combining both subsets, we reveal significant discrepancy among the state-of-the-art proprietary models. While they perform similarly on English benchmarks and even on culture-agnostic questions in Japanese, their performances are significantly different on CS subset. This finding is particularly alarming, as it indicates that evaluation exclusively on translation-based benchmark could risk overestimation of an LMMs multilingual capability without truly understanding the context of the individual cultures. Our findings indicate that English-centered performance evaluation may lead to biased development, neglecting non-English languages. We hope our findings not only spark interest in Japanese performance but also motivate the community to craft variety of high-standard benchmarks that encompass diverse cultures and their associated languages, thereby promoting more inclusive LMM development."
        },
        {
            "title": "2 Related Work",
            "content": "Large Multimodal Models (LMMs) Following the success of large language models (LLMs), many LMMs have been developed with improved knowledge and instruction-following capabilities (Liu et al., 2023b, 2024a,b; Li et al., 2024a; Ye et al., 2024; Zhao et al., 2023; Li et al., 2023; Monajatipoor et al., 2023; Zhao et al., 2024). However, the progress of these models is typically evaluated on English benchmarks (Yue et al., 2024; Liu et al., 2024c). Therefore, significant challenge remains in accurately evaluating the capabilities of other languages, highlighting the need for nonEnglish benchmarks. LMM Benchmarks Among various recent benchmarks (Li et al., 2024b; Liu et al., 2023b, 2024c; Lu et al., 2024; Yue et al., 2024; Miyai et al., 2024), MMMU (Yue et al., 2024) is the most widely used to measure the advancements of 2 cutting-edge LMMs. MMMU requires advanced university-level knowledge and reasoning across broader range of subjects, enabling more comprehensive and expert-level evaluation. Subsequently, CMMMU (Zhang et al., 2024) has been proposed as its Chinese counterpart. While CMMMU comprises entirely new culture-specific questions, our JMMMU has not only culture-specific subjects but also translation-based culture-agnostic subjects, facilitating one-to-one comparisons between English and Japanese using the exact same questions. In line with multilingual ability evaluation, several VQA benchmarks have been proposed (Gao et al., 2015; Changpinyo et al., 2022; Gupta et al., 2020; Liu et al., 2021; Pfeiffer et al., 2021; Tang et al., 2024; Romero et al., 2024). However, unlike the MMMU series, their primary focus is on daily knowledge, (e.g., Pop Culture, Sports in CVQA (Romero et al., 2024)), still leaving the multilingual expert-level reasoning skills as an important direction for future work. Japanese LMM Benchmarks The development of Japanese LMM benchmarks remains behind that of English benchmarks. While efforts have been made to create Japanese benchmarks as shown in Table 1, they still exhibit the following critical limitations: (i) Existing benchmarks (Shimizu et al., 2018; Turing, 2024c,b; Inoue et al., 2024b; SakanaAI, 2024c,a) focus primarily on common sense knowledge but do not adequately address expert-level knowledge, despite the advancement in LMMs and the importance of evaluating such capabilities. (ii) Many do not account for cultural differences. They are often created by directly translating existing English benchmarks (Shimizu et al., 2018; Turing, 2024c,b), resulting in questions that may feel unfamiliar to Japanese people due to cultural context. (iii) Although recent benchmarks attempt to consider cultural differences (Inoue et al., 2024b; SakanaAI, 2024c,a), they are limited in size (up to 102 questions), raising concerns about the reliability of quantitative evaluation. Our proposed JMMMU addresses all three of the aforementioned challenges, significantly advancing the benchmark in the realm of Japanese evaluation."
        },
        {
            "title": "3.1 Overview of JMMMU",
            "content": "As illustrated in Figure 1, JMMMU contains total of 1,320 questions and 1,118 images, covering Table 1: Overview of Japanese LMM benchmarks. JMMMU is the first benchmark that evaluates expertlevel skills and is the largest among culture-aware benchmarks. Benchmark Culture Level Questions Images Common sense JA-VG-VQA-500 (SakanaAI, 2024b) Common sense LLaVA-Bench-in-the-wild (Turing, 2024b) Common sense JA-Multi-Image-VQA (SakanaAI, 2024a) JA-VLM-Bench-in-the-wild (SakanaAI, 2024c) Common sense Common sense Heron Bench (Inoue et al., 2024b) JMMMU (Ours) Expert 500 60 55 50 102 500 24 39 42 21 1, 1,118 28 different subjects. This benchmark is strategically divided into two distinct categories: cultureagnostic and culture-specific subjects. Culture-agnostic subset consists of 24 subjects with 720 questions across five disciplines: (1) Art & Psychology, (2) Business, (3) Health & Medicine, (4) Science, and (5) Tech & Engineering. Culturespecific subset consists of 600 questions across four subjects: (1) Japanese Art, (2) Japanese Heritage, (3) Japanese History, and (4) World History. We provide sample questions in Appendix E"
        },
        {
            "title": "3.2 Data Curation Process",
            "content": "JMMMU is derived from the widely-used validation set of MMMU, consisting of 900 questions across 30 subjects. To construct JMMMU, we first examined the cultural dependencies in the original MMMU subjects. For culture-agnostic subjects, we translated the questions into Japanese. We further replaced culture-dependent subjects with new subjects that are conceptually similar, but better aligned with the Japanese context. All the process has been conducted with the help of 19 university students, including the authors, who have expert knowledge in the respective fields and native fluency in Japanese. Here, we describe the dataset creation process in detail. Examining Cultural Dependencies in MMMU Among the 30 subjects in MMMU, we identified that questions in six subjects are particularly unfamiliar to Japanese people and thus we categorized them as culture-specific subjects; Art, Art Theory, Geography, History, Literature, and Sociology. The remaining subjects (e.g., Biology, Chemistry, Computer Science, Electronics) exist in Japan with similar contents, and thus we categorized them as culture-agnostic subjects. As result, we excluded the six culture-specific subjects while keeping the remaining 24 culture-agnostic subjects in JMMMU. 3 cally covered in Japanese textbooks to better reflect the Japanese educational context than History in MMMU. The images are primarily sourced from Wikimedia Commons1, ensuring that all selected images are available under licenses suitable for public release. In crafting questions, we aimed to keep the text as simple as possible and ensure that no options stand out, making it hard to guess the correct choice without referring to the image. Figure 2: Example of the image translation process. English words in the image are manually overwritten with Japanese."
        },
        {
            "title": "Multimodal Benchmarks",
            "content": "Translating Culture-Agnostic Subjects The experts were provided with the original English texts, GPT-4o-translated question texts in Japanese, and corresponding images. For texts, their task involved: (i) refining the auto-translated Japanese text to ensure naturalness and fluency; (ii) confirming that technical terms and academic expressions adhere to conventional Japanese usage; and (iii) adjusting the currency to reflect typical digit lengths in Japanese yen (). For currency conversion, simplified conversion ($1 100) was employed to avoid making the calculation unnecessarily complicated. For images, we asked the experts to overwrite the English text with Japanese text by using an image editing tool. An example of the image translation process is presented in Figure 2. Consequently, we obtained 720 questions covering 24 culture-agnostic subjects fully translated and adapted for Japanese usage. Creating Culture-Specific Subjects Recognizing that most of the removed subjects are related to art or social studies, we created the following new subjects to test similar knowledge in the Japanese context: Japanese Art: Questions about traditional Japanese art, such as Ukiyo-e and Noh. Japanese Heritage: Questions about traditional, culturally significant locations and buildings in Japan such as temples and shrines. Japanese History: Questions about historical incidents in Japan. World History: Questions about global historical incidents, but based on the content typi4 Here, we compare JMMMU with other Japanese multimodal benchmarks, provided in Table 1, to demonstrate its uniqueness. First and foremost, JMMMU is the only benchmark that includes expert-level questions, while the rest of the benchmarks (Shimizu et al., 2018; Turing, 2024a; SakanaAI, 2024a,c; Inoue et al., 2024b) are focused on common knowledge. Further, JMMMU is carefully designed to take the Japanese cultural context into account. While some existing benchmarks consider Japanese culture, they are all limited in size (only up to 102 questions in Inoue et al. (2024b)), raising concerns about whether reliable quantitative evaluations can be conducted. In contrast, JMMMU contains more than 10 times larger than any of the existing culture-aware benchmarks."
        },
        {
            "title": "4.1 Setup",
            "content": "LMMs We evaluate diverse set of LMMs. Proprietary LMMs: GPT-4o (OpenAI, 2024) Gemini 1.5 Pro (DeepMind, 2024; Reid et al., 2024) and Claude 3.5 Sonnet (Anthropic, 2024). Japanese LMMs: LLaVA CALM2 (Inagaki, 2024) and EvoVLM JP v2 (Inoue et al., 2024a), which are trained on both English and Japanese datasets. Open-source LMMs: LLaVA-OneVision 0.5B & 7B (Li et al., 2024a), LLaVA1.6-13B & 34B (Liu et al., 2024b), Phi-3 & 3.5 Vision (Abdin et al., 2024), InternVL2-2B & 8B (Chen et al., 2023b), xGen-MM (Xue et al., 2024), Idefics2-8B (Laurençon et al., 2024b), Idefics3-8B (Laurençon et al., 2024a), 1https://commons.wikimedia.org/ Table 2: Overall results. CA (EN) shows the result on culture agnostic subset in English. The rest of the results are average and individual subjects scores on JMMMU. denotes Japanese LMMs. The best-performing model among open source and proprietary models are in bold. Overall, the performance is up to 40.5% for open-source, and 58.6% for proprietary models, leaving great room for improvement. Models Overall CS CA Jpn. World Art & CA Jpn. (EN) Art Heritage History History Psych. Jpn. Business Science (1,320) (600) (720) (720) (150) (150) (150) (150) (90) (150) (120) Health & Tech & Medicine Eng. (210) (150) Random 24.8 25.0 24.6 24.6 25.0 25. 25.0 25.0 25.4 25.0 22.8 25. 24.3 Open Source LLaVA-OV-0.5B InternVL2-2B xGen-MM Phi-3v LLaVA-1.6-13B Idefics2-8B Phi-3.5v LLaVA CALM2 Mantis 8B CogVLM2-19B Idefics3-8B EvoVLM JP v2 InternVL2-8B LLaVA-1.6-34B LLaVA-OV-7B Proprietary Claude 3.5 Sonnet Gemini 1.5 Pro GPT-4o Text Only 26.0 28.3 28.6 29.5 31.1 31.9 32.4 34.9 35.5 36.1 37.3 38.1 38.3 39.8 40.5 50.8 51.5 58.6 23.3 28.2 29.4 22.7 29.2 27.6 31.9 31.3 28.2 28.9 35.7 30.0 26.5 31.9 37.6 31.3 33.7 29.0 29.9 32.0 37.0 27.6 35.1 40.7 34.3 30.8 39.2 37.3 41.5 29.4 29.9 42.7 39.5 32.2 36.0 42.0 39.7 33.1 36.8 39.3 42.8 32.8 36.9 43.3 45.2 32.2 33.9 44.0 42.5 34.7 43.3 41.3 43.2 37.1 45.7 42.0 43.0 38.5 45.1 36.0 51.0 50.6 52.1 39.3 60.3 44.2 51.1 54.7 66.7 51.8 52.1 60. 22.7 22.7 20.7 18.7 24.0 24.0 27.3 36.7 30.0 24.0 24.7 40.0 38.0 36.0 30.7 46.7 55.3 70.7 24.0 30.7 22.7 29.3 32.0 30.0 35.3 40.0 35.3 36.0 42.0 42.0 35.3 40.7 37.3 54.7 55.3 58.7 24.0 32.0 39.3 26.7 46.7 53.3 37.3 46.7 50.7 59.3 61.3 54.7 55.3 54.0 68.0 63.3 76.0 76. 26.7 30.0 32.2 26.7 25.6 32.2 27.8 27.8 37.8 28.9 34.4 32.2 40.0 42.2 41.1 53.3 51.1 53.3 27.3 30.0 21.3 28.7 28.7 22.7 31.3 26.0 28.0 32.7 28.0 28.7 36.0 41.3 36.7 56.7 44.0 55.3 24.2 30.8 22.5 25.8 30.0 22.5 30.0 26.7 31.7 30.8 26.7 28.3 34.2 25.0 31.7 51.7 44.2 45. 30.7 25.3 36.7 37.3 34.0 32.0 36.7 34.0 37.3 30.0 38.0 38.7 34.0 36.7 38.7 55.3 48.0 61.3 30.0 24.8 31.0 36.2 26.7 29.0 28.1 31.0 29.5 38.6 35.2 32.4 32.4 39.0 42.4 41.0 38.6 45.2 GPT-4o text 38. 35.5 40.3 44.9 32.7 32.0 35.3 42.0 38.9 36. 41.7 45.3 39.5 CogVLM2-19B (Hong et al., 2024), and Mantis-8B (Jiang et al., 2024). not respond in parsable format, random choice is assigned as its answer. In Appendix A, we provide further details of these models, with particular focus on Japanese language support. Text-only LLM As reference, we present the accuracy of GPT-4o when provided only with the question text and choices, without images. Evaluation The evaluation method is based on the setup in MMMU (Yue et al., 2024). Prompts are translated as follows: for multiple-choice questions, 与えられた選択肢の中から最も適切な 回答のアルファベットを直接記入してくだ さい (Answer with the options letter from the given choices directly.) ; and for open-ended questions, 質問に対する回答を単語や短いフレー ズで記入してください (Answer the question using single word or phrase.). Following MMMU, (i) we prepare rule-based parser to extract the models choice from typical generation styles such as 答えはA (The answer is A), making the evaluation robust to some varieties of answer styles, and (ii) when model does"
        },
        {
            "title": "4.2 Main Result",
            "content": "Table 2 demonstrates the evaluation results on our JMMMU benchmark. We provide the average scores across all subjects, culture-agnostic (CA) subjects, and culture-specific (CS) subjects, as well as scores on individual subjects. For comparison, we also provide the performance on CA suset in English CA (EN) . Note that CA (EN) is often smaller than the overall average of MMMU given by Yue et al. (2024) because subjects selected as CA are relatively difficult among all subjects in MMMU as it often requires stronger reasoning capabilities (e.g., Math). Here, we summarize our key observations. Challenging Nature In our experiment, the performance is up to 40.5% for open-source, and 58.6% for proprietary models, leaving great room for improvement. This also highlights significant gap between open-source and proprietary models, presenting more difficult challenge for opensource models. 5 Table 3: The effect of translation. Each column shows the model performance when image (I) and text (T ) are in Japanese (jp) or in English (en). shows the difference from IenTen."
        },
        {
            "title": "IenTen",
            "content": "IenTjp(1) IjpTjp(2) LLaVA-1.6-13B Phi-3.5v LLaVA-CALM2 CogVLM2-19B EvoVLM JP v2 InternVL2-8B LLaVA-1.6-34B LLaVA-OV-7B 26.4 39.2 29.4 32.8 30.0 43.9 43.6 45.0 31.9 (+5.5) 33.6 (-5.6) 28.3 (-1.1) 31.9 (-0.9) 30.8 (+0.8) 38.3 (-5.6) 40.8 (-2.8) 38.3 (-6.7) 29.2 (+2.8) 31.1 (-8.1) 31.4 (+2.0) 34.4 (+1.6) 28.6 (-1.4) 37.2 (-6.7) 38.9 (-4.7) 35.6 (-9.4) whose gap from GPT-4o is 16.7%, indicating the particular inadequacy of the open-source model in Heritage domain. GPT-4o vs. Claude 3.5 Sonnet We reveal significant performance gap between the two leading models; GPT-4o and Claude 3.5. They are stateof-the-art models and their performance is known to be similar with only 0.8% difference on the MMMU benchmark in English (Anthropic, 2024). Further, their performance is similar even on CA split in Japanese (1.2% difference in Table 2). However, on the CS split, GPT-4o outperforms Claude 3.5 Sonnet by substantial 15.7%. This strongly indicates that models Japanese language skill and its understanding of Japanese culture should be separately discussed. Our research is pioneering in revealing this, discrepancy that would have remained obscured without combining translation-based CA subjects and brandnew CS subjects. Our finding underscores the limitations of relying exclusively on auto-translated benchmarks for thorough evaluation of model capabilities in non-English languages, highlighting the importance of evaluating models on culturespecific questions."
        },
        {
            "title": "5.1 Ablation on Image Translation",
            "content": "Here, we investigate how translating text and images affects the model performances. Using 360 questions from the culture-agnostic subset which involved translation of both texts and images, we compare the scores in English (IenTen), when only text is translated (IenTjp), and when both text and images are translated (IjpTjp). We provide scores for selected models in Table 3 and the full set in ApFigure 3: Score correlation between subsets. While proprietary models () perform the best on both subsets, Japanese LMMs () perform remarkably high on CS subset compared to models that perform similarly on CA subset. The Effect of Translation in CA Subset First, as general trend, the score on the CA subset is significantly lower than its English counterpart (CA (EN) in Table 2) with an average drop of 4.5%. This indicates that, even for the same questions, many models perform worse when asked in Japanese. Second, despite such general trend, Japanesemade LMMs (i.e., LLaVA CALM2 and EvoVLM JP v2) face minimal drop (up to 1.7 %), which implies that incorporating the Japanese dataset successfully mitigates the performance gap between English and Japanese. The Performance of Japanese LMMs Figure 3 demonstrates the correlation between the scores on the CA and CS subjects. The Japanese LMMs, LLaVA CALM2 and EvoVLM JP v2, show higher scores on CS subjects compared to other models that perform similarly on CA subjects. This strongly indicates their proficiency in CS subjects. On the other hand, however, compared to stronger models such as InternVL2-8b, LLaVA1.6-34b, and LLaVA-OV-7b, the Japanese LMMs show lower scores on CA subjects, suggesting room for improvement in their general reasoning and problemsolving capabilities in culture-agnostic context. Scores on Japanese Heritage Among CS subjects, the performance of open-source models is particularly low in Japanese Heritage  (Table 2)  . Even the best-performing open-source model (EvoVLM JP v2) scores 30.7% lower than GPT-4o in Japanese Heritage, while in other CS subjects, there is at least one open-source model (a) GPT-4os Error distribution in cultureagnostic subjects. Figure 5: Error distribution over culture-specific subjects. Lack of Knowledge is the majority error type at over 50%. guages on culture-agnostic split (only 0.3% difference in Table 2), we have found that there are significant amount (28.6 %) of questions to which it answered correctly only in either one of the languages. We now investigate this phenomenon. For questions answered correctly only in English (orange in Figure 4(a)), we observe simple performance degradation after translation. In contrast, we have found some distinctive examples in the opposite case (yellow). In an example of Figure 4(b), GPT-4o outputs only the direct answer in English, whereas in Japanese, the model includes the reasoning process in its response although the model is instructed to generate the choice directly by using the prompt in Section 4.1. For fair comparison with MMMU (Yue et al., 2024), we count response to be correct as far as the models response is accurate and can be parsed by rule-based algorithm, regardless of its instruction-following ability. As result, the scores can sometimes be counterintuitively overestimated due to the lack of instruction following skills in Japanese. While the primary focus of JMMMU is on evaluating expert knowledge and supporting the improvement of such capabilities, our findings highlight crucial direction for future work: measuring and enhancing instructionfollowing ability in non-English languages."
        },
        {
            "title": "5.3 Errors in Culture-specific Subjects",
            "content": "This section presents an analysis of the tendency of GPT-4os errors in the culture-specific subjects. To investigate the causes of these errors, we manually review GPT-4os responses and classify the errors into four categories: (i) Lack of Knowledge, where the model successfully extracts the necessary (b) An Example question where GPT-4o answers correctly only in Japanese. Figure 4: (a) There are considerable amount of questions to which GPT-4o answers correctly only in either one of the languages (yellow + orange). (b) In Japanese, the model relatively more often goes against the instruction that asks to answer directly and generates its reasoning process, leading to correct answer. pendix B.2. Many models experience drop in scores by text translation, with further degradation observed when images are also translated (i.e., 0 > 1 > 2). However, some models exhibit different performance trends, showing drop by text translation but an improvement by translating both (i.e., 1 < 0 < 2), or vice versa. Overall, while the trends are complex, our result indicates that textonly translation, as is done in many non-English benchmarks, could result in biased performance evaluation. Rigorous investigation on this point is left for future work."
        },
        {
            "title": "5.2 Errors in Culture-agnostic Subjects",
            "content": "JMMMU shares 720 culture-agnostic questions with MMMU, which allows us to compare the output one by one. Using these questions, we evaluate how translation affects model performance. Taking GPT-4o as an example, we classify the responses into four categories based on whether they are correct or incorrect in each language. Figure 4 presents the results before and after translation. The results on the other models are provided in Appendix B.1 While GPT-4o performs similarly in both lan7 (a) Lack of Knowledge (b) Image Recognition Errors (c) Answer Rejection (d) Textual Misunderstanding Figure 6: Examples from each error type: (a) Lack of Knowledge, where the model does not know the necessary information; (b) Image Recognition Errors, where the model fails to correctly interpret the image; (c) Answer Rejection, where the model rejects to answer; and (d) Textual Misunderstanding, where the response is not aligned with the question. information from the image but lacks the culturespecific knowledge required to produce correct answer, (ii) Image Recognition Errors, where it fails to correctly interpret the image during the visual understanding stage, (iii) Answer Rejection, where it declines to provide an answer, and (iv) Textual Misunderstanding, where the response is not aligned with the question. The overall distribution of these error types is shown in Figure 5. Lack of Knowledge is the overwhelming majority at over 50%, indicating that culture-specific knowledge is the most critical requirement to achieve high performance in JMMMU. In this section, we discuss notable examples for each error category. Lack of Knowledge (53.8%) Figure 6(a) shows an example of an error in Japanese Heritage. Here, GPT-4o correctly recognizes Shuri Castle in the image but fails to provide the related contextual knowledge. Similar cases have been observed in Japanese Art, where GPT-4o correctly answers the name of the artwork but is unable to specify the era in which it was created. Image Recognition Errors (30.8%) Figure 6(b) shows an example of an image recognition error of question. Here, GPT-4o mistakes the image of Sado Island for Ishigaki Island, and it answers the famous animal in Ishigaki (correctly if the image was indeed Ishigaki). Answer Rejection (10.6%) This type of error is particularly evident in Japanese History and World History, where GPT-4o declines to answer questions requiring the identification of historical figures from images. In Figure 6(c), GPT-4o responds that it is unable to identify the person in the image (Hideyo Noguchi), resulting in failure to select the option associated with him. We hypothesize this is due to their strong privacy awareness to avoid giving private information (Wang et al., 2024), even when the question asks for information that is widely known about historical figure. Textual Misunderstanding (4.8%) There are rare instances where GPT-4o provides an incorrect response despite correctly identifying the content of the image. For example, in Figure 6(d), GPT-4o accurately names the title of the artwork, but its answer does not correspond to the question."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose JMMMU, benchmark designed to comprehensively evaluate the expert-level knowledge, reasoning abilities, and understanding of Japanese culture. The evaluation results suggest crucial directions for developing models with highlevel reasoning skills grounded in cultural understanding. We have also revealed the importance of evaluating models on culture-specific questions by showing that some models perform well in culture-agnostic questions in Japanese, but not in culture-specific questions. We hope this work will serve as an important step towards comprehensive multilingual evaluation, motivate communities in other cultures and languages to craft their own high-standard benchmarks, and lead to LMM developments that are more inclusive and truly useful in diverse population."
        },
        {
            "title": "Limitations",
            "content": "Throughout our experiment and extensive analysis, we have shown number of critical directions of improvement in multilingual benchmarks and model developments. While they are outside of the scope of this paper, they are left as important directions for future work, and thus we summarize them here: Subject Set Expansion While JMMMU can assess the latest LMMs expert-level skills, it cannot evaluate model performance on subjects outside of those currently covered. As models gain more knowledge and improve their reasoning abilities, it will be necessary to expand the range of subjects and include more challenging questions. Benchmarks in Other Cultures Since JMMMU only covers the Japanese, evaluating model performance in other languages and cultural contexts remains an important area for future work. We hope these efforts will help mitigate the underrepresentation of diverse cultures and languages. Instruction Following Ability in Japanese In Section 5.2, we have shown gap in instructionfollowing ability between languages and that models go against the instruction and generate their reasoning more often in Japanese. While the primary focus of our benchmark is on evaluating expert knowledge and thereby helping improve such skills, it is left as an important future work to improve the instruction-following ability in Japanese. Further, it is also important to design an evaluation protocol to measure instruction-following ability to enhance the development of such skills. While there are some methods to evaluate the models instructionfollowing ability (Zhou et al., 2023; Qian et al., 2024), these should be appropriately incorporated in the context of multilingual performance evaluation."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was partially supported by JST BOOST, Japan Grant Number JPMJBS2418, JSPS KAKENHI Grant Number 24K23882, and JST JPMJCR22U4."
        },
        {
            "title": "References",
            "content": "2023. Vicuna: An open-source chatbot impressing gpt4 with 90%* chatgpt quality. Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. 2024. Evolutionary optimization of model merging recipes. arXiv preprint arXiv:2403.13187. Anthropic. 2024. Claude 3.5 sonnet. https://www. anthropic.com/news/claude-3-5-sonnet. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In ICCV. Soravit Changpinyo, Linting Xue, Michal Yarom, Ashish Thapliyal, Idan Szpektor, Julien Amelot, Xi Chen, and Radu Soricut. 2022. Maxm: Towards multilingual visual question answering. arXiv preprint arXiv:2209.05401. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. 2023a. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Internvl: Lu, Yu Qiao, and Jifeng Dai. 2023b. Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238. Google DeepMind. 2024. Gemini 1.5 pro. https:// deepmind.google/technologies/gemini/pro/. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, WeiChiu Ma, and Ranjay Krishna. 2024. Blink: Multimodal large language models can see but not perceive. In ECCV. Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015. Are you talking to machine? dataset and methods for multilingual image question. Advances in neural information processing systems, 28. Deepak Gupta, Pabitra Lenka, Asif Ekbal, and Pushpak Bhattacharyya. 2020. unified framework for multilingual and code-mixed visual question answering. In Proceedings of the 1st conference of the AsiaPacific chapter of the association for computational linguistics and the 10th international joint conference on natural language processing, pages 900913. 9 Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. 2024. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500. Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Visually grounded reasoning Elliott. 2021. arXiv preprint across languages and cultures. arXiv:2109.13238. Jinyi Hu, Yuan Yao, Chongyi Wang, SHAN WANG, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, dahai li, Zhiyuan Liu, and Maosong Sun. 2024. Large multilingual models pivot zero-shot multimodal learning across languages. In ICLR."
        },
        {
            "title": "Aozora",
            "content": "Inagaki. 2024. https://huggingface.co/cyberagent/ llava-calm2-siglip. llava-calm2-siglip. Yuichi Inoue, Takuya Akiba, and Shing Makoto. 2024a. Llama-3-evovlm-jp-v2. Yuichi Inoue, Kento Sasaki, Yuma Ochi, Kazuki Fujii, Kotaro Tanahashi, and Yu Yamaguchi. 2024b. Heronbench: benchmark for evaluating vision language models in japanese. In CVPR workshop. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. 2024. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483. Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. 2024a. Building and better understanding vision-language models: insights and future directions. arXiv preprint arXiv:2408.12637. Hugo Laurençon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. 2023. OBELICS: An open web-scale filtered dataset of interleaved image-text documents. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. 2024b. What matters when buildarXiv preprint ing vision-language models? arXiv:2405.02246. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023. Otter: multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2024b. Seed-bench: Benchmarking multimodal llms with generative comprehension. In CVPR. 10 Fuxiao Liu, Hao Tan, and Chris Tensmeyer. 2023a. Documentclip: Linking figures and main body arXiv preprint text arXiv:2306.06306. in reflowed documents. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. In CVPR. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024b. Llavanext: Improved reasoning, ocr, and world knowledge. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. In NeurIPS. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2024c. Mmbench: Is your multi-modal model an all-around player? In ECCV. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2024. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR. Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei Liu, and Kiyoharu Aizawa. 2024. Unsolvable problem detection: Evaluating trustworthiness of vision language models. arXiv preprint arXiv:2403.20331. Masoud Monajatipoor, Liunian Harold Li, Mozhdeh Rouhsedaghat, Lin Yang, and Kai-Wei Chang. 2023. Metavl: Transferring in-context learning ability from language models to vision-language models. arXiv preprint arXiv:2306.01311. OpenAI. 2024. Gpt-4o. Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, JanMartin Steitz, Stefan Roth, Ivan Vulic, and Iryna Gurevych. 2021. xgqa: Cross-lingual visual question answering. arXiv preprint arXiv:2109.06082. Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. 2024. Mia-bench: Towards better instruction following arXiv preprint evaluation of multimodal llms. arXiv:2407.01509. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, xgen-mm (blip-3): family of et al. 2024. arXiv preprint open large multimodal models. arXiv:2408.08872. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. 2024. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In CVPR. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2024. Mm-vet: Evaluating large multimodal models for integrated capabilities. In ICML. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. 2024. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR. Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, et al. 2024. Cmmmu: chinese massive multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2401.11944. Bo Zhao, Boya Wu, and Tiejun Huang. 2023. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087. Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. 2024. Mmicl: Empowering vision-language model with multi-modal in-context learning. In ICLR. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, et al. 2024. Cvqa: Culturally-diverse multilingual visual question answering benchmark. In NeurIPS Datasets and Benchmarks Track. SakanaAI. 2024a. Ja-multi-image-vqa. https://huggingface.co/datasets/SakanaAI/ JA-Multi-Image-VQA. SakanaAI. 2024b. Ja-vg-vqa-500. https: //huggingface.co/datasets/SakanaAI/ JA-VG-VQA-500. SakanaAI. 2024c. Ja-vlm-bench-in-the-wild. https://huggingface.co/datasets/SakanaAI/ JA-VLM-Bench-In-the-Wild. Nobuyuki Shimizu, Na Rong, and Takashi Miyazaki. 2018. Visual question answering dataset for bilingual image understanding: study of cross-lingual transfer using attention maps. In COLING. Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, et al. 2024. Mtvqa: Benchmarking multilingual text-centric visual question answering. arXiv preprint arXiv:2405.11985. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Turing. 2024a. Llava-bench-in-the-wild. https: //huggingface.co/datasets/liuhaotian/ llava-bench-in-the-wild/tree/main. Turing. 2024b. Llava-bench-in-the-wild (japanese). https://github.com/turingmotors/ heron/tree/main/playground/data/ llava-bench-in-the-wild. Turing. 2024c. Llava-bench-ja. https: //github.com/turingmotors/heron/tree/ main/playground/data/llava-bench-ja. Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. 2024. Do-not-answer: Evaluating safeguards in LLMs. In Findings of the Association for Computational Linguistics: EACL 2024, pages 896911, St. Julians, Malta. Association for Computational Linguistics. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. 2023. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846."
        },
        {
            "title": "Appendix",
            "content": "A LMMs Japanese Support Table A: LMMs Japanese support. JMMMU Japanese support Model Overall Base LLM LLM LMM To discuss the multilingual capabilities of LMMs, we summarize whether each model officially supports Japanese. Table presents the Japanese language support status for each model. indicates official support for Japanese, while indicates the absence of such support. Also, we denote ? for models of which we could not find the information. Even if model is marked as , it may still demonstrate some Japanese language capability due to the presence of Japanese data in publicly available datasets like ShareGPT-4V (Chen et al., 2024) and ShareGPT-4o2, or data crawled from the web. Proprietary commercial models, such as GPT4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet, do not publicly disclose detailed information about their training data. However, based on their release blog posts, it can be inferred that these models support many languages, including Japanese. LLaVA CALM2 is based on the Japanese LLM CALM23, and it has been trained using Japanese multimodal datasets, officially supporting Japanese. EvoVLM JP v2, merged model (Akiba et al., 2024), also incorporates Japanese data for optimization and is officially released as Japanese LMM. Phi-3.5 Vision does not officially support Japanese, despite its base model, Phi-3.5, having official support for multiple languages, including Japanese. Phi-3 Vision, likewise, does not support non-English languages. In the LLaVA series, LLaVA-OneVision explicitly mentions support for Chinese in its training but does not extend this to other non-English languages. However, Qwen2, the base LLM for the LLaVAOneVision models, officially supports Japanese. LLaVA-1.6 models are trained from different base LLMs, such as Vicuna v1.5 and Nous Hermes 2 Yi, neither of which officially support Japanese. Thus, Japanese language capabilities are not guaranteed in their visual instruction training. InternVL and its base model, InternLM2, officially support only English and Chinese. Similarly, CogVLM2 claims proficiency in both English and Open Source xGen-MM Mantis 8B Idefics2-8B Idefics3-8B CogVLM2-19B InternVL2-2B InternVL2-8B LLaVA-1.6 13B LLaVA-1.6 34B LLaVA-OneVision 0.5B 26.0 40.5 LLaVA-OneVision 7B 29.5 Phi-3 Vision 32.4 Phi-3.5 Vision 34.9 LLaVA CALM2 EvoVLM JP v2 38.1 Closed Source 28.6 Phi-3 35.5 Llama 3 ? 31.9 Mistral v0.1 37.3 Llama 3 36.1 Llama 3 28.3 InternLM2 38.3 InternLM2 31.1 Vicuna v1.5 39.8 Nous Hermes 2 Yi Qwen2 Qwen2 Phi-3 Phi-3.5 CALM2 (merged model) Claude 3.5 Sonnet Gemini 1.5 Pro GPT-4o 50.8 51.5 58.6 ? ? ? ? ? ? Chinese, with no explicit mention of Japanese support. Idefics2, Idefics3, xGen-MM, and Mantis use large-scale datasets for multimodal training. However, there is no clear evidence of Japanese data inclusion, and in some datasets, such as OBELICS (Laurençon et al., 2023), non-English data is explicitly filtered out. While Llama 3, the base model for some of these LMMs, mentions multilingual training, it does not explicitly confirm support for Japanese. Mistral v0.1 also does not disclose its training data. The performance of these models depends on complex interplay of factors, including the quantity and quality of the training data and the size and capabilities of the base language model. Official support for Japanese is not the only consideration; there are reports of models trained on English-only multimodal data generalizing to other languages (Hu et al., 2024), including Japanese. Moreover, since many models are designed with Chinese support, the cultural and linguistic proximity between Japanese and Chinese-speaking regions may result in high performance in Japanese."
        },
        {
            "title": "B More Result",
            "content": "B.1 Error Analysis in Culture-Agnostic subjects 2https://huggingface.co/datasets/OpenGVLab/ ShareGPT-4o 3https://huggingface.co/cyberagent/ calm2-7b-chat In Section 5.2, we present the error analysis for GPT-4o on the CA subjects. In this section, we provide the error analysis for all models. While 12 Figure A: Error in culture-agnostic subjects. This figure categorizes the correctness of answers in culture-agnostic subjects based on the original MMMU English responses (correct or incorrect) and the corresponding JMMMU translated responses (correct or incorrect). we have shown in Table 2 that most models perform worse in Japanese, there are some amount of questions where the model answers correctly only in Japanese for every model. The number of such questions is particularly high for LLaVA-OV 0.5B and InternVL2 2B. This occurrence, however, appears to be random phenomenon, likely attributable to the overall weaker performance of these models. B.2 Ablation on Image Translation The full set of Table 3 is presented in Table B. As discussed in Section 5.1, each model reacts differently as the translation proceeds, and the tendency is difficult to summarize. Notably, here, GPT-4o shows 7.2% improvement in score after text translation. This partly stems from its weak instructionfollowing skills in Japanese, as discussed in Section 5.2, which allows it to infer answers more easily. Note that our experiment here has been conducted by using questions that involved translation of both texts and images. Many of them consist of table data, which requires stronger reasoning based on data processing, so the result may vary when investigating different data types that do not exist in the CA subset of JMMMU. B.3 Score Correlation between languages Using the culture-agnostic subset, we have demonstrated in Section 4.2 that (i) models perform worse in Japanese and (ii) Japanese LMMs show robustness to translation. To illustrate these points, we provide Figure B. 13 Table B: The full set of the translation effect. Each column shows the model performance when image (I) and text (T ) are in Japanese (jp) or in English (en). shows the difference from IenTen. represents Japanese LMMs. IenTen IenTjp(1) IjpTjp(2) Open source LLaVA-OV-0.5B InternVL2-2B xGen-MM Phi-3v LLaVA-1.6-13B Idefics2-8b Phi-3.5v LLaVA-CALM2 Mantis 8B CogVLM2-19B Idefics3-8b EvoVLM JP v2 InternVL2-8B LLaVA-1.6-34B LLaVA-OV-7B Proprietary Claude 3.5 Sonnet Gemini1.5Pro GPT-4o 28.9 32.5 36.7 35.0 26.4 28.9 39.2 29.4 32.5 32.8 33.1 30.0 43.9 43.6 45.0 53.6 50.6 48.1 28.9 (0.0) 29.7 (-2.8) 28.3 (-8.4) 31.7 (-3.3) 31.9 (+5.5) 28.1 (-0.8) 33.6 (-5.6) 28.3 (-1.1) 31.1 (-1.4) 31.9 (-0.9) 31.7 (-1.4) 30.8 (+0.8) 38.3 (-5.6) 40.8 (-2.8) 38.3 (-6.7) 29.7 (+0.8) 28.6 (-3.9) 28.3 (-8.4) 29.7 (-5.3) 29.2 (+2.8) 28.1 (-0.8) 31.1 (-8.1) 31.4 (+2.0) 31.4 (-1.1) 34.4 (+1.6) 29.7 (-3.4) 28.6 (-1.4) 37.2 (-6.7) 38.9 (-4.7) 35.6 (-9.4) 56.4 (+2.8) 42.2 (-8.4) 55.3 (+7.2) 54.2 (+0.6) 42.2 (-8.4) 53.1 (+5.0)"
        },
        {
            "title": "C Further Experimental Details",
            "content": "C.1 Experimental Setup Computing Infrastructures We conduct all our evaluations of open-source models on single NVIDIA A100 (80GB) GPU. Parameters for LMM Inference maximum output length is set to 1,024 and temperature is set to 0 for all models during inference. C.2 Evaluation Protocol Answer Extraction in Multiple Choice Question While the models are instructed to answer their choice directly, they often generate some contextual information or unnecessary symbols. To tackle this point, following MMMU (Yue et al., 2024), we extract an answer from the model response with rule-based method. For multiple-choice questions, this parser can extract the models choice even when the choice is surrounded by some symbol (e.g., (A), A., ) or by text. For example, these answers, which are all some variants of The answer is A. in Japanese, can be parsed as A: 回答はA 答えはAであると考えられる 画像は首里城のため答えは(A) 答え: A. 15.3 14 Figure B: Score correlation between languages. represents proprietary models and represents Japanese LMMs. While all models perform worse in Japanese, Japanese LMMs perform similarly in both languages (i.e., close to the gray dashed line) While this allows an evaluation robust against some variety of answer generation styles, we have shown in Section 5.2 that this can sometimes overestimate the performance in Japanese because models instruction-following abilities are relatively low in Japanese."
        },
        {
            "title": "D Annotation Instruction",
            "content": "Recruitment and Payment Annotators were paid at least the minimum wage set in Japan, according to the time spent on the task. Data Consent They were informed that translated data would be used for evaluation purposes. Instructions Given to Participants The document containing the instructions presented to the annotators is shown in Figure C."
        },
        {
            "title": "E Examples",
            "content": "We provide sample questions from culture-agnostic subset in Figure D, and questions from culturespecific subset in Figure Figure C: Annotation Instruction. Annotators were provided with the Japanese version of this instruction. 15 Figure D: Examples in culture-agnostic subjects. Some images that contain English are translated. 16 Figure E: Examples in culture-specific subjects. The questions are created by Japanese native speakers and requires knowledge of Japanese culture."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "The University of Tokyo"
    ]
}