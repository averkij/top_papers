{
    "paper_title": "LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models",
    "authors": [
        "Fengqi Zhu",
        "Rongzhen Wang",
        "Shen Nie",
        "Xiaolu Zhang",
        "Chunwei Wu",
        "Jun Hu",
        "Jun Zhou",
        "Jianfei Chen",
        "Yankai Lin",
        "Ji-Rong Wen",
        "Chongxuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Masked Diffusion Models (MDMs), such as LLaDA, present a promising paradigm for language modeling, there has been relatively little effort in aligning these models with human preferences via reinforcement learning. The challenge primarily arises from the high variance in Evidence Lower Bound (ELBO)-based likelihood estimates required for preference optimization. To address this issue, we propose Variance-Reduced Preference Optimization (VRPO), a framework that formally analyzes the variance of ELBO estimators and derives bounds on both the bias and variance of preference optimization gradients. Building on this theoretical foundation, we introduce unbiased variance reduction strategies, including optimal Monte Carlo budget allocation and antithetic sampling, that significantly improve the performance of MDM alignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA, and the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor consistently and significantly across mathematical (GSM8K +4.7), code (HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard +4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical performance compared to strong language MDMs and ARMs. Project page: https://ml-gsai.github.io/LLaDA-1.5-Demo/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 3 2 2 9 1 . 5 0 5 2 : r LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models Fengqi Zhu1,2,3,,, Rongzhen Wang1,2,3,, Shen Nie1,2,3, Xiaolu Zhang5, Chunwei Wu5, Jun Hu5, Jun Zhou5, Jianfei Chen4, Yankai Lin1,2,3,, Ji-Rong Wen1,2,3, Chongxuan Li1,2,3,, 1 Gaoling School of AI, Renmin University of China 2 Beijing Key Laboratory of Research on Large Models and Intelligent Governance 3 Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE 4 Tsinghua University 5 Ant Group"
        },
        {
            "title": "Abstract",
            "content": "While Masked Diffusion Models (MDMs), such as LLaDA, present promising paradigm for language modeling, there has been relatively little effort in aligning these models with human preferences via reinforcement learning. The challenge primarily arises from the high variance in Evidence Lower Bound (ELBO)-based likelihood estimates required for preference optimization. To address this issue, we propose Variance-Reduced Preference Optimization (VRPO), framework that formally analyzes the variance of ELBO estimators and derives bounds on both the bias and variance of preference optimization gradients. Building on this theoretical foundation, we introduce unbiased variance reduction strategies, including optimal Monte Carlo budget allocation and antithetic sampling, that significantly improve the performance of MDM alignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA, and the resulting model, LLaDA 1.5, outperforms its SFTonly predecessor consistently and significantly across mathematical (GSM8K +4.7), code (HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard +4.3). Furthermore, LLaDA 1.5 demonstrates highly competitive mathematical performance compared to strong language MDMs and ARMs. Project page: https://ml-gsai.github.io/LLaDA-1.5-Demo/."
        },
        {
            "title": "Introduction",
            "content": "Recently, masked diffusion models (MDMs) [52, 2, 11, 38, 37, 47, 51, 42] have achieved significant progress in language modeling. By optimizing the evidence lower bound (ELBO) or its simplified variants, MDMs have demonstrated comparable or even superior performance to autoregressive models (ARMs) at small scale [37, 42, 39]. Furthermore, explorations of the scaling properties of MDMs have revealed their excellent scalability in various downstream tasks [39, 21, 40] and achieve competitive results to representative autoregressive models of the same size (e.g., LLaMA 3 [19]). Despite these advancements, existing work on MDMs has primarily focused on unsupervised pretraining and supervised fine-tuning (SFT) paradigms [40]. There has been relatively little effort in aligning these models with human preferences via reinforcement learning, even though RL has emerged as powerful and essential paradigm for aligning LLMs [48, 69, 43, 45, 49, 24]. * Equal contribution Work done during an internship at Ant Group Project leader Corresponding author Preprint. Under review. Figure 1: Benchmark results. The left panel shows that LLaDA 1.5 improves LLaDA consistently and significantly on various benchmarks. The right panel demonstrates that LLaDA 1.5 has highly competitive mathematical performance compared to strong language MDMs and ARMs. In this paper, we systematically investigate the RL-based alignment of MDMs, focusing on the direct preference optimization (DPO) [45] algorithm for its simplicity and strong empirical success. key challenge in applying DPO to MDMs lies in the intractability of the exact log-likelihoods required by the original DPO formulation. natural solution is approximating these log-likelihoods by their ELBOs, which involve double expectations over the diffusion time and masked data. This substitution yields preference score expressed as linear combination of four ELBO terms (see Eq. (7)). In practice, the ELBOs in the preference score are estimated via doubly Monte Carlo method [56, 17]. We demonstrate that this estimation process introduces additional bias and variance into the estimated loss and its gradient. To mitigate these errors, our theoretical analysis reveals crucial insight: the introduced bias and variance are governed by the variance of the preference score estimator. This finding underscores the need to control this variance for effective preference optimization. Building upon this, we introduce Variance-Reduced Policy Optimization (VRPO), method integrating principled techniques to reduce the variance of the preference score estimator: (1) increasing the sampling budget for ELBOs, (2) allocating sample budget across distinct diffusion timesteps with one masked sample per timestep, and (3) applying antithetic sampling [33] between ELBO estimates of the model and reference policies. Remarkably, such techniques have been theoretically proven to reduce the variance of the preference score estimator in an unbiased manner and empirically validated in both synthetic and large-scale real-world ablation studies (see Section 4.2). We further discuss on potential generalization of our variance reduction techniques to other alignment algorithms [48, 49]. Finally, we show the effectiveness of VRPO by applying it to LLaDA 8B Instruct [40], leading language MDM, using 350k preference pairs. As shown in Figure 1, the resulting model, LLaDA 1.5, improves LLaDA consistently and significantly on mathematics, coding, and alignment tasks. In addition, LLaDA 1.5 maintains highly competitive mathematical performance compared to strong MDMs [40, 61, 65] and ARMs [19, 6], achieving the highest score on Math. These results suggest that MDMs are compatible with RL-based alignment algorithms, laying foundation for future work."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 RL-based Alignment Methods Traditional alignment approaches [48, 69, 43] consist of two main stages. Reward modeling. In the first stage, static dataset of preference comparisons = {(x, yw, yl)} is constructed. For each prompt x, yw denotes the human-preferred response and yl denotes the less preferred one, respectively. parameterized reward model rϕ is trained to reflect these preferences by minimizing the following loss based on the Bradley-Terry formulation [8]: LReward(ϕ) E(x,yw,yl)D (cid:104) log σ (cid:0)rϕ(x, yw) rϕ(x, yl)(cid:1)(cid:105) , (1) where σ() is the sigmoid function. This encourages rϕ to assign higher scores to preferred responses. 2 Reinforcement Learning (RL). In the second stage, the language model policy πθ(y x), which defines the probability of generating response given prompt x, is then optimized via RL to maximize: (cid:2)rϕ(x, y)(cid:3) β DKL (cid:0)πθ( x) πref ( x)(cid:1) , ExD, yπθ(x) (2) max πθ where πref is fixed reference policy, often chosen as frozen SFT model, and β is coefficient controlling the regularization strength. Notably, in autoregressive models (ARMs), both sampling and likelihood evaluation for the policy are exactly characterized by the language model distribution. Direct Preference Optimization (DPO). DPO [45] offers simplified alternative to the two-stage paradigm above by avoiding explicit reward model training, while maintaining both theoretical grounding and strong empirical performance [23]. The DPO loss is defined as: (cid:34) (cid:18) log σ β log πθ(yw x) πref (yw x) β log πθ(yl x) πref (yl x) (cid:19)(cid:35) . (3) LDPO(θ) E(x,yw,yl)D 2.2 Masked Diffusion Models Masked Diffusion Models (MDMs) define model distribution via forwardreverse framework [52, 2]. Starting from the original input at = 0, the forward process progressively masks the input tokens with masking probability increasing over time, producing fully masked sequence at = 1. The reverse process learns to denoise this sequence by iteratively predicting the masked tokens as time reverses from = 1 to = 0. This framework enables principled modeling of complex data distributions, offering feasible exploration for non-autoregressive generation approaches. Likelihood estimation in MDMs. Unlike ARMs, the exact log-likelihood log π(y x) in MDMs is often approximated by its evidence lower bound (ELBO) [37, 42, 51, 47] as follows: Bπ(y x) EtU [0,1]Eytq(ytt,y,x)ℓπ(yt, t, x) log π(y x), where q(yt t, y, x) denotes the forward diffusion process at time given the full response and prompt x, and ℓπ represents the per-step loss of the mask prediction model, which admits multiple equivalent formulations elaborated in Appendix A. Notably, for continuous-time diffusion process (or equivalently, when the step size is infinitesimal), the bias of the ELBO for well-trained model relative to the exact likelihood is negligible, as demonstrated in prior works [52, 28, 53]. (4) Computing Bπ(y) exactly is generally intractable due to the double expectations. In practice, we approximate it by doubly Monte Carlo method. Letting nt and nyt be the numbers of samples for timesteps and masked data per timestep, we draw: i.i.d. U[0, 1] i.i.d. q(yt t(j), y), = 1, . . . , nt, St {t(j)}nt j=1 and Sy t(j) {y(k) t(j)}nyt k=1 (5) where we omit the conditions on for brevity. The ELBO is then estimated by: (cid:98)Bπ(y) 1 nt nt(cid:88) j=1 1 nyt nyt(cid:88) k=1 ℓπ(y(k) t(j) , t(j), y). (6) The estimator in Eq. (6) is an unbiased approximation of the ELBO. However, computational constraints prevent us from using large values for nt and nyt , as doing so would increase the batch size and training FLOPs by factor of nt nyt. Consequently, the variance of the estimator must be accounted for. In this work, we explore how to mitigate the adverse effects of ELBO estimation variance on preference optimization while operating under fixed computational budget. Large-scale language MDMs. LLaDA [40] is an 8B-parameter masked diffusion model for language generation. LLaDA is pretrained on 2.3 trillion tokens and fine-tuned on 4.5 million pairs of SFT data. It exhibits outstanding capabilities comparable with representative ARMs [19] in scalability, in-context learning, and instruction-following. In this paper, we adopt LLaDA 8B as base model to explore reinforcement learningbased alignment methods for MDMs."
        },
        {
            "title": "3 Method",
            "content": "We investigate how to align MDMs with human preferences using the DPO framework [45]. To address the intractability of the required log-likelihoods, we approximate them by unbiased ELBO 3 estimators. Remarkably, we prove that the bias and variance introduced by the approximation can be bounded by the variance of score estimator (a linear combination of four ELBOs). Therefore, we propose Variance-Reduced Preference Optimization (VRPO) by integrating multiple unbiased variance reduction techniques for better alignment. We also discuss potential extension beyond DPO. 3.1 Substituting Likelihoods with ELBOs in DPO We begin by adapting the DPO loss in Eq. (3) by substituting log-likelihoods with their ELBOs: LDPOE(θ) E(yw,yl)D (cid:20) log σ (cid:16) β (cid:0)Bπθ (yw)Bπref (yw)(cid:1)β (cid:0)Bπθ (yl)Bπref (yl)(cid:1)(cid:17)(cid:21) , (7) where the term highlighted in red is referred to as the preference score and denoted by sθ(yw, yl). Intuitively, the loss encourages the current model πθ to better prefer yw over yl than reference πref by comparing the ELBOs. As discussed around Eq. (4), the ELBO provides principled approximation to the log-likelihood with negligible bias. Moreover, the structure of the DPO lossspecifically its symmetric form and the smoothness of the sigmoid functioninherently helps mitigate the overall approximation gap, making LDPOE(θ) reliable surrogate for the original DPO objective. In practice, Eq. (7) is evaluated over batch of preference data Sdata {(y(i) each ELBO is estimated by Eq. (6). The resulting empirical loss is: i.i.d. D, and )}m i=1 , y(i) (cid:98)LDPOE(θ) = (cid:18) log σ β (cid:16) 1 (cid:88) i=1 (cid:98)Bπθ (y(i) ) (cid:98)Bπref (y(i) ) (cid:17) β (cid:16) (cid:98)Bπθ (y(i) ) (cid:98)Bπref (y(i) ) (cid:17)(cid:19) , (8) where we denote the preference score estimator, highlighted in red, by ˆsθ(y(i) ), and we use Sˆsyw,yl to denote the stochastic sampling introduced by ELBO estimators. We assume that the sampling procedures throughout the paper follow the standard independence conditions specified in Assumption 1. , y(i) Notably, the stochasticity of the preference score estimator introduces additional variance into the estimates of the overall preference optimization loss (i.e., (cid:98)LDPOE(θ)) and its gradient. Besides, due to the nonlinearity of log σ(), this also results in bias in these estimates (see Figure 2 (a)), although sθ(yw, yl) itself is an unbiased estimator for true preference score. In the remainder of this section, we address these challenges by first establishing how the variance of the preference score estimator (i.e. ˆsθ(yw, yl)) governs the overall bias and variance, and then proposing multiple principled variance reduction strategies to mitigate them. For clarity, we focus on the loss analysis in the main paper and defer the analogous gradient analysis to Appendix B.4. 3.2 Error Analysis of the Empirical Loss According to Eq. (8), in addition to the stochasticity introduced by the preference score estimator, the overall loss estimator is also affected by another irreducible source of randomnessnamely, the sampling of training preference datawhich is shared across all preference optimization methods. To disentangle these effects, we establish lemma that isolates and attributes the bias and variance in the overall loss estimation specifically to the ELBO-based stochasticity as follows. Lemma 1. The bias and variance of (cid:98)LDPOE(θ) over stochastic sampling satisfies: (cid:104) (cid:98)LDPOE(θ) LDPOE(θ) (cid:105) = Eyw,yl (cid:124) ESˆsyw ,yl (cid:2)log σ(ˆsθ(yw, yl)) log σ(sθ(yw, yl))(cid:3) (cid:125) , (cid:104) (cid:98)LDPOE(θ) (cid:105) = 1 Vyw,yl (cid:124) ESˆsyw ,yl (cid:2)log σ(ˆsθ(yw, yl))(cid:3) (cid:123)(cid:122) (cid:125) Vdata Eyw,yl (cid:124) VSˆsyw ,yl (cid:2)log σ(ˆsθ(yw, yl))(cid:3) (cid:123)(cid:122) (cid:125) Vpref . (cid:123)(cid:122) Bpref 1 + This identifies three error terms for (cid:98)LDPOE(θ): (i) Bpref , the expected gap between log σ(ˆsθ) and log σ(sθ), arising from the nonlinearity of log σ() applied to the preference score estimator ˆsθ; (ii) Vdata, the variance over preference data of the expected log σ(ˆsθ); (iii) Vpref , the expected 4 Figure 2: Toy example. (a) Non-linear transformation introduces bias as highlighted by the gap between the red and blue horizontal lines. (b) Both the bias and variance of log σ(X) exhibit monotonic trends with V[X], supporting the insight to jointly reduce these errors by reducing V[X]. variance of log σ(ˆsθ) over the Monte Carlo sampling in ˆsθ. Among these, Vdata captures the inherent variability in the preference dataset and is independent of the preference score estimation strategy. In contrast, Bpref and Vpref arise specifically from the score estimation required in MDMs and are absent in the original DPO formulation. The proof is provided in Appendix B.2.3. Building on Lemma 1, the following theorem demonstrates how preference-induced errors Bpref and Vpref can be directly bounded in terms of the variance of sθ. Intuitively, our goal is to address the effect of the nonlinearity introduced by log σ(), laying theoretical foundation for directly applying principled variance reduction techniques to sθ in Section 3.3. Theorem 1. The errors induced by score estimation in Lemma 1 can be bounded as: Bpref Eyw,yl (cid:20)(cid:113) VSˆsyw ,yl (cid:21) (cid:2)ˆsθ(yw, yl)(cid:3) , and Vpref 4Eyw,yl (cid:104) VSˆsyw ,yl (cid:2)ˆsθ(yw, yl)(cid:3)(cid:105) . Intuitively, the proof of Theorem 1 (see Appendix B.2.4) utilizes the Lipschitz continuity of log σ() and the unbiasedness of ˆsθ, which ensures that the variability in ˆsθ leads to controlled changes in the loss and keep it close to the true objective. Besides, in the toy example shown in Figure 2 (b), we plot how the variance of random variable influences the bias and variance of log σ(X). These curves exhibit trends that align well with Theorem 1. Collectively, these findings suggest that reducing the variance of ˆsθ can simultaneously mitigate both errors, motivating the design of VRPO. 3.3 Variance-Reduced Preference Optimization Building on Lemma 1 and Theorem 1, we now present VRPO (see Figure 3), set of principled and effective techniques designed to reduce the variance of the preference score estimator, thereby mitigating the bias and variance in the overall preference optimization as follows: (1) Sampling budget: Increase the number of samples = nt nyt used to estimate each ELBO. (2) Optimal allocation: Allocate the full budget to timesteps by setting nt = and nyt = 1. (3) Antithetic sampling: Share the same sampled timesteps and masked data between the ELBO estimates of the current policy πθ and the reference policy πref for the same input yw or yl. Practically, the first component increases the batch size and FLOPs of preference optimization by factor of n. However, in our default setting (n = 8), the additional overhead is fully affordable relative to the overall pretraining cost (see Section 4). Notably, the latter two components incur no additional sampling cost under fixed computational budget n: optimal allocation redistributes the existing samples across timesteps without increasing the total sample count, and antithetic sampling reuses samples across ELBO estimates, effectively serving as free lunch for preference optimization. Theoretically, none of the proposed techniques introduces bias into the preference score estimator. The first two components preserve the unbiasedness as they do not change the ELBO estimation form in Eq. (6), while antithetic sampling remains unbiased due to the linearity of expectation applied to the linear combination in Eq. (8) (proof in Appendix B.3). Below, we present the main theoretical results on variance reduction, with proofs deferred to Appendix B.3. We begin by characterizing the variance of the score estimator in the following lemma. 5 Figure 3: Illustration of VRPO. We compare VRPO (right) with naive strategy (left) without optimal allocation and antithetic sampling. VRPO allocates the sampling budget across timesteps and samples only one masked sequence per timestep (indicated by red arrows) and shares Monte Carlo samples between paired ELBOs (highlighted with the red annotations above the blocks). Lemma 2. The variance of the preference score estimator takes the form: Vˆsθ(yw, yl) = β2 (cid:20) (cid:16) (cid:98)Bπθ (yw)+V (cid:98)Bπref (yw)2Corr (cid:98)Bπθ (yw), (cid:98)Bπref (yw) (cid:17)(cid:113) + β2 (cid:20) (cid:16) (cid:98)Bπθ (yl)+V (cid:98)Bπref (yl)2Corr (cid:98)Bπθ (yl), (cid:98)Bπref (yl) (cid:17)(cid:113) (cid:21) (cid:98)Bπθ (yw)V (cid:98)Bπref (yw) (cid:21) (cid:98)Bπθ (yl)V (cid:98)Bπref (yl) . In Lemma 2, the subscript of variances and square brackets [] are omitted for brevity. This decomposition reveals two strategies to reduce Vˆsθ: (i) first, decreasing the variance of each ELBO estimation (cid:98)Bπ(y); (ii) second, increasing the correlation between the ELBO estimates for the same input y. The following theorem formalizes how the sampling budget and optimal allocation techniques in VRPO reduce variance through the first strategy. Theorem 2. Let (cid:98)Bπ(y) be estimated using total of = nt nyt masked samples. Then we have: (i) (cid:98)Bπ(y) = Θ( 1 ), and (ii) (cid:98)Bπ(y) is minimized when nt = and nyt = 1. Notably, Theorem 2 guarantees the effectiveness of allocating the entire sampling budget across timesteps: drawing one masked sample per timestep. Furthermore, the following theorem formally establishes how antithetic sampling leads to variance reduction as realized by the second strategy. Theorem 3. Suppose Corr( (cid:98)Bπθ (y), (cid:98)Bπref (y)) > 0 for {yw, yl}. Then we have: Using shared j=1 to compute (cid:98)Bπθ (y) and (cid:98)Bπref (y) yields lower Vˆsθ(yw, yl) Monte Carlo samples St and {Sy than using independent samples. t(j) y}nt Theorem 3 is inspired by the classical antithetic variates method [33], where shared randomness is leveraged to reduce the variance of the difference between paired estimates. In our setting, the assumption of positive correlation between the reference and current policies on the antithetic sample is natural: they typically share initialization and exhibit similarities in preferences on the same inputs. Finally, we validate the effectiveness of VRPO by applying it to LLaDA [40] (see details in Section 4), improving the performance significantly and consistently on extensive benchmarks. Besides, we provide systematic ablation studies to see the contribution of each component in Section 4.2. 3.4 Extension to Other Alignment Methods The variance reduction techniques in VRPO are not limited to DPO but have broader applicability to other alignment algorithms that involve estimating the ELBO or subtracting two correlated ELBOs. 6 This scenario commonly arises in algorithms [48, 43, 49]. When applied to MDMs, these methods require ELBOs to approximate intractable log-likelihoods within the KL divergence term in their objective functions (see Eq.(2) and Eq.(3) in Shao et al. [49]), as shown below: (cid:20) (cid:0)πθ(x) πref (x)(cid:1) = Eπθ(x) log DKL πθ(yx) πref (yx) (cid:21) 1 (cid:88) (cid:104) ˆBπθ (y(i)x) ˆBπref (y(i)x) (cid:105) . (9) i=1 The expression in magenta is in similar form of ELBO differences as in DPO (Eq. (8)), where VRPO techniques can be directly applied to reduce its variance. Other components in the objectives (e.g., reward signals [48, 43] or weighted per-token likelihood ratios [49]) can be exactly computed or approximated without requiring additional Monte Carlo sampling [65], and hence do not introduce extra variance that requires reduction."
        },
        {
            "title": "4 Experiments",
            "content": "We collect data across rich scenarios to align MDMs for general tasks and evaluate VRPO extensively on common benchmarks. We briefly present the setup, with more details provided in Appendix C. Data. We train LLaDA 8B Instruct [40] for one epoch on 350K preference pairs using VRPO, resulting in LLaDA 1.5. The data are collected internally at scale and undergo several processing steps, including filtering out low-quality samples, removing duplicates via similarity matching, ranking samples using reward model to select high-quality data, and replacing some responses with outputs from advanced LLMs. This process yields high-quality and diverse dataset covering wide range of topics such as writing, dialogue, knowledge Q&A, reasoning, mathematics, and coding. Training Efficiency. For VRPO, we use sampling budget = 8 by default. This results in an approximate 8 times increase in computation compared to methods without Monte Carlo estimation (e.g., ARMs or setting = 1). Despite this, we argue that the overall cost of VRPO remains modest less than 0.5% of pre-trainingmaking the added overhead practically acceptable. Moreover, under fixed n, the optimal allocation and antithetic sampling techniques in VRPO can also improve the effectiveness of preference optimization without introducing additional computation (see Section 4.2). Metrics and evaluation. Following common practice in open-source LLMs [23, 60, 35], we assess LLaDA 1.5 across three categories of tasks: mathematics and scientific reasoning (GSM8K [16], Math [27], GPQA [46]), code generation (HumanEval [13], MBPP [3]), and alignment (IFEval [68], Arena-Hard [34], AlignBench [36], MTBench [67]). Since MDMs can benefit from various sampling strategies to improve sample quality, we adopt the best sample strategy from diffusion sampling, diffusion semi-autoregressive remasking [40], and low-confidence remasking [12], for each task. 4.1 Benchmark Results Table 1 presents benchmark results for two models: LLaDA and LLaDA 1.5. LLaDA 1.5 consistently and significantly outperforms LLaDA across all benchmarks, demonstrating the effectiveness of VRPO. The efficacy of alignment is particularly pronounced on instruction following and aligning with human preferences, as illustrated in the case studies in Appendix D. While our goal is to enhance the general capabilities of LLaDA rather than optimize for specific tasks, we nonetheless observe strong mathematical performance in LLaDA 1.5. As shown in the right panel of Figure 1, compared to strong language MDMs [40, 61, 65] and ARMs [19, 6] of similar scale, LLaDA 1.5 maintains competitive mathematical performance. In particular, it achieves the highest four-shot score on GSM8K and the highest zero-shot score on Math among them. These experimental results not only demonstrate the effectiveness of VRPO, but also provide preliminary evidence for the compatibility between MDMs and RL-based alignment methods. This lays foundation for future work to further enhance the performance and reasoning capabilities of MDMs. 4.2 Ablation Experiments We conduct ablation studies to evaluate the impact of each variance reduction component in VRPO. We vary sampling configurations in three factors corresponding to these components: (1) the sampling budget = nt nyt, (2) the allocation strategy between the number of timesteps and masked samples 7 Table 1: Benchmark results. We compare the performance of LLaDA 1.5 against LLaDA Instruct [40] across various benchmarks. The consistent and significant improvements (as highlighted in green) demonstrate the effectiveness of VRPO. LLaDA 8B Instruct [40] LLaDA 1.5 8B Post-training"
        },
        {
            "title": "SFT",
            "content": "SFT + VRPO (Ours) Mathematics & Science GSM8K Math GPQA"
        },
        {
            "title": "HumanEval\nMBPP",
            "content": "IFEval Arena-Hard AlignBench MTbench 78.6 42.2 33."
        },
        {
            "title": "Code",
            "content": "49.4 41."
        },
        {
            "title": "Alignment Tasks",
            "content": "62.2 10.0 5.4 7.2 83.3 (+4.7) 42.6 (+0.4) 36.9 (+3.6) 52.4 (+3.0) 42.8 (+1.8) 66.2 (+4.0) 14.3 (+4.3) 5.9 (+0.5) 7.3 (+0.1) per timestep nt/nyt, and (3) the use of antithetic sampling. We set the base configuration as = 4, nt/nyt = 4/1, with antithetic sampling used. For each configuration, we measure: (i) the variance of preference score estimator Vˆsθ, (ii) the preference-induced variances Vpref of the loss and gradient (as defined in Lemma 1 and Appendix B.4), and (iii) benchmark results spanning mathematics, code, and alignment. Details of the empirical variance computation are provided in Appendix C. Table 2 summarizes all the results. We highlight four key observations: Effect of preference score estimator variance. Lower Vˆsθ generally leads to lower Vpref in both the loss and gradient, along with improved task performance. This empirical trend supports our theoretical insight from Theorem 1 to control the errors by Vˆsθ. Sampling budget. Increasing the sampling budget consistently reduces estimator variance and improves task performance. For instance, increasing from 1 to 8 reduces Vˆsθ from 44.0 to 1.0 and improves GSM8K accuracy from 80.1 to 83.3, validating our finding in Theorem 2 (i). Optimal allocation. Under fixed budget, allocating samples to distinct timesteps generally yields better results than repeating multiple mask samples per timestep. For example, nt/nyt = 4/1 outperforms nt/nyt = 1/4 across all metrics, supporting our theoretical result on the optimal allocation strategy in Theorem 2 (ii). Antithetic sampling. Removing antithetic sampling causes sharp rise in estimator variance, which confirms the theoretical prediction in Theorem 3. While we also observe that the downstream performance degrades more moderately. We argue that this is understandable. VRPO is theoretically designed to reduce the bias and variance in the DPO loss and gradient estimates, which directly improves optimization rather than generalization. In contrast, downstream task performance is influenced by multitude of factors, making its sensitivity to estimator variance harder to isolate. We hypothesize that disabling antithetic sampling may expose the model to broader diversity of data patterns, which could be beneficial for certain downstream tasks. To summarize, these results demonstrate strong empirical correlation between variance reduction and downstream task performance, supporting the theoretical analysis in Section 3. Crucially, they validate that the techniques in VRPO are necessary for effective preference optimization."
        },
        {
            "title": "5 Related Work",
            "content": "Alignment of LLMs and diffusion models. LLMs have achieved remarkable success through alignment with human preferences [54, 43, 4] and have demonstrated strong reasoning capabilities [41, 8 Table 2: Ablation of VRPO variance reduction strategies. We report estimator variances and benchmark results under different sampling configurations. As for biases, we refer to Figure 2 as an illustration since they are difficult to measure in practice. Results confirm that techniques in VRPO generally improve task performance, supporting the theoretical analysis in Section 3. # Timesteps nt # Masked samples nyt Antithetic sampling Base 4 1 Budget Allocation Antithetic 1 1 8 1 1 4 2 2 4 1 Vˆsθ Vpref of loss Vpref of grad GSM8K Math GPQA HumanEval MBPP IFEval Arena-Hard AlignBench MTbench Variances 2.2 3.1103 2.5 44.0 8.7102 13.0 1.0 2.6103 1.6 7.3 3.2102 4.7 4.7 7.3103 2. 2183.7 62.0 10.6 Mathematics & Science 80.1 41.7 34.3 50.6 40.6 83.3 42.6 36.9 52.4 42. Code Alignment Tasks 63.9 13.5 5.6 7.0 66.2 14.3 5.9 7.3 81.4 41.9 34.9 48.2 40. 64.8 13.8 5.8 7.0 82.8 42.3 36.4 51.2 42.8 66.1 13.9 5.9 7.4 82.3 42.4 36.4 48.8 41. 66.2 13.4 5.9 7.2 82.0 42.4 35.9 47.0 41.2 65.8 15.6 5.9 7.2 24]. With proximal policy optimization and variants [48, 49, 62] being prominent RL-based alignment methods. DPO [45] offers simple alternative by directly optimizing the model with preference data, eliminating explicit reward modeling. In contrast, the alignment of diffusion models has been less explored, with most existing research focused on the image domain [57, 7, 20, 14, 44]. Masked diffusion models. The inspiration for MDMs stems from advances in discrete diffusion models [52, 2], which introduced novel forward and reverse transition mechanisms and facilitated the development of various model variants [11, 30, 25, 59, 66]. Empirical studies have shown that MDMs can achieve perplexity comparable to ARMs, and some works have proposed simplified objectives for masked diffusion processes to enable efficient model training theoretically [37, 47, 51, 42]. Further research has explored the scaling properties of MDMs[39], including investigations into both training from scratch and adapting from pre-trained autoregressive models [40, 21, 61]. However, most existing studies on MDMs have primarily focused on pre-training and supervised fine-tuning paradigms, while RL-based alignment method remains relatively underexplored. Variance reduction techniques. Our work relates to the broad fields of variance reduction in Monte Carlo methods, doubly stochastic optimization, and variational inference. In Monte Carlo methods, variance reduction aims to enhance estimation accuracy by improving sampling strategies. Classic techniques include control variables and stratified sampling [33], where our approach adapts antithetic variates to couple correlated ELBO terms. The nested expectation in ELBOs further parallels variance analysis in doubly SGD [17, 56, 22, 32], motivating decomposition via the law of total variance to isolate distinct variance sources. Our approach also conceptually aligns with importance weighted variational inference [10, 31], where the outer bias is decreased by reducing the inner variance."
        },
        {
            "title": "6 Conclusion",
            "content": "We analyze the sources of bias and variance when applying DPO to MDMs, and propose the VRPO framework with theoretically grounded variance reduction techniques. Empirically, the resulting model LLaDA 1.5 with VRPO demonstrates improved performance in mathematics, coding, and 9 alignment benchmarks compared to LLaDA, and ablation studies show the effectiveness of VRPO. The work provides both theoretical and practical foundation for RL-based alignment of MDMs, and the proposed variance reduction techniques are potentially applicable to other RL alignment frameworks, paving the way for future research in the alignment of MDMs. Limitations. First, VRPO requires additional computational resources to achieve lower variance, resulting in slight increase in computational overhead. Second, although VRPO effectively reduces variance, the benefits are limited for certain downstream benchmarks, such as MTBench and ArenaHard. We leave further investigation into improving performance on these tasks for future work. Broader Impacts. Although LLaDA 1.5 demonstrates improved alignment with human preferences, it may still generate hallucinationsfactually incorrect content or information not present in the input. Nonetheless, leveraging larger-scale preference data and more advanced alignment methods may help mitigate this issue."
        },
        {
            "title": "References",
            "content": "[1] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. [2] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [5] Robert Bartle. The elements of integration and Lebesgue measure. John Wiley & Sons, 2014. [6] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [7] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. [8] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [9] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [10] Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. [11] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. [12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. [13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 10 [14] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. [15] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [16] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168, 9, 2021. [17] Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina Balcan, and Le Song. Scalable kernel methods via doubly stochastic gradients. Advances in neural information processing systems, 27, 2014. [18] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. [19] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [20] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. [21] Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. [22] Robert Gower, Mark Schmidt, Francis Bach, and Peter Richtárik. Variance-reduced methods for machine learning. Proceedings of the IEEE, 108(11):19681983, 2020. [23] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [24] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [25] Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029, 2022. [26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [27] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [29] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. [30] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:1245412465, 2021. 11 [31] Chin-Wei Huang and Aaron Courville. Note on the bias and variance of variational inference. arXiv preprint arXiv:1906.03708, 2019. [32] Kyurae Kim, Joohwan Ko, Yi-An Ma, and Jacob Gardner. Demystifying sgd with doubly stochastic gradients. arXiv preprint arXiv:2406.00920, 2024. [33] Dirk Kroese, Thomas Taimre, and Zdravko Botev. Handbook of monte carlo methods. John Wiley & Sons, 2013. [34] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. [35] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [36] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743, 2023. [37] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. [38] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:3453234545, 2022. [39] Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. [40] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [41] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. [42] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. [43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [44] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-toimage diffusion models with reward backpropagation. 2023. [45] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [46] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. [47] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524, 2024. [48] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 12 [49] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [50] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [51] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024. [52] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. [53] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [54] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. [55] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [56] Michalis Titsias and Miguel Lázaro-Gredilla. Doubly stochastic variational bayes for nonconjugate inference. In International conference on machine learning, pages 19711979. PMLR, 2014. [57] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. [58] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [59] Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu Wei, Jian Guo, Nan Duan, and Weizhu Chen. Ar-diffusion: Auto-regressive diffusion model for text generation, 2023. [60] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [61] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. URL https://hkunlp.github.io/blog/2025/dream. [62] Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. [63] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [64] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [65] Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning, 2025. URL https://arxiv.org/ abs/2504.12216. 13 [66] Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. [67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [68] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. [69] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Preliminaries"
        },
        {
            "title": "2.1 RL-based Alignment Methods\n2.2 Masked Diffusion Models",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Method"
        },
        {
            "title": "3.1 Substituting Likelihoods with ELBOs in DPO . . . . . . . . . . . . . . . .\n3.2 Error Analysis of the Empirical Loss . . . . . . . . . . . . . . . . . . . . .\n3.3 Variance-Reduced Preference Optimization . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . .\n3.4 Extension to Other Alignment Methods",
            "content": "4 Experiments"
        },
        {
            "title": "4.1 Benchmark Results .\n.\n4.2 Ablation Experiments .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Related Work 6 Conclusion Additional Formulation of MDMs 1 2 2 3 4 4 5 6 7 7 7 8 9 16 Proofs . . . . . B.1 Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1.1 Properties of log σ() (Lemma 3) . . . . . . . . . . . . . . . . B.1.2 Interchangeability of Expectation and Gradient (Lemma 4) . . . . . B.1.3 Bias and Variance of Transformed Random Variable (Lemma 5) . . . . . . . . . . . . . . . . B.1.4 Variance of ELBO Estimator (Lemma 6) 17 17 17 18 18 20 B.2 Bias and Variance of Estimated Loss . . . . . . . . . . . . . . . . . . . . . . 21 B.2.1 Standard Independence Assumptions (Assumption 1) . . . . . . . . . 21 . . . . . . 21 B.2.2 Unbiasedness of Preference Score Estimator (Theorem 4) 22 B.2.3 Bias and Variance Analysis of Estimated Loss (Lemma 1) . . . . . 23 B.2.4 Effect of Preference Score Estimator Variance (Theorem 1) . . . . . 23 B.3 Variance Reduction of Preference Score Estimator . . . . . . . . . . . . . . 23 B.3.1 Unbiasedness of VRPO (Theorem 5) . . . . . . . . . . . . . . . . 23 B.3.2 Variance Analysis of Preference Score Estimator (Lemma 2) . . . . 24 B.3.3 Sampling Budget and Allocation (Theorem 2) . . . . . . . . . . . . 24 . . . . . . . . . . . . . . . . . . B.3.4 Antithetic Sampling (Theorem 3) 24 B.4 Deferred Analysis of Estimated Gradient . . . . . . . . . . . . . . . . . . . 25 B.4.1 Bias and Variance Analysis of the Estimated Gradient (Lemma 7) . B.4.2 Effect of Preference Score Estimator Variance (Theorem 6) . . . . . 25 B.4.3 Variance Analysis of Preference Score Gradient Estimator (Lemma 8) 26 26 B.4.4 Sampling Budget and Allocation (Theorem 7) . . . . . . . . . . . . Experiments C.1 Implementation of VRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Model Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.5 Calculation of Variances . . . . . . . . Case Study 15 27 27 27 27 27 28 29 Analyze bias and variance of (cid:98)LDPOE(θ) and θ (cid:98)LDPOE(θ) (Lemma 1, Lemma 7) Identify score estimation-induced error terms: Bpref and Vpref Control Bpref and Vpref by Vˆsθ(yw, yl) and Vˆsθ(yw, yl) (Theorem 1, Theorem 6) Analyze Vˆsθ(yw, yl) and Vˆsθ(yw, yl) (Lemma 2, Lemma 8) Identify two variance reduction strategies: I. (cid:98)Bπ, Vθ (cid:98)Bπθ II. Corr( (cid:98)Bπθ , (cid:98)Bπref ) VRPO: (1) Sampling budget (2) Optimal allocation (3) Antithetic sampling (Section 3.3) Unbiasedness guarantee (Theorem 5) Variance reduction guarantees: I. Using (1) and (2) (Theorem 2, Theorem 7) II. Using (3) (Theorem 3) Figure 4: Illustration of the analysis process. This diagram outlines the conceptual flow that leads to the proposed VRPO method. Gray boxes represent theoretical analyses, white boxes denote resulting theoretical insights, and the blue box highlights the final sampling strategy. Starting from bias and variance analysis of estimated loss and gradient, we identify key error terms induced by score estimation and develop variance control strategies. These theoretical findings collectively motivate the design of the VRPO algorithm, which is equipped with provable properties (dashed lines): unbiasedness and guaranteed variance reduction."
        },
        {
            "title": "A Additional Formulation of MDMs",
            "content": "For completeness, we introduce the formulation of MDMs in this section. For further details, please refer to prior works [51, 47, 42]. In the forward process, MDMs corrupt an original sequence by progressively masking tokens. Each token is masked independently at noise level [0, 1]. Let {0, 1, . . . , 1}L be the original full response, where denotes the vocabulary size and denotes the sequence length, given prompt x, the forward process is formulated as: q(ytt, y, x) = (cid:89) i=1 q(yi tt, yi, x), q(yi tt, yi, x) = (cid:40) 1 t, t, = yi, yi yi = M, (10) where yi denotes the i-th token of response y, and denotes the mask token. 16 The reverse process starts from fully masked sequence and gradually unmask tokens to recover meaningful language sequences. For timesteps 0 < 1, the reverse process is defined as: q(yss, t, yt, x) = (cid:89) i=1 q(yi ss, t, yt, x), q(yi ss, t, yt, x) = ts pθ(yiyt, x), , 1, 0, yi = yi yi = yi = yi yi otherwise, = M, = M, = yi t, (11) where pθ is modeled by the mask prediction model. As stated in Section 2.2, the exact log-likelihood log π(yx) in MDMs is typically approximated by its ELBO [37, 42, 51, 47]: Bπ(yx) EtU [0,1]Eytq(ytt,y,x)ℓπ(yt, t, yx), where ℓπ(yt, t, yx) 1 L (cid:88) i=1 = M] log pθ(yiyt, x) . 1[yi As noted in [42, 40], the following formulation is an equivalent approximation: π(yx) ElU ({1,2,...,L})Eylq(yll,y,x)ℓ π(yl, l, yx), where π(yl, l, yx) ℓ L (cid:88) i=1 = M] log pθ(yiyl, x) , 1[yi (12) (13) (14) (15) with uniformly sampled from {1, 2, . . . , L}, and yl denoting the sequence obtained by masking tokens without replacement. In practice, although Eq. (14) and Eq. (12) are equivalent in expectation [42], the former typically yields lower variance during estimation [40]. Intuitively, Eq. (14) deterministically masks exactly out of tokens in each sequence, providing more consistent samples. In contrast, Eq. (12) relies on masking an expected fraction of the tokens, which introduces greater variability into the estimation process. In practice, we apply Eq. (14) as our log-likelihood estimator."
        },
        {
            "title": "B Proofs",
            "content": "Notations. We use SBπy and Sˆsyw,yl to denote the stochastic sampling in the ELBO estimates and t(j) be as defined in Eq. (5), Sdata be as the resulting preference score, respectively. Let St and Sy defined in Eq. (8). Θ() denotes functions of the same order. B.1 Auxiliary Lemmas B.1.1 Properties of log σ() (Lemma 3) Lemma 3 (Properties of log σ(x)). Let (x) = log σ(x), where and σ(x) = 1 the sigmoid function. Then satisfies the following properties: 1+ex denotes (i) concavity: (x) is concave; (ii) continuity: is 1-Lipschitz continuous on R, i.e., for all x1, x2 R, (x1) (x2) x1 x2; (iii) smoothness: is 1 4 -smooth on R, i.e., its derivative is 1 4 -Lipschitz continuous: for all x1, x2 R, (x1) (x2) 1 4 x1 x2. 17 Proof. We first compute the first and second derivatives of . Note that and (x) = ex 1 + ex = 1 1 + ex (0, 1), (x) = ex (1 + ex)2 [ 1 4 , 0). (i) Since (x) 0 for all R, we have is concave. (ii) We observe from above that (x) 1 for all R, implying that is 1-Lipschitz continuous. (iii) Since (x) 1 4 -Lipschitz continuous, and thus is 1 4 -smooth. 4 for all R, the derivative (x) is 1 B.1.2 Interchangeability of Expectation and Gradient (Lemma 4) Lemma 4 (Interchangeability of expectation and gradient). Let θ Rd, and let be random variable (or random vector) taking values in measurable space . Suppose fθ : Rd is differentiable with respect to θ for all , and there exists constant > 0 such that θfθ(X)2 for all . Then the expectation and gradient operators are interchangeable: θEfθ(X) = Eθfθ(X). Proof. Let θ Rd be fixed. For all X, for each {1, . . . , d}, define gi(X) := θi exists since fθ(X) is differentiable w.r.t. θ. By assumption, we have fθ(X), which gi(X) θfθ(X)2 C. For each i, by the mean value theorem and dominated convergence theorem [5, Chapter 5], we can interchange the expectation and derivative: θi Efθ(X) = θi fθ(X). Applying this for each coordinate and stacking the results gives the full gradient interchangeability: θEfθ(X) = Eθfθ(X). B.1.3 Bias and Variance of Transformed Random Variable (Lemma 5) Lemma 5 (Bias and variance of transformed random variable). Let Xθ be real-valued random variable with EXθ = µθ with parameter θ Rd, and define function (x) = log σ(x) on R, where σ(x) = 1 1+ex denotes the sigmoid function. Then: (i) The transformed random variable satisfies: E(cid:12) (cid:12)f (Xθ) (µθ)(cid:12) (cid:112)VXθ, (cid:12) Vf (Xθ) 4VXθ. (16) (17) (ii) Suppose there exists constant 0 such that the gradient of Xθ is uniformly bounded as θXθ2 C. Then, the gradient satisfies: (cid:13)θf (Xθ) θf (µθ)(cid:13) E(cid:13) (cid:13)2 trVθf (Xθ) 18 4 2 8 (cid:112)VXθ + (cid:112) trVθXθ, VXθ + trVθXθ. (18) (19) Proof. (i) As = log σ is 1-Lipschitz continuous by Lemma 3, for Eq. (16), we have: (cid:12)f (Xθ) (µθ)(cid:12) E(cid:12) (cid:12) EXθ µθ = E(cid:112)(Xθ µθ)2 (cid:112)E(Xθ µθ)2 = (cid:112)VXθ For Eq. (17), we have: Vf (Xθ) = E(cid:0)f (Xθ) Ef (Xθ)(cid:1)2 (cid:16)(cid:12) (cid:12)f (Xθ) (EXθ)(cid:12) 2E(cid:0)f (Xθ) (EXθ)(cid:1)2 = 2E(cid:0)f (Xθ) (EXθ)(cid:1)2 = 2E(cid:0)f (Xθ) (EXθ)(cid:1)2 2E(cid:0)f (Xθ) (EXθ)(cid:1)2 = 4E(cid:0)f (Xθ) (EXθ)(cid:1)2 4E(Xθ EXθ)2 = 4VXθ (cid:12) + (cid:12) (cid:17)2 (cid:12)f (EXθ) Ef (Xθ))(cid:12) (cid:12) + E(cid:0)f (EXθ) Ef (Xθ))(cid:1)2 + (cid:0)f (EXθ) Ef (Xθ))(cid:1)2 E(cid:0)f (EXθ) (Xθ)(cid:1)(cid:17)2 (cid:16) + + E(cid:0)f (EXθ) (Xθ)(cid:1) (Jensens inequality) (EXθ = µθ) (triangle inequality) ((a + b)2 2(a2 + b2)) (Jensens inequality) (f is 1-Lipschitz continuous by Lemma 3) (ii) Using the chain rule and the bounded gradient assumption, for Eq. (18), we have (cid:13) (cid:13)2 + (cid:13) (cid:12)f (Xθ) (µθ)(cid:12) (cid:13) (cid:13)θf (Xθ) θf (µθ)(cid:13) (cid:13)2 = (cid:13) (cid:13)f (Xθ)θXθ (µθ)θµθ (cid:13) (cid:13) (cid:13)f (µθ) (θXθ θµθ)(cid:13) (cid:0)f (Xθ) (µθ)(cid:1) θXθ (cid:13) (cid:13) (cid:13)2 (cid:13)2 (cid:13) = E[f (Xθ) (µθ) θXθ2] + (µθ) θXθ θµθ2 (cid:12) 4 4 4 4 Xθ µθ + θXθ EθXθ2 (cid:113) (cid:112)E(Xθ µθ)2 + Xθ µθ + θXθ θµθ2 (cid:12) + θXθ θµθ θXθ EθXθ2 2 (cid:112)VXθ + trVθXθ. (f is 1 (cid:112) = = (triangle inequality) (f is 1-Lipschitz continuous by Lemma 3) 4 -Lipschitz smooth by Lemma 3) (EXθ = µθ and Lemma 4) (Jensens inequality) To prove Eq. (19), we begin by decomposing the variance of the estimated gradient into three terms: trVθf (Xθ) = E(cid:13) E(cid:13) (cid:124) (cid:13) (cid:13)θf (Xθ) Eθf (Xθ)(cid:13) (cid:13)f (Xθ)θXθ (cid:2)f (Xθ)θXθ 2 2 = (cid:13) (cid:13) (cid:13) 2 (cid:13)f (Xθ)θXθ (EXθ)θXθ (cid:13) 2 (cid:123)(cid:122) (cid:125) (I) (cid:13)f (EXθ)θXθ (EXθ)EθXθ (cid:123)(cid:122) (II) + E(cid:13) (cid:124) (cid:3)(cid:13) 2 (cid:13) (cid:13) 2 (cid:13) 2 (cid:13) 2 (cid:125) (cid:13) (cid:13)f (EXθ)EθXθ (cid:2)f (Xθ)θXθ + (cid:13) (cid:124) (cid:123)(cid:122) (III) (cid:3)(cid:13) 2 (cid:13) (cid:13) 2 (cid:125) . We now bound each term separately. 19 Term (I). Using the bounded gradient assumption θXθ2 and the 1 (by Lemma 3), we have: 4 -Lipschitz smoothness of (I) = (cid:104)(cid:12) (cid:12)f (Xθ) (EXθ)(cid:12) (cid:12) (cid:12)f (Xθ) (EXθ)(cid:12) (cid:12) 2 16 EXθ EXθ2 = 2E(cid:12) 2 16 (cid:105) θXθ2 2 VXθ. Term (II). Since is bounded by 1 (by Lemma 3), we have: (II) = (cid:12) 2 EθXθ EθXθ2 2 (cid:12)f (EXθ)(cid:12) (cid:12) trVθXθ. Term (III). Applying Jensens inequality and again using the smoothness of and boundedness of θXθ, we have: (III) = (cid:13) (cid:13) 2 (cid:13)f (EXθ)EθXθ Ef (Xθ)θXθ (cid:13) 2 (cid:13) (cid:3)(cid:13) 2 E(cid:2)f (EXθ)θXθ (Xθ)θXθ (cid:13) (cid:13) = (cid:13) (cid:13) 2 E(cid:13) (cid:13) 2 (cid:13)f (EXθ)θXθ (Xθ)θXθ (cid:13) 2 (cid:105) (cid:104)(cid:12) (cid:12)f (EXθ) (Xθ)(cid:12) θXθ2 (cid:12) 2 2 16 (cid:12)f (EXθ) (Xθ)(cid:12) (cid:12) 2E(cid:12) VXθ. = 2 2 Summing all three terms yields: trVθf (Xθ) 2 8 VXθ + trVθXθ. B.1.4 Variance of ELBO Estimator (Lemma 6) Lemma 6 (Variance of ELBO estimator). Letting (cid:98)Bπ(y) be as defined in Eq. (6), we have: (i) The variance of the ELBO estimator satisfies: (cid:98)Bπ(y) = 1 nt VtEytt,yℓπ(yt, t, y) (cid:125) (cid:123)(cid:122) (cid:124) Vt + 1 ntnyt EtVytt,yℓπ(yt, t, y) (cid:125) (cid:123)(cid:122) (cid:124) Vyt . (ii) The variance of the gradient of the ELBO estimator for the model policy πθ satisfies: Vθ (cid:98)Bπθ (y) = 1 nt VtEytt,yθℓπθ (yt, t, y) (cid:123)(cid:122) (cid:125) (cid:124) + 1 ntnyt EtVytt,yθℓπθ (yt, t, y) (cid:123)(cid:122) (cid:125) (cid:124) yt . ) and Vyt (or yt The Vt (or ) capture variance across timesteps and variance due to the noise at each step, which are inherently determined by the data and the forward process and cannot be reduced. Proof. For (i), by the law of total variance, (cid:98)Bπ(y) = VSt (cid:124) {Sy t(j) y} (cid:123)(cid:122) (I) nyt j=1St (cid:98)Bπ(y) (cid:125) + ESt (cid:124) 20 {Sy t(j) y} (cid:123)(cid:122) (II) . nyt j=1St (cid:98)Bπ(y) (cid:125) Term (I). Conditioned on the t-sample, the inner expectation is: {Sy t(j) y} nyt j=1St (cid:98)Bπ(y) = 1 nt nt(cid:88) j=1 ESy t(j) ySt 1 nyt nyt(cid:88) k=1 ℓπ(y(k) t(j) , t(j), y) = 1 nt nt(cid:88) j=1 ytt(j),yℓπ(yt, t(j), y). Since terms in St are i.i.d. sampled, the outer variance is: (I) = VSt {Sy nyt j=1St (cid:98)Bπ(y) = t(j) y} 1 n2 VSt nt(cid:88) j=1 ytt(j),yℓπ(yt, t, y) = 1 nt VtEytt,yℓπ(yt, t, y). Term (II). Conditioned on the t-sample, the inner variance is: {Sy t(j) y} nyt j=1St (cid:98)Bπ(y) = = 1 n2 1 n2 nt(cid:88) j=1 nt(cid:88) j= VSy t(j) ySt 1 nyt nyt(cid:88) k=1 ℓπ(y(k) t(j), t(j), y) 1 n2 yt nyt(cid:88) k=1 ytt(j),yℓπ(yt, t(j), y) = 1 n2 nyt nt(cid:88) j=1 ytt(j),yℓπ(yt, t(j), y). Taking the expectation over St yields: ESt VSyt St (cid:98)Bπ(y) = 1 n2 nyt ESt nt(cid:88) j= ytt(j),yℓπ(yt, t(j), y) = 1 ntnyt EtVytt,yℓπ(yt, t, y). Combining (I) and (II) gives the result: (cid:98)Bπ(y) = 1 nt Vt + 1 ntnyt Vyt. For (ii), as θ (cid:98)Bπθ (y) has similar structure as (cid:98)Bπθ (y): nyt(cid:88) nt(cid:88) θ (cid:98)Bπθ (y) 1 nt 1 nyt j=1 k=1 θℓπθ (y(k) t(j), t(j), y), the proof closely follows that for (i), and thus we omit the details here. B.2 Bias and Variance of Estimated Loss B.2.1 Standard Independence Assumptions (Assumption 1) Assumption 1 (Standard independence assumptions). The following are independence conditions of common practice in Monte Carlo sampling of ELBOs and the ELBO-based DPO loss (Eq. (8)): 1. Given and St, Sy t(j) Sy t(j) for = j. 2. Given and y, SBπ1 SBπ2 for = where π1, π2 {πθ, πref }. 3. Given Sdata, Sˆsy(i) for = i. ˆsy(i ) ,y(i ) ,y(i) B.2.2 Unbiasedness of Preference Score Estimator (Theorem 4) Theorem 4 (Unbiasedness of preference score estimator). The preference score estimator defined in Eq. (8) is an unbiased estimator of the true preference score defined in Eq. (7): ESˆsyw ,yl (cid:2)ˆsθ(yw, yl)(cid:3) = sθ(yw, yl). Proof. First, by the i.i.d. sampling of timesteps and masked data, i.e., St {t(j)}nt j=1 i.i.d. U[0, 1] and Sy t(j) {y(k) t(j)}nyt k=1 i.i.d. q(yt t(j), y), = 1, . . . , nt, 21 and Sy t(j) Sy t(j ) for = as in Assumption 1, the ELBO estimator (Eq. (6)) is unbiased: St,{Sy t(j) y}nt j=1 (cid:98)Bπ(y) = ESt nt(cid:88) ESy 1 nt 1 nyt = EtU [0,1]Eytq(ytt,y)ℓπ(yt, t, x) = Bπ(y). t(j), t(j), y) ℓπ(y(k) t(j) k= j=1 nyt(cid:88) Since the preference score estimator is linear combination of four ELBO estimators, by the linearity of the expectation, we have: (cid:104) E(cid:2)ˆsθ(yw, yl)(cid:3) = βE (cid:98)Bπref (yl) = β (cid:0)Bπθ (yw)Bπref (yw)(cid:1)β (cid:0)Bπθ (yl)Bπref (yl)(cid:1) = sθ(yw, yl). (cid:105) (cid:98)Bπref (yw) (cid:105) (cid:98)Bπθ (yl) (cid:104) βE (cid:104) βE (cid:104) + βE (cid:98)Bπθ (yw) (cid:105) (cid:105) B.2.3 Bias and Variance Analysis of Estimated Loss (Lemma 1) Lemma 1. The bias and variance of (cid:98)LDPOE(θ) over stochastic sampling satisfies: (cid:104) (cid:98)LDPOE(θ) LDPOE(θ) (cid:105) ESˆsyw ,yl (cid:2)log σ(ˆsθ(yw, yl)) log σ(sθ(yw, yl))(cid:3) (cid:125) , = Eyw,yl (cid:124) (cid:104) (cid:98)LDPOE(θ) (cid:105) = 1 Vyw,yl (cid:124) ESˆsyw ,yl (cid:2)log σ(ˆsθ(yw, yl))(cid:3) (cid:125) (cid:123)(cid:122) Vdata Proof. As defined in Eq. (7) and Eq. (8), (cid:123)(cid:122) Bpref 1 + Eyw,yl (cid:124) VSˆsyw ,yl (cid:2)log σ(ˆsθ(yw, yl))(cid:3) (cid:125) (cid:123)(cid:122) Vpref . LDPOE(θ) = E(yw,yl)D (cid:104) log σ (cid:0)sθ(yw, yl)(cid:1)(cid:105) , and ˆLDPOE(θ) = 1 (cid:88) i=1 (cid:16) log σ ˆsθ(y(i) , y(i) ) (cid:17) . For the bias, based on the sampling procedure provided for Eq. (8) and the third independence condition in Assumption 1, we have: ESdata,{S }m (cid:105) (cid:104) ˆLDPOE(θ) LDPOE(θ) (cid:105) (cid:104) ˆLDPOE(θ) LDPOE(θ) ˆsy (i) (i) ,y i=1 =ESdata,{S }m i=1 ˆsy (i) (i) ,y (cid:88) ES i=1 1 ESˆsyw ,yl ESˆsyw ,yl =ESdata =Eyw,yl =Eyw,yl ˆsy (i) ,y (i) (cid:20) log σ (cid:16) ˆsθ(y(i) , y(i) ) (cid:17)(cid:21) Eyw,yl log σ (cid:0)sθ(yw, yl)(cid:1) (cid:2)log σ(ˆsθ(yw, yl))(cid:3) Eyw,yl log σ(sθ(yw, yl)) (cid:2)log σ(ˆsθ(yw, yl)) log σ(sθ(yw, yl))(cid:3). VSdata,{S For the variance, further by the law of total variance, we have: (cid:104) ˆLDPOE(θ) (cid:105) (cid:105) (cid:104) ˆLDPOE(θ) + ESdata =VSdata (i) (i) ,y V{S E{S }m }m i=1 ˆsy ˆsy (i) (i) ,y i=1 =VSdata 1 (cid:88) i=1 ES ˆsy (i) (i) ,y (cid:16) log σ ˆsθ(y(i) , y(i) ) (cid:17) (cid:105) (cid:104) ˆLDPOE(θ) ˆsy (i) ,y + ESdata }m i=1 (cid:88) (i) 1 m2 VS (i) (linearity of the expectation and Assumption 1) (i) ,y i=1 ˆsy (cid:16) log σ ˆsθ(y(i) , y(i) ) (cid:17) = 1 Vyw,yl ESˆsyw ,yl (cid:2)log σ(ˆsθ(yw, yl))(cid:3) + 1 Eyw,yl VSˆsyw ,yl (cid:2)log σ(ˆsθ(yw, yl))(cid:3). 22 B.2.4 Effect of Preference Score Estimator Variance (Theorem 1) Theorem 1. The errors induced by score estimation in Lemma 1 can be bounded as: Bpref Eyw,yl (cid:20)(cid:113) VSˆsyw ,yl (cid:21) (cid:2)ˆsθ(yw, yl)(cid:3) , and Vpref 4Eyw,yl (cid:104) VSˆsyw ,yl (cid:2)ˆsθ(yw, yl)(cid:3)(cid:105) . Proof. The proof is essentially based on the analysis of the bias and variance of the transformed random variable in Lemma 5. By the definition of Bpref and Vpref in Lemma 1, we know that: Bpref = Eyw,yl and Vpref = Eyw,yl ESˆsyw ,yl VSˆsyw ,yl (cid:2)log σ(ˆsθ(yw, yl)) log σ(sθ(yw, yl))(cid:3), (cid:2)log σ(ˆsθ(yw, yl))(cid:3). According to Lemma 5 presented previously, we have: Bpref Eyw,yl Eyw,yl and ESˆsyw ,yl (cid:113) VSˆsyw ,yl (cid:104)(cid:12) (cid:12)log σ(ˆsθ(yw, yl)) log σ(sθ(yw, yl))(cid:12) (cid:12) (cid:2)ˆsθ(yw, yl))(cid:3), (cid:105) (Lemma 5, Eq. (16)) Vpref Eyw,yl 4VSˆsyw ,yl (cid:2)ˆsθ(yw, yl))(cid:3). (Lemma 5, Eq. (17)) B.3 Variance Reduction of Preference Score Estimator B.3.1 Unbiasedness of VRPO (Theorem 5) Theorem 5 (Unbiasedness of VRPO). Under the variance reduction techniques in VRPO (Section 3.3), the preference score estimator defined in Eq. (8) remains an unbiased estimator of the true preference score defined in Eq. (7). Proof. The proof of Theorem 4 for the unbiasedness of ˆsθ(yw, yl) remains valid under variations in nt and nyt (corresponding to sampling budget and optimal allocation), as well as under the coupling of (cid:98)Bπθ(y) and (cid:98)Bπref(y) (corresponding to antithetic sampling). We refer readers to the original proof of Theorem 4 for details. B.3.2 Variance Analysis of Preference Score Estimator (Lemma 2) Lemma 2. The variance of the preference score estimator takes the form: Vˆsθ(yw, yl) = β2 (cid:20) (cid:16) (cid:98)Bπθ (yw)+V (cid:98)Bπref (yw)2Corr (cid:98)Bπθ (yw), (cid:98)Bπref (yw) (cid:17)(cid:113) + β (cid:20) (cid:16) (cid:98)Bπθ (yl)+V (cid:98)Bπref (yl)2Corr (cid:98)Bπθ (yl), (cid:98)Bπref (yl) (cid:17)(cid:113) (cid:21) (cid:98)Bπθ (yw)V (cid:98)Bπref (yw) (cid:21) (cid:98)Bπθ (yl)V (cid:98)Bπref (yl) . Proof. From Eq. (8), the preference score estimator is: (cid:17) (cid:16) ˆsθ(yw, yl) = β (cid:98)Bπθ (yw) (cid:98)Bπref (yw) (cid:16) β (cid:98)Bπθ (yl) + (cid:98)Bπref (yl) (cid:17) . Let us define: (cid:98)Bπθ (yw) (cid:98)Bπref (yw), (cid:98)Bπθ (yl) (cid:98)Bπref (yl). Then ˆsθ(yw, yl) = β(A B), and since and are independent by assumption, we have: Vˆsθ(yw, yl) = β2 (VA + VB) . Expanding each variance, we get: VA = (cid:98)Bπθ (yw) + (cid:98)Bπref (yw) 2 Corr(cid:0) (cid:98)Bπθ (yw), (cid:98)Bπref (yw)(cid:1) (cid:113) (cid:98)Bπθ (yw)V (cid:98)Bπref (yw), and similarly for B. Substituting the terms completes the proof. B.3.3 Sampling Budget and Allocation (Theorem 2) Theorem 2. Let (cid:98)Bπ(y) be estimated using total of = nt nyt masked samples. Then we have: (i) (cid:98)Bπ(y) = Θ( 1 ), and (ii) (cid:98)Bπ(y) is minimized when nt = and nyt = 1. Proof. The proof is essentially based on the variance analysis of the ELBO estimator in Lemma 6. According to Lemma 6, we know that: (cid:98)Bπ(y) = 1 nt Vt + 1 ntnyt Vyt. Given that = nt nyt and fixed allocation proportion nt [ , 1], we have: Then, we have and n, (cid:98)Bπ(y) = 1 cn Vt + 1 Vyt. (cid:98)Bπ(y) = Θ( 1 ), (cid:98)Bπ(y) = 1, arg min c[ 1 ,1] which gives the desired result. B.3.4 Antithetic Sampling (Theorem 3) Theorem 3. Suppose Corr( (cid:98)Bπθ (y), (cid:98)Bπref (y)) > 0 for {yw, yl}. Then we have: Using shared j=1 to compute (cid:98)Bπθ (y) and (cid:98)Bπref (y) yields lower Vˆsθ(yw, yl) Monte Carlo samples St and {Sy than using independent samples. t(j) y}nt Proof. This result yields naturally from Lemma 2 that when Corr( (cid:98)Bπθ (y), (cid:98)Bπref (y)) > 0, (cid:16) (cid:98)Bπθ (y) + (cid:98)Bπref (y) 2Corr (cid:98)Bπθ (y), (cid:98)Bπref (y) (cid:17)(cid:113) (cid:98)Bπθ (y)V (cid:98)Bπref (y) < (cid:98)Bπθ (y) + (cid:98)Bπref (y). B.4 Deferred Analysis of Estimated Gradient In this section, we present theoretical analysis of the effect of VRPO on gradient estimation, following structure analogous to the loss estimation analysis in the main paper. We first introduce an additional assumption on the gradient of per-step mask prediction loss ℓπθ , which serves as necessary condition for the subsequent derivations. Assumption 2 (Bounded gradient of per-step mask prediction loss). The gradient of the per-step masked prediction loss ℓπθ (yt, t, y) (Eq. (4)) is uniformly bounded. Specifically, there exists constant 0 < such that θℓπθ (yt, t, y)2 for all θ in the model parameter space, in D, and all [0, 1]. This boundedness assumption is reasonable in practice and leads directly to the following corollary. Corollary 1 (Bounded gradient of preference score estimator). Under Assumption 2, the gradient of the preference score estimator ˆsθ(yw, yl) is uniformly bounded. Specifically, there exists constant 0 < such that θ ˆsθ(yw, yl)2 for all θ in the model parameter space and (yw, yl) in D. 24 Proof. Recall that the preference score estimator is defined as: (cid:16) (cid:16) (cid:17) ˆsθ(yw, yl) = β (cid:98)Bπθ (yw) (cid:98)Bπref (yw) β (cid:98)Bπθ (yl) (cid:98)Bπref (yl) (cid:17) , where (cid:98)Bπ(y) = 1 nt nt(cid:88) j=1 1 nyt nyt(cid:88) k=1 ℓπ(y(k) t(j) , t(j), y). Taking the gradient with respect to θ leads to: θ ˆsθ(yw, yl) = βθ (cid:98)Bπθ (yw) βθ (cid:98)Bπθ (yl). Now expand each gradient term to get: θ (cid:98)Bπθ (y) = By Assumption 2, each term nt(cid:88) nyt(cid:88) θℓπθ (y(k) t(j), t(j), y). 1 ntnyt (cid:13) (cid:13)θℓπθ (y(k) (cid:13) (cid:13) (cid:13) 1 (cid:13) (cid:13) (cid:13)θ (cid:98)Bπθ (y) (cid:13)2 ntnyt = j=1 k=1 (cid:13) (cid:13) t(j), t(j), y) (cid:13)2 nyt(cid:88) nt(cid:88) (cid:13) (cid:13)θℓπθ (y(k) (cid:13) j=1 k=1 C, we have: (cid:13) t(j), t(j), y) (cid:13) (cid:13)2 C. Thus, (cid:13)θ ˆsθ(yw, yl)(cid:13) (cid:13) Setting = 2βC gives the desired result. (cid:13)2 β (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)θ (cid:98)Bπθ (yw) (cid:13)2 + β (cid:13) (cid:13) (cid:13)θ (cid:98)Bπθ (yl) (cid:13) (cid:13) (cid:13) 2βC < . B.4.1 Bias and Variance Analysis of the Estimated Gradient (Lemma 7) We begin by observing the bias and variance of the estimated gradient θ (cid:98)LDPOE(θ) in order to distinguish the errors introduced by preference score estimation. Lemma 7. The bias and variance of θ (cid:98)LDPOE(θ) over stochastic sampling satisfies: (cid:104) = Eyw,yl θ (cid:98)LDPOE(θ) θLDPOE(θ) (cid:124) (cid:2)θ log σ(ˆsθ(yw, yl)) θ log σ(sθ(yw, yl))(cid:3) , (cid:125) ESˆsyw ,yl (cid:105) (cid:123)(cid:122) pref (cid:104) (cid:105) θ (cid:98)LDPOE(θ) = 1 Vyw,yl (cid:124) ESˆsyw ,yl (cid:2)θ log σ(ˆsθ(yw, yl))(cid:3) (cid:125) (cid:123)(cid:122) data + 1 Eyw,yl (cid:124) VSˆsyw ,yl (cid:2)θ log σ(ˆsθ(yw, yl))(cid:3) (cid:125) (cid:123)(cid:122) pref . Proof. The proof proceeds analogously to the proof of Lemma 1 given in Appendix B.2.3, and is therefore omitted for brevity. B.4.2 Effect of Preference Score Estimator Variance (Theorem 6) pref and Building on the above analysis, we identify data as the inherent variance due to data, while pref and pref arise from preference score estimation and can be mitigated by improving the sampling strategy. We now present theorem that characterizes how the variance of the score estimator and the variance of its gradient influence Theorem 6. Suppose Assumption 2 holds. Then, there exists constant 0 < such that the errors induced by score estimation in Lemma 7 can be bounded as: (cid:34) 4 (cid:34) 2 8 ˆsθ(yw, yl) + trVSˆsyw ,yl (cid:113)VSˆsyw ,yl pref Eyw,yl trVSˆsyw ,yl θ ˆsθ(yw, yl) θ ˆsθ(yw, yl) VSˆsyw ,yl ˆsθ(yw, yl) + Eyw,yl (cid:13) (cid:13)B (cid:13) trV (cid:13) (cid:13) (cid:13)2 pref . (cid:35) , and (cid:113) pref (cid:35) . 25 Proof. The proof is essentially based on the analysis of the bias and variance of the transformed random variable in Lemma 5 presented previously. By the definition of Bpref and Vpref in Lemma 1, we know that: (cid:13) (cid:13)B (cid:13) pref (cid:13) (cid:13) (cid:13)2 Eyw,yl ESˆsyw ,yl (cid:13)θ log σ(ˆsθ(yw, yl)) θ log σ(sθ(yw, yl))(cid:13) (cid:13) (cid:13)2, and trV pref = Eyw,yltrVSˆsyw ,yl θ log σ(ˆsθ(yw, yl)). According to Corollary 1, under Assumption 2, there exists constant 0 < such that the gradient of ˆsθ(yw, yl) is uniformly bounded as ˆsθ(yw, yl)2 C. Then by Lemma 5, we have: ESˆsyw ,yl 4 (cid:13)θ log σ(ˆsθ(yw, yl)) θ log σ(sθ(yw, yl))(cid:13) (cid:13) (cid:13)2 (cid:113) (cid:113)VSˆsyw ,yl ˆsθ(yw, yl) + θ ˆsθ(yw, yl), trVSˆsyw ,yl 2 8 and trVSˆsyw ,yl θ log σ(ˆsθ(yw, yl)) VSˆsyw ,yl ˆsθ(yw, yl) + trVSˆsyw ,yl θ ˆsθ(yw, yl). Applying these bounds to the above equations gives the desired results. B.4.3 Variance Analysis of Preference Score Gradient Estimator (Lemma 8) Given Theorem 6, our goal is to reduce the variance associated with the preference score estimator, specifically Vˆsθ(yw, yl) and trVθ ˆsθ(yw, yl) (we omit the subscript on Sˆsyw,yl for brevity). The variance Vˆsθ(yw, yl) has been thoroughly analyzed in Appendix B.3, where VRPO is proposed as the variance reduction strategy. Now, we turn our focus to trVθ ˆsθ(yw, yl), showing that the first two techniques in VRPOincreasing the sampling budget and applying optimal allocationeffectively reduce this term. We begin by expanding Vθ ˆsθ(yw, yl) for detailed analysis. Lemma 8. The variance of the gradient of the preference score estimator takes the form: Vθ ˆsθ(yw, yl) = β2Vθ (cid:98)Bπθ (yw) + β2Vθ (cid:98)Bπθ (yl). Proof. According to the definition of ˆsθ(yw, yl) in Eq. (8), we have: Under the second condition in Assumption 1, we have: θ ˆsθ(yw, yl) = βθ (cid:98)Bπθ (yw) βθ (cid:98)Bπθ (yl). Vθ ˆsθ(yw, yl) = Vβθ (cid:98)Bπθ (yw) + Vβθ (cid:98)Bπθ (yl) = β2Vθ (cid:98)Bπθ (yw) + β2Vθ (cid:98)Bπθ (yl). B.4.4 Sampling Budget and Allocation (Theorem 7) Lemma 8 shows that Vθ ˆsθ(yw, yl) can be reduced by lowering the variance of θ (cid:98)Bπ(y). We next provide theoretical guarantee that increasing the sampling budget and adopting optimal allocation in VRPO lead to reduction in Vθ (cid:98)Bπ(y). Theorem 7. Let (cid:98)Bπ(y) be estimated using total of = nt nyt masked samples. Then we have: ), and (ii) Vθ (cid:98)Bπ(y) is minimized when nt = and nyt = 1 with fixed n. (i) Vθ (cid:98)Bπ(y) = Θ( 1 Proof. The proof relies on variance analysis of the gradient of the ELBO estimator established in Lemma 6. Since the argument closely parallels the proof of Theorem 2 in Appendix B.3.3, we omit the details here. Table 3: The architecture of LLaDA."
        },
        {
            "title": "FFN dimension",
            "content": "Key/Value heads"
        },
        {
            "title": "LLaDA",
            "content": "32 4096 32 126,464 12,"
        },
        {
            "title": "8.02 B",
            "content": "Non-embedding parameters"
        },
        {
            "title": "C Experiments",
            "content": "C.1 Implementation of VRPO We implement VRPO using packing strategy, where multiple preference data samples are packed into single sequence to maximize hardware utilization. For each sequence, we construct an attention mask so that tokens from distinct samples within the sequence cannot attend to each other. Furthermore, all sequences are padded to fixed length of 4096 with EOS tokens, which is consistent with the default pre-training context length used in LLaDA. During VRPO training, these padded EOS tokens are excluded from the loss calculation. C.2 Model Architecture In this section, we present the details of the SFT model LLaDA Instruct. The LLaDA architecture closely follows that of LLaMA [19]: it is masked diffusion model with 8B parameters, based on Transformer Encoder. Like LLaMA, LLaDA employs RMSNorm [64] for normalization, RoPE [55] for positional encoding, and SwiGLU [50] as the activation function. Detailed model specifications can be found in Table 3. C.3 Training To enhance the general capabilities of LLaDA, we use 350K preference pairs as our training data. These pairs were collected internally on large scale and processed by filtering out low-quality samples, removing duplicates based on similarity matching, ranking samples with reward models to select high-quality data, and replacing some chosen responses with outputs from state-of-theart LLMs, ultimately resulting in dataset comprising approximately 35% creative writing, 18% knowledge QA, 16% NLP tasks, 14% mathematics tasks, 7% recommendation tasks, 5% code generation, 3% reasoning tasks, and small portion of safety and other tasks. We trained the model for one epoch with batch size of 64 using the AdamW optimizer with weight decay of 0.01, β1 of 0.9, and β2 of 0.95. The learning rate schedule employed 15 warmup steps to maximum learning rate of 5 107, followed by cosine decay. We set β = 0.2 and initialize πref with LLaDA Instruct for VRPO. Training consumed approximately 405 H100 GPU hours for 8 Monte Carlo samples. Due to hardware resource constraints, we did not perform any hyperparameter search. C.4 Evaluation Similar to ARMs with diverse sampling methods [29, 9], MDMs also benefit from various sampling strategies that can enhance sample quality. Following prior work [12, 40, 47], we employ multiple methods to sample text from MDMs, including diffusion sampling, diffusion semi-autoregressive sampling, and low-confidence remasking. Table 4: Inference configurations for LLaDA and LLaDA 1.5. MDMs benefit from various sampling strategies. We list inference configurations for LLaDA and LLaDA 1.5 that achieve optimal performance. block length smaller than the answer length indicates the use of diffusion semiautoregressive sampling; otherwise, diffusion sampling is employed. LLaDA 8B Instruct LLaDA 1.5 8B"
        },
        {
            "title": "Block length Answer length Block length Answer length",
            "content": "GSM8K Math GPQA HumanEval MBPP IFEval Arena-Hard AlignBench MTBench 8 64 64 512 256 512 128 32 32 256 512 64 512 256 512 1024 512 512 16 128 16 32 32 32 128 32 16 256 1024 256 512 512 512 1024 512 256 Table 5: General tasks results. We compare the performance of LLaDA 1.5 against LLaDA Instruct [40] across general tasks. Post-training MMLU [26] MMLU-pro [58] HellaSwag [63] ARC-C [15] LLaDA 8B Instruct LLaDA 1.5 8B SFT + VRPO SFT 65.5 37.0 74.6 88.5 66.0 35.7 74.7 88. In diffusion semi-autoregressive sampling, to generate fixed length of tokens, the method divides the generation process into blocks, where is the number of tokens generated per block. Within each block, tokens are generated using the original reverse process, and then each block is generated autoregressively. Furthermore, the low-confidence remasking method remasks predicted tokens that exhibit the lowest confidence, based on the predictions. Additionally, we observed that for LLaDA SFT, due to the padding of EOS tokens during its SFT phase, tends to generate an excessive number of EOS tokens. This often leads to incomplete content generation, resulting in notably truncated outputs and adversely affecting model performance. Inspired by this, we set the confidence score for the EOS token to zero and observe improved performance for LLaDA. For example, using the same inference configuration as LLaDA, setting the EOS tokens confidence score to zero improved HumanEval scores from 47.6 to 49.4 and Math scores from 26.6 to 27.4. Consequently, we adopted this setting for evaluation. To ensure fair comparison, we employ both diffusion sampling and semi-autoregressive sampling for LLaDA and LLaDA 1.5 and report the best results. We tuned the answer length over {64, 128, 256, 512, 1024}, for semi-autoregressive sampling, we tuned the block length over {8, 16, 32, 64, 128}. As shown in Table 4, we detail the best inference configurations employed for each benchmark. We evaluated LLaDA 1.5 on general tasks using the same inference configurations, as presented in Table 5. Compared to LLaDA, the improvements of LLaDA 1.5 on general tasks are limited, and its performance even decreases on MMLU-pro. We hypothesize that this phenomenon may be attributed to the alignment tax [1, 4, 18], meaning that aligning the model with human preferences can sometimes lead to decrease in its fundamental capabilities. more rigorous understanding of this issue requires further investigation. C.5 Calculation of Variances We estimated Vˆsθ, Vpref of loss, and Vpref of grad in Table 2. We sampled 128 preference data samples, processed with batch size of 16. For each data point, 8 independent calculations were per28 formed. πref was initialized using LLaDA, while πθ was model checkpoint from the VRPO training process. Given the large model size, storing full gradients for each calculation was computationally prohibitive. Therefore, for estimating Vpref of grad, we specifically recorded the gradients of the up-projection layer within the Feed-Forward Network module of the first transformer block to serve as proxy for the full gradient."
        },
        {
            "title": "D Case Study",
            "content": "In this section, we present additional case studies of LLaDA 1.5. First, Table 6 and Table 7 illustrate the advantages of LLaDA 1.5 over LLaDA in alignment tasks. Table 8 and Table 9 provide comparison between LLaDA and LLaDA 1.5 on mathematical tasks. 29 Table 6: Alignment cases of LLaDA 8B Instruct and LLaDA 1.5 (1). Prompt Write brief biography of person named \"Brilliant Le Hou\". Do not use commas in your reply. Highlight at least 3 sections with markdown, i.e. *highlighted section*. The biography should start with the name of the person. LLaDA Brilliant Le Hou was born in 1985 in Paris, France. From young age, he showed keen interest for science and technology. He attended the prestigious École Polytechnique, where he excelled in mathematics and engineering. After graduating, Le Hou began his career at MIT, where he conducted groundbreaking research in artificial intelligence. *highlighted section* At MIT, Le Hou developed new algorithm that significantly improved the accuracy of machine learning models. This breakthrough earned him numerous awards and recognition in the scientific community. *highlighted section* After working at MIT, Le Hou founded tech company called BrilliantTech Inc. The company quickly became leader in the field of AI, with products that revolutionized industries such as healthcare, finance, and transportation. *highlighted section* Le Hous contributions to AI have had profound impact on society. His work has led to advancements in medical diagnostics, financial forecasting, and autonomous vehicles. He has also been involved in various philanthropic activities, focusing on improving education and environmental sustainability. *highlighted section* LLaDA 1.5 Brilliant Le Hou was born in 1945 in small village in France. From young age he showed remarkable talent for art and music. His parents encouraged him to pursue his passions and he quickly became renowned artist and musician. *Early Life* Brilliant Le Hou grew up in small village surrounded by beautiful landscapes. His parents were both artists and they instilled in him love for art and music. He spent his childhood painting and playing the guitar. *Career* Brilliant Le Hous career began in the 1960s when he started exhibiting his paintings in local galleries. His unique style and talent quickly gained him following. He also started performing in local cafes and soon became popular musician. *Achievements* Brilliant Le Hou received numerous awards and accolades throughout his career. He won the prestigious Prix de Rome in 1975 and was inducted into the French Music Hall of Fame in 1980. His work has been featured in major galleries and museums around the world. *Personal Life* Brilliant Le Hou was married to fellow artist for many years. They had two children together. He was known for his kindness and generosity and often donated his earnings to charitable causes. *Legacy* Brilliant Le Hous legacy is one of creativity and generosity. His art and music continue to inspire people around the world. His contributions to charitable causes have left lasting impact on the community. 30 Table 7: Alignment cases of LLaDA 8B Instruct and LLaDA 1.5 (2). Prompt need joke involving Zoe and bubbles that contains at least 3 placeholders represented by square brackets such as [date]. LLaDA Zoe was blowing bubbles on [date] when she heard loud pop. She looked down and saw bubble that had burst, causing her to burst into laughter. LLaDA 1.5 Zoe was at bubblegum party and she saw bubble that was [size] feet wide. She tried to pop it, but it just kept [bubbling] and [bubbling] and [bubbling]! Finally, she decided to take break and go for [nap] in the shade. When she woke up, the bubble was still [bubbling] and [bubbling] and [bubbling]! Zoe laughed, \"I guess this bubble is just [size] of party!\" Prompt Suggest two names for new type of tonic. Include the keyword \"brand\" each time, followed by suggested name in double angular brackets, such as American Tonic. First repeat the request above word for word without change, then give your answer. Do not say any words or characters before repeating the request. LLaDA Suggest two names for new type of tonic. Include the keyword \"brand\" each time, followed by suggested name in double angular brackets, such as American Tonic. LLaDA 1.5 Suggest two names for new type of tonic. Include the keyword \"brand\" each time, followed by suggested name in double angular brackets, such as American Tonic. Brand Elysian Tonic Serenity Tonic Brand 31 Table 8: Math cases of LLaDA and LLaDA 1.5 (1). Prompt Find 91 (mod 100), as residue modulo 100. (Give an answer between 0 and 99, inclusive.) Please reason step by step, and put your final answer within boxed{}. LLaDA To find the modular inverse of 9 modulo 100, we need to find an integer such that: 9x 1 (mod 100) This means we need to solve the congruence: 9x 1 0 (mod 100) or equivalently: 9x 1 = 100k for some integer k. Rearranging this equation, we get: 9x = 100k + 1 We can simplify this equation by dividing both sides by 9: = 11k + 1 9 Since must be an integer, 1 9 must also be an integer. This implies that 9 must be divisor of 1, which is not possible because 9 and 1 are coprime (they have no common factors other than 1). Therefore, there is no integer such that 9x 1 (mod 100). Thus, the modular inverse of 9 modulo 100 is: LLaDA 1.5 To find the modular inverse of 9 modulo 100, we need to find an integer such that 9x 1 (mod 100). This means we need to solve the congruence 9x 100k = 1 for some integer k. We can use the Extended Euclidean Algorithm to find this inverse. The Extended Euclidean Algorithm not only finds the greatest common divisor (gcd) of two numbers but also provides the coefficients and such that 9x 100k = 1. First, we apply the Euclidean Algorithm to find the gcd of 9 and 100: 1. 100 = 9 11 + 1 2. 9 = 1 9 + 0 The gcd of 9 and 100 is 1, which confirms that an inverse exists. Next, we use the Extended Euclidean Algorithm to express 1 as combination of 9 and 100: From the first equation: 1 = 100 9 11 This gives us: 1 = 100 9 11 Thus, = 11 and = 1. However, we need to be positive residue modulo 100. Therefore, we convert 11 to positive equivalent by adding 100: 11 + 100 = So, the modular inverse of 9 modulo 100 is: 89 32 Table 9: Math cases of LLaDA and LLaDA 1.5 (2). Prompt Question: The ratio of men to women on bus is 5:9. If the total number of passengers on the bus is 84, and 20 women alight from the bus at the next stop, how many women have remained on the bus? Lets think step by step Answer: LLaDA 1. The ratio of men to women on the bus is 5:9. This means for every 5 men, there are 9 women. 2. The total number of passengers on the bus is 84. 3. The total number of parts in the ratio is 5 + 9 = 14. 4. To find the number of men and women, we divide the total number of passengers by the total number of parts: - Number of men = 84 passengers / 14 parts = 6 men - Number of women = 84 passengers / 14 parts = 9 women 5. Now, 20 women alight from the bus at the next stop. 6. To find the number of women remaining on the bus, we subtract the number of women who alight from the initial number of women: - Number of women remaining = Initial number of women - Number of women who alight - Number of women remaining = 9 women - 20 women - Number of women remaining = 9 women Therefore, the number of women remaining on the bus is 9. The answer is 9.> LLaDA 1.5 1. The ratio of men to women on the bus is 5:9. This means for every 5 men, there are 9 women. 2. The total number of passengers on the bus is 84. 3. The total number of parts in the ratio is 5 + 9 = 14. 4. To find the number of men and women, we divide the total number of passengers by the total number of parts: - Number of passengers per part = 84 / 14 = 6 - Number of men = 5 parts * 6 passengers per part = 30 - Number of women = 9 parts * 6 passengers per part = 54 So, initially, there are 30 men and 54 women on the bus. 5. At the next stop, 20 women alight from the bus. 6. To find the number of women remaining on the bus, we subtract the number of women who alighted from the initial number of women: - Number of women remaining = 54 women - 20 women = 34 women The answer is"
        }
    ],
    "affiliations": [
        "Ant Group",
        "Beijing Key Laboratory of Research on Large Models and Intelligent Governance",
        "Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE",
        "Gaoling School of AI, Renmin University of China",
        "Tsinghua University"
    ]
}