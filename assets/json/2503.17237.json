{
    "paper_title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
    "authors": [
        "Yu-Hsi Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video is inherently challenging due to low contrast, environmental noise, and small target sizes. This paper provides a straightforward approach to address multi-UAV tracking in thermal infrared video, leveraging recent advances in detection and tracking. Instead of relying on the YOLOv5 with the DeepSORT pipeline, we present a tracking framework built on YOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies. We evaluate our approach following the metrics from the 4th Anti-UAV Challenge and demonstrate competitive performance. Notably, we achieve strong results without using contrast enhancement or temporal information fusion to enrich UAV features, highlighting our approach as a \"Strong Baseline\" for the multi-UAV tracking task. We provide implementation details, in-depth experimental analysis, and a discussion of potential improvements. The code is available at https://github.com/wish44165/YOLOv12-BoT-SORT-ReID ."
        },
        {
            "title": "Start",
            "content": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID Yu-Hsi Chen The University of Melbourne Parkville, Australia yuhsi@student.unimelb.edu.au 5 2 0 2 1 2 ] . [ 1 7 3 2 7 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video is inherently challenging due to low contrast, environmental noise, and small target sizes. This paper provides straightforward approach to address multi-UAV tracking in thermal infrared video, leveraging recent advances in detection and trackInstead of relying on the YOLOv5 with the Deeping. SORT pipeline, we present tracking framework built on YOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies. We evaluate our approach following the metrics from the 4th Anti-UAV Challenge and demonstrate competitive performance. Notably, we achieve strong results without using contrast enhancement or temporal information fusion to enrich UAV features, highlighting our approach as Strong Baseline for the multi-UAV tracking task. We provide implementation details, in-depth experimental analysis, and discussion of potential improvements. The code is available at https://github. com/wish44165/YOLOv12-BoT-SORT-ReID. 1. Introduction Multi-UAV tracking has emerged as crucial application in recent years, driven by significant advancements in hardware, detection models, and tracking algorithms. As UAVs equipped with sophisticated visual systems and advanced control dynamics continue to proliferate, wide range of UAV-based products have been introduced, as presented in [29]. However, these innovations also introduce new challenges, particularly in tracking UAV swarms. The need for effective swarm tracking has become increasingly urgent due to growing security concerns and the rising threat posed by unauthorized UAVs. Various UAV-related datasets have been developed to address these challenges to advance tracking and detection tasks. These datasets include trajectory reconstruction datasets, such as those in [14, 21], which provide UAV trajectories captured from single or multi-view cameras, and trajectory-based UAV datasets introduced in [3]. Additionally, RGB-based footage datasets, including those in [20, 30, 32, 34], have been widely used. Among these, thermal infrared video-based UAV datasets featuring both single-object tracking (SOT) and multi-object tracking (MOT) scenarios in [13, 17], have gained significant attention, particularly in major challenge events. These datasets have played pivotal role in improving UAV tracking and detection capabilities. Thermal infrared videos offer advantages over traditional RGB imagery, such as enhanced visibility in low-light and adverse weather conditions, making them ideal for security and surveillance applications. This paper focuses on using thermal infrared video for multi-UAV tracking, exploiting its importance in challenging environments where RGB-based methods may fail. Fig. 1 (a) illustrates thermal infrared frames with diverse backgrounds from the MOT training set, while Fig. 1 (b) highlights minor defects, such as annotation errors, redundancies, missed labels, and lowquality frames, that account for negligible portion of the dataset and can be safely disregarded during training. Additionally, Fig. 2 displays cropped image patches from bounding box annotations in the training set, illustrating the varying sizes of UAVs, from several pixels to single-digit pixels. We build complete UAV tracking workflow by leveraging the latest YOLOv12 [36] detector and BoT-SORT [1] tracking algorithm, which outperform the well-established YOLOv5 [18] with DeepSORT [40] combination. We also implement some strategies to enhance multi-UAV tracking performance further. Our contributions are as follows: 1. We establish multi-UAV tracking workflow based on YOLOv12 and BoT-SORT, setting strong baseline for thermal infrared video-based multi-UAV tracking tasks. 2. We provide insightful analysis of various trial adjustments, such as the impact of input image size and tracker buffer tuning, and offer essential considerations for future improvements starting from our strong baseline. 2. Related Work Existing perspectives for improving thermal infrared videobased multi-UAV tracking can be categorized into annota1 Figure 1. Demonstration using training and testing data from Track 3. (a) Shows UAV swarms with varying sizes and backgrounds in the training data. (b) Highlights annotation errors and frame defects: MultiUAV-230 (train) has incorrect annotations, MultiUAV-256 (train) contains redundant annotations, MultiUAV-294 (train) has missed annotations, and MultiUAV-068 (test) includes poor-quality frame. Figure 2. Illustration of cropped image patches from training data annotations. Each patch corresponds to bounding box from MultiUAV002, MultiUAV-013, MultiUAV-087, and MultiUAV-223, with sizes approximately 28 24, 10 10, 6 6, and 11 10, respectively. The top number denotes the frame sequence (1st to 12th frames), and each row represents the same object (same ID across frames). tion and benchmarking, spatial information enhancement, temporal and motion modeling, real-time optimization, unified frameworks, and detection-based tracking systems. As high-quality annotation is fundamental for robust tracking, prior studies have examined the impact of annotation errors on object detection [19], incorporated Multiple Hypothesis Tracking (MHT) to leverage temporal cues and reduce false positives [15], and introduced benchmarks to evaluate detection and tracking methods on UAV datasets [16]. Spatial information enhancement techniques, such as the Image Pyramid Guidance (IPG) module presented in [23], address feature imbalance by preserving fine-grained spatial details for accurate bounding box regression and classification, even in deep network layers. To further improve tracking robustness, temporal and motion modeling techniques exploit inter-frame correla2 tions, enhancing continuity and reducing fragmentation [11, 12, 22, 42]. Complementary to this, real-time optimization strategies reduce inference latency while maintaining accuracy, enabling efficient UAV tracking in real-world applications [7, 26, 39, 41]. Beyond these, unified frameworks integrate detection and tracking into end-to-end solutions, streamlining multi-UAV tracking pipelines [44, 46]. Additionally, detection-based methods incorporating cascading post-processing modules refine tracking accuracy by mitigating false positives and improving localization [35]. While prior works have contributed significantly to multiUAV tracking, our approach advances the field by leveraging the latest detector and tracker, setting new benchmark for thermal infrared video-based UAV tracking and guiding future research in the multi-UAV tracking task. 3. Methodology This section first defines the problem scope, followed by data analysis and preparation for model training. We then introduce the primary detection model, YOLOv12, and the tracking algorithm, BoT-SORT, before detailing our training and inference strategies. 3.1. Problem Statement The goal is to track UAVs as accurately as possible, with evaluation metrics detailed in Sec. 4.1. The challenge consists of three tracks, each corresponding to different scenario. Track 1 and Track 2 are SOT tasks, differing in whether the UAVs initial location is given. Track 3 is MOT task where the initial locations of UAVs are provided. 3.2. Data Analysis and Preparation We first analyze each tracks training and testing data, as summarized in Tab. 1. Track 1 and Track 2 share the same training set, consisting of 23 sequences at 512512 resolution and 200 sequences at 640512 resolution. The training set for Track 3 is composed of 200 sequences at 640512 resolution. For testing, Track 1 and Track 2 each contain 216 non-overlapping sequences. Track 1s test set is entirely at 640512 resolution, while Track 2 includes 16 sequences at 640512 and 200 at 512512. Track 3s test set consists of 100 sequences at 640512 resolution. Additionally, Tab. 1 reports the width, height, and area distributions, along with their mean and standard deviation, providing essential insights for model hyperparameter tuning. Note that there may be slight differences in the numbers compared to the official release, as we have removed redundant annotations and defect cases, as illustrated in Fig. 1 (b). After analyzing the data, we split it for model training preparation. The number of frames and bounding boxes used for training, validation, and testing in the SOT and MOT tasks are detailed in Tab. 2. Specifically, Track 1 and Track 2 use YOLOv12 with BoT-SORT, while Track 3 employs YOLOv12 with BoT-SORT-ReID. Note that some numbers are in parentheses since we found the test set to provide limited information for the SOT task. Thus, the values in parentheses reflect the data only split into training and validation sets. Additionally, for BoT-SORT training, 1/10 of the data is primarily used to train the ReID module. This approach provides more effective ReID module training since many scenes are visually similar. 3.3. YOLOv12 with BoT-SORT-ReID for MOT Based on the comprehensive evaluation results presented in [3], which benchmarks the YOLO series of detectors on UAV datasets featuring RGB footage, YOLOv12 was selected for all tracks due to its superior performance. YOLOv12 [36] represents the latest advancement in the YOLO series of object detectors, introducing key innovations to enhance accuracy and efficiency simultaneously. At its core, YOLOv12 adopts the Residual Efficient Layer Aggregation Network (R-ELAN), which addresses the optimization challenges associated with attention mechanisms, particularly in large-scale models. Building upon ELAN [37], R-ELAN introduces block-level residual design with adaptive scaling alongside refined feature aggregation strategy, jointly promoting effective feature reuse and stable gradient propagation with minimal overhead. Furthermore, YOLOv12 integrates an attention-centric architecture by combining FlashAttention [5, 6] with spatially aware modules, enabling enhanced contextual modeling while preserving low latency. Introducing 77 largekernel separable convolutions broadens the receptive field and strengthens object localization, particularly for small and medium-sized targets. The architecture is optimized for modern GPU memory hierarchies, delivering improved computational efficiency and reduced inference times without compromising detection performance. These innovations enable YOLOv12 to balance speed and accuracy, making it highly suitable for real-time applications, large-scale detection tasks, and tracking pipelines. BoT-SORT [1] combines Kalman Filter [40] with camera motion compensation (CMC) to stabilize tracking under dynamic conditions. CMC employs global motion compensation (GMC) via affine transformations, using image keypoints [33] tracked with pyramidal Lucas-Kanade optical flow [2] and outlier rejection. The affine transformation, estimated via RANSAC [8], compensates for background motion while maintaining object trajectory stability by adjusting Kalman Filter state vectors. BoT-SORT-ReID enhances multi-object tracking by integrating appearance cues from four distinct ReID architectures. The Bag of Tricks (Bagtricks) baseline employs ResNet-50 backbone with triplet loss, and cross-entropy loss batch normalization, for robust feature extraction. Attention Generalized-Mean Pooling with Weighted Triplet Loss (AGW) [43] improves"
        },
        {
            "title": "Testing Data",
            "content": "Track 1 Track 2 Track 3 Track 1 Track 2 Track Number of Sequences Number of Frames Resolutions Total Bounding Boxes Width Range (px) Width Mean Std (px) Height Range (px) Height Mean Std (px) Area Range (px2) Area Mean Std (px2) 23 / 200 16,022 / 231,557 512512 / 640512 229,839 [1, 146] 30.55 24.43 [1, 131] 19.43 13.63 [1, 17,161] 874.91 1158.50 200 151,831 640512 3,127,045 [0.96, 98.92] 10.56 5.75 [0.57, 55.5] 9.06 4.67 [0.95, 4344.63] 119.05 179.40 216 232,742 640512 216 [4, 140] 40.56 26.34 [3, 68] 23.76 13.34 [16, 7,956] 1241.12 1280.20 16 / 200 11,619 / 221,123 512512 / 640512 0 N/A N/A N/A N/A N/A N/A 100 75,487 640512 2,041 [1.86, 28.33] 9.71 4.06 [2.44, 25.78] 8.56 3.55 [10.88, 575.41] 95.14 86. Table 1. Summary of data characteristics for training and testing. The training data for Track 1 and Track 2 are identical, while the testing data for Track 1 and Track 2 have no overlapping parts. Since Track 2 does not provide initial bounding boxes, we use N/A to indicate that this information is not applicable. Note that some bounding boxes were removed in Track 1 and Track 2 training sets due to labeled non-existent, zero-sized width or height, and cases where the boxes covered the entire image."
        },
        {
            "title": "Characteristic",
            "content": "YOLOv12 YOLOv12 BoT-SORT-ReID Single-Object Tracking Multi-Object Tracking"
        },
        {
            "title": "Number of Frames\nTotal Bounding Boxes",
            "content": "148,547 (198,063) 138,084 (184,056) 49,515 (49,516) 45,848 (45,921) 49,517 46,045 121,355 2,501,753 30,337 625,292 75,913 (7,593) 1,580,931 (155,833) 75,918 (7,783) 1,546,092 (160,876) Table 2. Data preparation summary for YOLOv12 and BoT-SORT-ReID across single-object (Track 1 and Track 2) and multi-object (Track 3) tracking. For YOLOv12 in SOT, numbers in parentheses indicate an alternative split with only training and validation sets. For BoTSORT-ReID, numbers in parentheses indicate reduced ReID training set selecting only the first 1/10 of frames from each sequence. feature representation by incorporating non-local modules and generalized mean pooling. Strong Baseline (SBS) [25] enhances robustness with generalized mean pooling, circle softmax loss, and an advanced data augmentation strategy. Multiple Granularity Network (MGN) [38] extends SBS by introducing multiple feature branches to capture finegrained representations across different spatial scales. Additionally, linear tracklet interpolation with 20-frame gap, following ByteTrack [45], mitigates missed detections from occlusions or annotation errors. 3.4. Training and Inference Strategies To reduce the training time of the YOLOv12 detector, we adopt two-stage training strategy. First, we train YOLOv12 models (n, s, m, l, x) from scratch on the SOT dataset, which is split into training, validation, and testing subsets as detailed in Tab. 2. Subsequently, starting from this checkpoint, we fine-tune these models on the MOT dataset or with larger input image resolutions. This staged approach accelerates convergence, reduces overall training time, and enables the model to achieve competitive Average Precision (AP) within just few epochs. For the ReID module, we primarily employ reduced subset of the dataset to enhance training efficiency, as using the entire dataset for training would be highly time-consuming. The inference workflow is presented in Fig. 3. The overall procedure follows the original BoT-SORT scheme. However, we modify the output by reporting both online and lost targets for Track 1 and Track 2 while preserving the original output format for Track 3. We did not use linear track interpolation because ID switching frequently occurs due to camera motion or fast-moving UAVs, making interpolation ineffective for recovering missing detections. Instead, for the SOT task, we adopt strategy based on the assumption that each frame contains at most one UAV, following this priority order: (1) report the UAV with the highest confidence score among the online targets, (2) if no online target is available, continue reporting the previous ID as the lost target in the subsequent tracker buffer frames, (3) if no previous ID is available, report the last known location until new online targets are detected. This strategy leverages the Kalman Filters prediction to accurately estimate the UAVs location based on prior positions and velocity, significantly improving evaluation metrics in the SOT task. However, this strategy is not feasible for the MOT task due to the frequent overlap and ID switching between online and lost targets, which would lead to poor results. Therefore, we maintain the original output for Track 3 in this case. 4 Figure 3. YOLOv12n with BoT-SORT-SBS-S50 workflow diagram. The workflow follows the original BoT-SORT framework [1], with slight revision: incorporating lost tracks to compensate for uninformative frames and improve object continuity. Specifically, for Track 1 and Track 2, lost target information is used to annotate potential object locations, while Track 3 retains the BoT-SORT original output. 4. Experimental Results ized by the total number of ground-truth objects (GT): The experiments were conducted on two platforms: the first, system with an Intel Core i7-12650H CPU, an NVIDIA RTX 4050 GPU, and 16 GB of RAM; the second, high-performance computing (HPC) system [27], equipped with an NVIDIA H100 GPU and 80 GB of memory. All models were trained using the default settings (e.g., image input size of 640 and track buffer of 30 frames) unless otherwise specified in the content or tables. This section begins by outlining the evaluation metrics for the three tracks, followed by the results for both the SOT and MOT tasks. We then present the leaderboard rankings and discuss key considerations and potential image enhancement techniques that could further improve UAV tracking. 4.1. Evaluation Metrics Two evaluation metrics are used across the three competition tracks. The first metric applies to Track 1 and Track 2, where tracking accuracy is defined as: acc = (cid:88) t=1 IoUt δ(vt > 0) + pt (1 δ(vt > 0)) 0.2 (cid:32) (cid:88) t=1 pt δ(vt > 0) (cid:33)0.3 . (1) Here, IoUt is the Intersection over Union between the predicted and ground-truth bounding boxes at frame t. The variable pt denotes the predicted visibility flag, where pt = 1 if the predicted box is empty, and pt = 0 otherwise. The ground-truth visibility is given by vt, and δ(vt > 0) is an indicator function that equals 1 when the target is visible (vt > 0) and zero otherwise. The accuracy is averaged over all frames, with representing the number of frames in which the target is visible in the ground truth. The second metric used for Track 3 is the Multi-Object Tracking Accuracy (MOTA), which jointly penalizes false positives (FP), false negatives (FN), and identity switches (IDS), normalMOTA = 1 FP + FN + IDS GT . (2) MOTA ranges from to 1, with higher values indicating better tracking performance. The final score is obtained by averaging MOTA over all sequences. The following sections will present and evaluate all performance results based on the abovementioned metrics. 4.2. Evaluation Results on Track 1 and Track 2 We present the evaluation results for Track 1 and Track 2 together, as both are SOT tasks, with the only difference being the presence of the initial UAV location. Eight meaningful trials are selected for both tracks, as shown in Tab. 3. Trials 1 and 2 serve as an ablation study to assess the impact of BoT-SORT. The results demonstrate significant performance improvement: the score in Track 1 increases from 0.0786 to 0.5529, and in Track 2, it rises from 0.0992 to 0.3106 simply by adding BoT-SORT after the YOLOv12n detector. Trials 2 through 6 evaluate different detector model sizes (n, s, m, l, x), with the highest score achieved using YOLOv12l for both tracks. Trial 7 examines the effect of extended 300 epochs training, revealing decline in performance compared to the 100 epochs training, likely due to overfitting. Finally, Trial 8 for each track shows the highest score we submitted, tuning the minimum box area threshold from 10 to 4 for Track 1 and from 10 to 1 for Track 2 to better capture smaller UAVs that may have been missed with the default setting. 4.3. Evaluation Results on Track 3 The evaluation results for Track 3 can be categorized into four key observations. As shown in Tab. 4, Group 1 presents results using various YOLOv12 model sizes, revealing that YOLOv12n achieves the best performance despite being the smallest model. Group 2 examines the effect of different track buffer sizes, with the highest score observed using 60 buffer frames, suggesting that this configuration optimizes Configurations Trial 1 Trial 2 Trial 3 Trial 4 Trial Trial 6 Trial 7 Trial 8 Trial 1 Trial 2 Trial Trial 4 Trial 5 Trial 6 Trial 7 Trial 8 Track Track 2 YOLOv12 - epochs BoT-SORT - min box area Scores 100 10 0.0786 100 10 0.5529 100 10 0.5637 100 10 0. 100 10 0.5644 100 10 0.5548 300 10 0.5398 100 4 0.5813 100 10 0.0992 100 10 0. 100 10 0.3258 100 10 0.3283 100 10 0.3285 100 10 0.3132 300 10 0.3080 100 1 0. Table 3. Evaluation results for Track 1 and Track 2, summarizing eight trials per track. The first two rows detail YOLOv12 configurations, varying model sizes (n, s, m, l, x), and training epochs. The third row specifies the use of BoT-SORT ( indicates exclusion; indicates inclusion). The fourth row lists the min box area threshold for filtering tiny bounding boxes, adjusted to 4 and 1 to account for tiny UAVs. The final row reports the resulting scores. Trial 1 Trial 2 Trial 3 Trial 4 Trial Trial 6 Trial 7 Trial 8 Trial 9 Trial 10 Group Group 2 Configurations YOLOv12 BoT-SORT - track buffer Scores Configurations YOLOv12 - image size - epochs BoT-SORT - track buffer - ReID module - metric learning - epochs Scores 30 0. 30 0.635361 30 0.633887 30 0.631864 30 0.630822 Group 3 Trial Trial 12 Trial 13 Trial 14 1280 42 60 N/A N/A 0.749352 1600 20 60 N/A N/A 0.744046 640 100 30 sbs S50 TripletLoss 8 0. 640 100 30 sbs R101-ibn TripletLoss 58 0.646299 Trial 15 640 100 30 sbs S50 15 0.638609 Group 4 Trial 640 100 30 sbs S50 45 0.638781 60 0.638801 75 0.638788 90 0.638771 Trial 640 100 30 sbs S50 Trial 18 640 100 30 sbs S50 Trial 19 640 100 30 sbs S50 Final Trial 20 1600 11 60 sbs S50 TripletLoss CircleLoss CircleLoss CircleLoss CircleLoss CircleLoss 60 0.647056 120 0.647290 60 0. 33 0.647567 17 0.647591 17 0.760874 Table 4. Comprehensive evaluation results for Track 3. Experiments are grouped into four categories: (1) varying YOLOv12 model sizes, (2) tuning BoT-SORTs track buffer, (3) exploring input image resolutions, and (4) configuring ReID modules and training strategies. The final configuration, guided by these studies, achieves the highest score, confirming the effectiveness of our optimization. the ID reassociation process. Group 3 investigates the impact of varying image input sizes. Both 1280 and 1600 input sizes, compared to the default 640, result in significant performance boost. Group 4 discusses trials involving different ReID modules. Trial 13 uses the full ReID dataset, while Trials 14 through 19 are trained on reduced ReID dataset. This group also evaluates the influence of different configurations, including changes in the ReID module structure, metric learning strategies, and the number of training epochs. From these results, we draw the following conclusions: (1) ResNet-50 from the Strong Baseline Series outperforms ResNet-101 with Instance-Batch Normalization as the backbone for the ReID module, (2) replacing Triplet Loss with CircleLoss for metric learning leads to improved performance, and (3) ReID module training tends to overfit as the number of epochs increases. Based on all trials across the groups, we draw the following conclusions regarding score variations relative to Trial 1: (1) Model size affects performance by approximately 0.001, (2) Track buffer size influences the score by around 0.0001, (3) Image input size contributes the most significant impact, with score increase of about 0.1, and (4) the ReID module accounts for roughly 0.01. Leveraging these insights, Trial 20, which achieved the highest score we submitted, adopts the following configuration: YOLOv12n with an image size of 1600, trained for 11 epochs, combined with BoT-SORT-SBS-S50 equipped with CircleLoss, optimized with AdamW [24] and trained for 17 epochs. 4.4. Leaderboard Results Based on all trials across the three tracks, as summarized in Tab. 3 and Tab. 4, we report the leaderboard results in Tab. 5, which includes the top three scores for each track, our submitted scores, and the official baseline scores. While there remains gap between our scores and the top three, 0.1332, 0.1971, and 0.0502 for Tracks 1, 2, and 3, respectively, our performance shows substantial improvement over the baselines. Specifically, we achieve approximately twofold increase over the baseline scores in Tracks 1 and 3 and nearly fivefold improvement in Track 2. Notably, these results were obtained without employing image enhancement techniques or leveraging temporal information during train6 Figure 4. Demonstration of YOLOv12n with BoT-SORT-SBS-S50 predictions on Track 3 test data. (a) Predicted bounding boxes with object IDs. (b) Challenging scenarios: MultiUAV-0003 contains multiple overlapping UAVs; MultiUAV-135 includes an occluded UAV (red box, ID: 29) and flying creature misclassified as UAV (pink box, ID: 28); MultiUAV-173 features complex background, where IDs 16, 17, and 18 are misjudgments; and MultiUAV-261 presents nearly invisible UAVs, leading to missed detections and tracking failures. The last row presents heatmaps highlighting the models difficulty in UAV perception, especially in MultiUAV-261. ing. Integrating such advanced techniques from our strong baseline could significantly improve performance and make reaching top-three position highly feasible. 4.5. Discussion and Enhancement Techniques The evaluation results reveal several key insights. First, overfitting emerged due to our data-splitting strategy. To maximize scene diversity, we did not categorize videos by attributes such as fixed-camera setups or background types (e.g., sky or buildings). Instead, we directly split the dataset into training, validation, and testing sets, occasionally allowing frames from the same video to appear across splits. This likely contributed to overfitting, as evidenced by AP score discrepancies during local testing. Second, accurately rescaling the provided initial object positions to match the resolution used in training and inference is critical, as mismatches can mislead the tracker and degrade subsequent predictions. Third, increasing image resolution is key to breaking performance plateaus when parameter tuning fails to improve accuracy. For example, scaling from 640 to 1280 resolution yielded significant score improvement of approximately 0.1. However, further increases produced diminishing gains as training at 2560 pixels for 7 epochs reached score of 0.7072, and training at 3840 pixels for 1 epoch reached 0.7098, while both required significantly higher computational costs compared to training at 1280 7 Figure 5. Potential frame enhancement techniques for multi-UAV tracking on MultiUAV-262 video frame. From left to right: (1) original thermal infrared frame, (2) Sobel edge-based sharpening [10], (3) contrast enhancement via Contrast Limited Adaptive Histogram Equalization (CLAHE) [28], and (4) ReynoldsFlow+ visualization highlighting motion patterns to assist UAV detection [4]."
        },
        {
            "title": "Teams",
            "content": "Track 1 Track 2 Track 3 1st Place Team 2nd Place Team 3rd Place Team Strong Baseline (ours) The 4th Anti UAV Baseline 0.7323 0.7308 0.7145 0.5813 0.2965 0.6676 0.5712 0.5530 0.3559 0.0745 0.8499 0.8132 0.8111 0.7609 0.3747 Table 5. Leaderboard results for the top three teams, our approach, and the official baseline across all three tracks. Our method achieved scores of 0.5813 (19th), 0.3559 (14th), and 0.7609 (5th) in Tracks 1, 2, and 3, respectively, while the official baseline scored 0.2965 (32nd), 0.0745 (20th), and 0.3747 (20th). pixels. Fourth, memory consumption during inference with YOLOv12 and BoT-SORT-ReID accumulates over time, leading to program crashes. To address this, we executed inference on per-folder basis rather than processing all sequences in single run. Finally, there is clear performance gap between runs with accurate initial object positions and those without, as reflected in the performance difference between Track 1 and Track 2. This underscores the importance of promptly and reliably estimating initial positions to boost performance further. Additionally, as previously discussed, while our approach provides strong baseline, it remains insufficient for achieving top-tier performance without further refinement. Fig. 4 (a) displays our models predictions across various scenarios, while Fig. 4 (b) highlights several key failure cases: (1) overlapping UAVs frequently cause ID switches, (2) distinguishing UAVs from flying creatures remains difficult, with the model often reassigning new IDs to UAVs following brief occlusions, (3) complex backgrounds lead to missed detections and tracking failures, and (4) tiny UAVs in cluttered environments provide little to no valuable information, making detection highly unreliable. The corresponding heatmaps in the last row illustrate the models inability to perceive UAVs effectively in these challenging conditions. These limitations emphasize the importance of image enhancement techniques to improve performance further. Fig. 5 illustrates several potential image enhancement methods. From left to right: (1) the original thermal infrared frame, (2) Sobel edgebased sharpening [10], which highlights edges more clearly than the original, (3) Contrast Limited Adaptive Histogram Equalization (CLAHE) [28], which improves contrast, and (4) ReynoldsFlow+ [4], temporal enhancement method based on the Reynolds Transport Theorem [31], threedimensional generalization of the Leibniz integral rule [9], providing enhanced appearance for moving UAVs. 5. Conclusion This paper presents strong baseline for thermal infrared video-based multi-UAV tracking tasks. By integrating YOLOv12 with BoT-SORT, our approach significantly improves over the baseline. With additional strategies during training and inference, as discussed in the experimental results, we show that our method has the potential to rank in the top three, as seen in the Track 3 performance. We also identify key factors influencing performance compared to our initial trial: model size contributing approximately 0.001, track buffer size affecting the score by around 0.0001, image input size providing the most significant impact with score increase of about 0.1, and the ReID module adding roughly 0.01. While our approach is intuitive and straightforward, we propose several potential techniques for further improving accuracy. Overall, our method establishes strong baseline, primarily driven by the latest YOLOv12 detector and the advanced BoT-SORT tracking algorithm, making strong starting point in recent advancements in the UAV swarm tracking field. 6. Acknowledgments We thank the HPC system [27] at The University of Melbourne for providing the computational resources that significantly accelerated model training and enabled the completion of this paper."
        },
        {
            "title": "References",
            "content": "[1] Nir Aharon, Roy Orfaig, and Ben-Zion Bobrovsky. Botsort: Robust associations multi-pedestrian tracking. arXiv preprint arXiv:2206.14651, 2022. 1, 3, 5 [2] Jean-Yves Bouguet et al. Pyramidal implementation of the affine lucas kanade feature tracker description of the algorithm. Intel corporation, 5(1-10):4, 2001. 3 [3] Yu-Hsi Chen. Uavdb: Trajectory-guided adaptable bounding boxes for uav detection. arXiv preprint arXiv:2409.06490, 2024. 1, 3 [4] Yu-Hsi Chen and Chin-Tien Wu. Reynoldsflow: Exquisite arXiv flow estimation via reynolds transport preprint arXiv:2503.04500, 2025. 8 theorem. [5] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 3 [6] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. 3 [7] Houzhang Fang, Xiaolin Wang, Zikai Liao, Yi Chang, and Luxin Yan. real-time anti-distractor infrared uav tracker In Proceedings with channel feature refinement module. of the IEEE/CVF International Conference on Computer Vision, pages 12401248, 2021. 3 [8] MA FISCHLER AND. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM, 24(6):381 395, 1981. 3 [9] Harley Flanders. Differentiation under the integral sign. The American Mathematical Monthly, 80(6):615627, 1973. 8 [10] Suneet Gupta and Rabins Porwal. Combining laplacian and sobel gradient for greater sharpening. IJIVP, 6:12391243, 2016. 8 [11] Ruian He, Shili Zhou, Ri Cheng, Yuqi Sun, Weimin Tan, and Bo Yan. Motion matters: Difference-based multiscale learning for infrared uav detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30063015, 2023. 3 [12] Bo Huang, Junjie Chen, Tingfa Xu, Ying Wang, Shenwang Jiang, Yuncheng Wang, Lei Wang, and Jianan Li. Siamsta: Spatio-temporal attention based siamese tracker for tracking uavs. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12041212, 2021. 3 [13] Bo Huang, Jianan Li, Junjie Chen, Gang Wang, Jian Zhao, and Tingfa Xu. Anti-uav410: thermal infrared benchmark and customized scheme for tracking drones in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(5):28522865, 2023. 1 [14] Seobin Hwang, Hanyoung Kim, Chaeyeon Heo, Youkyoung Na, Cheongeun Lee, and Yeongjun Cho. 3d trajectory reconstruction of drones using single camera. arXiv preprint arXiv:2309.02801, 2023. [15] Kutalmis Gokalp Ince, Aybora Koksal, Arda Fazla, and Aydin Alatan. Semi-automatic annotation for visual object tracking. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12331239, 2021. 2 9 [16] Brian KS Isaac-Medina, Matt Poyser, Daniel Organisciak, Chris Willcocks, Toby Breckon, and Hubert PH Shum. Unmanned aerial vehicle visual detection and tracking using deep neural networks: performance benchmark. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12231232, 2021. 2 [17] Nan Jiang, Kuiran Wang, Xiaoke Peng, Xuehui Yu, Qiang Wang, Junliang Xing, Guorong Li, Guodong Guo, Qixiang Ye, Jianbin Jiao, et al. Anti-uav: large-scale benchmark for vision-based uav tracking. IEEE Transactions on Multimedia, 25:486500, 2021. 1 [18] Glenn Jocher, Alex Stoken, Jirka Borovec, Liu Changyu, Adam Hogan, Laurentiu Diaconu, Jake Poznanski, Lijun Yu, Prashant Rai, Russ Ferriday, et al. ultralytics/yolov5: v3. 0. Zenodo, 2020. 1 [19] Aybora Koksal, Kutalmis Gokalp Ince, and Aydin Alatan. Effect of annotation errors on drone detection with yolov3. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 10301031, 2020. [20] Jing Li, Dong Hye Ye, Timothy Chung, Mathias Kolsch, Juan Wachs, and Charles Bouman. Multi-target detection and tracking from single camera in unmanned aerial veIn 2016 IEEE/RSJ international conference hicles (uavs). on intelligent robots and systems (IROS), pages 49924997. IEEE, 2016. 1 [21] Jingtong Li, Jesse Murray, Dorina Ismaili, Konrad Schindler, and Cenek Albl. Reconstruction of 3d flight trajectories from In 2020 IEEE/RSJ International ad-hoc camera networks. Conference on Intelligent Robots and Systems (IROS), pages 16211628. IEEE, 2020. 1 [22] Yifan Li, Dian Yuan, Meng Sun, Hongyu Wang, Xiaotao Liu, and Jing Liu. global-local tracking framework driven by In Proboth motion and appearance for infrared anti-uav. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30263035, 2023. 3 [23] Ziming Liu, Guangyu Gao, Lin Sun, and Li Fang. Ipg-net: Image pyramid guidance network for small object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 10261027, 2020. 2 [24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. [25] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei Jiang. Bag of tricks and strong baseline for deep person In Proceedings of the IEEE/CVF conferre-identification. ence on computer vision and pattern recognition workshops, pages 00, 2019. 4 [26] Yanyi Lyu, Zhunga Liu, Huandong Li, Dongxiu Guo, and Yimin Fu. real-time and lightweight method for tiny airIn Proceedings of the IEEE/CVF borne object detection. Conference on Computer Vision and Pattern Recognition, pages 30163025, 2023. 3 [27] Bernard Meade, Lev Lafayette, Greg Sauter, and Daniel Tosello. Spartan hpc-cloud hybrid: delivering performance and flexibility. University of Melbourne, 10:49, 2017. 5, 8 [28] Purnawarman Musa, Farid Al Rafi, and Missa Lamsani. review: Contrast-limited adaptive histogram equalization detection guided by the spatial-temporal motion information. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30543063, 2023. 3 [43] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven CH Hoi. Deep learning for person reidentification: survey and outlook. IEEE transactions on pattern analysis and machine intelligence, 44(6):28722893, 2021. 3 [44] Qianjin Yu, Yinchao Ma, Jianfeng He, Dawei Yang, and Tianzhu Zhang. unified transformer based tracker for antiuav tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3036 3046, 2023. [45] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. In European conference on computer vision, pages 121. Springer, 2022. 4 [46] Jinjian Zhao, Xiaohan Zhang, and Pengyu Zhang. unified approach for tracking uavs in infrared. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12131222, 2021. 3 (clahe) methods to help the application of face recognition. In 2018 third international conference on informatics and computing (ICIC), pages 16. IEEE, 2018. 8 [29] Frederik Falk Nyboe, Nicolaj Haarhøj Malle, and Emad Ebeid. Mpsoc4drones: An open framework for ros2, px4, and fpga integration. In 2022 international conference on unmanned aircraft systems (ICUAS), pages 12461255. IEEE, 2022. 1 [30] Maciej Pawełczyk and Marek Wojtyra. Real world object detection dataset for quadcopter unmanned aerial vehicle detection. IEEE Access, 8:174394174409, 2020. 1 [31] M.C. Potter and J.F. Foss. Fluid Mechanics. John Wiley & Sons Canada, Limited, 1975. 8 [32] AWS Open Data Registry. Airborne object tracking dataset, 2023. Accessed: Feb. 19, 2025. 1 [33] Jianbo Shi et al. Good features to track. In 1994 Proceedings of IEEE conference on computer vision and pattern recognition, pages 593600. IEEE, 1994. 3 [34] Daniel Steininger, Verena Widhalm, Julia Simon, Andreas Kriegler, and Christoph Sulzbachner. The aircraft context dataset: Understanding and optimizing data variability in In Proceedings of the IEEE/CVF Internaaerial domains. tional Conference on Computer Vision, pages 38233832, 2021. 1 [35] Zongheng Tang, Yulu Gao, Zizheng Xun, Fengguang Peng, Yifan Sun, Si Liu, and Bo Li. Strong detector with simIn Proceedings of the IEEE/CVF Conference ple tracker. on Computer Vision and Pattern Recognition, pages 3047 3053, 2023. [36] Yunjie Tian, Qixiang Ye, and David Doermann. Yolov12: Attention-centric real-time object detectors. arXiv preprint arXiv:2502.12524, 2025. 1, 3 [37] Chien-Yao Wang, Alexey Bochkovskiy, and HongYuan Mark Liao. Yolov7: Trainable bag-of-freebies sets In Pronew state-of-the-art for real-time object detectors. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 74647475, 2023. 3 [38] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi Zhou. Learning discriminative features with multiple granularities for person re-identification. In Proceedings of the 26th ACM international conference on Multimedia, pages 274282, 2018. 4 [39] Zixuan Wang, Zhicheng Zhao, and Fei Su. Real-time tracking with stabilized frame. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 10281029, 2020. 3 [40] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with deep association metric. In 2017 IEEE international conference on image processing (ICIP), pages 36453649. IEEE, 2017. 1, [41] Han Wu, Weiqiang Li, Wanqi Li, and Guizhong Liu. realtime robust approach for tracking uavs in infrared videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 10321033, 2020. 3 [42] Xin Yang, Gang Wang, Weiming Hu, Jin Gao, Shubo Lin, Liang Li, Kai Gao, and Yizheng Wang. Video tiny-object"
        }
    ],
    "affiliations": [
        "The University of Melbourne, Parkville, Australia"
    ]
}