{
    "paper_title": "MONSTER: Monash Scalable Time Series Evaluation Repository",
    "authors": [
        "Angus Dempster",
        "Navid Mohammadi Foumani",
        "Chang Wei Tan",
        "Lynn Miller",
        "Amish Mishra",
        "Mahsa Salehi",
        "Charlotte Pelletier",
        "Daniel F. Schmidt",
        "Geoffrey I. Webb"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MONSTER-the MONash Scalable Time Series Evaluation Repository-a collection of large datasets for time series classification. The field of time series classification has benefitted from common benchmarks set by the UCR and UEA time series classification repositories. However, the datasets in these benchmarks are small, with median sizes of 217 and 255 examples, respectively. In consequence they favour a narrow subspace of models that are optimised to achieve low classification error on a wide variety of smaller datasets, that is, models that minimise variance, and give little weight to computational issues such as scalability. Our hope is to diversify the field by introducing benchmarks using larger datasets. We believe that there is enormous potential for new progress in the field by engaging with the theoretical and practical challenges of learning effectively from larger quantities of data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 2 2 1 5 1 . 2 0 5 2 : r a"
        },
        {
            "title": "MONSTER\nMonash Scalable Time Series Evaluation Repository",
            "content": "angus.dempster@monash.edu Angus Dempster Navid Mohammadi Foumani Chang Wei Tan Lynn Miller Amish Mishra Mahsa Salehi Charlotte Pelletier Daniel F. Schmidt Geoffrey I. Webb Monash University, Melbourne, Australia Universite Bretagne Sud, IRISA, Vannes, France Abstract We introduce Monsterthe MONash Scalable Time Series Evaluation Repositorya collection of large datasets for time series classification. The field of time series classification has benefitted from common benchmarks set by the UCR and UEA time series classification repositories. However, the datasets in these benchmarks are small, with median sizes of 217 and 255 examples, respectively. In consequence they favour narrow subspace of models that are optimised to achieve low classification error on wide variety of smaller datasets, that is, models that minimise variance, and give little weight to computational issues such as scalability. Our hope is to diversify the field by introducing benchmarks using larger datasets. We believe that there is enormous potential for new progress in the field by engaging with the theoretical and practical challenges of learning effectively from larger quantities of data. Keywords: time series classification, dataset, benchmark, bitter lesson"
        },
        {
            "title": "1 Introduction",
            "content": "State of the art in time series classification has become synonymous with state of the art on the datasets in the UCR and UEA archives (Bagnall et al., 2018; Dau et al., 2019; Bagnall et al., 2017; Middlehurst et al., 2024; Ruiz et al., 2021). However, most of these datasetsat least, most of those that are commonly used for evaluationare small: median training set size for the set of 142 canonical univariate time series datasets is just 217 examples. The preeminence of the datasets in the UCR and UEA archives as basis for benchmarking means that the field has become constrained by narrow focus on smaller datasets and models which achieve low 01 loss (classification error) on diversity of smaller datasets. 2025 Dempster et al.. Dempster et al. Empirical machine learning research relies heavily on benchmarking in one form or another (Liao et al., 2021). Benchmark datasets provide the data necessary for training and evaluating machine learning models. Certain datasets and benchmarks have become foundational to machine learning generally (Paullada et al., 2021). There is little doubt that the datasets in the UCR and UEA archives are as integral to the field of time series classification as are, for example, the MNIST, CIFAR, and ImageNet datasets to the field of image classification. [T]he ways in which we collect, construct, and share these datasets inform the kinds of problems the field pursues and the methods explored in algorithm development (Paullada et al., 2021). We might call this the dataset lottery or benchmark lotteryafter the hardware lotteryi.e., to paraphrase Hooker (2021), when method or set of methods win (predominate) because of their compatibility with existing benchmarks. benchmark should serve as proxy for broader task (e.g., image classification, or time series classification). given benchmark is only meaningful to the extent that performance on that benchmark reflects performance on broader task, and performance on that benchmark generalises to real-world problems (Liao et al., 2021). In the context of time series classification, current benchmarks favour models that have been optimised to achieve low classification error (01 loss) on diversity of smaller datasets, i.e., low-variance (high-bias) methods: see Section 2. Datasets currently used for benchmarking do not reflect either the theoretical or practical challenges of learning from largescale real-world data. This poses the risk that current benchmarks are unrepresentative of the broader task of time series classification, and that models considered state of the art on these benchmarks may not generalise toand therefore may have diminishing relevance forreal-world time series classification problems, especially those involving larger quantities of data. This also suggests that research in time series classification has only so far explored relatively narrow subset of ideas (see Hooker, 2021). We present Monsterthe Monash Scalable Time Series Evaluation Repositorya collection of large univariate and multivariate datasets for time series classification. Our aim is to complement the existing datasets in the UCR and UEA archives, while encouraging the field to diversify to include significantly larger datasets. We hope that, with the introduction of Monster, benchmarking in the field better represents the broader task of time series classification, and has increased relevance for real-world time series classification problems. We hope to inspire the field to engage with the challenges of learning from large quantities of data. We believe that there is enormous potential for new progress in the field. The rest of this paper is structured as follows. Section 2 expands on relevant background material. Section 3 provides further details of the Monster datasets. Section 4 provides preliminary baseline results for selected methods."
        },
        {
            "title": "2 Background",
            "content": "2.1 BiasVariance Tradeoff benchmark should reflect broader task, and performance on given benchmark should generalise to real-world problems. In this context, the UCR and UEA archives share one key limitation. Whereas historically, in the field of computer vision, different methods have 2 MONSTER Figure 1: Learning curves for low variance model vs low bias model on S2Agri-10pc-17. generally been evaluated on relatively small number of large datasets (e.g., ImageNet), in the field of time series classification, different methods are almost always evaluated on relatively large number of small datasets, i.e., the datasets in the UCR and UEA archives. Variance can be expected to be large when training sets are small and to decrease as training set size increases. As result, methods that effectively minimise variance will often achieve lower classification error on smaller datasets, while methods that minimise bias will often achieve lower classification error on larger datasets (Brain and Webb, 1999). This is illustrated in Figure 1, which shows learning curves for two models: low variance model (a single-layer CNN with random kernels) vs low bias model (a conventional two-layer CNN) on the S2Agri-10pc-17 dataset (further details are provided in Appendix B). Figure 1 shows that the low-variance model achieves lower 01 loss on smaller quantities of data, whereas the low-bias model achieves lower 01 loss on larger quantities of data. (Note that the appearance of learning curves such as these is potentially confounded by multiple factors, including how well the biases of the respective systems match the characteristics of the learning task.) We should not expect the same methods to achieve the lowest 01 loss on both smaller datasets and larger datasets, as these demand different learning characteristics. (The issue of dataset size is not just limited to the quantity of training data: small quantities of test data can mask large differences in real-world classification error: Liao et al., 2021.) As such, the methods currently considered state of the art in terms of accuracy on the datasets in the UCR and UEA archives are, by definition, likely dominated by methods optimised for smaller datasets or, in other words, methods that minimise variance (high bias, low variance methods). We see strategies for minimising variance in all or almost all state of the art methods for time series classification. Variance can be minimised via ensembling (e.g., InceptionTime (Ismail Fawaz et al., 2020), the HIVE-COTE models (Middlehurst 3 Dempster et al. et al., 2021), Proximity Forest (Lucas et al., 2019), and models such as DrCIF (Middlehurst et al., 2021) using ensembles of decision trees), explicit regularisation (e.g., methods using ridge classifier such as RDST (Guillaume et al., 2022), Weasel 2.0 (Schafer and Leser, 2023)), and/or overparameterisation (taking advantage of double descent, e.g., the Rocket family of methods (Dempster et al., 2020, 2021, 2023), and other methods making use of large feature space in combination with ridge regression classifier or other linear model, as well as large neural network models), or some combination of these approaches. 2.2 The Bitter Lesson It is not coincidence that, with some exceptions, deep learning methods have had relatively muted impact on the field. Models such as large deep neural networks are low bias models, and require significant quantities of training data in order to achieve competitive accuracy compared to less complex models. There has been significant amount of work applying deep learning methods in the field of time series classification (Foumani et al., 2024a). However, despite this, and despite the fact that some neural network models such as InceptionTime (Ismail Fawaz et al., 2020) are among the most accurate models, on average, on the datasets in the UCR and UEA archives, in large part deep learning methods have not had the kind of impact that they have had in other domains such as image classification or natural language processing. Arguably, time series classification has not yet had its ImageNet moment, simply because in almost all existing work the quantity of training data has been insufficient to allow for training low bias models such as large convolutional neural networks or transformer architectures effectively. (A not insubstantial amount of work involving deep learning in the context of time series is also problematic, e.g., involving directly or indirectly optimising test loss: Middlehurst et al. (2024).) It is not clear yet whether the bitter lessonthe only thing that matters in the long run is the leveraging of computation (Sutton, 2019)has yet been learned in the field of time series classification. The apparent diversity of methods considered state of the art may reflect diversity of inductive biases that are effective for extracting information from low quantities of data, but that actually limit the ability to learn effectively from large quantities of data. There is also the potential issue of overfitting benchmark itself, although this is of less immediate concern due to the recent additional of new datasets to the UCR archive (Middlehurst et al., 2024). Accordingly, as well as being larger, the Monster datasets also represent new datasets or, in other words, new out of sample collection of datasets on which to evaluate existing methods. 2.3 No Free Lunch Evaluation on large set of heterogeneous datasets has led to another difference (in contrast to, e.g., computer vision), namely, that in the field of time series classification, performance is typically measured in terms of accuracy over all of the datasets in the UCR and/or UEA archives. This kind of average performance represents an average over large set of highly heterogeneous input time series datasets. 4 MONSTER This favours, without necessarily any good reason, methods that perform well (achieve low classification error) on average, while not necessarily performing well on any particular subset of datasets or tasks. The no free lunch theorem suggests that, as the number of datasets included in the evaluation grows, the performance of all methods should converge on average, i.e., no one method will perform better than any other on all datasets (Wolpert and MacReady, 1997). In the real world, this kind of average performance is potentially of limited practical value. For example, given problem involving the classification of EEG data, we would rather use method demonstrated to have good classification performance on benchmark EEG data, rather than method that has low average classification error across both EEG data and data from one or more other domains. In other words, current research likely unjustifiably favours methods that not only minimise variance, but that achieve low 01 loss on average, with potentially limited relevance to any specific real-world application. In many cases it makes sense for model or architecture to be specialised to particular domain. For example, TempCNN uses short convolutional kernels, ideal for the short time series typical of Earth observation data, but which are not effective for capturing temporal relationships in long time series, e.g., those common in audio tasks. The lack of pooling layers allows TempCNN to locate temporal features important for tasks such as crop detection, but lacks the ability to detect scale-invariant features important in some other tasks (Pelletier et al., 2019). In contrast, ConvTran uses channel-wise convolutional kernels and attention to capture both relationships between channels and long-range temporal relationships, especially effective for EEG data (Foumani et al., 2024b), but which have potentially limited relevance to univariate and/or shorter time series. 2.4 Other Selection Pressures and the Hardware Lottery For the most part, the field has not been forced to contend with the practical challenges involved with learning from larger quantities of data. Just as smaller datasets favour methods that effectively minimise variance, different kinds of selection pressures exist in the context of larger datasets. In particular, larger datasets select for methods that are computationally suited to large datasets, and can make effective use of existing computational resources, i.e., the hardware lottery (Hooker, 2021). Methods with high computational and/or memory requirements quickly become impractical. Even for more efficient methods, training on large quantities of data presents significant practical challenges. 2.5 Opportunities The need for expanding benchmarking in the field to include larger datasets has been recognised for some time. Dau et al (2019) stated: [p]erhaps specialist archive of massive time series can be made available for the community in different repository (p 1295). Monster represents an opportunity for the field to diversify to include large datasets, to engage with the challenges of learning from larger datasets, to better reflect the broader task of time series classification, and to improve relevance for real-world time series classification problems. We believe that there is enormous opportunity for new progress in the field. 5 Dempster et al. Further, we make the following predictions in relation to the ways in which larger datasets might change the field of time series classification, which may or may not be borne out in practice in the long run: Only subset of existing methods will be practical, i.e., those which can take advantage of current hardware to train efficiently. The methods which achieve the lowest 01 loss on larger datasets will differ from the methods which achieve the lowest 01 loss on smaller datasets. Average performance (e.g., average 01 loss) will become less relevant than performance within meaningful subsets of tasks (e.g., classification of EEG data, vs classification of satellite image time series data)."
        },
        {
            "title": "3 The MONSTER Datasets",
            "content": "The initial release of the Monster benchmark includes 29 univariate and multivariate datasets with between 10,299 and 59,268,823 time series. Table 1 provides an overview of the datasets. (We consider this as an initial release, and we aim to continue to add datasets to the benchmark.) The datasets are available via HuggingFace: https://huggingface. co/monster-monash). Relevant code is available at: https://github.com/Navidfoumani/ monster. We provide the datasets in .npy format to allow for ease of use with Python and straightforward memory mapping. (We also provide the datasets in legacy .csv format.) All datasets are under creative commons licenses or in the public domain, or we otherwise have been given permission to include the dataset in this collection. All datasets are already publicly available in some form. We have processed the original time series into common format (.npy and .csv). The steps required to process each dataset were different and included, for example, extracting and labelling individual time series from broader time series data, interpolating irregularly sampled data, and resampling data where the original data was recorded at different sampling rates. We have endeavoured to lower the barrier of entry as much as possible while keeping the original data intact to the greatest extent possible. Further details for each of the datasets are set out below. Each dataset is provided with set of indices for 5-fold cross-validation, allowing for direct comparison between benchmark results. For some datasets, these simply represent stratified random cross-validation folds. For other datasets, the cross-validation folds have been generated taking into account important metadata, e.g., different experimental subjects (for EEG data), or different geographic locations (for satellite image time series data). We have assigned the datasets to one of six categories (audio, satellite, EEG, HAR, count, and other). The distribution of classes for the datasets in each category is shown in Figures 2, 3, 7, 8, 12, and 13, below (in each figure, the number in brackets corresponds to the number of classes). 6 MONSTER Dataset Instances Length Channels Classes AudioMNIST AudioMNIST-DS CornellWhaleChallenge FruitFlies InsectSound MosquitoSound WhaleSounds Audio 30,000 30,000 30,000 34,518 50,000 279,566 105,163 47,998 4,000 4,000 5,000 600 3,750 2,500 Satellite Image Time Series LakeIce S2Agri S2Agri-10pc TimeSen2Crop Tiselac CrowdSourced DreamerA DreamerV STEW Opportunity PAMAP2 Skoda UCIActivity USCActivity WISDM WISDM2 Pedestrian Traffic FordChallenge LenDB 129,280 59,268,823 5,850,881 1,135,511 99, EEG 12,289 170,246 170,246 28,512 161 24 24 365 23 256 256 256 256 Human Activity Recognition 17,386 38,856 14,117 10,299 56,228 17,166 149, Counts 189,621 1,460,968 Other 36,257 1,244,942 100 100 100 128 100 100 100 24 40 540 1 1 1 1 1 1 1 1 10 10 9 10 14 14 14 14 113 52 60 9 6 3 3 1 30 3 10 10 2 3 10 6 8 3 17 / 34 17 / 29 16 9 2 2 2 2 5 12 11 6 12 6 6 82 2 2 Table 1: Summary of Monster datasets. 7 Dempster et al. Figure 2: Class distributions for the audio datasets. 3.1 Audio 3.1.1 AudioMNIST and AudioMNIST-DS AudioMNIST consists of audio recordings of 60 different speakers saying the digits 0 to 9, with 50 recordings per digit per speaker (Becker et al., 2024b,a). The processed dataset contains 30,000 (univariate) time series, each of length 47,998 (approximately 1 second of data sampled at 44khz), with ten classes representing the digits 0 to 9. This version of the dataset has been split into cross-validation folds based on speaker (i.e., such that recordings for given speaker do not appear in both the training and validation sets). AudioMNIST-DS is variant of the same dataset downsampled to length of 4,000. 3.1.2 CornellWhaleChallenge CornellWhaleChallenge consists of hydrophone recordings (Karpiˇstˇsenko et al., 2013). The processed dataset consists of 30,000 (univariate) time series, each of length 4,000. The task is to distinguish right whale calls from other noises. (An abridged version of this dataset is included in the broader UCR archive.) This version of the dataset has been divided into stratified random cross-validation folds. 3.1.3 FruitFlies FruitFlies, taken from the broader UCR archive, consistst of 34,518 (univariate) time series, each of length 5,000, representing acoustic recordings of wingbeats for three species of fruit fly (Potamitis, 2016; Flynn, 2022). The task is to identify the species of fly based on the recordings. This version of the dataset has been split into stratified random crossvalidation folds. 3.1.4 InsectSound InsectSound , taken from the broader UCR archive, consists of 50,000 (univariate) time series, each of length 600, representing recordings of wingbeats for six species of insects, 8 MONSTER with 2 different genders for 4 of the 6 species (Chen et al., 2014; Chen, 2014). This version of the dataset has been split into stratified random cross-validation folds. 3.1.5 MosquitoSound MosquitoSound , taken from the broader UCR archive, consists of 279,566 (univariate) time series, each of length 3,750, representing recordings of wingbeats for six different species of mosquito (Fanioudakis et al., 2018; Potamitis, 2018). The task is to identify the species of mosquito based on the recordings. This version of the dataset has been split into stratified random cross-validation folds. 3.1.6 WhaleSounds WhaleSounds consists of underwater acoustic recordings around Antarctica, manually annotated for seven different types of whale calls (Miller et al., 2020, 2021). The dataset has been processed to extract the annotated whale calls from the original recordings. The processed dataset contains 105,163 (univariate) time series, each of length 2,500, with eight classes representing the seven types of whale call plus class for unidentified sounds. This version of the dataset has been split into stratified random cross-validation folds. 3.2 Satellite Time Series Figure 3: Class distributions for the satellite datasets. 3.2.1 LakeIce LakeIce consists of pixel-level backscatter (reflection) values from satellite images of Yukon, Canada (Shaposhnikova et al., 2022, 2023). The time series are extracted over three decades from ERS-1/2, Radarsat, and Sentinel-1 synthetic aperture radar satellites. The processed dataset contains 129,280 (univariate) time series each of length 161, representing 6 months of data (October to March), with three classes representing bedfast ice, floating ice, and land. This version of the dataset has been split into stratified random cross-validation folds. 9 Dempster et al. Figure 4: Map of France showing the location of the Sentinel-2 tile used in the S2Agri dataset. 3.2.2 S2Agri S2Agri is land cover classification dataset and contains single tile of Sentinel-2 data (T31TFM), which covers 12,100 km2 area in France: see Figure 4 (Garnot et al., 2020; Sainte Fare Garnot and Landrieu, 2022). Ten spectral bands are used, and these are provided at 10m resolution. The dataset contains time series of length 24, observed between January and October 2017. The area has wide range of crop types and terrain conditions. The original S2Agri dataset is designed for parcel-based processing and contains data for 191,703 land parcels, with data for each parcel provided in separate file. We have reorganised the data for pixel-based processing, leading to dataset containing 59,268,823 pixels. Two sets of land cover classification labels are provided, one with 19 classes and the other with 44 classes. However, some of the 44-classes are only represented by one land parcel. We have removed the pixels in these land parcels from the dataset. This means there are only 17 and 34 classes respectively that are represented in the final dataset. The class label of each parcel comes from the French Land Parcel Identification System. The dataset is unbalanced: the largest four of the 19-class labels account for 90% of the parcels. We thus provide two versions of the S2Agri dataset, S2Agri-17 , which uses the 17 class labels and S2Agri-34 , which uses the 34 class labels. Additionally, we have created smaller versions of the datasets consisting of data for randomly selected 10% of the land parcels, each containing 5,850,881 pixels. These are S2Agri-10pc-17 and S2Agri-10pc-34 for the 17-class and 34-class labels, respectively. To create the folds used for cross-validation, we split the data based on the original land parcels, thus ensuring that all pixels in land parcel are allocated to the same fold. Splits are stratified by class labels to ensure an even representation of the classes across the folds. 10 MONSTER (a) Map of Austria showing Sentinel-2 Tiles (b) Class counts by Sentinel-2 Tile Figure 5: Location of Sentinel-2 tiles and class counts for the TimeSen2Crop dataset 3.2.3 TimeSen2Crop TimeSen2Crop consists of pixel-level Sentinel-2 data at 10m resolution, extracted from the 15 Sentinel-2 tiles that cover Austria: see Figure 5a (Weikmann et al., 2021). The dataset contains 16 classes representing different land cover types. The original data contains all Sentinel 2 images covering Austria acquired between September 2017 and August 2018 plus images for one tile acquired between September 2018 and August 2019. As the tiles are from different Sentinel-2 tracks and have been processed to remove images with cloud cover greater than 80%, the image acquisition dates for each tile differ and are irregular. This version of the dataset has been processed to interpolate each pixel to daily time series representing one year of data (thus each pixel has time series length of 365) and removing the other crops class. The processed dataset contains 1,135,511 multivariate time series, each with 9 channels (representing 9 spectral bands) and 15 classes. Classes are unbalanced and unevenly distributed across the Sentinel-2 tiles: see Figure 5b. The dataset has been split into cross-validation folds based on geographic location by Sentinel-2 tile (i.e., such that, for each fold, time series from given location appear in either the training set or test set but not both). 3.2.4 TiSeLaC TiSeLaC (Time Series Land Cover Classification) was created for the time series land cover classification challenge held in conjunction with the 2017 European Conference on Machine Learning & Principles and Practice of Knowledge Discovery in Databases (Ienco, 2017). It was generated from time series of 23 Landsat 8 images of Reunion Island (Figure 6a), acquired in 2014. At the 30m spatial resolution of Landsat 8 images, Reunion Island is covered by 2866 2633 pixels, however only 99,687 of these pixels are included in the dataset. Class labels were obtained from the 2012 Corine Land Cover (CLC) map and the 2014 farmers graphical land parcel registration (Registre Parcellaire Graphique - RPG) and the nine most significant classes have been included in the dataset. The number of pixels 11 Dempster et al. (a) Map of Reunion Island and fold data distribution (b) Label counts by fold Figure 6: Map of Reunion Island and label counts by fold for the Tiselac dataset. Note (a); Map from Open Street Map, sample data pixels are not to scale. in each class is capped at 20,000. The data was obtained from the winning entrys GitHub repository (Di Mauro et al., 2017), which includes the spatial coordinates for each pixel. We provide training and testing splits designed to give spatial separation between the splits, which should lead to realistic estimations of the generalisation capability of trained models. We first divided the original pixel grid into coarse grid, with each grid cell sized at 300 300 pixels, then computed the number of dataset pixels in each cell (the cell size). These cells are processed in descending order of size, and allocated to the fold with the fewest pixels. The resulting spatial distribution of the folds is shown in Figure 6a and the distribution of classes across the folds is shown in Figure 6b. 3.3 EEG Figure 7: Class distributions for the EEG datasets. MONSTER 3.3.1 CrowdSourced CrowdsSourced consists of EEG data collected as part of study investigating brain activity during resting state task, which included two conditions: eyes open and eyes closed, each lasting 2 minutes. The dataset contains EEG recordings from 60 participants, but only 13 successfully completed both conditions. The recordings were captured using 14-channel EEG headsetsspecifically the Emotiv EPOC+, EPOC X, and EPOC devices. These devices provide high-quality, wireless brainwave data that is ideal for analyzing resting-state brain activity (Williams et al., 2023). The data was initially recorded at high frequency of 2048 Hz and later downsampled to 128 Hz for processing. To segment the data for analysis, we used 2-second window (equivalent to 256 time steps) with 32 time-step stride to capture the dynamics of brain activity while maintaining manageable data size. The raw EEG data for the 13 participants, along with preprocessing steps, analysis scripts, and visualization tools, are openly available on the Open Science Framework (Williams et al., 2022). This version of the dataset has been split into cross-validation folds based on participant. 3.3.2 DreamerA and DreamerV Dreamer is multimodal dataset that includes electroencephalogram (EEG) and electrocardiogram (ECG) signals recorded during affect elicitation using audio-visual stimuli (Katsigiannis and Ramzan, 2017b), captured with 14-channel Emotiv EPOC headset. It consists of data recording from 23 participants, along with their self-assessments of affective states (valence, arousal, and dominance) after each stimulus (Katsigiannis and Ramzan, 2017b). For our classification task, we focus on the arousal and valence labels, referred to as DreamerA and DreamerV respectively. The dataset is publicly available (Katsigiannis and Ramzan, 2017a), and we utilize the Torcheeg toolkit for preprocessing, including signal cropping and low-pass and high-pass filtering (Zhang et al., 2024). Note that only EEG data is analyzed in this study, with ECG signals excluded. Labels for arousal and valence are binarized, assigning values below 3 to class 1 and values of 3 or higher to class 2, and has been split into cross-validation folds based on participant. 3.3.3 STEW: Simultaneous Task EEG Workload STEW comprises raw EEG recordings from 48 participants involved in multitasking workload experiment (Lim et al., 2018). Additionally, the subjects baseline brain activity at rest was recorded before the test. The data was captured using the Emotiv Epoc device with sampling frequency of 128Hz and 14 channels, resulting in 2.5 minutes of EEG recording for each case. Participants were instructed to assess their perceived mental workload after each stage using rating scale ranging from 1 to 9, and these ratings are available in separate file. The dataset has been divided into cross-validation folds based on individual participants. Additionally, binary class labels have been assigned to the data, categorizing workload ratings above 4 as high and those below or equal to 4 as low. We utilize these labels for our specific problem. STEW can be accessed upon request through the IEEE DataPort (Lim et al., 2020). 13 Dempster et al. 3.4 Human Activity Recognition Datasets Figure 8: Class distributions for the HAR datasets. 3.4.1 Opportunity Opportunity is comprehensive, multi-sensor dataset designed for human activity recognition in naturalistic environment (Chavarriaga et al., 2013). Collected from four participants performing typical daily activities, the dataset spans six recording sessions per person: five unscripted Activities of Daily Living (ADL) runs, and one structured drill run with specific scripted activities. This dataset includes rich, multi-level annotations; however, for our analysis, we focus specifically on the locomotion classes, which consist of five primary categories: Stand, Walk, Sit, Lie, and Null (no specific activity detected). Data collection includes 113 sensor channels from body-worn, object-attached, and ambient sensors. These channels capture essential information on body movements, object interactions, and environmental contexts through inertial measurement units, accelerometers, and switches. The variety and placement of these sensors allow for detailed examination of physical activities and transitions in natural setting. To prepare the data for analysis, we segment it using sliding window approach with 100 time-step window and an overlap of 50 time steps. This segmentation enables the model to capture both the continuity of activities and subtle transitions, enhancing recognition accuracy across the locomotion classes. The dataset has been divided into cross-validation folds based on individual participants. 3.4.2 PAMAP2: Physical Activity Monitoring Dataset PAMAP2 is collection of data obtained from three Inertial Measurement Units (IMUs) placed on the wrist of the dominant arm, chest, and ankle, as well as 1 ECG heart rate (Reiss and Stricker, 2012). The data was recorded at frequency of 100Hz. The dataset includes annotated information about human activities performed by 9 subjects, each with their own unique physical characteristics. The majority of the subjects are male and have dominant right hand. Notably, the dataset includes only one female subject (ID 102) and 14 MONSTER Figure 9: Distribution of activity categories for PAMAP2. one left-handed subject (ID 108). In total, there are 18 different human activity classes represented in the dataset. Figure 9 provides visual representation of the distribution of these activities across all the data. To ensure an unbiased evaluation, we divide the dataset into cross-validation folds based on the subjects. 3.4.3 Skoda: Mini Checkpoint-Activity recognition dataset Skoda captures 10 specific manipulative gestures performed in car maintenance scenario. Its purpose is to investigate different aspects related to the gestures, such as fault resilience, performance scalability with the number of sensors, and power performance management. The dataset comprises 10 classes of manipulative gestures, which were recorded using 2x10 USB sensors positioned on the left and right upper and lower arm. The sensors have high sample rate of approximately 98Hz, ensuring precise capturing of the movements. In terms of activities, the dataset includes 10 distinct manipulative gestures commonly performed during car maintenance (Figure 10). The data was collected from single subject, with each gesture being recorded 70 times. In total, the dataset offers around 3 hours of recording time, enabling thorough analysis of the gestures in various scenarios. 3.4.4 UCIActivity UCIActivity is widely recognized benchmark for activity recognition research. It contains sensor readings from 30 participants performing six daily activities: walking, walking upstairs, walking downstairs, sitting, standing, and lying down. The data was collected using Samsung Galaxy S2 smartphone mounted on the waist of each participant, with 15 Dempster et al. Figure 10: Distribution of activity categories for Skoda. sampling rate of 50 Hz Anguita et al. (2013). To keep the evaluation fair, we perform subject-wise cross-validation. 3.4.5 USCActivity: USC human activity dataset USCActivity (Zhang and Sawchuk, 2012) consists of data collected from Motion-Node device, which includes six readings from body-worn 3-axis accelerometer and gyroscope sensor. The dataset contains samples from 14 male and female subjects with equal distribution (7 each) and specific physical characteristics and ages. The sensor data is sampled at rate of 100 Hz, and each time-step in the dataset is labeled with one of 12 activity classes (Figure 11). The USCActivity dataset presents challenge in learning feature representation and segmentation due to the placement of the sensors and the variability in activity classes. The data is collected from single accelerometer and gyroscope reading obtained from motion node attached to the subjects right hip. Therefore, this reading does not contribute significantly to the feature space transformation. Additionally, the activity classes involve various orientations, such as walking forward, left, or right, and even using the elevator up or down, which cannot be captured solely through accelerometer and gyroscope readings. Similar to other activity recognition datasets, we use subject-based cross-validation. 3.4.6 WISDM and WISDM2: Wireless Sensor Data Mining WISDM describes six daily activities collected in controlled laboratory environment. The activities include Walking, Jogging, Stairs, Sitting, Standing, and Lying Down, recorded 16 MONSTER Figure 11: Distribution of activity categories for USC-HAD. from 36 users using cell phone placed in their pocket. The data is sampled at rate of 20 Hz, resulting in total of 1,098,207 samples across 3 dimensions (Lockhart et al., 2012). WISDM2 extends the original WISDM dataset by collecting data in real-world environments using the Actitracker system. This system was designed for public use and provides more extensive collection of sensor readings from users performing the same six activities. The dataset contains 2,980,765 samples with three dimensions, and the data was recorded from larger and more diverse set of participants in naturalistic settings, offering valuable resource for real-world activity recognition (Weiss and Lockhart, 2012). Both WISDM and WISDM2 are split based on subjects. 3.5 Counts Figure 12: Class distributions for the count datasets. 17 Dempster et al. 3.5.1 Pedestrian Pedestrian represents hourly pedestrian counts at 82 locations in Melbourne, Australia between 2009 and 2022 (City of Melbourne, 2022). The processed dataset consists of 189,621 (univariate) time series, each of length 24. The task is to identify location based on the time series of counts. The dataset has been split into stratified random cross-validation folds. 3.5.2 Traffic Traffic consists of hourly traffic counts at various locations in the state of NSW, Australia (Transport for NSW, 2023). The processed dataset contains 1,460,968 (univariate) time series, each of length 24. The task is to predict the day of the week based on the time series of counts. The dataset has been split into stratified random cross-validation folds. 3.6 Other Figure 13: Class distributions for the uncategorised datasets. 3.6.1 FordChallenge FordChallenge is obtained from Kaggle and consists of data from 600 real-time driving sessions, each lasting approximately 2 minutes and sampled at 100ms intervals (Abou-Nasr, 2011). These sessions include trials from 100 drivers of varying ages and genders. The dataset contains 8 physiological, 11 environmental, and 11 vehicular measurements, with specific details such as names and units undisclosed by the challenge organizers. Each data point is labeled with binary outcome: 0 for distracted and 1 for alert. The objective of the challenge is to design classifier capable of accurately predicting driver alertness using the provided physiological, environmental, and vehicular data. 3.6.2 LenDB LenDB consists of seismograms recorded from multiple different seismic detection networks from across the globe (Magrini et al., 2020a,b). The processed dataset consists of 1,244,942 multivariate time series, with 3 channels, each of length 540, with two classes: earthquake and noise. This version of the dataset has been split into cross-validation folds based on seismic detection network (i.e., such that seismograms for given network do not appear in both training and validation fold). 18 MONSTER"
        },
        {
            "title": "4 Baseline Results",
            "content": "4.1 Models We provide baseline results on the Monster datasets for number of key models. In particular, we provide results for four deep learning models: ConvTran (Foumani et al., 2024b), FCN (Wang et al., 2017), HInceptionTime (Ismail-fawaz et al., 2022), and TempCNN (Pelletier et al., 2019). We include results for two more traditional, specialised methods for time series classification: Hydra (Dempster et al., 2023), and Quant (Dempster et al., 2024a). We also include results for standard, off the shelf classifierextremely randomised trees (Geurts et al., 2006)to act as naıve baseline. FCN is fully convolutional neural network. It consists of three temporal convolutional layers (one-dimensional convolutional layers that convolve along the time series), followed by global average pooling layer and finally the softmax classification layer (Wang et al., 2017). The convolutional layers have 128, 256, and 128 filters of length 8, 5, and 3, respectively. TempCNN is light-weight temporal convolutional neural network originally designed for land cover classification from time series of satellite imagery (Pelletier et al., 2019). It consists of three temporal convolutional layers followed by fully connected layer. Each convolutional layer has 64 filters of length 5 and the fully-connected layer has 256 units. H-InceptionTime (Hybrid-InceptionTime) is an ensemble of five Hybrid-Inception (HInception) models, each with different random weight initialisation (Ismail-fawaz et al., 2022). An H-Inception model consists of set of 17 hand-crafted filters combined with six Inception modules. The hand-crafted filters are sets of convolutional filters designed to detect peaks, and both increasing and decreasing trends. The hand-crafted filters range in length from 2 to 96 and are applied in parallel with the first inception module to the input time series. Inception modules combine convolutions with filter lengths of 10, 20 and 40, max pooling and bottleneck layers. Each set of convolutions and the max pooling layer have 32 filters thus each inception module has 128 filters. The resulting network has small number of parameters and large receptive field (Ismail Fawaz et al., 2020). ConvTran is deep learning model for multivariate time series classification (TSC) that combines convolutional layers with transformers to effectively capture both local patterns and long-range dependencies (Foumani et al., 2024b). It addresses the limitations of existing position encoding methods by introducing two novel techniques: tAPE (temporal Absolute Position Encoding) for absolute positions and eRPE (efficient Relative Position Encoding) for relative positions. These encodings, integrated with disjoint temporal and channel-wise convolutions (Foumani et al., 2021), allow ConvTran to capture both temporal dependencies and correlations between the channels. Hydra involves transforming input time series using set of random convolutional kernels arranged into groups, and counting the kernel representing the closest match with the input time series in each group. The counts are then used to train ridge regression classifier (Dempster et al., 2023). Here, we use the variant of Hydra presented in Dempster et al. (2024b), which integrates the Hydra transform into the process of fitting the ridge regression model, and all computation is performed on GPU. Quant involves recursively dividing the input time series in half, and computing the quantiles for each of the resulting intervals (subseries) (Dempster et al., 2024a). The computed quantiles are used to train an extremely randomised trees classifier. Quant acts on 19 Dempster et al. the original input time series, the first and second derivatives, and the Fourier transform. Here, we use the variant of Quant presented in Dempster et al. (2024b), which uses pasting to spread the extremely randomised trees over the dataset. Extremely Randomised Trees (ET) is well-established classifier, using an ensemble of decision trees where random subset of features and split points is considered at each node, with the feature/split chosen which minimises log loss (Geurts et al., 2006). Here, we use the same setup as for Quant, but remove the Quant transform, so that ET is training directly on the raw time series data (rather than the Quant features). ET serves as naıve baseline reference point for the other models. The four deep learning models are trained using the Adam optimiser (Kingma and Ba, 2015) and batch size of 256 for maximum of 100 epochs. The one exception is HInceptionTime with the AudioMNIST dataset, which used batch size of 64 to enable it to fit in the GPU memory. For all datasets, we implement early stopping and select the best epoch found as the final model, using validation set obtained by randomly selecting 10% of the training dataset. Training time on each fold is limited to approximately 24 hours or one epoch, whichever is longer. We provide results for 01 loss, log loss, and training time. (We also provide learning curves for Hydra and Quant: see Appendix A.4). Each method is evaluated on each dataset using 5-fold cross-validation, using predefined cross-validation folds. (Note that both Quant and ET are unable to train on one of the folds of the WISDM dataset, due limitation of the ET implementation where there is single example of given class.) These results serve as an initial survey on the relative performance of different methods on the Monster datasets, to serve as reference point for future work on large time series classification tasks. 4.2 Summary The multi-comparison matrix (MCM) in Figure 14 shows mean 01 loss as well as pairwise differences and win/draw/loss for the baseline methods over all 29 Monster datasets (see Ismail-Fawaz et al., 2023a). Figure 14 shows that Quant achieves the lowest overall mean 01 loss, slightly lower than that of ConvTran, although both ConvTran and HInceptionTime have lower 01 loss on more datasets (16 vs 13 and 17 vs 12 respectively). Hydra has higher overall mean 01 loss than HInceptionTime, but lower than TempCNN, ET, or FCN. TempCNN, ET, and FCN all have higher average 01 loss, due in large part to poor performance on the audio datasets: see Section 4.3. 4.3 By Category Figure 15 shows the 01 loss for each method on each dataset, organised by category (Audio, Count, ECG/EEG, HAR, Satellite, and Other). Each point represents single dataset. The horizontal bars represent mean 01 loss for each classifier within each category. Figure 15 shows that while for some categories the 01 loss for different methods is broadly similar, for other categories there are considerable differences. In particular, ConvTran, HInceptionTime, Hydra and Quant all achieve relatively low 01 loss on the audio datasets, while ET, TempCNN, and especially FCN have much higher 20 MONSTER Figure 14: Multi-comparison matrix showing mean 01 loss and pairwise differences. 01 loss. ET, Quant, and (to lesser extent) ConvTran and HInception achieve relatively low 01 loss on the count datasets. Quant and (to lesser extent) ET and Hydra achieve relatively low 01 loss on the other datasets. In contrast, mean 01 loss for ECG/EEG, HAR, and Satellite is broadly similar, with significant spread within the results for each method. Interestingly, it is only on the audio datasets, and to some extent the HAR datasets, that our naıve baseline, ET, appears to be meaningfully worse than the deep learning or specialised time series classification methods. ET achieves similar results to Quant on number of datasets, which is not surprising, as the raw time series are similar to subset of the features used in Quant. We speculate that the poor 01 loss for FCN and TempCNN on the audio datasets in particular may be related to the small receptive field of these models (relative to the relatively long time series in the audio datasets). With small receptive field, these models are in effect limited to high-frequency features in the data. The satellite datasets show some interesting extremes. All methods except for Quant and ET performed poorly on the S2Agri 10% datasets. In contrast, all methods achieved very low 01 loss on LakeIce as this dataset has strong temporal and spatial correlations between samples that could not be accounted for when splitting the data into folds. Dempster et al. Figure 15: 01 loss by category. 22 MONSTER Table 2: Total Training Time GPU CPU Hydra ConvTran TempCNN FCN HInception ET Quant 47m 44s 5d 6h 2d 9h 2d 12h 6d 6h 5h 10m 20h 10m Table 3: Number of Parameters ConvTran FCN HInception TempCNN Hydra Quant 27,039 Traffic 264,962 CornellWhale 869,570 CornellWhale 424,649 Tiselac 6,144 FordChallenge 275 CrowdSourced 486,941 Opportunity 380,037 Opportunity 167,936 Pedestrian num. parameters in ridge classifier; median num. leaves per tree 786,444,426 AudioMNIST 1,420,145 Opportunity 379,112 Traffic min max 4.4 Computational Efficiency 4.4.1 Training Time Table 2 shows total training time for each of the baseline methods, separated into methods using GPU and methods using CPU. This represents the total training time over all 29 Monster datasets (where the time for each dataset is the average training time across the five cross-validation folds). The five methods trained using GPUs were each trained using single GPU, either an Ampere A100 SMX4 with 80GB RAM, or an Ampere A40 with 48GB RAM. Table 2 shows that among these methods, Hydra is by far the fastest, taking less than 50 minutes to train over all 29 Monster datasets, almost 70 faster than the next-fastest GPU method (TempCNN). HInceptionTime is the least efficient method, requiring approximately week of training time, corresponding to approximately one month total training time across all five cross-validation folds. (We note that there is variant of H-InceptionTime, LITETime, with significantly fewer parameters which requires less than half of the training time of H-InceptionTime: Ismail-Fawaz et al. (2023b).) Although not directly comparable to methods using GPU, Quant requires approximately 20 hours of training time (using 4 CPU cores). ET requires only approximately quarter of this (5 hours), due to the smaller number of features used to train the classifier. 4.4.2 Parameter Counts Table 3 shows total number of parameters for each of the baseline methods. For each method the table shows the minimum and maximum number of parameters and the corresponding dataset. The number of parameters for both FCN and HInceptionTime is reasonably stable, with the largest model 1.4 and 1.6 times that of the smallest model, respectively. However, the number of parameters in the TempCNN models vary greatly, with the largest model being over 1,800 times the size of the smallest one. While the total number of parameters for all the deep learning methods is dependent on the number of classes and channels, the FCN and HInceptionTime architectures both include global average pooling layer, so the parameter count is independent of the length of the time series. However, TempCNN does not use global pooling and so its parameter count is highly dependent on the length of the 23 Dempster et al. Figure 16: Pairwise 01 loss for ConvTran. time series. For Hydra, we have used the number of parameters for the ridge classifier, and for Quant we have used the median number of leaf nodes, although these are not directly comparable to the number of trainable parameters in the deep learning models. 4.5 Pairwise Comparisons Figures 16, 17, and 18 show the pairwise 01 loss, log loss, and training time for ConvTran versus each of the other baseline methods. (Full pairwise results for all methods and metrics are provided in the Appendix.) Figure 16 shows that ConvTran achieves broadly similar 01 loss on most datasets compared to Quant, HInceptionTime, and Hydra, although ConvTran achieves noticeably lower 01 loss than Hydra on one dataset (and both Hydra and Quant achieve significantly lower 01 loss than ConvTran on one dataset). While ConvTran achieves similar 01 loss to FCN and TempCNN on most datasets, ConvTran achieves considerably lower 01 loss on small number of datasets. As noted above, these include the audio datasets, where FCN and TempCNN appear to struggle relative to the other methods. Figure 17 shows slightly different picture in terms of log loss. ConvTran is fairly evenly matched to Quant (and ET) in terms of the number of datasets on which each method achieves lower log loss, although there is considerable spread in values (i.e., they are not closely correlated). ConvTran achieves lower log loss on more datasets compared to Hydra, although Hydra does achieve lower log loss on 10 datasets, which is somewhat surprising, 24 MONSTER Figure 17: Pairwise log-loss for ConvTran. Dempster et al. Figure 18: Pairwise training time for ConvTran. as Hydra takes no account of log loss in training. ConvTran achieves lower log loss than FCN, HInceptionTime, and TempCNN on most datasets. Figure 18 shows that ConvTran is significantly faster than HInceptionTime, but slower than FCN or TempCNN, on most datasets. ConvTran is marginally faster than Quant on number of datasets, although the timings are not directly comparable (given that ConvTran uses GPU whereas Quant is limited to CPU). Both ET and Hydra are faster than ConvTran on all datasets (reflecting the overall differences in training time shown in Table 2)."
        },
        {
            "title": "5 Conclusion",
            "content": "We present Monster, new benchmark collection of large datasets for time series classification. The field of time series classification has become focused on smaller datasets. This has resulted in state-of-the-art methods being optimised for low average 01 loss over large number of small datasets, has insulated the field from engaging with the challenges of learning from large quantities of data, and has artificially disadvantaged low-bias methods such as deep neural network models in benchmarking comparisons. We hope that Monster encourages the field to engage with the challenges related to learning from large quantities of time series data. We hope that Monster will help better reflect the broader task of time series classification and improve relevance for real-world 26 MONSTER time series classification problems. We believe there is enormous potential for new research based on much larger datasets."
        },
        {
            "title": "Broader Impact Statement",
            "content": "We present new benchmark of 29 large datasets for time series classification. This could potentially have large impact on the field, as these datasets are significantly larger than those currently used for benchmarking and evaluation. This should allow for training lowerbias, more complex models, with greater relevance and more direct applicability to largescale, real-world time series classification problems. On the other hand, learning from larger quantities of data requires proportionally more computational time and resources. As such, it is important to always keep in mind the balance between computational expense and real-world relevance. There are also potential risks associated with the misuse of improved methods for time series classification in monitoring and surveillance, although we do not feel that there is any significant direct risk associated with this work."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported by an Australian Government Research Training Program Scholarship, and the Australian Research Council under award DP240100048. The authors would like to thank, in particular, Professor Eamonn Keogh, Professor Tony Bagnall, and all the people who have contributed to the UCR and UEA time series classification archives. The authors also thank Raphael Fischer for trialling our methods and datasets and providing invaluable feedback."
        },
        {
            "title": "References",
            "content": "Mahmoud Abou-Nasr. Stay Alert! The Ford Challenge. https://kaggle.com/ competitions/stayalert, 2011. Kaggle. Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge Luis Reyes-Ortiz, et al. public domain dataset for human activity recognition using smartphones. In 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN), volume 3, page 3, 2013. Anthony Bagnall, Jason Lines, Aaron Bostrom, James Large, and Eamonn Keogh. The great time series classification bake off: review and experimental evaluation of recent algorithmic advances. Data Mining and Knowledge Discovery, 31(3):606660, 2017. Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The UEA multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018. Soren Becker, Johanna Vielhaben, Marcel Ackermann, Klaus-Robert Muller, Sebastian Lapuschkin, and Wojciech Samek. AudioMNIST: Exploring explainable artificial intelli27 Dempster et al. gence for audio analysis on simple benchmark. Journal of the Franklin Institute, 361 (1):418428, 2024a. Soren Becker, Ackermann, Muller, Samek. https://github.com/soerenab/AudioMNIST, 2024b. MIT License. and Wojciech Lapuschkin, Vielhaben, Sebastian Johanna Marcel Klaus-Robert AudioMNIST. Damien Brain and Geoffrey Webb. On the effect of data set size on bias and variance in classification learning. In Proceedings of the Fourth Australian Knowledge Acquisition Workshop, University of New South Wales, pages 117128, 1999. Ricardo Chavarriaga, Hesam Sagha, Alberto Calatroni, Sundara Tejaswi Digumarti, Gerhard Troster, Jose del Millan, and Daniel Roggen. The opportunity challenge: benchmark database for on-body sensor-based activity recognition. Pattern Recognition Letters, 34(15):20332042, 2013. Yanping Chen. Flying insect classification with inexpensive sensors. https://sites. google.com/site/insectclassification/ (via Internet Archive), 2014. Public Domain. Yanping Chen, Adena Why, Gustavo Batista, Agenor Mafra-Neto, and Eamonn Keogh. Flying insect classification with inexpensive sensors. Journal of Insect Behavior, 27(5): 657677, 2014. City of Melbourne. Pedestrian counting system. https://data.melbourne.vic. gov.au/explore/dataset/pedestrian-counting-system-monthly-counts-perhour/information/, 2022. CC BY 4.0. Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh. The ucr time series archive. IEEE/CAA Journal of Automatica Sinica, 6(6):12931305, 2019. Angus Dempster, Francois Petitjean, and Geoffrey Webb. Rocket: Exceptionally fast and accurate time series classification using random convolutional kernels. Data Mining and Knowledge Discovery, 34:14541495, 2020. Angus Dempster, Daniel Schmidt, and Geoffrey Webb. Minirocket: very fast (almost) deterministic transform for time series classification. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 248257, New York, 2021. Association for Computing Machinery. Angus Dempster, Daniel Schmidt, and Geoffrey Webb. Hydra: Competing convolutional kernels for fast and accurate time series classifcation. Data Mining and Knowledge Discovery, 37(5):17791805, 2023. Angus Dempster, Daniel Schmidt, and Geoffrey Webb. Quant: minimalist interval method for time series classification. Data Mining and Knowledge Discovery, 38:2377 2402, 2024a. MONSTER Angus Dempster, Chang Wei Tan, Lynn Miller, Navid Mohammadi Foumani, Daniel Schmidt, and Geoffrey Webb. Highly scalable time series classification for very large datasets. In 9th Workshop on Advanced Analytics and Learning on Temporal Data, 2024b. Nicola Di Mauro, Antonio Vergari, Teresa M.A. Basile, Fabrizio G. Ventola, and Floriana Esposito. End-to-end learning of deep spatio-temporal representations for satellite image time series classification. In Proceedings of the European Conference on Machine Learning & Principles and Practice of Knowledge Discovery in Databases (PKDD/ECML), 2017. URL http://ceur-ws.org/Vol-1972/paper4.pdf. Eleftherios Fanioudakis, Matthias Geismar, and Ilyas Potamitis. Mosquito wingbeat analysis and classification using deep learning. In 26th European Signal Processing Conference, pages 24102414, 2018. Michael Flynn. Classifying Dangerous Species Of Mosquito Using Machine Learning. PhD thesis, University of East Anglia, 2022. Navid Mohammadi Foumani, Lynn Miller, Chang Wei Tan, Geoffrey Webb, Germain Forestier, and Mahsa Salehi. Deep learning for time series classification and extrinsic regression: current survey. ACM Computing Surveys, 56(9):145, 2024a. Navid Mohammadi Foumani, Chang Wei Tan, Geoffrey Webb, and Mahsa Salehi. Improving position encoding of transformers for multivariate time series classification. Data Mining and Knowledge Discovery, 38(1):2248, 2024b. Seyed Navid Mohammadi Foumani, Chang Wei Tan, and Mahsa Salehi. Disjoint-CNN for multivariate time series classification. In 2021 International Conference on Data Mining Workshops (ICDMW), pages 760769. IEEE, 2021. Vivien Sainte Fare Garnot, Loic Landrieu, Sebastien Giordano, and Nesrine Chehata. Satellite image time series classification with pixel-set encoders and temporal self-attention. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. Pierre Geurts, Damien Ernst, and Louis Wehenke. Extremely randomized trees. Machine Learning, 63(1):342, 2006. Antoine Guillaume, Christel Vrain, and Wael Elloumi. Random dilated shapelet transform: new approach for time series shapelets. In Pattern Recognition and Artificial Intelligence, pages 653664, Berlin, 2022. Springer. Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):5865, 2021. Dino Ienco. TiSeLaC : Time Series Land Cover Classification Challenge. https://sites.google.com/site/dinoienco/tiselac-time-series-land-coverclassification-challenge (via Internet Archive), 2017. Ali Ismail-fawaz, Maxime Devanne, Jonathan Weber, and Germain Forestier. Deep Learning For Time Series Classification Using New Hand-Crafted Convolution Filters. In IEEE Internation Conference on Big Data., 2022. 29 Dempster et al. Ali Ismail-Fawaz, Angus Dempster, Chang Wei Tan, Matthieu Herrmann, Lynn Miller, Daniel Schmidt, Stefano Berretti, Jonathan Weber, Maxime Devanne, Germain Forestier, and Geoffrey Webb. An approach to multiple comparison benchmark evaluations that is stable under manipulation of the comparate set, 2023a. arXiv:2305.11921. Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, and Germain Forestier. LITE: Light Inception with boosTing tEchniques for Time Series Classification. In 2023 IEEE 10th International Conference on Data Science and Advanced Analytics, pages 110, 2023b. Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier, Daniel F. Schmidt, Jonathan Weber, Geoffrey I. Webb, Lhassane Idoumghar, Pierre-Alain Muller, and Francois Petitjean. Finding AlexNet for Data Mining and Knowledge Discovery, 34(6): 10.1007/s10618-020-00710-y. 19361962, nov 2020. URL http://arxiv.org/abs/1909.04939http://dx.doi.org/10.1007/s10618-02000710-yhttp://link.springer.com/10.1007/s10618-020-00710-y. time series classification. ISSN 1384-5810. InceptionTime: doi: Andre Karpiˇstˇsenko, Eric Spalding, and Will Cukierski. The Marinexplore and Cornell University whale detection challenge. https://kaggle.com/competitions/whaledetection-challenge, 2013. Copyright 2011 Cornell University and the Cornell Research Foundation. Stamos Katsigiannis and Naeem Ramzan. Dreamer: database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices. https://zenodo. org/records/546113, 2017a. Stamos Katsigiannis and Naeem Ramzan. Dreamer: database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices. IEEE Journal of Biomedical and Health Informatics, 22(1):98107, 2017b. Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization. 3rd International Conference on Learning Representations (ICLR), pages 115, 2015. URL http://arxiv.org/abs/1412.6980. Thomas Liao, Rohan Taori, Deborah Raji, and Ludwig Schmidt. Are we learning yet? meta review of evaluation failures across machine learning. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. Wei Lun Lim, Olga Sourina, and Lipo Wang. Stew: Simultaneous task eeg workload data set. https://ieee-dataport.org/open-access/stew-simultaneous-task-eegworkload-dataset, 2020. CC BY 4.0. WL Lim, Sourina, and Lipo Wang. STEW: Simultaneous task EEG workload data set. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 26(11):21062114, 2018. MONSTER Jeffrey Lockhart, Tony Pulickal, and Gary Weiss. Applications of mobile activity recognition. In Conference on Ubiquitous Computing, pages 10541058, 2012. Benjamin Lucas, Ahmed Shifaz, Charlotte Pelletier, Lachlan Oneill, Nayyar Zaidi, Bart Goethals, Francois Petitjean, and Geoffrey I. Webb. Proximity forest: an effective and scalable distance-based classifier for time series. Data Mining and Knowledge Discovery, 33(3):607635, 2019. Fabrizio Magrini, Dario Jozinovic, Fabio Cammarano, Alberto Michelini, and Lapo Boschi. LEN-DB local earthquakes detection: benchmark dataset of 3-component seismograms built on global scale. https://zenodo.org/doi/10.5281/zenodo.3648231, 2020a. CC BY 4.0. Fabrizio Magrini, Dario Jozinovic, Fabio Cammarano, Alberto Michelini, and Lapo Boschi. Local earthquakes detection: benchmark dataset of 3-component seismograms built on global scale. Artificial Intelligence in Geosciences, 1:110, 2020b. Matthew Middlehurst, James Large, Michael Flynn, Jason Lines, Aaron Bostrom, and Anthony Bagnall. HIVE-COTE 2.0: new meta ensemble for time series classification. Machine Learning, 110:32113243, 2021. Matthew Middlehurst, Patrick Schafer, and Anthony Bagnall. Bake off redux: review and experimental evaluation of recent time series classification algorithms. Data Mining and Knowledge Discovery, 2024. Brian Miller, Kathleen Stafford, Ilse Van Opzeeland, et al. Whale sounds. https: //data.aad.gov.au/metadata/AcousticTrends_BlueFinLibrary, 2020. CC BY 4.0. Brian Miller, Kathleen Stafford, Ilse Van Opzeeland, Danielle Harris, Flore Samaran, Ana ˇSirovic, Susannah Buchan, Ken Findlay, Naysa Balcazar, Sharon Nieukirk, Emmanuelle Leroy, Meghan Aulich, Fannie Shabangu, Robert Dziak, Won Sang Lee, and Jong Kuk Hong. An open access dataset for developing automated detectors of Antarctic baleen whale sounds and performance evaluation of two commonly used detectors. Scientific Reports, 11, 2021. Amandalynne Paullada, Inioluwa Deborah Raji, Emily Bender, Emily Denton, and Alex Hanna. Data and its (dis)contents: survey of dataset development and use in machine learning research. Patterns, 2(11), 2021. Charlotte Pelletier, Geoffrey Webb, and Francois Petitjean. Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series. Remote Sensing, 11(5):523, mar 2019. doi: 10.3390/rs11050523. URL https://www.mdpi.com/2072-4292/11/5/ 523. Ilyas Potamitis. https://timeseriesclassification.com/ description.php?Dataset=FruitFlies, 2016. With Permission of Prof Tony Bagnall. FruitFlies dataset. Ilyas Potamitis. Wingbeats. https://timeseriesclassification.com/description.php?Dataset=MosquitoSound, Public Domain. https://www.kaggle.com/datasets/potamitis/wingbeats; 2018. 31 Dempster et al. Attila Reiss and Didier Stricker. Introducing new benchmarked dataset for activity monitoring. In 16th International Symposium on Wearable Computers, pages 108109. IEEE, 2012. Alejandro Pasos Ruiz, Michael Flynn, James Large, Matthew Middlehurst, and Anthony Bagnall. The great multivariate time series classification bake off: review and experimental evaluation of recent algorithmic advances. Data Mining and Knowledge Discovery, 35(2):401449, 2021. Vivien Sainte Fare Garnot and Loic Landrieu. S2Agri pixel set. https://zenodo.org/ records/5815488, 2022. CC BY 4.0. Patrick Schafer and Ulf Leser. WEASEL 2.0: random dilated dictionary transform for fast, accurate and memory constrained time series classification. Machine Learning, 112: 47634788, 2023. Maria Shaposhnikova, Claude Duguay, and Pascale Roy-Leveillee. Annotated timeseries of lake ice C-band synthetic aperture radar backscatter created using Sentinel1, ERS-1/2, and RADARSAT-1 imagery of Old Crow Flats, Yukon, Canada. https://doi.org/10.1594/PANGAEA.947789, 2022. CC BY 4.0. Maria Shaposhnikova, Claude Duguay, and Pascale Roy-Leveillee. Bedfast and floatingece dynamics of rhermokarst lakes using temporal deep-learning mapping approach: Case study of the Old Crow Flats, Yukon, Canada. The Cryosphere, 17(4):16971721, 2023. Rich Sutton. The bitter lesson, 2019. http://www.incompleteideas.net/IncIdeas/ BitterLesson.html. Transport for NSW. NSW road traffic volume counts hourly. https://opendata. dev.transport.nsw.gov.au/dataset/nsw-roads-traffic-volume-countsapi/resource/bca06c7e-30be-4a90-bc8b-c67428c0823a, 2023. CC BY 4.0. Zhiguang Wang, Weizhong Yan, and Tim Oates. Time series classification from scratch with deep neural networks: strong baseline. In 2017 International Joint Conference on Neural Networks (IJCNN), volume 2017-May, pages 15781585. IEEE, 2017. ISBN 978-1-5090-6182-2. doi: 10.1109/IJCNN.2017.7966039. URL http://ieeexplore.ieee. org/document/7966039/. Giulio Weikmann, Claudia Paris, and Lorenzo Bruzzone. TimeSen2Crop: million labeled samples dataset of Sentinel 2 image time series for crop-type classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 14:46994708, 2021. doi: 10.1109/JSTARS.2021.3073965. Gary Mitchell Weiss and Jeffrey Lockhart. The impact of personalization on smartphonebased activity recognition. In Workshops at the 26 AAAI Conference on Artificial Intelligence, 2012. MONSTER Nikolas Williams, William King, Geoffrey Mackellar, Roshini Randeniya, Alicia McCormick, and Nicholas Badcock. Crowdsourced eeg experiments: proof of concept for remote eeg acquisition using emotivpro builder and emotivlabs. Heliyon, 9(8), 2023. Nikolas Scott Williams, William King, Roshini Randeniya, and Nicholas Badcock. Crowdsourced. https://osf.io/9bvgh/, 2022. David Wolpert and William MacReady. No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation, 1(1):6782, 1997. Mi Zhang and Alexander Sawchuk. USC-HAD: daily activity dataset for ubiquitous In Conference on Ubiquitous Computing, activity recognition using wearable sensors. pages 10361043, 2012. Zhi Zhang, Sheng-Hua Zhong, and Yan Liu. TorchEEGEMO: deep learning toolbox towards EEG-based emotion recognition. Expert Systems with Applications, 2024. 33 Dempster et al. Appendix A. Additional Results A.1 01 loss Figure 19: Pairwise results (01 loss) for ET. Figure 20: Pairwise results (01 loss) for FCN. 34 MONSTER Figure 21: Pairwise results (01 loss) for HInceptionTime. Figure 22: Pairwise results (01 loss) for Hydra. 35 Dempster et al. Figure 23: Pairwise results (01 loss) for Quant. Figure 24: Pairwise results (01 loss) for TempCNN. 36 MONSTER A.2 Log Loss Figure 25: Pairwise results (log-loss) for ET. Figure 26: Pairwise results (log-loss) for FCN. Dempster et al. Figure 27: Pairwise results (log-loss) for HInceptionTime. Figure 28: Pairwise results (log-loss) for Hydra. 38 MONSTER Figure 29: Pairwise results (log-loss) for Quant. Figure 30: Pairwise results (log-loss) for TempCNN. 39 Dempster et al. A.3 Training Time Figure 31: Pairwise results (training time) for ET. Figure 32: Pairwise results (training time) for FCN. 40 MONSTER Figure 33: Pairwise results (training time) for HInceptionTime. Figure 34: Pairwise results (training time) for Hydra. 41 Dempster et al. Figure 35: Pairwise results (training time) for Quant. Figure 36: Pairwise results (training time) for TempCNN. 42 MONSTER A.4 Learning Curves (Hydra and Quant) Figure 37: Learning curves (1 of 2). 43 Dempster et al. Figure 38: Learning curves (2 of 2). 44 MONSTER Appendix B. BiasVariance Learning Curves Detail The low variance model comprises single layer of 100 fixed, random convolutional kernels. The low bias model comprises two layers of 100 learned convolutional kernels. Both models use ReLU activations, with global average pooling followed by batch normalisation and learned linear layer. The input time series are normalised by subtracting the mean and dividing by the standard deviation (per series). Both models were trained for 500 epochs for each dataset size using the Adam optimizer, with no explicit regularisation, using an initial learning rate of 102 reduced by factor of 10 after each 100 epochs."
        }
    ],
    "affiliations": [
        "Monash University, Melbourne, Australia",
        "Universite Bretagne Sud, IRISA, Vannes, France"
    ]
}