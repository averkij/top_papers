{
    "paper_title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing",
    "authors": [
        "Weiyan Xie",
        "Han Gao",
        "Didan Deng",
        "Kaican Li",
        "April Hua Liu",
        "Yongxiang Huang",
        "Nevin L. Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in text-to-image (T2I) models have enabled training-free regional image editing by leveraging the generative priors of foundation models. However, existing methods struggle to balance text adherence in edited regions, context fidelity in unedited areas, and seamless integration of edits. We introduce CannyEdit, a novel training-free framework that addresses these challenges through two key innovations: (1) Selective Canny Control, which masks the structural guidance of Canny ControlNet in user-specified editable regions while strictly preserving details of the source images in unedited areas via inversion-phase ControlNet information retention. This enables precise, text-driven edits without compromising contextual integrity. (2) Dual-Prompt Guidance, which combines local prompts for object-specific edits with a global target prompt to maintain coherent scene interactions. On real-world image editing tasks (addition, replacement, removal), CannyEdit outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent improvement in the balance of text adherence and context fidelity. In terms of editing seamlessness, user studies reveal only 49.2 percent of general users and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited when paired with real images without edits, versus 76.08 to 89.09 percent for competitor methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 7 3 9 6 0 . 8 0 5 2 : r CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-free Image Editing Weiyan Xie2 . Han Gao1,2 Didan Deng1 Kaican Li2 April Hua Liu3 Yongxiang Huang1 Nevin L. Zhang2 1 Huawei Hong Kong AI Framework & Data Technologies Lab 2 The Hong Kong University of Science and Technology 3 Shanghai University of Finance and Economics Project Page: vaynexie.github.io/CannyEdit"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in text-to-image (T2I) models have enabled training-free regional image editing by leveraging the generative priors of foundation models. However, existing methods struggle to balance text adherence in edited regions, context fidelity in unedited areas, and seamless integration of edits. We introduce CannyEdit, novel training-free framework that addresses these challenges through two key innovations: (1) Selective Canny Control, which masks the structural guidance of Canny ControlNet in user-specified editable regions while strictly preserving the source images details in unedited areas via inversion-phase ControlNet information retention. This enables precise, text-driven edits without compromising contextual integrity. (2) Dual-Prompt Guidance, which combines local prompts for object-specific edits with global target prompt to maintain coherent scene interactions. On real-world image editing tasks (addition, replacement, removal), CannyEdit outperforms prior methods like KV-Edit, achieving 2.93%10.49% improvement in the balance of text adherence and context fidelity. In terms of editing seamlessness, user studies reveal only 49.2% of general users and 42.0% of AIGC experts identified CannyEdits results as AI-edited when paired with real images without edits, versus 76.0889.09% for competitor methods."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in text-to-image (T2I) models have achieved substantial progress in quality and controllability [1, 2, 10, 13, 32]. These advancements have enabled diverse downstream applications, utilizing their enhanced quality, efficiency, and versatility. In this work, we address one of the most challenging applications: region-based image editing, which entails modifying user-specified image areas (e.g., adding, replacing, or removing objects) while maintaining overall image consistency. Region-based image editing extends standard T2I generation by introducing critical constraint: the generated content must align not only with the text prompt (Text Adherence) but also with the existing visual context of the image (Context Fidelity). This problem is commonly known as the editability-fidelity trade-off. straightforward approach to region-based image editing involves collecting paired training data (before and after editing), along with corresponding text prompts, to train models for editing [4, 17, Equal contribution. Work was done when Weiyan Xie was an intern at Huawei Hong Kong AI Framework & Data Technologies Lab. Contact: wxieai@cse.ust.hk (a) Add woman with her dog and student reading book on the lawn. (b) Add person running on the street. (c) Replace the woman tennis player with man tennis player. (d) Add baseball catcher on the field. (e) Remove the umbrellas. (f) Replace the pepper with three apples. Input CannyEdit KV-Edit GPT-4o Input CannyEdit KV-Edit GPT-4o Figure 1: Examples of edits using our CannyEdit, KV-Edit [55], and GPT-4o [28]. Actual text prompts and masks are omitted here. For more details, see Appendix F.1. 24, 44, 45, 51]. However, these methods often struggle to generalize beyond their training distribution particularly in cases requiring realistic interactions, such as inserting people into complex scenes. This limitation is largely due to the lack of diverse, high-quality training data capturing such interactions. To overcome this limitation, another line of research taps into the emergent capability of foundation T2I models to capture realistic object interactions learned from large-scale datasets.These trainingfree approaches, initially developed upon UNet-based diffusion models [7, 15, 39], has gradually shifted to leveraging more advanced rectified-flow-based multi-modal diffusion transformers (MMDiTs) [12, 33, 38, 41, 55]. key advantage of MM-DiTs is their flexibility to control the generation process, e.g., one can inject the query/key/value of source-image tokens (obtained via inversion [12, 33, 41]) into that of the generated tokens at each denoising timestep, improving context fidelity. However, the improved context fidelity often comes at cost of text adherence, as exemplified by results of RFSolver-Edit [41] under different injection steps in Figure 3 (b.1-b.6), where it is unable to strike good balance between the two criteria. quantitative study on this is shown in Figure 2. To achieve better trade-off, KV-Edit [55] introduces userprovided masks to separate regions to be edited from those to be preserved. During generation, KV-Edit only updates the image tokens in the unmasked regions while keeping the masked regions intact. Although this greatly improves the trade-off (as shown by the orange points in Figure 2), KV-Edit sometimes produces conspicuous artifacts and inconsistencies at mask boundaries, especially when the mask is not precise. Typical examples are shown in Figure 1 (b, d) and Figure 3 (c.1, d.1). This imperfection highlights another key aspect of region-based image editing: Editing Seamlessness, which is essential to good user experience. We hypothesize that the boundary artifacts of KV-Edit originate from its hard context replacement strategy, which ensures context fidelity but breaks the interdependency necessary for smooth boundary transitions. Building upon these insights, we propose novel trainingfree image editing method, CannyEdit, that achieves the state of the art in editability-fidelity trade-off, in addition to significant qualitative improvement in seamlessness as validated by comprehensive user study. Specifically, our approach involves two key innovations. First, we utilize Canny ControlNets outputs [52] in selective way to relax the structural constraints in the desired edit region while preserving the original image layout elsewhere. Second, we propose dual-prompt Figure 2: Quantitative study shows that CannyEdit achieves the best editabilityfidelity trade-off under varying hyperparameter settings. The technical details of the graph are in Appendix F.1. 2 Figure 3: Editing results of different methods. RFSolver-Edits outputs fail to balance context fidelity and successful addition simultaneously. While KV-Edit and CannyEdit deliver better trade-off, CannyEdit results in more natural and seamless edits whereas KV-Edits results introduce artifacts such as partially cropped subject in (c.1), and partially generated extra cat in (d.1). strategy that uses local prompt to provide fine guidance to edit regions along with global target prompt to ensure coherence between the edit regions and the context. In summary, we make the following key contributions in this paper: We propose CannyEdit, novel image editing method that enables high-quality region-based edits that seamlessly blend in with the context. Combined with dual-prompt guidance, CannyEdit produces text-adherent, context-aware results with strong editability-fidelity balance. CannyEdit outperforms strong baselines like KV-Edit [55], with up to 10.49% improvements in real-world image editing tasks. It also surpasses open-source training-based methods such as BrushEdit [24] and PowerPaint [56]. User study shows that CannyEdit produces more natural results with significantly lower likelihood of being identified as AI-edited than other methods. For future research, CannyEdit serves as flexible and versatile editing framework upon which various image editing solutions can be further developed. In principle, our framework can also be easily extended to incorporate other forms of control, such as IP control units [43, 50], pose control units [5, 40], etc. CannyEdit also supports multiple edits in one pass of generation."
        },
        {
            "title": "2 Related work",
            "content": "2.1 Training-based image editing methods significant number of studies focus on training-based methods for image editing, which rely on synthetic data generated via large language models and T2I models [4, 17]. While beneficial for teaching models to follow instructions, these methods may not capture all real-world nuances, potentially limiting performance on complex edits. Alternatively, some works collect real-world images and manually annotate data or use task specialists to generate target images [44, 45, 51]. However, scaling these datasets is challenging, and both dataset construction and model training incur considerable computational costs. The challenge of dataset scalability limits the generalization performance of training-based methods. We argue that training-free methods could be viable alternatives, as they are computationally efficient and preserve the high-quality generation capacity of the foundation T2I model, reducing the risk of overfitting or degradation in image quality. 2.2 Training-free image editing methods Training-free editing methods typically use two-stage attention-based pipeline: (1) Inversion: the source image is inverted into the diffusion models latent space to extract attention features during each denoising step; (2) Source-attention injection: these features are injected into text-image crossattention during target image generation. Early works like Prompt-to-Prompt [15] and MasaCtrl [7] manipulated cross-attention maps in the UNet model between text tokens and image regions. Recent methods [12, 33, 38, 41, 55] extend this to more advanced rectified-flow-based transformer models, such as FLUX [2]. As discussed in Section 1, balancing text adherence in edited regions, 3 preserving unedited context, and achieving seamless integration remain key challenges for existing methods. This work introduces novel approach that diverges from the attention-based pipeline, without altering the core attention computation process. 2.3 Controllability in T2I generation Introducing controllability in text-to-image (T2I) generation significantly advances precise image manipulation, unlocking various applications [22, 23, 37, 52, 54]. ControlNet [52] is pivotal framework integrating multiple conditional inputssuch as Canny edge maps for layout, depth maps for spatial arrangement, and skeleton maps for pose control. Specifically, we identify Canny ControlNet as effective for image editing. As an optional plug-and-play module for foundational T2I models, it ensures generated images adhere to both textual prompts and structural details from Canny edges [6]. For image editing, we propose to selectively apply the Canny control to enable precise editing in targeted regions while preserving layout consistency elsewhere."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries: FLUX and Canny ControlNet Our method builds upon FLUX [2], prominent open-source T2I foundation model. FLUX uses the Diffusion Transformer (DiT) architecture [29] and utilizes Rectified Flow (RF) [26] to model data-to-noise transformations. It processes multi-modal inputs via sequence of multistream layers (which use separate projection matrices for text and image tokens) and single-stream layers (which use shared projection matrices). The Canny ControlNet [47], designed for FLUX, integrates duplicates of two multi-stream blocks from FLUX to inject structural layout guidance into it, as shown in Figure 4. Figure 4: The architecture of multi-stream block in FLUX and in FLUX-ControlNet. The attention module within the FLUX multi-stream block, shown in Figure 4 (a), computes crossattention between image tokens (I) and text tokens (T): = (cid:21) (cid:20)ZI ZT = softmax (cid:19) (cid:18) QK dk V, where = (cid:21) (cid:20)QI QT , = (cid:21) (cid:20)KI KT , = (cid:21) (cid:20)VI VT . (1) Here, Q, K, represent the query, key, and value matrices, respectively. Figure 4 (b) illustrates the Canny ControlNets computational flow (dashed lines). The ControlNet is conditioned on embeddings of Canny edge map tokens and text tokens. Furthermore, the image embedding from the FLUX block is summed with the Canny edge map embedding within the ControlNet. The image token outputs from the FLUX block (ZI) and the ControlNet block (Z I) are subsequently combined via ZI ZI + β conv(Z I), where conv denotes 1 1 convolution layer, and β is hyperparameter controlling the strength of the Canny layout guidance. 3.2 Selective Canny control Image editing with diffusion models typically employs an inversion-denoising framework. This involves first inverting the source image to obtain an initial latent xtN that effectively captures its content at the final timestep, essentially mapping the image back through the diffusion process to its noisy latent representation. We utilize FireFlow [12] for this inversion step, chosen for its favorable balance between precision and computational cost. key aspect of our method is that, during this inversion process, while operating on the source image, we apply the Canny ControlNet and cache its outputs ( I) at relevant blocks and timesteps. These cached outputs encode structural guidance derived directly from the source images Canny edges and features. Optionally, intermediate inverted latents xt can also be cached at each timestep. 4 Figure 5: The inversion-denoising process of CannyEdit. 1) inversion: Starting from the source image, its Canny edge map, and source prompt Psource, we use FireFlow [12] to obtain the inverted noise xtN and corresponding Canny ControlNet outputs I. The immediate noisy latents during the inversion process {xt1 , ..., xtN 1 } are also cached for enhancing the context fidelity. 2) denoising: Using the inverted noise xtN , we perform guided generation with selective Canny control (via mask E), and dual prompts, Plocal and Ptarget, to provide multi-level text guidance. As illustrated in Figure 5, for the subsequent denoising phase, where the edited image is generated, we leverage these cached outputs via technique we call selective Canny control. Based on userprovided binary mask (where Eij = 1 indicates an editable patch), we apply the cached ControlNet guidance only to the masked region (1 E). This selective application is mathematically implemented by masking the cached ControlNet output before adding it to the output features: ZI ZI + β (1 E) conv( I). (2) By applying structural guidance from the source images Canny edges exclusively to the unedited background region, we effectively preserve its original layout and visual details. Concurrently, the deliberate absence of Canny control within the masked editing region allows the diffusion process there to be guided primarily by the target text prompt, facilitating the desired modifications. Using the pre-computed cached outputs also enhances computational efficiency during the generation phase. Further improvements to context preservation and transition smoothness can be achieved through optional cyclical blending with cached latents and attenuating the control strength β near the mask boundary. To be specific, when the immediate inverted noisy latents {xt1 , ..., xtN 1 } is cached, cyclical blending can be applied to the unedited region: xedit + (1 α) xtblend where tblend tblend is sampled at pre-defined frequency; The Canny control strength on the mask boundary in the background region is reduced by lowering the β value according its distance to the center of the mask (details in Appendix A). α xedit tblend 3.3 Dual-prompt guidance To achieve text-aligned and seamless edits, we introduce dual-prompt guidance strategy. This approach combines local prompts, which provide region-specific textual signals, with global target prompt, which captures interactions between the edited regions and the broader image context. Although termed dual-prompt, this strategy can accommodate multiple local prompts for simultaneous edits in several regions. For simplicity, we describe an implementation with two local prompts guiding two distinct regions; extension to more regions and prompts is straightforward. 3.3.1 General formulation Let the two image regions be I1 and I2, their corresponding local prompts be T1 and T2, and the global prompt be T*. The number of tokens of these regions/prompts is denoted by . At each timestep, the query matrix is formed by concatenating the query of these tokens: = [QI1, QI2, QT1 , QT2 , QT*]. The key and value matrices are constructed analogously. Following prior work [9], we implement multi-level and multi-region text guidance in training-free manner by applying regional text control via an attention mask within the self-attention module of 5 the FLUX blocks (originally defined in Equation (1)). The modified attention computation becomes: = softmax (cid:18) QK dk (cid:19) V, where = (cid:20)MII MIT MTI MTT (cid:21) (3) is the attention mask applied to the concatenated image-text tokens. For text-to-text (T2T) attention, MTT, we prevent information leakage between distinct guidance signals by restricting each text prompt to attend only to its own tokens. This is enforced using block-diagonal attention mask: MTT = diag(1T1T1, 1T2T2, 1T*T*), where 1nn is an all-ones matrix of size n. This structure enables full self-attention within each prompt while zeroing out off-diagonal blocks to disable cross-prompt interactions. For text-to-image (T2I) attention, MTI, local prompts provide guidance to their respective image regions, while the global target prompt interacts with all image regions to capture broader contextual information. This is achieved with: MTI = 1T1I1 0T1I2 0T2I1 1T2I2 1T*I1 1T*I2 , (4) where 0nm denotes an all-zeros matrix. Symmetrically, we let the image-to-text (I2T) attention MIT = (MTI) to allow bidirectional information flow between modalities. Finally, for image-to-image (I2I) attention, MII, by default we only allow attention only within each respective region: MII = diag(1I1I1, 1I2I2), thereby maintaining the integrity of region-specific processing. 3.3.2 Adjustments for practical editing tasks The default block-diagonal MII is suitable when all regions are considered independently editable. However, for practical editing tasks, this mask can be adjusted based on the roles of different regions. For instance, when adding new content, let I1 be the edit region and I2 be the background region. The local prompt T2 for the background region can be the source images original prompt. In this scenario, MII can be modified to: MII(I1 I2) = (cid:20)1I1I1 1I1I2 0I2I1 1I2I2 (cid:21) . (5) This adjustment, through the 1I1I2 block, allows the edit region I1 to attend to the background region I2. This enables the model to integrate contextual information from the background, leading to more context-aware edit. This masking approach can be extended to scenarios with multiple editable regions while preserving background integrity. To further enhance seamless blending between edited and background regions, we refine I2I attention involving the background area adjacent to the edit boundary. Let I1 be the edit region, I: be the portion of the background region at the boundary of I1, and I2 be the remaining background region (distinct from I: and I1). The I2I attention mask is then defined as MII(I1 I2; I1 I: I2) = 1I1I1 1I:I1 0I2I1 1I1I: 1I1I2 1I:I: 1I:I2 1I2I: 1I2I2 . (6) The key change from Equation (5) is the introduction of the boxed block 1I:I1, which allows the background region at the boundary (I:) to attend to the edit region (I1), enabling the model to incorporate contextual cues from the edited content into these boundary areas. Consequently, the boundary regions can better align visually and semantically with the edits, reducing artifacts and improving overall image coherence. 3.3.3 Prompting strategies for various editing tasks The specifics of local prompts vary depending on the editing task. For object insertion and replacement, local prompts describe the objects to be introduced or the new objects that will substitute existing ones. For object removal, the default local prompt that we use is empty background. We 6 also employ classifier-free guidance [16] by using descriptions of the objects targeted for removal as negative local prompts. more detailed discussion on applying these techniques to diverse editing taskssuch as object transfer while preserving object shapes, or modifying the environment/atmosphere of specific regionis provided in Appendix B, where we also explore how our method can be extended to handle multiple edits within single generation pass."
        },
        {
            "title": "4 Experiment",
            "content": "Baselines. We mainly compare CannyEdit with open-source methods: (1) Training-based approaches including BrushEdit [24] based on UNet diffusion models, FLUX Fill [2] and PowerPaint-FLUX [56] based on FLUX [2]. Note that the original PowerPaint in [2] is based on UNet diffusion model, for fairer comparison, we trained new version based on FLUX with the provided training dataset. (2) Training-free approaches including P2P [15], MasaCtrl [7] based on DDIM [36], and RF-Inversion [33], RFSolver-Edit [41], KV-Edit [55] based on Rectified Flow. Datasets. We introduce the Real Image Complex Editing Benchmark (RICE-Bench) to address the lack of realistic interactions among objects in existing real-world image editing benchmarks [14, 35]. RICE-Bench consists of 80 images capturing real, complex editing scenarios for object insertion, replacement, and removal. It is designed to better evaluate context fidelity, text adherence, and editing seamlessness in real-world editing tasks. Example images from the benchmark are shown in Figures 1, 3, with more examples and details of data curation provided in Appendix D. In short, we curated the data from open datasets [19, 44] and our own private data. We also conduct evaluations on PIE-Bench [19], dataset of 620 real-world and synthetic images spanning 9 diverse editing tasks. Following [24, 49, 55], we exclude the style transfer task to focus on region-based image editing. Implementation details. We implement our method based on FLUX.1-[dev] [2] and FLUX-CannyControlNet [47], using 50 denoising steps and guidance value of 4.0. The strength parameter of Canny control β is set to 0.8 in the inversion and for the non-mask-boundary background region. We apply cyclical blending with α = 0.5 every 5 steps. Other methods were implemented based on their official code releases default settings unless otherwise specified. More details, including machines used, hyperparameters, and computational cost analysis for CannyEdit, are provided in Appendix C. Metrics. For RICE-Bench, we evaluate edits using three metrics: Context Fidelity (CF), Text Adherence (TA), and Trade-off Score (TO Score). CF measures cosine similarity between original and edited images using DINO embeddings [8]. TA assesses adherence to text prompts via GroundingDINO [25] top-1 bounding-box prediction confidence pgdino(image, edited object) for the edited object, defined as the confidence difference between edited and source images: positive in pgdino(edited, edited object) pgdino(source, edited object) for addition/replacement edits and reversed in pgdino(source, removed object)pgdino(edited, removed object) for removal edits to measure the successful elimination of the object. TO Score balances fidelity and editability, computed as TO Score = CF + TA. RICE-Bench also includes user study evaluating editing seamlessness by gauging users ability to identify AI-generated edits. For PIE-Bench, following Zhu et al. [55], we use seven metrics: HPSv2 [46] and aesthetic scores [34] (image quality), PSNR [18], LPIPS [53], and MSE (background preservation), and CLIP score [30] and Image Reward [48] (text adherence). 4.1 Experiment results on RICE-Bench In Table 1, our method demonstrates superior performance in automatic evaluations compared to existing techniques. Achieving the highest TO score across three editing tasks, it strikes the best balance between image editability and fidelity. In contrast, methods like RFSolver-Edit face notable trade-off, either adhering closely to the text instruction while negatively impacting the image context with fewer steps, or preserving context but failing to fully implement the desired edits with more steps. KV-Edit similarly prioritizes context preservation but at the expense of significantly reduced text adherence. Training-based methods also struggle to achieve comparable equilibrium. To complement the automatic metrics and systematically assess the perceived seamlessness of the edits, we conducted user study with total of 137 participants, comprising 96 general users with limited AIGC experience and 41 experts with formal training or experience. The study involved two tasks where participants viewed pairs of images and were asked to identify which image was most likely AI-edited. In Task 1, an image edited by our method was compared against real, unedited image. In Task 2, an image generated by our method is compared against one from another method. To minimize bias, the presentation order of images and questions was randomized. Only successful edits 7 Table 1: Results of automatically computed metrics comparing with representative methods on RICE-Bench. The inject for RFSolver-Edit denotes the number of denoising steps where the source images attention features are injected. The metrics are context fidelity (CF), Text Adherence (TA), and Trade-off Score (TO Score), which measures the trade-off for editability and fidelity. Bold and underlined values represent the best and second-best TO scores respectively. Metrics CF 102 TA 102 TA 102 TA RFSolver-Edit (inject = 2) [41] RFSolver-Edit (inject = 8) [41] KV-Edit [55] BrushEdit [24] FLUX Fill [2] PowerPaint-FLUX [56] CannyEdit (Ours) 71.77 99.13 93.91 87.26 88.21 84.63 88. Add 102 TO Score CF 94.42 101.27 111.16 106.24 109.83 108.97 22.65 2.14 17.25 18.98 21.62 24.34 42.99 79.34 69.81 63.43 70.94 62.31 Removal Replace Avg. 102 TO Score CF 82.04 82.28 86.43 94.72 81.85 83.71 39.05 2.94 16.62 31.29 10.91 21.40 47.44 67.89 64.72 59.11 60.20 60.75 102 TO Score TO Score 9.22 5.03 12.36 7.40 8.13 8.92 56.66 72.92 77.08 66.51 68.33 69.67 77.71 85.49 91.56 89.16 86.67 87. 28.12 116.84 63.28 34.22 97.50 64. 16.77 81.20 98.51 Table 2: Results of user study comparing CannyEdit with representative methods on RICE-Bench ( denotes 95% confidence interval). In Task1, participants identify AI-edited images from paired real (Real) and generated (Gen) images. Seamless edits yield selection ratios close to random chance (50%), as achieved by CannyEdit. In Task2, users directly compare CannyEdit against other methods; lower selection ratios for ours indicate superior editing seamlessness of CannyEdit. General User (96 participants) Expert (41 participants) Generated vs. Real CannyEdit vs. Others Generated vs. Real CannyEdit vs. Others Ratio regarded as AIGC (%) Gen Real Ours Itself Gen Real Ours Itself KV-Edit [55] BrushEdit [24] PowerPaint-FLUX [56] 86.967.19 79.208.99 76.085.49 13.047.19 20.808.99 23.925.49 37.505.72 30.004.90 38.086.66 62.505.72 70.004.90 61.926. 89.099.24 82.0012.1 88.006.57 10.919.24 18.0012.1 12.006.57 37.6910.2 19.299.99 33.857.32 62.3110.2 80.719.99 66.157.32 CannyEdit (Ours) 49.203. 50.803.56 N/A N/A 42.008.12 58.008.12 N/A N/A with high text adherence scores were included. preliminary screening test filtered out participants with low accuracy in distinguishing AI-edited from real images, ensuring data reliability. More details of the user study are provided in Appendix E. The results, presented in Table 2, highlight our methods ability to produce highly seamless edits. In Task 1, general users identified images from our method as AI-edited only 49.20% of the time, while experts did so even less frequently at 42.00%. This indicates that our edited images were often indistinguishable from real ones. Conversely, images from alternative methods were much more readily identified as AI-generated, with detection rates ranging from 76.08% to 89.09%. Consistent with Task 1, in Task 2, images generated by our method were consistently less likely to be perceived as AI-edited when compared directly to outputs from other methods, further underscoring the superior seamlessness of our editing approach. 4.2 Experiment results on PIE-Bench Add hat to the table with flowers. Add deer in the forest. Input Mask CannyEdit KV-Edit Input Mask CannyEdit KV-Edit LPIPS(103) PSNR 51.84 22.85 22. 26.54 LPIPS(103) PSNR 35.52 26.61 16.44 27. Figure 6: Visual examples of our CannyEdit and KV-Edit [55] on PIE examples along with their quantitative results on background preservation. We extend our evaluation to PIE-Bench which involves more images in various editing tasks. The result is displayed in Table 3. As for text alignment, CannyEdit significantly outperforms other methods both in CLIP similarity(22.44 25.36) and Image Reward(5.71 8.20). Our method also keeps competitive level in image quality. The accuracy of masked region preservation achieved by CannyEdit is numerically inferior to that of KV-Edit. However, as illustrated by the examples in 8 Table 3: Comparison with other methods on PIE-Bench. VAE denotes the inherent reconstruction error through VAE reconstruction only. Except the result of ours, other results follow [55]. Metrics VAE Image Quality HPS 102 AS 6.37 24.93 Background Preservation 103 MSE 3.86 LPIPS 7.93 PSNR 37.65 Text Adherence IR 10 104 CLIP Sim P2P [15] MasaCtrl [7] RF-Inversion [33] RFSolver-Edit [41] KV-Edit [55] BrushEdit [24] FLUX Fill [2] CannyEdit (Ours) 25.40 23.46 27.99 27.60 27.21 25.81 25.76 27.19 6.27 5.91 6.74 6.56 6.49 6.17 6.31 6.38 17.86 22.20 20.20 24.44 35.87 32.16 32.53 32. 208.43 105.74 179.73 113.20 9.92 17.22 25.59 26.38 219.22 86.15 139.85 56.26 4.69 8.46 8.55 9.79 19.69 22.24 20.83 21.71 22.08 22.39 22.44 22. 25.36 -3.65 0.017 -1.66 4.34 5.18 5.63 3.33 5.71 8.20 Table 4: Ablation study for CannyEdit on adding and replacement examples of RICE-Bench. The evaluation metrics follow these in Table 1. CC denotes Canny Control, DP is for Dual-Prompt Guidance and CB means cyclical blending. For comparison, we also copy the results of KV-Edit here. KV-Edit (baseline) CannyEdit (Ours) Variants of CC Variants of DP selective CC from current T2I w/o CC full CC local prompt only target prompt only Variant of CB blend at every step 93.91 88.72 84.81 79.73 93.41 93.43 85.04 93.24 CF 102 TA Replace 102 TO Score 102 TA Add 102 TO Score CF 111.16 116.84 17.25 28.12 64.72 64.43 27.93 26.29 19. 10.23 24.96 22.09 112.74 106.02 113.14 103.66 109.00 115.33 55.24 51.51 64. 58.92 60.82 65.12 12.36 16.77 13.89 11.83 12.51 17.09 15.43 13. 77.08 81.20 69.13 63.34 76.53 76.01 76.25 78.20 Figure 6, the background preservation metrics tend to penalize the more natural and complete results generated by CannyEdit, which may not effectively reflect the quality of the background preservation. 4.3 Ablation study We evaluate key modules of CannyEdit using addition and replacement examples from RICE-Bench  (Table 4)  . Avoiding cached ControlNet outputs, by either using selective Canny ControlNets outputs for current T2I denoising process or completely omitting Canny control (1st and 2nd variant of CC), significantly reduces context fidelity. Conversely, using full Canny control improves context preservation but weakens adherence to textual instructions (still outperforms KV-Edit). Ablations on dual-prompt guidance confirm that both local and global prompts are crucial for effective edits. CannyEdit introduces cyclic blending, periodically integrating background features from the original image. While blending at every denoising step better preserves context, it reduces precision in following text instructions (yet remains superior to KV-Edit). Additionally, specialized boundary treatments, such as reduced Canny control and modification of attention masks on the mask boundary region, are essential for smooth visual transitions. Without these treatments, noticeable artifacts and unnatural boundaries can appear (see Appendix F.5 for examples). Our evaluations use oval-shaped masks for additions by default, but Appendix F.6 demonstrates that CannyEdit is robust to other mask shapes, including rectangular and user-drawn styles. 4.4 Qualitative examples Visual examples in Figures 1 and 3 demonstrate that CannyEdit achieves strong balance between context fidelity, text adherence, and editing seamlessness. More examples comparing CannyEdit with different open-source methods are in Appendix F.1. Examples of applying CannyEdit to other editing tasks and more comparison results with close-source models, GPT-4o [28] and Gemini 2.0 Flash [20] are provided in Appendix F.2 and F.3. Our method can also support multiple edits at one pass of generation, an example is given in Figure 1 (a), more multi-edit examples are given in Appendix F.4."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces CannyEdit, novel training-free method for region-based image editing. By utilizing the plug-and-play Canny ControlNet module, we apply the selective Canny control guidance to the foundational T2I model, enabling precise editability in the target region while preserving layout consistency and contextual integrity in other areas. Combined with dual-prompt text guidance, which incorporates both local and global textual information, our method achieves strong balance between text adherence, contextual fidelity, and seamless editing. We hope that CannyEdit inspires further exploration and innovation in applying Canny layout control into various generation tasks, providing more flexible and accessible tools for creative and practical applications. Additionally, we discuss the limitations of our method and potential avenues for improvement in Appendix G."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank the deep learning computing framework MindSpore (https://www.mindspore.cn) and its team for the support on this work."
        },
        {
            "title": "References",
            "content": "[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [2] Black-Forest Labs. Flux: Diffusion models for layered image generation. https://github. com/black-forest-labs/flux, 2024. [3] G. Bradski. The OpenCV Library. Dr. Dobbs Journal of Software Tools, 2000. [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, pages 1839218402, 2023. [5] Benito Buchheim, Max Reimann, and Jürgen Döllner. Controlling human shape and pose in text-to-image diffusion models via domain adaptation. In WACV, pages 36883697, February 2025. [6] Canny. computational approach to edge detection. PAMI, 8(6):679698, 1986. [7] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In ICCV, pages 2256022570, 2023. [8] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, pages 96509660, 2021. [9] Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, and Shanghang Zhang. Training-free regional prompting for diffusion transformers. arXiv preprint arXiv:2411.02395, 2024. [10] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [11] DeepSeek-AI, Liu Aixin, and et al. Deepseek-v3 technical report, 2025. URL https://arxiv. org/abs/2412.19437. [12] Yingying Deng, Xiangyu He, Changwang Mei, Peisong Wang, and Fan Tang. Fireflow: Fast inversion of rectified flow for image semantic editing. arXiv preprint arXiv:2412.07517, 2024. [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [14] Xin Gu, Ming Li, Libo Zhang, Fan Chen, Longyin Wen, Tiejian Luo, and Sijie Zhu. Multireward as condition for instruction-based image editing. arXiv preprint arXiv:2411.04713, 2024. [15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In ICLR, 2022. [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [17] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. [18] Quan Huynh-Thu and Mohammed Ghanbari. Scope of validity of psnr in image/video quality assessment. Electronics letters, 44(13):800801, 2008. [19] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In ICLR, 2024. Experiment with gemini 2.0 flash native im- [20] Kat Kampf and Nicole Brichtova. URL https://developers.googleblog.com/en/ age generation, March 2025. experiment-with-gemini-20-flash-native-image-generation/. Google Developers Blog. [21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, pages 40154026, 2023. [22] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving conditional controls with efficient consistency feedback: Project page: liming-ai. github. io/controlnet_plus_plus. In ECCV, pages 129147. Springer, 2024. [23] Wei Li, Xue Xu, Jiachen Liu, and Xinyan Xiao. Unimo-g: Unified image generation through multimodal conditional diffusion. arXiv preprint arXiv:2401.13388, 2024. [24] Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Ying Shan, Yuexian Zou, and Qiang Xu. Brushedit: All-in-one image inpainting and editing. arXiv preprint arXiv:2412.10316, 2024. [25] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, pages 3855. Springer, 2024. [26] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [27] Luca Medeiros. Language segment-anything: Sam with text prompt. https://github.com/ luca-medeiros/lang-segment-anything, 2024. [28] OpenAI. Introducing 4o image generation. https://openai.com/index/ introducing-4o-image-generation/, 2025. [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR, 2021. [31] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. [33] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and WenSheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. [34] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS Datasets and Benchmarks Track, 2022. 11 [35] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In CVPR, pages 88718879, 2024. [36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. [37] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. [38] Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, and Gal Chechik. Add-it: Training-free object insertion in images with pretrained diffusion models. In ICLR, 2025. [39] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In CVPR, pages 19211930, 2023. [40] Jiajun Wang, MORTEZA GHAHREMANI, Yitong Li, Björn Ommer, and Christian Wachinger. Stable-pose: Leveraging transformers for pose-guided text-to-image generation. In NeurIPS, 2024. [41] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. [42] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [43] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [44] Navve Wasserman, Noam Rotstein, Roy Ganz, and Ron Kimmel. Paint by inpaint: Learning to add image objects by removing them first. arXiv preprint arXiv:2404.18212, 2024. [45] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In ICLR, 2024. [46] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. Flux controlnet collections. https://huggingface.co/XLabs-AI/ [47] XLabs AI. flux-controlnet-collections, 2024. [48] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In NeurIPS, pages 1590315935, 2023. [49] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. Inversion-free image editing with natural language. In CVPR, 2024. [50] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [51] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. NeurIPS, 36:3142831449, 2023. [52] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38133824. IEEE Computer Society, 2023. [53] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [54] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. NeurIPS, 36:1112711150, 2023. [55] Tianrui Zhu, Shiyi Zhang, Jiawei Shao, and Yansong Tang. Kv-edit: Training-free image editing for precise background preservation. arXiv preprint arXiv:2502.17363, 2025. 12 [56] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In ECCV, pages 195211. Springer, 2024."
        },
        {
            "title": "A Treatments on the mask boundary region",
            "content": "To ensure seamless transition between the edited and unedited regions, we apply specific treatments to the boundary region. These treatments include adjusting the corresponding image-to-image attention mask, as detailed in Equation (6), and modifying the strength of the Canny control applied within this region. In particular, for controlling the Canny strength near the boundary, we set the control strength to zero within the editable region and to fixed strength parameter β within the background (unedited) region. To smoothly blend these two regions, we define boundary region around the editable area based on threshold distance from the editable patch center C. Formally, the boundary region is defined as: = {(i, j) dist((i, j), C) T, Eij = 0}, where Eij denotes the editable region indicator (with value 1 for editable pixels and 0 otherwise). Within this boundary region, we adaptively adjust the Canny control strength βij according to the distance from each pixel (i, j) to the editable patch center C: where: βij = β (dij), dij = (i, j) C2 represents the Euclidean distance between the pixel (i, j) and the editable patch center C. (d) is monotonically increasing normalization function mapping distances to the range [0, 1]: (dij) = dij dmin dmax dmin , where dmin and dmax denote the minimum and maximum distances, respectively, among all pixels within the boundary region to the center C."
        },
        {
            "title": "B Application of CannyEdit to different editing tasks",
            "content": "Our CannyEdit framework is versatile and can be applied to wide range of editing tasks. Table 5 summaries the the tasks CannyEdit supports with their definitions. The approach to each task varies based on the masks used, the Canny control applied, and the text prompt guidance used. To accommodate the unique requirements of certain tasks, we integrate CannyEdit with additional training-free techniques, as detailed below. More visual examples of adding, removal and replacement, and examples of other tasks are given in Appendix F.1 and F.2 respectively. Table 5: Definition to different editing tasks. Task Adding Removal Definition Introduce new object into specific region of the image. Remove specific object from the image. Replacement Replace an existing object with another object. Object Transfer Replace an object with another, preserving the original object shape. Context Modification IP-preserved Edit Modify the environment or atmosphere of specific region, such as changing clouds to rain or making the sky sunny. Edits like altering the subjects posture, expression, with the identity of the subject preserved. 13 Table 6: Masks and Canny Control applied in different editing tasks. Task Adding Removal Replacement Object Transfer Masks Canny Control Use oval mask in default (can be refined by segmentation models). Selective Canny Masking Dilated segmented mask of the object to be removed. Selective Canny Masking Dilated segmented mask of the object to be replaced. Selective Canny Masking Segmented mask of the object to be transferred. Selective Canny Weakening Context Modification Mask indicating the region to be modified. Selective Canny Weakening IP-preserved Edit Dilated segmented mask of the object to be edited. Selective Canny Masking Table 7: The regional text guidance used for different editing tasks (the symbol denotes that the prompt on its left provides the textual guidance for the image region on its right). Task Adding Removal Regional Text Guidance target prompt whole image, source prompt background region, local prompts edit regions. Positive regional text guidance: target prompt whole image, local prompts (use empty background in default) edit regions; Negative regional text guidance: source prompt whole image, local prompts (descriptions of objects to be removed) - edit regions. Other tasks target prompt whole image, local prompts edit regions. B.1 The masks used in different editing tasks We use user-provided oval masks to indicate the location of objects for the adding task in default. As demonstrated in Appendix F.6, CannyEdit exhibits robustness to variations in mask shapes. To minimize excessive changes to the image context caused by overly large masks in adding tasks, optional mask refinement can be applied using segmentation models [21, 27, 31]. These segmentation models help generate more precise masks for the added objects, enabling refined image generation. For the object transfer task, segmented masks of the objects to be transferred are employed to preserve their shapes. In the removal, replacement, and IP-preserved editing tasks, we apply dilation to the segmented masks of the objects to be edited using cv2.dilate() from OpenCV [3] to enhance editing flexibility. B.2 The Canny control applied in different editing tasks To enable region-specific editability, we introduce Selective Canny Masking strategy for edited regions. In this approach, the outputs of the Canny ControlNet corresponding to the edited regions are set to zero before being integrated into the respective blocks of the T2I model (as detailed in Equation (2) of the main paper). We apply selective Canny masking to editing tasks such as object addition, removal, and replacement. For certain editing scenarios, such as object transfer and context modification, our goal is to preserve the general shape and layout of objects within the edited regions of the original image, rather than generating entirely new or substantially different objects. In these cases, we selectively reduce the strength parameter of the Canny control within the edited region (Selective Canny Weakening), rather than completely eliminating the control by setting the ControlNets outputs to zero in that region. Practically, this approach maintains editing flexibility while effectively preserving essential shape information from the original image. 14 B.3 Prompting for different editing tasks We propose dual-prompt guidance strategy that combines local prompts with global target prompt to provide multi-level textual information. Table 7 summarizes the prompts used for different tasks. Additional details about the removal task are provided in Appendix B.4. B.4 Combining other training-free techniques for removal & IP-preserved Edits Removal. To eliminate the specified existing objects in the image, we apply modified version of classifier-free guidance [16]: xedit are computed using the prompts listed under the positive and negative regional text guidance for removal in Table 7 respectively. + wcf g(xedit xedit- ), where xedit xeditand xeditt IP-preserved Edit. To preserve the identity of the original subject while modifying certain aspects, such as pose or expression, we combine CannyEdit with source attention injection methods [12, 33, 38, 41]. The selective Canny control enables localized editability of the layout. Since structural layouts are primarily determined during the early diffusion steps, we avoid injecting attention features in the initial steps to achieve the desired changes. Instead, we inject the late-step key and value of source-image tokens to maintain the subjects identity. B.5 Multiple edits at one pass Our CannyEdit method can be seamlessly extended to support multiple edits in single generation pass. Specifically, the selective Canny control can be applied to multiple edit regions, and the dual-prompt guidance, enabled by attention masks in Equations (4), (5), and (6) of the main paper, can also be readily adapted to handle multiple edit regions. More visual examples of generating multiple edits at one pass are provided in Appendix F.4."
        },
        {
            "title": "C Implementation details",
            "content": "We implement our method based on FLUX.1-[dev] [2] and FLUX-Canny-ControlNet [47], using 50 denoising steps and guidance value of 4.0. The strength parameter of Canny control β is set to 0.8 in the inversion and for the non-mask-boundary background region. We apply cyclical blending with α = 0.5 every 5 steps. Other methods were implemented based on their official code releases default settings unless otherwise specified. Compared to previous training-free image editing methods based on FLUX [12, 38, 41], the additional computational cost of our method primarily arises from the integration of Canny ControlNet. However, the FLUX-Canny-ControlNet, with only 0.74B parameters, is lightweight compared to the FLUX model, which has 12B parameters. This is because the Canny ControlNet includes only two multi-stream blocks, whereas FLUX contains 19 multi-stream blocks and 38 single-stream blocks. The computation overhead introduced by our method is acceptable, as the lightweight nature of the Canny ControlNet ensures efficient performance without significantly increasing resource demands. Curation of RICE-Bench In this work, we focus on real-world image editing scenarios that involve complex interactions between the edited region and the surrounding image context. However, existing real-world image benchmarks [14, 35] primarily involve edits to minor objects and lack realistic interactions among objects (e.g., add glass of water on the table). To address this limitation, we introduce the Real Image Complex Editing Benchmark (RICE-Bench), designed to better evaluate context fidelity, text adherence, and editing seamlessness in real-world editing tasks. Compared to previous benchmark, the edits in RICE-Bench typically involve more significant changes to the image layout, posing greater challenges in balancing context fidelity and text adherence. RICE-Bench consists of 80 images depicting real scenes with complex editing scenarios, divided into adding, replacement, and removal tasks (30 for adding, and 25 each for replacement and removal). Each example includes source image, an input mask, and text prompts. Examples of source images, input masks, and text prompts from the benchmark are shown in Figure 9. In summary, we curated the datasets from three sources. Two of them are open datasets: PIPE [44], which contains semi-synthetic inpainting data, and PIE [19], which includes some natural images. We also utilize mainstream LLM (DeepSeek-V3 [11]) and an image generation model (FLUX [2]) to construct and filter high-quality synthetic images. The LLM and Vision-Language Model (VLM), Qwen2-VL-72B [42], are used to create local editing prompts and target prompts. Based on these sources, the data curation pipeline can be divided into four steps: 1. Source image construction and filtering. For the PIPE [44] and PIE [19] sources, we carefully filter images with relatively complex real-life scenarios and interactions. For synthetic data generation, as referred to in [38], we prepare an instruction prompt to ask the LLM to generate set of text prompts, which are utilized to generate source images by FLUX. The instruction prompt is shown in Prompt Example 1. With synthesized source images, we further filter high-quality samples without distortion or irrational content. 2. Mask generation. For the adding task, we use cv2.polylines() and cv2.fitEllipse() in OpenCV [3] to manually draw oval masks for possible locations to add objects. For replacement and removal tasks, we use LanguageSAM [27] to segment target objects based on their names. 3. Local prompt generation. For the adding task, we manually describe objects to be added and ask the VLM (Qwen2-VL-72B) to enrich the details of the description. For replacement and removal tasks, as referred to in [56], given the source image and corresponding editing masks, we first crop the target object and then utilize the VLM to describe the object only. 4. Source prompt and target prompt generation. Given the source image, we first ask the VLM to caption the image as the source prompt. Based on the source prompt, mask, and corresponding local prompt, we use the VLM to generate target prompt that describes the whole image after editing. For the adding task, an example of the instruction prompt used to generate the target prompt is shown in Prompt Example 2. Please generate JSON list of 100 sets . Each set consists of : an index , source prompt . The source prompt describes source image . The source prompt should include relatively complex real - life scenario and at least one person . Here is an example : { \" index \": 1 , \" src_prompt \": \" beautiful park with bench , man is sitting } on it \" Prompt Example 1: Instruction prompt for generating texts to create source images. Given the caption of an image : { source_prompt } , Now want to add { num_objects } objects or persons . Their corresponding descriptions are : { all_local_prompts }. According to these caption and descriptions , please summarize them into refined target prompt within 20 words . Return the target prompt only . Prompt Example 2: Instruction prompt for generating target prompts. Additional examples of the benchmark will be provided in the published codebase, along with the corresponding CannyEdit outputs. The data is organized as follows: - image_name.png: source image, - image_name_mask.png: mask, - image_name.json: prompt information."
        },
        {
            "title": "E Details of user study",
            "content": "The user study was conducted based on an online survey platform. In total, we recruited 137 participants, divided into two groups: the general public group (96 participants) and the expert group (41 participants). The general public group consisted of first-year undergraduate students from university, with no technical background in AI-Generated Content (AIGC). The expert group comprised individuals who at least had either studied AIGC models in credit-bearing course or trained an AIGC-related model. The study consists of two tasks. In both tasks, participants were shown two images and asked to answer the following question: Please select the image (a) or (b) you believe is most likely to have been edited by an AI model. The tasks differ from the image pairs provided: Task 1: An AI-edited image was compared with real, unedited image; Task 2: An image generated by CannyEdit was compared with the one from another editing method, using the same source image and editing instruction. To reduce bias, image and question order were randomized, and each participant saw 20 randomly selected questions. Only successfully edited examples (with high TA scores) were included in the study. screening test ensured data quality: participants were first required to evaluate 10 individual images (including images from different editing methods and unedited real images) as AI-edited or real, and those with poor accuracy (correct rate <50%) were excluded (2 from the group of general users). Further study details are provided in Appendix E. The results of the user study are presented in Table 2 of the main paper. In Task 1, seamless edit should result in selection ratios close to random guessing (i.e., 50%), which is achieved by our CannyEdit method. In Task 2 where images produced by CannyEdit are paired with those from other methods, lower selection ratio for Ours indicates that users found our edits more seamlessdemonstrating the superior realism of CannyEdits outputs. The instruction provided to the participants is detailed below: We are an AIGC (AI-Generated Content) research team. Thank you for taking the time to participate in our study! Study Objective This study aims to evaluate whether users like you can accurately identify if an image has been edited by an AI model. What to Expect There are two types of questions in this study: 1. Single Image Evaluation (10 questions) You will be shown single image and asked: \"Has this image been edited by an AI model?\" Please select either \"Yes\" or \"No\". 2. Image Comparison (20 questions) You will be shown two images, labeled (a) and (b). Your task is to decide: \"Which image is more likely to have been edited by an AI model?\" You may choose: Image (a) Image (b) Hard to pick (if youre uncertain) Hint for Better Accuracy When making your decisions, pay close attention to unrealistic object interactions, such as: Objects that float or have odd shadows Hands, fingers, or limbs that look distorted Lighting or reflections that dont match the scene Textures that are overly smooth, blurry, or warped Background elements blending unnaturally with the foreground Final Notes Your responses will help us better understand human perception of AI-edited content. There are no right or wrong answersplease go with your best judgment. The study should take around 1015 minutes to complete. The screenshots of the user study interface are provided in Figures 7 and 8. Figure 7: Two examples of the Task 1 interface. In the first example, (a) is an unedited real image and (b) is generated by CannyEdit. In the second example, (a) is generated by CannyEdit. Figure 8: Two examples of the Task 2 interface. In the first example, (a) is generated by KV-Edit [55], and (b) is created by our CannyEdit. In the second example, (a) is generated by CannyEdit, and (b) is created by KV-Edit [55]."
        },
        {
            "title": "F More visual examples",
            "content": "F.1 Examples on adding, replacement and removal tasks F.1.1 Details on Figure 1, 2 The source images, masks and corresponding text prompts fitted into the CannyEdit for generating the visual examples illustrated in Figure 1 is provided in Figure 9. An extended version of Figure 2 from the main paper is presented in Figure 10, with detailed descriptions of how the data point is generated provided in its caption. (a) Add woman with her dog and student reading book on the lawn. (b) Add person running on the street. SourceP: man in blue shirt is jogging on tree-lined path in sunny park. SourceP: construction site with barriers and signs. TargetP: man jogs on tree-lined path in sunny park, while woman TargetP: construction site with barriers and signs, with person walks her dog and student reads on the grass. jogging through the area. LocalP 1: student is engrossed in their book, sitting on the lush green lawn. LocalP: person is running on path, wearing athletic shoes and shorts. LocalP 2: woman in green jacket and ponytail is strolling through the park with her small brown dog on leash. (c) Replace the woman tennis player with man tennis player. (d) Add baseball catcher on the field. SourceP: female tennis player in action on court, with spectators watching. SourceP: baseball field with net and white line on the dirt. TargetP: male tennis player in action on court, with spectators watching. TargetP: baseball field with net and white line on the dirt, LocalP: male tennis player in action on court. catcher wearing protective gear and holding mitt. LocalP: baseball catcher wearing white uniform is kneeling on the ground and holding mit. (e) Remove the umbrellas. (f) Replace the pepper with three apples. SourceP: Two people walking in the rain with umbrellas. SourceP: red pepper on towel. TargetP: Two people walking in the rain. Positive LocalP: Empty background. Negative LocalP: Umbrellas. TargetP: Three red apples on towel. LocalP: Three red apples. Source image Mask CannyEdit Source image Mask CannyEdit Figure 9: Source images, input masks and corresponding text prompts (SourceP: source prompt; TargetP: global target prompt; LocalP: local edit prompt) provided to CannyEdit for generating the visual examples illustrated in Figure 1. In example (a), two additions are made in one generation pass and thus there are two local prompts. Figure 10: Extended version of Figure 2. Quantitative comparison of context preservation and text adherence for RFSolver-Edit [41], KV-Edit [55], and our proposed CannyEdit method, evaluated on 15 examples involving the addition of human subjects (a subset from the RICE-Beach). Context preservation is measured by computing the cosine similarity between DINO embeddings [8] of the source and edited images. Text adherence is quantified using GroundingDINOs [25] top-1 bounding-box confidence on the added object, defined as the difference in confidence scores between edited and source images. Different points for each method indicate varying hyperparameter settings: number of steps for injecting source-image features (RFSolver-Edit), skip steps and reinitialization (KV-Edit), and the frequency of cyclic blending applied to the background region (CannyEdit). F.1.2 More examples of adding, replacement and removal Additional visual examples demonstrating object addition, replacement, and removal are provided in Figure 11, which compares results generated by CannyEdit, KV-Edit, and the training-based method PowerPaint-FLUX. These examples illustrate that CannyEdit achieves compelling balance between context fidelity, text adherence, and seamless editing quality. In contrast, KV-Edit could struggle to accurately follow the provided text prompts (as observed in (a), (b), and (f)), while outputs of PowerPaint-FLUX exhibit noticeable degradation in overall image quality. (a) Add woman embracing the man. (b) Replace the man with woman (c) Replace the dog with boy. (d) Remove the dog. (e) Replace the male player with female player. (f) Remove the football player. Input CannyEdit KV-Edit PowerPaint-FLUX CannyEdit KV-Edit PowerPaint-FLUX Figure 11: Visual examples of our CannyEdit, KV-Edit [55], and PowerPaint-FLUX [56] across adding, removal and replacement tasks. The samples are from the RICE-Bench (Prompts and masks are omitted here; they are organized in the same manner as in Figure 9). F.2 Examples on other editing tasks We here show examples of the application of CannyEdit applied to object transfer, context modification and IP-preserved editing tasks in Figures 12, 13 and 14 respectively. (a) Transfer the fishes to sharks. (b) Transfer the cartoon boy to cartoon girl. (c) Transfer the bunny to pig. (d) Transfer the rat to pig. (e) Transfer the white tiger to white cat. (f) Transfer the cat to tiger. (g) Transfer the teacup to cake. (h) Transfer the bread to meat. Input CannyEdit Input CannyEdit Figure 12: Visual examples of object transfer using our CannyEdit. The objects are transferred while preserving their original shapes. All samples are from the PIE-Bench. 22 (a) Modify context: cover the table with food. (b) Modify context: fill the seats on the right with audience members. (c) Modify context: make the sky cloudy. (d) Modify context: change the cloudy sky to blue sky with white clouds. Input CannyEdit Input CannyEdit Figure 13: Visual examples of context modification using our CannyEdit (Prompts and masks are omitted here; they are organized in the same manner as in Figure 9.) (a) Make the cartoon dog jump up from the ground. (b) Make the bear stand and dance on the ground. (c) Make the greyhound run through the grass. (d) Make the woman run down the path at sunset. Input CannyEdit w/o feature injection Input CannyEdit w/o feature injection Figure 14: Visual examples of IP-preserved editings using our CannyEdit. Comparisons with the results of w/o feature injections highlight the importance of injecting the late-stage key and value tokens from the source image to preserve the subjects identity. All samples are from the PIE-Bench. F.3 Comparison of CannyEdit with close-source models We compare the outputs of CannyEdit with those of closed-source models, GPT-4o [28] and Gemini 2.0-Flash [20], as shown in Figures 15 and 16. While the closed-source models tend to excessively alter the image context or fail to follow text instructions, our CannyEdit achieves better balance between context preservation and adherence to textual guidance. 23 Input CannyEdit GPT-4o [28] Gemini-2.0-Flash [20] (a) Add lady crouched on the grass looking at the cat. (b) Add woman embracing the man. (c) Add woman with her dog and student reading book on the lawn. (d) Replace the woman tennis player with man tennis player. (e) Replace the dog with boy. (f) Replace the man with woman. (g) Remove the umbrellas. (h) Remove the football player. Figure 15: Visual comparisons of our CannyEdit method with GPT-4o [28] and Gemini 2.0-Flash [20], across adding, replacement, and removal tasks. 24 Input CannyEdit GPT-4o [28] Gemini-2.0-Flash [20] (i) Transfer the cat to tiger with shape unchanged. (j) Transfer the white tiger to white cat with shape unchanged. (k) Modify context: cover the table with food. (l) Modify context: fill the seats on the right with audience members. (m) IP-preserved: Make the bear stand and dance on the ground. (n) IP-preserved: Make the greyhound run through the grass. (o) Replace the man with woman + Replace the sofa with desk. (p) Replace the dog with cat + Add man standing on the roof. Figure 16: Visual comparisons of our CannyEdit method with GPT-4o [28] and Gemini 2.0-Flash [20], across object transfer, context modification, IP-preserved editing, and multi-edit tasks. 25 F.4 Multi-edit examples Figure 17 presents visual examples of conducting multiple edits in single generation pass. In principle, our CannyEdit method can be extended to generate two or more edits simultaneously. We demonstrate examples of performing two edits (which could be different tasks) in this figure, and leave the extension to three or more edits as future exploration. (a) Add female photographer + Add child. (b) Add boy + Add girl. (c) Add woman + Add another woman talking with traffic officer. (d) Add woman jogging on the treadmill + Add coach monitoring the training. (e) Replace the man with woman + Add child. (f) Replace the man with woman + Replace the sofa with desk. (g) Replace the dog with cat + Add man standing on the roof. (h) Add boy + Add girl. Input CannyEdit Input CannyEdit Figure 17: Visual examples of multiple edits at one generation pass using our CannyEdit. The input images are sourced from [19, 35] or generated by the FLUX model (Prompts and masks are omitted here; they are organized in the same manner as in Figure 9(a).) F.5 Examples of ablations without boundary treatment Visual examples in Figure 18 highlight the significance of boundary treatments. Subfigures (a.1d.1) illustrate unnatural transitions, whereas (a.2d.2) demonstrate smoother results achieved through reduced Canny control (detailed in Appendix A) and attention adjustments (as formalized in Equation (6) of the main paper) applied at the boundaries between edited and unedited regions. w/o treatment on boundary w/ treatment on boundary (a.1) (a.2) (b.1) (b.2) (c.1) (c.2) (d.1) (d.2) Figure 18: Visual examples illustrate the significance of applying special treatments at the boundaries between edited and unedited regions. Without these treatments, (a.1-d.1) exhibit unnatural transitions, whereas (a.2-d.2) achieve smoother and more seamless results through the application of reduced Canny control and attention adjustment. 27 F.6 Examples illustrating CannyEdits robustness to input masks We present examples in Figure 19 to demonstrate that CannyEdit consistently delivers high-quality outputs regardless of the mask shape, including the default oval mask, rectangular mask, and rectangular mask with random edge augmentations (simulating user-drawn masks). These results underscore CannyEdits robustness to variations in masks provided. (a) Add baseball catcher on the field. (b) Add person running on the street. Mask Output Mask Output (c) Add boy near the dog. (d) Add boy in blue shirt next to the boy in striped shirt. Mask Output Mask Output Figure 19: Visual examples showcasing CannyEdits outputs using masks of various shapes in the adding task: the default oval mask, rectangular mask, and rectangular mask with random edge augmentations. These results demonstrate the robustness of CannyEdit to different mask choices."
        },
        {
            "title": "G Limitations and societal impacts",
            "content": "G.1 Limitations and future improvements Compared with text-instructed editing methods that require no user-provided mask, our CannyEdit necessitates mask input to indicate the region to be changed. While this setting allows better controllability for the region to edit, and we have demonstrated our methods robustness to mask shape choices in Appendix F.6, it does demand more input from users. To alleviate the burden on user input, we plan to apply multimodal large language models (MLLMs) to automatically generate masks via chain-of-thought prompting in the future. Additionally, the MLLMs can facilitate generating the source prompt by captioning the source image, refining rough local editing prompts provided by users, and producing the target prompt. Another limitation of our method is that while we have shown that combining CannyEdit with source attention injection methods can enable identity-preserved edits, the preservation of identity is still not perfect, especially when extended to the preservation of human identity. Future improvements can be made by combining our method with IP control units [43, 50]. Furthermore, we have currently built our method based on FLUX.1-[dev] [2] and FLUX-Canny-ControlNet [47]. In principle, our CannyEdit can be extended to other models, which will be explored further in future work. G.2 Societal impacts The image editing technique introduced in this paper has the potential to offer significant benefits to society. By providing users with intuitive and versatile tools to modify their images, these techniques can greatly reduce the time and expense associated with image editing. This improved effectiveness and accessibility can democratize visual content creation and manipulation, empowering individuals from diverse backgrounds, regardless of their technical expertise. To mitigate the risk of misuse, we will implement robust safeguards when releasing the code and model. These measures will include content moderation systems, filters to prevent the generation of harmful, unethical, or inappropriate content, and adherence to ethical guidelines for AI deployment. Furthermore, we will provide comprehensive documentation and user guidelines to promote responsible usage of the technology. All data sources and codebases referenced in this work are open-source. In alignment with the principles of open science and collaboration, we will also open-source our own code and curated benchmark. When releasing our code and model, we will rigorously adhere to the licenses of the referenced resources, ensuring full compliance with their terms and conditions. By embracing transparency and sharing our work, we aim to contribute to the advancement of image editing techniques and foster ongoing innovation in the field."
        }
    ],
    "affiliations": [
        "Huawei Hong Kong AI Framework & Data Technologies Lab",
        "Shanghai University of Finance and Economics",
        "The Hong Kong University of Science and Technology"
    ]
}