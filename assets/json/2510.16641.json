{
    "paper_title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models",
    "authors": [
        "Young-Jun Lee",
        "Byung-Kwan Lee",
        "Jianshu Zhang",
        "Yechan Hwang",
        "Byungsoo Ko",
        "Han-Gyu Kim",
        "Dongyu Yao",
        "Xuankun Rong",
        "Eojin Joo",
        "Seung-Ho Han",
        "Bowon Ko",
        "Ho-Jin Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-and-Language Models (VLMs) have shown impressive capabilities on single-turn benchmarks, yet real-world applications often demand more intricate multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only partially capture the breadth and depth of conversational scenarios encountered by users. In this work, we introduce MultiVerse, a novel multi-turn conversation benchmark featuring 647 dialogues - each averaging four turns - derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484 tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from factual knowledge and perception to advanced reasoning tasks such as mathematics and coding. To facilitate robust assessment, we propose a checklist-based evaluation method that leverages GPT-4o as the automated evaluator, measuring performance across 37 key aspects, including perceptual accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve only a 50% success rate in complex multi-turn conversations, highlighting the dataset's challenging nature. Notably, we find that providing full dialogue context significantly enhances performance for smaller or weaker models, emphasizing the importance of in-context learning. We believe MultiVerse is a landscape of evaluating multi-turn interaction abilities for VLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 1 4 6 6 1 . 0 1 5 2 : r MULTIVERSE: Multi-Turn Conversation Benchmark for Evaluating Large"
        },
        {
            "title": "Vision and Language Models",
            "content": "Young-Jun Lee Byung-Kwan Lee Jianshu Zhang Yechan Hwang Byungsoo Ko Han-Gyu Kim Dongyu Yao Xuankun Rong Eojin Joo Seung-Ho Han Bowon Ko Ho-Jin Choi KAIST WHU NAVER CMU Figure 1. An overview of MULTIVERSE, multi-turn conversation benchmark designed to evaluate VLMs across eight main tasks (e.g. Reasoning, Mathematics, Knowledge) comprising 484 tasks, nine main interaction goals (e.g. Verification, Exploration) with 484 distinct interaction goals, and 25 diverse image domains (e.g. Charts and Graphs, Nature). MULTIVERSE provides instance-specific checklist evaluation items for each turn and is the first multi-turn conversation benchmark encompassing diverse and challenging tasks."
        },
        {
            "title": "Abstract",
            "content": "Vision-and-Language Models (VLMs) have shown impressive capabilities on single-turn benchmarks, yet real-world applications often demand more intricate multi-turn dialogues. Existing multi-turn datasets (e.g. MMDU, ConvBench) only partially capture the breadth and depth of In this conversational scenarios encountered by users. work, we introduce MULTIVERSE, novel multi-turn conversation benchmark featuring 647 dialogueseach averaging four turnsderived from diverse set of 12 popular VLM evaluation benchmarks. With 484 tasks and 484 interaction goals, MULTIVERSE covers wide range of topics, from factual knowledge and perception to advanced reasoning tasks such as mathematics and coding. To facilitate robust assessment, we propose checklist-based evaluation method that leverages GPT-4o as the automated evaluator, measuring performance across 37 key aspects, including perceptual accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on MULTIVERSE, revealing that even the strongest models (e.g., GPT-4o) achieve only 50% success rate in complex multi-turn conversations, highlighting the datasets challenging nature. Notably, we find that providing full dialogue context significantly enhances performance for smaller or weaker models, emphasizing the importance of in-context learning. We believe MULTIVERSE is landscape of evaluating multi-turn interaction abilities for VLMs. We make our source code and dataset publicly available. 1 1. Introduction Vision-and-Language Models (VLMs) [3, 5, 18] have recently demonstrated remarkable performance on various evaluation benchmarks [12, 14, 37, 50, 7174]. These benchmarks primarily assess the perception and reasoning capabilities of VLMs through simple binary, multiplechoice questions, or short free-form responses in singleturn interactions. However, real-world usage often involves users engaging in continuous, multi-turn conversations with models to solve problems iteratively. While multi-turn evaluation benchmarks have introduced for evaluating LLMs [4, 17, 26, 41, 59, 66, 78], comparatively few benchmarks (i.e. MMDU [47], ConvBench [45]) address VLMs abilities in multi-turn conversation. This gap raises the critical question: Can VLMs that excel in single-turn benchmarks also meet user needs in more interactive, multi-turn scenarios? Despite recent efforts to address this issue, existing benchmarks still exhibit significant shortcomings and do not adequately cover VLMs capabilities in multi-turn interactions, particularly across diverse tasks (breadth) and reasoning requirements (depth). As shown in Figure 2, primarily features knowledge-oriented images (e.g. landscapes, animals, art) derived from WIT [60], placing substantial emphasis on knowledge-specific reasoning tasks (e.g. Describe the object in the image). Hence, MMDU offers relatively narrow set of fewer than 15 tasks and limited range of question styles with low lexical diversity. In contrast, ConvBench encompasses three main task categories (i.e. perception, reasoning, creation) and covers 219 subtasks derived from VisITBench [6]. This design significantly broadens its task coverage. However, it lacks more advanced tasks, such as equation verification in Figure 1. Furthermore, codeand mathematics-related responses remain sparse, and users queries are comparatively simple 1https : / / passing2961 . github . io / multiverse - project-page/ Figure 2. Examples of multi-turn conversations from MMDU [47] and ConvBench [45], along with their limitations. The graph below compares MMDU, ConvBench, and MULTIVERSE in terms of query lexical diversity, the number of code/math tasks in responses, and the total number of tasks. in their linguistic structure. These shortcomings underscore the need for more comprehensive and holistic multi-turn conversation benchmark over more diverse and challenging tasks (breadth & depth ). In this paper, we introduce MULTIVERSE, novel multiturn conversation benchmark designed to evaluate VLMs on more diverse and advanced user tasks. MULTIVERSE comprises 647 conversations, each averaging around four turns, spanning 484 tasks (e.g. reasoning, mathematics) and 484 interaction goals (e.g. verification, analysis). We first collect source images from 12 widely used VLM evaluation benchmarks (e.g. MegaBench [8], CharXiv [67], MMMU [73]), covering broad range of domains such as nature, science, and mathematics. After selecting highquality seed images, we adopt personal background-toconversation approach [19, 23, 34], which ensures high lexical diversity-based multi-turn dialogues. Finally, we meticulously remove any unsuitable samples through manual review, focusing on natural conversation flow, factual correctness, and blindness criteria [15, 37]. Therefore, as shown in Figure 2, MULTIVERSE encompasses diverse range of tasks, enabling the handling of more advanced reasoning capabilities (e.g. mathematics, coding) to address inFigure 3. An overview of MULTIVERSE construction pipeline, consisting of five steps: (1) Source Image Collection, gathering images from various benchmarks; (2) Personal Background Generation, creating user personas with specific goals; (3) Multi-Turn Conversation Generation, where an AI assistant analyzes model performance based on evaluation tables; (4) Manual Reviewing, assessing response correctness and relevance; and (5) Checklist Generation, ensuring quality and coherence in AI responses. creasingly complex user queries with higher lexical diversity. For the automatic evaluation of MULTIVERSE, we adopt checklist-based evaluation metric inspired by prior works [33, 41], which utilize checklist items to assess openended generation tasks. Specifically, we employ GPT-4o as the evaluator VLM to measure response quality. Each checklist consists of multiple binary questions covering 37 key aspects, including perceptual understanding and factual correctness, as illustrated in Figure 1. We evaluate 18 VLMs, including both open-source and proprietary models, on MULTIVERSE. Our findings reveal that most VLMs struggle with multi-turn interactions, with even the strongest model, GPT-4o, achieving less than 50% performance. This highlights fundamental challenge in sustained dialogue reasoning. Interestingly, providing ground-truth dialogue history significantly enhances reasoning capabilities, even for smaller or weaker VLMsdemonstrating clear in-context learning effect that enables models to adapt and refine their responses over time. Additionally, scaling up model size consistently improves multi-turn interactivity, reinforcing the importance of model capacity in complex conversational tasks. Finally, we verify that our evaluation metrics remain robust against verbosity bias, ensuring reliable and fair assessment across models. These insights shed light on the limitations of current VLMs and underscore the necessity of improving contextual understanding in long-form interactions. 2. MULTIVERSE In this section, we will describe how we construct MULTIVERSE, with an overview shown in Figure 3. 2.1. Dataset Construction Step 1: Source Image Collection. We began by collecting source images spanning diverse topics (e.g. science, mathematics, and natural scenes) from 12 existing evaluation benchmarks: MMDU [47], GroundUI-1K [79], MMMU [73], MMMU-Pro [74], NaturalBench [37], VisITBench [6], MathVista [50], MM-Vet [71], MM-Vet v2 [72], CharXiv [67], MMBench (Eng) [46], and MegaBench [8]. In total, 49,700 images were obtained from the test sets of these benchmarks. We then applied rigorous three-step data refinement process to ensure high-quality selections: (1) Deduplication: We used pHash to detect and remove duplicate images, resulting in 13,998 images (29.1%) being discarded; (2) Image Quality Scoring: We employed GPTFigure 4. Detailed distribution of (a) Task, (b) Interaction Goal, and (c) Image Domain in MULTIVERSE. 4o-2024-11-20 to rate each image on 15 scale in terms of clarity, resolution, and real-world plausibility. Images scoring below 5 in any category were removed (21,808 images, 64.77%). See the Appendix for examples of excluded lowquality images; (3) Image Category Classification: We next asked GPT-4o to classify the category of each image, resulting in 57 categories obtained. The most common categories were Charts and Graphs (22.92%) and Diagrams and Schematics (11.17%). Categories with fewer than 50 images were excluded (213 images, 1.80%); (4) Weighted Image Category Sampling: To ensure our benchmark remains compact and accessible for other researchers to evaluate without excessive computational burden, we maintained relatively small scale. Accordingly, we performed weighted random sampling of images from each category, proportional to the overall category distribution, while capping the total number of selected images at 1K. Step 2: Personal Background Generation. To construct more plausible and realistic multi-turn conversations, we draw inspiration from real-world scenarios. Most people typically engage in dialogue with specific goal in mindsuch as resolving problem by providing an image alongside relevant queryand these problems often relate to the users situational context and personal background (e.g. age, profession, hobbies). Hence, before generating the multi-turn conversation based solely on the image, we first create fictional character and scenario context to represent multi-turn exchange between this character and an AI assistant, along with goal tied to the scenario, using GPT-4o.2 Leveraging the personal background-toconversation approach [19, 23, 34] enables the construction of multi-turn conversation datasets with LLMs, leading to richer, more diverse, and higher-quality dialogues. 2In pilot experiments, we observed that generating multi-turn conversations solely from an image led to less diverse and lower-quality outcomes. Step 3: Multi-turn Conversations Generation. Building on the fictional character, scenario context, and defined goal, we use GPT-4o to generate multi-turn conversations between the character and the AI assistant. These conversations follow four key principles: Detailed & Informative Responses: The assistants responses must be detailed, specific, expert-level, and highly informative. Increasing Complexity: Each subsequent user query becomes progressively more challenging, creative, and complex, as motivated by prior work [45]. Diverse Linguistic Styles: To emulate realistic human interactions, user utterances vary in linguistic style instead of relying on task-specific prompts commonly seen in existing benchmarks [47] (e.g. describe, what is, where is). Conversation Length: Each conversation consists of four turns, yielding total of eight utterances (four from the user and four from the assistant). In total, we obtained 839 multi-turn conversations after removing 161 samples due to degeneracy issues with GPT-4o, such as parsing failures or insufficient turn length (<6). Step 4: Manual Reviewing. To build realistic and rigorous multi-turn conversation benchmark, we manually review the generated conversations according to three primary criteria: Naturalness & Realism: We remove any conversation that does not flow realistically or contains user queries that are implausible given the image. For example, if the image is complex scatter plot with vibrant color dots, yet the users query solely focuses on the color rather than the actual content of the plot, we deem it unrealistic and remove the sample. Correctness: We conservatively remove conversations if the AI assistants response is clearly incorrect or provides misleading information."
        },
        {
            "title": "Statistic",
            "content": "Total Conversations Unique number of images Avg./Max. number of turns Interaction goal/sub-goal classes Image main/sub classes Task main/sub classes Avg./Max. question length Avg./Max. answer length Avg./Max. question diversity Avg./Max. answer diversity"
        },
        {
            "title": "Number",
            "content": "647 647 3.91/4 8/484 25/384 9/484 30.53/70 221.51/742 111.97/235.48 117.96/230.48 Unique number of checklist items Unique number of main/sub key aspect 21995 37/10115 Table 1. Statistics of MULTIVERSE. Lexical diversity of questions and answers is measured using MTLD [52], while question and answer length is computed with the LLaMA-3.1-8B tokenizer. Blindness: Inspired by prior works [12, 37] addressing the blind issue, we remove conversations in which the AI assistant can sufficiently solve the users query without referencing the provided image. Following this process, we retained 647 conversations and removed 192. Specifically, 48 were removed for unnatural flow, 65 for incorrect responses, and 104 for blindnessrelated issues, with some overlap across these criteria. 2.2. Analysis of MULTIVERSE Basic Statistics. As shown in Table 1, MULTIVERSE consists of 647 conversations, averaging 3.91 turns per conversation. It spans 8 primary interaction goals and 484 subinteraction goals, allowing for comprehensive evaluation of VLMs ability to handle diverse and realistic user interactions. Additionally, MULTIVERSE covers nine main tasks with 484 corresponding sub-tasks. MULTIVERSE exhibits high lexical diversity in both queries and responses, suggesting that it necessitates advanced reasoning to effectively address complex user queries. Detailed Distributions. Figure 4 presents the detailed distribution of (a) tasks, (b) interaction goals, and (c) image domains in MULTIVERSE. (a) MULTIVERSE exhibits high proportion of reasoning tasks, followed by knowledgerelated tasks. Compared to MMDU and ConvBench, MULTIVERSE includes more advanced reasoning tasks (e.g., (b) MULTImathematics, charts and diagrams, coding). VERSE highlights the diversity of interaction goals in MULTIVERSE, where Analysis is the primary motivation for multi-turn interactions in MULTIVERSE, followed by Verification, both of which reflect realistic and practical interaction scenarios. (c) MULTIVERSE contains substantial number of image domains, with Charts and Graphs Figure 5. Correlation between the checklist completion ratio and quality assessment. The red line represents the best-fit linear regression, indicating strong positive correlation (R2 = 0.44) between the two metrics. being the most predominant, followed by Diagrams and Schematics. These distributions demonstrate MULTIVERSEs capacity to comprehensively evaluate VLM capabilities across diverse and challenging tasks, realistic interaction goals, and broad spectrum of image types. 2.3. Automatic Evaluation Process Recently, the use of powerful LLMs/VLMs (e.g., GPT-4o) as evaluators has become standard approach for assessing response quality in open-ended tasks [24, 25, 32, 78]. This is typically done by assigning an integer score between 1 and 10 to measure response quality. While this method is both straightforward and effective, recent studies [33, 41] have raised concerns about its robustness. Therefore, checklist-based evaluation methods have been introduced to improve reliability and interpretability. In this work, we adopt an instance-specific, checklist-based evaluation approach to provide more interpretable and robust evaluation results. Specifically, we generate unique checklist for each user query at every turn in multi-turn conversation. Each checklist consists of multiple binary questions, with each question focusing on specific key aspect (e.g. perception, factual correctness). To ensure quality, we generate these checklists using GPT-4o and Claude-3.5-Sonnet, followed by manual review conducted by the authors to validate their accuracy and reliability. Evaluation Metrics. We employ GPT-4o-2024-11-20 as the evaluator VLM, prompting it to assess the quality of generated responses at every turn in multi-turn conversation based on checklist and reference answer 3. The 3Existing studies have demonstrated that providing reference answer improves the correlation between human and model judgments. Therefore, we incorporate reference answer in our evaluation."
        },
        {
            "title": "Models",
            "content": "Turn 1 Turn 2 Turn 3 Turn 4 Avg. InternVL2.5-1B InternVL2.5-2B InternVL2.5-4B Qwen2.5-VL-3B LLaVA-1.5-7B LLaVA-NeXT-7B LLaVA-OneVision-7B Qwen2-VL-7B Qwen2.5-VL-7B InternVL2-8B InternVL2.5-8B LLaMA-3.2-11B-Vision Qwen2.5-VL-72B LLaMA-3.2-90B-Vision Gemini-2.0-Flash Claude-3.5-Sonnet Claude-3.7-Sonnet GPT-4o 13.93 17.79 27. 32.80 9.10 13.59 24.83 35.93 45. 32.09 22.90 14.38 52.05 24.92 42. 46.60 43.38 48.56 21.36 28.96 37. 37.41 26.43 28.25 34.47 37.99 47. 39.37 37.85 20.02 47.72 37.63 49. 47.16 44.39 50.28 23.51 30.80 38. 38.55 29.14 29.94 37.72 39.35 49. 38.98 38.89 25.41 45.82 38.99 51. 48.30 44.77 50.54 26.08 34.32 39. 44.05 31.81 33.07 37.81 43.10 50. 38.88 42.15 26.63 46.19 41.58 48. 45.00 43.70 49.12 21.22 27.97 35. 38.20 24.12 26.21 33.71 39.09 48. 37.33 35.45 21.61 3.86 5.14 3. 3.49 7.08 6.01 4.22 2.29 1. 2.00 5.88 4.21 47.95 -1.95 35. 5.13 47.76 2.10 46.76 -0.37 44. 49.63 0.13 0.19 Figure 6. Average performance of 18 VLMs under two different evaluation settings: Oracle and Self-Prediction. Table 2. Average performance of 18 VLMs across multiple turns in MULTIVERSE under the Oracle setting. represents the slope of the performance increase as interactions progress. Figure 7. Comparison performance across 19 different VLMs between Oracle and Self-Prediction settings. prompt template used for evaluation is provided in the ApIntuitively, we posit that high-quality response pendix. should successfully satisfy the checklist items while achieving higher integer score on 1 to 10 scale. Based on this premise, our metric consists of two sub-metrics: Checklist Completion Ratio: This measures how well the generated response addresses the given checklist items by calculating the ratio of Yes. Quality Assessment: This assigns an integer score between 1 and 10 to evaluate the overall quality of the response. The score is then scaled up from 110 to 10100 for better interpretability. As shown in Figure 5, Checklist Completion Ratio and Quality Assessment exhibit strong positive correlation, indicating that responses meeting more checklist items tend to receive higher quality scores. Based on this, our final score is computed as their product. 3. Experiments 3.1. Experimental Setup Evaluated Models. We evaluate in total 18 VLMs, cat- (i) Proprietary models: GPT-4o-2024egorized into: 11-20 [18], Claude-3.5-Sonnet-2024-10-22 [3], Claude3.7-Sonnet-2025-02-19 [2], Gemini-2.0-Flash-001 [16]; (ii) Open-source models: LLaVA-1.5-7B [43], LLaVANeXT-7B [42], LLaVA-OneVision-7B [38], Qwen2-VL7B [64], Qwen2.5-VL-{3, 7, 72}B-Instruct [5], LLaMA3.2-{11, 90}B-Vision-Instruct [53], InternVL2-8B [10], InternVL2.5-{1,2,4,8}B [11]. Evaluation Settings. Since MULTIVERSE is multiturn conversational benchmark, the dialogue history significantly impacts response quality. To ensure fair comparison of different VLMs, we adopt the ground-truth dialogue history provided by MULTIVERSE as the default evaluation setting, Oracle. This approach is widely used in existing multi-turn conversation benchmarks [4, 45]. However, relying solely on ground-truth dialogue history may introduce biases, as variations in linguistic style and model capacity can disproportionately affect performance across different VLMs. Therefore, we additionally assess VLMs under the Self-Prediction setting, in which the model generates its own dialogue history. 3.2. Experimental Results and Analysis Powerful VLMs still struggle with multi-turn interactions. As shown in Table 2, all VLMs exhibit relatively low performance (< 50%), indicating that multi-turn interactions in MULTIVERSE remain challenging, even for high-performing VLMs in the Oracle setting. GPT-4o achieves the highest average performance. Overall, proprietary VLMs generally outperform open-source VLMs, with the exception of the Qwen2.5-VL series, which notably surpasses Claude-3.5-Sonnet, Claude-3.7-Sonnet, and Gemini-2.0-Flash. Additionally, Claude-3.7-Sonnet scores 2.7% lower than Claude-3.5-Sonnet. Among open-source models, LLaMA-3.2-11B-Vision achieves the lowest performance, performing even worse than LLaVA-1.5-7B. As interaction progresses, providing ground-truth dialogue context unlocks the reasoning capabilities. As shown in Figure 6, overall performance steadily improves across multiple turns in the Oracle setting. This result suggests that incorporating the golden dialogue history helps VLMs better understand contextual nuances, serving as guiding mechanism for resolving user queries in long-term conversations. By leveraging the golden dialogue, even smaller or weaker VLMs can adapt to similar linguistic styles and phrases from context, leading to improved performance. Further analysis in Table 2 presents the performance improvement of each model across multiturn interactions by measuring the slope (r). Overall, most VLMs benefit from extended interactions, demonstrating the effects of in-context learning. Interestingly, smaller or weaker VLMs experience the most significant gains, indicating that the ground-truth dialogue history in MULTIVERSE helps unlock their reasoning capabilities by providing strong contextual guidance. However, Qwen2.5VL-72B and Claude-3.5-Sonnet do not show improvement when using the ground-truth dialogue history. This negative slope may be attributed to linguistic differences between these models and GPT-4o, which serves as the reference dialogue in MULTIVERSE. of VLMs Self-Prediction. Figure 7 compares vs. Oracle under Oracle and performance the Self-Prediction. Overall, models exhibit significantly improved responses in the Oracle setting compared to Self-Prediction that relies on selfleading to substantial generated dialogue history, performance gap (with maximum drop of -44.64% observed in Qwen2.5-VL-72B). These findings indicate that providing ground-truth dialogue history serves as form of in-context learning, effectively guiding models to generate responses that are more closely aligned with reference answers. Interestingly, Qwen2.5-VL-Series models (7B and 72B) demonstrate notably larger performance gap compared to other VLMs (-30.44 for 7B, -44.64 for 72B). This suggests that the backbone language models of Qwen2.5-VL and GPT-4o may exhibit distinct linguistic styles or response patterns. These results underscore the critical role of supplying precise and coherent dialogue context to enhance overall model performance. VLMs demonstrate selective interactivity capabilities. As shown in Table 3, most VLMs exhibit strong perfor- ). However, mance in analysis ( their performance in optimization ( ) taskstypically demanding innovative thinkingremains ) and understanding ( ) and research ( Models InternVL2.5-1B InternVL2.5-2B InternVL2.5-4B 20.37 24.55 23.98 15. 16.95 22.03 18.36 22.34 26.67 31. 24.82 22.74 26.23 28.70 26.51 27. 34.43 38.56 32.14 31.56 34.46 40. 29.65 35.17 Qwen2.5-VL-3B 36.08 43.19 34. 29.43 37.61 43.46 32.21 35.91 LLaVA-1.5-7B 20.34 26.58 26.27 19.57 14.11 23. 28.70 27.59 LLaVA-NeXT-7B 21.78 30.36 28. 21.75 14.26 25.41 29.06 29.16 LLaVA-OneVision-7B 30.55 36.29 31.58 27.97 31.84 37. 31.73 36.14 Qwen2-VL-7B 36.28 43.66 35. 31.62 35.77 43.54 35.79 37.13 Qwen2.5-VL-7B 46.14 54.23 44.23 40.64 44.86 54. 39.73 44.45 InternVL2-8B 34.85 42.06 33. 28.76 33.74 41.76 33.19 37.96 InternVL2.5-8B 33.55 38.39 32.95 29.92 34.45 39. 30.64 36.98 LLaMA-3.2-11B-Vision 17.94 23.30 20. 18.84 16.51 22.48 23.41 26.03 Qwen2.5-VL-72B 45.58 52.10 42.86 42.59 47.15 55. 43.32 39.82 LLaMA-3.2-90B-Vision 33.96 38.19 33. 32.17 34.68 39.03 32.57 35.84 Gemini-2.0-Flash 43.68 52.25 40.73 44.46 48.92 54. 42.58 42.17 Claude-3.5-Sonnet 46.15 50.94 43. 40.43 47.72 53.95 37.99 41.08 Claude-3.7-Sonnet 42.58 47.43 43.18 38.83 45.33 51. 38.56 34.65 GPT-4o 46.80 53.67 42. 46.36 50.54 56.41 43.50 44.51 Table 3. Performance of 18 VLMs in MULTIVERSE for 8 differ- ), exploration ent interaction goalsverification ( ( ), research ( )under Oracle setting. ), understanding ( ), optimization ( ), and creation ( ), calculation ( ), analysis ( comparatively weak. Among these, the Qwen2.5-VL series demonstrates relatively stronger interactivity across all interaction goals. On the other hand, weaker or smaller VLMs with lower performance, such as LLaVA-1.5-7B and LLaVA-NeXT-7B, struggle particularly with verifiable tasks, including optimization ( ) and calculation ( ). Effect of Model Scaling. Figure 8 presents performance comparisons across varying model sizes in three differInternVL2.5, LLaMA-3.2-Vision, and ent VLM groups: larger models exhibit higher Qwen2.5-VL. In general, tasks. However, within overall performance on most the Qwen2.5-VL series, increasing the model size does not always guarantee improvements, and its impact appears task-dependent. For instance, Qwen2.5-VL-72B achieves stronger reasoning on verifiable tasks (e.g. mathematics, coding, and interpreting charts & diagrams), while Qwen2.5-VL-7B shows advantages in creative thinking. These findings suggest that larger models generally offer enhanced interactivity capabilities, although the benefit may vary based on the specific task. Does automatic evaluation metric possess verbosity bias? We explore whether our evaluation metric favors longer answers across multi-turns. As shown in Figure 9, the linear correlation (R2) between length of response and performance from GPT-4o is weaken as interaction increases (until turn 3). As interaction progress until turn 3, the p-value increases and R-value decreases, which suggests that our evaluation metric could reduce verbosity bias and length dependency. Figure 8. Performance comparison across different model sizes in three model families: InternVL2.5, LLaMA-3.2-Vision, and Qwen2.5VL. The radar charts show that larger models generally perform better across tasks, but scaling effects vary. Notably, Qwen2.5-VL-72B excels in structured reasoning tasks (e.g., mathematics, coding), while Qwen2.5-VL-7B shows stronger creative abilities, highlighting task-dependent scaling impacts. Figure 9. Linear correlation (R2) between response length and performance from GPT-4o. 4. Related Work Single-Turn Evaluation Benchmarks. To comprehensively evaluate multiple vision-language capabilities of recent powerful VLMs, many benchmarks have been introduced. These benchmarks assess integrated capabilities (i.e. perception and reasoning) [9, 14, 35, 36, 46, 56, 68, 7174, 77], text-richness (e.g. charts, diagrams) [22, 51], mathematical reasoning [21, 50, 76], scientific understanding [49], perception-specialized tasks that are easy for humans to perceive [15, 37, 55, 57, 62, 63, 75], and web & mobile applications [44]. Despite their breadth, most existing benchmarks rely on single-turn evaluationstypically in the form of binary, multiple-choice, or free-form responsesthus failing to capture the effectiveness of VLMs in multi-turn interaction. This gap highlights the need for new benchmark that better reflects practical, real-world conversational scenarios. Multi-Turn Evaluation Benchmarks. In the NLP domain, numerous multi-turn conversation benchmarks have recently been introduced to assess the interactive capabilities of LLMs [4, 17, 26, 41, 59, 66, 78]. By contrast, only few benchmarks exist for evaluating the multi-turn interaction capabilities of VLMs, such as MMDU [47] and ConvBench [45]. However, these existing benchmarks still fall short of assessing VLM performance in real-world humanAI interactions. In particular, they do not address more advanced and complex reasoning tasks (e.g. verifying equations shown in the image), and most queries can be resolved through straightforward statements rather than more elaborate formats, such as code or mathematical expressions. These limitations hinder comprehensive evaluation of VLMs under wide range of scenarios, underscoring the need for richer, more rigorous multi-turn conversation benchmark. 5. Conclusion In this work, we introduce MULTIVERSE, new multiturn conversation benchmark designed to evaluate VLMs. Our dataset encompasses diverse range of interaction goals (e.g. verification, analysis) and tasks (e.g. reasoning, mathematics), providing realistic and challenging testbed for assessing the multi-turn interactive capabilities of VLMs. Moreover, we propose an instance-specific, checklist-based evaluation framework comprising two key sub-metrics: checklist completion ratio and quality assessment. We reveal that even recent state-of-the-art VLMs struggle with handling multi-turn interactions effectively."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (RS2022-II220641, XVoice: Multi-Modal Voice Meta Learning)"
        },
        {
            "title": "References",
            "content": "[1] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com, 2024. 14 [2] Anthropic. Claude 3.7 sonnet system card. 2025. 6 [3] AI Anthropic. Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 3(6), 2024. 2, 6 [4] Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, et al. Mt-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv preprint arXiv:2402.14762, 2024. 2, 6, 8 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 2, 6 [6] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schmidt. Visit-bench: benchmark for visionlanguage instruction following inspired by real-world use. arXiv preprint arXiv:2308.06595, 2023. 2, 3, [7] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for multimodal llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1381713827, 2024. 14 [8] Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks. arXiv preprint arXiv:2410.10563, 2024. 2, 3, 14 [9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 8 [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 6 [11] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 6 [12] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 2, 5 [13] et al. Erfei Cui. Sharegpt-4o: Comprehensive multimodal annotations with gpt-4o, 2024. https://sharegpt4o. github.io/. 14 [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 2, 8 [15] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 2, 8 [16] Gemini. Gemini 2.0 flash. 2024. 6 [17] Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, et al. Multi-if: Benchmarking llms on multiturn and multilingual instructions following. arXiv preprint arXiv:2410.15553, 2024. 2, 8 [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 6, [19] Jihyoung Jang, Minseong Boo, and Hyounghun Kim. Conversation chronicles: Towards diverse temporal and rearXiv lational dynamics in multi-session conversations. preprint arXiv:2310.13420, 2023. 2, 4 [20] Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, and Ying Shen. Enhancing multimodal large language models with vision detection models: An empirical study. arXiv preprint arXiv:2401.17981, 2024. 14 [21] Amita Kamath, Jack Hessel, and Kai-Wei Chang. Whats up with vision-language models? investigating their struggle with spatial reasoning. arXiv preprint arXiv:2310.19785, 2023. 8 [22] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is In Computer VisionECCV 2016: worth dozen images. 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. 8 [23] Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, et al. Soda: Million-scale dialogue distillation with social commonsense contextualization. arXiv preprint arXiv:2212.10465, 2022. 2, 4 [24] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing finearXiv grained evaluation capability in language models. preprint arXiv:2310.08491, 2023. [25] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models, 2024. 5 [26] Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. Mt-eval: multi-turn capabilities evaluation benchmark for large language models. arXiv preprint arXiv:2401.16745, 2024. 2, 8 [27] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 14 [28] Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, and Yong Man Ro. Trol: Traversal of layers for large language and vision models. arXiv preprint arXiv:2406.12246, 2024. 14 [29] Byung-Kwan Lee, Chae Won Kim, Beomchan Park, and Yong Man Ro. Meteor: Mamba-based traversal of rationale for large language and vision models. arXiv preprint arXiv:2405.15574, 2024. 14 [30] Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and Yong Man Ro. Collavo: Crayon large language and vision model. arXiv preprint arXiv:2402.11248, 2024. [31] Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and intelligence arXiv preprint Yong Man Ro. for large language and vision models. arXiv:2403.07508, 2024. 14 Moai: Mixture of all [32] Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, and Minjoon Seo. Prometheus-vision: Vision-language model as judge for fine-grained evaluation, 2024. 5 [33] Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, and Pilsung Kang. Checkeval: Robust evaluation framework using large language model via checklist. arXiv preprint arXiv:2403.18771, 2024. 3, 5 [34] Young-Jun Lee, Dokyong Lee, Junyoung Youn, Kyeongjin Oh, Byungsoo Ko, Jonghwan Hyeon, and Ho-Jin Choi. conversation Stark: arXiv preprint with persona commonsense knowledge. arXiv:2407.03958, 2024. 2, 4 long-term multi-modal"
        },
        {
            "title": "Social",
            "content": "[35] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 8 [36] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. 8 [37] Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating vision-language models on natural adversarial samples. arXiv preprint arXiv:2410.14669, 2024. 2, 3, 5, 8, 14 [38] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 6, 14 [39] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 14 [40] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. [41] Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from real users in the wild. arXiv preprint arXiv:2406.04770, 2024. 2, 3, 5, 8 [42] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 6, 14 [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 6 [44] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024. 8 [45] Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi Shao, et al. Convbench: multi-turn conversation evaluation benchmark with hierarchical capability for large visionlanguage models. arXiv preprint arXiv:2403.20194, 2024. 2, 4, 6, 8 [46] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 3, 8, 14 [47] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, et al. Mmdu: multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms. arXiv preprint arXiv:2406.11833, 2024. 2, 3, 4, 8, 14 [48] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. 14 [49] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning In The via thought chains for science question answering. 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. [50] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 2, 3, 8, 14 [51] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 8 [52] Philip McCarthy and Scott Jarvis. Mtld, vocd-d, and hd-d: validation study of sophisticated approaches to lexical diversity assessment. Behavior research methods, 42(2):381 392, 2010. 5 [53] Meta. Llama 3.2 vision. 2024. 6 [54] OpenAI. Gpt-4v(ision) technical work and authors, 2023. https://openai.com/contributions/gpt-4v, Last accessed on 2024-02-13. 14 [55] Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, and Tong Zhang. Personalized visual instruction tuning. arXiv preprint arXiv:2410.07113, 2024. [56] Renjie Pi, Jianshu Zhang, Jipeng Zhang, Rui Pan, Zhekai Chen, and Tong Zhang. Image textualization: An automatic framework for creating accurate and detailed image descriptions. arXiv preprint arXiv:2406.07502, 2024. 8 [57] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. arXiv preprint arXiv:2407.06581, 2024. 8 [58] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 14 [59] Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: realistic multi-turn conversation evaluation arXiv preprint benchmark challenging to frontier llms. arXiv:2501.17399, 2025. 2, 8 [60] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 24432449, 2021. 2 [61] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 14 [62] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [63] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 8, 14 [64] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 [65] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 14 [66] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691, 2023. 2, 8 [67] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521, 2024. 2, 3, [68] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023. 8 [69] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 14 [70] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. 14 [71] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 2, 3, 8, 14 [72] Weihao Yu, Zhengyuan Yang, Linfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765, 2024. 3, 14 [73] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 2, 3, 14 [74] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. Mmmu-pro: more robust multiarXiv discipline multimodal understanding benchmark. preprint arXiv:2409.02813, 2024. 2, 3, 8, [75] Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, and Yi Fung. Vlm2-bench: closer look at how well vlms implicitly link explicit matching visual cues. arXiv preprint arXiv:2502.12084, 2025. 8 [76] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. 8 [77] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world arXiv preprint scenarios that are difficult for humans? arXiv:2408.13257, 2024. 8 [78] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 2, 5, 8 [79] Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, and Shuicheng Yan. Agentstudio: toolkit for building general virtual agents. arXiv preprint arXiv:2403.17918, 2024. 3,"
        },
        {
            "title": "Appendix Contents",
            "content": "A. Additional Related Works B. Additional Details of MULTIVERSE C. Further Analysis of MULTIVERSE . C.1. Interaction Goal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D. More examples on MULTIVERSE E. Prompt Templates used for MULTIVERSE 14 14 15 . 15 17 A. Additional Related Works Recent advancements in LLVMs have predominantly adopted simplistic yet highly effective architectures, notably through the model-stitching concept. Numerous prior studies have introduced various design modifications to bridge the performance gap with closed-source LLVMs, such as GPT-4v [54], GPT-4o [18], Gemini-Pro [61], and Claude-3 family [1]. These efforts include focusing intently on high-resolution processing [40, 42, 58], implementing locality-enhanced projectors [7], and incorporating knowledge embeddings [29], layer traversal technique [28] and leveraging diverse array of vision encoders [48, 63] have also been explored. Additionally, integrating external, task-specific computer vision modules [20, 27, 30, 31] and incorporating different modalities including video and audio [13, 38, 65, 69] have expanded the models capabilities. Moreover, enabling the handling of interleaved input formats [39, 70] has further broadened the versatility of these models. B. Additional Details of MULTIVERSE"
        },
        {
            "title": "Datasets",
            "content": "# Images"
        },
        {
            "title": "Licenses",
            "content": "CC-BY-NC-4.0 Apache License 2.0 MMDU [47] MegaBench [8] GroundUI-1K [79] MMMU [73] MMMU-Pro [74] NaturalBench [37] VisIT-Bench [6] MathVista [50] MM-Vet [71] MM-Vet v2 [72] CharXiv [67] MMBench (Eng) [46] 375 12091 1000 2015 3800 574 6141 218 2323 6718 https://huggingface.co/datasets/laolao77/MMDU https://huggingface.co/datasets/TIGER-Lab/MEGA-Bench https://huggingface.co/datasets/agent-studio/GroundUI-1K"
        },
        {
            "title": "MIT",
            "content": "https://huggingface.co/datasets/MMMU/MMMU https://huggingface.co/datasets/MMMU/MMMU_Pro https://huggingface.co/datasets/BaiqiL/NaturalBench Apache License 2.0 Apache License 2.0 Apache License 2. https://huggingface.co/datasets/mlfoundations/VisIT-Bench CC-BY-4.0 https://huggingface.co/datasets/AI4Math/MathVista https://huggingface.co/datasets/whyu/mm-vet https://huggingface.co/datasets/whyu/mm-vet-v2 https://huggingface.co/datasets/princeton-nlp/CharXiv CC-BY-SA-4. CC-BY-NC-4.0 CC-BY-NC-4.0 CC-BY-SA-4.0 https://huggingface.co/datasets/lmms-lab/MMBench_EN Apache License 2."
        },
        {
            "title": "All Images",
            "content": "48100 - - Table 4. Detailed information on the 16 source datasets collected for MULTIVERSE. In Step 1, we collect 55,500 images from 16 existing evaluation benchmarks. The detailed statistics, including the number of images, public links, and license information, are provided in Table 4. License. All evaluation benchmarks permit non-commercial use, allowing us to preprocess and redistribute the data. However, certain datasets, such as MathVista and CharXiv, are licensed under CC-BY-SA-4.0. This requires us to comply with the same licensing terms when publicly deploying MULTIVERSE. C. Further Analysis of MULTIVERSE C.1. Interaction Goal Verification (87) : Data Validation, Data Consistency, Statistical Significance, Financial Verification, Graph Intersection Points, Cost Accounting Verification, Strategy Validation, Equation and Calculation Verification, Answer Verification, Image Details Verification, Term Definition Matching, Mathematical Theorem and Calculation Verification, Graph Interpretations, Verification of Calculations and Materials, Anatomical Details Verification, Molecular Structure and Properties, Design Verification, Mechanical Dynamics Calculations, Image Verification, Graph Interpretation, Sequence Evaluation, Dimension Verification, Financial Viability, Biopsy Image Analysis, Geometric Problem Verification, Feedback Control System Design, Data Interpretation Verification, Data Accuracy Verification, Equation Verification, Data Interpretation, Structural Calculations, Feedback, Musical Score Interpretation, Location and Meaning, Financial Analysis, Code Verification, Mathematical Properties, Solution Verification, Data Correlation, Graph Theory, Model Interpretation Verification, Interpretation, Alignment with Theoretical Models, Interpretation Verification, Geometric Properties, Molecular Structure Verification, Optimization, Geometry, Compatibility Verification, Geometric Verification, Data Accuracy, Audio Issue Resolution, Verifying Distribution Shape, Logic Verification, Calculation Verification, Data Analysis, Theoretical Implications, Identification, Music Theory, Theoretical Consistency, Molecular Structure, Mathematical Method, Structural Analysis Calculations, Safety and Performance Evaluation, Circuit Calculation Verification, Circuit Verification, Function Identification, Formula Verification, Algorithm Verification, Molecular Structure Confirmation, Feedback Collection, Data Verification, Financial Projections, Confirmation, Enhancement, Mathematical Verification, Feedback Assessment, Parameter Suitability, Diagnosis, Solution Checking, Signal Analysis Verification, Cost Verification, Interpretation Checking, Design Feedback, Thermodynamic Properties, Financial Statement Compliance, Accuracy Verification Analysis (123) : Problem Solving, Chess Endgame Strategy, Weather Patterns, Model Convergence in Option Pricing, Mechanical Stress Distribution, Stress Analysis, Art Analysis, Pattern Recognition, Data Comparison, Trend Analysis, Pattern Detection, Impact of Sparsity on Data Recovery, Disease Outbreak Factors, Risk-Return Trade-offs, Schedule Optimization, Connectivity Analysis, Graph Data Interpretation, Statistical Methods Comparison, Travel Data Analysis, Impact Assessment, Technique Evaluation, Performance Metrics, Performance Comparison, Classification, Music Analysis, Health Disparities, Investment Strategy Analysis, Cost and Time Implication Analysis, Cost Optimization, Model Interpretation, Economic Analysis, Chess Endgame Analysis, Feedback Evaluation, Data Interpretation, Probability Distribution Models, Performance Interpretation, Performance Analysis, Bias Detection, Design Elements Assessment, Financial Analysis, Game Evaluation, Data Pattern Interpretation, Architectural Analysis, Economic Impact, Model Performance Evaluation, Geometric Pattern Analysis, Mechanical Forces, Diagnosing Plant Issues, Strategic Planning, Artistic Elements and Symbolism in Manga, Design Evaluation, Material Evaluation, Price Comparison, Trade-offs Assessment, Measurement Suitability, Causation Analysis, Interpretation, Geometric Relationships Analysis, Traffic and Urban Design, Investment Options, System Behavior Analysis, Behavioral Analysis and Health Assessment, Medical Condition Assessment, Project Management Insights, Symbolism Identification, Equity Changes, Force Distribution Study, Property Interpretation, Artistic Analysis, Strategy Evaluation, Risk-Return Trade-off, Ecosystem Health Assessment, Demographic Analysis, Financial Health Analysis, Data Analysis, Decision-making Analysis, Identification, Correlation Analysis, Risk and Return, Musical Analysis, Suitability Assessment, Design Elements, Trend Identification, Curve Identification, Strategic Analysis, Optimization Analysis, Statistical Analysis, Memory Management, Market Research, Function Identification, Spectral Analysis, Electricity Usage Analysis, Comparative Analysis, Performance and Robustness Evaluation, Enhancing Engagement Strategies, Summarization, Nuclear Reaction Analysis, Comparative Evaluation, Data Trends, Chess Strategy, Quantum Mechanics, Function Behavior Analysis, Financial Market Analysis, Cause Identification, System Stability, Research Analysis, Diagnosis, Empirical vs Theoretical Comparison, Investment Strategy Performance, Damage Assessment, Sales Trend Analysis, Medical Analysis, Infection Spread Dynamics, Interpretation of Experimental Data, Pattern Analysis, Sustainability, Visual Techniques, Comparison, Performance Metrics Interpretation, Data Visualization, Feedback on Composition, Correlation Study, Stock Market Trends Exploration (30) : Instructional Strategies, Idea Generation, Cultural and Historical Exploration, Geometric Properties, Design Elements, Information Gathering, Cultural and Historical Insights, Music Analysis, Ingredient Identification, Music Discovery, Weather and Photography, Geometric Structure Identification, Applications of Geometric Art in Architecture, Design Improvements, Algorithm Development, Species Identification, Model Recommendations, Design and Aesthetics, Comparison and Analysis of Options, Puzzle Solving Strategies, Interpretation of Visual Elements, Feature Discovery, Species Recognition, Assessment of Urban Environmental Features, Design Variations, Art and Design Inspiration, Chemical Compound Identification, Information Discovery, Creative Applications, Coordinate Conversion Optimization (48) : Process Improvement, Readability Enhancement, Algorithm Efficiency, Strategic Improvement, Project Management, Scheduling, Material Selection, Code Quality, Design Efficiency, Pathfinding, Translation Model Performance Enhancement, Improving Visual Appearance, Gameplay Strategy, Code Efficiency, Neural Network Configuration, Team Building, Cost Optimization, Image Enhancement, Inventory Management, Visualization Clarity, Decision Making, Route Efficiency, Rendering Techniques Improvement, Strategy Development, Code Optimization, Process Optimization, Eco-friendliness, Investment Portfolio, Clarity Enhancement, Pricing Strategy Optimization, Integration of auditory and visual elements, Schedule Optimization, Resource Allocation, Enhancing Real Estate Listings, Visual Design, System Efficiency Enhancement, Strategic Planning, Code Efficiency and Readability, Price Comparison, Resource Management, Workflow Efficiency, Organization, Improve Data Visualization, Script Reliability, Data Visualization, Robot Movement Strategy, Efficiency Enhancement, Vibration Isolation Methods Calculation (32) : Financial Calculation, Problem Solving, Arithmetic Problem, Project Management, Geometry Problem Solving, Geometric Calculation, Investment Evaluation, Geometry, Financial Computation, Structural Analysis, Force Calculation, Finance, Cost Analysis, Decision Making, Investment Returns, Weighted Average Calculation, Pressure Difference Calculation, Geometry Calculation, Financial Analysis, Investment Performance Analysis, Equilibrium Calculation, Trajectory Calculation, Load Analysis, Financial Modeling, Statistical Metric Calculation, Solving Equations, Interpolation, Structural Engineering, Cost Estimation, Stress Intensity Factors, Trigonometry, Geometrical Calculation Understanding (86) : Concept Clarification, Mechanics, Comprehension of Data Visualizations, Technical Concepts, Enrollment Disparities, Chemical Properties and Applications, Scientific Concepts, Model Performance, Implications of OOD detection in medical predictions, System Settings, Electrical Circuit Analysis, AI Capabilities, Mathematical Relationships, Material Impact, Comprehension of Concepts, Comprehension of Biological Systems, Comprehension of Graph Data, Thermodynamics Concepts, Solution Process, Educational Trends, Learning how to create geometric patterns, Conceptual Differences, Graph Interpretation, Deciphering, Structural Mechanics, Effects of Data Augmentation, Code Logic and Concepts, Economic Concepts, Behavior and Applications, Function Behavior, Seismic Data Interpretation, Simplification, Advanced Mathematics Problems, Understanding Electrostatic Potential, Data Interpretation, Performance Analysis, System Dynamics, Algorithm Logic Understanding, Comprehension of Construction Techniques, Ecological Roles, Explaining Properties and Applications, Comprehension of Theoretical Concepts, Graph Theory, Structural Engineering, Application of Frameworks, Automotive Dynamics, Historical Context, Geometric Properties, Historical and Cultural Analysis, Concept Explanation, Function and Importance of DVD Layers, Research Methodologies, Functionality Explanation, Plant Growth and Reproduction, Technology Applications, Symbolism Analysis, Sales Insights, Understanding Concurrency Issues, Music Theory Comprehension, Clarification, Historical and Architectural Significance, Geometric Concepts, Biology Concepts, Music Theory, Symbolism in Art, Application of Technology, Color Interaction and Emotional Impact, Circuit Configuration and Components, Cultural Significance, Conceptual Clarification, Conceptual Comprehension, Comparative Analysis, Comprehension of Map Projections, Comprehension of Tree Traversals, Code Structure and Functionality, Chemical Reactions, Behavior Analysis, Reinforcement Learning Policies, Conceptual Understanding, Terminology, Relationship Analysis, Clarification of Processes, Leadership Pyramid Levels, Algorithmic Optimization, Color Theory, Comprehension of Dynamics Research (72) : Data Retrieval, Historical and Route Information Retrieval, Teaching Techniques, Design Insights Gathering, Recipe Acquisition, Historical Information Gathering, Gathering Resources, Information Gathering, Product Identification and Supplier Sourcing, Historical and Architectural Research, Hotel Exploration, Historical Significance, Gathering Information, Historical and Architectural Information, Educational Resources, Economic Trends Analysis, Art Analysis, Historical and Technical Information, Cultural Studies, Information Retrieval, Biological Research, Trend Analysis, Data Interpretation, Species Identification, Art History, Architectural Patterns, Travel Logistics, Cultural and Historical Analysis, Trends Analysis, Historical Insights, Product Information Gathering, Safety Regulations and Design Practices, Ecological Study, Literature Review, Sports Analysis, Identification and Knowledge Acquisition, Art, Historical and Mythological Context, Ecological Information Gathering, Cultural and Historical Research, Film Analysis, Historical and Architectural Details, Gathering Expert Tips, Product Features and Benefits, Identification, Historical Research, Art Style Similarity, Symbiosis in Marine Biology, Design Origins, Historical Analysis, Horticulture, Mentorship and Collaboration Opportunities, Historical Events, Sustainable Design Insights, Historical and Architectural Analysis, Disease Identification and Management, Geological Study, Art Techniques, Historical Economic Impact, Comparison, Art and Artist Analysis, Art Historical Contextualization, Geology, Agricultural Knowledge Gathering, Historical Context, Historical Design Analysis, Cultural and Historical Facts Gathering, Ecology, Historical and Cultural Significance, Collaboration, Music History, Architectural Insights Creation (43) : Explain, Educational Activity Design, Interior Design, Creative Cooking, Design Brainstorming, Design Enhancement, Lyric Writing, Chart Creation, Infographic Design, Lesson Plan Design, Word Association, Design Inspiration, Content Generation, Activity Design, Schedule Design, Design Integration, Developing Photography Exhibit Concepts, Educational Content Development, Interactive Educational Activities Design, Guide Development, Design Generation, Question Development, Content Creation, Educational Material Development, Exercise Development, Data Compilation, Digital Transcription, Discussion Generation, Humor Enhancement, Culinary Innovation, Culinary Creation, Material Enhancement, Recipe Development, Creative Ideation, Cocktail Ingredient Arrangement, Data Visualization, Demonstration Development, Game Development, Data Visualization Enhancement, Engagement, Culinary Innovating, Visualization, Artistic Inspiration and Design D. More examples on MULTIVERSE We present more examples on MULTIVERSE in Figure 10, Figure 11, Figure 12, and Figure 13. Figure 10. An example of MULTIVERSE. Figure 11. An example of MULTIVERSE. Figure 12. An example of MULTIVERSE. Figure 13. An example of MULTIVERSE. E. Prompt Templates used for MULTIVERSE"
        },
        {
            "title": "Prompt Template for Scoring Image Quality",
            "content": "You are given an image. Your task is to evaluate the image quality, which is described as follows: [Image Quality Evaluation] Rate the given image on scale from 1 to 5 based on its quality, considering image clarity, resolution, and likelihood of occurrence in real-world conversations: - Score 1 (Very Low Quality): The image lacks clarity, and objects or content appear blurry with minimal detail, making it unlikely to appear in real conversations. Additionally, this score applies if the image includes specific brand logo or icon or if the image itself has been rotated. - Score 2 (Low Quality): The image is not sharp, with only some objects or elements visible, and is rarely used in real conversations. - Score 3 (Moderate Quality): The image is moderately clear in parts, with sufficient visibility of objects or content to be recognized in specific situations or by certain individuals in conversation. - Score 4 (High Quality): The image is clear, with most objects or elements recognizable, and is likely to appear in real conversations. - Score 5 (Very High Quality): The image is extremely clear, with even small objects or details highly visible, making it highly relevant for frequent use in real conversations. Please generate your answer by strictly following the guidelines below: [Guidelines] - The answer should be formatted as Python dictionary containing the following key: image quality score. - The image quality score should indicate the quality of the given image, range from 1 to 5. [Answer]"
        },
        {
            "title": "Prompt Template for Image Category",
            "content": "You are given an image. Your task is to assign subject/category to the image, which is described as follows: [Image Category Classification] Classify the subject/category of the given image based on its content. Choose from the following categories, along with the corresponding descriptions: - Vehicles and Transportation: Includes all types of transportation, such as cars, bikes, trains, planes, and ships. - Food and Cuisine: Covers cuisine styles, dishes, ingredients, and cooking activities. - People and Lifestyle: Encompasses everyday life, cultural lifestyles, and social activities involving people. - Sports and Recreation: Includes all sports, games, and leisure activities. - Animals and Wildlife: Covers both domestic animals and wildlife, with subcategories like birds, sea creatures, etc. - Objects, Clothing, and Accessories: Encompasses fashion, personal items, and any identifiable object. - Brands and Products: Relates to consumer goods, brand logos, and popular products. - Architecture and Landmarks: Includes buildings, bridges, monuments, and notable geographic features. - Tradition and History: Covers historical sites, artifacts, and traditional practices. - Fine Art and Illustrations: Includes classical paintings, digital art, and creative illustrations. - Celebrities and Public Figures: Specific to well-known individuals in popular culture or politics. - Science and Technology: Includes scientific equipment, laboratories, and research environments. - Chemistry and Lab Equipment: Covers chemicals, laboratory settings, and molecular structures. - Mathematics and Symbols: Encompasses mathematical diagrams, equations, and geometric shapes. - Nature and Landscapes: Includes scenery, forests, mountains, rivers, and natural phenomena. - Healthcare and Medicine: Covers medical devices, doctors, hospitals, and health-related content. - Programming and Coding: Depicts code, software interfaces, and development environments. - Web Design and User Interfaces: Encompasses UI/UX layouts, website mockups, and interface elements. - Mobile and Smart Devices: Includes smartphones, tablets, and other handheld devices. - Weather and Climate: Covers weather events, climate patterns, and related data visualization. - Festivals and Events: Encompasses celebrations, public gatherings, and cultural events. - Education and Learning: Covers classrooms, books, e-learning visuals, and study materials. - Entertainment and Media: Includes visuals from movies, shows, music, and gaming. - Interior Design and Furniture: Relates to home decor, furniture styles, and interior layouts. - Natural Disasters and Environmental Hazards: Includes earthquakes, floods, pollution, and environmental issues. - Abstract and Conceptual Art: Covers non-representational or symbolic images. - Business and Finance: Depicts financial graphs, business meetings, currency, and economy-related visuals. - Law and Government: Includes courts, governmental symbols, legal documentation, and policy. - Diagrams and Schematics: Covers flowcharts, technical diagrams, mind maps, and other schematic representations. - Charts and Graphs: Encompasses data visualizations like bar charts, pie charts, line graphs, and other statistical representations. - Documents and Paperwork: Covers text-heavy documents, official forms, written notes, and reports. - Receipts and Invoices: Includes financial documents, bills, purchase receipts, and transaction records. - Others Please generate your answer by strictly following the guidelines below: [Guidelines] - The answer should be formatted as Python dictionary containing the following key: image category, and image sub category. - The image category should contain the most appropriate classified category of the given image. - The image sub category should specify more detailed sub-category within the selected image category to provide finer level of classification. - If you select Others, then please generate new image category. [Answer]"
        },
        {
            "title": "Prompt Template for Personal Background Generation",
            "content": "You will be provided with an image. Your task is to: 1. Fictional Character Creation - You should create fictional character who plausibly engages in conversational interface with an AI assistant by providing given image. - The fictional character should initiate realistic and contextually appropriate conversation based on the image. - The fictional character should be represented in 2-3 sentences (NOT structured format), covering aspects like name, age, gender, occupation, preference, hobby, like/dislike, or background knowledge (e.g., basic, intermediate, expert). - If there is real human in the given image, please do NOT generate the fictional character corresponding to the real human in the given image. You MUST generate new fictional character who will share the given image with the AI assistant in the conversational interface. 2. Scenario Context Creation - You should create plausible and detailed scenario in which the fictional character interacts with the AI assistant about the given image through conversational interface. - The scenario context should realistically unfold in multi-turn conversation between the fictional character and the AI assistant. - The scenario should be practical, realistic, and engaging, reflecting real-world situations. 3. Goal Creation - You should create specific and realistic goal that the fictional character has when engaging in conversation with the AI assistant through conversational interface by providing given image, in alignment with the generated scenario context. - The goal should be relevant to the scenario context and represented as concise phrase. For example, the assistant might search for information, retrieve data, solve math or coding problem, etc. Please brainstorm the most appropriate, realistic, and highly plausible fictional character, scenario context, and goal that could naturally occur in real-world conversation based on the given image by strictly following the JSON format below: - The answer should be formatted as JSON-formatted Python dictionary containing the following keys: Character Description, Scenario, and Goal. Answer: Prompt Template for Multi-Turn Conversation Generation You will be provided with an image. Your task is to brainstorm creative, realistic and practical multi-turn conversation between fictional character (denoted as [Fictional Character]) and the AI assistant, based on given scenario context (denoted as [Scenario Context]) and the fictional characters goal (denoted as [Goal]). In the conversation, the fictional character interacts with the AI assistant through conversational interface by providing an image. The fictional character, scenario context, and goal are presented as follows: [Fictional Character] {character} [Scenario Context] {scenario} [Goal] {goal} Please brainstorm practical and realistic multi-turn conversation based on the given fictional character, scenario context, goal, and the provided image by strictly following the guidelines below: [Guidelines] - The generated multi-turn conversation should be presented in plain text, where the fictional characters utterances MUST begin with USER:, and the AI assistants utterances MUST begin with ASSISTANT:. - As the conversation progresses, the fictional character should ask progressively more challenging, creative, complex requests, such as follow-up questions, knowledge acquisition, refinement, style/content rephrasing, advanced reasoning expansion, etc. - The fictional characters utterances should be creative, reflect diverse linguistic styles, and include questions that require correct answers to difficult problems. - The AI assistants utterances MUST be very long detailed, specific, expert, helpful, and informative in addressing the fictional characters requests. - The AI assistants utterances should demonstrate advanced cognitive reasoning and do NOT include the fictional characters name. - If the AI assistants utterances involve code, mathematical equations, charts, tables, graphs, scientific concepts, API functions, planning, or any higher-order knowledge, they MUST be highly informative, expert, specific, detailed, and factual. They should also include clear and comprehensive explanations alongside the technical details (e.g., code, mathematical theorems, API function calls, etc.). - Each turn consists of two utterances: one from the fictional character and one from the AI assistant. - The conversation should not end with closing remark like See you next time or similar. - The conversation must consist of four turns (eight utterances), with each turn involving one utterance from the fictional character and one from the AI assistant. Note: - NEVER generate utterances from characters in cases where the AI assistant can resolve them precisely without viewing the given image. - ALWAYS refer to the given image using pronouns (e.g., it, them) in any fictional characters utterances. - NEVER explicitly mention the key entity or information depicted in the image in any fictional characters utterances. [Generated Multi-Turn Conversation]"
        },
        {
            "title": "Prompt Template for Checklist Generation",
            "content": "You will be provided with an image and the users utterances from multi-turn conversation. The conversation takes place in conversational interface where the user and the AI assistant interact while referring to the given image. Your task is to create an instance-specific evaluation checklist that will be used to assess the quality of the AI assistants response to the users query within the multi-turn conversation. In other words, you need to create the evaluation criteria (in question format) that the AI assistants response must meet to be considered the optimal response for the given user query, based on the given image. The checklist should include multiple questions, each satisfying the following conditions: - Each question must be answerable with Yes or No. - Each question must be relevant to one specific evaluation criteria from the provided criteria collection. - Each question should minimize subjectivity in the raters judgment. - Questions should be formulated so that Yes answer is positive evaluation. - Each question must evaluate aspects related to the conditions that need to be metby referring to the given image and each turn of the users queryto produce the optimal response, and these conditions must be strictly satisfied. ### Evaluation Criteria Collection Each evaluation criteria represents main capability to evaluate (alongside the definition): - Engagingness: Measures the models ability to sustain an engaging and interactive conversation by assessing flow, immersion, interactivity, and emotional connection with the user. - Tone & Style Appropriateness: Evaluates whether the response maintains an appropriate, positive, polite, and respectful tone. - Contextual Understanding: Assesses how well the model understands the previous conversational context, including anaphora resolution and maintaining consistency across multiple turns. - Memory: Evaluates whether the model accurately remembers and incorporates earlier dialogue details while tracking and retaining long-term contextual dependencies. - Proactiveness: Determines whether the model proactively identifies and fulfills user needs by making helpful suggestions and guiding the conversation. - Clarity & Logical Structure: Evaluates the clarity, coherence, and logical organization of responses, ensuring they are easy to understand. - Coherence: Ensures responses maintain consistency across turns and that dialogue progresses naturally without contradictions. - Knowledge Understanding: Determines whether the model demonstrates expertise and incorporates relevant domain knowledge with depth and accuracy. - Factual Correctness: Ensures responses contain accurate, verifiable facts and do not generate misinformation. - Specificity & Informativeness: Evaluates how specific and detailed the response is, measuring the amount of useful, non-generic information provided. - Cognitive Reasoning: Assesses the models ability to reason logically and make commonsense inferences, including problem-solving and inference-making. - Creativity: Evaluates the models ability to generate creative and original content in storytelling, idea generation, coding, and visual responses. - Problem-Solving Capability: Measures how effectively the model breaks down and addresses complex user queries, including step-by-step explanations. - Helpfulness: Determines whether the response directly addresses user needs and provides actionable suggestions. - Instruction Following: Assesses the models ability to interpret and adhere to explicit user instructions without deviation. - Harmlessness & Ethical Awareness: Ensures responses avoid harmful, offensive, or biased content while adhering to ethical considerations. - Perceptual Understanding: Evaluates how well the model interprets the given image and integrates it into coherent responses. - Fluency & Grammatical Accuracy: Assesses the linguistic correctness of responses, including grammar, spelling, sentence completeness, and verb tense consistency. - Adaptability: Measures how well the model adapts responses based on user background, preferences, and conversational history. - Multimodal Consistency: Ensures the model maintains semantic alignment between different modalities (e.g., text and images) and that textual responses accurately reflect visual inputs. ### Multi-Turn Conversation Below is the multi-turn conversation (the AI assistants responses are regarded as the optimal answers to the user queries). The conversation always starts at turn number 1. {conversation} Please brainstorm comprehensive, creative, and practical evaluation checklists with as many relevant questions as possible for each user query (one checklist per user query). You MUST strictly follow the guidelines below: [Guidelines] - The output should be formatted as JSON-formatted Python list. - Each entry in the list should be Python dictionary containing the following keys: utterance id and checklist. - The utterance id should indicate the utterance index of the users utterance in the conversation, with the utterance index always starting at 1. - Each entry in the checklist should be Python dictionary containing the following keys: main criteria, and sub criteria. - The question should indicate very detailed and specific question that should be answerable with the positive answer of Yes, and should be relevant to main criteria and sub criteria. - The main criteria should indicate the primary criteria of the question, from the given collections. question, - The sub criteria should indicate more fine-grained criteria (represented as concise noun phrase) belonging to main criteria. - In each question of the checklist, it is NOT necessary to have exactly one question for each main criteria. That is, multiple questions can be generated for single main criteria as needed. [Evaluation Checklists] Only generate the JSON-formatted evaluation checklists without any additional descriptions or explanations. Prompt Template used for MULTIVERSE Evaluation: Quality Assessment You will be provided with an image, previous dialogue history between the user and the model, reference answer that gets score of 10, and an evaluation checklist. Your task is to evaluate the quality of the models answer to the given dialogue history and image based on the provided evaluation checklist, which contains multiple questions. Compare the models answer to the reference answer. ### Previous Dialogue History {dialogue history} ### Models Answer to evaluate: {model answer} ### Reference Answer (Score 10): {reference answer} ### Checklist (Evaluation Items) {checklist} Please use this checklist to guide your evaluation, but do not limit your assessment to it. Compare the models answer to the reference answer based on the checklist and the detailed criteria below. Scores should range from 1 to 10, where 1 indicates very poor response and 10 signifies perfect response. Here are more detailed criteria for the scores: [How well does the model perform overall in terms of accuracy, coherence, reasoning, informativeness, and user satisfaction?] - Score 1-2: The models response is generally poor, with major issues in accuracy, coherence, reasoning, and relevance, making it unhelpful. - Score 3-4: The models response is somewhat useful but contains noticeable flaws in accuracy, logical reasoning, or relevance that limit its effectiveness. - Score 5-6: The models response is moderate, demonstrating reasonable accuracy, coherence, and informativeness, though some aspects may be improved. - Score 7-8: The models response is strong, showing high accuracy, logical reasoning, and relevance with only minor weaknesses. - Score 9-10: The response is excellent, thoroughly accurate, and offers all necessary information to solve the users problem. ### Output Format: Please provide your evaluation results in the following JSON format by filling in the placeholders in : json { score: [1 10] } Do not include any additional explanations or descriptions. Answer: Prompt Template used for MULTIVERSE Evaluation: Checklist Completion Accuracy You will be provided with an image, previous dialogue history between the user and the model, reference answer, and an evaluation checklist. Your task is to evaluate the quality of the models answer to the given dialogue history and image based on the provided evaluation checklist, which contains multiple questions. Compare the models answer to the reference answer. For each question, answer Yes or No. ### Previous Dialogue History {dialogue history} ### Models Answer to evaluate: {model answer} ### Reference Answer (Ground Truth): {reference answer} ### Checklist (Evaluation Items) {checklist} ### Output Format: Provide the final answer in the format of <Q >: <Yes or No >. Do not include any additional explanations or descriptions. Answer:"
        }
    ],
    "affiliations": [
        "CMU",
        "KAIST",
        "NAVER",
        "WHU"
    ]
}