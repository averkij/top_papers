{
    "paper_title": "LMK > CLS: Landmark Pooling for Dense Embeddings",
    "authors": [
        "Meet Doshi",
        "Aashka Trivedi",
        "Vishwajeet Kumar",
        "Parul Awasthy",
        "Yulong Li",
        "Jaydeep Sen",
        "Radu Florian",
        "Sachindra Joshi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods."
        },
        {
            "title": "Start",
            "content": "LMK > CLS Landmark Pooling for Dense Embeddings Meet Doshi 1 Aashka Trivedi 1 Vishwajeet Kumar 1 Parul Awasthy 1 Yulong Li 1 Jaydeep Sen 1 Radu Florian 1 Sachindra Joshi 1 6 2 0 2 9 2 ] . [ 1 5 2 5 1 2 . 1 0 6 2 : r Abstract Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse variable-length token sequence to single vector using pooling operatormost commonly special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it practical and scalable alternative to existing pooling methods. 1. Introduction Traditionally, document-level representations were constructed using vector space based feature extraction methods such as CBOW and Skip gram, which aggregate wordlevel statistics or embeddings into fixed size representations (Mikolov et al., 2013; Clinchant & Perronnin, 2013; Zuccon et al., 2015). These approaches were later supplanted by neural sequence encoders, including LSTMs and Transformers (Vaswani et al., 2017; Peters et al., 2018), primarily because classical methods fail to preserve rich document1IBM Research. <meet@ibm.com>."
        },
        {
            "title": "Correspondence",
            "content": "to: Meet Doshi 1 Figure 1. MLDR (English) dataset long context retrieval performance for different pooling strategies using modernbert-base finetuned on MSMarco Passages. level semantics and rely on fixed feature heuristics that cannot compete with contextualized representations learned by neural models. In particular, the Transformer architecture has consistently demonstrated superior performance over other neural sequence encoders in terms of representation quality and contextual understanding (Li et al., 2020). Moreover, with successive architectural refinements, Transformers have been shown to be universal approximators of arbitrary sequence-to-sequence functions over compact domains (Yun et al., 2020). common application of neural text encoders is the construction of fixed-dimensional, sentence-level representations for downstream tasks such as retrieval, classification, and clustering (Li et al., 2020; Yan et al., 2021; Feng et al., 2022). Formally, given an input text sequence , tokenization function Ftok is first applied to obtain sequence of tokens Xtok = (x1, x2, . . . , xt), which is augmented with special markers such as [CLS] and [SEP] to denote the beginning and end of the sequence, respectively. The resulting token sequence is then passed through an encoder θenc, producing sequence of contextualized hidden representations Henc = (h[CLS], h1, . . . , ht, h[SEP]), where each vector corresponds to token in the input sequence. key challenge is to transform this variable-length sequence of token representations Henc into single fixeddimensional representation of the entire text sequence, de-"
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "noted enc. This is typically accomplished using pooling function P. Existing work most commonly adopts simple pooling strategies such as [CLS] pooling, PCLS(Henc) = Henc[0, :] = h[CLS] (1) or mean pooling, PMEAN(Henc) = 1 Henc Henc (cid:88) i=0 hi. (2) Intuitively, mean pooling provides holistic aggregation by uniformly normalizing contributions from all token representations; however, this averaging can lead to dilution of salient or extreme features. In contrast, [CLS] pooling relies on single special token to aggregate and encode all relevant information into fixed-dimensional vector. While conceptually appealing, this approach places heavy representational burden on single position and, as we observe, often fails to generalize effectively to long contexts, property that is increasingly critical for modern text encoders. To address these limitations, we investigate simple yet effective alternative termed Landmark (LMK) pooling. Our approach is inspired by the method proposed by Luo et al. (2024), which introduces landmark tokens to mitigate abrupt chunking effects in retrieval-augmented generation (RAG) systems. In contrast, we extend this idea to the setting of Dense Passage Retrieval (DPR). Specifically, instead of relying on single special token at the beginning of the sequence (e.g., [CLS]), we insert multiple special landmark tokens at regular intervals after fixed-size chunks of input tokens. The final sequence representation is then obtained by applying mean pooling exclusively over the embeddings of these landmark tokens, rather than over all token embeddings. We posit that this strategy alleviates the representational bottleneck imposed by [CLS] in long-context settings, while simultaneously preserving salient local features that are often diluted under standard mean pooling. We provide empirical evidence supporting these claims and demonstrate that LMK pooling offers substantially simpler yet effective mechanism for improving long-context generalization in neural text encoders, making it practical alternative to commonly used pooling strategies. Our contributions: (1) We introduce LMK pooling, simple alternative to common pooling methods that delivers substantial gains on long-context embedding benchmarks across domains and languages. (2) Through systematic evaluation, we identify inherent limitations of popular pooling mechanisms for long-context generalization and characterize their strengths and weaknesses. (3) We demonstrate that LMK pooling is robust across training regimes and compatible with existing pretraining and retrieval optimizations. 2 2. Related Work Dense Embedders: Early work on dense embedding representations (Gao et al., 2011; Huang et al., 2013) learned transformations of token representations by minimizing distance objective between relevant queries and documents. These approaches were later superseded by stronger encoder architectures (Devlin et al., 2019; Warner et al., 2025) and improved training objectives (van den Oord et al., 2018), which significantly enhanced representation quality. However, despite these advances, most methods still reduce sequence of token representations to single vector using simple pooling strategies, an aspect that has received little attention for long-context retrieval. Pooling Methodologies: There is limited consensus in modern dense passage retrieval on how to aggregate token representations into single document-level embedding. Many recent models rely on CLS pooling (Awasthy et al., 2025; Zhang et al., 2024; Merrick et al., 2024), while others achieve comparable performance with mean pooling (Wang et al., 2022; Nussbaum et al., 2024). Some adopt learned weighting or hybrid strategies (Chen et al., 2024; Lee et al., 2025), with Chen et al. (2024) introducing multi-CLS pooling by inserting multiple CLS tokens at inference. Since these representations are fixed in size regardless of document length, they limit the amount of information that can be preserved (Santhanam et al., 2022; Weller et al., 2025), motivating more robust pooling strategies. For long-context retrieval and retrieval-augmented generation, Luo et al. (2024) introduce Landmark tokens to capture chunk-level representations in autoregressive language models; however, they do not investigate their application for pooling long documents into enhanced bidirectional dense embeddings. In contrast, our work systematically analyzes pooling mechanisms and evaluates their effectiveness across shortand long-context retrieval benchmarks. Additional studies have also highlighted limitations of existing pooling strategies; we refer readers to (Li et al., 2020; Chen et al., 2023; Lee et al., 2025; Ennadir et al., 2025) for further discussion. Training and Evaluation Datasets: Embedding training data have evolved from standard benchmarks such as MSMarco and NQ (Nguyen et al., 2016; Kwiatkowski et al., 2019) to large, heterogeneous mixtures used to train modern text embedders (Chen et al., 2024; Li et al., 2025a). At the same time, evaluation has expanded to broad benchmarks spanning multiple domains, tasks, and languages (Thakur et al., 2021; Enevoldsen et al., 2025). However, most widely used benchmarks focus on relatively short sequences (typically 512 tokens). In contrast, many real world applications require robust evaluation on long context embedding tasks, which remain underexplored, particularly with respect to how representation and pooling choices affect performance (Li et al., 2025b; Zhu et al., 2024; Chalkidis"
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "et al., 2021). In this work, we explicitly connect pooling mechanisms with long context evaluation to systematically compare existing approaches against LMK pooling. 3. Understanding Pooling Methodologies In this section, we analyze existing pooling methodologies as we motivate the need for LMK pooling. 3.1. CLS pooling [CLS] pooling is widely used strategy in text embedding models. Most state-of-the-art approaches pretrain models on long-context masked language modeling (MLM) objectives and then fine-tune them for embedding tasks using [CLS] pooling, where most training datasets contain relatively short sequences. We show that [CLS] pooling inherently biases the model toward the early positions in the text, and that training on shorter sequences amplifies this bias. To understand the origin of this behavior, we analyze how positional information is encoded in text encoders. In Transformers, to generate contextualized sequence representations, each input passes through an attention block. Let the input sequence be = [x1, . . . , xt], with xi Rd. For single attention head, we compute query, key, and value vectors: qi = WQxi, ki = WKxi, vi = WV xi, where WQ, WK, WV Rddh and dh is head dimension. Attention scores, obtained from queries and keys, are used to weight the values to form contextualized representations. Rotary Positional Embeddings (RoPE) (Su et al., 2024) encode positional information by applying positiondependent rotation Rθ,i to the query and key vectors. The angular frequencies are defined as θj = 2j dh , for = 0, . . . , dh 2 1, with as the base frequency (typically set to 10,000). Defining qm = Rθ,mqm and kn = Rθ,nkn, the attention logit between positions and is: Attn(m, n) := m kn = mR θ,mRθ,nkn = mRθ,nmkn, which shows that RoPE induces an explicit dependence on relative position, m. Long-term decay for CLS: The rotation matrices in RoPE induce relative distance-based attenuation of the attention kn for large relative offsets m, also referred to logit as the long-term decay of RoPE. Specifically, for [CLS] token at position = 0: CLS kn = CLSRθ,nkn, whose magnitude decreases as increases. After the softmax operation, this results in smaller attention weights for 3 distant tokens, biasing the [CLS] representation toward early positions. This effect becomes more pronounced with longer sequences, limiting the effectiveness of [CLS] pooling for long-context extrapolation. Increasing the RoPE base frequency (b) slows the positional decay and can improve long-context extrapolation at inference (Zhang et al., 2024; Peng et al., 2024), however, this does not fully mitigate the underlying bias. Figure 2. Normalized attention distribution over long MLDR documents (> 8192 tokens) for mmBERT-base fine-tuned with CLS and LMK pooling with varying maximum sequence lengths (MSL). We provide empirical evidence of this bias in [CLS] pooling in Figure 2. By analyzing the final-layer attention weights assigned to pooling tokens ([CLS] and LMK) across long documents sampled from the MLDR English subset, we observe distinct concentration of attention toward early tokens in the sequence. Fine-tuning on longer sequences reduces this bias but does not eliminate it. The effect worsens significantly when extrapolating beyond the training sequence length. In contrast, our LMK pooling strategy shows no such bias despite being trained on only 512 tokens. Although these results are visualized for the final layer, the findings remain relevant for recent architectures like ModernBERT (Warner et al., 2025), where attention to all tokens is restricted to specific layers. 3.2. Mean and Latent Attention pooling Given sequence of token embeddings Henc RSD, mean pooling (defined in Equation (2)) computes the average across all token embeddings in each latent dimension. Unlike [CLS] pooling, which relies on positional interactions, mean pooling is position-independent. While this simplicity is appealing, it encourages token representations to collapse toward shared centroid in the embedding space, which can weaken salient features. Moreover, mean pooling assigns equal weights to all tokens regardless of their semantic importance, thus tokens carrying little task-relevant information contribute uniformly to final embeddings in typical ℓ2-normalized (norm bounded) encoder representations. This uniform aggregation implicitly assumes that task rele-"
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "vant information is evenly distributed across the sequence, an assumption that often does not hold in practice, especially for short context tasks where critical evidence may be concentrated in small subset of tokens. Consequently, mean pooling can underperform approaches such as [CLS] pooling, which allow the model to learn an explicit aggregation mechanism through attention. We observe that mean pooling extrapolates well to long context at inference time, and it faces no inherent architectural limitations in doing so; however, it consistently underperforms on short context tasks relative to [CLS] pooling. Lee et al. (2025) introduce Latent Attention to address the limitation of assigning equal importance to all tokens. This method uses latent vectors to determine how to attend selectively to different token embeddings in the sequence. Formally, let Henc = (h[CLS], h1, . . . , ht, h[SEP]) denote the sequence of embeddings produced by the encoder, where each hi Rd. Let Rℓdl denote set of learnable latent vectors, where ℓ is the number of latents and dl is the latent head dimension. latent attention module first computes the query, key, and value representations for Henc and as qi = WQhi, = WKL, = WV L, where WQ Rddh and WK, WV R(ℓdl)dh, and dh denotes the head dimension corresponding to hi. Yi = Cross-Attn(Prenorm(qi), k, v) + qi, Zi = FFN(Prenorm(Yi)) + Yi, PLATENT-ATTN = 1 Z (cid:88) i=0 Zi. (3) Here, Prenorm refers to standard LayerNorm applied prior to the transformation, and FFN denotes the standard feedforward network used in Transformer architectures. This approach introduces additional parameters, including learnable latent vectors and feed-forward layers, which are used to determine how the latents are reweighted based on the sequence representations to produce new embeddings. The representations generated from these latent vectors are then mean pooled to obtain single sequence-level representation. Conceptually, this procedure is equivalent to mean pooling and, as such, should not introduce bias toward long-context extrapolation, which is consistent with our empirical results in Section 6. We therefore posit that latent attention inherits the same limitations as mean pooling, while incurring additional parameter and computational overhead that yields limited empirical gains. 4. Landmark (LMK) Pooling Algorithm 1 Landmark Tokenization input Text sequence , Tokenizer Ftok, Granularity g, Landmark token tLMK output Token IDs Xtok, Attention mask Xmask {Tokenize (no special tokens)} 1: Ftok(X ) 2: Split(T, g) {Split token sequence} 3: Xtok [tCLS] z1 [tLMK] z2 [tLMK] zn [tLMK] 4: Xmask 1(Xtok = tPAD) 5: return Xtok, Xmask Algorithm 2 Landmark (LMK) Pooling input Token embeddings Henc RSD, Token IDs Xtok, Attention mask Xmask, Landmark token tLMK output Sentence embedding enc RD 1: IL { Xtok,i = tLMK Xmask,i = 1 } {LMK index set} 2: enc 1 {Mean over LMK} IL 3: return enc Henc i,: iIL (cid:80) We highlight the key limitations of these pooling approaches, while noting the benefit that they provide. CLS pooling biases attention towards nearer tokens, attenuating the attention from distant positions. However, the CLS token acts as learned selector that adaptively attends to different parts of the sequencea desirable feature in representation learning. Furthermore, mean pooling dilutes salient features by uniformly averaging over all tokens, yet it allows multiple tokens to independently asses the full sequence, which may be more effective than single-token CLS representation. Finally, latent attention relies on frozen latent vectors to reweigh sequence embeddings, thereby adding undesirable complexity at inference time. Landmark (LMK) pooling provides simple yet effective solution to the limitations discussed above. It introduces two key modifications to standard text encoding: (1) change to the tokenization procedure and (2) pooling strategy that depends on the positions of LMK tokens. Instead of relying on single special token, LMK pooling inserts multiple special tokens throughout the input sequence. We use the SEP token as the LMK token, as it already serves as natural delimiter in most language encoders1. We explore several strategies for placing these tokens along the sequence. Sentence Chunking involves splitting the text into sentence prior to tokenization and inserting LMK tokens after each sentence. However, this approach may not generalize well to other domains, such as code or certain natural languages, where clear sentence delimiters may be absent. Alternatively, fixed length chunking inserts LMK tokens at fixed intervals after tokenization, but this can introduce positional bias while making performance sensitive to the granularity chosen during fine-tuning. To address these issues, variable chunking randomly samples the LMK 1 For tokenizers that do not have SEP token, the EOS token is an appropriate alternative."
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "insertion interval from fixed set of predefined intervals. This encourages robustness to granularity choice and allows single model to support different granularities at inference depending on the task. The final sequence representation is obtained by mean pooling the embeddings of all LMK tokens. While inserting additional special tokens increases the effective sequence length and incurs extra computation, the overhead is modest in practice since LMK tokens are added at coarse intervals. For example, on the LongEmbed benchmark with maximum evaluation sequence length (MSL) of 32K tokens, inserting an LMK token every 128 tokens adds only 256 additional tokens to the sequence while significantly outperforming alternative pooling mechanisms as seen in Table 1. Formal pseudocode for the tokenization procedure is provided in Algorithm 1, and the pooling operation is detailed in Algorithm 2. 5. Experimental Setup In this section, we describe the experimental setup used to obtain our results. First, we train models on standard retrieval datasets using different pooling strategies and compare their performance on widely used retrieval benchmarks. Second, we analyze the effect of long-context evaluation across different domains. Third, we study how long-context training influences the extrapolation behavior of pooling mechanisms. We divide our experiments into English and Multilingual settings, and further into shortand longcontext training. English: We select widely used Transformer-based text encoders that support long-context training and fine-tuning, namely gte-en-mlm-base and ModernBERT-base. Standard retrieval training datasets, including MS MARCO passage and document ranking as provided by Li et al. (2025a), are used. Training includes hard negatives and distillation scores, with 7 hard negatives for passage ranking and 1 for document ranking. Models are trained for 5k steps with learning rate of 2 105. Passage ranking uses an effective query batch size of 2,048 with maximum sequence length of 512 tokens, while document ranking uses batch size of 256 with sequences up to 8,192 tokens. To mitigate bias from domain overlap, we evaluate on diverse set of out of domain benchmarks. In domain performance is reported on the MS MARCO Dev set, followed by short context evaluations on BEIR (Thakur et al., 2021), MTEB-v2 (Enevoldsen et al., 2025), and MIRACL Retrieval (Zhang et al., 2023). For long context evaluation, we use MLDR, the COIR code retrieval benchmark (Li et al., 2025b), and LongEmbed, which includes documents of diverse lengths, often exceeding the 8k token pretraining context, with dispersed target information (Zhu et al., 2024). Multilingual: For the multilingual setting, we use the multilingual counterpart of ModernBERT, mmBERT-base. Models are fine-tuned on multilingual training data from Chen et al. (2024), which includes diverse datasets with hard negatives and distillation scores generated by multilingual re-ranker. Training is performed for 10k steps with learning rate of 2105, using an effective query batch size of 1024, maximum sequence length of 512 tokens, and 7 hard negatives. In ablations, we investigate fine-tuning with sequence lengths up to 8,192 tokens using mmBERT-base and gte-multilingual-base, reducing the batch size to 64. We evaluate multilingual retrieval on short-context benchmarks such as MIRACL Hard Negatives (18 languages) and long-context benchmarks like Multilingual Long Document Retrieval (MLDR, 13 languages), additionally reporting results on LongEmbed. Zero-shot multilingual long-document classification is evaluated on Multi-EURLEX across 23 languages (Chalkidis et al., 2021), reporting Macro-F1 scores. Pooling Strategies: We compare LMK pooling against multiple baselines from prior work, including CLS, Mean, and Latent Attention (Attn.) pooling with 64 latents, as described in Section 3.2. We also evaluate MultiCLS, where model trained with CLS pooling is modified at inference by inserting CLS tokens at fixed intervals and mean-pooling their representations, analogous to LMK at inference but without LMK-style training. As shown by Chen et al. (2024), this approach improves MLDR performance for CLS in the absence of long-context training. In addition, we include Mean@k baseline, which computes the final representation by mean pooling every k-th token, to assess whether special pooling tokens are necessary for strong performance. For Landmark pooling, we compare different insertion granularities of LMK tokens, as described in Section 4. We consider sentence-based splitter using heuristic English sentence splitter2 to study the role of sentence boundaries, fixed splitter that inserts LMK tokens at regular intervals after tokenization, and variable splitter where the granularity is uniformly sampled from {32, 64, 128, 256} during training. This provides robustness and enables single model to support multiple granularities at inference time. All pooling strategies are trained with identical hyperparameters and under the same data regime to eliminate training-related biases. During evaluation, we use maximum sequence length of 8,192 for all tasks except LongEmbed, which uses 32,768, and explicitly report pooling strategies and granularities in the corresponding tables and figures. Importantly, for LMK evaluations, we do not increase the token budget to account for additional special tokens. We report standard retrieval metrics such as NDCG@10 and Precision@1. Additional training and evaluation details are provided in Appendix B, with dataset descriptions in Appendix C. 2 github.com/mediacloud/sentence-splitter"
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "IN-DOMAIN OUT-OF-DOMAIN (SHORT CONTEXT) OUT-OF-DOMAIN (LONG CONTEXT) FINETUNING EVALUATION MSMarco Dev BEIR (15) MTEBv2 MIRACL Avg. Short (10) Dev MLDR Test COIR (8) LongEmbed (6) Avg. Long Pool Gran. Pool Gran. NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 P@1 NDCG@10 CLS CLS Mean Latent Attn Mean@k LMK - - - - 32 64 128 256 CLS MultiCLS Mean Latent Attn Mean@k LMK LMK Variable LMK - 128 - - 64 32 64 128 256 32 64 128 256 LMK Sentence LMK Sentence 39.8 39.0 39.8 40.1 38.6 39.7 39.9 40.0 39.9 40.3 40.2 40.2 40.2 40.2 42.8 33.6 41.6 41.3 39.7 43.9 43.5 42.9 41. 44.0 43.8 43.6 43.1 43.6 44.2 31.2 43.3 43.1 41.4 45.9 45.1 44.0 42.8 45.8 45.5 45.1 44.5 45. 48.7 31.0 48.0 48.3 44.1 48.5 48.2 48.4 47.0 48.6 48.6 48.1 48.1 48.2 45.2 31.9 44.3 44.2 41.7 46.1 45.6 45.1 43. 46.1 46.0 45.6 45.2 45.7 24.9 5.1 28.0 24.5 15.7 34.8 33.6 30.8 34.1 35.0 34.7 32.1 28.7 28. 43.5 35.4 44.1 43.3 39.8 46.4 46.6 46.3 46.4 47.0 46.9 46.8 46.4 42.7 43.3 43.9 62.7 62.8 44.8 60.2 60.0 54.1 58. 62.4 62.3 59.0 55.5 62.6 37.2 28.1 44.9 43.5 33.4 47.1 46.7 43.7 46.2 48.1 48.0 46.0 43.5 44. Table 1. Comparison of pooling strategies across in-domain and out-of-domain retrieval benchmarks for ModernBERT-base fine-tuned on MSMarco passage data. We report the pooling strategy and granularity (gran.) used during both fine-tuning and evaluation. 6. Results & Discussion We aim to answer the following research questions: 1) How does LMK pooling compare with other pooling strategies across shortand long-context tasks, and out-of-domain settings? 2) Is the positional bias learned by CLS pooling modeling limitation or data artefact? 3) Can training with longer contexts mitigate this bias? 4) Is LMK pooling subject to positional bias, and how does it generalize across different downstream tasks? 6.1. LMK Pooling Performance We examine English retrieval performance by traning on the MSMarco Passage dataset, showing results for ModernBERT-base and gte-en-mlm-base in Table 1 and Table 7 (Appendix) respectively. We observe that LMK pooling peforms marginally better than all pooling baselines on the in-domain MSMarco Dev set. For short-context tasks, LMK pooling maintains comparable performance to CLS pooling, both outperforming Mean and Latent attention pooling, indicating that LMK is strong alternative to CLS for such tasks. For long-context tasks, LMK pooling significantly outperforms all other baselines. Moreover, we find that using Variable splitting during LMK training generalizes well while providing the flexibility of choosing the pooling granularity at inference time. To assess whether LMK pooling generalizes across languages, we report the performance of mmBERT-base on widely uses multilingual retrieval and long-context classification benchmarks after contrastive finetuning. Table 2 shows that LMK pooling maintains substantial improvements on long-context evaluations across in and out-of-domain multilingual tasks. Full results are provided in Appendix A. POOL FT. EVAL. IN-DOMAIN OUT-OF-DOMAIN Gran. Gran. MIRACL (18) NDCG@10 MLDR (13) NDCG@10 M-Eurlex (23) F1 LongEmbed (6) P@1 NDCG@10 CLS Mean Latent LMK 128 LMK Variable 128 32 64 128 256 57.6 56.1 56.6 58.0 57.8 57.9 57.9 57.7 24.9 17.0 19. 37.2 38.7 38.6 38.3 38.0 27.5 27.5 28.0 31.5 33.7 33.4 33.2 33.0 44.1 48.4 46. 62.1 70.7 70.7 70.2 69.8 Table 2. Comparison of pooling strategies across in-domain and out-of-domain retrieval benchmarks for mmBERT-base when finetuned with multilingual BGE-m3 training data. 6.2. Long-Context Performance: Data Artefacts or Modeling Limitations? Having demonstrated the effectiveness of LMK pooling in long-context generalization, we evaluate whether this persists when any context-length bias of the training data is reduced by adding very long documents to the training mix. Specifically, we use the MSMarco Document Ranking dataset, increase the training context length to 8k, and report results for two models trained in this long-context setting in Table 3. For gte-en-mlm-base, CLS pooling shows the strongest performance on short-context tasks, with LMK pooling yielding comparable results. In contrast, on long-context tasks, Mean pooling outperforms both CLS and LMK, with CLS substantially underperforming. For ModernBERT-base, LMK pooling is the most consistent method, performing well across both shortand long-context tasks. CLS achieves compara-"
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "IN-DOMAIN OUT-OF-DOMAIN (SHORT CONTEXT) OUT-OF-DOMAIN (LONG CONTEXT) Model FINETUNING EVALUATION MSMarco Dev BEIR (15) MTEBv2 MIRACL Avg. Short (10) Dev MLDR Test COIR (8) LongEmbed (6) Avg. Long Pool Gran. Pool Gran. NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 P@1 NDCG@10 gte-en-mlm-base ModernBERT-base CLS - CLS Mean Mean - LMK Variable LMK CLS - CLS Mean Mean - LMK Variable LMK - - 32 - - 32 28.7 28.3 28.9 26.0 26.1 26.0 37.6 36.3 37. 38.0 37.1 38.7 39.0 37.7 38.1 39.8 38.4 40.4 32.3 29.0 32.1 32.0 31.4 33.1 36.3 34.3 35. 36.6 35.6 37.4 29.1 30.0 31.4 29.7 31.3 32.2 30.1 32.5 30.9 44.2 42.3 45.6 49.1 61.6 59. 58.3 62.4 58.1 36.1 41.4 40.7 44.1 45.3 45.3 Table 3. Comparison of pooling strategies across in-domain and out-of-domain retrieval benchmarks for gte-en-mlm-base and ModernBERT-base when fine-tuned with MSMarco document data. ble scores on LongEmbed but primarily due to the Passkey Retrieval dataset; on other long-context benchmarks, CLS substantially underperforms  (Table 9)  . Although the MSMarco document ranking dataset is popular long-context training source, its passages are not sufficiently long to enable robust long-context learning (Figure 3). We instead finetune on the same multilingual training data employed in our earlier experiments, which includes the MLDR training set with significantly longer documents. All models are finetuned with maximum sequence length of 8192 and batch size of 64 for 5k steps. We vary the training sequence length for both CLS and LMK pooling using mmBERT-base, and report results in Table 4. As training sequence length increases, CLS and LMK achieve comparable performance on MLDR suggesting that CLS can partially mitigate long-context bias with longer training sequences. However, this does not generalize to out-of-domain datasets such as Multi-EURLEX and LongEmbed, where substantial performance gap persists. Figure 3. Binned distribution of passage token lengths in the MS MARCO Document Ranking training set, computed using the ModernBERT-base tokenizer. We conduct similar experiment on gte-multilingual-base,  (Table 5)  , and observe consistent trend: while CLS, Mean, and Latent Attention pooling become competitive with LMK on MLDR, they substantially under-perform on out-ofdomain long-context tasks. These results suggest indicate that long-context training can reduce but not eliminate the positional bias of these pooling methods. 7 IN-DOMAIN OUT-OF-DOMAIN FT Train MLDR (13) Multi-EURLEX (23) LongEmbed (6)"
        },
        {
            "title": "LMK CLS",
            "content": "NDCG@10 LMK F1 Score CLS P@1 NDCG@"
        },
        {
            "title": "LMK",
            "content": "512 1024 2048 4096 8192 31.7 32.5 39.8 46.3 49.1 36.6 40.9 43.6 45.9 49.6 30.5 30.1 30.0 29.4 30.2 32.9 33.1 32.0 31.7 31.5 46.1 45.4 52.2 52.5 56. 70.6 71.1 72.6 73.3 71.3 Table 4. Effect of fine-tuning maximum sequence length (MSL) on long-context embedding tasks for mmBERT-base, comparing CLS and LMK pooling. POOL FT. EVAL. IN-DOMAIN OUT-OF-DOMAIN Gran. Gran. MIRACL CLS Mean Latent LMK LMK 32 Variable 32 32 (18) NDCG@10 66.7 66.3 66.2 66.1 66. MLDR (13) NDCG@10 M-Eurlex (23) F1 LongEmbed (6) P@1 NDCG@10 58.7 58.3 58.4 58.1 58.1 13.9 14.5 14. 16.4 18.0 66.1 65.8 66.9 72.9 73.1 Table 5. Long-context finetuning ablations on gte-multilingualbase with maximum sequence length of 8192. 6.3. Extension to Retrieval Pretraining Retrieval oriented pretraining, as proposed in RetroMAE (Xiao et al., 2022; Xiao & Liu, 2022), is popular selfsupervised training method to learn effective text representations without the use of labeled data. It is often an intermediate pretraining step to strengthen vector embeddings before large-scale contrastive learning (Awasthy et al., 2025; Chen et al., 2024; Sturua et al., 2024). The method employs two-tier architecture, where standard encoder processes masked input sequences while lightweight, bottleneck decoder attempts to reconstruct the original text from the encoder representations. While the CLS representation is typically used as an input to the enhanced decoder, we show that LMK pooling can be used instead, providing significant performance gains in downstream long context"
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "Model FINETUNING EVALUATION MSMarco Dev BEIR (15) MTEBv2 MIRACL-HN Avg. MLDR (10) EN Short EN COIR (8) LongEmbed (6) Avg. Long IN-DOMAIN OUT-OF-DOMAIN (SHORT CONTEXT) OUT-OF-DOMAIN (LONG CONTEXT) Gran. Pool Gran. NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 P@1 NDCG@10 CLS gte-en-mlm-base ModernBERT-base + CLS RetroMAE pretraining LMK Variable LMK + LMK RetroMAE pretraining CLS CLS + CLS RetroMAE pretraining LMK Variable LMK 32 + LMK RetroMAE pretraining 42.3 42.5 43.0 42.6 39.8 39. 40.3 40.2 44.0 44.7 44.2 44.5 42.8 42.8 44.0 41.3 44.7 45. 44.8 45.4 44.2 44.4 45.8 42.5 47.9 47.4 48.1 48.4 48.7 47. 48.6 47.8 45.5 45.9 45.7 46.1 45.2 45.0 46.1 43.9 27.3 31. 31.7 36.6 24.9 24.9 35.0 31.8 36.6 42.1 37.6 43.3 43.5 44. 47.0 46.2 52.0 60.8 65.2 67.7 43.3 42.1 62.4 66.4 38.6 44. 44.8 49.2 37.2 37.1 48.1 48.1 Pool CLS Table 6. Effect of RetroMAE pretraining followed by fine-tuning for CLS and LMK pooling across gte-en-mlm-base and ModernBERTbase models. retrieval tasks. Specifically, we follow the pretraining method proposed in Xiao et al. (2022) using about 225k samples from BookCorpus, Wikipedia and StackExchange, training for 2 epochs using learning rate of 2 105 and maximum sequence length of 8192 tokens. We use LMK pooling instead of the traditional CLS, experimenting with different granularities of inserting LMK tokens into the sequence, before mean pooling across these tokens to reconstruct the masked sentence with the decoder. Table 6 compares pretraining with LMK pooling and CLS pooling after finetuning on MSMarco passage data. We observe consistent performance gains from LMK pretraining on gte-en-mlm-base, whereas for ModernBERT-base, both pooling strategies yield comparable results with no clear advantage. 6.4. What does LMK capture? We examine what LMK pooling learns from token embeddings. LMK pooling partitions an input sequence into fixed size chunks, generating one LMK representation per chunk. Each LMK forms contextualized representation influenced by the full document. To understand what information LMK embeddings preserve locally, we encode each chunk independently using LMK and compare these non-contextualized embeddings with the contextualized LMK embeddings. We sample long documents from the MLDR (En) dataset such that each document contains at least 256 chunks, and compute directional Hit@k scores by retrieving the chunk embedding closest to each LMK embedding, as shown in Figure 4. Since each LMK token is positioned between left and right chunk, we can measure directional retrieval bias. We note two key findings. First, each left or right chunk embedding retrieves its corresponding LMK embedding with 58% Hit@10 accuracy, indicating that these representations preserve substantial local semantic information despite being contextualized by the entire document. Second, LMK embeddings show higher affinity for left neighboring chunks than for right neighbors, indicating directional biasthey emphasize earlier context more heavily than later context. Figure 4. Retrieval Hits@k between LMK token embeddings and left, right, or any (left or right) chunk embeddings for sequences of length 8,192, with LMK granularity 32 (248 chunks). 7. Conclusion In this work, we first highlight biases and shortcomings of existing pooling mechanisms across various embedding tasks. We then introduce Landmark (LMK) pooling, simple yet effective method that partitions sequences into chunks, inserts landmark tokens, and mean-pools their embeddings. Extensive experiments across multiple encoder architectures, languages, and training setups show that LMK pooling maintains competitive performance on short-context retrieval while providing substantial improvements on longcontext evaluations, and enables robust extrapolation beyond training sequence lengths with minimal computational overhead. Further analysis of LMK embeddings shows that they preserve local semantic information while incorporating global context, achieving an effective balance between representational capacity and discriminative power. The method integrates seamlessly with existing training and evaluation pipelines, offering practical and robust alternative to prior approaches, with broad applicability to long-context embedding tasks, multi-vector embeddings, and chunkingbased RAG systems."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Awasthy, P., Trivedi, A., Li, Y., Doshi, M., Bhat, R., P, V., Kumar, V., Yang, Y., Iyer, B., Daniels, A., Murthy, R., Barker, K., Franz, M., Lee, M., Ward, T., Roukos, S., Cox, D., Lastras, L., Sen, J., and Florian, R. Granite embedding r2 models, 2025. URL https://arxiv. org/abs/2508.21085. Chalkidis, I., Fergadiotis, M., and Androutsopoulos, I. MultiEURLEX - multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 69746996, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 559. URL https://aclanthology.org/2021. emnlp-main.559/. Chen, J., Xiao, S., Zhang, P., Luo, K., Lian, D., and Liu, Z. BGE m3-embedding: Multi-lingual, multifunctionality, multi-granularity text embeddings through self-knowledge distillation. CoRR, abs/2402.03216, 2024. doi: 10.48550/ARXIV.2402.03216. URL https:// doi.org/10.48550/arXiv.2402.03216. Chen, Q., Wang, W., Zhang, Q., Zheng, S., Deng, C., Yu, H., Liu, J., Ma, Y., and Zhang, C. Ditto: simple and efficient approach to improve sentence embeddings. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 58685875, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 359. URL https://aclanthology.org/2023. emnlp-main.359/. Clinchant, S. and Perronnin, F. Aggregating continuous word embeddings for information retrieval. In Allauzen, A., Larochelle, H., Manning, C., and Socher, R. (eds.), Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pp. 100109, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://aclanthology.org/ W13-3212/. Craswell, N., Mitra, B., Yilmaz, E., Campos, D., and Voorhees, E. M. Overview of the TREC 2019 deep learning track. CoRR, abs/2003.07820, 2020. URL https://arxiv.org/abs/2003.07820. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for lanIn Burstein, J., Doran, C., and guage understanding. Solorio, T. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423/. Enevoldsen, K. C., Chung, I., Kerboua, I., Kardos, M., Mathur, A., Stap, D., Gala, J., Siblini, W., Krzeminski, D., Winata, G. I., Sturua, S., Utpala, S., Ciancone, M., Schaeffer, M., Misra, D., Dhakal, S., Rystrøm, J., Solomatin, R., agatan, O. V., Kundu, A., and et al. MMTEB: massive multilingual text embedding benchmark. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview. net/forum?id=zl3pfz4VCV. Ennadir, S., Zolyomi, L., Smirnov, O., Wang, T., Pertoft, Pool me wisely: On J., Cornell, F., and Cao, L. the effect of pooling in transformer-based models. CoRR, abs/2510.03339, 2025. doi: 10.48550/ARXIV. 2510.03339. URL https://doi.org/10.48550/ arXiv.2510.03339. Feng, F., Yang, Y., Cer, D., Arivazhagan, N., and Wang, W. Language-agnostic BERT sentence embedding. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 878891, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. acl-long.62. URL https://aclanthology.org/ 2022.acl-long.62/. Gao, J., Toutanova, K., and Yih, W.-t. Clickthrough-based latent semantic models for web search. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 11, pp. 675684, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450307574. doi: 10.1145/2009916.2010007. URL https://doi. org/10.1145/2009916.2010007. Huang, P., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. P. Learning deep structured semantic models for web search using clickthrough data. In He, Q., Iyengar, A., Nejdl, W., Pei, J., and Rastogi, R. (eds.), 22nd ACM International Conference on Information and Knowledge"
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "Management, CIKM13, San Francisco, CA, USA, October 27 - November 1, 2013, pp. 23332338. ACM, 2013. doi: 10.1145/2505515.2505665. URL https: //doi.org/10.1145/2505515.2505665. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10. 1162/tacl 00276. URL https://aclanthology. org/Q19-1026/. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. Nv-embed: Improved techniques for training llms as generalist embedding models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview. net/forum?id=lgsyLSsDRe. Li, B., Zhou, H., He, J., Wang, M., Yang, Y., and Li, L. On the sentence embeddings from pre-trained language models. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9119 9130, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 733. URL https://aclanthology.org/2020. emnlp-main.733/. Li, C., Qin, M., Xiao, S., Chen, J., Luo, K., Lian, D., Shao, Y., and Liu, Z. Making text embedders few-shot learners. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025a. URL https: //openreview.net/forum?id=wfLuiDjQ0u. Li, X., Dong, K., Lee, Y. Q., Xia, W., Zhang, H., Dai, X., Wang, Y., and Tang, R. Coir: comprehensive benchmark for code information retrieval models. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pp. 2207422091. Association for Computational Linguistics, 2025b. doi: 10.18653/V1/2025.ACL-LONG. 1072. URL https://doi.org/10.18653/v1/ 2025.acl-long.1072. Luo, K., Liu, Z., Xiao, S., Zhou, T., Chen, Y., Zhao, J., and Liu, K. Landmark embedding: chunking-free embedding method for retrieval augmented long-context large language models. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 32683281, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.180. URL https: //aclanthology.org/2024.acl-long.180/. Merrick, L., Xu, D., Nuti, G., and Campos, D. Arcticembed: Scalable, efficient, and accurate text embedding models. CoRR, abs/2405.05374, 2024. doi: 10.48550/ ARXIV.2405.05374. URL https://doi.org/10. 48550/arXiv.2405.05374. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. In Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K. (eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips. cc/paper_files/paper/2013/file/ 9aa42b31882ec039965f3c4923ce901b-Paper. pdf. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. MS MARCO: human generated machine reading comprehension dataset. In Besold, T. R., Bordes, A., dAvila Garcez, A. S., and Wayne, G. (eds.), Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL https://ceur-ws.org/Vol-1773/ CoCoNIPS_2016_paper9.pdf. Nussbaum, Z., Morris, J. X., Duderstadt, B., and Mulyar, A. Nomic embed: Training reproducible long context text embedder, 2024. Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language In The Twelfth International Conference on models. Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=wHBfxhZu1u. Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. In Walker, M., Ji, H., and Stent, A. (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association"
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 22272237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https://aclanthology.org/N18-1202/. Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., and Zaharia, M. Colbertv2: Effective and efficient retrieval via lightweight late interaction. In Carpuat, M., de Marneffe, M., and Ruız, I. V. M. (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pp. 37153734. Association for Computational Linguistics, 2022. doi: 10.18653/V1/ 2022.NAACL-MAIN.272. URL https://doi.org/ 10.18653/v1/2022.naacl-main.272. CoRR, abs/2212.03533, 2022. doi: 10.48550/ARXIV. 2212.03533. URL https://doi.org/10.48550/ arXiv.2212.03533. Warner, B., Chaffin, A., Clavie, B., Weller, O., Hallstrom, O., Taghadouini, S., Gallagher, A., Biswas, R., Ladhak, F., Aarsen, T., Adams, G. T., Howard, J., and Poli, I. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 25262547, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-2510. doi: 10.18653/v1/2025.acl-long.127. URL https: //aclanthology.org/2025.acl-long.127/. Sturua, S., Mohr, I., Akram, M. K., Gunther, M., Wang, B., Krimmel, M., Wang, F., Mastrapas, G., Koukounas, A., Wang, N., and Xiao, H. jina-embeddings-v3: Multilingual embeddings with task lora, 2024. URL https: //arxiv.org/abs/2409.10173. Weller, O., Boratko, M., Naim, I., and Lee, J. On the theoretical limitations of embedding-based retrieval. CoRR, abs/2508.21038, 2025. doi: 10.48550/ARXIV. 2508.21038. URL https://doi.org/10.48550/ arXiv.2508.21038. Su, J., Ahmed, M. H. M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. doi: 10. 1016/J.NEUCOM.2023.127063. URL https://doi. org/10.1016/j.neucom.2023.127063. Thakur, N., Reimers, N., Ruckle, A., Srivastava, A., and Gurevych, I. BEIR: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum? id=wCu6T5xFjeJ. van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018. URL http://arxiv.org/ abs/1807.03748. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. Text embeddings by weakly-supervised contrastive pre-training. Xiao, S. and Liu, Z. Retromae v2: Duplex masked autoencoder for pre-training retrieval-oriented language models, 2022. URL https://arxiv.org/abs/2211. 08769. Xiao, S., Liu, Z., Shao, Y., and Cao, Z. RetroMAE: Pretraining retrieval-oriented language models via masked auto-encoder. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 538548, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.35. URL https:// aclanthology.org/2022.emnlp-main.35/. Yan, Y., Li, R., Wang, S., Zhang, F., Wu, W., and Xu, W. ConSERT: contrastive framework for self-supervised sentence representation transfer. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 50655075, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long. 393. URL https://aclanthology.org/2021. acl-long.393/. Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and Kumar, S. Are transformers universal approximators of sequence-to-sequence functions? In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,"
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "2020. URL https://openreview.net/forum? id=ByxRM0Ntvr. Zhang, X., Thakur, N., Ogundepo, O., Kamalloo, E., Alfonso-Hermelo, D., Li, X., Liu, Q., Rezagholizadeh, M., and Lin, J. MIRACL: multilingual retrieval dataset covering 18 diverse languages. Trans. Assoc. Comput. Linguistics, 11:11141131, 2023. doi: 10.1162/TACL 00595. URL https://doi.org/10.1162/ tacl_a_00595. Zhang, X., Zhang, Y., Long, D., Xie, W., Dai, Z., Tang, J., Lin, H., Yang, B., Xie, P., Huang, F., Zhang, M., Li, W., and Zhang, M. mgte: Generalized long-context text representation and reranking models for multilingual text retrieval, 2024. URL https://arxiv.org/abs/ 2407.19669. Zhu, D., Wang, L., Yang, N., Song, Y., Wu, W., Wei, F., and Li, S. LongEmbed: Extending embedding models for long context retrieval. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 802816, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.47. URL https:// aclanthology.org/2024.emnlp-main.47/. Zuccon, G., Koopman, B., Bruza, P., and Azzopardi, L. Integrating and evaluating neural word embeddings In Proceedings of the 20th in information retrieval. Australasian Document Computing Symposium, ADCS 15, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450340403. doi: 10. 1145/2838931.2838936. URL https://doi.org/ 10.1145/2838931.2838936. A. Additional Results"
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "In this section, we present additional results that complement the main findings. In Table 7, we report finetuning results on the MSMarco passage ranking dataset using gte-en-mlm-base, analogous to the ModernBERT-base results shown in Table 1 in the main text. We observe similar trends, where LMK pooling consistently exhibits strong long context extrapolation, and using variable granularity during finetuning provides added robustness when selecting granularity at inference time. We additionally illustrate the effects of long context extrapolation in Figures 5(a) to 5(f), showing how LMK performance on LongEmbed datasets improves with increasing context length, while CLS pooling fails to effectively leverage the benefits of longer contexts. Extending to pretraining with RetroMAE, Tables 13 and 14 report the performance of both base models after RetroMAE pretraining with CLS versus LMK pooling, indicating that such pretraining extensions can be readily applied to the proposed pooling mechanism. We further report results for finetuning on the MSMarco document ranking dataset in Table 8. Here, LMK pooling achieves the best overall in domain performance. For short context benchmarks, CLS pooling attains the highest average score, outperforming LMK by 0.5 points, while mean pooling leads to notable degradation. In long context evaluations, mean pooling performs best overall, with LMK closely following, whereas CLS pooling lags substantially behind. We additionally report the full multilingual results on MIRACL, MLDR, and Multi-EURLEX in Tables 10 to 12, corresponding to Table 2 in the main text. IN-DOMAIN OUT-OF-DOMAIN (SHORT CONTEXT) OUT-OF-DOMAIN (LONG CONTEXT) FINETUNING EVALUATION MSMarco Dev BEIR (15) MTEBv2 MIRACL Avg. Short (10) Dev MLDR Test COIR (8) LongEmbed (6) Avg. Long Pool Gran. Pool Gran. NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 P@1 NDCG@ CLS CLS Mean Latent Attn Mean@k LMK 64 32 64 128 256 CLS MultiCLS Mean Latent Attn Mean@k LMK LMK Variable LMK 128 64 32 64 128 256 32 64 128 LMK Sentence LMK Sentence 42.3 42.8 42.1 42.0 41.9 42.8 43.0 43.2 43.0 43.0 42.9 43.0 43. 42.6 44.0 42.5 42.4 42.3 43.2 44.2 44.1 44.2 44.0 44.0 44.0 44.0 44.2 44.3 44.7 42.5 43.9 43.8 44. 44.9 44.9 44.9 44.6 44.7 44.7 44.7 44.8 45.0 47.9 42.6 45.6 44.9 46.8 46.4 46.3 47.1 49.0 46.7 46.4 47.0 48. 46.9 45.5 42.5 44.0 43.7 44.7 45.2 45.1 45.4 45.9 45.1 45.0 45.2 45.7 45.4 27.3 31.0 31.4 30.2 29. 31.0 24.7 32.9 34.6 27.8 29.7 30.8 31.7 33.7 36.6 36.9 36.6 36.8 37.1 38.0 37.6 37.6 36.8 37.7 37.5 37.5 37. 36.6 52.0 60.5 60.8 61.7 56.0 63.9 64.0 64.8 59.7 65.0 65.1 65.7 65.2 61.0 38.6 42.8 42.9 42.9 40. 44.3 42.1 45.1 43.7 43.5 44.1 44.7 44.8 43.8 Table 7. Comparison of pooling strategies across in-domain and out-of-domain retrieval benchmarks for gte-en-mlm-base when fine-tuned with MSMarco passage data. IN-DOMAIN OUT-OF-DOMAIN (SHORT CONTEXT) OUT-OF-DOMAIN (LONG CONTEXT)"
        },
        {
            "title": "MSMarco\nDev",
            "content": "BEIR (15) MTEBv2 MIRACL Avg. Short (10)"
        },
        {
            "title": "MLDR\nTest",
            "content": "COIR (8) LongEmbed (6) Avg. Long"
        },
        {
            "title": "Pool",
            "content": "Gran. Pool Gran. NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 P@1 NDCG@"
        },
        {
            "title": "LMK",
            "content": "- -"
        },
        {
            "title": "LMK Variable LMK",
            "content": "- - 128 32 64 128 256 28.7 28.3 28.6 28.9 28.5 28.5 28. 37.6 36.3 36.7 37.1 36.9 37.1 36.9 39.0 37.7 37.5 38.1 37.9 38.0 37. 32.3 29.0 30.1 32.1 31.0 31.1 30.4 36.3 34.3 34.8 35.8 35.3 35.4 35. 29.1 30.0 30.0 31.4 31.1 30.3 29.7 30.1 32.5 31.6 30.9 30.6 30.4 30. 49.1 61.6 63.1 59.8 60.4 60.0 59.9 36.1 41.4 41.6 40.7 40.7 40.2 39. Table 8. Comparison of pooling strategies across in-domain and out-of-domain retrieval benchmarks for gte-en-mlm-base when fine-tuned with MSMarco document data."
        },
        {
            "title": "EVALUATION",
            "content": "LONG-EMBED RETRIEVAL"
        },
        {
            "title": "Pool",
            "content": "Gran. Pool Gran. NarrativeQA QMSum WikiMQA SummScreenFD Needle Passkey Avg. NDCG@10 NDCG@10 NDCG@10 NDCG@10 P@ P@1 CLS CLS Mean Mean LMK Variable LMK 32 30.9 47.1 44.4 32.3 35.7 34.7 75.0 81.1 79. 82.4 86.8 89.5 46.5 48.3 41.8 82.8 75.8 58.5 58.3 62.4 58.1 Table 9. Comparison of pooling strategies on LongEmbed retrieval benchmark for modernbert-base fine-tuned on MSMarco document data. FT. Pool FT. Gran. Eval. Pool Eval. Gran. ara ben deu eng fas fin fra hin ind jpn kor rus spa swa tel tha yor zho avg. CLS Mean Latent Attn CLS Mean Latent Attn LMK 128 LMK LMK Variable LMK 128 32 64 128 256 66.1 63.4 47.4 48.1 49.5 68.4 51.6 44.5 48.4 59.2 61.7 58.0 49.2 68.2 75.9 69.1 58.4 50.1 57.6 65.0 61.4 45.7 44.5 50.5 67.3 48.8 43.6 48.2 58.3 61.4 54.7 49.3 68.8 77.3 66.8 50.1 47.2 56.1 65.5 62.9 46.2 45.6 50.6 67.7 49.1 43.2 48.8 58.6 61.0 56.0 50.5 68.9 77.4 67.5 51.2 48.7 56. 67.2 65.6 45.5 45.2 51.0 68.9 50.8 44.9 50.3 57.5 61.6 56.3 50.2 71.0 78.1 68.9 59.5 52.1 58.0 66.5 63.7 45.8 44.7 50.5 69.1 50.0 44.8 50.1 59.9 60.9 57.6 49.8 69.9 77.1 68.9 61.2 49.8 57.8 66.4 63.3 46.2 45.3 50.7 69.0 49.8 44.9 50.1 60.4 61.3 57.7 49.7 69.8 77.1 69.1 61.7 50.2 57.9 66.5 63.8 46.1 44.8 50.9 68.9 50.0 45.2 49.9 59.8 61.1 57.4 50.1 70.1 77.2 68.9 61.5 50.9 57.9 66.5 62.8 46.0 44.8 50.9 68.5 49.4 44.5 49.9 59.8 61.0 57.2 50.0 70.0 77.3 68.8 60.5 51.0 57.7 Table 10. NDCG@10 retrieval performance on MIRACL-HN (18) dataset across languages under different pooling and granularity settings for mmBERT-base. B. Training and Evaluation Hyperparameters Our experiments are divided into English and Multilingual settings, with both shortand long-context training and evaluation. English: We use Transformer-based text encoders supporting long-context training, namely gte-en-mlm-base and ModernBERT-base. Models are trained on MS MARCO passage and document ranking datasets using hard negatives (7 for passage, 1 for document) and distillation scores provided by Li et al. (2025a). Training is performed for 5k steps with learning rate of 2 105, warmup steps of 250, and query maximum length capped at 128 tokens. Passage ranking uses an effective query batch size of 2,048 and maximum sequence length of 512 tokens; document ranking uses batch size 256 and sequences up to 8,192 tokens. Multilingual: We use mmBERT-base and gte-multilingual-base. Models are fine-tuned on multilingual datasets from Chen et al. (2024) with hard negatives (7) and distillation scores. Training is performed for 10k steps with learning rate 2 105, batch size 1,024, and maximum sequence length 512 tokens. For ablations with sequence lengths up to 8,192 tokens, batch size is reduced to 64 due to memory constraints. Training setup: We build our training pipeline on top of FlagEmbedding3. We use BF16 precision on 8 NVIDIA H100 80GB GPUs. InfoNCE loss temperature is set to τ = 0.02, and negatives are shared across devices to increase effective in-batch negative size. Training is optimized using DeepSpeed4 and HuggingFace Accelerate5. Evaluation: All embedding evaluations are conducted using MTEB6 and SentenceTransformers7. For English, shortcontext benchmarks include BEIR-15, MTEBv2, and MIRACL Retrieval; long-context benchmarks include MLDR, COIR, and LongEmbed. For multilingual experiments, short-context evaluation uses MIRACL Hard Negatives (18 languages) and long-context evaluation uses MLDR (13 languages) and LongEmbed. Zero-shot long-document classification is evaluated on Multi-EURLEX (23 languages) using Macro-F1. C. Dataset Information We begin with standard English retrieval training datasets commonly used in dense retrieval, such as MSMarco Passage Ranking, TREC-DL, and Natural Questions (NQ) (Nguyen et al., 2016; Craswell et al., 2020; Kwiatkowski et al., 2019). 3 github.com/FlagOpen/FlagEmbedding 6 github.com/embeddings-benchmark/mteb 4 github.com/deepspeedai/DeepSpeed 5 github.com/huggingface/accelerate 7 huggingface/sentence-transformers 14 FT. Pool FT. Gran. Eval. Pool Eval. Gran. ara cmn deu eng fra hin ita jpn kor por rus spa tha avg."
        },
        {
            "title": "CLS\nMean\nLatent Attn",
            "content": ""
        },
        {
            "title": "LMK",
            "content": ""
        },
        {
            "title": "Variable LMK",
            "content": "128 32 64 128 256 21.9 11.9 15.5 9.0 7.4 6.0 13.6 14.3 18. 8.5 6.0 4.3 24.9 42.1 13.1 35.7 28.5 13.3 50.9 35.3 45.0 36.3 26.5 39.8 7.9 17.0 11.2 15.3 30.2 41.5 30.9 46.2 11.1 19.6 11.8 11.3 20.9 30.8 7.2 9.4 5.4 6.3 31.0 15.5 31.5 24.7 60.9 19.3 48.3 36.8 21.2 63.4 46.4 60.6 23.5 37.2 30.8 16.9 33.2 27.1 60.3 20.2 51.8 39.3 25.2 63.7 48.2 63.1 23.8 38.7 31.0 16.3 32.8 27.0 60.4 19.5 51.8 38.6 25.4 63.9 47.9 63.6 23.6 38.6 30.4 15.6 32.3 27.2 60.9 18.5 52.1 39.5 24.0 63.1 47.7 63.4 23.7 38.3 30.2 14.3 32.4 27.4 60.7 18.2 51.8 39.8 23.1 64.0 46.2 63.6 21.8 38. Table 11. NDCG@10 retrieval performance on MLDR (13) dataset across languages under different pooling and granularity settings for mmBERT-base. FT. Pool FT. Gran. Eval. Pool Eval. Gran. deu eng bul por lit swe est spa nld pol fra slk fin mlt ita hun lav ces ell slv dan ron hrv avg. CLS Mean Latent Attn CLS Mean Latent Attn LMK 128 LMK LMK Variable LMK 128 32 64 128 256 27.2 26.8 28.7 27.3 29.6 26.8 27.6 26.0 26.4 28.5 27.7 27.3 27.1 24.6 27.3 29.1 30.5 30.4 25.1 26.9 27.6 29.3 25.2 27.5 27.3 27.9 30.3 27.0 29.0 26.9 28.7 26.9 25.8 28.6 26.5 27.7 26.5 24.9 26.7 28.8 30.6 29.9 25.9 27.4 26.3 29.2 23.5 27.5 28.1 27.3 30.8 27.4 29.3 27.2 29.2 27.6 27.4 28.5 26.9 28.2 27.3 25.1 27.2 30.0 31.8 30.3 26.4 28.3 26.7 29.2 24.0 28.0 31.5 30.0 33.5 29.4 33.1 31.3 30.3 30.5 30.3 33.1 31.9 31.8 31.3 29.5 30.3 33.4 33.9 34.2 28.5 31.6 31.2 33.6 29.8 31. 34.1 32.1 34.9 31.6 34.8 32.9 32.8 32.3 33.2 35.5 34.0 33.8 33.7 32.5 32.5 35.4 35.4 36.7 30.3 34.2 33.4 35.2 32.6 33.7 34.0 31.8 34.9 31.3 34.7 32.6 32.6 32.1 32.8 35.3 33.8 33.8 33.1 32.2 32.3 35.1 35.4 36.4 30.3 33.9 33.0 35.0 32.1 33.4 34.1 31.6 34.8 31.2 34.7 32.5 32.4 31.8 32.6 35.2 33.8 33.7 32.8 31.9 32.4 34.8 35.6 36.0 30.2 33.7 32.7 34.8 31.3 33.2 34.0 31.5 34.5 30.9 34.4 32.3 32.4 31.5 32.4 35.1 33.4 33.4 32.5 31.4 32.3 34.5 35.3 35.7 29.9 33.2 32.3 34.6 30.9 33.0 Table 12. Zero-shot long-document classification F1 scores on MultiEURLEX across 23 languages using fine-tuned mmBERT-base, evaluated with maximum sequence length of 8192. These datasets have been instrumental in training effective short-context retrievers; however, they primarily consist of relatively short passages and therefore do not expose models to long document structures during training. To partially address this limitation, the MSMarco Document Ranking dataset is often used, as it contains longer documents compared to passage-level data. However, we find that this dataset remains limited in terms of true long-context coverage, with document lengths falling well below those encountered in many real-world retrieval scenarios (Figure 3). More recently, embedding training datasets have diversified substantially. Modern training mixtures, such as those used in the bge-en-icl framework (Li et al., 2025a), combine heterogeneous data sources spanning multiple tasks, including semantic textual similarity, clustering, retrieval, classification, question answering, and natural language inference. Statistics for these datasets are reported in Table 16, where we show the number of samples and character length distributions, including the median, 25th, and 75th percentiles. Even under conservative estimate of four to five characters per token, the majority of these datasets have median sequence lengths of at most 250 tokens. The MSMarco Document dataset remains an exception, yet even at the 75th percentile its length is typically below 2000 tokens. This highlights that, despite the increased diversity of modern embedding training corpora, long-context supervision remains scarce in English retrieval training data. Training directly on long-context inputs is further constrained by practical considerations, including increased computational cost, reduced batch sizes, and longer training times. LMK pooling mitigates these limitations by enabling strong longcontext extrapolation even when trained predominantly on short-context data, as demonstrated consistently across our experimental results. For evaluation, we first report in-domain performance on the MSMarco Dev set and out-of-domain performance on the BEIR-15 benchmark, which is widely used for assessing retrieval generalization. We additionally evaluate on the MTEB-v2 retrieval subset, consisting of ten retrieval datasets designed to enable faster yet representative benchmarking (Enevoldsen et al., 2025), as well as the MIRACL English Dev set, which is constructed from human annotated relevance judgments over Wikipedia articles. Since these benchmarks primarily consist of short documents (most of which have of at most 512 tokens), they are insufficient for evaluating long-context retrieval behavior. To address this limitation, we further include the MLDR English Test set, the COIR code retrieval benchmark spanning diverse programming tasks and languages, and the LongEmbed benchmark, which focuses on extremely long documents with an average length exceeding 5.5k words. Together, these datasets enable comprehensive evaluation of retrieval performance under long-context settings, including sequence lengths beyond 32k tokens."
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "IN-DOMAIN OUT-OF-DOMAIN (SHORT CONTEXT) OUT-OF-DOMAIN (LONG CONTEXT) PRETRAINING FINETUNING EVALUATION MSMarco Dev BEIR (15) MTEBv2 MIRACL-HN Avg. Short (10) EN MLDR EN COIR (8) LongEmbed (6) Avg. Long Pool CLS LMK Gran. 64 128 256 Pool CLS LMK Gran. 64 128 256 Pool CLS LMK LMK Variable LMK Variable LMK Gran. NDCG@10 MRR@10 NDCG@10 NDCG@10 NDCG@ NDCG@10 NDCG@10 P@1 NDCG@10 64 128 256 32 64 128 256 42.5 42.6 42.8 42. 42.6 42.5 42.7 42.6 42.5 36.1 36.2 36.5 36.3 36.1 36.2 36.3 36.2 35. 44.7 44.5 44.2 44.5 44.3 44.3 44.4 44.5 44.5 45.5 45.5 44.8 45. 45.4 45.3 45.4 45.4 45.3 47.4 46.8 47.3 48.7 46.7 46.5 47.1 48.4 47. 45.9 45.6 45.4 46.1 45.5 45.4 45.6 46.1 45.7 31.2 36.7 34.6 36. 37.8 38.1 37.5 36.6 36.2 42.1 43.0 43.2 42.8 43.5 43.3 43.2 43.3 42. 60.8 66.1 65.7 67.6 68.4 68.4 68.1 67.7 63.6 44.7 48.6 47.8 48. 49.9 49.9 49.6 49.2 47.3 LMK Sentence LMK Sentence LMK Sentence Table 13. Comparison of pooling strategies on in-domain and out-of-domain retrieval benchmarks for gte-en-mlm-base, pretrained with CLS or LMK pooling and fine-tuned on MS MARCO passage data. IN-DOMAIN OUT-OF-DOMAIN (SHORT CONTEXT) OUT-OF-DOMAIN (LONG CONTEXT) PRETRAINING FINETUNING EVALUATION MSMarco Dev BEIR (15) MTEBv2 MIRACL-HN Avg. Short (10) EN MLDR EN COIR (8) LongEmbed (6) Avg. Long Pool CLS Gran. Pool CLS Gran. Pool CLS 64 LMK LMK LMK Variable LMK Variable LMK Sentence LMK Sentence LMK Sentence LMK LMK 32 32 32 Gran. NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 NDCG@10 P@1 NDCG@10 39.8 40.3 40.2 40. 42.8 42.5 41.3 42.0 44.4 44.1 42.5 43.2 47.9 47.7 47.8 46. 45.0 44.8 43.9 44.0 24.9 31.9 31.8 32.3 44.3 46.8 46.2 43. 42.1 59.8 66.4 64.9 37.1 46.2 48.1 46.9 Table 14. Comparison of pooling strategies on in-domain and out-of-domain retrieval benchmarks for ModernBERT-base, pretrained with CLS or LMK pooling and fine-tuned on MS MARCO passage data. For multilingual training, we use the BGE-m3 training data introduced by Chen et al. (2024), which comprises diverse collection of datasets spanning wide range of languages, as summarized in Table 17. This mixture includes the MLDR training set, which contains substantially longer documents than MSMarco Document Ranking and is therefore used for our long-context training experiments. However, MLDR constitutes only about 42k query document pairs out of the roughly three million total training pairs, limiting its overall influence during training and resulting in relatively weak long-context extrapolation when using conventional pooling methods. For multilingual evaluation, we use all 18 languages available in the MIRACL Dev set for short-context tasks and the MLDR 13-language test set for long-context tasks. Since the corresponding training sets were already seen during model training, we additionally evaluate out-of-domain long-context performance. To this end, we include the Multi-EURLEX dataset, which contains 65k EU legal documents in 23 languages labeled with EUROVOC concepts by the EU Publication Office. For this out-of-domain evaluation, we report the F1 score. Finally, we include LongEmbed as long-context benchmark to assess the models ability to handle long-context retrieval tasks."
        },
        {
            "title": "Model",
            "content": "gte-en-mlm-base ModernBERT-base"
        },
        {
            "title": "EVAL",
            "content": "Class. Clust. PairCls. Rerank. Retr. STS Summ. Avg."
        },
        {
            "title": "Pool",
            "content": "Gran. Pool Gran. (8)"
        },
        {
            "title": "CLS\nMean\nLMK",
            "content": ""
        },
        {
            "title": "CLS\nMean\nLMK",
            "content": "CLS CLS Mean Mean LMK LMK 32 LMK Variable LMK 256 32 128 69.5 70.1 70.0 69.5 69.6 69.9 69.9 (8) 41.6 40.9 41.5 41.6 41.2 42.2 42.5 (3) 83.3 83.3 83.3 80.4 79.4 80.2 80.5 (2) 44.8 44.5 44.9 44.1 43.6 44.3 44.1 (10) 44.7 44.9 44.6 44.2 43.3 45.9 45.1 (9) 77.1 75.9 76.5 76.0 75.3 74.9 75.2 (1) 27.6 30.4 27.2 24.2 24.8 25.6 25.7 (41) 58.5 58.3 58.4 57.8 57.3 58.1 58.1 Table 15. MTEB-v2 embedding benchmark results. We report task-wise averages across pooling strategies for gte-en-mlm-base and ModernBERT-base, trained on MSMarco Passage data. Dataset Count Median P75 amazon counterfactual classification amazon reviews classification arXiv abstract arguana arxiv title banking classification biorxiv abstract biorxiv title eli5 emotion classification fever fiqa hotpotqa imdb classification medrxiv abstract medrxiv title msmarco document msmarco passage mtop intent classification nli nq quora reddit clustering reddit clusteringP2P scidocsrr squad stack exchange clustering stack exchange clusteringP2P stack overflow dup questions sts toxic conversations classification trivial tweet sentiment extraction classification twenty news groups 3,907 20,000 20,000 4,065 20,000 10,003 4,070 4,070 325,475 14,341 29,096 5,500 84,516 25,000 1,157 1,157 367,013 485,823 15,667 275,601 58,568 60,202 20,000 20,000 12,655 87,599 20,000 20,000 19,847 2,006 25,000 60,315 27,481 11,854 80 78 825 813 66 35 1,614.5 102 393 46 1,633 870.5 396 57 1,836 113 3,897 336 18 37 624 47 49 493 1,006 693 46 649 47 56 26 625 114 28 80 67 581 595 50 27 1,325 83 232 45 859 503 270 57 1,440 90 2,160 281 16 27 599 37 32 306 753 559 31 407 36 39 26 601 110 20 80 78 1,135 1,098 86 41 1,975.75 125 621 49 2,496 1,453.25 559 57 2,271 141 7,758 454 26 55 652 61 70 850.25 1,299 895 64 1,049 60 122 26 652 114 Table 16. Character length statistics for positive documents in the bge-en-icl dataset, used to train English dense retrievers by Li et al. (2025a). We report the number of samples and the median, 25th (P25), and 75th (P75) percentiles. Assuming 45 characters per token, most datasets correspond to sequence lengths well below those required for learning robust long-context representations."
        },
        {
            "title": "Landmark Pooling for Dense Embeddings",
            "content": "(a) QMSum (b) SummScreenFD (c) WikiMQA (d) NarrativeQA (e) Needle (f) Passkey Figure 5. Long-context retrieval performance across datasets for different pooling strategies using ModernBERT-base fine-tuned on MSMarco Passages."
        },
        {
            "title": "Count Median",
            "content": "P25 P75 DuReader HotpotQA Law MIRACL (ar) MIRACL (bn) MIRACL (en) MIRACL (es) MIRACL (fa) MIRACL (fi) MIRACL (fr) MIRACL (hi) MIRACL (id) MIRACL (ja) MIRACL (ko) MIRACL (ru) MIRACL (sw) MIRACL (te) MIRACL (th) MIRACL (zh) MLDR (ar) MLDR (de) MLDR (en) MLDR (es) MLDR (fr) MLDR (hi) MLDR (it) MLDR (ja) MLDR (ko) MLDR (pt) MLDR (ru) MLDR (th) MLDR (zh) MSMARCO Mr. TyDi NQ SQuAD T2Ranking Trivia cMedQAv2 EN-NLI mMARCO ZH-NLI (ATEC) ZH-NLI (BQ) ZH-NLI (LCQMC) ZH-NLI (PAWSX) ZH-NLI (QBQTC v2) ZH-NLI (STS) ZH-NLI (AFQMC) 80,416 84,516 2,054 3,495 1,631 2,863 2,162 2,107 2,897 1,143 1,169 4,071 3,477 868 4,683 1,901 3,452 2,972 1,312 1,817 1,847 10,000 2,254 1,608 1,618 2,151 2,262 2,198 1,845 1,864 1,970 10,000 485,905 48,729 58,568 87,599 90,467 60,315 50,000 274,951 100,000 11,325 12,599 10,000 10,000 10,000 249 10,534 296 396 607 472 581 661 544.5 408 466 509 500 607 227 228 550 278 566 485 168 13,535 24,850 18,229 22,453.5 23,685 12,323.5 25,551 9,311.5 7,529.5 25,574 18,851 10,283.5 6,079.5 336 434 624 693 316 625 90 37 110 12 10 10 42 22 15 12 258 270 356.5 302 405.5 446 376 256 332 320 326 413 153 156 360 180 395 339 114 5,174 15,070 10,553.75 6,579.25 13,130.75 5,759.75 12,454.5 4,068.25 2,563.25 12,549 7,516.75 6,651.25 3,610 281 277 599 559 180 601 62 27 84 10 7 8 32 14 11 411 559 1455.75 713.5 831 901 758 624 651 769.5 754 836 324 332 821 428 946 691 246 21,921 29,238.5 25,636.25 28,463 27,501.25 20,120 29,046.5 11,003.75 11,906.5 29,334 23,289.5 22,316.5 8,624.75 454 647 652 895 604 652 125 55 152 15 14 12 55 31 19 16 Table 17. Character length statistics for positive documents across bge-m3 dataset, used by Chen et al. (2024). We report the number of samples along with the median, 25th (P25), and 75th (P75) percentiles. MLDR datasets feature substantially longer sequences than standard retrieval benchmarks, and without them, models using CLS or mean pooling struggle to generalize to long-context settings."
        }
    ],
    "affiliations": [
        "IBM Research"
    ]
}