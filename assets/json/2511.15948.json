{
    "paper_title": "Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click",
    "authors": [
        "Raphael Ruschel",
        "Hardikkumar Prajapati",
        "Awsafur Rahman",
        "B. S. Manjunath"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 2 8 4 9 5 1 . 1 1 5 2 : r Click2Graph: Interactive Panoptic Video Scene Graphs from Single Click Raphael Ruschel* UC Santa Barbara Electrical & Computer Engineering raphael251@ucsb.edu Md Awsafur Rahman UC Santa Barbara Electrical & Computer Engineering awsaf@ucsb.edu Hardikkumar Prajapati* UC Santa Barbara Electrical & Computer Engineering hprajapati@ucsb.edu B. S. Manjunath UC Santa Barbara Electrical & Computer Engineering manj@ucsb.edu"
        },
        {
            "title": "Abstract",
            "content": "State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From single user cue, such as click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts subject, object, predicate triplets to form temporally consistent scene graph. Our framework introduces two key components: Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding. 1. Introduction Understanding not only what appears in video but how entities interact is core challenge in intelligent perception. This capability is desired in applications in robotics, autonomous agents, assistive systems, and surveillance, where downstream decisions depend on correctly interpreting actions, intentions, and relationships. Scene Graph Generation (SGG) has emerged as powerful representation for such structured understanding, evolving from static image reasoning [30, 35] to dynamic, video-based formulations that capture temporal context [5, 12]. More recently, panoptic scene graph generation has advanced grounding fidelity by replacing bounding boxes with pixel-level masks [31, 33], enabling fine-grained grounding of classes, especially for objects with irregular shapes (commonly refered as stuff classes), such as floor and sky. Despite these advances, existing video SGG and PVSG pipelines remain fully automated and closed-loop. Once model overlooks an occluded object, misclassifies rare interaction, or drifts during tracking, the user has no mechanism to intervene. This lack of controllability is problematic in complex or safety-critical environments, where correcting errors or directing model attention is essential. At the same time, new class of promptable segmentation models, most notably SAM and SAM2 [13, 23], has demonstrated the power of direct visual prompting. With simple click or box, users can obtain precise, temporally consistent segmentation masks. Yet these models are inherently class-agnostic and relation-agnostic: they determine where objects are but not what they are or how they interact. This disconnect reveals fundamental gap: current PVSG systems lack user guidance, and current interactive segmentation models lack semantic structure. We address this gap with Click2Graph, the first framework for userguided Panoptic Video Scene Graph Generation. From single visual cue, such as clicking subject in any frame, Click2Graph: 1. Segments and tracks the prompted subject across time, 2. Autonomously discovers and segments interacting objects, and 3. Predicts subject, object, predicate relationships to form temporally consistent scene graph. Figure 1 illustrates how distinct scene graphs can be pro1 Figure 1. On the left example, the user clicked on the dog, and Click2Graph segmented the carpet and predicted the sitting activity. On the right, we have prompt on child which yields dog, playing as associated object and activity. duced depending on which entity the user prompts, highlighting the controllability of the system. Click2Graph introduces two components that supply the missing semantic and relational reasoning. Dynamic Interaction Discovery Module (DIDM) generates subjectconditioned object prompts, enabling automatic discovery of entities participating in interactions. Semantic Classification Head (SCH) performs joint subject, object, and predicate inference over the discovered segments, producing structured scene graph outputs. Together, these components elevate promptable segmentation from geometric mask extraction to full panoptic video scene graph generation. Our contributions are summarized as follows: User-Guided Panoptic Video Scene Graphs. We introduce the first interactive PVSG framework that converts single visual prompt into temporally consistent panoptic scene graph, enabling controllable, interpretable video analysis. Dynamic Interaction Discovery. We propose novel module that generates subject-conditioned prompts to discover interacting objects, naturally supporting multisubject and multi-object reasoning. Semantic Reasoning atop Promptable Segmentation. dedicated classification head predicts subjectobject pairs labels and the relationship between them, bridging the gap between prompt-based segmentation and structured semantic inference. Click2Graph establishes new paradigm for video scene understanding by combining human guidance, pixel-level grounding, and relational reasoning within unified architecture. As shown in our experiments on the OpenPVSG benchmark, this paradigm enables controllable and interpretable scene graph generation while offering practical path toward real-world interactive video analytics. 2. Related Works Our work lies at the intersection of video scene graph generation, panoptic-level scene understanding, and interactive visual analysis. Below, we position Click2Graph within each of these domains. Table 1 shows summary of the related domains. 2.1. Advances in Video Scene Graph Generation Scene Graph Generation (SGG) was first developed for static images [30, 35], and later extended to videos (VidSGG) to capture temporal dynamics [7, 12, 16, 18, 19, 27, 28, 36]. Transformer-based approaches such as STTran [5], DDS [11], and VSG-Net [26] improved longrange temporal reasoning and robustness to clutter. Another thread of work addresses the heavy long-tail distribution of predicates through debiasing methods such as TEMPURA [21] and VISA [17], while DiffVSGG [3] frames video SGG as an iterative denoising problem. Although these methods advance automated scene graph reasoning, they operate as closed-loop systems: once the model misdetects or misclassifies an entity, the user cannot intervene. Click2Graph introduces this missing interactive dimension, enabling subject-specific, user-directed scene graph construction. 2.2. Panoptic-Level Scene Understanding To improve spatial precision, recent works replace bounding boxes with pixel-level masks. Panoptic Scene Graph including Generation (PSG) [31] grounds all entities, stuff classes, in panoptic masks. This paradigm was extended temporally in the Panoptic Video Scene Graph (PVSG) task [33], which provides temporally consistent panoptic annotations through the OpenPVSG benchmark. Click2Graph builds on this foundation but differs from Inprior PVSG approaches by introducing user control. 2 Table 1. comparative analysis of Scene Graph Generation paradigms. Click2Graph is the first to unify video-level temporal reasoning, panoptic-level spatial precision, and user-guided visual prompting for end-to-end tracking and relationship prediction. Method Traditional SGG (e.g., MOTIFS [35]) Video SGG (e.g., STTran [5]) Panoptic SGG (e.g., PSGFormer [31]) Panoptic Video SGG (PVSG) [33] Text-Prompted SGG (e.g., VLPrompt [37]) Click2Graph (Ours) Modality Image Video Image Video Image Video+Image Granularity Box Box Mask Mask Mask Mask Interaction Type None None None None Text Visual (Click/Box) End-to-End Tracking Relationship Prediction N/A Yes N/A Yes N/A Yes Yes Yes Yes Yes Yes Yes stead of producing full-frame graph in fully automated manner, we allow user to specify subject of interest and generate an interaction-centric scene graph guided by that prompt. 2.3. Promptable and Interactive Scene Analysis Interactive reasoning has emerged in adjacent domains but remains underexplored for scene graph generation. Existing approaches fall into two categories: text-prompted and visually-guided methods. 2.3.1. Text-Prompted Generation Several SGG methods incorporate language guidance. OvSGG [9] and CaCao [34] use text prompts for openvocabulary predicate detection, while VLPrompt [37] integrates LLM-derived priors to improve panoptic SGG. Although language prompts provide rich semantics, they lack spatial specificity, text cannot uniquely and precisely ground pixel-level subjects. These systems also depend on language availability and may not generalize across settings. In contrast, Click2Graph uses direct visual prompts (points, boxes or masks), which are universal, unambiguous, and spatially precise. 2.3.2. Visually-Guided Interaction Visually guided interaction remains largely unexplored for scene graph generation. Prior work has examined interactive image or 3D scene graph editing [1, 14, 20], and interactive video object segmentation (VOS) allows tracking of single prompted object [10]. However, these methods lack interaction discovery and semantic relationship reasoning. To our knowledge, Click2Graph is the first framework to leverage direct visual prompts for end-to-end Panoptic Video Scene Graph Generation, including object discovery, segmentation, and predicate prediction. 2.4. Foundation Models for Segmentation Foundation models such as SAM [13] and SAM2 [23] provide powerful engines for promptable segmentation and video mask propagation. SAM2, in particular, delivers high-quality temporal consistency. However, these models are class-agnostic and relation-agnostic: they cannot identify object categories, infer interactions, or discover interacting entities from prompted subject. Click2Graph fills this gap by introducing two components, the Dynamic Interaction Discovery Module and the Semantic Classification Head, that transform SAM2s geometric outputs into pixel-accurate, temporally consistent scene graphs. 3. Methodology 3.1. Problem Formulation Given video = {I1, . . . , IT } with frames and an initial user prompt Pi (a point, box, or mask) specifying the subject of interest, the goal of Click2Graph is to generate structured interaction tracklets. Each tracklet describes subject si, one of its interacting objects oi,j, the relationship ri,j between them, and the corresponding panoptic masks over time. Formally, for subject i, the set of interaction tracklets is: ATi = (cid:8)ati = si, oi,j, ri,j, SM, OM, tstart, tend(cid:9)Mi j=1 , where SM and OM denote the subject and object panoptic masks across the active temporal window [tstart, tend] and Mi is the number of activities carried by subject si. Images are treated as special case with = 1. The model supports multiple subjects, and users may introduce new prompts at any time. 3.2. Network Architecture Click2Graph builds on SAM2 [24], promptable video segmentation model that produces fine-grained, temporally consistent masks from sparse visual prompts. SAM2 is class-agnostic and yields masks for one object per prompt. To enable interaction discovery and semantic reasoning, we introduce two modules: 1. Dynamic Interaction Discovery Module (DIDM) predicts set of subject-conditioned object prompts. 2. Semantic Classification Head (SCH) predicts subject, object, and predicate labels for discovered segments. Together, these modules transform SAM2 from geometric segmentation backbone into full panoptic video Figure 2. Overview of the Click2Graph architecture for user-guided Panoptic Video Scene Graph Generation. From single user prompt, the system segments and tracks the subject, discovers interacting objects via the Dynamic Interaction Discovery Module (DIDM), and predicts subjectobjectpredicate triplets using the Semantic Classification Head (SCH). scene graph generator, see Figure 2, for comprehensive overview of our architecture. 3.3. Dynamic Interaction Discovery Module It is lightweight, set-based transformer module designed to convert single user prompt into fixed set of spatially precise object prompts. DIDM: 1. Receives the encoded image features from the SAM2 backbone, 2. For given subject, generates dedicated subject feature token by combining learnable subject embedding with feature vector derived from the subjects segmentation mask. This token is then prepended to fixed set of Nq learnable object query embeddings, 3. Passes these combined query tokens through series of Transformer layers, where they perform cross-attention against the image features. The queries are trained to shift their attention and encode the presence and location of objects interacting with the subject, 4. Maps the refined object tokens to the normalized (x, y) coordinates for the discovered interacting object prompts The predicted points serve as prompt locations that SAM2 uses to segment candidate interacting objects. We empirically set Nq = 3 to exceed the typical number of objects interacting with subject. Figure 3 illustrates the modules design. 3.4. Semantic Classification Head This module bridges the gap between geometric outputs (masks) and structured, relational understanding (scene It performs the final semantic inference, classigraphs). fying both objects and their relationships. The Semantic Classification Head: 1. Extract semantic features by spatially aggregating the vision features (from the SAM2 encoder) over the predicted segmentation masks, 2. Passes the aggregated subject and object features through dedicated Multilayer Perceptron (MLP) to predict the subjects class label (si) and the objects class label (oi,j) respectively, 3. Concatenates the dedicated features from SAM2 Mask Decoder, specifically, the obj ptr query token for the subject and the discovered object to form subjectobject pair representation, 4. Passes this joint feature vector through separate MLP to predict the complex relationship predicate (ri,j) For each prompted subject i, the output is: O(It Pi) = Nq (cid:91) si, oi,j, ri,j. (1) j=1 3.5. Training Objective We formulate our objective as strategically composed multi-task loss to effectively optimize the heterogeneous output types of our framework: panoptic segmentation masks, precise control over object discovery, and structured semantic reasoning: Ltotal = Lmask + LL2 + Lsub + Lobj + Lrel. Figure 3. Architecture of the Dynamic Interaction Discovery Module (DIDM). single user-prompted subject prompt is transformed into Nq predicted object prompts. It combines feature vector derived from the subject mask with learnable object queries. These tokens pass through Transformer decoder, which performs cross-attention over the image features, enabling the module to autonomously predict the precise locations (via the Point Prediction Head) of all entities interacting with the prompted subject. Mask Loss. For both subject and discovered object masks, we use combination of: For each loss term Ll we use an appropriate loss weight λl which are set as: Lmask = LBCE + LIoU + LDice. λBCE = 10, λDice = 1, λIoU = 1, Prompt Localization Loss. Each DIDM-predicted point is supervised with: LL2 = ˆp p2 2, λL2 = 20, λsub = 10, λobj = 10, λrel = 20. Inference runs at 10 FPS on an NVIDIA A100 (40GB), with memory footprint of 7GB for Video input resolution of (1024x1024). where is ground-truth object interior point. 3.7. Ground-Truth Point Sampling Strategy Semantic Prediction Loss. We apply cross-entropy losses to: Lsub, Lobj, Lrel. Set-Based Hungarian Matching. Because DIDM and SCH generate fixed set of predictions, we adopt the bipartite matching strategy from DETR [2] to align predictions with ground-truth interaction sets. 3.6. Training and Inference Details We use SAM2.1-Large as the backbone and freeze its 224M parameters. DIDM and SCH introduce approximately 5M trainable parameters. Training uses AdamW with learning rate 5104 for SCH parameters and cosine annealing schedule with start value 5105 and end value 1105 for DIDM parameters. We train for 400 epochs, sampling 8frame clips per batch following SAM2s video-centric strategy. Training DIDM requires stable ground-truth points inside each object mask. Boundary points are ambiguous for promptable models like SAM2, whereas interior points yield clearer supervisory signals. We therefore: 1. Compute the distance transform of each object mask, 2. Assign each pixel sampling probability proportional to its distance from the mask boundary, 3. Sample core interior points as high-quality targets. This distance-weighted sampling generates robust supervision for DIDMs point regression and improves object discovery accuracy. 4. Dataset: OpenPVSG The Panoptic Video Scene Graph (PVSG) task requires pixel-level grounding of entities and their relationships across time. We evaluate Click2Graph on the Open Panoptic Video Scene Graph (OpenPVSG) dataset introduced 5 by Yang et al. [32], which provides the most comprehensive benchmark for panoptic-level video scene graph generation. spatial precision, prompt generation reliability, and overall scene graph accuracy. Scale and Composition: OpenPVSG contains 400 videos totaling approximately 150k frames at 5 FPS. On those videos, each subject is typically interacting with 2 objects per frame. The data spans wide range of environments and camera styles, aggregated from: VidOR [25] (289 videos), EPIC-Kitchens [6] (55 videos), Ego4D [8] (56 videos). The dataset includes both third-person and egocentric perspectives, enabling evaluation under diverse motion patterns, object configurations, and interaction types. Annotations: The annotation set includes: 126 object categories grounded with pixel-accurate panoptic segmentation, 57 relationship predicates covering spatial, contact, and interaction types, Temporally consistent instance masks and relation trajectories. Panoptic masks allow detailed grounding of both things and stuff classes, which is essential for modeling nonrigid entities and background interactions. Relevance to Click2Graph: OpenPVSG provides challenging testbed for user-guided PVSG for three reasons: 1. High visual and temporal diversity: videos include complex camera motion, occlusions, multiple interacting entities, and indoor/outdoor environments. 2. Fine-grained semantic space: the large number of closely related object and predicate classes exposes the difficulty of semantic reasoning. 3. Panoptic-level grounding: pixel-accurate masks are necessary for evaluating prompt localization, segmentation, and relationship prediction. These characteristics make OpenPVSG an ideal benchmark for assessing Click2Graphs ability to combine visual prompting, object discovery, panoptic segmentation, and relational reasoning in unified framework. 1. Recall@K (End-to-End Semantic Interaction Recall) Recall@K (R@K) measures full triplet correctness. prediction si, oi,j, ri,j is counted as correct if: 1. Subject, object, and predicate labels match the ground truth; and 2. The predicted subject and object masks both achieve IoU τ with the corresponding ground-truth masks. Predictions are ranked by confidence, and only the top-K are used. This metric evaluates the complete Click2Graph pipeline, combining DIDM, SAM2 segmentation, and SCH semantic reasoning. Following prior PVSG work, we set the IoU threshold to τ = 0.5. 2. Spatial Interaction Recall (SpIR) SpIR isolates the quality of spatial grounding. While calculating this metric, subjectobject pair is considered correct if it satisfies the following requirement: IoU( ˆSM, SM) τ and IoU( ˆOM, OM) τ, regardless of predicted class or predicate labels. This metric evaluates the combined effectiveness of DIDM in producing appropriate object prompts and SAM2 in propagating precise panoptic masks over time. 3. Prompt Localization Recall (PLR) PLR measures the accuracy of DIDMs predicted object prompt points. discovered object prompt ˆpi,j is counted as correct if it lies within the ground-truth object mask: ˆpi,j OM i,j. PLR thus assesses the reliability of interaction discovery independently of subsequent segmentation or semantic prediction. Evaluation Protocol. Because prompt-based systems are sensitive to initial user inputs, we evaluate robustness by repeating each experiment 25 times, sampling unique initial point from the subjects ground-truth mask for each run. We report all metrics as mean standard deviation across runs. 5. Evaluation Metrics 6. Results & Ablations Click2Graph integrates visual prompting, interaction discovery, panoptic segmentation, and semantic reasoning into unified pipeline. Standard SGG metrics such as Predicate Classification (PREDCLS), Scene Graph Classification (SGCLS), and Scene Graph Detection (SGDET) are therefore inappropriate to characterize system performance. We evaluate using three complementary recall-based metrics that provides fine-grained evaluation of our models 6 We evaluate Click2Graph on the OpenPVSG benchmark using the three metrics introduced in Section 5. These metrics allow us to separately assess (1) semantic triplet reasoning, (2) segmentation and interaction grounding quality, and (3) the reliability of object prompt generation. End-to-End compares Click2Graph with prior automated PVSG approaches. Performance: Table Table 2. Comparison of standard Recall@K metrics between Click2Graph and prior automated PVSG approaches. Prior methods generate full-frame proposals and must detect subjects; Click2Graph receives subject prompt, reflecting its interactive setting. Method Recall@3 Recall@20 PVSG [32] + IPS+T [4, 29] PVSG [32] + VPS [4, 15] MACL [22] + IPS+T MACL [22] + VPS Click2Graph (Ours) - - - - 2.23 3.88 0.42 4.51 0.84 - Unlike these methods, which generate dense full-frame proposals and must detect subjects, our work receives subject prompt and produces only the interaction-centric predictions associated with that target. Despite generating far fewer predictions per frame (Nq = 3, compared to 100 in automated baselines), Click2Graph achieves competitive R@K scores. This demonstrates that targeted, user-guided reasoning can reduce the search space while preserving strong semantic alignment. Furthermore, the interactive paradigm makes Click2Graph complementary to fully automated PVSG methods, offering practical path toward controllable and corrective scene graph generation. Robustness to Prompt Type: We study how the quality of user-specified prompts influences Click2Graph by comparing three forms of input: single point, bounding box, and full segmentation mask. During training, point and box prompts are sampled with high probability (0.49 each), reflecting low-effort user inputs, while mask prompts are used rarely (0.02). As shown in Table 3, performance varies modestly across prompt types: masks yield slightly higher scores, as expected, but all three provide stable results, with low variance across runs. This confirms that Click2Graph is robust to imperfect or low-precision user interactions, which is key requirement for practical deployment. Contribution of System Components: The three metrics jointly reveal the behavior of Click2Graphs submodules. High PLR scores indicate that the Dynamic Interaction Discovery Module reliably generates subject-conditioned prompts that fall inside the correct object regions. Strong SpIR performance demonstrates that SAM2, when guided by these prompts, yields accurate panoptic masks for both subjects and interacting objects. R@K remains the most challenging metric, reflecting the difficulty of fine-grained label and predicate classification. Most semantic errors arise from confusions between visually similar categories (e.g., child vs. baby, box vs. bag, floor vs. ground), consistent with the long-tail and high redundancy of the OpenTable 3. Ablation experiment showing robustness to different prompt types. Dataset Prompt R@3 SpIR PLR Epic K. Mask Point BBox Ego4d Mask Point BBox"
        },
        {
            "title": "Vidor Mask\nPoint\nBBox",
            "content": "1.78 24.22 1.140.38 23.041.08 32.060.81 2.080.06 25.020.09 31.960.09 30.67 0.73 17.22 0.560.04 16.211.04 39.870.38 0.720.06 17.490.32 38.970.11 38. 3.33 18.77 2.720.25 15.370.55 28.860.34 3.180.10 17.590.36 30.130.23 30.82 PVSG semantic space. Importance of DIDM: To isolate the contribution of the Dynamic Interaction Discovery Module, we replace it with heuristic that samples prompts from dataset-level objectprobability heatmap. The heuristic assigns high likelihood to locations where objects commonly appear but is not conditioned on the prompted subject. Table 4 shows that this replacement severely degrades PLR, SpIR, and R@K across all datasets. This highlights that subject-conditioned prompt generation is essential for interaction-centric reasoninggeneric object priors are insufficient to capture the relational structure required for PVSG. Table 4. Comparison of the different metric based on the interaction discovery strategy. Dataset Strategy Metric Epic K. Heuristic Ego4d Vidor DIDM(ours) Heuristic DIDM(ours) Heuristic DIDM(ours) R@3 SpIR PLR 0.62 2. 0.28 0.73 0.68 3.33 5.14 25.02 4.26 17.49 4.66 18.77 10.60 32. 9.30 39.87 10.19 30.82 Qualitative Analysis: Figure 4 illustrates Click2Graphs behavior across diverse scenarios. In the first row, the system correctly recovers multiple interacting objects and produces coherent triplets. The second row demonstrates temporal robustness: even after partial occlusion or momentary subject disappearance, the system continues to produce consistent predictions. Failure cases (third row) typically involve predicate granularity (on vs. sitting) or object categories with subtle visual differences (gift vs. box). 7 (a) Prediction: adult, box, holding GT: adult, box, holding (b) Prediction: adult, bag, holding GT: adult, bag, holding (c) Prediction: adult, spatula, holding GT: adult, spatula, holding (d) Prediction: ball, grass, on GT: ball, grass, on (e) Subject temporarily occluded by camera motion (f) Prediction: ball, grass, on GT: ball, grass, on (g) Prediction: child, floor, sitting GT: child, floor, on (h) Prediction: child, box, holding GT: child, gift, holding (i) Prediction: child, helmet, wearing Triplet not in Ground Truth Figure 4. Qualitative results illustrating correct predictions, occlusion robustness, and typical failure cases. These examples visually corroborate our quantitative findings: segmentation and interaction discovery are reliable, while semantic classification remains the primary bottleneck. 7. Conclusions and Future Work We introduced Click2Graph, the first user-guided framework for Panoptic Video Scene Graph Generation. By combining single visual prompt with subject-conditioned interaction discovery and semantic reasoning, Click2Graph interpretable video understanding. enables controllable, Central to the system is the Dynamic Interaction Discovery Module, which reliably generates object prompts conditioned on the user-specified subject, and the Semantic Classification Head, which elevates promptable segmentation into full triplet prediction. Together, these components transform SAM2 into complete PVSG pipeline capable of structured, interaction-centric reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph achieves strong spatial grounding and reliable object discovery, while highlighting the challenges of fine-grained semantic classification in large, diverse label space. Most errors arise from distinctions between visually similar object categories or predicates, suggesting that semantic reasoning, rather than segmentation or interaction discovery, is the primary bottleneck. limitation of the current system is that real-time user intervention is restricted to segmentation correction; users cannot directly modify predicted labels during inference, and such corrections do not yet feed back into the model. As future work, we plan to integrate lightweight feedback mechanism in which user-provided label corrections dynamically update set of learnable class embeddings. This would enable Click2Graph to adapt its semantic predictions over time and maintain consistency across future frames. Beyond label correction, Click2Graph opens several promising research directions, including (1) integrating language models to enhance predicate reasoning and reduce fine-grained semantic confusion, (2) developing multisubject prompting strategies for complex multi-agent interactions, and (3) leveraging interactive supervision to improve long-tail predicate learning. By unifying promptable segmentation with subject-conditioned relational inference, Click2Graph offers foundation for the next generation of interactive, human-centered video scene understanding systems."
        },
        {
            "title": "References",
            "content": "[1] Oron Ashual and Lior Wolf. Interactive scene generation via In Proceedings of the AAAI scene graphs with attributes. Conference on Artificial Intelligence, pages 1365113654, 2020. 3 [2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 5 [3] Mu Chen, Liulei Li, Wenguan Wang, and Yi Yang. Diffvsgg: Diffusion-driven online video scene graph generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2916129172, 2025. 2 [4] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. 7 [5] Yuren Cong, Wentong Liao, Hanno Ackermann, Bodo Rosenhahn, and Michael Ying Yang. Spatial-temporal transformer for dynamic scene graph generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1637216382, 2021. 1, 2, 3 [6] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epickitchens dataset. In European Conference on Computer Vision (ECCV), 2018. 6 [7] Shengyu Feng, Hesham Mostafa, Marcel Nassar, Somdeb Majumdar, and Subarna Tripathi. Exploiting long-term deIn Propendencies for generating dynamic scene graphs. ceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 51305139, 2023. [8] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. 6 [9] Tao He, Lianli Gao, Jingkuan Song, and Yuan-Fang Li. Towards open-vocabulary scene graph generation with promptbased finetuning. In European Conference on Computer Vision, pages 5673. Springer, 2022. 3 [10] Yuk Heo, Yeong-Jun Lee, and Chang-Su Kim. Guided interactive video object segmentation using reliability-based attention maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14134 14143, 2021. 3 [11] ASM Iftekhar, Raphael Ruschel, Satish Kumar, Suya You, and BS Manjunath. Dds: Decoupled dynamic scene-graph generation network. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 9670 9680. IEEE, 2025. 2 [12] Siddhesh Khandelwal and Leonid Sigal. Iterative scene graph generation. Advances in Neural Information Processing Systems, 35:2429524308, 2022. 1, [13] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, Piotr Dollar, arXiv preprint and Ross Girshick. arXiv:2304.02643, 2023. 1, 3 Segment anything. [14] Chaotao Li, Weixuan Wang, Fan Zhang, and Hao Zhang. Sgsg: Stroke-guided scene graph generation. IEEE Transactions on Visualization and Computer Graphics, 2025. 3 [15] Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen, Guangliang Cheng, Yunhai Tong, and Chen Change Loy. Video k-net: simple, strong, and unified baseline for video segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18847 18857, 2022. 7 [16] Yiming Li, Xiaoshan Yang, and Changsheng Xu. Dynamic scene graph generation via anticipatory pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1387413883, 2022. 2 [17] Yanjun Li, Zhaoyang Li, Honghui Chen, and Lizhi Xu. Unbiased video scene graph generation via visual and semantic dual debiasing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2 [18] Xin Lin, Chong Shi, Yibing Zhan, Zuopeng Yang, Yaqi Wu, and Dacheng Tao. Td2-net: Toward denoising and debiasing for dynamic scene graph generation. ArXiv, abs/2401.12479, 2024. [19] Jiale Lu, Lianggangxu Chen, Youqi Song, Shaohui Lin, Changbo Wang, and Gaoqi He. Prior knowledge-driven dynamic scene graph generation with causal inference. In Proceedings of the 31st ACM International Conference on Multimedia, page 48774885, New York, NY, USA, 2023. Association for Computing Machinery. 2 [20] Otniel-Bogdan Mittal, Han Zhang, Tae-Hyun Park, and Sohn. Interactive image generation from scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 00, 2019. 3 [21] Sayak Nag, Kyle Min, Subarna Tripathi, and Amit RoyChowdhury. Unbiased scene graph generation in videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2100821018, 2023. 2 9 Zhang, Chen Change Loy, et al. Panoptic video scene graph In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 18675 18685, 2023. 1, 2, 3 [34] Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, and Yueting Zhuang. Visually-prompted language model for fine-grained scene graph generation in an open world. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2156021571, 2023. [35] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing with global conIn Proceedings of the IEEE conference on computer text. vision and pattern recognition, pages 58315840, 2018. 1, 2, 3 [36] Yong Zhang, Yingwei Pan, Ting Yao, Rui Huang, Tao Mei, and Chang-Wen Chen. End-to-end video scene graph generation with temporal propagation transformer. IEEE Transactions on Multimedia, 26:16131625, 2023. 2 [37] Zijian Zhou, Miaojing Shi, and Holger Caesar. Vlprompt: Vision-language prompting for panoptic scene graph generation. arXiv preprint arXiv:2311.16492, 2023. 3 [22] Thong Thanh Nguyen, Xiaobao Wu, Yi Bin, Cong-Duy Nguyen, See-Kiong Ng, and Anh Tuan Luu. Motion-aware contrastive learning for temporal panoptic scene graph generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 62186226, 2025. 7 [23] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 1, 3 [24] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. [25] Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua. Annotating objects and relations in userIn Proceedings of the 2019 on Internagenerated videos. tional Conference on Multimedia Retrieval, pages 279287. ACM, 2019. 6 [26] Oytun Ulutan, ASM Iftekhar, and Bangalore Manjunath. Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1361713626, 2020. 2 [27] Guan Wang, Zhimin Li, Qingchao Chen, and Yang Liu. Oed: Towards one-stage end-to-end dynamic scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2793827947, 2024. 2 [28] Shuang Wang, Lianli Gao, Xinyu Lyu, Yuyu Guo, Pengpeng Zeng, and Jingkuan Song. Dynamic scene graph generation via temporal prior inference. In Proceedings of the 30th ACM International Conference on Multimedia, page 57935801, New York, NY, USA, 2022. Association for Computing Machinery. 2 [29] Zhongdao Wang, Hengshuang Zhao, Ya-Li Li, Shengjin Wang, Philip Torr, and Luca Bertinetto. Do different tracking tasks require different appearance models? Advances in Neural Information Processing Systems, 34:726738, 2021. 7 [30] Danfei Xu, Yuke Zhu, Christopher Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 54105419, 2017. 1, [31] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic scene graph generation. In European conference on computer vision, pages 175192. Springer, 2022. 1, 2, 3 [32] Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, Wayne Zhang, Chen Change Loy, and Ziwei Liu. Panoptic video scene graph generation. In CVPR, 2023. 6, 7 [33] Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, Wayne"
        }
    ],
    "affiliations": [
        "UC Santa Barbara Electrical & Computer Engineering"
    ]
}