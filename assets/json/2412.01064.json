{
    "paper_title": "FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait",
    "authors": [
        "Taekyung Ki",
        "Dongchan Min",
        "Gyoungsu Chae"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 1 4 6 0 1 0 . 2 1 4 2 : r FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait Taekyung Ki Dongchan Min2 Gyoungsu Chae1 1 DeepBrain AI Inc. 2 Graduate School of AI, KAIST taek@deepbrain.io alsehdcks95@kaist.ac.kr gc@deepbrain.io https://deepbrainai-research.github.io/float/ Figure 1. FLOAT generates talking portrait video from single source image and audio where the talking motion is generated by the motion latent flow matching. It can enhance the emotion-related talking motion by leveraging speech-driven emotion labels, natural way of emotion-aware motion control."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audiodriven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce transformer-based vector field predictor with simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency. Animating single image using driving audio (i.e., audiodriven talking portrait generation) has gained significant attention in recent years for its great potential in avatar creation, video conferencing, virtual avatar chat, and userfriendly customer service. It aims to synthesize natural talking motion from audio signals, including accurate lip synchronization, rhythmical head movements, and fine-grained facial expressions. However, generating such motion solely from audio is extremely challenging due to its one-to-many correlation between audio and motion. In the earlier stage of this field, many works [9, 23, 34, 54, 58, 98] focus on generating accurate lip movements by relying on learned audio-lip alignment losses [10, 52]. To comprehensively extend the range of motion, some works [52, 74, 96] incorporate probabilistic generative models, such as VAE [35] and normalizing flow [60], turning the motion generation into probabilistic sampling. However, these models still lack expressiveness in generated motion due to the limited capacity of these generative models. 1 Recent talking portrait generation methods [8, 25, 31, 43, 51, 70, 76, 80, 86, 89], powered by diffusion-based generative models [27, 68], successfully mitigate this expressiveness issue. Specifically, EMO [76] introduces promising approach to this field [8, 31, 80, 86, 89] by employing strong pre-trained image diffusion model (i.e., StableDiffusion [61]) and lifting it into video generation [29]. However, it still faces challenges in generating temporally coherent videos and achieving sampling efficiency, requiring tens of minutes for few seconds of video. Moreover, they heavily rely on auxiliary facial prior, such as bounding boxes [76, 89], 2D landmarks and skeletons [8, 31, 94], or 3D meshes [86], which significantly restricts the diversity and the fidelity of head movements due to their strong spatial bias. In this paper, we present FLOAT, an audio-driven talking portrait video generation model based on flow matching generative model. Flow matching [42, 44] has emerged as promising alternative to diffusion models due to its fast and high-quality sampling. By modeling talking motion within learned motion latent space [85], we can more efficiently sample temporally consistent motion latents. This is achieved by simple yet effective transformer-based [79] vector field predictor, inspired by DiT [55], which also enables natural emotion-aware motion generation driven by speech. Our contributions are summarized as follows: We present, FLOAT, flow matching based audio-driven talking portrait generation model using learned motion latent space, which is more efficient and effective than pixel-based latent spaces. We introduce simple yet effective transformer-based flow vector field predictor for temporally consistent motion latent sampling, which also enables the speechdriven emotional controls. Extensive experiments demonstrate that FLOAT achieves state-of-the-art performance compared to both diffusionand non-diffusion-based methods. 2. Related Works 2.1. Diffusion Models and Flow Matching Diffusion Models. Diffusion models or score-based generative models [14, 27, 53, 61, 67, 68] are generative models that gradually diffuse input signals into Gaussian noise and learn the denoising reverse process for the generative modeling. They have shown remarkable results in various generation tasks, such as unconditional image and video generation [4, 18, 55], text-to-image generation [59, 61, 62], text-to-video generation [4, 24], conditional image generation [29, 94], and 3D human generation [37, 71, 75]. Accelerating Diffusion Models. While diffusion models demonstrate superior performance, their iterative sampling nature still bottlenecks the efficient generation compared to VAEs [35], normalizing flow [60], and GANs [22]. To overcome this limitation, several works have been developed to boost the sampling speed of the diffusion models. StableDiffusion (SD) [61] partially mitigates this problem by moving the diffusion process from the pixel space to the spatial latent space, establishing itself as pivotal framework among diffusion models. Another line of research has developed the sampling solvers [47, 48] based on ordinary differential equations (ODEs). Meanwhile, model distillation [26] has been introduced to transfer the knowledge of the learned diffusion models into student model, enabling one (or few) steps of generation [32, 41, 45, 49, 69]. However, these approaches involve substantial effort to create welltrained diffusion model and suffer from training instability. Flow Matching. Flow matching [42, 44] stands out as an alternative to diffusion models for its high sampling speed and competitive sample quality compared to diffusion models [11, 20, 39, 42, 57]. It belongs to the family of flow-based generative models, which estimates transformation (referred to as flow) between prior distribution (e.g., Gaussian) and target distribution. Unlike the normalizing flow [15, 60] that directly estimates the noise-todata transformation under specific architectural constraints (e.g., affine coupling), flow matching regresses the timedependent vector field that generates this flow by solving its corresponding ODEs [7] with flexible architectures. One specific design of flow matching is an optimal transport (OT) based one, which transforms the data distribution along the straight path with constant velocity [42]. Our audio-driven talking portrait method employs flow matching to generate the natural talking motions. Thanks to the architectural flexibility of flow matching, we use transformer-encoder architecture [79] to estimate the generating vector field, allowing us to take the video temporal consistency into account. 2.2. Audio-driven Portrait Animation Audio-driven portrait animation is the task of generating realistic talking portrait video using single portrait image and driving audio [52, 82, 96, 99, 100]. Since audio-tomotion relation is basically one-to-many problem, several works utilize additional facial prior for driving conditions, e.g., 2D facial landmarks [8, 25, 31, 80, 86, 100], 3D prior [9, 50, 51, 91, 96], or emotional labels [30, 73, 90]. In earlier stages, most works [9, 23, 34, 58] focused on generating accurate lip motion from audio by utilizing the lip-sync discriminator [10]. These approaches have advanced to generating audio-related head poses in probabilistic way. For example, StyleTalker [52] uses normalizing flow [15, 60] to generate the head motion from audio, while SadTalker [96] uses audio-conditional variational inference [35] to learn the 3DMM coefficients [2], bridging the intermediate representations of pre-trained portrait animator [83]. 2 Meanwhile, several works [30, 73, 81, 87] focus on an emotion-aware talking portrait generation. In particular, EAMM [30] considers an emotion as the complementary displacement of facial motion, and learns these displacement from an emotion label extracted from the image. Recent audio-driven talking portrait methods powered by diffusion models show remarkable results [8, 31, 43, 51, 76, 80, 86, 89, 90]. Specifically, EMO [76] and subsequent extensions [8, 80, 86, 89] utilize the pre-trained SD [61] as their backbone to leverage generative prior trained on the large-scale image datasets. They introduce additional modules, e.g., ReferenceNet [29] and Temporal Transformer [24], to preserve input identity and enhance the video temporal consistency, respectively. However, these modules introduces additional computational cost, requiring several minutes for few seconds of video, and still suffer from video-level artifacts, such as noisy frames, and flickering. VASA-1 [90] addresses the sampling time issue by sampling motion latents [16], producing lifelike talking portraits. Our method can generate talking motion through the flow matching [42] in learned motion latent space and successfully addresses the inefficient sampling of diffusionbased methods. Moreover, our method can generate emotion-aware talking motions by using speech-driven emotional labels [56]. 3. Preliminaries: (Conditional) Flow Matching Let Rd be data, [0, 1] be the time, and be unknown target distribution. We can define flow as time-dependent transformation φt : [0, 1] Rd Rd that transforms tractable prior distribution p0 to the distribution p1 q. This flow φt further introduces probability flow path pt : [0, 1] Rd R>0 and generating vector field vt : [0, 1] Rd Rd where pt is defined by the push-forwarding pt(x) = p0(φ1 (x)) det (cid:12) (cid:12) (cid:12) (cid:12) φ1 (x) (cid:12) (cid:12) (cid:12) (cid:12) , (1) and vt generates φt by means of an ordinary differential equation (ODE) [7]: dt φt(x) = vt(φt(x)) and φ0(x) = x. (2) Flow matching [42] aims to estimate the target generating vector field ut with neural network parameterized by θ: LFM(θ) := vt(x; θ) ut(x)2 2, (3) where U[0, 1] and pt(x). However, the target generating vector field ut and the sample distribution pt are intractable. To address this issue, [42] proposes method for constructing conditional\" probability path pt(x1) as well as target conditional\" vector field ut(x1) using sample x1 as condition. And they prove that the following objective LCFM(θ) := vt(x; θ) ut(xx1)2 2, (4) where U[0, 1] and pt(xx1), is equivalent to (3) with respect to the gradient θ. One natural way of constructing ut(x1) is straight line\" that connects x0 p0 and x1 q, drawing an optimal transport (OT) path with constant velocity [42]. Specifically, linear time interpolation between x0 and x1 gives us the flow xt = φt(x) = (1 t)x0 + tx1, the conditional probability path pt(xx1) defined via the affine transformation pt(xx1) = (xtx1, t2I), and the target generating vector field ut(xx1) = x1 x0. This specific choice turns the objective (4) into LOT(θ) := vt((1 t)x0 + tx1; θ) (x1 x0)2 2, (5) where U[0, 1], x0 p0, and x1 q, all of which are tractable. Classifier-free Vector Field. [11] formulates classifierfree vector field (CFV) technique for flow matching, which enables class-conditional sampling more controllable manner without any extra classifier trained on noisy trajectory. Formally, CFV compute the modified vector field vt by vt(xt, c; θ) γvt(xt, c; θ) + (1 γ)vt(xt, = ; θ), (6) where γ denotes the guidance scale. vt(xt, = ; θ) is the predicted vector field without driving condition c. For more details, please refer to [11, 42]. 4. Method: Flow Matching for Audio-driven"
        },
        {
            "title": "Talking Portrait",
            "content": "We provide an overview of FLOAT at Fig. 2. Given source image R3HW , and driving audio signal a1:L RLda of length L, our method generates video ˆD1:L = ( ˆDl)L l=1 RL3HW (7) of frames, featuring audio-synchronized talking head motions, including both verbal and non-verbal motions. Our method consists of two phases. First, we pre-train motion auto-encoder, which provides us with the expressive and smooth motion latent space for the talking portraits (Sec. 4.1). Next, we employ flow matching [42] to generate sequence of motion latents with transformer-based vector field predictor using the driving audio, which is decoded to the talking portrait videos (Sec. 4.2). Thanks to simple yet powerful vector field architecture, we can also incorporate speech-driven emotions as the driving conditions, enabling emotion-aware talking portrait generation. Figure 2. Overview of FLOAT. We encode the source image R3HW into the latent with the explicit identity-motion decomposition ws = wsr + wrs Rd. Given audio segments aL:L R(L+L)da of the length + and the reference motion wrs Rd, and the speech-driven emotion label we R7, flow matching transformer estimates the generating vector field vt(φt(x0), ct; θ) RLd from noisy motion latents, which is used to solve corresponding ODE and generates the motion latents wr ˆD1:L . Finally, the sequence of latents wS ˆD1:L := (wSr + wr ˆDl )L l=1 are decoded into the video ˆD1:L RL3HW . 4.1. Motion Latent Auto-encoder Recent talking portrait methods utilize the VAE of StableDiffusion (SD) [61] due to its rich semantic pixel-based latent space. However, they often struggle to generate temporally consistent frames when lifted to video generating tasks [8, 29, 76, 89, 101]. Thus, our first goal for realistic talking portrait is to obtain good motion latent space, capturing both global (e.g., head motion) and fine-grained local (e.g., facial expressions, mouth movement, pupil motion) dynamics. To achieve this, we employ the latent image animator (LIA) [85] as our motion auto-encoder instead of the VAE of SD. The key difference lies in the training objective: LIA is trained to reconstruct driving image from source image sampled from the same video clip, which enforces latent encoding to contain implicit motions that can capture both temporally adjacent and distant motions. Our motion auto-encoder can encode the source into the latent wS Rd with following explicit decomposition: wS = wSr + wrS, (8) where wSr Rd is the identity latent and wrS = (cid:80)M m=1 λm(S) vm Rd is the motion latent with λ(S) = (λm(S))M m=1 RM being the source-dependent motion coefficients that span the learned source-agnostic motion orm=1 Rd. In this space, mothonormal basis = {vm}M tion has distinct (orthogonal) motions with its intensity λm(S). As shown in [85], this explicit decomposition is accomplished with introducing the source-agnostic motion basis . Since the expressiveness of generated motions and the image fidelity are determined by the motion latent space, we scale the original LIA architecture to synthesize higher resolution from 2562 to 5122. Additionally, we introduce simple yet effective facial component perceptual loss using [66, 95] that significantly improves the image fidelity (e.g., Figure 3. Flow matching transformer for predicting the generating vector field at the inference phase. teeth and eyes) as well as fine-grained motions (e.g., eyeball and eyebrows movements). Further details on our motion auto-encoder can be found in supplementary materials. 4.2. Flow Matching in Motion Latent Space In the learned motion latent space, we predict vector field vt(xt, ct; θ) RLd where xt is the sample at flow time [0, 1], and ct RLh represents the driving conditions for consequent frames. This vector field generates the flow φt : [0, 1] RLd RLd of frames by solving ODE (Eq. (2)). As illustrated in Fig. 3, we build our vector field predictor upon the transformer encoder [79] architecture. Specifically, we adopt DiT [55] architecture to decouple frame-wise conditioning from time-axis attention mechanism, which enables us to model temporally consistent motion latents. We refer to this predictor as flow matching transformer (FMT). In DiT [55], distinct semantic tokens are modulated by single diffusion time step embedding and class embedding through adaptive layer normalization (AdaLN). In contrast, FMT modulates each l-th input latent with its corresponding l-th condition and then combines their temporal relations through masked self-attention layer that attends to 2 neighboring frames. Formally, for each l-th frame, frame4 wise AdaLN and frame-wise gating are computed by LN(X γl t) + βl Rh and αl Rh, (9) i, γl i, βl respectively, where {1, 2}, is the hidden dimension, LN denotes layer normalization [40], and is the l-th input for each operation at flow time [0, 1]. The coeffii Rh are computed from the condition cl cients αl Rh through linear layer, ToScaleShift, as depicted in Fig. 3. Speech-driven Emotional Labels. How can we make talking motions more expressive and natural? During talking, humans naturally reflect their emotions through their voices, and these emotions influence talking motions. For instance, person who speaks sadly may be more likely to shake the head and avoid eye contact. This non-verbal motion derived from emotions crucially impacts the naturalness of talking portrait. Existing works [30, 81, 90] use image-emotion paired data or image-driven emotion predictor [63] to generate the emotion-aware motion. In contrast, we incorporate speechdriven emotions, more intuitive way of controlling emotion for audio-driven talking portrait. Specifically, we utilize pre-trained speech emotion predictor [56] that produces softmax probabilities of seven distinct emotions: angry, disgust, fear, happy, neutral, sad, and surprise, which we then input into the FMT. However, as people do not always speak with single, clear emotion, determining emotions solely from audio is often ambiguous [30]. Naive introduction of speech-driven emotion can make emotion-aware motion generation more challenging. To address this issue, we inject the emotions together with other driving conditions at training phase and modify them at inference phase. Driving Conditions. We concatenate the audio representation a1:L RLda of pre-trained Wav2Vec2.0 [1], the speech emotion label we R7, and the source motion latent wrS Rd. Next, we add the flow time step embedding Emb(t) Rh to these conditions, producing ct RLh via linear layer, ToCondition, as depicted in Fig. 2, where Emb(t) is computed using the sinusoidal position embedding [79]. For smooth transitions of sequences longer than the window length L, we follow the convention of [71, 75], extending ct RLh by appending the last audio features from the preceding window. We also use the last frames of target vector field ut as additional input latents. Note that we and wrS are shared across the +L frames. Training. We train FLOAT by reconstructing target vector field computed from driving frames using the corresponding audio segments and source motion latent. We choose pair of driving motions and corresponding audio (wrD1:L , a1:L), and construct the target vector field ut(xwrD1:L) = wrD1:L x0 RLd with noisy input φt(x0) = (1 t)x0 + twrD1:L (t U[0, 1] and x0 5 (01:L, I)). Additionally, we take the preceding audio feature aL:0 and target vector field ut(xwrDL :0) of frames. The flow matching objective LOT(θ) is defined by LOT(θ) = vt(φt(x0), ct; θ) ut(xwrDL :L), (10) where ct R(L+L)h is the driving condition consisting of [t, wrS, we, a1:L, aL:0]. Here, we omit ut(xwrDL :0) for simplicity. Since we predict the vector field rather than noise, we incorporate velocity loss [75] to supervise temporal consistency: Lvel(θ) = vt ut, (11) where vt and ut are the one-frame difference along the time-axis for vt and ut, respectively. The total objective Ltotal is Ltotal(θ) = λOTLOT(θ) + λvelLvel(θ), (12) where λOT and λvel are the balancing coefficients. During training, we apply dropout to wr, we, and a1:L with probability of 0.1 for CFV. Additionally, we apply dropout to the preceding audio and vector field with probability 0.5 for smooth transition in the initial window. Inference. During inference, we sample the generating vector field from noise x0, using the driving conditions wrS, we, and a1:L, as well as the frames of preceding audio and generated vector field. We extend the CFV [11] to an incremental CFV to separately adjust the audio and emotion, inspired by [3]: vt vt(x0, ct{a1:L,we}) (cid:2)vt(x0, ctwe) vt(x0, ct{a1:L,we} + γa + γe [vt(x0, ct) vt(x0, ctwe )] , (cid:3) (13) where γa and γe are the guidance scales for audio and emotion, respectively. ct{x,y} denotes the driving condition without the condition and y. After sampling, ODE solver receives the estimated vector field to compute the motion latents through numerical integration. We experimentally find that FLOAT can generate reasonable motion with around 10 number of function evaluation (NFE). Lastly, we add the source identity latent to the generated motion latents and decode them into video frames using the motion latent decoder. 5. Experiments 5.1. Dataset and Pre-processing For training the motion latent auto-encoder, we use three open-source datasets: HDTF [97], RAVDESS [46], and Table 1. Quantitative comparison results with state-of-the-art methods on HDTF [97] / RAVDESS [46]. The best result for each metric is : evaluated with raw 256 256 resolution outputs. in bold, and the second-best result is underlined. Method SadTalker [96] EDTalk [74] AniTalker [43] Hallo [89] EchoMimic [8] FLOAT (Ours) Image & Video Generation FID 71.952 / 119.430 50.078 / 75.020 39.512 / 70.430 25.363 / 57.648 33.552 / 81.839 21.100 / 31.681 FVD 339.058 / 376.294 211.284 / 304.933 184.454 / 265.341 197.196 / 375.557 296.757 / 320.220 162.052 / 166.359 CSIM 0.644 / 0.644 0.626 / 0.676 0.643 / 0.725 0.869 / 0.860 0.823 / 0.805 0.843 / 0. E-FID 1.914 / 3.500 1.579 / 3.468 1.830 / 2.330 1.039 / 2.492 1.234 / 3.201 1.229 / 1.367 P-FID 1.456 / 2.045 0.054 / 0.090 0.092 / 0.126 0.037 / 0.050 0.023 / 0.047 0.032 / 0.031 Lip Synchronization LSE-D 7.947 / 7.273 8.123 / 7.682 7.907 / 8.176 7.792 / 7.613 8.903 / 8.161 7.290 / 6.994 LSE-C 7.305 / 4.748 7.623 / 5.318 7.288 / 4.555 7.582 / 4.795 6.242 / 4.144 8.222 / 5.730 Figure 4. Qualitative comparison results with state-of-the-art methods on HDTF [97] / RAVDESS [46]. Please refer to supplementary videos. VFHQ [88]. When training FLOAT, we exclude VFHQ because it does not support the synchronized audio. HDTF [97] is for high-definition talking face generation, containing videos of over 300 unique identities. RAVDESS [46] includes more than 2,400 emotion-intensive videos of 24 different identities. VFHQ [88] is designed for highresolution video super-resolution and includes large number of unique identities, which compensates the limited number of identities of the preceding datasets. Following the strategy of [65], we first convert each video to 25 FPS and resample the audio into 16 kHz. Then, we crop and resize the facial region to 5122 resolution using Facealignment [5]. After the pre-processing, for HDTF, we use total of 11.3 hours of 240 videos featuring 230 different identities for training, and videos of 78 different identities, each 15 seconds long, for test. For RAVDESS, we use videos of 22 identities for training, and videos of the remaining 2 identities for test, with each 3-4 seconds long and representing 14 emotional intensities. Note that the identities in the training and test are disjoint in both datasets. 5.2. Implementation Details We use the Euler method [42] as the ODE solver. The motion latent dimension is set to = 512 with = 20 distinct orthogonal directions. For FMT, we use 8 attention heads, 6 Table 2. Ablation studies of FLOAT on HDTF [97]. The best result for each metric is in bold, and the second-best result is underlined. Table 3. Ablation studies of the different NFE of ODE on HDTF [97]. FPS is computed on single NVIDIA V100 GPU. Method Ours (w. Cross-Attention) Ours (w. Diffusion, ϵ-prediction) Ours (w. Diffusion, x0-prediction) FLOAT (Ours) FID 21.873 21.190 21.697 21.100 FVD 162.702 161.666 162.847 162.052 E-FID LSE-D 1.452 1.213 1.278 1. 7.757 9.922 9.048 7.290 Ours-NFE Ours-2 Ours-5 Ours-10 (default) Ours-20 FID 21.785 21.440 21.100 21.158 FVD 178.831 164.463 162.052 164.392 E-FID LSE-D 1.542 1.331 1.229 1. 7.559 7.155 7.290 7.343 FPS 45.22 44.74 41.37 38.20 Table 4. Ablation studies of the audio guidance scale γa and the emotion guidance scale γe on RAVDESS [46]. Guidance scales γa=1, γe=1 γa=1, γe=2 γa=2, γe=1 (default) γa=2, γe=2 FID 33.066 31.844 31.681 32.253 FVD 171.047 166.041 166.359 162. E-FID LSE-D 1.555 1.334 1.367 1.351 7.049 7.212 6.994 6.994 image and video quality, while FMT provides better expression generation and lip synchronization. We also conduct ablation studies on flow matching by comparing it with two types of diffusion models: ϵprediction and x0-prediction. The former predicts noise, while the latter predicts the signal itself [59, 75]. In both cases, we adopt our FMT architecture as denoising networks, and use diffusion 500 steps with cosine noise scheduler. Notably, diffusion and flow matching achieve competitive results on image quality while the latter achieves the better lip synchronization. In Fig. 5, we also compare the forward pass efficiency by measuring frames per second (FPS) of each model. Thanks to the compact motion latent representation, they run 125 faster than Hallo [89], and FLOAT further achieves higher FPS due to the fast sampling of flow matching. Ablation on NFE. In general, increasing the number of function evaluation (NFE) reduces the solution error of ODEs. As shown in Tab. 3, even with small NFE = 2, FLOAT can achieve competitive image quality (FID) and lip synchronization (LSE-D). However, it struggles to capture consistent and expressive motions (FVD and E-FID), resulting in shaky head motion and static expression. This is because FLOAT generates the motion in the latentspace , while image fidelity is determined by the auto-encoder. Ablation on Guidance scales. In Tab. 4, we conduct ablation studies on guidance scales: γa and γe, with the emotion intensive dataset RAVDESS [46]. Note that increasing γa leads to better temporal consistency (FVD) and lip synchronization quality (LSE-D). Moreover, increasing γe improves video consistency (FVD) and expressiveness (EFID). This enables balanced control over emotional audiodriven talking portrait generation. 5.5. Further Results Additional Driving Signals. In Tab. 5 and Fig. 6, we experiment additional driving conditions: driving head poses and image-driven emotion labels, to investigate more controllable talking portraits. In applications, it is often beneFigure 5. Comparison of the forward pass efficiency. We compute FPS on single NVIDIA V100 GPU. hidden dimension = 1024, and an attention window length = 2. We generate = 50 frames with preceding = 10 frames at once, encompassing 2.4 seconds of video. We employ the Adam optimizer [36] with batch size of 8 and learning late of 105. We use L1 distance for the norm in the training objective. We set the balancing coefficients to λOT = λvel = 1. The entire training takes about 2 days for 2, 000k steps on single NVIDIA A100 GPU. 5.3. Evaluation Metrics and Baselines. For evaluating the image and video generation quality, we measure Fréchet Inecption Distance (FID) [64] and 16 frames Fréchet Video Distance (FVD) [78]. For facial identity, expression and head motion, we measure Cosine Similarity of identity embedding (CSIM) [12], Expression FID (E-FID) [76] and Pose FID (P-FID), respectively. Lastly, we measure Lip-Sync Error Distance and Confidence (LSE-D and LSE-C [58]) for audio-visual alignment. We compare our method with state-of-the-art audiodriven talking portrait methods whose official implementations are publicly available. For non-diffusion methods, we compare with SadTalker [96] and EDTalk [74]. For diffusion methods, we compare with AniTalker [43], Hallo [89], and EchoMimic [8]. Comparison Results. In Tab. 1 and Fig. 4, we show the quantitative and qualitative comparison results, respectively. FLOAT outperforms other methods on most of the metrics and visual quality in both datasets. 5.4. Ablation Studies Ablation on FMT and Flow Matching. We compare FMT, which uses frame-wise AdaLN (and gating), followed by masked self-attention to separate conditioning from attending, with cross-attention-based transformer that performs conditioning and attending simultaneously. As shown in Tab. 2, both approaches achieve competitive 7 Figure 6. Additional conditioning results of FLOAT. 3DPose, S2E, and I2E denote 3D head pose parameters [13], speech-to-emotion [56], and image-to-emotion [63], respectively. Table 5. Quantitative results of FLOAT with additional conditions (HDTF [97] / RAVDESS [46]). S2E, I2E, and 3DPose denote speech-to-emotion [56], image-to-emotion [63], and 3DMM pose parameters [13], respectively. Configurations FLOAT (Ours) + 3DPose - S2E - S2E + I2E FID 21.100 / 31.681 19.721 / 29.721 21.235 / 32.035 21/528 / 31.609 FVD 162.052 / 166.359 126.663 / 112.894 155.032 / 166.866 158.577 / 162.369 E-FID 1.229 / 1.367 0.926 / 1.152 1.254 / 1.502 1.158 / 1.305 P-FID 0.032 / 0.031 0.012 / 0.016 0.031 / 0.025 0.034 / 0.022 LSE-D 7.290 / 6.994 7.516 / 7.047 7.264 / 7.222 7.183 / 7.150 ficial to create talking portrait with pose driving manner (e.g., stitching back to existing video). To achieve this, we employ 3DMM head pose parameters R6 [2] extracted from [13]. We concatenate sequence of pose parameters p1:L RL6 with the other driving conditions frame by RLh. We also conduct frame, and then map them to c1:L experiments on the driving emotion by dropping the speechdriven emotion and replacing it with the image-driven emotion [63]. Notably, introducing pose parameters significantly improves the image and video metrics. This is because the driving head poses help to capture the head poses of the target distribution. Moreover, both speech-driven emotion and image-driven emotion consistently improve the generated motion quality in the emotion-intensive dataset, where the image-driven emotion achieves slightly better metrics. This is because the image-driven approach is slightly less ambiguous compared to the speech-driven approach. Redirecting Speech-driven Emotion. Since FLOAT is trained on the emotional-intensive video dataset [46], we can change generated emotion-aware talking motion to different emotion at inference by manually redirecting the predicted emotion label to another one (e.g., one-hot label). As shown in Fig. 7, this enables manual redirection when the predicted emotion from speech is complex or ambiguous. User Study. In Tab. 6, we conduct mean opinion score (MOS) based user study to compare the perceptual quality of each method (e.g., teeth clarity and naturalness of emotion). We generate 6 videos by using the baselines and Figure 7. Redirecting the unclear emotion prediction to desirable one-hot encoding, which can be further intensified by the CFV. Table 6. Mean opinion score (MOS) study results with 95% confidence interval. The score ranges in 1 to 5. The best result for each metric is in bold. Method SadTalker [96] EdTalk [74] AniTalker [43] Hallo [89] EchoMimic [8] FLOAT (Ours) Lip Sync Accuracy 2.20 0.35 2.50 0.34 2.70 0.31 3.30 0.32 2.67 0.37 3.93 0. Head Motion Diversity 2.03 0.26 2.60 0.28 3.00 0.30 2.73 0.35 3.07 0.30 3.57 0.33 Teeth Clarity 1.53 0.19 1.17 0.17 2.13 0.27 2.23 0.27 2.20 0.34 4.13 0.27 Natural Emotion 1.80 0.28 2.07 0.36 3.17 0.27 2.67 0.35 2.50 0.37 3.77 0.30 Overall Visual Quality 1.97 0.23 1.83 0.27 2.63 0.26 2.27 0.33 2.70 0.36 3.87 0.30 FLOAT, and ask 15 participants to evaluate each generated video with five evaluation factors in the range of 1 to 5. As shown in Tab. 6, FLOAT outperforms the baselines. 6. Conclusion We proposed FLOAT, flow matching based audio-driven talking portrait generation model leveraging learned motion latent space. We introduced transformer-based vector field predictor, enabling temporally consistent motion generation. Additionally, we incorporated speech-driven emotion labels into the motion sampling process to improve the naturalness of the audio-driven talking motions. FLOAT addresses current core limitations of diffusion-based talking portrait video generation methods by reducing the sampling time through flow matching while achieving the remarkable sample quality. Extensive experiments verified that FLOAT achieves state-of-the-art performance in terms of visual quality, motion fidelity, and efficiency. Discussion. We leave further discussion considering limitations, future work, and ethical considerations in the supplementary materials."
        },
        {
            "title": "References",
            "content": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for selfsupervised learning of speech representations. Advances in neural information processing systems, 33:1244912460, 2020. 5 [2] Volker Blanz and Thomas Vetter. morphable model In Proceedings of the 26th for the synthesis of 3d faces. annual conference on Computer graphics and interactive techniques, pages 187194, 1999. 2, 8 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instrucIn Proceedings of the IEEE/CVF Conference on tions. Computer Vision and Pattern Recognition (CVPR), pages 1839218402, 2023. 5, 16 [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators, 2024. 2 [5] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem? (and dataset of 230,000 3d facial landmarks). In International Conference on Computer Vision, 2017. 6, 13 [6] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 62996308, 2017. [7] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. 2, 3 [8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions. arXiv preprint arXiv:2407.08136, 2024. 2, 3, 4, 6, 7, 8, 15, 16 [9] Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, and Nannan Wang. Videoretalking: Audio-based lip synchronization for talking head video editing in the wild. In SIGGRAPH Asia 2022 Conference Papers, pages 19, 2022. 1, 2 [10] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian Conference on Computer Vision, pages 251263, 2016. 1, 2, 15 [11] Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow matching in latent space. arXiv preprint arXiv:2307.08698, 2023. 2, 3, 5 [12] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 46904699, 2019. 7, [13] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In Proceedings of the IEEE/CVF conference on com9 puter vision and pattern recognition workshops, pages 00, 2019. 8, 15 [14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2 [15] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016. 2 [16] Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Aleksei Ivakhnenko, Victor Lempitsky, and Egor Zakharov. Megaportraits: One-shot megapixel neural head avatars. In Proceedings of the 30th ACM International Conference on Multimedia, pages 26632671, 2022. [17] Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, and Maja Pantic. Emoportraits: Emotion-enhanced multimodal oneshot head avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84988507, 2024. 13 [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2 [19] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and Taku Komura. Faceformer: Speech-driven 3d facial animaIn Proceedings of the IEEE/CVF tion with transformers. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1877018780, 2022. 15 [20] Johannes Fischer, Ming Gui, Pingchuan Ma, Nick Stracke, Stefan Baumann, and Björn Ommer. Boosting latent diffusion with flow matching. arXiv preprint arXiv:2312.07360, 2023. 2 [21] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 24142423, 2016. 14 [22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [23] Jiazhi Guan, Zhanwang Zhang, Hang Zhou, Tianshu Hu, Kaisiyuan Wang, Dongliang He, Haocheng Feng, Jingtuo Liu, Errui Ding, Ziwei Liu, et al. Stylesync: High-fidelity generalized and personalized lip sync in style-based genIn Proceedings of the IEEE/CVF Conference on erator. Computer Vision and Pattern Recognition (CVPR), pages 15051515, 2023. 1, [24] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3 [25] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, et al. Gaia: Zero-shot talking avatar generation. arXiv preprint arXiv:2311.15230, 2023. 2, 16 [26] Geoffrey Hinton. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. 2 [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 15 [28] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460, 2021. 15 [29] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 81538163, 2024. 2, 3, [30] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne Wu, Feng Xu, and Xun Cao. Eamm: One-shot emotional talking face via audio-based emotion-aware motion model. In ACM SIGGRAPH 2022 Conference Proceedings, pages 110, 2022. 2, 3, 5 [31] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audiodriven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634, 2024. 2, 3, 16 [32] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. arXiv preprint arXiv:2405.05967, 2024. 2 [33] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of the ing the image quality of stylegan. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 81108119, 2020. 13, 18 [34] Taekyung Ki and Dongchan Min. Stylelipsync: Style-based personalized lip-sync video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2284122850, 2023. 1, 2 [35] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 1, [36] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 7, 14 [37] Tobias Kirschstein, Simon Giebenhain, and Matthias Nießner. Diffusionavatars: Deferred diffusion for highfidelity 3d head avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 54815492, 2024. 2 [38] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. 14 [39] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guided multilingual universal speech generation at scale. Advances in neural information processing systems, 36, 2024. 2 [40] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. ArXiv e-prints, pages arXiv1607, 2016. 5 [41] Senmao Li, Taihang Hu, Fahad Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang. Faster diffusion: Rethinking the role of unet encoder arXiv preprint arXiv:2312.09608, in diffusion models. 2023. 2 [42] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 3, 6, [43] Tao Liu, Feilong Chen, Shuai Fan, Chenpeng Du, Qi Chen, Xie Chen, and Kai Yu. Anitalker: Animate vivid and diverse talking faces through identity-decoupled facial motion encoding. arXiv preprint arXiv:2405.03121, 2024. 2, 3, 6, 7, 8, 15 [44] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2 [45] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusionIn The Twelfth Internabased text-to-image generation. tional Conference on Learning Representations, 2023. 2 [46] Steven Livingstone and Frank Russo. The ryerson audio-visual database of emotional speech and song (ravdess): dynamic, multimodal set of facial and vocal expressions in north american english. PloS one, 13(5): e0196391, 2018. 5, 6, 7, 8, 14, 16 [47] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35: 57755787, 2022. 2 [48] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [49] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2 [50] Yifeng Ma, Suzhen Wang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, Zhidong Deng, and Xin Yu. Styletalk: One-shot talking head generation with controllable speaking styles. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 18961904, 2023. 2 [51] Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, and Zhidong Deng. Dreamtalk: When expressive talking head generation meets diffusion probabilistic models. arXiv preprint arXiv:2312.09767, 2023. 2, 3 [52] Dongchan Min, Minyoung Song, Eunji Ko, and Sung Ju style-based audioarXiv preprint Hwang. driven talking head video generation. arXiv:2208.10922, 2022. 1, 2 Styletalker: One-shot [53] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International 10 conference on machine learning, pages 81628171. PMLR, 2021. 2 [54] Se Jin Park, Minsu Kim, Joanna Hong, Jeongsoo Choi, and Yong Man Ro. Synctalkface: Talking face generation with precise lip-syncing via audio-lip memory. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2062 2070, 2022. 1 [55] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195 4205, 2023. 2, 4 [56] Leonardo Pepino, Pablo Riera, and Luciana Ferrer. Emotion recognition from speech using wav2vec 2.0 embeddings. arXiv preprint arXiv:2104.03502, 2021. 3, 5, 8 [57] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [58] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM International Conference on Multimedia, pages 484492, 2020. 1, 2, 7, 15 [59] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey text-conditional arXiv preprint Chu, and Mark Chen. image generation with clip latents. arXiv:2204.06125, 1(2):3, 2022. 2, 7, 15 Hierarchical [60] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 15301538. PMLR, 2015. 1, 2 [61] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 2, 3, 4, 15 [62] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [63] Andrey Savchenko. Hsemotion: High-speed emotion recognition library. Software Impacts, 14:100433, 2022. 5, [64] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https : / / github . com / mseitzer / pytorch - fid, 2020. Version 0.3.0. 7, 15 [65] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order moAdvances in neural tion model for image animation. information processing systems, 32, 2019. 6 [66] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 4, 13 [67] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, 16 and Stefano Ermon. arXiv preprint [68] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [69] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 2 [70] Michał Stypułkowski, Konstantinos Vougioukas, Sen He, Maciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads: Diffusion models beat gans on talking-face genIn Proceedings of the IEEE/CVF Winter Coneration. ference on Applications of Computer Vision, pages 5091 5100, 2024. 2 [71] Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, and Yong-jin Liu. Diffposetalk: Speech-driven stylistic 3d facial animation and head pose generation via diffusion models. ACM Transactions on Graphics (TOG), 43(4):19, 2024. 2, 5, [72] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with In Proceedings of the IEEE conference on convolutions. computer vision and pattern recognition, pages 19, 2015. 15 [73] Shuai Tan, Bin Ji, and Ye Pan. Emmn: Emotional motion memory network for audio-driven emotional talking In Proceedings of the IEEE/CVF Interface generation. national Conference on Computer Vision, pages 22146 22156, 2023. 2, 3 [74] Shuai Tan, Bin Ji, Mengxiao Bi, and Ye Pan. Edtalk: Efficient disentanglement for emotional talking head synthesis. In European Conference on Computer Vision, pages 398 416. Springer, 2025. 1, 6, 7, 8, 15 [75] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In The Eleventh International Conference on Learning Representations, 2023. 2, 5, 7, 15 [76] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. arXiv preprint arXiv:2402.17485, 2024. 2, 3, 4, 7, 15, 16 [77] Alex Trevithick, Matthew Chan, Michael Stengel, Eric R. Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan Chandraker, Ravi Ramamoorthi, and Koki Nagano. Realtime radiance fields for single-image portrait view synthesis. In ACM Transactions on Graphics (SIGGRAPH), 2023. [78] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7, 15 [79] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 2, 4, 5 [80] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei Yang. V-express: Conditional dropout for progres11 [93] Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and Wayne Wu. Celebv-text: large-scale facial text-video dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1480514814, 2023. [94] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2 [95] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In Proceedings of deep features as perceptual metric. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 586595, 2018. 4, 13, 14 [96] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 86528661, 2023. 1, 2, 6, 7, 8, 15, 16 [97] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with In Proceedings of high-resolution audio-visual dataset. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 36613670, 2021. 5, 6, 7, 8, 14, 16 [98] Zhimeng Zhang, Zhipeng Hu, Wenjin Deng, Changjie Fan, Tangjie Lv, and Yu Ding. Dinet: Deformation inpainting network for realistic face visually dubbing on high resoIn Proceedings of the AAAI Conference on lution video. Artificial Intelligence, pages 35433551, 2023. 1 [99] Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. Pose-controllable talking face generation by implicitly modularized audio-visual In Proceedings of the IEEE/CVF Conferrepresentation. ence on Computer Vision and Pattern Recognition (CVPR), pages 41764186, 2021. 2 [100] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. Makelttalk: speaker-aware talking-head animation. ACM Transactions On Graphics (TOG), 39(6):115, 2020. [101] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. arXiv preprint arXiv:2403.14781, 2024. 4 sive training of portrait video generation. arXiv preprint arXiv:2406.02511, 2024. 2, 3 [81] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. Mead: large-scale audio-visual dataset for emotional talking-face generation. In European Conference on Computer Vision, pages 700717. Springer, 2020. 3, 5, 16 [82] Suzhen Wang, Lincheng Li, Yu Ding, Changjie Fan, and Xin Yu. Audio2head: Audio-driven one-shot talkinghead generation with natural head motion. arXiv preprint arXiv:2107.09293, 2021. 2 [83] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. Oneshot free-view neural talking-head synthesis for video conferencing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1003910049, 2021. 2 [84] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. Towards real-world blind face restoration with generative facial prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 91689178, 2021. 13, 14 [85] Yaohui Wang, Di Yang, Francois Bremond, and Antitza Dantcheva. Latent image animator: Learning to animate images via latent space navigation. arXiv preprint arXiv:2203.09043, 2022. 2, 4, 13, 14, [86] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. 2, 3 [87] Yibo Xia, Lizhen Wang, Xiang Deng, Xiaoyan Luo, and Yebin Liu. Gmtalker: Gaussian mixture based emotional talking video portraits. arXiv preprint arXiv:2312.07669, 2023. 3 [88] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: high-quality dataset and benchIn Proceedings of mark for video face super-resolution. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 657666, 2022. 6, 14 [89] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. 2, 3, 4, 6, 7, 8, 15, 16 [90] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. arXiv preprint arXiv:2404.10667, 2024. 2, 3, 5, 16 [91] Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang, and Yujiu Yang. Styleheat: One-shot high-resolution editable talking face generation via pre-trained stylegan. In European conference on computer vision, pages 85101. Springer, 2022. [92] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 325341, 2018. 13 12 Supplementary Materials. In this supplement, we first provide more details on motion latent auto-encoder in Appendix A, regarding the model itself (Appendix A.1), methods for improving the fidelity of facial components (Appendix A.2), the training objective (Appendix A.3), and implementation details (Appendix A.4). In Appendix B, we provide more details on FLOAT, regarding details on evaluation metrics (Appendix B.1), baselines (Appendix B.2), and ablation studies (Appendix B.3). In Appendix C, we provide additional results, including comparison results (Appendix C.1), out-of-distribution results (Appendix C.2), and user study (Appendix C.3). Finally, we discuss ethical considerations, limitations, and future work in Appendix D. A. More on Motion Latent Auto-encoder In this section, we provide more details on our motion latent auto-encoder, including its model architecture, dataset, and training strategy. A.1. Model We provide detailed model architecture of our motion latent auto-encoder in Fig. 11. In Fig. 8a, Fig. 8b, Fig. 8c, and Fig. 8d, we present visualization results of the latent decomposition wS = wSr + wrS Rd (14) of source image S, following the approach of [85]. Notably, the identity latent wSr is decoded into image featuring the average head pose, expression, and field of view in pixel space. A.2. Improving Fidelity of Facial Components Facial Components: Texture vs. Structure. As highlighted in face restoration work [84], facial components such as eyeballs and teeth play important role in the perceptual quality of generated images. It treats the issue as lack of texture (lying in high frequencies) and mitigate it by introducing facial component discriminators with the gram matrix statistics matching. This approach is appropriate in face restoration, where training objective is to reconstruct clear image from degraded one that maintains the same spatial structure, ensuring that the low-frequency structure preserved. However, in the context of training motion autoencoder, spatial mismatches are inevitably involved. Therefore, naively applying such discriminators proves ineffective. Instead, achieving high-fidelity facial components in motion auto-encoder is more closely related to structural problems (lying in low frequencies) than to texture issues as shown in Fig. 8f. Facial Component Perceptual Loss. We introduce simple yet effective facial component perceptual loss, which leverages the standard perceptual loss Llp [95] known for its ability to capture structural features lying in low frequencies. Formally, the facial component perceptual loss is defined by (cid:88) i=1 1 Mi Mi ϕi( ˆD) Mi ϕi(D)1, (15) where is the driving, ˆD is the generated image, is the number of feature pyramid scales, ϕi(X) is the i-th feature of the input image computed by VGG-19 [66, 95], Mi is the binary mask of the facial components that has same size with ϕi(X), and Mi is the sum of all values in the binary mask Mi. We adopt multi-scale perceptual loss with 3-level image pyramid and use = 4 scales of VGG-19 feature pyramids for each image pyramid level. It is worth noting that we mask all the multi-resolution features (not only the image). To compute the facial component mask Mi, we utilize an off-the-shelf face segmentation model [92] for tight mouth regions and face landmark detector [5] for the bounding box regions of the eyes as illustrated in Fig. 8e. In Tab. 7, we conduct ablation studies on motion latent auto-encoders. Notably, facial component perceptual loss is consistently improves the image fidelity over three datasets. As illustrated in Fig. 9, an additional advantage of Lcomplp is its ability to directly supervise fine-grained motion (often neglected due to large head motion) such as eyeball movement without any external driving conditions such as eyegazing direction [17]. A.3. Training Objective We train our motion latent auto-encoder by reconstructing driving image from source image S, both sampled from the same video clip. The total loss function Ltotal for the motion latent autoencoder is defined as Ltotal = LL1 + λlpLlp + λcomp-lpLcomp-lp + λfull-advLfull-adv + λeye-advLeye-adv + λeye-FSMLeye-FSM + λlip-advLlip-adv + λlip-FSMLlip-FM, (16) where λlp, λcomp-lp, λeye-adv, λeye-FSM, λlip-adv, λlip-FSM, and λfull-adv are the balancing coefficients. Here, LL1 is the L1 loss, and Llp is the VGG-19 [66] based multi-scale perceptual loss [95] similar to Lcomp-lp. We incorporate 2-scale discriminator Lfull-adv with the non-saturating loss: Lfull-adv = log[Discfull( ˆD)], (17) where Disc denotes discriminator adopted from [33]. To improve the fidelity of the facial components, we also incorporate the facial component discriminators with the feature 13 Table 7. Quantitative comparison result (Same-identity) of motion latent auto-encoders on HDTF [97] / RAVDESS [46] / VFHQ [88]. The : Results generated by official implementation (256 256) best result for each metric is in bold. Method LIA [85] Ours (w.o. Lcomplp) Ours FID 47.481 / 67.541 / 89.209 21.061 / 28.866 / 46.950 19.803 / 23.350 / 43.992 FVD 172.195 / 130.836 / 342.964 150.340 / 103.145 / 299.757 147.089 / 100.345 / 291.560 LPIPS 0.184 / 0.122 / 0.245 0.110 / 0.072 / 0.165 0.108 / 0.062 / 0.161 E-FID 1.279 / 1.153 / 1.106 1.369 / 1.157 / 0.872 1.334 / 1.053 / 1.006 P-FID 0.120 / 0.005 / 0.013 0.011 / 0.010 / 0.014 0.010 / 0.008 / 0. (a) Source, (b) Driving, (c) Identity, wSr (d) Reconstruction, ˆD (e) Component mask (f) Component diff Figure 8. Visualization results of the motion latent auto-encoder. lation [21] and ψ is the multi-resolution features extracted by the learned component discriminators. A.4. Implementation Details We set the balancing coefficients λlp = 10, λcomp-lp = 100, λeye-adv = 1, λeye-FSM = 100, λlip-adv = 1, λlip-FSM = 100, and λfull-adv = 1. We employ Adam optimizer [36] with batch size of 8 and learning rate of 2104. Entire training takes about 9 days for 460k steps on single NVIDIA A100 GPU. For training our motion latent auto-encoder, we use VFHQ [88] to supplement the limited number of identities provided by HDTF [97] and RAVDESS [46]. After the same pre-processing, remaining 14,362 video clips are used for training, and 49 video clips are used for test, respectively. B. More on FLOAT In this section, we provide more details on FLOAT, including model, experiments, and further results. In Fig. 12, we provide detailed model architecture for the driving conditions ct. B.1. Evaluation Metrics We provide further details of following metrics. LPIPS [95] is used to measure the perceptual similarity between reconstructed image and real image based on the pre-trained AlexNet features [38]. Figure 9. Ablation study on Facial Component Loss. It significantly improves the image fidelity of facial component (e.g., teeth, highlighted in red box) and fined-grained motion (eyeball movement, highlighted in yellow box). style matching (FSM) [84], Lx-adv = log[Discx(1 ˆDx)], Lx-FSM = Gram(ψ(Dx)) Gram(ψ( ˆDx))1, (18) (19) where {eye, lip}. Dx and ˆDx represent the region of interest (RoI) for the component in the driving and reconstruction ˆD, respectively. Gram is gram matrix calcu14 FID [64] aims to measure the distance between the feature distributions of real and generated datasets. It is computed as: µr µg2 2 + Tr(Σr + Σg 2(ΣrΣg) 1 2 ), (20) where µr, Σr and µg, Σg are the means and covariances of the pre-trained InceptionNet [72] features from the real and generated datasets, respectively. FVD [78] is variant of FID [64], which is used to measure the spatio-temporal consistency between the real and generated datasets by leveraging the features of pretrained video model [6]. We compute this using 16 frames with sliding window manner for each video. CSIM [12] measures face similarity between the two face images by computing the cosine similarity between the pre-trained ArcFace features [12] of two images. E-FID [76] aims to measure expression similarity by computing the FID score (Eq. (20)) of 3DMM expression parameters (64-dim) [13] of generated videos and real videos. P-FID aims to measure the head pose similarity by computing the FID score (Eq. (20)) of 3DMM pose parameters (6-dim) [13] of generated videos and real videos. LSE-D and LSE-C [58] measure lip synchronization using the pre-trained SynNet [10]. LSE-D computes the distance between the predicted audio embedding and the predicted video embedding, while LSE-C represents the confidence of synchronization. B.2. Baselines For non-diffusion-based methods, we compare with SadTalker [96] and EDTalk [74]. For diffusion-based methods, we compare with AniTalker [43], Hallo [89], and EchoMimic [8]. SadTalker [96] employs an audio-conditional variational auto-encoder (VAE) to synthesize the head motion and eye blink in probabilistic way. EDTalk [74] uses normalizing for audio-driven head motion generation and can separately control the lip and head motion. AniTalker [43] introduces diffusion model to the learned motion latent space (similar to FLOAT) along with variance adapter to improve the motion diversity. We use HuBERT audio feature-based implementation [28] for improved lip synchronization and apply default guidance scales and denoising steps of the official implementation. Hallo [89] uilizes the pre-trained StableDiffusion [61] as its image generator, incorporating hierarchical audio attention module to separately control lip synchronization, expression, and head pose. We use default guidance scales and denoising steps provided in the official implementation. EchoMimic [8] is also StableDiffusion-based method, which leverages facial skeleton as additional driving signals. We use the default guidance scales and denoising steps provided in the official implementation. B.3. More on Experiments For evaluating our method, we use the first frame of each video clip as the source image. We use the first-order Euler method [42] as our ODE solver. We experimentally find that other ODE solvers, such as mid-point and Dopri5, do not lead to significant performance improvements. More on Different NFE. We provide supplementary videos, illustrating the impact of different NFE (Number of Function Evaluations). Notably, with small NFE of 2, the generated images exhibit good quality, but the head movements appear temporally unstable, and emotions may be exaggerated. More on Different Emotion Guidance γe. In Fig. 14, we visualize the effect of different emotion guidance scale γe. For this experiments, the predicted speech-to-emotion label is disgust with 99% probability. Notably, as increasing γe from 0 to 2, we can observe that emotion-related expressions and motions are enhanced. More on Ablation. To compare with FMT, we implement cross-attention-based transformer. We adopt the stand cross-attention mechanism described in [19, 71], using transformer encoder architecture for non-autoregressive sequence modeling. Similar to FMT, we use attention mask (T = 2) in the cross-attention, which attends to additional 2T adjacent frames for the l-th input latent: [l2, l1, l, l+ 1, + 2]. To compare with flow matching, we implement two types of diffusion models, ϵ-prediction and x0-prediction. Specifically, for ϵ-prediction, we directly predict diffusion noise by the noise predictor s(; θ) parameterized by θ with the following simple loss: Lsimple, noise(θ) = s(xt, ct; θ) ϵ2 2, (21) where U[0, 1], ϵ (0L:L, I), and the noise input xt R(L+L)d is sampled from forward diffusion process 1 βtxt1, βtI) [27]. In our case, q(xtxt1) = (xt; xt is noisy motion latents at diffusion time step t, starting from = 0 with x0 = wrD1:L R(L+L)d. For the variance schedule βt, we use cosine scheduler with diffusion 500 steps. For x0-prediction, we predict clean sample x0 (instead of noise) [59] by the predictor s(; θ) with the following simple loss: Lsimple,x0(θ) = s(xt, ct; θ) x02 2. To fairly compare with FLOAT, we incorporate velocity loss [75]: (22) Lvel,x0 (θ) = x02 2, (23) 15 C.4. Video Results We include video results to further illustrate the performance of our method, including emotion redirection, additional driving conditions, and OOD results. Please refer to provided videos. D. Discussion Ethical Consideration. This work aims to advance virtual avatar generation. However, as it can generate realistic talking portrait only from single image and audio, we considerably recognize the potential for misuse, such as deepfake creation. Attaching watermarks to generated videos and carefully restricted license can mitigate this issues. Additionally, we encourage researchers in deepfake detection to use our results as data to improve detection tools. Limitation and Further Work. While our method can generate realistic talking portrait video from single source image and driving audio, it has several limitations. First, our method cannot generate more vivid and naunced emotional talking motion. This is because the speech-driven emotion labels are restricted to seven basic emotions, making it challenging to capture more nuanced emotions like shyness. We believe this limitation can be addressed by incorporating textual cues (e.g., gazing forward with shyness\"), an idea we plan to explore in future work. Moreover, any other approaches to enhance the naturalness of talking motion are key directions for our future work. Second, we aim to build our method solely upon highdefinition open-source datasets. Since the training datasets are biased toward frontal head angles [46, 97], the generated results also exhibit similar bias, often producing suboptimal results for non-frontal (e.g., yaw angle 20) source images or images with notable accessories. Although we investigated other existing high-definite face video datasets, such as MEAD [81] and CelebV-Text [93], we found limitations in their suitability. MEAD [81] contains minimal head motion and limited number of identities, while CelebVText [93] is not organized for audio-driven talking portrait, containing out-of-sync audio and significant background inconsistencies. This limitations can be mitigated by introducing carefully curated external data, as demonstrated by other concurrent methods [25, 31, 76, 89, 90], or by incorporating multi-view supervision [77] when training our motion latent auto-encoder. We provide examples of failure case in Fig. 22 and supplementary video. Lastly, we believe that our method can be further optimized to real-time video generation, enabling application such as virtual avatar chats. Acknowledgment. The source images and audio used in this paper are taken from other talking portrait generation methods [8, 76, 89, 90, 96]. We sincerely thank the au- (Left) Test Sheet; Figure 10. Example of user study interface. (Right) Answer Sheet. Participants were asked to evaluate 5 questions for each video (total 180 videos). where and x0 are the one-frame difference along the time-axis for and x0, respectively. The total loss Ltotal,x0(θ) is Ltotal,x0(θ) = Lsimple,x0 (θ) + Lvel,x0 (θ). (24) For reverse process, we use the DDIM [67] sampling alIn our implementation, gorithm with 50 denoising steps. both ϵ-prediction and x0-prediction achieve the best results with guidance scales γa = γe = 1 (default) using the incremental classifier-free guidance method [3]. In Fig. 15, Fig. 16, Fig. 17, and Fig. 18, we provide qualitative comparisons between these approaches and FLOAT (including the corresponding videos). Notably, the crossAttention exhibits less diverse head motions compared to FLOAT, while diffusion-based approaches struggle to generate temporally stable lip and head motion, often resulting in out-of-sync movements or motion artifacts. C. Additional Results C.1. Additional Comparison Results We provide additional comparison results with baselines in Fig. 19, Fig. 20, and Fig. 21. C.2. Out-of-distribution (OOD) Results In Fig. 13 and Fig. 14, we present additional outincluding paintings, non-English of-distribution results, speech, and singing. C.3. User Study In Fig. 10, we provide an example of test and answer sheet used of the user study. We asked 15 participants to evaluate five questions for each generated video produced by the baselines and FLOAT. Consequently, each participant scores total 180 questions, with responses ranged from 1 to 5. Additionally, we include the supplementary videos used in the user study. 16 thors of these works for their valuable contributions. Note that the individuals depicted in our source images and the speech generated in our experiments are not associated with the actual persons they represent. 17 Figure 11. Detailed Model architecture of our motion latent auto-encoder. The notations are adopted from LIA [85] and StyleGAN2 [33]. Figure 12. Detailed model architecture for constructing the driving conditions ct R(L+L)h in FLOAT. Figure 13. Out-of-distribution results. The first row shows the result for Chinese audio, and the second row shows the result for singing audio. Please refer to supplementary video. Figure 14. Ablation on emotion guidance scale γe. The predicted speech-to-emotion label is disgust of 99.99%. Please refer to supplementary video. 19 Figure 15. Ablation results on FMT and flow matching. Please refer to supplementary video. Figure 16. Ablation results on FMT and flow matching. Please refer to supplementary video. Figure 17. Ablation results on FMT and flow matching. Please refer to supplementary video. Figure 18. Ablation results on FMT and flow matching. Please refer to supplementary video. 21 Figure 19. Qualitative comparison results with state-of-the-art methods. Please refer to supplementary video. 22 Figure 20. Qualitative comparison results with state-of-the-art methods. Please refer to supplementary video. 23 Figure 21. Qualitative comparison results with state-of-the-art methods. Please refer to supplementary video. 24 Figure 22. Failure case of FLOAT. It often struggles to handle non-frontal faces and accessories, such as glasses. Please refer to supplementary video."
        }
    ],
    "affiliations": [
        "DeepBrain AI Inc.",
        "Graduate School of AI, KAIST"
    ]
}