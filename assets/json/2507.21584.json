{
    "paper_title": "TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs",
    "authors": [
        "Kejia Zhang",
        "Keda Tao",
        "Zhiming Luo",
        "Chang Liu",
        "Jiasheng Tang",
        "Huan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) enable vision-language reasoning, yet often generate plausible outputs that are factually incorrect or visually ungrounded, thereby compromising their reliability. Direct preference optimization (DPO) is a common strategy for correcting hallucinations by aligning model outputs with human preferences. Existing DPO strategies typically treat hallucination-related preferences as fixed targets, relying on static supervision signals during training. This approach tends to overfit to superficial linguistic cues in preference data, leading to distributional rigidity and spurious correlations that impair grounding in causally relevant visual information. To overcome this limitation, we propose TARS, a token-adaptive preference strategy that reformulates DPO as a min-max optimization problem. TARS maximizes token-level distributional shifts under semantic constraints to simulate alignment uncertainty, and simultaneously minimizes the expected preference loss under these controlled perturbations. This joint objective preserves causal grounding while mitigating overfitting to preference patterns, thereby reducing hallucinations in multimodal reasoning. We evaluate TARS on multiple hallucination benchmarks and find consistently strong performance. Using only 4.8k preference samples and no expert feedback, TARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition value from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on several key metrics."
        },
        {
            "title": "Start",
            "content": ": MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs Project Page: https://kejiazhang-robust.github.io/tars web Kejia Zhang1, Keda Tao2, Zhiming Luo1*, Chang Liu4, Jiasheng Tang3,5, Huan Wang2* 1Department of Artificial Intelligence, Xiamen University 2School of Engineering, Westlake University 3DAMO Academy, Alibaba Group 4AWS AI Lab, Amazon 5Hupan Laboratory 5 2 0 2 1 3 ] . [ 2 4 8 5 1 2 . 7 0 5 2 : r Figure 1: Left: We present TARS, token-adaptive preference strategy for mitigating hallucinations in MLLMs. TARS reformulates direct preference optimization (DPO) as min-max objective that (1) minimizes behavioral misalignment via preference feedback and (2) maximizes adaptability through perturbations of visual-agnostic tokens. Right: Evaluation on LLaVA-v1.513B with preference optimization (PO) (Liu et al. 2023b) and industrial MLLMs under the AMBER benchmark (Wang et al. 2023) shows that TARS surpasses PO baselines and matches GPT-4o (Hurst et al. 2024) in hallucination suppression. Abstract"
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models (MLLMs) enable visionlanguage reasoning, yet often generate plausible outputs that are factually incorrect or visually ungrounded, thereby compromising their reliability. Direct preference optimization (DPO) is common strategy for correcting hallucinations by aligning model outputs with human preferences. Existing DPO strategies typically treat hallucination-related preferences as fixed targets, relying on static supervision signals during training. This approach tends to overfit to superficial linguistic cues in preference data, leading to distributional rigidity and spurious correlations that impair grounding in causally relevant visual information. To overcome this limitation, we propose TARS, token-adaptive preference strategy that reformulates DPO as min-max optimization problem. TARS maximizes token-level distributional shifts under semantic constraints to simulate alignment uncertainty, and simultaneously minimizes the expected preference loss under these controlled perturbations. This joint objective preserves causal grounding while mitigating overfitting to preference patterns, thereby reducing hallucinations in multimodal reasoning. We evaluate TARS on multiple hallucination benchmarks and find consistently strong performance. Using only 4.8k preference samples and no expert feedback, TARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition value from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on several key metrics. *Corresponding Author. Work done prior to joining Amazon. Large language models (LLMs) demonstrate strong realanguage soning capabilities across broad range of tasks (Gandhi et al. 2023; Chen et al. 2023; Wang et al. 2025). Building on this foundation, multimodal large language models (MLLMs) integrate visual inputs to enable grounded understanding and vision-language reasoning (Tong et al. 2024; Huang et al. 2023; Driess et al. 2025). Although this integration broadens their applicability in various tasks (Jiang et al. 2024b; Shao et al. 2025), it also introduces key challenges, among which hallucinations are particularly prominent (Kim et al. 2024; Jiang et al. 2024a; Huang et al. 2024). Hallucinations in MLLMs refer to outputs that may appear plausible yet are factually incorrect or lack grounding in the visual context (Sarkar et al. 2025b; Gunjal, Yin, and Bas 2024). Addressing these failures is essential for improving the reliability, safety, and practical applicability of MLLMs in real-world applications. Modern MLLMs are typically developed through twostage training pipeline that includes knowledge-intensive pretraining phase (Dai et al. 2023; Bao et al. 2022; Zhang et al. 2024), followed by instruction tuning (Liu et al. 2024a; Wang et al. 2024b; Liu et al. 2023a). These stages endow the model with broad world knowledge and the ability to follow instructions in natural language (Bao et al. 2022; Liu et al. 2023b). Despite these capabilities, hallucinations often stem not from knowledge deficits, but from behavioral Figure 2: Motivation illustration for TARS. (a) and (b) illustrate standard DPO and our token-adaptive strategy. (c) shows VQA example where DPO hallucinates, while TARS avoids ungrounded output. (d) and (e) visualize token-to-query attention maps during decoding. DPO over-attends to spurious tokens, while TARS attends to causally grounded visual-semantic cues. biases acquired during training that lead the model to generate plausible yet ungrounded outputs (Oh et al. 2024; Chen et al. 2025). To address these failures, preference optimization (PO) has become prominent strategy for reducing hallucinations by aligning model outputs with human expectations (Schulman et al. 2017; Achiam et al. 2017). PO finetunes models using ranked response pairs derived from either human feedback (Sun et al. 2024; Ouyang et al. 2022) or AI-generated preferences (Yu et al. 2025; Sharma et al. 2024), providing direct supervision to reinforce grounded and faithful responses. Such methods have proven effective in mitigating hallucinations across diverse tasks. Direct preference optimization (DPO) (Rafailov et al. 2023) has become widely adopted method for hallucination reduction (Fu et al. 2025; Yang et al. 2025). While generally effective, current DPO methods rely heavily on preference data, which can cause models to overfit to shallow textual cues, such as high-frequency phrases or repetitive patterns in the training set (Huang et al. 2024; Liu, Zheng, and Chen 2024). Prior studies have observed that this overfitting leads MLLMs to generate responses that appear plausible but lack visual grounding, as illustrated in Figure 2(c). In our analysis (Figure 2(d)), we further find that DPO-trained models often assign high preference to outputs containing spurious correlation tokens, including prepositions or frequently mentioned objects, even when these elements are not visually grounded (Xie et al. 2024; Wang et al. 2024a). These observations reveal core limitation of DPO: its reliance on static preference signals hinders generalization under shifting visualtextual contexts, leading to brittle crossmodal alignment and increased vulnerability to hallucination (Setlur et al. 2024; Fu et al. 2025). This rigidity also prevents the model from adapting to local semantic discrepancies, weakening its ability to ground responses in causally relevant visual information. We formulate the challenge of distributional rigidity in preference optimization as min-max token-adaptive alignment problem: maximizing distributional variation under semantic constraints, followed by minimizing the expected preference loss under these controlled perturbations. Specifically, we introduce perturbations to visual-agnostic tokens, textual elements with minimal cross-modal grounding, to simulate contextual variation and shift the input distribution without altering the semantic content. This setup enables the model to rely on causally grounded visual signals rather than superficial textual correlations, thereby mitigating hallucinations arising from overfitting to preference data (see Figure 2(b)). We refer to this approach as TARS (token-adaptive preference strategy), lightweight and generalizable approach that enhances preference learning by introducing distribution-aware variability during fine-tuning. To validate the effectiveness of our method, we evaluate TARS on LLaVA-v1.5 (Liu et al. 2023b) at 7B and 13B scales, comparing it against leading preference optimization approaches. Across comprehensive suite of hallucination benchmarks spanning generative and discriminative tasks, TARS-enhanced LLaVA-v1.5 achieves consistently strong performance and matches GPT-4o (Hurst et al. 2024) in several settings. These results underscore the effectiveness of token-adaptive preference optimization in reducing hallucinations and advancing the trustworthiness of MLLMs. Our contributions are as follows: We reformulate preference learning as min-max optimization objective that encourages token-level distributional shifts within semantic boundaries, while minimizing preference misalignment, thereby mitigating overfitting to rigid or spurious supervision signals. We introduce TARS, lightweight strategy that perturbs visual-agnostic tokens to simulate semantic variation, enhancing visual grounding and reducing hallucinations. TARS achieves SOTA hallucination reduction using only 4.8k preference samples and no expert feedback, matching GPT-4o on several key metrics."
        },
        {
            "title": "2 Preliminaries\nMultimodal Large Language Models. MLLMs extend\nlarge language models (LLMs) by incorporating visual in-\nputs alongside textual prompts (Zhang et al. 2024). For-\nmally, given an image x and a prompt q, the model gener-\nates a textual response y = (y1, . . . , yl) in an autoregressive\nmanner (Liu et al. 2023b):",
            "content": "yt πθ(yt y<t, x, q), (1) where πθ denotes the models conditional generation policy parameterized by θ. Given multimodal input of textual input and visual input x, the model tokenizes them into discrete sequences: textual tokens = {q1, . . . , qm} and visual tokens = {x1, . . . , xn}. These tokens are mapped to embeddings and fused via cross-attention to integrate semantic signals from both modalities. The resulting context is then used by the decoder to autoregressively generate the output sequence (Dou et al. 2022; Yang et al. 2021). Direct Preference Optimization. Direct preference optimization (DPO) (Rafailov et al. 2023) is an effective approach for aligning model behavior with human preferences. It bypasses explicit reward models by directly optimizing preferences from pairwise comparisons. Traditional methods such as reinforcement learning with human feedback (RLHF) (Ouyang et al. 2022) and AI feedback (RLAIF) (Yu et al. 2025) rely on training scalar reward model rψ(x, q, y) from preference pairs. This reward model is typically trained using the Bradley-Terry formulation (Bradley and Terry 1952): (yw yr x, q) = exp(rψ(x, q, yw)) exp(rψ(x, q, yw)) + exp(rψ(x, q, yr)) = σ (rψ(x, q, yw) rψ(x, q, yr)) , 1 (2) where (x, q, yw, yr) is sampled from the preference data distribution D, σ(z) = 1+exp(z) denotes the sigmoid function. yw and yr denote the preferred and dispreferred responses, respectively. rψ(x, q, y) is trained to maximize the log-likelihood of correctly ranking the preferred response: E(x,q,yw,yr)D [ log σ (rψ(x, q, yw) rψ(x, q, yr))] , (3) After training, the learned reward model rψ(x, q, y) is used to guide the fine-tuning of the policy πθ. Specifically, the policy is optimized to generate high-reward responses while minimizing divergence from fixed reference policy πref, typically using KL-regularized objectives: min rψ E(x,q)D, yπθ(yx,q) min πθ (cid:16) (cid:104) rψ(x, q, y) α DKL (πθ(y x, q) πref(y x, q)) (4) (cid:17)(cid:105) , where α controls the strength of regularization, which ensures alignment with the learned preferences without drifting too far from the pretrained behavior. Rather than relying on the explicitly trained reward model, DPO (Rafailov et al. 2023) simplifies the learning process by leveraging the insight that the optimal policy can be expressed in closed form using relative log-likelihoods under πθ and πref: E(x,q,yw,yr)D min πθ (cid:104) logσ (cid:16) α log α log πθ(yw x, q) πref(yw x, q) πθ(yr x, q) πref(yr x, q) (cid:17)(cid:105) , (5) This formulation enables direct policy optimization from preference pairs, aligning the output probabilities of MLLMs with human preferences."
        },
        {
            "title": "3.1 Min-Max Reformulation of DPO\nTo address the limitations of traditional DPO, we reformu-\nlate preference optimization as a token-adaptive min-max\ngame. The inner maximization introduces controlled token-\nlevel perturbations φ(·) to induce input distribution shifts,\nwhile the outer minimization aligns the policy πθ with pref-\nerence signals. Formally, we define the min–max preference\nobjective as:",
            "content": "min πθ max φΦ(A) E(x,q,yw,yr)D (cid:2)LTARS (cid:0)x, φ(q), yw, yr (cid:1)(cid:3) , (6) where φ is token-level perturbation function constrained to visually agnostic tokens, i.e., Φ(A) = {φ {i φ(qi) = qi} A(x, q)}. This minmax objective promotes preference alignment under distributional shifts, helping to mitigate spurious correlations and reduce hallucinated outputs."
        },
        {
            "title": "3.2 Maximizing with Token Perturbations\nAs shown in Equation (5), DPO aligns models with pre-\nferred responses via log-likelihood ratios against a refer-\nence model. However, we observe that this formulation can\nencourage overfitting to superficial patterns such as fre-\nquent phrases, stylistic tokens, which we find reduce effec-\ntive alignment with the visual context in multimodal set-\ntings (Setlur et al. 2024; Fu et al. 2025).",
            "content": "To counter this, we apply token-wise maximization to introduce distribution shifts and reduce overfitting to preference signals. Formally, we define: φ(q) = arg max φΦ(A) Sim(φ(q), q), (7) where Φ(A) denotes allowable perturbations constrained to A(x, q), and Sim(φ(q), q) measures token-level deviation. In practice, we approximate φ(q) by applying local tokenlevel transformations: φ(q) = {I[i A(x, q)] φ(qi) + I[i / A(x, q)] qi}q i=1 , (8) Figure 3: Overview of TARS. TARS reformulates preference optimization as MinMax problem: (1) The maximization branch perturbs visual-agnostic tokens to simulate semantically shifted contexts (red dashed box); (2) The minimization branch fine-tunes the model to align with human preferences via the DPO objective (purple dashed box). TARS encourages the model to attend to causally grounded visual signals rather than spurious textual correlations, thereby reducing hallucinations. where φ(qi) is constructed using masking or synonym substitution. This approximation simulates worst-case alignment uncertainty while preserving semantic integrity. To preserve semantics, we restrict changes to visualagnostic tokens with minimal impact on cross-modal alignment. We compute token-level visual relevance as the similarity between visual features Gv(x) and token embeddings Gt(qi). We then identify set of Nt visually agnostic tokens with the lowest cross-modal alignment scores: (cid:0)Gv(x)Gt(qi)T (cid:1) , Nt = (cid:4)ω 1(cid:5)+1, (9) = TopNt where denotes the floor operation, ω is scaling coefficient. = Gv(x)Gt(qi)T is the similarity score matrix, = maxj Pj maxk=j Pk quantifies the predictive uncertainty of MLLMs. We adapt Nt inversely to this margin: confident predictions lead to fewer perturbations, while greater uncertainty induces broader variation."
        },
        {
            "title": "3.3 Spectral Regularization for Token Alignment\nWhile our method introduces token-level perturbations to\nsimulate distribution shifts, the supervision derived from\npreference pairs (yw, yr) is static. This discrepancy between\nadaptive input representations and fixed supervision may en-\ncourage the model to learn distribution-specific artifacts, es-\npecially under strong alignment constraints (Fu et al. 2025;\nChowdhury, Kini, and Natarajan 2024).",
            "content": "In practice, semantic alignment does not require strict token-level correspondence. Enforcing such fine-grained constraints may reintroduce spurious correlations that our min-max strategy aims to mitigate (Zhou et al. 2024; Tian et al. 2025). To address this, we propose frequency-domain alignment, where local token perturbations translate into smooth global variations in the spectral space. This approach ensures semantic consistency between perturbed and original inputs without rigid token-wise matching. Specifically, we extract hidden states for (x, φ(q), yw) and compare them to those of (x, q, yw) and (x, q, yr) using the Fast Fourier Transform (FFT) (Cooley and Tukey 1965). Formally, let RLD denote sequence of hidden states, where is the token length. We compute the spectral representation via: F(z) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Re (cid:34)L1 (cid:88) l=0 zl e2πikl/L (cid:35)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)2 , for = 0, . . . , L1, (10) where the FFT is applied along the token axis and 2 computes the ℓ2 norm over real-valued frequency magnitudes. The resulting spectral preference loss is defined as: Lfreq = log σ (cid:104) (cid:16) β log (hθ(x, φ(q), yw)) (href(x, q, yw)) (cid:105)(cid:17) , (11) log (hθ(x, φ(q), yw)) (href(x, q, yr)) where hθ() and href() are the hidden states of the policy and reference models. This objective follows the logic of DPO but extends alignment to the spectral domain, improving consistency in frequency-aware representations and reducing hallucinations from overfitting to fixed preferences."
        },
        {
            "title": "3.4 Minimization Objective in TARS",
            "content": "We integrate the standard DPO loss with spectral regularization to yield the final TARS training objective. Given perturbed input φ(q) obtained from the inner maximization, and its original counterpart q, the overall loss is defined as: LTARS(x, q, φ(q), yw,yr) = LDPO(x, φ(q), yw, yr) + λ Lfreq(x, q, φ(q), yw, yr), (12) where λ is weighting coefficient that balances preference alignment and spectral consistency. This formulation encourages the model to preserve causal alignment with preference signals, thereby mitigating spurious correlation. Table 1: Comparison across hallucination benchmarks. We evaluate state-of-the-art MLLMs as reference baselines, denoted by . For algorithms with available checkpoints, results from re-testing are marked with ; for those without, we reproduce results using settings from (Fu et al. 2025; Li, Lin, and Pei 2024), denoted by . All experiments use greedy decoding with temperature set to 0 for consistency and reproducibility. Bold denotes the best performance, and underlined denotes the second-best. Algorithm Intern-VL2.5-7B (Chen et al. 2024e) Qwen-VL2.5-8B (Bai et al. 2025) DeepSeek-VL2-27B (Wu et al. 2024) GPT-4o (Hurst et al. 2024) LLaVA-v1.5-7B (Liu et al. 2023b) + RLHF (Sun et al. 2024) + RLAIF (Yu et al. 2025) + HALVA (Sarkar et al. 2025a) + DPO (Li, Lin, and Pei 2024) + CHiP-DPO (Fu et al. 2025) + OPA-DPO (Yang et al. 2025) + TARS (Mask) + TARS (Replace) LLaVA-v1.5-13B (Liu et al. 2023b) + RLHF (Sun et al. 2024) + HALVA (Sarkar et al. 2025a) + DPO (Li, Lin, and Pei 2024) + CHiP-DPO (Fu et al. 2025) + OPA-DPO (Yang et al. 2025) + TARS (Mask) + TARS (Replace) AMBER MMHal POPE OBJHal CHAIR Cover Hal-Rate Cog Score Hal-Rate Acc Pre CRs CRi 7.9 4.6 2.4 2.5 7.6 8.3 3.0 6.9 4.9 2.9 2.7 2.4 2.1 6.7 7.1 6.5 4.1 3.8 2.8 2.1 2.1 54.7 54.6 56.6 60.9 51.7 52.2 50.3 52.8 56.6 57.3 47.4 59.6 59. 52.1 51.4 53.4 56.7 58.6 48.4 59.8 59.4 37.1 21.1 16.3 17.6 35.4 41.8 16.5 33.2 26.4 19.9 12.5 13.2 14.9 32.5 36.3 30.1 24.3 20.8 13.5 12.5 13.6 3.2 1.3 0.9 0.8 4.2 4.5 1.0 3.5 2.5 1.0 0.9 0.4 0. 3.5 3.6 3.3 2.2 1.7 1.0 0.6 0.7 3.54 3.29 2.84 3.87 2.02 1.93 2.89 2.12 2.19 2.32 2.78 2.48 2.54 2.39 2.10 2.28 2.48 2.70 3.02 2.89 2.63 0.26 0.27 0.27 0.24 0.61 0.67 0.42 0.59 0.61 0.57 0.46 0.45 0. 0.53 0.67 0.56 0.50 0.46 0.40 0.45 0.47 - - - - 80.0 82.0 88.1 87.5 87.8 81.1 87.4 88.7 87.9 74.6 83.6 86.8 85.2 86.6 87.2 87.6 86.9 - - - - 61.8 69.3 88.0 79.6 82.0 91.8 86.2 97.5 97. 55.2 71.2 75.6 84.3 74.9 80.7 93.0 92.5 36.0 40.7 10.0 29.3 54.0 56.0 13.7 47.3 14.0 7.3 13.3 12.0 13.4 50.0 46.7 42.7 19.0 30.0 18.3 14.6 14.9 9.1 8.6 7.0 6.7 15.8 15.2 4.2 14.6 5.0 4.3 4.5 3.2 3. 14.5 11.6 12.1 7.2 6.2 5.1 2.8 3."
        },
        {
            "title": "4.1 Experiment Details",
            "content": "Experiment Setups. We evaluate our method using the multimodal large language model LLaVA-v1.5 (Liu et al. 2023b) at both 7B and 13B parameter scales. All evaluations are conducted using greedy decoding with temperature of 0 to ensure deterministic outputs and reproducibility. To enable fair comparison, we align our training configuration with the most data-efficient preference optimization baselines. Specifically, we randomly sample 4.8k instances from the RLHF-V-Dataset (Yu et al. 2024), consistent with OPA-DPO (Yang et al. 2025), and adopt the same training strategy as CHiP-DPO (Fu et al. 2025). All models are trained on eight NVIDIA A100 (80GB) GPUs. We set α = 1 in Equation (5) and β = 1 in Equation (11) for preference optimization. We implement φ() using both token masking (Mask) and replacement (Replace) strategies in Equation (8), and set the perturbation constraint strength to ω = 0.1 in the adversarial min-max formulation Equation (9). We use frequency-domain loss weight of λ = 0.1 in Equation (12). Full implementation details and ablation studies are provided in the Appendix. Evaluation Benchmark. We evaluate TARS across both generative and discriminative benchmarks to ensure that hallucination mitigation does not come at the cost of factual grounding. Our evaluation framework includes: AMBER (Wang et al. 2023), MMHal (Sun et al. 2023), OBJHal (Yu et al. 2024), POPE (Li et al. 2023). Baseline Methods. We compare against two categories: (1) Advanced multimodal foundation models: InternVL2.5-7B (Chen et al. 2024e), Qwen-VL2.5-8B (Bai et al. 2025), DeepSeek-VL2-27B (Wu et al. 2024), and GPT4o (Hurst et al. 2024) serve as reference benchmarks. (2) LLaVA-v1.5 with RL techniques: We evaluate multiple RL-based approaches applied to both the 7B and 13B variants of LLaVA-v1.5, including RLHF (Sun et al. 2024), RLAIF (Yu et al. 2025), HALVA (Sarkar et al. 2025a), as well as three state-of-the-art methods based on direct preference optimization (DPO): DPO (Pi et al. 2024), CHiPDPO (Fu et al. 2025), and OPA-DPO (Yang et al. 2025). comparison of algorithmic properties is provided in Table 3."
        },
        {
            "title": "4.2 Evaluation on Hallucination Benchmarks\nTable 1 presents the performance comparison on four mul-\ntimodal hallucination benchmarks. For TARS, we employ\ntwo perturbation strategies: token masking and synonym\nreplacement (detailed analysis provided in the Appendix).\nTARS consistently mitigates hallucinations, operates under\nminimal supervision, scales well, preserves factual accuracy,\nand remains competitive with proprietary models. Our anal-\nysis reveals the following key findings:",
            "content": "(1) Hallucination mitigation. TARS achieves consistent reductions in hallucination across benchmarks. On the 7B scale, it lowers the AMBER hallucination rate from 35.4% to 13.2%, 22.2 percentage point (pp) improvement. Concurrently, object coverage rises from 51.7% to 59.6% (+7.9 pp), and cognitive inconsistency (Cog) drops from 4.2 to 0.4 (3.8 pp). On OBJHal, the response-level hallucination rate (CRs) decreases from 54.0% to 12.0%. Table 2: Ablation results of token-level perturbation (TP), crossmodal alignment score (CAS), and spectral preference alignment (SPA) on the AMBER and OBJHal using 7B scale. Algorithm LLaVA (Liu et al. 2023b) TARS w/o TP w/o CAS w/o SPA w/o CAS&SPA AMBER OBJHal Cover Hal-Rate Cog CRs CRi 15.8 51.7 35.4 54.0 4. 59.6 56.6 55.9 58.3 55.1 13.2 26.4 17.7 15.1 18.5 0.4 2.5 1.3 0.7 1.5 12.0 14.0 12.7 12.5 12.6 3.2 5.0 3.5 3.7 3.8 Figure 4: AMBER hallucination rate vs. preference data scale across 7B and 13B models. (a) LLaVA (b) DPO (c) TARS Figure 5: Distribution of hidden representations across preference-aligned, non-hallucinated, and hallucinated responses of different MLLMs. Top and right margins show marginal distributions along key feature dimensions. We extract representations from 100 preference training instances and 200 AMBER inputs across text and vision modalities. Responses to AMBER inputs are categorized as non-hallucinated or hallucinated based on factual coherence. TARS aligns with preference data while avoiding overfitting to spurious correlations, demonstrating superior factual fidelity. (2) Data and supervision efficiency. TARS achieves strong hallucination mitigation without relying on expertlabeled feedback or high-resource teacher models. As shown in Table 3, it uses only 4.8k public preference samples, matching the data budget of OPA-DPO, yet achieves superior performance on AMBER (13B) by improving object coverage from 48.4% to 59.8% (+11.4 pp) and reducing the hallucination rate from 13.5% to 12.5% (1.0 pp). (3) Scalability. TARS exhibits consistent performance as model capacity increases. From 7B to 13B, CHAIR improves from 2.4 to 2.1 (-0.3 pp) and hallucination rate drops from 13.2% to 12.5% (-0.7 pp). TARS-13B also surpasses all 13B baselines, confirming the scalability. (4) Factual consistency. TARS effectively suppresses hallucinations without impairing factual understanding. It achieves 88.7% accuracy on POPE (+8.7 pp over LLaVA7B) in fine-grained visual reasoning and reduces object-level hallucination on OBJHal to 3.2% (1.1 pp). performance (5) Competitiveness with proprietary models. TARS delivers industrial-scale comparable MLLMs. At 13B, it approaches GPT-4o in AMBER Cover (59.8% vs. 60.9%) and surpasses it in Hal-Rate (12.7% vs. 17.6%), while also outperforming DeepSeek-VL2-27B. to"
        },
        {
            "title": "4.3 Ablation Analyses on Component\nWe analyze the contribution of key TARS components\nthrough ablations in Table 2, focusing on three elements:",
            "content": "(1) Token-level perturbation (TP) in Equation (6), which introduces distributional shifts and proves essential for revealing token-level vulnerabilitiesits removal increases Cog from 0.4 to 2.5. (2) Cross-modal alignment score (CAS) in Equation (9), which targets visually agnostic tokens to preserve semantic fidelityits absence leads to 4.5-point increase in hallucination and 0.9 rise in Cog, indicating weaker suppression of spurious correlations. (3) Spectral preference alignment (SPA) in Equation (11), which regularizes frequency-aware consistencyits removal increases the hallucination rate by 1.9 points and CRi from 3.2 to 3.7, suggesting degraded finegrained factual grounding."
        },
        {
            "title": "4.4 Ablation Analyses on Preference Scale Impact\nWe investigate how the scale of preference data affects hal-\nlucination suppression in TARS by training on subsets of the\n4.8k dataset and comparing with standard DPO (Figure 4).\nTARS consistently achieves lower hallucination rates across",
            "content": "Table 3: Comparison of preference optimization strategies. TARS achieves hallucination mitigation and causal alignment with minimal data and no expert feedback. Algorithm Data Size Feedback Reward-Free Hallucination Mitigation Causal Alignment LLaVA-v1.5 (Liu et al. 2023b) + RLHF (Sun et al. 2024) + RLAIF (Yu et al. 2025) + HALVA (Sarkar et al. 2025a) + DPO (Li, Lin, and Pei 2024) + CHiP-DPO (Fu et al. 2025) + OPA-DPO (Yang et al. 2025) + TARS (Ours) - 122k 16k 22k 5k 5k 4.8k 4.8k - self-reward LLaVA-Next GPT-4V self-reward self-reward GPT-4V self-reward all scales and shows sharper improvements in early stages. From 0 to 1.8k examples, the 7B and 13B variants reduce hallucinations by over 18 and 15 percentage points, respectively. While the gains taper beyond 3.6k, performance remains stable, indicating strong data efficiency and effective use of limited supervision compared to DPO."
        },
        {
            "title": "4.5 Stability of Semantic Representations\nWe analyze how preference optimization reshapes hidden-\nstate distributions in Figure 5.",
            "content": "(1) Disentanglement of hallucinated and preference characteristics. TARS yields more structured latent space, where hallucinated and preference-aligned representations are separated. This separation indicates that hallucinations are not artifacts of spurious preference associations. In contrast, DPO exhibits entangled clusters, where hallucinated points are interwoven with preference representations, suggesting that DPO-trained models overfit superficial signals. (2) Selective alignment with non-hallucinated features. TARS selectively aligns non-hallucinated responses with preference features while isolating hallucinated content in the representation space. This alignment distinguishes TARS from both the scattered representations in base LLaVA and the feature entanglement in DPO models. Our findings show TARS creates semantically faithful space by reinforcing only factually grounded responses that match learned preferences, avoiding amplification of spurious preference correlations."
        },
        {
            "title": "5 Related Work\nMultimodal large language models (MLLMs) extend LLMs\nby integrating visual inputs to support multimodal reason-\ning (Chen et al. 2024b; Feng et al. 2025; Jain, Yang, and Shi\n2024). Typically, visual features are extracted by a vision\nencoder, aligned through a connector, and processed by the\nLLM (Liu et al. 2023b; Parekh et al. 2024). Despite strong\nperformance, MLLMs often produce factually incorrect or\nvisually ungrounded outputs, undermining reliability (Bai\net al. 2024; Chen et al. 2025). This issue is more severe than\nin unimodal LLMs (Chen et al. 2024c; Jiang et al. 2024a),\nmainly due to modality imbalance (Ma et al. 2024; He et al.\n2024) and ineffective fusion (Bellagente et al. 2023; Ji et al.\n2023). Recent studies attribute these failures to persistent\nmisalignment between multimodal representations and hu-\nman expectations, rather than model capacity (Chen et al.",
            "content": "2024d; Liu et al. 2024b; Ruan et al. 2025). key bottleneck in addressing MLLM hallucinations lies in aligning model outputs with human preferences for factual consistency. Unlike knowledge-intensive pretraining (Chang et al. 2024; McKinzie et al. 2024) and instruction tuning (Chen et al. 2024a; Liu et al. 2023b), recent methods typically leverage small-scale human preference data refined via reinforcement learning (Yu et al. 2024; Casper et al. 2023). Direct preference optimization (DPO)(Rafailov et al. 2023; Pi et al. 2024) has become leading approach due to its simplicity and effectiveness, demonstrated in ChiPDPO(Fu et al. 2025) and OPA-DPO (Yang et al. 2025). However, DPOs reliance on limited data can cause overfitting to superficial linguistic cues (Setlur et al. 2024; Fu et al. 2025), leading to distributional rigidity and reduced adaptability to modality-specific semantics (Ouali et al. 2024; Song et al. 2024). These limitations call for more adaptive alignment strategies that capture token-level variability and cross-modal dependencies. To address these challenges, we propose token-adaptive min-max alignment strategy that enhances preference learning without relying on high-resource expert feedback (e.g., GPT-4V). Using only small public preference dataset, our method effectively mitigates hallucinations and consistently outperforms RL-based baselines across benchmarks. Table 3 compares preference optimization methods in terms of data scale, supervision, and alignment performance."
        },
        {
            "title": "6 Conclusion\nIn this work, we introduce TARS, a novel lightweight strat-\negy that reformulates direct preference optimization (DPO)\nas a min-max objective. TARS maximizes token-level distri-\nbutional shifts under semantic constraints to simulate align-\nment uncertainty, while simultaneously minimizing the ex-\npected preference loss under these controlled perturbations.\nThis formulation encourages the model to align more faith-\nfully with causally grounded visual cues rather than over-\nfit to superficial textual correlations, effectively mitigating\nhallucinations. TARS achieves strong hallucination suppres-\nsion and consistently outperforms prior methods across most\nstandard evaluation metrics, despite using only 4.8k public\npreference samples and no expert-labeled feedback or large-\nscale teacher models. Empirical results underscore the ef-\nfectiveness of token-level alignment strategies for mitigating\nhallucinations in low-supervision settings.",
            "content": "References Achiam, J.; Held, D.; Tamar, A.; and Abbeel, P. 2017. Constrained policy optimization. In ICML. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Bai, Z.; Wang, P.; Xiao, T.; He, T.; Han, Z.; Zhang, Z.; and Shou, M. Z. 2024. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930. Bao, H.; Wang, W.; Dong, L.; Liu, Q.; Mohammed, O. K.; Aggarwal, K.; Som, S.; Piao, S.; and Wei, F. 2022. Vlmo: Unified vision-language pre-training with mixtureof-modality-experts. In NeurIPS. Bellagente, M.; Brack, M.; Teufel, H.; Friedrich, F.; Deiseroth, B.; Eichenberg, C.; Dai, A. M.; Baldock, R.; Nanda, S.; Oostermeijer, K.; et al. 2023. Multifusion: Fusing pretrained models for multi-lingual, multi-modal image generation. In NeurIPS. Bradley, R. A.; and Terry, M. E. 1952. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika, 39(3/4): 324345. Casper, S.; Davies, X.; Shi, C.; Gilbert, T. K.; Scheurer, J.; Rando, J.; Freedman, R.; Korbak, T.; Lindner, D.; Freire, P.; et al. 2023. Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. TMLR. Chang, H.; Park, J.; Ye, S.; Yang, S.; Seo, Y.; Chang, D.-S.; and Seo, M. 2024. How do large language models acquire factual knowledge during pretraining? In NeurIPS. Chen, C.; Liu, M.; Jing, C.; Zhou, Y.; Rao, F.; Chen, H.; Zhang, B.; and Shen, C. 2025. PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training. In ICLR. Chen, C.; Zhu, J.; Luo, X.; Shen, H.; Song, J.; and Gao, L. 2024a. Coin: benchmark of continual instruction tuning for multimodel large language models. In NeurIPS. Chen, D.; Chen, R.; Zhang, S.; Wang, Y.; Liu, Y.; Zhou, H.; Zhang, Q.; Wan, Y.; Zhou, P.; and Sun, L. 2024b. Mllm-asa-judge: Assessing multimodal llm-as-a-judge with visionlanguage benchmark. In ICML. Chen, L.; Li, B.; Shen, S.; Yang, J.; Li, C.; Keutzer, K.; Darrell, T.; and Liu, Z. 2023. Large language models are visual reasoning coordinators. In NeurIPS. Chen, X.; Ma, Z.; Zhang, X.; Xu, S.; Qian, S.; Yang, J.; Fouhey, D.; and Chai, J. 2024c. Multi-object hallucination in vision language models. In NeurIPS. Chen, X.; Wang, C.; Xue, Y.; Zhang, N.; Yang, X.; Li, Q.; Shen, Y.; Liang, L.; Gu, J.; and Chen, H. 2024d. Unified Hallucination Detection for Multimodal Large Language Models. In ACL. Chen, Z.; Wang, W.; Cao, Y.; Liu, Y.; Gao, Z.; Cui, E.; Zhu, J.; Ye, S.; Tian, H.; Liu, Z.; et al. 2024e. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Chowdhury, S. R.; Kini, A.; and Natarajan, N. 2024. Provably Robust DPO: Aligning Language Models with Noisy Feedback. In ICML. Cooley, J. W.; and Tukey, J. W. 1965. An algorithm for the machine calculation of complex Fourier series. Mathematics of computation, 19(90): 297301. Dai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.; Li, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. arXiv preprint arXiv:2305.06500. Dou, Z.-Y.; Xu, Y.; Gan, Z.; Wang, J.; Wang, S.; Wang, L.; Zhu, C.; Zhang, P.; Yuan, L.; Peng, N.; et al. 2022. An empirical study of training end-to-end vision-and-language transformers. In CVPR. Driess, D.; Xia, F.; Sajjadi, M. S.; Lynch, C.; Chowdhery, A.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.; Huang, W.; et al. 2025. Palm-e: An embodied multimodal language model. In ICLR. Feng, S.; Fang, G.; Ma, X.; and Wang, X. 2025. EfarXiv preprint ficient Reasoning Models: Survey. arXiv:2504.10903. Fu, J.; Huangfu, S.; Fei, H.; Shen, X.; Hooi, B.; Qiu, X.; and Ng, S.-K. 2025. CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs. In ICLR. Gandhi, K.; Franken, J.-P.; Gerstenberg, T.; and Goodman, N. 2023. Understanding social reasoning in language models with language models. In NeurIPS. Gunjal, A.; Yin, J.; and Bas, E. 2024. Detecting and preventing hallucinations in large vision language models. In AAAI. He, B.; Li, H.; Jang, Y. K.; Jia, M.; Cao, X.; Shah, A.; Shrivastava, A.; and Lim, S.-N. 2024. Ma-lmm: Memoryaugmented large multimodal model for long-term video understanding. In CVPR. Huang, Q.; Dong, X.; Zhang, P.; Wang, B.; He, C.; Wang, J.; Lin, D.; Zhang, W.; and Yu, N. 2024. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In CVPR. Huang, S.; Dong, L.; Wang, W.; Hao, Y.; Singhal, S.; Ma, S.; Lv, T.; Cui, L.; Mohammed, O. K.; Patra, B.; et al. 2023. Language is not all you need: Aligning perception with language models. In NeurIPS. Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh, A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Radford, A.; et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Jain, J.; Yang, J.; and Shi, H. 2024. Vcoder: Versatile vision encoders for multimodal large language models. In CVPR. Ji, Y.; Wang, J.; Gong, Y.; Zhang, L.; Zhu, Y.; Wang, H.; Zhang, J.; Sakai, T.; and Yang, Y. 2023. Map: Multimodal uncertainty-aware vision-language pre-training model. In CVPR. Jiang, C.; Xu, H.; Dong, M.; Chen, J.; Ye, W.; Yan, M.; Ye, Q.; Zhang, J.; Huang, F.; and Zhang, S. 2024a. Hallucination augmented contrastive learning for multimodal large language model. In CVPR. Jiang, Y.; Sun, K.; Sourati, Z.; Ahrabian, K.; Ma, K.; Ilievski, F.; Pujara, J.; et al. 2024b. Marvel: Multidimensional abstraction and reasoning through visual evaluation and learning. In NeurIPS. Kim, J.; Kim, H.; Yeonju, K.; and Ro, Y. M. 2024. Code: Contrasting self-generated description to combat hallucination in large multi-modal models. In NeurIPS. Li, S.; Lin, R.; and Pei, S. 2024. Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models. In ACL. Li, Y.; Du, Y.; Zhou, K.; Wang, J.; Zhao, W. X.; and Wen, J.- R. 2023. Evaluating Object Hallucination in Large VisionLanguage Models. In EMNLP. Liu, F.; Lin, K.; Li, L.; Wang, J.; Yacoob, Y.; and Wang, L. 2023a. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565. Liu, H.; Li, C.; Li, Y.; and Lee, Y. J. 2024a. Improved baselines with visual instruction tuning. In CVPR. Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023b. Visual instruction tuning. In NeurIPS. Liu, S.; Zheng, K.; and Chen, W. 2024. Paying more attention to image: training-free method for alleviating hallucination in lvlms. In ECCV. Liu, W.; Wang, X.; Wu, M.; Li, T.; Lv, C.; Ling, Z.; JianHao, Z.; Zhang, C.; Zheng, X.; and Huang, X.-J. 2024b. Aligning Large Language Models with Human Preferences through Representation Engineering. In ACL. Ma, F.; Xue, H.; Zhou, Y.; Wang, G.; Rao, F.; Yan, S.; Zhang, Y.; Wu, S.; Shou, M. Z.; and Sun, X. 2024. Visual perception by large language models weights. In NeurIPS. McKinzie, B.; Gan, Z.; Fauconnier, J.-P.; Dodge, S.; Zhang, B.; Dufter, P.; Shah, D.; Du, X.; Peng, F.; Belyi, A.; et al. 2024. MM1: methods, analysis and insights from multimodal LLM pre-training. In ECCV. Oh, J.; Kim, S.; Seo, J.; Wang, J.; Xu, R.; Xie, X.; and Whang, S. 2024. ERBench: An entity-relationship based automatically verifiable hallucination benchmark for large language models. In NeurIPS. Ouali, Y.; Bulat, A.; Martinez, B.; and Tzimiropoulos, G. 2024. Clip-dpo: Vision-language models as source of preference for fixing hallucinations in lvlms. In ECCV. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. In NeurIPS. Parekh, J.; Khayatan, P.; Shukor, M.; Newson, A.; and Cord, M. 2024. concept-based explainability framework for large multimodal models. In NeurIPS. Pi, R.; Han, T.; Xiong, W.; Zhang, J.; Liu, R.; Pan, R.; and Zhang, T. 2024. Strengthening multimodal large language model with bootstrapped preference optimization. In ECCV. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In ICML. Rafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Ermon, S.; and Finn, C. 2023. Direct preference optimizaIn tion: Your language model is secretly reward model. NeurIPS. Rohrbach, A.; Hendricks, L. A.; Burns, K.; Darrell, T.; and Saenko, K. 2018. Object Hallucination in Image Captioning. In EMNLP. Ruan, H.; Lin, J.; Lai, Y.; Luo, Z.; and Li, S. 2025. HCCM: Hierarchical Cross-Granularity Contrastive and Matching In ACM Learning for Natural Language-Guided Drones. MM. Sarkar, P.; Ebrahimi, S.; Etemad, A.; Beirami, A.; Arık, S. O.; and Pfister, T. 2025a. Data-augmented phrase-level alignment for mitigating object hallucination. In ICLR. Sarkar, P.; Ebrahimi, S.; Etemad, A.; Beirami, A.; Arik, S. O.; and Pfister, T. 2025b. Mitigating Object Hallucination in MLLMs via Data-augmented Phrase-level Alignment. In ICLR. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Setlur, A.; Garg, S.; Geng, X.; Garg, N.; Smith, V.; and Kumar, A. 2024. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. In NeurIPS. Shao, K.; Tao, K.; Qin, C.; You, H.; Sui, Y.; and Wang, H. 2025. HoliTom: Holistic Token Merging for Fast Video Large Language Models. arXiv preprint arXiv:2505.21334. Sharma, A.; Keh, S. S.; Mitchell, E.; Finn, C.; Arora, K.; and Kollar, T. 2024. critical evaluation of ai feedback for aligning large language models. In NeurIPS. Song, Y.; Swamy, G.; Singh, A.; Bagnell, J.; and Sun, W. 2024. The importance of online data: Understanding preference fine-tuning via coverage. In NeurIPS. Sun, Z.; Shen, S.; Cao, S.; Liu, H.; Li, C.; Shen, Y.; Gan, C.; Gui, L.; Wang, Y.-X.; Yang, Y.; et al. 2024. Aligning Large Multimodal Models with Factually Augmented RLHF. In Findings: ACL. Sun, Z.; Shen, S.; Cao, S.; Liu, H.; Li, C.; Shen, Y.; Gan, C.; Gui, L.-Y.; Wang, Y.-X.; Yang, Y.; et al. 2023. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525. Tian, X.; Zou, S.; Yang, Z.; He, M.; and Zhang, J. 2025. Black Sheep in the Herd: Playing with Spuriously Correlated Attributes for Vision-Language Recognition. In ICLR. Tong, P.; Brown, E.; Wu, P.; Woo, S.; IYER, A. J. V.; Akula, S. C.; Yang, S.; Yang, J.; Middepogu, M.; Wang, Z.; et al. 2024. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS. Wang, F.; Zhou, W.; Huang, J. Y.; Xu, N.; Zhang, S.; Poon, H.; and Chen, M. 2024a. mDPO: Conditional Preference Optimization for Multimodal Large Language Models. In EMNLP. Wang, J.; Wang, Y.; Xu, G.; Zhang, J.; Gu, Y.; Jia, H.; Wang, J.; Xu, H.; Yan, M.; Zhang, J.; et al. 2023. Amber: An llmfree multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397. Wang, S.; Lin, J.; Guo, X.; Shun, J.; Li, J.; and Zhu, Y. 2025. Reasoning of Large Language Models over Knowledge Graphs with Super-Relations. In ICLR. Wang, Y.; Yu, Z.; Yao, W.; Zeng, Z.; Yang, L.; Wang, C.; Chen, H.; Jiang, C.; Xie, R.; Wang, J.; Xie, X.; Ye, W.; Zhang, S.; and Zhang, Y. 2024b. PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. In ICLR. Wu, Z.; Chen, X.; Pan, Z.; Liu, X.; Liu, W.; Dai, D.; Gao, H.; Ma, Y.; Wu, C.; Wang, B.; et al. 2024. Deepseek-vl2: Mixture-of-experts vision-language models arXiv preprint for advanced multimodal understanding. arXiv:2412.10302. Xie, Y.; Li, G.; Xu, X.; and Kan, M.-Y. 2024. V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization. In Findings: EMNLP. Yang, X.; Zhang, H.; Qi, G.; and Cai, J. 2021. Causal attention for vision-language tasks. In CVPR. Yang, Z.; Luo, X.; Han, D.; Xu, Y.; and Li, D. 2025. Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key. In CVPR. Yu, T.; Yao, Y.; Zhang, H.; He, T.; Han, Y.; Cui, G.; Hu, J.; Liu, Z.; Zheng, H.-T.; Sun, M.; et al. 2024. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In CVPR. Yu, T.; Zhang, H.; Yao, Y.; Dang, Y.; Chen, D.; Lu, X.; Cui, G.; He, T.; Liu, Z.; Chua, T.-S.; et al. 2025. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. In CVPR. Zhang, J.; Huang, J.; Jin, S.; and Lu, S. 2024. VisionLanguage Models for Vision Tasks: Survey. IEEE TPAMI, 46(8): 56255644. Zhou, Y.; Xu, P.; Liu, X.; An, B.; Ai, W.; and Huang, F. 2024. Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. In ACL. Appendix Overview This appendix provides additional details to support the main paper. It is organized as follows: Section 7 details model configurations, training settings for DPO and TARS, and token perturbation procedures. Section 8 presents the min-max optimization algorithm of TARS in pseudocode form. Section 9 includes extended ablations analyzing the perturbation magnitude and spectral regularization strength. Section 10 reports additional benchmark results and finegrained hallucination metrics. Section 11 discusses model behavior, including sensitivity and design insights. Section 12 showcases qualitative comparisons on representative examples."
        },
        {
            "title": "Implementation Details",
            "content": "7 7.1 Base Model Setups We evaluate our method on LLaVA-v1.5 (Liu et al. 2023b) models with 7B and 13B parameters. LLaVA-v1.5 adopts Vicuna-7B/13B (Chiang et al. 2023) as the language backbone and CLIP-ViT-L/14 (Radford et al. 2021) as the vision encoder. The vision encoder also serves as the similarity function G() used in Eq. (9) to compute alignment between visual inputs and text tokens. All experiments are conducted using greedy decoding with temperature of 0 to ensure deterministic outputs and reproducibility."
        },
        {
            "title": "7.2 DPO Training Setups\nFor fair comparison, DPO (Wang et al. 2024a), CHiP (Fu\net al. 2025), and TARS follow the same training protocol\nas described in CHiP (Fu et al. 2025). Specifically, we set\nthe number of epochs to 3, learning rate to 5e-7, warmup\nratio to 0.03, maximum sequence length to 2048, and gra-\ndient clipping threshold to 20.0. Notably, TARS requires no\ntask-specific hyperparameter tuning and demonstrates gen-\neralization across different base models and datasets. All ex-\nperiments are conducted on 8×A100 GPUs (80GB). Each\ntraining run takes approximately 3.0 hours on LLaVA-v1.5-\n7B and 3.4 hours on LLaVA-v1.5-13B.",
            "content": "To generate perturbed inputs, we apply two token-level adversarial strategies: replace and mask. Both are guided by token similarity scores that estimate the alignment between each text token and the visual context. The similarity matrix is normalized into perturbation scores, such that tokens with lower alignment are more likely to be modified. In replace mode, these tokens are substituted with random vocabulary tokens. In mask mode, they are replaced with special token such as [MASK], [UNK], or [PAD], depending on tokenizer availability. Special tokens (e.g., [BOS], [EOS], [PAD]) are explicitly excluded from perturbation."
        },
        {
            "title": "7.3 Evaluation Benchmark Setups\nWe follow the original evaluation settings and benchmark\nsplits for AMBER (Wang et al. 2023), MMHal (Sun et al.",
            "content": "2023), and OBJHal (Yu et al. 2024) as specified in their respective papers. For POPE (Li et al. 2023), we construct new benchmark of 9,000 VQA pairs by sampling using the popular, random, and adversarial strategies."
        },
        {
            "title": "For",
            "content": "four adopt evaluation metrics, we responselevel hallucination measures across different benchmarks: CHAIR (Rohrbach et al. 2018) for object hallucination detection, object coverage (Cover) for completeness measurement, response-level hallucination rate (Hal-Rate) for overall hallucination assessment, sentence-level hallucination rate (CRs) for holistic response evaluation, and object mention-level hallucination rate (CRi) for fine-grained object-level analysis. For evaluation feedback collection, we employ the en-core-web-lg English NLP pipeline for AMBER to extract structured semantic cues as lightweight and reproducible evaluators. For MMHal and OBJHal, we utilize the expert GPT-4V model (Hurst et al. 2024) (gpt-4-1106-vision-preview) for feedback evaluation, following the established protocols of benchmarks."
        },
        {
            "title": "9.1\nImpact of token-level perturbation magnitude\nWe vary the token-level perturbation ratio ω and report re-\nsults in Table 4 to investigate how perturbation strength af-\nfects model performance. In our method, Equation (9) gov-\nerns the selection of tokens for perturbation based on their\nvisual irrelevance. Specifically, we compute the similarity\nbetween visual features Gv(x) and text token embeddings\nGt(qi) to estimate token-level visual alignment. Tokens with\nthe lowest scores are considered visual-agnostic and thus are\nmost eligible for perturbation. The perturbation budget Nt\nis adaptively determined via the scaling coefficient ω and\nmodel uncertainty ∆P as:",
            "content": "A = TopNt (Gv(x) Gt(qi)T ), Nt = (cid:4)ω 1(cid:5) + 1, (21) where = maxj Pj max = jPk quantifies the margin between the top two token-level alignment scores. This formulation encourages stronger perturbations under high uncertainty and milder changes when the model is confident. Algorithm 1: TARS Training Procedure Inputs: Trainable policy πθ, reference policy πref, and preference dataset = {x, q, yw, yr}N . Encoders: Visual encoder Gv; text encoder Gt. Hyperparameters: DPO scaling α, perturbation ratio ω, frequency scaling β, loss weight λ. 1: for each epoch do 2: 3: Max Part: 4: Sample preference tuple (x, q, yw, yr) D. Compute token-level visual relevance: 5: 6: 7: 8: Estimate model confidence margin: Determine adaptive perturbation budget: Select visually agnostic tokens: Pi = Gv(x) Gt(qi)T . = max Pj max k=j Pk. Nt = (cid:4)ω 1(cid:5) + 1. = TopNt (P ). Apply controlled perturbation to obtain φ(q): φ(q) = {I[i A(x, q)] φ(qi) + I[i / A(x, q)] qi}q i=1 . 9: Min Part: 10: Compute the preference alignment loss via DPO: LDPO = log σ α log (cid:18) πθ(yw x, φ(q)) πref(yw x, φ(q)) α log πθ(yr x, φ(q)) πref(yr x, φ(q)) (cid:19) . 11: Apply frequency-domain regularization: Lfreq = log σ β log (cid:18) (cid:20) (hθ(x, φ(q), yw)) (href(x, q, yw)) log (hθ(x, φ(q), yw)) (href(x, q, yr)) (cid:21)(cid:19) . 12: Compute final objective: LTARS = LDPO + λ Lfreq. Update πθ via gradient descent. 13: 14: end for Learned Policy: Optimized policy π θ . (13) (14) (15) (16) (17) (18) (19) (20) As shown in Table 4, moderate values of ω lead to optimal hallucination suppression across both AMBER and OBJHal. Excessively low or high perturbation strengths either underregularize or destabilize training. When ω is too small (e.g., 1e4), the induced distributional shift is limited, resulting in marginal improvement over the baseline. This insufficient perturbation fails to adequately expose the models reliance on spurious token-level correlations, leading to suboptimal alignment correction. Conversely, overly large values (e.g., 5e3 or 1e2) introduce excessive perturbation into visual regions, disrupting the semantic coherence of inputs. This degrades both hallucination control and downstream task accuracy, as the model overfits to unstable signals. The best results are obtained at ω = 1e3, which achieves balance between perturbation diversity and input integrity."
        },
        {
            "title": "9.2\nTo assess the contribution of spectral regularization, we con-\nduct an ablation study by varying the frequency loss weight\nλ in the TARS objective (Equation (12)). The term Lfreq\nencourages semantic consistency between original and per-\nturbed representations by aligning their frequency spectra.\nWhile conceptually appealing, the strength of this constraint\nis modulated by λ and may interact non-trivially with pref-\nerence supervision.",
            "content": "We evaluate λ {0.01, 0.02, 0.05, 0.10, 0.20, 0.50, 1.00} and present results in Table 5. We omit λ = 0.00 here as it reduces the method to DPO with perturbation. As shown in the table, introducing spectral alignment leads to consistent improvements across all benchmarks. Performance improves steadily as λ increases from 0.01 to 0.20, with halTable 4: Ablation study on the effect of token-level perturbation magnitude ω in TARS. We evaluate how varying the perturbation ratio influences hallucination suppression, semantic coherence, and grounding performance across four benchmarks. All experiments use LLaVA-v1.5-13B as the base model and adopt greedy decoding with temperature set to 0 for consistency. Bold results indicate the best-performing configuration."
        },
        {
            "title": "OBJHal",
            "content": "CHAIR Cover Hal-Rate Cog Score Hal-Rate Acc Pre CRs CRi Referenced Results LLaVA-v1.5-7B TARS (Ours) ω = 1e4 ω = 3e4 ω = 5e4 ω = 1e3 ω = 5e3 ω = 1e2 7.6 3.3 3.0 2.9 2.4 3.3 4. 51.7 58.0 58.3 58.7 59.6 57.8 56.9 35.4 15.4 15.1 14.5 13.2 15.9 20.2 4.2 1.3 1.1 0.8 0.4 1.4 1. 2.02 2.35 2.38 2.41 2.48 2.29 2.23 0.61 80.0 61.8 54. 15.8 0.50 0.50 0.47 0.45 0.48 0.51 86.0 86.9 87.7 88.7 86.9 84.0 95.0 95.4 96.1 97.5 91.2 85.7 16.2 15.4 13.6 12.0 16.1 21.9 4.6 4.3 3.8 3.2 3.9 5. lucination rates (CHAIR, Hal-Rate, CRs, CRi) decreasing and coverage improving. The best trade-off is achieved at λ = 0.20, where TARS achieves the lowest hallucination and strongest grounding. Beyond this point, we observe diminishing or adverse effects. For example, at λ = 0.50 and λ = 1.00, performance begins to degrade, particularly on MMHal and OBJHal. This trend suggests that overly aggressive regularization may constrain the models ability to accommodate subtle semantic variations introduced by token-level perturbations, leading to underfitting or conservative outputs. These results confirm that spectral alignment is an effective regularizer when applied with moderate strength. It improves semantic coherence across perturbed samples without rigidly enforcing token-level correspondence, thus allowing preference optimization to remain robust yet expressive under controlled distributional shifts."
        },
        {
            "title": "10 Additional Experimental Results",
            "content": "We present extended results on the AMBER benchmark in Table 6, evaluating hallucination performance from both generative and discriminative perspectives. The left portion of the table reports generative metrics, including CHAIR, Coverage, Hallucination Rate, and Cognitive Score. TARS achieves substantial improvements across all, reducing hallucination by over 13 points compared to DPO, and significantly improving image grounding as reflected by higher coverage and cognitive consistency. Beyond generative evaluation, we further introduce finegrained discriminative metrics that assess hallucination across four categories: Existence, Relation, Attribute, and Action. This allows more detailed understanding of where hallucinations occur and how well each method suppresses them. As shown in the right half of the table, TARS consistently outperforms both DPO and the LLaVA baseline in all dimensions. Notably, it excels in Relation and Attribute grounding, where conventional methods often struggle due to subtle cross-modal mismatches. Together, these results underscore the strength of our token-adaptive perturbation strategy, which not only reduces hallucinations at global level but also enhances semantic fidelity in specific visual grounding aspects, without relying on hand-crafted heuristics or additional supervision. To further dissect model performance on hallucinationprone scenarios, we report average scores across different question categories in the MMHal benchmark (Figure 6). TARS consistently achieves higher scores across all categories, particularly excelling in fine-grained tasks involving spatial reasoning and attribute identification. These results suggest that our method improves not only overall hallucination rates but also robustness to diverse multimodal challenges, highlighting its effectiveness in aligning responses with nuanced visual semantics."
        },
        {
            "title": "11.1 Why TARS outperforms DPO: Beyond",
            "content": "numbers While TARS consistently outperforms standard DPO across hallucination benchmarks, its effectiveness stems not only from empirical gains, but from the design principles that enable better preference alignment under uncertainty. Below, we outline the key factors contributing to the superior performance of TARS. Token-level perturbation enhances alignment robustness. DPO relies on static textual inputs, making it susceptible to overfitting on superficial linguistic patterns such as high-frequency phrases or stylistic biases present in preference data. TARS addresses this issue by introducing controlled perturbations on visually agnostic tokens. These perturbations simulate semantically equivalent variations, thereby exposing the model to distributional shifts during training. As result, the learned policy becomes more robust to alignment uncertainty and is encouraged to rely on visual grounding cues rather than memorized textual artifacts. Visual-agnostic targeting preserves grounding fidelity. Unlike random or uniform perturbation strategies, TARS selectively perturbs tokens with low cross-modal relevance, those that carry minimal visual grounding. This design enTable 5: Ablation study on the effect of spectral alignment weight λ. We evaluate the impact of varying λ on hallucination suppression and multimodal alignment. All experiments are conducted using LLaVA-v1.5-13B as the base model and employ greedy decoding with temperature set to 0 for consistency. Bold numbers indicate the best across each metric."
        },
        {
            "title": "Spectral\nCoefficient",
            "content": "Referenced Results LLaVA-v1.5-13B TARS (Ours) λ = 0.01 λ = 0.02 λ = 0.05 λ = 0.10 λ = 0.20 λ = 0.50 λ = 1."
        },
        {
            "title": "OBJHal",
            "content": "CHAIR Cover Hal-Rate Cog Score Hal-Rate Acc Pre CRs CRi 6.7 2.9 2.7 2.6 2.4 2.1 2.6 3.0 52.1 58.7 59.0 59.3 59.6 59.8 59.0 58. 32.5 14.8 14.1 13.5 13.2 12.5 13.9 15.1 3.5 1.0 0.8 0.6 0.4 0.6 0.9 1.3 2.39 2.80 2.83 2.85 2.88 2.89 2.86 2. 0.53 74.6 55.2 50.0 14.5 0.48 0.46 0.46 0.45 0.45 0.46 0. 87.2 87.5 87.6 88.2 88.5 87.8 86.7 92.5 93.2 93.5 94.3 95.0 92.4 91.0 15.4 14.8 14.7 13.2 12.8 14.4 15.6 3.7 3.4 3.1 2.9 2.8 3.5 4.1 Figure 6: Comparison of average scores across question categories on the MMHal benchmark. sures that semantic shifts are injected without disrupting the causal connection between image and text. By isolating visually agnostic components for perturbation, TARS avoids damaging critical multimodal alignments, resulting in faithful responses that remain sensitive to visual semantics while being resilient to linguistic noise. Spectral alignment encourages semantic consistency. To bridge the mismatch between input perturbations and static supervision, TARS introduces spectral regularizer that aligns representations in the frequency domain. This global constraint allows for flexible modifications while maintaining semantic coherence at the sequence level. Unlike rigid token-level matching, frequency alignment smooths over local variations and discourages the model from latching onto spurious token-level correlations. This helps prevent overfitting to distribution-specific artifacts and improves generalization under preference supervision."
        },
        {
            "title": "11.2 Token perturbation sensitivity",
            "content": "Impact of perturbation strength. TARS determines the number of perturbed tokens based on the models confidence and scaling factor ω. In Section 9.1, we conduct systematic evaluation of perturbation magnitude to assess sensitivity. Results show that moderate perturbation levels (e.g., ω = 1e3) yield optimal hallucination suppression, while excessively small or large values lead to underor overregularization. These findings confirm that the model benefits from controlled perturbation, and that TARS is robust across range of ω values when properly calibrated. Stability across perturbation strategies. We also compare two token-level perturbation strategies used in TARS: token masking and synonym replacement. Despite their distinct mechanismsmasking introduces structural noise, while replacement maintains fluent semanticsboth consistently outperform unperturbed DPO. Notably, masking yields slightly better hallucination mitigation, likely due to its stronger distributional shift. The close performance of both strategies suggests that the core effectiveness of TARS is not overly sensitive to the specific perturbation operator, as long as semantic integrity is preserved. Effectiveness of adaptive perturbation scope. Instead of applying fixed number of perturbations, TARS dynamically scales the perturbation budget according to the models alignment uncertainty. This adaptive strategy ensures that confident predictions remain mostly intact, while uncertain ones are regularized more aggressively. Such input-aware perturbation improves training stability and avoids unnecessary semantic drift, reinforcing the models ability to distinguish visually grounded content from spurious correlations."
        },
        {
            "title": "11.3 Spectrum-based alignment vs token-level",
            "content": "alignment Motivation. While TARS perturbs visually agnostic tokens to expose latent alignment vulnerabilities, the preference supervision signal remains static. This creates mismatch: dynamic inputs versus fixed feedback. common solution is to enforce token-level representation consistency, such as applying cosine similarity or contrastive loss between the perturbed and original hidden states. However, such approaches assume strict token-wise correspondence, which may not Table 6: Comparison of generative and fine-grained discriminative hallucination metrics on the AMBER benchmark. TARS achieves consistent gains over DPO and the LLaVA baseline across both holistic and category-specific evaluations, demonstrating enhanced visual-textual alignment and robust hallucination suppression. Algorithm Generative Discriminative CHAIR Cover Hal-Rate Cog Existence Relation Attribute Action LLaVA-v1.5-7B (Liu et al. 2023b) + DPO (Li, Lin, and Pei 2024) + TARS (Ours) LLaVA-v1.5-13B (Liu et al. 2023b) + DPO (Li, Lin, and Pei 2024) + TARS (Ours) 7.9 4.9 2.4 6.7 4.1 2.1 54.7 56.6 59. 52.1 56.7 59.8 37.1 26.4 13.2 32.5 54.3 12.5 3.2 2.5 0.4 3.5 2.2 0.6 82.9 87.1 95. 94.1 95.0 98.9 58.6 59.7 62.8 45.5 58.8 67.0 65.6 74.6 78.6 70.1 73.1 82.0 70.1 79.4 86. 76.2 81.5 86.6 Table 7: Comparison of spectrum-based alignment (TARS) versus token-level contrastive alignment. Both models use identical perturbation policies. Spectrum-based alignment achieves lower hallucination and better semantic consistency. Alignment Strategy AMBER MMHal POPE OBJHal CHAIR Cover Hal-Rate Cog Token-level (contrastive) Spectrum-based (TARS) 3.4 2.4 57.2 59.6 16.3 13. 1.4 0.4 Score Hal-Rate Acc Pre CRs CRi 4.9 87.1 3.2 88.7 93.3 97.5 15.8 12.0 2.36 2.48 0.49 0. hold under semantic-preserving perturbations. Limitations of token-level alignment. We empirically observe that enforcing rigid token-level matching often leads to instability during training and degrades hallucination mitigation. Token-level losses tend to penalize even small, semantically irrelevant shifts introduced by benign perturbations. This over-constraint reintroduces the spurious correlation that TARS aims to alleviate. In contrast, frequencybased alignment allows for local flexibility while enforcing global consistency in the hidden representation spectrum. Comparative evaluation. To validate this claim, we compare the spectral preference loss in Equation (11) with baseline token-level contrastive alignment strategy: Contrastive loss (Token-level): Hidden states from (x, q, yw) and (x, ϕ(q), yw) are aligned using an InfoNCE objective. Spectrum loss (TARS): Frequency magnitude alignment via log-ratio of FFT-transformed hidden states. As shown in Table 7, spectrum-based alignment achieves lower hallucination rates and better factual grounding across AMBER and OBJHal. This confirms that semantic-level flexibility, rather than rigid token consistency, is key to robust preference optimization."
        },
        {
            "title": "11.4 Limitations\nIn this work, we adopt two simple perturbation strategies:\ntoken masking and synonym replacement. These methods\nare chosen for their clarity, efficiency, and ease of interpre-\ntation, allowing us to isolate the effects of token-level align-\nment without introducing unnecessary complexity. How-\never, their simplicity may limit the generality and flexibil-\nity of the approach. Future work could explore adaptive or\ndata-driven perturbation mechanisms that better balance se-\nmantic preservation with distributional shift. Additionally,\nthe current token selection strategy, based on cross-modal\nsimilarity heuristics, could be enhanced by learning-based",
            "content": "relevance estimation or causal attribution techniques to enable more precise and effective perturbations."
        },
        {
            "title": "12 Qualitative Examples\nWe provide qualitative comparisons between standard DPO\nand our proposed TARS in Table 8, across diverse image-\nquestion pairs. TARS consistently demonstrates improved\ngrounding and hallucination suppression, outperforming tra-\nditional DPO in several key aspects:\nReduced hallucination via improved visual grounding.\nCompared to DPO, TARS produces responses that more ac-\ncurately reflect the image content. In all cases, DPO intro-\nduces visual details not present in the input, while TARS\nremains faithful to the scene.\nNo degradation in response completeness. Importantly,\nTARS maintains response richness without sacrificing\nlength or informativeness. As illustrated in (d), TARS gener-\nates a detailed yet grounded answer, whereas DPO provides\nlonger responses but includes hallucinated attributes.\nBetter fine-grained grounding. TARS exhibits improved\nalignment at the fine-grained level, such as object color and\ncount. In examples (b) and (e), it correctly identifies color\nattributes that DPO misrepresents.\nEnhanced descriptive richness. TARS responses incorpo-\nrate more relevant visual details, indicating stronger cross-\nmodal alignment. The generated answers are not only more\naccurate but also semantically richer, reflecting a deeper un-\nderstanding of the visual input.\nThese examples highlight",
            "content": "the effectiveness of tokenadaptive perturbation in guiding the model to learn robust visual-textual associations, ultimately reducing hallucinations without compromising informativeness. (a) (b) (c) (d) (e) Table 8: Qualitative comparisons between DPO and our proposed TARS across five diverse image-prompt pairs, denoted as (a)(e). Each row presents the same visual input and accompanying question, with model responses shown for both methods. Hallucinated content is highlighted in red, while accurate visual grounding is marked in green. TARS consistently produces more faithful and informative responses, demonstrating superior grounding and hallucination mitigation even under visually ambiguous or linguistically subtle conditions."
        }
    ],
    "affiliations": [
        "AWS AI Lab, Amazon",
        "DAMO Academy, Alibaba Group",
        "Department of Artificial Intelligence, Xiamen University",
        "Hupan Laboratory",
        "School of Engineering, Westlake University"
    ]
}