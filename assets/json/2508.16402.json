{
    "paper_title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions",
    "authors": [
        "Zihan Wang",
        "Jiaze Chen",
        "Zhicheng Liu",
        "Markus Mak",
        "Yidi Du",
        "Geonsik Moon",
        "Luoqi Xu",
        "Aaron Tua",
        "Kunshuo Peng",
        "Jiayi Lu",
        "Mingfei Xia",
        "Boqian Zou",
        "Chenyang Ran",
        "Guang Tian",
        "Shoutai Zhu",
        "Yeheng Duan",
        "Zhenghui Kang",
        "Zhenxing Lin",
        "Shangshu Li",
        "Qiang Luo",
        "Qingshen Long",
        "Zhiyong Chen",
        "Yihan Xiao",
        "Yurong Wu",
        "Daoguang Zan",
        "Yuyi Fu",
        "Mingxuan Wang",
        "Ming Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Competitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking a substantial gap between LLMs and elite human programmers. This gap arises from two key limitations: insufficient difficulty and scope of benchmark problems, and evaluation bias from low-quality test cases. To address these shortcomings, we present AetherCode, a new benchmark that draws problems from premier programming competitions such as IOI and ICPC, offering broader coverage and higher difficulty. AetherCode further incorporates comprehensive, expert-validated test suites built through a hybrid of automated generation and human curation, ensuring rigorous and reliable assessment. By combining challenging problem design with robust evaluation, AetherCode provides a more faithful measure of LLM capabilities and sets a new standard for future research in code reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 2 0 4 6 1 . 8 0 5 2 : r AetherCode: Evaluating LLMs Ability to Win In Premier Programming Competitions ByteDance, M-A-P"
        },
        {
            "title": "Abstract",
            "content": "Competitive programming has emerged as critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking substantial gap between LLMs and elite human programmers. This gap arises from two key limitations: insufficient difficulty and scope of benchmark problems, and evaluation bias from low-quality test cases. To address these shortcomings, we present AetherCode, new benchmark that draws problems from premier programming competitions such as IOI and ICPC, offering broader coverage and higher difficulty. AetherCode further incorporates comprehensive, expert-validated test suites built through hybrid of automated generation and human curation, ensuring rigorous and reliable assessment. By combining challenging problem design with robust evaluation, AetherCode provides more faithful measure of LLM capabilities and sets new standard for future research in code reasoning. Date: August 25, 2025 Correspondence: Zihan Wang at zh.wang@bytedance.com, Jiaze Chen at chenjiaze@bytedance.com Project Page: https://huggingface.co/datasets/m-a-p/AetherCode"
        },
        {
            "title": "Introduction",
            "content": "Competitive programming is widely regarded as crucial benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs) [14]. Solving complex competitive programming problems demands not only sophisticated reasoning abilities but also knowledge from diverse domains, including mathematics, data structures, and algorithms. Recent years have witnessed rapid advancements in the reasoning capabilities of LLMs, key indicator of which is their success on majority of existing code reasoning benchmarks. State-of-the-art models now achieve over 90% Pass@1 accuracy on MBPP [1] and HumanEval [2], and over 80% on LiveCodeBench [6]. These encouraging developments might lead one to ask: has competitive programming been mastered by LLMs? In this paper, we argue that significant gap still exists between the performance of LLMs and top-tier human competitors in programming contests. We propose that the perception of LLM dominance stems primarily from the limitations in the breadth and rigor of current code reasoning benchmarks, which are no longer sufficient to fully assess the capabilities of todays increasingly powerful models. Specifically, we identify two main shortcomings in existing benchmarks: Insufficient Difficulty and Scope. Early benchmarks such as HumanEval [2] and MBPP [1] consist of basic coding tasks, for instance, sorting or reversing list, which present minimal reasoning challenges 1 for state-of-the-art LLMs. More recent competition-level benchmarks often source problems from limited set of websites. For example, LiveCodeBench [6] collects problems mainly from LeetCode and AtCoder, while CodeELO [15] and LiveCodeBench Pro [21] focus solely on CodeForces. Each of these websites has inherent limitations. LeetCode problems are generally easier and often require only the implementation of single function rather than complete program. CodeForces contests, which typically feature 5-7 problems within 2-3 hour timeframe, constrain the design space for problem setters, for example, leading to scarcity of problems that require complex, large-scale implementations. Evaluation Bias from Low-Quality Test Cases. The correctness of piece of code is verified using comprehensive set of test cases (input-output pairs). An incomplete test suite may fail to detect incorrect submissions, particularly those with subtle flaws, such as the mishandling of corner cases or solutions that exceed time limits under specific, extreme conditions. Consequently, designing high-quality test cases is huge challenge that requires deep understanding of potential failure points, skill typically honed through extensive competitive programming experience. Most past benchmarks lack sufficiently rigorous test cases. HumanEval [2] and MBPP [1], for instance, rely on small number of handwritten test cases. Others, including EvalPlus [10], CodeContests [8], and LiveCodeBench [6], employ naive test case generation pipelines, such as random mutation, which fall far short of the quality of expert-designed test suites. Furthermore, recent research [19] has revealed issues with test case correctness itself; for example, many test cases in the CodeContests dataset do not adhere to the problems constraints, causing even correct solutions to fail. It is worth noting that some recent benchmarks, such as CodeELO [15] and LiveCodeBench Pro [21], have attempted to leverage the official CodeForces judging service to indirectly access its high-quality, expert-crafted test cases. However, this approach presents two significant issues. First, it raises compliance risks, as CodeForces explicitly prohibits the use of crawlers on its judging interface. Second, this method is constrained by submission frequency limits, which impedes agile and flexible experimentation. Therefore, we contend that an open-source benchmark with high-quality, self-contained test cases remains critically important for the LLM community. To address these challenges, we introduce AetherCode, new benchmark with the following key contributions: Problem Curation from Top-Tier Competitions. AetherCode is the first benchmark to systematically collect problems from premier programming competitions worldwide, including the Olympiad in Informatics (OI) and the International Collegiate Programming Contest (ICPC). Our process involved comprehensive collection, meticulous cleaning, and format conversion of problems from PDF to Markdown+LaTeX structure. Each problem statement was manually proofread for correctness, and team of competitive programming experts annotated each problem with classification tags. High-Quality Test Case Generation. We developed hybrid methodology, combining automated generation with expert annotation, to create high-quality test cases for every problem. We evaluated the correctness and comprehensiveness of our test cases by validating them against large corpus of collected solutions, enforcing standard of zero false positives and zero false negatives. This paper is organized as follows: Section 2 details the benchmark curation process. Section 3 presents our evaluation results. Section 4 concludes the paper and discusses directions for future research."
        },
        {
            "title": "2 Benchmark Curation",
            "content": "This Section details the curation process of the AetherCode Benchmark. Sections 2.1 and 2.2 describe the specifics of problem collection and categorizing, respectively. Section 2.3 explains how we construct high-quality test cases for each problem, and Section 2.4 presents the statistical data of AetherCode."
        },
        {
            "title": "2.1 Problem Collection",
            "content": "We source our problems from premier programming competitions worldwide rather than from online programming websites. Based on their target audience, these competitions can be broadly categorized into two main series: the Olympiad in Informatics (OI) series, which is aimed at pre-college school students, and the International Collegiate Programming Contest (ICPC) series, which is designed for college students. 2 Table 1 Comparison between AetherCode and other code reasoning benchmarks Dataset Difficulty # Problems Updates HumanEval [2] MBPP [1] APPS [5] USACO [17] CodeContests [8] LiveCodeBench [6] CodeELO [15] LiveCodeBench Pro [21] AetherCode 164 974 5,000 307 165 1055 387 584 456 Test Cases Construction Handcrafted Handcrafted Crawled Publicly accessible Mutation Semi-automatic Source Original Original CodeForces, AtCoder etc. USACO CodeForces, AtCoder etc. LeetCode, AtCoder CodeForces CodeForces G-V Agent[19] & Experts Premier Contests OI Series. The Olympiad in Informatics is series of competitions aimed at popularizing computer science knowledge among middle-school students and cultivating outstanding talents in computer science. The OI competitions usually require participants to solve algorithm-related problems by programming. Take the International Olympiad in Informatics (IOI), the top-level event of OI, as an example. Each contestant competes individually, and each country can send up to 4 players. During the two-day competition, players need to independently solve 3 problems within 5 hours each day, mainly using C++. Furthermore, various countries and regions host their own national or regional OI competitions, such as the National Olympiad in Informatics (NOI) in China and the USA Computing Olympiad (USACO) in the United States. Top-performing contestants in these competitions earn the opportunity to advance to the IOI. ICPC Series. The ICPC is the oldest, largest, and most prestigious university-level programming contest in the world. Each team consists of up to 3 students and uses one computer to solve 10 - 13 problems in 5 hours, using programming languages such as C, C++, Java, or Python. The team that correctly solves the most problems with the least total time wins. The world is divided into several regions for the ICPC. In Europe, there are Central Europe (CERC), North Europe (NWERC), South-East Europe (SEERC), and South-West Europe (SWERC) regions. Other regions include Asia-Pacific, Asia East Continent, North America, Latin America, Africa, and Arab region, etc. The ICPC is multi-tiered event. First, there are regional contests held worldwide from September to November each year. The top-performing teams in the regional contests advance to the regional finals or championships. Then, the best teams from these finals or championships qualify for the ICPC World Finals, which is usually held from April to June each year. This is the highest-level stage of the ICPC, where the best teams from around the world compete for the championship. In addition to the official ICPC events, we also incorporated problems from other large-scale and renowned collegiate programming contests, such as the China Collegiate Programming Contest (CCPC). For each problem, we collected the following components: Problem Statement. The statement typically comprises title, detailed problem description, input/output specifications, sample inputs and outputs with explanations, data range constraints, and time/memory limits. The majority of the problem statements was originally in PDF format. To enhance comprehension for LLMs, we converted these PDFs into Markdown format with LaTeX for mathematical notations. Each converted file was then manually proofread to ensure its accuracy. Solutions. We curated collection of over 30,000 human-written solutions for these problems, encompassing both correct and incorrect submissions. For each problem, we ensured minimum of 5 correct and 20 incorrect solutions. The primary purpose of collecting these solutions is to evaluate the quality of the subsequently generated test cases, process detailed in Section 2.3. Test Cases. minority of the competitions, e.g., USACO, publicly released their official test cases, which we collected and standardized. For problems where official test cases were not available, we 3 constructed our high-quality test cases. The methodology for this construction is described in Section 2.3. Metadata. We also gathered auxiliary information, such as the date of the competition (for decontamination purposes) and human contestant performance data (to facilitate difficulty assessment), among other available data points."
        },
        {
            "title": "2.2 Problem Categorization",
            "content": "Beyond curating problems, an equally critical step in constructing AetherCode was the systematic categorization of each problem to ensure comprehensive coverage and facilitate fine-grained evaluation. To this end, we adopted multi-dimensional categorization framework designed with the input of competitive programming experts: 1. Difficulty Segmentation. Problems were divided into four levels of difficulty: Easy, Medium, Hard, and Extreme. This classification was guided by expert judgment as well as official contest results. Notably, problems that no human contestant was able to solve during competition were classified as Extreme, representing challenges that push the boundaries of algorithmic reasoning. 2. Temporal and Contextual Dimensions. Each problem was annotated with metadata to enable both decontamination and longitudinal analysis of model performance: Year of the contest, allowing chronological tracking of trends in problem design and model capabilities. Competition type, primarily distinguishing between Olympiad in Informatics (OI) and International Collegiate Programming Contest (ICPC) series. Competition scope, categorizing contests as regional-level, national-level, continental-level, or world finals. 3. Problem Format Constraints. Some problems require additional considerations beyond standard inputoutput interface: Problems dependent on visual or image-based input were excluded from the benchmark. Problems requiring special judges or custom checkers were explicitly labeled to ensure proper handling during evaluation. 4. Algorithmic and Domain Categories. To capture the breadth of algorithmic knowledge tested in programming contests, we implemented hierarchical taxonomy as shown in Table 5: Primary categories correspond to major domains such as Dynamic Programming, Graph Theory, Computational Geometry, Data Structures, and Mathematics. Secondary categories provide finer granularity, such as tree dynamic programming, flow algorithms, convex hull geometry, or modular arithmetic. Problems can belong to multiple categories to reflect their cross-disciplinary nature. This structured categorization enables targeted evaluation of model strengths and weaknesses while also ensuring that AetherCode serves as scalable resource for future research. In particular, it allows progress to be tracked across difficulty levels, problem types, and algorithmic domains, providing more comprehensive understanding of model capabilities."
        },
        {
            "title": "2.3 Test Case Construction",
            "content": "Recent studies [10, 19] have highlighted concerns regarding the quality of test cases in several existing code datasets. For instance, benchmarks such as MBPP [1] and HumanEval [2] include only limited number of handwritten test cases per problem. Others, like CodeContests [8] and EvalPlus [10], rely on naive methods such as mutation to generate test cases. Consequently, such test cases are insufficient for comprehensively 4 evaluating the correctness and efficiency of program. Therefore, we contend that the quality of test case construction is critical factor determining the overall quality of benchmark. Notably, some recent benchmarks [15, 21] directly utilize the CodeForcess judging service for evaluation. This approach allows them to indirectly access high-quality test cases created by professional problem setters, thereby circumventing the challenge of test case construction. However, this method presents potential compliance risks, as CodeForces explicitly prohibits the use of crawlers on its judging interface. Furthermore, this approach is constrained by submission frequency limits, which impedes agile and flexible evaluation. Therefore, we argue that benchmark equipped with its own high-quality test cases remains critically important for the LLM community. To ensure AetherCode possesses sufficiently high-quality test cases, we approached the task from two perspectives. First, we established more stringent evaluation criteria for test case quality, which is presented in Section 2.3.1. Second, we employed hybrid approach, combining automated generation with expert annotation, to construct the test cases, which are presented in Sections 2.3.2 and 2.3.3."
        },
        {
            "title": "2.3.1 Test Case Quality Assessment",
            "content": "Previous research on test case quality has predominantly focused on quantity, operating under the assumption that greater number of test cases correlates with higher quality [7, 8]. However, recent studies [19] indicate that quantity is not direct proxy for quality. This discrepancy arises from two primary issues. First, test cases in some older datasets, despite their volume, suffer from significant correctness issues, often violating the problems explicit constraints. Second, conventional test case generation methods that merely amass large volumes of random data fail to provide adequate coverage of various special and corner cases. Consequently, we depart from evaluating test cases by their quantity and instead propose direct assessment of their ability to discriminate between correct and incorrect solutions. In our framework, we conceptualize the entire test suite for problem as binary classifier, that is, classifier that distinguishes between correct and incorrect solutions. We then evaluate the performance of this classifier using large, curated collection of both correct and incorrect submissions. We adopt the True Positive Rate (TPR) and True Negative Rate (TNR) as our primary evaluation metrics. TPR = TNR = True Positive True Positive + False Negative True Negative True Negative + False Positive = = Number of Passed Correct Solutions Number of Correct Solutions Number of Rejected Incorrect Solutions Number of Incorrect Solutions (1) (2) The TPR measures the correctness of the test cases; high TPR indicates that correct solutions are not erroneously failed, which is expected when the test cases themselves are valid. Conversely, the TNR measures the comprehensiveness or coverage of the test cases, quantifying their ability to detect (or hack) incorrect solutions. By employing hybrid approach that combines automated generation with expert curation, we have achieved 100% TPR and 100% TNR on our collected solution set. This signifies that all collected correct solutions pass our test cases, while all collected incorrect solutions are successfully rejected. To the best of our knowledge, AetherCode is the first benchmark that sets such high standard for test cases. 2.3.2 Automatic Construction of Test Cases We employed the Generator-Validator Agent System [19] to automatically construct the test cases, methodology whose effectiveness has been well-established in prior research [19]. Building upon this foundation, we incorporated an additional step of manual verification for the Validator. This step ensures the Validators correctness, thereby guaranteeing that all generated test cases adhere to every constraint specified in the problem description. 5 Recognizing that the initial Automatic Construction phase could not achieve 100% TNR on its own, we introduced an additional expert annotation stage to further strengthen the test cases."
        },
        {
            "title": "2.3.3 Expert Annotation of Test Cases",
            "content": "To this end, we recruited 67 competitive programming experts. The majority of them hold Codeforces ratings above 2000, with one expert exceeding 2600 and achieving the title of International Grandmaster. These experts were tasked with constructing targeted test cases specifically designed to fail the various incorrect solutions we had collected. These manually crafted test cases were then merged with the automatically generated ones to form the final test suite. Furthermore, we recognized that for certain problems with limited number of collected incorrect solutions (fewer than 50), achieving 100% TNR might not sufficiently guarantee the robustness of the test cases. To address this, we subjected the test cases for all problems to manual quality audit by specialized review team. Each member of this elite team holds at least three ICPC gold medals and has minimum of two years of experience in competitive programming problem-setting. Their deep understanding of potential pitfalls and common errors in each problem allows them to leverage their extensive experience to further ensure the quality and comprehensiveness of the test cases. Additionally, for problems that accept multiple valid outputs, customized judging scripts (a.k.a. checker, or special judge) were provided and thoroughly reviewed by these experts to ensure correct evaluation."
        },
        {
            "title": "2.4 Statistics",
            "content": "The number of problems in different difficulties and years of AetherCode v1 is presented in Table 2. The number of problems in different categories of AetherCode v1 is presented in Table 3. Table 2 The number of problems in different difficulties and years of AetherCode v1 (2401-2505). Difficulty Easy Medium Hard Extreme Year 2024 159 145 132 20 400 Table 3 The number of problems in different categories of AetherCode v1 (2401-2505). Category Count Basic 225 Search 50 DP 110 Str. Math 96 DS 120 Graph Geo. 36 64 Tech. 147 Tree"
        },
        {
            "title": "3 Evaluation",
            "content": "Our evaluation includes 8 reasoning models and 5 non-reasoning models. The reasoning models comprise o4-mini-high [13], Gemini-2.5-Pro/Flash [3], Seed-1.6-Thinking [16], DeepSeek-R1 [4], and Qwen3 [20], among others. The non-reasoning models consist of GPT-4.1 [11], GPT-4o [12], Kimi-K2 [18], DeepSeek-V3 [9], and Qwen3-Coder. All models are configured with maximum output length of 32,768 tokens. Each model is evaluated four times in each problem, and the average results are reported."
        },
        {
            "title": "3.1 Main Result",
            "content": "Table 4 presents comprehensive performance evaluation of several prominent models on AetherCode. For full results, please refer to the online leaderboard. The analysis yields the following key conclusions: 6 Table 4 Performance comparison between reasoning models and non-reasoning models on AetherCode v1 (%, 24012505). Model Reasoning Models o4-mini-high Gemini-2.5-Pro Seed-1.6-thinking-0715 DeepSeek-R1-0528 Gemini-2.5-Flash Qwen3-235B-A22B Qwen3-32B Qwen3-8B Non-Reasoning Models GPT-4.1 Kimi-K2 DeepSeek-V3-0324 Qwen3-Coder-480B-A35B GPT-4o Difficulty Year Pass@ Easy Medium Hard Extreme 2024 2025 1 2 65.3 60.1 53.9 46.2 42.1 37.6 34.8 23.7 23.9 23.1 20.8 19.7 11.6 32.1 28.6 20.2 16.0 15.2 12.4 10.9 4.8 5.7 4.7 4.0 2.2 1.0 8.0 8.5 4.7 3.8 2.7 1.9 2.7 0.8 1.1 1.0 0 0.6 0. 3.8 2.5 0 0 0 0 0 0 0 0 0 0 0 35.8 33.7 28.3 23.4 22.0 19.1 17.7 11.1 11.3 10.6 8.9 8.6 4.9 32.6 25.0 14.7 14.3 8.0 7.1 6.7 2.7 4.5 4.0 5.4 1.8 1. 35.5 32.7 26.6 22.3 20.3 17.6 16.3 10.0 10.5 9.8 8.5 7.7 4.4 43.0 39.8 33.0 27.4 24.5 21.7 20.4 13.0 13.2 12.2 10.5 9.9 5.6 46.6 46.0 38.5 32.4 28.5 25.2 23.9 15.5 15.3 14.5 12.3 11.8 7. Significant Performance Gap between Models The performance of o4-mini-high and Gemini-2.5-Pro is exceptional, establishing significant performance gap that places them in tier of their own above other models. Furthermore, they are the only two models capable of successfully solving problems at the \"Extremely Difficult\" level. Across all difficulty tiers, the performance of these two models substantially surpasses that of their competitors. Reasoning Models Comprehensively Outperform Non-Reasoning Models As anticipated, reasoning models demonstrate markedly superior performance compared to non-reasoning models. For instance, models from the Qwen3 series, such as Qwen3-32B, outperform several non-reasoning models despite having fewer parameters. More notably, even with four sampling attempts (Pass@4 ), the performance of non-reasoning models still falls short of that achieved by reasoning models. This phenomenon indicates that for complex tasks like coding competitions, the solution space exploration capabilities of non-reasoning models are constrained, making it difficult to find correct solutions through limited sampling. This bottleneck is particularly pronounced in weaker models. Top-Tier Models Exhibit Great Exploration Potential comparison of Pass@1 and Pass@4 scores reveals that increasing the number of samples yields more substantial performance improvement for top-tier models. For example, o4-mini-highs score improved by 11.1% (from 35.5% to 46.6%), whereas the weaker Qwen3-32B only saw gain of 7.6% (from 16.3% to 23.9%). Particularly noteworthy is Gemini-2.5-Pro, which achieved remarkable performance increase of 13.3% (from 32.5% to 46.0%). This demonstrates its vast exploration potential in solving complex programming problems, enabling it to generate more diverse and high-quality solutions through multiple attempts."
        },
        {
            "title": "3.2 Performance Across Algorithms",
            "content": "The performance comparison in Table 6 reveals significant differentiation in model capabilities across various problem categories. All models, regardless of being reasoning or non-reasoning types, uniformly excel at pattern-based tasks such as Basic Algorithms and Strings. However, their limitations become equally apparent when handling highly abstract problems. Most models struggle to tackle Computational Geometry and Tree Structures, with the performance of o4-mini-high in computational geometry being notable exception. Furthermore, the shortcomings of non-reasoning models are particularly pronounced, as their capability bottlenecks extend into domains that also demand deep logic and abstract thinking, such as Dynamic Programming and Mathematics."
        },
        {
            "title": "4 Conclusion",
            "content": "In this paper, we introduced AetherCode, challenging, rigorously evaluated benchmark purpose-built to assess LLMs coding and reasoning capabilities. AetherCode distinguishes itself by sourcing all its problems from premier global programming competitions, including OI series and ICPC series, which ensures high degree of challenge and relevance. Furthermore, it features comprehensive and meticulously validated suite of test cases, created through hybrid model of automated generation and expert curation. By validating against dataset of over 30,000 human submissions, our test suite achieves 100% TPR and 100% TNR on our collected solution set, guaranteeing exceptional accuracy and reliability in evaluation. Our comprehensive evaluation of several leading-edge models on AetherCode yielded critical insights. We observed significant performance disparity among models, with top performers like o4-mini-high and Gemini-2.5-Pro establishing distinct upper tier. Reasoning models demonstrated clear and consistent advantage over their non-reasoning counterparts across all difficulty levels, highlighting the crucial role of logical deduction in solving complex algorithmic problems. Overall, even the most advanced models today can only solve small fraction of problems in AetherCode. This indicates that current LLMs still have considerable room for improvement in reasoning and coding, and there remains significant gap compared to top human experts. 8 Table 5 Category division and detailed tag distribution of AetherCode. Category Algorithm Basics Search Dynamic Programming Strings Mathematics Data Structures Graph Theory Computational Geometry Common Techniques Problems on Trees Tags Enumeration, Simulation, Recursion, Greedy, Sorting, Divide and Conquer, Binary Search, Doubling, Recurrence DFS, BFS, Bidirectional Search, Heuristic Search, A*, Iterative Deepening Search, IDA*, Dancing Links Basic DP, Memorization Search, Knapsack DP, Range DP, DP on DAGs, Tree DP, Bitmask DP, Digit DP, Plug DP, Counting DP, Dynamic DP, Probability DP, DP Optimization String Matching, String Hashing, Trie, Palindrome Automation, Prefix Function, Z-function, Automation, AC Automation, Suffix Array, Suffix Automation, Suffix Balanced Tree, Generalized Suffix Automation, Suffix Tree, Manachers Algorithm, KMP Algorithm, Sequence Automation, Minimal Representation, Lyndon Factorization, MainLorentz Algorithm Number Theory, Linear Algebra, Linear Programming, Abstract Algebra, Probability Theory, Game Theory, Young Matrix, InclusionExclusion Principle, Combinatorics, Polynomials Stack, Queue, Linked List, Hash Table, Disjoint Set Union, Heap, Block Structure, Monotonic Queue, ST Table, Binary Indexed Tree, Segment Tree, Balanced Tree, Binary Tree & Balanced Tree, Block Decomposition, Persistent Data Structures, Tree-in-Tree, K-D Tree, Cartesian Tree, Huffman Tree, STL-based Data Structure Matrix-Tree Theorem, Directed Acyclic Graph, Topological Sort, Minimum Spanning Tree, Minimum Diameter Spanning Tree, Minimum Tree Spanning, Connectivity, Shortest Path, 2-SAT, Difference Constraints, Hamiltonian Graph, Modular Shortest Path, Graph Coloring, Eulerian Graph, Dominating Tree, Bipartite Graph, Prüfer Sequence, Planar Graph, Chordal Graph, Network Flow, Graph Matching, Random Walk on Graphs, LGV Lemma, Strongly Connected Components Euclidean Distance, Manhattan Distance, Chebyshev Distance, Picks Theorem, Triangulation, Convex Hull, Sweep Line, Rotating Calipers, Half-Plane Intersection, Closest Pair of Points, Random Increment Method, Reflection Transformation, Misc. CG Discretization, Two Pointer Technique, Prefix Sum & Difference, Fractional Programming, Randomization, Hanging Line Method, Binary Thinking, Pattern Recognition, Gray Code, Expression Evaluation, Construction, Properties of Bitwise Operations, Conjecture of Conclusions, Interactive Problems, Meet in Middle, Ad-hoc, Uncertainty Algorithms, Square Root Decomposition LCA, DSU on Tree, Divide and Conquer on Points, Block Decomposition on Tree, Heavy-Light Decomposition, Chain Decomposition, Tree Diameter and Centroid, LCT 9 Table 6 Performance comparison (Pass@1) between reasoning models and non-reasoning models across different categories of algorithmic problems. The abbreviations Basic, Search, DP, Str., Math, DS, Graph, Geo., Tech., Tree represent Algorithm Basics, Search, Dynamic Programming, Strings, Mathematics, Data Structures, Graph Theory, Computational Geometry, Common Techniques, and Problems on Trees, respectively. Model Basic Search DP Str. Math DS Graph Geo. Tech. Tree Reasoning Models o4-mini-high Gemini-2.5-Pro Seed-1.6-thinking DeepSeek-R1-0528 Gemini-2.5-Flash Qwen3-235B-A22B Qwen3-32b Qwen3-8B Non-Reasoning Models GPT-4.1 Kimi-K2 DeepSeek-V3-0324 Qwen3-Coder-480B-A35B GPT-4o 38.1 36.1 32.2 26.3 24.1 22.2 19.7 13. 13.9 13.7 12.1 11.1 7.2 28.5 24.5 17.0 16.0 16.5 13.0 11.5 9.0 9.5 7.5 7.0 5.5 4.5 27.7 24.6 17.3 14.6 11.8 8.4 10.9 3.9 3.4 3.6 1.8 1.8 0.7 35.6 29.8 26.0 23.1 19.2 20.2 18.3 15. 19.2 15.4 14.4 14.4 11.5 31.8 31.5 24.2 19.3 16.7 13.5 14.1 7.6 4.2 7.0 3.9 4.2 1.6 25.8 25.4 17.9 16.3 16.3 11.0 11.0 7.9 8.3 8.1 6.3 5.2 2.9 28.5 26.2 18.8 15.6 17.2 12.5 9.4 6. 5.5 6.6 4.3 4.3 0.4 27.1 18.1 12.5 10.4 13.2 11.1 6.9 1.4 6.3 0.7 0 1.4 0 26.9 23.0 19.2 13.8 11.4 9.4 11.2 4.9 6.0 3.6 3.6 2.9 1.5 7.3 7.3 1.0 7.3 4.2 4.2 0 1. 0 0 0 1.0 0 Table 7 Curated Contest Source of AetherCode v1 (2401-2505). Competition Name Category Croatian Open Competition in Informatics 2023/2024 Contest #3 USACO 2024 January Contest (Platinum) The 2023-2024 ICPC Southwestern Europe Regional Contest Croatian Open Competition in Informatics 2023/2024 Contest #4 USACO 2024 February Contest (Platinum) USACO 2024 US Open Contest (Platinum) Singapore National Olympiad in Informatics 2024 Final Contest Croatian Open Competition in Informatics 2023/2024 Contest #5 The 2024 ICPC Latin America Championship The 2024 ICPC Europe Championship The 2024 British Informatics Olympiad Final Baltic Olympiad in Informatics 2024 Day 1 Baltic Olympiad in Informatics 2024 Day 2 Asia-Pacific Informatics Olympiad 2024 (APIO 2024) The 2024 ICPC North America Championship Central European Olympiad in Informatics 2024 Day 1 (CEOI 2024 Day 1) Central European Olympiad in Informatics 2024 Day 2 (CEOI 2024 Day 2) China National Olympiad in Informatics 2024 Day 1 China National Olympiad in Informatics 2024 Day 2 European Girls Olympiad in Informatics 2024 Day 1 European Girls Olympiad in Informatics 2024 Day 2 International Olympiad in Informatics 2024 Day 1 International Olympiad in Informatics 2024 Day 2 The 2024 ICPC World Finals Astana The 2024 ICPC Kunming Invitational Contest The 2024 Nordic Collegiate Programming Contest Croatian Open Competition in Informatics 2024/2025 Contest #1 CCPC 2024 Harbin Site The 2024 ICPC Asia Chengdu Regional Contest The 2024 ICPC Asia Nanjing Regional Contest Croatian Open Competition in Informatics 2024/2025 Contest #2 2024-2025 ICPC Latin American Regional Programming Contest 2024 Rocky Mountain Regional Contest 2024 North Central NA Regional Contest 2024 Mid-Central USA Programming Contest CCPC 2024 Chongqing Site The 2024 ICPC Greater NY Regional Contest The 2024 ICPC Asia Hangzhou Regional Contest Croatian OI USACO Platinum ICPC Regional Contests Croatian OI USACO Platinum USACO Platinum NOI (SG) Croatian OI ICPC Regional Championships/Finals ICPC Regional Championships/Finals British OI Baltic OI Baltic OI APIO ICPC Regional Championships/Finals Central European OI Central European OI NOI NOI European Girls OI European Girls OI IOI IOI ICPC World Finals ICPC Regional Contests NCPC Croatian OI CCPC ICPC Regional Contests ICPC Regional Contests Croatian OI ICPC Regional Championships/Finals ICPC Regional Contests ICPC Regional Contests ICPC Regional Contests CCPC ICPC Regional Contests ICPC Regional Contests Date 2024/1/13 2024/1/26 2024/1/28 2024/2/10 2024/2/16 2024/3/15 2024/3/16 2024/3/16 2024/3/17 2024/3/24 2024/4/6 2024/5/4 2024/5/5 2024/5/18 2024/5/27 2024/6/25 2024/6/27 2024/7/18 2024/7/20 2024/7/23 2024/7/25 2024/9/3 2024/9/5 2024/9/19 2024/9/28 2024/10/5 2024/10/5 2024/10/26 2024/10/27 2024/11/3 2024/11/9 2024/11/9 2024/11/9 2024/11/9 2024/11/9 2024/11/10 2024/11/10 2024/11/ 10 Competition Name CCPC 2024 Jinan Site The 2024 ICPC Pacific Northwest Regional Contest (Div. 1) The 2024 ICPC Pacific Northwest Regional Contest (Div. 2) ICPC NA South Division 2024 - Division 2 ICPC NA South Division 2024 - Division 1 The 2024 ICPC Southern California Regional Contest The 2024 ICPC Southeastern Europe Regional Contest (SEERC 2024) The 2024 ICPC Asia Shanghai Regional Contest The 2024 ICPC Asia Seoul Regional Contest The 2024 ICPC Northwestern Europe Regional Contest (NWERC 2024) The 2024 ICPC Asia Shenyang Regional Contest Romanian Master of Informatics 2024 Day 1 Romanian Master of Informatics 2024 Day 2 The 2024 ICPC Asia Kunming Regional Contest Croatian Open Competition in Informatics 2024/2025 Contest #3 USACO 2024 December Contest (Platinum) The 2024 ICPC Northern Eurasia Finals The 2024 ICPC Central Europe Regional Contest CCPC 2024 Zhengzhou Site The 2024 ICPC Asia Yokohama Regional Contest The 2024 ICPC Asia Hong Kong Regional Contest The 2024 ICPC Asia East Continent Final Contest USACO 2025 January Contest (Platinum) Croatian Open Competition in Informatics 2024/2025 Contest #4 The 24th Japanese Olympiad in Informatics Final Round (JOI 2024/2025) Croatian Open Competition in Informatics 2024/2025 Contest #5 USACO 2025 February Contest (Platinum) The 2025 ICPC Europe Championship 2025 ICPC Asia West Finals The 2025 ICPC Latin America Championship USACO 2025 US Open Contest (Platinum) Singapore National Olympiad in Informatics 2025 Final Contest The 2025 British Informatics Olympiad Final Baltic Olympiad in Informatics 2025 Day 1 Baltic Olympiad in Informatics 2025 Day 2 The 2025 ICPC China Zhejiang Province Programming Contest (22nd) CCPC Final 2024 Asia-Pacific Informatics Olympiad 2025 (APIO 2025) The 2025 ICPC Asia Wuhan Invitational Contest The 2025 ICPC North America Championship Category CCPC ICPC Regional Contests ICPC Regional Contests ICPC Regional Contests ICPC Regional Contests ICPC Regional Contests ICPC Regional Contests ICPC Regional Contests ICPC Regional Contests ICPC Regional Contests ICPC Regional Contests Romanian OI Romanian OI ICPC Regional Contests Croatian OI USACO Platinum ICPC Regional Championships/Finals ICPC Regional Contests CCPC ICPC Regional Contests ICPC Regional Contests ICPC Regional Championships/Finals USACO Platinum Croatian OI Japanese OI Croatian OI USACO Platinum ICPC Regional Championships/Finals ICPC Regional Championships/Finals ICPC Regional Championships/Finals USACO Platinum NOI (SG) British OI Baltic OI Baltic OI ICPC Regional Contests CCPC Final APIO ICPC Regional Contests ICPC Regional Championships/Finals Date 2024/11/16 2024/11/16 2024/11/16 2024/11/16 2024/11/16 2024/11/16 2024/11/17 2024/11/17 2024/11/23 2024/11/ 2024/11/24 2024/11/28 2024/11/29 2024/12/1 2024/12/12 2024/12/13 2024/12/15 2024/12/15 2024/12/21 2024/12/22 2024/12/22 2024/12/28 2025/1/24 2025/1/25 2025/2/2 2025/2/15 2025/2/21 2025/3/2 2025/3/7 2025/3/16 2025/3/21 2025/3/22 2025/4/12 2025/4/26 2025/4/27 2025/5/10 2025/5/11 2025/5/17 2025/5/17 2025/5/"
        },
        {
            "title": "Contributions",
            "content": "Research & Development Zihan Wang1,2, Jiaze Chen1, Zhicheng Liu"
        },
        {
            "title": "Management",
            "content": "Markus Mak1, Yidi Du"
        },
        {
            "title": "Operations",
            "content": "Geonsik Moon1, Luoqi Xu1, Aaron Tua"
        },
        {
            "title": "Expert Partner",
            "content": "Kunshuo Peng1, Jiayi Lu1, Mingfei Xia"
        },
        {
            "title": "Data Operations",
            "content": "Boqian Zou1, Chenyang Ran1, Guang Tian1, Shoutai Zhu1, Yeheng Duan1, Zhenghui Kang"
        },
        {
            "title": "Data Platform",
            "content": "Front-End Developer: Zhenxing Lin1, Shangshu Li1 Back-End Developer: Qiang Luo1, Qingshen Long1 Product Manager: Zhiyong Chen1, Yihan Xiao"
        },
        {
            "title": "Writing",
            "content": "Yurong Wu1, Daoguang Zan"
        },
        {
            "title": "Supervision",
            "content": "Yuyi Fu1, Mingxuan Wang1, Ming Ding"
        },
        {
            "title": "Affiliations",
            "content": "1ByteDance 2M-A-P"
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Siyao Liu, Jinxin Chi, Haojie Pan, Jingjing Xu, Ge Zhang, Wenhao Huang, Yonghui Wu, as well as other colleagues at ByteDance, and more importantly, the anonymized competitive programming expert team, for their support for the AetherCode project."
        },
        {
            "title": "Disclaimer",
            "content": "Your access to and use of this dataset are at your own risk. We do not guarantee the accuracy of this dataset. The dataset is provided as is and we make no warranty or representation to you with respect to it and we expressly disclaim, and hereby expressly waive, all warranties, express, implied, statutory or otherwise. This includes, without limitation, warranties of quality, performance, merchantability or fitness for particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. In no event will we be liable to you on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this public license or use of the licensed material. The disclaimer of warranties and limitation of liability provided above shall be interpreted in manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability."
        },
        {
            "title": "References",
            "content": "[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [2] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. [3] Google DeepMind. Gemini. https://deepmind.google/models/gemini/. [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [5] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with APPS. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/ 2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html. [6] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. [7] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. TACO: Topics in Algorithmic COde generation dataset, December 2023. URL http://arxiv.org/abs/2312.14852. arXiv:2312.14852 version: 3. [8] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode. Science, 378(6624):10921097, 2022. doi: 10.1126/science.abq1158. URL https://www.science.org/doi/abs/10.1126/science.abq1158. [9] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [10] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/ hash/43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html. [11] OpenAI. Gpt-4.1. https://openai.com/index/gpt-4-1/, . Accessed: 2025-04-14. [12] OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, . Accessed: 2024-05-13. [13] OpenAI. o4-mini-high. https://openai.com/index/introducing-o3-and-o4-mini/, . Accessed: 2025-04-16. [14] OpenAI, Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark 14 Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, and Wenda Zhou. Competitive programming with large reasoning models, 2025. URL https://arxiv.org/abs/2502.06807. [15] Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, et al. CodeElo: Benchmarking competition-level code generation of llms with human-comparable Elo ratings. arXiv preprint arXiv:2501.01257, 2025. [16] ByteDance Seed. Doubao-1.6-thinking. https://seed.bytedance.com/en/seed1_6. [17] Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. Can language models solve olympiad programming? arXiv preprint arXiv:2404.10952, 2024. [18] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. [19] Zihan Wang, Siyao Liu, Yang Sun, Hongyan Li, and Kai Shen. Codecontests+: High-quality test case generation for competitive programming, 2025. URL https://arxiv.org/abs/2506.05817. [20] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [21] Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Hansen He, Dongruixuan Li, Stanley Wei, Hangyi Hao, Jianzhu Yao, Peiyao Sheng, Zixuan Wang, Wenhao Chai, Aleksandra Korolova, Peter Henderson, Sanjeev Arora, Pramod Viswanath, Jingbo Shang, and Saining Xie. Livecodebench pro: How do olympiad medalists judge llms in competitive programming?, 2025. URL https://arxiv.org/abs/2506.11928."
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}