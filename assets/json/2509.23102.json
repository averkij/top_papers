{
    "paper_title": "Multiplayer Nash Preference Optimization",
    "authors": [
        "Fang Wu",
        "Xu Huang",
        "Weihao Xuan",
        "Zhiwei Zhang",
        "Yijia Xiao",
        "Guancheng Wan",
        "Xiaomin Li",
        "Bing Hu",
        "Peng Xia",
        "Jure Leskovec",
        "Yejin Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 2 0 1 3 2 . 9 0 5 2 : r Preprint, Under Review"
        },
        {
            "title": "MULTIPLAYER NASH PREFERENCE OPTIMIZATION",
            "content": "Fang Wu, Xu Huang, Weihao Xuan,, Zhiwei Zhang, Yijia Xiao Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi Stanford University, Georgia Institute of Technology, The University of Tokyo RIKEN AIP, Pennsylvania State University, University of California, Los Angeles, Harvard University, Independent Researcher, UNCChapel Hill fangwu@stanford.edu, xu.huang@gatech.edu, yejin@cs.stanford.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the BradleyTerry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an n-player game, where each policy competes against population of opponents while being regularized toward reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have achieved remarkable progress in instruction following and openended reasoning, primarily through reinforcement learning from human feedback (RLHF) (Christiano et al., 2017). In RLHF, annotators provide preference comparisons between model outputs, and these signals are used to align LLMs with human expectations. While reward-based RLHF pipelines built upon the Bradley-Terry (Bradley & Terry, 1952) model have enabled widely deployed systems (e.g., InstructGPT (Ouyang et al., 2022), Claude (Bai et al., 2022), and Gemini (Team et al., 2023)), they assume transitive preferences and scalar reward functions. Recent empirical studies in LLM alignment have revealed that human preferences often exhibit non-transitive patterns and heterogeneous structures that challenge these assumptions (Ethayarajh et al., 2024; Wu et al., 2024). This limitation has motivated recent attempts to develop game-theoretic formulations of alignment. Rather than relying on scalar rewards that enforce transitivity constraints, recent work treats preference optimization as finding Nash equilibria in games defined by general preference oracles (Munos et al., 2023). In this Nash learning from human feedback (NLHF) paradigm, well-aligned policy represents Equal contribution. Corresponding authors. 1 Preprint, Under Review an optimal strategy that cannot be exploited by competing policies. At equilibrium, policies achieve balanced win rate that reflects strategic optimality rather than mediocrity. Subsequent studies have explored this paradigm through various algorithmic lenses. For instance, Zhang et al. (2025b) leverages no-regret learning to approximate the Nash equilibrium via self-play. Zhang et al. (2025a) accelerates convergence using optimistic mirror descent. Zhou et al. (2025) establishes last-iterate convergence with extragradient updates. These approaches advance both theoretical guarantees and empirical stability compared to traditional RLHF. Despite advances, the existing NLHF remains constrained to two-player settings where single policy competes against one opponent. However, real-world preference alignment often involves diverse annotators, heterogeneous evaluation criteria, or mixtures of historical model checkpoints - contexts that are better modeled as multiplayer games (Freund & Schapire, 1999). Extending NLHF to the multiplayer regime raises new challenges: (i) how to formulate objectives that balance fairness and competitiveness among multiple policies, (ii) how to design algorithms that converge to multiplayer Nash equilibria, and (iii) how to ensure tractability when scaling to larger LLMs (Sokota et al., 2022). We address these challenges by proposing Multiplayer Nash Preference Optimization (MNPO), principled framework that generalizes two-player preference optimization to n-player games. In MNPO, each policy simultaneously competes against all other policies while being regularized toward reference policy, creating competitive equilibrium that balances performance against the population with adherence to trusted baseline. Our contributions are threefold: Theoretical Framework: We establish that MNPO admits natural equilibrium characterizations, including well-defined Nash policies and duality gaps that measure alignment quality. We prove that MNPO inherits the desirable convergence properties of existing two-player formulations while enabling richer equilibrium dynamics. Algorithmic Innovation: We introduce time-dependent MNPO (TD-MNPO), where opponent sets evolve adaptively using weighted combinations of historical policies. This approach enables models to dynamically incorporate past knowledge while maintaining training stability. Empirical Validation: Through comprehensive experiments on instruction-following and reasoning benchmarks, we demonstrate that MNPO consistently outperforms existing NLHF baselines, particularly excelling in scenarios involving diverse preferences and complex evaluation criteria. Our analysis reveals that MNPO provides unifying perspective on preference optimization, subsuming many existing methods as special cases while offering improved robustness in multi-agent alignment scenarios. By bridging recent advances in NLHF with the practical demands of aligning LLMs to diverse and potentially non-transitive human preferences, MNPO establishes scalable foundation for next-generation alignment techniques."
        },
        {
            "title": "2 RLHF PRELIMINARIES",
            "content": "Notation. denotes prompt and is the prompt space. is assumed to be sampled from fixed but unknown distribution d0. An LLM is characterized by policy π : (Y) that takes prompt as input and outputs distribution over the response space Y. The response is then sampled from the distribution π( x). Bradley-Terry Model Assumption. The prevalent RLHF framework (Christiano et al., 2017; Ouyang et al., 2022) assumes the Bradley-Terry model. It assumes that there exists reward function exp(r(x,y1)) such that for any and y1, y2 Y, we have (cid:0)y1 y2 x(cid:1) = exp(r(x,y1))+exp(r(x,y2)) = σ (cid:0)r (cid:0)x, y1(cid:1) (cid:0)x, y2(cid:1)(cid:1) . After learning reward function R(, ), RLHF algorithms aim to maximize the following KLregularized objective with preference optimization RL algorithms such as PPO (Schulman et al., 2017): J(π) = Exd0 (cid:2)Eyπ(x)[R(x, y)] τ KL (π( x)πref( x))(cid:3) . (1) Here πref is the reference policy, which is usually supervised fine-tuned LLM, and τ > 0 is the regularization parameter. By maximizing the objective, the obtained policy simultaneously achieves 2 Preprint, Under Review high reward and stays close to πref, which can mitigate reward hacking (Tien et al., 2022; Skalse et al., 2022) to some extent. General Preference Oracle. The aforementioned algorithms rely on the Bradley-Terry model assumption, which may not always hold in practice. Recent studies (Munos et al., 2023; Calandriello et al., 2024; Ye et al., 2024; Zhang et al., 2025b;a) have instead directly considered the general preference distribution without imposing additional assumptions, framing the preference optimization problem as two-player game. These works assume the existence of preference oracle : [0, 1]. It can be queried to obtain binary preference signals as Ber (cid:0)P (cid:0)y1 y2 x(cid:1)(cid:1), where = 1 indicates that y1 is preferred to y2, and = 0 indicates the opposite. The preference distribution is introduced as λP (x, y, y) = (y, y) [U < (y x)] + (y, y) [U (y x)], where Uniform(0, 1) is random variable. Given two policies π1 and π2, LLMs are aligned using this general preference oracle. Consequently, the game objective is written as: (π1, π2) = ExD[Ey1π1,y2π2 [P (y1 y2 x)] τ KL (π1πref) + τ KL (π2πref )] (2) where the max-player π1 aims to maximize the objective, and the min-player π2 aims to minimize the objective. The goal of both players is to maximize their winning rates against the opponent while not deviating too far from πref, which shares similar spirit with the objective of Eq. 1. Nash Policy and Duality Gap. Without loss of generality, we restrict our attention to the policy class Π containing the policies with the same support set as πref. The Nash equilibrium of the game in Eq. 2 is then defined as: 1, π π 2 := argmax π1Π argmin π2Π (π1, π2) . (3) Due to the symmetry of the two players, their Nash policies are unique and coincide, meaning π 2 = π (Ye et al., 2024). Interestingly, (π, π) 0.5 always holds for π Π, as 1 = π (π, π) = 0.5 indicates that π is the best response against itself. To quantify how well policy π approximates the Nash policy π, we define the duality gap as: (π1, π) min π2 DualGap(π) := max (π, π2) . (4) π1 The duality gap is nonnegative and DualGap(π) = 0 if and only if π = π. Hence, our goal is to find policy that minimizes the duality gap. Once we achieve DualGap(π) ϵ, we say that π is an ϵ-approximate Nash policy."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 RLHF AS MULTIPLAYER GAMES To extend the two-player preference optimization objective to multiplayer setting {π1, π2, ..., πn}, we consider framework where each policy seeks to maximize its average preference probability against all other policies while regularizing toward reference policy. We present the formulation of multiplayer games under two distinct assumptions: (i) the preference ranking assumption and (ii) the general preference oracle. Plackett-Luce Reward Learning Assumption. To generalize the maximum-likelihood reward learning objective for the Bradley-Terry model to one-vs-many comparisons, we adopt the PlackettLuce framework. Specifically, we replace the pairwise logistic term log σ (cid:0)R (cid:0)x, y1(cid:1) (cid:0)x, y2(cid:1)(cid:1) with softmax over multiple alternatives, extending the Bradley-Terry model to accommodate listwise comparisons. This Plackett-Luce model (Debreu, 1960; Plackett, 1975) maintains the interpretability of reward-based preferences while scaling to more complex decision-making scenarios. Formally, given learned reward function r(x, y) and dataset containing tuples (cid:0)x, (cid:8)y1, y2, . . . , yk(cid:9)(cid:1), where (cid:8)y1, y2, . . . , yk(cid:9) are pool of to-be-ranked items, the probability that yi is preferred over the remaining pool of entities (cid:8)yj(cid:9) (cid:18) yi (cid:110) yj(cid:111) j=i j=i under the Plackett-Luce model is: (cid:12) (cid:12) (cid:12) exp(R (cid:0)x, yi(cid:1)) exp(R (x, yi)) + (cid:80) = (cid:19) j=i exp(R (x, yj)) . (5) Preprint, Under Review The corresponding negative log-likelihood for single comparison is: log (cid:18) yi (cid:110) yj(cid:111) (cid:19) (cid:12) (cid:12) (cid:12) j=i = log exp (cid:16) x, yi(cid:17)(cid:17) (cid:16) + (cid:88) (cid:16) x, yj(cid:17)(cid:17) (cid:16) exp j=i (cid:16) x, yi(cid:17) . (6) The generalized reward learning objective becomes: ˆR arg max RR (x,{y1:k})D yi{y1:k} (cid:124) (cid:16) x, yi(cid:17) log exp (cid:16) x, yi(cid:17)(cid:17) (cid:16) + (cid:88) (cid:16) x, yj(cid:17)(cid:17) (cid:16) exp j=i (cid:123)(cid:122) Per-comparison log-likelihood . (cid:125) (7) Several key observations arise from this formulation. First, when = 2 for one-vs-one comparison, Eq. 7 reduces to the vanilla Bradley-Terry objective, given by log σ (R (x, y) (x, y)), since σ(a) = ea 1+ea . Second, this objective maximizes the gap between the reward of and the log-sum-exp (LSE) of all alternatives, effectively applying soft maximum over competitors. This penalizes cases where yi fails to dominate the collective \"strength\" of the dispreferred items (cid:8)yj(cid:9) j=i. Multiplayer General Preference Optimization. In addition to the Plackett-Luce assumption, we consider n-player preference optimization and leverage universal preference oracle : {Y}n1 [0, 1]. It directly compares yi with group of responses {yj}j=i, leading to the binary (cid:17)(cid:17) . Subsequently, each policy πi competes against preference signals Ber other 1 players, and the objective function becomes: yj(cid:111) (cid:110) τ KL (πi( x)πref ( x)) yi (cid:8)yj(cid:9) j=i = ExD yi (cid:16) πi, {πj}j=i (cid:19)(cid:21) (cid:18) (cid:16) (cid:16) (cid:17) (cid:20) (cid:20) (cid:21) . yiπi,{yj yj πj}j=i (cid:12) (cid:12) (cid:12) j=i (8) Here, each policy πi seeks to maximize its expected preference over all other policies {πj}j=i. Meanwhile, the regularization term is kept to penalize πi with KL divergence from the reference policy πref weighted by τ . This ensures that policies remain anchored to πref to prevent overoptimization and maintain behavioral consistency. Regarding the dynamics of this multiplayer game, all policies {πi}n i=1 are updated concurrently. Each policy πi optimizes its own J, leading to competitive equilibrium where policies balance performance against the population and adherence to πref . Eq.8 has several key properties. (i) The symmetric treatment. All policies are treated equally, competing in symmetric fashion. Therefore, π n. (ii) Decentralized optimization. Each policys update depends only on its own actions and aggregate opponent behavior, avoiding complex interdependencies. (iii) Generalization of the two-player case. When = 2, each policys objective reduces to maximizing its pairwise preference probability minus its own KL penalty as in Eq. 2, aligning conceptually with competitive regularization. 2 = ... = π 1 = π Nash Equilibrium and Duality Gap. where no player can improve their objective by unilaterally deviating. Formally, for all πi Π : In this n-player game, the Nash equilibrium is policy π (cid:0)π , {π }j=i (cid:1) (cid:0)πi, {π }j=i (cid:1) , {1, . . . , n}. (9) Due to symmetry, and assuming the KL terms vanish, the average win rate of the equilibrium policy π against 1 copies of itself is 1 , analogous to the two-player case. The duality gap quantifies how far given policy π is from the Nash policy π. Extending the two-player definition to an n-player setting, we define the set of opponent players as Oπ = {πj}n1 j=1 . The duality gap in multiplayer games is then given by: . That is, (π, {π}j=i) = 1 DualGap(π) := Eπj Oπ (cid:20) max πj min Oπ πj (πj, π Oπ πj) min Oπ (π, Oπ) . (10) (cid:21) This gap quantifies the maximum advantage player could gain by unilaterally deviating from π against the worst-case configuration of opponents, relative to the minimum payoff π could incur if the opponents themselves deviate. Notably, the equilibrium condition is satisfied if and only if DualGap (π) = 0, ensuring no player has an incentive to deviate unilaterally. Furthermore, if DualGap(π) ϵ, then π is defined as an ϵ-approximate Nash policy in the multiplayer setting. 4 Preprint, Under Review Multiplayer Nash Preference Optimization. There are well-known algorithms that approximately solve the Nash equilibrium in constant-sum multiplayer game. In this work, we follow (Freund & Schapire, 1999) to establish an iterative framework that can asymptotically converge to the optimal policy on average. Given learning rate η of online mirror descent update, we start with theoretical analysis that conceptually solves the multiplayer game as follows 1: π(t+1) (y x) 1 n1 π(t) (y x) exp (cid:89) j=i η 1 (cid:88) j=i (cid:16) π(t) (cid:17) (11) Eq. 11 is an iterative framework that relies on the multiplicative weight update and enjoys clear structure. Initially, we have base policy π(0), usually from some supervised fine-tuned model πref. In each iteration t, the updated policy π(t+1) is obtained from the reference policy π(t) following the multiplicative weight update. Particularly, response should have higher probability weight if it has higher average advantage over the current policy π(t). Equivalently, Eq. 11 can be written as (cid:16) π(t+1) π(t) /Zπ(t)(x), where Zπ(t)(x) is the normalization factor (a.k.a, the partition function). Then for any fixed and y, each ideal update policy π(t+1) (y x) = Πj=iπ(t) should satisfy: (y x) (cid:16) η n1 n1 exp (cid:17)(cid:17) 1 1 1 (cid:88) j=i log π(t+1) (y x) π(t) (y x) = η 1 (cid:88) j=i (cid:16) π(t) (cid:17) log Zπ(t) . (12) Note that direct computation of π(t+1) involves normalization factor, which is intractable for the exponentially large response space Y. To avoid computing this normalization factor, we consider the logarithmic ratio between response pair and y, and define the function ht (π, y, y) as: (cid:0)π, y, y(cid:1) = log ht π(y x) π(y x) 1 1 (cid:32) log π(t) (y x) π(t) (y x) (cid:33) . (cid:88) j=i (13) From Eq.12, we know that the following equality holds for any response pair y, Supp (πref): (cid:16) (cid:16) (cid:17) (cid:17) π(t) y π(t) (14) π(t+1), y, y(cid:17) (cid:16) = ht η 1 (cid:88) j=i Based on this observation, we define the loss function Lt(π) and update the policy π(t+1) as: 2 π(t+1) argmin π Eyw ,ylDt ht (π, yw, yl) (cid:124) η 1 j=i (cid:123)(cid:122) Lt(π) (cid:88) (cid:16) π(t) (cid:17) (cid:16) π(t) (cid:17) (15) (cid:125) It is clear to see that π(t+1) is the minimizer of Lt(π) since Lt following lemma, we show that π(t+1) is the unique minimizer of Lt within the policy class Π. Lemma 1. For each [T ], πt+1 in Eq. 15 is the unique minimizer of Lt(π) within Π. (cid:0)π(t+1)(cid:1) = 0. Furthermore, in the The proof is deferred to Appendix E.1. Moreover, we replace the tricky term hyperparameter η and propose the following loss to bypass it: (cid:16) π(t) (cid:17) with t(π) = Ey,yπt, yw,ylλP(y,y) (cid:34)(cid:18) ht (π, yw, yl) (cid:19)2(cid:35) . 1 2η (16) Proposition 1. For any policy π Π and any iteration [T ], differing only by an additive constant that is independent of π. t(π) is equivalent to Lt(π), See the proof in Appendix E.2. Here, the response pair (y, y) is directly sampled from the current policy π(t), which is crucial for the equivalence between t(π) and Lt(π). 1For notational conciseness and avoiding confusion, we use superscript to denote time here, so that π(t) is equivalent to πt. 5 Preprint, Under Review Table 1: Time-dependent MNPO recovers many existing offline or online preference optimization algorithms. We denote the target reward gap as δr := η (r (x, y1) (x, y2)) . Dsq and Dbwd represent the squared distance and backward Bernoulli KL divergence, respectively. Algorithm Num. Players Opponents Importance Weights SimPO DPO Distill-DPO DNO SPIN SPPO IPO INPO = 1 = 2 = 2 = 2 = 2 = 2 = 2 = πref πref πt πt πt πref πt, πref λj = λj = 1 λj = 1 λj = 1 λj = β λj = 1 λj = 1 (cid:40) τ η , ητ η , if = if = 1 Dist. Dbwd Dbwd Dsq Dbwd Dbwd Dsq Dsq Dsq Target Reward Gap (cid:98)P (y πt x) 1 2 1 2τ (cid:17) (cid:16) η 1 2τ"
        },
        {
            "title": "3.2 MULTIPLAYER NASH PREFERENCE OPTIMIZATION",
            "content": "Reward-Enhanced MNPO. While our framework is motivated by moving beyond the limitations of purely scalar reward-based approaches, this does not preclude the beneficial incorporation of reward information when available. The key distinction lies in how rewards are utilized: rather than relying solely on reward-based rankings with implicit transitivity assumptions (as in classical RLHF), MNPO can leverage reward information as auxiliary guidance while maintaining the flexibility to handle non-transitive preferences through its game-theoretic structure. Reward-aware preference optimization (RPO) (Sun et al., 2025) demonstrates how quantitative reward signals can complement qualitative preference comparisons. This approach aligns learned implicit preference models with explicit reward models that provide graded assessments of response quality. This shift allows MNPO to move beyond binary preference margins and instead minimize discrepancies between the learned reward function rπθ (x, y) and the target explicit reward model r(x, y). Concretely, instead of optimizing only the preference order between two responses, RPO defines the loss over preference pairs as: RPO (cid:0)πθ, (cid:0)x, y1, y2(cid:1) r, πref, β, η(cid:1) =: (cid:2)rπθ (17) where (cid:0)x, y1, y2(cid:1) represents preference pair and the distance metric : measures alignment between the models implicit reward differences and the scaled reference reward differences. The hyperparameters η and β control the reward scale and regularization, respectively. This formulation directly encourages the policy πθ to internalize human-aligned reward values, bridging qualitative preference optimization with quantitative reward modeling. (cid:0)x, y2(cid:1) ηr (cid:0)x, y1(cid:1) ηr (cid:0)x, y2(cid:1)(cid:3) , (cid:0)x, y1(cid:1) rπθ Importantly, the loss function Eq. 16 can be interpreted as special case of RPO under squared distance metric Dsq . Specifically, and reference reward model gap δr := η (r (x, y1) (x, y2)) = 1 2η . This connection highlights how integrating reward-awareness into multiplayer preference games enhances stability, interpretability, and alignment fidelity. t(π) employs learned reward model rπθ = Eπj log π(yx) πj (yx) (cid:105) (cid:104) π(t) Time-dependent Multiplayers central challenge in multiplayer optimization lies in defining (cid:111)n1 (cid:110) and updating the set of opponent players in Eq.14. Inspired by recent iterative preference j=1 optimization methods such as DNO (Rosset et al., 2024), SPIN (Chen et al., 2024), and INPO (Zhang et al., 2025b), which typically rely on past policy iterations (e.g., πref and π(t1)) to construct opponents, we adopt time-dependent opponent selection mechanism. At any iteration t, we construct the opponent set from mixture of recent historical policies {πtj}t (n + 1), weighted by coefficients λj with λj [0, 1] and optionally (cid:80) time-dependent MNPO (TD-MNPO) loss is formulated as follows: j=0 λj 1. The resulting Lt,D TD-MNPO(πβ, {λj}, η) = log π(yw x) π(yl x) n2 (cid:88) j=0 λj log πtj(yw x) πtj(yl x) (cid:13) (cid:13) (cid:13) (cid:13) ηδ , (18) 6 Preprint, Under Review where δ encodes the target reward gap. By blending multiple past policies, this formulation stabilizes training, mitigates overfitting to transient fluctuations, and preserves temporal consistency. Connections to Existing RLHF. This unified MNPO formulation in Eq. 18 reveals that many preference optimization algorithms can be recovered as special cases by varying the number of players n, choice of opponents Oπ, distance metric D, and target reward gap δ. For instance, DPO emerges by setting = 2, Oπ = πref, and λj = 1. Table 1 summarizes these reductions, showing how time-dependent MNPO unifies offline and online preference optimization under one principled framework. We provide broader overview of RLHF objectives in Appendix D. Compared to static reference-based approaches, the reward-aware multiplayer formulation offers: (i) Smoother policy evolution. Unlike methods that rely solely on the most recent past policy, MNPO gradually incorporates multiple past policies, preventing abrupt shifts and stabilizing policy updates. (ii) Greater robustness. By weighted mixture of historical opponents, MNPO mitigates the risk of overfitting to transient fluctuations in recent iterations. (iii) Unified interpretation. TDMNPO seamlessly extends existing approaches into unified formulation, allowing for flexible adaptation to different training scenarios. (iv) Stable convergence.. The weighting scheme ensures that recent policies exert stronger influence while preserving the broader learning trajectory, leading to more stable convergence. By dynamically leveraging historical policies as opponent players, TD-MNPO enhances preference optimization, making it more adaptable and robust in evolving learning environments. The pseudo-algorithm is described in Appendix B."
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "Models and Training Settings. We implement an online RLHF framework (Dong et al., 2024) with Gemma-2-9B-it (Team et al., 2024) as the base model. Our MNPO training consists of = 3 iterations, where each iteration generates responses from the current policy on fresh prompt set and updates the policy using preference feedback. To eliminate the need for costly human annotations, we employ the reward model ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024a) to provide preference signals. Hyperparameter optimization proves critical, as optimal configurations vary both across different base models and between iterations of the same model. Through empirical analysis, we find that maintaining β within the range [0.01, 10] consistently produces strong results. Furthermore, we observe that gradually increasing β throughout training effectively mitigates training degradation while enabling continued model improvement. Complete implementation details and hyperparameter specifications are provided in Appendix C. Evaluation Benchmarks. We evaluate primarily on three widely used open-ended instructionfollowing benchmarks: MT-Bench (Zheng et al., 2023), AlpacaEval 2 (Li et al., 2023), and ArenaHard v0.1 (Li et al., 2024). As suggested by Dubois et al. (2024), we report the win rate (WR) for Arena-Hard and the length-controlled (LC) WR for AlpacaEval 2, as judged by GPT-5-mini rather than the outdated GPT-4 Turbo (Preview-1106). Since RLHF alignment is known to sometimes degrade reasoning, calibration, and factual accuracy (Ouyang et al., 2022; Dong et al., 2024), we further assess performance on broader set of eleven academic benchmarks. These benchmarks span multiple abilities, including explicit instruction following (Zhou et al., 2023), general knowledge (Clark et al., 2018; Rein et al., 2024; Hendrycks et al., 2020), commonsense reasoning (Sakaguchi et al., 2021; Lin et al., 2021; Zellers et al., 2019), and math/coding problem solving (Lewkowycz et al., 2022; Chen et al., 2021). compare MNPO with group of open-source LLMs, including In addition, we LLaMA-3.1-8B-it (Dubey et al., 2024), Tulu-2-DPO-70B, LLaMA-3.3-70B-it, Mixtral-8x22B-it, and Qwen3-235B-it (Yang et al., 2025), and closed-source LLMs such as Gemini-2.5-Pro, GPT-5, and Claude-Sonnet-4."
        },
        {
            "title": "5 EMPIRICAL RESULTS",
            "content": "Instruction-Following and Preference Alignment. Table 2 presents the performance of MNPO compared to existing preference optimization methods on three widely-used instruction-following 7 Preprint, Under Review Table 2: Performance of various models on instruction-following and preference-alignment benchmarks (AlpacaEval 2.0, Arena-Hard, and MT-Bench), evaluated using GPT-5-mini as the judge. Size AlpacaEval 2.0 Arena-Hard MT-Bench Model SFT Model DPO SimPO SPPO INPO MNPO LLaMA-3.1-8B-it Tulu-2-DPO-70B LLaMA-3.3-70B-it Mixtral-8x22B-it Qwen3-235B-it OpenAI/GPT-5 Anthropic/Claude-Sonnet-4 Google/Gemini-2.5-Pro - - - 9B 9B 9B 9B 9B 9B 8B 70B 70B 141B 235B 50.15 54.35 55.16 55.97 56.09 57.27 5.24 8.82 29.44 9.57 84. 72.80 62.24 90.93 44.97 45.63 45.04 43.89 48.03 52.26 39.16 27.88 59.21 40.98 88.71 41.42 77.58 86.98 6.49 6.87 6.87 6.86 6. 7.03 5.37 5.91 7.73 6.90 8.27 8.46 8.26 7.78 Table 3: Model performance on instruction, knowledge, and commonsense benchmarks. Model SFT Model DPO SimPO SPPO INPO MNPO Instruction Knowledge Commonsense IFEval GPQA MMLU ARC HellaSwag TruthfulQA Winogrande 72.27 72.96 73.79 75.47 73.20 73.94 28.28 29.29 32.32 26.26 27.78 33.33 75.35 75.77 76.79 75.37 74.79 75. 91.29 91.26 91.09 91.17 91.07 91.15 80.30 80.37 78.90 80.10 80.22 80.18 70.75 71.24 63.40 71.48 71.24 70. 73.72 73.88 72.93 73.48 73.48 73.09 AVG 70.28 70.68 69.60 70.19 70.25 71.08 benchmarks: AlpacaEval 2.0 (Length-Controlled Win Rate, %), Arena-Hard (Win Rate, %), and MT-Bench (Score/10). All models were evaluated using GPT-5-mini as the judge. MNPO consistently outperforms all baseline methods across all three benchmarks. On AlpacaEval 2.0, MNPO achieves score of 57.27, representing improvements of 2.92 points over DPO (54.35), 2.11 points over SimPO (55.16), 1.30 points over SPPO (55.97), and 1.18 points over INPO (56.09). The improvements are even more pronounced on Arena-Hard, where MNPO scores 52.26 compared to the next best method INPO at 48.03, representing 4.23-point improvement. Notably, on this challenging benchmark, MNPO not only excels over other preference optimization algorithms but also demonstrates strong competitiveness against much larger open-source fine-tuned models and even lastest closed-source models. It surpasses prominent models like Tulu2-DPO (70B) and Mixtral-it (141B), and, surprisingly, even outperforms GPT-5. On MT-Bench, MNPO achieves 7.03, outperforming all baselines, with the closest competitor being INPO at 6.95. These results demonstrate that the multiplayer formulation in MNPO provides significant advantages for instruction-following tasks. The consistent improvements across all three benchmarks suggest that the frameworks ability to handle diverse preferences and non-transitive relationships leads to better alignment with human expectations in open-ended generation tasks. Knowledge and Reasoning Capabilities. Table 3 evaluates model performance on academic benchmarks covering instruction following, knowledge, and commonsense reasoning. The results show that MNPO maintains strong performance across diverse cognitive tasks while achieving preference alignment. MNPO achieves the highest average score of 71.08 across all benchmarks, outperforming the SFT baseline (70.28) and all other preference optimization methods. Notably, MNPO achieves the best performance on GPQA (33.33), demonstrating strong graduate-level reasoning capabilities. The method also performs competitively on instruction-following (IFEval: 73.94) and maintains solid performance on knowledge benchmarks like MMLU (75.63) and commonsense reasoning tasks. Importantly, unlike some preference optimization methods that show degradation on certain 8 Preprint, Under Review Table 4: Model performance on math and coding benchmarks. Model SFT Model DPO SimPO SPPO INPO MNPO Math Code GSM8K Minerva-Math AIME-24 HumanEval 81.96 82.03 82.56 82.11 82.94 82. 44.12 45.96 43.38 47.43 46.32 44.85 0 0 0 0 0 3.33 60.37 59.76 57.32 59.76 59.15 61. AVG 46.61 46.94 45.82 47.33 47.10 48.10 academic benchmarks (e.g., SimPOs drop to 63.40 on TruthfulQA), MNPO maintains relatively stable performance across all domains. This suggests that the multiplayer framework helps preserve the models foundational capabilities while improving preference alignment. Mathematical and Coding Performance. Table 4 presents results on mathematical reasoning and coding benchmarks. MNPO achieves the highest average score of 48.10 across math and coding tasks, outperforming all baseline methods, including SPPO (47.33) and INPO (47.10). On the challenging AIME-24 benchmark, MNPO is the only method to achieve non-zero performance (3.33), while all other methods including the SFT baseline score 0. This demonstrates MNPOs superior capability in handling complex mathematical reasoning tasks. On HumanEval, MNPO achieves 61.59, representing the best coding performance among all methods. The results on GSM8K and Minerva-Math show that MNPO maintains competitive performance with existing methods while achieving superior results on the most challenging tasks. This pattern suggests that the multiplayer optimization framework is particularly beneficial for complex reasoning tasks that require handling multiple solution strategies or approaches."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Reward-modelbased RLHF. Classical RLHF trains reward model on human preference data and optimizes policy with KL-regularized policy gradients (e.g., PPO) (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022; Schulman et al., 2017). While effective, PPO-style updates suffer from instability and high memory cost. To address this, GRPO removes the critic for more stable and memory-efficient training at scale (e.g., DeepSeek-R1) (Shao et al., 2024; Guo et al., 2025), and DAPO further improves sample efficiency through dynamic sampling and refined loss objectives (Yu et al., 2025). VAPO shows that value learning can strengthen RLHF when done properly (Yuan et al., 2025). Nonetheless, optimizing against imperfect reward proxies remains vulnerable to reward hacking (Weng, 2024; Wen et al., 2024). RLHF with General Preference. DPO bypasses reward modeling by directly optimizing logodds margin between preferred and dispreferred responses (Rafailov et al., 2023). This idea has inspired family of extensions, including IPO (Azar et al., 2024), KTO (Ethayarajh et al., 2024), SimPO (Meng et al., 2024), and WPO (Zhou et al., 2024), which refine the constraint, scaling, or sampling strategy. Iterative and online forms introduce exploration and continual updates (Xiong et al., 2023; Dong et al., 2024; Xie et al., 2024), but most remain limited to static pairwise supervision."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduce Multiplayer Nash Preference Optimization, extending Nash learning from human feedback to multiplayer settings. Our framework admits well-defined Nash equilibria and unifies existing preference optimization methods as special cases. Empirically, MNPO outperforms baselines across instruction-following and reasoning benchmarks. These results validate that multiplayer formulations better capture heterogeneous human preferences and provide more robust alignment for LLMs. 9 Preprint, Under Review"
        },
        {
            "title": "REFERENCES",
            "content": "Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from In International Conference on Artificial Intelligence and Statistics, pp. human preferences. 44474455. PMLR, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, et al. Human alignment of large language models through online preference optimisation. arXiv preprint arXiv:2403.08635, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. Gerard Debreu. Individual choice behavior: theoretical analysis, 1960. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, and Jonathan Berant. Robust preference optimization through reward model distillation. arXiv preprint arXiv:2405.19316, 2024. Yoav Freund and Robert Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29(1-2):79103, 1999. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 10 Preprint, Under Review Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline. Blog post.[Accessed 07-02-2025], 2024. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Xufei Lv, Haoyuan Sun, Xuefeng Bai, Min Zhang, Houde Liu, and Kehai Chen. The hidden link between rlhf and contrastive learning. arXiv preprint arXiv:2506.22578, 2025. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with referencefree reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 18, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Junshu Pan, Wei Shen, Shulin Huang, Qiji Zhou, and Yue Zhang. Pre-dpo: Improving data utilization in direct preference optimization using guiding reference model. arXiv preprint arXiv:2504.15843, 2025. Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. arXiv preprint arXiv:2403.19159, 2024. Robin Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series C: Applied Statistics, 24(2):193202, 1975. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715, 2024. Preprint, Under Review Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. Samuel Sokota, Ryan DOrazio, Zico Kolter, Nicolas Loizou, Marc Lanctot, Ioannis Mitliagkas, Noam Brown, and Christian Kroer. unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games. arXiv preprint arXiv:2206.05825, 2022. Shengyang Sun, Yian Zhang, Alexander Bukharin, David Mosallanezhad, Jiaqi Zeng, Soumye Singhal, Gerald Shen, Adithya Renduchintala, Tugrul Konuk, Yi Dong, et al. Reward-aware preference optimization: unified mathematical framework for model alignment. arXiv preprint arXiv:2502.00203, 2025. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. ModelScope Team. EvalScope: Evaluation framework for large models, 2024. URL https: //github.com/modelscope/evalscope. Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca Dragan, and Daniel Brown. Causal confusion and reward misidentification in preference-based reward learning. arXiv preprint arXiv:2204.06601, 2022. Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints. arXiv preprint arXiv:2309.16240, 2023. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In EMNLP, 2024a. Mingzhi Wang, Chengdong Ma, Qizhi Chen, Linjian Meng, Yang Han, Jiancong Xiao, Zhaowei Zhang, Jing Huo, Weijie Su, and Yaodong Yang. Magnetic preference optimization: Achieving last-iterate convergence for language model alignment. arXiv preprint arXiv:2410.16714, 2024b. Jiaxin Wen, Ruiqi Zhong, Akbir Khan, Ethan Perez, Jacob Steinhardt, Minlie Huang, Samuel Bowman, He He, and Shi Feng. Language models learn to mislead humans via rlhf. arXiv preprint arXiv:2409.12822, 2024. Lilian Weng. Reward hacking in reinforcement learning. lilianweng.github.io, Nov 2024. URL https://lilianweng.github.io/posts/2024-11-28-reward-hacking/. Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024. Tengyang Xie, Dylan Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, and Alexander Rakhlin. Exploratory preference optimization: Harnessing implicit q*-approximation for sample-efficient rlhf. arXiv preprint arXiv:2405.21046, 2024. 12 Preprint, Under Review Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. arXiv preprint arXiv:2312.11456, 2023. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. arXiv preprint arXiv:2401.08417, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Chenlu Ye, Wei Xiong, Yuheng Zhang, Hanze Dong, Nan Jiang, and Tong Zhang. Online iterative reinforcement learning from human feedback with general preference model. Advances in Neural Information Processing Systems, 37:8177381807, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback. Advances in Neural Information Processing Systems, 36:1093510950, 2023. Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Yuheng Zhang, Dian Yu, Tao Ge, Linfeng Song, Zhichen Zeng, Haitao Mi, Nan Jiang, and Dong Yu. Improving llm general preference alignment via optimistic online mirror descent. arXiv preprint arXiv:2502.16852, 2025a. Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, and Dong Yu. Iterative nash policy optimization: Aligning LLMs with general preferences via no-regret learning. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=Pujt3ADZgI. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Runlong Zhou, Maryam Fazel, and Simon Du. Extragradient preference optimization (egpo): arXiv preprint Beyond last-iterate convergence for nash learning from human feedback. arXiv:2503.08942, 2025. Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, and Chenguang Zhu. Wpo: Enhancing rlhf with weighted preference optimization. arXiv preprint arXiv:2406.11827, 2024. Preprint, Under Review ADDITIONAL RELATED WORK ON GAME-THEORETIC RLHF Another perspective casts preference optimization as game between the model and its opponents, where equilibrium-seeking methods provide stronger last-iterate guarantees. Self-play methods like SPIN (Chen et al., 2024), SPPO (Wu et al., 2024), and INPO (Zhang et al., 2025b) use noregret dynamics, while Pre-DPO (Pan et al., 2025) and MPO (Wang et al., 2024b) adapt mirror descent. More recent methods, such as ONPO (Zhang et al., 2025a) and EGPO (Zhou et al., 2025), introduce optimism and extragradient techniques for stable convergence under noisy preferences. These advances primarily focus on two-player games but highlight the importance of equilibrium views in preference optimization. Extending RLHF to multiplayer interactions, as pursued by MNPO, generalizes beyond pairwise dynamics and opens the door to richer equilibrium structures for alignment. PSEUDO-ALGORITHM OF MNPO Algorithm 1 Multiplayer Nash Preference Optimization (MNPO) 1: Require: Number of iterations , distance metric D, weight coefficients {λj}, reward scaling parameter η, regularization parameter β, external policy {πj}, reference policy πref, policy class Π, preference oracle P. 2: for iteration = 1, 2, . . . , do 3: Use current policy πt to generate response pairs (cid:8)y1 (cid:9)m i=1 where y1 Query the preference oracle to get the preference dataset Dt = (cid:8)y+ Calculate πt+1 as: πt+1 argmin , y2 Lt,D TD-MNPO , y2 , πt. (cid:9)m i=1. 4: 5: πΠ 6: end for 7: Output πT +"
        },
        {
            "title": "C EXPERIMENTAL DETAILS",
            "content": "Hardware and Implementation All experiments are conducted on 8 NVIDIA H100 GPUs with 96GB memory. For the implemented algorithms, DPO (Rafailov et al., 2023) is trained using the official Hugging Face DPO Trainer, while SimPO (Meng et al., 2024)2 and SPPO (Wu et al., 2024)3 follow their official GitHub implementations. INPO (Zhang et al., 2025b) is reproduced according to the settings described in the paper. Hyperparameters For MNPO, we adopt hyperparameters consistent with SimPO and INPO, using cosine learning-rate scheduler with peak learning rate of 5 107, warmup ratio of 0.1, and global batch size of 128. The optimizer is AdamW (Loshchilov & Hutter, 2017) without weight decay. We further perform grid search for history weights at timesteps 1 and 2, selecting from {0, 0.1, 0.333, 0.5, 0.667, 0.9}. In addition, we perform grid search for η over {0.1, 0.01, 0.0075, 0.005, 0.002}, and set η = 0.0075. Training Data For training data, we use Gemma2-Ultrafeedback-Armorm (Cui et al., 2023)4, which contains approximately 60K training samples and 2K test samples. We retain both prompts and responses in iteration 1, while only prompts are used in iterations 2 and 3. Evaluation Framework For evaluation, we adopt the EvalScope framework (Team, 2024)5 (version 1.0.2) across all datasets in Tables 3 and 4, and also applied it for AlpacaEval 2.0 and Arena-Hard in Table 2. MT-Bench6 is tested and implemented following its official GitHub repository. The LLM 2https://github.com/princeton-nlp/SimPO 3https://github.com/uclaml/SPPO 4https://huggingface.co/datasets/princeton-nlp/gemma2-ultrafeedback-armorm 5https://github.com/modelscope/evalscope 6https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge Preprint, Under Review judge is configured with gpt5-mini-aug7-2025, where the reasoning effort is set to minimal, and all other parameters follow the default EvalScope settings."
        },
        {
            "title": "D FORMULATIONS OF PREFERENCE OPTIMIZATION OBJECTIVES",
            "content": "Table 5 provides consolidated overview of various preference optimization objectives that have been proposed in the literature. Each method is presented in terms of its optimization objective given preference data = (x, y+, y), where denotes the input prompt, and y+ and denote the preferred (winning) and dispreferred (losing) responses, respectively. The table also specifies whether the method explicitly depends on reference policy πref, whether it leverages the current policy πt during training, and whether the algorithm falls into the offline or online reinforcement learning regime. Table 5: Various preference optimization objectives given preference data = (x, y+, y), where is an input, and y+ and are the winning and losing responses. is class of divergence functions. Γ(x, y) is the uncertainty estimator. is convex decreasing loss function. (cid:98)P (y πt x) is the win rate over the distribution estimated by the average win rate over all the sampled responses y1:K πt( x). Objective πref πt RL Type Method DPO (Rafailov et al., 2023) -DPO (Wang et al., 2023) R-DPO (Park et al., 2024) E(x,y+,y)D log σ (cid:18) β log E(x,y+,y)D log σ (cid:18) βf (cid:18) πθ(y+x) πref (y+x) πθ(y+x) πref(y+x) β log (cid:19) βf (cid:19) πθ(yx) πref(yx) (cid:18) πθ(yx) πref (yx) (cid:19)(cid:19) E(x,y+,y)D log σ (cid:18) β log πθ(y+x) πref(y+x) β log (cid:19) πθ(yx) πref(yx) + (α y+ α y) Distill-DPO (Fisch et al., 2024) E(x,y+,y)D (cid:20) log πθ(y+x) πref(y+x) log GSHF (Xiong et al., 2023) E(x,y+,y)D log σ (cid:18) β log πθ(yx) πref(yx) (r(x, y+) r(x, y)) (cid:19) (cid:21)2 πθ(y+x) πref(y+x) β log πθ(yx) πref(yx) + (Γ (x, y+) Γ (x, y)) KTO (Ethayarajh et al., 2024) IPO (Azar et al., 2024) E(x,y+,y)D λwσ (cid:18) β log πθ (y+ x) πref (y+ x) (cid:19) (cid:18) zref + λlσ zref β log πθ (y x) πref (y x) (cid:19) , where zref = E(x,y)D [βKL (πθ(y x)πref(y x))] E(x,y+,y)D (cid:20) log πθ(y+x) πref(y+x) log πθ(yx) πref(yx) 2τ (cid:21)2 SLiC-HF (Zhao et al., 2023) E(x,y+,y)D max (0, δ log πθ (y+ x) + log πθ (y x)) λ log πθ (y+ x) RRHF (Yuan et al., 2023) SimPO (Meng et al., 2024) CPO (Xu et al., 2024) ORPO (Hong et al., 2024) DNO (Rosset et al., 2024) E(x,y+,y)D max (cid:16) 0, y+ log πθ (y+ x) + 1 (cid:17) log πθ (y x) E(x,y+,y)D log σ (cid:16) β y+ log πθ (y+ x) β E(x,y+,y)D log σ (β log πθ (y+ x) β log πθ (y x)) λ log πθ (y+ x) (cid:18) pθ (y x) 1 pθ (y x) pθ (y+ x) 1 pθ (y+ x) log πθ (y x) γ log log E(x,y+,y)D log pθ (cid:19) , λ log πθ (y+ x) (cid:17) where pθ(y x) = exp (cid:0)y+ x(cid:1) λ log σ (cid:18) 1 log πθ(y x) (cid:19) E(x,y )Dt σ (rt (x, t,y t) rt (x, )) log σ (rt (x, ) rt (x, t)) log (cid:20) (cid:18) σ η log (cid:20) (cid:18) σ η log π (y πt (y π (y πt (y x) x) x) x) η log η log (cid:19)(cid:21) (cid:19)(cid:21) π (y πt (y π (y πt (y x) x) x) x) DNO-Prct (Rosset et al., 2024) SPIN (Chen et al., 2024) SPPO (Wu et al., 2024) INPO (Zhang et al., 2025b) ONPO (Zhang et al., 2025a) (cid:20) (cid:18) (x,y+ ,y )Dt log σ (x,y,y )Dt (cid:16) ℓ (cid:101)η log x) x) (cid:101)η log π(y+ x) π(y πt(y πt(y+ x) (cid:17) πt(yx) β log πθ(yx) πt(yx) β log πθ(yx) (cid:19)(cid:21) (x,y+ ,y )Dt (x,y, (cid:98)P (yπtx))Dt (cid:20) (cid:18) τ η log ) πθ(y+ ) πref(y+ (cid:104) log log πθ(yx) πt(yx) η (cid:19) ) πθ(y ) πref(y + ητ η (cid:16) (cid:98)P (y πt x) 1 (cid:17)(cid:105)2 (x,y+ ,y )Dt (cid:34) log πθ π (cid:1) (cid:1) log (cid:0)y+ (cid:0)y+ πθ π (cid:34) where π = argmin π (x,y+ ,y )Dt (cid:1) (cid:1) (cid:0)y (cid:0)y log (cid:1) π (cid:0)y+ (cid:1) log (cid:0)y+ πt ) πθ(y+ ) πt(y+ log (cid:19) πθ(y ) ) πt(y 1 2τ (cid:21)2 (cid:18) log (cid:35)2 , η 2 (cid:1) π (cid:0)y (cid:1) (cid:0)y πt (cid:18) + 1 2 log 1 + (cid:35)2 η 2 (cid:19)(cid:21) πθ(yx) πref(yx) MIO (Lv et al., 2025) E(x,y+,y)D (cid:20) (cid:18) log 1 + (cid:19) πref(y+x) πθ(y+x) + 1 2 log (cid:18) 1 + (cid:19) πθ(y+x) πref(y+x) 15 Offline Offline Offline Offline Offline Offline Offline Offline Offline Offline Offline Offline Online Online Online Online Online Online Offline Preprint, Under Review"
        },
        {
            "title": "E MATHEMATICAL ANALYSIS",
            "content": "E.1 PROOF OF LEMMA 1 Proof. First, by construction Lt(π(t+1)) = 0, hence π(t+1) is minimizer. Suppose, for contradiction, there exists π Π with π = π(t+1) and Lt(π) = 0. Then for every pair y, Supp(πref ), we must have ht(π, y, y) = η 1 (cid:88) (cid:16) j=i P(y π(t) ) P(y π(t) ) (cid:17) ."
        },
        {
            "title": "Unrolling ht and rearranging yields the pairwise ratio identity",
            "content": "π(y x) π(y x) = exp (cid:16) η n1 exp (cid:16) η (cid:80) j=i (cid:80) j=i P(y π(t) P(y π(t) x) x) (cid:17) (cid:81) (cid:17) (cid:81) j=i π(t) j=i π(t) (y x) 1 . 1 n1 (y x) Because Supp(π) = Supp(πref ) and (cid:80) π(y x) = 1, these pairwise ratios determine π( x) uniquelyand that unique solution is exactly the normalized distribution implied by Eq. 11 and 12, i.e., π(t+1). Hence π = π(t+1), contradiction. Therefore the minimizer is unique. E.2 PROOF OF PROPOSITION 1 Proof. Introduce an indicator Ber(P(y x)) for pair (y, y) π(t) π(t). Consider (cid:101)Lt(π) = y,yπ(t), (cid:16) ht(π, y, y) η (cid:17)2 . Expanding and comparing (cid:101)Lt(π) with Lt(π) shows the only difference lies in the cross-term E[ ht(π, y, y)(P(y π(t)) P(y π(t))) ] versus E[ ht(π, y, y)I ]. One verifies these are equal by writing ht as linear form in log π, log π(t) and using that and are i.i.d. draws from π(t): Ey,y (cid:2)ht(π, y, y)(cid:0)P(y π(t)) P(y π(t))(cid:1)(cid:3) = Ey,y,I (cid:2)ht(π, y, y) I(cid:3). (See INPO Appendix A.5 for the algebraic steps; the same symmetry argument applies verbatim.) t(π) by conditioning on (y, y) and the preference sampler λP(y, y): Now expand (cid:17) (cid:16) t(π) = Ey,y (cid:104) P(y y) ht(π, y, y) 1 2η + (cid:0)1 P(y y)(cid:1)(cid:16) ht(π, y, y) 1 2η (cid:17)2(cid:105) . Using ht(π, y, y) = ht(π, y, y) and completing the square shows t(π) = (cid:101)Lt(π) + const, where the constant depends only on the distribution of (y, y) and P(), not on π. Combining with the equality of cross-terms above gives t(π) = Lt(π) + const, as claimed. E.3 THEORETICAL ANALYSIS OF EQ. 11 Here, we provide theoretical analysis of the iterative update in Eq. 11: π(t+1) (y x) (cid:33) 1 n1 (cid:32) π(t) (y x) exp (cid:32) (cid:89) j=i η 1 (cid:88) (cid:16) π(t) j=i (cid:33) . (cid:17) (cid:12) (cid:12) (cid:12) (19) Derivation from Mirror Descent. Eq. 19 can be viewed as an instance of online mirror descent (OMD) with the KL divergence as the Bregman potential. At each step, player seeks to maximize subject to KL regularization toward the expected win probability against the population (cid:110) (cid:111) π(t) j=i the opponent mixture: π(t+1) = arg max πΠ 1 1 (cid:88) (cid:68) j=i π, ( π(t) ) DKL π (cid:69) 1 η (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:16)(cid:89) j=i π(t) (cid:17) 1 n1 . Taking first-order conditions yields exactly the multiplicative-weights update in Eq. 19. Thus, the MNPO update inherits the regret guarantees of OMD: the average regret after rounds scales as O(1/ ), ensuring convergence to equilibrium in the no-regret learning sense. 16 Preprint, Under Review Pairwise Ratio Dynamics. To avoid computing the intractable partition function, we analyze the pairwise log-ratio ht(π, y, y) = log π(y x) π(y x) 1 1 log (cid:88) j=i π(t) (y x) π(t) (y x) . Eq. 12 in the main text shows that at the fixed point π(t+1), these ratios satisfy η 1 x) (y π(t) π(t+1), y, y(cid:17) (y π(t) (cid:88) ht = (cid:16) (cid:16) j=i (cid:17) x) . Hence the log-ratio dynamics correspond to consistent linearization of the preference margins across all opponents, ensuring that the update increases probability mass on responses with strictly higher average advantage. Equilibrium Properties. By standard arguments in no-regret game dynamics (Freund & Schapire, 1999), if all players update according to Eq. 19, the joint empirical distribution converges to an n-player Nash equilibrium. Moreover, because the update is multiplicative in form, probabilities remain strictly positive on the support of πref , preventing premature collapse. Interpretation. Eq. 19 admits two complementary interpretations: Population averaging: the geometric mean term aggregates beliefs of all opponents, ensuring stability against heterogeneous policies. Advantage weighting: the exponential term amplifies responses that consistently outperform others, with learning rate η controlling the explorationexploitation trade-off. Together, these properties explain why MNPO achieves robust convergence in multiplayer preference optimization while generalizing the two-player INPO update."
        },
        {
            "title": "F LIMITATIONS AND FUTURE WORK",
            "content": "Like other algorithms in the RLHF paradigm, the performance of MNPO is fundamentally linked to the quality of its preference data. Two primary limitations warrant consideration for future work. First, the fidelity of the preference oracle serves as performance ceiling. As the policy model improves and its generations become consistently high-quality, the task of distinguishing between chosen and rejected responses becomes increasingly difficult for the preference oracle in practice. This diminishing discriminative capability can become bottleneck for further improvement. Second, the paradigm of simply increasing the probability of chosen responses and decreasing that of rejected ones may face diminishing returns as the preference gap narrows. When rejected responses are themselves of high quality, the binary preference signal becomes less informative, potentially slowing down or stalling the convergence of the policy model. Future research could explore more nuanced feedback mechanisms to address learning in this high-performance regime. F.1 EXTERNAL OPPONENT PLAYERS Formulation. An alternative to using past-time policies as opponents is leveraging external LLMs to introduce broader range of competitive dynamics. Given set of 1 LLM policies {πj}n1 j=1 , these opponent models can come from different sources: they may belong to distinct model families, have varying parameter scales within the same architecture, be trained on different data distributions, or specialize in different domains. The external opponent MNPO (EO-MNPO) loss in this case is defined as: Lt,D EO-MNPO(πβ, {λj}, η) = (cid:88) (cid:18) λj log π(y x) πj(y x) log π(y x) πj(y x) (cid:19) (cid:13) (cid:13) ηδ (cid:13) (cid:13) . (20) 17 Preprint, Under Review Here, the constraint (cid:80) λj = 1 ensures that the contributions of different opponent policies are appropriately weighted. This formulation introduces key advantage: it enables preference optimization across diverse knowledge sources rather than being restricted to single training trajectory. To better interpret this formulation, we can draw parallels to knowledge distillation. Specifically, consider scenario where we have collection of teacher models (cid:8)πteacher (cid:9) and seek to refine an updated policy π that remains close to both these teacher models and the original reference model πref. The objective function can be expressed as: (cid:34) J(π) = Exd0 Eyπ[R(x, y)] τ0 KL (ππref) τi KL (cid:16) ππteacher (cid:35) (cid:17) . (cid:88) (21) This formulation illustrates that MNPO with external opponent players can be viewed as an extension of knowledge distillation, where the learned policy integrates information from multiple expert models while balancing divergence from the reference model. Using the same derivation technique as DPO, we can recover Eq. 20, linking preference optimization to generalized knowledge alignment framework. Proposition 2. The optimal solution to the maximum reward objective in Eq. 21 is equivalent to the learned reward model in Eq. 20. It is straightforward to show that the solution to the maximum reward objective in Eq. 21 takes the form: π(y x) = 1 (y x)τi/τ , where Z(x) is the partition function and τ = τ0 + (cid:80) Z(x) exp(R(x, y)/τ )πref (y x)τ0/τ (cid:81) πteacher τi. Analysis. EO-MNPO gains several benefits. Firstly, Unlike the self-comparison TD-MNPO, using diverse pool of external LLMs exposes the policy to broader range of feedback, leading to more robust optimization. Secondly, leveraging domain-specific expert models allows MNPO to fine-tune policies for specialized applications, improving generalization. Thirdly, this framework aligns with broader trends in multi-agent RL, where agents iteratively refine their strategies against multiple opponents, driving more sophisticated decision-making. By considering dynamic set of external LLMs as opponent players, EO-MNPO extends beyond self-referential optimization, making it more flexible and generalizable approach for preference optimization in LLMs. THE USE OF LARGE LANGUAGE MODELS (LLMS) We used large language models, including ChatGPT, Gemini, and Claude as writing assistants in the preparation of this manuscript. Their role was strictly confined to language enhancement tasks, such as polishing grammar, improving readability, and generating alternative phrasings. All intellectual contributions, such as research ideas, experimental designs, analyses, and conclusions, were developed solely by the authors, who take full responsibility for the content of this paper."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "Harvard University",
        "Independent Researcher",
        "Pennsylvania State University",
        "RIKEN AIP",
        "Stanford University",
        "The University of Tokyo",
        "UNC Chapel Hill",
        "University of California, Los Angeles"
    ]
}