{
    "paper_title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
    "authors": [
        "Jialong Wu",
        "Xiaoying Zhang",
        "Hongyi Yuan",
        "Xiangcheng Zhang",
        "Tianhao Huang",
        "Changjing He",
        "Chaoyi Deng",
        "Renrui Zhang",
        "Youbin Wu",
        "Mingsheng Long"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 2 ] . [ 1 4 3 8 9 1 . 1 0 6 2 : r Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models Jialong Wu1,2,, Xiaoying Zhang2,, Hongyi Yuan2, Xiangcheng Zhang1,2,, Tianhao Huang1, Changjing He1, Chaoyi Deng1,2,, Renrui Zhang2, Youbin Wu2, Mingsheng Long1, 1Tsinghua University, 2ByteDance Seed Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Humans construct internal models of the world and reason by manipulating the concepts within these models. Recent advances in artificial intelligence (AI), particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems, which rely predominantly on verbal reasoning as their primary information-processing pathway. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though clear consensus on their benefits has not yet been reached. From world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasksparticularly those grounded in the physical worldvisual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as core component of deliberate CoT reasoning and analyze distinctions among different forms of world models from both informativeness and knowledge aspects. Empirically, we identify and design tasks that necessitate interleaved visual-verbal CoT reasoning, constructing new evaluation suite, VisWorld-Eval. Through controlled experiments on state-of-the-art UMM, we show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling. Conversely, it offers no clear advantage for tasks that do not require explicit visual modeling. Together, these insights and findings clarify the applicability and potential of multimodal world modeling and reasoning for more powerful, human-like multimodal AI. We publicly release our evaluation suite to facilitate further research. Date: January 28, 2026 Project Lead: Jialong Wu at wujialong0229@gmail.com Correspondence: Mingsheng Long at mingsheng@tsinghua.edu.cn, Xiaoying Zhang at zhangxiaoying.xy@bytedance.com Project Page: https://thuml.github.io/Reasoning-Visual-World 1 Figure 1 Overview of world-model perspective on multimodal reasoning. (a) Humans construct mental models of the world, representing information and knowledge through two complementary channelsverbal and visualto support reasoning, planning, and decision-making. (b) Recent advances in large language models (LLMs) and vision language models (VLMs) largely rely on verbal chain-of-thought reasoning, leveraging primarily verbal and symbolic world knowledge. (c) Unified multimodal models (UMMs) open new paradigm by using visual generation for visual world modeling, advancing more human-like reasoning on tasks grounded in the physical world. Examples of reasoning with verbal world modeling are adapted from Chen et al. [9], Du et al. [14], Guo et al. [18], Zhang et al. [72]."
        },
        {
            "title": "Introduction",
            "content": "Humans construct internal mental models of the external world that represent objects and concepts, along with their relationships, structures, and operational mechanisms [11, 16]. These models support reasoning and decision-making by enabling mental simulation, allowing individuals to anticipate the outcome of actions without actually taking them [40]. For example, if glass of water is spilled on the table, people can rapidly mentally simulate the ensuing events: the water falling downward, spreading across the surface, and potentially dripping onto the floor. Such predictions lead them to quickly move valuable items away or reach for towel. Beyond physical systems, mental models also extend to any domain where relational structures can be simulated, such as mathematics and logic [31, 32], making them fundamental to how humans understand and interact with all aspects of the world. Cross-disciplinary researchers in philosophy, psychology, cognitive science, and related fields have long history of developing computational models of human mental models [44]. Among them, artificial intelligence (AI) shares core ambition of building machines that reason like people. Although debates remain, recent breakthroughs, especially in large language models (LLMs) and chain-of-thought (CoT) reasoning, have made substantial step towards approximating human reasoning grounded in mental models of the world, often referred to as world models [24, 34] in the AI literature. During chain-of-thought reasoning, LLMs explore, reflect, and backtrack within the structured solution space, guided by world knowledge acquired 2 through large-scale pre-training. These capabilities have already driven progress in diverse domains, including programming [18], mathematics [18, 57], scientific discovery [53], clinical medicine [58], and robotics [42]. Such reasoning capabilities have also been extended to multimodal AI systems, particularly vision language models (VLMs) [6, 19, 28, 70]. These systems typically incorporate visual inputs by aligning visual representations with the embedding space of LLMs, resulting in reasoning that remains primarily constrained to linguistic space. In contrast, human mental models operate over multiple forms of mental representations. Dual-coding theory [45] suggests that the mind processes information through two complementary codes: verbal and imagery (particularly visual) representations. These pathways can function independently but often collaborate to support reasoning. Indeed, visual imagery has been shown to have advantages over words in memory encoding and retrieval [33]; and individuals with aphantasia, who lack the ability to visualize mental imagery, exhibit worse performance on tasks such as visual search [43]. These evidence from psychology and cognitive science therefore suggest that the absence of dedicated visual information pathway may explain why current multimodal AI systems excel in formal and abstract domains dominated by verbal world knowledge, yet continue to fall far short of human performance on tasks involving physical and spatial reasoning [8, 49], which fundamentally depend on visual world modeling. Next-generation multimodal AI systems are evolving to be built upon unified multimodal models (UMMs) [13, 54, 62, 63], which seamlessly integrate both verbal and visual generation capabilities. The newly introduced visual generation component offers the potential to explicitly realize visual world modeling, critical element of multimodal world models in human-like reasoning that current systems largely lack. This naturally makes us ponder: Can current UMMs truly leverage their visual generation capability to enhance reasoning and thereby narrow the performance gap between multimodal AI and humans? growing body of preliminary research [17, 36, 38, 76, 77] has begun exploring this question from different perspectives. However, the findings so far remain inconclusive. Reported empirical results are mixed, showing no consistent trends that visual generation reliably improves reasoning performance. Moreover, the evaluation tasks used in current studies are designed heuristically, lacking principled basis for understanding when and how visual generation can meaningfully contribute to multimodal reasoning. In this paper, we present the first principled study of when and how visual generation benefits reasoning from world-model perspective (see Figure 1), making both theoretical and empirical contributions. Theoretically, we rigorously bridge the concepts of world models and reasoning. (1) World model formulations: We formulate multimodal world models to approximate the underlying multi-observable Markov decision processes (MOMDP) of tasks, and define two fundamental capabilities of world models, namely world reconstruction and world simulation. (2) World model-based reasoning: To realize world models for reasoning, we introduce three reasoning formulations. Two rely solely on verbal CoTs through implicit or verbal world modeling, while the third interleaves verbal and visual CoTs that explicitly incorporate visual generation as form of visual world modeling. (3) The visual superiority hypothesis: Under this framework, we analyze the distinctions among different world models, highlighting the richer informativeness and complementary prior knowledge afforded by visual world modeling. These insights motivate our central hypothesis that visual world modeling is superior for certain tasks, particularly those grounded in the physical world. Empirically, we validate these insights through series of controlled experiments. (4) The VisWorld-Eval suite: We identify and design tasks that specifically isolate and demand each atomic world model capability, forming new evaluation suite to facilitate future research. This suite, VisWorld-Eval, collects seven tasks spanning both synthetic and real-world domains. (5) Empirical evaluation: Experiments with state-of-the-art UMM [13] on VisWorld-Eval reveal findings consistent with our insights and theoretical analysis. In tasks where verbal world modeling suffers from representational bottlenecks or insufficient prior knowledge, interleaved CoT delivers substantial performance improvements. By contrast, it offers no clear advantages in tasks such as mazes and Sokoban, whose simple states do not require explicit visual world modeling. We further conduct dedicated analyses, including evidence revealing emergent implicit world modeling in the maze task. We hope this work provides early evidence for the central role of multimodal world models in general-purpose AI, in which complementary verbal and visual knowledge emerge from generative modeling across modalities, with the latter being especially valuable for bringing human-like intelligence into the physical world."
        },
        {
            "title": "2 Related Work",
            "content": "World models. The field of world models is rapidly evolving, yet remains far from reaching consensus on definitions or methodologies. Although psychology and cognitive science suggest that human mental models rely on compact representations that discard irrelevant details, how to scale approaches capable of learning such abstract representations [26, 34, 48] to arbitrary domains and modalities is still unclear. Consequently, most current techniques preserve complete information of observations, either through reconstructable latent representations [24, 25] or directly at the level of raw data. Prominent examples include modern video generation world models [1, 2, 12, 64] which capture concrete pixel-level dynamics. In contrast, language inherently provides higher level of abstraction, making it more similar to human mental representations [9, 59, 60, 65, 71]. This motivates the promise of unified multimodal models that generate both languages and visuals as new direction for building more human-like world models. Unified multimodal models. Multmodal understanding [6, 19, 28] and visual generation [47, 50] have long developed in isolation. Recently, there has been growing interest in integrating these two capabilities into single unified model. This can be straightforwardly achieved by forwarding the representations of vision language models to an external visual generation module [46, 56]. more unified approach is to model both language and visual modalities within single backbone. While language is predominantly modeled through autoregressive next-token prediction, the design space of visual modalities spans wide spectrum, from discrete tokenization with autoregressive [54, 62, 63] or masked modeling [22, 66], to continuous tokenization with diffusion or flow-based modeling [13, 41, 75]. Among these efforts, BAGEL [13] is one of the most widely adopted open-source models achieving state-of-the-art performance. Despite substantial progress in building unified multimodal models (UMMs), existing evaluations still primarily assess their understanding and generation capabilities separately. One widely recognized advantage of UMMs lies in leveraging reasoning abilities of handling complex instructions to enhance visual generation or editing [21, 74]. Yet when and how visual generation, in turn, enhances reasoning remains insufficiently explored, lacking solid empirical evidence and community consensus. Benchmarking visual generation for reasoning. This paper contributes to growing line of research on visual generation for reasoning. RealUnify [52] and Uni-MMMU [77] design tasks in which generation is expected to enhance reasoning, but report mixed results without revealing clear trends regarding the benefits of visual generation. ROVER [38] reveals fundamental limitations of current models in generating meaningful visual reasoning steps, often resulting in minimal or even negative gains in final accuracy. In contrast, MIRA [76] conducts sanity test by providing manually annotated visual cues, thereby bypassing the evaluation of visual world modeling capability. While the aforementioned works evaluate zero-shot performance, ThinkMorph [17] fine-tunes UMMs to reveal emergent reasoning behaviors but restricts each CoT to single intermediate image, thereby not fully exploiting the potential of interleaved CoT. Our work distinguishes itself through world-model perspective that enables principled investigation, allowing us to both demonstrate and systematically explain when visual generation yields positive gains and when it does not."
        },
        {
            "title": "3 A World Model Perspective on Multimodal Reasoning",
            "content": "Inspired by the aforementioned connections between human cognition and artificial intelligence, we formalize our world-model perspective on multimodal reasoning (see Figure 2) in this section."
        },
        {
            "title": "3.1 Formulation: Multiple Observations of the World",
            "content": "Without loss of generality, the world of specific task can be formulated as multi-observable Markov decision process (MOMDP) = (S, A, p, Φ, Oϕ, eϕ), where denotes the state space, the action space, the transition function, Φ the parameter space of observation functions, Oϕ the observation space, and eϕ the observation function. Each represents the underlying state of the world, which is typically hidden and not directly observable. Instead, it can be perceived through different instantiations of observations (hereafter also referred to as views) [27], given by = eϕ(s) Oϕ, parameterized by ϕ Φ. As illustrated in Figure 2a, such views can span multiple modalitiesfor example, visual observations corresponding to different camera 4 Figure 2 Theoretical formulation of the world model perspective on multimodal reasoning. (a) Observations of the same underlying world state can span multiple modalities, including verbal and visual observations, each reflecting different views or emphases. (b) Two atomic capabilities of world models are defined: world reconstruction, which infers complete structure from partial observations and enables novel view synthesis, and world simulation, which models dynamics to predict future observations. (c) Chain-of-thought reasoning includes internal world modeling, by explicitly maintaining an evolving sequence of observations, generated through either of the atomic world model capabilities. poses, or verbal descriptions expressed with different emphases or styles. When an action is applied to the current state, the world transits according to the dynamics p(ss, a) and yields new observations."
        },
        {
            "title": "3.2 Atomic Capabilities of World Models",
            "content": "A world model, analogous to human mental models, is then expected to support two fundamental capabilities [34], illustrated in Figure 2b. The first is called world reconstruction. Humans are remarkably skilled at mentally reconstructing the structure of an environment from only few partial observations [71], grounded in their prior knowledge of the world. Such mental reconstruction allows them to imagine novel views of the same underlying state, supporting skills such as mental rotation. Formally, the perception component of world model encodes observations from limited views into an internal representation: ˆs = enc(oϕ1, . . . , oϕn ) s. This representation approximates the true state1, and can then be decoded to generate an unseen observation: 1We set aside the debate between compact and comprehensive representations. By treating abstract (e.g., sketches) and high-fidelity observations as different view specifications, this formulation allows the internal representation to flexibly adjust to 5 ˆoϕn+1 = dec(ˆs, ϕn+1) eϕn+1(s), providing an internal \"experience\" of navigating the world. In modern generative models, including UMMs, since their latent representations are not explicitly defined, the world reconstruction capability can be realized through end-to-end novel view generation: pθ(oϕn+1 oϕ1 , . . . , oϕn ), (1) which implicitly learns the internal representations required to synthesize the new view. The second capability is world simulation. Humans can mentally simulate how the world evolves into the future, supporting reasoning and decision-making, either purely in their minds or with external aids such as scratchpad. Formally, this corresponds to the prediction component of world model, which predicts the transition of the current state and action: ˆs pred(ˆs, a), providing an internal \"experience\" of interacting with the world. Similarly, for modern generative models, this capability is more typically realized through predictions of future observations: In our new evaluation suite, we deliberately curate tasks that specifically demand each capability, allowing us to independently validate its contribution to multimodal reasoning (see Section 4.1). pθ(ot+1 t, t). (2)"
        },
        {
            "title": "3.3 Deliberate Reasoning with World Modeling Across Modalities",
            "content": "We then formalize how world-modeling capabilities within multimodal models contribute to reasoning. Given question and input images I, the chain-of-thought reasoning process of multimodal AI system can be expressed as sequence of intermediate steps (or thoughts) = τ1, τ2, . . . , τH , followed by the answer A. Although this general formulation treats each reasoning step τi as an unconstrained, free-form operation, our world model perspective suggests that humans reason by prediction and planning, and each step inherently manipulates the underlying world observations of the problem [10, 59, 72]. We therefore refine the reasoning formulation as τi = (ri, oi) to explicitly incorporate an evolving sequence of observations: = (r1, o1) , (r2, o2) , . . . , (rH , oH ) , (3) 2 denotes logical reasoning step based on the accumulated context, typically expressed in text, and where ri oi denotes the observation generated at that step. Specifically, the input images serve as the initial observation o0 = I, and subsequent observations are generated from previous reasoning and observations, by invoking atomic world modeling capabilities: world reconstruction (Eq. (1)) and world simulation (Eq. (2)), where reasoning steps imply actions and view transformations ϕ, as illustrated in Figure 2c. This formulation is modality-agnostic, allowing observationsand thus world modelingto arise across various modalities. We focus specifically on verbal and visual observations, motivated by dual-coding theory in human cognition and by the fact that UMMs are equipped to generate both. This yields several concrete CoT instantiations. Specifically, verbal world modeling produces purely verbal CoTs, with oi as verbal descriptions, whereas visual world modeling produces verbal-visual interleaved CoTs, with oi as generated images. In addition, prior work has discovered that language models can implicitly learn world models with emergent internal representations of board-game states without explicit supervision [37]. Motivated by this, we also consider implicit world modeling, in which no explicit observation is generated (oi = )3."
        },
        {
            "title": "3.4 The Visual Superiority Hypothesis",
            "content": "Contemporary LLMs and VLMs have achieved impressive performance in structured and abstract domains, such as mathematics and programming, largely driven by large-scale language-centric pre-training and verbal the level of detail required by the desired views. 2We use to index reasoning steps in order to distinguish them from the true time step of the underlying MOMDP. The twos are not generally aligned, as we may include branching and backtracking in the reasoning. 3In practice, strictly distinguishing implicit from verbal world modeling can be difficult, because there are often partial descriptions of the current state in the reasoning part ri. In this work, we treat verbal world modeling as explicitly expressing world states or observations in text, such as coordinates or symbolic matrices. chain-of-thought post-training. Although these models have accumulated extensive verbal and symbolic knowledge, their understanding of the visual world remains limited when trained under purely verbal supervision. As result, they continue to struggle with tasks grounded in basic physical and spatial intuition that even young children naturally master [8, 49]. Visual world modeling is therefore essential for endowing multimodal AI with complementary forms of information and knowledge. (1) In terms of informativeness, while verbal and symbolic representations capture high-level semantic abstractions, they often suffer from ambiguity and representational bottlenecks. In contrast, visual observations are more concrete and information-rich, directly encoding physical properties such as motion and spatial relationships. This provides precise, fine-grained grounding for reasoning about the complex real world, particularly in spatial and physical tasks. (2) In terms of prior knowledge, visual world knowledge is inherently complementary to symbolic knowledge. Humans and animals acquire much of this knowledge (e.g., physical interactions and spatial transformations) through perception, largely independent of language. Consequently, humans naturally represent and communicate such knowledge visuallyfor example, by sketching an approximate parabolic trajectory without performing explicit calculations. This suggests that different aspects of world knowledge are concentrated in different data modalities, and learning from large-scale generative modeling of visual data can thereby expand the effective knowledge landscape available for multimodal reasoning. We next formalize and justify these insights through theoretical analysis, with formal statements and proofs provided in Appendix A. Informativeness. For notational convenience, we denote the question as r0, the input images as o0, 1), Ri = and the final answer as rH+1. Prefixes of CoT are defined as Ri = (r0, o0, r1, o1, . . . , ri 1, ri). We use H() and I(; ) to denote Shannon entropy and mutual information, (r0, o0, r1, o1, . . . , ri respectively. We first establish that the end-to-end answer error admits an upper bound that naturally decomposes into reasoning and world-modeling errors. 1, oi 1, oi Theorem 1. Let denote the distribution over optimal chain-of-thoughts and answers, and let pθ be learned reasoning model. Then the following inequality holds: KL(p(A Q, I) pθ(A Q, I)) KL(p(R, Q, I) pθ(R, Q, I)) (cid:88) H+1 (cid:88) = i=1 Ep [KL(p(riRi) pθ(riRi))] (cid:123)(cid:122) (cid:125) (cid:124) reasoning errors + i= Ep (cid:124) (cid:105) (cid:104) KL(p(oi Ri) pθ(oi Ri)) (cid:123)(cid:122) world-modeling errors (cid:125) . (4) This decomposition reveals fundamental trade-off between the informativeness of world models for reasoning and the fidelity of the world model itself. In the case of implicit world modeling, where oi = , we get rid of the world-modeling error. However, this typically comes at the cost of increased uncertainty and learning difficulty in reasoning, as all state transitions must be implicitly encoded. Empirically, world models that explicitly track the task states, serving as verbal or visual sketchpads, are generally beneficial for reasoning. We dive into the reasoning component of Eq. (4) to elucidate the factors underlying these benefits. Theorem 2. Let si denote the latent states associated with the observations oi. Under appropriate assumptions, the reduction in reasoning uncertainty achieved by explicit world modeling satisfies the following properties: 1. Reasoning uncertainty does not increase: H(rio0, r0:i 1) H(riRi) = I(o1:i 1; rio0, r0:i 1) 0. 2. The reasoning uncertainty improvement is bounded by both (i) the information that observations provide about the underlying states and (ii) the information that the reasoning step requires about those states: I(o1:i 1; rio0, r0:i 1) min (I(o1:i 1; s1:i 1), I(ri; s0:i 1, r0:i 1)) . (5) The uncertainty of the target distribution is closely related to sample efficiency and learning difficulty. Consequently, the upper bound on the improvement of reasoning uncertainty (Eq. (5)) highlights another trade-off in the choice of observation modality for world modeling. The first term indicates that observations 7 should be sufficiently informative about the underlying latent states. In contrast, the second suggests that they need only preserve the task-relevant aspects of the states required to select appropriate reasoning steps. Excessively detailed observations may be unnecessary and even detrimental, increasing world modeling errors. Prior knowledge. Although visual world models are more informative, they are intrinsically more difficult to learn from scratch due to the high dimensionality and complexity of visual observations. Fortunately, modern AI systems are typically large-scale pre-trained, which endows them with strong prior knowledge and enables faster convergence and improved generalization during downstream post-training. As discussed earlier, humans tend to represent different aspects of world knowledge through different modalities. Consequently, for given downstream task, the distribution shift between its transition distribution and that learned during large-scale Internet pre-training can vary substantially across modalities. The generalization bound in Theorem 6 of Appendix A.2 suggests that this modality-dependent distribution shift is closely related to the post-training sample efficiency of the corresponding world model. This highlights the importance of acquiring broad prior knowledge across modalities during pre-training, and of leveraging the proper modality whose priors are best aligned with the downstream task. Drawing on the above analysis, we formulate our central hypothesis regarding when and how visual generation benefits reasoning, thereby helping narrow the gap between multimodal AI and human capabilities. The Visual Superiority Hypothesis: In multimodal reasoning tasks grounded in the physical world, visual generation as world model yields representations that are more informative and knowledge-rich than those produced by verbal world models."
        },
        {
            "title": "4 Experiment Settings",
            "content": "Finally, we empirically validate the insights and theoretical analyses presented above through series of controlled experiments. In this section, we describe the evaluation tasks and model training procedures."
        },
        {
            "title": "4.1 VisWorld-Eval: Task Suite for Reasoning with Visual World Modeling",
            "content": "While prior work has primarily designed evaluation tasks heuristically, we principledly evaluate multimodal reasoning across tasks designed to specific world model capabilities. Building on related benchmarks, we identify and curate total of seven tasks, forming an evaluation suite tailored to assess reasoning with visual world modeling. All tasks are framed as question answering with concise, verifiable answers, and performance is measured by answer accuracy. We refer to this suite as VisWorld-Eval, and summarize it in Figure 3. World simulation. We consider the following tasks that primarily require simulating world dynamics over time: (1) Paper folding: Adapted from SpatialViz-Bench [61], this task presents sequence of paper folds followed by hole punching, and asks for the distribution of holes after the paper is unfolded. Successfully solving this task requires simulating the unfolding process, relying on prior knowledge of symmetry and spatial transformations that is commonly grounded in visual experience. (2) Multi-hop manipulation: Build upon CLEVR [30], this task features scene containing objects with various shapes and colors that undergo sequence of operations, such as addition, removal, or color changes. The final question queries properties of the resulting layouts. Since target objects of operations are often specified via relative spatial relationships, this task places strong demands on state tracking and spatial understanding. (3) Ball tracking: Adapted from RBench-V [20], this task evaluates physical dynamics simulation by requiring the model to infer the trajectory of ball undergoing ideal specular reflections within given scene and predicting which numbered hole it will ultimately enter. In addition, we include (4) Maze [29] and (5) Sokoban [55], as these two grid-world tasks are commonly used in prior work of studying visual generation for reasoning [36, 67]. World reconstruction. We also evaluate tasks that emphasize reconstructing underlying world structure from partial observations: (6) Cube 3-view projection: Adapted from SpatialViz-Bench [61], this task provides an isometric view and two orthographic views of connected cube stack, and asks about an unseen viewpoint. Solving the task requires reconstructing the full 3D structure and mentally rotating or projecting it into the queried view, process closely aligned with human visual mental representations. (7) Real-world spatial reasoning: We focus on the positional relationship subset of MMSI-Bench [69]. Given multiple views of 8 Figure 3 The VisWorld-Eval suite for assessing multimodal reasoning with visual world modeling. VisWorld-Eval comprises seven tasks spanning both synthetic and real-world domains, each designed to isolate and demand specific atomic world-model capabilities. Table 1 Zero-shot evaluation of advanced VLMs on VisWorld-Eval. We report the average accuracy over five tasks (excluding Maze and Sokoban) and over all seven tasks."
        },
        {
            "title": "Paper\nFolding",
            "content": "Multi-Hop Manip."
        },
        {
            "title": "Ball\nTracking",
            "content": "Cube 3-View MMSI (Pos. Rel.)"
        },
        {
            "title": "Sokoban",
            "content": "Overall (5 tasks) Overall (7 tasks) Gemini 3 Flash Gemini 3 Pro Seed 1.8 GPT 5.1 o3 25.6 27.0 10.6 6.4 13.5 Qwen3-VL-8B-Thinking [5] BAGEL-7B-MoT [13] 11.0 11."
        },
        {
            "title": "Proprietary Models",
            "content": "55.3 44.7 24.4 34.8 24.7 52.7 53.3 42.5 44.5 37.7 Open-Source Models 17.8 19.4 21.2 26.8 41.3 49.6 38.8 44.8 44. 27.7 27.2 75.4 74.5 75.2 73.9 68.1 49.3 31.6 73.9 33.5 83.9 0.6 0.0 99.3 90.2 68.3 62.8 36.0 0.0 0. 5.8 0.2 50.0 49.8 38.3 40.8 37.6 25.4 23.2 60.5 53.2 49.1 38.2 32.0 18.9 16.6 realistic scene, these tasks ask about positional relationships among the cameras, objects, and regions. Successfully answering these questions requires constructing coherent spatial mental model of the scene from limited viewpoints to support accurate spatial reasoning. For each task, we construct SFT data by designing different CoT patterns with implicit, verbal, or visual world modeling, enabling controlled comparative evaluations. Data construction pipeline and examples across tasks are presented in Appendix B.1. Evaluation of advanced VLMs. Table 1 reports the zero-shot performance of advanced VLMs on VisWorld-Eval. Overall, these models perform suboptimally, highlighting limitations of current multimodal AI systems. Among them, Gemini 3 Flash and Gemini 3 Pro remarkably outperform the other models; however, their performance remains far from satisfactory on challenging tasks like paper folding, ball tracking, cube 3-view projection, and real-world spatial reasoning."
        },
        {
            "title": "4.2 Unified Multimodal Model Training and Evaluation",
            "content": "Evaluation protocol. To investigate the benefits of visual generation in multimodal reasoning, we evaluate post-trained UMMs, rather than the zero-shot performance of base models. To the best of our knowledge, no open-source model has been natively optimized for interleaved verbal-visual generation for reasoning. Even commercial closed-source models currently exhibit fundamental limitations in generating visual intermediate reasoning steps [38, 76]. Focusing on post-trained models, therefore, provides more meaningful estimate of the upper bound for multimodal reasoning performance, while reducing confounding effects arising from insufficient pre-training due to limited interleaved data availability or quality. Model training. We adopt BAGEL [13], state-of-the-art open-source unified multimodal model, as our base model. Most experiments are conducted by supervised fine-tuning (SFT) on task-specific datasets, where verbal and visual generation in both chain-of-thought reasoning and final answers are optimized using cross-entropy and flow-matching loss. Specifically, the loss for reasoning with visual world modeling is as follows: Lθ(Q, I, R, A) = H+1 (cid:88) ri (cid:88) i= j=1 log pθ (ri,j ri,<j, Ri) + (cid:88) i=1 Et,ϵ (cid:13) (cid:13) (cid:13)vθ(ot (cid:13) 2 i, Ri) (ϵ oi) (cid:13) (cid:13) 2 , (6) where ot = toi + (1 t)ϵ are noisy observations. We emphasize that in our formulation, ri refers to verbal reasoning step, instead of reward. We also perform reinforcement learning from verifiable rewards (RLVR) following SFT. During RL, only the verbal generation component is optimized by GRPO [18], while visual generation is regularized via the KL-divergence with respect to the SFT-trained reference model: Jθ(Q, I) = Eo,r pθold (cid:34) H+1 (cid:88) ri (cid:88) (cid:32) i=1 j= (cid:88) i=1 Et,ϵ min (cid:16) pθ (ri,j ri,<j, Ri) pθold (ri,j ri,<j, Ri) A, clip (cid:16) pθ (ri,j ri,<j, Ri) pθold (ri,j ri,<j, Ri) , 1 ε, 1 + ε (cid:33) (cid:17) (cid:17) (cid:13) (cid:13) (cid:13)vθ(ot i, Ri) vθref(ot (cid:13) 2 i, Ri) (cid:13) (cid:13) 2 (cid:35) . (7) Full implementation details and hyperparameters are provided in Appendix B.2."
        },
        {
            "title": "5 Experimental Results",
            "content": "In this section, we demonstrate that visual world modeling boosts multimodal reasoning through two atomic capabilities: world simulation (Section 5.1) and world reconstruction (Section 5.2). We also identify tasks in which it is unhelpful (Section 5.3), where implicit or verbal world modeling is sufficient. We conduct analysis in detail. Interestingly, we reveal emergent internal representations in UMMs that support implicit world modeling on simple maze tasks."
        },
        {
            "title": "5.1 Visual World Simulation Boosts Multimodal Reasoning",
            "content": "Main results. Figure 4 summarizes the performance of SFT-trained UMMs under different chain-of-thought formulations across all tasks. We observe that interleaved CoT with visual world modeling significantly outperforms its purely verbal counterparts on three world simulation tasks: paper folding, multi-hop manipulation, and ball tracking. These gains are attributed to both the richer expressiveness and stronger prior knowledge afforded by the visual modality. In particular, it is difficult for models to precisely ground object coordinates and perform arithmetic operations without external tools in tasks such as multi-hop manipulation and ball tracking, with the latter being especially challenging. Thus, verbal world modeling is inappropriate and omitted in these tasks. This exacerbates ambiguity and hallucinations in purely verbal reasoning. Similarly, in paper folding, although models can track the states of holes, it remains difficult to completely depict the paper contour during unfolding. Moreover, as showcased in Figure 9 and 16, the spatial transformation involved in paper unfolding critically relies on an understanding of geometric symmetry, which can be more naturally learned from visual data like images and videos. 10 Figure 4 Performance of SFT-trained UMMs with different world model-based chain-of-thought formulations across seven tasks from VisWorld-Eval. Refer to Table 1 for zero-shot performance of advanced VLMs. Sample efficiency. To further demonstrate the stronger prior knowledge embedded in the visual modality, we experiment comparing the sample efficiency of verbal and visual world modeling on the paper folding task. As shown in Figure 6a, reasoning with visual world modeling exhibits substantially higher sample efficiency, achieving performance comparable to verbal world modeling while using more than 4 less SFT data."
        },
        {
            "title": "5.2 Visual World Reconstruction Boosts Multimodal Reasoning",
            "content": "Main results. As shown in Figure 4, multimodal reasoning tasks that rely on world reconstruction capabilities also benefit substantially from visual world modeling. In the cube 3-view task, predicting novel view of stacked cubes, denoted symbolic character matrices, suffers from limited prior knowledge, whereas visually rotating objects has been rich experience during pre-training with large-scale Internet videos. For MMSI tasks, fully describing novel view of realistic scene using text alone is similarly ill-suited as in the previous subsection, and we also discover hallucinations in pure verbal reasoning, which lacks grounding to visual generation. We do not observe consistent improvements on other positional-relationship subtasks in MMSI-Bench, except camera-object and camera-region, which we attribute to current UMMs limitations in both spatial understanding during verbal reasoning and generation quality in visual world modeling. Full quantitative results and qualitative examples are provided in Appendix C. We expect these limitations to be mitigated in future work with stronger base models. Effects of task difficulties. Figure 6b analyzes performance on the cube 3-view projection task across varying sizes of input cube stacks. We observe consistent advantage of reasoning with visual world modeling over verbal world modeling across all difficulty levels. Notably, for cube stacks of size sixout of the training distributionvisual world modeling still yields approximately 10% performance improvement. World model fidelity. Modern AI models are known to exhibit hallucinations along their reasoning trajectories, even when producing correct final answers [38]. We therefore evaluate the fidelity of world modeling in the cube 3-view projection task by comparing ground-truth views with the intermediate views generated verbally or visually during reasoning. To focus on structural correctness, we compare only the shapes of the views and completely ignore color information. Even under this relaxed evaluation setting, Figure 6b shows that verbal world modeling exhibits dramatically low fidelity, with scores degrading to near zero. Notably, approximately half of the samples require predicting the opposite view of given input view, transformation that only involves horizontal mirroring. Visual world modeling, benefiting from stronger prior knowledge of such geometric transformations, captures these patterns effectively and achieves fidelity scores consistently exceeding 50%."
        },
        {
            "title": "5.3 Visual World Modeling is Unhelpful for Certain Tasks",
            "content": "Main results. (Un)surprisingly, we do not observe notable improvements on grid-world tasks, including maze and Sokoban. In the maze tasks, reasoning with implicit world modelingwithout explicitly tracking coordinatesachieves the best performance with slight advantage. These results are consistent with recent 11 (a) Sample efficiency. (b) World model fidelity. (c) Implicit world modeling. Figure 6 Model analysis: (a) Performance of UMMs on the paper-folding task with varying numbers of SFT samples. Reasoning with visual world modeling achieves 4 improvement in sample efficiency. WM = world modeling. (b) Performance of UMMs on the cube 3-view projection task with increasing sizes of input cube stacks, evaluated using both answer accuracy and world-model fidelity. Visual world modeling demonstrates dramatically better fidelity of view synthesis. (c) Prediction accuracy of masked point coordinates in CoTs using representations extracted from different layers of different UMMs, revealing emergent internal world representations. PT = Pre-trained. empirical findings [14]. We argue that this is also well explained by our world model perspective. In these tasks, state tracking is relatively simple, typically requiring the maintenance of only one or two two-dimensional coordinates, which can be adequately handled through verbal reasoning alone. Furthermore, in the maze task, we hypothesize that such world modeling can be implicitly encoded in the models hidden representations [37], which helps explain the competitive performance of verbal reasoning without explicit coordinate tracking. Demystifying implicit world modeling. To validate this hypothesis, we probe the internal representations of models, as illustrated in Figure 5. We consider the same architecture, BAGEL, with three different sets of weights: randomly initialized model, the pre-trained model, and the model supervised fine-tuned on CoT data in the implicit world modeling format, in which special tokens mask all explicit point coordinates during the reasoning process. For each model, we extract the hidden representations of these special tokens at each layer. We then train multilayer perceptrons (MLPs) on these representations to predict the underlying true point coordinates. Figure 5 Probing implicit world models, by training set of probes, i.e., MLPs which infer the masked point coordinates during reasoning from internal representations. Figure 6c reports the prediction accuracy on validation set. As expected, the randomly initialized model completely fails to internally track point states, achieving only random-guess accuracy on 5 5 mazes. In contrast, the pre-trained model [13] already exhibits emergent representations that are predictive of maze states. Notably, we observe non-monotonic trend across layers: prediction accuracy increases from lower layers (which capture low-level features) to middle layers, and then decreases toward the final layers, which are likely specialized for next-token prediction. Finally, supervised fine-tuning on domain-specific data, despite providing no explicit coordinate supervision, substantially enhances this internal predictability, achieving near-perfect accuracy. These in-depth results help explain our main experimental findings: as the model already possesses the capability for implicit world modeling, it does not necessarily benefit from explicit verbal world modeling, let alone more complex forms of visual world modeling."
        },
        {
            "title": "5.4 Comparison with VLMs: Do UMMs Compromise Verbal Reasoning Capabilities?",
            "content": "One may argue that UMMs are typically trained with stronger emphasis on visual generation [13], which could compromise verbal reasoning capabilities, and bias comparisons in favor of visual world modeling. To address this concern, we compare with pure VLM baseline, Qwen2.5-VL-7B-Instruct [6], which shares the 12 Figure 7 Performance of SFT-trained VLMs compared with UMMs across three tasks. Figure 8 Performance of RLVR-trained VLMs and UMMs with different world-model-based CoT formulations across three tasks. same Qwen 2.5 LLM base model, with BAGEL. We fine-tune Qwen2.5-VL on the same verbal CoT datasets used in the previous subsections and evaluate it on three representative tasks: paper folding, cube 3-view projection, and multi-hop manipulation. Results. As shown in Figure 7, the SFT performance of Qwen2.5-VL with implicit and verbal world modeling is comparable to that of BAGEL, without exhibiting significant advantages. It still lags behind BAGEL in settings that leverage visual world modeling. These results indicate that our findings arise from the inherent advantages of visual world modeling rather than from compromised verbal reasoning capabilities in UMMs."
        },
        {
            "title": "5.5 RL Enhances Various CoTs, Yet Does Not Close the Gap",
            "content": "Reinforcement learning from verifiable rewards (RLVR) has been major driver of recent progress in reasoning models equipped with verbal chain-of-thoughts, achieving strong performance across domains such as mathematics [18]. While Figure 4 shows clear advantage of reasoning with visual world modeling after SFT, RLVR may further incentivize emergent reasoning behaviors that improve verbal CoTs. We thus conduct comparative RLVR experiments across different world modelbased CoT formulations on three representative tasks. Results. Figure 8 presents the learning curves under RLVR for different models. We observe consistent improvements during RLVR for different CoT formulations. However, the performance gap persists. We also find that VLMs and UMMs generally perform similarly with verbal CoTs. These results suggest that the superiority arises from inherent advantages of the world modeling approach, rather than insufficient post-training. Notably, RL enhances reasoning with visual world modeling, even though only the verbal 13 generation components of interleaved CoTs are directly optimized. We envision that the full potential of interleaved CoTs will be further released with the development of RL algorithms tailored for verbal-visual interleaved generation."
        },
        {
            "title": "6 Discussions",
            "content": "By bridging concepts from human cognition and artificial intelligence, we revisit the mechanisms underlying human reasoning and the central role that world models play. This provides new perspective on the use of visual generation for reasoning in multimodal AI, highlighting its potential to serve as visual world models that complements the verbal world models embedded in LLMs, thereby enabling more human-like reasoning on scenarios grounded in the physical world. For the first time, this perspective is studied in principled manner, through theoretical formulations that bridge world models and reasoning, as well as through empirical evaluations whose results are well explained by and strongly support the proposed insights. We hope this work helps address longstanding questions about the synergistic effects between generation and reasoning, and more broadly contributes to the development of more human-like AI that thinks and acts with multimodal world models. Limitations and future work. This work primarily focuses on spatial and physical reasoning tasks, where multimodal AI systems exhibit pronounced performance gap relative to humans. Many other tasks proposed in the related literature can also be interpreted through our world model perspective. For example, prominent class of benchmarks involves visual jigsaw tasks [17, 38, 52, 77], in which input image patches are cropped, masked, or shuffled. Such tasks essentially probe form of world reconstruction capability, as corrupted images and videos are commonly treated as specific views within the world model literature [3, 4, 7]. Another active area of interest lies in STEM reasoning. Recent work [51] leverages visual generation for mathematical diagram editing, such as constructing auxiliary geometric lines. This closely resembles how humans use visual sketchpads to support math understanding and reasoning, constructing visual world models of symbolic system. However, as symbolic representations in mathematics are largely complete, and mathematical reasoning has been extensively optimized in modern LLMs, it remains unclear whether multimodal interleaved CoT can fundamentally break through the performance limit, warranting further investigation. We do not apply reinforcement learning to the visual generation components of verbalvisual interleaved CoTs [39]. Prior work has shown that world models themselves can be improved through RLVR [65]. As discussed in Section 5.5, developing RL algorithms specifically tailored to interleaved verbalvisual generation may further improve world-model fidelity during reasoning and incentivize the emergence of stronger and intriguing world-modeling capabilities. The analysis of emergent representations for implicit world modeling in Figure 6c is intriguing but preliminary. We hope this result will rekindle interest in probing approaches [37] for interpreting the latent representations learned by different models. In particular, we are interested in comparing the internal representations of VLMs and UMMs, as the latter may capture complementary aspects of world knowledge through training for multimodal generation. Artificial intelligence is increasingly being embodied in the physical world [23]. Our work, particularly the visual superiority hypothesis, suggests that learning visual world models is therefore essential for embodied intelligence. Visual world modeling enables embodied agents to better understand their environments, from imagining occluded regions to interpreting user intentions from an egocentric perspective, thereby supporting more reliable and natural everyday services. It also facilitates planning and decision-making by allowing agents to mentally simulate the precise outcomes of potential actions, leading to more effective interaction with the world. Rather than relying on loosely coupled modules [15] or performing only single-step reasoning [73], we envision future direction in which flexible multimodal world modeling and reasoning, empowered by interleave verbal-visual generation within unified model, form core components of physical and embodied AI. Figure 9 Showcases of interleaved verbal-visual chain-of-thought reasoning, generated by post-trained UMMs, where visual generation serves as world models. <image> denotes placeholder indicating the position of generated image."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Yanwei Li, Rui Yang, Ziyu Zhu, and Feng Cheng for their assistance in constructing some preliminary training and test data. We also appreciate Xinchen Zhang, Jianhua Zhu, Yifan Du, Yuezhou Ma, Xingzhuo Guo, Ningya Feng, Shangchen Miao, and many colleagues for their valuable discussion."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, and François Fleuret. Diffusion for world modeling: Visual details matter in atari. In NeurIPS, 2024. [3] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In CVPR, 2023. [4] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. [5] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, et al. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [7] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471, 2024. [8] Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Xuanke Shi, et al. Has gpt-5 achieved spatial intelligence? an empirical study. arXiv preprint arXiv:2508.13142, 2025. [9] Delong Chen, Theo Moutakanni, Willy Chung, Yejin Bang, Ziwei Ji, Allen Bolourchi, and Pascale Fung. Planning with reasoning using vision language world model. arXiv preprint arXiv:2509.02722, 2025. [10] Jade Copet, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, et al. Cwm: An open-weights llm for research on code generation with world models. arXiv preprint arXiv:2510.02387, 2025. [11] Kenneth James Williams Craik. The nature of explanation, volume 445. CUP Archive, 1967. [12] Google DeepMind. Genie 3: new frontier for world models. 2025. [13] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [14] Yifan Du, Kun Zhou, Yingqian Min, Yue Ling, Wayne Xin Zhao, and Youbin Wu. Revisiting the necessity of lengthy chain-of-thought in vision-centric reasoning generalization. arXiv preprint arXiv:2511.22586, 2025. [15] Yunhai Feng, Jiaming Han, Zhuoran Yang, Xiangyu Yue, Sergey Levine, and Jianlan Luo. Reflective planning: Vision-language models for multi-stage long-horizon robotic manipulation. arXiv preprint arXiv:2502.16707, 2025. [16] Jay Forrester. Counterintuitive behavior of social systems. Theory and decision, 2(2):109140, 1971. [17] Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna, and Yu Cheng. Thinkmorph: Emergent properties in multimodal interleaved chain-of-thought reasoning. arXiv preprint arXiv:2510.27492, 2025. [18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 2025. [19] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [20] Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, Yi Zhang, et al. Rbench-v: primary assessment for visual reasoning models with multi-modal outputs. arXiv preprint arXiv:2505.16770, 2025. 16 [21] Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, and Pheng-Ann Heng. Thinking-while-generating: Interleaving textual reasoning throughout visual generation. arXiv preprint arXiv:2511.16671, 2025. [22] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Rui Huang, Haoquan Zhang, Manyuan Zhang, Jiaming Liu, Shanghang Zhang, Peng Gao, et al. Can we generate images with cot? lets verify and reinforce image generation step by step. In CVPR, 2025. [23] Agrim Gupta, Silvio Savarese, Surya Ganguli, and Li Fei-Fei. Embodied intelligence via learning and evolution. Nature Communications, 2021. [24] David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. [25] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse control tasks through world models. Nature, 2025. [26] Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive control. In ICML, 2022. [27] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In ICML, 2024. [28] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [29] Michael Igorevich Ivanitskiy, Rusheb Shah, Alex F. Spies, Tilman Räuker, Dan Valentine, Can Rager, Lucia Quirke, Chris Mathwin, Guillaume Corlouer, Cecilia Diniz Behn, and Samy Wu Fung. configurable library for generating and manipulating maze datasets. arXiv preprint arXiv:2309.10498, 2023. [30] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017. [31] PN Johnson-Laird. Mental models: Towards cognitive science of language, inference, and consciousness. Harvard University Press, 1983. [32] George Lakoff and Rafael Núñez. Where mathematics comes from, volume 6. New York: Basic Books, 2000. [33] David Landy and Robert Goldstone. How abstract is symbolic thought? Journal of Experimental Psychology: Learning, Memory, and Cognition, 33(4):720, 2007. [34] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1): 162, 2022. [35] Ang Li, Charles Wang, Deqing Fu, Kaiyu Yue, Zikui Cai, Wang Bill Zhu, Ollie Liu, Peng Guo, Willie Neiswanger, Furong Huang, et al. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746, 2025. [36] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. In ICML, 2025. [37] Kenneth Li, Aspen Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring sequence model trained on synthetic task. In ICLR, 2023. [38] Yongyuan Liang, Wei Chow, Feng Li, Ziqiao Ma, Xiyao Wang, Jiageng Mao, Jiuhai Chen, Jiatao Gu, Yue Wang, and Furong Huang. Rover: Benchmarking reciprocal cross-modal reasoning for omnimodal generation. arXiv preprint arXiv:2511.01163, 2025. [39] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. In NeurIPS, 2025. [40] JM Loomis, RL Klatzky, RG Golledge, and JG CicineIli. Mental models, psychology of. Psychology, 14:5689, 1991. [41] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In CVPR, 2025. 17 [42] Ruaridh Mon-Williams, Gen Li, Ran Long, Wenqian Du, and Christopher Lucas. Embodied large language models enable robots to complete complex tasks in unpredictable environments. Nature Machine Intelligence, 2025. [43] Merlin Monzel and Martin Reuter. Wheres wanda? the influence of visual imagery vividness on visual search speed measured by means of hidden object pictures. Attention, Perception, & Psychophysics, 86(1):2227, 2024. [44] Donald Norman. Some observations on mental models. In Mental models, pages 714. Psychology Press, 2014. [45] Allan Paivio. Mental representations: dual coding approach. Oxford university press, 1990. [46] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [48] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 2020. [49] Luca Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in multimodal large language models. Nature Machine Intelligence, 2025. [50] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. [51] Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, et al. Mathcanvas: Intrinsic visual chain-of-thought for multimodal mathematical reasoning. arXiv preprint arXiv:2510.14958, 2025. [52] Yang Shi, Yuhao Dong, Yue Ding, Yuran Wang, Xuanyu Zhu, Sheng Zhou, Wenting Liu, Haochen Tian, Rundong Wang, Huanqian Wang, et al. Realunify: Do unified models truly benefit from unification? comprehensive benchmark. arXiv preprint arXiv:2509.24897, 2025. [53] Kyle Swanson, Wesley Wu, Nash Bulaong, John Pak, and James Zou. The virtual lab of ai agents designs new sars-cov-2 nanobodies. Nature, 2025. [54] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [55] Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, et al. Game-rl: Synthesizing multimodal verifiable game data to boost vlms general reasoning. arXiv preprint arXiv:2505.13886, 2025. [56] Shengbang Tong, David Fan, Jiachen Li, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. In ICCV, 2025. [57] Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. [58] Tao Tu, Mike Schaekermann, Anil Palepu, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Yong Cheng, et al. Towards conversational diagnostic artificial intelligence. Nature, 2025. [59] Kangrui Wang, Pingyue Zhang, Zihan Wang, Yaning Gao, Linjie Li, Qineng Wang, Hanyang Chen, Chi Wan, Yiping Lu, Zhengyuan Yang, et al. Vagen: Reinforcing world model reasoning for multi-turn vlm agents. In NeurIPS, 2025. [60] Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Côté, Peter Clark, and Peter Jansen. Can language models serve as text-based world simulators? In ACL, 2024. [61] Siting Wang, Luoyang Sun, Cheng Deng, Kun Shao, Minnan Pei, Zheng Tian, Haifeng Zhang, and Jun Wang. Spatialviz-bench: Automatically generated spatial visualization reasoning tasks for mllms. arXiv preprint arXiv:2507.07610, 2025. 18 [62] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [63] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In CVPR, 2025. [64] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. In NeurIPS, 2024. [65] Jialong Wu, Shaofeng Yin, Ningya Feng, and Mingsheng Long. Rlvr-world: Training world models with reinforcement learning. In NeurIPS, 2025. [66] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In ICLR, 2025. [67] Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan Vulić. Visual planning: Lets think only with images. arXiv preprint arXiv:2505.11409, 2025. [68] Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, et al. Visual spatial tuning. arXiv preprint arXiv:2511.05491, 2025. [69] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: benchmark for multi-image spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. [70] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Chi Chen, Haoyu Li, Weilin Zhao, et al. Efficient gpt-4v level multimodal large language model for deployment on edge devices. Nature Communications, 2025. [71] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. In Structural Priors for Vision Workshop at ICCV25, 2025. [72] Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, et al. Agent learning via early experience. arXiv preprint arXiv:2510.08558, 2025. [73] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In CVPR, 2025. [74] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. In NeurIPS, 2025. [75] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In ICLR, 2025. [76] Yiyang Zhou, Haoqin Tu, Zijun Wang, Zeyu Wang, Niklas Muennighoff, Fan Nie, Yejin Choi, James Zou, Chaorui Deng, Shen Yan, et al. When visualizing is the first step to reasoning: Mira, benchmark for visual chain-of-thought. arXiv preprint arXiv:2511.02779, 2025. [77] Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, and Ziwei Liu. Uni-mmmu: massive multi-discipline multimodal unified benchmark. arXiv preprint arXiv:2510.13759, 2025."
        },
        {
            "title": "A Theorectical Analysis",
            "content": "A."
        },
        {
            "title": "Informativeness",
            "content": "In this section, we provide the rigorous version of our world model-based chain-of-thought formulations, and proofs for Theorem 1 and Theorem 2. Formal problem setup and assumptions. Given question and input images I, multimodal reasoning generates chain-of-thought process R, followed by final answer A. We explicitly formulate the reasoning process as an interleaving of logic reasoning steps and observations of the underlying MOMDP defined in Section 3.1: = (r1, o1), (r2, o2), . . . , (rH , oH ) where denotes the (fixed) CoT length. For notation convenience, we denote the input image(s) as the initial observation o0. We assume that each MOMDP observation function admits two-stage decomposition: eϕ(s) = gϕm (fϕs(s)) , Φ = Φs Φm, where the inner modality-agnostic mapping fϕs (parameterized by ϕs Φs) extracts slice of the underlying state s, retaining only partial state information, and the outer modality-specific mapping gϕm (parameterized by ϕm Φm) renders the extracted slice into particular observation modality. Under this decomposition, we assume that reasoning across different modalities of observations shares common underlying oracle reasoning process: p(Q, s0, r1, s1, . . . , rH , sH , A) = p(Q) (cid:34) (cid:89) i=1 p(ris0:i 1, r1:i 1, Q)p(sis0:i 1, r1:i, Q) p(Ar1:H , s0:H , Q), (cid:35) where si = (si, ϕsi) Φs denotes modality-agnostic sliced state. Each logic step ri is assumed to reason (cid:17) on sufficient sliced state information: p(ri s0:i , 1, r1:i and produces actionable outcomes that either (i) transit previous world state sj<i via an implicit action ai: si = (si, ϕsj), si p(sj, ai) or (ii) query the same underlying world state with new slice ϕsi, yielding (cid:0)fϕsi(si)(cid:1) . si = (sj, ϕsi) The oracle reasoning process is then rendered into specific modality via oi = gϕm Unless otherwise specified, we abuse notation and use si to denote si = (si, ϕsi) in the remainder of our analysis. ri fϕs0 (s0), . . . , fϕsi1 (si 1, Q) = 1), r1:i 1, (cid:16) Given the above oracle CoT generation process, we learn model pθ whose joint distribution over CoTs and answers factorizes into reasoning component and world-modeling component: pθ(R, AQ, I) = pθ(r1, o1, r2, o2, . . . , rH , oH , rH+1r0, o0) = H+1 (cid:89) i= pθ(riRi) (cid:89) i=1 pθ(oi Ri), (8) where we denote the question as r0, the initial observation (input image(s)) as o0, and the final answer as rH+1. The CoT prefixes are defined as Ri = (r0, o0, r1, o1, . . . , ri 1), Ri = (r0, o0, r1, o1, . . . , ri 1, ri). 1, oi 1, oi Proofs. We provide proofs of Theorem 1 and Theorem 2 below. Theorem 3 (Restatement of Theorem 1). For any observation modality m, the following inequality holds: KL(p(A Q, I) pθ(A Q, I)) KL(p(R, Q, I) pθ(R, Q, I)) (cid:88) H+1 (cid:88) = i=1 Ep [KL(p(riRi) pθ(riRi))] (cid:124) (cid:123)(cid:122) (cid:125) reasoning errors + i=1 Ep (cid:124) (cid:105) (cid:104) KL(p(oi Ri) pθ(oi Ri)) (cid:123)(cid:122) world modeling errors (cid:125) . (9) Proof. The first inequality follows from the data processing inequality: marginalizing out cannot increase the KL divergence. For the equality, we apply the chain rule for KL divergence together with the CoT factorization in Eq. (8). In particular, substituting the factorizations of p(R, Q, I) and pθ(R, Q, I) into KL(p(R, Q, I) pθ(R, Q, I)) leads to the stated decomposition. 20 Theorem 4 (Restatement of Theorem 2). For any observation modality m, the reduction in reasoning uncertainty achieved by explicit world modeling satisfies: 1. Reasoning uncertainty does not increase: H(ri o0, r0:i 1) H(riRi) = I(o1:i 1; ri o0, r0:i 1) 0. 2. Uncertainty reduction is upper-bounded by both (i) the information that observations provide about the underlying states and (ii) the information that the reasoning step requires about those states: I(o1:i 1; ri o0, r0:i 1) min (I(o1:i 1; s1:i 1), I(ri; s0:i 1, r0:i 1)) . (10) Proof. The first property follows the definition and the non-negativity of mutual information. For the second property, denote the conditioning context as = (o0, r0:i mutual information: I(X; ; Z) = I(X; ) I(X; Z). we obtain 1). Using the properties of ternary I(o1:i 1; ri C) = I(o1:i = I(o1:i 1; ri C) I(o1:i 1; s1:i 1 C) I(o1:i 1; ri s1:i 1; s1:i 1, C) = I(s1:i 1 ri, C) I(o1:i 1, C) = 0 follows from the conditional independence ri o1:i 1; o1:i 1; ri C) 1 C), 1; s1:i 1 s1:i 1. where I(o1:i 1; ris1:i (11) Further, due to as the deterministic function of s, we have: I(o1:i 1; s1:i 1 C) = H(o1:i H(o1:i 1 C) H(o1:i 1) H(o1:i 1 s1:i 1 s1:i 1, C) 1) = I(o1:i 1; s1:i 1), where H(o1:i 1 s1:i Symmetrically, we have: 1) = H(o1:i 1 s1:i 1, C) = 0. I(o1:i 1; riC) = I(s1:i 1; o1:i H(ri) H(ris0:i 1; ri C) I(s1:i 1, r0:i 1; riC) = H(riC) H(ris1:i 1), 1, r0:i 1) = I(ri; s0:i 1, C) where H(ris0:i 1, r0:i 1) H(ris1:i 1, o0, r0:i 1) due to data processing inequality. Combining the two upper bounds proves Eq. (10). Corollary 1. If observations are fully informative about the underlying states, i.e., H(si oi) = 0 for all i, and the state transition dynamics are deterministic, then explicit world modeling provides no reduction in reasoning uncertainty: I(o1:i 1; ri o0, r0:i 1) = 0. Proof. By Eq. (11), we have I(o1:i 1; ri o0, r0:i 1) I(o1:i 1; s1:i 1 o0, r0:i 1) H(s1:i 1 o0, r0:i 1). Under the assumption H(s0 o0) = 0, the initial observation o0 uniquely determines s0. Moreover, under deterministic state transitions, the trajectory s1:i 1 is uniquely determined by (s0, r1:i 1). Hence, Therefore, I(o1:i 1; ri o0, r1:i 1) = H(s1:i H(s1:i 1 o0, r1:i 1) = 0, which proves the corollary. 1 s0, r1:i 1) = 0. Remarks. Corollary 1 shows that in deterministic and fully observable environments, given sufficient data and model capacity, explicit world modeling provides no additional benefit. This theoretical result is consistent with our empirical findings on the simple maze task. A.2 Prior Knowledge In this section, we first derive generalization bound for transfer learning under distribution shift, and relate it to our perspective on prior knowledge in multimodal reasoning. A.2.1 General Transfer Learning Analysis Problem setup. standard transfer learning setup involves pre-training data distribution and the fine-tuning data distribution over samples (x, y) Y, and loss function ℓθ(x, y) [0, 1]. Define the population risks LD(θ) := Θ LD(θ), {P, Q} We assume we can obtain θP as the pre-trained model given sufficient data arg minθ and optimization. For radius > 0, we then define the fine-tuning constraint set (local neighborhood around the pre-trained model) D[ℓθ(x, y)], {P, Q}, and the population minimizers θ (x,y) Θr := {θ Θ : θ θP r}. Given i.i.d. samples from Q: = {(xi, yi)n (cid:80)n over Θr, (cid:98)LQ(θ) := 1 i=1} , (xi, yi) Q, the fine-tuned model θQ minimize empirical risk Q). From distribution shift to parameter drift. We first derive how the distribution shift relates to the shift of the population minimizer. i=1[ℓθ(xi, yi)]. Our analysis focus on the excess risk on Q: EQ(θQ) := LQ(θQ)LQ(θ Lemma 1 (Uniform Loss Shift under Total Variation). For any subset Θ, (cid:12)LQ(θ) LP (θ)(cid:12) (cid:12) (cid:12) TV(P, Q). sup θ Proof. Fix any θ and define fθ(h, a, o) := ℓθ(h, a, o) [0, 1]. By the definition of total variation and the standard inequality for bounded functions, (cid:12) (cid:12)EQ[fθ] EP [fθ](cid:12) (cid:12) TV(P, Q). Taking the supremum over θ yields the claim. Lemma 2 (Risk Proximity of θ under ). LP (θ Q) LP (θ ) + 2TV(P, Q). (12) Proof. By Lemma 1, LP (θ Lemma 1 again, LQ(θ Q) LQ(θ Q) + TV(P, Q). By optimality of θ on Q, LQ(θ Q) LQ(θ ). Applying ) LP (θ ) + TV(P, Q). Chaining the three inequalities proves (12). Assumption 1 (Local Quadratic Growth / Sharpness of LP ). There exists µ > 0 such that for all θ in neighborhood containing θ Q, LP (θ) LP (θ ) + θ θ 2. µ 2 Lemma 3 (Parameter Drift Controlled by TV(P, Q)). Under Assumption 1, θ θ (cid:114) 4 µ TV(P, Q). (13) Proof. By Assumption 1 with θ = θ LP (θ Q, LP (θ ). Applying Lemma 2 yields µ Q) LP (θ Q) LP (θ θ 2 θ ) + µ 2 θ θ 2. Rearranging, µ 2 θ θ 2 2 2TV(P, Q), and hence (13). Control of the bias term. Recall the fine-tuning bias induced by restricting to Θr: εbias(r) := inf θ LQ(θ Q). Θr LQ(θ) Assumption 2 (LQ is Locally Lipschitz). There exists LQ > 0 such that for all θ, θ Θr, LQ(θ) LQ(θ) LQθ θ. Theorem 5 (Bias Bound via Distribution Shift). Under Assumption 1 and Assumption 2, εbias(r) LQ (cid:18)(cid:114) 4 µ TV(P, Q) (cid:19) , + (14) where (x)+ := max{x, 0}. In particular, if (cid:113) 4 µ TV(P, Q), then εbias(r) = 0. Proof. If θ θ , then θ Θr and thus inf θ Θr LQ(θ) LQ(θ Q), implying εbias(r) = 0. Now consider < θ θ Then θr Θr and θr θ . Let θr be the projection of θ = θ r. Therefore, θ onto the closed ball Θr, i.e., θr := θP + θ θ θ θ . εbias(r) = inf Θr θ LQ(θ) LQ(θ Q) LQ(θr) LQ(θ Q) LQθr θ = LQ(θ θ r). Using Lemma 3 to bound θ θP completes the proof of (14). Fine-tuning excess risk bound. We then arrive at the final result: Theorem 6 (Fine-tuning Excess Risk Bound with Shift-Controlled Bias). Assume Assumptions 1 and 2 and uniform convergence over Θr holds: with probability at least 1 δ over samples S, (cid:12)LQ(θ) (cid:98)LQ(θ)(cid:12) (cid:12) (cid:12) εgen = sup Θr θ (cid:32)(cid:114) RadQ,n(Θr) + log(1/δ) (cid:33) , where RadQ,n(Θr) is the Rademacher complexity of of the function class {ℓθ, θ Θr} with respect to for sample size n. Then with probability at least 1 δ, EQ(θQ) 2εgen + LQ (cid:18)(cid:114) 4 µ TV(P, Q) (cid:19) . + (15) Proof. Decompose the excess risk as EQ(θQ) = Θr LQ(θ) by standard ERM argument using uniform convergence: LQ(θQ) inf θ is bounded by Lemma 5. Combining the two bounds yields (15). LQ(θQ) inf θ (cid:16) (cid:17) + εbias(r). The first term is bounded Θr LQ(θ) 2εgen. The second term A.2.2 Remarks on Multimodal Reasoning Theorem 6 reveals trade-off between modality complexity and distribution shift. This general transfer learning analysis can be instantiated in our setting of learning world models and reasoning policies. Specifically, training pairs (x, y) can be instantiated as ((o0:i, r0:i+1), oi+1) for world modeling and ((o0:i, r0:i), ri) for reasoning, respectively. Crucially, the distribution shift between large-scale pre-training data and downstream tasks may differ substantially across modalities. For example, there are abundant visual demonstrations of paper folding on the Internet, whereas detailed verbal descriptions of folding dynamics are comparatively scarce. This suggests that downstream tasks should be formulated under the most appropriate observation modality for world modeling and reasoningi.e., the modality that best aligns with pre-training datain order to achieve stronger generalization at inference time and higher sample efficiency during post-training."
        },
        {
            "title": "B Experiment Details",
            "content": "B.1 VisWorld-Eval and Training Data In this section, we elaborate on the construction of training and test data for each task in VisWorld-Eval. Paper folding. This task involves folding paper grid with varying grid sizes (38) and folding steps (14). After folding, holes of different shapescircles, triangles, stars, diamonds, and squaresare punched into the paper. The model is then asked to predict the distribution of holes after the paper is completely unfolded, including queries such as the total number of holes, the number of holes of specific shape, or the difference in counts between shapes. All test prompts are constructed at the highest difficulty level (grid size 8 with 4 folding steps). For SFT, we generate chain-of-thoughts using rule-based templates that follow fixed procedure: unfold the paper step-by-step and then count the resulting holes by shape. These CoTs are then rewritten with Gemini 2.5 Pro to improve clarity and logical coherence. Under visual world modeling, we interleave reasoning steps with images of partially unfolded paper states. Under verbal world modeling, we 23 represent intermediate states using two matrices encoding grid coverage status and hole shape at each position. Under implicit world modeling, we directly skip the explicit tracking of states from original CoTs. Multi-hop manipulation. This task begins with an initial arrangement of several geometric objects (cubes, spheres, and cylinders) in various colors, rendered by Blender4. sequence of text-based instructions is then provided, describing operations such as changing or swapping objects color or shape, adding new objects, or removing existing ones. To ensure these commands can be interpreted unambiguously in 3D space, the instructions consistently use relative spatial references, with each object uniquely identified by its combined color and shape attributesfor example: \"Place purple cylinder between the black sphere and the yellow cube.\" The model is asked to infer the resulting spatial layout. Queries may include the total number of objects of specific shape, the directional relationship between two objects, or which object lies in given direction relative to reference object. Test prompts are constructed by varying both the number of initial objects (between 3 and 6) and the frequency (between 1 and 5) of different operation types. For SFT, chain-of-thought reasoning is generated using rule-based templates that simulate the stepwise execution of instructions before answering the final query, and these CoTs are subsequently refined with Gemini 2.5 Pro. Ball tracking. This task features red point-mass ball that moves at constant speed, reflects elastically off solid walls, and travels in the initial direction indicated by green arrow. The model is asked to predict which numbered hole at the top of the image the ball will enter first. We generate input images with randomized resolution, initial ball position and direction, and random number of holes (48). For test prompts, we select cases in which the ball trajectory reflects off at least one wall before entering hole. For SFT, CoTs are generated by Seed 1.6, which is asked to explain the ball dynamics between adjacent frames. Sokoban. Sokoban is classic grid-based puzzle game. We generate instances with grid sizes ranging from 6 to 10, containing single box and target position. Test prompts are sampled from the same distribution as the training data. To construct CoTs, we use search algorithm to compute an optimal solution path. To avoid excessively long trajectories, we render only key intermediate steps, including: (i) the player moving toward the box, (ii) pushing the box in direction, and (iii) changing the pushing direction. To encourage reflective behavior, we additionally augment trajectories with randomized detours that involve walking into walls, reflecting, and backtracking to rejoin the optimal path. CoTs are generated by Seed 1.6, which explains the dynamics between adjacent frames. For visual world modeling, the rendered intermediate steps are interleaved with verbal CoTs. For pure verbal world modeling, these intermediate renderings are removed. For implicit world modeling, we additionally mask all explicit coordinates during CoTs with special tokens [masked]. Maze. Maze is classic grid-based puzzle task. We generate both training and test samples with fixed grid size of 5 5. To construct CoTs, we use rule-based templates followed by rewriting for improved naturalness. Under visual world modeling, rendered intermediate steps through points and lines are interleaved with verbal CoTs. The settings for verbal and implicit world modeling follow the same protocol as in Sokoban, with masking special tokens as <point>[masked]</point>. Cube 3-view projection. This task considers stacks of colored cubes arranged on grids of varying sizes (35), with two cube colors. The input consists of one isometric view (either front-left or front-right) and two orthographic views of the stack. The question asks for the number of cubes of specified color visible from another orthogonal view. Both the questions and answer choices account for ambiguity caused by occlusions, leading to uncertainty in the cube count. All test prompts are constructed using uniformly random grid sizes between 3 and 5. We generate CoTs using rule-based templates: the model first constructs the queried view, marks potentially occluded cubes using third (auxiliary) color, and then counts cubes by color. These CoTs are subsequently rewritten by Gemini 2.5 Pro for improved naturalness. Under visual world modeling, we interleave reasoning steps with an image of the queried view. Under verbal world modeling, we represent intermediate views using character matrices, where different colors are encoded by different symbols. Real-world spatial reasoning. For this real-world task, we directly adopt test samples from MMSI-Bench, focusing on cameraobject and cameraregion positional relationship questions. We construct training prompts following pipeline similar to Yang et al. [68]. To obtain training CoTs, we run visual-CoT model, which uses 4https://www.blender.org/ 24 an SFT-trained BAGEL model for novel view synthesis as tool. The resulting visual CoTs are subsequently filtered and rewritten by Gemini 2.5 Pro. We summarize the training and test sample counts for each task in VisWorld-Eval, along with the corresponding original or referenced benchmarks, in Table 2. Table 2 Overview of VisWorld-Eval and corresponding training data: features, statistics, and references."
        },
        {
            "title": "Test\nSamples",
            "content": "Source/Reference Synthetic Paper folding Synthetic Multi-hop manipulation Synthetic Ball tracking Synthetic Maze Synthetic Sokoban Cube 3-view projection Synthetic Real-world spatial reasoning Reconstruction Real-world"
        },
        {
            "title": "Simulation\nSimulation\nSimulation\nSimulation\nSimulation\nReconstruction",
            "content": "2,357 2,000 2,254 8,448 7,715 2,500 10,661 480 480 1,024 480 480 480 522 SpatialViz [61] ZebraCoT [35], CLEVR [30] RBench-V [20] maze-dataset [29] GameRL [55] SpatialViz [61] MMSI-Bench [69] Examples of training CoTs are presented in Figure 10, 11, 12, 13, and 14. B.2 Model Training We perform supervised fine-tuning (SFT) of BAGEL based on its official repository5, using 8 GPUs, and conduct reinforcement learning from verifiable rewards (RLVR) using verl6 on 64 GPUs. Hyperparameters for SFT and RLVR are reported in Table 3 and Table 4, respectively. Table 3 Hyperparameters for supervised fine-tuning UMMs."
        },
        {
            "title": "Hyperparameter",
            "content": "Learning rate LR Schedule Optimizer Loss weight (CE:MSE) Warm-up steps Training steps Gen. resolution"
        },
        {
            "title": "Value",
            "content": "5 3 10 Constant AdamW 1:10 200 4000 (256, 1024) for paper folding, cube 3-view (240, 1024) for multi-hop manipulation (256, 512) otherwise Und. resolution Sequence length per rank Num. ranks (224, 980) 32K 8 Table 4 Hyperparameters for reinforcement learning UMMs."
        },
        {
            "title": "Hyperparameter",
            "content": "Learning rate Batch size GRPO mini batch size Group size KL loss coefficient for visual gen. KL loss coefficient for verbal gen."
        },
        {
            "title": "Value",
            "content": "5 1 10 128 32 16 0.1 0.0 5https://github.com/ByteDance-Seed/Bagel 6https://github.com/volcengine/verl 25 Figure 10 Examples of chain-of-thought SFT data for the paper folding task, under visual world modeling (left) and verbal world modeling (right). Figure 11 Examples of chain-of-thought SFT data for the ball tracking and multi-hop manipulation task. 27 Figure 12 Examples of chain-of-thought SFT data for the maze and sokoban task. 28 Figure 13 Examples of chain-of-thought SFT data for the cube 3-view projection task, under visual world modeling (left) and verbal world modeling (right). Figure 14 Examples of chain-of-thought SFT data for the real-world spatial reasoning task. 30 The Qwen-VL baselines are trained using LLaMA-Factory7 for supervised fine-tuning (SFT) and verl for reinforcement learning from verifiable rewards (RLVR). B.3 Analytic Experiments Sample efficiency. For Figure 6a, we randomly subsample either 500 or 1000 training examples. The resulting models are evaluated under two settings: (i) hard setting with the maximum difficulty (grid size 8 and 4 folding steps, default in VisWorld-Eval), and (ii) an in-distribution setting (denoted as Normal in the figure) with randomly sampled grid sizes (38) and folding steps (14). Task difficulties and world model fidelity. For Figure 6b, we generate test samples with varying cube-stack sizes (36), where size 6 is out-of-distribution relative to the training data. To assess world-model fidelity, we compare the generated views with the ground-truth views: for verbal world modeling, we use string pattern matching; for visual world modeling, we use Gemini 3 Pro to compare images. Since accurately inferring colors becomes particularly challenging at larger stack sizes, we evaluate only the shapes of the views and ignore color information. We also find that overall accuracy can be bottlenecked by verbal subskills (e.g., counting holes) after SFT, thus we report the accuracy of RL-trained models in Figure 6b. In contrast, RL can distract verbal world modeling capabilities, leading to invalid formats of generated symbolic matrices, thus we report world-model fidelity of SFT-trained models. Implicit world modeling. For Figure 6c, we supervised fine-tune (SFT) BAGEL on CoTs with implicit world modeling, in which all explicit point coordinates are replaced by the placeholder token sequence <point>masked<point>. After training, we extract the hidden representations at the position of the token masked from each transformer layer. We then split the extracted representations from different CoTs into training and validation sets with an 8:2 ratio and train two-layer MLP (hidden size 4096) to predict the ground-truth point coordinates. Since all samples are 5 5 mazes, we formulate coordinate prediction as two 5-way classification tasks (for and y, respectively). We compute classification accuracy for each coordinate and report the average of the two."
        },
        {
            "title": "C Extended Experimental Results",
            "content": "C.1 Full Results on MMSI-Bench We report all scores on positional relationship tasks of MMSI-Bench in Table 5. Table 5 Full results of SFT-trained UMMs on MMSI-Bench positional relationship tasks."
        },
        {
            "title": "Models",
            "content": "MMSI-Bench (Positional Relationship) Cam.-Cam. Obj.Obj. Reg.Reg. Cam.Obj. Obj.Reg. Cam.Reg. Overall"
        },
        {
            "title": "Implicit WM\nVisual WM",
            "content": "33.1 29.6 31.2 29.5 31.8 31.6 46.5 60.9 29.1 25.8 37.3 54. 34.8 38.4 C.2 Additional Qualitative Evaluation We provide additional qualitative evaluation of trained UMMs reasoning, particularly failure cases. Real-world spatial reasoning. As shown in Figure 15a, reasoning with implicit world modeling is prone to hallucinations. In contrast, visual generation (Figure 15b) yields more faithful world models, but still suffers from insufficient quality, including blurring and corrupted details. Moreover, we find that current VLMs and UMMs continue to exhibit limited understanding of positions and directions across different viewpoints. We expect that stronger base models and better-curated post-training data will enable more effective use of visual world models for spatial reasoning in future work. 7https://github.com/hiyouga/LLaMA-Factory Figure 15 Showcases of reasoning generateed by post-trained UMMs in the real-world spatial reasoning task. We highlight hallucinations or incorrect reasoning steps in red. Paper folding. As illustrated in Figure 16, verbal reasoning about geometric symmetry is prone to hallucinations, leading to inaccurate verbal world modeling. In contrast, visual world models, benefiting from stronger prior knowledge, generate correct intermediate unfolding steps even in the presence of erroneous verbal reasoning. Cube 3-view projection. As shown in Figure 17, visual world models are able to approximately generate novel views of cube stacks even in the challenging out-of-distribution setting with an unseen stack size of 6, indicating strong prior knowledge of spatial transformations. Nevertheless, overall task performance remains limited by subtle shape-generation errors (Figure 17b,d) and inaccurate color inference (Figure 17c). We expect these issues to be alleviated through improved post-training and stronger base models. 32 Figure 16 Showcases of reasoning generated by post-trained UMMs in the paper folding task. We highlight hallucinations or incorrect reasoning steps in red, but also mark correctly generated visual unfolding intermediate steps with green borders. Figure 17 Showcases of reasoning generated by post-trained UMMs in the paper folding task. We mark correct and incorrect generated cube views with green and red borders, respectively. For incorrect generations, the corresponding ground-truth views are provided for reference (note that these are shown only for readers and are never provided to the models during reasoning)."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Tsinghua University"
    ]
}