{
    "paper_title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward",
    "authors": [
        "Shudong Liu",
        "Hongwei Liu",
        "Junnan Liu",
        "Linchen Xiao",
        "Songyang Gao",
        "Chengqi Lyu",
        "Yuzhe Gu",
        "Wenwei Zhang",
        "Derek F. Wong",
        "Songyang Zhang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier."
        },
        {
            "title": "Start",
            "content": "Preprint. CompassVerifier: Unified and Robust Verifier for LLMs Evaluation and Outcome Reward Shudong Liu1,2,, Hongwei Liu1,, Junnan Liu1, Linchen Xiao1, Songyang Gao1, Chengqi Lyu1, Yuzhe Gu1, Wenwei Zhang1, Derek F. Wong2,, Songyang Zhang1,,ℵ, Kai Chen1, Shanghai AI Laboratory NLP2CT Lab, University of Macau 1 5 2 0 2 5 ] . [ 1 6 8 6 3 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of meta error patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier."
        },
        {
            "title": "Introduction",
            "content": "Answer verification plays critical role in the evaluation and training of large language models (LLMs), particularly for objective questions with verifiable answers (Achiam et al., 2023; Yang et al., 2024; Liu et al., 2024a;b). At the evaluation level, it enables precise measurement of performance differences across models (Chang et al., 2024); at the training level, it serves as quality check for self-improvement (Hosseini et al., 2024; Song et al., 2025). With the rapid development of large reasoning models (LRMs) and reinforcement learning (RL), answer verification has further become key component in constructing rule-based rewards, providing feedback signals to guide model optimization and iteration (Guo et al., 2025; OpenAI, 2024c; Luong et al., 2024; Wang et al., 2025a; Zhong et al., 2025). Existing answer verification methods can be broadly categorized into two types. The first type relies on regularized string matching, such as extracting content following The answer is to compare with reference answers, or using tools like math-verify (huggingface, 2024) to check formula equivalence in mathematical tasks. The second type employs general LLMs for consistency judgment, where specific prompt is designed to instruct the model to evaluate the alignment between candidate and reference answers. However, both approaches Equal contribution. ℵProject lead. Corresponding authors. Work done during Shudongs internship at Shanghai AI Laboratory. Email: nlp2ct.shudong@gmail.com; {liuhongwei,zhangsongyang}@pjlab.org.cn 1 Preprint. Figure 1: Overview of VerifierBench pipeline. Using OpenCompass (OC-Contributors, 2023), we collected more than 1 million LLM responses, applying multi-stage, multi-model verification with tool-assisted cleaning and filtering to create VerifierBenchs test/base training sets and catalog common verification error patterns. suffer from significant limitations: the former requires repetitive customization of matching rules for different tasks and is prone to verification failures due to extraction errors; the latter demands frequent prompt adjustments to accommodate diverse tasks, domains, and answer types, while also facing the risk of misjudgment caused by model hallucination. Meanwhile, there is still no challenging benchmark available to evaluate and distinguish the verification capabilities of different models, nor to guide the development and iteration of verifiers. In this paper, we establish systematic framework for evaluating and training answer verification systems. We first introduce VerifierBench, challenging benchmark for answer verification that aggregates numerous samples where rule-based methods frequently err or LLMs tend to produce incorrect judgments or hallucinations. We integrated over one million data samples through the OpenCompass (OC-Contributors, 2023) evaluation framework, encompassing responses from more than 50 models across 15 carefully selected datasets. Following large-scale data collection, each sample underwent multi-stage filtering pipeline culminating in rigorous domain expert review and calibration. VerifierBench facilitates precise measurement of verification capabilities across diverse models, addressing complex scenarios where both rule-based matching and general models often fail, and offering manually analyzed summaries of prevalent error patterns. We further present CompassVerifier, series of lightweight yet robust and accurate verification models. The training data originates from three key sources: 1) The original training set from VerifierBench, which undergoes multi-model validation with simple, easily verifiable samples removed; 2) Formula-enhanced data, where we leverage the powerful DeepSeek-V3 model to generate numerous equivalent complex formulas with corresponding reasoning processes to improve formulaic answer evaluation; 3) Hallucination-specific data, where we systematically analyze failure patterns from human validation cases and synthesize targeted training samples to address common hallucination errors. Our contributions are threefold: We propose VerifierBench, novel and challenging benchmark meticulously designed for fine-grained evaluation of verification abilities. We develop CompassVerifier, series of robust and efficient verification models enhanced through our three proposed techniques, achieving state-of-the-art performance across diverse domains and tasks. CompassVerifier can also effectively serve as reward model in RL training, delivering more precise and reliable feedback signals for policy optimization. 2 Preprint. Through systematic analysis of prevalent failure modes in LLM-based verification, including characteristic hallucination phenomena and error propagation, we derive actionable insights aimed at advancing the design and robustness of future verification systems."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Answer Verification Unlike traditional discriminative models with well-defined classification labels, the unstructured outputs of generative LLMs pose unique verification challenges (Cobbe et al., 2021). Current approaches to verifying LLM-generated answers can be broadly categorized into outcome verification and process verification (Kawabata & Sugawara, 2024; Zhang et al., 2025). Outcome verification focuses on assessing the correctness of final answers, typically through string-based pattern matching (OC-Contributors, 2023; Gao et al., 2024; OpenAI, 2023). Common practice instructs LLMs to output answers in predefined formats for characterlevel comparison with ground truth. For formulaic answers, specialized tools like MathVerify (huggingface, 2024) have been developed to handle equivalence checking. However, due to the inherent unpredictability of LLM outputs, such methods often suffer from matching failures or inaccuracies. Many studies thus employ general LLMs as verifiers via tailored prompts. While effective, both methods demand task-specific customization through either regex patterns or verified prompts, creating labor-intensive workflows. Process verification, requiring detection of reasoning errors in intermediate steps, has seen recent advances in both LLM-based verifiers and evaluation benchmarks (Lu et al., 2024; o1 Team, 2024; Lightman et al., 2023; Zheng et al., 2024; Zhou et al., 2024). However, process verifiers remain less frequently adopted in evaluations due to instability and high resource costs, and have not demonstrated substantially superior performance compared to outcome verifiers in RL. We focus on scalable and robust outcome verification by developing unified verifier that serves dual purposes: 1) as an evaluation model for benchmarking model performance, and 2) as real-time reward model for RL training. By addressing the limitations of existing methods, such as ad-hoc prompt engineering and brittleness to output variations, CompassVerifier prioritizes efficiency, generalizability, and reliability across diverse tasks. 2.2 LLM-as-a-Judge The comprehensive capabilities of LLMs enable them to serve as cost-effective alternatives to human experts in evaluation tasks, concept known as LLM-as-a-Judge (Gu et al., 2024; Li et al., 2024a), which can be categorized into two approaches: subjective judgment and objective judgment. Subjective judgment typically operates in scenarios without ground-truth answers, where LLMs score individual responses (Pointwise) (Zhu et al., 2025) or express preferences between paired responses (Pairwise) (Wang et al., 2024a). This requires the LLM to evaluate various aspects of responses, including usefulness, harmlessness, and creativity, and even identify reasoning stepwise errors in the responses (Cao et al., 2024; Li et al., 2024c; 2023). Recent studies also employ RL and inference-time scaling like generative critiques, long-CoT, and multi-sampling voting for judgment, albeit with high computational costs (Liu et al., 2025; Shi & Jin, 2025). objective judgment is more straightforward approach, requiring only the evaluation of response correctness against ground-truth. Beyond simple string matching, the prevalent method employs large-scale LLMs with carefully designed evaluation prompts for judgment. Recently, to enable smaller models to achieve comparable verification capabilities to large LLMs, Chen et al. (2025) proposes xVerify and its accompanying benchmark, which trains smaller verifier models by distilling GPT-4os capabilities. Other concurrent studies have also focused on distilling verification capabilities from large models to smaller ones to achieve better cost-effectiveness (Ma et al., 2025; Su et al., 2025). 3 Preprint. We claim that objective judgment with ground-truth has yet to reach maturity, lacking both challenging benchmarks to discriminate model abilities and robust unified models. To address these gaps, we are committed to developing VerifierBench to rigorously test different models verification capabilities and CompassVerifier to provide the research community with an accurate evaluation tool."
        },
        {
            "title": "3 VerifierBench",
            "content": "The primary challenge in verifier development lies in the lack of comprehensive benchmarks and rigorous evaluation methodologies. Large-scale commercial models are often preferred for answer-matching tasks due to the prevailing assumption of scaling laws. However, critical questions remain unanswered: 1) To what extent do answer matching and objective judgment tasks adhere to scaling laws? 2) How should we balance model performance against computational costs in verification? To answer these questions, in this work, we present VerifierBench, systematic benchmark for evaluating diverse models judgment and verification capabilities. VerifierBench addresses this gap through: 1) Large-scale data collection for answer matching (3.1); 2) Multi-round validation involving multiple LLMs and human annotators (3.2); 3) Case analysis of typical error patterns to identify failure modes (3.3). 3.1 Data Collection The crux of the answer verification task hinges on its capacity to encompass comprehensive range of verifiable answer types and heterogeneous model responses. To comprehensively gather such data, we employed the OpenCompass framework (OC-Contributors, 2023) to conduct large-scale evaluations across multiple models and datasets. Our systematic approach yielded more than 1,325,293 samples covering four key domains: knowledge, mathematics, science, and general reasoning. The collected data features: Answer Type Diversity: Multiple response formats including multiple-choice question options, mathematical formulations, short texts, multi-subproblem items, and long-sequence responses, etc. Prompt Variability: Input prompts covering few-shot, zero-shot, and dataset-specific formatting requirements. Response Characteristics: Model outputs ranging from short and long chain-of-thought (CoT) answers to direct responses and anomalous outputs (e.g., repetitions, truncations). Diverse Model Coverage: Comprehensive representation across commercial LLMs, opensource LLMs, and emerging LRMs, spanning diverse model scales. )}N Formally, our collected data consists of triplets: = {(qi, i=1, where qi represents the i-th question, is the response generated by model M. The primary objective of VerifierBench construction is to augment these triplets with verification labels, resulting in verified quadruples: denotes the corresponding reference answer, rm , rm DVerifierBench = {(qi, , rm , vi)}N i=1, (1) where vi {Correct, Incorrect, Invalid} is the verification label indicating the correctness of with respect to rm . Notably, during data collection and curation, we identified numerous responses exhibiting abnormal or exceptional behaviors. These include abruptly truncated outputs, excessive repetition, and cases where models refused to answer due to ethical considerations or other constraints. We therefore categorize such instances as invalid responses to enable more fine-grained evaluation. 4 Preprint. 3.2 Data Construction Pipeline Our multi-stage verification pipeline, integrating LLMs, human annotators, and rule-based tools, efficiently identifies high-value training and testing samples from large collected dataset. Multi-Expert Voting. Initially, samples undergo direct verification (no CoT reasoning) by Qwen2.5-Instruct models (7B, 14B, 32B). Samples with consensus are deemed trivial cases reliably handled by weaker models and are removed, offering minimal value. For mathematical domains (Math, GSM8K, and AIME datasets), we also incorporated MathVerify (huggingface, 2024) as an additional expert verifier. Multi-prompt Voting. Disputed samples advance to second verification stage, where DeepSeek-V3 is employed with multiple prompts to generate diverse CoT reasoning paths. Consensus samples from this stage, representing moderately challenging instances, constitute our training pool. Our experiments revealed significant challenges in developing universal verification prompt applicable across all datasets, evidenced by substantial residual disagreements after the second verification round. To address this, we implemented an additional verification phase for selected datasets, featuring domain-optimized prompts. For instance, the Chinese SimpleQA dataset required specially crafted Chinese-language prompts to achieve reliable verification outcomes. Human Annotation and Analysis. The remaining disputed samples are human-annotated, with high-value cases primarily allocated to the test set. For the VerifierBench test set, we systematically excluded proof-based questions, open-ended problems, and numerical answers with ambiguous acceptability thresholds. These non-binary judgment cases, requiring specialized verification tools or domain expertise, are deferred to future work, ensuring VerifierBench focuses on clearly verifiable samples. Finally, we get the VerifierBench dataset, and we also make sure there is no overlap between VerifierBench test set and the train set for training CompassVerifier model. Identification of Flawed Samples. Human annotation also identified distinct category: flawed samples. Errors in these samples stem not from model deficiencies in problemsolving but from issues inherent to the questions (e.g., ambiguity, incorrect standard answers) or external factors (e.g., improper output truncation, generation of meaningless repetitive text, model refusal to answer). Such flawed samples, if not distinguished, can skew model capability assessment and hinder effective model iteration. These issues are often overlooked in traditional evaluation paradigms. Consequently, we explicitly label these samples as Invalid and integrate them into the VerifierBench test set. This approach enables more granular, multi-dimensional, and realistic perspective for model performance verification. 3.3 Statistics and Analysis Statistics. We present the statistical characteristics of the VerifierBench test set across three dimensions: label categories  (Table 4)  , problem domains  (Table 5)  , and answer types  (Table 6)  . After filtering and balancing, the dataset composition shows an approximate 4:6 ratio between Category and samples, with Category representing about 7% of the total. Regarding problem domains, general reasoning, and mathematical reasoning constitute the majority, aligning with the current needs of RL training on LLMs. Classified by DeepSeekV3, the answer types comprise seven categories: multiple-choice, numerical values, short answers, formulas, multi-subproblem, sequences, and binary answers. The detailed dataset sources are provided in Table 3, with concrete examples illustrated in Section 7.6. Error Analysis and Patterns. VerifierBench is designed not merely as benchmark dataset for model evaluation, but as comprehensive framework incorporating extensive human analysis and case studies. During annotation, we required annotators to provide detailed judgment rationales in addition to final labels. Through systematic collection and analysis of these rationales, we identified and categorized over 30 meta error patterns (Section 7.4), which represent fundamental causes of mistakes and hallucinations in LLM-based answer 5 Preprint. Figure 2: Overview of CompassVerifier training pipeline. verification. For example, while mathematically equivalent formulas are conventionally accepted as correct answers by LLMs or tools, they should be rejected for expression simplification problems. Similarly, for questions admitting multiple valid answers listed in the reference answer, model response matching any one option should be considered correct, rather than complete coverage. We have found these meta patterns invaluable for both diagnostic analysis and targeted model improvement, and have incorporated them into our training framework."
        },
        {
            "title": "4 CompassVerifier",
            "content": "CompassVerifier is designed to deliver efficient, high-performance, and robust answer verification. The system leverages filtered (question, reference answer, model response) triples from VerifierBench with golden judgments as training supervision. We also propose three key techniques to drive its performance: Complex Formula Augmentation enhances formula variants verification, Error-Driven Adversarial Augmentation fortifies against failures, and Generalizability Augmentation ensures cross-domain and cross-prompt applicability. Figure 2 shows the whole pipeline of training CompassVerifier. Details of the composition of the training Data in Section 7.8. 4.1 Error-Driven Adversarial Augmentation To address potential annotation inaccuracies in our filtered data (see Section 3.2), we employ three-phase adversarial augmentation strategy. Human-in-the-Loop Analysis. Domain experts manually verify 5,000 annotated samples, identify and document failure rationales such as LLM misunderstandings of task constraints, misinterpretation of critical information in questions, and divergent penalty thresholds among judge models. Pattern Clustering. We apply density-based clustering to these rationales, revealing over 20 high-impact error categories, particularly vulnerabilities in perspective-taking and format adherence. Analysis and details are shown in Section 7.4. Meta-Judge Template Generation. For each error cluster, we develop structured templates that encode: 1) Question Characteristics (domain-specific requirements, content/format constraints) and 2) Response Error Patterns (failure types, localization, severity). This aligns model judgments with human values and improves robustness against: (1) overstrict format-based rejection, (2) underpenalization of conceptual errors in fluent responses, and (3) context-sensitive scoring variations. 4.2 Complex Formula Augmentation Verifying answers in domains such as the natural sciences is challenging due to the prevalence of complex expressions. These expressions often exhibit diverse notational conventions 6 Preprint. (e.g., symbolic, algebraic, floating-point, integer). Consequently, automated verifiers lacking robust mathematical equivalence understanding may erroneously reject semantically correct responses that differ superficially from reference solutions. To address this issue, we introduce Complex Formula Augmentation strategy that systematically generates multiple, notation-variant answers for each problem instance. Our procedure is as follows: Reference Normalization. For each original questionanswer pair in our dataset, we first convert the reference answer into canonical representation, normalizing numeric precision and symbolic structure. Variant Generation. We leverage the DeepSeek-v3 (Ma et al., 2025) to produce between one and three alternative formulations of the canonical answer. These variants include: 1) Symbolic rearrangements (e.g., rationalizing denominators, applying algebraic identities). 2) Precision-preserving floating-point expansions. 3) Equivalent integer or fraction representations. We enforce strict constraints to avoid precision loss and ensure each variant remains mathematically equivalent to the original answer within the problem context. Quality Control. All generated variants are automatically checked for equivalence using symbolic algebra engine, and subset is manually reviewed by subject-matter experts to confirm correctness and naturalness of presentation. By exposing the verifier to diverse but equivalent formulae, we markedly improve its ability to recognize correct answers regardless of notational differences, thereby reducing false negative rates in formula-intensive tasks. 4.3 Generalizability Augmentation Existing verifier models often rely on task-specific prompts, limiting their generalizability across different problems and subtle answer variations (e.g., numerical precision in TheoremQA (Chen et al., 2023)). To address this, we propose Generalizability Augmentation strategy to enhance adaptability by systematically expanding prompt and response diversity in training data. We collect diverse prompts from public datasets (e.g., TheoremQA, GPQA (Rein et al., 2024), GAOKAOBench (Zhang et al., 2023)) and real-world scenarios, covering over 20 task types. For each prompt type, we design multiple variants, varying questioning styles, context lengths, linguistic registers, and instruction granularity. Our augmentation employs two key techniques: Prompt Rewriting and Perturbation. We use LLMs (e.g., DeepSeek-v3) to automatically generate paraphrases, structural modifications, and detail-enriched prompt variants, while maintaining consistency with the final judgment. Furthermore, during training, we introduce prompt random sampling, dynamic mixing, and prompt-invariance mechanism to prevent overfitting and encourage consistent judgments across different prompt formulations, thereby enhancing generalization. Long-context Generalization. To improve robustness in long-context scenarios, we apply various perturbations to responses collected from LRMs (e.g., DeepSeek-R1 and its distilled variants) in the training set, including truncating different portions (e.g., first 20%, 40%, 60%) of the thinking process, replacing thinking tags (e.g., <think> or </think>) with alternative labels, or removing them entirely, while ensuring the final judgment remained consistent with the original response."
        },
        {
            "title": "5 Experiments",
            "content": "Baselines and Setup. We conduct comprehensive evaluations on VerifierBench across various model scales of CompassVerifier, ranging from 3B to 32B parameters. Baseline models include: (1) general LLMs such as Qwen2.5 (Yang et al., 2024), Qwen3 (Yang et al., 2024), DeepSeek-V3 (Guo et al., 2025), and GPT-4o (OpenAI, 2024a); and (2) two recently 7 Preprint. Table 1: Main results on the VerifierBench benchmark. For fair comparison, we treat the Invalid instances in VerifierBench as incorrect labels, presenting results in binary classification framework. We report Accuracy and F1 scores (%) across four categories and their average. Model Math General Reasoning Knowledge Science Average Acc. F1 Acc. Acc. F1 Acc. F1 Acc. Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen3-8B Qwen3-14B Qwen3-30B-A3B Qwen3-32B Qwen3-235B-A22B GPT-4.1-2025-04-14 GPT-4o-2024-08-06 DeepSeek-V3-0324 53.0 51.6 53.1 57.0 53.0 65.1 59.7 64.4 64.2 66.6 63.9 69. xVerify-0.5B-I 61.7 xVerify-8B-I 64.3 64.3 xVerify-9B-C Tencent-Qwen2.5-7B-RLVR 71.2 CompassVerifier-3B CompassVerifier-7B CompassVerifier-32B 76.3 79.4 84.1 General LLMs 58.9 57.3 64.6 61.4 61.6 76.8 63.4 74.9 78. 85.4 78.7 81.5 51.1 44.9 42.2 49.0 61.8 66.7 63.2 70.3 73.7 79.5 68.2 76.6 Verifier Models 84.0 84.3 82.8 80. 78.5 78.9 77.0 73.8 CompassVerifiers 88.9 89.9 92.1 85.9 87.7 90.3 30.0 37.4 31.6 37.5 51.6 44.1 62.4 54.6 53. 42.0 34.9 54.7 42.6 42.6 48.0 55.3 71.0 74.8 80.8 55.8 50.9 60.0 70.0 63.8 69.8 61.5 68.7 67.4 84.0 79.8 80. 87.1 86.1 82.7 78.0 87.9 92.8 95.1 50.7 37.8 46.4 68.5 69.4 66.7 64.4 69.5 73.1 82.9 78.3 81.2 86.2 85.1 81.7 76. 87.7 92.6 94.8 64.0 70.0 77.4 77.9 57.9 81.6 59.5 74.7 74.0 88.4 83.2 84.7 86.3 88.7 86.3 84.0 86.8 87.9 91. 36.6 47.9 48.8 60.5 42.9 56.8 48.7 52.8 50.0 75.0 54.9 68.5 72.6 74.9 69.8 62.6 77.1 78.5 84.7 57.9 57.4 63.8 66. 59.1 73.3 61.0 70.7 71.0 81.1 76.4 79.1 79.8 80.8 79.0 78.5 85.0 87.5 90.8 42.1 42.0 42.2 53.9 56.4 58.6 59.7 61.8 62. 69.8 59.1 70.3 70.0 70.4 69.1 67.1 80.4 83.4 87.7 Figure 3: Model performances with size on VerifierBench. We show the F1 score in main results. proposed specialized verifier models: xVerify (Chen et al., 2025) and Tencent-Qwen2.5-7BInstruct-RLVR (Su et al., 2025). We ask the model directly generate the final judgment of the given response and report F1 and Accuracy as metrics. More evaluation and training details are shown in Section 7.3. 5.1 Main Results From the Perspective of the Domain. We show the main results of VerifierBench in Table 1. Our CompassVerifier establishes new state-of-the-art performance across all VerifierBench 8 Preprint. (a) Seven answer types. (b) Ternary class labels. Figure 4: Results (F1) on VerifierBench across 7 answer types and 3 correctness labels. Figure 5: Ablation study on CompassVerifier-7B with different training technologies. categories, achieving 84.195.1% accuracy and 80.894.8% F1-score in the 32B configuration. Three findings emerge: 1) As shown in Figure 3, verification capability exhibits progressive improvement with increasing scale, demonstrating accuracy gains from 85.0% to 90.8% and F1-score improvements from 80.4% to 87.7% as parameters scale from 3B to 32B. 2) Verification-specific architectures yield substantial gains: CompassVerifier-7B surpasses the similarly-sized original Qwen2.5-7B-Instruct by an absolute F1-score improvement of 41.3%. 3) Despite progress, mathematical verification remains challenging (80.8% best F1score vs. 94.8% for knowledge), highlighting persistent gaps in stepwise logical validation. Our smallest 3B variant outperforms GPT-4.1 by an absolute F1-score improvement of 10.6%, demonstrating parameter efficiency. Consistent performance across domains further underscores the models robustness. For instance, our CompassVerifier-32B model achieves high F1-scores across all evaluated categories. Such consistency indicates well-generalized verification capability, effectively handling diverse types of information and reasoning processes. From the Perspective of the Answer Type. Figure 4a demonstrates the performance comparison of similarly-sized models across different answer/question types. Notably, CompassVerifier-7B achieves consistent improvements across all categories. As evident from the results, multiple-choice questions emerge as the easiest category, with most models attaining strong performance, finding attributable to their prevalence in evaluation benchmarks. However, baseline models show marked deficiencies in handling formulabased answers, multi-subquestions, and sequential answers, particularly struggling with sequential answers where none exceed 40 F1-score. This likely stems from the inherent complexity of sequential answers, which often require element-by-element matching of multiple components, significantly increasing verification difficulty. These challenging cases represent precisely the focus of CompassVerifier and constitute critical directions for future research. The complete results are presented in Table 7. 5.2 Analysis of CompassVerifier Beyond Binary Verification: Identifying Invalid Responses. Figure 4b presents the threeclass classification performance of six top-performing models. Notably, even advanced general LLMs like GPT-4o and DeepSeek-V3 without task-specific training exhibit significant performance bias, demonstrating substantially better results on categories and compared to C. Our manual analysis reveals that general models show particular insensitivity to duplicated patterns or truncated responses. To address this, we implemented duplicate string detection script during data filtering (Section 3.2). Crucially, we argue that Category requires distinct treatment as they are particularly susceptible to reward hacking in RL training scenarios. Full results of the ternary classification performance are shown in Table 8. Impact of Data Augmentation Components. Figure 5 details the impact of our data augmentation strategies on CompassVerifier-7B. The baseline model (CompassVerifier-7B-Base) achieves 84.0% accuracy and 79.8% F1. Introducing Complex Formula Augmentation alone 9 Preprint. Table 2: Performance on VerifyBench using different prompt strategies. We report Accuracy and F1 scores (%) for both model-specific prompts and the standard VerifyBench prompts. Model Model-specific Prompt VerifyBench Prompt Acc F1 Acc F1 General LLMs Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct DeepSeek-V3 65.4 78.8 78.5 81. Verifier Models 77.9 xVerify-0.5B-I 83.2 xVerify-8B-I xVerify-9B-C 83.2 Tencent-Qwen2.5-7B-RLVR 82.4 39.8 58.9 61.7 62.2 66.2 70.7 71.0 68.9 CompassVerifier-3B CompassVerifier-7B CompassVerifier-32B CompassVerifiers 87.4 88.1 89.7 77.4 79.0 81.1 60.9 72.0 63.0 78.6 - - - - 86.2 86.0 86.8 45.0 55.8 50.0 60. - - - - 75.0 73.3 74.3 Figure 6: Experimental results of CompassVerifier as reward model. We employ MathVerify along with various general LLMs and verifier models as reward models for RL training, reporting the avg@32 performance on AIME24, AIME25, and MATH500. improves accuracy to 86.7% (+2.7) and F1 to 82.8% (+3.0). This demonstrates the strategys effectiveness in enhancing the models capability to handle diverse formulaic expressions. Similarly, Error-Driven Adversarial Augmentation alone boosts accuracy to 86.4% (+2.4) and F1 to 82.0% (+2.2), underscoring its utility in fortifying the model against previously identified failure modes. Combining both strategies yields the best performance, with accuracy reaching 87.5% (+3.5) and F1 at 83.4% (+3.6), demonstrating their complementary and synergistic contributions to overall verification capabilities. Details are shown in Table 9. Generalization of CompassVerifier. To evaluate the generalization capability of CompassVerifier, we also conduct tests on the hard subset of VerifyBench (Yan et al., 2025), recent concurrent work for benchmarking verification abilities. This subset primarily contains standard answers that involve long reasoning COT, making it particularly challenging to verify. Table 2 presents the performance comparison across different models. Here, Model-specific Prompt indicates that xVerify/Tencent-RLVR employs their respective training prompts while other models use ours, whereas VerifyBench Prompt denotes that all models utilize the same prompt provided with the VerifyBench dataset. Our analysis leads to the following findings: 1) CompassVerifier still outperforms both general LLMs of similar size, specialized verifier models, and even DeepSeek-V3; 2) Due to our Generalizability Augmentation, even under VerifyBenchs prompt (deeper out-of-distribution setting), CompassVerifier maintains robust performance (score >86), while xVerify and Tencent-Qwen2.5-7B-RLVR completely fail to follow instructions. Preprint. 5.3 CompassVerifier as Reward Model To validate the efficacy of CompassVerifier as reward model in RL training, we examine its influence on enhancing the reasoning performance of models trained using RL. Specifically, we utilize GRPO (Shao et al., 2024) to train base LLMs with rule-based verifier Math-Verify (huggingface, 2024) and CompassVerifier and rigorously evaluate the reasoning capabilities of the trained models. We use the challenging Open-S1 (Dang & Ngo, 2025) as the RL training corpus, which can also be considered an out-of-distribution dataset for CompassVerifiers. More experimental settings are provided in Section 7.9. Comparative results are shown in Figure 6 (Details in Table 10). Experimental results demonstrate that models trained with CompassVerifier outperform the base model, surpass those trained with the rule-based verifier (Math-Verify), and exceed models using general LLMs or alternative verifiers as reward models. This highlights CompassVerifiers superior potential as reward model, providing more precise evaluation for rollout trajectories generated in RL training. Additionally, CompassVerifiers enhanced capacity to provide more effective signals (i.e., rewards) during training substantially improves the convergence efficiency of RL training. The results also reveal noticeable performance gap between rule-based and model-based verifiers. As the data types and disciplines covered by Reinforcement Learning from Verifiable Rewards (RLVR) (Wang et al., 2025b) training continue to expand, rule-matching tools have become increasingly inadequate, which precisely motivated the development of CompassVerifier."
        },
        {
            "title": "6 Conclusion",
            "content": "To address the critical gap in large-scale answer verification evaluation, we present VerifierBench, featuring meticulously designed pipeline for large-scale data collection, filtering, and annotation. We also introduce CompassVerifier, novel verification model specifically engineered to handle multi-domain scenarios, diverse answer types, varied prompt formats, and irregular responses. CompassVerifier achieves superior accuracy and robustness compared to larger general LLMs and baseline verifier models. We anticipate that VerifierBench and CompassVerifier would significantly advance research in answer verification for evaluation frameworks and reward modeling for RLVR."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. AI-MO. Aime 2024, aimo-validation-aime. 2024. URL https://huggingface.co/datasets/AI-MO/ Anthropic. Claude 3.5 sonnet, 2024. claude-3-5-sonnet. URL https://www.anthropic.com/news/ Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. Compassjudger-1: All-in-one judge model helps model evaluation and evolution. arXiv preprint arXiv:2410.16256, 2024. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145, 2024. Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, and Zhiyu Li. xverify: Efficient answer verifier for reasoning model evaluations. arXiv preprint arXiv:2504.10481, 2025. 11 Preprint. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. TheoremQA: theorem-driven question answering dataset. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 78897901, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.489. URL https://aclanthology.org/2023.emnlp-main.489/. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. XTuner Contributors. Xtuner: toolkit for efficiently fine-tuning llm. https://github.com/ InternLM/xtuner, 2023. Quy-Anh Dang and Chris Ngo. Reinforcement learning for reasoning in small llms: What works and what doesnt, 2025. URL https://arxiv.org/abs/2503.16219. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.acl-long.211/. Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, et al. Chinese simpleqa: chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024b. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024. 12 Preprint. huggingface. Math-verify: robust mathematical expression evaluation system designed for assessing large language model outputs in mathematical tasks. https://github.com/ huggingface/Math-Verify, 2024. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Akira Kawabata and Saku Sugawara. Rationale-aware answer verification by pairwise self-evaluation. arXiv preprint arXiv:2410.04838, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In SIGOPS, 2023. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024a. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath tir. Hugging Face repository, 2024b. URL https://huggingface.co/AI-MO/NuminaMath-TIR. Dataset documentation available at https://github.com/project-numina/aimo-progress-prize/ blob/main/report/numina_dataset.pdf. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, hai zhao, and Pengfei Liu. Generative judge for evaluating alignment. In The Twelfth International Conference on Learning Representations, 2024c. URL https://openreview.net/forum?id=gtkFw6sZGS. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen. Are your llms capable of stable reasoning? CoRR, abs/2412.13147, 2024b. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025. Jianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang Guo. Autopsv: Automated process-supervised verifier. Advances in Neural Information Processing Systems, 37:7993579962, 2024. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 3, 2024. Kaijing Ma, Xinrun Du, Yunran Wang, Haoran Zhang, Zhoufutu Wen, Xingwei Qu, Jian Yang, Jiaheng Liu, Minghao Liu, Xiang Yue, et al. Kor-bench: Benchmarking language models on knowledge-orthogonal reasoning tasks. arXiv preprint arXiv:2410.06526, 2024. 13 Preprint. Xueguang Ma, Qian Liu, Dongfu Jiang, Zejun Ma, and Wenhu Chen. General-reasoner: https://github.com/TIGER-AI-Lab/ Advancing llm reasoning across all domains. General-Reasoner, 2025. Skywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork. OC-Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. OpenAI. Openai evals: Evals is framework for evaluating llms and llm systems, and an open-source registry of benchmarks. https://github.com/openai/evals, 2023. OpenAI. Gpt-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/. OpenAI. URL gpt-4o-mini-advancing-cost-efficient-intelligence. Gpt-4o mini, 2024b. https://openai.com/index/ OpenAI. O1-preview, 2024c. URL https://openai.com/index/ introducing-openai-o1-preview. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. In First Conference on Language Modeling, 2024. ByteDance Seed. Doubao-1.5-pro, 2025. URL https://seed.bytedance.com/en/special/ doubao_1_5_pro. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. In EuroSys, pp. 12791297. ACM, 2025. Wenlei Shi and Xing Jin. Heimdall: test-time scaling on the generative verification. arXiv preprint arXiv:2504.10337, 2025. Yuda Song, Hanlin Zhang, Carson Eisenach, Sham M. Kakade, Dean Foster, and Udaya Ghai. Mind the gap: Examining the self-improvement capabilities of large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=mtJSMcF3ek. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging bigbench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv:2312.11805, 2023. 14 Preprint. Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, Jiahao Qiu, Derek Fai Wong, Heng Ji, and Kam-Fai Wong. Harnessing the reasoning economy: survey of efficient reasoning for large language models. arXiv preprint arXiv:2503.24377, 2025a. Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimization. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=5Nn2BLV7SB. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025b. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. Yuchen Yan, Jin Jiang, Zhenbang Ren, Yijun Li, Xudong Cai, Yang Liu, Xin Xu, Mengdi Zhang, Jian Shao, Yongliang Shen, et al. Verifybench: Benchmarking reference-based reward systems for large language models. arXiv preprint arXiv:2505.15801, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474, 2023. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. Jialun Zhong, Wei Shen, Yanzeng Li, Songyang Gao, Hua Lu, Yicheng Chen, Yang Zhang, Wei Zhou, Jinjie Gu, and Lei Zou. comprehensive survey of reward models: Taxonomy, applications, challenges, and future. arXiv preprint arXiv:2504.12328, 2025. Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek Wong, Xiaowei Huang, Qiufeng Wang, and Kaizhu Huang. Is your model really good math reasoner? evaluating mathematical reasoning with checklist. arXiv preprint arXiv:2407.08733, 2024. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. JudgeLM: Fine-tuned large language models are scalable judges. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=xsELpEPn4A. Preprint."
        },
        {
            "title": "7 Appendix",
            "content": "7.1 Details of VerifierBench Statistics Table 3: Dataset source distribution. Source Count Percentage (%) BBH GaokaoBench Math MMLU Pro GPQA Diamond GSM8K AIME2024 SimpleQA Numina Train HLE KorBench OlympiadBench ARC Prize Public Evaluation TheoremQA 639 201 182 172 51 14 3 97 106 355 395 345 175 22.68 7.14 6.46 6.11 1.81 0.50 0.11 3.44 3.76 12.60 14.02 12.25 6.21 2.91 Table 4: Category distribution. Category Count Percentage (%) 1092 1526 199 38.76 54.17 7. Table 5: Domain distribution. Domain Count Percentage (%) General Reasoning Mathematical Reasoning Knowledge Scientific Reasoning 1151 900 387 379 40.86 31.95 13.74 13. Table 6: Answer type distribution. Answer Type Count Percentage (%) Multiple Choice Short Text Numerical Formula Multi-subproblem Sequence Boolean Answer 891 354 434 343 281 468 46 31.63 12.57 15.41 12.18 9.98 16.61 1. 7.2 Details of VerifierBench Construction Data Collection. Our experimental evaluation encompasses comprehensive collection of 53 LLMs, including representative examples such as Qwen-2.5 (Yang et al., 2024), 16 Preprint. LLaMA3 (Grattafiori et al., 2024), DeepSeek-V3 (Liu et al., 2024a), DeepSeek-R1 (Guo et al., 2025), GPT-4o (OpenAI, 2024a), GPT-4o-mini (OpenAI, 2024b), Gemini (Team et al., 2023), claude3-5 (Anthropic, 2024), Doubao-1.5-Pro (Seed, 2025), InternLM (Cai et al., 2024) and Mixtral (Jiang et al., 2024). All specific models are listed in Table 12. These models are evaluated across sixteen diverse benchmarks: GSM8K (Hosseini et al., 2024), Math (Hendrycks et al., 2021), AIME2024 (AI-MO, 2024), BBH (Suzgun et al., 2022), GaokaoBench (Zhang et al., 2023), HLE (Phan et al., 2025), KorBench (Ma et al., 2024), GPQA (Rein et al., 2024), SimpleQA (Wei et al., 2024), ChineseSimpleQA (He et al., 2024b), MMLU-Pro (Wang et al., 2024b), ARC (Chollet et al., 2024), OlympiadBench (He et al., 2024a), TheoremQA (Chen et al., 2023), NuminaMath (Li et al., 2024b), and Drop (Dua et al., 2019). Through the OpenCompass (OC-Contributors, 2023) framework, we collected more than 1.32 million response models, creating the most comprehensive response datasets to date. VerifierBench Construction Details. For samples with inconsistent verification results across multiple models and prompts, we identified numerous cases that were either redundant or unworthy of human annotation. We employed string-matching script to detect and remove duplicate responses, which predominantly belonged to category (invalid responses). Additionally, we utilized DeepSeek-V3 to identify problematic cases, including: (1) questions with obvious open-ended nature, (2) incomplete reference answers, and (3) proof-based problems - all of which cannot be objectively evaluated solely based on reference answers and may introduce ambiguity in test set evaluation. After deduplication, approximately 5,000 samples underwent human annotation, where annotators further flagged the aforementioned problematic types. Annotation results revealed that most of the inconsistent samples were ultimately labeled as category (incorrect responses), suggesting potential tendency of LLM judges toward false positives. To maintain better label balance, we further applied similarity-based filtering to remove redundant samples within the category subset. This rigorous filtering process yielded final high-quality dataset of 2,817 samples. 7.3 Details of CompassVerifier Experiments Evaluation Setup. We use OpenCompass (OC-Contributors, 2023) and employ both F1 score and Accuracy as evaluation metrics, with particular emphasis on the F1 score, as it provides more comprehensive assessment considering the precision, recall, and balance of the class distribution simultaneously. For all open-source models, we use vllm Kwon et al. (2023) for the acceleration of inference. For all models, we employ temperature=1.0 for data synthesis and temperature=0.0 for evaluation/verification, with both max_gen_len and max_model_len set to their maximum values. We use the official prompt for Xverify and Tencent-Qwen2.5-7B-Instruct-RLVR, and general non-cot prompt for CompassVerifier and general LLMs, which can be found in the first prompt in Section 7.7. Training Setup. We use XTuner (Contributors, 2023) for training our CompassVerifier model on Qwen2.5 (Yang et al., 2024) series models, largely adhering to the original hyperparameters. Fine-tuning is conducted using learning rate of 2 105 with max sequence length 32768. multiplicative learning rate decay is applied after each epoch, with gamma value of 0.85. The batch sizes are set to 32. All models are trained for one epoch on the training set and fully fine-tuned on 8A100 80GB GPUs. 17 Preprint. Table 7: Detailed results on VerifierBench across different question types. We report Accuracy (Acc.) and F1 scores (%) for various problem categories and their average. Bold numbers indicate the best performance in each column. Model Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen3-8B Qwen3-14B Qwen3-30B-A3B Qwen3-32B Qwen3-235B-A22B GPT-4.1-2025-04-14 GPT-4o-2024-08-06 DeepSeek-V3-0324 67.4 xVerify-0.5B-I 71.7 xVerify-8B-I 67.4 xVerify-9B-C Tencent-Qwen2.5-7B-Instruct-RLVR 71.7 CompassVerifier-3B CompassVerifier-7B CompassVerifier-32B 87.0 91.3 95.7 Boolean Multi-sub Numerical Short Text Formula Multi-choice Sequence Average Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. 63.0 63.0 58.7 73.9 73.9 69.6 71.7 80.4 67.4 80.4 65.2 63.0 41.4 66.7 53.7 71.4 77.8 66.7 69.8 80.9 57.1 80.0 63.6 56. 59.5 71.1 70.6 71.1 86.4 91.7 95.8 45.9 54.5 65.8 65.8 50.2 69.8 45.9 63.4 60.9 68.3 63.7 61.2 66.9 73.0 76.9 69. 80.8 85.1 93.6 40.2 45.0 37.7 46.7 48.5 52.0 44.9 55.9 52.6 44.7 37.0 52.0 25.6 51.3 50.4 51.4 69.3 75.0 89. General LLMs 49.5 57.4 56.7 62.0 52.5 64.8 66.1 64.8 63.8 64.1 63.6 68.2 11.3 39.3 33.9 36.8 44.3 39.0 66.4 51.4 48. 31.6 29.5 48.9 65.0 59.9 61.3 57.9 52.3 76.6 53.7 68.6 67.8 83.1 79.7 81.6 Verifier Models 63.6 65.2 65.2 74. 37.8 36.3 40.8 59.2 64.7 65.3 58.8 71.2 CompassVerifiers 75.8 77.0 80.9 65.1 67.5 74.7 78.8 87.6 88. 38.0 42.3 27.7 47.7 47.4 56.1 47.4 57.1 56.1 64.7 54.4 66.3 36.6 28.1 34.8 28.2 59.9 79.1 79.8 53.5 53.8 59.3 57. 54.7 66.6 48.8 64.2 62.5 68.6 67.2 69.5 60.8 66.6 63.4 69.8 68.8 71.1 79.9 18.4 26.9 19.5 27.5 47.7 27.7 51.4 44.3 43. 22.9 11.0 39.3 22.0 24.8 30.0 40.2 57.4 61.2 71.4 62.0 49.0 55.8 61.9 70.4 72.4 74.9 74.3 79.0 89.4 80.0 85. 95.7 92.6 92.3 84.2 95.7 95.6 96.2 65.0 45.9 52.5 62.4 76.8 73.8 79.8 77.8 82.6 91.0 81.9 87.6 96.6 94.0 93.6 86. 96.6 96.6 97.0 59.2 68.8 80.6 74.8 53.0 84.6 55.1 78.4 83.3 88.3 86.8 85.5 85.5 88.3 85.9 85.0 87.6 90.2 93. 23.9 34.8 19.5 40.4 30.4 39.0 28.1 46.0 50.4 43.3 35.4 54.1 35.0 35.3 29.8 27.1 52.5 67.1 74.6 56.9 58.0 62.6 64. 58.1 72.0 59.5 70.6 69.2 77.4 72.3 73.5 72.1 74.7 72.8 75.1 82.1 85.4 89.2 34.0 43.0 34.9 47.6 53.3 50.6 55.4 59.1 55. 54.0 44.7 57.8 44.7 48.7 50.0 52.0 69.3 76.0 83.0 Table 8: Three-label classification performance on VerifierBench. Beyond binary correctness (correct/incorrect), this evaluation requires models to identify invalid responses. We report Accuracy and macro-F1 scores (in %) across four distinct categories and their overall average. Model Math General Reasoning Knowledge Science Average Acc. macro-F1 Acc. macro-F1 Acc. macro-F1 Acc. macro-F1 Acc. macro-F Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen3-8B Qwen3-14B Qwen3-30B Qwen3-32B Qwen3-235B-A22B GPT-4.1-2025-04-14 GPT-4o-2024-08-06 DeepSeek-V3-0324 CompassVerifier-3B CompassVerifier-7B CompassVerifier-32B 39.6 44.2 46.0 51.1 48.2 61.3 53.3 57.2 58. 61.7 57.9 63.2 73.4 77.7 82.0 29.2 37.7 35.7 43.0 35.8 57.3 45.6 54.2 42.8 59.6 53.9 49.1 68.8 74.3 79. General LLMs 37.8 40.1 47.8 48.6 42.3 63.5 42.1 54.4 55.0 73.6 62.9 66.2 45.2 42.9 55.6 67.4 56.1 65.4 54.8 60.2 65. 78.3 73.4 76.5 CompassVerifiers 85.6 87.6 90.7 86.3 91.5 94.3 49.2 50.9 59.8 57.3 54.0 72.3 49.6 61.6 73. 78.1 68.3 77.4 87.4 88.1 90.0 34.6 37.6 45.7 52.2 41.1 54.7 50.2 51.7 48.6 69.7 66.0 60.3 87.1 92.6 95. 50.3 57.1 70.8 72.9 47.9 74.7 45.0 58.7 67.6 79.5 71.1 80.5 87.6 86.0 91.3 34.2 44.1 52.5 58.8 36.5 61.9 39.0 50.0 52. 68.4 57.1 67.8 80.8 79.1 86.8 46.1 48.8 58.0 62.2 51.5 68.4 50.7 59.4 66.4 74.4 67.7 74.4 83.7 85.8 89. 34.0 39.9 45.4 50.7 38.9 59.4 44.2 52.6 49.7 67.8 60.0 60.9 80.6 83.4 88.3 Table 9: Ablation study on CompassVerifier-7B with different augmentation strategies on VerifierBench main results. Complex Formula Augmentation enhances formula variants verification, Error-Driven Adversarial Augmentation fortifies against failure cases. Setting Accuracy (%) Acc (%) CompassVerifier-7B-Base + Complex Formula Augmentation + Error-Driven Adversarial Augmentation + Both Augmentations 84.0 86.7 86.4 87.5 - +2.7 +2.4 +3.5 F1 (%) F1 (%) 79.8 82.8 82.0 83. - +3.0 +2.2 +3.6 18 Preprint. Table 10: Experimental results of CompassVerifier as reward model. We report the avg@32 performance on AIME24, AIME25, and MATH500."
        },
        {
            "title": "Model",
            "content": "AIME24 AIME25 MATH"
        },
        {
            "title": "Original Model Performance",
            "content": "Qwen3-4B-Base 2.7 1.8 RL with Rule-based Verifier Math-Verify 8. 7.2 RL with Model-based Verifier Tencent-RLVR Qwen3-14B Qwen2.5-32B CompassVerifier-7B CompassVerifier-32B 17.4 19.8 19.6 21.2 21.2 16.2 16.6 15.4 17.3 17.2 34. 63.1 80.5 81.2 81.6 82.2 83.3 7.4 Details of Meta Error Patterns We display the meta error patterns in three categories: (Correct), (Incorrect), and (Invalid) as shown in the following figures. Meta Pattern: (Correct) The units in the LLM Response differ from those in the final answer, resulting in different numerical expressions, but they are consistent upon conversion, should be judged as Correct. The reference answer is an extremely complex formula, and the LLM Response appears very different in form but simplifies to an equivalent expression, with no explicit requirement for simplification in the question, should be judged as Correct. The question requires calculating numerical decrease, and the LLM Response has the opposite sign of the reference answer because either uses negative signs to represent decrease, but they are equivalent, should be judged as Correct. The reference answer provides multiple candidate answers without requiring all possibilities. The LLM Response provides one of them, should be judged as Correct. The question doesnt explicitly specify answer format (numerical or formula). The LLM Response and reference answer differ in form but are equivalent when calculated, should be judged as Correct. The question requires specific formatting (order, capitalization, etc.). While the LLM Response appears different from the reference answer in formatting, upon inspection it fully complies, should be judged as Correct. When calculating values with units, the reference answer and LLM Response may differ in unit representation or numerical values, but are equivalent after unit conversion, should be judged as Correct. For multiple-choice or true/false questions, the LLM Response ultimately gives the correct answer despite showing significant uncertainty, should be judged as Correct. The question requires expressions meeting simple conditions (sum, product, logical relations, etc.), and the reference answer may include multiple valid forms. The LLM Response differs in form but meets all requirements, should be judged as Correct. The LLM initially provides an incorrect answer but corrects it after reflection, should be judged as Correct. 19 Preprint. The reference answer consists of multiple sub-questions. The LLM answers all sub-questions correctly during reasoning, even if not presented together at the end, should be judged as Correct. Meta Pattern: (Inorrect) For multiple-choice questions, the LLM Response selects the correct option but follows with unrelated option content, should be judged as Incorrect. The question requires formula simplification. The LLM answer isnt fully simplified to minimal form, even if equivalent to the reference answer, should be judged as Incorrect. The reference answer is formula with specified output format. The LLM answer doesnt comply with this format, even if equivalent, should be judged as Incorrect. The question requires an expression where the sum equals certain value with each number used once. The LLM Response repeats numbers while satisfying the sum, should be judged as Incorrect. The reference answer is an un-simplified logical formula after substitution. The LLM Response is incorrect due to simplification causing format errors, should be judged as Incorrect. The LLM Response only provides solution code without final results, should be judged as Incorrect. The LLM Response (formula/numerical) and reference answer arent equivalent when calculated, should be judged as Incorrect. When describing numerical intervals, the reference answer and LLM Response differ in endpoint inclusion (open/closed), should be judged as Incorrect. For sequence decryption requiring exact matching, the LLM Response doesnt match the reference answer, should be judged as Incorrect. The reference answer is long sequence requiring exact correspondence. The LLM Response has minor differences with some errors, should be judged as Incorrect. The question explicitly requires multiple candidate answers (provided in reference), but the LLM Response gives only one, should be judged as Incorrect. The LLM initially provides correct answer but changes to incorrect or \"unanswerable\" after reflection, should be judged as Incorrect. For symbolic sequences, the LLM Response contains garbled characters, should be judged as Incorrect. The reference answer is numerical, and the LLM Response provides more decimal places but rounds differently, should be judged as Incorrect. The reference answer is an extremely large number, and the LLM Response provides high-order power expression that doesnt match after calculation, should be judged as Incorrect. After detailed reasoning, the LLM Response fails to provide clear answer or states the question is unanswerable, should be judged as Incorrect. For multi-part questions, the number of final answers in the LLM Response doesnt match the reference answer, should be judged as Incorrect. Meta Pattern: (Invalid) The question contains multiple sub-questions, but the number of reference answers doesnt match, indicating quality issues, should be judged as Invalid. The reference answer has serious omissions, truncation, or formatting issues, should be judged as Invalid. The question itself has serious omissions, truncation, or formatting issues, should be judged as Invalid. The LLM doesnt answer normally, stating it needs more information or internet access, should be judged as Invalid. The LLM Response is clearly truncated and incomplete, should be judged as Invalid. 20 Preprint. The LLM Response is mostly garbled text with no valuable information extractable, should be judged as Invalid. The LLM Response contains extensive meaningless repetition, making correct answers unidentifiable, should be judged as Invalid. 7.5 Meta-Judge Template Generation Fields Table 11: Meta-Judge Template Generation Fields (Academic Disciplines and Subfields) Category Discipline Subfields Natural Sciences Mathematics Physics Chemistry Biology Earth Sciences Statistics Mechanical Engineering Computer Science & Technology Engineering Electronic Information Engineering Civil Engineering Materials Science & Engineering Chemical Engineering Environmental Engineering Aerospace Engineering Biomedical Engineering Energy & Power Engineering Differential calculus, Integral calculus, Probability statistics, Operations research, Mathematical logic, Financial mathematics, Topology, Algebraic geometry Theoretical physics, Quantum mechanics, Condensed matter physics, Astrophysics, Nuclear physics, Optics, Acoustics Analytical chemistry, Organic chemistry, Inorganic chemistry, Physical chemistry, Materials chemistry, Environmental chemistry, Chemical biology Molecular biology, Genetics, Ecology, Cell biology, Biochemistry, Microbiology Geology, Geophysics, Atmospheric sciences, Oceanography, Environmental science, Paleontology Data science, Biostatistics, Economic statistics, Machine learning algorithms, Bayesian analysis Mechanical design & manufacturing, Automatic control, Robotics, Vehicle engineering, Thermal & power engineering, MEMS Artificial intelligence, Computer networks, Software engineering, Computer vision, Cybersecurity, Big data analytics Communication engineering, IC design, Optoelectronic technology, Wireless sensor networks, Smart grid Structural engineering, Bridge & tunnel design, Geotechnical engineering, Hydraulic engineering, Urban planning Nanomaterials, Metallic materials, Polymer materials, Composite materials, Material processing Chemical process design, Petroleum refining, Biochemical engineering, Catalytic reaction engineering, Separation technology Pollution control technology, Environmental monitoring, Ecological restoration, Solid waste treatment, Clean energy development Aircraft design, Propulsion systems, Aerodynamics, Satellite navigation, Aerospace materials Medical imaging technology, Biomaterials, Artificial organs, Biosensors, Rehabilitation engineering Nuclear technology, Wind energy development, Solar energy utilization, Fuel cells, Thermal system optimization 21 Preprint. 7.6 Cases in VerifierBench Example I: Verification as Correct (Gold Judgment: A) 2 and g(x) = x2. Find (g( (g( (8))))). Question Let (x) = Gold Answer 2 LLM Response To solve the problem (g( (g( (8))))), we need to evaluate the functions step by step, starting from the innermost function and working our way out. 1. Evaluate (8): (x) = (8) = 8 2. Evaluate g( (8)): 3. Evaluate (g( (8))): g( 4. Evaluate g( (g( (8)))): 5. Evaluate (g( (g( (8))))): g(x) = x2 8) = ( 8)2 = 8 8 (8) = g( 8) = 8 (8) = 8 8 (8) = g( (8) = 8) = 8 8 g( 8) = 8 (8) = g( (8) = g( (8) = 8) = 8 8) = 8 8 8 After evaluating the functions step by step, we find that: (g( (g( (8))))) = 8 Thus, the final answer is: Gold Judgment (Correct) Meta Data Data Source: math Domain: Math Answer Type: Numerical 8 22 Preprint. Example II: Verification as Incorrect (Gold Judgment: B) Question Sort the following words alphabetically: List: marshmallow doge alpenstock ocean accept angus drool jutish resistive chromium vociferous castigate prize octennial stonewort concision elizabethan Gold Answer accept alpenstock angus castigate chromium concision doge drool elizabethan jutish marshmallow ocean octennial prize resistive stonewort vociferous LLM Response So the answer is accept, alpenstock, angus, castigate, chromium, concision, doge, drool, elizabethan, jutish, marshmallow, octennial, ocean, prize, resistive, stonewort, vociferous. Gold Judgment (Incorrect) Meta Data Data Source: BBH/bbh-word_sorting Domain: General Reasoning Answer Type: Sequence Example III: Verification as Invalid (Gold Judgment: C) Question droplet of molten lead of average 1/8 inch diameter, at 750F, falls from height of 60 ft, and solidifies by the cooling effect of the surrounding air. The surrounding temperature is 70F. If the lead solidifies at 621F, calculate the coefficient of heat transfer. Given properties for lead: Cp = 0.031 Btu/lbm-F ρ = 710 lbm/ft3 usion = 10.6 Btu/lbm Options: A. 59.2 Btu/hr-ft2-F B. 45.7 Btu/hr-ft2-F C. 52.8 Btu/hr-ft2-F D. 63.4 Btu/hr-ft2-F Gold Answer LLM Response To solve this problem, we need to calculate the coefficient of heat transfer (h) using the formula: = * * where: - is the heat transferred (in Btu) - is the surface area of the droplet (in ft2) - is the temperature difference (in F) First, lets find the volume (V) of the droplet using the given average diameter (1/8 inch): = π * (d/2)3 = π * (1/16)3 = π * (1/4096) ft3 Now, lets find th Gold Judgment (Invalid) Meta Data Data Source: mmlu_pro/mmlu_pro_engineering Domain: Science Answer Type: Multiple Choice 23 Preprint. 7.7 Prompt List (cid:11) Please as grading expert , judge whether the final answers given by the candidates below are consistent with the standard answers , that is , whether the candidates answered correctly . Here are some evaluation criteria : 1. Please refer to the given standard answer . You don ' need to re - generate the answer to the question because the standard answer has been given . You only need to judge whether the candidate ' answer is consistent with the standard answer according to the form of the question . THE STANDARD ANSWER IS ALWAYS CORRECT AND THE QUESTION IS PERFECTLY VALID . NEVER QUESTION THEM . 2. ONLY compare the FINAL ANSWER - COMPLETELY IGNORE any potential errors in the REASONING PROCESSES . 3. Some answers may be expressed in different ways , such as some answers may be mathematical expression , some answers may be textual description , as long as the meaning expressed is the same . Before making judgment , please understand the question and the standard answer first , and then judge whether the candidate ' answer is correct . 4. Some answers may consist of multiple items , such as multiple - choice questions , multiple - select questions , fill -in - the - blank questions , etc . Regardless of the question type , the final answer will be considered correct as long as it matches the standard answer , regardless of whether the reasoning process is correct . For multiple - select questions and multi - blank fill -in - the - blank questions , all corresponding options or blanks must be answered correctly and match the standard answer exactly to be deemed correct . 5. If the prediction is given with boxed {{}} , please ignore the boxed {{}} and only judge whether the candidate ' answer is consistent with the standard answer . 6. If the candidate ' answer is invalid (e.g., incomplete ( cut off mid - response ) , lots of unnormal repetitive content , or irrelevant to the question , saying it can 't answer the question because some irresistible factors , like ethical issues , no enough information , etc .) , select option ( INVALID ). Please judge whether the following answers are consistent with the standard answer based on the above criteria . Grade the predicted answer of this new question as one of : A: CORRECT B: INCORRECT C: INVALID Just return the letters \"A\", \"B\", or \"C\", with no text around it . Here is your task . Simply reply with either CORRECT , INCORRECT , or INVALID . Don ' apologize or correct yourself if there was mistake ; we are just trying to grade the answer . < Original Question Begin >: { question } < Original Question End > < Standard Answer Begin >: { gold_answer } < Standard Answer End > < Candidate ' Answer Begin >: { llm_response } < Candidate ' Answer End > Judging the correctness of the candidate 's answer : (cid:10) (cid:11) Prompt 1: Prompt for general LLM evaluation As grading expert , your task is to determine whether the candidate ' final answer matches the provided standard answer . Follow these evaluation guidelines precisely : Evaluation Protocol : 1. Reference Standard : - The standard answer is definitive and always correct (cid:8) (cid:9) (cid:8) Preprint. - The question is perfectly valid - never question them - Do not regenerate answers ; only compare with the given standard 2. Comparison Method : - Carefully analyze the question ' requirements and the standard answer ' structure * Determine whether the question expects exact matching of the entire standard answer or allows partial matching of its components . * This determination must be made based on the question ' phrasing and the nature of the standard answer . - Compare ONLY the candidate ' final answer ( ignore all reasoning / explanation errors ) - Disregard any differences in formatting or presentation style - For mathematical expressions : calculate step by step whether the two formulas are equivalent - For multiple - choice questions : compare only the final choice and corresponding option content 3. Multi - part Answers : - For questions requiring multiple responses ( e. g. , multi - select ): - All parts must match the standard answer exactly . - Compare each sub - answer step by step . Partial matches are considered incorrect . 4. Validity Check : - Reject answers that are : * Incomplete ( cut off mid - sentence in the final sentence , lacking complete response ) - Label as INCOMPLETE * Repetitive ( repetition of words or phrases in loop ) - Label as REPETITIVE * Explicit refusals (e.g., directly return \"I cannot answer / provide / access ...\") - Label as REFUSAL - For invalid answers , specify the type in the judgment ( e. g. , boxed { C} - INCOMPLETE ). Grading Scale : boxed {A} - CORRECT : - Answer matches standard exactly ( including equivalent expressions ) - For numerical answers : consider as equivalent if values match when rounded appropriately - Semantically equivalent responses boxed {B} - INCORRECT : - Any deviation from standard answer - Partial matches for multi - part questions boxed {C} - INCOMPLETE / REPETITIVE / REFUSAL : - Fails validity criteria above ( must specify : INCOMPLETE / REPETITIVE / REFUSAL ) Execution Steps and Output Formats : Analysis step by step : [ Thoroughly evaluate the candidate ' answer including : (1) First check if the answer is INCOMPLETE ( cut off mid - sentence ) , REPETITIVE ( looping repetition ) , or REFUSAL ( explicit denial ) - if so , immediately classify as boxed {C} with the corresponding type . (2) Analyze the question ' core requirements and the standard answer ' structure , for example : - Strict requirements : Identify mandatory constraints (e.g., simplification , answer order , multi - part completeness ) - Tolerant allowances : Ignore non - critical deviations (e.g., missing option labels in MCQs , equivalent but unformatted expressions ) - Required answer type , precision level , etc . 25 Preprint. (3) Perform detailed comparison between the candidate ' final answer and the standard answer , for example : - Content equivalence - Permitted variations in numerical precision - Allowed expression formats ] Final Judgment : boxed {A/B/C} - < CORRECT / INCORRECT / INCOMPLETE / REPETITIVE / REFUSAL > Here is your task . < Original Question Begin > { question } < Original Question End > < Standard Answer Begin > { gold_answer } < Standard Answer End > < Candidate ' Answer Begin > { llm_response } < Candidate ' Answer End > (cid:9) (cid:8) Analysis step by step and Final Judgment : (cid:10) Prompt 2: Prompt for CoT answer verification (cid:11) As grading expert , your task is to determine whether the candidate ' final answer matches the provided standard answer . Follow these evaluation guidelines precisely : Evaluation Protocol : 1. Reference Standard : - The standard answer is definitive and always correct - The question is perfectly valid . Never question them - Do not regenerate answers ; only compare with the given standard answer 2. Thoroughly evaluate the candidate ' answer follow these steps - Carefully analyze the question ' content and requirements * Strict requirements : Identify mandatory constraints (e.g., simplification , answer order , multi - part completeness ) * Tolerant requirements : Ignore non - critical deviations (e.g., missing option labels in MCQs , equivalent but unformatted expressions ) - Carefully analyze the standard answer ' content and structure . Determine whether the question expects exact matching of the entire standard answer or allows partial matching of its components - Validity Check for the candidate ' answer . Reject answers that are : * Incomplete ( cut off mid - sentence in the final sentence , lacking complete response ) - Label as INCOMPLETE * Repetitive ( repetition of words or phrases in loop ) - Label as REPETITIVE * Explicit refusals (e.g., directly return \"I cannot answer / provide / access ...\") - Label as REFUSAL - Perform detailed comparison between the candidate ' final answer and the standard answer * Compare ONLY the candidate ' final answer ( ignore all reasoning / explanation errors ) * Disregard any differences in formatting or presentation style * For mathematical expressions : calculate step by step whether the two formulas are equivalent * For multiple - choice questions : compare only the final choice and the corresponding option content 26 Preprint. * For questions requiring multiple sub - answers (e.g., multi - select ): All parts must match the standard answer exactly . Compare each sub - answer step by step . Partial matches are considered incorrect . 3. Grading Scale : boxed {A} - CORRECT : - Answer matches standard exactly ( including equivalent expressions ) - For numerical answers : consider as equivalent if values match when rounded appropriately - Semantically equivalent responses boxed {B} - INCORRECT : - Any deviation from standard answer - Partial matches for multi - part questions boxed {C} - INCOMPLETE / REPETITIVE / REFUSAL : - Fails validity criteria above ( must specify : INCOMPLETE / REPETITIVE / REFUSAL ) Output Formats : Analysis : [ Analysis and evaluate step by step here .] Final Judgment : boxed {A/B/C} - < CORRECT / INCORRECT / INCOMPLETE / REPETITIVE / REFUSAL > Here is your task . < Original Question Begin > { question } < Original Question End > < Standard Answer Begin > { gold_answer } < Standard Answer End > < Candidate ' Answer Begin > { llm_response } < Candidate ' Answer End > Analysis : Final Judgment : (cid:10) (cid:11) Prompt 3: Prompt for CoT answer verification (cid:9) (cid:8) As grading expert , your task is to determine whether the candidate ' final answer matches the provided standard answer . Follow these evaluation guidelines precisely : Evaluation Protocol : 1. Reference Standard : - The standard answer is definitive and always correct - The question is perfectly valid - never question them - Do not regenerate answers ; only compare with the given standard 2. Comparison Method : - Extract ONLY the candidate ' final answer ( ignore all reasoning / explanation errors ) - If no complete final answer exists (e.g., response is cut off or contains only reasoning ) - INVALID - Compare this directly with the standard answer - Disregard any differences in formatting or presentation style - For mathematical expressions : compare semantic equivalence , not syntax - For boxed {} format : ignore the boxed notation when comparing 3. Multi - part Answers : - For questions requiring multiple responses ( e. g. , multi - select ): 27 Preprint. - All parts must match the standard answer exactly - Partial matches are considered incorrect 4. Validity Check : - Reject answers that are : * Incomplete ( cut off mid - response or missing final answer ) * Purely reasoning without final answer * Repetitive or uninterpretable * Irrelevant to the question * Explicit refusals (e.g., \" cannot answer / provide / access ...\") Grading Scale : boxed {A} - CORRECT : - Answer matches standard exactly ( including equivalent expressions ) - For numerical answers : allow 1% tolerance for floating - point variations - Semantically equivalent responses boxed {B} - INCORRECT : - Any deviation from standard answer - Partial matches for multi - part questions boxed {C} - INVALID : - Fails validity criteria above Execution Steps and Output Formats : Analysis : 1. Completeness and Validity Check : [ confirm if candidate 's answer is complete and include the final answer ] 2. Extracted Final Answer : [ state what was identified as final answer ] 3. Standard Comparison : [ describe how it matches / mismatches ] Final Judgment : [ boxed {A/B/C }] Here is your task . < Original Question Begin > { question } < Original Question End > < Standard Answer Begin > { gold_answer } < Standard Answer End > < Candidate ' Answer Begin > { llm_response } < Candidate ' Answer End > Analysis and Final Judgment : (cid:10) Prompt 4: Prompt for CoT answer verification (cid:9) 28 Preprint. Table 12: List of models used in the experiment with response counts Model Family Model Name Response Count Yi GPT Doubao Qwen Gemini DeepSeek-R1 Llama Mixtral Claude Gemma Yi-Lightning Yi-1.5-9B-Chat GPT-4o GPT-4o-mini GPT-4-1-2025-0414 GPT-4.5-preview-2025-02-27 Doubao-Pro-32k-241215 Doubao-Pro-1.5-32k-250115 Doubao-Pro-32k-240828 Qwen-Max-0919 Qwen-Max-2025-01-25 Qwen2.5-Max Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct QwQ-32B Gemini-2.0-Flash-Exp Gemini-1.5-Pro Gemini-2-5-Pro-03DeepSeek-Chat-R1 DeepSeek-R1-distill-Qwen-1.5B DeepSeek-R1-distill-Qwen-7B DeepSeek-R1-distill-Llama-8B DeepSeek-R1-distill-Qwen-14B DeepSeek-R1-distill-Qwen-32B DeepSeek-R1-distill-Llama-70B Llama-3-1-8B-Instruct Llama-3-1-70B-Instruct Llama-3-2-3B-Instruct Llama-3-3-70B-Instruct Mistral-Small-Instruct-2409 Mistral-Small-3.1-24B-Instruct Ministral-8B-Instruct-2410 Mixtral-Large-Instruct-2411 Claude-3-5-Sonnet-20241022 Claude-3-7-Sonnet-20250219 Claude-3-7-Sonnet-20250219-Thinking Gemma-2-9B-It Gemma-2-27B-It Gemma3-27B-It DeepSeek-Chat DeepSeek-V2. InternLM Phi GLM MiniMax Moonshot Hunyuan StepFun DeepSeek-Chat-V3 InternLM2.5-7B-Chat InternLM2.5-20B-Chat InternLM3-8B-Instruct Phi-4 GLM-4-9B-Chat GLM-4-Plus MiniMax-TextMoonshot-V1-32k Hunyuan-Standard-256K Step-2-16k 29 18496 17722 18495 44502 2673 6378 18517 5692 18434 29173 18320 49003 32116 37477 37568 20623 17303 18429 669 16556 16012 16364 15731 16671 16042 15772 44857 18018 28618 28307 18233 14331 17962 18521 18474 4723 34541 34704 13120 31896 31950 43336 37594 15976 18360 17537 39570 18067 18082 18405 Preprint. 7.8 Details of Training Data for CompassVerifier For the composition of CompassVerifier train dataset, we use 54420 consist samples from the VerifierBench pipeline as shown in Figure 1 as the base train set, we then use Error-Driven Adversarial Augmentation and Complex Formula Augmentation to construct extra data comprehensively enhance the capabilities of the CompassVerifier model. The composition of our train data list in Table 13. Table 13: Composition of CompassVerifier training data Data Source Number of Samples Percentage (%) Base Train Set (VerifierBench) Error-Driven Adversarial Augmentation Complex Formula Augmentation Total 54,420 24,294 18,118 96,832 56.20 25.09 18.71 100.00 Error-Driven Adversarial Augmentation. Using DeepSeek-v3, we generate 34 Meta-Judge Templates covering common and extreme error scenarios then generate 224294 synthetic examples that emphasize decision boundary cases, especially where human judges tolerate minor errors that baseline verifiers over-penalize. Complex Formula Augmentation. Applying this augmentation pipeline, we have synthesized approximately 18118 enhanced examples spanning 14 distinct scientific and engineering disciplines. 7.9 Details of CompassVerifier-as-Reward Experimental Settings Base LLMs. We utilize Qwen3-4B-Base (Yang et al., 2025) as the base LLM for the GRPO training. Training Template. We utilize the following training template to prompt the base LLM to generate response for each question. We only verify the format correctness to ensure the final answer is encapsulated within boxed{...final answer...}. Training Template of CompassVerifier conversation between User and an Assistant. The User poses question, and the Assistant provides solution. The Assistants response follows these structured steps: 1. **Reasoning Process**: The Assistant comprehensively thinks about the problem through reasoning process. 2. **Conclusion**: The Assistant reaches conclusion, which is enclosed within <conclusion> and </conclusion> tags. The final answer is highlighted within boxed{...final answer...}. 3. **Response Format**: The complete response should be formatted as: ...reasoning process... <conclusion> ...conclusion... The answer is boxed{...final answer...} </conclusion> Training Data. We utilize the challenging mathematical reasoning dataset Open-S1 (Dang & Ngo, 2025) as the RL training corpus. To increase the difficulty of our validation, we curate the final training set by specifically excluding problems with integer solutions from the original Open-S1 dataset. 30 Preprint. Evaluation. We employ Math-Verify (huggingface, 2024) as our evaluation tool since the answers in these three benchmarks are readily verifiable, making them particularly well-suited for Math-Verifys verification mechanism. Reward Design. We design simple reward scheme: 0 for answer errors, and 1 for correct responses. Training Parameters. We utilize the following loss function, with Table 14 detailing the training parameters: = (q,a)D,{oi}G πθold (cid:34) 1 i=1 oi i=1 i=1 oi t= min (q) (cid:18) πθ (oi,tq, oi,<t) (oi,tq, oi,<t) πθold ai,t, clip (cid:18) πθ (oi,tq, oi,<t) (oi,tq, oi,<t) πθold , 1 ϵmin,, 1 ϵmax (cid:19) (cid:19)(cid:35) ai,t , (2) where denotes the training data, (q, a) represents the question-answer pair, signifies the group size, and ai,t = ri mean({ri}G (3) In this context, ai,t signifies the advantage of response oi at the t-th position, and ri denotes the reward of response oi. Essentially, the KL penalty of the original GRPO loss is omitted, and zero mean normalization is employed to estimate the advantage. i=1). Table 14: Training parameters of CompassVerifier as reward experiments. Parameters train batch size train epochs learning rate max prompt length max response length ϵmin ϵmax Value 256 2 1e-6 4096 12288 8 0.2 0.28 Hardware. All experiments are conducted on clusters equipped with 8 NVIDIA A800-SXM4-80GB GPUs and Intel(R) Xeon(R) Platinum 8336C CPUs, implementing with veRL (Sheng et al., 2025)."
        }
    ],
    "affiliations": [
        "NLP2CT Lab",
        "Shanghai AI Laboratory",
        "University of Macau"
    ]
}