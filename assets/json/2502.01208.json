{
    "paper_title": "Almost Surely Safe Alignment of Large Language Models at Inference-Time",
    "authors": [
        "Xiaotong Ji",
        "Shyam Sundhar Ramesh",
        "Matthieu Zimmer",
        "Ilija Bogunovic",
        "Jun Wang",
        "Haitham Bou Ammar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 8 0 2 1 0 . 2 0 5 2 : r Almost Surely Safe Alignment of Large Language Models at Inference-Time Xiaotong Ji * 1 2 Shyam Sundhar Ramesh * 1 3 Matthieu Zimmer * 1 Ilija Bogunovic 3 Jun Wang 3 Haitham Bou Ammar"
        },
        {
            "title": "Abstract",
            "content": "to ensure their outputs are free from controversial content. Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with probability approaching one. We achieve this by framing the safe generation of inference-time responses as constrained Markov decision process within the LLMs latent space. Crucially, we augment safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses. Contains potentially harmful examples. 1. Introduction LLMs have demonstrated impressive capabilities across diverse set of tasks, such as summarization (Koh et al., 2022; Stiennon et al., 2020), code generation (Gao et al., 2023; Chen et al., 2021), and embodied robotics (Mower et al., 2024; Kim et al., 2024). However, since those models are primarily trained on vast, unsupervised datasets, their generated responses can often be biased, inaccurate, or harmful (Deshpande et al., 2023; Ganguli et al., 2022; Weidinger et al., 2021; Gehman et al., 2020). As result, LLMs require alignment with human values and intentions *Equal contribution Work done during internship at Huawei 1Huawei Noahs Ark Lab 2Imperial College London 3University College London. Correspondence to: Haitham Bou Ammar <haitham.ammar@huawei.com>. 1 The traditional LLM alignment approach involves finetuning the model on human-labeled preference data. For instance, works such as (Ouyang et al., 2022; Christiano et al., 2017) use the reinforcement learning from human feedback (RLHF) framework to fine-tune LLMs, constructing reward model from human feedback and optimizing the model using standard reinforcement learning algorithms like PPO (Schulman et al., 2017). More recent approaches, such as (Tutnov et al., 2025; Yin et al., 2024; Rafailov et al., 2023), bypass reward learning and instead align pre-trained models directly with human preferences. RLHF alignment can be costly and risks overfitting partly since they modify the LLMs model weights. To tackle these challenges, alternative approaches adjust the models inference time while leaving the responses directly at pre-trained model weights fixed. Several techniques have been proposed for this purpose, such as Best-of-N (Nakano et al., 2021; Stiennon et al., 2020; Touvron et al., 2023), FUDGE (Yang & Klein, 2021), COLD (Qin et al., 2022), CD-Q Mudgal et al. (2023), RE-control (Kong et al., 2024), among others. Importantly, those techniques are designed modularly, allowing the alignment module to integrate seamlessly with the pre-trained model. This modularity enables flexible inference-time reconfigurability and quick adaptation to new reward models and datasets. Moreover, it reduces reliance on resource-intensive and often hard-tostabilize RL processes inherent in the RLHF paradigm. Despite the successes of inference-time alignment, these methods safety aspects have received limited attention so far. While some works have attempted to tackle those issues in inference-time-generated responses, they mainly focus on prompt-based alignment (Hua et al., 2024; Zhang et al., 2024b), trainable safety classifiers (Niu et al., 2024; Zeng et al., 2024) or protections against adversarial attacks and jailbreaks (Dong et al., 2024; Guo et al., 2024; Inan et al., 2023). That said, prompt-based methods cannot be guaranteed to consistently produce safe responses, as ensuring safety is heavily reliant on user intervention, requiring extensive engineering and expertise to manage the models output effectively. Trainable classifiers focus only on the safety of decoded responses using hidden states Almost Surely Safe Alignment of Large Language Models at Inference-Time or virtual tokens, ignoring task alignment and lacking theoretical guarantees. Moreover, while adversarial robustness is crucial, our work focuses on the key challenge of generating inherently safe responses from the LLM. to produce probability distribution over the vocabulary space. Moreover, ht comprises all key-value pairs accumulated from previous time steps 1. The system evolves until the end-of-sequence (EOS) token is reached. In this work, we aim to develop LLMs that generate safe responses at inference time, following principled approach that guarantees safety almost surely, i.e., with probability approaching one. To do so, we reformulate the safe generation of inference-time responses as an instance of constrained Markov decision processes (cMDP). We map the cMDP to an unconstrained one through safety state augmentation, bypassing Lagrangian approaches limitations that struggle to balance reward maximization and safety feasibility. Focusing on practical test-time inference algorithm, we adopt critic-based approach to solve the augmented MDP, eliminating the need for gradients in the LLM. To ensure efficiency, we train our critic in the latent space of the LLM, keeping it small in size and fast during inference. This shift to the latent space complicates the theoretical framework, requiring extensions from (HernandezLerma & Munoz de Ozak, 1992; Sootla et al., 2022). By doing so, we establish, for the first time, that one can guarantee almost sure safety in the original token space. To leverage this theoretical guarantee in practice, we build upon the augmented MDP framework and introduce two novel implementations for safe inference-time alignment: one that learns compact critic in the latent space for cases where safety costs can only be queried after the generation of complete responses and another that leverages direct cost queries for efficient inference-time optimization. Finally, we integrate these components into lookahead algorithm (e.g., Beam Search or Blockwise Decoding (Mudgal et al., 2023)) proposing InferenceGuard. Our experiments tested InferenceGuard starting from safetyaligned and unaligned models. Our results achieved high safety rates98.02% on Alpaca-7B and 100% on Beaver7B-v3. Notably, this was accomplished while maintaining strong balance with rewards, setting new state-of-the-art. 2. Background 2.1. LLMs as Stochastic Dynamical Systems LLMs can be viewed as stochastic dynamical systems, where the models behavior evolves probabilistically over time, governed by its internal parameters and the inputs In this perspective, each new token is genit receives. erated based on the models evolving hidden state (Kong et al., 2024; Zimmer et al., 2024). Formally, an LLM tran- [ht+1, ot+1]T = fLLM(ht, yt), with sitions as follows: yt SoftMax(Wot). Here, fLLM(), denotes the aggregation of all decoding layers, yt generated token at each time step t, and ot the logits which are linearly mapped by 2.2. Test-Time Alignment of LLMs Test-time alignment ensures that pre-trained LLM generates outputs consistent with desired behaviors by solving MDP, whose initial state is determined by the test prompt. Rewards/costs for alignment can come from various sources, including but not limited to human feedback (Tutnov et al., 2025; Zhong et al., 2024a), environmental feedback from the task or verifiers (Zeng et al., 2023; Yang et al., 2024; Trinh et al., 2024; An et al., 2024; Liang et al., 2024; Mower et al., 2024), or pre-trained reward models (Wang et al., 2024b; Zhang et al., 2024a; Li et al., 2025b). Several approaches have been developed to address the challenge of test-time alignment. For instance, beam search with rewards (Choo et al., 2022) extends traditional beam search technique by integrating reward signal to guide LLM decoding at test time. Monte Carlo tree search, on the other hand, takes more exploratory approach by simulating potential future token sequences to find the path that maximizes the reward function (Zhang et al., 2024a). Bestof-N (BoN) generates multiple candidate sequences and selects the one with the highest reward (Stiennon et al., 2020; Nakano et al., 2021; Touvron et al., 2023). We focus on beam search and look-ahead methods for more scalability when developing InferenceGuard. 3. Safe Test-Time Alignment of LLMs We frame the problem of safely aligning LLMs at test time as constrained Markov decision process (cMDP). As noted in (Achiam et al., 2017; Sootla et al., 2022) cMDP is defined as the following tuple: = S, A, Ctask, Csafety, P, γ, with and denoting the state and action spaces, respectively. The cost function Ctask : dictates the tasks cost2, while Csafety : represents safety cost, which encodes the constraints that the actor must satisfy during inference. The transition model : [0, 1] captures the probability of transitioning to new state given the current state and action. Meanwhile, the discount factor γ [0, 1) trades off immediate versus long-term rewards. The goal of constrained MDPs is to find policy π : [0, 1] that minimizes the tasks cost while simultaneously satisfying 1Note, ht = (cid:110) K(l) , V(l) (cid:111)L l=1 for : [1 : t], i.e., keys and values from all layers accumulated up to the previous time step. 2We define costs as negative rewards, which transforms the problem into an equivalent cost-based MDP. Almost Surely Safe Alignment of Large Language Models at Inference-Time Figure 1. To aid readability, this figure summarizes the key transitions and notations used throughout the paper. the safety constraints. Given safety budget d, we write: EP,π min π s.t. EP,π (cid:104) (cid:88) (cid:105) γtCtask(st, at) (1) (cid:104) (cid:88) γtCsafety(st, at) (cid:105) d, 3.1. Safe Test-Time Alignment as cMDPs We treat the generation process of safe test-time alignment of LLMs as the solution to specific cMDP. We introduce our state variable st = {x, y<t}, which combines the input prompt with the tokens (or partial responses) decoded until step t. Our policy generates new token yt that we treat as an action in the models decision-making process. The transition function of our MDP is deterministic, where the state st is updated by incorporating the generated action yt, i.e., st+1 = st yt = {x, yt}. We also assume the existence of two cost functions Ctask and Csafety to assess the correctness and safety of the LLMs responses. As described in Section 2.2, those functions can originate from various sources, such as human feedback, environmental verifiers, or pre-trained models. While we conduct experiments with these functions being LLMs (see Section 5), our method can be equally applied across various types of task and safety signals. Regarding the tasks cost, we assume the availability of function Ctask that evaluates the alignment of the LLM with the given task. This function assigns costs to the partial response based on the input prompt such that: Ctask([x, yt]) := (cid:40) 0 ctask([x, yt]) if yt = EOS if yt = EOS (2) For the safety cost Csafety, we assume the function assigns non-zero costs to any partial answer without waiting for the final token. This is crucial because we want to flag unsafe responses early rather than waiting until the end of the generation process. Many pre-trained models are available for this purpose on Hugging Face, which we can leveragemore details can be found in Section 5. With this, we write safe test-time alignment as an instance of Equation 1: Eπ min π s.t. Eπ (cid:104) (cid:88) (cid:104) (cid:88) st (cid:125)(cid:124) (cid:105) (cid:122) {x, y<t}, yt) (cid:123) γtCtask( (3) (cid:105) γtCsafe({x, y<t}, yt) d. The above objective aims to minimize the task cost Ctask while ensuring that the safety cost does not exceed predefined budget d. The expectation is taken over the actions (tokens) generated at each timestep. 3.2. State Augmented Safe Inference-Time Alignment We could technically use off-the-shelf algorithms to solve Equation 3, such as applying Lagrangian approach as proposed in (Dai et al., 2023). However, there are two main issues with using these standard algorithms. First, they generally require gradients in the model itselfspecifically, the LLMwhich we want to avoid since our goal is to perform inference-time alignment without retraining the model. Second, these methods rely on tunable Lagrangian multiplier, making it challenging to maximize rewards while satisfying almost sure constraints optimally. Instead of using Lagrangian approach, we take different direction by augmenting the state space and extending the method proposed by (Sootla et al., 2022) to large language models. In our approach, we augment the state space of the constrained MDP with constraint tracker, effectively transforming the problem into an unconstrained one. This allows us to apply Bellman equations and conduct rigorous proofs with almost sure constraint satisfaction results. However, applying the techniques and proofs from (Sootla et al., 2022) to our test-time setting is not entirely straightforward due to two main challenges: first, the differences in the constrained MDP setting, and second, the process by which we train critics, as we will demonstrate next. Augmenting the State Space. The following exposition builds on (Sootla et al., 2022), extending their method to address LLM-specific challengers, an area they did not cover. The core idea is to transform the constrained MDP into an unconstrained one by augmenting the state with an additional variable that tracks the remaining budget of the constraint. While doing so, we must ensure that: PI) our augmented state variable tracks the constraints and maintains the Markovian nature of transition dynamics; and PII) Almost Surely Safe Alignment of Large Language Models at Inference-Time our task cost Ctask accounts for this new state representation and is correctly computed w.r.t. the augmented state. To solve PI, we investigate the evolution of the constraint in Equation 3 and track scaled-version of the remaining safety budget ωt = (cid:80)t k=1 γkCsafety({x, y<k}, yk) that is defined as zt = ωt1/γt. The update of zt satisfies: zt+1 = (ωt1 γtCsafety({x, y<t}, yt))/γt+1 (4) = (zt Csafety({x, y<t}, yt))/γ, with z0 = d. Since the dynamics of zt are Markovian due to the dependence of zt+1 only on zt, yt and current the state {x, y<t}, we can easily augment our original state space with zt, such that st = [st, zt] = [{x, y<t}, zt]. The original dynamics can also be redefined to accommodate for st: original transition (cid:123) (cid:125)(cid:124) (cid:122) st+1 = [ {x, y<t} yt, zt+1], with zt+1 as in Eq. 4. Concerning PII, we note that enforcing the original constraint in Equation 3 is equivalent to enforcing an infinite number of the following constraints: (cid:88) k=0 γkCsafety({x, y<k}, yk) 1. (5) As noted in (Sootla et al., 2022), this observation holds when the instantaneous costs are nonnegative, ensuring that the accumulated safety cost cannot decrease. In our case, it is natural to assume that the costs are nonnegative for LLMs, as safety violations or misalignments in the output typically incur penalty, reflecting the negative impact on the models performance or ethical standards. Clearly, if we enforce zt 0 for all 0, we automatically get that ωt = (cid:80)t k=0 γkCsafety({x, y<k}, yk) 0 for all 0, thus satisfying the infinite constraints in Equation 5. We can do so by reshaping the taskss instantaneous cost to account for the safety constraints: Using gradient-based techniques, one could optimize the augmented MDP in Equation 7. However, since our goal is to enable safety at test time without retraining, we adopt critic-based approach that does not require gradients during inference, as we show next. 4. InferenceGuard: Safety at Test-Time When designing our critic, we considered several crucial factors for test-time inference. These included its size, ease of training for quick adaptation, and flexibility to operate in real-time without significant latency. As such, we chose to train the critic in the latent space of the LLM rather than directly in the textual space, enabling more efficient solution that meets the constraints of test-time alignment. Even if we train the critic in the latent space, the question of what inputs to provide remains. Fortunately, the works of (Kong et al., 2024; Zimmer et al., 2024) demonstrated that LLMs can be viewed as dynamical systems, where ht (hidden state) and ot (logits) serve as state variables that capture sufficient statistics to predict the evolution of the LLM and the generation of new tokens; see Section 2.1 for more details. Those results made ht and ot ideal inputs for our critic4, as they encapsulate the relevant information for evaluating the models behavior during test-time alignment while being relatively low-dimensional, reducing the size of our critics deep network. To fully define our critic, we require representation of the embedding of our augmented state st = [st, zt] within the latent space. As noted above, we can acquire (ht, ot) from the transformer architecture. We call this mapping ϕ, whereby (ht, ot) = ϕ({x, y<t}). Furthermore, we use an identity mapping to embed zt, which enables us to input the actual tracking of the constraints directly to the critic without any loss of information. 4.1. Theoretical Insights task(st, yt) := (cid:40) Ctask([x, yt]) + zt > 0 zt 0, In this section, we show that optimizing in the latent space preserves safety constraints in the original token space, and we prove that our approach guarantees almost sure safety. (6) with Ctask([x, yt]) representing the original MDPs task cost function as described in Equation 2. Of course, in practice, we avoid working with infinities and replace task with Cn task for big > 03. We can now reformulate the constrained problem into an unconstrained one as follows: Eπ min π (cid:104) (cid:88) γt (cid:105) task(st, yt) . (7) 3Note that the introduction of instead of + requires additional theoretical justifications to ensure constraint satisfaction of the true augmented MDP. We carefully handle this in Section 4.1. We should answer two essential questions: i) Can we still compute an optimal policy in the latent space? and ii) If we enforce safety constraints in the latent space, do they still hold in the original token space? While the prior work in (Sootla et al., 2022) established theoretical results for safety-augmented MDPs in standard (non-LLM) RL settings, their work does not address how guarantees in the latent space translate to the original token space. To handle 4In our implementation, we set the variable for the first input to our critic ht = llm-outputs.past-key-values(x, y<t) and ot = llm-outputs.hidden-states(x, y<t)[-1]. 4 Almost Surely Safe Alignment of Large Language Models at Inference-Time those problems, we extend the theorems from (HernandezLerma & Munoz de Ozak, 1992; Sootla et al., 2022) to ensure the following three properties: Prop I) The latent MDP indeed satisfies the Bellman equations (Theorem 1 (a)) and, hence, allows us to compute an optimal policy in this space, Prop II) Policies and value functions in the latent space are valid in the original token space, implying that optimizing in the latent space preserves the constraints in original token space (Theorem 1 (b,c)) Prop III) The resulting policy satisfies safety constraints almost surely (Theorem 2), meaning if policy is safe in the latent and original token space with finite expected cost w.r.t. Equation 7, it is also almost surely safe in the actual LLM token space. We begin by defining the latent space MDPs cost and transition function: Definition 4.1. ϕ() and functions Cn task and such that: embedded aug. state augmented state ϕ({x, y<t}), zt, yt) = Cn Cn task( task( P(ϕ({x, yt}), zt+1ϕ({x, y<t}), zt, yt) = P(st+1st, yt), {x, y<t}, zt, action (cid:120) yt ) where st is the augmented state in the original token space. According to Definition 4.1, we ensure that the cost incurred by the augmented state st = [{x, y<t}, zt] w.r.t. Cn task is equal to the latent cost incurred by the latent state [ϕ({x, y<t}), zt] w.r.t Cn task. Additionally, we ensure that the transition dynamics of the augmented state in the original token space and the corresponding latent state in the latent space are equivalent. This equivalence enables us to derive an optimal policy for the latent MDP and apply it to minimize the cost objective in the original augmented MDP (see Equation 7). We proceed to analyze the existence of such an optimal policy in the latent space through the following standard assumptask, and P: A1. The function tions (Sootla et al., 2022) on Cn Cn task(h, o, z, y) is bounded, measurable, nonnegative, lower semi-continuous w.r.t. (h, o, z) for given y, and A2. The transition law is weakly continuous for any y. Next, we define π as policy in the latent space that maps (h, o, z) y, and its value function for an iniV n(π, h0, o0, z0) = tial state (h0, o0, z0) as follows: t=0 γt Cn Eπ . Then, one can define the optimal value function: (cid:105) task(ht, ot, z, y) (cid:104) (cid:80) ,n(h, o, z) = min π n(π, h, o, z). (8) policy in the latent MDP preserves key properties of the original problem. The following theorem formalizes this by proving the existence of the optimal policy and its mapping to the original MDP. The proof is in Appendix B.3. Theorem 1. (Optimality in the Latent Space) Given the latent MDP in Definition 4.1 and with A1-A2, we can show: a) (Prop I) For any finite n, the Bellman equation holds, i.e., there exists function ,n(h, o, z) such that: (cid:16) Cn task(h, o, z, y) ,n(h, o, z) = min yV (cid:17) + γ ,n(h, o, z) , such that (h, o, z) P(h, o, z, y) Furthermore, the optimal policy solving Equation 8 has the representation π,n( h, o, z); b) (Prop II) The optimal value functions ,n converge monotonically to ,. c) (Prop II) The optimal policy in the latent space π,n is also optimal in the original token space if used as π,n(ϕ()), minimizing Equation 7, even as . The above theorem ensures that finding and securing the existence of the optimal policy in the latent space is sufficient to solve Equation 7 optimally5. Informally, the latent space acts as faithful representation, preserving constraints and making optimization computationally efficient. This implies that the optimal policies and value functions in the latent space remain valid in the original space. Almost Sure Guarantee. Now, we derive Prop III that ensures the safety cost constraints are almost surely satisfied. This is more challenging than Equation 3, where only the expected safety cost is constrained: (cid:104) (cid:88) γtCtask({x, y<t}, yt) (cid:105) (9) γtCsafe({x, y<t}, yt) almost surely. Eπ min π (cid:88) s.t. While the formulation in Equation 9 is stronger than Equation 3, solving for the augmented MDP formulation with objective as Equation 7 can yield policy satisfying the above almost sure constraints. We formally state this result in Theorem 2 and relegate the proof to Appendix B.3. Theorem 2. (Almost Sure Safety) Consider an augmented MDP with cost function task. Suppose an optimal policy exists π solving Equation 7 (see Theorem 1) with finite cost, then π is an optimal policy for Equation 9, i.e., π is safe with probability approaching one or almost surely. Since we cannot optimize directly in the original constrained MDP, we first show that solving for an optimal 5As noted in Section 3.2, we analyze the transition from large to , and confirm that the results hold even for task. 5 Almost Surely Safe Alignment of Large Language Models at Inference-Time 4.2. Algorithm and Practical Implementation We define Ecritic as: Building on our theoretical framework, we propose i) pre-training search algorithm with two approaches: small latent-space critic for cases where costs are available only for complete trajectories and ii) directly leveraging intermediate costs for search optimization. Training Latent-Space Critic. We make the usual assumption that trajectories terminate at maximum length . In this case, the value function simplifies to become: n(ht, ot, zt) = Eπ[γT ctask(hT , oT )] if there is safety budget left, i.e., if zT > 0, or if zT 0, where ctask(ht, ot) = ctask([x, yt]) in the latent MDP. the sign of zT and the Hence, it is sufficient to predict: value of γT ctask(hT , oT ) to assess the quality of state. We estimate those through Monte Carlo (MC) sampling. Specifically, we generate multiple trajectories from the initial state (h0, o0, z0) using the reference policy, and compute the mean terminal cost, the sign of zT to serve as targets for the critic training. The usual alternative to MC sampling is Temporal Difference (TD) learning, where the critic is updated based on the difference between the current estimate and bootstrapped estimate from the next state. However, MC sampling offers two advantages: i) it simplifies training by using separate supervised signals for quality and safety, unlike TD, which combines both, and ii) it allows dynamic adjustment of without retraining. We train critic network with two heads by sampling responses from the base model and scoring them using the cost function. We define J1 as the binary cross-entropy for predicting the sign of zT and J2 as the mean squared error for predicting γT ctask(hT , oT ). Our critic training miniθ (ht, ot, zt), zT > 0(cid:1) + mizes: (θ) = Eπ t=1 J1 θ and 2 θ (cid:104) (cid:80)T θ (ht, ot, zt), γT ctask(hT , oT )(cid:1) (cid:105) (cid:0)f 2 J2 are the heads of our parameterized critic. , where 1 (cid:0)f Search method. We build on the beam search strategy from (Mudgal et al., 2023; Li, 2025) wherein we sequentially sample beams of tokens into set from the pre-trained model and choose beams with the highest scores as possible continuations of the prompt (see Algorithm 1 in Appendix C). This step ensures that we focus on the most promising continuations. The goal of the scoring function is to balance the immediate task cost and the predicted future task cost while ensuring safety. This is repeated until we complete trajectories. Given token trajectory yt:t+d, we present scoring function Ecritic where we assume that we cannot evaluate intermediate answers with the cost functions. However, when immediate safety costs are available, simpler scoring function, see Appendix C. Ecritic(yt:t+d) = γT ctask() 2 θ () + = and zt+d > 0 + = and zt+d 0 1 θ () > 0.5 otherwise. This Ecritic scoring function evaluates token sequences by balancing safety and task performance. At the final step (t + = ), it assigns score based on the task cost Ctask if safety constraints are met (zt+d > 0); otherwise, it applies high penalty n. For intermediate steps, it relies on trained critic. If the critic confidently predicts safety (f 1 θ (ht+d, ot+d, zt+d) > 0.5), it uses the estimated future cost (f 2 θ (ht+d, ot+d, zt+d)); otherwise, it assigns the penalty as conservative safeguard. Sampling Diversity. Finally, if the right selection strategy can guarantee that we will converge on safe solution, it does not consider how many samples would be necessary. To increase the search speed, we introduce diversity term in the sampling distribution when no safe samples were found based on the token frequency of failed beams. We denote as the frequency matrix counting the tokens we previously sampled from to t+d. Instead of sampling from SoftMax(Wot), we sample from SoftMax(Wot n2(Ft > 0)) where n2(Ft > 0) is vector where each component is n2 if Ft,j > 0 and 0 otherwise. The addition of n2(Ft > 0) disables the possibility of sampling the same token at the same position observed in unsuccessful beams, thus increasing diversity. It is worth noting that as we sample from the reference LLM and rank responses directly or via the critic, block sampling ensures small Kullback-Leibler (KL) divergence from the original LLM without explicitly adding KL regularizer into our objective, preserving coherence and natural flow; see (Mudgal et al., 2023). 5. Experiments Baselines. We evaluate the helpfulness (task cost) and harmlessness (safety cost) of our method on both nonsafety-aligned and safety-aligned base models, Alpaca7B (Taori et al., 2023) and Beaver-7B (Ji et al., 2024b). We compare InferenceGuard to the following stateof-the-art test-time alignment methods: Best-of-N (BoN), beam search, ARGS (Khanov et al., 2024) and RECONTROL (Kong et al., 2024). These test-time alignment methods were originally designed to maximize rewards without considering safety. To ensure fair and meaningful comparison, we extend them to incorporate safe alignment for LLMs. We incorporate Lagrangian-based approach for all baselines that penalizes safety violations, ensuring balanced trade-off between task performance and safety. This helps us evaluate our performance against other algorithms and highlights the importance of safety augmentation in6 Almost Surely Safe Alignment of Large Language Models at Inference-Time Figure 2. Trade-offs between safety, reward, and inference time for BoN, ARGS, RECONTROL, Beam Search, and InferenceGuard on the PKU-SafeRLHF test dataset, evaluated for Alpaca-7B (top) and Beaver-v3-7B (bottom). Reward is the average score evaluated by the reward model, safety rate is the percentage of tasks completed within budget d, and inference time is the average per-task duration. The left column (reward vs. safety) and right column (inference time vs. safety) categorize methods by performance. InferenceGuard achieves balanced trade-off, positioning in the Optimal Region and Optimal Efficiency quadrants. Figure 3. Reward and cost distributions on PKU-SafeRLHF test tasks using Alpaca-7B (top) and Beaver-v3 (bottom) as base models. The left y-axis shows reward distribution. The right y-axis shows the maximum cumulative cost. InferenceGuard outperforms others, achieving higher rewards with lower costs across both models. stead of Lagrangian approach for effectively balancing rewards and constraint satisfaction. our comparison, we introduce safety-augmented versions of BoN and beam search as additional baselines. For beam search and Best-of-N (BON), we adopt Lagrangian approach to select solutions with ctask + λCsafety where λ is the Lagrangian multiplier. Similarly, we extend ARGS so that token selection follows: ωπ(t) + ctask + λCsafety, with ω adjusting the influence of the reference policy. We also considered state augmentation for ARGS and RE-control but found it ineffective. Since these methods decode token-by-token, they cannot recover once zt flips the sign, and before that, zt has no influence. Thus, we excluded it from our evaluation. To further strengthen Datasets. We evaluate all the above methods on the PKUSafeRLHF dataset (Ji et al., 2024b), widely recognized benchmark for safety assessment in the LLM literature. This dataset contains 37,400 training samples and 3,400 testing samples. Of course, our training samples are only used to train the critic value network. Here, we construct dataset by generating responses using the base models with five sampled trajectories for each PKU-SafeRLHF prompt from the training set. Almost Surely Safe Alignment of Large Language Models at Inference-Time Evaluation Metrics. We assess the performance using several metrics: the Average Reward is computed using the reward model of (Khanov et al., 2024) as ctask on the complete response to reflect helpfulness, where higher reward indicates better helpfulness; the Average Cost is evaluated with the token-wise cost model from (Dai et al., 2023) as Csafety, indicating harmfulness, with higher cost values reflecting more resource-intensive or harmful outputs; the Safety Rate is defined as the proportion of responses where the cumulative cost does not exceed the safety budget zt=0 = 10, and is formally given by Safety Rate = 1 test zt=0), where is the total number of responses; and Time refers to the inference time taken to generate response in seconds. I(Ci (cid:80)N i=1 Results. We present our main results in Figure 2 and additional ones in Table 2 in Appendix D. InferenceGuard achieves the highest safety rates with both models (91.04% and 100% for Alpaca and Beaver respectively). With Beaver, our method dominates the Patero front, achieving the highest rewards without any unsafe responses. Although Lagrangian methods can have reasonable average cost, they all fail to satisfy the safety constraints. In the meantime, they are too safe on already safe answers, hindering their rewards. The RE-control intervention method underperforms for both models in our setting. ARGS can provide safe answers but with very poor rewards because most answers are very short to avoid breaking the safety constraint. Best-of-N and beam search with augmented safety came after us. However, Best-of-N cannot leverage intermediate signal, and beam search is blind to its previous mistakes, yielding more unsafe answers. Figure 3 provides better view of the reward and safety distributions. The figure shows that InferenceGuard consistently achieves higher rewards while maintaining low cumulative costs, outperforming other methods across both models. Its maximum cumulative cost stays just under the safety budget while maximizing the reward as suggested by our theoretical contributions. Finally, we observe that the trained critic helps to better guide the search on intermediate trajectories in both settings. We also compare generated answers in Appendix qualitatively. 6. Related Work Safe RL: Safe RL employs the cMDP framework, formulated in (Altman, 1999). Without prior knowledge, (Turchetta et al., 2016; Koller et al., 2018; Dalal et al., 2018) focus on safe exploration, while with prior knowledge, (Chow et al., 2018; 2019; Berkenkamp et al., 2017) learn safe policies using control techniques. (Achiam et al., 2017; Ray et al., 2019; Stooke et al., 2020) enforce safety constraints via Lagrangian or constrained optimization which can often lead to suboptimal 8 safety-reward trade-offs. Instead, we extend safety state augmentation (Sootla et al., 2022) to LLMs and latent MDPs to ensure almost sure inference time safety. LLM alignment and safety: Pre-trained LLMs are often aligned to specific tasks using RL from Human Feedback (RLHF), where LLMs are fine-tuned with learned reward model (Stiennon et al., 2020; Ziegler et al., 2019; Ouyang et al., 2022) or directly optimized from human preferences (Rafailov et al., 2023; Azar et al., 2023; Zhao et al., 2023; Tang et al., 2024; Song et al., 2024; Ethayarajh et al., 2024). (Bai et al., 2022; Ganguli et al., 2022) first applied fine-tuning in the context of safety, and (Dai et al., 2023) proposed safe fine-tuning via Lagrangian optimization. Other safety-focused methods such as (Gundavarapu et al., 2024; Gou et al., 2024; Hammoud et al., 2024; Hua et al., 2024; Zhang et al., 2024b; Guo et al., 2024; Xu et al., 2024; Wei et al., 2024; Li et al., 2025a) are either orthogonal, handle different problem to ours, or can not ensure almost sure safety during inference. Inference time alignment: Inference-time alignment of LLMs is often performed through guided decoding, which steers token generation based on rewards (Khanov et al., 2024; Shi et al., 2024; Huang et al., 2024) or trained value function Han et al. (2024); Mudgal et al. (2023); Kong et al. (2024). Other safety-focused inference-time methods include (Zhong et al., 2024b; Banerjee et al., 2024; Niu et al., 2024; Wang et al., 2024a; Zeng et al., 2024; Zhao et al., 2024). Compared to those methods, we are the first to theoretically guarantee almost sure safe alignment with strong empirical results. Operating in the latent space enables us to train smaller, inference-efficient critics while optimally balancing rewards and safety constraints without introducing extra parameters, e.g., Lagrangian multipliers. We detail other related works extensively in Appendix A. 7. Conclusion We introduced InferenceGuard, novel inferencetime alignment method that ensures large LLMs generate safe responses almost surelyi.e., with probability approaching one. We extended prior safetyaugmented MDP theorems into the latent space of LLMs and conducted new analysis. Our results demonstrated that InferenceGuard significantly outperforms existing test-time alignment methods, achieving state-of-the-art safety versus reward tradeoff results. In the future, we plan to improve the algorithms efficiency further and generalize our setting to cover jailbreaking. While our method is, in principle, extendable to jailbreaking settings, we aim to analyze whether our theoretical guarantees still hold. Almost Surely Safe Alignment of Large Language Models at Inference-Time"
        },
        {
            "title": "Broader Impact Statement",
            "content": "This work contributes to the safe and responsible deployment of large language models (LLMs) by developing InferenceGuard, an inference-time alignment method that ensures almost surely safe responses. Given the increasing reliance on LLMs across various domains, including healthcare, education, legal systems, and autonomous decision-making, guaranteeing safe and aligned outputs is crucial for mitigating misinformation, bias, and harmful content risks. To further illustrate the effectiveness of our approach, we have included additional examples in the appendix demonstrating that our method successfully produces safe responses. These examples were generated using standard prompting with available large language models LLMs. Additionally, we have added warning at the beginning of the manuscript to inform readers about the nature of these examples. Our primary motivation for this addition is to highlight the safety improvements achieved by our method compared to existing alternatives. We do not foresee these examples being misused in any unethical manner, as they solely showcase our models advantages in ensuring safer AI interactions. Finally, we emphasize that our method is designed specifically to enhance AI safety, and as such, we do not anticipate any potential for unethical applications. InferenceGuard enhances the scalability and adaptability of safe AI systems by introducing formally grounded safety mechanism that does not require model retraining while reducing the resource costs associated with traditional RLHF methods. The proposed framework advances AI safety research by providing provable safety guarantees at inference time, an area that has received limited attention in prior work. While this method significantly improves safety in LLM outputs, it does not eliminate all potential risks, such as adversarial manipulation or emergent biases in model responses. Future work should explore robustness to adversarial attacks, contextual fairness, and ethical considerations in deploying safety-aligned LLMs across different cultural and regulatory landscapes. Additionally, transparency and accountability in AI safety mechanisms remain essential for gaining public trust and ensuring alignment with societal values. This work aims to empower developers and policymakers with tools for ensuring safer AI deployment while contributing to the broader conversation on AI ethics and governance."
        },
        {
            "title": "Acknowledgments",
            "content": "Ziomek, and Zafeirios Fountas for their help in improving the manuscript. We also would like to thank the original team from Pekin University, who made their code available and ensured we could reproduce their results."
        },
        {
            "title": "References",
            "content": "Achiam, J., Held, D., Tamar, A., and Abbeel, P. Constrained policy optimization. In Proceedings of the 34th International Conference on Machine Learning (ICML), pp. 2231, 2017. Akametalu, A. K., Fisac, J. F., Zeilinger, M. N., Kaynama, S., Gillula, J., and Tomlin, C. J. Reachability-based safe learning with gaussian processes. In Proceedings of the 53rd IEEE Conference on Decision and Control (CDC), pp. 14241431. IEEE, 2014. Altman, E. Constrained Markov Decision Processes: Stochastic Modeling. CRC Press, 1999. An, C., Chen, Z., Ye, Q., First, E., Peng, L., Zhang, J., Wang, Z., Lerner, S., and Shang, J. Learn from failure: Fine-tuning llms with trial-and-error data for intuitionistic propositional logic proving. arXiv preprint arXiv:2404.07382, 2024. Arora, S., Li, J., Raji, D., et al. Director: GeneratorIn Proclassifiers for supervised language modeling. ceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1478 1489. Association for Computational Linguistics, 2022. Azar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello, D., Valko, M., and Munos, R. general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036, 2023. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with arXiv reinforcement learning from human feedback. preprint arXiv:2204.05862, 2022. Banerjee, S., Tripathy, S., Layek, S., Kumar, S., Mukherjee, A., and Hazra, R. Safeinfer: Context adaptive decoding time safety alignment for large language models. arXiv preprint arXiv:2406.12274, 2024. Berkenkamp, F., Turchetta, M., Schoellig, A. P., and Krause, A. Safe model-based reinforcement learning with stability guarantees. In Advances in Neural Information Processing Systems, pp. 908918, 2017. The authors would like to thank Rasul Tutunov, Abbas Shimary, Filip Vlcek, Victor Prokhorov, Alexandre Maraval, Aivar Sootla, Yaodong Yang, Antonio Filieri, Juliusz Bertsekas, D. and Shreve, S. E. Stochastic optimal control: the discrete-time case, volume 5. Athena Scientific, 1996. 9 Almost Surely Safe Alignment of Large Language Models at Inference-Time Bertsekas, D. P. Dynamic programming: deterministic and stochastic models. Prentice-Hall, Inc., 1987. Bharadhwaj, H., Chow, Y., Ghavamzadeh, M., Pavone, M., and Sangiovanni-Vincentelli, A. Conservative safety critics for exploration. In Proceedings of the 37th International Conference on Machine Learning, pp. 923932, 2020. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Cheng, R., Orosz, G., Murray, R. M., and Burdick, J. W. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS), 2019. Choo, J., Kwon, Y.-D., Kim, J., Jae, J., Hottung, A., Tierney, K., and Gwon, Y. Simulation-guided beam search Advances in for neural combinatorial optimization. Neural Information Processing Systems, 35:87608772, 2022. Chow, Y., Nachum, O., Duenez-Guzman, E., LyapunovGhavamzadeh, M., and Pavone, M. based safe policy optimization for continuous control. In Proceedings of the 35th International Conference on Machine Learning, pp. 13151324, 2018. Chow, Y., Nachum, O., Duenez-Guzman, E., Ghavamzadeh, M., and Pavone, M. Lyapunovbased safe policy optimization for continuous control. arXiv preprint arXiv:1901.10031, 2019. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Dai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang, Y., and Yang, Y. Safe rlhf: Safe reinforcement learning from arXiv preprint arXiv:2310.12773, human feedback. 2023. Dalal, G., Gilboa, E., Mannor, S., and Shashua, A. Safe exploration in continuous action spaces. In Proceedings of the 35th International Conference on Machine Learning, pp. 14371446, 2018. Dean, S., Fisac, J. F., Tomlin, C. J., and Recht, B. Safeguarding resource-constrained cyber-physical systems In Proceedings of the 36th Inwith adaptive control. ternational Conference on Machine Learning, pp. 1664 1673, 2019. Deshpande, A., Murahari, V., Rajpurohit, T., Kalyan, A., Toxicity in chatgpt: Analyzand Narasimhan, K. ing persona-assigned language models. arXiv preprint arXiv:2304.05335, 2023. Ding, Y., Deisenroth, M. P., and Trimpe, S. Natural policy gradient for safe reinforcement learning with c-mdps. In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 2825 2835, 2020. Dong, Z., Zhou, Z., Yang, C., Shao, J., and Qiao, Y. Attacks, defenses and evaluations for llm conversation safety: survey, 2024. URL https://arxiv.org/ abs/2402.09283. Dynkin, E. B. and Yushkevich, A. A. Controlled markov processes, volume 235. Springer, 1979. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Feng, D., Qin, B., Huang, C., Huang, Y., Zhang, Z., and Lei, W. Legend: Leveraging representation engineering to annotate safety margin for preference datasets. arXiv preprint arXiv:2406.08124, 2024. Fisac, J. F., Akametalu, A. K., Zeilinger, M. N., Kaynama, S., Gillula, J., and Tomlin, C. J. Bridging model-based safety and model-free reinforcement learning through system identification and safety-critical control. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pp. 26032610, 2019. Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 1076410799. PMLR, 2023. Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462, 2020. Gou, Y., Chen, K., Liu, Z., Hong, L., Xu, H., Li, Z., Yeung, D.-Y., Kwok, J. T., and Zhang, Y. Eyes closed, safety on: Protecting multimodal llms via image-to-text transformation. arXiv preprint arXiv:2403.09572, 2024. Gundavarapu, S. K., Agarwal, S., Arora, A., and Jagadeeshaiah, C. T. Machine unlearning in large language models. arXiv preprint arXiv:2405.15152, 2024. Almost Surely Safe Alignment of Large Language Models at Inference-Time Guo, X., Yu, F., Zhang, H., Qin, L., and Hu, B. Coldattack: Jailbreaking llms with stealthiness and controllability. arXiv preprint arXiv:2402.08679, 2024. Hammoud, H. A. A. K., Michieli, U., Pizzati, F., Torr, P., Bibi, A., Ghanem, B., and Ozay, M. Model merging and safety alignment: One bad model spoils the bunch. arXiv preprint arXiv:2406.14563, 2024. Han, S., Shenfeld, I., Srivastava, A., Kim, Y., and Agrawal, P. Value augmented sampling for language model alignment and personalization. arXiv preprint arXiv:2405.06639, 2024. Hernandez-Lerma, O. and Munoz de Ozak, M. Discretetime markov control processes with discounted unbounded costs: optimality criteria. Kybernetika, 28(3): 191212, 1992. Hua, W., Yang, X., Jin, M., Li, Z., Cheng, W., Tang, R., and Zhang, Y. Trustagent: Towards safe and trustworthy llmbased agents through agent constitution. In Trustworthy Multi-modal Foundation Models and AI Agents (TiFA), 2024. Huang, J. Y., Sengupta, S., Bonadiman, D., Lai, Y.-a., Gupta, A., Pappas, N., Mansour, S., Kirchhoff, K., and Roth, D. Deal: Decoding-time alignment for large language models. arXiv preprint arXiv:2402.06147, 2024. URL https://arxiv.org/abs/2402.06147. Inan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., Tontchev, M., Hu, Q., Fuller, B., Testuggine, D., et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674, 2023. Ji, J., Chen, B., Lou, H., Hong, D., Zhang, B., Pan, X., Dai, J., Qiu, T., and Yang, Y. Aligner: Efficient alignment by learning to correct. arXiv preprint arXiv:2402.02416, 2024a. Ji, J., Hong, D., Zhang, B., Chen, B., Dai, J., Zheng, B., Qiu, T., Li, B., and Yang, Y. Pku-saferlhf: safety alignment preference dataset for llama family models. arXiv preprint arXiv:2406.15513, 2024b. Khanov, M., Burapacheep, J., and Li, Y. Alignment as reward-guided search. arXiv:2402.01694, 2024. Args: arXiv preprint Kim, S. et al. Using critic in an actor-critic framework for controlled text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 21052118. Association for Computational Linguistics, 2023. 11 Kim, Y., Kim, D., Choi, J., Park, J., Oh, N., and Park, D. survey on integration of large language models with Intelligent Service Robotics, 17(5): intelligent robots. 10911107, August 2024. ISSN 1861-2784. doi: 10. 1007/s11370-024-00550-5. URL http://dx.doi. org/10.1007/s11370-024-00550-5. Koh, H. Y., Ju, J., Liu, M., and Pan, S. An empirical survey on long document summarization: Datasets, models, and metrics. ACM computing surveys, 55(8):135, 2022. Koller, T., Berkenkamp, F., Turchetta, M., and Krause, A. Learning-based model predictive control for safe exploration. In 2018 IEEE Conference on Decision and Control (CDC), pp. 60596066. IEEE, 2018. Kong, L., Wang, H., Mu, W., Du, Y., Zhuang, Y., Zhou, Y., Song, Y., Zhang, R., Wang, K., and Zhang, C. Aligning large language models with representation editing: control perspective. arXiv preprint arXiv:2406.05954, 2024. Krause, B., Goyal, S., et al. Gedi: Generative discriminator guided sequence generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 506519. Association for Computational Linguistics, 2021. Li, M., Si, W. M., Backes, M., Zhang, Y., and Wang, Y. Salora: Safety-alignment preserved low-rank adaptation. arXiv preprint arXiv:2501.01765, 2025a. Li, S., Dong, S., Luan, K., Di, X., and Ding, C. Enhancing reasoning through process supervision with monte carlo tree search. arXiv preprint arXiv:2501.01478, 2025b. Li, X. survey on llm test-time compute via search: Tasks, llm profiling, search algorithms, and relevant frameworks. arXiv preprint arXiv:2501.10069, 2025. Liang, J., Xia, F., Yu, W., Zeng, A., Arenas, M. G., Attarian, M., Bauza, M., Bennice, M., Bewley, A., Dostmohamed, A., et al. Learning to learn faster from human feedback with language model predictive control. arXiv preprint arXiv:2402.11450, 2024. Meng, Y. et al. Nado: Near-autoregressive decoding optimization for text generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 11571168. Association for Computational Linguistics, 2022. Mower, C. E., Wan, Y., Yu, H., Grosnit, A., GonzalezBillandon, J., Zimmer, M., Wang, J., Zhang, X., Zhao, Y., Zhai, A., Liu, P., Palenicek, D., Tateo, D., Cadena, C., Hutter, M., Peters, J., Tian, G., Zhuang, Y., Shao, K., Quan, X., Hao, J., Wang, J., and Bou-Ammar, H. Ros-llm: ros framework for embodied ai with task Almost Surely Safe Alignment of Large Language Models at Inference-Time feedback and structured reasoning, 2024. URL https: //arxiv.org/abs/2406.19741. Mudgal, S., Lee, J., Ganapathy, H., Li, Y., Wang, T., Huang, Y., Chen, Z., Cheng, H.-T., Collins, M., Strohman, T., Chen, J., Beutel, A., and Beirami, A. Controlled decoding from language models. arXiv preprint arXiv:2310.17022, 2023. URL https://arxiv. org/abs/2310.17022. Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Niu, T., Xiong, C., Yavuz, S., and Zhou, Y. Parameterefficient detoxification with contrastive decoding. arXiv preprint arXiv:2401.06947, 2024. Ohnishi, M., Nakka, A., and Chowdhary, G. Barriercertified adaptive reinforcement learning with applicaIn Proceedings of the tions to brushbot navigation. 36th International Conference on Machine Learning, pp. 50425051, 2019. Ray, A., Achiam, J., and Amodei, D. Benchmarking safe exploration in deep reinforcement learning. In Proceedings of the 2nd Conference on Robot Learning (CoRL), pp. 113, 2019. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms, URL https://arxiv.org/abs/1707. 2017. 06347. Shi, R., Chen, Y., Hu, Y., Liu, A., Smith, N., Hajishirzi, H., and Du, S. Decoding-time language model arXiv preprint alignment with multiple objectives. arXiv:2406.18853, 2024. Song, F., Yu, B., Li, M., Yu, H., Huang, F., Li, Y., and Wang, H. Preference ranking optimization for human alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024. Sootla, A., Cowen-Rivers, A. I., Jafferjee, T., Wang, Z., Mguni, D. H., Wang, J., and Bou-Ammar, H. Saute rl: Almost surely safe reinforcement learning using state augmentation. In International Conference on Machine Learning, pp. 2042320443. PMLR, 2022. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33: 30083021, 2020. Peng, X., Kumar, A., et al. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. In Proceedings of the 2022 Conference on Advances in Neural Information Processing Systems (NeurIPS), pp. 110. Neural Information Processing Systems Foundation, 2019. Qi, X., Panda, A., Lyu, K., Ma, X., Roy, S., Beirami, A., Mittal, P., and Henderson, P. Safety alignment should be made more than just few tokens deep. arXiv preprint arXiv:2406.05946, 2024. Qin, L., Welleck, S., Khashabi, D., and Choi, Y. Cold decoding: Energy-based constrained text generation with langevin dynamics. Advances in Neural Information Processing Systems, 35:95389551, 2022. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model. arXiv preprint arXiv:2305.18290, 2023. Ramesh, S. S., Hu, Y., Chaimalas, I., Mehta, V., Sessa, P. G., Ammar, H. B., and Bogunovic, I. Group robust preference optimization in reward-free rlhf. arXiv preprint arXiv:2405.20304, 2024. Stooke, A., Achiam, J., and Abbeel, P. Responsive safety in reinforcement learning by monitoring risk and adapting policies. In Proceedings of the 37th International Conference on Machine Learning (ICML), pp. 89498958, 2020. Tang, Y., Guo, Z. D., Zheng, Z., Calandriello, D., Munos, R., Rowland, M., Richemond, P. H., Valko, M., Pires, B. A., and Piot, B. Generalized preference optimization: unified approach to offline alignment. arXiv preprint arXiv:2402.05749, 2024. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Trinh, T. H., Wu, Y., Le, Q. V., He, H., and Luong, T. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. 12 Almost Surely Safe Alignment of Large Language Models at Inference-Time Turchetta, M., Berkenkamp, F., and Krause, A. Safe exploration in finite markov decision processes with gaussian processes. In Advances in Neural Information Processing Systems, pp. 43124320, 2016. Tutnov, R., Grosnit, A., and Bou-Ammar, H. Many of your dpos are secretly one: Attempting unification through mutual information, 2025. URL https://arxiv. org/abs/2501.01544. Wachi, A. and Sui, Y. Safe exploration and optimization of constrained mdps using gaussian processes. In Proceedings of the 36th International Conference on Machine Learning, pp. 36603669, 2018. Wang, H., Wu, B., Bian, Y., Chang, Y., Wang, X., and Zhao, P. Probing the safety response boundary of large language models via unsafe decoding path generation. arXiv preprint arXiv:2408.10668, 2024a. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce In Prollms step-by-step without human annotations. ceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024b. Wei, B., Huang, K., Huang, Y., Xie, T., Qi, X., Xia, M., Mittal, P., Wang, M., and Henderson, P. Assessing the brittleness of safety alignment via pruning and low-rank modifications. arXiv preprint arXiv:2402.05162, 2024. Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M., Balle, Ethical and social risks B., Kasirzadeh, A., et al. arXiv preprint of harm from language models. arXiv:2112.04359, 2021. Xu, Z., Jiang, F., Niu, L., Jia, J., Lin, B. Y., and Poovendran, R. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. arXiv preprint arXiv:2402.08983, 2024. Yang, F., Nishio, M., and Ishii, S. Relative value learning for constrained reinforcement learning. In Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS), pp. 1664616656, 2019. Yang, K., Swope, A., Gu, A., Chalamala, R., Song, P., Yu, S., Godil, S., Prenger, R. J., and Anandkumar, A. Leandojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems, 36, 2024. Yang, T. B. and Klein, D. Fudge: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 121132. Association for Computational Linguistics, 2021. 13 Yin, Y., Wang, Z., Gu, Y., Huang, H., Chen, W., and Zhou, M. Relative preference optimization: Enhancing llm alignment through contrasting responses across identical and diverse prompts, 2024. URL https: //arxiv.org/abs/2402.10958. Zeng, F., Gan, W., Wang, Y., Liu, N., and Yu, P. S. Large language models for robotics: survey. arXiv preprint arXiv:2311.07226, 2023. Zeng, X., Shang, Y., Zhu, Y., Chen, J., and Tian, Y. Root defence strategies: Ensuring safety of llm at the decoding level. arXiv preprint arXiv:2410.06809, 2024. Zhang, D., Zhoubian, S., Hu, Z., Yue, Y., Dong, Y., Rest-mcts*: Llm self-training via arXiv preprint and Tang, J. process reward guided tree search. arXiv:2406.03816, 2024a. Zhang, J., Elgohary, A., Magooda, A., Khashabi, D., and Van Durme, B. Controllable safety alignment: Inference-time adaptation to diverse safety requirements. arXiv preprint arXiv:2410.08968, 2024b. Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. Zhao, Z., Zhang, X., Xu, K., Hu, X., Zhang, R., Du, Z., Guo, Q., and Chen, Y. Adversarial contrastive decoding: Boosting safety alignment of large language models via opposite prompt optimization. arXiv preprint arXiv:2406.16743, 2024. Zhong, H., Feng, G., Xiong, W., Zhao, L., He, D., Bian, J., and Wang, L. Dpo meets ppo: Reinforced token optimization for rlhf. arXiv preprint arXiv:2404.18922, 2024a. Zhong, Q., Ding, L., Liu, J., Du, B., and Tao, D. Rose doesnt do that: Boosting the safety of instruction-tuned large language models with reverse prompt contrastive decoding. arXiv preprint arXiv:2402.11889, 2024b. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Finetuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. Zimmer, M., Gritta, M., Lampouras, G., Ammar, H. B., and Wang, J. Mixture of attentions for speculative decoding, 2024. URL https://arxiv.org/abs/ 2410.03804. Almost Surely Safe Alignment of Large Language Models at Inference-Time A. Additional Related Work Safe RL: Safe RL employs the cMDP framework (Altman, 1999) to enforce safety constraints during exploration and policy optimization. When no prior knowledge is available, methods focus on safe exploration (Turchetta et al., 2016; Koller et al., 2018; Dalal et al., 2018; Wachi & Sui, 2018; Bharadhwaj et al., 2020). With prior knowledge, such as environmental data or an initial safe policy, methods learn safe policies using control techniques like Lyapunov stability (Chow et al., 2018; 2019; Berkenkamp et al., 2017; Ohnishi et al., 2019) and reachability analysis (Cheng et al., 2019; Akametalu et al., 2014; Dean et al., 2019; Fisac et al., 2019). Safety constraints are enforced via Lagrangian or constrained optimization methods (Achiam et al., 2017; Ray et al., 2019; Stooke et al., 2020; Yang et al., 2019; Ding et al., 2020; Ji et al., 2024b), but can often lead to suboptimal safety-reward trade-offs. In contrast, our approach extends safety state augmentation (Sootla et al., 2022) to LLMs and latent MDPs to ensure almost sure inference time safety without relying on Lagrangian multipliers. LLM alignment and safety: Methods for aligning pre-trained LLMs with task-specific data include prompting, guided decoding, and fine-tuning. Among fine-tuning methods, RL from Human Feedback (RLHF) has proven effective, where LLMs are fine-tuned with learned reward model (Stiennon et al., 2020; Ziegler et al., 2019; Ouyang et al., 2022) or directly optimized from human preferences (Rafailov et al., 2023; Azar et al., 2023; Zhao et al., 2023; Tang et al., 2024; Song et al., 2024; Ethayarajh et al., 2024; Ramesh et al., 2024). Recent works have explored fine-tuning for helpful and harmless responses (Bai et al., 2022; Ganguli et al., 2022), while (Dai et al., 2023) introduced safe RL approach incorporating safety cost functions via Lagrangian optimization, requiring model weight fine-tuning. Other safety-focused methods, including machine unlearning (Gundavarapu et al., 2024), safety pre-aligned multi-modal LLMs (Gou et al., 2024), safety-aware model merging (Hammoud et al., 2024), prompting-based safety methodologies (Hua et al., 2024), test-time controllable safety alignment (Zhang et al., 2024b), defenses against adversarial attacks and jailbreaking (Guo et al., 2024; Qi et al., 2024; Xu et al., 2024), identifying safety criticial regions in LLMs (Wei et al., 2024), safety preserved LoRA fine-tuning (Li et al., 2025a), alignment using correctional residuals between preferred and dispreferred answers using small model (Ji et al., 2024a), and identifying safety directions in embedding space (Feng et al., 2024).Those methods are either orthogonal, handle different problem to ours, or can not ensure almost sure safety during inference. Inference time alignment: The closest literature to ours is inference-time alignment. Those methods offer flexible alternatives to fine-tuning LLMs, as they avoid modifying the model weights. common approach is guided decoding, which steers token generation based on reward model. In particular, (Khanov et al., 2024; Shi et al., 2024; Huang et al., 2024) perform this guided decoding through scores from the reward model whereas Han et al. (2024); Mudgal et al. (2023); Kong et al. (2024) use value function that is trained on the given reward model. These inference-time alignment methods build on previous works like (Yang & Klein, 2021; Arora et al., 2022; Krause et al., 2021; Kim et al., 2023; Meng et al., 2022; Peng et al., 2019), which guide or constrain LLMs towards specific objectives. Other safety-focused inference-time methods include, reverse prompt contrastive decoding (Zhong et al., 2024b), adjusting model hidden states combined with guided decoding (Banerjee et al., 2024), soft prompt-tuned detoxifier based decoding (Niu et al., 2024), jail-break value decoding (Wang et al., 2024a), speculating decoding using safety classifier (Zeng et al., 2024), and opposite prompt-based contrastive decoding (Zhao et al., 2024). Compared to those methods, we are the first to achieve almost sure safe alignment with strong empirical results. Operating in the latent space enables us to train smaller, inference-efficient critics while optimally balancing rewards and safety constraints (see Section 5) without introducing additional parameters like Lagrangian multipliers. B. Theoretical Analysis For our theoretical results, we consider similar setup to that of Sootla et al. (2022); Hernandez-Lerma & Munoz de Ozak (1992) but with discrete action space. Consider an MDP = {S, A, P, c, γc} with discrete, non-empty, and finite action set for each defined as A(s). The set = {(s, a) S, A(s)} defines the admissible state-action pairs and is assumed to be Borel subset of A. function is inf-compact on if the set {a A(s) u(s, a) r} is compact for every and R. Note that, since the action space is finite and discrete every function is inf-compact on K. function is lower semi-continuous (l.s.c.) in if for every s0 we have lim inf ss0 u(s) u(s0). 14 Almost Surely Safe Alignment of Large Language Models at Inference-Time Let L(S) denote the class of all functions on that are l.s.c. and bounded from below. For given action, A(s), distribution (y s, a) is called weakly continuous w.r.t. s, if for any function u(s), continuous and bounded w.r.t. on S, the map (s, a) (cid:55) (cid:90) u(y)P (dy s, a) is continuous on for given a. We also make the following assumptions: B1. The function c(s, a) is bounded, measurable on K, nonnegative, lower semi-continuous w.r.t. for given A(s); B2. The transition law is weakly continuous w.r.t. for given A(s); B3. The set-value function map : A(s) satisfies the following, s0 S, there exists ϵ > 0, such that satisfying x0 ϵ, A(x) = A(x0) Note that, the assumptions B1-B3, share similar essence to that of the Assumptions 2.1-2.3 in Hernandez-Lerma & Munoz de Ozak (1992) and B1-B3 in Sootla et al. (2022) but suited for discrete action space. In particular, Assumption B3, is similar to the lower semi continuity assumption on the set-value function map A(s) taken in Sootla et al. (2022); Hernandez-Lerma & Munoz de Ozak (1992) but modified for discrete action space. Our first goal is to recreate Hernandez-Lerma & Munoz de Ozak (1992, Lemma 2.7) for our discrete action setting. Let Π denote the set of functions from A. Lemma 3. (a) If Assumption B3 holds and v(s, a) is l.s.c. w.r.t. for any given A(s) and bounded from below on K, then the function belongs to L(S) and, furthermore, there is function π Π such that v(s) = v(s, π(s)) S. v(s) := inf aA(s) v(s, a) (b) If the Assumptions B1-B3 hold, and L(S) is nonnegative, then the (nonnegative) function u(s) := inf aA (cid:20) c(s, a) + (cid:90) u(y)P (dy s, a) (cid:21) belongs to L(S), and there exists π Π such that u(s) = c(s, π(s)) + (cid:90) u(y)P (dy s, π(s)) S. (c) For each = 0, 1, . . . , let vn be l.s.c. function, bounded from below. If vn v0 as , then lim inf aA(s) vn(s, a) = inf aA(s) v0(s, a) S. Proof. For part a) We have v(s, a) is l.s.c. w.r.t. for any given a. This implies from the definition of lower semi -continuity for any s0 and A(s0), if v(s0, a) > y, then there exists ϵ > 0, s.t. satisfying s0 ϵ, v(s, a) > y. Assume for some s0 and y, the function inf aA(s0) v(s0, a) satisfies, inf aA(s0) v(s0, a) > v(s0, a) > A(s0) (10) Using Assumption B3, we have A(s) = A(s0) for s0 ϵ. Moreover, using the fact that v(s, a) is l.s.c. at given a, we have if v(s0, a) > y, A(s0) we have, v(s, a) > s0 ϵ, A(s) (11) 15 Almost Surely Safe Alignment of Large Language Models at Inference-Time Since, this holds for all A(s), it also holds for inf aA(s) v(s, a) > s0 ϵ (12) This proves the lower semi continuity of v(s) = inf aA(s) v(s, a). Further, due to the discrete nature of A(s), the inf aA(s) v(s, a) is always attained by an action π(s) A(s). Hence, there exists function Π : > A(s) s.t. v(s) = inf aA(s) v(s, a) = v(s, π(s)) S. (13) For part b), note that c(s, a) + (cid:82) u(y)P (dy s, a) is l.s.c. w.r.t. for an given a, based on Assumptions B1-B2. Hence, using part a) we have, u(s) := inf aA (cid:20) c(s, a) + (cid:90) u(y)P (dy s, a) = c(s, π(s)) + (cid:21) (cid:90) u(y)P (dy s, π(s)) L(S) S (14) for some Π. For part c), we begin by defining l(s) = limn inf aA(s) vn(s, a). Note that, since {vn} is an increasing sequence, we have for any This implies, Next, we define for any S, inf aA(s) vn(s, a) inf aA(s) v0(s, a) l(s) inf aA(s) v0(s, a) = 0(s) An := {a A(s)vn(s, a) 0(s)} (15) (16) (17) We note that An are compact sets as is finite and discrete. Further, note that An is decreasing sequence converging to A0 (compact, decreasing and bounded from below by A0). Also, note that We consider the sequence {an} where an An and an satisfies, A1 A2 A3 A0 vn(s, an) = inf aA(s) vn(s, a) inf aA(s) v0(s, a) 0(s) (18) (19) This sequence {an} belongs to the compact space to n=1An = A1. n=1An = A1, hence it has convergent subsequence {ani } converging ani Ani = nniAn a0 nAn = A0 (20) (21) Since, the converging sequence ani a0 belongs to the discrete, compact space, there exisits Ni, such that for all ni Ni, ani = a0. Further, using the increasing nature of vn, we have, As , this implies, vni(s, ani) vn(s, ani) ni lim As vn v0, l(s) v0(s, a0) = 0(s). lim inf aA(s) vni(s, ani) vn(s, a0) vni(s, a) vn(s, a0) l(s) vn(s, a0) 16 (22) (23) (24) (25) Almost Surely Safe Alignment of Large Language Models at Inference-Time B.1. Optimality Equation Next, we characterize the solution to the Bellman (Optimality) equation. We begin by recalling the Bellman operator: (cid:26) v(s) = min aA(s) c(s, a) + γ (cid:90) v(y)P (dy s, a) . (cid:27) To state our next result we introduce some notation: Let L(S)+ be the class of nonnegative and l.s.c. functions on S, and for each L(S)+ by Lemma 3(b), the operator maps L(S)+ into itself. We also consider the sequence {vn} of value iteration (VI) functions defined recursively by That is, for 1 and S, v0(S) := 0, and vh := vh1 for = 1, 2, . . . (cid:18) vh(s) := min aA(s) c(s, a) + vh1(y)P (dy s, a) . (cid:19) (cid:90) (4.3) Note that, by induction and Lemma 3(b) again, vh L(S)+ for all 0. From elementary Dynamic Programming Bertsekas (1987); Bertsekas & Shreve (1996); Dynkin & Yushkevich (1979), vh(s) is the optimal cost function for an h-stage problem (with terminal cost v0(s) = 0) given s0 = s; i.e., where, Π is the set of policies and VH (π, s) denotes the value function for the Hstage problem: vh(s) = inf π Vh(π, s), VH (π, s0) = Eπ (cid:34)H1 (cid:88) (cid:35) γhc(sh, ah) . h=0 Here, Eπ stands for the expectation with actions sampled according to the policy π and the transitions . For , let the value functions be denoted as follows: and (π, s0) = Eπ (cid:35) γhc(sh, ah) , (cid:34) (cid:88) h=0 (s) = inf π (π, s). We want to prove similar results to that of Hernandez-Lerma & Munoz de Ozak (1992, Theorem 4.2) on the optimality of the Bellman operator, however in the discrete action setting. In particular, we want to show the following theorem Theorem 4. Suppose that Assumptions B1-B3 hold, then: (a) vh ; hence (b) is the minimal pointwise function in L(S)+ that satisfies = Proof. We follow similar proof strategy to that of Hernandez-Lerma & Munoz de Ozak (1992, Theorem 4.2). To begin, note that the operator is monotone on L(S)+, i.e., > implies > v. Hence {vh} forms nondecreasing sequence in L(S)+ and, therefore, there exists function L(S)+ such that vh u. This implies (by the Monotone Convergence Theorem) that (cid:90) c(s, a) + vh1(y)P (dy s, a) c(s, a) + (cid:90) u(y)P (dy s, a), 17 Almost Surely Safe Alignment of Large Language Models at Inference-Time Using Lemma 3(c), and vh = inf aA(s){c(s, a) + (cid:82) vh1(y)P (dy s, a)} yields lim inf aA(s) {c(s, a) + (cid:90) vh1(y)P (dy s, a)} = inf aA(s) (cid:90) {c(s, a) + u(y)P (dy s, a)}, lim vh = u, = u. This shows vh u, such that L(S)+ satisfies the Optimality equation. Next, we want to show = . Using that u, and by Lemma 3(b), we have that there exists π Π, stationary policy that satisfies u(s) inf {c(s, a) + aA(s) (cid:90) u(y)P (dy s, a)} c(x, π) + α (cid:90) u(y)P (dy s, π) s. Applying the operator iteratively, we have u(s) u(s) s, H1 (cid:88) E{sh},π[ h=0 αhc(sh, π)] + αH (cid:90) u(y)P (dy s, π) s, H, (26) (27) (28) (29) where (B s, π) = ({sH B}) denotes the H-step transition probability of the Markov chain {sh} (see HernandezLerma & Munoz de Ozak (1992, Remarks 3.1,3.2)). Therefore, since is nonnegative, Letting , we obtain Next, note that, and letting , we get u(s) E{sh},π[ H1 (cid:88) h=0 αhc(sh, π)] s, H, u(s) (π, s) (s) s. vh(s) = inf πΠ Vh(π, s) Vh(π, s) s, h, π u(s) (π, s) s, π. (30) (31) (32) This implies u(s) (s). We have thus shown that = . Further, if there is another solution satisfying = u, it holds that , Hence, is the minimal solution. B.2. Limit of sequence of MDPs Consider now sequence of Markov Decision Processes (MDPs) Mn = {S, A, P, cn, γn}, where, without loss of generality, we write cn, and Mn, M. Consider now sequence of value functions {V } n=0: Vn(π, s0) = Eπ (cid:34) (cid:88) (cid:35) γtcn(st, at) , t= 18 Almost Surely Safe Alignment of Large Language Models at Inference-Time The limit value functions (with = ) are still denoted as follows: (s) = inf π Vn(π, s). (π, s0) = Eπ (cid:34) (cid:88) (cid:35) γtc(st, at) , t=0 (s) = inf π (π, s)."
        },
        {
            "title": "We also define the sequence of Bellman operators",
            "content": "(cid:26) Tnv(s) = min aA(s) cn(s, a) + γ (cid:90) v(y)P (dy s, a) , (cid:27) (cid:26) v(s) = min aA(s) c(s, a) + γ (cid:90) v(y)P (dy s, a) . (cid:27) In addition to the previous assumptions, we make an additional one, while modifying Assumption B1: B1 For each n, the functions cn(s, a) are bounded, measurable on K, nonnegative, lower semi-continuous; B4 The sequence {cn(s, a)} n=0 is such that cn c. For each n, the optimal cost function (s) is the bounded function in L(S)+ that satisfies the Optimality equation in Theorem 4 : Theorem 5. The sequence is monotone increasing and converges to . = TnV , Proof. We follow similar proof strategy to that of Hernandez-Lerma & Munoz de Ozak (1992, Theorem 5.1) To begin with, note that since cn c, it is clear that function L(S)+ such that u. is an increasing sequence in L(S)+, and therefore, there exists Moreover, from Lemma 3(c), letting , we see that = u, i.e., satisfies the optimality equation. This implies that , since, by Theorem 4, is the minimal solution in L(X)+ to the optimality equation. On the other hand, it is clear that for all n, so that . Thus = , i.e., = . B.3. Latent MDP Analysis Theorem 1. (Optimality in the Latent Space) Given the latent MDP in Definition 4.1 and with A1-A2, we can show: a) (Prop I) For any finite n, the Bellman equation holds, i.e., there exists function ,n(h, o, z) such that: ,n(h, o, z) = min yV (cid:16) Cn task(h, o, z, y) + γ ,n(h, o, z) (cid:17) , such that (h, o, z) P(h, o, z, y) Furthermore, the optimal policy solving Equation 8 has the representation π,n( h, o, z); b) (Prop II) The optimal value functions ,n converge monotonically to ,. c) (Prop II) The optimal policy in the latent space π,n is also optimal in the original token space if used as π,n(ϕ()), minimizing Equation 7, even as . 19 Almost Surely Safe Alignment of Large Language Models at Inference-Time Proof. We begin by comparing our assumptions to that of the assumptions B1-B4, closely aligned to those used in Hernandez-Lerma & Munoz de Ozak (1992); Sootla et al. (2022). To prove a),b) of Theorem 1 we need to verify that the latent MDP satisfying Assumptions A1-A2 also satisfies Assumptions B1, B2-B4. According to Assumption A1, we consider bounded costs Cn task continuous w.r.t. state (h, o, z) for given with discrete and finite action space V, hence Assumptions B1, B3, and B4 are satisfied. Assumptions B2 and A2 are identical. This proves a) and b). For c), note that the state value function () and latent space value function () w.r.t. policy π : that acts on the latent space directly and on the original space as π(ϕ()) : are related as follows: (π(ϕ()), s0, z0) = Est+1,zt+1P(st,zt,yt) ytπ(ϕ(st),zt) = Est+1,zt+1P(st,zt,yt) ytπ(ϕ(st),zt) (cid:104) (cid:88) t=0 (cid:104) (cid:88) t= γt Cn (cid:105) task(st, zt, yt) γt Cn (cid:105) task(ϕ(st), zt, yt) (cid:104) (cid:88) γt Cn task(ϕ(st), zt, yt) (cid:105)(cid:12) (cid:12) (cid:12) γt Cn task(ht, ot, zt, yt) (cid:105)(cid:12) (cid:12) (cid:12)h0,o0=ϕ(s0) = ϕ(st+1),zt+1 P(ϕ(st),zt)(cid:1) ytπ(ϕ(st),ot) = ht+1,ot+1,zt+1 P(ht,ot,zt)(cid:1) ht+1,ot+1=ϕ(st+1) ytπ(ht,ot) (cid:12) = (π, h0, o0, z0) (cid:12) (cid:12)h0,o0=ϕ(s0) t=0 (cid:104) (cid:88) t= Hence, we can show π is optimal for Vn() as follows: Vn(π n, s, z) = Vn(π (cid:12) (cid:12) n, h, o, z) (cid:12)h,o=ϕ(s) (cid:12) Vn(π, h, o, z) (cid:12) (cid:12)h,o=ϕ(s) Vn(π(ϕ()), s, z) Vn(π, s, z) = min π = min π = min π (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) Here, the minimization of π is over set of all policies covered by π(ϕ()) and we show that π for the original space over this set of policies. n(ϕ()) is the optimal policy Theorem 2. (Almost Sure Safety) Consider an augmented MDP with cost function task. Suppose an optimal policy exists π solving Equation 7 (see Theorem 1) with finite cost, then π is an optimal policy for Equation 9, i.e., π is safe with probability approaching one or almost surely. Proof. We first note that if any trajectory with infinite cost has finite probability, the cost would be infinite. Hence, all the trajectories with finite/positive probability have finite costs. This implies, the finite cost attained by π w.r.t. Equation 7 implies the satisfaction of constraints (Equation 9) almost surely (i.e. with probability 1). Combined with the fact that the policy π was obtained by minimizing the exact task cost as in Equation 9, Theorem 2 follows. 20 Almost Surely Safe Alignment of Large Language Models at Inference-Time C. Algorithmic details Algorithm 1 InferenceGuard Input: Initial prompt s0, beam depth d, number of beams, max depth D, top beams, max retry Initialize the beam {s0} for iterations do {1, . . . d}, {1, ..., V}, Fi,j 0 for rounds do Bnew Generate continuations of length from with πpen and Evaluate(Bnew) with Einter, Ecritic or Emix. if i, Ei > 0 or last round then Bnew break end + token requency(Bnew) end Keep top beams in according to their scores end return Best trajectory in according to If the cost functions allow intermediate evaluations, we evaluate beam yt, , yt+d but using our augmented cost function: Einter(yt, , yt+d) = (cid:40) γT ctask(ht+d, ot+d) zt+d > 0 otherwise. When we only have critic, we use: Ecritic(yt, , yt+d) = γT ctask(ht+d) 2 θ (ht+d, ot+d, zt+d) + = and zt+d > 0 + = and zt+d 0 1 θ (ht+d, ot+d, zt+d) > 0.5 otherwise. If we can do both, as zt only decreases overtime, the critic head predicting the safety would only act as an additional filter. We introduce another hyper-parameter η to balance the confidence in the second head of the critic predicting the future cost: Emix(yt, , yt+d) = γT ctask(ht+d, ot+d) γT ctask(ht+d, ot+d) + ηf 2 θ (ht+d, ot+d, zt+d) + = and zt+d > 0 + = and zt+d 0 1 θ (ht+d, ot+d, zt+d) > 0.5 and zt+d > 0 otherwise. D. Experimental Details D.1. Experiment Setup We conducted all experiments on NVIDIA V100 GPUs, utilizing Python 3.11.11 and CUDA 12.4, for model development and training. The experimental setup was based on the PKU-SaferRLHF dataset.Our experiments involved two primary models: the unsafe-aligned reproduced Alpaca-7B model6 (Taori et al., 2023) and the safe-aligned Beaver-v3-7B model (Ji et al., 2024b). We employed the reward model (Khanov et al., 2024)7 and cost model (Ji et al., 2024b)8 during the training stage for critic network, test-time inference and evaluation stages. 6https://huggingface.co/PKU-Alignment/alpaca-7b-reproduced 7https://huggingface.co/argsearch/llama-7b-rm-float32 8https://github.com/PKU-Alignment/safe-rlhf 21 Almost Surely Safe Alignment of Large Language Models at Inference-Time D.2. Baselines and Hyper-parameter Settings The baseline methods we compare against include BoN, Beam Search, RECONTROL (Kong et al., 2024), and ARGS (Khanov et al., 2024). For BoN, we use strategy where the top outputs are sampled, and the best result is selected based on predefined criterion. Similarly, Beam Search explores multiple beams during the search process and selects the best output based on beam-width parameter. In RECONTROL, an MLP network is employed as the value network to intervene in the decision-making process, guiding the generation through reinforcement learning (Kong et al., 2024). ARGS, on the other hand, implements logits-based greedy token-wise search strategy, where tokens are generated sequentially based on the maximum likelihood of the next token (Khanov et al., 2024). Given the limited research on safe alignment with inference-time methods, we adapt these baseline methods to enable safe inference, ensuring fair comparison. To this end, we incorporate Lagrangian multiplier term to control the balance between reward and cost, allowing for safe inference based on their open-source implementations. Notably, BoN and Beam Search utilize form of blocking sampling, while ARGS and RECONTROL employ token sampling methods. In our setup, we modify the inference process of the baseline methods with Lagrangian multiplier by using the following score for token selection: ctask + λCsafety where λ is the Lagrangian multiplier, that controls the influence of the safety cost score Csafety. We unify λ = 5 and also the sampling parameters for baseline methods and InferenceGuard to ensure fair comparison. Each method has its own specific settings, as follows: Method Sample Width (d) Num Samples (N ) Other Parameters ARGS RECONTROL BoN Beam Search InferenceGuard 1 1 32 32 32 20 N/A 100, 200, 500 128, 256 128, 512 N/A steps = 30, step size = 0.5 N/A D=128, K=N/4 M=2, D=128, K=N/4 Table 1. Hyperparameters for Baselines and InferenceGuard. These hyperparameter settings ensure fair and controlled comparison of the baseline methods and the proposed InferenceGuard model. We compare the performance of baseline methods, with reward model, cost model and lagriangian multiper setting, under these hyperparameter settings, and summarised the result in Table 2. When sampling from the base model, we used temperature of 1.0 without top nor top p. D.3. Critic Network and Training Process This section outlines the critic network architecture used for InferenceGuard. The critic network is designed to estimate the cost of partial responses and guide optimization during inference. We assume that trajectories terminate at maximum time , and the critic aims to predict the sign of the safety compliance metric zT , and the discounted cumulative task cost γT ctask. Critic Network Architecture The critic network takes two types of input: the hidden states (ht) and the key-value pairs (ot), representing contextual and state information, respectively. These are passed through series of layers to estimate the required outputs. The network utilizes downscaling and attention layers to reduce the dimensionality of the input data, ensuring efficient processing of large-scale representations. In terms of model size, the total parameter count of the critic network is approximately 0.7 billion parameters, providing balance between model capacity and computational efficiency. Training Process The critic network is trained using combination of optimization techniques aimed at predicting the safety compliance and task cost accurately. The network is optimized with the hyperparameters in Table 3. During training, the network is fed batches of hidden states and key-value pairs, and the weights are updated to minimize the loss between predicted and true values. The critic networks ability to predict both the safety compliance and task cost ensures it can guide the optimization process at inference time, while adhering to safety constraints. 22 Almost Surely Safe Alignment of Large Language Models at Inference-Time Table 2. Performance Comparison on Dataset PKU-SafeRLHF Method Average Reward Average Cost Safety Rate Inference Time (s) Alpaca-7B Beaver-7B-v3 Base RECONTROL RECONTROL + Lagrangian multiplier Best-of-N + Lagrangian multiplier = 100, λ = 5 Best-of-N + Lagrangian multiplier = 200, λ = 5 Best-of-N + Lagrangian multiplier = 500, λ = 5 Best-of-N + Lagrangian multiplier = 500, λ = 10 Best-of-N + Augmented safety = 200 Best-of-N + Augmented safety = 500 Beam search + Lagrangian multiplier = 128, λ = 5 Beam search + Lagrangian multiplier = 256, λ = 5 Beam search + Augmented safety = 128 Beam search + Augmented safety = 256 ARGS ω = 2.5 ARGS + Lagrangian multiplier ω = 2.5 ARGS + Cost Model ω = 2.5 InferenceGuard (N=128) InferenceGuard with Critic (N=128) Base RECONTROL RECONTROL + Lagrangian multiplier Best-of-N + Lagrangian multiplier = 100 Best-of-N + Lagrangian multiplier = 200 Best-of-N + Augmented safety = 100 Best-of-N + Augmented safety = 200 Beam search + Lagrangian multiplier = 64, λ = 5 Beam search + Lagrangian multiplier = 128, λ = 5 Beam search + Augmented safety = 64 Beam search + Augmented safety = 128 ARGS ω = 2.5 ARGS ω = 2.5 + Lagrangian multiplier ARGS ω = 2.5 + Cost Model InferenceGuard (N=128) InferenceGuard with Critic (N=128) 6.15 ( 1.51) 6.2 ( 1.56) 6.19 ( 1.50) 5.35 ( 1.62) 5.25 ( 1.64) 6.04 ( 1.85) 5.51 ( 1.66) 7.51 ( 1.89) 7.78 ( 2.09) 6.58 ( 1.95) 6.69 ( 2.08) 8.29 ( 2.02) 8.69 ( 2.15) 6.74 ( 1.70) 3.21 ( 1.59) 0.19 ( 1.65) 7.08 ( 2.49) 6.81 ( 2.7) 5.83 ( 1.62) 5.9 ( 1.56) 5.91 ( 1.50) 6.52 ( 1.88) 6.61 ( 1.89) 8.55 ( 1.58) 9.01 ( 1.63) 8.33 ( 1.79) 8.63 ( 1.80) 9.84 ( 1.4) 10.31 ( 1.37) 6.72 ( 1.83) 2.26 ( 1.56) 0.01 ( 1.37) 10.26 ( 1.42) 10.27 ( 1.50) 1.33 1.33 1.33 -0.46 -0.72 -1.27 -1.44 0.67 0.42 -1.02 -1.28 0.64 0.55 1.47 -0.85 -2.21 -0.63 -1.01 -2.89 -2.90 -2.91 -3.63 -3.62 -2.96 -2.98 -4.09 -4.21 -2.93 -2.94 -2.59 -1.64 -3.27 -2.96 -2.94 29.47% 29.5% 29.7% 48.22% 54.2% 52.17% 54.01% 60.07% 65.74 % 50.19% 52.43% 58.89% 61.79 % 28.19% 75.8% 81.6% 88.14% 91.04% 75.89% 75.9% 75.9% 85.7% 85.8% 97.23% 97.76% 87.08% 87.35 % 95.38 % 97.36% 78.5% 81% 98.4% 99.7% 100% 1.1 1.7 2 29 58 145 145 58 145 32 60 39 82 82 111 78 65 1.2 2 2.6 40 58 40 58 36 64 22 39 94 127 90 39 39 Hyperparameter Value Hidden Dimension Learning Rate Number of Epochs Discount Factor (γ) Batch Size Safety Budget 4096 1 105 50 0.999 8 10 Table 3. Hyperparameters for Critic Network Training. The model uses penalty term to enforce the safety budget constraint. This penalty discourages the network from violating the safety threshold, steering for safer responses during intervention. D.4. Qualitative Comparisons We present several examples from the PKU-SafeRLHF test dataset to demonstrate how InferenceGuard steers the base model outputs towards safer responses. These examples highlight differences in response safety between InferenceGuard and baseline methods, including BoN, Beam Search, RECONTROL, and ARGS, evaluated on the Alpaca and Beaverv3 models, as shown in Figure 4 and 5. In each case, InferenceGuard successfully adheres to safety constraints while maintaining task performance. 23 Almost Surely Safe Alignment of Large Language Models at Inference-Time (a) Alpaca Example (b) Alpaca Example 2 Figure 4. Generated response by different methods on the Alpaca-7B 24 Almost Surely Safe Alignment of Large Language Models at Inference-Time (a) Beaver Example 1 (b) Beaver Example Figure 5. Generated response by different methods on the Beaver-v3-7B"
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab",
        "Imperial College London",
        "University College London"
    ]
}