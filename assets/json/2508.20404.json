{
    "paper_title": "AWorld: Orchestrating the Training Recipe for Agentic AI",
    "authors": [
        "Chengyue Yu",
        "Siyuan Lu",
        "Chenyi Zhuang",
        "Dong Wang",
        "Qintong Wu",
        "Zongyue Li",
        "Runsheng Gan",
        "Chunfeng Wang",
        "Siqi Hou",
        "Gaochi Huang",
        "Wenlong Yan",
        "Lifeng Hong",
        "Aohui Xue",
        "Yanfeng Wang",
        "Jinjie Gu",
        "David Tsai",
        "Tao Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The learning from practice paradigm is crucial for developing capable Agentic AI systems, yet it is severely hampered by inefficient experience generation, a bottleneck especially pronounced in complex benchmarks like GAIA. To address this, we introduce AWorld, an open-source system engineered for large-scale agent-environment interaction. By distributing tasks across a cluster, AWorld accelerates experience collection by 14.6x compared to standard single-node, sequential execution. This critical speedup makes extensive reinforcement learning practical and scalable. Leveraging this capability, we trained a Qwen3-32B-based agent that significantly outperforms its base model, increasing its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most challenging levels, our agent achieves a score of 16.33%, surpassing the performance of leading proprietary models. Our open-source system and resulting agent provide a practical blueprint for a complete agentic AI training pipeline, from efficient interaction to demonstrable model improvement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 0 4 0 2 . 8 0 5 2 : r AWorld: Orchestrating the Training Recipe for Agentic AI Chengyue Yu1,, Siyuan Lu1,2,3,, Chenyi Zhuang1,, Dong Wang1, Qintong Wu1, Zongyue Li1, Runsheng Gan1, Chunfeng Wang1, Siqi Hou1, Gaochi Huang1, Wenlong Yan1, Lifeng Hong1, Aohui Xue1, Yanfeng Wang1, Jinjie Gu1, David Tsai1, Tao Lin3,1 1AWorld Team, Inclusion AI 2Shanghai Innovation Institution 3Westlake University https://github.com/inclusionAI/AWorld/tree/main/train"
        },
        {
            "title": "Abstract",
            "content": "The learning from practice paradigm is crucial for developing capable Agentic AI systems, yet it is severely hampered by inefficient experience generation, bottleneck especially pronounced in complex benchmarks like GAIA. To address this, we introduce AWORLD, an open-source system engineered for large-scale agent-environment interaction. By distributing tasks across cluster, AWORLD accelerates experience collection by 14.6x compared to standard single-node, sequential execution. This critical speedup makes extensive reinforcement learning practical and scalable. Leveraging this capability, we trained Qwen332B-based agent that significantly outperforms its base model, increasing its overall GAIA accuracy from 21.59% to 32.23%. On the benchmarks most challenging levels, our agent achieves score of 16.33%, surpassing the performance of leading proprietary models. Our open-source system and resulting agent provide practical blueprint for complete agentic AI training pipeline, from efficient interaction to demonstrable model improvement. Figure 1: AWORLD: High Efficiency Enables High Performance on GAIA. (Left) By using the AWORLD framework to fine-tune and conduct reinforcement learning on the Qwen3-32B base model, our resulting agent (Qwen3-32B-AWORLD) demonstrates substantial performance gain. It achieves pass@1 score that is highly competitive with frontier proprietary models like GPT-4o. (Right) This effective training is made practical by AWORLDs core design. Its distributed architecture accelerates the critical experience generation (rollout) phase by factor of 14.6x, overcoming the primary bottleneck faced by standard single-node, sequential processes. Equal contributions. Work was done during Siyuans internship in Ant Group. Corresponding Authors. 1 AWorld: Orchestrating the Training Recipe for Agentic AI Figure 2: The architecture of AWORLD, framework for Agentic AI designed around the learning from practice paradigm. The framework operates in two main flows: Forward Pass (top; ①-③), where agents are constructed and interact with complex environments to generate task-solving trajectories; and Backward Pass (bottom; ④-⑥), where these trajectories are used as experience to train agents and optimize the entire system via reinforcement learning. This closed-loop design enables agents to continuously improve their performance on real-world problems."
        },
        {
            "title": "Introduction",
            "content": "Since the release of ChatGPT (Achiam et al., 2023) by OpenAI in 2022, Large Language Models (LLMs) (Anthropic, 2025; Touvron et al., 2023; Google et al., 2023) have demonstrated remarkable capabilities across diverse domains, achieving expert human-level performance in many domains like mathematics (The Google DeepMind Team, 2024), demonstrating that AI has reached intellectual capabilities that rival human expertise in numerous domains. However, despite the impressive capabilities of individual LLMs, current AI systems still struggle to solve real-world, complex tasks effectively. For instance, on the challenging GAIA benchmark (Mialon et al., 2023), even closed-source models like GPT-4 (Achiam et al., 2023) achieve only 3.99% accuracy, highlighting the significant gap between LLM capabilities and agent performance in complex, multi-step reasoning scenarios. The future direction of AI development lies in addressing real-world problems that require complex multiturn interactions with environments through long trajectories called Agentic AI (Wooldridge & Jennings, 1995; Sapkota et al., 2025). This paradigm emphasizes learning from practice, where agents must continuously engage with external environments to solve complex, multi-step reasoning tasks. As highlighted by recent research (Yao, 2025; Silver & Sutton, 2025), the training recipe for agentic AI consists of three fundamental components: (1) Algorithm - the learning mechanisms that enable agents to adapt and improve from environmental interactions; (2) Environment - the complex, interactive settings that provide rich feedback and diverse challenges for agent learning; and (3) Priors - the foundational capabilities of current large models in reasoning, mathematics, vision, and other domains that serve as the starting point for agent specialization. However, realizing the learning from practice paradigm faces critical challenges tied directly to these core components. For the Algorithm component, the high cost of curating realistic and complex tasks often leads to data scarcityfor instance, the entire GAIA (Mialon et al., 2023) validation set contains only 165 questions. This limited availability of high-quality data underscores the continuous need for more sample-efficient learning methods, as current techniques often require vast amounts of experience to achieve proficiency. Simultaneously, for the Environment component, while valuable interactive environments for tasks like browser navigation (Zhou et al., 2024), computer control (Xie et al., 2024), and web shopping (Yao et al., 2022) have emerged, they remain scarce and often present significant challenges in deployment and scalability. Even with advanced algorithms and rich environments, the sheer volume of interaction required for an agent to gather meaningful experience becomes daunting logistical hurdle. These challenges converge on central bottleneck: the inefficiency of the agent-environment interaction loop, which is the primary obstacle to 2 AWorld: Orchestrating the Training Recipe for Agentic AI building more capable agents. Facing these multifaceted challenges, the community urgently needs comprehensive framework that integrates all components required for learning from practice, focusing on end-to-end optimization of the training recipe for agentic AI. To address this need, we propose AWORLD, framework that provides comprehensive support across three critical dimensions: 1. Prior Model Selection: AWORLD provides unified interface for model integration, supporting flexible configuration of target models across different training frameworks (detailed in Section 2.1). 2. Runtime Construction: Beyond high-concurrency support, AWORLD encapsulates communication protocols between models and tools, as well as inter-agent communication protocols (Section 2.2), while implementing robust state management capabilities to handle complex tasks with extended contexts (Section 2.3). 3. RL Algorithm Design: Although AWORLD is not itself training framework, it seamlessly integrates with reinforcement learning frameworks including OpenRLHF (Hu et al., 2024), VeRL (Sheng et al., 2025), AReaL (Fu et al., 2025), and SWIFT (Zhao et al., 2025), thereby unifying model training clusters with environment inference systems. Details of this integration are discussed in Section 2.4. Collectively, these foundational features position AWORLD as an ideal platform for implementing End-to-End learningfrom-practice pipelines, as demonstrated in Figure 2. Overall, in this paper, we have made the following contributions: We design and implement AWORLD, modular and scalable open-source framework that serves as the core infrastructure for the entire learning from practice lifecycle of AI agents. AWORLD provides unified solutions for agent construction, communication, distributed execution, and training orchestration to tackle complex, long-horizon tasks. We conduct systematic analysis on the GAIA benchmark to empirically demonstrate that agent performance is critically bottlenecked by the efficiency of experience generation (rollouts). We then show that AWORLDs distributed architecture resolves this bottleneck, achieving significant speedup in data collection compared to standard sequential approach, making large-scale agent training feasible. By leveraging the AWORLD framework, we successfully train an open-source agent based on Qwen332B. This agent not only significantly surpasses its base model but also achieves highly competitive performance on the challenging GAIA benchmark, even outperforming strong proprietary models like Claude-3.7-Sonnet on the highest difficulty levels."
        },
        {
            "title": "2 AWORLD Framework",
            "content": "In this section, we present the AWORLD framework, which serves as general-purpose infrastructure for building, deploying, and training intelligent agents in complex environments. We introduce the four key components of AWORLD: Agent Construction: We begin at the foundational level by simplifying the instantiation of an individual agent, defining its core logic, toolset, and planning capabilities. Communication Protocols: With single agent defined, we then establish unified message-passing architecture, enabling it to interact reliably with itself, other agents, various tools and environments. Runtime State Management: To scale these interactions for complex tasks, the next logical layer provides robust distributed execution, managing the state of numerous concurrent agents across cluster. Training Orchestration: Finally, to complete the learning from practice loop, the framework channels the vast experiential data generated by the runtime into external training modules, allowing the agents core policy to be continuously improved. Collectively, these components provide the foundational infrastructure for the entire learning from practice lifecycle. They empower agents to autonomously interact with complex environments at scale, thereby enabling the efficient synthesis of high-quality experiential data needed for continuous improvement. 2.1 Agent Construction As illustrated in Figure 3, the agent architecture consists of several key components that enable flexible and effective task handling. Upon receiving user query, the agent first gathers relevant contextual information, including available tools (such as MCP tools), instructions, memory, and other environmental context. This information is synthesized to generate prompt. The agent autonomously selects the most appropriate tool from diverse pool, which may include built-in functions, externally registered tools via MCP, or even 3 AWorld: Orchestrating the Training Recipe for Agentic AI Figure 3: An illustration of the runtime in AWORLD, showing the message workflow when an agent receives query from user. other agents acting as tools. Subsequently, the backend LLM determines the next action to be taken. The selected action is then executed within an isolated environment that can interact with sandbox, thereby ensuring secure and reproducible execution. The results from tool execution are processed by the agent and, if necessary, communicated to other agents or system components through unified message-passing mechanism. This event-driven communication enables agents to notify, coordinate, and delegate tasks dynamically, supporting both single-agent and multi-agent workflows. The agent runtime further supports extensibility through custom message handlers, allowing agents to respond to external events and collaborate on complex operations. Our framework offers comprehensive support for agent construction, enabling users to flexibly assemble agents for diverse scenarios. Specifically, we provide: Prompt Assembly: Users can define system prompts to guide agent behavior and tailor responses to specific application scenarios. Custom Toolsets: The framework allows users to specify the set of environments and tools accessible to each agent, including browser-based interfaces, terminal emulators, and agent-as-tool functionalities. Agent Topology Configuration: For multi-agent systems, both automated and user-defined custom topologies or workflows are supported, enabling dynamic team formation and tailored collaboration strategies. 2.2 Communication Protocols Drawing inspiration from the design of Google (2025b), AWORLD adopts similar architecture. As illustrated in Figure 3, AWORLD utilizes the Message object as the core abstraction to unify three primary communication channels: (1) user-to-agent communication; (2) intra-agent communication between models and tools (e.g., Anthropics MCP (Anthropic, 2024)); and (3) inter-agent communication (e.g., Googles A2A protocol (Google, 2025a)). The structure of the Message object is defined as follows: Message API Specification Message attributes id (str): Unique message identifier (UUID). session id (str): Identifier for the task/session context. sender (str): The current sender agent/tool. receiver (Optional[str]): The designated recipient. caller (Optional[str]): The senders parent caller in call chain. payload (Any): Main content (e.g., ActionModel, Observation, TaskItem). category (str): Event type or message category. 4 AWorld: Orchestrating the Training Recipe for Agentic AI Figure 4: Orchestrating Massively Parallel Rollouts in AWorld. The systems distributed architecture, managed by Kubernetes, is engineered to generate vast amounts of training data by concurrently executing agent tasks across numerous, sandboxed environments. topic (Optional[str]): Topic-based routing channel (for pub-sub patterns). priority (int): Execution priority assigned by sender. headers (Dict[str, Any]): Additional metadata (e.g., task id, trace info). timestamp (float): Epoch timestamp for message creation. The payload field, as the core information carrier, contains standard objects such as ActionModel, Observation, or TaskItem, which are sent by Agents, Tools, and Tasks, respectively. Our communication protocol ensures robust execution through built-in mechanisms for parameter validation, error handling, and result interpretation. For example, if communication with an unavailable or failed agent is attempted, our framework automatically generates an error notification for the sender. This enables systematic exception handling and enhances the overall robustness of distributed task execution. 2.3 Runtime State Management To effectively address complex real-world tasks, AWORLD adopts distributed architecture that prioritizes robustness and scalability. As illustrated in Figure 4, this architecture is essential for supporting long-horizon agent interactions, with key features including: High Concurrency. To enable agents to accumulate sufficient interactive experience for self-learning, the framework supports high-concurrency execution. This is achieved through dynamic task management module powered by Kubernetes, which orchestrates the scheduling, distribution, and prioritization of large number of concurrent tasks across distributed cluster of worker nodes. By efficiently managing heterogeneous workloads and maximizing resource utilization, the system accelerates the generation of rollout samples necessary for both evaluation and continuous agent improvement. State Consistency. State consistency across distributed nodes is maintained via synchronized remote data storage and centralized trace server, ensuring coherent task execution and rapid recovery from potential disruptions. The architecture also systematically collects and stores agent trajectories and metrics, further supporting large-scale evaluation and self-improving learning processes. 2.4 Training Orchestration There are two commonly used methods to improve the capabilities of the underlying LLM in post-training phase, namely SFT and RL. SFT typically requires high-quality human-labeled data or complex data synthesis pipeline, which is costly and generally not scalable. In contrast, through RL, agents could learn 5 AWorld: Orchestrating the Training Recipe for Agentic AI Figure 5: An action-state rollout demonstration utilizing AWORLDs distributed environments. AWORLD leverages Kubernetes to manage parallel environments, where each environment is encapsulated within fundamental execution unit known as pod. In our setup, multiple pods run concurrently across the cluster to enable massive-scale experience generation. from environment feedback, which is more accessible and scalable. standard RL algorithm, such as GRPO, generally involves three key stages: exploration (rollout), feedback (reward), and learning (gradient update). Among these, exploration becomes the primary bottleneck when dealing with complex, realistic tasks. In the case of GAIA tasks, single rollout can take up to 20 minutes to complete, significantly slowing down training. The high-concurrency execution capabilities of AWORLD can substantially improve exploration efficiency in such scenarios. As shown in Figure 5, AWORLD provides decoupled, high-concurrency execution engine designed to integrate seamlessly with external RL training frameworks such as SWIFT (Zhao et al., 2025). Specifically, the rollout module in conventional frameworks is replaced with the AWORLD Executor. During the rollout phase, tasks are dispatched to the AWORLD Executor, which interacts with the inference engine of the RL framework to query actions at each step. The selected action is executed in the environment, and the corresponding feedback is collected. This interaction continues iteratively, forming complete trajectory of experience. Once the trajectory is collected, the training framework assumes control of the learning stage: it performs gradient updates and synchronizes the updated model parameters with the inference engine. Summary. AWORLD offers modular, high-performance framework for building and training intelligent agents. Its general-purpose design accommodates both single-agent and multi-agent settings, enables scalable interaction with realistic environments, and supports seamless integration with external LLMs and RL frameworks."
        },
        {
            "title": "3 Experiment",
            "content": "To validate the necessity and effectiveness of the AWORLD framework, we conduct series of experiments. Our evaluation is designed to first establish the relationship between the volume of rollouts and agent performance on complex tasks. Subsequently, we quantify the efficiency gains AWORLD provides in generating this experience, demonstrating its critical role in making the learning from practice paradigm computationally feasible. 3.1 Experimental Settings This section outlines the experimental setup, including the benchmark, models, and infrastructure used to validate our claims. Benchmark. All experiments are performed on the GAIA benchmark (Mialon et al., 2023), challenging testbed for agentic AI that mirrors the complexity of real-world problems. GAIAs difficulty stems from two primary factors. First, it presents Large Search Space, characterized by combinatorial action space of diverse tools and their parameters, vast observation space filled with variable and often noisy tool outputs, and the necessity for long-horizon reasoning across extended trajectories. Second, this complexity leads to 6 AWorld: Orchestrating the Training Recipe for Agentic AI Low Search Efficacy in current agents, which commonly exhibit suboptimal behaviors such as insufficient planning, performing redundant actions, and demonstrating path dependence without reflecting on past failures. Collectively, these challenges establish GAIA as an ideal benchmark for rigorously evaluating Agentic AI systems, while also highlighting the critical need for scalable experience generation to overcome its inherent complexity. Foundation Models. Our experiments are centered on training Qwen3-32B (Yang et al., 2025), powerful open-source foundation model. To benchmark its performance, we compare our results against several state-of-the-art models. This includes leading closed-source models like GPT-4o (Hurst et al., 2024) and Claude-3.7-Sonnet (Anthropic, 2025), as well as another powerful open-source competitor DeepSeek-V3 (Liu et al., 2024). Hardware Infrastructure. Our setup employs train-inference decoupled architecture to optimize resource utilization for agent training. The training process runs on dedicated node featuring 8 NVIDIA A100 (80GB) GPUs and 96-core CPU. This node is allocated 1200GB of system memory to support memory-intensive optimization strategies such as DeepSpeed ZeRO3 (Rajbhandari et al., 2020). separate, parallel node is dedicated to environment interaction and rollout generation. This inference node is equipped with an identical set of 8 NVIDIA A100 GPUs and 96-core CPU, but is configured with 800GB of system memory, which is ample for high-throughput agent inference. Agent Development Framework. Our agent development is powered by AWORLD, which integrates specialized frameworks to create seamless pipeline for experience generation and model training. For the rollout phase, AWORLD leverages the vLLM (Kwon et al., 2023) to manage high-throughput agent inference and interaction with the environment. Subsequently, the collected trajectories are processed by the SWIFT framework (Zhao et al., 2025), which orchestrates the models fine-tuning and reinforcement learning updates. This integrated approach allows us to efficiently manage the entire practice-then-learn cycle. We will detail the training process in Section 3.4. Tool Integrations. To equip the agent with versatile set of capabilities for tackling complex tasks, AWORLD integrates suite of powerful tools, summarized in Table 1. These tools provide sandboxed execution environments, web automation, and specialized data processing services. Table 1: Overview of the integrated tools within the AWORLD framework. Tool Functionality sandboxed code instance that compiles and executes arbitrary code snippets. e2b-code-server1 terminal-controller Enables terminal command execution, directory navigation, and file system operations. excel calculator ms-playwright2 audio server image server google-search Lightweight headless Excel engine that reads and writes on .xlsx sheets. Basic arithmetic and symbolic expression evaluator. Automates browser tasks such as page interaction, web scraping and screenshots. On-the-fly audio processing via FFmpeg and Whisper pipelines3. VLM-based image Question & Answering service powered by models. Google Search interface that returns ranked web URLs and summary snippets. 3.2 The Impact of Rollout Scale on Performance In reinforcement learning, agent improvement hinges on learning from successful examples. For complex, multi-step tasks like those in GAIA, single attempt has low probability of success, making the discovery of these positive reward signals significant challenge. To investigate the relationship between interaction budget and problem-solving success, we conducted comprehensive experiment to quantify the pass rate as function of the number of rollouts per task. We evaluated three state-of-the-art modelsClaude-3.7Sonnet, Gemini 2.5 Pro (Comanici et al., 2025), and GPT-4oon the entire 165-question GAIA validation set, allowing up to 32 rollouts per question. As illustrated in Figure 6, the results reveal consistent and crucial trend: increasing the number of rollouts directly and substantially improves the pass rate for all models. For instance, Claude-3.7-Sonnets performance climbs from pass@1 of 47.9% to peak of 76.4%a gain of nearly 30 percentage points. Similarly, 1https://e2b.dev/ 2https://github.com/microsoft/playwright 3https://github.com/openai/whisper 7 AWorld: Orchestrating the Training Recipe for Agentic AI Figure 6: Pass Rate as Function of Rollout Scale on the GAIA Validation Set. We plot the pass@k success rate for three leading models on the full 165-question GAIA validation set, varying the number of rollouts (k) from 1 to 32. clear and universal trend emerges: all models, regardless of their initial capability, achieve substantial performance gains with more interaction attempts. GPT-4os success rate more than doubles, rising from 27.3% to 65.5%. For most models, the sharpest gains occur within the first 10-15 rollouts, after which performance begins to plateau as they approach their peak problem-solving capacity. This finding empirically confirms that sufficient rollout count is essential not merely for data volume, but for ensuring the agent has successful examples to learn from. Consequently, the efficiency of this rollout process becomes the critical bottleneck for the entire learning from practice loop. 3.3 Efficiency of Distributed Rollouts with AWORLD Table 2: Time comparison for one cycle of rollout and training. AWORLDs distributed executor reduces rollout time by factor of 14.6 compared to local sequential setup, demonstrating its necessity for scalable agent training."
        },
        {
            "title": "Rollout Method",
            "content": "Rollout Time (s) Training Time (s) Total Time (s) AWORLD Executor (Distributed) Sequential Executor (Single-Node) 525 7695 144 144 669 7839 Given the established need for large-scale data generation, this section evaluates the core efficiency of AWORLD. We measure the wall-clock time for full cycle of experience generation and model training, comparing our distributed approach against standard single-node setup. One might ask why the single-node baseline is sequential rather than parallel. naive parallel implementation on single node is not viable baseline for complex environments like GAIA. The combination of resource-intensive tools (e.g., full browser engine) and long-horizon tasks creates significant CPU and memory demands. Attempting to run multiple such rollouts concurrently on one machine leads to severe resource contention and process instability, making sequential execution the only stable and practical configuration for single-node setup and fair point of comparison. As shown in Table 2, the efficiency gains are substantial. The AWORLD Executor completes the rollout phase in just 525 seconds, while the Sequential Executor requires 7695 seconds. This translates to 14.6-fold speedup in experience generation. Crucially, as the training time (144s) is constant, the total cycle time 8 AWorld: Orchestrating the Training Recipe for Agentic AI Table 3: Performance comparison on the GAIA test set (pass@1) and xbench-DeepSearch(pass@1). Our RL-enhanced model, Qwen3-32B-AWORLD, demonstrates significant improvements over the base model. Scores are reported in percentage (%). Best results are in bold, second-best are underlined. Model GAIA Avg. (%) Level 1 (%) Level 2 (%) Level 3 (%) GPT-4o Claude 3.7 Sonnet DeepSeek-V3 Qwen3-32B (Base) Qwen3-32B-AWORLD 32.23 (+10.6%) 27.91 43.85 31.89 21.59 40.86 64.52 52.69 30.11 47.31 (+17.2%) 24.53 40.88 25.16 22.01 28.30 (+6.3%) 14.29 14.29 14.29 4.08 16.33 (+12.3%) xbenchDeepSearch 30 45 35 12 32 (+20.0%) is reduced from 7839 seconds to mere 669 seconds. This result provides clear quantitative evidence for central thesis of our work: for complex agentic tasks, the primary bottleneck has shifted from training computation to environmental interaction. AWORLD is purpose-built to dismantle this bottleneck, making it an indispensable tool for the scalable learning from practice paradigm. 3.4 Training an Agent with AWORLD: Performance on the GAIA Benchmark To validate the frameworks effectiveness for both evaluation and training, this section presents our training experiments and performance on benchmarks. Implementations. To provide the agent with strong initial policy and mitigate the cold-start problem, we first perform SFT. For this initial training phase, we curate dataset of 886 successful trajectories, sampled using the Claude 3.7 Sonnet model. This SFT-trained model then serves as the starting point for our reinforcement learning process. The subsequent reinforcement learning loop is composed of three key steps, as shown in Figure 5: 1. Rollout. Tasks are submitted to the AWORLD Executor. During each action step, the AWORLD Executor queries the vLLM module of SWIFT to generate an action. The action is then executed in the environment, and the corresponding feedback is collected. This process iterates over multiple steps to produce complete trajectory. In particular, the model weights underlying the vLLM module are updated at each training step. For each task, we set the number of rollouts to 32. 2. Reward Calculator. We employ rule-based reward mechanism, where the agent receives reward of 1 if its generated answer exactly matches the ground truth, and 0 otherwise. 3. Gradient Update. Following the approach in Shao et al. (2024), we employ the GRPO algorithm for advantage estimation and gradient update computation. These updates are executed within the SWIFT framework, after which the updated model is synchronized with the vLLM server. Result Analysis. The main performance results of our trained agent on the GAIA test set, supplemented by the xbench-DeepSearch benchmark (Chen et al., 2025), are presented in Table 3. The analysis reveals two key findings. First, the reinforcement learning process, enabled by the AWORLD framework, yields substantial performance gains over the base Qwen3-32B model. The overall pass@1 accuracy on GAIA improves by 10.6 absolute percentage points (from 21.59% to 32.23%), with significant improvements observed across all difficulty levels. Furthermore, the strong performance on xbench-DeepSearchimproving from 12% to 32% without any direct training on its samplesindicates that the agent has acquired robust, generalizable problem-solving skills rather than overfitting to the GAIA environment. Second, when compared with state-of-the-art models, our Qwen3-32B-AWORLD agent demonstrates highly competitive performance. Its overall GAIA score is comparable to that of DeepSeek-V3 and surpasses GPT-4o. Notably, the most significant achievement is observed on the benchmarks most challenging Level 3 questions. Here, our agent achieves pass@1 score of 16.33%, outperforming all other listed models, including leading proprietary systems. This specific result highlights the effectiveness of our training methodology in enhancing complex, multi-step reasoning capabilities."
        },
        {
            "title": "4 Future work",
            "content": "Building on the foundation of AWORLD, our future work will focus on advancing towards collective and self-improving intelligence. Our roadmap consists of three main stages. First, we will extend the framework to support the deployment of multi-agent systems in diverse and heterogeneous environments, enabling 9 AWorld: Orchestrating the Training Recipe for Agentic AI collaborative problem-solving. Second, we aim to cultivate specialized, capability-centric agents that achieve expert-level performance in distinct domains such as complex reasoning or web navigation, forming society of experts. Ultimately, our goal is to enable these systems to achieve degree of autonomous self-improvement, where agents learn continuously from their collective practice to refine not only their skills but also their collaboration strategies, creating truly self-sustaining learning loop."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced AWORLD, an open-source framework designed to realize the learning from practice paradigm for Agentic AI. We established that for complex tasks, exemplified by the challenging GAIA benchmark, the primary obstacle to this paradigm is the inefficiency of experience generation. AWORLD tackles this bottleneck directly, leveraging distributed architecture to achieve 14.6-fold speedup in data collection. This efficiency enabled us to train Qwen3-32B-based agent that not only significantly surpasses its base model but also delivers highly competitive performance against leading proprietary models. Our work provides both practical infrastructure and empirical validation for the learning from practice paradigm, paving the way for the development of more capable and self-improving agents. 10 AWorld: Orchestrating the Training Recipe for Agentic AI"
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anthropic. Model context protocol, 2024. URL https://www.anthropic.com/news/model-context-protocol. Anthropic. Claude 3.7 sonnet system card. Technical report, Anthropic, 2025. URL https://assets. anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf. System Card. Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, et al. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. arXiv preprint arXiv:2506.13651, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, et al. Areal: large-scale asynchronous reinforcement learning system for language reasoning. arXiv preprint arXiv:2505.24298, 2025. Google. Agent2agent protocol, 2025a. URL https://github.com/google-a2a/A2A. Google. Events - adk documentation, 2025b. URL https://google.github.io/adk-docs/events/. Gemini Team Google, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Jian Hu, Xibin Wu, Zilin Zhu, Weixun Wang, Dehao Zhang, Yu Cao, et al. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pp. 611626, 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Gregoire Mialon, Clementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward In SC20: International Conference for High Performance Computing, training trillion parameter models. Networking, Storage and Analysis, pp. 116. IEEE, 2020. Ranjan Sapkota, Konstantinos Roumeliotis, and Manoj Karkee. Ai agents vs. agentic ai: conceptual taxonomy, applications and challenges. arXiv preprint arXiv:2505.10468, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 1, 2025. The Google DeepMind Team. Ai solves imo problems at silver-medal level, jul 2024. URL https://deepmind. google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/. Blog post. 11 AWorld: Orchestrating the Training Recipe for Agentic AI Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Michael Wooldridge and Nicholas Jennings. Intelligent agents: Theory and practice. The knowledge engineering review, 10(2):115152, 1995. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shunyu Yao. The second half, 2025. URL https://ysymyth.github.io/The-Second-Half/. Blog post. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35: 2074420757, 2022. Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, et al. Swift: scalable lightweight infrastructure for fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2973329735, 2025. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, 2024."
        }
    ],
    "affiliations": [
        "AWorld Team, Inclusion AI",
        "Shanghai Innovation Institution",
        "Westlake University"
    ]
}