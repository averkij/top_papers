{
    "paper_title": "CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation",
    "authors": [
        "Faria Huq",
        "Zora Zhiruo Wang",
        "Frank F. Xu",
        "Tianyue Ou",
        "Shuyan Zhou",
        "Jeffrey P. Bigham",
        "Graham Neubig"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent's capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 9 0 6 6 1 . 1 0 5 2 : r COWPILOT: Framework for Autonomous and Human-Agent"
        },
        {
            "title": "Faria Huq Zora Zhiruo Wang",
            "content": "Frank F. Xu Tianyue Ou"
        },
        {
            "title": "Shuyan Zhou",
            "content": "Jeffrey P. Bigham Graham Neubig School of Computer Science, Carnegie Mellon University {fhuq, jbigham, gneubig}@cs.cmu.edu Equal Supervision"
        },
        {
            "title": "Abstract",
            "content": "While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agents capabilities effectively. We propose COWPILOT, framework supporting autonomous as well as humanagent collaborative web navigation, and evaluation across task success and task efficiency. COWPILOT reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agents by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. COWPILOT can serve as useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https: //oaishi.github.io/cowpilot.html"
        },
        {
            "title": "Introduction",
            "content": "Agents supported by large language models (LLMs) have become increasingly capable of automating digital tasks such as web navigation (Zhou et al., 2023; Deng et al., 2024). While existing frameworks for web agents mostly focus on solo, autonomous agents (Zheng et al., 2024b; Iong et al., 2024; Drouin et al., 2024), we argue that, for many practical tasks, users interact with the LLM agent for varied purposes such as supervision and collaboration, i.e., the copilot mode. While 1 existing frameworks (Lù et al., 2024; Drouin et al., 2024; Wang et al., 2024a; Zheng et al., 2024b) mainly support users communicating with agents via natural language (NL) feedback, or recording actions of human users alone (Pan et al., 2024b), they do not support dynamic human-agent collaboration within task session, where humans and LLM agents take actions alternately to solve tasks. We ask: How can we enable human-agent collaborative task-solving? and further, how do agents perform under such collaborative settings? To facilitate studying these questions, we introduce COWPILOT (2), lightweight framework that can be seamlessly integrated into user web activities as Chrome extension. COWPILOT starts with the LLM agent proposing actions for the next step, meanwhile allowing human to pause or reject the agent-suggested action and take alternative ones to drive the process; human can also choose to resume the agent-driven process at any time to ease the effort (2.1). To systematically evaluate this collaborative process, we propose several metrics for task accuracy, user experience, and efficiency aspects (2.2). Beyond agent web automation, COWPILOT enables wide range of use cases (3), including web automation3.1), data collection for agent trajectories and user feedback (3.2), as well as evaluations for single or multiple agents (3.3). We conduct studies on five common websites across shopping, social, and technical domains (4). We show COWPILOT in collaborative mode achieves higher success rates over autonomous agents by 47%, and even human-only settings by 6%. Moreover, the LLM agent takes 84.8% of the steps and can drive up to half of the task success, greatly easing human efforts. These results suggest the potential for accuracy and efficiency improvement with COWPILOT. Overall, COWPILOT showcases the potential of human-agent collaborative web navigation, and Figure 1: step-by-step illustration of how human intervention enables the agent to overcome failure point during task execution. The figure uses gray edges to represent the agents autonomous actions and blue edges to indicate human intervention. The process begins with the agent attempting the task independently (Step 1 ) and navigating to the interface to list available forums (Step 2 ). At this stage, the agent gets stuck, unable to locate the desired space forum. human intervenes (Step 3 ), guiding the agent to the correct forum. The user then resumes the agents operation (Step 4 ), allowing it to retrieve the required post and complete the task by navigating to the comments section (Step 5 ).\" serves as useful tool for future web automation, data collection, and evaluation research."
        },
        {
            "title": "2 COWPILOT",
            "content": "In this section, we introduce the COWPILOT framework (2.1) and evaluation metrics for task accuracy and collaboration quality (2.2)."
        },
        {
            "title": "2.1 The COWPILOT System",
            "content": "Given an objective stated in natural language (NL) (e.g., book flight from New York to Pittsburgh) for the web environment, we define two agents: one agent instantiated with an LLM policy πL, and one human agent πH. At each time step t, based on the observation ot from the environment state st, either the LLM agent or human agent generates an action at, formalized as at = π(t, ot, a0:t1). Executing at on the environment results in new state st+1 that gives observation ot+1 that drives the next step. The two agents collectively generate sequence of actions a0:n over steps, until it reaches task termination condition, e.g., output STOP or maximum number of steps. By default, the LLM agent starts generating actions aL 0 from = 0 unless intervened by the human agent H. Table 1 shows the action space for LLM and human agents. Actions taken by the human agent πH are critical for optimizing COWPILOTs decision-making pipeline. When the human agent intervenes, they provide contextual feedback by identifying and correcting prior mistakes made by the LLM agent. This redirection helps the agent recover from suboptimal path and proceed with more viable course of action. At the same time, by integrating human actions into its action history, COWPILOT ensures that the LLM agent is aware of human corrections since its last decision, preventing redundant actions and enabling efficient task progression. To ensure effective integration of these human actions, COWPILOT incorporates the following core modules: Suggest-then-Execute under Human Supervision At any time step, the human agent πH can decide to take over by generating action aH . More concretely, the LLM agent πL generates an action ai and presents it as suggestion for the tentative next step to the user (Figure 2, 1 ), which includes visual indicator highlighting the target element for the proposed action, accompanied by textual explanation of the agents reasoning. This tentative step is presented to the human agent for at most five seconds, and is automatically executed if the human agent does not oppose. Otherwise, the human agent can choose to reject or pause the action (Figure 2, 3 ) and take over. They can also transfer the action back to the LLM-based agent by hitting the resume button (Figure 2, 4 ). This takeover-then-back process can be conducted unlimited times per task-solving session. This mechanism balances operational efficiency with user oversight, allowing users to intercept potential errors without the burden of manually approving every step. Pause LLM Agent: Extract Human Actions Whenever the human agent πH rejects the LLMproposed action, our COWPILOT system starts tracking human activity on the websites, particularly what webpages and UI elements they interact with. To capture this micro-level metadata, we uti-"
        },
        {
            "title": "Raw Human Action Description",
            "content": "click(elem) hover(elem) type(elem, text) scroll(dir) goto(url) goto(tab) click mouseover input wheel Click on webpage element using the mouse. Hover the mouse over an element without clicking it. Enter text into text area or text box. Scroll the webpage up/down/left/right. Tabs.onUpdated - Navigate to specific URL. Navigate to specific tab. finishwithanswer(text) finish() failure() - - - For information retrieval task, terminate the task with retrieved textual information. Mark the task as completed. Mark the task as failed and uncompleted. Table 1: Action space of agents in COWPILOT. LLM agent supports all actions in the Action column. Human actions are captured by entries in the Raw Human Action column and transformed into Actions. lize HTML event listener1, that are attached to the interactive elements (e.g., text field, buttons, dropdown menu) in the current webpage and triggered each time the elements are accessed by the user. Note that, actions captured by the HTML event listener can include noisy actions irrelevant to the task (Siqueira and Baldochi, 2018; Cheng and Kumar, 2015), such as unintentional mouseover events. Hence, we transform the listener-captured actions to the LLM agent actions space, from Raw Human Action to Action column in Table 1, which are also coped with proper textual descriptions that can help the agent interpret user inputs effectively. To facilitate this transformation, we use an off-theshelf LM (gpt-4o-2024-08-06) and provide the raw human actions as input. The model outputs the transformed and cleaned version of the actions. (Find the prompt for this transformation in A.1.) Resume LLM Agent: Predict next Action using Human Input If the human agent chooses to resume the LLM agent at any given step, our COWPILOT stops tracking human actions and restarts LLM agent generation (Figure 2). Note that the LLM agent has access to all previous actions generated by itself and the human agent."
        },
        {
            "title": "2.2 Evaluation Metrics",
            "content": "To evaluate the agent performance in COWPILOT, we report general agent task success. In addition, to better quantify human-agent collaboration, we introduce five evaluation metrics to measure various aspects throughout task execution. General Task Success To measure generic task success, we measure end-to-end task accuracy, 1https://developer.mozilla.org/en-US/docs/Web/ API/EventTarget/addEventListener which measures if the task objective is achieved after the agent task-solving process. At the end of the task, the agent self-marks its success or failure by generating finish or failure action highlighted in Table 1. Optionally, the user can overwrite the success measure if they disagree with the agents self-evaluation. Human-Agent Collaboration To measure how human and agent interacted with each other throughout the task execution, we first measure the engagement of both parties, by: (1) Agent step count: how many steps are taken by the agent per task; (2) Human step count: how many steps are taken by the human per task; (3) Total step count: the sum of steps taken by human and agent. Meanwhile, we measure agent capabilities via (4) Human intervention count: how many times does the user pause the agent to take actions themselves. Note that single intervention may involve multiple steps performed by the human, as the intervention continues until the agent resumes. higher value potentially suggests that the agent made frequent errors and users had to step in to resolve the mistakes; and (5) Agent-driven completion accuracy: how many tasks are completed by the agent, i.e., the terminating step was taken by the agent. higher value indicates the agents ability to recover and complete tasks autonomously after human intervention, whereas lower value reflects its reliance on human assistance."
        },
        {
            "title": "3 Use Cases of COWPILOT",
            "content": "COWPILOT has numerous potential use cases. We particularly highlight three use cases under the scope of this work. 3 Figure 2: Example of COWPILOTs core interaction modules during task execution. At step 1 , the LLM agent generates suggestion, highlighting the textual description and the UI element where the action will be performed. At step 2 , the user identifies an erroneous action, chooses to pause the LLM agent, and proceeds to perform corrective actions manually (step 3 , e.g., typing in the textfield, highlighted in blue). At step 4 , the user chooses to resume the LLM agent, allowing it to continue generating actions. The agent resumes successfully and proceeds to execute subsequent steps autonomously (step 5 )."
        },
        {
            "title": "3.1 Web Automation",
            "content": "COWPILOT can be standalone agent framework to automatically conduct web tasks for end users is implemented as Chrome extension where all computations other than the LLM calls are handled locally with minimal storage requirement (<50MB). Any users can easily install COWPILOT with just four clicks and use it with their personal API key. We use LiteLLM2 proxy server for our backend LLM, enabling COWPILOT to support all models available via LiteLLM, including closedsource (e.g., GPT) and open-weight models (e.g., LLAMA). Depending on whether the user wants to participate in task-solving, our agent can operate in two modes: 1) Fully autonomous mode: the agent conducts user-issued task start-to-end; 2) CoPilot mode: human and agent collaboratively solve task, often useful for complex tasks where the agent is more prone to make mistakes."
        },
        {
            "title": "3.2 Data Collection from Websites",
            "content": "COWPILOT can also be used as data annotation tool to collect task trajectories across any website accessible via the Chrome browser. Deployed as 2https://docs.litellm.ai/docs/ Chrome extension, COWPILOT requires no additional setup and supports both simulated, sandboxed and self-hosted websites. We can track all actions conducted by LLM agents and humans. COWPILOT can only track all actions from LLM and human agents, but also collect human feedback at both (i) step-level: whether the user judges the current step correctly leads to task success, and (2) task-level: whether the entire trajectory correctly solves the task. These rich data collections can easily facilitate various studies such as user behavior studies and advanced agent learning strategies."
        },
        {
            "title": "Agent Performance",
            "content": "COWPILOT can be used to evaluate and compare agent performance. We support wide range of open-weight and closed-source models served via LiteLLM. While this paper focuses on comparing GPT and LLAMA, the framework can easily extend to other open and closed-source models. To evaluate particular model, the user can select model before initiating task. Once the task is completed, COWPILOT presents results evaluated in the metrics from 2.2. To compare different"
        },
        {
            "title": "CoPilot",
            "content": "Human-only GPT-4O LLAMA 8B GPT-4O LLAMA 8B - End-to-End Task Accuracy () 0.48 0.04 0.95 0.81 0.89 Agent Step Count () 5.48 7.00 6.36 4.77 0.00 Human-Agent Collaboration Metrics Human Step Count () 0.00 0.00 1.14 4.15 9.93 Human Intervention Count () 0.00 0.00 0.73 1.15 - Total Step Count () 5.48 7.00 7.50 8.92 9.93 Agent-driven Completion Accuracy () 0.48 0.04 0.52 0.05 - Table 2: Evaluation on WebArena subset using COWPILOT. models on the same task, the user can re-do the task with different models, allowing for clear, unbiased comparisons under identical conditions."
        },
        {
            "title": "4 Exemplar Findings via COWPILOT",
            "content": "To demonstrate the usage of COWPILOT, we evaluate on subset of WebArena (Zhou et al., 2023) benchmark, including 27 tasks categorized into easy, medium, and hard difficulty levels. We categorize the difficulty by the number of examples successfully solved by the topperforming agent (Wang et al., 2024b) on WebArena, and assign them as easy, medium, hard if they have <2, 24, and >4 correctly solved examples among the same task template group. We evaluate under two settings: fully autonomous and copilot mode, using gpt-4o-2024-08-06 and Llama-3.1-8B-Instruct as backbones for the LLM agent. For this study, three authors served as human agents, independently performing the tasks for both settings. The results reported represent the average performance across these evaluations. Additionally, we included baseline where tasks were executed solely by humans without any agent participation. Table 2 reports results on all metrics introduced in 2.2."
        },
        {
            "title": "4.1 Copilot Mode Achieves the Best Accuracy",
            "content": "CoPilot mode with GPT-4O achieves 95% task accuracy, significantly outperforming the 48% accuracy under autonomous mode (relatively by 97.9%), and even surpassing human task-solving accuracy by 6.7%. This suggests potential productivity increases when solving tasks together with strong LLM-based agents. On the other hand, copilot mode with the smaller LLAMA 8B model does not bring similar accuracy increases, but slightly degrades the task accuracy by 8%. Figure 3: Correlation between Human Step Count and End-to-End Task Accuracy."
        },
        {
            "title": "Intervention",
            "content": "Despite the high task success rates, the GPT-based agent easily achieves the highest accuracy with an average of 1.1 human steps, taking only 15.2% of the entire trajectory. Instead, the LLM agent performs the majority, more precisely 84.8% of task steps. Similarly, when shifting to the weaker LLAMA model, the human-llm collaboration process requires two times more human involvement, resulting in humans and LLM agents spending roughly similar amounts of effort, taking 4.47 and 4.15 respectively. Figure 3 shows the correlation between human step count and end-to-end task accuracy. Qualitatively, humans often choose to intervene when they observe that the LLM has gotten stuck (e.g., producing the same invalid actions multiple times) or performs an obviously wrong action (e.g., clicking Customers instead of Orders tab when searching for particular order), especially when the webpage layout is less common or has confusingly large number of elements."
        },
        {
            "title": "Dynamic\nWebsite",
            "content": "End-to-End Human Annotation Human-Agent Interaction Human-Agent Co-task Execution"
        },
        {
            "title": "WebArena\nSeeAcT\nBrowserGym\nWebLinX\nWebCanvas\nWebOlympus\nCOWPILOT",
            "content": "Table 3: Comparison of COWPILOT with existing agent web navigation frameworks."
        },
        {
            "title": "4.3 Agents Drive Up to Half of the Success",
            "content": "In CoPilot mode, we notice that agent-drive completion accuracy was up to 52% of the time with GPT-4O model. Note that, given the task accuracy was 0.95, the copilot-mode agent successfully initiated half of the successes. These findings highlight that agents can follow the task objective and understand user actions to drive the task up to succeed."
        },
        {
            "title": "5.1 Web Agent Plugin",
            "content": "The rise of LLM agents has led to the development of open-source toolkits for web automation, available as APIs, simulated environments, and Chrome extensions. Tools like MultiOn (MultiOn, 2024) and Anthropic (Anthropic, 2024) provide APIs for agent use but require setting up Docker images, posing barriers for non-technical users. BrowserGym (Drouin et al., 2024), AgentLab (Chezelles et al., 2024), WebArena (Zhou et al., 2023) utilize dedicated Chromium browser instance to perform tasks on specified websites. However, this approach isolates browsing sessions, restricts multi-tab navigation, and diverges from standard workflows, which limits practical usability. Chrome extensions, as adopted by tools like WebCanvas (Pan et al., 2024b), WebOlympus (Zheng et al., 2024b), OpenWebAgent (Iong et al., 2024), and Taxy (TaxyAI, 2024), present more userfriendly alternative. They are easy to install, lightweight, and integrate seamlessly into standard browsing environments, making them accessible to end-users. While similar to COWPILOT, the extensions above lack features for fostering richer human-agent collaboration. Table 3 further compares how COWPILOT with the existing frameworks by illustrating its novel features."
        },
        {
            "title": "5.2 LLM Agents for Web automation",
            "content": "Web automation has evolved through advancements in LLM-based agents and benchmarks. Early systems relied on HTML structures and accessibility trees (Deng et al., 2024; Gur et al., 2023, 2022; Kim et al., 2023). Visual-based systems such as SeeACT (Zheng et al., 2024a), VisualWebArena (Koh et al., 2024), WebGUM (Furuta et al., 2023) integrate spatial and visual understanding, enhancing agent performance in multimodal tasks. Benchmarks such as MiniWoB (Shi et al., 2017) laid the foundation for evaluating these interactions, while systems like WebShop (Yao et al., 2022), WebArena (Zhou et al., 2023), WebLINX (Lù et al., 2024) expanded to complex multi-step tasks in ecommerce and real-world websites. Despite these advances, existing systems focus largely on full autonomy, with limited support for human-in-the-loop collaboration. In contrast, COWPILOT bridges this gap by enabling dynamic, real-time human-agent interaction. Features like suggest-then-execute, pause, and resume facilitate adaptive task execution, make COWPILOT robust platform for developing and evaluating agents in practical, real-world settings."
        },
        {
            "title": "6 Limitation and Future Work",
            "content": "Currently COWPILOT requires human to act as an observer to oversee the task execution. This setup is intentional so that we can simulate task execution in live setting. We would like to extend our work so that it does not require constant human observation. Rather, we would detect the critical steps that require human observation only. In the future, we would extend COWPILOT for multiLLM agent setup where we can simulate user by second LLM agent. Such setup would help us to approximate human decisions automatically using LLM autorater (Pan et al., 2024a) and incorporate an active learning framework (Bai et al., 2024). We acknowledge potential ordering bias in the comparative evaluation of autonomous and CoPilot modes. We are currently conducting large-scale study across diverse demographic to assess and mitigate the impact of such biases."
        },
        {
            "title": "Societal Impact",
            "content": "Web agents have significant potential to promote web accessibility and enhance user efficiency. However, their deployment raises important privacy and security concerns. For instance, tracking user actions may expose sensitive information, which could be exploited for malicious purposes (e.g. data theft). Additionally, agents may inadvertently perform harmful or irreversible actions (e.g. confirming financial transactions without explicit user consent). Beyond inadvertent risks, there is also the potential for intentional misuse, where malicious actors could exploit COWPILOT for unethical purposes. We firmly discourage any such misuse of COWPILOT. To balance accessibility with safety, we have chosen not to open-source our codebase. Instead, we will release the extension publicly through the Chrome Web Store. Users must provide their own API key for supported LLMs, or they can modify the extension to use different LLM if they prefer, ensuring that their information is not shared with us. Future work can focus on addressing such safety risks and transparency, including developing robust safeguards to prevent unintended actions."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Daniel Fried, Tianqi Chen, and Yonatan Bisk for their valuable feedback on the design of COWPILOT. We also acknowledge the creators of Taxy (TaxyAI, 2024) and BrowserGym (Drouin et al., 2024) for open-sourcing their codebases on GitHub which served as the foundational building blocks for COWPILOTs development."
        },
        {
            "title": "References",
            "content": "Anthropic. 2024. Computer use (beta). Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. 2024. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. arXiv preprint arXiv:2406.11896. Hsin-Jung Cheng and Akhil Kumar. 2015. Process mining on noisy logs - can log sanitization help to improve performance? Decis. Support Syst., 79:138 149. Thibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan Salakhutdinov, Nicolas Chapados, and Alexandre Lacoste. 2024. The browsergym ecosystem for web agent research. Preprint, arXiv:2412.05467. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2024. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36. Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, et al. 2024. Workarena: How capable are web agents at solving common knowledge work tasks? arXiv preprint arXiv:2403.07718. Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. 2023. Multimodal web navigation with instruction-finetuned foundation models. ArXiv, abs/2305.11854. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2023. real-world webagent with planning, long context understanding, and program synthesis. ArXiv, abs/2307.12856. Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. 2022. Understanding html with large language models. ArXiv, abs/2210.03945. Iat Long Iong, Xiao Liu, Yuxuan Chen, Hanyu Lai, Shuntian Yao, Pengbo Shen, Hao Yu, Yuxiao Dong, and Jie Tang. 2024. OpenWebAgent: An open toolkit to enable web agents on large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 7281, Bangkok, Thailand. Association for Computational Linguistics. Geunwoo Kim, Pierre Baldi, and Stephen Marcus McAleer. 2023. Language models can solve computer tasks. ArXiv, abs/2303.17491. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. 2024. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649. Xing Han Lù, Zdenˇek Kasner, and Siva Reddy. 2024. Weblinx: Real-world website navigation with multiturn dialogue. arXiv preprint arXiv:2402.05930. MultiOn. 2024. Agent api. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. 2024a. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474. 7 Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, et al. 2024b. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. 2017. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 31353144. PMLR. Wesley G. Siqueira and Laércio Augusto Baldochi. 2018. Leveraging analysis of user behavior from In web usage extraction over dom-tree structure. International Conference on Web Engineering. TaxyAI. 2024. Taxy ai. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. 2024a. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741. Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. 2024b. Agent workflow memory. arXiv preprint arXiv:2409.07429. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024a. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614. Boyuan Zheng, Boyu Gou, Scott Salisbury, Zheng Du, Huan Sun, and Yu Su. 2024b. WebOlympus: An open platform for web agents on live websites. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 187197, Miami, Florida, USA. Association for Computational Linguistics. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. 2023. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854."
        },
        {
            "title": "A Appendix",
            "content": "Figure 4 shows screenshot of evaluation results by COWPILOT. After the task is completed, the summary will be shown containing the metric values covered by subsection 2.2. The scores are autocalculated based on user intervention count and pause/resume statistics. The user can also modify each entry and save local copy of the trajectory data by clicking on the download icon. A.1 Prompt for Action Transformation the prompt we used for Figure 5 shows GPT-4o-2024-08-06 to transform the raw user action into filtered actions. The prompt is provided with the event log data structure as well as the agent action space structure. These structures help the LLM to be aware of the structural representation of input raw action. The LLM replies with list of actions together with their natural language description. The output will be used for the LLM agent action prediction when it is resumed. Figure 4: Screenshot of COWPILOT evaluation result page. After each task is completed, the evaluation metric values are shown as summary. 9 You will be shown list to HTML eventlistener logdata of the following format: export interface EventLogStructure{ action_type: string; // event type (click/scroll/keyup/input/KeyboardEvent/mouseover/contextmenu) nodeID?: string; // if set, unique ID of the element acted on elementName?: string; DOM?: string; elementouterHTML?: string; AXTree?: string; // accessibility tree of the HTML page Screenshot?: string; coordinateX?: number; coordinateY?: number; clickType?: string; position?: string; URL?: string; // URL of the current page whre the events are taking place scrollData?: { deltaX: number; deltaY: number; deltaMode: number; isLine: boolean; isPage: boolean; isPixel: boolean;}; keyData?: { key: string; code: string; isCtrlPressed: boolean; isShiftPressed: boolean; isAltPressed: boolean; isMetaPressed: boolean; fulltextentry: string;}; urldata?: { // when new tab is opened, the information of the new url and tab id url_name: string; tab_id: number;};} Your task is to clean up the raw event data and make clean list of user actions in the following format: Agent Action Space Rules: 1. Try to merge consecutive UserLogStructure whenever possible. For example, you can merge multiple keyup actions in the same input field as setvalue event. For consecutive input in textbox, always pick the final one. For example, 1) setValue(20, Hello) ... 10) setValue (20, Hello world) can be merged into single action setValue (20, Hello world) 2. If there are repetitive user actions of the same type in the same place, feel free to discard duplicates. This might specially be true for scroll and mouseover event. For example: two consecutive scrolls in the same direction can be merged. Or, random, disjoint scroll can be considered as noise to be ignored. 3. Only reply with availableActions.name(args) format. Do not write any code. 4. Mouseover user log can often be noisy, only add this to the final list if it is meaningful with the rest of the action trajectory in prior and after the mouseover event. For example, mouseover while tying into textfield is not useful and can be discarded. 5. Your response must follow json format: [{\"thought\": short summary of the action, \"action\": your generated action}]. Input: Raw User Actions Figure 5: Prompt for Action Transformation from Raw Event to Agent Action Space."
        }
    ],
    "affiliations": [
        "School of Computer Science, Carnegie Mellon University"
    ]
}