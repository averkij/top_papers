{
    "paper_title": "AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization",
    "authors": [
        "Ashutosh Chaubey",
        "Jiacheng Pang",
        "Maksim Siniukov",
        "Mohammad Soleymani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 4 5 0 7 0 . 2 0 6 2 : r Accepted as conference paper at ICLR AVERE: IMPROVING AUDIOVISUAL EMOTION REASONING WITH PREFERENCE OPTIMIZATION Ashutosh Chaubey, Jiacheng Pang, Maksim Siniukov & Mohammad Soleymani Institute for Creative Technologies University of Southern California Los Angeles, CA 90007, USA achaubey@usc.edu & soleymani@ict.usc.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models (MLLMs) have shown strong performance on this task, two key challenges remain: (i) spurious associations between emotions and irrelevant audiovisual cues (reasoning errors) and (ii) hallucination of audiovisual cues (perception errors) driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, benchmark designed to evaluate MLLMs for cueemotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over (i) responses exhibiting spurious associations or hallucinations and (ii) audiovisual input pairs guided by textual prompts. We also include regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models (6-19% of relative performance) in zero-shot settings. By providing both rigorous benchmark and robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at avere-iclr.github.io."
        },
        {
            "title": "INTRODUCTION",
            "content": "Emotion understanding is essential for social AI agents to generate tailored responses and foster meaningful humanmachine interactions (Chaturvedi et al., 2023; Kolomaznik et al., 2024; Elyoseph et al., 2024). Emotion perception also finds applications in domains such as health (Balcombe & De Leo, 2022; Litendahl et al., 2025) and education (Salloum et al., 2025), where appropriately responding to affective states can improve therapeutic alliance and learning outcomes. Traditional multimodal emotion recognition methods (Sun et al., 2023; Wang et al., 2023; Chen et al., 2024) lack interpretability, as they only perform classification without grounding responses in audiovisual cues. Moreover, emotion is complex and multi-componential construct that extends beyond the basic emotion labels that can be assigned by supervised learning methods (Ekman & Friesen, 1978; Scherer, 2005). To address these challenges, recent approaches leverage multimodal large language models (MLLMs) to generate detailed emotion descriptions for interpretability (Cheng et al., 2024; Huang et al., 2025a) and to output emotion-related keywords that cover broader spectrum of emotional states (Lian et al., 2024; 2025a). However, audiovisual MLLMs are susceptible to hallucinations, frequently generating inaccurate or fabricated responses (Li et al., 2023; Sahoo et al., 2024). In the context of emotion understanding, they face two critical bottlenecks, as illustrated in Fig. 1. First, these models often ground emotion predictions on irrelevant cues (e.g., attire color, ambient noise) reasoning errors. Second, they tend to hallucinate additional cues in their responses to justify emotions perception errors. Such hallucinations are largely driven by text priors in the language model backbone, which bias the model to include cues that commonly co-occur with specific emotions (e.g., associating tears 1 Accepted as conference paper at ICLR 2026 Figure 1: Existing MLLMs (i) include spurious associations between AV cues and emotions reasoning errors (blue highlight) and (ii) hallucinate AV cues to explain emotions perception errors (red highlight). AV: audiovisual. with the sound of crying). The scarcity of high-quality, emotion-specific instruction tuning datasets (Cheng et al., 2024; Lian et al., 2025a) further aggravates these issues. Addressing these challenges is essential, as they compromise the reliability of MLLM agents in social interactions and complex emotion reasoning scenarios. Existing emotion reasoning benchmarks (Lian et al., 2023b; 2024) lack the diverse and complex samples needed to fully evaluate these issues. Additionally, current audiovisual hallucination benchmarks (Sung-Bin et al., 2025; Leng et al., 2025) predominantly focus on object-level hallucinations in audio or video, rather than on emotion-specific reasoning. Moreover, many existing MLLMs (Cheng et al., 2024; Lian et al., 2025a) rely on two-stage evaluation pipelines involving an external (often proprietary) LLM such as GPT (OpenAI et al., 2024), making replication and benchmarking difficult. To address these limitations, we introduce the EmoReAlM benchmark, comprehensive suite of multiple-choice questionanswer (MCQA) tasks designed to evaluate audiovisual emotion reasoning, modality agreement and hallucination-related stress tests  (Fig. 2)  . The MCQA format enables transparent, reproducible and scalable evaluation of MLLMs on emotion-centric tasks without requiring additional LLMs during inference. Evaluation of recent MLLMs on our benchmark highlights spurious association and hallucination issues outlined in Fig. 1. To address these limitations, we propose AVEm-DPO multimodal direct preference optimization (DPO) technique (Rafailov et al., 2023) to enhance the emotion reasoning capabilities of MLLMs. In particular, we design explicit prompt-based audiovisual input preferences to mitigate hallucinations caused by cross-modal interactions. We also introduce textprior debiasing, which penalizes policy reward for responses to text-only inputs. Together, these techniques significantly improve the performance of reference MLLMs, outperforming all baselines in zero-shot evaluation on both our benchmark and existing emotion recognition and reasoning datasets. To summarize, the main contributions of our work are: We introduce the EmoReAlM benchmark with 4000 human-verified MCQA samples to evaluate emotion reasoning and emotion-related hallucinations in MLLMs, highlighting bottlenecks such as spurious audiovisual cue associations and hallucinated cues for explaining emotions. We propose AVEm-DPO, direct preference optimization technique that enforces explicit prompt-based modality preferences and reduces text-only model biases through regularizer that penalizes over-reliance on text priors. We conduct extensive evaluations of existing MLLMs, demonstrating current bottlenecks and showing the superior performance of the proposed DPO-trained models in zero-shot settings."
        },
        {
            "title": "2 RELATED WORK",
            "content": "MLLMs for Emotion. While general MLLMs (Zhang et al., 2024; Lin et al., 2024; Zhang et al., 2025a; Xu et al., 2025b; Li & team, 2025) show non-trivial emotion recognition ability (Cheng et al., 2024), several studies pursue domain-specific instruction tuning (Xie et al., 2024; Chaubey et al., 2025; Yang et al., 2025). EmotionLLaMA(Cheng et al., 2024) is an audiovisual LLM for emotion recognition and captioning, finetuned on limited dataset (30k samples). Lian et al. (2024) introduces open-vocabulary emotion recognition (OV-ER), and AffectGPT (Lian et al., 2025a) employs lightweight audiovisual fusion projector for OV-ER. EmotionQwen (Huang et al., 2025a) improves emotion understanding while preserving general skills via mixture-of-experts router. Han 2 Accepted as conference paper at ICLR 2026 Figure 2: EmoReAlM Tasks. In addition to basic emotion reasoning, we include tasks for Modality Agreement and Emotion Reasoning - Stress Test to test spurious cue-emotion associations and cue hallucinations. Red text is hallucinated cue, blue text is an emotion-irrelevant cue and green text is cue relevant for emotion understanding. Correct choices are underlined. et al. (2025b) use modality-specific experts with attention reallocation to handle audiovisual emotion mismatch, and Wen et al. (2025) leverage retrieval-augmented generation with chain-of-thought for better reasoning. In contrast, we improve reasoning through multimodal preference optimization and text-prior debiasing. Rigorous evaluation of multimodal emotion reasoning requires diverse, systematic benchmarks. Lian et al. (2023b) provide detailed descriptions of transcript, audio and visual cues for emotion reasoning, which can support GPT-based evaluation (Cheng et al., 2024; Han et al., 2025b). Xing et al. (2025) present holistic benchmark spanning text, image, video and audio hallucinations related to emotions. Our benchmark instead focuses squarely on audiovisual emotion understanding with standardized pipeline and tasks beyond hallucination, including modality agreement and spurious cueemotion associations. Preference Optimization. Direct preference optimization (DPO) (Rafailov et al., 2023; Liu et al., 2025a) was introduced to align LLMs to human preferences. DPO has also emerged as leading approach for mitigating hallucinations in vision LLMs (Yu et al., 2024; Wang et al., 2024; Sarkar et al., 2025; Huang et al., 2025b; Liu et al., 2025b; Zhang et al., 2025b), but its use in audiovisual LLMs remains limited. VistaDPO (Huang et al., 2025b) increases video LLM robustness by building instance-level, temporal-level and object-level preferences of video inputs. Sun et al. (2025) apply process DPO for step-wise audiovisual reasoning, while Tang et al. (2025) use multi-round DPO for audiovisual captioning. Luo et al. (2025) employ DPO for emotional speech alignment to improve Omni-LLM outputs. Ye et al. (2025) construct multimodal preference data via ambiguity scoring, and Lian (2025) use group relative policy optimization for AffectGPT. Concurrently, Omni-DPO (Chen et al., 2025) studies audiovisual modality preference. Our method differs by constructing prompt-based audiovisual preference pairs for fine-grained alignment and by introducing text-prior debiasing to reduce hallucinations in MLLMs."
        },
        {
            "title": "3 EMOREALM BENCHMARK",
            "content": "Fig. 2 shows different tasks present in the proposed EmoReAlM Benchmark. The goal of this benchmark is to test the reasoning capabilities of MLLMs to judge the emotion experienced by the character in the given video, specifically over the following verticals (i) reasoning the correct emotion with relevant audiovisual cues (ii) identifying whether the inferred emotion from audio and video are in agreement (iii) testing the association of perceived audiovisual cues with different 3 Accepted as conference paper at ICLR 2026 Figure 3: EmoReAlM Creation Pipeline. We first disentangle the audiovisual information by separate captioning and verify the cues with text-based emotion prediction to find emotion-relevant cues. Finally, GPT-4o is used to generate MCQA samples that are later verified manually. emotions (reasoning errors) and (iv) testing audiovisual hallucination due to text-only emotionrelated biases (perception errors). 3.1 TASK DESCRIPTIONS Emotion Reasoning Basic. This task evaluates an MLLMs ability to identify and reason about the emotion experienced by person in video by linking appropriate audio (e.g., speech transcription, tone) and visual (e.g., facial expression, body language) cues to specific emotions. To increase difficulty, the ground-truth emotion is not provided in the question. Incorrect options are constructed by modifying the correct answer to include either emotion-irrelevant cues present in the video or hallucinated cues that falsely justify the emotion. Modality Agreement. This task assesses whether the audio and visual modalities convey the same emotional state. Unlike AVHBench (Sung-Bin et al., 2025), which focuses on general cross-modal alignment, this task specifically targets agreement in emotional interpretation across modalities. Emotion Reasoning Stress Test. MLLMs are vulnerable to both reasoning errors and perception errors: the former lead the model to base its responses on irrelevant audiovisual cues present in the input, while the latter cause it to rely on hallucinated cues that are not actually present. This task probes MLLMs for susceptibility to spurious cue-emotion associations (perception errors) and hallucinated explanations driven by language model biases (reasoning errors). Each question follows the format: Does the {audio/visual cue} suggest {emotion} of the character?. For modality X, we define three sub-tasks: (i) No Hallucination correctly associating an audio/visual cue with the appropriate emotion. (ii) Spurious Cue-Emotion Association linking emotion-irrelevant cues to the correct emotion. (iii) Emotion-Relevant X-Hallucination associating the correct emotion with hallucinated cue that typically co-occurs with it. For example, in Fig. 2, man is not clapping (per the visual caption), yet hallucination-based question associates clapping with happinesssince clapping is commonly linked to positive emotions like joy. 3.2 AUTOMATIC DATA CREATION Fig. 3 shows the automatic pipeline used to construct the EmoReAlM benchmark. Our approach builds on existing manually labeled audiovisual emotion recognition datasets that provide singleword emotion annotations. For each video, we first use an MLLM to extract detailed audio and visual captions separately, effectively disentangling the two modalities. These captions describe both emotion-relevant and irrelevant cues. To verify whether either modality reflects an emotion, we prompt an LLM to classify the audio and video captions independently into one of seven categories of neutral, in addition to six basic emotions Ekman (2005). Samples are discarded if neither caption yields valid emotion label. Given the validated captions and emotion label, we then generate tailored prompts and question templates for each task described in Section 3.1. This modalitywise captioning and emotion verification process ensures the construction of high-quality, verifiable MCQA pairs that reflect meaningful audiovisual cue associations. More details and prompts are present in Section B. Details. All videos are sourced from the DFEW dataset (Jiang et al., 2020). GPT-4o (OpenAI et al., 2024) is used for caption extraction, emotion classification and questionanswer pair generation. 4 Accepted as conference paper at ICLR"
        },
        {
            "title": "3.3 POST-PROCESSING AND HUMAN VERIFICATION",
            "content": "We employ GPT-4o (OpenAI et al., 2024), Gemini-2.5 (Gemini-Team et al., 2025) and Qwen-2.5 (Qwen-Team et al., 2025) to predict the correct answer to the generated questions just by using question text as input. We remove all the QA pairs for which all the models identified the correct answer just with the text information. Finally, since the QA samples are generated automatically leveraging MLLMs, which can hallucinate themselves, we perform human verification over the samples generated by recruiting over 470 participants using the crowd-sourcing platform Prolific. Details are present in Section B.2."
        },
        {
            "title": "3.4 BENCHMARK STATISTICS",
            "content": "Task Reasoning Basic Table 1 summarizes the data statistics of the proposed EmoReAlM Benchmark, which comprises total of 4,000 questions over 2,649 unique videos. Samples from the benchmark are present in Section B.5. Importantly, for tasks which always have fixed set of answer choices (Emotion Reasoning - Stress Test and Modality Agreement Yes/No), we ensure that there is uniform distribution of correct answer texts over the possible answer choice texts. Additionally, we ensure that the distribution of emotion labels over the videos in the benchmark matches the video source dataset (refer to Section B.3 for details). It is also important to note that EmoReAlM is only used as test set to evaluate the reasoning capabilities of MLLMs, and we use different dataset for preference optimization (refer Section 4.3). Table 1: EmoReAlM Benchmark Statistics. # QA # vid. Rand. Acc. 972 1024 456 820 728 4000 784 883 456 655 593 2649 25% 25% 50% 50% 50% Modality Agreement Reas. Stress Test Audio Visual Audio Visual Total"
        },
        {
            "title": "4 AVEM-DPO",
            "content": "Direct preference optimization (DPO) (Rafailov et al., 2023) aligns LLMs to human preferences, bypassing the need to develop reward model. In the context of audiovisual LLMs, given reference model πref, we can reformulate the DPO objective to learn an optimal policy πθ as the following, max πθ E(a,v,x)D,yπθ (a,v,x) [r(a, v, x, y)] βDKL(πθ( a, v, x) πref( a, v, x)) (1) where (a, v) is audiovisual input, is text prompt, is text response and r(a, v, x, y) is the reward function for given input-output pair. Optimizing Eq. (1) to find optimal policy results in the following reward formulation, r(a, v, x, y) = β log πθ(y a, v, x) πref(y a, v, x) + β log Z(a, v, x) (2) where Z() is the partition function derived in Rafailov et al. (2023). With access to preference dataset Dpref with samples (a, v, x, yw, yl) and using the Bradley-Terry preference model (Bradley & Terry, 1952) to model preference of chosen response (yw) over rejected response (yl), the final DPO objective becomes LDPO = (a,v,x,yw ,yl)Dpref (cid:20) (cid:18) log σ β log πθ(yw a, v, x) πref(yw a, v, x) β log πθ(yl a, v, x) πref(yl a, v, x) (cid:19)(cid:21) (3) 4.1 MULTIMODAL PREFERENCE OPTIMIZATION Naive DPO (Eq. (3)) applied to MLLMs, when relying only on response preference, often causes the policy model to overfit to the input prompt while neglecting the multimodal inputs during alignment (Wang et al., 2024; Sarkar et al., 2025). To address this limitation, preference optimization can be extended to incorporate audiovisual inputs as follows: DPO = E(cid:2) log σ(u(aw, vw, al, vl, x, yw))(cid:3), u() = β log Lav πθ(yw aw, vw, x) πref(yw aw, vw, x) β log πθ(yw al, vl, x) πref(yw al, vl, x) (4) 5 Accepted as conference paper at ICLR 2026 Figure 4: Preference pairs in AVEm-DPO. (Top) Fine-grained preference over modality input based on current prompt. (Bottom) Each chosen response yw has two rejected responses yvr relel vant to the video but with spurious emotion association and yer irrelevant to the video (hallucinated) but related to the emotion. where (aw, vw) and (al, vl) denote the chosen and rejected multimodal inputs. This objective ensures that the policy model aligns its response yw to the correct (chosen) audiovisual input (aw, vw). Prompt-based Modality Preference (PMP). While Eq. (4) enforces preference over non-text inputs, in the case of audiovisual (or omni) LLMs the input prompt xm may relate to both audio and visual modalities, or to only one of them (m = {AV, A, V}). This often leads to crossmodality-induced hallucinations in MLLMs (Sung-Bin et al., 2025), where response to prompt concerning one modality xm1 is spuriously influenced by another modality m2 {m1}. To mitigate this issue, we construct the preference dataset Dpref av with fine-grained modality-level preferences conditioned on the input prompt xm, as illustrated in Fig. 4 (Top). For example, for query specific to one modality xm (e.g., visual: How does the characters body language support their angry state?), we modify only the corresponding input(s) of modality (i.e. visual) in the rejected pair, thereby enforcing that the models response remains grounded in that modality. Thus, our prompt-based modality preference objective becomes, Lavprompt = E(cid:2) log σ(u(aw, vw, aPMP , vPMP , xm, yw))(cid:3) (5) DPO = aw, iff = and vPMP where aPMP = vw, iff = A. We perform multiple forms of negative sampling for constructing (al, vl) (see Section 5.2); however, because our task is emotion reasoning, the best results were achieved when we choose the rejected audiovisual input to be sample with an emotion different from the chosen input (aw, vw). Emotion-based Response Preference. To mitigate spurious cue-emotion associations and hallucinations described in Section 1, for given input (aw, vw, x) we construct two rejected responses that are variations of the chosen response yw, as illustrated in Fig. 4(Bottom). Specifically, yvr inl cludes an audio/visual cue that is relevant to the audiovisual input but does not explain the emotion, whereas yer introduces audio/visual cues related to the emotion but absent from the audiovisual inl put (hallucinated). Following Huang et al. (2025b), we assign weights to these rejected responses in the DPO loss in Eq. (3) as, Ly DPO = (aw ,vw ,x,yw ,yvr log σ β log ,yer )D pref πθ (yw aw , vw , x) πref(yw aw , vw , x) (cid:88) βi log i{vr,er} πθ (yi πref(yi aw , vw , x) aw , vw , x) (6) where βer + βvr = 1. This formulation establishes strong contrasts between chosen and rejected responses, encouraging the policy model to ground its outputs in correct and emotion-relevant audiovisual cues. Unlike Huang et al. (2025b), however, we do not include completely irrelevant responses as rejections in DPO based on empirical findings in Section E.6. 4.2 TEXT PRIOR DEBIASING (TPD) Audiovisual LLMs have strong text priors that cause them to hallucinate and include cues in their response, which usually occur together (e.g., the presence of crying person accompanied by the 6 Accepted as conference paper at ICLR 2026 sound of crying). To suppress such behaviour, we propose to penalize the reward r(a, v, x, y) derived in Eq. (2) to generate the response using only text input as follows, r(a, v, x, y) = β log πθ(y a, v, x) πref(y a, v, x) + β log Z(a, v, x) γTPD log πtext(y x) (7) where πtext is trained language model and γTPD is hyperparameter. In our experiments, we choose πtext to be the language model backbone in πref. This penalty ensures that the responses that are explainable purely by text priors get discounted and responses supported by audio/video get relative credit. Plugging Eq. (7) in the Bradley Terry model results in the following objective, LDPO-TPD = (a,v,x,yw ,yl )Dpref (cid:34) (cid:32) (cid:32) log σ β log πθ (yw (a, v, x)) πref(yw (a, v, x)) log πθ (yl (a, v, x)) πref(yl (a, v, x)) (cid:33) (cid:16) γTPD log πtext(yw x) log πtext(yl x) (cid:17) (cid:33)(cid:35) (8) where (a, v) denote (aw, vw) for simplicity. During training, we stop gradients through πtext as it is just used to identify the text priors that language model has. To maintain the text-only capabilities of the language model backbone, we attach LoRA module (Hu et al., 2022) to it for training. To accommodate two rejected responses, we perform scaling similar to Eq. (6) on the rejected responses in the TPD term as described in Section C.1 (Eq. (8)) to get the final TPD objective Ly DPO-TPD. The final objective function of AVEm-DPO is as follows, LAVEm-DPO = Ly DPO-TPD + λavLavprompt DPO (9) where λav is hyperparameter. Implementation details are present in Section C.3. 4.3 PREFERENCE DATA For AVEm-DPO training, we construct preference data using pipeline similar to Fig. 3. This preference dataset is different from EmoReAlM, which we exclusively use for testing. We use MAFW (Liu et al., 2022) and subset of MER2025 (Lian et al., 2025b) Track-1 train set as the source datasets to create preference samples. We prompt Gemini-2.5 Gemini-Team et al. (2025) to generate variations of the correct answers (chosen responses) to the questions where the audiovisual cue is altered to be either spurious emotion-related video-relevant cue (yvr ) or hallucinated cue related to the emotion present (yer ). Note that we do not perform any manual verification on the generated data, which still results in performance gain demonstrating the efficiency of the proposed approach. Details in Section C.2."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Datasets & Metrics. For EmoReAlM benchmark, we report the average accuracy per task for all the tasks. For tasks with Yes/No responses, we additionally report the precision, recall and F1 score following previous multimodal hallucination benchmarks (Sung-Bin et al., 2025; Li et al., 2023). Beyond EmoReAlM, we also evaluate on established emotion recognition datasetsDFEW (Jiang et al., 2020), RAVDESS (Livingstone & Russo, 2018), MER2023 (Lian et al., 2023a)and the emotion reasoning dataset EMER (Lian et al., 2023b). None of these datasets is used in training to ensure zero-shot evaluation. Following prior work (Cheng et al., 2024; Han et al., 2025b), we report unweighted and weighted average recalls for DFEW and RAVDESS and weighted F1 for MER2023. For emotion reasoning, we adopt GPT-based evaluation (Cheng et al., 2024), comparing generated responses against ground truth. In addition to clue and label overlap, we assess two dimensions: (i) spurious cueemotion associations, where irrelevant cues are linked to emotions, and (ii) hallucinatory cues, where non-existent audiovisual cues are fabricated. For all metrics, higher values indicate better performance. Further details are provided in Section D.1. Reference models. We use two audiovisual MLLMs as reference EmotionLLaMA (Cheng et al., 2024) and our own developed base model. Our model is similar to EmotionLLaMA in architecture with changes to the audio encoder (whisper-large-v3(Radford et al., 2023)) and video encoder (LanguageBind (Zhu et al., 2024)). For EmotionLLaMA, we remove the text (subtitle) input branch to be consistent with the other baselines and retrain the model on the original dataset without subtitles denoted as EmotionLLaMA (Cheng et al., 2024). More details in Section D.2. 7 Accepted as conference paper at ICLR 2026 Table 2: Zero-shot performance comparison of different methods on existing audiovisual emotion recognition benchmarks. Mod. are the modalities input to the model with the prompt. A: Audio, V:Video, T: Text Subtitles. : evaluation without text subtitle input. DFEW RAVDESS MER EMER Model Mod. A,V A,V A,V A,V A,V,T A,V A,V,T A,V A,V VideoLLaMA 2 OLA VITA-1.5 Qwen-2.5 Omni EmotionLLaMA EmotionLLaMA MoSEAR Our base + Naive-DPO + Vista-DPO + AVEm-DPO EmotionLLaMA + Naive-DPO + Vista-DPO + AVEm-DPO UAR WAR UAR WAR 31.62 43.65 22.11 38.17 46.88 39.31 28.05 46.94 29.24 45.59 30.45 42.72 - 44.48 53.01 56.78 52.94 55.67 53.64 56.42 55.48 58.54 48.12 54.89 49.01 54.97 50.96 56.28 51.03 57.06 48.66 41.73 42.56 54.34 59.37 54.06 56.60 60.14 59.90 62.33 64.24 58.26 58.12 61.58 62.12 41.81 27.45 50.67 32.88 28.20 30.36 - 53.59 53.63 56.94 58.66 52.59 52.69 56.42 56.21 F1 50.79 55.82 66.94 79.72 90.36 89.05 90.27 89.19 88.59 90.06 92.18 90.01 89.35 91.19 91.68 Clue Label 3.80 3.82 3.33 3.80 4.72 4.77 6.78 5.85 6.99 6.03 2.78 2.76 - - 6.45 5.63 6.30 5.81 6.89 6.08 7.08 6.37 6.21 5.78 6.35 5.89 6.56 6.05 6.99 6.02 Spurious Halluc. 4.25 3.93 5.16 6.39 5.89 3.44 - 5.41 5.96 6.58 7.09 5.36 5.89 6.85 7.02 4.23 4.22 5.70 6.21 5.26 2.36 - 5.19 5.48 6.07 6.75 5.23 5.62 6.31 6.62 Baseline Preference Optimization Approaches. We compare with original Naive-DPO (Rafailov et al., 2023) using single rejected samples from our DPO data and modified Vista-DPO (Huang et al., 2025b) for audiovisual inputs denoted as Vista-DPO (Section D.3 for details). 5.1 EMOTION REASONING AND RECOGNITION RESULTS EmoReAlM Results. Table 3 presents the performance of different approaches on the proposed EmoReAlM benchmark. AVEm-DPO achieves substantial gains over the reference models, demonstrating the effectiveness of multimodal preference optimization and text-prior debiasing. While the baselines perform strongly on basic reasoning tasks, Table 3 shows that they struggle on Modality Agreement and Stress-Test evaluations (Expanded table in Section E.1 and Table 13). Table 3: Performance comparison of different methods on the proposed EmoReAlM Benchmark. Model VideoLLaMA2 OLA VITA-1.5 Qwen 2.5 Omni Our base + Naive-DPO + Vista-DPO + AVEm-DPO Emot.-LLaMA + Naive-DPO + Vista-DPO + AVEm-DPO Reas. Basic Audio Visual Acc. Acc. 66.8 63.1 60.4 63.2 84.3 63.1 89.2 76.8 85.3 69.2 85.9 71.3 87.8 72.4 92.5 77.9 84.9 64.8 85.7 67.2 86.9 69.0 89.9 76.5 Modality Agree. F1 52.5 42.7 30.2 33.3 34.6 41.6 52.1 60.0 33.1 42.8 40.9 56.8 Reas. - Stress Audio Visual F1 53.2 56.6 52.8 55.0 50.3 54.8 73.6 80.9 46.7 52.6 68.6 75. F1 58.4 54.8 56.3 56.8 59.9 65.9 86.7 94.6 63.2 67.6 87.3 91.7 Notably, our preference optimization also surpasses Vista-DPO and Naive-DPO by significant margins. To further examine the bottlenecks in baseline models, Section E.1 reports results on samples probing spurious audiovisualemotion correlations and hallucinated cues. For state-of-the-art systems such as Qwen 2.5 Omni (Xu et al., 2025b) and VITA-1.5 Fu et al. (2025), hallucination emerges as more severe issue than spurious cue-emotion associations. Moreover, unlike findings from Sung-Bin et al. (2025), our results indicate that audio and visual hallucinations are equally prevalent in emotion reasoning tasks. Additionally. Table 13 shows the performance of video-only and audio-only baselines and reveals that multimodal inputs hurt reasoning capabilities. Emotion Recognition and Reasoning on Existing Benchmarks. Table 2 (expanded in Section E.3) shows the performance on existing emotion benchmarks mentioned before. We can notice that our reference models outperform baselines, showing the efficacy of reference in understanding emotion. Moreover, preference tuning additionally boosts the performance, especially for emotion reasoning on 8 Table 4: User evaluation on EMER. Model VideoLLaMA 2 OLA VITA 1.5 Qwen 2.5 Omni EmotionLLaMA Our + AVEm-DPO 54.74% 43.35% Emot. Assoc. Incons. 0.75% 9.82% 15.38% 9.36% 7.46% 5.58% 6.04% 11.60% 17.25% 10.75% 18.57% 10.13% 1.89% 11.53% 68.61% 4.67% Accepted as conference paper at ICLR 2026 Figure 5: Effect of AVEm-DPO on (Left two plots) the distribution of attention over video and audio tokens taken as percentage over the total attention over all multimodal tokens for audio and visual reasoning tasks in EmoReAlM; (Right two plots) the log-likelihood distribution shift of the correct answer for visual reasoning tasks on corrupting the audio input aori with adversary aadv. EMER, reducing spurious cue-emotion associations and hallucinations. It is important to note that previous emotion MLLM baselines (Cheng et al., 2024; Han et al., 2025b) use text subtitle as additional input. Qualitative comparison to baselines is present in Section F. While most baselines perform poorly on the out-of-domain RAVDESS dataset, our reference and preference-tuned models perform significantly better, showing their generalizability. User evaluation. We perform user evaluation with 40 participants on EMER generations from different models and report results in Table 4. Participants chose our model the most for emotion description and emotion-cue associations and the least for inconsistencies. (Details in Section E.4). 5.2 ANALYSIS Ablation Study. Table 5 shows the performance of the preference-tuned model after removing the proposed components of AVEmDPO. We perform this analysis on EmoReAlM and report the average metrics over audio and visual reasoning (Section D.5 for details). Removal of any of the key components results in significant performance drop, especially for the reasoning tasks. Moreover, ablating TPD results in huge performance drop on the hallucination stress test samples, underlining its efficacy in eliminating cue hallucinations in audiovisual emotion reasoning. Table 5: Ablation study over different components of the proposed AVEm-DPO approach. PMP: Prompt-based Modality Preference, ERP: Emotion-based Response Preference, TPD: Text Prior Debiasing. Method Our base + AVEm-DPO w/o PMP w/o ERP w/o TPD + Contr. Dec. Basic. Agree. 77.3 85.2 81.0 81.8 83.8 79.1 34.6 60.1 54.9 56.2 58.9 51. Stress 55.1 87.8 79.6 79.4 78.8 61.7 Spur. Hall. 39.2 47.3 97.6 92.7 88.1 86.2 88.4 84.9 77.8 87.1 54.8 50.9 Comparison with training-free contrastive decoding. Similar to VCD Leng et al. (2024), we perform contrastive decoding using diffused audiovisual inputs and report results in Table 5 (last row), showcasing it is significantly worse than AVEm-DPO. Design Choices and Sensitivity to Hyperparams. Section E.5 shows that prompt-based modality preference using different emotion audiovisual (AV) input as (al, vl) works better compared to using random videos or diffused versions of the inputs. Section E.6 shows that using emotionrelevant and video-relevant rejected responses (yer ) works better compared to only using one or using completely irrelevant response. Section E.7 detail the sensitivity of AVEm-DPO to various hyperparameters, highlighting the role of various components in eliminating spurious cue-emotion associations and hallucinations. , yvr Attention redistribution after AVEm-DPO. To analyze the effect of preference optimization on model attention, we plot the distribution of aggregate multimodal input attention over audio and visual tokens averaged over all attention heads for audio and visual reasoning tasks in EmoReAlM in Fig. 5 (left two plots). We can observe that the attention over relevant modality increases after AVEm-DPO, ensuring consistent model responses grounded on the relevant modality. More attention redistribution experiments are present in Section E.8. Robustness to adversarial inputs. As shown in Fig. 12 (Section E.9), the model response on prompt relevant to one modality should not change on changing the input of the irrelevant modality. 9 Accepted as conference paper at ICLR 2026 To test this robustness on visual reasoning tasks, we plot the distribution of log-likelihoods of correct responses for our base and AVEm-DPO models and show the distribution shift using Kernel Density Estimation (KDE) on changing the audio input in Fig. 5(right two plots). AVEm-DPO trained model results in negligible shifts, showing its robustness. Detailed analysis in Section E.9."
        },
        {
            "title": "5.3 VALIDITY OF GENERATED PREFERENCE DATA",
            "content": "Response type Table 6: Human verification statistics on generated preference data. As mentioned in Section 4.3, our preference dataset is automatically generated using Gemini 2.5 (Gemini-Team et al., 2025). Performing human verification on the entire training data is too costly. Therefore, to show the validity of the generated preference tuning data, we perform human verification on subset of 1000 random samples from the generated data with the help of 90 participants recruited through Prolific (Prolific). Each generated sample is verified by three or more annotators. As shown in Table 6, for the different categories of preference responses mentioned in Section 4.1 chosen (yw), video-relevant rejected (yvr ) we report the number of samples in which the majority of annotators found the generated responses correct. These results validate our automatically generated preference data. Chosen (yw) Rejected - Video Relevant (yvr ) Rejected - Emotion Relevant (yer ) ), and emotion-relevant rejected (yer # One or more correct # Majority correct 912 # Total verified 1000 1000 1000 895"
        },
        {
            "title": "6 LIMITATIONS AND FUTURE WORK",
            "content": "The proposed EmoReAlM benchmark is derived from the DFEW (Jiang et al., 2020) dataset, leveraging its emotion labels, and hence, it may inherit its cultural biases. Additionally, since our benchmark and training data are derived from existing emotion recognition datasets with short videos ( 2-10 seconds), long video emotion understanding and reasoning remain an open topic that can be addressed in future work. Although the proposed AVEm-DPO significantly improves the reference models performance, few limitations remain. Similar to other baselines, our model trained with AVEm-DPO performs poorly on the recognition for disgust (an ambiguous emotion (Hendel et al., 2023)) as shown in Section E.3 and Table 15. We attribute this to the limited amount of training samples available for this emotion class. Moreover, closer look at the performance on the subtasks of the Emotion Reasoning - Stress Test task of EmoReAlM (Section E.2 and Table 14) reveals that there is still room for improvement to mitigate spurious audio cue-emotion associations."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This work addresses the bottlenecks of emotion reasoning in MLLMs, with two major contributions EmoReAlM Benchmark for evaluating emotion reasoning over complex and diverse set of tasks and AVEm-DPO preference optimization technique to mitigate bottlenecks of MLLMs such as spurious audiovisual cue-emotion associations and audiovisual cue hallucinations. The proposed method outperforms open-source baselines on the proposed and existing emotion understanding benchmarks under zero-shot setting. Moreover, detailed ablation study with analysis of attention redistribution and log-likelihood shift upon preference tuning supports the efficacy of the proposed prompt-based modality preference and text-prior debiasing approaches. ETHICS STATEMENT This work builds upon publicly available audiovisual datasets for research purposes, specifically DFEW for benchmark creation (Section 3.2) and MAFW/MER2025 for preference optimization (Section 4.3). We did not collect new audiovisual data, ensuring no additional privacy risks. All data usage complies with the licensing terms of the original datasets. To mitigate potential harms, the released EmoReAlM benchmark will only contain automatically generated and human-verified questionanswer pairs; users must independently obtain the underlying videos from the original sources under appropriate licenses. For human verification (Section 3.3) and user studies  (Table 4)  , 10 Accepted as conference paper at ICLR 2026 participants were recruited via Prolific and compensated at fair rates commensurate with task requirements and participant location, aligning with ethical standards for crowd work. We ensured informed consent, anonymity and the right to withdraw at any point. The proposed methods aim to improve reliability in emotion reasoning by reducing hallucinations and spurious cue associations in multimodal large language models. However, emotion recognition and inference from audiovisual data can carry risks of misinterpretation, bias reinforcement, or misuse in surveillance and highstakes applications. Moreover, users of the proposed method are advised to read the limitations of the proposed approach mentioned in Section 6 to avoid potential safety concerns. We emphasize that our benchmark and models are intended strictly for academic research, with the goal of advancing robust, interpretable and socially responsible AI. We caution against deployment in sensitive realworld contexts (e.g., healthcare, hiring, law enforcement) without careful domain-specific validation and safeguards."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure reproducibility and transparency, we provide additional details about data creation and experiments in the Appendix. All the prompts used for data creation are present in Section B.1. Implementation details for the proposed method, along with hyperparameter settings, are provided in Sections C.3 and D.2, while the details about the baseline approaches are present in Sections D.3 and D.4. Details about human verification of the benchmark and user evaluation are present in Sections B.2 and E.4. Evaluation metrics are detailed in Section D.1. We also provide the detailed setup for our ablations in Section D.5. Our benchmark, code and model weights will be made publicly available upon acceptance to ensure reproducibility and ease of use for the proposed work. Code, models and benchmark will be released at avere-iclr.github.io. ACKNOWLEDGEMENTS Research was sponsored by the Army Research Office and was accomplished under Cooperative Agreement Number W911NF-25-2-0040. Work was also in part supported by the National Science Foundation under Grant IIS-2211550 and the National Institute of Mental Health of the National Institutes of Health under Award Number R61MH135407. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office, NSF, NIH, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Luke Balcombe and Diego De Leo. Human-computer interaction in digital mental health. Informatics, 9(1):14, February 2022. ISSN 2227-9709. doi: 10.3390/informatics9010014. URL http://dx.doi.org/10.3390/informatics9010014. Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. Biometrika, 39:324, 1952. URL https://api. the method of paired comparisons. semanticscholar.org/CorpusID:125209808. Rijul Chaturvedi, Sanjeev Verma, Ronnie Das, and Yogesh K. Dwivedi. Social companionship with artificial intelligence: Recent trends and future avenues. Technological Forecasting and Social Change, 193:122634, 2023. ISSN 0040-1625. doi: https://doi.org/10.1016/j.techfore. 2023.122634. URL https://www.sciencedirect.com/science/article/pii/ S0040162523003190. Ashutosh Chaubey, Xulang Guan, and Mohammad Soleymani. Face-llava: Facial expression and attribute understanding through instruction tuning, 2025. URL https://arxiv.org/abs/ 2504.07198. 11 Accepted as conference paper at ICLR 2026 Junzhe Chen, Tianshu Zhang, Shiyu Huang, Yuwei Niu, Chao Sun, Rongzhou Zhang, Guanyu Zhou, Lijie Wen, and Xuming Hu. Omnidpo: preference optimization framework to address omnimodal hallucination. arXiv preprint arXiv:2509.00723, 2025. Yin Chen, Jia Li, Shiguang Shan, Meng Wang, and Richang Hong. From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos. IEEE Transactions on Affective Computing, pp. 115, 2024. doi: 10.1109/TAFFC.2024.3453443. Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Kai Wang, Yuxiang Lin, Zheng Lian, XiEmotion-llama: Multimodal emotion recogaojiang Peng, and Alexander Hauptmann. nition and reasoning with instruction tuning. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 110805110853. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2024/ 2024. file/c7f43ada17acc234f568dc66da527418-Paper-Conference.pdf. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025. Paul Ekman. Basic Emotions. In Handbook of Cognition and Emotion, pp. 4560. John Wiley & Sons, Ltd, 2005. doi: 10.1002/0470013494.ch3. Paul Ekman and Wallace V. Friesen. Facial Action Coding System: Technique for the Measurement of Facial Movement. Consulting Psychologists Press, Palo Alto, CA, 1st edition, 1978. Zohar Elyoseph, Efrat Refoua, Keren Asraf, Michael Lvovsky, Yair Shimoni, and Dalit HadarShoval. Capacity of generative ai to interpret human emotions from visual and textual data: Pilot evaluation study. JMIR Mental Health, 11:e54369, Feb 2024. doi: 10.2196/54369. Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction. arXiv preprint arXiv:2501.01957, 2025. Gemini-Team et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/ 2507.06261. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all, 2023. URL https: //arxiv.org/abs/2305.05665. Arushi Goel, Sreyan Ghosh, Jaehyeon Kim, Sonal Kumar, Zhifeng Kong, Sang-gil Lee, ChaoHan Huck Yang, Ramani Duraiswami, Dinesh Manocha, Rafael Valle, et al. Audio flamingo 3: Advancing audio intelligence with fully open large audio language models. arXiv preprint arXiv:2507.08128, 2025. Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language, 2025a. URL https://arxiv.org/abs/2312.03700. Zhiyuan Han, Beier Zhu, Yanlong Xu, Peipei Song, and Xun Yang. Benchmarking and bridging emotion conflicts for multimodal emotion reasoning. arXiv preprint arXiv:2508.01181, 2025b. Emalie Hendel, Ad`ele Gallant, Marie-Pier Mazerolle, Sabah-Izayah Cyr, and Annie Roy-Charland. Exploration of visual factors in the disgust-anger confusion: the importance of the mouth. Cogn. Emot., 37(4):835851, May 2023. 12 Accepted as conference paper at ICLR 2026 Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Dawei Huang, Qing Li, Chuan Yan, Zebang Cheng, Zihao Han, Yurong Huang, Xiang Li, Bin Li, Xiaohui Wang, Zheng Lian, Zhi-Qi Cheng, and Xiaojiang Peng. Emotion-qwen: unified framework for emotion and vision understanding, 2025a. URL https://arxiv.org/abs/ 2505.06685. Haojian Huang, Haodong Chen, Shengqiong Wu, Meng Luo, Jinlan Fu, Xinya Du, Hanwang Zhang, and Hao Fei. Vista dpo: Video hierarchical spatial-temporal direct preference optimization for large video models. In Forty-second International Conference on Machine Learning, 2025b. URL https://openreview.net/forum?id=O2jukIZR50. Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia, Cheng Lu, and Jiateng Liu. Dfew: large-scale database for recognizing dynamic facial expressions in the wild. In Proceedings of the 28th ACM International Conference on Multimedia, pp. 28812889, 2020. Zeyu Jin, Jia Jia, Qixin Wang, Kehan Li, Shuoyi Zhou, Songtao Zhou, Xiaoyu Qin, and Zhiyong Wu. Speechcraft: fine-grained expressive speech dataset with natural language description. In ACM Multimedia 2024, 2024. URL https://openreview.net/forum?id=rjAY1DGUWC. Michal Kolomaznik, Vladimir Petrik, Michal Slama, and Vojtech Jurik. The role of socio-emotional attributes in enhancing human-ai collaboration. Frontiers in Psychology, 15, October 2024. ISSN 1664-1078. doi: 10.3389/fpsyg.2024.1369957. URL http://dx.doi.org/10.3389/ fpsyg.2024.1369957. Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1387213882, 2024. doi: 10.1109/CVPR52733.2024.01316. Sicong Leng, Yun Xing, Zesen Cheng, Yang Zhou, Hang Zhang, Xin Li, Deli Zhao, Shijian Lu, Chunyan Miao, and Lidong Bing. The curse of multi-modalities: Evaluating hallucinations of large multimodal models across language, visual, and audio, 2025. URL https: //openreview.net/forum?id=VeSsiD0DP9. Yadong Li and team. Baichuan-omni-1.5 technical report, 2025. URL https://arxiv.org/ abs/2501.15368. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id= xozJw0kZXF. Zheng Lian. Affectgpt-r1: Leveraging reinforcement learning for open-vocabulary emotion recognition, 2025. URL https://arxiv.org/abs/2508.01318. Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying Li, Jinming Zhao, et al. Mer 2023: Multi-label learning, modality robustness, and semi-supervised In Proceedings of the 31st ACM international conference on multimedia, pp. 9610 learning. 9614, 2023a. Zheng Lian, Haiyang Sun, Licai Sun, Hao Gu, Zhuofan Wen, Siyuan Zhang, Shun Chen, Mingyu Xu, Ke Xu, Kang Chen, et al. Explainable multimodal emotion recognition. arXiv preprint arXiv:2306.15401, 2023b. Zheng Lian, Haiyang Sun, Licai Sun, Lan Chen, Haoyu Chen, Hao Gu, Zhuofan Wen, Shun Chen, Siyuan Zhang, Hailiang Yao, et al. Open-vocabulary multimodal emotion recognition: Dataset, metric, and benchmark. ICML, 2024. 13 Accepted as conference paper at ICLR 2026 Zheng Lian, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang Cheng, Bin Liu, Rui Liu, Xiaojiang Peng, et al. Affectgpt: new dataset, model, and benchmark for emotion understanding with multimodal large language models. ICML, 2025a. Zheng Lian, Rui Liu, Kele Xu, Bin Liu, Xuefei Liu, Yazhou Zhang, Xin Liu, Yong Li, Zebang Cheng, Haolin Zuo, et al. Mer 2025: When affective computing meets large language models. arXiv preprint arXiv:2504.19423, 2025b. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-LLaVA: In Yaser Al-Onaizan, Learning united visual representation by alignment before projection. Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 59715984, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.342. URL https://aclanthology.org/2024.emnlp-main.342/. Maija Litendahl, Juulia Kaihlaniemi, Olli Autio, Outi Kahkonen, and Anne Oikarinen. Healthcare professionals perceptions of emotional intelligence in remote counsellinga descriptive qualitative study. Nursing Open, 12(4), April 2025. ISSN 2054-1058. doi: 10.1002/nop2.70218. URL http://dx.doi.org/10.1002/nop2.70218. Aiwei Liu, Haoping Bai, Zhiyun Lu, Yanchao Sun, Xiang Kong, Xiaoming Simon Wang, Jiulong Shan, Albin Madappally Jose, Xiaojiang Liu, Lijie Wen, Philip S. Yu, and Meng Cao. TIS-DPO: Token-level importance sampling for direct preference optimization with estimated weights. In The Thirteenth International Conference on Learning Representations, 2025a. URL https: //openreview.net/forum?id=oF6e2WwxX0. Yuanyuan Liu, Wei Dai, Chuanxu Feng, Wenbin Wang, Guanghao Yin, Jiabei Zeng, and Shiguang Shan. MAFW: Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild. ACM, New York, NY, USA, 2022. ISBN 978-1-4503-9203-7. URL https://doi.org/10.1145/3503161.3548190. Ziyu Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Conghui He, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. MIA-DPO: Multi-image augmented direct preference optimization for large vision-language models. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id= f7WBRSuf9l. Steven R. Livingstone and Frank A. Russo. The ryerson audio-visual database of emotional speech and song (ravdess): dynamic, multimodal set of facial and vocal expressions in north american english. PLOS ONE, 13(5):e0196391, May 2018. ISSN 1932-6203. doi: 10.1371/journal.pone. 0196391. URL http://dx.doi.org/10.1371/journal.pone.0196391. Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, et al. Openomni: Advancing open-source omnimodal large language models with progressive multimodal alignment and real-time self-aware emotional speech synthesis. arXiv preprint arXiv:2501.04561, 2025. OpenAI et al. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus In 2015 IEEE International Conference on Acoustics, based on public domain audio books. Speech and Signal Processing (ICASSP), pp. 52065210, 2015. doi: 10.1109/ICASSP.2015. 7178964. Prolific. Prolific Easily collect high-quality data from real people prolific.com. https: //www.prolific.com/. [Accessed 23-09-2025]. Qualtrics. Qualtrics XM - Experience Management Software qualtrics.com. https://www. qualtrics.com/. [Accessed 23-09-2025]. Qwen-Team et al. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412. 15115. 14 Accepted as conference paper at ICLR Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=HPuSIXJaa9. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv. org/abs/1908.10084. Pranab Sahoo, Prabhash Meharia, Akash Ghosh, Sriparna Saha, Vinija Jain, and Aman Chadha. comprehensive survey of hallucination in large language, image, video and audio foundation models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1170911724, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.685. URL https://aclanthology.org/2024.findings-emnlp.685/. Said A. Salloum, Khaled Mohammad Alomari, Aseel M. Alfaisal, Rose A. Aljanada, and Azza Basiouni. Emotion recognition for enhanced learning: using ai to detect students emotions and adjust teaching methods. Smart Learning Environments, 12(1), February 2025. ISSN 2196-7091. doi: 10.1186/s40561-025-00374-5. URL http://dx.doi.org/10.1186/ s40561-025-00374-5. Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan Arik, and Tomas Pfister. Mitigating object hallucination in MLLMs via data-augmented phrase-level alignment. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=yG1fW8igzP. Klaus Scherer. What are emotions? And how can they be measured? Social Science Information, 44(4):695729, 2005. doi: 10.1177/0539018405058216. Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all, 2023. URL https://arxiv.org/abs/2305.16355. Guangzhi Sun, Yudong Yang, Jimin Zhuang, Changli Tang, Yixuan Li, Wei Li, Zejun MA, and Chao Zhang. video-SALMONN-o1: Reasoning-enhanced audio-visual large language model. In Fortysecond International Conference on Machine Learning, 2025. URL https://openreview. net/forum?id=y62fhuA69I. Licai Sun, Zheng Lian, Bin Liu, and Jianhua Tao. Mae-dfer: Efficient masked autoencoder for selfsupervised dynamic facial expression recognition. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 61106121, 2023. Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, Arda Senocak, Joon Son Chung, and Tae-Hyun Oh. AVHBench: cross-modal hallucination benchmark for audio-visual large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=jTEKTdI3K9. Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zejun Ma, and Chao Zhang. video-salmonn 2: Captioning-enhanced audio-visual large language models. arXiv preprint arXiv:2506.15220, 2025. Fei Wang, Wenxuan Zhou, James Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. mdpo: Conditional preference optimization for multimodal large language models. 2024. Hanyang Wang, Bo Li, Shuang Wu, Siyuan Shen, Feng Liu, Shouhong Ding, and Aimin Zhou. Rethinking the learning paradigm for dynamic facial expression recognition. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1795817968, 2023. doi: 10.1109/CVPR52729.2023.01722. 15 Accepted as conference paper at ICLR Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Zhi Hou, Haoran Hao, Tianyi Zhang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Songyang Zhang, Maosong Cao, Junyao Lin, Kexian Tang, Jianfei Gao, Haian Huang, Yuzhe Gu, Chengqi Lyu, Huanze Tang, Rui Wang, Haijun Lv, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Weijie Su, Bowen Zhou, Kai Chen, Yu Qiao, Wenhai Wang, and Gen Luo. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency, 2025. URL https://arxiv.org/abs/2508.18265. Zhuofan Wen, Zheng Lian, Shun Chen, Hailiang Yao, Longjiang Yang, Bin Liu, and Jianhua Tao. Listen, watch, and learn to feel: Retrieval-augmented emotion reasoning for compound emotion generation. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 11313 11327, 2025. Hongxia Xie, Chu-Jun Peng, Yu-Wen Tseng, Hung-Jen Chen, Chan-Feng Hsu, Hong-Han Shuai, and Wen-Huang Cheng. Emovit: Revolutionizing emotion insights with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Bohao Xing, Xin Liu, Guoying Zhao, Chengyu Liu, Xiaolan Fu, and Heikki Kalviainen. Emotionhallucer: Evaluating emotion hallucinations in multimodal large language models, 2025. URL https://arxiv.org/abs/2505.11405. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report, 2025a. URL https://arxiv.org/abs/2503.20215. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report, 2025b. URL https://arxiv.org/abs/2503.20215. Qize Yang, Detao Bai, Yi-Xing Peng, and Xihan Wei. Omni-emotion: Extending video mllm with detailed face and audio modeling for multimodal emotion analysis, 2025. URL https: //arxiv.org/abs/2501.09502. Qilang Ye, Zitong Yu, Rui Shao, Yawen Cui, Xiangui Kang, Xin Liu, Philip Torr, and Xiaochun Cao. Cat+: Investigating and enhancing audio-visual understanding in large language models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47(10):86748690, 2025. doi: 10.1109/TPAMI.2025.3582389. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1380713816, 2024. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, and Deli Zhao. Videollama 3: Frontier multimodal foundation models for image and video understanding, 2025a. URL https://arxiv.org/abs/2501.13106. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. URL https: //arxiv.org/abs/2306.02858. Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, and Yiming Yang. Direct preference optimizaIn Luis Chiruzzo, Alan tion of video large multimodal models from language model reward. 16 Accepted as conference paper at ICLR 2026 Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 694717, Albuquerque, New Mexico, April 2025b. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.30. URL https://aclanthology.org/2025.naacl-long.30/. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. URL https://arxiv.org/abs/2410.02713. Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, WANG HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=QmZKc7UZCy. 17 Accepted as conference paper at ICLR"
        },
        {
            "title": "TABLE OF CONTENTS",
            "content": "LLM USAGE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . BENCHMARK DETAILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . PROMPTS USED IN BENCHMARK CREATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 HUMAN VERIFICATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 BENCHMARK STATISTICS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 FRAME SAMPLING RATE FOR AUTOMATIC VISUAL CAPTIONING . . . . . . . . . . . . . . . . . B.4 BENCHMARK SAMPLES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 METHODOLOGICAL DETAILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . TEXT-PRIOR DEBIASING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 PREFERENCE DATA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 IMPLEMENTATION DETAILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 EXPERIMENTAL DETAILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . EVALUATION METRICS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1 REFERENCE MODELS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 BASELINE PREFERENCE OPTIMIZATION TECHNIQUES . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 BASELINE IMPLEMENTATIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 EXPERIMENTAL SETUP FOR ABLATION STUDY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 DETAILED RESULTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . EMOREALM RESULTS - EXPANDED . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1 EMOREALM RESULTS ON DIFFERENT STRESS TEST SUBTASKS . . . . . . . . . . . . . . . . . . E.1 EMOTION RECOGNITION RESULTS - EXPANDED . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 USER EVALUATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 MODALITY PREFERENCE ABLATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 RESPONSE PREFERENCE ABLATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 SENSITIVITY TO HYPERPARAMETERS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.7 ATTENTION REDISTRIBUTION AFTER PREFERENCE OPTIMIZATION . . . . . . . . . . . . . . . . E.8 REASONING WITH ADVERSARIAL MODALITY INPUTS . . . . . . . . . . . . . . . . . . . . . . . . . . . E.9 EFFECT OF INDIVIDUAL MODALITIES FOR EMOTION PREDICTION . . . . . . . . . . . . . . . E.10 QUALITATIVE SAMPLES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . PROMPT POOL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G"
        },
        {
            "title": "A LLM USAGE",
            "content": "We used GPT-5 to polish the text we added to the paper for grammar and consistency checks. We verify the grammar changes suggested by GPT to ensure its validity. No significant part of the text in the paper is written by any LLM. Apart from polishing the paper, we use LLMs for data annotation and automatic evaluation as mentioned in Sections 3.2, C.2 and D.1 to 4.3."
        },
        {
            "title": "B BENCHMARK DETAILS",
            "content": "B.1 PROMPTS USED IN BENCHMARK CREATION In this section, we detail the prompts that are used in various parts of the benchmark creation pipeline mentioned in Section 3 and Fig. 3. Note that the text prompts themselves are present at the end of the document in Section G. 18 Accepted as conference paper at ICLR 2026 Table 7: Statistics of human verification on EmoReAlM Benchmark. Task Reasoning - Basic Modality Agreement Reasoning - Stress Test Total Audio Visual Audio Visual # Ques. verified 1200 1200 1000 1000 1000 5400 # Ques. at least one correct 1168 1137 489 956 845 4595 # Ques. majority correct 968 1014 458 806 719 3959 # Ques discre. # Ques. Final 8 10 0 14 9 41 972 1024 456 820 728 4000 Audio and Video Captioning. Figs. 19 and 20 contains the prompts used to caption the audio and visual content separately for given video as described in Section 3.2 and Fig. 3. For visual captioning, we sample eight uniform frames from the video and pass those to GPT-4o. For audio captioning, we only pass the audio as WAV file to GPT-4o-audio. Emotion prediction from audio and video captions separately. Figs. 21 and 22 contain prompts used to predict the emotion (out of the seven basic categories) just using the audio and video captions separately. If the ground truth emotion label cannot be predicted by both the audio and video captions, then we do not proceed with such video for the subsequent data pipeline. EmoReAlM QA Generation. Figs. 23 and 24 contains the prompts to generate questions related to Emotion Reasoning - Basic as described in Section 3.1 for audio and visual reasoning respectively. We use the ground truth emotion label already present in the source emotion recognition dataset, as well as the audio/video captions, to generate the question answers. Note that audio and visual reasoning samples are only generated for those samples in which emotion was predicted correctly from the audio and visual captions, respectively (using prompts in Figs. 21 and 22). We use prompt in Fig. 25 to generate questions related to Modality Agreement (Section 3.1) by passing the audio captions, video captions and the ground truth emotion label present in the source dataset. We also verify the answers to the generated questions using the ground truth emotion label present for the video and the emotions predicted using only audio and video captions. If both the audio and the video caption predict the ground truth emotion label from the captions (using prompts in Figs. 21 and 22), then the correct answer should be Yes, else it should be No. For the Emotion Reasoning - Stress Test (Section 3.1), we generate questions using prompts present in Figs. 26 to 31. We use separate prompts for generating questions related for the different subtasks No hallucination (Figs. 26 and 29), Spurious Cue-Emotion Association (Figs. 27 and 30) and Emotion-relevant Hallucination (Figs. 28 and 31). Note that the No hallucination prompts only apply to cases where the emotion prediction from the audio and/or visual captions using Figs. 21 and 22 is same as the ground truth emotion label. Text Only Guess - Post Processing. We use the prompt in Fig. 32 to guess the correct answer for the generated question and answer choices using only the text (i.e., without audiovisual input). This is done as post-processing step as described in Section 3.3 to ensure that the answer for the MCQA sample is not predictable using only the text inputs. B.2 HUMAN VERIFICATION As mentioned in Section 3.3, we perform human verification for the generated QA samples to ensure high data quality by removing samples that contain some discrepancy. We conducted survey using Qualtrics and recruited participants using the crowd-sourcing platform Prolific. In total, we conducted the survey on 471 participants and ensured that the participants were paid fairly for their time. To ensure participants are capable of answering the questions, we included pre-survey to test their emotional intelligence. Moreover, we included attention checks using questions that are already verified by us to ensure the quality of the participant responses. We conduct the survey as MCQ task where the participants are shown the questions and the answer choices created in the benchmark and we ask them to choose the correct answer as shown in Fig. 6. Each participant was also shown follow-up question after each question to flag the text present in Accepted as conference paper at ICLR 2026 Figure 6: Human verification survey questions. (Left) An example question from the benchmark shown to the participant. (Right) Follow-up questions shown to the participant about each question. Figure 7: (Left) Distribution of QA samples across different tasks in EmoReAlM benchmark. (Right) Distribution of ground truth emotion labels for the videos present in EmoReAlM compared with the distribution in the source dataset DFEW (Jiang et al., 2020). the question or answer choice or to report any other discrepancy. Since some videos in the DFEW (Jiang et al., 2020) dataset are not in English, the participants were also shown the English subtitle for the video that the MCQ is about. Table 7 contains the statistics of human verification. Due to budget constraints, we ran the survey only on 5400 questions across different tasks. We only use the samples from the benchmark for which the majority of the participants selected the correct answer, automatically annotated in the benchmark. Additionally, we manually correct some samples that had discrepancies and add them to the final set of questions as well. B.3 BENCHMARK STATISTICS Fig. 7 (Right) shows the distribution of ground truth emotion labels in the EmoReAlM benchmark compared to that present in the source dataset - DFEW (Jiang et al., 2020). We can see that the distribution of samples over different emotions is similar to DFEW. Fig. 9 shows the distribution of subtasks within the Emotion Reasoning - Stress Test task (Section 3.1) of EmoReAlM bench20 Accepted as conference paper at ICLR 2026 Figure 8: Distribution of different languages present in the audiovisual samples present in EmoReAlM benchmark. Figure 9: Distribution of subtasks in the Emotion Reasoning - Stress Test of EmoReAlM benchmark. mark. Due to the way we formulate the questions for this subtask Does the {audio/visual cue} suggest {emotion} of the character?, the samples belonging to No hallucination subtask have the answer Yes, and the samples in the Spurious Association and Audio/Visual Hallucination subtasks have answer No. Fig. 9 shows that the number of samples with Yes/No answers are equally distributed. Moreover, for all the samples with answers as No, the samples are almost equally distributed to test spurious cue-emotion associations and audiovisual cue hallucinations. Furthermore, to show the cultural and linguistic diversity in the benchmark, Fig. 8 shows the distribution of languages present in the samples of EmoReAlM benchmark. We obtain this by using automatic language detection using Whisper (Radford et al., 2023). We can observe that although the majority language is English, our benchmark contains samples from wide range of languages. Table 8: Effect of using different number of frames for visual captioning using GPT-4o. # frames SBERT-sim 1 2 4 8 16 0.646 0.660 0.676 0.689 0.688 BERT Score Prec. Rec. 0.853 0.851 0.856 0.851 0.857 0.851 0.861 0.858 0.862 0.858 F1 0.852 0.853 0.854 0.860 0.860 21 Accepted as conference paper at ICLR 2026 Table 9: Samples from the EmoReAlM Benchmark for the Emotion Reasoning-Basic Task. Task Video Question Answer Reasoning Basic (Audio) Reasoning Basic (Audio) Reasoning Basic (Visual) Subtitle: tried Subtitle: You havent spoken to me in 10 years Subtitle: Stanford University? What are you guys talking about? Reasoning Basic (Visual) Subtitle: How does the speakers choice of words in the video reflect their emotional state? (A) The speaker mentions struggling to move forward despite past setbacks, indicating reflective state. (B) The speakers tone reflects somber atmosphere, accompanied by soft, resigned voice. (C) The speakers phrase portrays deep sense of regret and resignation, reflecting failed attempt. (D) The speaker uses soft background music to enhance the somber mood, suggesting unfulfilled efforts. In what way does the tone of the mans voice impact his emotional expression in the video? (A) The presence of soft whispers and gentle music in the background could imply an underlying tension and hidden emotion. (B) The mans tone is marked by tightness and sharpness, resonating with his underlying frustration and simmering anger. (C) The phrase cant believe youve done this again reflects an underlying resentment connected to long-standing grievance. (D) The mans voice holds lively and enthusiastic tone, mistakenly suggesting sense of joy and contentment. How does the womans facial expression contribute to the overall feeling in the scene? (A) The woman displays joyful expression with open arms, conveying her happiness and openness. (B) The womans cheerful smile and lively eyes reveal her happiness and engagement. (C) The womans yellow turtleneck adds vibrant touch, symbolizing her happiness and contentment. (D) The womans long dark hair frames her face, enhancing the appearance of happiness and delight. What does the individuals body language indicate about their emotional state in the video? (A) The individuals quivering movements and uncertain footing create palpable sense of fear. (B) The persons tense facial expression with slightly open mouth and wide eyes enhances their fearful demeanor. (C) The person is leaning cautiously towards the door, their body tense, which highlights their fear or anxiety. (D) The individuals dark-colored shirt amplifies their sense of fear, overshadowing their surroundings. B B.4 FRAME SAMPLING RATE FOR AUTOMATIC VISUAL CAPTIONING Since the visual cues used to express and infer emotions can be subtle, it is important to ensure that the visual captions obtained using GPT-4o in the first stage of data creation (Section 3.2 and Fig. 3) are of high quality. To identify the ideal number of frames to be sampled from the video for captioning, we ran small experiment on the emotion captioning dataset EMER (Lian et al., 2023b). It is important to note that EMER (mean duration: 3.78s) contains videos of similar duration as DFEW (mean duration: 3.42s) which we use to construct EmoReAlM. We extract different number of frames per video and obtain the visual caption from GPT-4o using prompt in Fig. 20. Then, compute the similarity between the generated captions and the ground truth using BERTScore (Zhang et al.) and Sentence BERT (Reimers & Gurevych, 2019) similarity score. Table 8 shows that using 8 frames for visual captioning leads to good captioning results. Furthermore, using 16 frames is not significantly better than using 8 frames, but it increases the costs significantly. Hence we choose to use 8 frames uniformly sampled from the video to extract visual captions from GPT-4o automatically. B.5 BENCHMARK SAMPLES We present samples belonging to different categories of the benchmark in Tables 9 to 11. Note that the subtitles shown in the tables are just for reference and we do not pass the subtitle as an input to the model during evaluation. 22 Accepted as conference paper at ICLR 2026 Table 10: Samples from the EmoReAlM Benchmark for the Modality Agreement Task. Task Video Question Answer Subtitle: was... Modality Agreement Modality Agreement Subtitle: That is exactly what am Do the visual elements of the video align with the audio in conveying the feeling of happiness of the person in the video? (A) Yes (B) No Do the audio and video modalities align for the expression of anger of the person in the video? (A) Yes (B) No Table 11: Samples from the EmoReAlM Benchmark for the Emotion Reasoning-Stress Test. Task Video Question Answer Stress Test (Audio No Hallucination) Stress Test (Audio - Spurious Association) Stress Test (Audio - Hallucination) Stress Test (Visual No Hallucination) Stress Test (Visual - Spurious Association) Stress Test (Visual - Hallucination) Subtitle: (chuckles) Subtitle: (sonar ping) Subtitle: It aint Alans fault... Do the chuckling sounds in the audio enhance the feeling of joy conveyed for the person in the video? (A) Yes (B) No Is the presence of sonar ping sound effect crucial to the feeling of surprise conveyed by the person in the video? (A) Yes (B) No Does the sound of slamming door contribute to the anger experienced by the person in the video? (A) Yes (B) No Is the downward gaze of the older woman significant factor in expressing the sadness of the older woman portrayed in the video? (A) Yes (B) No Subtitle: Subtitle: Subtitle: Is the presence of the vibrant checkered pattern on the walls factor in conveying the neutral emotion of the person/character in the video? (A) Yes (B) No Is the man displaying clenched fist as sign of his anger in this video? (A) Yes (B) No B 23 Accepted as conference paper at ICLR Table 12: Examples of the preference dataset used for AVEm-DPO. Video Prompt (x) Chosen Response (yw) Rejected Response (video-relevant - yvr ) Rejected Response (emotion-relevant - yer ) How do the facial expressions of the young person contribute to the emotional intensity during the exchange? The young persons furrowed eyebrows and open mouth emphasize their intense emotional state and frustration. The dark top worn by the young person underlines the seriousness of their mood. The young persons hands clenching into fists and subtle scowling underline their frustration. How does the womans message in the video reflect her emotional state? She communicates deep sense of exhaustion and emotional weariness through her words, saying Im so tired, which indicates her sadness. The melancholic piano music in the background underscores the emotional heaviness she is experiencing. Her loud expressive crying, typically associated with sadness, conveys the depth of her emotional state. Do the audio and video convey the same emotional state for the woman in the video? Yes, both the audio and video convey profound sense of sadness through the sounds of crying and the womans distraught facial expression. No, the tone of voice in the audio appears sad, but the stark background in the video suggests more calm atmosphere. No, the womans facial expression indicates sense of fear, while her words can not take it anymore suggest sadness. Subtitle: Youre bully. But can never fight back, because you are JJ! Subtitle: Im so tired. Subtitle: (crying)"
        },
        {
            "title": "C METHODOLOGICAL DETAILS",
            "content": "C.1 TEXT-PRIOR DEBIASING Similar to Eq. (6), we scale the TPD term to accommodate multiple rejected responses as follows, Ly DPO-TPD = (a,v,x,yw ,yl)Dpref (cid:34) (cid:32) (cid:32) log σ β log πθ(yw (a, v, x)) πref(yw (a, v, x)) (cid:88) βi log i{vr,er} (cid:33) πθ(yi πref(yl (a, v, x)) (a, v, x)) (cid:16) γTPD log πtext(yw x) (cid:88) βi log πtext(yi x) (cid:33)(cid:35) (cid:17) i{vr,er} (10) where βvr + βer = 1. Also, for succinctness, we denote (aw, vw) with (a, v) in the above equation. C.2 PREFERENCE DATA As mentioned in Section 4.3, we use pipeline similar to Fig. 3 to construct our preference data using MAFW (Liu et al., 2022) and MER2025 (Lian et al., 2025b) Track 1 train set as the source datasets. Note that we use Gemini 2.5 Flash (Gemini-Team et al., 2025) for all automatic annotations required to create the training dataset. Use of Gemini for training data creation reduces annotation budget and ensures that the training dataset is not biased to have similar language as the test dataset EmoReAlM. Since the pipeline in Fig. 3 creates MCQA samples, we use another round of automatic annotations through Gemini-2.5 Flash over the generated MCQA samples to create the preference data. Specifically, we use prompts in Figs. 33 to 35 to generate rejected responses for the generated emotion reasoning QA samples. Since, we also want to improve the performance on emotion description tasks present in EMER (Lian et al., 2023b) we use prompts for audio  (Fig. 33)  and visual reasoning  (Fig. 34)  to modify emotion descriptions generated from Gemini 2.5 Flash (using prompt in Fig. 36), combining audio and visual captions of MAFW and MER2025 (obtained using prompts in Figs. 19 and 20). After Gemini annotation, we end up with total of 41687 preference samples combining tasks, which we use for AVEm-DPO training. Table 12 contains samples from the constructed preference dataset using the described pipeline. C.3 IMPLEMENTATION DETAILS We train the reference models using AVEm-DPO for one epoch, with learning rate of 5e7 and per GPU batch size of 2 on an NVIDIA DGX node with 8 NVIDIA H100 GPUs. We choose β as 0.1 similar to (Huang et al., 2025b). Moreover, λav is set to 1.0, βer and βvr are both set to 0.5, and γTPD is set to 0.2 (refer to Section E.7 for details on choice). We attach LoRA module with 24 Accepted as conference paper at ICLR 2026 rank 8 and scale 4 to the LLM backbone for training. Gradient accumulation is used to accumulate gradients over 4 iterations."
        },
        {
            "title": "D EXPERIMENTAL DETAILS",
            "content": "D.1 EVALUATION METRICS GPT Evaluation on EMER. As mentioned in Section 5, we perform GPT-4o evaluation on the generated emotion descriptions in EMER (Lian et al., 2023b) dataset. We perform the evaluation over the following criterias (i) clue overlap - similarity of the audiovisual cues present in the generation with the ground truth, (ii) label overlap - similarity of the emotion label described in the generation with the ground truth, (iii) spurious cue-emotion associations - how good are the audiovisual cues associated with emotions in the generation, and (iv) hallucinatory cues - presence of cues that are absent in the ground truth but present in the generations. The prompt used to evaluate the generations is present in Fig. 37. EmoReAlM Evaluation Metrics. For all the tasks in EmoReAlM, we report the average accuracy over the task, computed as the number of correct responses out of the total number of samples in the task. Additionally, for tasks with Yes/No responses (Modality Agreement and Emotion Reasoning - Stress Test), we report the precision, recall and F1 score. Precision and recall are the ratios of correctly answered questions that have correct answers as Yes and No, respectively. F1 score is the harmonic mean of precision and recall. D.2 REFERENCE MODELS We describe the reference models mentioned in Section 5 below. Our base. We modify EmotionLLaMA (Cheng et al., 2024) to replace the visual encoder with LanguageBind Video Encoder (Zhu et al., 2024) and audio encoder with Whisper Large v3 (Radford et al., 2023). We pretrain the visual projector using the pretraining data of VideoLLaVA (Lin et al., 2024) and the audio projector is pretrained using LibriSpeech (Panayotov et al., 2015) and SpeechCraft (Jin et al., 2024) to enhance paralinguistic capabilities of the model. We finetune on the EmotionLLaMA dataset, however, we include additional instruction data by annotating MAFW (Liu et al., 2022) and MER2025 (Lian et al., 2025b) Track 1 train set through Gemini 2.5 Flash. Specifically, we use the prompts mentioned in Section B.1 to create finetuning dataset with similar tasks as in the proposed EmoReAlM benchmark. We also use prompt in Fig. 36 to generate emotion descriptions from MAFW and MER2025. EmotionLLaMA. Since the pretrained EmotionLLaMA model is not trained on tasks similar to EmoReAlM, we finetune EmotionLLaMA on additional datasets created using MAFW and MER2025, similar to our base model described in the previous paragraph. Moreover, we do not provide subtitle text as input to the model during finetuning, in contrast to the original EmotionLLaMA, to eliminate external subtitle dependence. D.3 BASELINE PREFERENCE OPTIMIZATION APPROACHES We describe the implementation of baseline DPO approaches mentioned in Section 5 below. We use the same training setup as mentioned in Section C.3 unless stated otherwise. Naive-DPO. For Naive-DPO (Rafailov et al., 2023) we use the objective in Eq. (3). We use the preference samples from our preference data (Section C.2), and pick the rejected response randomly between yvr . and yer l Vista-DPO. We adapt Vista-DPO (Huang et al., 2025b) for audiovisual inputs using Eqs. (4) and (6). Also, we use our preference data (Section C.2) to optimize Eq. (4) and drop their temporal (clip-based) and object-based preferences. Instead of prompt-based modality preference, we use (al, vl) to be an audiovisual input that has different emotion than that of (aw, vw), always irrespective of the input prompt. 25 Accepted as conference paper at ICLR 2026 D.4 BASELINE IMPLEMENTATIONS Audiovisual baselines. We use the official code for Qwen 2.5 Omni - 7B (Xu et al., 2025a) and run inference using flash attention 2. We use their default system prompt during inference. For Video-LLaMA (Zhang et al., 2023), we use the official video-language checkpoint finetunevicunna7b-v2 and audio-language checkpoint finetune-vicuna7b-audiobranch. We also use the default conversation template for inference. For PandaGPT (Su et al., 2023), we use their official pretrained checkpoint pandagpt-7b with 1,024 max len, built upon ImageBind (Girdhar et al., 2023). The system prompt remains unchanged during inference. For OneLLM (Han et al., 2025a), we use the released pretrained checkpoint OneLLM-7B; for inference, we manually prepend the multimodal representations before the textual prompt. We use VITA-1.5 (Fu et al., 2025) with its official code and checkpoint, including the InternViT300M vision tower and the pretrained audio encoder. We use the default conversation template for inference. Audio-only baselines. We use the official Qwen2-Audio-7B-Instruct (Chu et al., 2024) checkpoint and its default conversation template with the original system prompt. For Kimi-Audio (Ding et al., 2025), we use the released Kimi-Audio-7B-Instruct checkpoint with the default system message. For Audio Flamingo 3 (Goel et al., 2025), we use the official repository, pretrained checkpoint, and the default empty conversation template. Video-only baselines. We use the official code for InternVL3.5 (Wang et al., 2025). Unlike others, this is an 8B model. For Qwen2.5-VL (Bai et al., 2025), we use the released Qwen2.5-VL-7B-Instruct checkpoint with the default system prompt. For VideoLLaMA3-7B (Zhang et al., 2025a), we used the default system message and run inference with flash attention 2. D.5 EXPERIMENTAL SETUP FOR ABLATION STUDY We describe the setup for the ablations mentioned in Section 5.2 in detail below. For Tables 5 and 17 and Fig. 11, the metric reported for Emotion Reasoning Basic (denoted as Basic) is the unweighted average of the visual and audio reasoning accuracy on the Emotion Reasoning Basic task. For Emotion Reasoning Stress Test (denoted as Stress), the reported metric is the unweighted average of the F1 scores for visual and audio reasoning samples within the Emotion Reasoning Stress Test task. For Modality Agreement (denoted as Agree), we report the F1 score over samples from the Modality Agreement task. Additionally, for the subtasks Spurious CueEmotion Association (denoted as Spur.) and Emotion-Relevant Cue Hallucination (denoted as Hall.), we use the unweighted average accuracy across visual and audio reasoning samples for each respective subtask. Ablation Study. For Table 5, the model without prompt-based modality preference (w/o PMP) is trained only using Ly DPO-TPD (Eq. (10)). The model without emotion-based response preference (w/o ERP) is trained using the the following loss, Lw/o ERP = LDPO-TPD + Lavprompt DPO (11) refer Eqs. (5) and (8) for the involved terms. Finally, the model without text prior debiasing (w/o TPD) is trained on the following objective, Lw/o TPD = Ly DPO + Lavprompt DPO (12) refer Eqs. (5) and (6) for the involved terms. Accepted as conference paper at ICLR 2026 Table 13: Performance comparison of different methods on the proposed EmoReAlM Benchmark. Bold are best results and underline are second-best results over open-source models. Model Reas. Basic Audio Visual Acc. Acc. Modality Agreement Reasoning - Stress Test Audio Visual Acc. Pre. Rec. F1 Acc. Pre. Rec. Acc. Pre. Rec. F1 Gemini 2.5 Flash Gemini 2.5 Pro 78.0 72.7 VideoLLaMA 3 Qwen 2.5 VL InternVL 3.5 Qwen 2 Audio Kimi-Audio Audio Flamingo VideoLLaMA PandaGPT OneLLM VideoLLaMA2 OLA VITA-1.5 Qwen 2.5 Omni Our base + Naive-DPO + Vista-DPO + AVEm-DPO % (relative) Emot.-LLaMA + Naive-DPO + Vista-DPO + AVEm-DPO % (relative) - - - 56.6 69.8 76.8 21.7 37.4 42.0 63.1 63.2 63.1 76.8 69.2 71.3 72.4 77.9 12.6 64.8 67.2 69.0 76.5 18.1 88.9 87.0 86.2 88.1 92. - - - 22.2 35.7 55.6 66.8 60.4 84.3 89.2 85.3 85.9 87.8 92.5 8.4 84.9 85.7 86.9 89.1 4.9 57.0 54.7 - - - - - - 34.1 53.7 54.8 52.6 51.7 51.7 52.2 51.4 57.3 63.1 68.9 34.1 51.2 56.1 58.2 65.6 28. 75.9 76.0 Closed-source models 39.0 33.3 74.0 74.0 Open-source video-only models 51.5 46.3 63.5 63.8 51.0 53. 60.4 62.0 - - - - - - - - - - - - - - - - - - - - - - - - - - - 55.1 54.0 52.6 Open-source audio-only models 84.2 95.8 96. 28.3 15.5 11.9 Open-source audiovisual (omni) models 50.6 30.1 28.9 47.3 41.9 37.2 39.6 40.8 48.9 80.4 94.6 131. 38.5 46.8 75.2 89.4 132. 37.4 50.3 64.3 52.0 78.9 87.1 86.1 86.3 87.2 89.4 93.4 8.2 82.9 83.4 85.9 89.5 8.0 41.3 62.9 87.1 60.6 86.8 91.0 90.4 65.4 62.3 67.8 70.7 8.1 59.2 60.1 63.1 65.2 10.1 30.9 56.9 45.9 53.0 29.8 18.2 20.7 21.6 27.3 36.8 44.3 105. 20.7 28.8 30.4 41.6 101. 33.9 53.4 53.5 52.5 42.7 30.2 33.3 34.6 41.6 52.1 60.0 73.4 33.1 42.8 40.9 56.8 71.6 46.1 45.8 56.8 53.7 63.5 63.0 64.0 53.1 55.6 74.1 82.6 55.6 48.9 53.5 69.2 77.3 58. - - - 42.3 26.6 21.2 45.5 40.7 43.4 53.2 56.6 52.8 55.0 50.3 54.8 73.6 80.9 60.8 46.7 52.6 68.6 75.4 61.5 73.2 73.1 64.9 75.2 68.3 - - - 48.8 47.1 62.0 59.4 62.3 66.1 67.8 66.4 70.6 87.0 94.6 42.5 69.1 71.9 87.6 91.8 32.9 75.3 84.0 97.9 98.6 91.6 - - - 48.4 59.9 97.6 67.9 85.0 92.7 96.4 87.2 88.8 92.1 93.1 6.8 89.3 89.5 92.5 92.6 3.7 70.9 59. 33.0 52.6 45.8 - - - 49.2 34.7 27.6 51.2 40.4 40.4 40.3 45.6 52.4 81.9 96.1 110. 48.9 54.3 82.6 90.9 85.9 73.0 69.8 49.4 68.5 61.1 - - - 48.8 43.9 43.1 58.4 54.8 56.3 56.8 59.9 65.9 86.7 94.6 57.9 63.2 67.6 87.3 91.7 45.1 Avg. Acc. 72.1 70.3 - - - - - - 37.1 44.0 54.2 59.1 60.2 65.6 70.0 65.1 68.1 76.9 83.3 28.0 63.8 66.9 74.2 80.1 25."
        },
        {
            "title": "E DETAILED RESULTS",
            "content": "E.1 EMOREALM RESULTS - EXPANDED Table 13 shows the the expanded version of Table 3 with accuracy, precision and recall metrics for Modality Agreement and Emotion Reasoning - Stress Test categories. We also report the unweighted average accuracy over all five tasks in the benchmark in the last column. The relative percent improvement of the AVEm-DPO trained model over the reference models is present as the % row. Moreover, we also report the performance of video-only and audio-only baselines in Table 13. We can see that for visual reasoning tasks (Basic and Stress Test), video-only baselines perform slightly better than the audiovisual (omni) baselines, aligning with the findings of Sung-Bin et al. (2025). However, for audio reasoning tasks, audiovisual baselines outperform audio-only baselines, which have very poor recall on the Emotion reasoning - Stress Test. This can be attributed to the limited amount of audio-emotion datasets that the baselines (Chu et al., 2024; Ding et al., 2025; Goel et al., 2025) are trained on resulting in poor emotion reasoning. E.2 EMOREALM RESULTS ON DIFFERENT STRESS TEST SUBTASKS Table 14 shows the performance of different baselines as well as AVEm-DPO on different subtasks of Emotion Reasoning - Stress Test, which have answer as No Spurious Cue-Emotion Association and Emotion-relevant Cue Hallucination (refer to Sections and 3.1 for definitions). We can observe that within audio and visual reasoning, hallucination seems to be bigger bottleneck than spurious cue-emotion associations. Moreover, similar to Table 13, we can observe that the audio-only models perform worse compared to audiovisual models, whereas the video-only model performance is better compared to audiovisual models. AVEm-DPO improves the model performance over all the subtasks significantly compared to the reference model. 27 Accepted as conference paper at ICLR Table 14: Performance of different baselines on different Reasoning Stress-Test sub-tasks in EmoReAlM Benchmark. This experiment is done only using samples from the Stress-Test category of the benchmark which have correct answer as No. Bold are best results and underline are secondbest results over open-source models. Model Audio Spur. Hall. Visual Spur. Hall. Open-source video-only models VideoLLaMA 3 Qwen 2.5 VL InternVL 3. - - - - - - 37.4 64.7 50.4 29.1 41.8 41.8 Qwen 2 Audio Kimi Audio Audio Flamingo 3 Open-source audio-only models - - - 41.8 26.8 15.7 16.9 6.0 8.7 Open-source audiovisual (omni) models VideoLLaMA PandaGPT OneLLM VideoLLaMA2 OLA VITA-1.5 Qwen 2.5 Omni Our base + Naive-DPO + Vista-DPO + AVEm-DPO 27.5 43.1 47.7 61.4 52.9 46.4 53.4 45.2 49.9 85.7 88.6 35.0 19.1 13.1 35.5 32.8 29.5 28.1 36.4 47.9 75.1 99. 33.1 47.5 36.7 57.6 56.8 46.0 51.9 49.3 56.8 87.1 96.5 - - - 37.4 23.4 19.6 45.6 25.9 35.4 30.1 41.9 48.0 76.7 95.8 Table 15: Class-wise recall for different emotion classes in DFEW dataset. Bold are best results and underline are second-best results over open-source models. Model Mod. Hap. Sad. Neu. Ang. Sur. Dis. Fea. UAR WAR VideoLLaMA 3 Qwen 2.5 VL InternVL 3.5 Qwen 2 Audio Kimi Audio Audio Flamingo 3 A PandaGPT VideoLLaMA OneLLM VideoLLaMA2 OLA VITA-1.5 Qwen 2.5 Omni EmotionLLaMA MoSEAR Our base +AVEm-DPO A,V A,V A,V A,V A,V A,V A,V A,V,T A,V,T A,V A,V Open-source video-only models 77.92 64.21 79.49 41.38 52.37 77.20 40.88 69.49 45.42 42.53 39.09 21. 26.44 11.38 53.02 34.26 7.20 12.61 Open-source audio-only models 0.06 12.66 6.12 0.00 71.24 83.01 25.08 42.97 19. 2.28 37.50 12.92 64.55 50.34 2.98 Open-source audiovisual (omni) models 60.50 85.04 47.91 87.50 52.00 61.46 45.45 71.98 79.35 70.75 75.21 58.61 20.84 52.35 58.56 48.95 23.19 70.64 71.95 69.66 77.04 73.96 0.0 4.17 3.23 7.94 15.65 23.54 61.11 61.99 40.45 29.64 44.07 9.95 8.41 54.33 57.93 82.20 79.96 73.84 76.25 75.20 72.07 72.03 0.00 3.95 26.08 42.08 9.65 8.05 4.40 33.67 42.86 61.54 62. 2.07 10.34 15.86 0.00 0.00 1.80 15.00 10.00 0.90 0.00 0.00 0.00 27.59 17.24 72.30 75.03 62.10 53.55 29.93 41.46 0.00 1.14 70.21 36.54 48.72 78.07 73.15 3.31 3.87 58.87 65.00 47.96 45.54 50. 21.08 36.43 26.05 18.44 17.65 36.74 43.65 38.17 39.31 46.94 45.59 44.48 56.78 58.54 49.47 52.32 55.46 22.24 43.30 26.39 24.20 24.09 37.60 48.66 41.73 42.56 54.33 59.37 56.60 60.14 64.24 Accepted as conference paper at ICLR 2026 Figure 10: User Evaluation using Qualtrics. (Left) We show anonymized model responses for given video to the user as different captions. (Right) We ask multiple questions to the user to select the best-suited caption for each question. Questions check the captions for their quality of emotion description, association of emotions with audiovisual cues, and presence of inconsistencies (hallucinations). Model Table 16: User evaluation on EMER dataset. Incons. Emot. Assoc. 15.38% 0.75% 9.82% 5.58% 9.36% 7.46% 6.04% 11.60% 17.25% 10.75% 18.57% 10.13% 1.89% 11.53% 68.61% 4.67% VideoLLaMA 2 OLA VITA 1.5 Qwen 2.5 Omni EmotionLLaMA Our + AVEm-DPO 54.74% 43.35% E.3 EMOTION RECOGNITION RESULTS - EXPANDED Table 15 (expanded from Table 2) shows the results on DFEW (Jiang et al., 2020) emotion recognition benchmark over different emotion classes. Note that both our base model and AVEm-DPO trained model achieve the best and second-best results in terms of unweighted and weighted average recalls over all the emotion classes. Moreover, Table 15 shows that the proposed method ensures fair performance over all the emotion categories, unlike baselines, which perform too well on some classes and too poorly on the others. E.4 USER EVALUATION We perform user study on 40 participants recruited through Prolific (Prolific) and create user survey using Qualtrics (Qualtrics) as shown in Fig. 10. We randomly sample videos from EMER (Lian et al., 2023b) dataset and display anonymized model generations as captions to the user along with the video. Then we ask the users to pick the most suited caption over different criteria (i) best caption describing the emotional state of the person, (ii) best caption associating the emotion with audiovisual cues, and (iii) worst caption with the most inconsistencies with the video (to test model hallucinations). Table 16 (duplicate of Table 4) reports the average percent of times each model is selected for the mentioned three criteria. The participants selected our model the most number of times as the best model for emotion description and association of audiovisual cues for emotion. Moreover, our model was chosen the least number of times for inconsistent audiovisual information present in the caption. 29 Accepted as conference paper at ICLR 2026 Table 18: Performance variation over various choices of rejected response. yirr pletely irrelevant to the audiovisual content and emotion, yer that generally co-occur with given emotion, yvr incorrectly with emotion. : response coml : response mentions hallucinated cues : response associates audiovisual cues in the input y1 y2 Our base yirr - yer - yvr - yirr yer yirr yvr yvr yer Basic Agree. 77.3 82.4 84.0 83.2 83.4 83.1 85.2 34.6 56.7 58.3 58.0 57.6 57.3 60.1 Stress 55.1 81.4 86.0 85.3 85.8 84.9 87. Spur. Hall. 39.2 47.3 88.9 85.1 97.9 88.5 90.9 91.6 97.8 88.2 90.8 90.3 97.6 92.7 E.5 MODALITY PREFERENCE ABLATION Table 17: Performance variation over various choices of rejected multimodal input. Change denotes which among (aw, vw) should be changed to create (al, vl). Choice of al/vl Table 17 shows AVEm-DPOs performance for different choices of multimodal preferences. We perform experiment using random tensor, random video, (al, vl) infused with diffusion noise similar to VCD (Leng et al., 2024) and an audiovisual input with different emotion than (aw, vw) as the possible choices for (al, vl) and show that using different emotion video leads to the best results. Moreover, we also show the effect of changing both (aw, vw) vs. changing based on the input prompt (aw for audio reasoning, vw for visual reasoning and both for other tasks), justifying the effectiveness of prompt-based modality preference. Change Both al, vl Prompt-based Both al, vl Prompt-based Both al, vl Prompt-based Both al, vl Prompt-based Diffuse (aw, vw) Random tensor Random video Diff. emotion Basic Agree. 81.9 83.0 81.8 83.6 82.7 84.6 83.9 85.2 56.1 56.0 58.2 58.2 58.5 59.4 60.1 60.0 Stress 80.1 81.6 80.3 82.1 80.9 86.7 81.3 87. E.6 RESPONSE PREFERENCE ABLATION Table 18 shows the variation of performance over different tasks of EmoReAlM for different choices of rejected responses. There are three types of rejected responses that we test on (i) yvr is videorelevant response that contains audiovisual cue present in the video, but it does not associate with the emotion, (ii) yer is emotion-relevant response that correctly associates with the emotion displayed in the video but with audiovisual cues that are hallucinated (not present in the video), and (iii) yirr is completely irrelevant to the given video and emotion (similar to that present in Huang et al. (2025b)). and y2 y1 in Table 18 denote the first and second rejected responses for preference tuning in Eq. (10). We can see that our choice of using yvr and yer in Eq. (10) for AVEm-DPO results in the best performance of the model across all tasks. We also perform experiments using single rejected response (Eq. (8)), and we can see that using yer individually results in improvement over the base, specifically for the Spurious Cue-Emotion Association and Emotion-relevant Cue Hallucination subtasks, respectively. Moreover, similar to Vista-DPO (Huang et al., 2025b), we perform an experiment using yirr as the second rejected response, which results in the same or worse performance than using yvr and yer as the second rejected response, we set βirr = 0.3 following Huang et al. (2025b). alone. When using yirr and yvr l E.7 SENSITIVITY TO HYPERPARAMETERS Fig. 11 shows AVEm-DPOs accuracy on different subtasks of EmoReAlM on varying the hyperparameters βvr/βer in Eq. (6). We can observe that while spurious cue-emotion associations mitigate on increasing βvr, model performance on hallucinated cue samples improves on increasing βer. For text-prior debiasing (TPD), we can see that performance on hallucinated cue samples significantly improves even with γTPD = 0.1 and gets saturated at γTPD > 0.2. Finally, increasing the strength of PMP using λav (Eq. (9)) improves performance but it gets saturated at λav > 1.0. 30 Accepted as conference paper at ICLR 2026 Figure 11: Sensitivity of performance to βvr/βer, γTPD and λav. Figure 12: (Left) For visual reasoning question, we compare the model responses on using the original video with the original audio and an adversarial audio as input. We can observe that Vista-DPO and even AVEm-DPO without prompt-based modality preference (PMP) struggle in the adversarial settings; however, AVEm-DPO produces the desired response. (Right) We perform similar experiment to show the visual reasoning robustness of AVEm-DPO. E.8 ATTENTION REDISTRIBUTION AFTER PREFERENCE OPTIMIZATION. As described in Section 5.2, to analyze the effect of preference optimization on attention, we plot the distribution of aggregate multimodal input attention over audio and visual tokens averaged over all attention heads for different tasks in EmoReAlM in Fig. 15 (left two plots). For reasoning tasks, we can observe that the attention over relevant modality increases after AVEm-DPO. For the Modality Agreement task, the attention is redistributed in way that there is fair distribution of attention between both modalities to ensure reliable predictions. To show the effect of text-prior debiasing Section 4.2, we plot the percentage of total attention (averaged over attention heads) over multimodal input tokens (audio and visual combined) and observe that AVEm-DPO increases the attention over multimodal tokens by significant margins (Fig. 15 right). This shows that AVEm-DPO training ensures that the model attends to the relevant audiovisual tokens for generating the response rather than relying only on the input text prompt. E.9 REASONING WITH ADVERSARIAL MODALITY INPUTS To test the robustness of AVEm-DPO against cross-modality hallucinations, we conduct an adversarial test by replacing the audio in visual reasoning task to see if the models response stays 31 Accepted as conference paper at ICLR 2026 Figure 13: Adversarial Audio Reasoning Testing. For samples related to audio reasoning in the EmoReAlM benchmark (Emotion Reasoning-Basic and Emotion Reasoning - Stress Test), we plot the Kernel Density Estimation (KDE) shift in log likelihoods of the correct answer when the irrelevant video modality input (vori) is replaced with random video as adversary (vadv). AVEm-DPO is least affected by the addition of an adversary in the irrelevant modality (i.e., video). Figure 14: Adversarial Visual Reasoning Testing. Similar to Fig. 13, for samples related to visual reasoning in the EmoReAlM benchmark (Emotion Reasoning-Basic and Emotion Reasoning - Stress Test), we plot the Kernel Density Estimation (KDE) shift in log likelihoods of the correct answer when the irrelevant audio modality input (aori) is replaced with random video as adversary (aadv). AVEm-DPO is least affected by the addition of an adversary in the irrelevant modality (i.e., audio). the same. As shown in Fig. 12, changing the prompt-irrelevant modality does not change the response of AVEm-DPO, showing its adversarial robustness. It is interesting to note that removing the prompt-based modality preference (PMP) from AVEm-DPO results in wrong predictions, showing its efficacy. To quantitatively show the effect of AVEm-DPO with PMP, we perform adversarial testing using Emotion Reasoning-Basic and Emotion Reasoning - Stress Test samples in EmoReAlM. For testing the robustness of AVEm-DPO for audio reasoning  (Fig. 13)  , we compute the shift of log likelihoods of the correct response when the prompt-irrelevant video modality is replaced with an adversary (i.e., some random video). We use Kernel Density Estimation (KDE) to estimate the shift in the distributions. We can see that AVEm-DPO is robust to adversaries in the prompt-irrelevant modality. Moreover, removing PMP from AVEm-DPO significantly increases the shift between the original and adversarial distributions. Fig. 14 shows similar plots for the tasks related to visual reasoning. E.10 EFFECT OF INDIVIDUAL MODALITIES FOR EMOTION PREDICTION To show the effect of individual modalities for emotion recognition, Table 19 reports the performance on using only the video, only the audio and using audiovisual inputs from the RAVDESS (Livingstone & Russo, 2018) dataset for emotion prediction. We can observe that using the individual modalities for emotion prediction leads to reduced performance, indicating that using the audiovisual inputs for emotion prediction is indeed helpful compared to using single modality. Table 19: Performance of different models for emotion prediction using audiovisual, video-only, and audio-only inputs from the RAVDESS (Livingstone & Russo, 2018) dataset. Model VideoLLaMA 2 Qwen 2.5 Omni Our base + AVEm-DPO EmotionLLaMA + AVEm-DPO Audio-only Audiovisual Video-only UAR WAR UAR WAR UAR WAR 27.56 41.81 25.55 32.88 37.74 53.59 39.67 58.66 37.27 52.59 37.04 56.21 32.41 27.67 40.98 46.31 39.56 43.09 31.62 28.05 53.01 55.48 48.12 51.03 30.44 28.56 38.18 44.05 38.57 40. 36.12 29.38 41.27 46.13 41.27 46.10 32 Accepted as conference paper at ICLR 2026 Figure 15: Effect of AVEm-DPO on the distribution of attention over (i) (Left three plots) video and audio tokens taken as percentage over the total attention over all multimodal tokens for different subtasks in EmoReAlM and (ii) (Right) multimodal tokens as percentage over the total input tokens (including text) for the entire EmoReAlM. Moreover, we can observe that the performance using only the visual modality is better compared to using only audio, indicating the importance of visual modality for emotion prediction. Accepted as conference paper at ICLR 2026 Figure 16: Comparison of baseline MLLMs with our base model trained with AVEm-DPO on sample from EMER (Lian et al., 2023b). Correct audiovisual cues and emotion are in green, emotionirrelevant cues are in blue, and hallucinated cues (and incorrect emotion) are present in red ."
        },
        {
            "title": "F QUALITATIVE SAMPLES",
            "content": "Emotion Descriptions on EMER. Figs. 16 and 17 shows samples from the EMER (Lian et al., 2023b) dataset and the output of different MLLM baselines on those samples using the prompt Describe the audiovisual content relevant to emotion in detail.. We can see that AVEM-DPO leads to correct emotion descriptions and consistent audiovisual cues to reason for the emotions. Moreover, compared to baselines, our method does not associate irrelevant and/or background information with emotions. EmoReAlM Sample Outputs. Fig. 18 shows the model responses for some samples in the Emotion Reasoning - Stress Test of EmoReAlM Benchmark. We can notice that AVEm-DPO improves the model responses in cases with spurious-emotion cue associations and emotion-cue hallucinations. 34 Accepted as conference paper at ICLR 2026 Figure 17: Comparison of baseline MLLMs with our base model trained with AVEm-DPO on sample from EMER (Lian et al., 2023b). Correct audiovisual cues and emotion are in green, emotionirrelevant cues are in blue, and hallucinated cues (and incorrect emotion) are present in red . Figure 18: Qualitative examples comparing the output responses using different approaches for some samples present in the Emotion Reasoning - Stress Test of EmoReAlM benchmark. 35 Accepted as conference paper at ICLR"
        },
        {
            "title": "G PROMPT POOL",
            "content": "Figure 19: Audio caption prompt used to caption only the audio content from video. Note that the audio is passed along with the prompt to GPT-4o-audio as WAV file. Figure 20: Video caption prompt used to caption only the visual content in video. We blur the captions if they are already present in the video and explicitly ask the model to ignore them if they are present in the visual content. Figure 21: Audio emotion prediction prompt used to predict the emotion into one of the 7 basic categories from only the audio caption. 36 Accepted as conference paper at ICLR 2026 Figure 22: Video emotion prediction prompt used to predict the emotion into one of the 7 basic categories from only the video caption Figure 23: EmoReAlM Basic Reasoning Prompt - Audio used to generate questions which ask about the audio cues that suggest the emotion of the person in the video. 37 Accepted as conference paper at ICLR 2026 Figure 24: EmoReAlM Basic Reasoning Prompt - Visual used to generate questions which ask about the visual cues that suggest the emotion of the person in the video. Figure 25: EmoReAlM Modality Agreement Prompt used to generate questions which ask whether the audio and video in the audiovisual input align with each other to express the emotion in the video. Accepted as conference paper at ICLR 2026 Figure 26: EmoReAlM Stress Test Prompt - Audio - No Hallucination used to generate questions where the audio cue mentioned in the question is present in the audiovisual input and supports the emotion of the person in the video. Figure 27: EmoReAlM Stress Test Prompt - Audio - Spurious Associations used to generate questions where the audio cue mentioned in the question is present in the audiovisual input and but it is spuriously related to the emotion of the person in the video. 39 Accepted as conference paper at ICLR 2026 Figure 28: EmoReAlM Stress Test Prompt - Audio - Hallucination used to generate questions where the audio cue mentioned in the question is hallucinated (not present in the audiovisual input) and but it usually explains the emotion experienced by the person in the video. Figure 29: EmoReAlM Stress Test Prompt - Video - No Hallucination used to generate questions where the visual cue mentioned in the question is present in the audiovisual input and supports the emotion of the person in the video. 40 Accepted as conference paper at ICLR 2026 Figure 30: EmoReAlM Stress Test Prompt - Video - Spurious Associations used to generate questions where the visual cue mentioned in the question is present in the audiovisual input and but it is spuriously related to the emotion of the person in the video. Figure 31: EmoReAlM Stress Test Prompt - Video - Hallucination used to generate questions where the visual cue mentioned in the question is hallucinated (not present in the audiovisual input) and but it usually explains the emotion experienced by the person in the video. Accepted as conference paper at ICLR 2026 Figure 32: Text Only Guess Prompt used to prompt GPT-4o, Gemini 2.5 and Qwen 2.5 to predict the answer to the generated QA samples using only the question text and answer choices to eliminate responses which can be answered just with the text. Figure 33: Preference Data Generation Prompt - Audio Reasoning used to generate rejected responses for generated question-answer pair related to audio reasoning tasks. 42 Accepted as conference paper at ICLR 2026 Figure 34: Preference Data Generation Prompt - Visual Reasoning used to generate rejected responses for generated question-answer pair related to visual reasoning tasks. 43 Accepted as conference paper at ICLR 2026 Figure 35: Preference Data Generation Prompt - Modality Agreement used to generate rejected responses for generated question-answer pair related to modality agreement tasks. 44 Accepted as conference paper at ICLR 2026 Figure 36: Audiovisual Caption Prompt used to combine audio and visual captions to create combined audiovisual caption. Figure 37: EMER Evaluation Prompt used to evaluate the model generations against the provided ground truths for the EMER dataset."
        }
    ],
    "affiliations": [
        "Institute for Creative Technologies, University of Southern California"
    ]
}