{
    "paper_title": "MUR: Momentum Uncertainty guided Reasoning for Large Language Models",
    "authors": [
        "Hang Yan",
        "Fangzhi Xu",
        "Rongman Xu",
        "Yifei Li",
        "Jian Zhang",
        "Haoran Luo",
        "Xiaobao Wu",
        "Luu Anh Tuan",
        "Haiteng Zhao",
        "Qika Lin",
        "Jun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved impressive performance on reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it often leads to overthinking, wasting tokens on redundant computations. This work investigates how to efficiently and adaptively guide LLM test-time scaling without additional training. Inspired by the concept of momentum in physics, we propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically allocates thinking budgets to critical reasoning steps by tracking and aggregating stepwise uncertainty over time. To support flexible inference-time control, we introduce gamma-control, a simple mechanism that tunes the reasoning budget via a single hyperparameter. We provide in-depth theoretical proof to support the superiority of MUR in terms of stability and biases. MUR is comprehensively evaluated against various TTS methods across four challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate that MUR reduces computation by over 50% on average while improving accuracy by 0.62-3.37%."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 8 5 9 4 1 . 7 0 5 2 : r MUR: Momentum Uncertainty Guided Reasoning For Large Language Models MUR : Momentum Uncertainty guided Reasoning for Large Language Models Hang Yan* Fangzhi Xu* Rongman Xu Yifei Li Jian Zhang Haoran Luo Xiaobao Wu Luu Anh Tuan Haiteng Zhao Qika Lin Jun Liu Xian Jiaotong University Nanyang Technological University Peking University National University of Singapore hyan@stu.xjtu.edu.cn, fangzhixu98@gmail.com, zhaohaiteng@pku.edu.cn liukeen@xjtu.edu.cn * means equal contribution denotes corresponding authors (cid:135) https://github.com/yayayacc/MUR"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have achieved impressive performance on reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it often leads to overthinkingwasting tokens on redundant computations. This work investigates how to efficiently and adaptively guide LLM test-time scaling without additional training. Inspired by the concept of momentum in physics, we propose Momentum Uncertainty-guided Reasoning (MUR ), which dynamically allocates thinking budgets to critical reasoning steps by tracking and aggregating step-wise uncertainty over time. To support flexible inference-time control, we introduce γ-control, simple mechanism that tunes the reasoning budget via single hyperparameter. We provide in-depth theoretical proof to support the superiority of MUR in terms of stability and biases. MUR is comprehensively evaluated against various TTS methods across four challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate that MUR reduces computation by over 50% on average while improving accuracy by 0.623.37%."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) Brown et al. (2020); Dubey et al. (2024) demonstrate remarkable performance in reasoning-intensive scenarios, including logic, mathematics, and game-playing tasks. critical advancement in optimizing their reasoning quality is TestTime Scaling (TTS). Existing methods either incentivize long thinking patterns through reinforcement learning with verifiable rewards (RLVR) Ye et al. (2025); Jaech et al. (2024); Guo et al. (2025), or employ stepwise optimization via parallel sampling Yao et al. (2023); Lightman et al. (2023); Wang et al. (2024b); Ma et al. (2024); Xu et al. (2025a) and sequential critique Lan et al. (2024); Li et al. (2025). While effective, the issue of overthinking Chen et al. (2024b); Sui et al. (2025) is widely observed that degrades the inference efficiency. As shown in Figure 1, the performance can even be slightly improved, despite >50% reduction in thinking tokens. This demonstrates that there is significant room for improvement in making long thinking concise. Intuitively, LLMs should spend more token budgets on complex steps to deliberately enhance output quality, while generating simple steps directly to avoid overthinking. 1 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models Thus, it still remains challenging to identify key steps and dynamically allocate computes. Recent works Xia et al. (2025a); Jiang et al. (2025); Yang et al. (2025c); Yu et al. (2025); Yang et al. (2025b) explore training methods to adaptively allocate token usage on different steps, which introduce additional training costs and lack generalization. Off-the-shelf trainingfree methods Kim et al. (2025); Xu et al. (2025a); Wang et al. (2025) scale thinking tokens in fixed manner, failing to adapt to problem complexity or on-going reasoning process. Therefore, the pursuit of efficiently and adaptively guiding LLM test-time scaling without training is both intriguing and understudied. To answer this question, we are the first to model LLM reasoning with the concept of momentum. In physics, momentum accumulates historical information over time and resists sudden changes. Based on this and the successful application of Gradient Descent with Momentum Qian (1999), we propose Momentum Uncertainty guided Reasoning (MUR ), novel approach that dynamically evaluates the overall uncertainty of reasoning path by aggregating historical step-level uncertainties, mirroring the smooth and consistent evolution observed in physical dynamics. Without requiring any training, MUR selectively allocates computation only to critical steps during inference. Based on the approach, we introduce the concept of γ-control, where we can flexibly control the thinking budget and the performance, with only one hyperparameter γ. Further, this work proves that MUR is theoretically grounded in terms of discounted credit assignment, stability, and convergence while maintaining compatibility with existing TTS methods. Extensive experiments across four challenging benchmarks and three backbone model sizes demonstrate that MUR reduces the thinking budget by over 50% on average while even improving accuracy by 0.623.37%. Figure 1: Comparisons between average accuracy and token usage. Per-Step Scale refers to test-time scaling methods that optimize every step without computesaving mechanisms. MUR is computationally efficient approach that selectively scales only key steps. The key contributions include: (1) Adaptive Scaling Technique. We propose novel concept of momentum uncertainty and offer training-free solution MUR to dynamically allocate computes to key reasoning steps guided by momentum uncertainty, which is compatible with various TTS methods. (2) Efficiency and Performance Gains: MUR reduces the thinking costs by 50% even with obvious performance gains, across wide range of benchmarks and model sizes. The proposed γ-control offers flexible solution to balance performance and efficiency. (3) Theoretical Support: MUR is theoretically grounded in terms of discounted credit assignment, stability, and convergence, which support its practical superiority."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Test-Time Scaling Test-Time scaling (TTS) methods allocate additional token usage during inference, revealing scaling law Brown et al. (2024); Wu et al. (2024) that more computes lead to better performance. Training-based methods elicit long thinking patterns through reinforcement learning with verifiable rewards (RLVR) Ye et al. (2025); Jaech et al. (2024); Guo et al. (2025). Training-free methods can be categorized into parallel scaling and sequential scaling. Parallel scaling Yao et al. (2023); Ma et al. (2024); Xu et al. (2025a) samples several answers for the same input, followed by selecting the best one. Sequential scaling Lan et al. (2024); Li et al. (2025) utilizes feedback from self-evaluation or external models to optimize current an2 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models swer. Although these researches show remarkable achievements, they allocate unnecessary computes for simple steps. Our work MUR , as an orthogonal method to these researches, optimizing test-time scaling methods by guiding them to scale only the key steps, reducing unnecessary computes largely. 2.2 Overthinking Although LLMs demonstrate significant performance gains through test-time scaling methods, they are likely to introduce computational overhead and reasoning latency Chen et al. (2024b); Sui et al. (2025). One line of mitigating overthinking is to shorten reasoning length through post-training Xia et al. (2025a); Jiang et al. (2025); Yang et al. (2025c); Yu et al. (2025); Yang et al. (2025b), which introduces training overhead and limits their generalization. Another line is training-free methods Kim et al. (2025); Xu et al. (2025a); Wang et al. (2025), reducing token usage in fixed manner, which lacks adaptation to the on-going reasoning process. Our work MUR , as training-free method, adaptively saves unnecessary computes during the whole reasoning process. 2.3 Uncertainty Estimation The reasoning path of LLM often contains reliability issues, like hallucinations or biased responses Xia et al. (2025b). One line of uncertainty estimation is scaling more computes, including verbalizing methods Tian et al. (2023); Tanneru et al. (2024), consistency-based methods Hou et al. (2024); Chen & Mueller (2024); Gao et al. (2024), and semantic clustering methods Kuhn et al. (2023); Farquhar et al. (2024); Nikitin et al. (2024). Another line of uncertainty estimation is utilizing the internal information during decoding Ahdritz et al. (2024); Chen et al. (2024a); Sriramanan et al. (2024); Xu et al. (2025b), which estimates the uncertainty of generated path through aggregating token-level probabilities, lacking the adaptation to different reasoning steps. Our method MUR , assigns more attention to recent steps, while reducing the impact of early steps."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first formulate the stepwise test-time scaling, adaptive scaling and steplevel uncertainty (Sec. 3.1). Then we formally propose momentum uncertainty, followed by theoretical proof of its superiority (Sec. 3.2). Based on the momentum uncertainty, we introduce γ-control mechanism to flexibly scale inference-time scaling (Sec. 3.3). The overview of MUR is presented in Figure 2. Figure 2: Comparison of reasoning methods. (a) Vanilla CoT: Standard stepwise reasoning without test-time scaling. (b) Per-Step Scale: scales computes per reasoning step. (c) MUR : Adaptive test-time scaling framework (ours). 3 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models 3.1 Preliminary Stepwise test-time scaling LLM reasoning can be formulated as auto-regressively generating step at at each timestamp t, based on the inputs and previous steps: at pθ(x, a<t), (1) where is the concatenation of input question and instruction. a<t represents previous steps . θ denotes the parameters of pre-trained LLM, and pθ is the probability distribution. To optimize the quality of the reasoning path, current methods apply test-time scaling at each step, which can be formulated as follows: ˆat Q(x, a<t), (2) where ˆat is the optimized step. denotes the specific test-time scaling method, such as Best-of-N Brown et al. (2024). Adaptive Scaling Conventional test-time scaling methods typically apply optimization at every decoding step, leading to excessive token usage and computational overhead. However, not all steps require such enhancement, and current research on adaptive compute allocation remains limited, often overlooking this inefficiency. We therefore pose the central question: When should compute be scaled during inference? To address this, we model this research question with binary detector that selectively activates test-time scaling based on contextual reasoning dynamics: ˆat = (cid:26)Q(x, a<t) at , D(t) = True , D(t) = False . (3) Here, determines whether to invoke test-time scaling method at each step based on historical information. Our work focuses exclusively on designing the detector to assess the reasoning trajectory and adaptively decide whether to allocate additional compute to the current step at. Step-level Uncertainty Uncertainty estimation quantifies an LLMs confidence in its output, where higher uncertainty implies lower confidence. For step at consisting of tokens, we compute the step-level uncertainty based on token-wise probabilities. Specifically, we define the average negative log-likelihood of the tokens as: mt = 1 j=1 logpθ(a (j) x, a<t, (<j) ), (4) where mt the uncertainty of step t. (1) token sequence (j1) , ..., (2) , . (j) is j-th token of step at. And (<j) denotes the prefix 3.2 Momentum Uncertainty LLM can maintain an uncertainty estimation for the reasoning process, reflecting the global assessment of both input and generated steps at. Ideally, this uncertainty should evolve smoothly, adapting to new steps, as they are generated while preserving calibrated estimate of earlier steps. Inspired by the concept of momentum in physics, which retains and updates an objects motion by accumulating past forces while resisting abrupt changes. We propose momentum uncertainty, recursive formulation of that dynamically tracks overall uncertainty during reasoning as follows: Mt = αMt1 + (1 α)mt, (5) where Mt is the momentum uncertainty at timestamp t, with initial value M0 = 0. And α (0, 1) is hyper-parameter controlling the momentum changing. With recursive definition, momentum uncertainty aggregates all generated step-level uncertainties to represent the overall estimation of the reasoning process. Further, we introduce the excellent property of momentum uncertainty with in-depth theoretical analysis. 4 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models Proposition 1: Momentum uncertainty is an exponentially weighted sum of step-level uncertainties, emphasizing recent steps and fading earlier ones. Proof. We provide detailed derivation in Appendix A.1. It transforms Eq. (5) into the exponential weighting of step-level uncertainties as follows: Mt = (1 α) i=1 αtimi. (6) Through Eq. (6), Mt assigns different weights αti to historical step-level uncertainty mi, emphasizing recent uncertainties while smoothing early fluctuations, balancing the attention among different steps. Proposition 2: Momentum uncertainty brings more stable estimation, with lower variance than step-level uncertainty. Let Var() denotes the variance calculation, Var(Mt) < Var(mt). Proof. LLM decoding contains unavoidable noise Wang et al. (2024a); Zhou et al. (2024), introducing variance to uncertainty estimation. We assume that each step-level uncertainty mt contains two parts: (7) where µt is the pure step-level uncertainty without noise, and ϵt is noise originating from training, randomly sampling, etc. And is the gaussian distribution and σt is the corresponding standard deviation. mk = µk + ϵk, ϵk (0, σ2 ), From Eq. (7), we can infer that the variance of step-level uncertainty Var(mt) is σ2 . Based on this assumption, we infer the variance of momentum uncertainty as follows, detailed proof is in Appendix A.2: Var(Mt) = (1 α)(1 α2t) 1 + α (cid:123)(cid:122) coefficient (cid:125) (cid:124) < σ2 σ2 = Var(mt). (8) In Eq. (8), α (0, 1), making the denominator of the coefficient lager than 1 and the numerator smaller than 1. So the coefficient is between 0 and 1, making the variance of momentum uncertainty less than the variance of step-level uncertainty, leading to more stable uncertainty estimation. Proposition 3: With the steps increasing, momentum uncertainty offers better convergence. Let E[Bias()] denotes the expectation of bias, then E[Bias(Mt)] converge 0. Proof. Regardless of above-mentioned noise in Proposition 2, LLM may still generate steplevel uncertainty bias. Combining this with Eq. (6) and Eq. (7), we can infer the upper bound of momentum uncertainty bias as follows: [Bias(Mt)] Kρt, (9) where is finite constant, ρ is between 0 and 1. Detailed definition and proof are in Appendix A.3. This ensures rapid convergence of Kρt 0 as increases. While step-level uncertainty introduces bias, momentum-weighted uncertainty Mt effectively dampens oscillatory behavior during inference, enhancing stability. Noticeably, calculating the average of step-level uncertainty is common method Ren et al. (2022); Manakul et al. (2023); Huang et al. (2024). The theoretical superiority of our proposed Mt compared to it is in Appendix A.4. Experimental comparison is in Sec. 4.2. 5 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models 3.3 Scalable Thinking with γ-control Since momentum uncertainty captures the overall confidence in the reasoning trajectory, we propose γ-control mechanism to identify whether the current step is incompatible with prior reasoning. This mechanism balances reasoning performance and computational cost. Scale High-uncertainty Steps At each step, the step-level uncertainty mt reflects the models confidence in the current generation at, while Mt1 aggregates uncertainty over previous steps. If mt > Mt1, the current step is more uncertain than the reasoning history, suggesting it may be erroneous. To address this, we introduce checking mechanism that selectively scales uncertain steps. To tolerate minor fluctuations while flagging significant deviations, we apply γ-control threshold. Specifically, we define detector in Eq. (3) as: ˆat = (cid:26)Q(x, a<t) at , exp(mt) > exp(Mt1)/γ , others , (10) where γ is the controllable scaling rate, ranging from (0,1) in practice. The scaling factor 1 γ effectively raises the detection boundary, allowing slight uncertainty increases while catching large deviations. Smaller γ values result in fewer steps being scaled, enabling flexible control over the computational budget. The inequality in Eq. (10) flags when step diverges significantly from the previous reasoning, corrective test-time scaling is triggered to improve output quality. theoretical analysis of γ-control is provided in Appendix A.5 and empirical results of γ-control is presented in Sec. 5.1. Orthogonal to Test-Time Scaling Methods Our momentum uncertainty-based detector is orthogonal and complementary to current test-time scaling methods, such as best-of-N and thinking model. It identifies uncertain steps and selectively triggers compute-intensive optimization, maintaining or even improving performance while reducing redundancy."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Benchmarks We evaluate our proposed method MUR on three widely adopted math reasoning benchmarks MATH-500 Hendrycks et al. (2021), AIME24, and AIME25. In addition, we include GPQA-diamond Rein et al. (2024) to validate the generalization to the science domain. Metrics We adopt pass@1 rate as our Acc. metric. We also report the average token usage of backbone model as #Token for each solution, providing an aspect of efficiency evaluation. For AIME24 and AIME25, to reduce the infection of randomness, we sample 4 times for each query and report the average accuracy. Test-Time Scaling Settings We adopt three test-time scaling methods as the basic setting. Guided Search. It can be viewed as step-level Best-of-N Brown et al. (2024), where candidate steps are sampled in parallel at each timestep, and the optimal one is selected. We adopt GenPRM 1.5B Zhao et al. (2025) as an external reward model for candidate selection. LLM As Critic. The LLM receives feedback after generating each step and iteratively refines its output based on the critique Lan et al. (2024); Li et al. (2025). We also adopt GenPRM 1.5B for stepwise feedback generation. ϕ-Decoding. It does not require external models but selects the best step from several candidates using the foresight sampling strategy. Thinking Mode. It generates long thoughts for each step. 6 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models MATHAIME24 AIME25 GPQA-diamond Acc. #Tokens Acc. #Tokens Acc. #Tokens Acc. #Tokens Acc. Avg. #Tokens 69. 1,047 15.00 3,868 7.50 3,952 26. 1,086 29.49 - 2,488 - Qwen3-1.7B 70.80 70.20 70.80 71.20 70.20 68.60 70.40 71.20 68.00 69.00 70.20 69.80 3,460 2,398 3.128 1,321 1,098 1,019 878 902 5,501 2,844 3,848 2, 20.83 18.33 20.83 24.17 19.17 19.17 16.67 22.50 17.50 19.17 16.67 19.17 22,498 8,243 11,887 6,460 4,108 5,561 2,945 6,259 20,518 9,449 10,992 9, 11.67 10.83 11.67 15.83 9.17 10.00 10.00 10.83 9.17 9.17 10.00 12.50 17,882 8,662 15,845 5,919 3,609 4,433 3,014 5,319 20,629 10,389 13,693 10, Qwen3-4B 27.27 25.76 24.74 32.83 28.28 26.77 28.28 32.32 25.76 25.25 23.23 27.27 6,739 3,404 3,825 2,005 892 1,417 1,446 1, 9,261 2,431 3,338 1,827 - 32.64 31.28 (-1.36) 31.96 (-0.68) 36.01 (+3.37) 12,645 5,677 8,671 3,926 - (-55.10%) (-31.43%) (-68.95%) - 31.71 31.14 (-0.57) 31.34 (-0.37) 34.21 (+2.50) - 2,427 3,108 (+28.06%) 2,071 (-14.67%) 3,543 (+45.98%) - 30.11 30.65 (+0.54) 30.02 (-0.09) 32.19 (+2.08) 13,977 6,278 7,968 5, - (-55.08%) (-42.92%) (-57.89%) 79.40 772 25.83 2,422 15. 2,087 39.90 612 40.03 - 1, - 79.80 79.80 81.60 81.40 80.80 81.40 80.60 81.60 76.80 80.60 79.40 79.60 3,048 1,911 2,476 824 777 741 813 4,690 1,866 2,776 1,796 25.83 25.83 26.67 28.33 30.00 29.17 26.67 30.83 28.33 25.83 26.67 30.00 13,445 6,962 11,087 2,791 4,477 2,933 3,611 4, 13,483 9,090 11,671 4,144 22.50 21.67 15.83 22.50 17.50 18.33 16.67 18.33 17.50 18.33 20.00 18.33 12,324 8,688 11,604 4,009 2,983 3,005 2,934 2, 14,378 9,752 12,512 9,954 Qwen3-8B 42.42 39.90 43.43 41.92 40.91 39.90 43.43 40.91 41.41 39.90 40.40 41.92 3,517 1,354 2,116 737 804 724 699 4,263 1,511 2,195 944 - 42.64 41.80 (-0.84) 41.88 (-0.76) 43.54 (+0.90) - 42.30 42.20 (-0.10) 41.84 (-0.46) 42.92 (+0.62) - 41.01 41.17 (+0.16) 41.62 (+0.59) 42.46 (+1.45) 8,084 4,729 6,821 2,138 2,244 1,871 2,021 2,134 9,204 5,555 7,289 4,210 - (-41.50%) (-15.62%) (-73.55%) - (-16.58%) (-9.94%) (-4.86%) - (-39.65%) (-20.81%) (-54.26%) 81.40 1,131 31.50 3, 19.17 3,097 39.90 859 42.99 - 2,212 - 83.20 82.80 82.60 83.20 83.40 82.40 83.20 83.80 84.20 81.80 83.20 84.40 4,069 2,427 3,502 2, 1,022 1,086 1,167 1,132 5,841 3,222 4,782 2,854 37.50 36.67 37.50 40.83 31.67 30.83 35.83 34.17 33.33 35.00 37.50 36.67 27,672 11,088 15,333 7, 4,472 5,029 4,245 4,531 25,181 14,233 20,947 14,865 20.83 25.00 21.67 25.00 20.83 20.00 20.00 23.33 22.50 23.33 20.83 25.00 27,871 10,765 17,988 7, 3,886 4,705 4,125 3,678 24,555 14,202 21,598 13,719 46.46 43.94 46.97 46.97 44.44 41.92 44.95 44.95 43.43 45.45 44.44 47.47 4,252 2,213 3,797 3, 1,172 1,375 1,069 1,007 4,726 2,087 4,167 2,359 - 47.00 47.10 (+0.10) 47.19 (+0.19) 49.00 (+2.00) - 15,966 6,623 (-58.52%) 10,155 (-36.40%) (-66.13%) 5,408 - 45.09 43.79 (-1.30) 46.00 (+0.91) 46.56 (+1.47) - 2,638 3,049 (+15.58%) (+0.53%) 2,652 (-1.93%) 2,587 - 45.87 46.40 (+0.53) 46.49 (+0.62) 48.39 (+2.52) - 15,051 8,433 (-43.97%) 12,834 (-14.73%) (-43.86%) 8, Vanilla CoT Guided search + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) LLM as critic + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) ϕ-Decoding + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) Vanilla CoT Guided search + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) LLM as critic + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) ϕ-Decoding + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) Vanilla CoT Guided search + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) LLM as critic + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) ϕ-Decoding + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) Table 1: Main results. The best results are highlighted in bold. Acc. denotes pass@1 rate and #Tokens denotes the backbone models average token usage for each query, more details concerning external model token usage is in Appendix C.1. We also report the delta compared to Per-Step Scale method, including the accuracy difference and the percentage of saved tokens. Red indicates worse performance, while green indicates better performance. Here, denotes that lower values are better, whereas means higher values are preferable. Baselines CoT. Standard stepwise reasoning without scaling. Per-Step Scale. Test-time scaling methods that scale the computation for each step. Avg uncertainty. Average the uncertainty across all generated steps Ren et al. (2022); Manakul et al. (2023); Huang et al. (2024) to represent the overall uncertainty of the reasoning process, then scale steps with uncertainty higher than this average. SMART. Following the original work by Kim et al. (2025), the backbone model generates reasoning steps autonomously. If the token-level confidence (TLC) falls below predefined threshold, we apply TTS methods. 7 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models MATHAIME24 AIME25 GPQA-diamond Acc. #Tokens Acc. #Tokens Acc. #Tokens Acc. #Tokens Acc. Avg. #Tokens Qwen3-1.7B Vanilla CoT 69.20 1,047 15.00 3,868 7.50 3, 26.26 1,086 29.49 - 2,488 - Thinking Mode + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) 87.60 88.80 89.60 89.20 5,841 4,528 5,214 5,041 41.83 42.50 45.00 45.83 14,832 14,940 17,502 16, 33.33 30.83 32.50 35.83 16,215 16,157 17,873 15,594 Qwen3-4B 38.89 39.39 38.38 39.90 6,032 5,819 7,678 5,231 - 50.42 50.38 (-0.04) 51.37 (+0.95) 52.69 (+2.27) - 10,730 10.361 (-3.44%) 12,067 (+12.46%) 10,557 (-1.61%) Non-Thinking Mode 79.40 25.83 2,422 15.00 2,087 39.90 40.03 - 1,473 - Thinking Mode + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) 89.20 93.80 94.00 94.00 4,598 3,846 4,932 3,607 70.83 65.00 70.00 75.00 13,103 12,359 15,088 12,117 57.50 51.67 58.33 58.33 16,588 15,267 17,884 15, Qwen3-8B 51.01 52.53 53.53 54.04 6,547 5,561 8,024 4,801 - 67.14 65.75 (-1.39) 68.97 (+1.83) 70.34 (+3.20) - (-9.32%) 10,209 9,258 11,482 (+12.47%) (-12.51%) 8,932 Non-Thinking Mode 81.40 1131 31.50 3, 19.17 3,097 39.90 859 42.99 - 2,212 - Thinking Mode + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) 94.60 90.60 93.00 93.80 5,227 4,385 5,482 5, 68.33 63.33 65.83 71.67 13,761 11,689 12,899 14,390 65.83 53.33 57.50 68.33 17,053 12,634 14,659 17,103 56.06 55.05 54.04 57.58 6,910 6,579 8,726 6, - 71.21 65.58 (-5.63) 67.59 (-3.62) 72.85 (+1.64) - (-17.85%) 10,738 8,822 10,442 (-2.76%) 10,742 (+0.06%) Table 2: Results of Thinking Switch. Vanilla CoT represents the non-thinking mode. Per-Step Scale here denotes the thinking mode of Qwen3 model. Implementation Details We conduct all experiments on different models from Qwen3series Yang et al. (2025a), including Qwen3-1.7B, Qwen3-4B, and Qwen3-8B. The hyperparameter α and γ are both set to 0.9 as default if no additional explanation is provided. For more implementation details, please to Appendix B. 4.2 Main Results Table 1 and Table 2 report 4 widely adopted reasoning benchmarks across 3 sizes of models. MUR consistently outperforms strong baselines. The main results demonstrate the superior token saving capacity of MUR in most scenarios, and consistently improves the accuracy against Per-Step Scale methods (from 0.62% to 3.37%). This benefits from reducing overthinking on simple steps, while keeping deliberate optimization for difficult steps. MUR outperforms average uncertainty and SMART on both token usage and accuracy (2.20%, 1.88% for average, respectively). Although the two baselines generate fewer tokens than MUR in few cases, the accuracy drops even lower than Per-Step Scale. This indicates that they cant well evaluate the reasoning process, which laterally proves the superiority of MUR . Figure 3: Random scaling accuracy. For each dataset, we average the three test-time scaling reasoning methods (Guided search, LLM as critic, ϕ-decoding). ticks stand for different sizes of Qwen3-series models. stands for accuracy. External critic reduces backbone token usage. For LLM as critic setting, we observe that the token usage saving of the backbone is not as significant as other test-time scaling 8 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models methods. MUR generates 45.98% more tokens than Per-Step Scale method when using the backbone of Qwen3-1.7B, and the Per-Step Scale method even generates fewer tokens than CoT based on Qwen3-1.7B. This token usage reverse origins from the critic of external models, which contains hints for generating the next step, so the backbone generates fewer tokens. Table 1 only records the tokens generated by the backbone. To further demonstrate the token usage saving capacity of MUR , we report the token usage of both the backbone and the external model in Appendix C.1, from which we can observe that MUR is still more efficient than Per-Step Scale methods. Mur can generalize to LRMs. Large reasoning models (LRMs) optimize performance by generating overlong reasoning path, leading excessive token usage. To overcome this, we directly output steps detected as needing no scaling by MUR , which avoids the heavy computes introduced by thinking process. More implementation details are in Appendix B. Results in Table 2 demonstrate that MUR outperforms all three baselines, improving accuracy from 1.64% to 3.20% against Per-Step Scale baseline, which indicates that MUR adaptively identifies key steps during reasoning. Effectiveness on both reasoning models  (Table 2)  and non-reasoning models  (Table 1)  further validates the generality of MUR ."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we firstly present scaling law of γ-control (Sec. 5.1), which well balance the performance and budget. Then we analysis the number of steps and token usage (Sec. 5.2), reveling that MUR only scales minor portion of steps. Finally, we randomly scale some steps (Sec. 5.3), laterally demonstrating that MUR can identify crucial steps. Additional analysis of the impact of hyperparameter α and case study can be found in Appendix C. 5.1 Scaling Law of γ-control Figure 4: The scaling law of hyperparameter γ. We analyze MATH-500 based on Qwen-1.7B. The axis stands for different values of γ. (a) reports the average of Guided search, LLM as critic and ϕDecoding. (b) reports the scaling law of thinking switch. γ-control well balance performance and budget. The hyperparameter γ adjusts the detection process in Eq. (10), with lower γ leading to less scaling and less token usage. We report this analysis in Figure 4. The accuracy improves with more token usage, indicating that we can control the reasoning performance by adjusting hyperparameter γ. It is worth noting that γ = equivalents to Per-Step Scale reasoning, whose accuracy drops lower with excessive token usage. More detailed results can be found in Appendix C.2. 5.2 Step and Token Usage Analysis MUR only scales minor portion of steps. We report the number of reasoning steps and corresponding token usage under different settings in Figure 5. With the guidance of MUR , the backbone generates 4.23-6.24 steps for average, scaling only 0.41-1.15 steps for each query. This indicates that for some simple questions, the backbone directly outputs the whole reasoning process, without any scaling. MUR exhibits superior token efficiency. MUR significantly reduces Per-Step Scales token usage over 50%. Qwen3-4B generates the least tokens, while Qwen3-8B generates the most tokens, indicating that the former is more efficient and suitable for real-world scenarios. 9 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models More scaling leads to fewer token usages. Interestingly, the number of steps is inversely proportional to total token usagemore scaling results in fewer steps, as illustrated in Figure 5. For instance, PerStep Scale methods allocate the most tokens yet generate the fewest steps on average. This occurs because test-time scaling brings the backbone model closer to the final answer, reducing the need for future steps. Detailed statistics (Appendix C.3) reveal that harder benchmarks lead to higher proportion of scaled steps. 5.3 Results of Random Scaling Figure 5: Comparison of average steps and token usages across different methods. Under each setting, the result is the average across all four benchmarks and the three TTS methods (Guided search, LLM as critic, ϕ-decoding). ticks stand for different sizes of Qwen3series models. For MUR , we report both the scaled steps and not scaled steps. MUR identifies crucial steps to scale. For each setting, we randomly scale the same number of steps as in Table 1 (see Appendix C.3 for details). Results in Figure 3 show the average accuracy across three PerStep Scale methods. Random scaling underperforms Per-Step Scale by 1.02% on average, demonstrating that scaling arbitrary steps harms performance. In contrast, MUR using the same number of scaled stepsoutperforms both random and Per-Step Scale methods, proving its ability to identify and prioritize key reasoning steps."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we emphasize the key insight that off-the-shelf test-time scaling methods allocate excessive token usage, leading to degradation of both effectiveness and efficiency. To address this, we propose MUR , training-free algorithm for LLM reasoning, which can be orthogonally combined with other test-time scaling methods. We only scale key steps detected by MUR . Theoretical analysis and extensive experiments on both LLMs and LRMs demonstrate the superiority of MUR . Future efforts could work on adaptively deciding how much computes to be applied on different reasoning steps. 10 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models"
        },
        {
            "title": "References",
            "content": "Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin Edelman. Distinguishing the knowable from the unknowable with language models. In Proceedings of the 41st International Conference on Machine Learning, pp. 503549, 2024. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. Inside: Llms internal states retain the power of hallucination detection. In ICLR, 2024a. Jiuhai Chen and Jonas Mueller. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 51865200, 2024. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. CoRR, 2024b. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. CoRR, 2024. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. Xiang Gao, Jiaxin Zhang, Lalla Mouatadid, and Kamalika Das. Spuq: Perturbation-based uncertainty quantification for large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 23362346, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, and Yang Zhang. Decomposing uncertainty for large language models through input clarification ensembling. In Proceedings of the 41st International Conference on Machine Learning, pp. 1902319042, 2024. Xinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed Hassani, Insup Lee, Osbert Bastani, and Edgar Dobriban. Uncertainty in language models: Assessment through rank-calibration. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 284312, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. CoRR, 2024. Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, and Furu Wei. Think only when you need with large hybrid-reasoning models. arXiv preprint arXiv:2505.14631, 2025. 11 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models Yujin Kim, Euiin Yi, Minu Kim, Se-Young Yun, and Taehyeon Kim. Guiding reasoning in small language models with llm assistance. arXiv preprint arXiv:2504.09923, 2025. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, and Xian-Ling Mao. Criticeval: Evaluating large-scale language model as critic. Advances in Neural Information Processing Systems, 37:6690766960, 2024. Yansi Li, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Qiuzhi Liu, Rui Wang, Zhuosheng Zhang, Zhaopeng Tu, Haitao Mi, et al. Dancing with critiques: Enhancing llm reasoning with stepwise natural language self-critique. CoRR, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. Non-myopic generation of language models for reasoning and planning. In The Thirteenth International Conference on Learning Representations, 2024. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. Generalreasoner: Advancing llm reasoning across all domains. arXiv:2505.14652, 2025. URL https://arxiv.org/abs/2505.14652. Potsawee Manakul, Adian Liusie, and Mark Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 90049017, 2023. Alexander Nikitin, Jannik Kossen, Yarin Gal, and Pekka Marttinen. Kernel language entropy: Fine-grained uncertainty quantification for llms from semantic similarities. Advances in Neural Information Processing Systems, 37:89018929, 2024. Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):145151, 1999. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level googleIn First Conference on Language Modeling, 2024. URL https: proof q&a benchmark. //openreview.net/forum?id=Ti67584b98. Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter Liu. Out-of-distribution detection and selective generation for conditional language models. In The Eleventh International Conference on Learning Representations, 2022. Gaurang Sriramanan, Siddhant Bharti, Vinu Sankar Sadasivan, Shoumik Saha, Priyatham Kattakinda, and Soheil Feizi. Llm-check: Investigating detection of hallucinations in large language models. Advances in Neural Information Processing Systems, 37:3418834216, 2024. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. Sree Harsha Tanneru, Chirag Agarwal, and Himabindu Lakkaraju. Quantifying uncertainty in natural language explanations of large language models. In International Conference on Artificial Intelligence and Statistics, pp. 10721080. PMLR, 2024. 12 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 54335442, 2023. Bin Wang, Chengwei Wei, Zhengyuan Liu, Geyu Lin, and Nancy Chen. Resilience of large language models for noisy instructions. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1193911950, 2024a. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024b. Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, and Rui Wang. Sampling-efficient test-time scaling: Self-estimating the best-of-n sampling in early decoding. arXiv preprint arXiv:2503.01422, 2025. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Scaling inference computation: Compute-optimal inference for problem-solving with language models. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS, volume 24, 2024. Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025a. Zhiqiu Xia, Jinxuan Xu, Yuqian Zhang, and Hang Liu. survey of uncertainty estimation methods on large language models. arXiv preprint arXiv:2503.00172, 2025b. Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Jun Liu, Qika Lin, and Zhiyong Wu. ϕ-decoding: Adaptive foresight sampling for balanced inference-time exploration and exploitation. arXiv preprint arXiv:2503.13288, 2025a. Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Qiushi Sun, Kanzhi Cheng, Junxian He, Jun Liu, and Zhiyong Wu. Genius: generalizable and purely unsupervised self-training framework for advanced reasoning. arXiv preprint arXiv:2504.08672, 2025b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Junjie Yang, Ke Lin, and Xing Yu. Think when you need: Self-adaptive chain-of-thought learning. arXiv preprint arXiv:2504.03234, 2025b. Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning. CoRR, 2025c. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Zhaojian Yu, Yinghao Wu, Yilun Zhao, Arman Cohan, and Xiao-Ping Zhang. Z1: Efficient test-time scaling with code. CoRR, 2025. Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Li Xiu, et al. Genprm: Scaling test-time compute of process reward models via generative reasoning. CoRR, 2025. Zhanke Zhou, Rong Tao, Jianing Zhu, Yiwen Luo, Zengmao Wang, and Bo Han. Can language models perform robust reasoning in chain-of-thought prompting with noisy rationales? Advances in Neural Information Processing Systems, 37:123846123910, 2024. 13 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models"
        },
        {
            "title": "A Theoretical Derivation",
            "content": "A.1 The Formulation of Momentum Uncertainty Proposition 1: Momentum uncertainty is an exponentially weighted sum of step-level uncertainties, emphasizing recent steps and fading earlier ones. Proof. Recursive expansion of Mt: Mt = αMt1 + (1 α)mt = α (αMt2 + (1 α)mt1) + (1 α)mt = α2 Mt2 + α(1 α)mt1 + (1 α)mt ... = αt M0 + (1 α) i=1 αtimi. Substituting M0 = 0, we obtain: Mt = (1 α) i=1 αtimi. (11) (12) This shows Mt assigns weights αti to historical mi, emphasizing recent uncertainties while smoothing early fluctuations. Let the average probability of the models output at step t, mt follow mt = mt1 ηgt, where gt denotes the custom update term at step t. The momentum mechanism implicitly applies decayed weights 1 αti to historical updates. Define cumulative updates mt = t1 i=1 gi. Substituting into Eq. (5): Mt = αMt1 + (1 α)mt = αtm1 + (1 α) i=1 αtimi (from Eq. (11)) = m1 t1 i=1 (cid:16) 1 αti(cid:17) gi. (13) Compared to the baseline update mt = m1 t1 weights 1 αti that decay exponentially with step distance i. i=1 gi, the momentum term introduces From the above proof, we have two properties as following: Property 1: Momentum Uncertainty is the Exponential Weighting of Historical Uncertainties. Property 2: Momentum Uncertainty has Gradient Descent Equivalence with Decaying Weights. A.2 Proof of Low Variance Proposition 2: Momentum uncertainty brings more stable estimation, with lower variance than step-level uncertainty. Let Var() denotes the variance calculation, Var(Mt) < Var(mt). The momentum uncertainty Mt is defined by Eq. (12) as: Mt = (1 α) i=1 αtimi, α (0, 1). 14 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models Assume historical uncertainties mt contain independent noise: mt = µt + ϵt, ϵt (0, σ2 ). (14) µ is the ideal value without variance and bias that can represent the current reasoning and overall reasoning path status. However, it is impractical to get µ, and we can only get step-level uncertainty which contains noise. Therefore, in our method, we aggregate each step-level uncertainty as momentum uncertainty to represent the overall reasoning process. (15) (16) (17) Var(Mt) = (1 α)2 i=1 α2(ti)σ2 = (1 α)2σ2 t i=1 α2(ti). Let = i. The summation becomes finite geometric series: i=1 α2(ti) = = α2j t1 j=0 1 α2t 1 α2 . Substituting Eq. (16) into Eq. (15): Var(Mt) = (1 α) 1 α2t 1 + α σ2 . Because 1 α2t < 1 and 1 + α > 1, we have: 1 α2t 1 + α < 1. Therefore: Var(Mt) < (1 α) σ2 1 σ2 , the variance has an upper bound of σ2 . Fix t, let α 1: lim α1 Var(Mt) = lim α1 (1 α)σ2 1 α2t 1 + α = 0 σ2 0 2 = 0. This shows that when α is sufficiently close to 1, the variance can be arbitrarily small. From the expression: Var(Mt) = σ2 1 α2t 1 + α (1 α) (cid:125) (cid:123)(cid:122) (cid:124) denoted as (α,t) , When α 0+: When α 1: (α, t) (1 0) 1 0 1 + = 1. (α, t) 0. For example, when we set α = 0.9, = 20: (0.9, 20) = (1 0.9) 1 (0.9)40 1 + 0.9 0.1 0.985 1.9 0.0518. The minimum occurs when α approaches 1, but remains strictly greater than 0. 15 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models A.3 Proof of Convergence Proposition 3: With the steps increasing, momentum uncertainty offers better convergence. Let Bias() denotes the bias calculation function, then Bias(Mt) According to Eq. (14), we have mt (µt, σ2 converge 0. ). Therefore, Mt ((1 α) i=1 αtiµi, (1 α) i=1 α2(ti)σ2 ). As claimed in Sec. A.2, µ is the ideal value which can best represent the overall reasoning process without variance and bias, so we define the bias of Mt as: Then we have: Bias(Mt) = Mt µt. E[Bias(Mt)] = E[Mt µt] = E[Mt] µt = (1 α) i=1 αtiµi µt. (18) (19) Consider non-stationary signals with drift: µt = µt1 + δt. The term δt represents the drift term at time t, which quantifies the change in µt relative to the previous time point µt1. During inference, each step is generated based on previous steps, which is well compatible with this assumption. Since α (0, 1), we have αt (0, 1), 1 αt (0, 1). (1 α) i=1 αti = (1 α) 1 αt 1 α = 1 αt. Then we have: and since µt > 0, it follows that: (1 α) i=1 αti < 1, µt < (1 α) i=1 αtiµt. E[Bias(Mt)] = (1 α) αtiµi µt i=1 i=1 i= < (1 α) = (1 α) αtiµi (1 α) i=1 αtiµt αti (µi µt). Substitute = i: E[Bias(Mt)] < (1 α) t1 j=0 αj (µtj µt). 16 (20) (21) (22) MUR: Momentum Uncertainty Guided Reasoning For Large Language Models As time increases, in this asymptotic regime, the model is posited to have exhaustively explored the entirety of its accessible state space and potential cognitive trajectories. Consequently, the associated epistemic uncertainty converges to stationary value. Assume µi converges exponentially to µ as timestamp increases: µi µ Dβi, 1, for > 0 and β (0, 1). The constant > 0 is scale factor for the error bound, representing the magnitude of the initial deviation, which is independent of the index i. The constant β is the exponential convergence rate, governing the decay speed of the error µi µ. Apply the triangle inequality: µtj µt = µtj µ + µ µt µtj µ + µt µ Dβtj + Dβt, E[Bias(Mt)] (1 α) (1 α) = (1 α) t1 j=0 t1 j=0 t1 j=0 αj µtj µt (cid:16) Dβtj + Dβt(cid:17) αj Dβt (cid:19) )j + αj (cid:18) ( α β = (1 α)Dβt t1 j=0 (cid:18) ( α β )j + αj (cid:19) . The second series in Eq. (24) is bounded by: t1 j=0 αj = 1 αt 1 α 1 1 α . Define = α/β. For the first series in Eq. (24), consider two cases: When α < β (r < 1): t1 j=0 rj = 1 rt 1 1 1 = β β α . When α β (r 1): t1 j=0 rj = rt 1 rt 1 = αt βt1(α β) . Substitute the series bounds: When α < β: E[Bias(Mt)] (1 α)Dβt (cid:18) β β α = Dβt (cid:18) (1 α)β β α + 1 (cid:19) 1 1 α + (cid:19) . 17 (23) (24) (25) MUR: Momentum Uncertainty Guided Reasoning For Large Language Models When α β: E[Bias(Mt)] (1 α)Dβt (cid:18) αt βt1(α β) + + (1 α)βt = = = Dαt (cid:20) (1 α)αtβ α β (cid:18) (1 α)β α β (cid:18) (1 α)β α β (cid:18) (1 α)β α β (cid:19) (cid:19) αt + βt αt + αt (cid:19) . + 1 In both cases, we have the derived bound: E[Bias(Mt)] Kρt, where ρ = max(α, β) and is constant: = (cid:18) (1 α)β α β (cid:19) . + 1 (cid:19) (cid:21) 1 1 α 1 1 α (26) (27) It demonstrates that the bias converges exponentially to zero. This convergence holds because ρ = max(α, β) < 1, since both α (0, 1) and β (0, 1). The constant = is finite and independent of t. And the exponential decay ρt 0 occurs at αβ + 1 geometric rate. (cid:16) (1α)β (cid:17) In practice, we set α close to 1, making (1 α) close to 0, minimizing K. Combined with ρt, ρ < 1, this ensures rapid convergence of Kρt 0 as increases. While step-level uncertainty introduces bias, momentum-weighted uncertainty Mt effectively dampens oscillatory behavior during inference, enhancing stability. A.4 Proof of Superiority over Simple Average Proposition 4: Momentum uncertainty is superior to average uncertainty on both variance and bias. According to Eq. (6), Mt is defined as: Mt = (1 α) be: i=1 αtimi, and let the simple average Mt = 1 i=1 mi. (28) We establish Mts superiority through the following analysis. Comparison between Momentum Average Variance and Simple Average Variance. Historical uncertainties mt contain independent noise is claimed in Eq. (14). The variance of Mt is derived in Eq. (17). The vast majority of inference steps are less than twenty (as illustrated in Table 4), so is set to 20. For 20 and α (0, 1), α2t 0. Thus: For Mt: Var(Mt) σ2 (1 α)2 1 α = σ2 1 α 1 + α . Var( Mt) = 1 t2 i= σ2 = σ2 . 18 (29) (30) MUR: Momentum Uncertainty Guided Reasoning For Large Language Models When α 1 : which implies Var(Mt) < Var( Mt). Momentum achieves superior noise suppression through exponentially decaying weights. 1 α 1 + α < 1 for 20, (31) Comparison between Momentum Average Bias and Simple Average Bias. For the momentum uncertainty Mt illustrated in Eq. (27): For the simple average Mt: E[Bias(Mt)] Kρt. Bias( Mt) = Mt µt = 1 i=1 (mi µt) E[Bias( Mt)] = 1 t i=1 (µi µt) = 1 i=1 j=i δj. (32) (33) It accumulates historical drifts and scales linearly with t. Under non-stationary environments with additive noise, the momentum-based estimator Mt, exhibits superior variance-bias tradeoff and convergence properties compared to the baseline estimator Mt. A.5 Proof of Dynamic Compute Scaling Proposition 5: Optimization should be triggered with high confidence when the step-level uncertainty exhibits significant deviation from the momentum-based uncertainty. Problem Formulation and Notation Let mt denote the uncertainty of the models output at step + 1, and Mt1 represent the momentum uncertainty defined as an exponentially weighted sum, and α (0, 1) be the momentum rate. The decision rule for computes scaling is formulated as: exp(mt) > exp(Mt1)/γ. boundary violation is flagged when this inequality holds, triggering corrective test-time scaling. We formalize the robustness guarantee below. Based on the following two lemmas, we establish that the misjudgment probability of historical momentum uncertainty Mt1 exceeding the threshold τt = mt + ln γ approaches zero, demonstrating: When the scaling condition exp(mt) > exp(Mt1)/γ holds, the model identifies abnormal elevation in current uncertainty mt with near-certain confidence, thereby efficiently triggering resource scaling. We now provide theoretical bound on the probability that stable reasoning step is mistakenly flagged as uncertain. Lemma 1: Chernoff Bound for Single Random Variable. By using the distribution of random variables, more precise boundary is provided for the large deviation probability of random variables. Let be real-valued random variable with moment generating function ϕ(s) = E[esX]. For any threshold τ R, the upper tail probability satisfies: P(X τ) inf s>0 esτϕ(s). is sub-Gaussian with parameter ˆσt, ϕ(s) esν+ (cid:18) P(X τ) exp s2 ˆσ2 , then: (cid:19) (τ ν)2 2 ˆσ2 , where ν = E[X]. τt = mt + ln(γ), γ (0, 1). 19 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models Lemma 2: Hoeffdings inequality. Hoffdings inequality provides the upper limit of the probability that the sum of random variable deviates from its expected value. Assume that for each i, Xi [ai, bi]. Consider the sum of these random variables: Sn = i=1 Xi = X1 + X2 + X3 + + Xn1 + Xn. Then Hoeffdings inequality states that for all > 0: P(Sn E[Sn] t) exp (cid:18) P(Sn E[Sn] t) 2 exp 2t2 i=1(bi ai)2 (cid:19) . 2t2 i=1(bi ai)2 n (cid:18) (cid:19) . Here E[Sn] denotes the expectation of Sn. Let the momentum uncertainty sequence Mt1 be an exponentially weighted sum of historical step-level uncertainties {mi}t1 i=1: Mt1 = t1 i=1 ωimi, ωi = αt1i(1 α), t1 i=1 ωi = 1, where mi [0, 1] are bounded random variables. The threshold has been defined above, which is: τt = mt + ln γ. When the scaling condition exp(mt) > exp(Mt1)/γ holds, applying Lemma 1, we have: P(Mt1 τt) exp (cid:18) (τt ˆνt1)2 2 ˆσt (cid:19) , where ˆνt1 = E[Mt1] , and the decay rate is controlled by α. Proof. By the exponential smoothing definition: Mt1 = t1 i= ωimi, ωi = (1 α)αt1i, where mi [0, 1] are independent or weakly dependent random variables. Define Xi = ωimi, which satisfies: Xi [0, ωi]. bi ai = ωi 0 = ωi. Applying Lemma 2: [exp (Mt1 E[Mt1] ζ)] exp (cid:32) (cid:32) = exp (cid:33) t1 2ζ2 i=1 (bi ai)2 (cid:33) 2ζ2 t1 i=1 ω2 . Mt1 is sub-Gaussian with parameter: ˆσ2 t1 = 1 4 i=1 ω2 (cid:32) . Thus: (cid:33) ζ2 2 ˆσ2 t1 P(Mt1 ˆνt1 ζ) exp 20 . (34) MUR: Momentum Uncertainty Guided Reasoning For Large Language Models Substitute ζ = τt ˆνt1: (cid:32) P(Mt1 τt) exp (cid:32) = exp (cid:33) (τt ˆνt1)2 t1 2 1 i=1 ω2 4 (τt ˆνt1)2 4 ((1 α)2 t2 2 1 j=0(α2)j) (cid:33) = exp (cid:18) (cid:18) = exp = exp ) (τt ˆνt1)2 4 ((1 α)2 1α2(t1) 2 1 1α2 (cid:19) 2(τt ˆνt1)2(1 + α) (1 α)(1 α2(t1)) 2(mt + ln γ ˆνt1)2(1 + α) (1 α)(1 α2(t1)) (cid:19) . (35) Since 1 α2 = (1 α)(1 + α), α (0, 1): t1 i=1 ω2 = (1 α) 1 α2(t1) 1 + α 1 α 1 + α . Substituting the weight sum upper bound: P(Mt1 τt) exp (cid:18) 2(mt + ln γ ˆνt1)2(1 + α) 1 α (cid:19) . (36) As those in practice, we set α = 0.9 in the probability bound here: P(Mt1 τt) exp Define the confidence parameter ε as: (cid:16) = exp (cid:18) 2(mt + ln γ ˆνt1)2(1 + α) 1 α 38(mt + ln γ ˆνt1)2(cid:17) 0. (cid:19) ε = exp (cid:18) 2(ln γ + mt ˆνt1)2(1 + α) 1 α (cid:19) . This exponential decay ensures that deviations above τt = ln γ + mt are asymptotically improbable. With α = 0.9, the bound becomes: ε = exp (cid:0)38(ln γ + mt ˆνt1)2(cid:1) 0, P(Mt1 τt) = ε 0, (cid:18) P(Mt1 < τt) = exp(mt) > (cid:19) exp(Mt1) γ = 1 ε. This validates the scaling decision: The scaling condition exp(mt) > exp(Mt1)/γ holds with confidence 1 ε. This result establishes generalization error control for exponential smoothing: The weighted average Mt1 converges to the expected uncertainty level, while the scaling condition controls abrupt deviations via tail probability analysis."
        },
        {
            "title": "B Implementation Details",
            "content": "Implementation of Main Experiments The temperature is set to 0.6 for all experiments. In practice, we do not scale the first step. Because there is no valid momentum uncertainty when identifying the first step. For math problem evaluation, We conduct all of our experiments based on vLLM Kwon et al. (2023) reasoning tool. We use general reasoner Ma et al. (2025) for math problem evaluation. 21 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models Implementation of Generating One Step For generating one step, we prompt the backbone LLM to automatically define one step. Specifically, we add Always end your solution with the phrase the answer is followed by your final answer. Start your solution with Step {stepidx}: to the end of each input query. For the update of momentum uncertainty, we use the step-level uncertainty of optimized step. Implementation of Thinking Switch Based on the switch interface between non-thinking mode and thinking mode provided by Qwen3-series, we propose to reduce token usage for large reasoning models with MUR . Specifically, we use non-thinking mode as default reasoning method, and switch to thinking mode when current step is detected as needing scaling by MUR . We set γ to 0.9, 0.8, 0.7 for MATH, AIME, GPQA-diamond, respectively. To avoid overthinking in each step, we limit the max thinking length to 2048 and extract all the completed sentences. Additionally, we add Okay, so need to to the beginning of each prompt to correctly elicit thinking in thinking mode."
        },
        {
            "title": "C More Experiment Results",
            "content": "C.1 Token Usage We report the token usage of both the backbone and the external model in Table 3. There is no external model under ϕ-Decoding setting, so we only report the token usage under Guided search and LLM as critic settings. In Table 1, MUR generates more tokens in some cases. This is because we only record the backbone token usage in Table 1. However, in Table 3, by adding up both backbone token usage and external model token usage, we can observe in the last column that MUR consistently generates fewer tokens than Per-Step Scale method, validating the token saving capacity of MUR . Furthermore, the trend of token usage of the Guided search setting in Table 3 is compatible with those in Table 1. C.2 Flexible Control with Hyperparameter γ To further demonstrate the flexible control using hyperparameter γ, we report the detailed information concerning three model sizes and four test-time scaling methods (Guided search, LLM as critic, ϕ-Decoding, thinking switch) on MATH-500 in Figure 6. It can be observed that by increasing γ, the reasoning accuracy would improve along with the token usage. It is worth noting that in some scenarios, we observe performance degradation when we set γ to 0.9. This is consistent with our main findings: the reasoning performance drops with excessive reasoning token usage. In other words, we scale abundant steps in these scenarios. And the accuracy of Per-Step Scale methods drops even lower with more token usage. Additionally, we observe that MUR outperforms Per-Step Scale in most scenarios. In practice, we set γ to 0.9 as the default. C.3 Number of Steps We report the number of steps generated by the backbone model and the number of scaled steps with MUR in Table 4. Additionally, we calculate the percentage of scaled steps on each benchmark. For MATH-500, AIME24, AIME25, GPQA-diamond, the percentage is 8.38%, 13.19%, 17.83%, 13.75%, respectively. We can infer that among them, more difficult benchmark leads to higher percentage of scaled steps. C.4 Impact of α The hyperparameter α controls the update of momentum uncertainty, with lower α leading to more intense updates. We report the impact of changing α in Figure 7. We can observe that MUR outperforms vanilla in most cases, which demonstrates the insensitivity and effectiveness of MUR . For α = 0.1 setting, the momentum uncertainty changes too fast 22 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models MATH-500 Ext Sum Bac Bac AIME24 Ext AIME25 Ext Sum Sum Bac Qwen3-1.7B GPQA-diamond Ext Sum Bac Bac Avg. Ext Sum CoT 1, - 1,047 3,868 - 3,868 3, - 3,952 1,086 - 1,086 2, - 2,488 Guided search + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) LLM as critic + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) 3,460 2,398 3,128 1,321 1,098 1,019 878 902 3,186 1,565 2.049 320 1,271 1,075 670 337 6,646 3,963 5,177 1,641 2,369 2,094 1,548 1, 22,498 8,243 11,887 6,460 23,055 3,219 10,722 896 45,553 11,462 22,609 7,356 17,882 8,662 15,845 5,919 17,859 3,558 13,112 1,263 35,741 12,220 28,957 7, 4,108 5,561 2,945 6,259 1,877 611 1,676 483 5,985 6,172 4,621 6,742 3,609 4,433 3,014 5,319 1,737 660 1,542 542 5,346 5,093 4,556 5, Qwen3-4B 6,739 3,404 3,825 2,005 892 1,417 1,446 1,693 9,258 3,512 5753 1,502 2,249 2,001 763 1,282 15,997 6,916 9,578 3, 12,645 5,677 8,671 3,926 13,339 2,964 7,909 995 25,984 8,641 16,580 4,921 3,141 3,418 2,209 2,975 2,427 3,108 2,071 3,543 1,783 1,087 1,163 4,210 4,194 3,234 4,204 CoT Guided search 772 - 2,422 - 2,422 2,087 - 2, 612 - 612 1,473 - 1, + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) 3,048 1,911 2,476 824 LLM as critic + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) 777 741 813 745 3,346 1,845 2,212 1,373 957 855 443 6,394 3,756 4,688 1,089 2,150 1,698 1,668 1,188 13,445 6,962 11,087 2,791 13,229 3,416 13,784 1,863 26,674 10,378 24,871 4, 12,324 8,688 11,604 4,009 15,464 3,836 13,830 1,702 27,788 12,524 25,434 5,711 3,517 1,354 2,116 929 4,477 2,933 3,611 4,561 2,757 622 1,089 1, 7,234 3,555 4,700 6,454 2,983 3,005 2,934 2,531 1,867 900 1,094 1,447 4,850 3,905 4,028 3,978 737 804 724 699 Qwen3-8B 6,437 2,483 3409 641 2,462 1,795 320 266 9,954 3,837 5,525 1,570 3,199 2,599 1,044 965 8,084 4,729 6,821 2,138 2,244 1,871 2,021 2, 9,619 2,895 8,309 1,118 2,115 1,069 839 1,012 17,703 7,624 15,130 3,256 4,359 2,940 2,860 3,146 CoT 1, - 1,131 3,761 - 3,761 3, - 3,097 859 - 859 2, - 2,212 Guided search + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) LLM as critic + Per-Step Scale + Avg uncertainty + SMART + MUR (ours) 4,069 2,427 3,502 2,607 1,022 1,086 1,167 1,132 3,688 2,037 3,287 1,986 2,025 842 1,160 783 7,757 4,464 6,789 4,593 3,047 1,928 2,327 1, 27,672 11,088 15,333 7,930 18,843 4,139 6,221 1,715 46,515 15,227 21,554 9,645 27,871 10,765 17,988 7,972 16,090 4,367 7,094 2,026 43,961 15,132 25,082 9, 4,472 5,029 4,245 4,531 2,612 1,137 1,769 1,953 7,084 6,166 6,014 6,484 3,886 4,705 4,125 3,678 2,691 1,392 2,177 2,120 6,577 6,097 6,302 5, 4,252 2,213 3,797 3,122 1,172 1,375 1,069 1,007 7,468 3,382 6,135 4,524 3,102 1,588 2,366 2,211 11,720 5,595 9,932 7,646 15,966 6,623 10,155 5, 11,522 3,481 5,684 2,563 27,488 10,104 15,839 7,971 4,274 2,963 3,435 3,218 2,638 3,049 2,652 2,587 2,608 1,240 1,868 1,767 5,246 4,289 4,520 4, Table 3: Token usage of both backbone and external model. Bac stands for backbone model, Ext stands for external model, and the sum of them is denoted as Sum. means better for lower values. Figure 6: Detail scaling law of γ. The axis stands for different values of γ. The axis stands for accuracy. Due to the reason described in Appendix C.1, we additionally report the external model token usage (denoted as Critic Tokens) under LLM as critic setting to comprehensively reflect the overall computes. to well represent the overall estimation of query and generated steps, so the accuracy is relatively lower than other settings. In practice, we set α = 0.9 as default. 23 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models Datasets MATH-500 AIME24 AIME25 Total Scaled Total Scaled Total Scaled GPQA-diamond Total Scaled Avg Total Scaled CoT 5.33 - 8. - 8.40 - 6.41 - 7. - Qwen3-1.7B Guided search + Per-Step Scale + MUR (ours) LLM as critic + Per-Step Scale + MUR (ours) ϕ-Decoding + Per-Step Scale + MUR (ours) 2.89 5.31 4.35 5.86 2.97 5.80 2.89 0. 4.35 0.40 2.97 0.39 4.03 6.60 4.40 6.63 3.10 7.57 4.03 0. 4.40 0.87 3.10 0.53 2.97 6.27 3.87 6.00 3.67 6.53 2.97 0. 3.87 0.97 3.67 0.70 4.55 7.02 3.93 5.77 3.91 5.59 4.55 0. 3.93 0.81 3.91 0.76 3.61 6.30 4.14 6.07 3.41 6.37 3.61 0. 4.14 0.76 3.41 0.60 Qwen3-4B CoT 5.84 - 5.70 - 5.00 - 5.57 - 5.53 - Guided search + Per-Step Scale + MUR (ours) LLM as critic + Per-Step Scale + MUR (ours) ϕ-Decoding + Per-Step Scale + MUR (ours) 2.73 4.31 3.83 4.36 2.77 4.38 2.73 0. 3.83 0.18 2.77 0.19 3.20 4.20 3.80 5.00 3.43 5.23 3.20 0. 3.80 0.33 3.43 0.60 3.80 4.40 4.50 4.07 4.63 4.30 3.80 0. 4.50 0.57 4.63 0.30 2.71 3.59 2.47 3.31 3.10 3.89 2.71 0. 2.47 0.35 3.10 1.02 3.11 4.13 3.65 4.19 3.48 4.45 3.11 0. 3.65 0.36 3.48 0.53 Qwen3-8B CoT 7.45 - 10.00 - 12.33 - 6.90 - 9.17 - Guided search + Per-Step Scale + MUR (ours) LLM as critic + Per-Step Scale + MUR (ours) ϕ-Decoding + Per-Step Scale + MUR (ours) 3.27 5.32 5.01 5.93 3.20 4.45 3.27 0. 5.01 0.55 3.20 1.20 3.80 6.43 4.60 6.53 3.70 4.83 3.80 1. 4.60 1.00 3.70 1.60 3.93 6.43 4.17 6.77 6.07 4.80 3.93 1. 4.17 1.27 6.07 2.27 3.83 5.20 3.92 5.17 3.45 4.32 3.83 0. 3.92 0.67 3.45 2.11 3.71 5.85 4.43 6.10 4.11 4.60 3.71 0. 4.43 0.87 4.11 1.80 Table 4: Total number of steps generated by the backbone and the number of scaled steps with MUR . Figure 7: Impact of changing α. The axis stands for different values of α. The axis stands for accuracy. C.5 Case Study In Figure 8, we conduct case study based on the thinking mode of Qwen3-1.7B. We analyze AIME24 and show the comparison between MUR and Per-Step Scale. We can observe that when MUR faces high uncertainty step, it triggers the thinking process, allocating more 24 MUR: Momentum Uncertainty Guided Reasoning For Large Language Models computes to optimize current steps quality. For simple steps showing low uncertainty, MUR directly output it without thinking. On the contrary, Per-Step Scale thinks for every step, regardless of whether the backbone is confident with the current step or not. Figure 8: Case study. Red denotes the backbone faces high uncertain step. Green denotes key words of thinking."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "National University of Singapore",
        "Peking University",
        "Xian Jiaotong University"
    ]
}