{
    "paper_title": "DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder",
    "authors": [
        "Junyu Chen",
        "Wenkun He",
        "Yuchao Gu",
        "Yuyang Zhao",
        "Jincheng Yu",
        "Junsong Chen",
        "Dongyun Zou",
        "Yujun Lin",
        "Zhekai Zhang",
        "Muyang Li",
        "Haocheng Xi",
        "Ligeng Zhu",
        "Enze Xie",
        "Song Han",
        "Han Cai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen."
        },
        {
            "title": "Start",
            "content": "DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Junyu Chen, Wenkun He, Yuchao Gu, Yuyang Zhao, Jincheng Yu, Junsong Chen, Dongyun Zou, Yujun Lin, Zhekai Zhang, Muyang Li, Haocheng Xi, Ligeng Zhu, Enze Xie, Song Han, Han Cai NVIDIA Equal Contribution https://github.com/dc-ai-projects/DC-VideoGen Abstract: We introduce DC-VideoGen, post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) Deep Compression Video Autoencoder with novel chunk-causal temporal design that achieves 32/64 spatial and 4 temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8 lower inference latency than their base counterparts without compromising quality, and further enable 21603840 video generation on single GPU. 5 2 0 2 9 ] . [ 1 2 8 1 5 2 . 9 0 5 2 : r Figure 1 DC-VideoGen can generate high-quality videos on single NVIDIA H100 GPU with resolutions ranging from 480px, 720px, 1080px, and 2160px. On 21603840 resolution, DC-VideoGen delivers 14.8 acceleration compared to the Wan-2.1-T2V-1.3B model. Corresponding author(s): Han Cai (hcai@nvidia.com). 2025 NVIDIA. All rights reserved. DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder 1. Introduction Video generation has rapidly become central research focus in generative modeling, driven by its potential to enable applications in creative media, digital communication, virtual product visualization, and world simulation for autonomous driving and robotics. Recent advances in diffusion models and large-scale training have made it possible to synthesize high-quality, temporally coherent videos, substantially narrowing the gap between synthetic and real-world content [1, 2]. Industry-scale systems such as Veo3 [3], Kling [4], Wan [5], and Seedance [6] have shown that increasing model size and training data leads to significant improvements in video fidelity. Despite these advances, such models remain extremely computationally demanding in both training and inference, limiting their accessibility and practical deployment. This paper introduces DC-VideoGen, novel post-training framework for accelerating video diffusion models. Figure 1 showcases high-resolution video samples generated by models accelerated with DC-VideoGen. The framework supports video generation at up to 21603840 resolution on single NVIDIA H100 GPU, achieving 14.8 inference speedup over the base model. Moreover, DC-VideoGen dramatically reduces training costs compared with training video diffusion models from scratch. For instance, accelerating Wan-2.1-14B [5] with DC-VideoGen requires only 10 H100 GPU days 230 less than the training cost of Wan-2.1-14B [5]. DC-VideoGen is built upon two core innovations. i) Deep Compression Video Autoencoder. Video data exhibits redundancy across both spatial and temporal dimensions [7, 8]. To mitigate training and inference costs, modern video diffusion models typically employ video autoencoder that compresses raw videos into more compact latent space. However, existing video autoencoders generally achieve only moderate compression ratios (e.g., 8 spatial and 4 temporal), which remain insufficient for generating high-resolution or long-duration videos. In Section 3.2, we introduce the Deep Compression Video Autoencoder (DC-AE-V), which achieves 32/64 spatial compression and 4 temporal compression while preserving high reconstruction quality. The core design is novel chunk-causal temporal modeling approach (Figure 4), which integrates bidirectional information flow within chunks and causal information flow across chunks. This design substantially improves reconstruction quality under deep compression settings (Figure 5) while preserving generalization to longer videos during inference (Figure 3). ii) AE-Adapt-V. After obtaining the deep compression latent space from DC-AE-V, we introduce AEAdapt-V in Section 3.3.2, which efficiently adapts pre-trained video diffusion models to this latent space through lightweight finetuning (Figure 2). The core design is video embedding space alignment stage (Figure 7), which helps recover the base models knowledge and semantics in the new latent space by aligning the patch embedder and output head. These aligned modules provide robust initialization, enabling rapid recovery of the base models quality (Figure 6) through LoRA finetuning (Figure 8). Extensive evaluations on video reconstruction  (Table 1)  , text-to-video generation (Tables 2, 3), and image-tovideo generation  (Table 4)  demonstrate the effectiveness of DC-VideoGen. Across tasks, it consistently provides substantial efficiency gains while achieving comparable or even superior VBench scores. We summarize our main contributions below: We introduce DC-VideoGen, general framework for accelerating video diffusion models. With low-cost post-training finetuning, it delivers substantial efficiency gains in video generation. We introduce DC-AE-V, which drastically reduces the number of latent space tokens while preserving high reconstruction quality and generalization to longer videos. We introduce AE-Adapt-V, which enables rapid adaptation of pre-trained diffusion models to the latent spaces of new autoencoders. DC-VideoGen provides accelerated video diffusion models that preserve the quality of the base models while achieving exceptional efficiency, supporting video generation at up to 21603840 resolution on single GPU. This offers practical advantages for applications requiring efficient video synthesis. Moreover, our accelerated models incur lower fine-tuning and training costs than their base counterparts, enabling faster innovation in the video generation community. 2 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Figure 2 DC-VideoGen Overview. DC-VideoGen is post-training acceleration framework for video diffusion models. It achieves acceleration by transferring models to DC-AE-Vs latent space (Section 3.2) and rapidly recovering the base models quality and semantics using AE-Adapt-V (Section 3.3.2). Compared with training from scratch, DC-VideoGen is far more efficient for example, DC-VideoGen-14B requires only 10 NVIDIA H100 GPU days, 230 reduction in training cost relative to Wan-2.1-14B. 2. Related Work Video Autoencoder. To circumvent the prohibitive costs of training and running diffusion models in pixel space, latent video diffusion models commonly employ video autoencoders [9, 10, 11, 12] to compress raw videos into compact latent space, enabling more efficient generation. typical configuration in recent works [13, 14, 15, 16, 17, 18, 19, 20, 21, 5] uses 8 spatial and 4 temporal compression. Some studies [2, 5, 22] explore 16 spatial compression to further reduce latent token counts. However, these configurations are often insufficient for high-resolution or long video generation. Inspired by the success of 32 spatial compression in image autoencoders [23], [24, 25] investigate video autoencoders with 32 spatial compression ratio, but they suffer from low reconstruction quality or poor generalization to longer videos. In contrast, our DC-AE-V achieves up to 64 spatial compression while maintaining superior reconstruction quality and generalization. Efficient Autoencoder Adaptation for Video Diffusion Models. Closely related to our work, OpenSora 2.0 [25] explored adapting pre-trained video diffusion models to their autoencoders by directly loading the pretrained DiT backbone weights while randomly initializing the patch embedder and output head. Their experiments show that this approach produces noticeably blurry videos and fails to match the performance of training from scratch, finding consistent with our experiments (Figure 6). To address this challenge, we introduce video embedding space alignment stage that recovers the base models knowledge in the new latent space. Video Diffusion Model Acceleration. To accelerate video diffusion models, one line of research focuses on reducing the number of diffusion steps [26, 27, 28, 29, 30, 31, 32, 33, 34]. Another line explores model compression, including sparsity [7, 8, 35, 36, 37, 38, 39, 40] and quantization [41, 42, 43, 44, 45, 46, 40, 47]. Our DC-VideoGen is complementary to them, as it accelerates video generation by reducing token redundancy. 3. Method 3.1. DC-VideoGen Overview Generating high-resolution or long videos with video diffusion models is computationally expensive due to the large number of latent tokens. Furthermore, the prohibitive pre-training costs make developing new video diffusion models both challenging and risky [48]. This paper addresses these challenges from two complementary perspectives (Figure 2, left). First, we drastically reduce the number of tokens using our deep compression video autoencoder. Second, we introduce cost-efficient post-training strategy to adapt pre-trained models to new autoencoders. This approach substantially lowers the risk, training cost, and reliance on large high-quality datasets. As shown in Figure 2 (right), applying our post-training strategy to Wan-2.1-14B [5] takes 10 H100 GPU days just 0.05% of the training cost of MovieGen-30B [49]. 3 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Figure 3 Video Autoencoder Reconstruction Visualization. Under deep compression settings, causal video autoencoders suffer from low reconstruction quality. In contrast, non-causal video autoencoders achieve better reconstruction quality but generalize poorly to longer videos. 3.1.1. Preliminaries and Notation We use xtycz to denote the configuration of video autoencoder. For example, f8t4c16 represents video autoencoder that compresses an input video of size 3 𝑇 𝐻 𝑊 into latent of size 16 𝑇 8 𝑊 8 . The compression ratio is defined as 4 𝐻 Compression Ratio = 3𝑇 𝐻𝑊 𝑡 𝐻 𝑓 𝑊 𝑓 𝑐 𝑇 = 3𝑓 2𝑡 𝑐 . (1) Given the same reconstruction quality, higher compression ratio is generally preferred [50]. We refer to diffusion models with an fxtycz autoencoder as an fxtycz model. video diffusion model typically comprises single-layer patch embedder that maps the latent space to the embedding space, transformer blocks, and an output head that projects back to the latent space (Figure 7c). The patch embedder includes hyperparameter called the patch size 𝑝, which further spatially compresses the latent by factor of 𝑝. As shown in [23], for the same total compression ratio, allocating more spatial compression to the autoencoder rather than the patch embedder yields better generation results. 3.2. Deep Compression Video Autoencoder Existing video autoencoders can be categorized into two groups based on their temporal modeling design: causal and non-causal. Causal. In causal video autoencoders, information flows only from earlier frames to later frames (Figure 4b). This design naturally supports longer videos during inference, since the encoding and decoding of later frames do not affect earlier ones. However, because each frame can only leverage redundancy from preceding frames, reconstruction accuracy is limited under deep compression settings, as illustrated in Figure 3. Building on the causal design, IV-VAE [20] notes that, since every 𝑡 input frames are compressed into single latent frame, enforcing causality within each group of 𝑡 frames is unnecessary. To address this, IV-VAE introduces grouped causal convolution with group size 𝑡 to improve reconstruction performance. 4 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Figure 4 Illustration of Chunk-Causal Temporal Modeling in DC-AE-V. Our chunk-causal temporal modeling preserves causal information flow across chunks while enabling bidirectional flow within each chunk. This design improves reconstruction quality over non-causal temporal modeling, while maintaining generalization to longer videos at inference time. Video Autoencoder VideoVAEPlus [21] CogVideoX VAE [18] HunyuanVideo VAE [19] IV VAE [20] Wan 2.1 VAE [5] Wan 2.2 VAE [5] StepVideo VAE [22] Config f8t4c16 f8t4c16 f8t4c16 f8t4c16 f8t4c16 f16t4c48 f16t8c64 Video DC-AE Video DC-AE [25] LTX Video VAE [24] tiling & blending [25] f32t4c128 f32t4c128 f32t8c128 DC-AE-V f32t4c256 f32t4c128 f32t4c64 f32t4c32 f64t4c128 Compress. Panda70m UCF101 Ratio PSNR SSIM LPIPS PSNR SSIM LPIPS FVD 48 48 48 48 48 64 96 96 192 48 96 192 384 384 36.88 0.968 35.54 0.961 35.46 0.960 35.32 0.959 34.15 0.952 35.12 0.958 32.17 0.930 34.10 0.952 31.73 0.915 32.41 0.928 39.56 0.979 37.37 0.968 35.03 0.953 33.07 0.933 32.79 0.932 0.009 0.021 0.015 0.017 0.017 0.013 0. 0.023 0.040 0.039 0.008 0.013 0.019 0.027 0.030 35.79 0.959 34.53 0.949 34.40 0.950 34.84 0.955 33.81 0.943 34.27 0.948 32.17 0.930 0.016 0.034 0.024 0.025 0.024 0.022 0.043 2.11 8.32 3.80 3.71 3.71 4.02 8.23 33.65 0.945 31.52 0.914 31.12 0. 0.034 14.22 0.047 26.30 0.059 70.92 37.14 0.967 34.83 0.951 32.71 0.931 30.83 0.909 30.60 0.907 1.95 0.018 0.026 5.26 0.035 12.15 0.046 29.11 0.048 29.35 Table 1 Video Autoencoder Reconstruction Results. Video DC-AE achieves higher PSNR with tiling and blending, but still exhibits poor generalization when applied to longer videos at inference time, as shown in Figure 3. However, the group size is strictly tied to the temporal compression ratio, and as shown in Figure 5, it provides only limited improvements in reconstruction quality over the standard causal design under deep compression settings. Non-Causal. Non-causal autoencoders allow bidirectional information flow between frames (Figure 4a). This enables each frame to leverage redundancy from both past and future frames, yielding better reconstruction quality under deep compression settings. However, because earlier frames depend on later ones, generalization to longer videos becomes challenging. Techniques such as temporal tiling and blending [25] can partially alleviate this issue but often introduce artifacts, including temporal flickering and boundary blurring, as shown in Figure 3. We introduce new temporal modeling design, chunk-causal, to overcome these limitations (Figure 4c). The key idea is to divide the input video into fixedsize chunks, where the chunk size is treated as an independent hyperparameter. Within each chunk, we apply bidirectional temporal modeling to fully exploit redundancy across frames. Across chunks, however, we enforce causal flow so that the model can effectively generalize to longer videos at inference time. Figure 5 presents the ablation study on chunk size. We observe that increasing the chunk size consistently improves reconstruction quality. In our final design, we adopt chunk size of 40, as the benefits plateau beyond this point while training costs continue to rise. Figure 5 Ablation Study on Chunk Size. 5 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Text-to-Video Generation Results on VBench 480832 Text-to-Video Diffusion Model Video Autoencoder Patch Latency Score Size (s) Overall Quality Semantic Wan-2.1-1.3B [5] Wan-2.1-VAE-f8t4c16 [5] LTX-Video-f32t8c128 [24] OpenSora2-f32t4c128 [25] Wan-2.2-VAE-f16t4c48 [5] DC-AE-V-f64t4c128 DC-AE-V-f32t4c32 1 1 2 1 1 89.30 83.32 85.01 76. 6.98 14.57 14.59 3.97 14.55 83.30 82.27 80.38 83.38 84.48 85.04 84.53 83.20 85.13 86. 76.34 73.24 69.08 76.38 78.33 Table 2 Comparison of Video Generation Results across Autoencoders. We adopt the same training setup for all models to ensure apples-to-apples comparisons, i.e., using AE-Adapt-V (Section 3.3.2) to adapt the pretrained Wan-2.1-T2V-1.3B to different autoencoders. Figure 6 Direct fine-tuning without AE-Adapt-V leads to training instability and suboptimal quality. In contrast, AE-Adapt-V provides robust initialization that preserves semantics in the new latent space for the video diffusion model, enabling rapid recovery of visual quality and allowing the model to match the base models performance with lightweight fine-tuning. Video Reconstruction Results. We summarize the comparison between DC-AE-V and prior state-of-theart video autoencoders in Table 1. Compared with causal video autoencoders such as LTX Video VAE [24], DC-AE-V achieves higher reconstruction accuracy at the same compression ratio, as well as higher compression ratios for given accuracy target. Compared with non-causal video autoencoders such as Video DC-AE [25], DC-AE-V delivers better reconstruction quality under the same compression ratio while also generalizing better to longer videos (Figure 3). Video Generation Results. In addition to reconstruction performance, we also evaluate DC-AE-V against prior autoencoders on video generation. Table 2 reports ablation results on Wan-2.1-1.3B [5], showing that DCAE-V achieves the best video generation performance. Compared with the base model, DC-AE-V-f64t4c128 provides 22 speedup while attaining slightly higher VBench scores. 3.3. Post-Training Video Autoencoder Adaptation 3.3.1. Naïve Approach, Challenge and Analysis As discussed in Section 3.1.1, the patch embedder and output head are inherently tied to the latent space representation and thus cannot be transferred when replacing the autoencoder. Consequently, straightforward approach for adapting pre-trained video diffusion models to new autoencoders is to retain the pre-trained DiT blocks while randomly initializing the patch embedder and output head (Figure 7c, right). This strategy was explored in [25], where it was found to yield unsatisfactory results. We evaluate this approach under our settings and observe similar outcomes. As shown in Figure 6a (green dashed line), it fails to match the base models semantic score. Furthermore, we observe training instability: 6 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Figure 7 Illustration of Video Embedding Space Alignment. We present detailed ablation studies in Figure 11, showing that both alignment stepspatch embedder alignment and output head alignmentare essential for effective video embedding space alignment. the models output degrades to random noise after 20K training steps (Figure 6b, top). We conjecture that this instability arises from the substantial embedding space gap introduced by the new latent space and the randomly initialized patch embedder, which prevents the model from effectively retaining knowledge from the pre-trained DiT weights. 3.3.2. Our Solution: AE-Adapt-V To address this challenge, we introduce video embedding space alignment stage prior to end-to-end finetuning, bridging the gap between embedding spaces and preserving the pre-trained models knowledge while adapting to new latent space. AE-Adapt-V Stage 1: Video Embedding Space Alignment. Figure 7b illustrates the general concept of our video embedding space alignment, where we first align the patch embedder and then align the output head. For patch embedder alignment, we freeze the base models patch embedder and train new patch embedder to map from the new latent space to the embedding space. The objective is to minimize the distance between the base models embeddings and the embeddings produced by the new patch embedder. Formally, let the embedding of the base model be denoted as 𝑒𝑏 with shape 𝐻𝑏 𝑊𝑏 𝐷, and the embedding of the new model as 𝑒𝑛 with shape 𝐻𝑛 𝑊𝑛 𝐷, where 𝐷 is the embedding channel dimension and 𝐻𝑛 < 𝐻𝑏, 𝑊𝑛 < 𝑊𝑏 in our settings. We first spatially downsample 𝑒𝑏 using average pooling to match the shape of 𝑒𝑛, denoting the result as 𝑒 𝑏. The randomly initialized patch embedder is then trained to minimize the following loss function: ℒ = MSELoss(𝑒𝑛, 𝑒 𝑏). (2) With the aligned patch embedder, the output head is then aligned by jointly finetuning it and the patch embedder using the diffusion loss, while keeping the DiT blocks frozen. This process stops once the diffusion loss converges, which takes up to 4K steps in our experiments. Figure 7a illustrates the visual effect of our video embedding space alignment. Using the aligned patch embedder and output head, we can recover the knowledge and semantics of the base model in the new latent space without updating the DiT blocks. Additional ablation studies are provided in Figure 11, which show that aligning the patch embedder plays the most critical role in video embedding space alignment, while aligning the output head further enhances the quality. DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Text-to-Video Generation Results on VBench 480832 Text-to-Video Trainable Score Method Diffusion Model Params (M) Overall Quality Semantic Wan-2.1-1.3B [5] Full-Tune 1418. 79.81 84.02 62.98 LoRA-Tune 350.37 84. 86.02 78.33 (a) Quantitative Comparison (b) Visual Comparison Figure 8 Ablation Study on End-to-End Tuning Strategies. LoRA attains higher scores than full fine-tuning while requiring far fewer trainable parameters. AE-Adapt-V Stage 2: End-to-End Fine-Tuning with LoRA. Video embedding space alignment alone cannot fully match the base models quality. To close this gap, we perform end-to-end finetuning. Since stage 1 provides strong initialization, we employ LoRA [51] tuning during this stage. Figure 8 compares LoRA tuning with full finetuning. We find that LoRA not only reduces training cost by requiring fewer trainable parameters, but also achieves higher VBench scores and improved visual quality compared with full finetuning. We conjecture that this is because LoRA better preserves the knowledge of the base model. 3.4. DC-VideoGen Application DC-VideoGen can be applied to any pre-trained video diffusion model. In our experiments, we evaluate it on two representative video generation tasks: text-to-video (T2V) and image-to-video (I2V) generation. We use pre-trained Wan-2.1 models [5] as our base models, and denote the resulting accelerated models as DC-VideoGen-Wan-2.1. The Wan-2.1-I2V models incorporate the image condition by concatenating it with the latent. Since Wan-2.1VAE and DC-AE-V employ different temporal modeling designs (causal vs. chunk-causal), DC-VideoGenWan-2.1 I2V models cannot directly adopt the same approach as Section 3.3.2. To address this, we replicate the given image condition four times and append blank frames to form chunks matching the shape of the video. We then encode these chunks with DC-AE-V and concatenate the resulting features with the latent, which can subsequently be processed in the same manner as in Wan-2.1-I2V. 4. Experiments 4.1. Setups Implementation Details. We implement and train all models using PyTorch 2 [52] on 16 NVIDIA H100 GPUs. Three pretrained video diffusion models are employed: Wan-2.1-T2V-1.3B, Wan-2.1-T2V-14B, and Wan-2.1-I2V-14B, each adapted from the original Wan-2.1-VAE to our DC-AE-V. For training, we collected 257K synthetic videos using Wan-2.1-T2V-14B and combined them with 160K high-resolution videos selected from Pexels1 . Detailed training hyperparameters are provided in Table 8. Efficiency Testbed. We benchmark the inference latency of all models using TensorRT2 on single H100 GPU. For simplicity, we focus exclusively on the transformer backbone, as it constitutes the primary efficiency bottleneck. Evaluation Metrics. Following common practice, we use VBench [53] to evaluate text-to-video (T2V) diffusion models and VBench 2.0 [54] for image-to-video (I2V) diffusion models. In addition, we provide visual results generated by our models. 1https://www.pexels.com/videos/ 2https://github.com/NVIDIA/TensorRT 8 - - - - - - - - - - DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Text-to-Video Generation Results on VBench 7201280 Video Autoencoder (B) (min) Overall Quality Semantic #Params Latency Score Text-to-Video Diffusion Model MAGI-1 [55] Step-Video [22] CogVideoX1.5 [18] Skyreels-V2 [56] HunyuanVideo [19] OpenSora-2.0 [25] Wan-2.1-1.3B [5] DC-VideoGen-Wan-2.1-1.3B DC-AE-V-f32t4c Wan-2.1-VAE-f8t4c16 Wan-2.1-14B [5] DC-VideoGen-Wan-2.1-14B Wan-2.1-VAE-f8t4c16 DC-AE-V-f32t4c32 4.5 30 5 1.3 13 14 1.3 1.3 14 21.22 13.16 6.73 9.48 30.35 32.83 5.76 0.70 27.52 3.58 79.18 81.83 82.17 82.67 83.24 84.34 83.38 84.63 83.73 84. 82.04 84.46 82.78 84.70 85.09 85.40 85.67 86.67 85.77 86.80 67.74 71.28 79.76 74.53 75.82 80.12 74.22 76.48 75.58 76. Table 3 Results on Text-to-Video Generation. Native Wan-2.1-T2V-1.3B is limited to 480832 resolution, so we fine-tune it on our dataset to support 7201280 generation. Image-to-Video Generation Results on VBench 7201280 Video Autoencoder #Params Latency Score (B) (min) Overall Quality I2V Image-to-Video Diffusion Model CogVideoX-5b-I2V [18] HunyuanVideo-I2V [19] Step-Video-TI2V [22] MAGI-1 [55] Wan-2.1-14B [5] DC-VideoGen-Wan-2.1-14B DC-AE-V-f32t4c Wan-2.1-VAE-f8t4c16 5 13 30 4.5 14 14 6.72 30.39 13.18 21.25 27.88 3.67 86.70 86.82 88.36 89. 86.86 87.73 78.61 78.54 81.22 82.44 80.83 81.39 94.79 95.10 95.50 96.12 92.90 94.08 Table 4 Results on Image-to-Video Generation. 4.2. Text-to-Video Generation Table 3 compares DC-VideoGen with leading T2V diffusion models on VBench at 7201280 resolution. We follow the extended prompt sets provided by the VBench team and conduct all experiments at the same resolution to ensure fair, apples-to-apples comparisons. Compared with the base Wan-2.1 models, DC-VideoGen-Wan-2.1 achieves higher scores while being significantly more efficient. For example, DC-VideoGen-Wan-2.1-14B reduces latency by 7.7 and improves the VBench score from 83.73 to 84.83 relative to Wan-2.1-14B. Compared with other T2V diffusion models, DC-VideoGenWan-2.1 achieves both the highest VBench score and the lowest latency. Video samples can be found in Figure 13 and the webpage in supplementary material. 4.3. Image-to-Video Generation Table 4 reports our results on VBench I2V at 7201280 resolution. Consistent with the T2V findings, DC-VideoGen-Wan-2.1-14B outperforms the base Wan-2.1-14B by achieving higher VBench score while reducing latency by 7.6. Compared with other I2V diffusion models, DC-VideoGen-Wan-2.1-14B provides highly competitive results with exceptional efficiency, running 5.8 faster than MAGI-1 and 8.3 faster than HunyuanVideo-I2V. Video samples can be found in Figure 12 and the webpage in supplementary material. 9 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder 5. Conclusion We introduce DC-VideoGen, post-training framework that accelerates video diffusion models by combining deep compression video autoencoder with an efficient adaptation strategy. DC-VideoGen achieves up to 14.8 faster inference and drastically reduces training costs, while preserving or even improving video quality. These findings highlight that efficiency and fidelity in video generation can advance together, making large-scale video synthesis more practical and accessible for both research and real-world applications. We further discuss potential extensions and current limitations of our framework in Section A.8."
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. [2] NVIDIA. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [3] Google DeepMind. Veo 3. https://https://deepmind.google/models/veo/, 2025. [4] Kuaishou. Kling ai. https://klingai.kuaishou.com/, 2024. [5] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [6] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [7] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. [8] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. arXiv preprint arXiv:2505.18875, 2025. [9] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. [10] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [11] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-a-video: Temporalconsistent diffusion model for real-world video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 25352545, 2024. [12] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. [13] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 10 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder [14] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [15] Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, and Ying Shan. Cv-vae: compatible video vae for latent generative video models. Advances in Neural Information Processing Systems, 37:1284712871, 2024. [16] Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinhua Cheng, and Li Yuan. Od-vae: An omni-dimensional video compressor for improving latent video diffusion model. arXiv preprint arXiv:2409.01199, 2024. [17] Junke Wang, Yi Jiang, Zehuan Yuan, Bingyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. Advances in Neural Information Processing Systems, 37:2828128295, 2024. [18] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [19] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [20] Pingyu Wu, Kai Zhu, Yu Liu, Liming Zhao, Wei Zhai, Yang Cao, and Zheng-Jun Zha. Improved video vae for latent video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1812418133, 2025. [21] Yazhou Xing, Yang Fei, Yingqing He, Jingye Chen, Jiaxin Xie, Xiaowei Chi, and Qifeng Chen. Large motion video autoencoding with cross-modal video vae. arXiv preprint arXiv:2412.17805, 2024. [22] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. [23] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. [24] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [25] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al. Open-sora 2.0: Training commercial-level video generation model in $200 k. arXiv preprint arXiv:2503.09642, 2025. [26] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya Zhang, Changxin Gao, and Nong Sang. Videolcm: Video latent consistency model. arXiv preprint arXiv:2312.09109, 2023. [27] Shanchuan Lin and Xiao Yang. Animatediff-lightning: Cross-model diffusion distillation. arXiv preprint arXiv:2403.12706, 2024. [28] Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. Advances in neural information processing systems, 37:7569275726, 2024. [29] Yuanhao Zhai, Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Chung-Ching Lin, David Doermann, Junsong Yuan, and Lijuan Wang. Motion consistency model: Accelerating video diffusion with disentangled motion-appearance distillation. Advances in Neural Information Processing Systems, 37:111000111021, 2024. [30] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast causal video generators. arXiv e-prints, pages arXiv2412, 2024. [31] Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Accelerating the animation of personalized diffusion models and adapters with decoupled consistency learning. CoRR, 2024. 11 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder [32] Zhixing Zhang, Yanyu Li, Yushu Wu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Dimitris Metaxas, Sergey Tulyakov, et al. Sf-v: Single forward video generation model. Advances in Neural Information Processing Systems, 37:103599103618, 2024. [33] Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang, and Wenhan Luo. Osv: One step is enough for high-quality image to video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1258512594, 2025. [34] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025. [35] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025. [36] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. [37] Xin Tan, Yuetao Chen, Yimin Jiang, Xing Chen, Kun Yan, Nan Duan, Yibo Zhu, Daxin Jiang, and Hong Xu. Dsv: Exploiting dynamic sparsity to accelerate large-scale video dit training. arXiv preprint arXiv:2502.07590, 2025. [38] Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, and Song Han. Radial attention: 𝒪(𝑛 log 𝑛) sparse attention with energy decay for long video generation. arXiv preprint arXiv:2506.19852, 2025. [39] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint arXiv:2502.18137, 2025. [40] Tianchen Zhao, Ke Hong, Xinhao Yang, Xuefeng Xiao, Huixia Li, Feng Ling, Ruiqi Xie, Siqi Chen, Hongyu Zhu, Yichong Zhang, et al. Paroattention: Pattern-aware reordering for efficient sparse and quantized attention in visual generation models. arXiv preprint arXiv:2506.16054, 2025. [41] Jintao Zhang, Jia Wei, Haofeng Huang, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. arXiv preprint arXiv:2410.02367, 2024. [42] Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. arXiv preprint arXiv:2411.10958, 2024. [43] Shilong Tian, Hong Chen, Chengtao Lv, Yu Liu, Jinyang Guo, Xianglong Liu, Shengxi Li, Hao Yang, and Tao Xie. Qvd: Post-training quantization for video diffusion models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1057210581, 2024. [44] Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, et al. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation. arXiv preprint arXiv:2406.02540, 2024. [45] Lei Chen, Yuan Meng, Chen Tang, Xinzhu Ma, Jingyan Jiang, Xin Wang, Zhi Wang, and Wenwu Zhu. Q-dit: Accurate post-training quantization for diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2830628315, 2025. [46] Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong Qin, and Jun Zhang. Qvgen: Pushing the limit of quantized video generative models. arXiv preprint arXiv:2505.11497, 2025. [47] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. [48] Yuxian Gu, Qinghao Hu, Shang Yang, Haocheng Xi, Junyu Chen, Song Han, and Han Cai. Jet-nemotron: Efficient language model with post neural architecture search. arXiv preprint arXiv:2508.15884, 2025. [49] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [50] Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen, Enze Xie, Song Han, and Han Cai. Dc-ae 1.5: Accelerating diffusion model convergence with structured latent space. arXiv preprint arXiv:2508.00413, 2025. 12 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder [51] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [52] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pages 929947, 2024. [53] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [54] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. VBench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. [55] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. [56] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. [57] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [58] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. [59] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. [60] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. [61] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021. [62] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In Proceedings of the IEEE international conference on computer vision, pages 49904999, 2017. [63] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36:2709227112, 2023. [64] Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Wider face: face detection benchmark. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 55255533, 2016. In [65] Shifeng Zhang, Yiliang Xie, Jun Wan, Hansheng Xia, Stan Li, and Guodong Guo. Widerperson: diverse dataset for dense pedestrian detection in the wild. IEEE Transactions on Multimedia, 22(2):380393, 2019. [66] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In European conference on computer vision, pages 742758. Springer, 2020. [67] Unsplash. Unsplash lite dataset 1.3.0, 2020. [68] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 13 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder [69] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [70] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [71] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [72] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. [73] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. short note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018. [74] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. [75] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, and Yukang Chen. Longlive: Real-time interactive long video generation. 2025. 14 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder A. Appendix A.1. The Use of LLM The use of LLMs was strictly limited to the final manuscript writing stage, specifically for the purpose of correcting grammatical errors and polishing the text. A.2. Additional Details of DC-AE-V A.2.1. Model Architecture Figure 9 Model Architecture of DC-AE-V. Figure 9 presents the detailed model architecture of an f32t4c32 DC-AE-V. Both the encoder and decoder are composed of six stages, each built from 3D ResNet [57] blocks. The first five stages perform only spatial downsampling and upsampling, while the final stage handles temporal downsampling and upsampling. Following DC-AE [23], we incorporate Residual Autoencoding to facilitate optimization during downsampling and upsampling. For adversarial training, we extend the StyleGAN2 discriminator [58] to process video inputs. A.2.2. Dataset Our DC-AE-V is trained on mixture of video and image datasets. The video datasets include subsets of Panda70m [59] and OpenVid1m [60]. The image datasets include ImageNet21k [61], Mapillary Vistas [62], DataComp [63], WiderFace [64], WiderPerson [65], TextCaps [66], and Unsplash [67]. 15 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Video Autoencoder VideoVAEPlus [21] CogVideoX VAE [18] HunyuanVideo VAE [19] IV VAE [20] Wan 2.1 VAE [5] Wan 2.2 VAE [5] StepVideo VAE [22] Config f8t4c16 f8t4c16 f8t4c16 f8t4c16 f8t4c16 f16t4c48 f16t8c64 Video DC-AEtiling & blending [25] f32t4c128 f32t4c128 Video DC-AE [25] f32t8c128 LTX Video VAE [24] DC-AE-V f32t4c256 f32t4c128 f32t4c64 f32t4c32 f64t4c128 Compress. Ratio Panda70m UCF101 ActivityNet Kinetics600 PSNR SSIM LPIPS PSNR SSIM LPIPS FVD PSNR SSIM LPIPS FVD PSNR SSIM LPIPS FVD 48 48 48 48 48 64 96 96 96 48 96 192 384 384 36.88 35.54 35.46 35.32 34.15 35.12 32.17 34.10 31.73 32.41 39.56 37.37 35.03 33.07 32.79 0.968 0.961 0.960 0.959 0.952 0.958 0.930 0.952 0.915 0. 0.979 0.968 0.953 0.933 0.932 0.009 0.021 0.015 0.017 0.017 0.013 0.043 0.023 0.040 0.039 0.008 0.013 0.019 0.027 0.030 35.79 34.53 34.40 34.84 33.81 34.27 32.17 33.65 31.52 31. 37.14 34.83 32.71 30.83 30.60 0.959 0.949 0.950 0.955 0.943 0.948 0.930 0.945 0.914 0.910 0.967 0.951 0.931 0.909 0.907 0.016 0.034 0.024 0.025 0.024 0.022 0.043 0.034 0.047 0. 0.018 0.026 0.035 0.046 0.048 2.11 8.32 3.80 3.71 3.71 4.02 8.23 14.22 26.30 70.92 1.95 5.26 12.15 29.11 29.35 35.68 34.47 34.41 35.03 33.82 34.41 32.08 33.55 31.34 31. 37.29 35.06 33.02 31.08 30.86 0.955 0.946 0.943 0.948 0.938 0.943 0.922 0.938 0.901 0.900 0.965 0.948 0.927 0.901 0.898 0.016 0.034 0.024 0.025 0.025 0.021 0.047 0.033 0.049 0. 0.016 0.024 0.034 0.045 0.048 0.96 5.16 3.16 1.88 1.76 1.56 5.29 7.92 17.52 45.51 0.89 2.46 5.64 13.83 13.63 36.73 35.40 35.40 36.27 35.04 35.57 33.02 34.73 32.39 32. 38.12 35.91 33.87 32.01 31.73 0.960 0.951 0.950 0.956 0.946 0.950 0.931 0.946 0.915 0.911 0.969 0.953 0.934 0.912 0.909 0.014 0.032 0.022 0.022 0.022 0.019 0.044 0.030 0.044 0. 0.015 0.023 0.032 0.042 0.046 0.93 4.18 2.59 1.52 1.49 1.51 4.62 6.81 15.43 42.30 0.82 2.36 5.70 13.05 13.60 Table 5 Additional Video Reconstruction Results. A.2.3. Evaluation We evaluate all video autoencoders on 80 256 256 videos using PSNR, SSIM [68], LPIPS [69], and FVD [70]. The evaluation set includes 1,000 unseen videos from Panda70m [59], 3,783 test videos from UCF101 [71], 5,044 test videos from ActivityNet 1.3 [72], and the first 5,000 test videos from Kinetics600 [73]. For VideoVAEPlus [21], we use the 16z version. For CogVideoX VAE [18], we use the model from THUDM/CogVideoX-2b. For HunyuanVideo VAE [19], we use the model from hunyuanvideo-community. For IV VAE [20], we use the ivvae_z16_dim96 version. For Wan 2.1 VAE [5], we use the model from WanAI/Wan2.1-T2V-14B-Diffusers, and for Wan 2.2 VAE [5], we use the model from Wan-AI/Wan2.2-TI2V-5B. For StepVideo VAE [22], we use the vae_v2 from stepfun-ai/stepvideo-t2v. For Video DC-AE [25], we use the model from hpcai-tech/Open-Sora-v2-Video-DC-AE. Finally, for LTX Video VAE [24], we use the model from Lightricks/LTX-Video-0.9.7-dev. When an autoencoder cannot process 80-frame videos, we pad extra frames at the end and exclude them from the reconstructions when computing evaluation metrics. A.2.4. Additional Reconstruction Results Table 5 presents the full reconstruction results. Our DC-AE-V consistently achieves superior accuracy and generalizes effectively to longer videos across range of benchmarks. Figure 10 presents additional reconstruction examples. Our DC-AE-V demonstrates superior reconstruction accuracy and generalization ability to longer videos, especially for small texts and human faces. A.3. Ablation Study on Video Embedding Space Alignment Figure 11 presents additional ablation studies on video embedding space alignment. Aligning both the patch embedder and output head yields the best results, with the patch embedder alignment playing the most critical role in overall performance. A.4. Detailed Evaluation Results on VBench Table 6 reports detailed metrics on VBench. DC-VideoGen-Wan-2.1-T2V-1.3B outperforms the base Wan-2.1T2V-1.3B on 11 of the 16 metrics. A.5. Detailed Efficiency Benchmark Results Table 7 presents detailed efficiency results measured on an NVIDIA H100 GPU. A.6. Detailed Training Hyperparameters of AE-Adapt-V Table 8 lists the detailed hyperparameters for AE-Adapt-V. 16 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Figure 10 Additional Video Autoencoder Reconstruction Visualization. Under deep compression settings, causal video autoencoders suffer from low reconstruction quality. In contrast, non-causal video autoencoders achieve better reconstruction quality but generalize poorly to longer videos. Text-to-Video Generation Results on VBench 7201280 Models Temporal Flickering Consistency Subject Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Background Quality Consistency Consistency Overall Wan-2.1-T2V-1.3B 99.15 DC-VideoGen-Wan-2.1-T2V-1.3B 99.18 Models Wan-2.1-T2V-1.3B Object Class 89.11 DC-VideoGen-Wan-2.1-T2V-1.3B 88.73 94. 96.58 Multiple Objects 72.07 75.98 98.36 98. Human Action 94.05 94.64 67.78 72.78 Color 81.54 89.16 70.20 72.00 Spatial Relationship 65. 78.20 68.44 68.72 Scene 44.83 44. 97.99 98.00 25.97 25.41 Appearance Style Temporal Style 21.61 21.20 23.22 22.97 Table 6 Detailed Results on VBench. (a) Latency (minutes per video) (b) Latency (minutes per video) Models Resolution Models 480832 7201280 10801920 21603840 Number of Frames 80 160 320 640 Wan-2.1-1.3B [5] DC-VideoGen-Wan-2.1-1.3B Speedup 1.49 0.24 6.2 5.76 0.70 8.2 25.46 2.27 11.2 375.12 25.41 14.8 Wan-2.1-1.3B [5] DC-VideoGen-Wan-2.1-1.3B 0.70 1.99 5.76 20.18 75.77 296.30 20.86 6.03 Speedup 8.2 10.1 12.6 14.2 Table 7 Detailed Efficiency Benchmark Results. A.7. Qualitative Comparison with the Pre-trained Models Figure 12 and Figure 13 provide qualitative comparison between DC-VideoGen and the base models. We observe that DC-VideoGen-Wan2.1-I2V-14B and DC-VideoGen-Wan2.1-T2V-14B retain the generation quality 17 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Figure 11 Ablation Study on Video Embedding Space Alignment. Figure 12 Visual Comparison of DC-VideoGen-Wan2.1-I2V-14B and the Base Model Wan2.1I2V-14B. of Wan2.1-I2V-14B and Wan2.1-T2V-14B while reducing the latency by around 87%. A.8. Limitation and Future Work Limitations. As post-training framework, DC-VideoGen accelerates video diffusion models through lightweight fine-tuning, effectively leveraging the rich knowledge encoded in the pre-trained model. Consequently, its performance is strongly dependent on the quality of the pre-trained model. Future Work. DC-VideoGen substantially reduces the training and inference costs of video diffusion models, especially when scaling to higher resolutions. For the next step, we plan to extend our framework for long video generation, leveraging techniques from [74, 75]. 18 DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder Training Stage Hyperparameter Value Patch Embedder Alignment Output Head Alignment End-to-End Fine-Tuning learning rate warmup steps batch size training steps optimizer learning rate warmup steps batch size training steps optimizer learning rate warmup steps training steps batch size optimizer weight decay LoRA (rank, alpha) 1e-4 0 4 20k AdamW, betas=[0.9, 0.999] 1e-4 0 32 4k (Wan-2.1-1.3B) / 3k (Wan-2.1-14B) AdamW, betas=[0.9, 0.999] 5e-5 1k 20k (Wan-2.1-1.3B) / 6k (Wan-2.1-14B) 32 AdamW, betas=[0.9, 0.999] 1e-3 (256, 512) Resolution Increasing 480px720px, training steps 720px1080px, training steps 1080px2160px, training steps 1000 500 200 Table 8 Training Hyperparameters of AE-Adapt-V. Figure 13 Visual Comparison of DC-VideoGen-Wan2.1-T2V-14B and the Base Model Wan2.1T2V-14B."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}