{
    "paper_title": "The Curse of Depth in Large Language Models",
    "authors": [
        "Wenfang Sun",
        "Xinyuan Song",
        "Pengxiang Li",
        "Lu Yin",
        "Yefeng Zheng",
        "Shiwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models(LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training."
        },
        {
            "title": "Start",
            "content": "Wenfang Sun * 1 Xinyuan Song * 2 Pengxiang Li 3 Lu Yin 4 Yefeng Zheng 1 Shiwei Liu 5 Abstract In this paper, we introduce the Curse of Depth, concept that highlights, explains, and addresses the recent observation in modern Large Language Models (LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training. Our code is available at LayerNorm-Scaling. 5 2 0 2 9 ] . [ 1 5 9 7 5 0 . 2 0 5 2 : r 1. Introduction Recent studies reveal that the deeper layers (Transformer blocks) in modern LLMs tend to be less effective than the *Equal contribution 1Medical Artificial Intelligence Laboratory, Westlake University, China 2Emory University, USA 3Dalian University of Technology, China 4University of Surrey, UK 5University of Oxford, UK. Correspondence to: Shiwei Liu <shiwei.liu@maths.ox.ac.uk>. 1 earlier ones (Yin et al., 2024; Gromov et al., 2024; Men et al., 2024). On the one hand, this interesting observation provides an effective indicator for LLM compression. For instance, we can compress deeper layers significantly more (Yin et al., 2024; Lu et al., 2024; Dumitru et al., 2024) to achieve high compression ratios. Even more aggressively, entire deep layers can be pruned completely without compromising performance for the sake of more affordable LLMs (Muralidharan et al., 2024; Siddiqui et al., 2024). On the other hand, having many layers ineffective is undesirable as modern LLMs are extremely resource-intensive to train, often requiring thousands of GPUs trained for multiple months, let alone the labor used for data curation and administration (Achiam et al., 2023; Touvron et al., 2023). Ideally, we want all layers in model to be well-trained, with sufficient diversity in features from layer to layer, to maximize the utility of resources (Li et al., 2024b). The existence of ill-trained layers suggests that there must be something off with current LLM paradigms. Addressing such limitations is pressing need for the community to avoid the waste of valuable resources, as new versions of LLMs are usually trained with their previous computing paradigm which results in ineffective layers. To seek the immediate attention of the community, we introduce the concept of the Curse of Depth (CoD) to systematically present the phenomenon of ineffective deep layers in various LLM families, to identify the underlying reason behind it, and to rectify it by proposing LayerNorm Scaling. We first state the Curse of Depth below. The Curse of Depth. The Curse of Depth refers to the observed phenomenon where deeper layers in modern large language models (LLMs) contribute significantly less to learning and representation compared to earlier layers. These deeper layers often exhibit remarkable robustness to pruning and perturbations, implying they fail to perform meaningful transformations. This behavior prevents these layers from effectively contributing to training and representation learning, resulting in resource inefficiency. Empirical Evidence of CoD. The ineffectiveness of deep layers in LLMs has been previously reported. Yin et al. (2024) found that deeper layers of LLMs can tolerate significantly higher levels of pruning compared to shallower layers, achieving high sparsity. Similarly, Gromov et al. The Curse of Depth in Large Language Models Figure 1. Layerwise output variance. This figure compares the output variance across various layers for different setups: (1) Pre-LN; (2) Pre-LN with Scaled Initialization; and (3) LayerNorm Scaling. The experiments are conducted on the LLaM-130M model trained for 10,000 steps. The proposed LayerNorm Scaling effectively controls the variance across layers. (2024) and Men et al. (2024) demonstrated that removing early layers causes dramatic decline in model performance, whereas removing deep layers does not. Lad et al. (2024) showed that the middle and deep layers of GPT-2 and Pythia exhibit remarkable robustness to perturbations such as layer swapping and layer dropping. Recently, Li et al. (2024a) highlighted that early layers contain more outliers and are therefore more critical for fine-tuning. While these studies effectively highlight the limitations of deep layers in LLMs, they stop short of identifying the root cause of this issue or proposing viable solutions to address it. To demonstrate that the Curse of Depths is prevalent across popular families of LLMs, we conduct layer pruning experiments on various models, including LLaMA2-7/13B, Mistral-7B, DeepSeek-7B, and Qwen-7B. We measure performance degradation on the Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2021) by pruning entire layers of each model, one at time, and directly evaluating the resulting pruned models on MMLU without any fine-tuning in Figure 2. Results: 1). Most LLMs utilizing Pre-LN exhibit remarkable robustness to the removal of deeper layers, whereas BERT with Post-LN shows the opposite trend. 2). The number of layers that can be pruned without significant performance degradation increases with model size. Identifying the Root Cause of CoD. We theoretically and empirically identify the root cause of CoD as the use of PreLayer Normalization (Pre-LN) (Baevski and Auli, 2019; Dai et al., 2019), which normalizes layer inputs before applying the main computations, such as attention or feedforward operations, rather than after. Specifically, while stabilizing training, we observe that the output variance of Pre-LN accumulates significantly with layer depth (see Appendix C), causing the derivatives of deep Pre-LN layers to approach an identity matrix. This behavior prevents these layers from introducing meaningful transformations, leading to diminished representation learning. Mitigating CoD through LayerNorm Scaling. We prol pose LayerNorm Scaling, which scales the output of Layer Normalization by the square root of the depth 1 . LayerNorm Scaling effectively scales down the output variance across layers of Pre-LN, leading to considerably lower training loss and achieving the same loss as Pre-LN using only half tokens. Figure 1 compares the layerwise output variance across different setups: (1) Pre-LN, (2) Pre-LN with Scaled Initialization (Takase et al., 2023b), and (3) LayerNorm Scaling. As shown, Pre-LN exhibits significant variance explosion in deeper layers. In contrast, LayerNorm Scaling effectively reduces output variance across layers, enhancing the contribution of deeper layers during training. This adjustment leads to significantly lower training loss compared to Pre-LN. Unlike previous LayerNorm variants (Li et al., 2024b; Liu et al., 2020), LayerNorm Scaling is simple to implement, requires no hyperparameter tuning, and introduces no additional parameters during training. Furthermore, we further show that the model pre-trained with LayerNorm Scaling achieves better performance on downstream tasks in self-supervised fine-tuning, all thanks to the more effective deep layers learned. Contributions. We introduce the Curse of Depth to highlight, understand, and rectify the phenomenon in LLMs that is commonly overlookeddeep layers fail to contribute as effectively as they should. We identify the root cause as Pre-LN, which causes output variance to grow exponentially with model depth. This leads to deep Transformer blocks having derivatives close to the identity matrix, rendering them ineffective during training. While scaled initialization (Shoeybi et al., 2020) helps mitigate variance at initialization, it does not prevent explosion during training. To mitigate this issue, we propose LayerNorm Scaling, which inversely scales the output of Pre-LN by the square root of the depth. This adjustment ensures that all layers contribute effectively to learning, thereby improving LLM performance. The Curse of Depth in Large Language Models Figure 2. Performance drop of layer pruning across different LLMs. (a) BERT-Large (Post-LN), (b) Mistral-7B (Pre-LN), (c) Qwen-7B (Pre-LN), (d) DeepSeek-7B (Pre-LN), (e) LLaMA2-7B (Pre-LN), and (f) LLaMA2-13B (Pre-LN). The results show that Pre-LN models exhibit significant inefficiency in deeper layers, while Post-LN models maintain strong deep-layer contributions. We hope this work brings greater attention to this issue, contributes to the improvement of LLMs, and maximizes the utilization of computational resources dedicated to training large models. 2. Empirical Evidence of the Curse of Depth To empirically analyze the impact of layer normalization on the Curse of Depth in LLMs, we conduct series of evaluations inspired by Li et al. (2024b), extending their methodology to compare Pre-LN and Post-LN models. 2.1. Experimental Setup Methods: We evaluate Pre-LN and Post-LN models by assessing the impact of layer pruning at different depths. Our hypothesis is that Pre-LN models exhibit diminishing effectiveness in deeper layers, whereas Post-LN has less effective early layers. To verify this, we empirically quantify the contribution of individual layers to overall model performance across diverse set of LLMs. LLMs: We conduct experiments on multiple widely adopted LLMs: BERT-Large (Devlin, 2019), Mistral7B (Jiang et al., 2023), LLaMA2-7B/13B (Touvron et al., 2023), DeepSeek-7B (Bi et al., 2024), and Qwen-7B (Bai et al., 2023). These models were chosen to ensure architectural and application diversity. BERT-Large represents Post-LN model, whereas the rest are Pre-LN-based. This selection enables comprehensive evaluation of the effects of layer normalization across varying architectures and model scales. Evaluation Metric: To empirically assess the impact of deeper layers in LLMs, we adopt the Performance Drop metric (ℓ), inspired by Li et al. (2024b). This metric quantifies the contribution of each layer by measuring the degradation in model performance following its removal. Specifically, it is defined as: (ℓ) = (ℓ) pruned Poriginal, (1) where Poriginal represents the performance of the unpruned model, and (ℓ) pruned denotes the performance after removing layer ℓ. lower (ℓ) suggests that the pruned layer plays minor role in the models overall effectiveness. For BERT-Large, we evaluate performance on the SQuAD v1.1 dataset (Rajpurkar, 2016), which measures reading comprehension. For Mistral-7B, LLaMA2-13B, and Qwen7B, we assess model performance on the MMLU benchmark (Hendrycks et al., 2021), widely-used dataset for multi-task language understanding. 2.2. Layer Pruning Analysis Figure 2 presents the performance drop (P (ℓ)) across different layers for six LLMs, including one Post-LN model (BERT-Large) and five Pre-LN models (Mistral-7B, LLaMA2-13B, Qwen-7B, DeepSeek-7B and LLaMA2-7B). As shown in Figure 2 (a), pruning deeper layers in BERTLarge leads to significant decline in accuracy on SQuAD v1.1, while pruning earlier layers has minimal impact. The 3 The Curse of Depth in Large Language Models performance drop (ℓ) becomes particularly severe beyond the 10th layer, highlighting the crucial role of deeper layers in maintaining overall performance in Post-LN models. In contrast, removing layers in the first half of the network results in negligible changes, indicating their limited contribution to the final output. However, as shown in Figure 2 (b)-(f), Pre-LN models exhibit contrast pattern, where deeper layers contribute significantly less to the overall model performance. For instance, as shown in Figure 2 (b) and (c), pruning layers in the last third of Mistral-7B and Qwen-7B results in minimal performance drop on MMLU, indicating their limited contribution to overall accuracy. In contrast, pruning the first few layers leads to substantial accuracy degradation, highlighting their crucial role in feature extraction. Similarly, Figure 2 (d) and (e) show that DeepSeek-7B and LLaMA2-7B follow similar pattern, where deeper layers have little impact on performance, while earlier layers play more significant role. Finally, as shown in Figure 2 (f), more than half of the layers in LLaMA2-13B can be safely removed. This suggests that as model size increases, the contrast between shallow and deep layers becomes more pronounced, with earlier layers playing dominant role in representation learning. This observation underscores the need for the community to address the Curse of Depth to prevent resource waste. 3. Analysis of the Curse of Depth 3.1. Preliminaries This paper primarily focuses on Pre-LN Transformer (Baevski and Auli, 2019; Dai et al., 2019). Let xℓ Rd be the input vector at the ℓ-th layer of Transformer, where denotes the feature dimension of each layer. For simplicity, we assume all layers to have the same dimension d. The layer output is calculated as follows: = xℓ+1 = ℓ + FFN(LN(x ℓ)), ℓ = xℓ + Attn(LN(xℓ)), (2) (3) where LN denotes the layer normalization function. In addition, the feed-forward network (FFN) and the multihead self-attention (Attn) sub-layers are defined as follows: FFN(x) = W2F(W1x), Attn(x) = WO(concat(head1(x), . . . , headh(x))), headi(x) = softmax (cid:18) (WQix)(WKiX) (cid:19) (WV iX), dhead (4) where is an activation function, concat concatenates input vectors, softmax applies the softmax function, and W1 Rdffnd, W2 Rddffn , WQi Rdheadd, WKi Rdheadd, WV Rdheadd, and WO Rdd are parameter matrices, and dFFN and dhead are the internal dimensions of FFN and multi-head self-attention sub-layers, respectively. Rds, where is the input sequence length. The derivatives of Pre-Ln Transformers are: Pre-LN(x) = + (LN(x)) LN(x) LN(x) , (5) where here represents either the multi-head attention funcLN(x) tion or the FFN function. If the term (LN(x)) beLN(x) comes too small, the Pre-LN layer Pre-LN(x) behaves like an identity map. Our main objective is to prevent identity map behavior for very deep Transformer networks. The first step in this process is to compute the variance σ2 xℓ of vector xℓ. 3.2. Pre-LN Transformers Assumption 1. Let xℓ and ℓ denote the input and intermediate vectors of the ℓ-th layer. Moreover, let Wℓ denote the model parameter matrix at the ℓ-th layer. We assume that, for all layers, xℓ, ℓ, and Wℓ follow normal and independent distributions with mean µ = 0. Lemma 1. Let σ2 denote the variances of and σ2 xℓ ℓ ℓ and xℓ, respectively. These two variances exhibit the same overall growth trend, which is: σ2 xℓ = σ2 x1 (cid:16)ℓ1 (cid:89) Θ (cid:18) 1 + k=1 (cid:19)(cid:17) , 1 σxk (6) where the growth of σ2 xl following bounds: is sub-exponential, as shown by the Θ(L) σ2 xl Θ(exp(L)). (7) if (x) Θ(cid:0)g(x)(cid:1), then Here, the notation Θ means: there exist constants C1, C2 such that C1 g(x) (x) C2 g(x) as . The lower bound Θ(L) σ2 xℓ indicates that σ2 xℓ grows at least linearly, while the upper bound σ2 Θ(exp(L)) implies that its growth does not exceed xℓ an exponential function of L. Based on Assumption 1 and the work of (Takase et al., 2023b), we obtain the following: Theorem 1. For Pre-LN Transformer with layers, using Equations (2) and (3), the partial derivative yL can be x1 written as: yL x1 = L1 (cid:89) ℓ=1 (cid:18) yℓ ℓ (cid:19) . ℓ xℓ The Euclidean norm of yL x1 is given by: (cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 L1 (cid:89) l= (cid:18) 1 + 1 σxℓ + (cid:19) , 1 σ2 xℓ (8) (9) 4 The Curse of Depth in Large Language Models where and are constants for the Transformer network. Then the upper bound for this norm is given as follows: when σ2 grows exponentially, (i.e., at its upper bound), we xℓ have: σ2 xℓ exp(ℓ), M, (10) (cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 where the gradient norm converges to constant . Conversely, when σ2 grows linearly (i.e., at its lower bound), xℓ we have σ2 xℓ ℓ, (cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 Θ(L), (11) which means that the gradient norm grows linearly in L. The detailed description of and B, as well as the complete proof, are provided in Appendix A.2. yL x1 (cid:13) (cid:13) (cid:13)2 From Theorem 1, we observe that when the variance grows exponentially, as the number of layers , the norm (cid:13) (cid:13) is bounded above by fixed constant . This result (cid:13) implies that even an infinitely deep Transformer remains stable, and by the Weierstrass Theorem, the network is guaranteed to converge. Consequently, this implies that for very large L, deeper layers behave nearly as an identity map from xℓ to yℓ, thereby limiting the models expressivity and hindering its ability to learn meaningful transformations. This outcome is undesirable, therefore, we would instead prefer the variance to increase more graduallye.g., linearlyso that exhibits linear growth. This observation highlights the necessity of appropriate variance control mechanisms, such as scaling strategies, to prevent excessive identity mappings and enhance network depth utilization. (cid:13) (cid:13) (cid:13)2 yL (cid:13) (cid:13) (cid:13) 3.3. Post-LN Transformers For Post-LN Transformers, we continue to adopt Assumption 1. In this setting, each layer is followed by layer normalization (LN) step, ensuring that the variances σ2 xℓ and σ2 remain fixed at 1 across all layers. Consequently, ℓ (cid:13) (cid:13) (cid:13) the norm to the next, indicating stable gradient propagation. exhibits minimal variation from one layer (cid:13) (cid:13) (cid:13) yℓ xℓ Since the variance is effectively controlled by LN in PostLN Transformers, an explicit variance-based analysis becomes less critical. Nonetheless, there remain other important aspects to investigate in deeper Post-LN architectures, such as the evolution of feature mappings and the behavior of covariance kernels over deep layers. These directions will be pursued in future work. 4. LayerNorm Scaling Our theoretical and empirical analyses indicate that Pre-LN amplifies output variance, leading to the Curse of Depth and 5 Figure 3. Comparison between Pre-LN (a) and LayerNorm Scaling (b). LayerNorm Scaling applies scaling factor inversely proportional to the square root of the layer index l, preventing excessive variance growth and stabilizing training dynamics across layers. reducing the effectiveness of deeper layers. To mitigate this issue, we propose LayerNorm Scaling, simple yet effective normalization strategy. The core idea of LayerNorm Scaling is to control the exponential growth of output variance in Pre-LN by scaling the normalized outputs according to layer depth. Specifically, we apply scaling factor inversely proportional to the square root of the layer index to scale down the output of LN layers, stabilizing gradient flow and enhancing the contribution of deeper Transformer layers during training. LayerNorm Scaling is illustrated in Figure 3. Formally, for Transformer model with layers, the output of Layer Normalization in each layer ℓ is scaled by factor of 1 . Let h(ℓ) denote the input to Layer Normalization at ℓ layer ℓ. The modified output is computed as: h(l) = LayerNorm(h(ℓ)) 1 ℓ , (12) where ℓ {1, 2, . . . , L}. This scaling prevents excessive variance growth with depth, addressing key limitation of Pre-LN. Unlike Mix-LN, which stabilizes gradients in deeper layers but suffers from training instability caused by Post-LN (Nguyen and Salazar, 2019; Wang et al., 2024), LayerNorm Scaling preserves the stability advantages of Pre-LN while enhancing the contribution of deeper layers to representation learning. Applying LayerNorm Scaling leads to notable reduction of layerwise output variance, resulting in lower training loss and faster convergence than vanilla Pre-LN. Moreover, compared with previous LayerNorm variants (Li et al., 2024b; Liu et al., 2020), LayerNorm Scaling is hyperparameter-free, easy to implement, and does not introduce additional learnable parameters, making it computationally efficient and readily applicable to existing Transformer architectures. The Curse of Depth in Large Language Models 4.1. Theoretical Analysis of LayerNorm Scaling Lemma 2. After applying our scaling method, the variances of ℓ and xℓ, denoted as σ2 , respectively, exhibit ℓ the same growth trend, which is: and σ2 xℓ σ2 xℓ+ = σ2 xℓ Θ(1 + 1 ℓσxℓ ), with the following growth rate bounds: Θ(L) σ2 xL Θ(L(2ϵ)). where ϵ is small number with 0 < ϵ 1/4. (13) (14) From Lemma 2, we can conclude that our scaling method effectively slows the growth of the variance upper bound, reducing it from exponential to polynomial growth. Specifically, it limits the upper bound to quadratic rate instead of an exponential one. Based on Theorem 1, after scaling, we obtain the following: Theorem 2. For the scaled Pre-LN Transformers, the Euclidean norm of yL x1 is given by: (cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 L1 (cid:89) (cid:18) 1 + ℓ=1 1 ℓσxℓ + 1 ℓ2σ2 xℓ (cid:19) , (15) where and are dependent on the scaled neural network parameters. Then the upper bound for the norm is given as follows: when σ2 grows at ℓ(2ϵ), (i.e., at its upper bound), xℓ we obtain: σ2 xℓ ℓ(2ϵ), (cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 ω(1), (16) where ω denotes that (x) limx (i.e., at its lower bound), we obtain: g(x) = . Meanwhile, when σ2 xℓ if (x) = ω(g(x)), then grows linearly σ2 xℓ ℓ, (cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 Θ(L). (17) The detailed descriptions of and B, and ϵ, along with the full proof, are provided in Appendices A.3 and A.4. By comparing Theorem 1 (before scaling) with Theorem 2 (after scaling), we observe substantial reduction in the upper bound of variance. Specifically, it decreases from exponential growth Θ(exp(L)) to at most quadratic growth Θ(L2). In fact, this growth is even slower than quadratic expansion, as it follows Θ(L(2ϵ)) for some small ϵ > 0. When we select reasonable upper bound for this expanno longer possesses strict upper sion, we find that (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) bound. That is, as the depth increases, (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13)2 continues to grow gradually. Consequently, fewer layers act as identity mappings compared to the original Pre-LN where nearly all deep layers collapsed into identity transformations. Instead, the after-scaled network effectively utilizes more layers, even as the depth approaches infinity, leading to improved expressivity and trainability. 5. Experiments 5.1. LLM Pre-training To evaluate the effectiveness of LayerNorm Scaling, we follow the experimental setup of Li et al. (2024b), using the same model configurations and training conditions to compare it with widely used normalization techniques, including Post-LN (Nguyen and Salazar, 2019), DeepNorm (Wang et al., 2024), and Pre-LN (Dai et al., 2019). In line with Lialin et al. (2023) and Zhao et al. (2024), we conduct experiments using LLaMA-based architectures with model sizes of 130M, 250M, 350M, and 1B parameters, ensuring consistency in architecture and training settings. The architecture incorporates RMSNorm (Shazeer, 2020) and SwiGLU activations (Zhang and Sennrich, 2019), which are applied consistently across all model sizes and normalization methods. For optimization, we use the Adam optimizer (Kingma, 2015) and adopt size-specific learning rates: 1 103 for models up to 350M parameters, and 5 104 for the 1B parameter model. All models share the same architecture, hyperparameters, and training schedule, with the only difference being the choice of normalization method. Unlike Mix-LN (Li et al., 2024b), which introduces an additional hyperparameter α manually set to 0.25, LayerNorm Scaling requires no extra hyperparameters, making it simpler to implement. Table 1 shows that LayerNorm Scaling consistently outperforms other normalization methods across different model sizes. While DeepNorm performs comparably to Pre-LN on smaller models, it struggles with larger architectures like LLaMA-1B, showing signs of instability and divergence in loss values. Similarly, Mix-LN outperforms Pre-LN in smaller models but faces convergence issues with LLaMA-350M, indicating its sensitivity to architecture design and hyperparameter tuning due to the introduction of Post-LN. Notably, Mix-LN was originally evaluated on LLaMA-1B with 50,000 steps (Li et al., 2024b), while our setting extends training to 100,000 steps, where Mix-LN fails to converge, highlighting its instability in large-scale settings caused by the usage of Post-LN. In contrast, LayerNorm Scaling solves the Curse of Depth without compromising the training stability thanks to its simplicity. LayerNorm Scaling achieves the lowest perplexity across all tested model sizes, showing stable performance improvements over existing methods. For instance, on LLaMA-130M and LLaMA-1B, LayerNorm The Curse of Depth in Large Language Models Table 1. Perplexity () comparison of various layer normalization methods across various LLaMA sizes. LLaMA-130M LLaMA-250M LLaMA-350M LLaMA-1B Training Tokens Post-LN (Ba, 2016) DeepNorm (Wang et al., 2024) Mix-LN (Li et al., 2024b) Pre-LN (Baevski and Auli, 2019) Pre-LN + LayerNorm Scaling 2.2B 26.95 27.17 26.07 26.73 25.76 3.9B 1409.79 22.77 21.39 21.92 20. 6.0B 1368.33 1362.59 1363.21 19.58 18.20 8.9B 1390.75 1409.08 1414.78 17.02 15. Scaling reduces perplexity by 0.97 and 1.31, respectively, compared to Pre-LN. Notably, LayerNorm Scaling maintains stable training dynamics for LLaMA-1B, model size where Mix-LN fails to converge. These findings demonstrate that LayerNorm Scaling provides robust and computationally efficient normalization strategy, enhancing large-scale language model training without additional implementation complexity. Comparison with Other Layer Normalization. In addition, we conducted comparisons using LLaMA-130M to evaluate LayerNorm Scaling against recently proposed normalization methods, including Admin (Liu et al., 2020), Sandwich-LN (Ding et al., 2021), and Group-LN (Wu and He, 2018; Ma et al., 2024). Table 2 shows that Admin and Group-LN degrade performance. Sandwich-LN slightly outperforms Pre-LN. Both Mix-LN and LayerNorm Scaling improve over Pre-LN by good margins. However, Mix-LN fails to reduce perplexity under 26, falling short of LayerNorm Scaling. Table 2. Comparison against other normalization methods on LLaMA-130M. Perplexity () is reported. Pre-LN Admin Group-LN Sandwich-LN Mix-LN LayerNorm Scaling 26.73 27.91 28. 26.51 26.07 25.76 5.2. Supervised Fine-tuning We believe that LayerNorm Scaling allows deeper layers in LLMs to contribute more effectively during supervised fine-tuning by alleviating gradient vanishing associated with increasing depth. Compared to models trained with PreLN, the deeper layers with LayerNorm Scaling maintain stable output variance, preventing uncontrolled growth and ensuring effective feature representation. As result, deeper layers contribute more effectively to feature transformation, enhancing representation learning and improving generalization on complex downstream tasks. To verify this, we follow the fine-tuning methodologies in Li et al. (2024b) and Li et al. (2024a), applying the same optimization settings as pre-training. We fine-tune models from Section 5.1 on the Commonsense170K dataset (Hu et al., (Hu et al., 2023)) across eight downstream tasks. The results, presented in Table 3, demonstrate that LayerNorm Scaling consistently surpasses other normalization techniques in all evaluated datasets. For the LLaMA-250M model, LayerNorm Scaling improves average performance by1.80% and achieves 3.56% gain on ARC-e compared to Mix-LN. Similar trends are observed with the LLaMA-1B model, where LayerNorm Scaling outperforms Pre-LN, Post-LN, Mix-LN, and DeepNorm on seven out of eight tasks, with an average gain of 1.86% over the best baseline. These results confirm that LayerNorm Scaling, by improving gradient flow and deep-layer representation quality, achieves better fine-tuning performance, demonstrating robustness and enhanced generalization on diverse downstream tasks. 5.3. LayerNorm Scaling Reduces Output Variance As LayerNorm Scaling aims to reduce output variance, we validate this by comparing it with two scaling approaches: LayerScale (Touvron et al., 2021) and Scaled Initialization (Shoeybi et al., 2020). LayerScale applies per-channel weighting using diagonal matrix, diag(λ1, . . . , λd), where each weight λi is initialized to small value (e.g., λi = ϵ). Unlike LayerNorm Scaling, LayerScale learns the scaling factors automatically, which does not necessarily induce down-scaling effect. Scaled Initialization scales the initialization of W0 and W2 to small values by where is the total number of transformer layers. Since scaling is applied only at initialization, we argue that Scaled Initialization may not effectively reduce variance throughout training. We further verify this in Figure 1, where we can see the output variance of Scaled Initialization is as large as Pre-LN. Table 4 presents the results of LLaMA-130M and LLaMA-250M. First, we can see that LayerScale degrades performance. While Scaled Initialization slightly improves over Pre-LN, it falls short of LayerNorm Scaling and the gap becomes larger for the larger model. 1 2L 5.4. Enhancing Deep Layers with LayerNorm Scaling To evaluate how LayerNorm Scaling improves deep layer effectiveness, we conduct layer pruning experiment on LLaMA-130M, systematically removing individual layers and measuring the performance drop (P (ℓ)) on the ARCe benchmark (Clark et al., 2018). Figure 4 compares the pruning effects between standard Pre-LN and LayerNorm Scaling. In the Pre-LN, removing deep layers results in minimal performance degradation, indicating their limited 7 The Curse of Depth in Large Language Models Table 3. Fine-tuning performance () of LLaMA with various normalizations. Method MMLU BoolQ ARC-e PIQA Hellaswag OBQA Winogrande Average Post-LN (Ba, 2016) DeepNorm (Wang et al., 2024) Mix-LN (Li et al., 2024b) Pre-LN (Baevski and Auli, 2019) Pre-LN + LayerNorm Scaling Post-LN (Ba, 2016) DeepNorm (Wang et al., 2024) Mix-LN (Li et al., 2024b) Pre-LN (Baevski and Auli, 2019) Pre-LN + LayerNorm Scaling 22.95 23.60 26.53 24.93 27. 22.95 23.35 23.19 26.54 28.69 LLaMA-250M 26.94 36.62 41.68 40.15 45.24 52.72 61.10 66.34 63. 67.38 LLaMA-1B 25.08 27.06 25.08 45.70 48.85 49.51 52.94 49.51 67.79 67. 37.83 37.86 56.12 38.35 58.17 37.82 37.83 37.83 62.20 61.80 26.17 25.69 30.16 26.34 32. 25.04 26.19 25.04 30.96 33.94 11.60 15.00 18.00 16.20 18.80 13.80 11.80 11.80 17.40 18. 49.56 49.57 50.56 49.01 52.49 49.57 49.49 49.57 50.51 54.30 32.54 35.63 41.34 36.93 43. 31.96 32.67 31.72 43.01 44.87 Figure 4. Performance drop of layer pruning on LLaMA-130M. LayerNorm Scaling enables deep layers to make meaningful contribution to the model. Table 4. Comparison against other scaling methods. Perplexity () Training Tokens Pre-LN + LayerScale + Scaled Initialization + LayerNorm Scaling LLaMA-130M LLaMA-250M 2.2B 26.73 27.93 26.04 25.76 3.9B 21.92 23.45 20.98 20.35 role in representation learning. In contrast, with LayerNorm Scaling, pruning deeper layers leads to more pronounced drop, suggesting they now play more active role in learning. While early layers remain critical in both models, the performance degradation in the LayerNorm Scaling is more evenly distributed across layers, reflecting more balanced learning process. These findings confirm that LayerNorm Scaling mitigates the Curse of Depth by ensuring deeper layers contribute effectively to training. 6. Related Work Layer Normalization in Language Models. LN (Ba, 2016) was initially applied after the residual connection in the original Transformer (Vaswani, 2017), which is known as Post8 LN. Later on, Pre-LN (Baevski and Auli, 2019; Dai et al., 2019; Nguyen and Salazar, 2019) dominated LLMs, due to its compelling performance and stability (Brown et al., 2020; Touvron et al., 2023; Jiang et al., 2023; Bi et al., 2024). Prior works have studied the effect of Pre-LN and Post-LN. Xiong et al. (2020) proves that Post-LN tends to have larger gradients near the output layer, which necessitates smaller learning rates to stabilize training, whereas Pre-LN scales down gradients with the depth of the model, working better for deep Transformers. Wang et al. (2019) empirically confirmed that Pre-LN facilitates stacking more layers and Post-LN suffers from gradient vanishing. The idea of connecting multiple layers was proposed in previous works (Bapna et al., 2018; Dou et al., 2018; Wang et al., 2019). Adaptive Model Initialization (Admin) was introduced to use additional parameters to control residual dependencies, stabilizing Post-LN. DeepNorm (Wang et al., 2024) enables stacking 1000-layer Transformer by upscaling the residual connection before applying LN. Additionally, Ding et al. (2021) proposed Sandwich LayerNorm, normalizing both the input and output of each transformer sub-layer. Takase et al. (2023a) introduced B2T to bypass all LN except the final one in each layer. Li et al. (2024b) recently combines Post-LN and Pre-LN to enhance the middle layers. The Curse of Depth in Large Language Models 7. Conclusion In this paper, we introduce the concept of the Curse of Depth in LLMs, highlighting an urgent yet often overlooked phenomenon: nearly half of the deep layers in modern LLMs are less effective than expected. We discover the root cause of this phenomenon is Pre-LN which is widely used in almost all modern LLMs. To tackle this issue, we introduce LayerNorm Scaling. By scaling the output variance inversely with the layer depth, LayerNorm Scaling ensures that all layers, including deeper ones, contribute meaningfully to training. Our experiments show that this simple modification improves performance, reduces resource usage, and stabilizes training across various model sizes. LayerNorm Scaling is easy to implement, hyperparameter-free, and provides robust solution to enhance the efficiency and effectiveness of LLMs. 8. Impact Statement This paper introduces the Curse of Depth in LLMs to call attention to the AI community an urgent but often overlooked phenomenon that nearly half of layers in modern LLMs are not as effective as we expect. The impact of this phenomenon is large that significant amount of resources used to train LLMs are somehow wasted. We further introduce LayerNorm Scaling to ensure that all layers contribute meaningfully to model training. The result is significant improvement in model efficiency, enabling better performance with fewer computational resources and training tokens. This innovation not only enhances LLM effectiveness across variety of tasks but also reduces the environmental and financial costs of training large-scale models, making LLM development more sustainable and accessible. LayerNorm Scaling presents simple, hyperparameter-free solution that can be easily adopted, offering immediate practical benefits to the AI research community."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jimmy Lei Ba. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. ICLR, 2019. Ankur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao, and Yonghui Wu. Training deeper neural machine translation models with transparent attention. EMNLP, 2018. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling opensource language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. ACL, 2019. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS, 34:1982219835, 2021. Zi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, and Tong Zhang. Exploiting deep representations for neural machine translation. EMNLP, 2018. Razvan-Gabriel Dumitru, Vikas Yadav, Rishabh Maheshwary, Paul-Ioan Clotan, Sathwik Tejaswi Madhusudhan, and Mihai Surdeanu. Layer-wise quantization: pragmatic and effective method for quantizing llms beyond integer bit-levels. arXiv preprint arXiv:2406.17415, 2024. Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ICLR, 2021. 9 The Curse of Depth in Large Language Models Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy KaWei Lee. Llm-adapters: An adapter family for parameterefficient fine-tuning of large language models. EMNLP, 2023. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Diederik Kingma. Adam: method for stochastic optimization. ICLR, 2015. Toan Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention. IWSLT, 2019. Rajpurkar. Squad: 100,000+ questions for machine comprehension of text. EMNLP, 2016. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatronlm: Training multi-billion parameter language models using model parallelism. ICML, 2020. Vedang Lad, Wes Gurnee, and Max Tegmark. The remarkarXiv able robustness of llms: Stages of inference? preprint arXiv:2406.19384, 2024. Shoaib Ahmed Siddiqui, Xin Dong, Greg Heinrich, Thomas Breuel, Jan Kautz, David Krueger, and Pavlo Molchanov. deeper look at depth pruning of llms. ICML, 2024. Pengxiang Li, Lu Yin, Xiaowei Gao, and Shiwei Liu. Owlore: Outlier-weighed layerwise sampled low-rank projection for memory-efficient llm fine-tuning. arXiv preprint arXiv:2405.18380, 2024a. Pengxiang Li, Lu Yin, and Shiwei Liu. Mix-ln: Unleashing the power of deeper layers by combining pre-ln and postln. arXiv preprint arXiv:2412.13795, 2024b. Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. B2t connection: Serving stability and performance in deep transformers. ACL, 2023a. Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023b. Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky. Relora: High-rank training through low-rank updates. In ICLR, 2023. Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herve Jegou. Going deeper with image transformers. In ICCV, pages 3242, 2021. Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. EMNLP, 2020. Haiquan Lu, Yefan Zhou, Shiwei Liu, Zhangyang Wang, Michael Mahoney, and Yaoqing Yang. Alphapruning: Using heavy-tailed self regularization theory for improved layer-wise pruning of large language models. NeurIPS, 2024. Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length. NeurIPS, 2024. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Bhuminand Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact language models via pruning and knowledge distillation. In NeurIPS, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani. Attention is all you need. NeurIPS, 2017. Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. TPAMI, 2024. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek Wong, and Lidia Chao. Learning deep transformer models for machine translation. ACL, 2019. E. T. Whittaker and G. N. Watson. Course of Modern Analysis. Cambridge Mathematical Library. Cambridge University Press, 4 edition, 1996. Yuxin Wu and Kaiming He. Group normalization. In ECCV, pages 319, 2018. 10 The Curse of Depth in Large Language Models Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In ICML, pages 1052410533. PMLR, 2020. Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl): missing secret sauce for pruning llms to high sparsity. ICML, 2024. Biao Zhang and Rico Sennrich. Root mean square layer normalization. NeurIPS, 32, 2019. Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. ICML, 2024. The Curse of Depth in Large Language Models A. Proofs of the Theorems A.1. Proof of Lemma 1 Proof. Given Equation (2) from (Takase et al., 2023b), we have: = xℓ+1 = x ℓ = xℓ + Attn(LN(xℓ)). ℓ + FFN(LN(x ℓ)), Based on our Assumption 1, let Var(Attn(LN(xℓ))) = σ2 Attn. Then we can write: Var(x ℓ) = Var(xℓ) + Var(Attn(LN(xℓ))) + Cov(Attn(LN(xℓ)), Var(xℓ)) = σ2 xℓ + σ2 Attn + ρ1 σxℓ σAttn, where ρ1 is the correlation factor. Similarly, let Var(FFN(LN(x ℓ))) = σ2 FFN. Then we have: where ρ2 is the correlation factor. Thus, the relationship between Var(xℓ+1) and Var(xℓ) becomes: σ2 xℓ+1 = σ(x ℓ)2 + σ2 FFN + ρ2 σx ℓ σFFN, σ2 xℓ+1 = σ2 xℓ + σ2 Attn + σ2 FFN + ρ1 σxℓ σAttn + ρ2 σx σFFN. ℓ (18) (19) (20) (21) A.1.1. VARIANCE OF THE ATTENTION The scaled dot-product attention mechanism is defined as: Attn(Q, K, ) = softmax (cid:18) QK dk (cid:19) V. The softmax function outputs probability distribution over the keys. Let the softmax output be = softmax , where is matrix with each row summing to 1. The final attention output is obtained by multiplying the softmax output with the value matrix : (cid:17) (cid:16) QKT dk Attn(Q, K, ) = AV. To simplify the analysis, we make the following additional assumptions: The softmax output is approximately uniform, meaning each element of is roughly 1/n, where is the number of keys/values. Given this assumption, the variance of the attention is: Var(Attn(Q, K, )) Var(AV ) = 1 n (cid:88) i=1 Var(Vi) = 1 where is the universal weight matrix defined as before. A.1.2. VARIANCE OF THE FEED-FORWARD NETWORK σ2 = σ2 = σ2 . (22) The feed-forward network (FFN) in transformers typically consists of two linear transformations with ReLU activation in between. The FFN can be written as: FFN(x) = W2 ReLU(W1 + b1) + b2. (23) where W1 and W2 are weight matrices, and b1 and b2 are bias vectors. Using the result obtained by Wang et al. (2024), we get: 12 The Curse of Depth in Large Language Models FFN σ2 σ2 W1 σ2 W2 = σ4 . In conclusion: σ2 ℓ = σ2 xℓ = σ2 xℓ = σ2 xℓ + σ2 (1 + + + ρ2 σxℓ σW σ2 σW σ2 σxℓ xℓ 1 σxℓ ). ) Θ(1 + For simplicity, we set the numerator part to 1. Substitute σx ℓ (cid:114) = σxℓ 1 + σ2 σ2 xℓ + ρ2 σW σxℓ . into Equation (21) we get: σ2 xℓ+1 = σ2 xℓ = σ2 xℓ + σ2 + σ4 + ρ1 σxℓ σW + ρ2 σx σ2 ℓ + σ2 + σ4 + ρ1 σxℓ σW + ρ2 σxℓ σ2 + ρ2σ4 2σxℓ + 2σ3 ρ2 σxℓ 2 = σ2 xℓ Θ(1 + 1 σxℓ ). From the result we can generally infer that the variance accumulates layer by layer. The variance with regard to σx1 : σ2 xℓ = σ2 x1 Θ (cid:16)ℓ1 (cid:89) (cid:18) 1 + k=1 (cid:19)(cid:17) . 1 σxk (24) (25) (26) (27) We can also obtain similar result for σ2 ℓ . We observe that for any σ2 xk the entire product is bounded above by: 1, the sequence is increasing, meaning each term in the product is bounded. Consequently, σ2 xℓ σ2 x1 ℓ1 (cid:89) (cid:16) k=1 1 + (cid:115) (cid:17) 1 σx1 = σ2 (cid:0)1 + (cid:17)ℓ1 (cid:115) 1 σx1 = exp Θ(L). (28) Taking the natural logarithm of both sides: (cid:32) log(σ2 xℓ ) = log σ2 x1 (cid:33)(cid:33) ℓ1 (cid:89) k=1 (cid:32) (cid:115) 1 + 1 σ2 xk = ℓ1 (cid:88) k=1 (cid:32) (cid:115) log 1 + (cid:33) 1 σ2 xk + log(σ2 x1 ) (cid:115) ℓ1 (cid:88) (cid:16) k=1 1 σ2 xk 1 2 (cid:32)(cid:115) (cid:33)2 (cid:17) 1 σ2 xk + log(σ2 ). (29) Exponentiating both sides to find the lower bound for σ2 σ2 xℓ σ2 x1 exp xℓ, we obtain: (cid:32)(cid:115) (cid:32)ℓ1 (cid:88) k=1 (cid:33)(cid:33) . 1 2σ2 xk 1 σ2 xk This provides tighter lower bound for σ2 of variance grows exponentially, the lower bound must be sub-exponential. Therefore, for σ2 xℓ xℓ compared to the upper bound of Equation (28). Since we know the upper bound = ℓ, we must have: σ2 xℓ σ2 exp (cid:32)ℓ1 (cid:88) k=1 (cid:18) 1 1 2k (cid:19)(cid:33) = Θ(exp( L)) Θ(L). 13 The Curse of Depth in Large Language Models Therefore, the increasing lower bound for σ2 sub-exponential. xℓ must grows faster than linear function. So, the increasing of variance is A.2. Proof of Theorem 1 . In this proof, we will divide the argument into two parts: first, the calculation of the Lemma 3, and second, the analysis of yℓ x1 Lemma 3. For an L-layered Pre-LN Transformer, yL x1 using Equations (2) and (3) is given by: yL x1 = L1 (cid:89) n=1 (cid:18) yℓ ℓ (cid:19) . ℓ xℓ (30) The upper bound for the norm of yL x1 is: (cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 L1 (cid:89) (cid:16)(cid:16) 1 + l=1 σ2 + ( σx ℓ dFFN)2 (cid:17) (cid:16) 1 + 2dh (cid:18) + 2 + 1 (cid:19) σ2 σxℓ (cid:16) (cid:112) σ2d dhead + (cid:16) 1 + (cid:112)dhead/d (cid:17) (cid:17)(cid:17) . (31) Here, denotes the number of heads, is the sequence length, and d, dFFN, and dhead are the dimension of the embedding, FFN layer and multi-head attention layer, respectively. The standard deviation of WQ, WK, WV , and WFFN at layer ℓ is σ based on Assumption 1. A.2.1. PROOF OF LEMMA 3 Proof. Our derivation follows results in (Takase et al., 2023b), specifically Equation (7), which provides an upper bound on the norm of yℓ x1 as: (cid:13) (cid:13) (cid:13) (cid:13) yℓ (cid:13) (cid:13) (cid:13) (cid:13)2 = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) L1 (cid:89) l=1 yℓ ℓ ℓ xℓ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 . (32) Thus, we can estimate the upper bound of the gradient norm of yℓ x1 for the FFN layer and the self-attention layer, namely, by analyzing the spectral norms of the Jacobian matrices FFN: (cid:13) (cid:13) (cid:13) (cid:13) yℓ ℓ (cid:13) (cid:13) (cid:13) (cid:13)2 Attention: (cid:13) (cid:13) (cid:13) (cid:13) ℓ xℓ (cid:13) (cid:13) (cid:13) (cid:13)2 . We now derive an upper bound of yℓ ℓ 2 as follows: (cid:13) (cid:13) (cid:13) (cid:13) yℓ ℓ (cid:13) (cid:13) (cid:13) (cid:13)2 1 + (cid:13) (cid:13) (cid:13) (cid:13) FFN(LN(x LN(x ℓ) ℓ)) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13) (cid:13) LN(x ℓ) ℓ (cid:13) (cid:13) (cid:13) (cid:13)2 . (33) (34) Let σw1ℓ and σw2ℓ be the standard deviations of 1 and 2 ℓ are given by their standard deviations and dimensions (Vershynin, 2018), so wo have: ℓ and 2 ℓ , respectively. From Assumption 1, the spectral norms of 1 ℓ W12 σ (cid:113) (cid:112) + dFFN. . For simplicity, we assume that d, and dFFN are equal, thus, (cid:13) (cid:13) (cid:13) (cid:13) FFN(LN(x LN(x ℓ) ℓ)) (cid:13) (cid:13) (cid:13) (cid:13)2 = 1 ℓ ℓ 2 σ1σ2( + (cid:112) dffn)2. (35) 14 The Curse of Depth in Large Language Models Finally, we have the following bound: (cid:13) (cid:13) (cid:13) (cid:13) yℓ ℓ (cid:13) (cid:13) (cid:13) (cid:13) 1 + σx ℓ σw1ℓσw2ℓ + ( dFFN) = 1 + σ2 ℓ + ( . dFFN)2 σx ℓ Following similar procedure for the FFN, we rewrite (cid:13) (cid:13) (cid:13) (cid:13) x (cid:13) (cid:13) (cid:13) (cid:13)2 1 + (cid:13) (cid:13) (cid:13) (cid:13) 2 in Equation (33) as: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13)2 Attn(LN(x)) LN(x) LN(x) (cid:13) (cid:13) (cid:13) (cid:13) . (36) (37) Let Z() = concat(head1(), . . . , headh()) and denote the Jacobian of the Z(). We can now express the spectral norm of the Jacobian matrix of attntion as: (cid:13) (cid:13) (cid:13) (cid:13) Attn(LN(xℓ)) LN(xℓ) (cid:13) (cid:13) (cid:13) (cid:13) = (cid:13) (cid:13) (cid:13) (cid:13) ℓ Z(LN(xℓ)) Z(LN(xℓ)) LN(xℓ) (cid:13) (cid:13) (cid:13) (cid:13)2 From (Vershynin, 2018), we know that: = ℓ ℓ 2. (cid:16) (cid:18) ℓ 2 s + 2 + (cid:19) 1 σ3(cid:112) d3dhead + σℓ (cid:16) (cid:112) dhead (cid:17) (cid:17) . + (38) (39) Here is the number of heads, is the sequence length, and the standard deviation of WQ, WK, and WV is σ. By combining the inequalities (36), (39) and (37), and assuming that all σ values are the same for simplicity. we obtain: (cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 L1 (cid:89) (cid:16)(cid:16) 1 + l=1 σ2 + ( σx ℓ dFFN)2 (cid:17) (cid:16) 1 + 2dh (cid:18) + 2 + 1 (cid:19) σ2 σxℓ A.2.2. ANALYSIS OF THE UPPER BOUND (cid:16) (cid:112) σ2d dhead + (cid:16) 1 + (cid:112)dhead/d (cid:17) (cid:17)(cid:17) . (40) As discussed in (Takase et al., 2023b), σ should be sufficiently small, and the standard deviation, σx the condition σ2 σx from 1 to L: or σxℓ should satisfy to maintain the lazy training scheme. Thus, we obtain the following bound for the product over ℓ ℓ ℓ yℓ To find the bound for x1 Equation (25), σxℓ is only one layer ahead of σx deep Transformer networks. Furthermore, based on Lemma 1, we assume that σx . Based on , and this layer does not significantly affect the overall performance of = σxℓ. with respect to ℓ, we simplify the given inequality by approximating σxℓ and σx ℓ ℓ ℓ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 Equation (3) can be expressed in traditional product form (Whittaker and Watson, 1996) for σxℓ: (cid:13) (cid:13) (cid:13) (cid:13) yL (cid:13) (cid:13) (cid:13) (cid:13)2 L1 (cid:89) l=1 (cid:18) 1 + 1 σxℓ + (cid:19) , 1 σ2 xℓ where and = ( + σ2 dFFN)2 (cid:18) + 2dh + 2 + (cid:19) 1 σ2 (cid:16) (cid:112) (cid:17) dhead + 1 + (cid:112)dhead/d , = 2dh (cid:18) + 2 + (cid:19) 1 (cid:112) σ4d dhead, 15 (41) (42) (43) The Curse of Depth in Large Language Models where and are independent of σxℓ, and under our assumption, are treated as constants. From classical infinite series analysis, it is known that as σxℓ grows at faster rate, the upper bound of the product decreases. The proof is omitted here for brevity. For the upper bound on the convergence rate of σ2 = exp(ℓ) without loss of generality. Under this condition, we can derive the following result: xℓ , we assume σ2 xℓ Taking the natural logarithm of the product: (cid:32)L1 (cid:89) (cid:18) 1 + log k=1 (cid:19)(cid:33) ek + e2k = L1 (cid:88) k=1 (cid:18) log 1 + ek + e2k (cid:19) . Using the Taylor series expansion for log(1 + x), and applying this to our sum, we get: (cid:88) k=1 (cid:18) log 1 + ek + e2k (cid:32) (cid:88) (cid:19) = k= ek + e2k 1 2 (cid:19)2 (cid:18) ek + e2k + 1 3 (cid:18) ek + e2k (cid:19)3 (cid:33) . By evaluating the sums for each order of terms, we find that the result is constant. Carrying this out for each term, we obtain: (cid:32)L1 (cid:89) (cid:18) log k=1 1 + ek + e2k (cid:19)(cid:33) 1 + e2 1 1 2 (cid:18) A2 e2 1 + 2 e3 1 + B2 e4 1 (cid:19) . Thus, the product is approximately: (cid:13) (cid:13) (cid:13) (cid:13) yL (cid:13) (cid:13) (cid:13) (cid:13)2 exp (cid:18) 1 + e2 1 2 (cid:18) A2 e2 1 + 2 e3 + B2 e4 1 (cid:19)(cid:19) = M, (44) where is constant. For the lower bound on the convergence rate of σ2 = ℓ without loss of generality. Under this condition, we derive the following result. Taking the logarithm of the product, applying the Taylor series expansion for log(1 + x), and applying this to our sum: xℓ, we assume σ2 xℓ (cid:18) log 1 + (cid:88) k=1 + ek2 (cid:32) (cid:88) (cid:19) = k=1 + ek2 1 2 (cid:19)2 (cid:18) + ek2 + (cid:18) 1 3 + ek2 (cid:19)3 (cid:33) . For the first-order terms: (cid:88) k=1 (cid:18) (cid:19) + ek2 = (cid:88) k= 1 + (cid:88) k=1 1 ek2 . The series (cid:80) k=1 γ and the fact recognize that the harmonic series grows logarithmically: 1 is the harmonic series, which diverges. However, we approximate it using the Euler-Mascheroni constant (cid:88) k=1 1 log + γ (for large n). The other series such as (cid:80) k=1 1 ek2 converge because ek2 grows very rapidly. For higher-order terms, they converge to constant, involving the series (cid:80) k=1 constant. Exponentiating both sides, we get: 1 k2 converges to π2 6 , so they contribute The Curse of Depth in Large Language Models (cid:89) (cid:18) 1 + k=1 (cid:19)"
        },
        {
            "title": "A\nk",
            "content": "+ ek2 exp (A(log + γ) + const) . Thus, the growth rate of the upper bound for (cid:13) (cid:13) (cid:13) yL (cid:13) (cid:13) (cid:13)2 is: (cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 Θ(L). A.3. Proof of Lemma 2 Proof. After scaling, the equation becomes: = xℓ+1 = ℓ + FFN( 1 ℓ LN(x ℓ)), ℓ = xℓ + Attn( 1 ℓ LN(xℓ)). Folloing the same analysis as before, we scale the Attention and FFN sub-layers, yielding: σ2 Attn = 1 nℓ σ2 = 1 ℓ σ2 = σ2 ℓ , σ2 FFN σ2 W1 ℓ σ2 W2 ℓ = σ4 ℓ2 . In conclusion: Similarly, we obtain: σ2 ℓ = σ2 xℓ + σ + ρ2 σxℓ σW ℓ = σ2 xℓ Θ(1 + 1 ℓσxℓ ). σ2 xℓ+1 = σ2 xℓ Θ(1 + 1 ℓσxℓ ). Taking the natural logarithm of both sides: (cid:32) log(σ2 xℓ ) = log σ2 (cid:33)(cid:33) (cid:115) (cid:32) ℓ1 (cid:89) k=1 1 + 1 ℓσ2 xk = ℓ1 (cid:88) k=1 (cid:32) (cid:115) log 1 + (cid:33) 1 ℓσ2 xk + log(σ2 x1 ) (cid:115) ℓ1 (cid:88) (cid:16) k=1 1 ℓσ2 xk 1 2 (cid:32)(cid:115) (cid:33)2 (cid:17) 1 ℓσ2 xk + log(σ2 x1 ). (45) (46) (47) (48) (49) (50) To establish lower bound for σ2 xℓ, we exponentiate both sides. Setting σ2 xℓ = ℓ, we must have: σ2 xℓ σ2 x1 exp (cid:32)ℓ1 (cid:88) k=1 (cid:18) 1 1 2k (cid:19)(cid:33) = Θ(exp(log L)) Θ(L). (51) Therefore, the increasing lower bound σ2 xℓ is greater than linear function. Similarly, assuming σ2 xℓ = ℓ(2ϵ), we have: σ2 xℓ = σ2 x1 ℓ1 (cid:89) (cid:18) 1 + k=1 (cid:19) 1 ℓ2ϵ/2 exp (cid:32)ℓ1 (cid:88) k=1 (cid:33) 1 k2ϵ/2 exp (cid:18) ℓϵ/21 1 ϵ/2 1 (cid:19) Θ(ℓ(2ϵ)) Θ(ℓ2). (52) Here ϵ is small constant with 0 < ϵ 1/4. Therefore, the increasing upper bound of σ2 leading to: xℓ is slower than the ℓ3 function, . σ2 xℓ Θ(L2) 17 The Curse of Depth in Large Language Models A.4. Proof of Theorem 2 Proof. Similarly, after applying the scaling transformation, we derive an upper bound for yℓ ℓ 2 as follows: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) yℓ ℓ (cid:13) (cid:13) (cid:13) (cid:13)2 1 + = 1 + FFN(LN(x LN(x ℓ) σ2 ℓ + ( ℓσx ℓ . dFFN)2 ℓ)) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13) (cid:13) 1 ℓ (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13) (cid:13) LN(x ℓ) ℓ (cid:13) (cid:13) (cid:13) (cid:13)2 Similarly, rewriting Equation (33) after scaling, we have (cid:13) (cid:13) (cid:13) (cid:13) x (cid:13) (cid:13) (cid:13) (cid:13)2 1 + (cid:13) (cid:13) (cid:13) (cid:13) Attn(LN(x)) LN(x) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 ℓ (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13) (cid:13) LN(x) (cid:13) (cid:13) (cid:13) (cid:13) . By combining the bound (53), and inequality (54), and assuming all σ are equal for simplicity, we obtain: (53) (54) (cid:13) (cid:13) (cid:13) (cid:13) yL (cid:13) (cid:13) (cid:13) (cid:13)2 L1 (cid:89) l=1 (cid:16)(cid:16) 1 + σ2 + ( ℓσx ℓ dFFN) (cid:17) (cid:16) 1 + 2dh (cid:18) + 2 + 1 (cid:19) σ2 ℓσxℓ (cid:16) σ2d (cid:112) dhead + (cid:16) 1 + (cid:112)dhead/d (cid:17) (cid:17)(cid:17) . (55) (56) Equation (55) is traditional product form (Whittaker and Watson, 1996) for σxℓ . After scaling, it becomes: (cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 L1 (cid:89) l= (cid:18) 1 + 1 ℓσxℓ + (cid:19) , 1 ℓ2σ2 xℓ where and retain their forms from Equation (42) and Equation (43) and are treated as constants. Regarding the upper bound on the convergence rate of σ2 xℓ , we assume σ2 xℓ the product can be approximated using the properties of infinite products: = ℓ(2ϵ) without loss of generality. For large L, (cid:18) 1 + L1 (cid:89) ℓ=1 (cid:19) ℓ2ϵ/2 + ℓ4ϵ exp (cid:32)L1 (cid:88) (cid:18) ℓ2ϵ/2 ℓ= (cid:19)(cid:33) . + ℓ4ϵ Then, by evaluating the sum in the exponent, we obtain: L1 (cid:89) (cid:18) 1 + ℓ=1 ℓ2ϵ/2 + ℓ4ϵ (cid:19) (cid:18) exp ℓϵ/21 1 ϵ/2 1 + ℓϵ3 1 ϵ 3 (cid:19) . Therefore, we establish the upper bound: (cid:13) (cid:13) (cid:13) (cid:13) yL (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:18) (cid:18) Θ exp ℓϵ/21 1 ϵ/2 1 + (cid:19)(cid:19) ℓϵ3 1 ϵ 3 = ω(1), where ω(1) denotes growth strictly greater than constant as defined before. B. Training Loss Curve We report the training loss curve of Pre-LN and LayerNorm Scaling in Figure 5. 18 (57) (58) (59) The Curse of Depth in Large Language Models Figure 5. Training loss of LLaMA-1B with Pre-LN and LayerNorm Scaling. Figure 6. Variance growth across layers in LLaMA-130M with Pre-LN. Each subplot shows the variance at different training stages (1000, 3000, and 6000 epochs). In all cases, the variance follows an exponential growth pattern as depth increases, indicating that deeper layers experience uncontrolled variance amplification regardless of training progress. C. Variance Growth in Pre-LN Training To analyze the impact of Pre-LN on variance propagation, we track the variance of layer outputs across different depths during training. Figure 6 illustrates the layer-wise variance in LLaMA-130M with Pre-LN at 1000, 3000, and 6000 epochs. Across all stages, variance remains low in shallow layers but grows exponentially in deeper layers, confirming that this issue persists throughout training rather than being temporary effect. This highlights the necessity of stabilization techniques like LayerNorm Scaling to control variance and ensure effective deep-layer learning."
        }
    ],
    "affiliations": [
        "Dalian University of Technology, China",
        "Emory University, USA",
        "Medical Artificial Intelligence Laboratory, Westlake University, China",
        "University of Surrey, UK"
    ]
}