{
    "paper_title": "SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory",
    "authors": [
        "Cheng-Yen Yang",
        "Hsiang-Wei Huang",
        "Wenhao Chai",
        "Zhongyu Jiang",
        "Jenq-Neng Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when managing crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of memories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incorporating temporal motion cues with the proposed motion-aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, showcasing its ability to generalize without fine-tuning. In evaluations, SAMURAI achieves significant improvements in success rate and precision over existing trackers, with a 7.1% AUC gain on LaSOT$_{\\text{ext}}$ and a 3.5% AO gain on GOT-10k. Moreover, it achieves competitive results compared to fully supervised methods on LaSOT, underscoring its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments. Code and results are available at https://github.com/yangchris11/samurai."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 1 2 2 9 1 1 . 1 1 4 2 : r SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory Cheng-Yen Yang Hsiang-Wei Huang Wenhao Chai"
        },
        {
            "title": "Zhongyu Jiang",
            "content": "Jenq-Neng Hwang"
        },
        {
            "title": "University of Washington",
            "content": "{cycyang, hwhuang, wchai, zyjiang, hwang} @ uw.edu"
        },
        {
            "title": "Abstract",
            "content": "The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when managing crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of memories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incorporating temporal motion cues with the proposed motionaware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, showcasing its ability to generalize without fine-tuning. In evaluations, SAMURAI achieves significant improvements in success rate and precision over existing trackers, with 7.1% AUC gain on LaSOText and 3.5% AO gain on GOT-10k. Moreover, it achieves competitive results compared to fully supervised methods on LaSOT, underscoring its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments. Code and results are available at https://github.com/yangchris11/samurai. 1. Introduction Segment Anything Model (SAM) [26] has demonstrated impressive performance in segmentation tasks. Recently, SAM 2 [35] incorporates streaming memory architecture, which enables it to process video frames sequentially while maintaining context over long sequences. While SAM 2 has shown remarkable capabilities in Video Object Segmentation (VOS [46]) tasks, generating precise pixel-level masks for objects throughout video sequence, it still faces challenges in Visual Object Tracking (VOT [36]) scenarios. The primary concern in VOT is maintaining consistent object identity and location despite occlusions, appearance changes, and the presence of similar objects. However, SAM 2 often neglects motion cues when predicting masks for subsequent frames, leading to inaccuracies in scenarios with rapid object movement or complex interactions. This limitation is particularly evident in crowded scenes, where SAM 2 tends to prioritize appearance similarity over spatial and temporal consistency, resulting in tracking errors. As illustrated in Figure 1, there are two common failure patterns: confusion in crowded scenes and ineffective memory utilization during occlusions. To address these limitations, we propose incorporating motion information into SAM 2s prediction process. By leveraging the history of object trajectories, we can enhance the models ability to differentiate between visually similar objects and maintain tracking accuracy in the presence of occlusions. Additionally, optimizing SAM 2s memory management is crucial. The current approach [14, 35] of indiscriminately storing recent frames in the memory bank introduces irrelevant features during occlusions, compromising tracking performance. Addressing these challenges is essential to adapt SAM 2s rich mask information for robust video object tracking. To this end, we propose SAMURAI, SAM-based Unified and Robust zero-shot visual tracker with motionAware Instance-level memory. Our proposed method incorporates two key advancements: (1) motion modeling system that refines the mask selection, enabling more accurate object position prediction in complex scenarios, and (2) an optimized memory selection mechanism that leverages hybrid scoring system, combining the original mask affinity, object, and motion scores to retain more relevant historical information, so as to enhance the models overall tracking reliability. In conclusion, this paper makes the following contributions: We enhance the visual tracking accuruacy of SAM 2 1 Figure 1. Illustration of two common failure cases in visual object tracking using SAM 2: (1) In crowded scene with similar appearances between target and background objects, SAM 2 tends to ignore the motion cue and predict where the mask has the higher IoU score. (2) The original memory bank simply chooses and stores the previous frames into the memory bank, resulting in introducing some bad features during occlusion. by incorporating motion information through motion modeling, to effectively handle the fast-moving and occluded objects. We proposed motion-aware memory selection mechanism that reduces error in crowded scenes in contrast to the original fixed-window memory by selectively storing relevant frames decided by mixture of motion and affinity scores. Our zero-shot SAMURAI achieves state-of-the-art performance on LaSOT, LaSOText, GOT-10k, and other VOT benchmarks without additional training or fine-tuning, demonstrating strong generalization of our proposed modules across diverse datasets. 2. Related Works 2.1. Visual Object Tracking (VOT) Visual Object Tracking (VOT) [36] aims to track objects in challenging video sequences that include variations in object scale, occlusions, and complex backgrounds so as to elevate the robustness and accuracy of tracking algorithms. Siamese-based [10, 52] and transformer-based [12, 47] trackers are common approaches by learning embedding similarity. However, due to lacking self-correction of these trackers in the single forward pass evaluation scheme, they can easily drift toward distractors. To this end, recent works [18, 49] further introduce memory bank and attention to find better mapping between current frame and history information. 2.2. Segment Anything Model (SAM) The Segment Anything Model (SAM) [26] has sparked considerable follow-up research since its introduction. SAM introduces prompt-based segmentation approach, where users could input points, bounding boxes, or text to guide the model in segmenting any object within an image. The use of SAM has wide-ranging applications like in video understanding [7, 38, 39] and editing [6]. Since then, various works have built upon SAM. For example, SAM 2 [35] expands the models capabilities to video segmentation [11], incorporating memory mechanisms for tracking objects across multiple frames in dynamic video sequences. Additionally, efforts have been made to create more efficient variants of SAM for resource-constrained environments, aiming to reduce its computational demands [45, 54]. Research in medical imaging has also adopted SAM for specialized tasks [30]. Recently, SAM2Long [14] uses treebased memory to enhance object segmentation for long video. However, their higher FPS video sequences and deeper memory tree architectures require exponentially more computing power and memory storage due to the overhead of storing exact paths and time-constrained memory paths. On the other hand, our proposed SAMURAI model, which is built upon SAM 2, has been trained on large-scale segmentation datasets to overcome these challenges and ensure good performance and generalization ability. 2.3. Motion Modeling Motion modeling is an important component in tracking tasks, which can be categorized into heuristic and learnable approaches. Heuristic methods, such as the widelyused Kalman Filter (KF) [24], rely on fixed motion priors and predefined hyper-parameters to predict object trajectories. While KF has proven effective in many tracking benchmarks, it often fails in scenarios with intense or abrupt motion. Other methods [1] attempt to counteract intense or abrupt object motion by compensating for camera movement before applying traditional KF-based prediction. However, both the standard and noise scale adaptive (NSA) Kalman Filters [15] come with multitude of hyperparameters, potentially restricting their effectiveness to specific types of motion scenarios. In contrast, learnable motion models have attracted increasing interest due to their data-driven nature. Tracktor [2] is the first to use trajectory boxes as Regions of Interest (RoI) in Faster-RCNN to extract features and regress the objects position across frames. MotionTrack [43] enhances tracking by learning past trajectory representations to predict future movements. MambaTrack [22] further explores different learning-based motion models architecture like transformer [40] and statespace model (SSM) [21]. Our approach is also learningbased motion modeling with an enhanced heuristic scheme. 3. Revisiting Segment Anything Model 2 Segment Anything Model 2 [34] contains (1) an image encoder, (2) mask decoder with prompt encoder, (3) memory attention layer, and (4) memory encoder. We will introduce some preliminaries of SAM 2 and specifically point out the part where SAMURAI is being added. Prompt Encoder. The prompt encoder design follows SAM [35], in which it takes two types of prompts, including sparse (e.g., points, bounding boxes) and dense (e.g., masks). The prompt tokens output by the prompt encoder can be represented as xprompt Ntokens d. In the visual object tracking, where the ground-truth bounding box of the target object of the first frame t0 is provided, SAM 2 takes the positional encoding for the top-left and bottomright points as inputs while the rest of the sequence uses the predicted mask Mt1 from the previous frame as the input to the prompt encoder. Mask Decoder. The memory decoder is designed to take the memory-conditioned image embeddings produced by the memory attention layer along with the prompt tokens Its multi-head from the prompt encoder as its inputs. branches can then generate set of predicted masks, along with the corresponding mask affinity score smask (it is referred to as IoU score in [34, 35]), and one object score sobj for the frame as outputs. = {(M0, smask,0), (M1, smask,1), . . . }. (1) The affinity mask score prediction of SAM 2 is supervised with MAE loss as it can represent the overall confidence of the mask, while the object prediction is supervised with cross-entropy loss to determine whether mask should exist in the frame or not. In the original implementation, the final output mask, = Mi, is selected based on the highest affinity score among the Nmask output masks. = arg max smask,i where sobj,i > 0 (2) i[0,Nmask1] However, the affinity score is not very robust indicator in the case of visual tracking, especially in crowded scenarios where similar objects self-occlude with each other. We introduce an extra motion modeling to keep track of the motion of the target and provide an additional motion score to aid the selection of the prediction. Memory Attention Layer. The Memory attention block first performs self-attention with the frame embeddings and then performs cross-attention between the image embeddings and the contents of the memory bank. The unconditional image embeddings, therefore, get contextualized with the previous output masks, previous input prompts, and object pointers. Memory Encoder and Memory Bank. After the mask decoder generates output masks, the output mask is passed through memory encoder to obtain memory embedding. new memory is created after each frame is processed. These memory embeddings are appended to Memory Bank, which is first-in-first-out (FIFO) queue of the latest memories generated during video decoding. At any given time in the sequence, we can form the memory bank Bt as: Bt = [mt1, mt2, . . . , mtNmem ] (3) which takes the past Nmem frames output as the components of the memory bank. This straightforward fixed-window memory implementation may suffer from encoding the incorrect or lowconfidence object, which will cause the error to propagate considerably when in the context of long sequence visual tracking task. Our proposed motion-aware memory selection will replace the original memory bank composition to ensure that better memory features can be kept and conditioned onto the image feature. 4. Method SAM 2 has demonstrated strong performance in basic Visual Object Tracking (VOT) and Video Object Segmentation (VOS) tasks. However, the original model can mistakenly encode incorrect or low-confidence objects, leading to substantial error propagation in long-sequence VOT. To address the above issues, we propose Kalman Filter (KF)-based motion modeling on top of the multi-masks selection (in 4.1) and an enhanced memory selection based on hybrid scoring system that combines affinity and motion scores (in 4.2). These enhancements are designed to Figure 2. The overview of our SAMURAI visual object tracker. strengthen the models ability to track objects accurately in complex video scenarios. Importantly, this approach does not require fine-tuning, nor does it require additional training, and it can be integrated directly into the existing SAM 2 model. By improving the selection of predicted masks without additional computational overhead, this method provides reliable, real-time solution for online VOT. and the bounding box derived from the Kalman filters predicted state. We then select the mask that maximizes weighted sum of the KF-IoU score and the original affinity score: = arg max Mi (αkf skf (Mi) + (1 αkf ) smask(Mi)). 4.1. Motion Modeling Finally, the update is performed using: Motion modeling has long been an effective approach to Visual Object Tracking (VOT) and Multiple Object Tracking (MOT) [1, 5, 51] in resolving association ambiguities. We employ the linear-based Kalman filter [24] as our baseline to demonstrate the incorporation of motion modeling in improving tracking accuracy. In our visual object tracking framework, we integrate the Kalman filter to enhance bounding box position and dimension predictions, which in turn helps select the most confident mask out of candidates from M. We define the state vector as: (4) = [x, y, w, h, x, y, w, h]T where x, represents the center coordinate of the bounding box, and denote its width and height, respectively, and their corresponding velocities are represented by the dot notation. For each mask Mi, the corresponding bounding box di is derived by computing the minimum and maximum and coordinates of the masks non-zero pixels. The Kalman filter operates in predict-correct cycle, where the state prediction ˆxt+1t is given by: ˆxt+1t = ˆxtt, (5) skf = IoU (ˆxt+1t, M) the KF-IoU score skf is then computed by calculating the Intersection over Union (IoU) between the predicted masks (6) (7) (8) ˆxtt = ˆxtt1 + Kt(zt ˆxtt1) where zt is the measurement, the bounding box derived from the mask we selected, used to update. is the linear state transition matrix, Kn is the Kalman gain, and is the observation matrix. Furthermore, to ensure the robustness of the motion modeling after the targeted object reappears or the poor mask qualities for certain period of time, we also maintain stable motion state where we take consideration of the motion module if and only if the tracked object is being successfully update in the past τkf frames. 4.2. Motion-Aware Memory Selection The original SAM 2 prepares the conditioned visual feature of the current frame based on selecting Nmem from the preIn [34], the implementation simply selects vious frames. the Nmem most recent frames based on the qualities of the target. However, this approach has the weakness of not being able to handle longer occlusion or deformation, which is common in visual object tracking tasks. To construct an effective memory bank of object cues considering motion, we employ selective approach for choosing frames from previous time steps based on three scoring: the mask affinity score, object occurrence score, and motion score. We select the frame as an ideal candidate for memory if and only if all three scores meet their corresponding thresholds (e.g., τmask, τobj, τkf ). We iterate Table 1. Visual object tracking results on LaSOT [16], LaSOText [17], and GOT-10k [23]. LaSOText are evaluated on trackers to be trained with LaSOT. GOT-10k protocol only allows trackers to be trained using its corresponding train split. The T, S, B, represents the size of the ViT-based backbone while the subscript is the search region. Bold represents the best while underline represents the second."
        },
        {
            "title": "Source",
            "content": "CVPR19 CVPR20 CVPR21 ICCV21 ICCV21 Supervised method SiamRPN++ [27] DiMP288 [13] TransT256 [8] AutoMatch255 [53] STARK320 [48] SwinTrack-B384 [28] NeurIPS22 CVPR22 MixFormer288 [12] ECCV22 OSTrack384 [50] CVPR23 ARTrack-B256 [41] CVPR23 SeqTrack-B384 [9] GRM-B256 [20] CVPR23 ICCV23 ROMTrack-B256 [4] WACV24 TaMOs-B384 [32] AAAI24 EVPTrack-B384 [37] AAAI24 ODTrack-B384 [55] AAAI24 ODTrack-L384 [55] CVPR24 HIPTrack-B384 [3] CVPR24 AQATrack-B256 [44] CVPR24 AQATrack-L384 [44] ECCV24 LoRAT-B224 [29] ECCV24 LoRAT-L224 [29] Zero-shot method SAMURAI-T SAMURAI-S SAMURAI-B SAMURAI-L"
        },
        {
            "title": "Ours\nOurs\nOurs\nOurs",
            "content": "AUC(%) LaSOT Pnorm(%)"
        },
        {
            "title": "LaSOText",
            "content": "GOT-10k P(%) AUC(%) Pnorm(%) P(%) AO(%) OP0.5(%) OP0.75(%) 49.6 56.3 64.9 58.2 67.1 71.4 69.2 71.1 70.8 71.5 69.9 69.3 70.2 72.7 73.2 74.0 72.7 71.4 72.7 71.7 74. 69.3 70.0 70.7 74.2 56.9 64.1 73.8 67.5 76.9 79.4 78.7 81.1 79.5 81.1 79.3 78.8 79.3 82.9 83.2 84.2 82.9 81.9 82.9 80.9 83.6 76.4 77.6 78.7 82.7 49.1 56.0 69.0 59.9 72.2 76.5 74.7 77.6 76.2 77.8 75.8 75.6 77.8 80.3 80.6 82.3 79.5 78.6 80.2 77.3 80.9 73.8 75.2 76.2 80.2 34.0 - - - - - - 50.5 48.4 50.5 - 47.2 - 53.7 52.4 53.9 53.0 51.2 52.7 50.3 52. 55.1 58.0 57.5 61.0 41.6 - - - - - - 61.3 57.7 61.6 - 53.5 - 65.5 63.9 65.4 64.3 62.2 64.2 61.6 64.7 65.6 69.6 69.3 73.9 39.6 - - - - - - 57.6 53.7 57.5 - 52.9 - 61.9 60.1 61.7 60.6 58.9 60.8 57.1 60.0 63.7 67.7 67.1 72.2 51.7 61.1 67.1 65.2 68.8 72.4 70.7 73.7 73.5 74.5 73.4 72.9 - 76.6 77.0 78.2 77.4 73.8 76.0 72.1 75. 79.0 78.8 79.6 81.7 61.6 71.7 76.8 76.6 78.1 80.5 80.0 83.2 82.2 84.3 82.9 82.9 - 86.7 87.9 87.2 88.0 83.2 85.2 81.8 84.9 89.6 88.7 90.8 92.2 32.5 49.2 60.9 54.3 64.1 67.8 67.8 70.8 70.9 71.4 70.4 70.2 - 73.9 75.1 77.3 74.5 72.1 74.9 70.7 75.0 72.3 72.9 72.9 76.9 back in time from the current frame and repeat the verification. We select Nmem memories based on the above scoring function and obtain motion-aware memory bank Bt: 5. Experiments 5.1. Benchmarks Bt = {mif (smask, sobj, skf ) = 1, Nmax < t} (9) where Nmax is the maximum number of frames to look back. The motion-aware memory bank Bt is subsequently passed through the memory attention layer and then directed to mask decoder Dmask to perform mask decoding at current timestamp. Note that we follow the Nmem = 7 as the SAM 2 is trained under these specific memory bank settings. The proposed motion modeling and memory selection module can significantly enhance visual object tracking without the need for retraining and does not add any computational overhead to the existing pipeline. It is also modelagnostic and potentially applicable to other tracking frameworks beyond SAM 2. By combining motion modeling with intelligent memory selection, we can enhance tracking performance in challenging real-world applications without sacrificing efficiency. We evaluate the zero-shot performance of our SAMURAI on the following VOT benchmarks: LaSOT [16] is visual object tracking dataset comprising 1,400 videos across 70 diverse object categories with an average sequence length of 2,500 frames. It is divided into training and testing sets, consisting of 1,120 and 280 sequences, respectively, with 16 training and 4 testing sequences for each category. LaSOText [17] is an extension to the original LaSOT dataset, introducing an additional 150 video sequences across 15 new object categories. These new sequences are specifically designed to focus on occlusions and variations in small objects, which is more challenging, and the standard protocol is to evaluate the models trained on LaSOT directly on the LaSOText. GOT-10k [23] comprises over 10,000 video segments of real-world moving objects, spanning more than 560 object 5 Table 2. Visual object tracking results on AUC (%) of our proposed method with state-of-the-art methods on TrackingNet [33], NFS [25], and OTB100 [42] datasets. Bold represents the best while underline represents the second."
        },
        {
            "title": "Trackers",
            "content": "TrackingNet NFS OTB100 Supervised method DiMP288 [13] TransT256 [8] STARK320 [48] KeepTrack [31] AiATrack320 [19] OSTrack384 [50] SeqTrack-B384 [9] HIPTrack-B384 [3] AQATrack-L384 [44] LoRAT-L224 [29] Zero-shot method SAMURAI-L (Ours) 74.0 81.4 82.0 - 82.7 83.9 83.9 84.5 84.8 85.0 61.8 65.7 - 66.4 67.9 66.5 66.7 68.1 - 66.0 - - 68.5 70.9 69.6 55.9 - 71.0 - 72. 85.3 69.2 71.5 Table 3. Ablation on the effectiveness of the proposed modules. Motion Memory AUC(%) Pnorm(%) P(%) 68.32 70.81 72.67 74.23 76.16 78.87 80.67 82.69 73.59 76.47 78.23 80. Table 4. Ablation on the sensitivity of the motion weight αkf . αkf 0.00 0.15 0.25 0.50 AUC(%) Pnorm(%) P(%) 72.67 74.23 73.76 72.92 80.67 82.69 81.86 80.49 78.23 80.21 79.53 78.34 Results on GOT-10k. Table 1 also presents the visual object tracking results on the GOT-10k dataset. Note that the GOT-10k protocol only allows trackers to be trained using its corresponding train split, as some papers may refer to them as one-shot method. SAMURAI-B shows 2.1% improvement on AO and 2.9% on OP0.5 over SAM2.1-B while SAMURAI-L shows 0.6% improvement on AO and 0.7% on OP0.5. All SAMURAI models surpass the stateof-the-art on all metrics on GOT-10k. Results on TrackingNet, NFS, and OTB100. Table 2 presents the visual object tracking results on four widely compared benchmarks. Our zero-shot SAMURAI-L model is comparable to or can surpass the state-of-the-art supervised method on AUC, showcasing the capability of our model on various datasets and generalization ability. Figure 3. SUC and Pnorm plots of LaSOT and LaSOText. classes and 80+ motion patterns. key aspect of GOT-10k is its one-shot evaluation protocol, which requires trackers to be trained exclusively on the designated training split, with 170 videos reserved for testing. TrackingNet [33] is large-scale tracking dataset that covers wide selection of object classes in broad and diverse contexts in the wild. It has total of 30,643 videos split into 30,132 training videos and 511 testing videos. NFS [25] consists of 100 videos with total of 380k frames captured with higher frame rate (240 FPS) cameras from real-world scenarios. We use the 30 FPS version of the data with artificial motion blur following other VOT works. OTB100 [42] is one of the earliest visual tracking benchmarks that annotated sequences with attribute tags. It contains 100 sequences with an average length of 590 frames. 5.2. Quantitative Results Results on LaSOT and LaSOText. Table 1 presents the visual object tracking results on the LaSOT and LaSOText datasets. Our method, SAMURAI, demonstrates significant improvements over both the zero-shot and supervised methods on all three metrics (shown in Figure 3). Although the supervised VOT method such as [29, 55] show quite impressive results, the zero-shot SAMURAI in contrast show its great generalization ability with comparalbe zero-shot performance. Furthermore, all SAMURAI models surpass the state-of-the-art on all metrics on LaSOText. 6 Table 5. Visual object tracking results of the proposed SAMURAI compare to the baseline SAM-based tracking method."
        },
        {
            "title": "Trackers",
            "content": "SAM2.1-T [34] SAMURAI-T SAM2.1-S [34] SAMURAI-S SAM2.1-B [34] SAMURAI-B SAM2.1-L [34] SAMURAI-L AUC(%) 66.70 69.28 (+2.58) 66.47 70.04 (+3.57) 65.97 70.65 (+4.68) 68.54 74.23 (+5.69) LaSOT Pnorm(%) 73.70 76.39 (+2.69) 73.67 77.55 (+3.88) 73.54 78.69 (+4.15) 76.16 82.69 (+6.53) P(%) AUC(%) 71.22 73.78 (+2.56) 71.25 75.23 (+3.98) 70.96 76.21 (+5.25) 73.59 80.21 (+6.62) 52.25 55.13 (+2.88) 56.11 57.99 (+1.88) 55.51 57.48 (+1.97) 58.55 61.03 (+2.48) LaSOText Pnorm(%) 62.03 65.60 (+2.57) 67.57 69.60 (+2.03) 67.17 69.28 (+2.11) 71.10 73.86 (+2.76) P(%) 60.30 63.72 (+3.42) 65.81 67.73 (+1.92) 64.55 67.09 (+2.54) 68.83 72.24 (+3.41) Table 6. Attribute-wise AUC(%) Results for LaSOT [16] and LaSOText [17]. Trackers ARC BC CM DEF FM FOC IV LR MB OV POC ROT SV VC LaSOT SAM2.1-B [34] SAMURAI-B % Gain 67.7 62.8 64.7 69.6 73.1 68.0 +7.6% +8.3% +8.0% 59.8 67.1 72.0 64.1 +7.3% +11.4% +9.4% +10.5% +14.1% +4.6% +14.8% +7.0% +8.3% +7.3% +7.2% 56.1 62.5 63.0 69.6 56.2 64. 62.8 68.0 67.1 70.2 57.6 63.0 65.5 70.3 64.6 69.1 55.4 63. SAM2.1-L [34] SAMURAI-L % Gain Trackers SAM2.1-B SAMURAI-B % Gain SAM2.1-L SAMURAI-L % Gain 70.8 67.3 73.1 75.7 +8.9% +8.1% +11.0% +6.9% 64.3 69. 69.4 77.0 61.9 58.4 70.4 63.9 +9.4% +12.7% +14.3% +13.2% +8.0% +7.8% 63.9 72.8 67.8 73.8 59.3 66.8 59.7 67. 68.0 72.8 +7.1% +8.2% +9.2% +16.8% 61.1 71.4 67.2 72.7 68.1 73.7 ARC BC CM DEF FM 42.1 58.6 53.4 54.8 45.9 67.8 +2.6% +6.5% +15.7% +10.6% +9.0% 75.4 73.3 49.3 52. 75.6 56.6 58.4 77.5 +3.2% +4.0% +16.5% +5.2% 53.2 55.4 62.8 73.1 46.1 50.7 +9.9% LaSOText FOC 42.5 45.9 +8.0% 47.6 50.9 +6.9% IV 69.5 67.3 -2.3% 71.4 69.0 -3.5% LR MB OV POC ROT SV VC 57.1 56.3 45.3 47.4 61.8 56.7 +4.6% +2.6% +6.1% +11.5% +2.1% +3.3% +8.2% 46.1 48.1 61.6 62.9 42.6 43.7 54.4 56.2 50.9 47.1 48.8 52.1 53.3 49.4 +6.3% +5.4% +4.3% 61.9 60.0 60.9 66.6 +3.2% +2.6% +4.0% +7.4% 63.2 64.7 57.7 59.9 5.3. Ablation Studies Effect of the Individual Modules. We demonstrate the effect of the with or without memory selection on various settings in Table 3. Both of the proposed modules had positive impact on the SAM 2 model, while combining both can achieve the best AUC on the LaSOT dataset with an AUC of 74.23% and Pnorm of 82.60%. Effect of the Motion Weights. We showcase the effect of the weighting of the score of deciding which mask to trust in Table 4. The trade-off between motion score and mask affinity score demonstrates significant impact on tracking performance. Our experiments reveal that setting the motion weight αmotion = 0.2 yields the best AUC and Pnorm score on the LaSOT dataset, indicating an optimal balance enhances both accuracy and robustness in mask selection. Baseline Comparison. To demonstrate the effectiveness of the proposed motion modeling and motion-aware memory selection mechanism in SAMURAI, we conduct detailed apple-to-apple comparison of the SAM 2 [34] at all of the backbone variations on LaSOT and LaSOText. The baseline SAM 2 employs the original memory selection and directly predicts the mask with the highest IoU score. Table 5 shows that the proposed method consistently improves upon the baseline with significant margin on all three metrics, which underscores the robustness and generalization of our approach across different model configurations. Attribute-Wise Performance Analysis. We analysis the LaSOT and LaSOText based on the 14 attributes defined in [16, 17]. In Table 6, SAMURAI shows consistent success in improving upon the original baseline across all attributes in both datasets but the label IV (Illumination Variation) label on LaSOText. By considering motion scoring, the performance gains on attributes like CM (Camera Motion) and FM (Fast Motion) are the largest among the rest, the SAMURAI has 16.5% and 9.9% gain on CM and FM respectively from LaSOText dataset which is considered one of the most challenging datasets in VOT. Furthermore, the occlusion-related attributes like FOC (Full Occlusion) and POC (Partial Occlusion) also greatly benefited from the proposed motion-aware instance-level memory selection, which showed steady improvement across all model variants and datasets. These findings suggest that the SAMURAI incorporates simple motion estimation to better account for global camera or rapid object movements for better tracking. Runtime Analysis. The incorporation of the motion modeling and an enhanced memory selection method into"
        },
        {
            "title": "LoRAT",
            "content": "SAMURAI (Ours) GT SAM2.1 (Baseline) SAMURAI (Ours) GT Figure 4. Visualization of tracking results comparing SAMURAIwith existing methods. (Top) Conventional VOT methods often struggle in crowded scenarios where the target object is surrounded by objects with similar appearances. (Bottom) The baseline SAM-based method suffers from fixed-window memory composition, leading to error propagation and reduced overall tracking accuracy due to ID switches. our tracking framework introduces minimal computational overhead, and the runtime measurements conducted on one NVIDIA RTX 4090 GPU remain consistent with the baseline model. ing systematic way of deciding which to trust is valuable. These enhancements benefit the existing framework by providing better guidance for visual tracking without the need to retrain the model or fine-tune it. 5.4. Qualitative Results Qualitative comparison between SAMURAI and other methods [3, 29, 34] are shown in Figure 4. SAMURAI demonstrates superior visual object tracking results in scenes where multiple objects with similar appearances are present in the video. The short-term occlusions in these examples make it challenging for existing VOT methods to predict or localize the same object consistently over time. Furthermore, the comparison between SAMURAI and the original baseline with visualized masks showcases the improvement gained by adding the motion modeling and memory selection modules, the predicted masks are not always reliable source to serve as memory therefore hav6. Conclusion We present SAMURAI, visual object tracking framework built on top of the segment-anything model by introducing the motion-based score for better mask prediction and memory selection to deal with self-occlusion and abrupt motion in crowded scenes. The proposed modules show consistent improvement on all variations of the SAM models across multiple VOT benchmarks on all metrics. This method does not require re-training nor fine-tuning while demonstrating robust performance on multiple VOT benchmarks with the capability of real-time online inferences."
        },
        {
            "title": "References",
            "content": "[1] Nir Aharon, Roy Orfaig, and Ben-Zion Bobrovsky. Botsort: Robust associations multi-pedestrian tracking. arXiv preprint arXiv:2206.14651, 2022. 2, 4 [2] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. In Proceedings of Tracking without bells and whistles. the IEEE/CVF international conference on computer vision, pages 941951, 2019. 3 [3] Wenrui Cai, Qingjie Liu, and Yunhong Wang. Hiptrack: Visual tracking with historical prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 5, 6, 8 [4] Yidong Cai, Jie Liu, Jie Tang, and Gangshan Wu. Robust object modeling for visual tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 95899600, 2023. 5 [5] Jinkun Cao, Jiangmiao Pang, Xinshuo Weng, Rawal Khirodkar, and Kris Kitani. Observation-centric sort: Rethinking sort for robust multi-object tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 96869696, 2023. 4 [6] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2304023050, 2023. 2 [7] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jeng-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. [8] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81268135, 2021. 5, 6 [9] Xin Chen, Houwen Peng, Dong Wang, Huchuan Lu, and Han Hu. Seqtrack: Sequence to sequence learning for visual object tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14572 14581, 2023. 5, 6 [10] Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang, and Rongrong Ji. Siamese box adaptive network for visual In Proceedings of the IEEE/CVF conference on tracking. computer vision and pattern recognition, pages 66686677, 2020. 2 [11] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13161326, 2023. 2 [12] Yutao Cui, Cheng Jiang, Limin Wang, and Gangshan Wu. Mixformer: End-to-end tracking with iterative mixed attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1360813618, 2022. 2, 5 the IEEE/CVF conference on computer vision and pattern recognition, pages 71837192, 2020. 5, [14] Shuangrui Ding, Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Yuwei Guo, Dahua Lin, and Jiaqi Wang. Sam2long: Enhancing sam 2 for long video segmentation with training-free memory tree. arXiv preprint arXiv:2410.16268, 2024. 1, 2 [15] Yunhao Du, Junfeng Wan, Yanyun Zhao, Binyu Zhang, Zhihang Tong, and Junhao Dong. Giaotracker: comprehensive framework for mcmot with global information and In Proceedings of optimizing strategies in visdrone 2021. the IEEE/CVF International conference on computer vision, pages 28092819, 2021. 3 [16] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: high-quality benchmark for large-scale single object tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53745383, 2019. 5, 7 [17] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Harshit, Mingzhen Huang, Juehuan Liu, et al. Lasot: high-quality large-scale single object tracking benchmark. International Journal of Computer Vision, 129: 439461, 2021. 5, 7 [18] Zhihong Fu, Qingjie Liu, Zehua Fu, and Yunhong Wang. Stmtrack: Template-free visual tracking with space-time In Proceedings of the IEEE/CVF conmemory networks. ference on computer vision and pattern recognition, pages 1377413783, 2021. 2 [19] Shenyuan Gao, Chunluan Zhou, Chao Ma, Xinggang Wang, and Junsong Yuan. Aiatrack: Attention in attention for transIn European Conference on Comformer visual tracking. puter Vision, pages 146164. Springer, 2022. [20] Shenyuan Gao, Chunluan Zhou, and Jun Zhang. Generalized relation modeling for transformer tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1868618695, 2023. 5 [21] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 3 [22] Hsiang-Wei Huang, Cheng-Yen Yang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. Exploring learningbased motion models in multi-object tracking. arXiv preprint arXiv:2403.10826, 2024. 3 [23] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: large high-diversity benchmark for generic object tracking in the wild. IEEE transactions on pattern analysis and machine intelligence, 43(5):15621577, 2019. 5 [24] Rudolph Emil Kalman. new approach to linear filtering and prediction problems. 1960. 2, 4 [25] Hamed Kiani Galoogahi, Ashton Fagg, Chen Huang, Deva Ramanan, and Simon Lucey. Need for speed: benchmark for higher frame rate object tracking. In Proceedings of the IEEE international conference on computer vision, pages 11251134, 2017. 6 [13] Martin Danelljan, Luc Van Gool, and Radu Timofte. ProbIn Proceedings of abilistic regression for visual tracking. [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 1, 2 [27] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. Siamrpn++: Evolution of siamese viIn Proceedings of sual tracking with very deep networks. the IEEE/CVF conference on computer vision and pattern recognition, pages 42824291, 2019. 5 [28] Liting Lin, Heng Fan, Zhipeng Zhang, Yong Xu, and Haibin Ling. Swintrack: simple and strong baseline for transformer tracking. Advances in Neural Information Processing Systems, 35:1674316754, 2022. 5 [29] Liting Lin, Heng Fan, Zhipeng Zhang, Yaowei Wang, Yong Xu, and Haibin Ling. Tracking meets lora: Faster training, In European Conferlarger model, stronger performance. ence on Computer Vision, pages 300318. Springer, 2025. 5, 6, 8 [30] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15(1):654, 2024. 2 [31] Christoph Mayer, Martin Danelljan, Danda Pani Paudel, and Luc Van Gool. Learning target candidate association to keep track of what not to track. In Proceedings of the IEEE/CVF international conference on computer vision, pages 13444 13454, 2021. [32] Christoph Mayer, Martin Danelljan, Ming-Hsuan Yang, Vittorio Ferrari, Luc Van Gool, and Alina Kuznetsova. Beyond sot: Tracking multiple generic objects at once. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 68266836, 2024. 5 [33] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, and Bernard Ghanem. Trackingnet: large-scale In dataset and benchmark for object tracking in the wild. Proceedings of the European conference on computer vision (ECCV), pages 300317, 2018. 6 [34] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 3, 4, 7, 8 [35] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 1, 2, 3 [36] Giorgio Roffo, Simone Melzi, et al. The visual object trackIn Computer VisionECCV ing vot2016 challenge results. 2016 Workshops: Amsterdam, The Netherlands, October 810 and 15-16, 2016, Proceedings, Part II, pages 777823. Springer International Publishing, 2016. 1, 2 [37] Liangtao Shi, Bineng Zhong, Qihua Liang, Ning Li, Shengping Zhang, and Xianxian Li. Explicit visual prompts for visual object tracking. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 48384846, 2024. [38] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 2 [39] Enxin Song, Wenhao Chai, Tian Ye, Jenq-Neng Hwang, Xi Li, and Gaoang Wang. Moviechat+: Question-aware sparse memory for long video question answering. arXiv preprint arXiv:2404.17176, 2024. 2 [40] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 3 [41] Xing Wei, Yifan Bai, Yongchao Zheng, Dahu Shi, and Yihong Gong. Autoregressive visual tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 96979706, 2023. [42] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Object tracking benchmark. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(9):18341848, 2015. 6 [43] Changcheng Xiao, Qiong Cao, Yujie Zhong, Long Lan, Xiang Zhang, Zhigang Luo, and Dacheng Tao. Motiontrack: Learning motion predictor for multiple object tracking. Neural Networks, 179:106539, 2024. 3 [44] Jinxia Xie, Bineng Zhong, Zhiyi Mo, Shengping Zhang, Liangtao Shi, Shuxiang Song, and Rongrong Ji. Autoregressive queries for adaptive tracking with spatio-temporal transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19300 19309, 2024. 5, 6 [45] Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, et al. Efficientsam: Leveraged masked image pretraining for efficient segment anything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1611116121, 2024. 2 [46] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327, 2018. 1 [47] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformer for visual tracking. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1044810457, 2021. 2 [48] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformer for visual tracking. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1044810457, 2021. 5, 6 [49] Tianyu Yang and Antoni Chan. Learning dynamic memory networks for object tracking. In Proceedings of the European conference on computer vision (ECCV), pages 152 167, 2018. [50] Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Joint feature learning and relation modeling for tracking: one-stream framework. In European Conference on Computer Vision, pages 341357. Springer, 2022. 5, 6 [51] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every 10 detection box. In European conference on computer vision, pages 121. Springer, 2022. 4 [52] Zhipeng Zhang and Houwen Peng. Deeper and wider siamese networks for real-time visual tracking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45914600, 2019. 2 [53] Zhipeng Zhang, Yihao Liu, Xiao Wang, Bing Li, and Weiming Hu. Learn to match: Automatic matching network design for visual tracking. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1333913348, 2021. [54] Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment anything. arXiv preprint arXiv:2306.12156, 2023. 2 [55] Yaozong Zheng, Bineng Zhong, Qihua Liang, Zhiyi Mo, Shengping Zhang, and Xianxian Li. Odtrack: Online dense In Proceedtemporal token learning for visual tracking. ings of the AAAI Conference on Artificial Intelligence, pages 75887596, 2024. 5,"
        }
    ],
    "affiliations": ["University of Washington"]
}