{
    "paper_title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "authors": [
        "Yuan Cheng",
        "Fengzhuo Zhang",
        "Yunlong Hou",
        "Cunxiao Du",
        "Chao Du",
        "Tianyu Pang",
        "Aixin Sun",
        "Zhuoran Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the $Δ$-th sub-diagonal for some offset $Δ$. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 7 9 2 8 0 . 1 0 6 2 : r Demystifying the Slash Pattern in Attention: The Role of RoPE Yuan Cheng1, Fengzhuo Zhang1,, Yunlong Hou1, Cunxiao Du2 Chao Du2 Tianyu Pang2 Aixin Sun3 Zhuoran Yang4 . Figure 1: Illustration of the emergence of Slash-Dominant Heads (SDHs). Attention scores are determined by pre-PE queries, keys, and Rotary Position Embedding (RoPE) (left bottom). Because token embeddings lie approximately on cone, queries/keys are almost rank-one, and nearly identical across tokens (left top), so RoPE primarily governs variation of attention scores across tokens. Then RoPEs highand medium-frequency components interact constructively at specific lags, producing the attention score peaks at offset (right top). As result, SDHs emerge and are Out-Of-Distribution (OOD) generalizable (right bottom). Abstract Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the -th sub-diagonal for some offset . These patterns play key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by mediumand high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between mediumand high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, 1National University of Singapore, 2Sea AI Lab, 3Nanyang Technological University, 4Yale University Email:{yuan.cheng,fzzhang,yhou}@u.nus.edu, cnsdunm@gmail.com, {duchao, tianyupang}@sea.com, axsun@ntu.edu.sg, zhuoran.yang@yale.edu Equal contribution, Project Lead. we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across wide range of domains, including natural language processing, reasoning, and planning (Brown et al., 2020). Given prompt that contains question, an LLM can interactively generate coherent and contextually appropriate answer. crucial ingredient behind this ability is the models capability to pass information across different tokens in the sequence, most notably, from the prompt tokens to the answer tokens. Understanding how information propagation is implemented within LLMs is therefore an important question in its mechanistic interpretability. Since modern LLMs are built on the Transformer architecture, where the communication between tokens is achieved primarily by the self-attention mechanism, the models information-passing behavior is closely linked to specific structural patterns in its attention scores. Prior work has identified several characteristic attention score patterns, including antidiagonal, blocksparse, vertical and slash patterns (Jiang et al., 2024; Xu et al., 2025). Among these, the slash pattern, where the attention score concentrates along the -th sub-diagonal of the attention score matrix for some offset N, is particularly intriguing. We refer to attention heads exhibiting slash patterns as SDHs. SDHs and their slash patterns play important algorithmic roles in LLMs. For example, they enable In-context Learning (ICL) via the induction head circuit (Elhage et al., 2021; Olsson et al., 2022), which is special case of an SDH with = 1. Concretely, the induction head circuit (Figure 4) consists of forwarding head and feature-matching head. In the forwarding head, attention scores concentrate along the first sub-diagonal, so each token attends primarily to its immediate predecessor (prefix), effectively forwarding semantic features from the prefix to the current token. In addition, another line of work leverages slash patterns to help accelerate long-context inference (Jiang et al., 2024; Xu et al., 2025; Zhao et al., 2025; Lai et al., 2025; Li et al., 2025). More generally, SDHs with diverse values of are prevalent in modern open-source LLMs (see Figure 3). These SDHs enable token at position to attend directly to the token at position , thereby passing information from earlier tokens to later ones. Their widespread presence and functional importance naturally motivate our central research question: How do pretrained LLMs implement SDHs using their transformer architectures? We tackle this question via both thorough empirical studies and rigorous theoretical analysis. In nutshell, we find that the the emergence of SDHs is intrinsic to the transformer architecture itself, and mainly attributed to the low-rankness of query and key matrices, and Position Embedding (PE), particularly, RoPE (Su et al., 2024). We show this via thorough empirical studies and rigorous theoretical analysis. Empirical Studies (Section 4). We focus on open-source LLMs such as Gemma7B, Qwen2.5-7B-Instruct, and Llama38B-Instruct (Gemma Team, 2024; Grattafiori et al., 2024; Qwen Team, 2025), which are decoder-only Figure 2: Mind map of our empirical studies. 2 (a) Average attention score matrix of L18H7. (b) Average attention score matrix of L21H15. (c) Average attention score matrix of L1H3. (d) Average attention score matrix of L0H7. (e) Average attention score matrix of L1H11. (f) Average attention score matrix of L2H6. Figure 3: Average of attention score matrices in Qwen2.5-7B-Instruct with prompts from LongBench. We denote the a-th head at b-th layer as LbHa in this paper. In panels (a)(c), attention concentrates on the sub-diagonals with small offsets 0, 1 and 2, respectively. In panels (d)(f), it also concentrates on sub-diagonals with large offsets exceeding 500. transformers with RoPE. Our empirical studies consist of three parts, detailed below. For clarity, their overall structure is illustrated in the mind map shown in Figure 2. (i) (SDHs are Intrinsic) First, we study whether SDHs depend on the input prompt or are intrinsic to the model itself. To this end, we compare the attention scores of the in-distribution prompts to those generated by randomly generated prompts. We observe that the slash dominance pattern persists even in the OOD setting. This implies that SDHs are largely independent of the specific input prompt, and they arise mainly due to an intrinsic algorithmic mechanism of the transformer model. Note that attention scores are determined by the interplay of the queries and keys, and the rotational matrices of RoPE. We proceed to examine how each of these components contributes to the emergence of SDHs. (ii) (Role of Queries and Keys) Perhaps surprisingly, our experiments reveal that the pre-PE queries and keys are low-rank, and particularly, almost rank-one. As result, for any token, the pre-PE queries and keys of an SDH point in the same direction, which implies that the semantic contents of queries and keys contribute little to differentiating attention scores. Consequently, the variations in attention scores across tokens has to be driven almost entirely by RoPE. Furthermore, we show that these rank-one queries and keys appear because (a) token embeddings lie on cone and (b) the weight matrices WK and WQ project these embeddings to the main axis of the cone. (iii) (Role of RoPE) As result of RoPE and rank-one queries and keys, for any i, N, the pre-softmax attention logit from position attending to position admits Fourier-like decomposition. Specifically, we 3 have AttnLogit(i, j) = d/2 l= Al cos θl (i j) + φl , (1) (cid:88) (cid:1) l=1 and {φl}d/2 l=1 are the RoPE frequencies. Here {Al}d/ where is the hidden dimension, {θl}d/2 l=1 are amplitudes and phases, which are almost invariant across tokens thanks to the rank-one structure of the pre-PE queries/keys. Consequently, the slash pattern, peaks of the attention logits in (1) when = , is entirely determined by the frequencies {θl}d/2 l=1. For more fine-grained understanding, we compare the contributions of individual frequency components in the attention logits. We observe that highand medium-frequency components play dominant role in forming slash patterns, whereas low frequencies contribute little. (cid:0) We illustrate the mechanism of SDH in Figure 1 and summarize the main takeways of the empirical studies as follows. Takeaways of Empirical Experiments: SDHs are an intrinsic architectural effect of the transformer models. They emerge from an almost rank-one structure of the pre-PE queries and keys, which suppresses the variations of semantic information across tokens. Moreover, the highand medium-frequencies of RoPE interact constructively at specific offset N, producing attention-score peaks at . Theoretical Analysis (Section 5). To complement the empirical studies, we further show that SDHs emerge from gradient-based training under conditions found by the empirical experiments. In particular, we adopt shallow attention-only transformer equipped with RoPE, which is trained on ICL regression tasks. We assume that the token embeddings lie on cone, which is premise for the queries and keys being almost rank-one, and is empirically validated by pretrained LLMs. Moreover, we introduce slash-dominance frequency condition on RoPE that quantitatively characterizes the behavior of RoPE frequencies. Under these two conditions, we prove that SDHs emerge from gradient-based training and characterize the full training dynamics. These results provide theoretical guarantees for the hypothesized mechanisms of SDHs found by experiments. Takeaways of Theoretical Analysis: When RoPE satisfies slash-dominance frequency condition and the token embeddings lie on cone, models trained under these conditions are proven to exhibit SDHs and generalize effectively to OOD input, theoretically demonstrating the sufficiency of these conditions for the emergence of SDHs. Roadmap. The rest of the paper is organized as follows: Section 2 reviews related works, Section 3 introduces the background of transformer architecture. Section 4 presents our empirical studies for small , while Section 5 develops theoretical results in the same small regime. Section 6 then extend these results to large . Finally, Section 7 discusses potential applications and extensions of our results. Detailed experiment results are provided in Appendices to D. Proof sketches and detailed proofs are provided in Appendices to H."
        },
        {
            "title": "2 Related Works",
            "content": "Rotary Position Embeddings (RoPE). Proposed by Su et al. (2024), RoPE introduced multiplicative rotations on queries and keys so attention implicitly depends on relative positions, and became the default PE in many LLMs (Qwen Team, 2025; Yang et al., 2024; Gemma Team, 2024; Grattafiori et al., 2024). Following RoPE, series of works extended the context length window of the pretrained models by modifying 4 the base frequency of RoPE (Xiong et al., 2024; Roziere et al., 2023), interpolating the position indexes and frequencies (Chen et al., 2023; Peng et al., 2024; Ding et al., 2024; Zhong et al., 2025), and controlling the RoPE feature gaps (Wang et al., 2024b). Another line of work investigated the mechanism behind RoPE. Barbero et al. (2025) showed that rather than simply inducing attention decay with distance, models like Gemma-7B used RoPEs high frequencies to build robust positional attention heads and low frequencies to carry semantic information, and further proposed modified variant (p-RoPE) that improves performance. Induction Head Circuit. First introduced by Elhage et al. (2021); Olsson et al. (2022), induction head circuit is specialized cascade of attention heads in transformer models that play central role in incontext learning. The induction head circuit typically involves two attention heads, forwarding head and feature-matching head, working in tandem through forward-then-match process to transmit semantics and complete an answer, as in Figure 4. To examine its emergence closely, subsequent empirical work has explored controlled synthetic settings (Reddy, 2024; Bietti et al., 2023; Edelman et al., 2024; Singh et al., 2024). In particular, Edelman et al. (2024) designed Markov-chainbased in-context learning task and showed that transformers develop statistical induction heads through multi-phase training. Bietti et al. (2023) revealed two-phase learning process: the rapid acquisition of global bigram statistics, followed by the slower development of induction heads. From theoretical perspective, Nichani et al. (2024); Chen et al. (2024b); Edelman et al. (2024); Ekbote et al. (2025) investigated induction heads under underlying causal structures such as trees or n-gram Markov chains, Barbero et al. (2025)and Wang et al. (2024a) demonstrated that two-layer transformers can efficiently represent both standard and generalized induction head mechanisms. However, all these works neglected the contribution of RoPE to the induction head. Concretely, they employed either vanilla one-hot PE (Nichani et al., 2024) or ALiBi (Wang et al., 2024a) and its variants (Chen et al., 2024b; Ekbote et al., 2025). As result, their assumptions and conclusions did not fully reflect the empirical behavior of many real-world LLMs. Xie et al. (2022); Zhang et al. (2025) considered in-context learning from different Bayesian perspective. Figure 4: The forwarding head forwards semantic information from the prefix to the current token. In the feature-matching head, the target question token (is) matches tokens whose prefixes exhibit similar semantics to the target, enabling the model to generate the correct continuation PE. We provide additional discussion of related work in Appendix A."
        },
        {
            "title": "3 Preliminaries",
            "content": "In this section, we introduce the mathematical details of the Causal Self-Attention (CSA) model with RoPE, which serves as the building block of the LLMs studied in this paper. Transformer Architecture. We consider decoder-only transformer models, which are composed of three main parts: token embedding module, stack of identical transformer blocks, and final output layer 5 (usually softmax layer for language modeling). The model takes sequence of tokens as input and maps it to sequence of vectors via the token embedding module, known as token embeddings. These embeddings are then processed by the transformer blocks. Each transformer block processes sequence of vectors and outputs sequence of vectors of the same dimension. Concretely, the input to the first transformer block is the token embeddings, and the input to any subsequent block is the output of the preceding block. The output of the final block is then passed to the output layer to produce the final result, such as predicting the next token in the sequence. Throughout this paper, we let denote prompt, which is sequence of tokens of length = (P ). We let denote the hidden dimension of the transformer model. That is, each transformer block maps sequence of vectors of dimension to sequence of vectors of dimension d. Rotary Position Embedding (RoPE). For any angle ϕ, rotation by angle ϕ in R2 is represented by the unitary matrix ρ(ϕ) = (cid:20) cos ϕ sin ϕ cos ϕ sin ϕ (cid:21) R22. (2) Without loss of generality, we assume that is even to simplify the presentation. RoPE encodes the absolute positional information into rotation matrices involving sequence of pre-specified set of decreasing frequencies, denoted by ϑ = (θ1, , θd/2). Here, θℓ is the ℓ-th frequency of ϑ for each ℓ [d/2]. Specifically, the information of each token position [N ] is embedded into matrix Rϑ,i = diag ρ(i θ1), , ρ(i θd/2) Rdd, (3) (cid:0) (cid:1) where diag() denotes the diagonal matrix with the given entries on the diagonal. Thus, intuitively, RoPE proposes to represent each position as rotations with angles θ1, , θd/2. Practically, these frequencies are chosen as an exponentially decreasing sequence, with high frequencies (small ℓ, large θℓ) corresponding capturing local syntactic structures and low frequencies (large ℓ, small θℓ) capturing long-range dependencies. classic choice of ϑ, proposed by Su et al. (2024), is θℓ = 100002ℓ/d for ℓ [d/2]. In this paper, we consider general ϑ and will investigate the conditions under which it performs well. To simplify the notation, for any ϑ and [N ], we let ℜϑ,i : Rd Rd denote the RoPE operator with frequency sequence ϑ applied to vector at position i. Then, for any vector Rd, we write ρ(iθ1) ℜϑ,i(v) = Rϑ,iv = . . . ρ(iθd/2) v1:2 ... vd1:d = ρ(iθ1)v1:2 ... ρ(iθd/2)vd1:d , where we let vj:k denote the sub-vector of from the j-th to the k-th components, for all j, [d]. In other words, after applying RoPE, the (2ℓ 1)-th and 2ℓ-th components of the vector are rotated by an angle of θℓ, for all ℓ [d/2]. In the sequel, we omit the subscripts ϑ and in ℜϑ,i when there is no ambiguity, and with slight abuse of notation, we allow the operator ℜ to act on row vector and, row-wise, on matrix. Causal Self-Attention with RoPE. key component of each transformer block is the Causal Self-Attention (CSA) mechanism. We focus on the CSA that uses RoPE to encode positional information. In the following, we provide the mathematical details of the CSA with RoPE. The trainable parameters of the CSA are the attention weight matrices WQ, WK, WV Rdd1. Here d1 = d/num heads < d, where num heads stands for the number of attention heads. For any prompt with tokens, we let := H(P ) = (h1, . . . , hN ) RN denote the hidden states of the prompt. Here, is either the token embeddings or the output of the preceding transformer blocks. CSA takes as the input and maps it to sequence of vectors of dimension d. We define the CSA output in the following four steps. (i) (Computing Queries, Keys, and Values.) First, the CSA maps hidden states to sequence of queries, keys, and values, which are denoted by = HWQ = (q1, , qN ), = HWK = (k1, , kN ), and = HWV = (v1, , vN ), respectively1. (ii) (Applying RoPE to Queries and Keys.) Then, we apply RoPE to the query matrix and key matrix K. Specifically, for each qi, ki Rd at position [N ], we get the rotated vectors as qi = ℜϑ,i(qi) and qN ) and ki = ℜϑ,i(ki). We denote q1, , k1, , kN ). = ( = ( (iii) (Computing Attention Scores using (cid:101) according to (cid:101) (cid:101) and (cid:101) K.) Then we compute the attention scores using (cid:101) (cid:101) (cid:101) (cid:101) (cid:101) (cid:101) = S(P ) := softmax(M( )) RN ."
        },
        {
            "title": "Q and",
            "content": "K, (cid:101) (cid:101) (4) Here, is causal mask operator such that for any matrix A, M(A)i,j is Ai,j when and otherwise. Moreover, softmax() is the softmax operator, which is applied in row-wise manner. For any row vector R1N , the i-th component of softmax(u) is given by softmax(u)i = eui/ j=1 euj . Each entry of ) is called logit. By the definition of RoPE in (3) and the causal mask M, the (i, j)-th logit is M( given by (cid:101) (iv) Finally, the CSA output is computed as weighted sum of the values, where the weights are the attention scores in (4). Specifically, the CSA output is given by kj = qiRϑ,jikj when and otherwise. (cid:80) (cid:101) (cid:101) (cid:101) (cid:101) (cid:101) CSA H; W{Q,K,V }, ϑ = S(P )V = softmax(M( ))HWV RN d. (5) (cid:1) (cid:0) By now, we have defined CSA on the hidden states RN of given prompt . From the next section, we consider that the prompt is randomly drawn either from the pretraining prompt distribution D, on which the pretrained models are trained, or from an explicitly specified distribution D, whose support may extend beyond that of the pre-training distribution D, i.e., it may generate out-of-distribution inputs. In D[f (P )], where {D, D}. that case, for statistic of interest (P ), we analyze its expected behavior In particular, we will study the statistical properties of CSA and the attention scores in Sections 4 to 6. (cid:101) (cid:101)"
        },
        {
            "title": "4 Empirical Study of Slash-Dominance of Attention",
            "content": "In this section, we empirically study the emergence of slash-dominance of attention in LLMs, which is special attention pattern observed in various LLMs. As shown in Figure 3, on certain heads, the attention scores have relatively high magnitude on certain sub-diagonals with various values of offsets N. We refer to such sub-diagonal winning pattern as slash-dominance. To systemically examine the emergence of the slash-dominant pattern, we first define slash-dominance with mathematical rigor, then report the SDHs found on various LLMs empirically. Finally, we investigate why these special heads emerge. In particular, we focus on three models: Gemma-7B, Llama3-8B-Instruct, and Qwen2.5-7B-Instruct, all of which adopt RoPE as their PE. Mathematical Definition of Slash-Dominance. Recall that the attention score of an attention head is computed as in (4), which involves the queries, keys, and RoPE. Slash-dominance appears when this attention score matrix has high values on certain sub-diagonal line with offset . Definition 4.1 ((κ, )-Slash-Dominance). Let be the distribution of prompts. For any given lag and threshold κ [0, 1], an attention head is said to be κ slash-dominant at lag , if average slash score := EP 1 (P ) (cid:20) (P ) i=+1 Si,i(P ) (cid:21) κ. (6) (cid:88) 1Different from the standard formulation, Qwen2.5 family introduces special modification where the query vector qi (similarly for ki) includes an additional bias term bQ, such that qi = hi + bQ. 7 Here, (P ) is the length of the prompt , S(P ) is the attention score matrix on this head for the prompt , and Si,j(P ) is the (i, j) component in the attention score matrix S(P ), denoting the attention score from position to j. We highlight that both (P ) and S(P ) depend on the prompt , which is randomly sampled according to the pretraining data distribution D. In the following, we refer to the left-hand side of (6) as the average slash score at lag . Intuitively, it measures the average attention paid to tokens that are positions before the current token. In the attention score matrix, these connections form slash line parallel to the main diagonal, which motivates the name slash-dominance. As shown in this definition, an attention head is an SDH if the average attention score on some sub-diagonal line with offset is larger than threshold κ. It only concerns single slash line with lag as in Figure 5. This definition includes the induction head circuits as special case. In particular, the forwarding head of an induction head circuit always copies the information from the previous token, which corresponds to specialized SDH with = 1 and κ 1 (Olsson et al., 2022). Furthermore, the choice of κ in (6) is crucial. Setting it too low will identify too many heads as SDHs and thus not specific enough, while setting it too high will identify too few heads and thus not sensitive enough. Empirical Identification of SDHs. We identify SDHs in three LLMs: Qwen2.5-7B-Instruct (Qwen Team, 2025), Llama3-8B-Instruct (Grattafiori et al., 2024), and Gemma-7B (Gemma Team, 2024). To ensure that the identified SDHs are prominent, we set κ = 0.1 with fixed context length of 6000. We let denote the distribution of prompts in the LongBenchV2 benchmark (Bai et al., 2025), which provides heterogeneous long-context workload and yields stable cross-model statistics. To approximate the expectation in (6), we sample 500 prompts from LongBenchV2 and truncate each to 6000 tokens, ensuring consistent context length across models and avoiding context-extension artifacts. Following prior observations that early tokens (including BOS and warm-up positions) exhibit disproportionately large attention scores (Xiao et al., 2023), we exclude attention entries involving positions 14 when computing Si,i to prevent these anomalous values from dominating the slash-dominance statistics. Figure 5: Slash-Dominance at . Illustration of We find that all three models exhibit SDHs with < 5, reflecting short-range dependencies in attention. In Fig. 3(a)(c), we plot the attention maps of the identified SDHs in Qwen2.5-7B-Instruct with = 0, 1, 2, respectively. In these heads, tokens at relative distances of 0, 1, or 2 from any target token receive prominent attention. We further remark that when smaller value of κ is used, as in Fig. 3(d)(f), slash dominance also arises at large offsets > 1000, with much smaller average slash scores on the order of 103. Accordingly, in this section, we study the characteristics and mechanisms of SDHs with small offsets, i.e., < 5, while the analysis of long-range SDHs is deferred to Section 6. Additional experimental details are provided in Appendix B, and the full list of identified SDHs is provided in Appendix D."
        },
        {
            "title": "4.1 OOD Generalization of SDHs",
            "content": "To understand the mechanism of SDHs, we begin by posing the following question: Are SDHs sensitive to prompt distribution or intrinsic to the model itself ? We answer this question by examining whether SDHs generalize to arbitrary OOD contexts. When we change the prompt distribution from to another distribution D, if the condition in (6) still holds for the same threshold κ, then we say that the SDHs are OOD generalizable. This means that the identified SDHs are independent of the choice of the prompt distribution, and thus intrinsic to the LLM model itself. In other words, regardless of the prompted tokens, the heads identified in Appendix consistently apply the same mechanism to form the slash pattern. 8 To test OOD generalization, we evaluate the average slash scores defined in (6) with special prompt distribution D, where each token of the prompt is sampled i.i.d. from the uniform distribution over the alphabet. We sample 500 OOD prompts, each with 6000 random tokens. We use these OOD prompts to compute the average slash scores as given in (6) for the identified SDHs. (a) Average attention score matrix of L18H7. (b) Average attention score matrix of L21H15. (c) Average attention score matrix of L1H3. Figure 6: Average attention score matrices of SDHs with small in Qwen2.5-7B-Instruct with prompts whose tokens are i.i.d. samples from the uniform distribution over the alphabet. SDHs can generalize to OOD prompts. We plot average attention scores of identified SDHs under OOD prompts in Figure 6. These SDHs are from Qwen2.5-7B-Instruct and include L18H7, L21H5, and L1H3 for = 0, 1, 2, and they are the same SDHs reported in Figure 3. As shown in Figure 6, the same slash pattern persists under OOD prompts. We observe the same results for Llama3-8B-Instruct and Gemma-7B, whose details are deferred to Appendix D. Furthermore, we quantitatively assess OOD generalization by computing the average slash scores for all the identified SDHs with in-distribution and OOD prompts, and comparing the ratios. That is, we compute ratio = average slash score with OOD prompts average slash score with LongBench prompts for all the identified SDHs, whose complete list is in Appendix D. We show the box plot of these ratios in Figure 7. We observe that the average slash scores under OOD prompts are generally higher, or at least comparable to those under in-distribution prompts. Detailed headlevel results for the OOD case are provided in Table 3 in Appendix D. Furthermore, for the identified SDHs, as long as the average slash scores on OOD prompts remain high, even if they differ from the in-distribution scores, these heads can still retrieve and forward the semantics of preceding tokens to the current token. Indeed, as shown in the box plot, most of the average slash scores on OOD prompts are larger than half of those on the in-distribution prompts and are therefore at least 0.05 (recall that we set κ = 0.1 when identifying SDHs). As result, most of the identified SDHs remain valid when is replaced by OOD prompts and κ is reduced to 0.05. Therefore, the slash-dominance pattern generalizes beyond the pretraining distribution. Emergence of the slash-dominance pattern is not relevant to the semantic meaning of the prompts, but is intrinsic to the model architecture. The main finding is summarized as follows. Figure 7: Ratio of average slash scores with OOD and in-distribution prompts for SDHs with small . 9 Takeaway 1: The SDHs identified by taking 0 4 and κ = 0.1 in (6) are able to generalize to OOD prompts. As result, the slash-dominance behavior is intrinsic to the model architecture and not due to the semantic meaning of the prompts."
        },
        {
            "title": "4.2 Approximate Low-Rankness of pre-PE Queries and Keys",
            "content": "Since the emergence of SDHs is intrinsic to the model, it is natural to ask how the transformer architecture contributes to it. Note that attention scores of CSA with RoPE are determined by the pre-PE queries and keys, and RoPE, as shown in (4). In the following, we examine these components in detail. We first focus on the pre-PE queries and keys and study the following question: How do the pre-PE queries and keys contribute to the emergence of SDHs? We show that, interestingly, on SDHs, the pre-PE queries and keys make almost no contribution to differentiating attention scores. In particular, as we will show in the following, on SDHs, the pre-PE queries and keys are almost low-rank, particularly rank-one. This means that the semantic contents of the queries and keys are nearly identical across tokens and thus contribute little to the emergence of SDHs. (a) Hidden state of L18H7 after PCA. (b) Queries of L18H7. (c) Keys of L18H7. (d) Hidden state of L21H15 after PCA. (e) Queries of L21H15. (f) Keys of L21H15. Figure 8: These figures show the queries and keys before the RoPE implementation, as well as the hidden states for Qwen2.5-7B-Instruct. The queries and keys each have dimension 128, and the hidden states are reduced to 128 dimensions for demonstration. To motivate the exploration, we visualize two SDHs of Qwen2.5-7B-Instruct: L18H7 with = 0 and L21H15 with = 1 in Figure 8. As shown in the right two columns, the pre-PE queries and keys are highly similar across tokens, implying that the query and key matrices are almost low-rank, particularly rank-one. In contrast, the hidden states of different tokens exhibit substantial variation. Additional visualizations for more heads and models are provided in Figures 29 and 32 in Appendix D, which consistently exhibit the same structural pattern as Figure 8. This preliminary observation motivates us to examine the low-rankness of the pre-PE queries and keys in detail. Assessing Low-Rankness of Matrix. We adopt the following procedure to assess the low-rankness of matrix RN d, which contains d-dimensional features of tokens. We perform Singular-Value Decomposition (SVD) of and obtain the singular values σ1 σ2 . . . σd. Then we measure the spectral decay of using the following two quantities: rℓ(X) = σ2 ℓ i=1 σ2 , and Rτ (X) = min ℓ [d] (cid:110) ℓ i=1 (cid:88) ri(X) τ for τ [0, 1]. (7) (cid:111) (cid:80) Here, rℓ(X) measures the proportion of power captured by the ℓ-th singular value in terms of the squared singular values. Moreover, Rτ (X) is the minimal subspace dimension required to account for τ of the matrixs total power, which can be viewed as the effective rank. In the following, we use r1(X) (rℓ(X) with ℓ = 1) to measure how close matrix is to being rank-one. In particular, if is close to being rank-one, r1(X) should be close to one. Similarly, we use Rτ (X) with τ 1 to measure how close matrix is to being low-rank. For example, if is close to rank-r matrix for some < d, then Rτ (X) = for some τ close to one. In the sequel, we set τ = 0.95 in experiments. When the meaning of r1(X) and Rτ (X) is clear from the context, we write r1 and Rτ for brevity. (cid:12) (cid:12) (cid:12) (a) Average r1() of all heads and SDHs with small . (b) Average R0.95() of all heads and SDHs with small . Figure 9: Comparison of average r1, R0.95 of SDHs with those of all heads. For small , at least one of or exhibits substantially lower average ranks for SDHs compared to all heads. Pre-PE Queries and Keys are Almost Rank-One. According to (7), we compute r1 and R0.95 for the query and key matrices and on each attention head. In Figure 9, we respectively report the average values of these metrics over all heads and over SDHs only. Head-wise values are provided in Table 3 of Appendix D. Particularly, Figure 9(a) shows that, for each model, at least one of r1(Q) and r1(K) strictly exceeds 0.9 on average over SDHs only, and these values are strictly lower on the other heads. Figure 9(b) shows that the effective ranks of and are low only on the SDHs, while these matrices on the other heads have much higher ranks. These results show that the pre-PE queries and keys have substantially lower ranks on SDHs, and at least one of the queries and keys is almost rank-one. This rank-one observation is consistent with Figure 8-(b) and (c). We highlight this finding as follows. 11 Takeaway 2: On the SDHs identified by taking 0 4 and κ = 0.1 in (6), the pre-PE queries and keys all have low effective ranks, measured via R0.95 defined in (7). Moreover, at least one of and is close to being rank-one."
        },
        {
            "title": "4.3 How to Get Approximately Rank-One Queries and Keys?",
            "content": "A natural follow-up question is: How SDHs achieve the approximately rank-one queries and keys, and what the shared low-dimensional space induced by and represents? (a) r1() related to queries. (b) r1() related to keys. (c) R0.95() related to queries. (d) R0.95() related to keys. Figure 10: Comparison of average r1 and R0.95 for query/key matrices relative to the hidden states and their corresponding weight matrices, for SDHs with small . Both hidden states and the weight matrices WQ, WK exhibit much higher rank than the resulting pre-PE queries and keys. Since the queries and keys are obtained by = HWQ and = HWK, where consists of the hidden states, and WQ and WK are the weight matrices, natural conjecture is that either or one of WQ and WK is low-rank. To test this hypothesis, we compute the average values of r1 and R0.95 for H, WQ (resp. WK), and (resp. K) over the identified SDHs. These values are plotted in Figure 10. This figure clearly shows that the effective ranks of H, WQ, and WK are much higher than those of and K, and thus clearly refutes this hypothesis. Therefore, low-rankness of and must arise from the interaction between the hidden states and weight matrices WQ and WK. Analysis of Token Embeddings. In general, analyzing the hidden states is difficult, since at each layer they are formed through complex transformations of all preceding layers and vary with the prefix sequence due to prior attention. To simplify, we focus on the 0-th layer, where is the input to the first transformer layer, i.e., the token embeddings. In this case, we set the prompt as the concatenation of all tokens, whose 12 length is the size of the alphabet. Let = H(P ) denote the token embeddings of this prompt, whose i-th vector is denoted by hi Rd for each i. With these hidden states, in the first transformer layer, the i-th query of CSA is given by qi = hi + bQ for Qwen2.5-7B-Instruct. Recall that Qwen2.5-7B-Instruct has additional bias terms. We have similar expressions for the i-th key ki using hi, WK (and bK). hi for Gemma-7B and Llama3-8B-Instruct, and qi = Metrics for Subspace Alignment. Similar to rℓ and Rτ defined in (7), we introduce metrics that characterize how well vector Rd aligns with the leading subspaces of WQ and WK. Recall that WQ Rdd1 where is the latent dimension of the model and d1 = d/num heads with num heads being the ℓ=1 are the sigular number of attention heads. Let WQ = values in descending order. Note that {vℓ}d1 ℓ=1 Rd. For any ℓ [d1], we let ℓ(x) denote the index of ℓ-th larget element among {(σi ℓ denote the SVD of WQ, where {σℓ}d1 d1 ℓ=1 σℓvℓu i=1. Then we define x)2}d1 (cid:80) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) rℓ(x, WQ) := σℓ(x)v i=1 ℓ(x)x σiv (cid:1) (cid:0) 2 2 , and Rτ (x, WQ) := min ℓ [d1] (cid:26) ℓ i=1 ri(x, WQ) τ , (8) (cid:27) (cid:101) (cid:0) (cid:1) (cid:101) (cid:80) rℓ(x, WK) and (cid:101) rℓ(x, WQ) characterizes Rτ (x, WK) in similar way. Intuitively, where τ [0, 1]. We can define Rτ (x, WQ) is small when is only spanned by how well aligns with the ℓ(x)-th singular vector of WQ, and limited number of vectors among {vi}d1 i=1. This quantity enables us to quantify how transformed hidden (cid:101) (cid:101) states allocate energy across parameter-induced subspaces and, in turn, assess the coupling between hidden Rτ (Q, WQ)) as states and the attention weights. In the following, for brevity, we write rℓ(Q) (resp. Rτ (Q)), and analogously for K. rℓ(Q, WQ) (resp. (cid:101) (cid:101) (cid:101) (cid:101) (cid:101) (cid:101) (a) (cid:101)r1() and r1() related to queries. (b) R0.95() and (cid:101)R0.95() related to queries. Figure 11: Comparison of average r1, R0.95, R0.95 for query/key matrices relative to the 0-th layer hidden states (token embeddings) for SDHs with small in Gemma-7B, Llama3-8B-Instruct, and Qwen2.5- (cid:101) 7B-Instruct. r1, and (cid:101) r1 and Demystifying Why and are Approximately Rank-One in Zeroth Layer. We plot the average R0.95 in Figures 11, and compare them with r1 and R0.95 respectively. These average values of values are computed over all the tokens and the layer zero SDHs. This figure shows that the behaviors of R0.95 closely match those of r1 and R0.95 in Gemma-7B and Llama3-8B-Instruct, but differ in (cid:101) Qwen2.5-7B-Instruct. This confirms that in Gemma-7B and Llama3-8B-Instruct, each token embedding concentrates most of its energy approximately in one-dimensional principal subspace of WQ and WK. That is, each token embedding aligns well with low-rank (and almost rank-one) subspaces of WK and WQ. r1 and (cid:101) (cid:101) (cid:101) It is natural to ask whether there exists special one-dimensional principal subspace of each WQ or WK that aligns with all the token embeddings. To examine this, we define the dominating singular vector of each 13 Figure 12: Average of the relative variation of token embeddings projected onto the dominant subspace of the SDHs in the 0-th layer of Gemma-7B and Llama3-8B-Instruct. Figure 13: The average norms of hi and hi over the alphabet and SDH, together with the average norms of bQ and bK over SDH in the 0-th layer of Qwen2.5-7B-Instruct."
        },
        {
            "title": "WQ as",
            "content": "{vQ, uQ} = {vℓ , uℓ }, where ℓ = arg maxℓ[d] σℓv ℓ RMSN(x) 2 . xX Here, is the set of token embeddings, and RMSN() denotes root-mean-square (RMS) normalization (Zhang and Sennrich, 2019), which is implemented before the attention modules in the pretrained models. We define {vK, uK} similarly for each WK. Note that each head has different {WK, WQ} and thus it is possible to have different dominating singular vectors. We examine how well the token embeddings align with these vectors by computing the relative variation (RV), which is defined as (cid:88) (cid:0) (cid:1) RV(v) = std mean (cid:0) {vRMS(x) X} {vRMS(x) X} (cid:1) . (cid:0) (cid:12) (cid:12) Here, std() and mean() denote the standard deviation and mean of set, respectively. We report the average values of RV(vQ) and RV(vK) over the attention heads in layer zero in Figure 12. The detailed values of each head are deferred to Table 11 in Appendix D.5. As baseline, we also report RV(vrand) where vrand is drawn from the multivariate normal distribution N(0, Id) on Rd. This figure shows that QRMS(x) and KRMS(x) are highly concentrated for Gemma-7B and moderately concentrated for Llama3-8B-Instruct. In other words, for each x, the component perpendicular to vQ or vK has small magnitude, compared with the parallel component. This implies that all token embeddings align well with vQ and vK on each head. Thus, for Gemma-7B and Llama3-8B-Instruct, in the zeroth layer, (resp. K) is approximately rank-one because all the token embeddings lie approximately on cone centered around vQ (resp. vK). (cid:1)(cid:12) (cid:12) For Qwen2.5-7B-Instruct, we additionally calculate the norms of bias parameters bQ (resp. bK) and the average values of hi) over the alphabet, which are reported in Figure 13. Detailed values of each SDHs are deferred to Table 12 in Appendix D.5. This figure shows that the average norm of hidden states is much larger than that of bias parameters, suggesting that the approximate rank-one structure of and is primarily driven by overly large bias parameters. hi (resp. Combining all these facts, we have complete picture of why and are approximately rank-one in the zeroth layer for the three models. The conclusion is summarized as follows. Takeaway 3: In the zeroth layer, or of SDHs are approximately rank-one because of one of the following two reasons: (i) the overly large bias parameter bQ or bK (Qwen2.5-7B-Instruct) or (ii) the fact that token embeddings are approximately on cone C(v, c) = {x Rd vx/x = c} for some axis vector and scalar (Gemma-7B and Llama3-8B-Instruct). In this subsection, we show that at least one of pre-PE queries and keys of SDHs is approximately low-rank and, in particular, rank-one, as indicated by their high r1 values. This implies that queries and 14 keys across different tokens are largely similar. These shared query and key vectors are then transformed by RoPE, which introduces the position information by rotating different two-dimensional sub-vectors in queries and keys with different frequencies, as in (3). The inner product between the RoPE-transformed queries and keys form the attention logit matrix. Therefore, in the SDHs, the slash pattern, which involves difference in attention scores across different tokens, is primarily determined by the position information introduced by RoPE. In the following subsection, we aim to answer the question: How do RoPE and its frequencies contribute to SDHs?"
        },
        {
            "title": "4.4 Collaboration of Frequencies in RoPE Determines Slash Pattern",
            "content": "qi and Recall that we use kj to denote the RoPE-transformed queries and keys, respectively. (See Section 3 for the definition of CSA with RoPE.) We first examine how RoPE gives rise to SDHs when pre-PE queries and keys are approximately rank-one. To this end, we decompose each entry of the attention logit matrix according to the used frequencies in RoPE as (cid:101) (cid:101) kj := d/2 ℓ=1 Rϑ,iqi 2ℓ1:2ℓ Rϑ,jkj (cid:101) (cid:88) (cid:2) Rϑ,iqi (cid:3) 2ℓ1:2ℓ (cid:2) Rϑ,jkj 2ℓ1:2ℓ = (cid:3) (cid:88) d/2 ℓ=1 InP(i, j, ℓ), (9) (cid:101) where we define InP(i, j, ℓ) = 2ℓ1:2ℓ. Here, RoPE multiplies the input with the matrix Rϑ,i (defined in (3)), which is block-diagonal and consists of sequence of 2 2 rotation matrices. (cid:3) Each 2 2 rotation matrix rotates two-dimensional sub-vector of qi by an angle θℓ. In this way, the sum above can be expressed as Fourier-like form with the inner product (cid:3) (cid:2) (cid:2) InP(i, j, ℓ) = Aℓ cos θℓ (i j) + φℓ , (10) (cid:0) (cid:1) where Aℓ and φℓ are almost independent of and j, and θℓ comes from RoPE. Since and are approximately rankone, the direction of qi is approximately the same for all (similar for kj). We further compute the relative variation of the norms of query and key vectors and report their averages over tokens and SDHs in Figure 14. As shown in the figure, the relative variation is at most 0.12, indicating that the norms of qi (and similarly kj) are nearly constant. Consequently, both the directions and magnitudes of the queries and keys are approximately invariant across token pairs. Thus, Aℓ and φℓ are approximately identical across token pairs, as they only depend on queries and keys. From (10), it follows that for any token pair (i, j), the corresponding logit, and hence the attention score, depends only on the relative distance j, which leads to the slash pattern. Figure 14: Average of the relative variation of the norm of the query vector qi and key ki for small in Gemma-7B, Llama3-8B-Instruct and Qwen2.5-7B-Instruct. Contributions of RoPE Frequencies. Next, we examine how the different RoPE frequencies collectively contributed to the slash pattern according to (9). Note that InP(i, j, ℓ) only involves the influence of the ℓ-th frequency on the logit from position to j. We visualize InP(i, j, ℓ) with fixed to = 100 and varying within [1, i] in Figure 15, which is based on Qwen2.5-7B-Instruct. Here, each column corresponds to the influence of one frequency on different token positions. We observe clear periodic patterns within columns, which reflect the rotational effect of RoPE on queries and keys. The magnitude of influence is proportional to the variation within each column. Qualitatively, highand medium-frequency components (ℓ [1, 42]) exhibit greater variation than low-frequency components (ℓ [43, 64]). 15 (a) InP(100, j, ℓ) of L18H7. (b) InP(100, j, ℓ) of L21H15. (c) InP(100, j, ℓ) of L1H3. Figure 15: Heatmaps of InP(i, j, ℓ) on various SDHs of Qwen2.5-7B-Instruct. We set = 100 and let vary. Qwen2.5-7B-Instruct has 64 frequencies (1 ℓ 64). The variation within each column shows the influence of each frequency. Qualitatively, highand medium-frequency components ℓ [1, 42] exhibit greater variation. For more quantitative characterization, we evaluate slash-dominance after selectively removing certain frequencies when computing the attention score. Specifically, we split the 64 frequencies into three groups high (ℓ [1, 21]), medium (ℓ [22, 42]), and low (ℓ [43, 64]). For each group, we remove six frequencies uniformly in each interval, e.g., ℓ = 1, 5, 9, 13, 17, 21 for ℓ [1, 21] (10% of the total), whose corresponding sub-vectors are left unrotated, and compute the attention logits still as in (9). Using these modified attention logits, we compute attention scores using the softmax function and average slash score as Definition 4.1. We compute the ratio of the new score to the original score for each identified SDHs, and show the box plot in Figure 16. We observe that, when removing high frequencies, the average slash score decreases in all three models. Removing the medium frequencies leads to mild decrease, while the low frequencies yield the least drop. Therefore, the high frequencies are the most critical for SDHs, medium frequencies have moderate impact, and low frequencies contribute the least. Figure 16: The figure quantifies the effect of low-, medium-, and high-frequency components on SDHs by reporting, for each band, the ratio of the average slash score after removing that band to the original average score. Takeaway 4: The slash pattern appears because the attention logits approximately admit Fourier-like decomposition as in (9) and (10). The highand medium-frequency components in RoPE are more important than the low-frequency components for SDHs. In summary, in this section, we demystify the emergence of SDHs via empirical studies. Our main findings are listed as in Takeaways 1-4. In particular, our reasoning logic goes as follows: (i) First, we mathematically characterize the slash pattern in Definition 4.1 and use that to identify SDHs empirically on three LLMs. (ii) We show that these identified SDHs remain valid on OOD prompts, which implies that slash patterns are intrinsic to the model architectures. (iii) We show that the pre-PE queries and keys on the SDHs are approximately low rank. This implies that the pre-PE queries and keys of the same SDH approximately point to the same direction across different tokens. This further means that RoPE is the only architectural component of the LLM that contributes to the across-token differences in attention scores of the slash pattern. (iv) By writing the attention logits into Fourier-like decomposition, we show that the highand medium-frequencies of RoPE play more important role in forming the slash patterns. Furthermore, in Section 4.3, by focusing on the token embeddings, we investigate why the queries and keys are approximately rank-one. The key observation is that the token embeddings lie approximately on cones, which are then transformed into almost the same direction by WQ or WK."
        },
        {
            "title": "5 Theoretical Study of Shallow Transformers",
            "content": "In this section, we provide theoretical support for the empirical findings in the small regime in Section 4. We show that under two conditions similar to those identified by experiments (Takeaways 2 to 4): (i) token embeddings lie approximately on cone and (ii) RoPE is dominated by mediumand highfrequency components, SDHs provably emerge. Moreover, the learned SDHs are provably OOD generalizable (Takeaway 1). In particular, we focus on simple but fundamental ICL setting, and train shallow transformer with RoPE via Gradient Descent (GD). We show that the transformer learns two-layer induction-head circuit, and the first-layer attention head is an SDH. The theoretical results are organized as follows. We present the ICL data model in Section 5.1 and then propose slash-dominance frequency condition that quantitatively characterizes the RoPE frequency interactions in Section 5.2. It can be viewed as mathematically rigorous formulation of Takeaway 4. We present the transformer architecture and training algorithm in Section 5.3 and 5.4. Finally, we present the main theoretical results in Section 5.5."
        },
        {
            "title": "5.1 Data Model",
            "content": "yq := We consider regression task under the ICL framework, where the SDH plays an important role. Such setting is commonly adopted in theoretical studies of ICL and induction heads (Zhang et al., 2024a; Huang et al., 2024; Chen et al., 2024a; Yang et al., 2024; Zhang et al., 2024b). The goal of in-context regression is to correctly learn predictor y(xq) that approximates the true response = (xq) for question xq DX and target function DF, based on prompt containing demonstration examples with the same function . Specifically, the data are sampled as follows. We first draw function DF. Next, we independently sample collection of Nin inputs x1, . . . , xNin together with question xq, all from DX. For each xi, we define the label as yi = (xi). Then we construct the prompt = (x1, y1, , xNin , yNin , xq), which has fixed length of = 2Nin + 1. : (x) = Task Distribution. We consider specialized linear regression family: = w, with RdX , w2 , which is widely adopted in recent studies for in-context learning (Zhang et al., 2024a; Huang et al., 2024). As result, the distribution DF is induced by the distribution of the random weight vector w, denoted by DΩ. For simplicity, we consider normalized case where E[w] = 0 and Var[w] = IdX . dX, RdX (cid:8) (cid:9) (cid:98) (cid:98) 17 Data Distribution. We consider the feature learning setting, where = {v1, . . . , vK} RdX are finite set of orthogonal and normalized features. Here, with slight abuse of notation, we use to denote the number of features. Each data point is sampled from with the probability pk for sampling vk, where pk (0, 1) for [K] and k[K] pk = 1. Such data model has been widely employed in the theoretical studies of deep learning, including ensemble methods (Allen-Zhu and Li, 2023), and in-context learning (Huang et al., 2024). For simplicity, we consider the balanced case where pk = 1/K for all [K]. The extension to the imbalanced case, where pk takes non-uniform values, can be derived similarly to Huang et al. (2024). (cid:80)"
        },
        {
            "title": "5.2 Slash-Dominance Frequency Condition",
            "content": "From observation in Section 4.4, we find that RoPE and, especially, its frequencies play crucial role in the slash-dominant pattern. We introduce quantitative slash-dominance frequency condition that precisely characterizes frequency interaction in this section. Because RoPE operations depend on the token embeddings, we first introduce the token embedding of prompts as follows. Token Embedding. For each [Nin], we consider embedding information of input xi at position 2i 1 0 0], and and its label yi at position 2i into orthogonal subspaces. Concretely, we consider E2i1 = [c E2i = [c 0 1 yi] Rd, where Rdc , c2 = 1 represents the semantically independent information and is identical across all token embeddings, and = dc + dX + 2. In addition, Eq = [c 0 0]. As result, the token embedding is defined as follows. = E(P ) = E1 E2Nin Eq RN d. (cid:2) In the definition above, all token embeddings lie on cone whose axis is given by [c 0 0 0]. This is motivated by Takeaway 3, and to further distinguish the semantically independent information (the cone axis) from the semantically dependent information, we embed the semantically dependent and independent components in orthogonal coordinate subspaces. For brevity, we further denote the semantically dependent subspace Ex,y = E:,dc+1:d RN (dX+2) and Ey = E:,d RN , which are the last dX + 2 columns and the last column of E, respectively. (cid:3) Note that RoPE has d/2 frequencies. In the following, we state the frequency assumptions for the cone component (the first dc dimensions) and the semantic component (the last dc dimensions), respectively. Assumption 5.1 (Slash-Dominance Frequency sequence). Let be the fixed length of the prompt and be the hidden dimension of embeddings. Denote the Kronecker delta function by δ0(x) := 1{x = 0}. We assume that frequency sequences ϑ = (θ1, , θd/2) satisfy the following: 1. The last (d dc)/2 low frequencies are small enough. For any dc/2 + 1 d/2, θs O(N α) is low frequency with α 2. 2. Sinusoal components of the first dc/2 frequencies approximate the pulse. There exists constants L, C1 0, C2 R, and small enough noise tolerance ϵFN LC1/N such that for any and , we have dc/2 s=1 cos(θsx) C1 δ0(x) C2 ϵFN. (11) (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) We assume that the last (d dc)/2 frequencies, corresponding to semantically dependent content x, y, are all low. This is aligned with the observation in Barbero et al. (2025) that low frequencies act primarily on semantic content. Conversely, the first dc/2 frequencies correspond to the semantically independent content c. This assumption states that if the selected frequency components have identical amplitudes 1, then the resulting sum of sinusoidal components approximates pulse, which is represented by the Kronecker delta function δ0(x) as in (11). Here, the uniform amplitudes are intrinsic to the Fourier expansion of the approximation target δ0(x), which distributes equal amplitude across all basis frequencies. (a) Ideal first dc/2 frequencies. (b) Gemma-7B: L0H7. (c) Llama3-8B-Instruct: L0H2. (d) Qwen2.5-7B-Instruct: L13H13. Figure 17: Summed cosine functions of frequencies. Panel (a) is the ideal case using the first dc/2 frequencies in Assumption 5.1; Panel (b)(d) use the active frequencies in practice (defined as those with average InP(i, j, l) greater than one-tenth of the largest InP(i, j, l)). Results (b)(d) for Gemma-7B, Llama3-8B-Instruct, and Qwen2.5-7B-Instruct show pattern consistent with (a). After training, however, the relative contributions of frequencies need not remain uniform, as their effective amplitudes are determined by the learned weight matrices and token embeddings. Assumption 5.1 further verifies that not all frequencies in RoPE are required for the SDHs. In particular, the low frequencies do not affect the validity of Assumption 5.1, consistent with Takeaway 4. On the contrary, if medium-frequencies or high-frequencies are set to zero, Assumption 5.1 may be violated, and slash-dominance no longer holds. As shown in Figure 16, setting part of the mediumor high-frequencies to zero substantially reduces the average slash scores. Empirical Validation of Assumption 5.1. We remark that the slash-dominance frequency condition is satisfied by pretrained models. Specifically, we first identify the active frequencies, defined as those whose average InP(i, j, l) exceeds one-tenth of the maximum. These active frequencies contribute primarily to the attention scores. We plot their sinusoidal sum given on the left-hand side of (11). As shown in Figure 17, the active frequencies employed in open-source LLMs (e.g., Gemma-7B, Llama3-8B-Instruct, Qwen2.5-7B-Instruct) satisfy this condition. Takeaway 5: Assumption 5.1 is empirically validated and serves as sufficient condition on the RoPE frequencies for inducing SDHs."
        },
        {
            "title": "5.3 Network Architecture",
            "content": "We next introduce the network structure, which takes the token embedding as input and outputs the desirable prediction Reduced Two-layer single head Disentangled Transformer. We consider two-layer single head transformer, and for simplicity in analyzing the residual stream and output of each CSA layer separately, yq. (cid:98) 19 we adopt the disentangled transformer (see Definition 5.2 and (12)), which feeds the concatenation, rather than the sum, of the residual and CSA layer output as the input into the subsequent layer. Notably, the disentangled transformer is equivalent to standard decoder-based, attention-only transformer (Nichani et al., 2024), and thus its use entails no loss of generality. Definition 5.2 (Two-layer Disentangled Transformer). Let RN d0 with d0 = be the input token embedding and ϑ(0) = ϑ denote the initial RoPE frequency sequence. Define the hidden dimensions of two subsequent layers as d1 = 2d0 and d2 = 2d1. For each layer ℓ = 1, 2, let (ℓ) } Rdℓ1dℓ1 be weight matrices, and let WO Rd2dout be the output matrix. Set all parameters θ = (ℓ) {WO}. The frequency sequence is updated at each layer by ϑ(ℓ) = (ϑ(ℓ1), ϑ(ℓ1)). Then the hidden states (ℓ) RN dℓ at each layer ℓ = 0, 1, 2 and the final output of TFθ are defined as follows: (cid:8) {Q,K,V } := {W (ℓ) , (ℓ) , (ℓ) 2 ℓ=1 {Q,K,V } (cid:9) (cid:83) (0) = E, (ℓ) = (ℓ1), CSA (ℓ1); (ℓ) {Q,K,V }, ϑ(ℓ) , ℓ = 1, 2 TFθ(E) = (2)WO. (cid:2) (cid:0) (cid:1)(cid:3) We abbreviate CSA (4), we denote Layers 1 and 2 attention scores by S(1)(E; θ) and S(2)(E; θ), respectively. (cid:1) as CSA {Q,K,V }, ϑ(ℓ) (ℓ1) (ℓ1); (ℓ) (cid:0) (cid:0) (cid:1) whenever the context is clear. Moreover, as in Before formally giving the expression of the transformer output, we first simplify the weight matrices {WO} for ease of analysis. visualization of the simplification is provided in (ℓ) {Q,K,V } θ = Figures 37 and 38 in Appendix E. (cid:9) First, for ℓ = 1, 2, we reduce (ℓ) ℓ=1,2 (cid:83) (cid:8) K,Q to sparse matrices as follows. (1) = (1) 0(dX+2)dc (cid:34) (cid:102) 0dc(dX+2) 0(dX+2)(dX+2)(cid:35) , (2) = 0dcdc 0(dX+2)dc 0dc(dX+2) (2) 0dd (cid:102) , (12) 0dd 0dd (1) Rdcdc is the only non-zero block in (1) where the block . This reduction follows from Takeaway 2 and 3. Concretely, in this simplified model, query exhibits almost rank-one and lies within the subspace generated by the cone axis [c 0 0 0]. Consequently, the first-layer weights need only take nonzero values on the corresponding subspace to map the token embeddings distributed on cone onto the cone axis. In addition, the block follows from the structure of the disentangled transformer, and the second layer focuses on the semantically dependent part of the input (1). (cid:102) R(dX+2)(dX+2) is the only non-zero block in (2) . The sparsity of (2) (2) (cid:102) In addition, since only the relative correlation (ℓ) and (ℓ) matters when calculating attention scores, we only make (ℓ) trainable while keeping (ℓ) fixed during training as follows. (1) = (cid:34) (1) 0(dX+2)dc (cid:102) 0dc(dX+2) 0(dX+2)(dX+2)(cid:35) , (2) = 0dd Id (cid:20) 0dd 0dd(cid:21) , )c = (1, 0, , 1, 0) := where we assume that (1) Finally, we specify the value and output matrices as (1) is fixed and satisfies ( = Id, (2) (1) (cid:102) (cid:102) = I2d, WO = [0dd 0dd Id 0dd]. Rdc. (cid:101) (13) (14) As result, the trainable weights reduce to }. θ defined above, and since we are only interested Output of the Reduced Model. With reduced trainable (cid:102) in the predicting xq at position , the predictor is given by (cid:101)θ(E)(N,d). The explicit form of (cid:101) this predictor is obtained by plugging in the fixed parameters in (13) and (14), and is provided in (21) in Appendix E. θ) = TF yq(E; θ = { (cid:102) (cid:101) (cid:101) (1) , (2) (cid:98)"
        },
        {
            "title": "5.4 Training Settings",
            "content": "Loss Function. We choose the standard squared loss used in linear regression task as follows. L( θ) = 1 2 Ew,P yq(E(P ); ( θ) w, xq)2 , (15) (cid:104) (cid:105) (cid:101) (cid:98) (cid:101) (1) (0) = 0dd, Algorithm 1 Two-stage Training Algorithm 1: Input: learning rate η1, η2 > 0; iteration τ1, τ2. 2: Initialize 3: // Stage (cid:102) 4: for = 1 to τ1 + 1 do (t) = 5: where the expectation is taken with respect to the joint distribution of the task and the prompt . Training Algorithm. As shown in Algorithm 1, we train the reduced two-layer disentangled transformer with two-stage gradient descent on the squared loss. This two-stage procedure is motivated by the empirical observation that different layers tend to converge in distinct phases (Bietti et al., 2023). The reduced weights are initialized as = Id. In the = 0dd, (2) (0) = Id is fixed, and gradient (cid:102) (1) (0) = Id with learndescent operates on ing rate η1 until timestep τ1 + 1, and outputs (1) (τ1 + 1). Then, in the second stage, the (1) (τ1 + 1) is fixed, and gradient descent weight (cid:102) operates on (0) = Id with learning rate η2 until timestep τ1 + τ2 + 1. (cid:102) θ(t) = ( 6: (cid:102) 7: end for (cid:101) 8: // Stage II 9: for = τ1 + 1 to τ1 + τ2 + 1 do (2) (t 1) η2 10: 11: 12: end for (cid:101) 13: Output: (1) (τ1 + 1), (cid:102) (t 1) η1 (1) (t), (cid:102) θ(τ1 + τ2 + 1). (0) = Id. θ(t) = ( (cid:102) first stage, (t) = (0)). (t)). θ(t1)). θ(t1)). (1) (2) (2) (2) (1) (2) (2) (1) (2) (cid:102)W (1) (cid:102)W (2) (cid:102) θ = L( L( (cid:102) (cid:102) (cid:102) (cid:102) (cid:102) (cid:102) (cid:102) (cid:102) (cid:101) (cid:101) Q"
        },
        {
            "title": "5.5 Main Theoretical Result",
            "content": "(cid:98) (cid:101) In this section, we characterize the convergence of Algorithm 1, and analyze its performance in Stages and II. Our theoretical regression setting follows general induction head circuit mechanism as introduced in Figure 4 in Section 2. To output an accurate prediction, (i) after Stage I, the first layer focuses on the prefix positions and thereby forms an SDH, enabling each yi to integrate information from its associated prefix xi; (ii) After Stage II, the learned second layer identifies the yi whose prefix matches the question token, i.e., xi = xq, and outputs the corresponding value. To formalize these behaviors, we introduce notations for tracking the attention scores during training. θ, we denote the first layer attention score from the position to θ), and the second layer attention score from the question token to the position by Under the reduced model with parameters the position by S(1) i,j (E; S(2) (cid:101) score from the question token to the feature by (E; (cid:101) θ). To further characterize Layer 2 behavior, for any [K], we define the aggregated attention (cid:101) S(2) (E; θ) := S(2) 2i (E; θ), iVk(P ) (16) (cid:88) (cid:101) θ(t) at step t, we abbreviate S(1) where Vk := Vk(P ) [Nin] is the feature index set, such that xi = vk for Vk(P ). For simplicity, given the parameters (t), and yq(t), respectively. (cid:101) These quantities allow us to monitor how the network forms an SDH in Layer 1 and outputs an accurate prediction in Layer 2. Specifically, two attention patterns are desired: (i) In the first layer, an SDH requires S(1) i,i1(τ1 + 1) 1. (ii) In the second layer, when xq = vk, we similarly require S(2) (τ1 + τ2 + 1) 1. Under these conditions, the model outputs an accurate prediction: i,j (t), S(2) θ(t)) as S(1) θ(t)), S(2) (cid:101) θ(t)) and (E; i,j (E; yq(E; (cid:101) (cid:101) (cid:98) (cid:101) (cid:98) yq(τ1 + τ2 + 1) = i[Nin] S(2) 2i (τ1 + τ2 + 1) yi w, vk. (17) (cid:98) (cid:80) 21 We formally state our main theorem below, which characterizes the convergence of loss and the dynamics of these attention scores. Theorem 5.3 (Training Dynamics). Suppose Assumption 5.1 holds. Consider poly(K) polylog(N ), d, and pk = 1/K for any [K]. We set ϵ1 = O(N 1 Ω(ϵ1) as the attention concentration errors of the first and second layer, respectively. Then the following holds for the training of Algorithm 1, Stage I: SDH Emergence in First Layer. In Stage with τ1 at most 2 ) and ϵ2 = O(N 1 4 ) (cid:84) KN log(N ) η1C1 + log(K 1ϵ1 1 ) ϵ1η1C1 , (cid:19) (cid:18) the attention head of the first layer is trained to be 1 ϵ1 slash-dominant at 1. Formally, for any token at position [N ], it almost attends to the immediately previous token, i.e., 1 ϵ1 S(1) Stage II: Feature Matching in Second Layer. In Stage II with τ2 at most i,i1(τ1 + 1). 2 log(K) η2 + log(Kϵ1/2 η2ϵ2 2 ) , (cid:19) (cid:18) the attention head of second layer has concentrated attention score on the same feature with the question token. Formally, if xq = vk, then with high probability, the second layer almost attends to input tokens featuring vk: (1 S(2) End: The Loss Convergence. At the end of Stage II, the loss converges. Formally, L( (τ1 + τ2 + 1))2 O(ϵ2). θ) ϵ2. The above theorem shows that if the frequencies satisfy Assumptions 5.1, the training of Algorithm 1 converges to the minimum of loss via GD, with polynomial time efficiency, and proceeds in two distinct stages: (i) The first layer captures positional dependencies and rapidly learns slash-dominant attention pattern with = 1, given sequence of frequencies satisfying Assumption 5.1, and (ii) The second layer captures semantic and feature dependencies and achieves feature matching. Notably, the error in the second layer, ϵ2, is larger than that in the first layer, ϵ1, because feature matching relies on accurate prefix recognition by the SDH formed in the first layer. Detailed proofs of Theorem 5.3 are provided in Appendices to H. (cid:98) Extension to SDHs with General offsets . Theorem 5.3 focused on the specific case of small offset = 1. This is because, in our prompt, each yi is paired with its desired prefix xi at distance of 1. This setup requires the SDH to attend to the immediately preceding token. However, the proof is not restricted to this particular offset. In the general case, if the data were constructed such that the distance between yi and xi were an arbitrary , the same reasoning would demonstrate the emergence of an SDH at offset . θ and test prompt for linear task (possibly outside the OOD Generalization. Given the learned model support of DΩ), let the question token be xq = vk. By Stage II attention concentration, Eq. (17) shows that, with high probability, the question prediction (τ1 + τ2 + 1)w, vm w, vk. This showcases the remarkable OOD generalization capability of SDHs. yq is S(2) (cid:98) (τ1 + τ2 + 1)w, vk + m=k S(2) (cid:98) (cid:80) Takeaway 6: Theorem 5.3 shows that, under the structural conditions in Takeaways 2-4 and the slash-dominance frequency condition Assumption 5.1, shallow transformer learns an SDH that are OOD generalizable to tasks. We further remark that the convergence rate in Theorem 5.3 does not depend on the two parameters α and dc in Assumption 5.1, as α 2 and dc, making the terms involving α and dc higher order term, and asymptotically negligible in the final convergence bound. 22 We finally comment on why GD performs well despite the nonconvex loss in (15). The key is that the optimization landscape is highly structured, containing many subglobal-optimal parameter configurations. In particular, for both Layer 1 and Layer 2, any configuration that produces sufficiently concentrated attention scores, as characterized in the two stages of Theorem 5.3, already lies in such subglobal-optimal region. Owing to the monotonicity and saturation properties of the softmax function, once logit at given position becomes sufficiently larger than the others, the attention scores concentrate almost entirely on that position. As result, GD does not require carefully tuned descent direction or precise learning-rate scheduling. The gradients naturally guide the parameters toward these large regions of concentrated-attention solutions. Indeed, our convergence rate bounds show that in both Stage and Stage II, sufficiently large learning rates η1 and η2 move the parameters into their subglobal-optimal regions in essentially one step. In much deeper real-world LLMs, interactions across many layers introduce considerably more complex dynamics, and the behavior of GD may not be nearly as simple or predictable. Nevertheless, the emergence of SDHs is still expected."
        },
        {
            "title": "6 Extensions to SDHs with Large ∆",
            "content": "In this section, we show that the definition of SDHs for small introduced in Theorem 4.1 can be naturally extended to the large (e.g., > 500) regime, by appropriately adjusting the threshold κ. This complements our findings for small {0, 1, . . . , 4} in Section 4. Intriguingly, most of the empirical findings hold similarly to small , which demonstrates the robustness of our insights. (a) Average attention score matrix of L0H7. (b) Average attention score matrix of L1H11. (c) Average attention score matrix of L2H6. Figure 18: Average attention score matrices of SDHs with large in Qwen2.5-7B-Instruct with prompts whose tokens are i.i.d. samples from the uniform distribution over the alphabet. Definition of SDHs with Large . Definition 4.1 identifies slash patterns in attention score matrices by examining the average slash scores. When κ is chosen sufficiently large relative to the context length (e.g., κ = 0.1 for context length of 6000 in Section 4), this definition reliably detects slash patterns with small . As illustrated in Figure. 3(d)(f), however, there also exist slash patterns with large . These long-range slash patterns have much smaller average slash scores, on the order of 103. However, directly setting κ = 103 leads to many spurious detections. For example, it would label offsets < 10 as slash-dominant for almost all heads. This high false-positive rate arises from the locality of natural language, which induces irregularly high attention values to nearby tokens rather than consistent slash patterns. To mitigate this locality effect, we restrict the offset range to 500 5000 and set κ = 103. The goal of this section is to verify that the properties of SDHs with small generalize to subset of SDHs with large . Empirically, with the hyperparameters 500 5000 and κ = 103, we identify SDHs (with large ) only in Llama3-8B-Instruct 23 and Qwen2.5-7B-Instruct since the context length of Gemma-7B is insufficient for this setting. Accordingly, we report experimental results only for these two models in this section. The detailed lists of selected heads are provided in Appendix D. refined definition and more comprehensive investigation of SDHs with large are left to future work. OOD Generalization of SDHs. Similar to Section 4.1, we will show that SDHs with large is OOD generalizable and intrinsic to the models. We test OOD generalization by evaluating the average slash scores under special OOD prompt distribution D, in which every token is sampled independently from uniform alphabet. We plot the average attention score matrices of identified SDHs under OOD prompts in Figure 18. These SDHs are from Qwen2.5-7B-Instruct and include L0H7, L1H11 and L2H6 for = 937, 804, 1934, which are the same SDHs reported in Figure 3. Figure 18 shows that the same slash pattern persists under OOD prompts. Same results are also observed for Llama3-8B-Instruct (see Appendix D). We further quantify OOD generalization by computing the average slash scores for the identified SDHs with in-distribution and OOD prompts, and comparing the ratios. The resulting box plot of these ratios is shown in Figure 19. The average slash scores under OOD prompts are generally higher, or at least comparable to those obtained from in-distribution prompts. Detailed head-level results for the OOD case are provided in Table 6 in Appendix D. Figure 19: Ratio of average slash scores with OOD and indistribution prompts (large ). Hence, our Takeaway 1 generalizes to large . The emergence of the slash-dominance pattern is not relevant to the semantic meaning of the prompts, but is intrinsic to the model architecture. (a) Hidden state of L0H7 after PCA. (b) Queries of L0H7. (c) Keys of L0H7. Figure 20: These figures show the queries and keys before the RoPE implementation, as well as the hidden states for SDHs with large in Qwen2.5-7B-Instruct. The queries and keys each have dimension 128, and the hidden states are reduced to 128 dimensions for demonstration. Approximate Low-Rankness of pre-PE Queries and Keys. Similar to Section 4.2, we show the approximate low-rankness of pre-PE queries and keys. We first visualize SDH of Qwen2.5-7B-Instruct L0H7 in Figure 20. This SDH demonstrates slash dominance across multiple large offsets, all with 900. As shown in Figure 20 (b) and (c), the pre-PE queries and keys are highly similar across tokens, implying that the query and key matrices are almost low-rank, particularly rank-one. Additional visualizations for more heads and models are provided in Figure 33 in Appendix D, which consistently exhibit the same structural pattern. 24 (a) Average r1() of all heads and SDHs with large . (b) Average R0.95() of all heads and SDHs with large . Figure 21: Comparison of average r1, R0.95 of SDHs with those of all heads. For large , at least one of the Qor exhibits substantially lower average ranks for SDHs compared to all heads. We then quantify the low-rankness of the pre-PE queries and keys in detail by r1 and Q0.95 according to (7) in Figure 21. In particular, Figure 21(a) shows that, for each model, at least one of r1(Q) and r1(K) strictly exceeds 0.88 on average over SDHs only, and these values are strictly lower on the other heads. Figure 21(b) shows that the effective ranks of and are low only on the SDHs, while these matrices on the other heads have much higher ranks. Hence, our Takeaway 2 generalizes to the large regime. The pre-PE queries and keys all have low effective ranks. Moreover, at least one of and is close to being rank-one. How to Get Approximately Rank-One Queries and Keys? Similar to Section 4.3, we next answer how SDHs achieve the approximately rank-one queries and keys. As in Figures 22, we compute the average values of r1 and R0.95 for H, WQ (resp. WK), and (resp. K) and average these values over the identified SDHs. Figures 22 clearly shows that the effective ranks of H, WQ, and WK are much higher than those of and K. Thus, low-rankness of and must arise from the interaction between the hidden states and weight matrices WQ and WK. r1 and We further plot the average values of R0.95, and compare them with r1 and R0.95 respectively in Figures 23. These average values are computed over all the tokens and the SDHs in Layer 0. Figures 23 shows that the behaviors of R0.95(Q) closely match those of r1(Q) and R0.95(Q) in Llama3-8B-Instruct, but differ in Qwen2.5-7B-Instruct. This confirms that in Gemma-7B and Llama3-8B-Instruct, each token embedding concentrates most of its energy approximately in one-dimensional principal subspace of WQ and WK. That is, each token embedding aligns well with low-rank (and almost rank-one) subspaces of WK and WQ. Hence, our Takeaway 3 generalizes to large . r1(Q) and (cid:101) (cid:101) (cid:101) (cid:101) Collaboration of Frequencies in RoPE Determines Slash Pattern. Similar to Section 4.4, given that pre-PE queries and keys are approximately rank-one, we proceed to examine how the different RoPE frequencies collectively contributed to the slash pattern according to (9). We compute the relative variation of the norms of the query and key vectors and plot their averages over tokens and SDHs in Figure 25. As shown in the figure, the relative variation is at most 0.093, indicating that the norms of qi (and similarly kj) are nearly constant. As result, qi is approximately the same for all i, and similarly for kj. This implies that the amplitudes Aℓ and initial phases φℓ in (10) are approximately identical across token pairs, which mirrors the behavior observed in the smallregime and explains how RoPE gives rise to SDHs in the large setting. (a) r1() related to queries. (b) r1() related to keys. (c) R0.95() related to queries. (d) R0.95() related to keys. Figure 22: Comparison of average r1 and R0.95 for query/key matrices relative to the hidden states and their corresponding weight matrices, for SDHs with large . Both hidden states and the weight matrices WQ, WK exhibit much higher rank than the resulting pre-PE queries and keys, indicating that the low-rankness arises from the interaction between and the weights WQ, WK. To further characterize the collaboration of frequencies, we then visualize InP(i, j, ℓ) with fixed to = 5000 and varying within [1, 5000] in Figure 24, based on Qwen2.5-7B-Instruct. We observe that highand medium-frequency components (ℓ [1, 42]) exhibit greater variation than low-frequency components (ℓ [43, 64]). For more quantitative characterization, we evaluate slash-dominance after selectively removing certain frequencies when computing the attention score using the same criteria as the small regime. We compute the ratios of the new score to the original score for each identified SDHs in Llama3-8B-Instruct and Qwen2.5-7B-Instruct, and show the box plot in Figure 26. We observe that, when removing high frequencies, the average slash score decreases in both models. Removing the medium frequencies leads to mild change, or even no decrease, while the low frequencies yield the least drop. Hence, our Takeaway 4 generalizes to large . The highand medium-frequency components in RoPE are more important than the low-frequency components for SDHs."
        },
        {
            "title": "7 Discussion",
            "content": "In the preceding sections, we investigated the mechanistic interpretability of SDHs, showing that this phenomenon arises as an intrinsic algorithmic effect contributed by RoPE. Our empirical and theoretical findings in Sections 4 to 6, provide new insights into how RoPE interacts with attention mechanisms. In this discussion part, we first outline two potential directions where our findings could be applied to improve the efficiency and efficacy of the current model architectures. Then we discuss the implications of our results for the models with other kinds of PE. 26 (a) (cid:101)r1() and r1() related to queries. (b) R0.95() and (cid:101)R0.95() related to queries. R0.95 for query/key matrices relative to the 0-th layer Figure 23: Comparison of average r1, R0.95, hidden states (token embeddings) for SDHs with large in Gemma-7B, Llama3-8B-Instruct, and Qwen2.5- (cid:101) 7B-Instruct. r1, and (cid:101) (a) InP(5000, j, l) of L0H7. (b) InP(5000, j, l) of L1H11. (c) InP(5000, j, l) of L2H6. Figure 24: Heatmaps of InP(i, j, ℓ) on various SDHs of Qwen2.5-7B-Instruct. where we set = 5000 and let vary. Qwen2.5-7B-Instruct has 64 frequencies (1 ℓ 64). The variation within each column shows the influence of each frequency. Qualitatively, highand medium-frequency components ℓ [1, 42] exhibit greater variation. Parameter Efficiency of Queries and Keys. Motivated by the observed approximate low-rankness, we suggest that the parameter matrices WQ and WK of some heads can be constrained to low rank. This could reduce the number of parameters and save computational resources during both training and inference, without significant loss in performance. We take an initial step to verify this. In Table 1 in Appendix C, we compress the query and key matrices of pretrained Qwen2.5-7B-Instruct and Llama3-8B-Instruct, which yields notable parameter reduction while preserving accuracy. promising direction for future work is to explicitly enforce low-rank constraints during training to see whether we can achieve further efficiency gains. Effective Length Generalization. Our result reveals that highand medium-frequency components of RoPE predominantly contribute to slash-dominant behavior, whereas low frequencies play negligible role. Based on this observation, one potential application is to edit or reweight the low-frequency components of RoPE to enhance length generalization in LLMs. We leave the systematic exploration of this idea to future experimental work. Implications for Other Kinds of PEs. Our analysis focuses on models that use RoPE (e.g., the Gemma, Llama, and Qwen families). Other positional-encoding schemes, Alibi (Press et al., 2021), NoPE (Kazemnejad 27 Figure 25: Average of the relative variation of the norm of the query vector qi and key ki for large in Llama3-8B-Instruct and Qwen2.5-7B-Instruct. Figure 26: The figure quantifies the effect of low-, medium-, and high-frequency components on SDHs by reporting, for each band, the ratio of the average slash score after removing that band to the original average score. et al., 2023), and sinusoidal positional embeddings (Vaswani et al., 2017), can exhibit behavior similar to or distinct from RoPE. For example, Alibi adds relative bias directly to querykey inner products; it is length-extrapolative by design and may therefore support OOD generalization. In contrast, NoPE (no explicit positional signal) and sinusoidal embeddings (often injected only at the input layer) may render SDHs more context dependent. detailed study of SDHs under these alternatives is left to future work."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we investigated the mechanism underlying SDHs. Our results demonstrate that, owing to the geometric structure of token embeddings approximately on cone, slash-dominance is an intrinsic algorithmic effect primarily driven by the mediumto high-frequency components of RoPE. We further establish slash-dominance frequency condition that is satisfied by real open-source LLMs, and prove these conditions on token embeddings and frequencies are sufficient for the emergence of SDHs by analyzing the training dynamics of shallow Transformer. Our theoretical and empirical findings provide new insights into how RoPE interacts with attention mechanisms. Our work focused on the analysis of the pretrained LLMs with RoPE, and we leave the investigation of other kinds of PE as future work."
        },
        {
            "title": "References",
            "content": "Allen-Zhu, Z. and Li, Y. (2023). Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. In International Conference on Learning Representations. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J. and Li, J. (2024). LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Bai, Y., Tu, S., Zhang, J., Peng, H., Wang, X., Lv, X., Cao, S., Xu, J., Hou, L., Dong, Y., Tang, J. and Li, J. (2025). LongBench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics. Barbero, F., Vitvitskyi, A., Perivolaropoulos, C., Pascanu, R. and Veliˇckovic, P. (2025). Round and round we go! what makes rotary positional encodings useful? In International Conference on Learning Representations. Bietti, A., Cabannes, V., Bouchacourt, D., Jegou, H. and Bottou, L. (2023). Birth of Transformer: memory viewpoint. In Advances in Neural Information Processing Systems, vol. 36. 28 Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., In Advances in Neural Sastry, G., Askell, A. et al. (2020). Language models are few-shot learners. Information Processing Systems, vol. 33. Chen, S., Sheen, H., Wang, T. and Yang, Z. (2024a). Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality. In Annual Conference on Learning Theory. Chen, S., Sheen, H., Wang, T. and Yang, Z. (2024b). Unveiling induction heads: Provable training dynamics and feature learning in Transformers. In Advances in Neural Information Processing Systems, vol. 37. Chen, S., Wong, S., Chen, L. and Tian, Y. (2023). Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595. Cheng, Y., Huang, Y., Xiong, Z., Liang, Y. and Tan, V. Y. (2025). Transformers provably learn directed acyclic graphs via kernel-guided mutual information. arXiv preprint arXiv:2510.25542. Ding, Y., Zhang, L. L., Zhang, C., Xu, Y., Shang, N., Xu, J., Yang, F. and Yang, M. (2024). LongRoPE: Extending LLM context window beyond 2 million tokens. In International Conference on Machine Learning. PMLR. Edelman, E., Tsilivis, N., Edelman, B., Malach, E. and Goel, S. (2024). The evolution of statistical induction heads: In-context learning Markov chains. In Advances in Neural Information Processing Systems, vol. 37. Ekbote, C., Bondaschi, M., Rajaraman, N., Lee, J. D., Gastpar, M., Makkuva, A. V. and Liang, P. P. (2025). What one cannot, two can: Two-layer Transformers provably represent induction heads on any-order Markov chains. arXiv preprint arXiv:2508.07208. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T. et al. (2021). mathematical framework for Transformer circuits. Transformer Circuits Thread, 1 12. Gemma Team (2024). Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295. Gould, R., Ong, E., Ogden, G. and Conmy, A. (2024). Successor heads: Recurring, interpretable attention heads in the wild. In International Conference on Learning Representations. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A. et al. (2024). The Llama 3 herd of models. arXiv preprint arXiv:2407.21783. Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58 1330. Huang, R., Liang, Y. and Yang, J. (2025a). How transformers learn regular language recognition: theoretical study on training dynamics and implicit bias. In International Conference on Machine Learning. Huang, Y., Cheng, Y. and Liang, Y. (2024). Conference on Machine Learning. PMLR. In-context convergence of Transformers. In International Huang, Y., Wen, Z., Singh, A., Chi, Y. and Chen, Y. (2025b). Transformers provably learn chain-of-thought reasoning with length generalization. arXiv preprint arXiv:2511.07378. Jelassi, S., Sander, M. and Li, Y. (2022). Vision Transformers provably learn spatial structure. In Advances in Neural Information Processing Systems, vol. 35. 29 Jiang, H., Li, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A. H., Li, D., Lin, C.-Y. et al. (2024). Minference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. In Advances in Neural Information Processing Systems, vol. 37. Kazemnejad, A., Padhi, I., Natesan Ramamurthy, K., Das, P. and Reddy, S. (2023). The impact of positional encoding on length generalization in Transformers. In Advances in Neural Information Processing Systems, vol. 36. Kim, J. and Suzuki, T. (2025). Transformers provably solve parity efficiently with chain of thought. In International Conference on Learning Representations. Lai, X., Lu, J., Luo, Y., Ma, Y. and Zhou, X. (2025). FlexPrefill: context-aware sparse attention mechanism for efficient long-sequence inference. In International Conference on Learning Representations. Li, H., Wang, M., Liu, S. and Chen, P.-Y. (2023). theoretical understanding of shallow vision Transformers: Learning, generalization, and sample complexity. In International Conference on Learning Representations. Li, W., Zhang, C., Jiang, H., Li, Y., Yang, Y. and Qiu, L. (2025). Mtraining: Distributed dynamic sparse attention for efficient ultra-long context training. arXiv preprint arXiv:2510.18830. Merullo, J., Eickhoff, C. and Pavlick, E. (2024). Circuit component reuse across tasks in Transformer language models. In International Conference on Learning Representations. Mohri, M., Rostamizadeh, A. and Talwalkar, A. (2018). Foundations of Machine Learning. MIT press. Nichani, E., Damian, A. and Lee, J. D. (2024). How Transformers learn causal structure with gradient descent. In International Conference on Machine Learning. PMLR. Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A. et al. (2022). In-context learning and induction heads. arXiv preprint arXiv:2209.11895. Peng, B., Quesnelle, J., Fan, H. and Shippole, E. (2024). YaRN: Efficient context window extension of large language models. In International Conference on Learning Representations. Press, O., Smith, N. A. and Lewis, M. (2021). Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409. Qwen Team (2025). Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Reddy, G. (2024). The mechanistic basis of data dependence and abrupt learning in an in-context classification task. In International Conference on Learning Representations. Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T. et al. (2023). Code Llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Singh, A. K., Moskovitz, T., Hill, F., Chan, S. C. and Saxe, A. M. (2024). What needs to go right for an induction head? mechanistic study of in-context learning circuits and their formation. In International Conference on Machine Learning. PMLR. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W. and Liu, Y. (2024). RoFormer: Enhanced Transformer with rotary position embedding. Neurocomputing, 568 127063. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, (cid:32)L. and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. 30 Wang, M., Yu, R., Wu, L. et al. (2024a). How Transformers implement induction heads: Approximation and optimization analysis. arXiv preprint arXiv:2410.11474. Wang, S., Kobyzev, I., Lu, P., Rezagholizadeh, M. and Liu, B. (2024b). Resonance RoPE: Improving context length generalization of large language models. In Findings of the Association for Computational Linguistics. Wen, K., Zhang, H., Lin, H. and Zhang, J. (2025). From sparse dependence to sparse attention: Unveiling how chain-of-thought enhances transformer sample efficiency. In International Conference on Learning Representations. Wu, W., Wang, Y., Xiao, G., Peng, H. and Fu, Y. (2025). Retrieval head mechanistically explains longcontext factuality. In International Conference on Learning Representations. Xiao, G., Tian, Y., Chen, B., Han, S. and Lewis, M. (2023). Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. Xie, S. M., Raghunathan, A., Liang, P. and Ma, T. (2022). An explanation of in-context learning as implicit Bayesian inference. In International Conference on Learning Representations. Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S. and Ma, H. (2024). Effective long-context scaling of In Proceedings of the 2024 Conference of the North American Chapter of the foundation models. Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). Xu, R., Xiao, G., Huang, H., Guo, J. and Han, S. (2025). Xattention: Block sparse attention with antidiagonal scoring. In International Conference on Machine Learning. Yang, T., Huang, Y., Liang, Y. and Chi, Y. (2024). In-context learning with representations: Contextual generalization of trained Transformers. In Advances in Neural Information Processing Systems, vol. 37. Yang, T., Huang, Y., Liang, Y. and Chi, Y. (2025). Multi-head transformers provably learn symbolic multistep reasoning via gradient descent. In Advances in Neural Information Processing Systems. Zhang, B. and Sennrich, R. (2019). Root mean square layer normalization. Advances in neural information processing systems, 32. Zhang, R., Frei, S. and Bartlett, P. L. (2024a). Trained Transformers learn linear models in-context. Journal of Machine Learning Research, 25 155. Zhang, R., Wu, J. and Bartlett, P. (2024b). In-context learning of linear Transformer block: Benefits of the MLP component and one-step GD initialization. Advances in Neural Information Processing Systems, 37 1831018361. Zhang, Y., Zhang, F., Yang, Z. and Wang, Z. (2025). What and how does in-context learning learn? Bayesian model averaging, parameterization, and generalization. In International Conference on Artificial Intelligence and Statistics. PMLR. Zhao, T., Hong, K., Yang, X., Xiao, X., Li, H., Ling, F., Xie, R., Chen, S., Zhu, H., Zhang, Y. et al. (2025). PAROAttention: Pattern-aware reordering for efficient sparse and quantized attention in visual generation models. arXiv preprint arXiv:2506.16054. Zhong, M., Zhang, C., Lei, Y., Liu, X., Gao, Y., Hu, Y., Chen, K. and Zhang, M. (2025). Understanding the RoPE extensions of long-context LLMs: An attention perspective. In Proceedings of the 31st International Conference on Computational Linguistics."
        },
        {
            "title": "5.1 Data Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.2 Slash-Dominance Frequency Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3 Network Architecture\n5.4 Training Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5 Main Theoretical Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "6 Extensions to SDHs with Large 7 Discussion 8 Conclusion Additional Related Work Experimental Details Parameter Compression Experiment More results of Slash-Dominant Heads D.1 More Results of Slash-dominant Heads with Small . . . . . . . . . . . . . . . . . . . . . . . D.1.1 Full List of SDHs with Small . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1.2 Attention Scores and Ranks of Q, K, and . . . . . . . . . . . . . . . . . . . . . . . D.2 Figures of Slash-dominant Heads with Small and Large Average Slash Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2.1 Results of Llama3-8B-Instruct D.2.2 Results of Gemma-7B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 More Results of Slash-dominant Heads with Large . . . . . . . . . . . . . . . . . . . . . . . D.3.1 Full List of SDHs with Large . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3.2 Attention Scores and Ranks of Q, K, and . . . . . . . . . . . . . . . . . . . . . . . D.4 Figures of Slash-dominant Heads with Large and Small Average Slash Score . . . . . . . . D.4.1 Results of Qwen2.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4.2 Results of Llama3-8B-Instruct D.5 Tables related to token embeddings in the 0-th layer . . . . . . . . . . . . . . . . . . . . . . . Notations in Theory Sections and Expression of Reduced Model 32 2 5 7 8 10 12 15 17 17 18 19 21 21 23 26 33 34 35 36 36 36 38 42 42 43 44 45 47 51 51 52 53 54 Proof of Stage of Theorem 5. F.5 Stage I: Phase II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.1 Roadmap of the Proof F.2 Stage I: Preliminary Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Stage I: Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Stage I: Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4.1 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4.2 End of Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.5.1 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.5.2 End of Phase II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.6 Stage I: Phase III . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.6.1 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.6.2 End of Phase III . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.7 Proof of Stage of Theorem 5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Stage II of Theorem 5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.1 Roadmap of the Proof G.2 Stage II: Preliminary Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 Stage II: Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4 Stage II: Phase G.4.1 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4.2 End of Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5 Stage II: Phase II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5.1 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.5.2 End of Phase II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Theorem 5."
        },
        {
            "title": "A Additional Related Work",
            "content": "55 55 57 61 64 65 68 69 70 71 72 73 74 75 75 75 76 79 79 80 82 83 84 86 87 Other Special Attention Patterns and Heads. In addition to the slash patterns and SDHs, line of work proposed different kinds of heads with special patterns to explain the information aggregation in transformers. For example, successor heads (Gould et al., 2024), content-gathering head (Merullo et al., 2024), and retrieval head (Wu et al., 2025). Different from them, in our work, we also show that SDHs can be generalized to out-of-distribution cases. Moreover, by understanding certain attention score patterns of special heads (Jiang et al., 2024; Xu et al., 2025; Zhao et al., 2025; Lai et al., 2025; Li et al., 2025), one can improve the efficiency of LLMs. Specifically, Xu et al. (2025) observed that the sum of the antidiagonal values in the attention score matrix serves as strong proxy for block importance, based on which they proposed plug-and-play framework that accelerates long-context inference in transformer models using sparse attention. Li et al. (2025) proposes distributed dynamic sparse attention framework for ultra-long context training. Their system leverages the empirical observation that many attention heads attend primarily along fixed relative offsets induced by RoPE, enabling efficient training at million-token scale. Training Dynamics of Transformer. Huang et al. (2024) and Zhang et al. (2024a) are the first two works to establish the in-context convergence results for the softmax attention and linear attention trained with gradient descent, respectively. Then, series of works relaxed their assumptions and studied more general network structures. Chen et al. (2024a) first explored the training dynamics of the multi-task incontext learning using multi-head transformers. Yang et al. (2024) explored in-context learning in multi-head transformers applied to nonlinear task functions. More recently, Nichani et al. (2024); Chen et al. (2024b); Edelman et al. (2024); Cheng et al. (2025) have investigated transformer learning dynamics in settings 33 underpinned by causal structure, deepening our understanding of how such models acquire and represent inductive capabilities. Wen et al. (2025); Kim and Suzuki (2025); Yang et al. (2025); Huang et al. (2025a,b) investigated transformer dynamics in the chain-of-thought reasoning setting. Another line of work studies the learning dynamics of transformers on vision tasks. For example, Jelassi et al. (2022) analyzed the inductive biases of Vision Transformers (ViTs) with focus on specialized positional attention. Building on this foundation, Li et al. (2023) analyzed the training process of shallow ViTs in supervised classification settings and extended their study to in-context learning, under strict assumptions on network initialization."
        },
        {
            "title": "B Experimental Details",
            "content": "In our experiments, we identify SDHs in Qwen2.5-7B-Instruct (Qwen Team, 2025), Llama3-8B-Instruct (Grattafiori et al., 2024), and Gemma-7B (Gemma Team, 2024). We evaluate two offset regimes that allow clean isolation of the slash pattern: (i) local offsets ( < 5), which capture short-range structural effects, and (ii) extreme long-range offsets (500 5000), where slash linesif presentmanifest as stable diagonal structures across large positional gaps. We intentionally exclude the intermediate offsets (5 < 500), because attention in this range is dominated by semantic relatedness between nearby tokens. In this confounded regime, the slash-dominance score Si,i reflects multiplicative interaction between semantic dependencies and positional alignment, making it impossible to isolate the geometric slash pattern. By focusing on the shortand long-range extremeswhere semantic contributions are either concentrated near the diagonal or negligible at large distanceswe obtain measurements of slash-dominance that cleanly reflect positional structure. To approximate the expectation in (6), we prefill each model with 500 diverse prompts from LongBenchV2 (Bai et al., 2025), which provides heterogeneous long-context distributions and yields stable cross-model statistics. Each prompt is truncated to 6000 tokens to ensure consistent context length across models and to avoid context-extension artifacts. We collect the prefill attention matrices, which deterministically reflect each models attention geometry without introducing sampling noise. Following prior observations that early tokens (including BOS and warm-up positions) exhibit disproportionately large attention scores (Xiao et al., 2023), we exclude attention entries involving positions 14 from the computation of Si,i. Removing these anomalous early positions prevents them from dominating the slash-dominance statistics. After excluding these positions, each prompt still contributes at least 1000 valid pairs (i, ) for the ranges we study. In total, we average over more than 5 105 valid samples. For slash-dominance detection, we match the threshold κ to the natural scale of attention in each regime. For local offsets ( < 5), we set κ = 0.1, reflecting the high concentration of attention near the diagonal. However, the number of SDHs for = 0, 1, 2 is quite large, so we only report the first three heads for each in the tables in Appendix D. For extreme long-range offsets ( 500), we use κ = 103. Although small in magnitude, this threshold corresponds to 5 enrichment over the uniform baseline 1/5000 = 2 104 at context length 5000, making it meaningful marker of strong long-range alignment. Empirically, we do not find any SDHs with large in Gemma-7B. Thus, we only report results for SDHs with large in Llama3 and Qwen2.5. For the experiments on OOD prompts, we construct prompts from i.i.d. tokens. Specifically, we independently sample each token from the uniform distribution over the models vocabulary. For example, for Qwen2.5-7B-Instruct, the vocabulary size is 151,642, so each token is sampled with probability 1/151,642. Using this procedure, we generate 500 prompts of length 6000 by sampling 500 6000 tokens in total. Our experiments are conducted on NVIDIA A100 GPUs with 80G memory."
        },
        {
            "title": "C Parameter Compression Experiment",
            "content": "Motivated by the low-rankness of the queries and keys, we can compress the parameters of the WQ and WK matrices in the attention modules. Experimental Setup. As we will compute the effective singular values/subspaces of the WQ and WK over LongBench v2 dataset (Bai et al., 2025) with heterogeneous prompts, we extend our power computation method in (8) to the hidden states of sequence of tokens, which is originally defined for single token embedding x. To take the bias of Qwen2.5-7B-Instruct model into account, for simplicity, we define σ0 = 1 and v0 = b. For other models without the bias term, we define σ0 = 1 and v0 = 0. For each head, we first compute rl(H) = H2 σlv 2 H2 i=0 σiv (18) for all {0} [d], hidden states of prompts in LongBench v2 dataset (Bai et al., 2025), where vl is the singular vector of the weights, which is defined in (8). This quantity characterizes the power of this singular value σl for this prompt x. We then average the power over all the prompts to get the average power (cid:80) rl = rl(H) LongBench v2 (cid:80) . (19) Subsequently, we sort the singular values in the decreasing order of {rl}l[d] to get {rsorted(l)}l[d], where we force rsorted(0) = r0 to keep the bias term. We then calculate the effective rank Rthre such that the cumulative average power hits the power threshold: Rthre = min (cid:110) (cid:12) (cid:12) (cid:12) i=0 (cid:88) j=1 (cid:88) rj1{sorted(j) = i} > thre . (20) (cid:111) For those WQ and WK weight matrices whose effective ranks are smaller than compression threshold rankthre, we adopt its low rank approximations corresponding to the singular values {rlsorted(l) Rthre}; otherwise, we leave the weight matrices untouched. In the end, we obtain the low-rank model. We evaluate low-rank models with different energy thresholds thre and compression limits rankthre on LongBench (Bai et al., 2024), which spans 21 benchmarks across 6 task types and consists of in total 4750 prompts. For each model, we report the mean score within each task type, the overall mean across all 21 benchmarks, and the reduction in effective parameter count computed via (N QK total, where QK eff denote, respectively, the total and effective parameter counts of all WQ and WK matrices. For low-rank matrix (WQ or WK for an attention head) with retained rank Rthre (after truncation), the effective parameter count for low-rank weight matrix is Rthre (dhead + dmodel), instead of the original size of dhead dmodel. total and QK total QK eff )/N QK Experimental Result. We present the result in Table 1. It can be observed that we can save decent parameter count while preserving the performance of the original model. Since the Qwen2.5-7B-Instruct model includes an additional bias term whose norm is relatively large compared to that of the weight matrix (see Table 12), we are able to discard more singular values while still maintaining performance. Specifically, we can set the power threshold thre = 0.92 without compromising its overall performance (37.84 v.s. 37.80 from the baseline). It saves 6.51% of the original WQ and WK weight parameters. 35 Model SdQA MdQA Sum. Few-shot Syn. Code Avg. Reduced Params Llama-3-8B-Instruct (cid:44) thre = 0.98, rankthre = 64 (cid:44) thre = 0.96, rankthre = 32 Qwen2.5-7B-Instruct (cid:44) thre = 0.96, rankthre = 64 (cid:44) thre = 0.92, rankthre = 32 27.86 26.96 26.46 18.82 18.53 19. 23.07 22.21 24.10 14.65 15.59 17.34 25.86 24.46 25.40 22.98 22.76 25.09 60.72 61.73 62.59 62.50 60.85 62. 54.04 47.03 57.29 63.55 61.40 61.36 49.27 47.34 35.99 63.65 60.88 57.13 38.60 37.01 38.00 37.80 36.99 37. 0 8.62% 5.90% 0 9.54% 6.51% Table 1: Parameter Compression Results. The bold number indicates better performance than that of the baseline. The models are evaluated across 6 task types: Single-doc QA (SdQA), Multi-doc QA (MdQA), Summarization (Sum.), Few-shot, Synthetic (Syn.) and Code. The performance of the model is maintained with decent parameter reductions. More results of Slash-Dominant Heads In this section, we present additional results on SDHs with small . For notational convenience, we slightly abuse E[Si,i] to represent the average slash score reported in the tables. In the following, we will abbreviate Gemma-7B, Llama3-8B-Instruct, and Qwen2.5-7B-Instruct as Gemma, Llama3, and Qwen2.5, respectively. D.1 More Results of Slash-dominant Heads with Small For each , we list at most three heads with the average slash score larger than 0.1. Due to space limitations, only subset is shown; more slash-dominant heads with small exist but are not included here. D.1.1 Full List of SDHs with Small We report the full list of SDHs with {0, 1, 2, 3, 4} for each model as follows. These SDHs are identified by using (6) with κ = 0.1, where the average slash scores are computed over 500 random prompts from LongBenchV2 (Bai et al., 2025) dataset. Gemma-7B Model. For = 0, the found SDHs are: (Layers 0-2) L0H2, L0H4, L0H6, L0H10, L0H15, L1H2, L1H7, L2H0, L2H3, L2H8; (Layers 3-5) L3H1, L3H7, L3H8, L4H0, L4H9, L4H12, L5H6, L5H11, L5H13; (Layers 6-8) L6H1, L6H3, L6H11, L7H3, L7H6, L7H11, L7H14, L8H1, L8H4, L8H5, L8H9; (Layers 9-11) L9H5, L9H8, L9H14, L10H2, L10H5, L10H7, L10H15, L11H4, L11H7, L11H8; (Layers 12-20) L12H12, L13H1, L13H4, L14H14, L16H0, L17H8, L19H1, L19H15, L20H3, L20H7; (Layers 21-25) L20H3, L20H7, L21H10, L22H5, L23H2, L23H14, L24H0, L24H12, L25H2, L25H4, L25H8; (Layer 26) L26H0, L26H1, L26H3, L26H7, L26H9, L26H15; (Layer 27) L27H0, L27H1, L27H2, L27H3, L27H4, L27H5, L27H6, L27H7, L27H8, L27H10, L27H13, L27H14. For = 1, the found SDHs are: (Layers 0-2) L0H0, L0H1, L0H7, L0H8, L0H9, L0H14, L1H0, L1H7, L1H9, L1H14, L1H15, L2H2, L2H5; (Layers 3-5) L3H2, L3H5, L3H9, L3H11, L3H14, L3H15, L4H6, L4H14; (Layers 6-18) L6H7, L9H10, L10H10, L11H15, L13H3, L16H9, L17H0, L17H15, L18H11; (Layers 19-27) L19H13, L20H5, L22H8, L23H3, L23H15, L25H2, L26H4, L26H11, L27H5. 36 For = 2, the found SDHs are: L0H1, L1H7, L2H2, L2H9, L3H9, L13H3. For = 3, the found SDHs are: L0H1, L2H9. For = 4, the found SDH is: L0H1. Llama3-8B-Instruct Model. For Llama3 and = 0, the found SDHs are: (Layers 0-8) L0H2, L0H24, L4H5, L4H26, L6H5, L6H18, L7H0, L8H5, L8H13, L8H16, L8H23; (Layers 9-14) L9H10, L10H0, L10H10, L12H12, L12H20, L12H30, L13H28, L14H8, L14H16, L14H19; (Layers 15-19) L15H6, L15H10, L15H24, L16H22, L16H29, L16H30, L17H6, L17H12, L19H8, L19H11; (Layers 20-26) L20H0, L20H3, L21H3, L21H8, L21H10, L22H9, L22H19, L25H21, L26H0, L26H1, L26H3, L26H22; (Layers 27-29) L27H11, L28H10, L28H11, L28H17, L28H21, L28H29, L28H30, L29H8, L29H10, L29H25; (Layer 30) L30H0, L30H24, L30H25, L30H26, L30H27; (Layer 31) L31H0, L31H1, L31H2, L31H3, L31H5, L31H6, L31H7, L31H9, L31H11, L31H12, L31H14, L31H15, L31H18, L31H23, L31H24, L31H25, L31H26, L31H27. For Llama3 and = 1, the found SDHs are: (Layers 0-1) L0H0, L0H1, L0H2, L0H3, L0H6, L0H10, L0H24, L0H26, L1H16, L1H18, L1H20; (Layers 2-11) L4H12, L6H8, L6H16, L7H1, L7H2, L7H5, L8H22, L9H10, L9H11, L11H16, L11H19; (Layers 12-17) L12H19, L14H8, L14H9, L14H11, L14H26, L15H0, L15H7, L16H22, L16H29, L17H4, L17H5; (Layers 18-28) L18H26, L19H8, L21H4, L21H7, L21H10, L25H20, L25H22, L25H23, L27H29, L28H26; (Layers 29-31) L29H8, L29H26, L29H27, L30H10, L31H6, L31H15, L31H24 For Llama3 and = 2, the found SDHs are L0H0, L0H2, L1H21, L7H2, L9H11, L14H8, L14H26. For Llama3 and = 3, the found SDH is L0H0. For Llama3 and = 4, the found SDH is L0H0. Qwen2.5-7B-Instruct Model. For Qwen2.5 and = 0, the found SDHs are: (Layers 0-17) L14H9, L14H20, L14H25, L15H8, L15H23, L16H19, L16H26, L17H4; (Layers 18-31) L18H0, L18H7, L18H17, L18H25, L19H0, L19H11, L19H13, L20H1, L20H2, L20H14, L24H18, L25H2, L25H3, L25H4, L25H20. For Qwen2.5 and = 1, the found SDHs are: (Layer 0) L0H6, L0H7, L0H16, L0H20, L0H23, L0H24, L0H26; (Layer 1) L1H3, L1H10, L1H11, L1H12, L1H14, L1H15, L1H17, L1H18, L1H19, L1H23, L1H24; (Layers 2-3) L2H1, L2H2, L2H5, L2H6, L2H27, L3H1, L3H3, L3H10, L3H15, L3H22, L3H24, L3H26, L3H27; (Layers 4-6) L4H18, L4H23, L5H3, L6H1, L6H2, L6H4, L6H6, L6H15, L6H16, L6H21, L6H25, L6H26; (Layers 7-14) L7H4, L7H5, L10H4, L10H22, L10H25, L10H27, L11H1, L11H18, L12H21, L13H8, L13H13; (Layers 15-16) L15H5, L15H8, L15H9, L15H11, L15H13, L15H23, L15H27, L16H21, L16H23; (Layers 17-20) L17H2, L17H3, L17H9, L17H11, L18H9, L18H20, L19H8, L19H10, L19H12, L19H13, L20H1, L20H5; (Layers 21-23) L21H15, L21H16, L21H18, L21H19, L21H20, L21H21, L23H24, L23H26, L23H27; (Layers 24-27) L24H0, L24H1, L24H3, L24H5, L24H6, L24H20, L25H5, L25H14, L25H18, L25H19, L26H11, L26H26, L26H27, L27H8. For Qwen2.5 and = 2, the found SDHs are: (Layer 0) L0H1, L0H5, L0H7, L0H16, L0H23, L0H24, L0H26; (Layer 1) L1H3, L1H12, L1H14, L1H15, L1H16, L1H17, L1H19, L1H24; (Layers 2-7) L2H27, L3H1, L3H9, L3H10, L3H22, L3H24, L3H25, L3H26, L3H27, L6H15, L7H4; (Layers 8-15) L10H4, L10H22, L13H8, L13H10, L13H13, L15H5, L15H8, L15H9, L15H11, L15H12; (Layers 16-27) L16H21, L17H2, L18H9, L19H8, L20H1, L21H16, L21H20. For Qwen2.5 and = 3, the found SDHs are L0H1, L0H5, L1H0, L1H3, L1H17, L3H9, L6H15, L13H10. For Qwen2.5 and = 4, the found SDHs are L1H0, L1H17, L6H15. D.1.2 Attention Scores and Ranks of Q, K, and In this section, we present per-head statistics for SDHs with small . For conciseness, we report at most three heads for each in each model. Specifically, for each , we rank the SDHs by their average slash score and list the three heads with the largest values in the following table. Model Gemma Gemma Gemma Llama3 Llama Head L0H4 L0H7 L0H1 L0H2 L0H0 r1(Q) 0.895 0.953 (cid:101) 0.954 0.860 0.753 Qwen2.5 L0H6 Qwen2.5 L0H5 0.264 0. R0.95(Q) 2 1 1 (cid:101) 6 7 59 67 r1(K) 0.941 0.978 (cid:101) 0.994 0.738 0. 0.165 0.165 R0.95(K) 2 1 1 (cid:101) 16 16 73 73 Table 2: The values of and Qwen2.5-7B-Instruct. r1 and R0.95 for the SDHs located in the 0-th layer of Gemma-7B, Llama3-8B-Instruct, (cid:101) (cid:101) 38 Model Head E[Si,i] OOD E[Si,i] r1(Q) R0.95(Q) r1(K) R0.95(K) r1(H) R0.95(H)"
        },
        {
            "title": "Gemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma",
            "content": "Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama"
        },
        {
            "title": "0\nL0H4\n0 L26H3\n0 L27H8\n1\nL0H7\n1 L19H13\n1 L13H3\nL0H1\n2\nL3H9\n2\nL2H2\n2\nL0H1\n3\nL2H9\n3\nL0H1\n4",
            "content": "L0H"
        },
        {
            "title": "0 L31H14\n0 L12H12\n0 L16H30\n1\n1 L14H26\n1 L1H20\n2\nL0H0\nL0H2\n2\n2 L1H21\nL0H0\n3\nL0H0\n4",
            "content": "Qwen2.5 0 L18H7 Qwen2.5 0 L19H0 Qwen2.5 0 L14H25 Qwen2.5 1 L21H15 Qwen2.5 1 L13H13 L0H6 Qwen2.5 1 Qwen2.5 2 L1H3 Qwen2.5 2 L1H17 L0H5 Qwen2.5 2 Qwen2.5 3 L0H5 Qwen2.5 3 L1H17 L1H3 Qwen2.5 3 Qwen2.5 4 L1H0 Qwen2.5 4 L6H15 Qwen2.5 4 L1H17 1.000 1.000 1.000 0.904 0.364 0.333 0.302 0.148 0.127 0.197 0.115 0.101 0.961 0.698 0.539 0.560 0.516 0.333 0.180 0.169 0.165 0.159 0.109 1.000 1.000 0.742 0.699 0.664 0.655 0.297 0.200 0.192 0.161 0.156 0.154 0.174 0.114 0.107 1.000 1.000 1.000 0.999 0.741 0.752 0.352 0.278 0.298 0.172 0.144 0.072 0.991 0.667 0.538 0.656 0.701 0.749 0.160 0.141 0.410 0.125 0. 1.000 1.000 0.800 0.840 0.904 0.572 0.327 0.231 0.100 0.121 0.165 0.089 0.205 0.049 0.098 0.822 0.775 0.890 0.888 0.895 0.885 0.918 0.824 0.801 0.918 0.875 0.918 0.876 0.946 0.892 0.957 0.965 0.978 0.956 0.957 0.977 0.957 0.957 0.986 0.982 0.875 0.963 0.952 0.912 0.920 0.869 0.813 0.813 0.869 0.920 0.920 0.898 0.869 2 14 3 2 20 16 3 36 52 3 18 3 4 2 9 1 1 1 1 1 1 1 1 1 12 1 1 3 3 5 23 23 5 3 3 6 5 0.854 0.726 0.918 0.942 0.896 0.914 0.991 0.891 0.884 0.991 0.944 0.991 0.801 0.759 0.848 0.860 0.788 0.838 0.860 0.860 0.838 0.860 0.860 0.703 0.952 0.589 0.712 0.699 0.999 0.999 0.975 0.999 0.999 0.975 0.999 0.999 0.919 0.975 2 17 2 2 18 8 1 11 23 1 2 1 26 26 15 6 23 19 6 6 19 6 26 1 34 33 32 1 1 1 1 1 1 1 1 5 1 0.343 0.283 0.363 0.343 0.225 0.274 0.343 0.273 0.241 0.343 0.241 0.343 0.243 0.265 0.254 0.310 0.271 0.388 0.310 0.310 0.388 0.310 0.310 0.215 0.232 0.225 0.235 0.244 0.171 0.458 0.458 0.171 0.171 0.458 0.458 0.458 0.331 0.458 180 348 241 180 408 375 180 364 334 180 334 180 424 448 388 191 400 160 191 191 160 191 419 374 324 360 366 278 155 155 278 278 155 155 155 317 155 Table 3: This table lists the average attention scores of prompts in LongBench and OOD prompts, the rank information of Q, K, and H."
        },
        {
            "title": "Model\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma",
            "content": "Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Head L0H4 L26H3 L27H8 L0H7 L19H13 L13H3 L0H1 L3H9 L2H2 L2H9 L31H14 L12H12 L16H30 L0H2 L14H26 L1H20 L0H0 L1H21 Qwen2.5 L18H7 L19H0 Qwen2.5 Qwen2.5 L14H25 Qwen2.5 L21H15 Qwen2.5 L13H13 L0H6 Qwen2.5 L1H3 Qwen2.5 L1H17 Qwen2.5 L0H5 Qwen2.5 L1H17 Qwen2.5 L1H0 Qwen2.5 L6H15 Qwen2.5 r1(WQ) R0.95(WQ) 0.479 0.091 0.255 0.182 0.052 0.044 0.054 0.026 0.022 0.039 10 148 47 122 221 203 210 217 218 0.157 0.141 0.075 0.370 0.178 0.114 0.417 0.109 0.197 0.178 0.041 0.060 0.097 0.061 0.086 0.084 0.029 0.084 0.094 0.044 109 101 107 61 80 69 60 68 77 81 106 109 106 90 82 73 107 73 80 105 r1(WK) R0.95(WK) 0.534 0.081 0.244 0.219 0.049 0.039 0.362 0.045 0.029 0. 0.040 0.040 0.051 0.187 0.037 0.047 0.187 0.047 0.031 0.048 0.024 0.024 0.025 0.021 0.072 0.042 0.020 0.042 0.072 0.032 10 160 47 135 203 203 194 215 218 215 107 99 102 62 98 71 62 71 98 90 95 105 102 107 82 80 107 80 82 98 Table 4: The table reports r1 and R0.95 for the query and key projection matrices, WQ and WK, across attention heads in LLMs."
        },
        {
            "title": "Model",
            "content": ""
        },
        {
            "title": "Head",
            "content": "E[Si,i]"
        },
        {
            "title": "Gemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma\nGemma",
            "content": "Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Llama3 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 0 0 0 1 1 1 2 2 2 3 3 4 0 0 0 1 1 1 2 2 2 3 4 0 0 0 1 1 1 2 2 2 3 3 3 4 4 4 L0H4 L26H3 L27H8 L0H7 L19H13 L13H3 L0H1 L3H9 L2H2 L0H1 L2H9 L0H L31H14 L12H12 L16H30 L0H2 L14H26 L1H20 L0H0 L0H2 L1H21 L0H0 L0H0 L18H7 L19H0 L14H25 L21H15 L13H13 L0H6 L1H3 L1H17 L0H5 L0H5 L1H17 L1H3 L1H0 L6H15 L1H17 1.000 1.000 1.000 0.904 0.364 0.333 0.302 0.148 0.127 0.197 0.115 0.101 0.961 0.698 0.539 0.560 0.516 0.333 0.180 0.169 0.165 0.159 0.109 1.000 1.000 0.742 0.699 0.664 0.655 0.297 0.200 0.192 0.161 0.156 0.154 0.174 0.114 0.107 E[Si,i] w/o high freqs 0.884 0.807 0.680 0.043 0.282 0.274 0.085 0.103 0.045 0.143 0.054 0. E[Si,i] w/o med freqs 1.000 0.999 1.000 0.902 0.282 0.274 0.072 0.103 0.046 0.125 0.055 0.172 E[Si,i] w/o low freqs 1.000 1.000 1.000 0.902 0.282 0.274 0.072 0.104 0.046 0.125 0.042 0.173 0.653 0.431 0.306 0.114 0.077 0.035 0.066 0.121 0.006 0.076 0.083 0.967 0.943 0.473 0.114 0.108 0.120 0.137 0.078 0.062 0.074 0.089 0.120 0.109 0.076 0.094 0.908 0.691 0.493 0.530 0.509 0.331 0.155 0.164 0.164 0.140 0.098 0.967 0.993 0.704 0.653 0.629 0.558 0.295 0.175 0.149 0.127 0.138 0.152 0.173 0.095 0. 0.956 0.701 0.542 0.557 0.515 0.341 0.179 0.170 0.168 0.159 0.109 1.000 0.999 0.741 0.693 0.654 0.647 0.296 0.199 0.188 0.161 0.156 0.153 0.174 0.114 0.106 Table 5: This table quantifies the effect of low-, medium-, and high-frequency components on SDHs by reporting, for each band, the average slash score after removing that band. 41 D.2 Figures of Slash-dominant Heads with Small and Large Average Slash"
        },
        {
            "title": "Scores",
            "content": "D.2.1 Results of Llama3-8B-Instruct (a) Average attention score matrix of L31H14. (b) Average attention score matrix of L0H2. (c) Average attention score matrix of L0H0. Figure 27: Average of attention score matrices in Llama3-8B-Instruct with prompts from LongBench V2. (a) Average attention score matrix of L31H14. (b) Average attention score matrix of L0H2. (c) Average attention score matrix of L0H0. Figure 28: Average of attention score matrices in Llama3-8B-Instruct with prompts whose tokens are i.i.d. sampled from the uniform distribution on the alphabet. 42 (a) Hidden state of L31H14 after PCA. (b) Queries of L31H14. (c) Keys of L31H14. (d) InP(100, j, l) of L31H14. (e) Hidden state of L0H2 after PCA. (f) Queries of L0H2. (g) Keys of L0H2. (h) InP(100, j, l) of L0H2. (i) Hidden state of L0H0 after PCA. (j) Queries of L0H0. (k) Keys of L0H0. (l) InP(100, j, l) of L0H1. Figure 29: This figure shows the hidden states, queries, keys, and InP(100, j, l) for [100], [64] for Llama3-8B-Instruct. Here, the example prompt contains 100 tokens. The dimensions of queries and keys are both 128. D.2.2 Results of Gemma-7B (a) Average attention score matrix of L0H4. (b) Average attention score matrix of L0H7. (c) Average attention score matrix of L0H1. Figure 30: Average of attention score matrices in Gemma-7B with prompts from LongBench V2. 43 (a) Average attention score matrix of L0H4. (b) Average attention score matrix of L0H7. (c) Average attention score matrix of L0H1. Figure 31: Average of attention score matrices in Gemma-7B with prompts whose tokens are i.i.d. sampled from the uniform distribution on the alphabet. (a) Hidden state of L0H4 after PCA. (b) Queries of L0H4. (c) Keys of L0H4. (d) InP(100, j, l) of L0H4. (e) Hidden state of L0H7 after PCA. (f) Queries of L0H7. (g) Keys of L0H7. (h) InP(100, j, l) of L0H7. (j) Queries of L0H1. (i) Hidden state of L0H1 after PCA. Figure 32: This figure shows the hidden states, queries, keys, and InP(100, j, l) for [100], [128] for Gemma-7B. Here, the example prompt contains 100 tokens. The dimensions of queries and keys are both 256. We implement PCA for the Hidden state to reduce its dimension to 100. D.3 More Results of Slash-dominant Heads with Large (l) InP(100, j, l) of L0H1. (k) Keys of L0H1. In this section, we present additional results on SDHs with large . For notational convenience, we slightly abuse E[Si,i] to represent the average slash score reported in the tables. In the following, we will abbreviate Llama3-8B-Instruct, and Qwen2.5-7B-Instruct as Llama3, and Qwen2.5, respectively. 44 D.3.1 Full List of SDHs with Large In the following, we report each head and its corresponding in the form (LaHb, ). As mentioned in Section 6, we set κ = 103 and do not aim to find all the slash patterns for large but select subset of them. Llama3-8B-Instruct Model. (Layer 0) (L0H29, 563), (L0H29, 564), (L0H29, 565), (L0H29, 566), (L0H29, 619), (L0H29, 620), (L0H29, 621), (L0H29, 689), (L0H29, 690), (L0H29, 849), (L0H29, 850), (L0H29, 851), (L0H29, 852), (L0H29, 853), (L0H29, 920), (L0H29, 921), (L0H29, 922), (L0H29, 934), (L0H29, 935), (L0H29, 1037), (L0H29, 1044), (L0H29, 1045), (L0H29, 1046), (L0H29, 1283), (L0H29, 1575), (L0H29, 1576), (L0H29, 2117), (L0H29, 2247), (L0H29, 2248), (L0H29, 2354), (L0H29, 2355), (L0H29, 2362), (L0H29, 2363), (L0H29, 2364), (L0H29, 2371), (L0H29, 2372), (L0H29, 2373), (L0H29, 2374), (L0H29, 2409), (L0H29, 2410), (L0H29, 2902), (L0H29, 2910), (L0H29, 2911), (L0H29, 2912), (L0H29, 3073), (L0H29, 3574), (L0H30, 599), (L0H30, 600), (L0H30, 601), (L0H30, 685), (L0H30, 686), (L0H30, 687), (L0H30, 692), (L0H30, 693), (L0H30, 1270), (L0H30, 1271), (L0H30, 1272), (L0H30, 1273), (L0H30, 2112), (L0H30, 2113), (L0H30, 2114), (L0H30, 2899), (L0H30, 2900), (L0H30, 3909), (L0H30, 3910), (L0H30, 3911), (L0H30, 3912), (L0H30, 3913), (L0H30, 3914), (L0H30, 3915). (Layer 1) (L1H16, 3827), (L1H18, 3827). Qwen2.5-7B-Instruct Model. (Layer 0) (L0H0, 683), (L0H0, 684), (L0H0, 731), (L0H0, 732), (L0H0, 733), (L0H0, 740), (L0H0, 741), (L0H0, 1487), (L0H0, 1488), (L0H1, 673), (L0H2, 657), (L0H2, 658), (L0H2, 659), (L0H2, 660), (L0H2, 731), (L0H2, 732), (L0H2, 733), (L0H2, 734), (L0H2, 739), (L0H2, 740), (L0H2, 741), (L0H2, 742), (L0H2, 743), (L0H2, 744), (L0H2, 745), (L0H2, 746), (L0H2, 753), (L0H2, 754), (L0H2, 755), (L0H7, 912), (L0H7, 924), (L0H7, 925), (L0H7, 936), (L0H7, 937), (L0H7, 938), (L0H7, 939), (L0H8, 922), (L0H8, 923), (L0H8, 924), (L0H8, 925), (L0H8, 935), (L0H8, 936), (L0H8, 937), (L0H8, 938), (L0H8, 939), (L0H8, 940), (L0H8, 1030), (L0H8, 1031), (L0H9, 912), (L0H9, 913), (L0H9, 914), (L0H9, 915), (L0H9, 916), (L0H9, 917), (L0H9, 918), (L0H9, 919), (L0H9, 920), (L0H9, 921), (L0H9, 922), (L0H9, 923), (L0H9, 924), (L0H9, 925), (L0H9, 926), (L0H9, 927), (L0H9, 928), (L0H9, 929), (L0H9, 930), (L0H9, 936), (L0H9, 937), (L0H11, 923), (L0H11, 924), (L0H11, 925), (L0H11, 926), (L0H11, 927), (L0H11, 928), (L0H11, 929), (L0H11, 930), (L0H11, 937), (L0H11, 938), (L0H11, 939), (L0H11, 940), (L0H11, 941), (L0H15, 917), (L0H15, 918), (L0H15, 936), (L0H15, 1513), (L0H15, 1514), (L0H15, 1515), (L0H15, 1531), (L0H15, 1532), (L0H15, 1533), (L0H15, 1534), (L0H15, 1535), (L0H15, 3094), (L0H15, 3095), (L0H15, 3096), (L0H15, 3097), (L0H15, 3098), (L0H15, 3140), (L0H15, 3141), (L0H15, 3142), (L0H15, 3143), (L0H15, 3303), (L0H15, 3304), (L0H15, 3305), (L0H15, 3306), (L0H15, 3307), (L0H15, 3323), (L0H15, 3324), (L0H15, 3362), (L0H15, 3363), (L0H15, 3364), (L0H15, 3365), (L0H15, 3366), (L0H15, 3367), (L0H15, 3368), (L0H15, 3412), (L0H15, 3413), (L0H15, 3586), (L0H15, 3587), (L0H15, 3588), (L0H15, 3617), (L0H15, 3618), (L0H15, 3619), (L0H15, 3620), (L0H15, 3644), (L0H15, 3645), (L0H15, 3646), (L0H15, 3647), (L0H15, 3648), (L0H15, 3693), (L0H15, 3694), (L0H15, 3695), (L0H16, 1521), (L0H16, 1538), (L0H16, 1539), (L0H16, 1540), (L0H23, 500), (L0H23, 501), (L0H23, 502), (L0H23, 503), (L0H23, 504), (L0H23, 505), (L0H24, 500), (L0H24, 501), (L0H24, 502), (L0H24, 503), (L0H24, 504), (L0H24, 505), (L0H25, 500), (L0H25, 501), (L0H25, 503), (L0H25, 504), (L0H25, 505), (L0H25, 506), (L0H25, 507), (L0H25, 508), (L0H25, 509), (L0H25, 510), (L0H25, 511), (L0H26, 500), (L0H26, 501), (L0H26, 502), (L0H26, 503), (L0H26, 504), (L0H26, 505). (Layer 1) (L1H6, 502), (L1H6, 503), (L1H6, 504), (L1H6, 505), (L1H6, 506), (L1H6, 507), (L1H6, 508), (L1H6, 514), (L1H6, 515), (L1H6, 516), (L1H6, 517), (L1H6, 520), (L1H6, 521), (L1H6, 522), (L1H6, 523), (L1H6, 524), (L1H6, 525), (L1H6, 526), (L1H6, 527), (L1H6, 528), (L1H6, 529), (L1H6, 533), (L1H6, 534), (L1H6, 535), (L1H6, 536), (L1H6, 537), (L1H6, 538), (L1H6, 539), (L1H6, 540), (L1H6, 541), (L1H6, 542), (L1H6, 543), (L1H6, 553), (L1H11, 553), (L1H11, 554), (L1H11, 609), (L1H11, 610), (L1H11, 641), (L1H11, 710), (L1H11, 717), (L1H11, 718), (L1H11, 804), (L1H11, 805), (L1H11, 835), (L1H11, 1239), (L1H11, 1521), 45 (L1H11, 3532), (L1H12, 546), (L1H12, 602), (L1H12, 640), (L1H12, 641), (L1H13, 640), (L1H13, 641), (L1H13, 647), (L1H13, 648), (L1H15, 516), (L1H15, 523), (L1H15, 554), (L1H15, 2294), (L1H16, 541), (L1H16, 554), (L1H16, 555), (L1H16, 556), (L1H16, 611), (L1H16, 671), (L1H16, 672), (L1H16, 673), (L1H16, 2778), (L1H19, 553), (L1H19, 554), (L1H19, 610), (L1H19, 611), (L1H19, 648), (L1H20, 554), (L1H23, 553), (L1H23, 554), (L1H23, 678), (L1H23, 679), (L1H23, 685), (L1H23, 686), (L1H23, 687), (L1H23, 709), (L1H23, 710), (L1H23, 717), (L1H23, 718), (L1H23, 998), (L1H23, 999), (L1H23, 1036), (L1H23, 1037), (L1H23, 1162), (L1H23, 1163), (L1H23, 1201), (L1H23, 1848), (L1H23, 1849), (L1H23, 2129), (L1H23, 2130), (L1H23, 2293), (L1H23, 2294), (L1H23, 2295), (L1H23, 2613), (L1H23, 2620), (L1H23, 2621), (L1H23, 2622), (L1H23, 2777), (L1H23, 2932), (L1H23, 2933), (L1H23, 2934), (L1H23, 3097), (L1H23, 3438), (L1H24, 671), (L1H24, 834), (L1H24, 835), (L1H24, 836), (L1H24, 1145), (L1H24, 1146), (L1H24, 1147), (L1H24, 1148), (L1H24, 1149), (L1H24, 2238), (L1H24, 2683), (L1H24, 2684), (L1H24, 2721), (L1H24, 3368), (L1H24, 3369). (Layer 2) (L2H6, 514), (L2H6, 515), (L2H6, 741), (L2H6, 936), (L2H6, 998), (L2H6, 1933), (L2H6, 1934), (L2H6, 1935), (L2H6, 2159), (L2H6, 2160), (L2H6, 2161), (L2H6, 2185), (L2H6, 2186), (L2H6, 2198), (L2H6, 2273), (L2H6, 2274), (L2H6, 2275), (L2H6, 2362), (L2H6, 2363), (L2H6, 2758), (L2H6, 2845), (L2H6, 2846), (L2H6, 3695), (L2H7, 1277), (L2H7, 1278), (L2H7, 1551), (L2H7, 1552), (L2H7, 1553), (L2H7, 1554), (L2H7, 1557), (L2H7, 1558), (L2H7, 1559), (L2H7, 1560), (L2H7, 1561), (L2H7, 2439), (L2H7, 2440), (L2H7, 2721), (L2H8, 1275), (L2H8, 1276), (L2H8, 1277), (L2H8, 1278), (L2H8, 1279), (L2H8, 1551), (L2H8, 1552), (L2H8, 1553), (L2H8, 1554), (L2H8, 1555), (L2H8, 1556), (L2H8, 1557), (L2H8, 1558), (L2H8, 1559), (L2H8, 1560), (L2H8, 1561), (L2H8, 2439), (L2H8, 2440), (L2H8, 2441), (L2H8, 2716), (L2H8, 2717), (L2H8, 2718), (L2H8, 2720), (L2H8, 2721), (L2H9, 1561), (L2H10, 1552), (L2H10, 1553), (L2H10, 1554), (L2H10, 1555), (L2H10, 1556), (L2H10, 1557), (L2H10, 1558), (L2H10, 1559), (L2H10, 1560), (L2H10, 1561), (L2H11, 1559), (L2H11, 1560), (L2H12, 1559), (L2H12, 1560), (L2H13, 1552), (L2H13, 1553), (L2H13, 1554), (L2H13, 1558), (L2H13, 1559), (L2H13, 1560), (L2H13, 1561). (Layer 3) (L3H1, 534), (L3H1, 535), (L3H1, 666), (L3H1, 667), (L3H1, 668), (L3H1, 669), (L3H1, 736), (L3H1, 737), (L3H1, 738), (L3H1, 739), (L3H1, 740), (L3H1, 741), (L3H1, 742), (L3H1, 743), (L3H1, 744), (L3H2, 736), (L3H2, 737), (L3H2, 738), (L3H2, 739), (L3H2, 740), (L3H2, 741), (L3H2, 742), (L3H2, 743), (L3H2, 744), (L3H2, 745), (L3H2, 746), (L3H2, 747), (L3H3, 531), (L3H3, 532), (L3H3, 533), (L3H3, 534), (L3H3, 665), (L3H3, 666), (L3H3, 667), (L3H3, 734), (L3H3, 735), (L3H3, 736), (L3H3, 737), (L3H3, 738), (L3H3, 739), (L3H3, 740), (L3H3, 741), (L3H3, 742), (L3H3, 743), (L3H3, 744), (L3H3, 745), (L3H3, 2217), (L3H4, 532), (L3H4, 533), (L3H4, 534), (L3H4, 535), (L3H4, 667), (L3H4, 740), (L3H4, 741), (L3H4, 742), (L3H4, 743), (L3H5, 668), (L3H5, 737), (L3H5, 738), (L3H5, 739), (L3H5, 740), (L3H5, 741), (L3H5, 742), (L3H5, 743), (L3H5, 744), (L3H5, 745), (L3H5, 746), (L3H5, 747), (L3H5, 748), (L3H6, 736), (L3H6, 737), (L3H6, 738), (L3H6, 739), (L3H6, 740), (L3H6, 741), (L3H6, 742), (L3H6, 743), (L3H6, 744), (L3H6, 745), (L3H6, 746), (L3H6, 747), (L3H6, 748), (L3H6, 749), (L3H6, 2218), (L3H6, 2219), (L3H6, 2220), (L3H6, 2221), (L3H6, 2222), (L3H6, 2223), (L3H6, 2224), (L3H6, 2225), (L3H6, 2226), (L3H15, 3368), (L3H15, 3369), (L3H15, 3370), (L3H21, 922), (L3H21, 923), (L3H22, 1030), (L3H22, 1031), (L3H22, 1032), (L3H23, 920), (L3H23, 921), (L3H23, 922), (L3H23, 923), (L3H23, 924), (L3H23, 925), (L3H23, 1016), (L3H23, 1017), (L3H23, 1025), (L3H23, 1026), (L3H23, 1027), (L3H23, 1028), (L3H23, 1029), (L3H23, 1030), (L3H23, 1031), (L3H24, 735), (L3H24, 736), (L3H24, 742), (L3H24, 743), (L3H24, 804), (L3H24, 805), (L3H24, 806), (L3H24, 943), (L3H24, 944), (L3H24, 954), (L3H24, 955), (L3H24, 986), (L3H24, 987), (L3H24, 1014), (L3H24, 1015), (L3H24, 2684), (L3H24, 3712), (L3H24, 3713), (L3H24, 3714), (L3H24, 3715), (L3H25, 3714), (L3H25, 3715), (L3H25, 3716), (L3H26, 743), (L3H26, 922), (L3H26, 923), (L3H26, 924), (L3H26, 1013), (L3H26, 1014), (L3H26, 1015), (L3H26, 1016), (L3H26, 1017), (L3H26, 3713), (L3H26, 3714), (L3H27, 1015), (L3H27, 1016), (L3H27, 1017), (L3H27, 1028), (L3H27, 1029), (L3H27, 1030), (L3H27, 1031), (L3H27, 3712), (L3H27, 3713), (L3H27, 3714), (L3H27, 3715), (Layer 4) (L4H10, 3839), (L4H10, 3923), (L4H10, 3956), (L4H10, 3957), (L4H10, 3980), (L4H10, 3987), (L4H18, 1094), 46 (L4H18, 1754), (L4H18, 1755), (L4H23, 1258), (L4H23, 1684), (L4H23, 1685), (L4H23, 1741), (L4H23, 1742), (L4H23, 2018), (L4H23, 2276), (L4H23, 3532), (L4H23, 3960), (L4H25, 1740). (Layer 6) (L6H0, 923), (L6H0, 1037), (L6H0, 1038), (L6H0, 2294), (L6H0, 2295), (L6H0, 2828), (L6H0, 3329), (L6H0, 3330), (L6H0, 3331), (L6H0, 3332), (L6H0, 3650), (L6H0, 3651), (L6H0, 3745), (L6H0, 3814), (L6H1, 1037), (L6H1, 1038), (L6H1, 2294), (L6H1, 2295), (L6H1, 3330), (L6H1, 3331), (L6H2, 922), (L6H2, 923), (L6H2, 924), (L6H2, 1037), (L6H2, 1038), (L6H2, 1452), (L6H2, 1905), (L6H2, 1974), (L6H2, 2293), (L6H2, 2294), (L6H2, 2295), (L6H2, 2827), (L6H2, 2828), (L6H2, 2941), (L6H2, 3330), (L6H2, 3331), (L6H2, 3425), (L6H2, 3745), (L6H2, 3746), (L6H2, 3814), (L6H2, 3853), (L6H3, 923), (L6H3, 924), (L6H3, 925), (L6H3, 931), (L6H3, 1037), (L6H3, 1038), (L6H3, 1039), (L6H3, 1358), (L6H3, 1359), (L6H3, 1910), (L6H3, 1911), (L6H3, 2295), (L6H3, 2296), (L6H3, 2828), (L6H3, 2829), (L6H3, 2942), (L6H3, 2943), (L6H3, 3330), (L6H3, 3331), (L6H3, 3332), (L6H3, 3650), (L6H3, 3651), (L6H3, 3652), (L6H4, 1037), (L6H4, 2293), (L6H4, 2294), (L6H4, 2295), (L6H4, 2828), (L6H4, 2941), (L6H4, 3329), (L6H4, 3330), (L6H4, 3331), (L6H4, 3425), (L6H4, 3532), (L6H4, 3533), (L6H5, 922), (L6H5, 923), (L6H5, 2294), (L6H5, 3329), (L6H5, 3330), (L6H5, 3331), (L6H6, 919), (L6H6, 1038), (L6H6, 1239), (L6H6, 1974), (L6H6, 2294), (L6H6, 2295), (L6H6, 3212), (L6H6, 3331), (L6H6, 3425), (L6H6, 3426), (L6H6, 3532), (L6H6, 3533), (L6H6, 3746), (L6H6, 3853), (L6H25, 1239), (L6H25, 1268), (L6H25, 1269), (L6H25, 1270), (L6H25, 1445), (L6H25, 1446), (L6H26, 1269), (L6H26, 1270), (Layers 10 11) (L10H1, 967), (L10H1, 3204), (L10H1, 3217), (L10H1, 3694), (L10H1, 3695), (L10H22, 1094), (L10H22, 1095), (L11H3, 2161), (L11H3, 2162), (L11H14, 2089), (L11H14, 2090), (L11H14, 2091), (L11H18, 2090), (L11H18, 2091), (Layers 21 14) (L21H15, 2175), (L24H0, 880), (L24H0, 881), (L24H0, 1025), (L24H0, 1026), (L24H0, 1414), (L24H0, 1415), (L24H0, 2294), (L24H0, 2295), (L24H0, 3249), (L24H0, 3250), (L24H0, 3368), (L24H0, 3369), (L24H1, 880), (L24H1, 881), (L24H1, 1163), (L24H1, 1415), (L24H1, 2294), (L24H1, 2295), (L24H1, 3249), (L24H1, 3250), (L24H1, 3356), (L24H1, 3368), (L24H1, 3369), (L24H2, 879), (L24H2, 880), (L24H2, 1413), (L24H2, 1414), (L24H2, 2293), (L24H2, 2294), (L24H2, 3248), (L24H2, 3367), (L24H2, 3368), (L24H4, 880), (L24H4, 2294), (L24H4, 3249), (L24H4, 3250), (L24H5, 880), (L24H5, 881), (L24H5, 2294), (L24H5, 2295), (L24H5, 3248), (L24H5, 3249), (L24H5, 3250), (L24H6, 880), (L24H6, 881), (L24H6, 2294), (L24H6, 3249), (L24H6, 3250). (Layers 25 26) (L25H14, 2701), (L25H14, 2702), (L25H18, 2701), (L25H18, 2702), (L25H19, 2701), (L25H19, 2702), (L26H8, 610), (L26H8, 2294), (L26H8, 3494), (L26H8, 3532) D.3.2 Attention Scores and Ranks of Q, K, and In this section, we present per-head statistics for SDHs with large . The previous section shows that single head can exhibit multiple slash patterns. For conciseness, we report at most one values of for each head in each model, since slash patterns within the same head share the same Q, K, and H. Specifically, for each head, we rank all by their average slash score and list the largest values in the following table. 47 Model Head E[Si,i](103) Llama3 Llama3 Llama3 Llama3 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 L0H29 563 685 L0H30 3827 L1H16 3827 L1H L0H7 937 L0H23 503 L0H24 503 L0H26 503 L1H11 804 L1H15 554 L1H16 554 610 L1H19 2293 L1H23 L1H24 835 1934 L2H6 3713 L3H24 1685 L4H23 L6H1 3331 L6H2 2294 L6H3 3651 L6H4 2294 1974 L6H6 3694 L10H1 3369 L24H1 1.321 1.161 1.021 1.002 3.064 2.049 2.035 2.077 2.644 2.207 2.969 2.206 3.866 2.599 2.651 2.940 3.636 2.217 7.480 2.573 5.650 2.241 2.275 2.650 OOD E[Si,i](103) 0.725 0.561 4.490 7.999 6.664 1.326 1.398 1.436 2.873 2.106 3.222 2.868 10.523 12.972 8.729 1.070 2.229 0.992 23.225 2.864 12.231 2.678 4.549 2.249 r1(Q) R0.95(Q) r1(K) R0.95(K) r1(H) R0.95(H) 0.837 0.939 0.922 0.895 0.748 0.941 0.913 0.910 0.824 0.875 0.778 0.741 0.909 0.812 0.906 0.770 0.950 0.961 0.961 0.933 0.942 0.977 0.943 0.921 3 2 3 6 46 3 13 14 6 6 9 10 3 7 9 27 1 1 1 3 2 1 2 0.719 0.719 0.847 0.847 0.999 0.996 0.996 0.996 0.946 0.975 0.975 0.975 0.934 0.934 0.645 0.959 0.680 0.721 0.721 0.721 0.721 0.721 0.761 0.755 4 4 11 11 1 1 1 1 2 1 1 1 2 2 51 1 44 51 51 51 51 51 27 51 0.304 0.304 0.386 0.386 0.001 0.160 0.160 0.160 0.507 0.002 0.507 0.507 0.002 0.002 0.001 0.187 0.163 0.333 0.333 0.333 0.333 0.333 0.317 0. 294 294 227 227 3584 590 590 590 227 3584 227 227 3584 3584 3584 554 382 669 669 669 669 669 716 645 Table 6: This table lists the average attention scores of prompts in LongBench and OOD prompts for Llama3-8B-Instruct and Qwen2.5-7B-Instruct, the rank information of Q, K, and H. The unit of the attention scores is 103. 48 Model Llama3 Llama3 Llama3 Llama3 Head L0H29 L0H30 L1H16 L1H r1(WQ) R0.95(WQ) 0.339 0.487 0.044 0.035 24 26 69 66 Qwen2.5 L0H7 Qwen2.5 L0H23 Qwen2.5 L0H24 Qwen2.5 L0H26 Qwen2.5 L1H11 Qwen2.5 L1H15 Qwen2.5 L1H16 Qwen2.5 L1H19 Qwen2.5 L1H23 Qwen2.5 L1H24 Qwen2.5 L2H6 Qwen2.5 L3H24 Qwen2.5 L4H23 L6H1 Qwen2.5 L6H2 Qwen2.5 L6H3 Qwen2.5 L6H4 Qwen2.5 Qwen2.5 L6H6 Qwen2.5 L10H1 Qwen2.5 L24H1 0.018 0.031 0.027 0.025 0.066 0.058 0.087 0.080 0.066 0.095 0.043 0.030 0.050 0.021 0.033 0.017 0.034 0.023 0.028 0.032 98 118 117 117 80 73 75 73 66 65 99 105 99 108 108 108 108 107 101 112 r1(WK) R0.95(WK) 0.344 0.344 0.033 0.033 0.016 0.022 0.022 0.022 0.042 0.042 0.042 0.042 0.049 0.049 0.019 0.021 0.018 0.015 0.015 0.015 0.015 0.015 0.025 0.015 25 25 72 72 100 116 116 116 86 80 80 80 74 74 97 101 101 103 103 103 103 103 102 111 Table 7: The table reports r1 and R0.95 for the query and key projection matrices, WQ and WK, across attention heads in LLMs. Model Llama3 Llama Head L0H29 L0H30 Qwen2.5 L0H7 Qwen2.5 L0H23 Qwen2.5 L0H24 Qwen2.5 L0H26 r1(Q) 0.904 .943 (cid:101) 0.221 0.250 0.218 0.215 R0.95(WQ) 3 2 (cid:101) 68 72 73 r1(K) 0.877 0.877 (cid:101) 0.079 0.155 0.155 0.155 R0.95(K) 4 4 (cid:101) 73 75 75 75 Table 8: The values of Qwen2.5-7B-Instruct. r1 and R0.95 for the SDHs located in the 0-th layer of Llama3-8B-Instruct, and (cid:101) (cid:101)"
        },
        {
            "title": "Model",
            "content": ""
        },
        {
            "title": "Head",
            "content": "E[Si,i] Llama3 Llama3 Llama3 Llama3 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 L0H29 563 685 L0H30 3827 L1H16 3827 L1H18 L0H7 937 L0H23 503 L0H24 503 L0H26 503 L1H11 804 L1H15 554 L1H16 554 610 L1H19 2293 L1H23 835 L1H24 L2H6 1934 3713 L3H24 1685 L4H23 L6H1 3331 L6H2 2294 L6H3 3651 L6H4 2294 1974 L6H6 3694 L10H1 3369 L24H1 1.321 1.161 1.021 1. 3.093 2.063 2.063 2.079 2.717 2.340 3.018 2.245 3.884 2.773 2.617 2.957 3.638 2.253 7.417 2.666 5.698 2.264 2.179 2.566 E[Si,i] w/o high freqs 0.598 0.872 0.710 0.774 E[Si,i] w/o med freqs 1.742 1.417 0.974 0.970 E[Si,i] w/o low freqs 1.657 1.223 2.466 2.764 1.523 1.446 1.456 1.409 0.831 0.422 0.975 0.605 0.257 0.169 1.348 1.448 0.651 0.710 1.360 0.832 1.466 0.422 0.555 0.613 1.522 1.263 1.230 1.241 2.808 1.924 2.712 2.407 4.143 6.733 2.842 1.974 10.284 2.934 13.771 7.652 9.087 4.368 4.799 2. 3.164 2.073 2.060 2.104 2.800 2.220 2.975 2.207 5.779 2.754 2.670 2.916 3.637 2.226 7.481 2.583 5.638 2.241 4.768 2.655 Table 9: This table quantifies the effect of low-, medium-, and high-frequency components on SDHs by reporting, for each band, the average slash score after removing that band. The unit of attention scores is 103. Head L0H7 L0H23 L0H24 L0H26 Avg. x 7.403 5.632 5.782 5. x bQ Avg. 10.039 9.343 9.173 17.083 9.173 16.097 9.173 15.828 bK 281.761 108.957 108.957 108.957 Table 10: This table lists the average norms of norms of biases bQ and bK and across all the token embeddings and the 50 D.4 Figures of Slash-dominant Heads with Large and Small Average Slash"
        },
        {
            "title": "Score",
            "content": "D.4.1 Results of Qwen2.5 (a) Hidden state of L0H7 after PCA. (b) Queries of L0H7. (c) Keys of L0H7. (d) InP(5000, j, l) of L0H7. (e) Hidden state of L1H11 after PCA. (f) Queries of L1H11. (g) Keys of L1H11. (h) InP(5000, j, l) of L1H11. (i) Hidden state of L2H6 after PCA. (j) Queries of L2H6. (k) Keys of L2H6. (l) InP(5000, j, l) of L2H6. Figure 33: This figure shows the hidden states, queries, keys, and InP(5000, j, l) for [5000], [64] for Qwen2.5-7B-Instruct. Here, the example prompt contains 5000 tokens. The dimensions of queries and keys are both 128. 51 D.4.2 Results of Llama3-8B-Instruct (a) Average attention score matrix of L0H29. (b) Average attention score matrix of L0H30. (c) Average attention score matrix of L1H16. Figure 34: Average of attention score matrices in Llama3-8B-Instruct with prompts from LongBench V2. (a) Average attention score matrix of L0H29. (b) Average attention score matrix of L0H30. (c) Average attention score matrix of L1H16. Figure 35: Average of attention score matrices in Llama3-8B-Instruct with prompts whose tokens are i.i.d. sampled from the uniform distribution on the alphabet. 52 (a) Hidden state of L0H29 after PCA. (b) Queries of L0H29. (c) Keys of L0H29. (d) InP(100, j, l) of L0H29. (e) Hidden state of L0H30 after PCA. (f) Queries of L0H30. (g) Keys of L0H30. (h) InP(100, j, l) of L0H30. (i) Hidden state of L1H16 after PCA. (j) Queries of L1H16. (k) Keys of L1H16. (l) InP(100, j, l) of L0H1. Figure 36: This figure shows the hidden states, queries, keys, and InP(100, j, l) for [100], [64] for Llama3-8B-Instruct. Here, the example prompt contains 100 tokens. The dimensions of queries and keys are both 128. D.5 Tables related to token embeddings in the 0-th layer 8.0% 6.6% 6.9% 6.2% 4.3% 6.0% Model Head RV(vQ) RV(vK) RV(vrand) 953% Gemma L0H4 953% Gemma L0H7 Gemma L0H1 953% Llama3 L0H0 34.1% 24.8% 1312% Llama3 L0H2 20.1% 24.8% 1312% Llama3 L0H29 9.8% 14.6% 1312% Llama3 L0H30 8.2% 14.6% 1312% 6.324 6.439 7. hi bQ Head hi bK 7.092 466.607 8.518 L0H5 7.092 466.607 13.522 L0H6 L0H7 10.039 281.761 9.343 L0H15 12.112 160.007 7.469 237.963 9.173 108.957 17.083 L0H23 5.632 9.173 108.957 16.097 L0H24 5.782 9.173 108.957 15.828 L0H26 5.810 Table 11: This table reports the relative variation of token embeddings projected onto the dominant subspace of the SDHs in the 0-th layer of Gemma-7B and Llama3-8B-Instruct. hi and Table 12: This table lists the average norms of hi over the alphabet, together with the norms of bQ and bK for SDH in the 0-th layer of Qwen2.5-7B-Instruct."
        },
        {
            "title": "Descriptions",
            "content": "The logit from position to before softmax in Layer 1 ((21)). Ai,j S(1) i,j A, S(1) RN The attention logit and attention score matrix in Layer 1. The attention score from position to in Layer 1. S(1) i,x S(1) i,y S(2) , S(2) (2) u, S(2) R1N (2) , Attention scores on x, i.e., odd position: ji Attention scores on y, i.e., even position: (cid:80) 1{j 1(mod 2)}S(1) i,j . 1{j 0(mod 2)}S(1) i,j . ji Attention scores to i-token and k-th feature in Layer 2 (Equation (16)). (cid:80) Approximate expectation version of S(2) , S(2) in Stage (Theorem F.6). The attention logit and score vector of the question Eq in Layer 2. ui The attention logit from Eq to Ei in Layer 2 ((23) and (47)). Constant vector (1, 0, , 1, 0). Table 14: Summary of frequently used notations in proof. We omit (t) here for simplicity. (cid:101) Notations. For two functions and of n, we write (n) = O(g(n)) or (n) g(n) if g(n) 0, and there exist constants C, n0 > 0 such that for all n0, (n) g(n), and (n) = O(g(n)) or (n) g(n) if g(n) 0, and there exist constants > 0 and n0 such that for all n0, (n) g(n). Similarly, we write (n) = Ω(g(n)) or (n) g(n), if g(n) 0, and there exist constants > 0 and n0 such that for all n0, (n) g(n), and we write (n) = Ω(g(n)) or (n) g(n) if g(n) 0, and there exist constants > 0 and n0 such that for all n0, (n) g(n). We write (n) = Θ(g(n)) or (n) g(n) if (n) = O(g(n)) and (n) = Ω(g(n)). = Ex,y In addition, we abuse the notation 1{j = 2r 1} to indicate that for given j, if there exists Z, s.t. = 2r 1, then it takes 1, and 0 otherwise. For simplicity, we further denote different sub-matrices of as Ec = E:,1:dc RN dc , Ex,y = E:,dc+1:d RN (dX+2), Ey = E:,d RN . For each i, we write Ex,y N,: R1(d+2). We summarize additional frequently used notations related to transformers in Table 14, and we omit the timestep (t) for simplicity when there is no ambiguity. Expression of Reduced Model. We first visualize the reduction of Layer 1 and 2 weights in Figures 37 and 38. i,: R1(d+2), and we denote the last row by Ex,y = Ex,y Then we finally write the expression of the prediction under the reduced model as follows: yq(E; θ) = softmax (2) )ℜϑdc+1:d (softmax (A) Ex,y) Ey, (21) for i, and otherwise, and the RoPE operator ℜϑdc+1:d and Rϑ1:dc ,ji (cid:102) (cid:17) ℜϑdc+1:d (Ex,y (cid:16) (cid:101) (1) where Ai,j = (cid:98) Rϑ1:dc ,ji are defined in (3). (cid:102) (cid:101) 54 Figure 37: Illustration of the Reduction of Layer 1. In Figures 37 and 38, the softmax operator is abbreviated as sf. The output of Layer 1 is (1) = [E, CSA(E)]. In the figure, we show the reduction of Layer 1 parameters (1) {Q,K,V } and how CSA(E) is generated by the reduced parameters. In particular, the weight matrices are only non-zero in the semantically independent subspace associated with c, and the key and value weight matrices (1) {K,V } are fixed during training. Hence, CSA(E) only depends on (1) . Proof of Stage of Theorem 5. (cid:102) F.1 Roadmap of the Proof We analyze the emergence of slash-dominance in Stage via three phases of dynamics (Sections F.4 to F.6). In each phase, we formulate an induction hypothesis and derive several lemmas describing the values and evolution of key statistics, which collectively govern the attention scores. We prove the induction hypothesis and the lemmas by induction. The main idea of the proof lies in tracking the update dynamics of the attention logit in layer 1 Al,r(t), l,r (t). From Theorem F.2, we find that for any 0 , which decides the Layer 1 attention score S(1) the update of Al,r(t) : Al,r(t) = Al,r(t + 1) Al,r(t) can be written as: Al,r(t) = η1 C1 ai,i+rl(t) + C2 ai,j(t) ai,j(t)O(ϵFN) , (cid:16) i=lr+1 (cid:88) 1jiN (cid:88) 1jiN (cid:88) (cid:17) where we denote ai,j(t) = (cid:104) yq w, xq) S(2) ( (t)( yq Ey )S(1) i,j (t)(I(i)j S(1) i,ℓ (t)I(i)ℓ) . ℓi (cid:88) (cid:105) (cid:98) (cid:98) The value of Al,r(t) largely depends on ai,j(t) for all i, [N ]. The expression of ai,j(t) can be further simplified by considering different cases of (i, j), as shown in Theorem F.3, where we demonstrate that ai,j(t) is determined by the attention scores in Layer 1 and Layer 2, S(1) (t). In addition, from Theorem F.7, S(2) (t) = Θ(1/K) in stage I. schematic of the interaction of statistics is shown in Figure 39. In addition, from the interaction above, we observe that for different i, [N ], the attention scores S(1) i,j (t) with the same offset are of the same order. The similar conclusion also holds for S(1) l,r (t) and the increments A(1) i,j (t) and S(2) l,r (t), A(1) l,r (t). Figure 38: Illustration of the Reduction of Layer 2 and Transformer Output. First, with reduced (2) = I2d and WO = [0dd 0dd Id 0dd], the CSA output CSA(H (1)) = S(2)H (1)W (2) = S(2)H (1) RN 2d, Layer 2 output (2) = [E, CSA(E), CSA(H (1))] RN 4d, and the transformer output TFθ(E) = (2)WO = S(2)E. In this figure, we show the reduction of S(2) under Here, S(2) is the Layer 2 attention score matrix. sparse trainable query weight (2) . Hence, the output TFθ(E) only depends on (1) , and fixed key weight (2) (2) . (cid:102) (cid:102) The learning process can be divided into three phases. Throughout all three phases, for any [N ], the attention logit corresponding to the immediately preceding token, Al,l1(t), keeps growing, although its growth rate, denoted by Al,l1(t), varies across the phases. In contrast, other logits Al,r(t), = 1 oscillate, typically with much smaller rate than the growth rate of Al,l1(t). As result, S(1) l,l1(t) keeps increasing and exhibits different orders of magnitude across the three phases as described below. l1 Phase I: Emergence of Slash-Dominance. (t [0, (1) l,r are relatively uniform across r: S(1) any [N ], the attention scores S(1) S(1) , = 1. As for the attention logits, Al,l1(t) keeps growing, but Al,r(t), = 1 l,r (t) = may oscillate. In addition, the growth rate Al,l1(t) is much higher than that of other Al,r(t) with = 1, since for any [N ], ai,i1 is always positive and has much larger order of magnitude than ai,j with = i. Specifically, from Theorem F.10, Al,l1(t) maxrl,r=l1 Al,r(t) η1C1K 1N 1. Therefore, the increase in A(t) ], Section F.4). During phase I, for , and l,l1(t) = Ω l,l1 dominates the learning dynamics during phase I. (cid:1) (cid:0) (cid:1) (cid:0) 1 Phase II: Rapip Growth of Slash-Dominance. (t (T (1) , (1) ], Section F.5). After rapid growth 2 of Al,l1(t) in phase I, S(1) l,l1(t) = Ω(1) grows to constant order. However, for other = 1, S(1) l,l1 still l,r (t) dominates the learning dynamics during phase II, and the growth rate gets much larger than Phase I. Specifically, from Theorem F.15, Al,l1(t) maxrl,r=l1 Al,r(t) η1C1K 2. l,r is no longer uniform. The increase in A(t) . The attention scores S(1) 1 (cid:1) (cid:0) 1 Phase III: Convergence (t (T (1) 2 , (1) 3 ], Section F.6). for any [N ], Al,l1(t) keep growing but in Θ( 1 ) in Stage Theorem F.7 S(1) ℓ,r (t), ℓ, [N ] S(2) (t), [K] S(1) ℓ,r (t + 1), ℓ, [N ] Theorem F.3 Aℓ,r (t + 1) ai,j(t), i, [N ] Aℓ,r(t), ℓ, [N ] Theorem F. Figure 39: Schematic of dependencies among key statistics and attention scores at timestep and + 1 in Stage I. smaller rate. As result, S(1) phase III, at step = (1) 3 + 1, S(1) l,l1(t) finally exceeds 1 ϵ1. l,l1(t) keeps growing but can not exceed 1 ϵ1. Finally, after the end of We summarize the upcoming sections as follows: In Section F.2, we compute and simplify the gradients to identify the key update variables. In Section F.3, we introduce several useful auxiliary lemmas for Stage I. In Sections F.4 to F.6, we analyze the three phases of the dynamics. F.2 Stage I: Preliminary Development Computations of Gradients. We first calculate Stage gradient with respect to (1) . We omit (t) in this section when there is no ambiguity and write L( θ) as here for simplicity. Lemma F.1 (Layer 1 Gradient). In Stage I, where with respect to (1) is given by is kept as Id. The gradient of the loss function (cid:102) = (cid:34) (cid:102)W (1) (cid:98) yq w, xq) ( S(2) (Ey yq) S(1) i,j I(i)j S(1) i,ℓ I(i)ℓ cRϑ,ij jiN (cid:88) (cid:16) ℓi (cid:88) (cid:17) , (cid:35) where I(i) = Ex,yRϑ,N i(Ex,y embeddings. The notation I(i)ℓ refers to the ℓ-th entry of I(i), and Ex,y, Ex,y dependent subspaces of the token embeddings and Eq, respectively. ) RN , capturing the correlation of the question embedding with the prompt correspond to the semantic Proof We first write out the expression of yq. In Stage I, (2) is fixed as Id, then where S(2) = softmax(u) R1N , and (cid:102) (cid:98) (cid:102) i=1 (cid:88) yq( (cid:98) (1) , (2) ) = Nin (cid:102) S(2) 2i yi, ui = S(1) i,: I(i) = (S(1) i,: )Ex,y Rϑ,iN (Ex,y). (22) (23) (2) (cid:101) (cid:102) (cid:98) (cid:102) (cid:101) Here S(1) i,: is the i-th row of the attention score matrix. Then we can compute the gradient. We first obtain: (cid:102)W (1) L = E[( yq w, xq) (cid:102)W (1) yq] = (cid:104) yq w, xq) ( where the last equation follows from (22). We next compute R1d, softmax(U ) = diag(softmax(U )) softmax(U )softmax(U ), we have (cid:102)W (1) (cid:98) (cid:98) (cid:98) ( (cid:102)W (1) S(2) 2ℓ )yℓ , (24) (cid:88)ℓ[Nin] S(2) 2ℓ by chain rule. Recall that for any (cid:105) S(2) 2ℓ S(1) i,n = m=1 (cid:88) S(2) 2ℓ um um S(1) i,n (i) = S(2) 2ℓ ui ui S(1) i,n (ii) = (S(2) 1{2ℓ = i} S(2) S(2) 2ℓ ) I(i)n, (25) where (i) follows from that um/S(1) addition, i,n = 0 if = i, and (ii) follows from the definition of ui in (23). In S(1) i,n Ai,j = S(1) i,j 1{n = j} S(1) i,j S(1) i,n . Then combine (25) and (26), we have S(2) 2ℓ Ai,j (i) = ni (cid:88) S(2) 2ℓ S(1) i,n S(1) i,n Ai,j = S(2) (1{2ℓ = i} S(2) 2ℓ )S(1) i,j I(i)j S(1) i,ℓ I(i)ℓ , (cid:16) ℓi (cid:88) (cid:17) (26) (27) where (i) follows from the chain rule and the definition of causal mask. In addition, recall, if i, then Ai,j = c, and we have (1) Rϑ,ji (cid:102) (cid:101) (cid:102)W (1) Ai,j = c(Rϑ,ji) = cRϑ,ij. As result, combine (27) and (28), we have (cid:101) (cid:101) (cid:102)W (1) S(2) 2ℓ = S(2) jiN (cid:88) Then, by direct calculation, we have (1{2ℓ = i} S(2) 2ℓ )S(1) i,j I(i)j S(1) i,ℓ I(i)ℓ cRϑ,ij. (cid:16) ℓi (cid:88) (cid:17) (cid:101) S(2) 2ℓ ) yℓ = (cid:102)W (1) ( (cid:88)ℓ[Nin] jiN (cid:88) S(2) (Ey yq)S(1) i,j I(i)j S(1) i,ℓ I(i)ℓ cRϑ,ij. (cid:16) ℓi (cid:88) (cid:17) (cid:101) (cid:98) As result, plugging (29) into (24) proves Theorem F.1. Thus, we conclude the proof of Lemma F.1. (28) (29) Computations of Logits Update. In stage I, the logits Al,r(t) determine the attention scores. Hence, to get the dynamics of attention scores, we only need to track the update of Al,r(t) : Al,r(t) = Al,r(t + 1) Al,r(t) as follows. Lemma F.2 (Track Al,r(t) Update). Suppose Assumption 5.1 holds. In Stage I, for any 0 , we have Al,r(t) = η1 ai,i+rl(t) + C2 ai,j(t) + (ai,j(t))O(ϵFN) , 1jiN (cid:88) where C1, C2, ϵFN are parameters in Assumption 5.1, and i=lr+1 (cid:88) (cid:16) 1jiN (cid:88) (cid:17) ai,j(t) = (cid:104) yq w, xq) S(2) ( (cid:98) (t)( yq Ey )S(1) i,j (t)(I(i)j (cid:98) 58 S(1) i,ℓ (t)I(i)ℓ) . ℓi (cid:88) (cid:105) Proof For any 1 , Al,r(t) = Theorem F.1, we have (1) (t)Rϑ,rl c. Considering the GD update rule and applying Al,r(t) = η1c( (cid:102)W (1) Following from Assumption 5.1, we have dc/2 (cid:102) L(t))Rϑ,rl (cid:101) = η1 ai,j(t) cRϑ,ij+rl . (30) jiN (cid:88) (cid:0) (cid:101) (cid:101) (cid:1) (cid:101) cRϑ,ij+rl = cos (θs(i + l)) = C1δ0(i + l) + C2 O(ϵFN). s=1 (cid:88) By plugging (31) into (30), we conclude the proof of Theorem F.2. (cid:101) (cid:101) (31) Simplification of Logits Update. From Theorem F.2, we find that the update Ar,l(t) depends significantly on ai,j(t). However, the expression of ai,j(t) is quite complicated and depends on i, j. To simplify it, we next characterize ai,j(t) by cases in the next lemma. We omit (t) in Theorem F.3 for abbreviation. The key auxiliary lemmas used in the characterization of ai,j(t) are deferred to Section F.3. Lemma F.3 (Characterization of ai,j). In Stage I, consider versions of S(2) holds. (2) are approximate expectation for any [N ] and [K]. For different cases of [N ], i, the following and S(2) and (2) k 1. If = 2n and = 2n 1 for some [Nin], then we have ai,j S(1) i,j KN 1{xq = vk} (cid:104) (cid:110)(cid:16) k=1 (cid:110) (cid:88) 1 (1 (2) ) + 1 (2) i,x S(1) S(1) i,i1 + (S (cid:16) o=1 (cid:88) (2) )2 2S (2) + 1 S(1) i,y (cid:17) (cid:111)(cid:105) (cid:18) o=k (cid:88) 1 S(1) i,i1 (cid:17)(cid:16) log 2 + (cid:16) (cid:17) 1 α1 (cid:17)(cid:19)(cid:27) (cid:16) 1 3 . (cid:17) 2. If = 2n and = 2m 1 for some < Nin, then we have ai,j S(1) i,j KN 1{xq = vk} (S (2) )2 (2) k=1 (cid:110) (cid:88) (S (cid:16) o=1 (cid:88) (cid:104) (cid:110)(cid:16) (2) )2 2S (2) + 1 o=1 (cid:88) S(1) i,i1 (cid:18) (cid:17) (cid:111)(cid:105) (2) o=1 + 1 i,y + S(1) S(1) i,i1 (cid:80) (1S(1) i,j ) log(N ) 2 (cid:17)(cid:16) 1S(1) i,j KN α1 (cid:17) 1 3 . (cid:17) (cid:16) (cid:19)(cid:27) (cid:19) (cid:18) 3. If = 2n and = 2m for some Nin, then we have ai,j S(1) i,j KN E 1{xq = vk} (S (2) )2 (2) (2) o=1 + 1 i,x S(1) S(1) i,i1 k=1 (cid:110) (cid:88) (cid:104) (S (cid:16) o=1 (cid:88) (2) )2 2S (cid:110)(cid:16) (2) + 1 o=1 (cid:88) S(1) i,i1 (cid:17)(cid:16) (cid:17)(cid:111)(cid:105) (cid:18) (cid:80) S(1) i,x log(N ) 2 (cid:17)(cid:16) (cid:16) S(1) i,x KN α (cid:17)(cid:17) 1 3 (cid:16) . (cid:17) (cid:19)(cid:27) (cid:19) (cid:18) 4. If = 2n 1 and = 2m for some 1 Nin 1, then we have ai,j S(1) i,j KN 1{xq = vk} (cid:16) o=1 (cid:88) (S (2) )2 (2) S(1) i,x (cid:17)(cid:16) (cid:17)(cid:105) (cid:18) S(1) i,x log (cid:19) S(1) i,x KN α1 (cid:18) (cid:19)(cid:27) k=1 (cid:26) (cid:88) 1 3 (cid:104) . (cid:16) (cid:17) 59 5. If = 2n 1 and = 2m 1 for some Nin, then we have ai,j S(1) i,j KN 1{xq = vk} (cid:16) (cid:104) o=1 (cid:88) (S (2) )2 (2) S(1) i,y (cid:17) (cid:105) (cid:18) (1S(1) i,j ) log 1S(1) i,j α1 (cid:19) (cid:18) (cid:19)(cid:27) k=1(cid:26) (cid:88) 1 3 . (cid:17) Proof First, we notice that (cid:16) S(2) vo xq ww o vo 1{i 0 (mod 2)}x S(2) 2 S(1) S(2) i,j (I(i)j vo 1{i 0 (mod 2)}x S(2) (cid:17) S(1) S(2) i,j (I(i)j (cid:19) S(1) i,ℓ I(i)ℓ) (cid:105) ℓi (cid:88) S(1) i,ℓ I(i)ℓ) ℓi (cid:88) (cid:105) o=1 (cid:88) o=1 (cid:88) ai,j = (cid:104)(cid:16) (i) = (cid:104)(cid:16) (ii) (cid:17) S(2) vo xq o=1 (cid:88) (cid:16) (cid:17) S(2) vo xq (cid:18) o=1 (cid:88) o=1 (cid:104)(cid:16) (cid:88) + 2P(E c), (cid:16) (cid:17) o=1 (cid:88) vo 1{i 0 (mod 2)}x S(2) 2 S(1) S(2) i,j I(i)j S(1) i,ℓ I(i)ℓ (cid:17) (cid:16) ℓi (cid:88) 1{E S} (cid:105) (cid:17) where (i) follows from taking the conditional expectation over w, and (ii) follows from the fact that vo xq o=1 S(2) By Theorem F.6, and taking δ = 3, we can replace the random variables S(2) (cid:12) (cid:16)(cid:80) (cid:12) (cid:12) (cid:12) deterministic approximation, which enables simpler form of ai,j. Then we have vo 1{i 0 (mod 2)}x i,j (I(i)j S(1) S(2) o=1 S(2) ℓi S(1) (cid:16)(cid:80) (cid:80) (cid:17) (cid:17) 2 i,ℓ I(i)ℓ) 2. (cid:12) and S(2) (cid:12) (cid:12) (cid:12) with their ai,j (cid:20)(cid:18) (2) S o=1 (cid:16) (cid:88) 1 (2) min (cid:18)(cid:112) 1, i1 log(N ) vo xq (2) (cid:19)(cid:19) log(2N /δ) o=1(cid:16) (cid:88) (cid:19) (cid:18) S(1) i,j I(i)j log(N ) (cid:18)(cid:112) S(1) i,ℓ I(i)ℓ (cid:112) S (cid:16) (cid:9)(cid:1)(cid:1) ℓi (cid:88) (2) vo 1{i 0 (mod 2)}x 2 (cid:17)(cid:105) (cid:0) (i) (cid:8) (cid:0) (2) vo xq (cid:17) (cid:20)(cid:26)(cid:16) o=1 (cid:88) o=1 (cid:88) 11{i 0 (mod 2)} (cid:16) (cid:18)(cid:18) (cid:112) (cid:19)(cid:112) log(N ) log(N ) vo 1{i 0 (mod 2)}x 2 (cid:19) (cid:19)(cid:19) 1 3 (cid:16) (cid:17) (cid:17) I(i)j S(1) i,ℓ I(i)ℓ (cid:19)(cid:27)(cid:16) ℓi (cid:88) (cid:17)(cid:21) S(1) i,j 1 (cid:16) , (cid:17) (32) (2) = Θ(1/N ). Notice that the choice of 3 where (i) follows from that we only care about the order, and here ensures that it remains negligible term in the later analysis. Starting from (32), we consider by cases. We only prove Case (1), and the other cases follow by the same argument as Case (1). Case (1): = 2n, = 2n 1. Because = 2n is even, by direct calculation, we have ai,j = K (2) vo xq (cid:17) (cid:16) o=1 (cid:88) (2) vo xn (cid:17) (cid:16) log(N ) 2 I(i)j S(1) i,ℓ I(i)ℓ (cid:17)(cid:19)(cid:16) ℓi (cid:88) S(1) i,j (cid:17)(cid:21) (cid:20)(cid:18)(cid:16) o=1 (cid:88) 1 (cid:19) (cid:18) = k=1 (cid:88) 1{xq = vk} (cid:20) (cid:18) (S (2) )2 (2) o=1 (cid:88) o=1 (cid:88) 60 (2) 1{xn = vo} + 1{xn = vk} log(N ) 2 (cid:18) (cid:19)(cid:19) S(1) i,j (cid:19)(cid:21) O"
        },
        {
            "title": "1\nN 3",
            "content": "(cid:18) (cid:19) I(i)j S(1) i,ℓ I(i)ℓ (cid:18) S(1) i,j KN = k=1 (cid:88) ℓi (cid:88) (I) + O"
        },
        {
            "title": "1\nN 3",
            "content": "(cid:18) . (cid:19) (33) Here, the dominant term (I) is defined and further simplified as follows (S (2) )2 (2) (2) +1{k = k} (I) = k=1 (cid:88) 1{xq = vk} (cid:18) (cid:20) (i) 1{xq = vk} = (cid:20) k=1 (cid:18) (cid:88) o=1 (cid:88) o=1 (cid:88) (S (2) )2 (2) (2) log(N ) 2 (cid:18) I(i)j S(1) i,ℓ I(i)ℓ (cid:19)(cid:19)(cid:16) ℓi (cid:88) (cid:17)(cid:21)"
        },
        {
            "title": "1\nK",
            "content": "K (cid:19)(cid:18) S(1) i,ℓ 1{ℓ = 2r 1} (cid:19)(cid:21) ℓi2 (cid:88) (S (2) )2 2S (2) + 1 1 S(1) i,i1 (cid:18) o=1 (cid:88) (cid:19) (cid:0) (cid:19)(cid:21) (cid:1) (cid:19) (2) S i,x S(1) S(1) i,i1 (cid:19) o=k (cid:88) (cid:0) (1 S(1) i,i1) (cid:18) (cid:18) (cid:1) log 2 + 1 α1 , (cid:19)(cid:19) (34) 1{ℓ = 2r 1} +"
        },
        {
            "title": "1\nK",
            "content": "S(1) i,ℓ + 1{xq = vk} (cid:20) (cid:18) ℓi2 (cid:88) i,i1) log(N ) KN (cid:19) (1 S(1) (cid:18) 1{xq = vk} 1 (1 (cid:26)(cid:18) 1 S(1) i,i1 KN α1 (cid:18) (2) ) + 1 (S (2) )2 2S (2) + 1 S(1) i,y (cid:18) o=1 (cid:88) (cid:19) (cid:27)(cid:21) = (cid:20) + where (i) follows from Theorem F.5, which states that I(i)j 1{j = 2r 1, xr = vk} 5N 1α when xq = vk. It also uses the fact that {xr} i1 2 j=1, and taking partial expectation over {xr} i1 2 r=1 . is independent with {S o=1, {S(1) i,j }i1 (2) }K r=1 Plugging (34) into (33) proves the result of Case (1). The analyses for the other cases proceed analogously to Case (1), and we therefore omit the details. This concludes the proof of Theorem F.3. F.3 Stage I: Auxiliary Lemmas The next lemma characterizes the influence of the low-frequency components of RoPE on the tokens, which ). will be useful for simplifying I(i) = Ex,yRϑ,N i(Ex,y Lemma F.4. Suppose Assumption 5.1 holds. Let Ex,y, Ex,y token embeddings and Eq. Then, for any 1 , the partial token embedding Ex,y satisfies: denote the semantic dependent subspaces of the at position j 1. If is odd, and Ex,y 2. If is odd, and Ex,y has the same feature with Ex,y , i.e. Ex,y = Ex,y , then 1 Ex,y Rϑ,N i(Ex,y ) 2 2(α1) . has different feature with Ex,y , i.e. Ex,y Ex,y , then Ex,y Rϑ,N i(Ex,y ) (cid:12) (cid:12) (cid:12) (cid:12) 5 α1 . 3. If is even, then Ex,y Rϑ,N i(Ex,y ) = 0. Proof We prove by cases. Case (1). If is odd, and Ex,y has the same feature with Ex,y , i.e. Ex,y = Ex,y , then 1 Ex,y Rϑ,N i(Ex,y d/2 ) (Ex,y )2 2ℓ1 + (Ex,y )2 2ℓ (Ex,y )2 2ℓ1 + (Ex,y )2 2ℓ cos(θℓ(N i)) (cid:9) 1 2 1 (cid:9) (cid:110) (θℓ(N i))2 (cid:111) = 1 1 (cid:88)ℓ=dc/2+1 d/2 (cid:8) (cid:8) (cid:88)ℓ=dc/2+1 2 2(α1) 2 α1 , where the first inequality follows from that 1 x2 cos(x), and the second inequality follows from the that Ex,y 2 = 1. Case (2). If is odd, and Ex,y has different feature with Ex,y , i.e. Ex,y Ex,y , ) Ex,y Rϑ,N i(Ex,y (i) )Ex,y (Ex,y cos(θd/2(N i)) (cid:12) (cid:12) + + (cid:12) (cid:12) (cid:12) (cid:12) 2 (Ex,y )2 2ℓ1 + (Ex,y (cid:12) (cid:12) )2 2ℓ1 + (Ex,y )2 2ℓ + (Ex,y ℓ=1 (cid:110) (cid:88) 2 (Ex,y )2 2ℓ1 + (Ex,y 2ℓ1 + (Ex,y )2 )2 2ℓ + (Ex,y (θℓ θd/2)(N i) )2 2ℓ (cid:111)(cid:26) 2 α1 (cid:27)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:8) ℓ=1 (cid:88) 8 2(α1) + 4 α1 5 α1 , )2 2ℓ θℓ(N i) (cid:9) (cid:12) (cid:12) (cid:12) (cid:12) where (i) follows from adding and subtracting cos(θd/2(N i)), the inequality sin(x) and the fact that for 0 x1 x2 1, cos(x1) cos(x2) x1 x2 sin(x2). Case (3). If is even, then the sub-vector of Ex,y corresponding to is also zero, we immediately have Ex,y Ex,y we finish the proof of Theorem F.4. corresponding to is zero. Consider the sub-vector of ) = 0 for all is even. Hence, Rϑ,N i(Ex,y As direct result of Theorem F.4, we have the following corollary to control I(i). Corollary F.5 (Bounds of I(i)). Suppose Assumption 5.1 holds, for any 1 , I(i)j 1{j 1 (mod 2)}1{Ej = Eq} 5N 1α. In addition, conditioned on the event {xq = vk}, we have I(i)j 1{j 1 (mod 2)}1{x(j+1)/2 = vk} 5N 1α. We omit the proof here, as it follows directly from Theorem F.4. We next show concentration results of (2) S(2) and S(2), which is important in simplifying ai,j. We define their approximate expectation versions and , then control the approximation errors. (2) (cid:12) (cid:12) (cid:12) (cid:12) 62 Lemma F.6 (Concentration of S(2),S(2)). Suppose the prompt is randomly sampled according to the (2) data model in Section 5.1. Define 2i as the approximate expectation versions of S(2) and S(2). For any [N ], with probability at least 1 δ, we have j=1 exp (E[uj])) and (2) = exp (E[ui])/( (2) = 1 Nin i=1 N S(2) = (2) 1 min S(2) = (2) (cid:0) 1 (cid:0) log(2K/δ) (cid:8) (cid:112) , [K]. (cid:9)(cid:1)(cid:1) (cid:80) 1, i1 log(2N /δ) , [N ], (cid:80) S, we have P(E Furthermore, denote the event above by (cid:112) (cid:0) S) 1 δ. (cid:1) Proof We first prove the concentration result of S(2). Recall that from (23), we have S(2) = exp (ui) j=1 exp (uj) , ui = j=1 (cid:88) S(1) i,j Ex,y Rϑ,N i(Ex,y ). (cid:80) From the definition, S(1) is independent with prompt . Consequently, when is odd, the terms S(1) are conditionally independent with each other for different given Ex,y . Furthermore, for each i, the term S(1) i,j . Then for all [N ], following from the standard Hoeffding inequality techniques (Hoeffding, 1963; Mohri et al., 2018), with probability at least 1 δ/2, we have ) is bounded between S(1) Rϑ,N i(Ex,y i,j and S(1) i,j Ex,y i,j Ex,y Rϑ,N i(Ex,y ) ui E[ui] (cid:115) i,j ) j=1 4(S(1) 2i2 ji,j1 (mod 2) S(1) i,j (cid:80) log(2N /δ) 2 log(2N /δ) . (cid:112) (35) 5N 1α, and (35), with probability at least By Theorem F.5, which shows 1 δ/2, we have (cid:12) (cid:12) S(2) = E[ui] 1 (cid:80) exp (E[ui])(1 + (ui E[ui]) + o(ui E[ui])) j=1 exp (E[uj])(1 + (uj E[uj]) + o(uj E[uj])) (cid:12) (cid:12) = (2) (cid:80) 1 i1 log(2N /δ) , (36) In addition, by noticing that (2) (cid:16) and S(2) (cid:16) are in the same order, i.e., S(2) (cid:17)(cid:17) (cid:112) = Θ(S (2) ), we have S(2) = (2) 1 min 1, i1 log(2N /δ) . We next prove the concentration of S(2). With probability at least 1 δ, for any [K], we have (cid:9)(cid:1)(cid:1) (cid:8) (cid:0) (cid:0) (cid:112) Nin S(2) (i) = 1{xi = vm}S (2) 2i 1 log(2N /δ) (cid:18) (cid:18) (cid:112) log(2N /δ) i=1 (cid:88) 1 (ii) = i=1 (cid:88) (2) = Nin (2) 2i 1 (cid:18) (cid:18) (cid:112) log(2K/δ) (cid:18) (cid:112) 2i , (cid:19) 2i (cid:19)(cid:19) (cid:19)(cid:19) (cid:18) (cid:112) log (2K/δ) (cid:19) where (i) follows from the definitions of S(2) inequality and that pk = 1/K. Hence, we finish the proof of Theorem F.6. (2) and (36), and (ii) follows from the standard Hoeffding , Because 1 uj 1 for all [N ], by the definition of (2) and via simple calculations based on Theorem F.6, which provides the order of (2) , we obtain the following corollary . and (2) (2) Corollary F.7. Through Stage I, for all [N ] and [K], we have (2) = Θ (1/N ) and (2) = Θ (1/K). In the next section, we will analyze the training dynamics of Stage I. F.4 Stage I: Phase In this section, we shall study the initial phase of learning the relationship between any position and its previous tokens. We define the Phase as all iterations 0 (1) , where 1 (1) 1 max : min i[N ] Ai,i1(t) max ji,j=i1 Ai,j(t) log(N ) . (cid:110) (cid:16) (cid:17) (cid:111) We then state the following induction hypothesis, which holds throughout Phase I. This hypothesis is proved by induction together with the technical lemmas in Section F.4.1. Induction Hypothesis F.1. For each 0 (1) 1 and any [N ], the following holds: 1. Ai,i1(t)maxji,j=i1 Ai,j(t) is monotonically increasing and for all [N ], 0 Ai,i1(t)maxji,j=i1 Ai,j(t) log 1 + 1log . 2. maxji,j=i1 Ai,j(t) minli,l=i1 Ai,l(t) (cid:1)(cid:1) (cid:0) (cid:0) 1(log )2 . Proof We first prove Claim (1), as Ai,j(0) = 0 for all , we only need to prove that Ai,i1(t 1) Ai,j(t 1) for all i, = 1. By Theorem F.10, we have (cid:1) (cid:0) Ai,i1(t) max ji,j=i1 Ai,j(t) η1C1 KN , which is larger than 0. As result, Ai,i1(t) max ji,j=i1 Ai,j(t) t1 τ =0 (cid:88) Ai,i1(τ ) min ji,j=i1 Ai,j(τ ) = 1 + K 1log τ =0 (cid:88) (cid:0) 1 + (cid:0) 1log (cid:0) 1 + (cid:0) 1log (cid:1)(cid:1) (cid:1)(cid:1) min l[N ] (cid:18) log N, min l[N ] Al,l1(τ ) max rl,r=l1 Al,r(τ ) (cid:18) (cid:19) Al,l1(t) max rl,r=l1 Al,r(t) (cid:19) (cid:1)(cid:1) where the second inequality follows from that for each i, Ai,i1(t)minji,j=i1 Ai,j(t) = mini[N ]{Ai,i1(t) maxji,j=i1 Ai,j(t)}(1+O(log N/K)) as in Theorem F.11, and the last inequality follows from the definition of Phase I. Hence, we prove Claim (1). (cid:0) (cid:0) Then we proceed to prove Claim (2) as follows. t1 max ji,j=i1 Ai,j(t) min li,l=i Ai,l(t) (i) max ji,j=i1 Ai,j(τ ) min li,l=i1 Ai,l(τ ) (cid:19) Ai,i1(τ ) max ji,j=i1 Ai,j(τ ) (cid:19) (cid:17)(cid:18) Ai,i1(t) max ji,j=i1 Ai,j(t) (cid:19) τ =0 (cid:18) (cid:88) t1 log (cid:16) log (cid:17)(cid:18) (log )2 τ =0 (cid:88) (cid:16) (ii) = (cid:16) , (cid:17) where (i) follows from Theorem F.11, and (ii) follows from Claim (1). Hence, we finish the proof of Induction Hypothesis F.1. F.4.1 Technical Lemmas We first introduce several useful technical lemmas, which characterize the important values including S(1), S(2), ai,j, across phase I. By induction, for all these lemmas, we only need to show that (1) for = 0, the lemmas hold, and (2) if we assume that for S(1)(t 1), S(2)(t 1), A(t 1), ai,j(t 1), A(t 1), the lemmas hold, then they still hold for S(1)(t), S(2)(t), A(t), ai,j(t), A(t). Lemma F.8. For iteration 0 (1) and F.10 hold at iteration 1, then 1 , if Induction Hypothesis F.1 holds at iteration t, and Theorems F.9 1. For any [N ], we have S(1) i,i1(t) = Ω (1/i) and S(1) i,j (t) = (1/i) for = 1. 2. For any [N ], i,i1(t) Ω(i/N ). i,x (t) S(1) i,y (t) Ω(i/N ). (a) S(1) (b) S(1) (c) 1 S(1) i,i1(t) Ω(i/N ). Proof We first prove Claim (1). For any [N ] S(1) i,i1(t) (i) = 1 1 + (i 1) exp (maxji,j=i1 Ai,j(t) Ai,i1(t)) 1 1 + (i 1) exp (maxji,j=i1 Ai,j(0) Ai,i1(0)) 1 , where (i) follows from that 1/(1 + (i 1)ex) is monotonically decreasing with and Induction Hypothesis F.1. Furthermore, we have that S(1) i,j (t) (i) 1 + li,l=j exp ( (maxji,j=i1 Ai,j(t) minli,l=i1 Ai,l(t))) 1 (ii) (cid:80) 1 1 + (i 1) exp (O (K 1(log )2)) = 1 1 + (i 1)Ω(1) = , 1 (cid:16) (cid:17) where (i) follows from that 1/(1 + ex) decrease with of x, and Induction Hypothesis F.1 that Ai,i1(t) maxji,j=i1 Ai,j(t) 0, and (ii) follows from Induction Hypothesis F.1. Hence, we have proved Claim (1). We then proceed to prove Claim (2) as follows. 1 S(1) i,i1(t) (i) (ii) ji,j=i1 exp (minji,j=i1 Ai,j(t) Ai,i1(t)) 1 + (cid:80) 1 + (cid:80) ji,j=i1 exp (cid:80) ji,j=i1 exp (minji,j=i1 Ai,j(t) Ai,i1(t)) exp 1log ji,j=i1 exp ( log(N ) (1 + (K 1log ))) exp (O (K 1(log )2)) (cid:1)(cid:1) 1(log )2 log(N ) 1 + (cid:1)(cid:1)(cid:1) (cid:0) (cid:0) (cid:0) (cid:0) (cid:0) (cid:80) 65 Ω 1 , (cid:17) (cid:16) where (i) follows from that ex/(1 + ex) increase with of x, and (ii) follows from Theorem F.11 and Induction Hypothesis F.1. Similarly, we can prove Hence, we finish the proof of Theorem F.8. S(1) i,y (t) Ω 1 , S(1) i,x (t) S(1) i,i1(t) Ω (cid:16) (cid:17) 1 . (cid:17) (cid:16) In the following lemma, we control the order of ai,j(t). Lemma F.9 (Order of ai,j(t)). During stage I, suppose Induction Hypothesis F.1 and Theorem F.8 hold at iteration 0 (1) . For different cases of i, j, we have the following results. 1. For = 2n, = 2n 1, ai,i1(t) (KN 2)1. 2. For = 2n, = 2m 1, n, (iKN )1 1 + i1 ai,j(t) (iKN )1 1 i1 . 3. For = 2n, = 2m, n, (iKN )1 1 + i1 (cid:0) ai,j(t) (iKN )1 (cid:1) 1 (cid:0) . (cid:1) 4. For = 2n 1, = 2m, 1, ai,j(t) (iK 2N )1. (cid:1) (cid:0) (cid:0) (cid:1) 5. For = 2n 1, = 2m 1, n, ai,j(t) (iK 2N )1. Proof We only need to apply Theorem F.3 and evaluate all ai,j(t)s order for the timestep given S(1)(t), S(2)(t), S(2)(t), ai,j(t 1) by cases. Case (1): = 2n, = 2n 1. We have ai,i1(t) (i) 1 iKN (cid:40) k=1 (cid:88) log 2 1{xq = vk} (cid:20) (cid:19) (cid:18) 1 α1 (cid:18)(cid:18) (cid:19)(cid:27) 1 1 2K 1 + 1 4K (cid:18) 1 (cid:19) +1 (cid:19) 1 (cid:19)(cid:21) 1 1 (cid:19)(cid:18) 1 (cid:18) (cid:19) 1 (cid:18) KN 2 > 0, where (i) follows from expression of ai,j(t) in Theorem F.3, that S(1) (2) = Θ (1/N ) and (2) = Θ (1/K) as in Theorem F.7. Hence, we prove Claim (1). i,i1(t) Ω(1/i) as in Theorem F.8, and Similar to Case (1), following Theorems F.3, F.7 and F.8, we can compute for the other cases and finish the proof of Theorem F.9. As result of Theorem F.9, for any [N ], we can characterize the relative increments gap of across different tokens as follows. Lemma F.10. Suppose Induction Hypothesis F.1, Theorems F.8 and F.9 hold at iteration 0 (1) any [N ], we have the following results. 1 , for 1. Upper Bound: Al,l1(t) maxrl,r=l1 Al,r(t) η1C1/(KN ). 2. Lower Bound: maxjl,j=l1 Al,j(t) minrl,r=l1 Al,r(t) η1C1 log(N )/(N 2). Proof From Theorem F.2, the increment of Al,r(t) can be decomposed as follows Al,r(t) = η1 C1 ai,i+rl(t) + ai,j(t) + (ai,j(t)Θ(ϵFN)) , (cid:18) i=lr+1 (cid:88) 1 l,r(t) 1jiN (cid:88) 2 l,r(t) 1jiN (cid:88) 3 l,r(t) (cid:19) which has three parts: 1 we only need to discuss the first part l,r(t) and the error 3 (cid:125) l,r(t). The second part 2 l,r(t) and the third part 3 l,r(t). (cid:123)(cid:122) (cid:124) (cid:124) (cid:124) l,r(t), 2 (cid:123)(cid:122) (cid:125) (cid:123)(cid:122) (cid:125) l,r(t) is independent with r, so First Part. We discuss 1 For Case (1): = 1 and 1 l,r(t) by cases, l,l1(t), we have Nin Nin 1 l,l1(t) = a2n,2n1(t) + a2m+1,2m(t) n=1 (cid:88) Nin (i) C1 (cid:32) C1 KN n=1 (cid:88) > 0,"
        },
        {
            "title": "1\nKN 2 −",
            "content": "m=1 (cid:88) Nin 1 (2m + 1)K 2N (cid:33) (37) m=1 (cid:88) where (i) follows from Theorem F.9. direct result from the equation above is that for any l, 1 same, and is lower bounded by the same order Ω(C1/(KN )). l,l(t), we have Similar to Case (1), for Case (2) = and 1 l,l1(t) is the Nin Nin+1 1 l,l(t) = a2n,2n(t) + a2m1,2m1(t) (cid:16) n=1 (cid:88) m=1 (cid:88) (cid:17) C1 log 2N . (38) For Case (3), 2 and 0 (mod 2), we have 1 l,r(t) = C1 a2n,2n+rl(t) + a2m+1,2m+1+rl(t) Nin Nin (cid:16) (cid:88)n=(lr+1)/2 (cid:88)m=(lr+1)/2 (cid:17) C1 log 2K 2N . (39) For Case (4), 2 and 0 (mod 2), we have 1 l,r(t) = C1 a2n,2n+rl(t) + a2m+1,2m+1+rl(t) Nin Nin (cid:16) (cid:88)n=(lr+1)/2 (cid:88)m=(lr+1)/2 (cid:17) C1 log 2K 2N . (40) Combine (37) to (40), we have 1 l,l1(t) max rl,r=l1 l,r(t) C1 1 KN . (41) Third Part. Then for 3 l,r(t), and any [N ], we have 3 l,l1(t) max rl,r=l1 3 l,r(t) O (ϵFN) ai,in(t) (i) (ϵFN) n=0,n=1 (cid:88) i=n+1 (cid:88) n=0,n=1 (cid:88) (cid:18) 67 log 2N (cid:19) ϵFN log , (cid:19) (cid:18) where (i) follows from Theorem F.9. Finally, combine (41) and (42), we have Al,l1(t) max rl,r=l1 Al,r(t) η1C1 KN η1ϵFN log 2 η1C1 KN , where the last equation follows from Assumption 5.1 that ϵFN Ω(C1/N ). Hence, we prove Claim (1). Then we prove Claim (2). Similar to Claim (1), combine (41) and (42), we have max jl,j=l1 Al,j(t) min rl,r=l1 Al,r(t) η1 log K 2 C1 (cid:18) + ϵFN (cid:19) η1C1 log . Hence, we finish the proof of Theorem F.10. direct corollary of Theorem F.10 is as follows. Corollary F.11. Under the same conditions of Theorem F.10, for any [N ], we have (42) maxjl,j=l1 Al,j(t) minrl,r=l1 Al,r(t) minl[N ]{Al,l1(t) maxrl,r=l1 Al,r(t)} Al,l1(t) minrl,r=l1 Al,r(t) minl[N ]{Al,l1(t) maxrl,r=l1 Al,r(t)} Al,l1(t) maxrl,r=l1 Al,r(t) minl[N ]{Al,l1(t) maxrl,r=l1 Al,r(t)} = log , (cid:18) = 1 + = 1 + (cid:19) log log (cid:18) (cid:18) , . (cid:19) (cid:19) F.4.2 End of Phase Lemma F.12. With (1) 1 at most 1 1 η1 1 KN log , for any [N ], at iteration = (1) 1 + 1, we have 1. Ai,i1(T (1) 1 + 1) maxji,j=i1 Ai,j(T (1) (cid:0) 1 + 1) log(N ), (cid:1) 2. S(1) i,i1(T (1) 1 + 1) = Ω(1). Proof Claim (1) holds because of the definition of Phase I. Then, we only need to show the upper bound of (1) 1 . As Induction Hypothesis F.1 holds, there exists an for (1) such that 1 log(N ) Ai,i1(T (1) ) max ji,j=i1 Ai,j(T (1) 1 ) Ai,i1(τ ) max ji,j=i1 (1) 1 1 τ =0 (cid:88) (1) 1 η1C1 KN , (i) Ai,j(τ ) (43) where (i) follows from Theorem F.10. As result, we must have (1) inequality would fail to hold. Hence, we prove Claim (1). 1 = We then prove Claim (2), for all [N ], 1 1 η1 1 KN log , otherwise the (cid:0) (cid:1) i,i1(T (1) S(1) 1 + 1) = exp (Ai,i1(T (1) 1 + 1)) exp (Ai,i1(T (1) 1 + 1)) + ji,j=i1 exp (Ai,j(T (1) 1 + 1)) (cid:80) 68 (i) 1 + (i 1) exp (maxji,j=i1 Ai,j(T (1) 1 + 1) Ai,i1(T (1) 1 + 1)) 1 1 1 + (i 1)e log(N ) = Ω(1), where (i) follows from Claim (1) of this lemma. Hence, we finish the proof of this lemma. F.5 Stage I: Phase II In this section, we study the rapid growth phase of learning the relationship between any position and its previous tokens. We define the Phase II as all iterations (1) , where 1 + 1 (1) (1) 2 max (1) 1 : min i[N ] Ai,i1(t) max ji,j=i1 Ai,j(t) log(KN ) . (cid:110) (cid:16) (cid:17) (cid:111) Then, we state the following induction hypothesis, which holds throughout Phase II. This hypothesis is proved by induction with the technical lemmas in Section F.5.1. Induction Hypothesis F.2. For each iteration (1) 1 + 1 (1) and any [N ], the followings hold: 1. Ai,i1(t)maxji,j=i1 Ai,j(t) is monotonically increasing and Ai,i1(t)maxji,j=i1 Ai,j(t) log(N ), 1 + 1log log (KN ) . 2. maxji,j=i1 Ai,j(t) minli,l=i1 Ai,l(t) 1(log )2 . (cid:2) (cid:0) (cid:0) (cid:1)(cid:1) (cid:3) Proof We first prove Claim (1), the monotonicity can be proved similarly to Induction Hypothesis F.1. As in Theorem F.15, (cid:1) (cid:0) Ai,i1(t) max ji,j=i1 Ai,j(t) η1C1 2 0. By direct calculation, we have Ai,i1(t) max ji,j=i1 Ai,j(t) Ai,i1(T (1) 1 ) max ji,j=i Ai,j(T (1) 1 ) (i) 1 + 1K 1 (cid:0) (cid:88)τ =T (1) 1 + (cid:0) 1K min l[N ] (cid:16) min l[N ] Al,l1(τ ) max rl,r=l1 (cid:16) (cid:1)(cid:1) Al,l1(t) Al,l1(T (1) ) max rl,r=l1 (cid:17) Al,r(τ ) (cid:17) (cid:0) (cid:1)(cid:1) (cid:0) (ii) (1 + O(N 1K)) (cid:16) min l[N ] Al,l1(t) max Al,r(t) rl,r=l1 min l[N ] (cid:110) (cid:16) (cid:17) ) 1 Al,r(t) Al,r(T (1) (cid:16) Al,l1(T (1) (cid:16) ) max rl,r=l1 (cid:17) (cid:17) Al,r(T (1) 1 ) , (44) (cid:17)(cid:111) where (i) follows from Theorem F.16, and (ii) follows from max(A B) max + max B, and min(A B) min min B. As result of (44), we have Ai,i1(t) max ji,j=i1 Ai,j(t) (i) 1 + 1K + 1log min l[N ] Al,l1(t) max rl,r=l1 Al,r(t) (45) (cid:0) 1 + (cid:0) 1log (cid:1)(cid:1) log (KN ) , (cid:16) (cid:17) (cid:1)(cid:1) where (i) follows from Induction Hypothesis F.1, and that Al,l1(t) maxrl,r=l1 Al,r(t) is monotonically increasing. Hence, we prove Claim (1). (cid:0) (cid:0) We next prove Claim (2), max ji,j=i1 Ai,j(t) min li,l=i1 Ai,l(t) max ji,j=i Ai,j(T (1) 1 ) min li,l=i1 Ai,l(T (1) ) t1 1K (cid:88)τ =T (1) 1 (cid:0) 1K = (cid:0) (cid:1)(cid:16) (cid:16) (cid:17) Ai,i1(τ ) max ji,j=i1 Ai,j(τ ) (cid:1) (cid:16) Ai,i1(t) max ji,j=i1 Ai,j(t) (cid:17) Ai,i1(T (1) (cid:16) 1 ) max ji,j=i1 Ai,j(T (1) 1 ) , (cid:17)(cid:17) where the first inequality follows from Theorem F.16. By direct calculation, we have max ji,j=i1 Ai,j(t) min li,l=i1 Ai,l(t) (i) K 1(log )2 + 1K 1 + 1log log (KN ) (cid:0) 1(log )2 (cid:1) , (cid:0) (cid:1)(cid:0) (cid:0) (cid:1)(cid:1) where (i) follows from Induction Hypothesis F.1. Hence, we finish the proof of Induction Hypothesis F.2. (cid:1) (cid:0) F.5.1 Technical Lemmas Similar to phase I, we next introduce several useful technical lemmas. The proofs follow arguments analogous to those in Phase and are omitted for brevity. Lemma F.13. For iteration (1) Theorems F.14 and F.15 hold at iteration 1, then the followings hold: 1 + 1 (1) 2 , if Induction Hypothesis F.2 holds at iteration t, and 1. For any [N ], (a) S(1) i,i1(t) = Ω (1), (b) Ω (1/(KN )) S(1) i,j (t) (1/N ) , = 1. 2. For any [N ], (a) S(1) (b) S(1) i,x (t) S(1) i,y (t) Ω(i/(2KN )), i,i1(t) Ω(i/(KN )), (c) 1 S(1) i,i1(t) Ω(i/(2KN )). Similar to Theorem F.9, and following from the expression of ai,j(t) in Theorem F.3, control of attention scores in Theorems F.7 and F.13, we can control the order of ai,j(t) as follows, Lemma F.14 (Order of ai,j(t)). Suppose Induction Hypothesis F.2 and Theorem F.13 hold at iteration 1 + 1 (1) (1) 1. = 2n, = 2n 1, ai,i1(t) iK 2N 2. . For different cases of i, j, we have the following results. 2 2. = 2n, = 2m 1, n, 1N 2 ai,j(t) 2N 2. 3. = 2n, = 2m, n, 1N 2 ai,j(t) 2N 2. 4. = 2n 1, = 2m, 1, ai,j(t) 2N 2. 5. = 2n 1, = 2m 1, n, ai,j(t) 2N 2. 70 As result of Theorem F.14, for any [N ], we can characterize the relative increments gap of across different tokens as follows. Lemma F.15. Suppose Induction Hypothesis F.2 and Theorems F.13 and F.14 hold at iteration (1) (1) 1. Upper Bound: Al,l1(t) maxrl,r=l1 Al,r(t) η1C1K 2. 2. Lower Bound: maxjl,j=l1 Al,j(t) minrl,r=l1 Al,r(t) η1C1(KN )1. . For any [N ], we have the following results. 2 1 + 1 As direct corollary Theorem F.15, we can bound the increment differences ratios as follows. Corollary F.16. Under the same condition of Theorem F.15, for any [N ], we have maxjl,j=l1 Al,j(t) minrl,r=l1 Al,r(t) minl[N ]{Al,l1(t) maxrl,r=l1 Al,r(t)} Al,l1(t) minrl,r=l1 Al,r(t) minl[N ]{Al,l1(t) maxrl,r=l1 Al,r(t)} Al,l1(t) maxrl,r=l1 Al,r(t) minl[N ]{Al,l1(t) maxrl,r=l1 Al,r(t)} We omit the proof here, as it follows directly from Theorem F.15."
        },
        {
            "title": "K\nN",
            "content": "= (cid:16) = 1 + = 1 + ,"
        },
        {
            "title": "K\nN",
            "content": "(cid:17) (cid:16) (cid:16) , . (cid:17) (cid:17) F.5.2 End of Phase II Lemma F.17. With (1) we have 1. Ai,i1(T (1) (cid:0) 2 + 1) maxji,j=i1 Ai,j(T (1) (cid:1) 2 + 1) log(KN ), 2 (1) at most 1 1 η1 1 2 log , and at iteration = (1) 2 + 1, for any [N ], 2. 1 S(1) i,i1(T (1) 2 + 1) = O(1/K). Proof Claim (1) holds from the definition of Phase II. We only need to show the upper bound of (1) Induction Hypothesis F.2 holds, there exists an for (1) such that 2 . As log(KN ) Ai,i1(T (1) 2 ) max ji,j=i1 Ai,j(T (1) 2 ) Ai,i1(τ ) max ji,j=i1 Ai,j(τ ) + Ai,i1(T (1) 1 + 1) max ji,j=i1 Ai,j(T (1) 1 + 1) (1) 2 1 1 +1 (cid:88)τ =T (1) (i) (T (1) 2 (1) 1 1) η1C1 2 + log(N ), where (i) follows from Theorem F.15 and Induction Hypothesis F.1. By direct calculation, we have (1) . Hence, we prove Claim (1). 1 2 log η1 1 1 We then prove Claim (2), for all [N ], we have 2 (1) 1 = (cid:0) (cid:1) 1 S(1) i,i1(T (1) 2 + 1) = (i) ji,j=i1 exp (Ai,j(T (1) 2 + 1) Ai,i1(T (1) 2 + 1)) ji,j=i1 exp (Ai,j(T (1) 1 + (cid:80) (i 1) exp ( log(KN )) 1 + (i 1) exp ( log(KN )) 2 + 1) Ai,i1(T (1) KN (cid:80) , 2 + 1)) where (i) follows from that x/(1 + x) increase with and maxji,j=i1 Ai,j(T (1) log(KN ). Hence, we finish the proof of this lemma. (cid:16) (cid:17) 2 + 1) Ai,i1(T (1) 2 + 1) 71 F.6 Stage I: Phase III In this section, we study the convergence phase. For ϵ1 = O(N 1 2 + 1 (1) (1) , where 3 2 ), we define the Phase III as all iterations (1) 3 max (1) 2 : min i[N ] Ai,i1(t) max ji,j=i1 Ai,j(t) log ϵ1 1 . We then state the following induction hypothesis, which holds throughout Phase III. This hypothesis is proved by induction with the technical lemmas in Section F.6.1. (cid:110) (cid:16) (cid:17) (cid:0) (cid:1) (cid:111) Induction Hypothesis F.3. For each (1) 2 + 1 (1) 3 and any [N ], the following holds: 1. Ai,i1(t)maxji,j=i1 Ai,j(t) is monotonically increasing and Ai,i1(t)maxji,j=i1 Ai,j(t) log(N ), 1 + O((KN ϵ1)1 + 1 log ) log 1 ϵ1 , 2. maxji,j=i1 Ai,j(t) minli,l=i1 Ai,l(t) K 1(log )2 + (KN ϵ1)1log 1 ϵ1 . (cid:2) (cid:0) (cid:1) (cid:0) (cid:1)(cid:3) Proof We first prove Claim (1), and the monotonicity can be proved similarly to Induction Hypothesis F.2. Following from Theorem F.20, we have (cid:0) (cid:1)(cid:1) (cid:0) Ai,i1(t) max ji,j=i1 Ai,j(t) η1C1ϵ1 0. Furthermore, following from Theorem F.21, and following the same steps as in (44), except for the change in coefficients, we obtain Ai,i1(t) max ji,j=i1 Ai,j(t) Ai,i1(T (1) 2 ) max ji,j=i1 Ai,j(T (1) 2 ) 1 + min l[N ] (cid:0) Al,l1(t) max Al,r(t) rl,r=l1 min l[N ] (cid:1) Al,l1(T (1) 2 ) max rl,r=l1 Al,r(T (1) 2 ) . (cid:17)(cid:111) (cid:16) (cid:16) By direct calculation, we have (cid:17)(cid:17)(cid:110) (cid:16) Ai,i1(t) max ji,j=i1 Ai,j(t) (i) 1 + (cid:16) 1 + (cid:16) 1 KN ϵ1 1 KN ϵ + (cid:17) + (cid:16) log log (cid:17) (cid:16) min l[N ] Al,l1(t) max rl,r=l1 Al,r(t) (cid:17)(cid:17) log (cid:16) ϵ1 1 , (cid:17) (cid:17)(cid:17) where (i) follows from Induction Hypothesis F.2 and Eq. (45), and that Al,l1(t) maxrl,r=l1 Al,r(t) is monotonically increasing with t. Hence, we prove Claim (1). (cid:16) (cid:17) (cid:16) (cid:16) (cid:1) (cid:0) We next prove Claim (2), max ji,j=i1 Ai,j(t) min li,l=i1 Ai,l(t) max ji,j=i Ai,j(T (1) 2 ) min li,l=i1 Ai,l(T (1) ) (cid:16) (cid:17) max ji,j=i1 Ai,j(τ ) min li,l=i Ai,l(τ ) (cid:17) t1 2 (cid:16) (cid:88)τ =T (1) t1 (cid:88)τ =T (1) 2 (cid:16) 1 KN ϵ = (cid:16) 1 KN ϵ1 (cid:17)(cid:16) Ai,i1(τ ) max ji,j=i Ai,j(τ ) (cid:17) Ai,i1(t) max ji,j=i1 Ai,j(t) Ai,i1(T (1) 2 ) max ji,j=i1 Ai,j(T (1) 2 ) , (46) (cid:17)(cid:16) (cid:16) 72 (cid:17)(cid:17) where the last inequality follows from Theorem F.21. By direct calculation, we have max ji,j=i1 Ai,j(t) min li,l=i1 Ai,l(t) (log )2 (log )2 (i) (cid:16) (cid:16)"
        },
        {
            "title": "1\nKN ϵ1\n−1)",
            "content": "+ (cid:17) + (cid:16) log(N ϵ1 KN ϵ1 1 + O"
        },
        {
            "title": "1\nKN ϵ1",
            "content": "(cid:16) + (cid:17) (cid:16) log log ϵ1 1 (cid:17)(cid:17) (cid:0) (cid:1) (cid:17)(cid:16) , (cid:17) where (i) follows from Induction Hypothesis F.2 and Eq. (46). Hence, we finish the proof of Induction Hypothesis F.3. F.6.1 Technical Lemmas Similar to Phases and II, we next introduce several useful technical lemmas. The proofs follow arguments analogous to those in Phase and are omitted for brevity. Lemma F.18. Suppose Induction Hypothesis F.3 holds at iteration t, and Theorems F.19 and F.20 hold at iteration 1. For iteration (1) , we have the following results. 2 + 1 (1) 3 1. For any [N ], (a) S(1) i,i1(t) = Ω (1), (b) Ω (ϵ1/N ) S(1) i,j (t) (1/(KN )) , = 1. 2. For any [N ], (a) S(1) (b) S(1) i,x (t) S(1) i,y (t) Ω(iϵ1/N ), i,i1(t) Ω(iϵ1/N ), (c) 1 S(1) i,i1(t) Ω(iϵ1/N ). Similar to Theorems F.9 and F.14, and following from the expression of ai,j(t) in Theorem F.3, control of attention scores in Theorems F.7 and F.18, we can control the order of ai,j(t). Lemma F.19 (Order of ai,j(t)). Suppose Induction Hypothesis F.3 and Theorem F.18 hold at iteration 2 + 1 (1) (1) 1. For = 2n, = 2n 1, ai,i1(t) iϵ1/(KN 2). . For different cases of i, j, we have the following results. 3 2. For = 2n, = 2m 1, n, 1/(K 2N 2) ai,j(t) ϵ1/(KN 2). 3. For = 2n, = 2m, n, 1/(K 2N 2) ai,j(t) ϵ1/(KN 2). 4. For = 2n 1, = 2m, 1, ai,j(t) 1/(K 3N 2). 5. For = 2n 1, = 2m 1, n, ai,j(t) 1/(K 3N 2). As result of Theorem F.19, for any [N ], we can characterize the relative increments gap of across different tokens as follows. Lemma F.20. Suppose Induction Hypothesis F.3 and Theorems F.18 and F.19 hold at iteration (1) (1) , for any [N ], the following holds 2 + 1 3 73 1. Upper Bound: Al,l1(t) maxrl,r=l1 Al,r(t) 1η1C1ϵ1. 2. Lower Bound: maxjl,j=l1 Al,j(t) minrl,r=l1 Al,r(t) 2N 1η1C1 log N. As direct corollary of Theorem F.20, we can bound the increment differences ratios as follows. Corollary F.21. Under the same condition of Theorem F.20, for any [N ], we have maxjl,j=l1 Al,j(t) minrl,r=l1 Al,r(t) minl[N ]{Al,l1(t) maxrl,r=l1 Al,r(t)} Al,l1(t) minrl,r=l1 Al,r(t) minl[N ]{Al,l1(t) maxrl,r=l1 Al,r(t)} Al,l1(t) maxrl,r=l1 Al,r(t) minl[N ]{Al,l1(t) maxrl,r=l1 Al,r(t)}"
        },
        {
            "title": "1\nKN ϵ1",
            "content": ", = (cid:16) = 1 + (cid:17) 1 KN ϵ1 1 KN ϵ1 , . (cid:17) (cid:17) (cid:16) (cid:16) = 1 + We omit the proof here, as it follows directly from Theorem F.20. F.6.2 End of Phase III Lemma F.22. With (1) any [N ], we have 3 (1) 2 1 1 η1 1 ϵ1 1 log(K 1ϵ1 1 ) at most (cid:0) , and at iteration = (1) 3 + 1, for (cid:1) 1. Ai,i1(T (1) 3 + 1) maxji,j=i1 Ai,j(t) log(N ϵ1 1 ), 2. 1 S(1) i,i1(T (1) 3 + 1) ϵ1. Proof Claim (1) holds from the definition of Phase III, so we only need to show the upper bound of (1) As Induction Hypothesis F.3 holds, there exists an for (1) such that, 3 3 . log ϵ 1 Ai,i1(T (1) 3 ) max ji,j=i1 Ai,j(T (1) 3 ) (cid:0) (1) 3 1 (cid:1) Ai,i1(τ ) max ji,j=i1 Ai,j(τ ) + Ai,i1(T (1) 2 + 1) max ji,j=i Ai,j(T (1) 2 + 1) 2 +1 (cid:88)τ =T (1) (i) (T (1) 3 (1) 2 1) η1C1ϵ1 + log(KN ), where (i) follows from Theorem F.20 and Induction Hypothesis F.2. As result, (1) 3 (1) 2 = 1 1 η 1 ϵ1 1 log(K 1ϵ1 1 ) . We then prove Claim (2), for all [N ], we have (cid:0) (cid:1) 1 S(1) i,i1(T (1) 3 + 1) = (i) ji,j=i1 exp (Ai,j(T (1) 3 + 1) Ai,i1(T (1) 3 + 1)) (cid:80) ji,j=i1 exp (Ai,j(T (1) 1 + (cid:80) (i 1) exp (maxji,j=i1 Ai,j(T (1) 1 + (i 1) exp (maxji,j=i1 Ai,j(T (1) (i 1)e log(N ϵ1 1 ) 1 + (i 1)e log(N ϵ1 1 ) 3 + 1) Ai,i1(T (1) 3 + 1)) 3 + 1) Ai,i1(T (1) 3 + 1)) 3 + 1) Ai,i1(T (1) 3 + 1)) ϵ1, where (i) follows from that x/(1 + x) increase with and maxji,j=i1 Ai,j(T (1) log(N ϵ1 1 ). Hence, we finish the proof of this lemma. 2 + 1) Ai,i1(T (1) 2 + 1) 74 F.7 Proof of Stage of Theorem 5.3 Proof As direct result of Theorems F.12, F.17 and F.22, with at most τ1 = (1) 3 + 1 KN log η1C1 KN log η1C1 = = (cid:16) (cid:16) + 2 log η1C1 (cid:17) + (cid:16) (cid:17) log(K 1ϵ1 1 ) η1C1ϵ1 log(K 1ϵ1 1 ) η1C1ϵ (cid:17) + (cid:16) , (cid:17) we have 1 S(1) i,i1(τ1) ϵ1. Hence, we finish the proof. Proof of Stage II of Theorem 5.3 In Stage II, (1) is fixed as (1) (τ1). As result, S(1) i,j is fixed as S(1) i,j (τ1). We omit τ1 for simplicity. We also omit (t) and write L( (cid:102) (cid:102) θ) as here when there is no ambiguity. G.1 Roadmap of the Proof (cid:101) We analyze the feature matching in stage II via two phases of dynamics (Sections G.4 and G.5). In each phase, we formulate an induction hypothesis and derive several lemmas describing the values and evolution of key statistics, which collectively govern the attention scores. We prove the induction hypothesis and the lemmas by induction. We begin by introducing the key statistic that is tracked throughout the different phases. Notice that after stage training, Layer 1 attention scores almost concentrate on the immediate prefix, i.e., for each l, 1 S(1) l,l1(t) ϵ1. Under this condition, and by Theorem G.2, we observe that Layer 2 attention logit ui(t) depends strongly on the features associated with xq and xi. Since the frequencies are sufficiently small, given fixed xq, both ui(t) and its update take nearly the same value across all positions where xi share the same feature vm, [K]. Consequently, we denote this common value of ui(t) by Bm(t) and that of its update by bm(t) for feature vm. Both quantities are random, as they depend on the random variable xq. The main idea of the proof lies in analyzing the GD dynamics of Bm(t), which decides Layer 2 attention (t). Note (t). By tracking Bm(t), we obtain the attention scores on the features, S(2) scores S(2) that when xq = vk, achieving good prediction only requires that S(2) (t) and S(2) (t) 1. For any [K], conditioned on xq = vk, the learning process can be divided into two phases. Phase I: Growth (t [τ1 + 1, τ1 + (2) 1,k ], Section G.4). During phase I, Bk(t) keeps growing at rate of bk(t) = Ω(ηK 2), while for = k, Bk(t) oscillates with smaller rate of bk(t), which satisfying bk(t) = O(K 1bk(t)). Therefore, the increase in Bk(t) will dominate the learning dynamics during phase I. The attention score S(2) (t) keeps increasing, while still satisfying 1 S(2) (t) = Ω(1). 1,k + 1, τ1 + (2) Phase II: Convergence (t (τ1 + (2) 2,k ], Section G.5). After the rapid growth of self-attention module parameters in phase I, the question token featuring vk is aligned with these input tokens also featuring vk effectively and disregards other features. Then the process proceeds to the convergence phase, where Bk(t) monotonically increases and for = k, B(t) monotonically decreases, which finally contributes to the convergence of the loss. As result, Bk(t) keeps increasing and for = k, Bk(t) keeps deceasing. After the end of phase II, Bk(T (2) 2 ), and 1 S(2) 1,k + 1) log(Kϵ1 1,k + 1) (ϵ2/2) 1 2 . (T (2) 75 We summarize the upcoming sections as follows: In Section G.2, we compute and simplify the gradients to identify the key update variables. In Section G.3, we introduce several useful auxiliary lemmas for Stage II. In Sections G.4 and G.5, we analyze the two phases of the dynamics. G.2 Stage II: Preliminary Development Computations of Gradients. We first calculate Stage II gradient with respect to (2) . Lemma G.1 (Layer 2 Gradient). In Stage II, the gradient of the loss function with respect to by (cid:102) (2) is given (cid:102)W (2) = yq w, xq) ( yℓS(2) 2ℓ (cid:104) (cid:88)ℓ[Nin] (cid:98) (1{2ℓ = i} S(2) ) Rϑ,N S(1) i,i1(Ex,y i1) + S(1) i,j (Ex,y ) Ex,y (cid:102) , where Ex,y respectively. and Ex,y i=1 (cid:88) correspond to the semantic dependent subspaces of the token embeddings Ej and Eq, j=i1 (cid:88) (cid:17) (cid:17) (cid:16) (cid:16) (cid:105) Proof We first write out the expression of yq. In Stage II, (1) is fixed, then yq(Wq(τ1), (cid:98) (2) ) = (cid:102) S(2) 2i yi = S(2) w, vk. ConsiderS(2) = softmax(u) R1N , and (cid:98) (cid:102) i=1 (cid:88) k=1 (cid:88) ui = Ex,y (2) Rϑ,N i(S(1)Ex,y) i,: = Ex,y (2) (cid:102) (t)Rϑ,N S(1) i,i1(Ex,y i1) + S(1) i,j (Ex,y ) , (47) (cid:16) j=i1 (cid:88) i,: (t)Ex,y = S(1) j=i1 S(1) i,j Ex,y j,: . (cid:17) i,i1Ex,y i1,: + (cid:102) where the last inequality follows from that (S(1)Ex,y)i,: = S(1) Then we can calculate the gradient. We first obtain: (cid:102)W (2) = E[( yq w, xq) (cid:102)W (2) yq] = yq w, xq) ( ( (cid:98) (cid:104) (cid:98) (cid:98) (cid:88)ℓ[Nin] Recall that for any R1d, softmax(U ) = diag(softmax(U )) softmax(U )softmax(U ), we have (cid:80) S(2) 2ℓ )yℓ . (cid:102)W (2) (cid:105) (48) (cid:102)W (2) S(2) 2ℓ = i=1 (cid:88) = S(2) 2ℓ S(2) 2ℓ ui (cid:102)W (2) ui (1{2ℓ = i} S(2) ) Rϑ,N S(1) i,i1(Ex,y i1) + S(1) i,j (Ex,y ) Ex,y i=1 (cid:88) (cid:16) (cid:16) j=i1 (cid:88) (cid:17) (cid:17) Plug (49) into (48), we finish the proof of Theorem G.1. . (49) We find that for stage II, ui(t) is important in determining the attention scores. As result, we track the update of ui(t) as follows. 76 Lemma G.2 (Track variable update). In Stage II, the attention logit at position can be expressed as ui(t) = Ex,y (2) (t)Rϑ,N S(1) i,i1(t)(Ex,y i1) + S(1) i,j (t)(Ex,y ) . Conditioned on the event that xq = vk, we can characterize the increments of ui(t) over for even and upper bound ui(t) for odd as follows. (cid:102) (cid:16) j=i1 (cid:88) (cid:17) 1. For any [K], if is even and xi/2 = vm, we define the following variables conditioned on the features, Bi,m(t) = ui(t), where the subscript specifies the particular feature vm taken by xi/2. Then for each [K], the update of Bi,m(t) denoted by bi,m(t) = Bi,m(t + 1) Bi,m(t) can be characterized as follows. (a) If = k, then bi,k(t) = η2E (cid:104) 1{xq = vk}S(2) (t) (cid:16) (S(2) (t) 1)2 + (S(2) (t))2 m=k (cid:88) (cid:17)(cid:105) (cid:16) η2ϵ1dX . (cid:17) (b) For = k, we have bi,m(t) = η2E (cid:104) 1{xq = vk}S(2) (t) (cid:16) (cid:88)m[K] (S(2) (t))2 S(2) (t) S(2) (t) (cid:17)(cid:105) (cid:16) η2ϵ1dX . (cid:17) Since the dependence on is negligible from RHS above, we omit the position subscripts in Bi,m(t) and bi,m(t) when no ambiguity arises. 2. If is odd, then ui(t) is always relatively small and can be controlled Proof We first prove Claim (1), where is even, and 1 is odd. S(1)(t), S(2)(t) and S(2)(t) for abbreviation. From (47), we have In the following, we omit (t) of ui(t) ϵ1Θ max m[K] Bm(t) . (cid:16) (cid:17) ui(t + 1) ui(t) = η2Ex,y (cid:102)W (2) L(t)Rϑ,N S(1) i,i1(Ex,y i1) + S(1) i,j (Ex,y ) (cid:16) j=i1 (cid:88) (cid:17) yℓS(2) 2ℓ (1{2ℓ = m} S(2) ) (i) = η2Ex,y yq w, xq) ( (Ex,y ) (cid:16) (cid:16) (cid:104) (cid:98) S(1) m,m1Ex,y m1 + (cid:88)ℓ[Nin] j=m1 (cid:88) (ii) = η2E (cid:104) Ex,y (cid:98) 1{xq = vk} η2ϵ1E (cid:104) (cid:16) (cid:1) (cid:0) (iii) 1{xq = vk}( = η2E (cid:104) m=1 (cid:88) S(1) m,jEx,y Rϑ,mi S(1) i,i1(Ex,y i1) + S(1) i,j (Ex,y ) 1{xq = vk} ( yq w, xq) yℓS(2) 2ℓ (1{2ℓ = m}S(2) ) (cid:17) (cid:17)(cid:105)(cid:16) (cid:17) j=i1 (cid:88) Ex,y m1Rϑ,mi (cid:0) (cid:1) (cid:105) m=1 (cid:88)ℓ[Nin] (cid:88) yq w, xq) ( yℓS(2) 2ℓ (cid:12) (cid:12) (cid:12) (cid:98) (cid:88)ℓ[Nin] (cid:105)(cid:17) (cid:12) (cid:12) (cid:12) yq w, xq) yℓS(2) 2ℓ S(2) 2ℓ1Rϑ,2ℓi Ex,y Ex,y m1Rϑ,mi (cid:98) (cid:88)ℓ[Nin] m=1 (cid:88) (cid:0) 77 (cid:1)(cid:105) Ex,y i1 K 1η2ϵ1dX , (50) (cid:0) (cid:1) (cid:0) where (i) follows from Theorem G.1 and the property of RoPE operator, (ii) follows from Ex,y and the property that 1 S(1) follows from that w2 ) = 1, i,i1(τ1) ϵ1 for all [N ] at the end of Stage I, as in Theorem F.22, and (iii) dX and vk2 = 1 for all [K]. (Ex,y (cid:1) For the dominant term (first term) in (50), conditioned on the event xq = vk, we can further divide it into two terms and discuss by cases. Case (a): If the corresponding xi/2 of Ex,y i1 is vk, then with yq w, xq = w( ℓ[Nin] S(2) 2ℓ xℓ xq), we have the first term as 1{xq = vk} ( (cid:104) yq w, xq) yℓS(2) 2ℓ (cid:98) Ex,y S(2) 2ℓ1Rϑ,2ℓi Ex,y i1 (cid:80) (cid:88)ℓ[Nin] S(2) 2r xr xq m=1 (cid:88) (cid:105) (cid:0) xℓS(2) 2ℓ (xℓ) Rϑ,2ℓi (cid:1) vk (cid:105) (cid:98) (i) = 1{xq = vk} (cid:104) (ii) 1{xq = vk} = = (cid:104) 1{xq = vk} (cid:104) (cid:88)ℓ[Nin] (cid:17) (cid:16) (cid:88)r[Nin] S(2) 2r xr xq (cid:17) S(2) 1 (cid:16) (cid:88)r[Nin] S(2) (cid:88)ℓ[Nin] (S(2) )2O (cid:0) α+1 (cid:0) (cid:0) m=k (cid:88) (cid:1) (cid:0) xℓS(2) 2ℓ 1{xℓ = vk} 1{xℓ = vk}O α+ (cid:0) (cid:1)(cid:1)(cid:105) (51) , (cid:1) (cid:1)(cid:105) where (i) follows from taking the conditional expectation over and that the last 2-dimensional subspace of embedding of the input is null space, and (ii) follows similarly from Theorem F.4. And the second term is yq w, xq) 1{xq = vk} ( (cid:104) (i) = (cid:98) 1{xq = vk} yℓS(2) 2ℓ S(2) Ex,y m1Rϑ,mi (cid:88)ℓ[Nin] S(2) 2r xr xq m=1 (cid:88) (cid:0) xℓS(2) 2ℓ Ex,y i1 (cid:1) (cid:105) (cid:0) (cid:1) S(2) Ex,y m1Rϑ,mi Ex,y (cid:104) (ii) = (cid:16) (cid:88)r[Nin] (cid:17) (S(2) )2 S(2) (cid:16) (cid:88)ℓ[Nin] S(2) (1 Ω (cid:17) m=1 (cid:88) α+1 (cid:0) ) (cid:1) (cid:105) (cid:0) O(N α+1) S(2) (cid:1) , (52) 1{xq = vk} (cid:104) (cid:16) (cid:88)m[K] (cid:17)(cid:16) (cid:0) (cid:1) m=k (cid:88) (cid:17)(cid:105) where (i) follows from taking the conditional expectation over and that the last 2-dimensional subspace of embedding of the input is null space, and (ii) follows from the fact that the summation over ℓ and are independent of the summation over m, and Theorem F.4. Sum (51) and (52), we get the dominant term in (50) as follow η2E (cid:104) S(2) 1{xq = vk} (cid:16) (cid:16) (S(2) 1)2 (S(2) )2 N α+1 . m=k (cid:88) (cid:17) (cid:0) (cid:1) (cid:17)(cid:105) Plugging into (50), we have bi,k(t) = η2E (cid:104) 1{xq = vk}S(2) (cid:16) (S(2) 1)2 + (S(2) )2 m=k (cid:88) (cid:17)(cid:105) η2ϵ1dX . (cid:19) (cid:18) Case (b): If the corresponding xi/2 of Ex,y i1 is vm, = k, similar to Case (a), we can get bi,m(t) = η2E (cid:104) 1{xq = vk}S(2) + S(2) S(2) (S(2) )2 (cid:16) (cid:88)m[K] (cid:17)(cid:105) η2ϵ1dX . (cid:19) (cid:18) 78 We note that with the low frequency small enough, the dependence of the increments of ui(t) on the position is dominated and can be controlled by minor term. Since the dependence on is negligible, we omit the position subscripts when no ambiguity arises. Hence we prove Claim (1). We next prove Claim (2), where is odd, and 1 is even. Then, Ex,y (2) Rϑ,N iS(1) i,i1(Ex,y i1) = 0, as result, (cid:102) ) (cid:17) ui(t) = Ex,y (2) (t)Rϑ,N S(1) i,j (Ex,y (cid:102) (i) ϵ1 max m[K] j=i1 (cid:16) (cid:88) (t)Rϑ,N ivm (2) (ii) ϵ1Θ (cid:102) Bm(t) max m[K] , (cid:16) (cid:17) where (i) follows from the property that i,i1(τ1) ϵ1 for all [N ] at the end of Stage I, as in Theorem F.22, and that the last 2-dimensional subspace of embedding of the input is null (cid:80) space, and (ii) follows from the definition of Bm(t). Hence, we prove Theorem G.2. j=i1 S(1) i,j (τ1) = 1 S(1) G.3 Stage II: Auxiliary Lemmas Recall that given prompt = (x1, y1, . . . , xNin, yNin , xq), we denote Pin as the collection of input tokens in the example, i.e., {xi}Nin i=1. Similar to Huang et al. (2024), it is worth noting that, based on our data distribution, the occurrence count of the k-th feature in Pin, denoted as Vk, follows multinomial distribution. Leveraging the concentration property inherent to multinomial distributions, we can identify high-probability event to which Pin belongs. As = 2Nin + 1, we can use instead of Nin in the following lemma. Lemma G.3 (High-probability Event for Pin). Suppose that pk = Θ (1/K) for any [K] and 3 . For some constant 20K 3/N , define (cid:112) := Pin : Vk pkN (cid:110) (cid:104) cN , pkN + cN (cid:105) for [K] . (cid:111) Then , we have P(Pin P) 1 3 exp c2N 25K 2 . (cid:19) (cid:18) Let us denote Lk = pkK and Uk = pkK + c. Note that Lk, Uk are at the order of the constant level since pk = Θ (1/K). Then for any Pin belonging to P, Vk [LkN /K, UkN /K] = Θ(N/K). Note that we can properly choose to guarantee Lk > 0 for [K]. G.4 Stage II: Phase In this section, we study the initial phase of learning the relationship between the question token Eq and its previous tokens with different features. We define the Phase as all iterations τ1 + 1 τ1 + (2) 1,k , where (2) 1,k max {t : Bk(τ1 + t) log K} . Then, we state the following induction hypothesis, which holds throughout Phase I. This hypothesis is proved by induction with the technical lemmas in Section G.4.1. 79 Induction Hypothesis G.1. Suppose Theorems G.4 and G.5 and Induction Hypothesis G.1 hold at iteration 1, where τ1 + 1 τ1 + (2) 1,k . Given any [K], conditioned on the events that xq = vk and Pin P, the following holds for iteration t: 1. Bk(t) is monotonically increasing, and Bk(t) [(1 Ω(ϵ1 + 1α)), log(K)]. 2. For any = k, Bk(t) = (Bk(t)/K) = O((log K)/K). 3. Specially, for the initialization of Stage II, = τ1 + 1, Bk(τ1 + 1) 1 Ω(ϵ1 + 1α), 1 + O(ϵ1) , Bk(τ1 + 1) (ϵ1). Proof We first prove Claim (3). By the definition of Bm in Theorem G.2, we only need to consider odd positions. For any [Nin], by definition of u2i(τ1 + 1) and (τ1 + 1) = Id, we have (2) (cid:2) (cid:3) u2i(τ1 + 1) = Ex,y Rϑ,N 2i S(1) 2i,2i1(Ex,y (cid:102) 2i1) + S(1) 2i,j(Ex,y ) . (cid:0) j=i (cid:88) (cid:1) Then applying Theorem G.2, together with xq = xi = vk and 1 S(1) i,i1(τ1 + 1) ϵ1 for all Nin, we have B2i,k(τ1 + 1)= S(1) i,i1(1 Ω(N 1α)) O(ϵ1) (1 Ω(ϵ1 + 1α)), 1 + O(ϵ1) . Hence, Bk(τ1 + 1) (1 Ω(ϵ1 + 1α)), 1 + O(ϵ1) (cid:0) . And for = k, we have (cid:1) (cid:2) (cid:3) (cid:2) B2i,k(τ1 + 1) = u2i(τ + 1){xi = vk} = (ϵ1). (cid:3) Hence Bk(τ1 + 1) = (ϵ1), and we finish the proof of Claim (3). We next prove Claim (1). For any τ1 + 1 τ1 + (2) 1,k , we first need to show bk(t) 0, which is obvious from Theorem G.5. The lower bound follows from Claim (3), and the upper bound follows from the definition of Phase I. We finally prove Claim (2). By Theorem G.5, we have for any = k, bk(t 1) (bk(t 1)/K) at time 1. Then, Bk(t) Bk(t 1) + bk(t 1) Bk(t 1) + (cid:19) (cid:18) bk(t 1) (cid:19) (cid:18) Bk(t) . (cid:19) (cid:18) Hence, we finish the proof of Induction Hypothesis G.1. G.4.1 Technical Lemmas Similar to stage I, we next introduce several useful technical lemmas and prove them by induction. Lemma G.4. Suppose Induction Hypothesis G.1 holds at iteration τ1 + 1 τ1 + (2) the events xq = vk and Pin P, the following holds for iteration t. 1,k . Conditioned on 1. For the attention scores related to feature k, (a) S(2) (b) 1 S(2) (t) = Ω(1/K), (t) = Ω(1). 2. For the attention scores related to feature = k, S(2) (t) = Θ (1 S(2) (t))/K = Θ(1/K). (cid:0) (cid:1) 80 Proof We first prove Claim (1)-(a). S(2) (t) = = (i) i=2n,n[Nin] exp (ui(t))1{xi = vk} m[K] i=2n,n[Nin] exp (ui(t))1{xi = vm} + (cid:80) i=2n1,n[Nin+1] exp (ui(t)) (cid:80) (cid:80) Vk exp (Bk(t)) + Vk exp (Bk(t)) m=k Vm exp (Bm(t)) + (cid:80) i=2n1,n[Nin+1] exp (ui(t)) (cid:80) 1 (cid:80) 1 + m=k(Vm/Vk) exp (Bm(t) Bk(t)) + (Nin/Vk) exp ((1 Θ(ϵ1))Bk(t)) (53) (ii) (cid:80) 1 1 + m=k(Vm/Vk) exp (Bm(t) Bk(t)) + Nin/Vk , (cid:80) where (i) follows from Theorem G.2, which gives ui(t) ϵ1Θ(maxm[K] Bm(t)) for odd i, together with the fact that maxm=k Bm(t) Bk(t) in Induction Hypothesis G.1, and (ii) follows from that Bk(t)(1 Θ(ϵ1)) 0. Furthermore, by Induction Hypothesis G.1, for any = exp ( log O((log K)/K)) exp(B(t) ) exp (O((log K)/K)). B(t) By direct calculation and Theorem G.3, we have S(2) (t) 1 eO((log K)/K)(Nin/Vk 1) + 1 + Nin/Vk 1 eO((log K)/K)(K/Lk 1) + 1 + K/Lk Hence, we prove Claim (1)-(a) and next prove Claim (1)-(b). Similar to Claim (1)-(a), we have S(2) (t) 1 + m=k(Vm/Vk) exp (Bm(t) Bk(t)) 1 1 e1(1/Uk 1/K) + 1 . Together with Uk = Θ(1), we have (cid:80) 1 S(t) (1/Uk 1/K) (1/Uk 1/K) + Ω(1). Hence, we prove Claim (1)-(b). Finally, we prove Claim (2). For any = k, we have S(2) (t) = Vk exp (Bk(t)) + Vk exp (Bk(t)) m=k Vm exp (Bm(t)) + i=2n1,n[Nin+1] exp (ui(t)) . = Ω 1 . (cid:17) (cid:16) Because exp (ui(t)) 0 for all i, we have S(2) (t) 1 S(2) 1 + (t) (cid:80) (cid:80) 1 m=k,k(Vm/Vk) exp (Bm(t) Bk(t)) 1 , (cid:19) (cid:18) where the last inequality follows from Theorem G.3 and Um/Lk = Θ(1). For another direction, we have (cid:80) S(2) (t) 1 S(2) (t) (i) (ii) 1 1 + m=k,k(Vm/Vk) exp (Bm(t) Bk(t)) + (Nin/Vk) exp (ϵ1Θ (Bk(t)) Bk(t)) (cid:80) 1 1 + m=k,k(Vm/Vk)eO((log K)/K) + (Nin/Vk)eϵ1 log(K)+O((log K)/K) (iii) (cid:80) 1 + ( 1 m=k,k Ume/Lk) + Ke/Lk Ω 1 , (cid:19) (cid:18) (54) where (i) follows from Theorem G.2, (ii) follows from Induction Hypothesis G.1, and (iii) follows from Theorem G.3. Hence, we finish the proof of Theorem G.4. (cid:80) In the following lemma, we control the order of bk(t), [K]. 81 Lemma G.5 (Order of updates b(t)). Suppose Induction Hypothesis G.1 and Theorem G.4 hold at iteration τ1 + 1 τ1 + (2) 1,k , under the same assumption of Theorem G.4, we have 1. For k, bk(t) 0 and bk(t) = Ω η2/K 2 . 2. For = k, bk(t) O(bk(t)/K). (cid:0) (cid:1) Proof We first prove Claim (1). By Theorem G.2, we only need to consider the dominated term of bk(t) as follows: η2E 1{xq = vk} S(2) (S(2) 1)2 + (S(2) )2 (cid:2) η2pkP(Pin (cid:0) P) (cid:0) m=k (cid:88) S(2) 1 (cid:16) (cid:17) 2 S(2) (cid:2) (cid:1)(cid:1)(cid:3) {xq = vk} {Pin P} + 3η2pkP(Pin / P) (cid:3) (i) Ω η2 2 , (cid:16) (cid:17) where (i) follows from Theorems G.3 and G.4. Hence, we prove Claim (1). We next prove Claim (2). By Theorem G.2, for any = k, we denote the dominated term of bk(t) as follows (I) = η2E (cid:104) 1{xq = vk}S(2) (cid:16) (cid:88)m[K] On one hand, we have (S(2) )2 (S(2) + S(2) ) . (cid:17)(cid:105) (I) η2pkP(Pin P)E S(2) max m=k,k S(2) {xq = vk} {Pin P} + η2pkP(Pin / P) (cid:2) (cid:0) (cid:1) (cid:3) , (55) (i) η2 3 (cid:16) (cid:17) where (i) follows from Theorem G.3, and Theorem G.4 and 3. On the other hand, we have (I) (i) η2 2pk P(Pin / 1 S(2) (t) P) + pk P(Pin P) 1 S(2) (t) Θ (cid:8) Θ (cid:17)(cid:16) (cid:16) (cid:17) (cid:104) (ii) (cid:16) bk(t) (cid:16) , (cid:17) + S(2) (1 S(2) (t)) {xq = vk} {Pin P} (cid:17)(cid:12) (cid:12) (cid:12) (cid:105)(cid:111) (56) where (i) follows from Theorem G.4, (ii) follows from Theorem G.3, Claim (1), which states bk(t) = Ω and the fact that η2/K 2 exp and this lemma. η2/K 2 . As result, combining (55) and (56), we prove Claim (2) (cid:1) c2N /(25K 2) (cid:0) (cid:0) (cid:1) G.4.2 End of Phase Lemma G.6. Conditioned on the events that xq = vk and Pin and at iteration = τ1 + (2) 1,k + 1, we have P, with (2) 1,k at most (K 2 log K)/η2 , (cid:0) (cid:1) 1. Bk(τ1 + (2) 1,k + 1) log(K), 2. S(2) (τ1 + (2) 1,k + 1) = Ω(1). 82 Proof Claim (1) holds from the definition of Phase I. We only need to show the upper bound of (2) direct calculation, we have 1,k . By log(K) Bk(τ1 + (2) 1,k ) τ1+T (2) 1,k 1 Bk(τ1 + 1) + Bk(t + 1) Bk(t) 1 Ω(ϵ1 + 1α) + (T (2) 1,k 2)Ω(η2/K 2), t=τ1+1 (cid:88) where the last inequality follows from Theorem G.5 and Induction Hypothesis G.1. As result, (2) . We next prove Claim (2). Similar to (53) and by Bk(τ1 + (2) 1,k 1,k + 1) log(K), we have (K 2 log K)/η2 (cid:0) (cid:1) (τ1 + (2) S(2) 1,k + 1) 1 1 + m=k Vm(VkK) + Nin(VkK) (i) Θ(1), (cid:80) where (i) follows from the fact Vk = Θ(N/K) in Theorem G.3. Hence, we prove Claim (2). G.5 Stage II: Phase II We define the Phase II as all iterations τ1 + (2) 1,k + 1 τ1 + (2) 2,k , where (2) 2,k max : Bk(τ1 + t) max m=k Bm(τ1 + t) log (cid:110) Kϵ 2 2 . (cid:0) (cid:1)(cid:111) We then state the following induction hypothesis, which holds throughout Phase II. This hypothesis is proved by induction with the technical lemmas in Section G.5.1. Induction Hypothesis G.2. Suppose Theorems G.7 and G.8 and Induction Hypothesis G.1 hold at iteration 1, where τ1 + (2) 2,k . Given any [K], conditioned on the event that xq = vk and Pin 1,k + 1 τ1 + (2) P, the following holds for iteration t: 1. Bk(t) is monotonically increasing and Bk(t) [log(K), O(log(K/ϵ2))]. 2. For any = k, B(t) is monotonically decreasing and Bk(t) = O(Bk(t)/K). Proof We first prove Claim (2). By Theorem G.8, we have for any = k, bk(t 1) (bk(t 1)/K) at time 1, then Bk(t) Bk(t 1) + bk(t 1) (Bk(t)/K) . Hence, we prove Claim (2). We next prove Claim (1). For any τ1 + (2) 2,k , we first need to show bk(t) 0, which is obvious from Theorem G.8. Then, by the monotonicity and Theorem G.6, we have log(K) Bk(t). In addition, we have 1,k + 1 τ1 + (2) (1 (1/K)) Bk(t) Bk(t) max m=k Bm(t) log(Kϵ 2 2 ) (log(K/ϵ2)) , where the first inequality follows from Claim (2), and the second inequality follows from the definition of Phase II. Hence, we prove Claim (1). 83 G.5.1 Technical Lemmas Similar to phase I, we next introduce several useful technical lemmas and prove them by induction. Lemma G.7. Suppose Induction Hypothesis G.2 holds at iteration τ1 + (2) on the events that xq = vk and Pin P, the following holds for iteration t: 1,k + 1 τ1 + (2) 2,k , conditioned 1. For the attention scores related to feature k, we have that (t) = Ω(1), (a) S(2) (b) (1 S(2) (t))2 Ω(ϵ2). 2. For the attention scores related to feature = k, S(2) (t) = Θ 1 S(2) (t) /K . Proof We first prove Claim (1)-(a). Similar to Theorem G.4, we have (cid:0)(cid:0) (cid:1) (cid:1) S(2) (t) (i) m[K](Vm/Vk) exp (Bm(t) Bk(t)) + (Nin/Vk) exp ϵ1Θ maxm[K] Bm(t) Bk(t) 1 1 (cid:80) 1 + (Nin/Vk 1) exp (maxm=k Bm(t) Bk(t)) + (Nin/Vk) exp (Bk(t)(1 Θ(ϵ1))) (cid:0) (cid:0) (cid:1) (cid:1) (ii) 1 1 + (K/Lk) (ϵ1/2 2 /K) + Θ(ϵ1)/Lk = Ω(1), where (i) follows from the fact maxm=k Bm(t) Bk(t) in Induction Hypothesis G.2, and (ii) follows from the bound of Bk(t) in Induction Hypothesis G.2 and Theorem G.3. Hence, we prove Claim (1)-(a). Similarly, for Claim (1)-(b), we have 1 S(2) (t) = m=k Vm exp (Bm(t)) + i=2n1,n[Nin+1] exp (ui(t)) Vk exp (Bk(t)) + (cid:80) m=k Vm exp (Bm(t)) + (cid:80) i=2n1,n[Nin+1] exp (ui(t)) (i) (ii) (cid:80) m=k(Vm/Vk) exp (Bm(t) Bk(t)) (cid:80) 1 + m=k(Vm/Vk) exp (Bm(t) Bk(t)) + (N/Vk) exp (Bk(t)(1 Θ(ϵ1))) exp (maxm=k Bm(t) Bk(t) B(t))(K/Uk 1) 1+exp (maxm=k Bm(t)Bk(t)B(t))(K/Uk 1)+(K/Uk) exp (Bk(t)(1 Θ(ϵ1))) (cid:80) (cid:80) (iii) 1ϵ1/2 exp (O(K 1 log(K/ϵ2))) (K/Uk 1) 2 exp O(K 1 log(K/ϵ2)))(K/Uk 1) + Θ(ϵ1)/Lk 1 + (K 1ϵ1/2 2 Ω 1 2 2 ϵ , (cid:16) (cid:17) where (i) follows from Theorem G.2 and Induction Hypothesis G.2, (ii) follows by defining B(t) = maxm=k Bm(t) minm=k Bm(t), and (iii) follows from Induction Hypothesis G.2 and the fact that B(t) = (Bk(t)/K) = . Hence, we prove Claim (1)-(b). 1 log(K/ϵ2) We next prove Claim (2). For any = k, we have (cid:0) (cid:1) S(2) (t) = Vk exp (Bk(t)) + Vk exp (Bk(t)) m=k Vm exp (Bm(t)) + i=2n1,n[Nin+1] exp (ui(t)) . Then, similar to proof of Claim (2) in Theorem G.4, we have (cid:80) (cid:80) S(2) (t) 1 S(2) (t) = Vk exp (Bk(t)) + Vk exp (Bk(t)) m=k,k Vm exp (Bm(t)) + 1 (cid:80) i=2n1,n[Nin+1] exp (ui(t)) (cid:80) 1 + m=k,k(Vm/Vk) exp (Bm(t) Bk(t)) (cid:80) 84 1 + m=k,k(Um/Lk) exp (O(K 1 log(K/ϵ2))) = O"
        },
        {
            "title": "1\nK",
            "content": ", (cid:19) (cid:18) where the last inequality follows from the fact Um/Lk = Θ(1) in Theorem G.3 and Induction Hypothesis G.2. (cid:80) Similar to (54), we have S(2) (t) 1 S(2) (t) (i) 1 + m=k,k(Vm/Vk) exp (Bm(t) Bk(t)) + (Nin/Vk) exp (ϵ1Θ (Bk(t)) Bk(t)) 1 (cid:80) 1 1 + m=k,k Ume/Lk + Ke/Lk Ω"
        },
        {
            "title": "1\nK",
            "content": ", (cid:19) (cid:18) where (i) follows from Theorem G.2 and Induction Hypothesis G.2. Hence, we prove Theorem G.7. (cid:80) In the following lemma, we control the order of bk(t), [K]. Lemma G.8 (Order of updates b(t)). Suppose Induction Hypothesis G.2 and Theorem G.7 hold at iteration τ1 + (2) 2,k , under the same assumption of Theorem G.7, we have that 1,k + 1 τ1 + (2) 1. For k, bk(t) 0 and bk(t) = Ω(η2ϵ2/K), 2. For = k, bk(t) 0, and bk(t) O(bk(t)/K). Proof We first prove Claim (1), by Theorem G.2, and ϵ2 ϵ1, we only need to consider the dominated term of bk(t) as follows: S(2) 1{xq = vk} (cid:16) η2E (cid:104) η2pkP(Pin (i) η2ϵ2 Ω , (cid:16) (cid:17) (S(2) 1)2 + (S(2) )2 (cid:16) P) m=k (cid:88) S(2) 2 S(2) (cid:2) (cid:0) (cid:1) (cid:17)(cid:17)(cid:105) {xq = vk} {Pin P} + 3η2pkP(Pin / P) (cid:3) where (i) follows from Theorems G.3 and G.7. Hence, we prove Claim (1). We next prove Claim (2). By Theorem G.2, for any = k, denote the dominated term of bk(t) as follows (I) = η2E (cid:104) S(2) 1{xq = vk} (cid:16) (cid:88)m[K] On one hand, we have (S(2) )2 (S(2) + S(2) ) . (I) η2E (cid:104) (i) η2 (cid:16) (ii) Ω pkE (cid:104) η2ϵ2 (cid:16) (cid:17) 1{xq = vk} {Pin P}S(2) (S(2) )2 S(2) (1 S(2) ) )2 (1 S(2) 0, (cid:16) (cid:88)m /k,k {xq = vk} {Pin P} (cid:12) (cid:105) (cid:12) (cid:12) + 3pk exp (cid:16) (cid:17)(cid:105) c2N 25K 2 (cid:17)(cid:17) (57) where (i) follows from Theorem G.3 and the fact that S(2) (ii) follows from the fact (1 S(2) On the other hand, we have 1 S(2) (t) (t))2 Ω(ϵ2) in Theorem G.4 and 3. (t) = Θ (cid:0)(cid:0) /K in Theorem G.7, and (cid:1) (cid:1) (cid:17)(cid:105) + η2pkP(Pin / P) (I) (i) η2 (cid:8) 2pk P(Pin / P) + pk P(Pin P) 85 1 S(2) (t) Θ 1 S(2) (t) (cid:16) S(2) (t)(1 S(2) (t))2 (cid:17)(cid:16) Θ (cid:104) (ii) η2 (iii) (cid:16) pkE (cid:104) bk(t) (cid:110) (cid:16) (cid:16) , (cid:17) + S(2) (1 S(2) (t)) {xq = vk} {Pin P} (cid:17) (cid:17) {xq = vk} {Pin P} (cid:105) (cid:17)(cid:12) (cid:12) (cid:12) + 6pk exp (cid:16) (cid:105)(cid:111) c2N 25K 2 (cid:17)(cid:111) (58) where (i) follows from Theorem G.7, (ii) follows from Theorem G.3, and (iii) follows from Claim (1). Finally, combining (57) and (58), we prove Claim (2). G.5.2 End of Phase II Lemma G.9. Conditioned on the events that xq = vk and Pin and at iteration = τ1 + (2) 2,k + 1, we have that P, with (2) 2,k (2) 1,k at most O(η1 2 ϵ1 2 log(Kϵ 2 2 )), 1. Bk(τ1 + (2) 2,k + 1) maxm=k Bm(τ1 + (2) 2,k + 1) log 2. 1 S(2) (τ1 + (2) 2,k + 1) ϵ 1 2 2 . Kϵ 2 2 , (cid:1) (cid:0) Proof Claim (1) holds from the definition of Phase II. We only need to show the upper bound of (2) Induction Hypothesis G.2 and Theorem G.8, for any τ1 + (2) 1,k + 1 τ1 + (2) 2,k , we have 2,k . By Bk(t + 1) max m=k Bm(t + 1) Bk(t) max m=k Bm(t) (1 O(1/K))bk(t) = Ω(η2ϵ2/K). (cid:0) By direct calculation, we have (cid:1) (cid:0) (cid:1) Bk(τ1 + (2) 2,k ) max m=k Bm(τ1 + (2) 2,k ) Bk(τ1 + (2) 1,k + 1) max m=k Bm(τ1 + (2) 1,k + 1) (cid:0) τ1+T (2) 2,k 1 (cid:1) (cid:0) (cid:1) = Bk(t + 1) max m=k Bm(t + 1) Bk(t) max m=k Bm(t) (cid:1) (cid:0) (cid:1) (cid:88)t=τ1+T (2) 1,k +1 (cid:0) 2,k (2) (2) 1,k 1 Ω η2ϵ2/K . Dividing both side by Ω (cid:0) η2ϵ2/K (cid:1) (cid:0) , we have (cid:1) (cid:0) (cid:1) 2,k (2) (2) 1,k (cid:18) log(Kϵ 1 η2ϵ 2 2 ) . (cid:19) Hence, we prove Claim (1). Then we prove Claim (2). For abbreviation, we omit the iteration τ1 + (2) in the following. By definition, we have 1,k + 1 1 S(2) = (i) m=k Vm exp (Bm) + m[K] Vm exp (Bm) + m=k(Vm/Vk) exp (Bm Bk) + (Nin/Vk) exp (Bk(1 Θ(ϵ1))) i=2n1,n[Nin+1] exp (ui) i=2n1,n[Nin+1] exp (ui) (cid:80) (cid:80) (cid:80) (cid:80) (ii) (cid:80) exp (maxm=k Bm Bk)(K/Lk 1) + K/Lk exp (Bk(1 Θ(ϵ1))) m=k(Vm/Vk) exp (Bm Bk) 1 + (cid:80) 1 + exp (maxm=k Bm(T (2) 1,k + 1) Bk(T (2) 1,k + 1))(K/Lk 1) 86 (iii) 1ϵ1/2 2 (K/Lk 1) + (K/Lk) 1ϵ1/2 1 + 1ϵ1/2 (K/Lk 1) 2 2 1 2 2 ϵ , (cid:0) (cid:1) where (i) follows from the fact ui(t) ϵ1Θ(maxm[K] Bm(t)) for all in Theorem G.2, (ii) follows from the bound of Vk in Theorem G.3, and (iii) follows from Claim (1) and the definition of phase II. Hence, we finish the proof of Theorem G.9. Proof of Theorem 5.3 Proof The Claims about Stage and Stage II follow directly from Theorems F.22 and G.9, respectively. Then we only need to show the loss convergence. The loss can be rewritten as S(2) (τ1 + τ2 + 1))Ey yN +1 2 (cid:17) (cid:105) L(θ(τ1 + τ2 + 1)) = = 1 2 1 2 (cid:104)(cid:16) i=1 (cid:88) k=1 (cid:88) (cid:104) 1{xq = vk} (cid:16) m=1 (cid:88) (τ1 + τ2 + 1)wvm wvk S(2) 2 (cid:17) (cid:105) (S(2) (τ1 + τ2 + 1))2 + (1 S(2) (τ1 + τ2 + 1)) m=k (cid:16) (cid:88) 1 S(2) (τ1 + τ2 + 1) (cid:17)(cid:105) 2 (cid:1) (cid:3) (i) = 1 2 k=1 (cid:88) 1{xq = vk} (cid:104) 1{xq = vk} k=1 (cid:88) (ii) ϵ2, (cid:2) (cid:0) where (i) follows from taking the conditional expectation of conditioned on and direct calculation, and (ii) follows from Theorem G.9. Hence, we finish the proof."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "National University of Singapore",
        "Sea AI Lab",
        "Yale University"
    ]
}