{
    "paper_title": "MS4UI: A Dataset for Multi-modal Summarization of User Interface Instructional Videos",
    "authors": [
        "Yuan Zang",
        "Hao Tan",
        "Seunghyun Yoon",
        "Franck Dernoncourt",
        "Jiuxiang Gu",
        "Kushal Kafle",
        "Chen Sun",
        "Trung Bui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization."
        },
        {
            "title": "Start",
            "content": "MS4UI: Dataset for Multi-modal Summarization of User Interface Instructional Videos Yuan Zang1*, Hao Tan2, Seunghyun Yoon2, Franck Dernoncourt2, Jiuxiang Gu2, Kushal Kafle2, Chen Sun1, Trung Bui2 1Brown University 2Adobe Research yuan_zang@brown.edu, bui@adobe.com 5 2 0 2 4 1 ] . [ 1 3 2 6 2 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization."
        },
        {
            "title": "Introduction",
            "content": "Multi-modal summarization proposes an effective way to learn from lengthy instructional videos. Researchers have proposed datasets and methods for multi-modal summarization of news (Li et al., 2020a; Zhu et al., 2018a), movies (Rao et al., 2020) and daily tasks (Sanabria et al., 2018). Meanwhile, data and methods for multimodal summarization of User Interface (UI) instructional videos are scarce. Compared with daily task videos, it is more challenging to comprehend and summarize UI instructional videos. First, the modeling of UI and UIrelated videos requires deep understanding of the visual elements in the UIs, which are structured, symbolic and abstract. Furthermore, in order to *Work done while the first author was an intern at Adobe"
        },
        {
            "title": "Research",
            "content": "Figure 1: An example of the summarization of UI tutorial videos in our dataset. It shows the summarization of the task Resize the layer in Adobe Photoshop. First, we segment the video into different steps. For each step, we summarize the key operations into concise text instruction and select key image frame that describing the corresponding operation. The red circles note the key elements related to the operations in the instruction. align the technical terms in the text instructions and the visual elements in the video, it requires domain-specific and fine-grained vision-language grounding. Recently, many UI related tasks including command language grounding (Li et al., 2020b), UI element retrieval (He et al., 2021), UI description generation (Huang et al., 2019) have been proposed. In this paper, we focus on summarizing UI instructional videos into compact and executable step-by-step instructions. Solving the proposed tasks of UI instructional video summarization is challenging. Existing multimodal summarization methods and benchmarks, mainly focus on document-level summarization (e.g. topic sentence and cover image for the news video), are not suitable for UI instructional video summarization because of following reasons. First, the instructional videos usually contain steps and thus require step-by-step summarization, but most existing multi-modal summarization methods do not support step segmentation. In addition, while existing multi-modal summarization tasks mainly stress semantic-remaining summarization, the step summarization for UI instructional videos should be executable and contain accurate technical details for specific actions. For example, to be executable, the instruction for certain action usually contain the details like which item to select in the toolbar and how to set the parameters, which might be neglected in general-purpose summarization. Therefore, we propose three core tasks for UI instructional video summarization, Video Segmentation, Text Summarization and Video Summarization. Given an instructional video, we first segment it into different steps. Then we generate the text instruction for each step according to the video content. To make the instructions easy-to-execute, we select representing frame which demonstrates the key actions for each step from the video. Figure 4 shows an example the three tasks. In this paper, we collect multi-modal summarization dataset of 2,413 UI instructional videos for Adobe Creative Cloud UIs from media including Adobe HelpX and Youtube and hire human annotators to perform the proposed tasks on the collected videos. We develop evaluation metrics for the proposed tasks and perform comprehensive experiments to evaluate state-of-the-art video summarization methods on the dataset. The results indicate that existing methods struggle with understanding and summarizing UI videos in the proposed dataset. In short, we make three key contributions: We propose novel multi-modal summarization dataset, which firstly focus on UI instructional video summarization. We introduce three core tasks and develop comprehensive evaluation metrics for the purpose of generating concise and executable step-by-step multi-modal summarization. We conduct exhaustive experiments to evaluate existing state-of-the-art methods on our dataset and demonstrate the significance of paying attention to the unique features of UI instructional video summarization."
        },
        {
            "title": "2 Related Work",
            "content": "With the development of multi-modal learning and its application on multi-media contents like videos, multi-modal summarization has attracted more and more attention. Unlike traditional video summarization (Gygli et al., 2015; Zhang et al., 2016) and text summarization (Nallapati et al., 2016; Zhu et al., 2018b; Celikyilmaz et al., 2018) that only rely on single vision or text modality, researchers (Krubinski and Pecina, 2023; He et al., 2023; Fu et al., 2020) have proposed to utilize multi-modal fusion modules to introduce the information from different modality into the representation of videos and texts to generate vision and text summarization. Researchers have developed benchmarks for multimodal summarization of videos in various domains, including news (Zhu et al., 2018a; Li et al., 2020a), medical analysis (Liu et al., 2022), movies (Rao et al., 2020) and daily tasks (Song et al., 2015; Gygli et al., 2014; Sanabria et al., 2018). However, most of those benchmarks focus on news or story video summarization, and the summarization of instructional videos is scarce. Previous work proposed to identify the significant frames in the instructional videos (Narasimhan et al., 2022) to build the summarization. However, it lacks clear step segmentation and textual instruction. Sanabria et al. (2018) collected instructional video data to build dataset How2, which contains the textual instruction for each step of the instructional videos but lacks the key image frames. In this paper, we build multi-modal summarization dataset containing segmentation as well as text and video summarization for UI instructional videos."
        },
        {
            "title": "3.1 Data Collection",
            "content": "We propose new dataset for multi-modal summarization of UI instructional videos. We collect instructional videos for Adobe Creative Cloud products from Adobe Support 1 and Youtube 2. The collected videos include tutorials for various UIs including Photoshop, Illustrator, Acrobat and Premiere. These tutorials provide detailed illustration of diverse functions in these UIs, such as editing images, modifying PDFs, and editing videos."
        },
        {
            "title": "Verification",
            "content": "In order to generate step-by-step summarization for the tutorial videos, we utilize GPT-3.5 to segment and summarize the transcriptions of these videos. Utilizing pre-designed prompts and examples, we can obtain step segmentation with time stamps and text summarization for each step that includes key operations. We ask human workers to verify the generated segmentation and summarization to ensure the fidelity. The human workers are tasked with revising the step segmentation, which 1https://helpx.adobe.com/support/creative-cloud.html 2https://www.youtube.com Type Num of Video Total Duration (Hours) Avg. Video Duration (Mins) Have Segmentation Avg. Num of Key Frame Avg. Text Summary Length TVSum SumMe VMSMO How2 MS4UI General General News Instruction UI Instruction 50 4 3. No 70 NA 25 1 2. No 44 NA 184,920 79,114 2, 3082 2000 1.0 No 1 1.5 Yes NA 20 167 4. Yes 5 27 Table 1: Comparison of different dataset for video summarization. Figure 2: Detailed statistics of the dataset, which show the distribution of the video duration, step numbers, step duration and step summarization length. includes adding necessary steps and removing duplicate ones, as well as verifying the time interval of each step and revising the step summarization. For the video summarization task, we ask human workers to select representing image frame that describes key operations for each step."
        },
        {
            "title": "3.3 Dataset Statistics",
            "content": "Our dataset contains 2,413 UI instructional videos of 167 hours length in total. Each video contains 5.08 steps in average. Each step is 32.47 second long and contains 27.28 words in average. Figure 2 shows the detailed statistics of the proposed dataset."
        },
        {
            "title": "3.4 Comparison with Existing Datasets",
            "content": "Table 1 provides comparison between the proposed dataset and existing video summarization datasets. Previous datasets such as TVSum (Song et al., 2015) and SumMe (Gygli et al., 2014) primarily emphasize visual aspects and do not include segmentation or text summaries. While news summarization datasets like VMSMO (Li et al., 2020a) do include text summaries, they often lack segmentation due to the short duration of news videos compared to instructional videos. The How2 dataset focuses on instructional videos and provides segmentation and text summaries, but it does not include key frame annotations that illustrate the actions in the instructions. The proposed dataset is the first dataset containing segmentation and multi-modal summarization tasks."
        },
        {
            "title": "4.1 Baseline Models",
            "content": "Video Segmentation For video segmentation, we compare text-based and vision based baselines. For text-based segmentation, we implement Cross Textseg (Lukasik et al., 2020) which utilizes hierarchy Transformer architecture to firstly encode the sentences and then identify the boundary sentences based on the sentence embeddings. For visionbased segmentation, we implement LGSS (Rao et al., 2020) which proposes boundary network to model the shot boundary for video segmentation. We also utilize the recently released video segmentation toolbox PySceneDetect (Castellano, 2021) as vision-based baseline. Text Summarization For text summarization, we firstly utilize pre-trained language models including BERT2BERT (Chen et al., 2021), BART (Lewis et al., 2019) and T5 (Raffel et al., 2020) to summarize the transcriptions for each step into concise instructions. In order to utilize the vision information, we also implement multi-modal summarization frameworks MLASK (Krubinski and Pecina, 2023) and A2Summ (He et al., 2023). Video Summarization For video summarization, we also use the multi-modal summarization models MLASK and A2Summ to model the image frames and transcriptions simultaneously and select the key image frames. We also compare vision-only method VSumm (De Avila et al., 2011) which extracts image features with VGG16 (Simonyan and Zisserman, 2014) and utilizes K-means to cluster the features to select the key frame."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "Video Segmentation We introduce Intersection over Union (IOU) to evaluate different video segmentation methods. Considering segmentations and as unions of image frames, the IOU is calculated as IOU = (1) Given the ground-truth video segmentation = {g1, g2, ..., gn} and the predicted segmentation = {p1, p2, ..., pm}, we can calculate the mean MIOU F1@0.1 F1@0.25 F1@0. Recall @ 1 Recall @ 2 Recall @"
        },
        {
            "title": "LGSS",
            "content": "14.61 20.53 20."
        },
        {
            "title": "PySceneDetect",
            "content": "19.21 34.94 43.47 44.04 41.55 25. 31.24 30.58 29.89 8.62 12.77 11. 11.73 Table 2: Video segmentation results of baseline methods. Rouge-1 Rouge-2 Rouge-L"
        },
        {
            "title": "Random",
            "content": "BERT2BERT T-5 BART-CNN BART-XSUM"
        },
        {
            "title": "MLASK",
            "content": "A2Summ 2.77 2.78 2.89 3.12 3. 5.10 3.82 0.31 0.28 0.47 0. 0.42 1.31 0.65 2.56 2.53 2. 3.01 3.10 3.98 3.27 Table 3: Text summarization results of baseline methods. IOU (MIOU) for the predicted segmentations as MIOU = 1 (cid:88) i=0 max IOU(pi, gj). (2) The MIOU metric is widely used to evaluate the segmentation accuracy in the domain of video. Based on IOU, we can also calculate the Precision, Recall and F1 score of the predicted segmentations. Given an IOU threshold th, the predicted segmentation that has an IOU high than th with ground-truth segmentation is regarded as true positive prediction. Following previous works (Rao et al., 2020; Qiu et al., 2023), we report the MIOU and F1 with different IOU threshold (0.1, 0.25, 0.5) for the evaluation of video segmentation. Text Summarization For text summarization, we introduce the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004) as the evaluation metric. The ROUGE measures the overlap between the n-grams of the generated summarization and the reference summarization verified by human. Following previous works, we utilize the ROUGE-1, ROUGE-2 and ROUGE-2 metrics to evaluate the text summarization methods. Video Summarization For video summarization, we select image frames from the video with regular intervals (1fps) and regard the image closest to the human-labeled key image frame as the groundtruth frame. We calculate the recall at different position (Recall@1, Recall@2, Recall@5) to evaluate the selection of key image frames. Accurate"
        },
        {
            "title": "ASumm",
            "content": "3.09 7.78 10.46 9.82 6.18 11. 14.35 13.20 15.62 22.58 27.47 27. Table 4: Video summarization results of baseline methods. key frame selection indicates precise grounding of text instructions and high executability of the summarization."
        },
        {
            "title": "4.3 Evaluation Results",
            "content": "Video Segmentation Table 2 shows the video segmentation results of different methods. The results indicate that none of the baseline models perform satisfactorily on our dataset. The textbased method outperforms both the vision-based and multi-modal methods. This suggests that existing methods struggle to effectively utilize visual signals of the UIs for video segmentation. Text Summarization Table 3 demonstrates the text summarization results of baseline methods. From the results we can observe that baseline methods exhibit substandard performance in term of the Rouge scores. In contrast to segmentation, segmentation, multi-modal methods outperform text-only methods on text summarization, which indicates that the vision signals of the UI layouts can benefit the understanding of text instructions. Video Summarization Table 4 shows the video summarization results of baseline methods. The results indicate that the performance of baseline methods is suboptimal, especially regarding the Recall @ 1 metric. This might be attributed to the vision module of baseline models struggling to distinguish similar images of the same UI and detect key operations. Overall, all baseline methods show unsatisfactory performance on the three core tasks of the proposed dataset, which indicates that the proposed dataset and tasks are challenging and requires specific design for understanding structured and finegrained UI layouts and actions."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we collect new video summarization dataset. We firstly focus on UI instructional videos and propose three core tasks for this domain. Experiments demonstrate that existing methods for video summarization struggle with the proposed tasks on the dataset, highlighting the challenge and necessity of the proposed dataset."
        },
        {
            "title": "6 Ethical Considerations",
            "content": "All videos in our dataset are publicly available. We have asked the human workers to make sure that there is no biased and discriminatory content in these videos. For personal privacy, we have anonymized and de-identified any personal information in the videos in the proposed dataset."
        },
        {
            "title": "7 Limitations",
            "content": "The proposed dataset currently consists solely of UI videos for Adobe Creative Cloud products. We plan to broaden its scope in the future to include more diverse range of UI videos. According to the experiments, existing video summarization methods show unsatisfoctory performance on the proposed dataset. It is of vital significance to develop methods which consider the unique features of UI videos to achieve better summarization performance."
        },
        {
            "title": "References",
            "content": "Brandon Castellano. 2021. Intelligent scene cut detection and video splitting tool. Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and Yejin Choi. 2018. Deep communicating agents arXiv preprint for abstractive summarization. arXiv:1803.10357. Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. 2021. bert2bert: Towards reusable pretrained language models. arXiv preprint arXiv:2110.07143. Sandra Eliza Fontes De Avila, Ana Paula Brandao Lopes, Antonio da Luz Jr, and Arnaldo de Albuquerque Araújo. 2011. Vsumm: mechanism designed to produce static video summaries and novel evaluation method. Pattern recognition letters, 32(1):5668. Xiyan Fu, Jun Wang, and Zhenglu Yang. 2020. Multimodal summarization for video-containing documents. arXiv preprint arXiv:2009.08018. Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool. 2014. Creating summaries from user videos. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13, pages 505520. Springer. Michael Gygli, Helmut Grabner, and Luc Van Gool. 2015. Video summarization by learning submodular mixtures of objectives. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 30903098. Bo He, Jun Wang, Jielin Qiu, Trung Bui, Abhinav Shrivastava, and Zhaowen Wang. 2023. Align and attend: Multimodal summarization with dual contrastive losses. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1486714878. Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu, Nevan Wichers, Gabriel Schubiner, Ruby Lee, and Jindong Chen. 2021. Actionbert: Leveraging user actions for semantic understanding of user interfaces. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 59315938. Forrest Huang, John Canny, and Jeffrey Nichols. 2019. Swire: Sketch-based user interface retrieval. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pages 110. Mateusz Krubinski and Pavel Pecina. 2023. Mlask: Multimodal summarization of video-based news articles. In Findings of the Association for Computational Linguistics: EACL 2023, pages 880894. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461. Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao, and Rui Yan. 2020a. VMSMO: Learning to generate multimodal summary for videobased news articles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 93609369, Online. Association for Computational Linguistics. Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. 2020b. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Ziyi Liu, Jiaqi Zhang, Yongshuai Hou, Xinran Zhang, Ge Li, and Yang Xiang. 2022. Machine learning for multimodal electronic health records-based research: Challenges and perspectives. In China Health Information Processing Conference, pages 135155. Springer. Michal Lukasik, Boris Dadachev, Kishore Papineni, and Gonçalo Simões. 2020. Text segmentation by cross segment attention. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 47074716, Online. Association for Computational Linguistics. Ramesh Nallapati, Bowen Zhou, Gulcehre, Xiang, et al. 2016. neural attention model for abstractive sentence summarization. arXiv preprint arXiv: 1602.06023. Medhini Narasimhan, Arsha Nagrani, Chen Sun, Michael Rubinstein, Trevor Darrell, Anna Rohrbach, and Cordelia Schmid. 2022. Tl; dw? summarizing instructional videos with task relevance and crossmodal saliency. In European Conference on Computer Vision, pages 540557. Springer. Jielin Qiu, Jiacheng Zhu, William Han, Aditesh Kumar, Karthik Mittal, Claire Jin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Bo Li, et al. 2023. Multisum: dataset for multimodal summarization and arXiv preprint thumbnail generation of videos. arXiv:2306.04216. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551. Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, and Dahua Lin. 2020. local-to-global approach to multi-modal movie scene segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1014610155. Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Loïc Barrault, Lucia Specia, and Florian Metze. 2018. How2: large-scale dataset for multimodal language understanding. arXiv preprint arXiv:1811.00347. Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. 2015. Tvsum: Summarizing web videos using titles. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 51795187. Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman. 2016. Summary transfer: Exemplar-based subset selection for video summarization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 10591067. Junnan Zhu, Haoran Li, Tianshang Liu, Yu Zhou, Jiajun Zhang, and Chengqing Zong. 2018a. MSMO: Multimodal summarization with multimodal output. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 41544164, Brussels, Belgium. Association for Computational Linguistics. Junnan Zhu, Long Zhou, Haoran Li, Jiajun Zhang, Yu Zhou, and Chengqing Zong. 2018b. Augmenting neural sentence summarization through extractive summarization. In Natural Language Processing and Chinese Computing: 6th CCF International Conference, NLPCC 2017, Dalian, China, November 812, 2017, Proceedings 6, pages 1628. Springer."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Examples In this subsection, we provide several examples of the video summarization data in our dataset. The whole videos and annotations are in the uploaded supplementary data. A."
        },
        {
            "title": "Implementation Details",
            "content": "For Cross Textseg we choose the pre-trained BERTlarge model to encode the text folowing the settings of the original paper. For LGSS we follow the implementation in the open-sourced code 3. For PySceneDetect we utilize the open-sourced toolkit 4. For language models, we utilize BERT-large to implement BERT2BERT and utilize T-5-large, BART-CNN-large, BART-XSUM-large model with the Huggingface 5 toolkit. For MLASK 6 and A2Summ 7 we follow the implementation in the open-sourced code. The dataset is split into 8:1:1 for train, validation, and test during training and evaluation. 3https://github.com/AnyiRao/SceneSeg 4https://www.scenedetect.com 5https://huggingface.co/docs/transformers/en/index 6https://github.com/ufal/MLASK 7https://github.com/boheumd/A2Summ Figure 3: An example of the summarization of UI tutorial videos in our dataset. It shows the summarization of the task Manipulation of shadows in an image. First, we segment the video into different steps. For each step, we summarize the key operations into concise text instruction and select key image frame that describing the corresponding operation. Figure 4: An example of the summarization of UI tutorial videos in our dataset. It shows the summarization of the task Hidden Features for Using Paint Symmetry. First, we segment the video into different steps. For each step, we summarize the key operations into concise text instruction and select key image frame that describing the corresponding operation."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Brown University"
    ]
}