{
    "paper_title": "Metis: Training Large Language Models with Advanced Low-Bit Quantization",
    "authors": [
        "Hengjie Cao",
        "Mengyi Chen",
        "Yifeng Yang",
        "Ruijun Huang",
        "Fang Dong",
        "Jixian Zhou",
        "Anrui Chen",
        "Mingzhi Dong",
        "Yujiang Wang",
        "Jinlong Hou",
        "Yuan Cheng",
        "Fan Wu",
        "Fan Yang",
        "Tun Lu",
        "Ning Gu",
        "Li Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work identifies anisotropic parameter distributions as a fundamental barrier to training large language models (LLMs) with low-bit quantization: a few dominant singular values create wide numerical ranges that conflict with the inherent bias of block-wise quantization. This bias disproportionately preserves high-magnitude values while discarding smaller ones, causing training instability and low model performance. This work introduces Metis, a training framework that combines (i) spectral decomposition with random embedding to efficiently disentangle dominant from long-tail components, compressing broad distributions into quantization-friendly narrow ranges; (ii) adaptive learning rates in the spectral domain to amplify underrepresented directions and better capture diverse features critical for performance; and (iii) a dual-range regularizer that jointly constrains numerical precision and parameter range distribution, ensuring stable, unbiased low-bit training. With Metis, FP8 training surpasses FP32 baselines, and FP4 training achieves accuracy comparable to FP32, paving the way for robust and scalable LLM training under advanced low-bit quantization. The code implementation for Metis is available at: https://github.com/typename-yyf/Metis-quantization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 4 0 4 0 0 . 9 0 5 2 : r Metis: Training Large Language Models with Advanced Low-Bit Quantization Hengjie Cao1 Mengyi Chen1, Yifeng Yang1, Ruijun Huang1 Jixian Zhou1 Anrui Chen1 Mingzhi Dong2 Yujiang Wang3 Yuan Cheng4 1 Fudan University Fan Wu5 2 University of Bath Fan Yang Tun Lu1 Ning Gu1 Fang Dong1 Jinlong Hou4 Li Shang1 3 Oxford Suzhou Centre for Advanced Research 4 Shanghai Innovation Institute 5 Huawei"
        },
        {
            "title": "Abstract",
            "content": "This work identifies anisotropic parameter distributions as fundamental barrier to training large language models (LLMs) with low-bit quantization: few dominant singular values create wide numerical ranges that conflict with the inherent bias of block-wise quantization. This bias disproportionately preserves high-magnitude values while discarding smaller ones, causing training instability and low model performance. This work introduces Metis, training framework that combines (i) spectral decomposition with random embedding to efficiently disentangle dominant from long-tail components, compressing broad distributions into quantizationfriendly narrow ranges; (ii) adaptive learning rates in the spectral domain to amplify underrepresented directions and better capture diverse features critical for performance; and (iii) dual-range regularizer that jointly constrains numerical precision and parameter range distribution, ensuring stable, unbiased low-bit training. With Metis, FP8 training surpasses FP32 baselines, and FP4 training achieves accuracy comparable to FP32, paving the way for robust and scalable LLM training under advanced low-bit quantization. The code implementation for Metis is available at: https://github.com/typename-yyf/Metis-quantization."
        },
        {
            "title": "Introduction",
            "content": "Training large language models (LLMs) under low-bit quantization faces fundamental dilemma: the exponentially constrained numerical precision and range imposed by low-bit formats conflict with the inherently wide distribution of model data, including parameters, activations, and gradients. This study investigates the root causes of such wide distributions and analyzes their impact on training stability and effectiveness under low-bit quantization. The key findings are as follows: Anisotropy is universal in modern LLMs: Across openly released models including DeepSeek, Mistral, LLaMA, and Qwen, we consistently observe that small leading fraction of singular values captures the majority of spectral energy in weight matrices. Quantitatively, single-digit percentage of dominant spectral components accounts for this concentration across model sizes, architectures, layers, and scales, establishing anisotropy with energy concentrated in the top singular values as universal structural feature rather than model-specific artifact. Anisotropy drives wide numerical distributions: The wide value ranges in parameters, activations, and gradients arise directly from anisotropic training matrices, where few directions capture most of the energy. Large singular values along these directions amplify aligned components, producing high-magnitude entries; small singular values suppress the remaining components toward near-zero, Co-first authors. Preprint. Work undergoing. filling the distributions tails. As training progresses, this spectral concentration typically sharpens, increasing contrast and widening the overall dynamic range. Bias in block-level quantization: There is an inherent bias in the de facto block-level quantization which prioritizes preserving larger-magnitude values at the expense of smaller ones. Within each block, single scaling factor is selected to minimize the groups reconstruction error, disproportionately allocating quantization levels to high-magnitude entries and resulting in coarser resolution for smaller values. This bias exacerbates instability in settings with wide numerical distributions, leading to degraded performance and reduced training stability in low-bit quantization. In response to these findings, we posit that an unbiased narrow-range distribution is necessary to robust and effective low-bit training. We introduce the Metis framework to achieve this goal. Spectral Decomposition with Random Embedding: The anisotropic nature of parameter, gradient, and activation matrices necessitates disentangling dominant and long-tail singular vectors to mitigate uneven quantization effects across diverse magnitudes. Isolating dispersed singular values within the dominant subspace leverages the scale-invariance of singular vectors, enabling transformation of broad distributions into quantization-friendly, unbiased, narrow-range form. random embedding algorithm efficiently approximates the dominant subspace, enabling robust and unbiased low-bit training with low computational overhead. Adaptive Spectral Learning Rate: Uneven singular value distributions in matrices cause underupdating of moderate singular vectors, impairing the learning of associated features. Spectral decomposition maps training into the spectral domain, where singular directions are explicitly identified. This enables adaptive learning rates per spectral component, amplifying underrepresented directions while tempering dominant ones, thus balancing optimization across anisotropic vectors and enhancing the capture of diverse features essential for model performance. Dual-Range Regularization: dual-range regularizer that impartially constrains both the numerical precision and the parameter range distribution, facilitating stable and unbiased low-bit quantization during training. Metis enables unbiased, narrow-range distributions throughout model training. We evaluate its effectiveness using FP8 and FP4 formats and report the following key findings: FP8 matches or surpasses FP32: Training with full W8A8G8 quantization, where weights (W), activations (A), and gradients (G) are quantized to 8-bit floating point (FP8, E4M3) during General Matrix Multiplication (GeMM), achieves training loss and downstream performance on par with, and in some cases exceeding, the FP32 baseline. FP4 becomes feasible: Training with full W4A4G4 quantization, where weights, activations, and gradients are quantized to 4-bit floating point (FP4, E2M1) during GeMM, enabled by Metis achieves accuracy comparable to FP32 across downstream tasks. These findings establish advanced low-bit quantization as practical new standard for efficient, high-fidelity LLM training. Meanwhile, this work is ongoing. At the time of this first draft on arXiv, the proposed method has been validated on moderately sized model (1B parameters) using publicly available dataset of 88B tokens. Updated results with larger models and more extensive training data will be released in the coming weeks. For ease of adoption, the FP8 implementation is being integrated into existing training frameworks, with open-source code to be made available for immediate use. Further optimization of FP4 quantization depends on forthcoming software stack support for Blackwell architectures. We invite comments, feedback, and collaboration from the community to advance low-precision quantization training."
        },
        {
            "title": "2 Analysis",
            "content": "This section analyzes how the dynamic evolution of the parameter space during training becomes misaligned with low-bit quantization schemes. We begin by revealing the anisotropic trend, in which dominant singular values grow and increasingly govern the singular value distribution. We then demonstrate that this trend reshapes the parameter spectrum, widening the distribution of parameter magnitudes. Finally, we show that block-wise quantization interacts destructively with this spectrum, introducing bias that disproportionately affects small-magnitude parameters and undermines quantization performance. 2 2.1 Anisotropy: Universal Property of Modern LLMs For weight matrix Rmn, we perform Singular Value Decomposition (SVD) to obtain singular values {σi}min(m,n) i=1 Rm, and right singular vectors {vi}n i=1 Rn, i=1 such that = (cid:80)min(m,n) . We assume throughout that singular values are sorted in descending order, i.e., σ1 σ2 σr > 0, where = min(m, n). , left singular vectors {ui}m σiuiv i= To investigate the structural properties of industrial-scale language models, including Qwen2.5, Qwen3, and DeepSeek, ranging from 7B to 671B parameters, we analyze the singular value spectra of the final feed-forward (FFN) network module, as shown in Figure 1, which displays anisotropic singular value distributions. Using the final FFN module as representative example , Figure 1 plots the singular value spectra for QWEN2.5-7B, QWEN3-32B, QWEN2.5-72B, and DEEPSEEK-R1671B. Across all models, the spectra are sharply concentrated, with small prefix of singular values carrying most of the energy. We define the elbow index as the point of maximum curvature and compute the elbow fraction = k/r, where = min(m, n) is the matrix rank. The estimated fractions are = 1.9%, 2.2%, 2.1%, and 2.4% for the 7B, 32B, 72B, and 671B models, respectively, indicating that only few percent of directions account for the bulk of spectral energy. This consistently small across architectures and scales underscores anisotropy, energy concentration in the largest singular values, as universal property of transformer-based models, with no significant variation in the elbow fraction as model size increases. Figure 1: Singular value spectra of the final FeedForward module in pretrained models (Qwen2.5-7B, Qwen3-32B, Qwen2.5-72B, DeepSeek-R1-671B). small fraction of singular values dominates the spectrum (1.9%, 2.2%, 2.1%, 2.4%), with the share remaining relatively stable across model scales, indicating consistent anisotropy. Beyond this global concentration, we observe consistent temporal trends on 1B-parameter GPT-2 and an 8B-parameter LLaMA-3 trained on the DCLM dataset: (i) all singular values grow as training proceeds, and (ii) the leading singular values grow disproportionately faster, increasing spectral skew and strengthening the dominant directions. Mechanism of Anisotropy Propagation. The anisotropy in parameter space emerges as dominant singular vectors of weight matrices progressively align with the anisotropic principal directions of the representation space during next token prediction optimization. This alignment occurs through gradient updates where the parameter gradient decomposes into the outer product of gradient with respect to output and input representations. 3 σi uiv Critically, since input representations exhibit energy concentration along few principal directions, the gradient projections onto singular vectors become biased: singular component whose singular vectors ui align with representation spaces dominant directions receive disproportionately strong updates. We empirically explain this by introducing the gradient singular alignment metric. For weight matrix = (cid:80) with gradient G, we define the gradient singular alignment for the i-th singular component as ai = G vi, and in practice we use ai to measure the magnitude of the gradient update along that singular direction. From matrix perturbation theory, first-order expansion of the singular value under small update ηG gives σi(W ηG) σi(W) η G vi, implying σi η ai. Thus, ai directly quantifies the first-order sensitivity of σi to single training step: larger ai means larger per-step change in the corresponding singular value. Empirically, in our 1B-parameter GPT-2 model, we measure ai for the attention key projection and the first feed-forward linear transformation. As shown in Figure 2, ai exhibits clear monotonic decline with σi, indicating that gradient updates concentrate disproportionately on dominant singular values. This preferential allocation accelerates the growth of large singular values and further amplifies their influence over the spectrum. Figure 2: Gradient singular alignment ai = G vi for two representative weight matrices from middle layer of 1B-parameter GPT-2 model: the attention key projection (left) and the first feed-forward linear transformation (right). Curves are colored by training step. Across training, ai decreases systematically with σi, indicating that gradient energy is concentrated on dominant singular values. This preferential allocation accelerates the growth of large singular values, reinforcing their dominance in the spectrum. 2.2 Anisotropy Trend Induces Wide Numerical Distributions The anisotropy trend, characterized by the accelerated growth of leading singular values, significantly increases the entrywise variance of the parameter matrix. For Rmn with empirical mean µ, the variance is: Var(W) = 1 mn W2 µ2 = 1 mn (cid:88) i=1 µ2. σ (1) Thus, an increase in spectral energy (cid:80) , driven by dominant singular values, directly amplifies Var(W). This variance expansion widens the numerical distribution, as quantified by Popovicius inequality, which states that for any bounded random variable X, the range satisfies range(X) 2(cid:112)Var(X). Applying this to the entries of W, assumed bounded in practice, we derive: σ2 range(W) 2(cid:112)Var(W) = 2 (cid:115) 1 mn (cid:88) σ2 µ2. (2) 4 This shows that growing spectral energy, due to anisotropic singular value distributions, enforces larger lower bound on the entrywise range, broadening the parameter distribution in practice. To empirically validate these theoretical insights, we analyze the FFN module of 1B-parameter GPT-2 model at 10k training steps, consistent with prior analyses. Figure 3 presents the singular value spectra and numerical distributions of the parameter matrix, its activation input, and the back-propagated gradient. The spectra exhibit strong anisotropy, with few leading singular values dominating the energy, while most remain small. This imbalance manifests in the numerical distributions, shown as log-log histograms in the bottom row, which reveal heavy-tailed patterns driven by the dominant singular components (uiσiv ). Figure 3: Analysis of weight, activation, and gradient matrices from 1B GPT-2 model at 10k steps. The top row shows singular value distributions, and the bottom row displays numerical distributions (log-log scale, frequency-based), overlaid with dashed histograms of rank-1 components (uiσivT for = 0, 16, 128, 1024). Large singular values dominate, driving wide numerical distributions. Beyond widening the overall value range, the anisotropy trend also induces an imbalance in how information is distributed across the numerical spectrum of the parameter matrix. The contribution of the i-th singular component to every entry of the matrix scales as σi/ mn, with unit-norm singular vectors distributing energy across coordinates. Large singular values thus generate high-magnitude entries, forming the heavy tails observed in the histograms, while smaller singular values contribute to near-zero entries, concentrating information in the low-value region. As dominant singular values grow, they increasingly govern the high-value tail, marginalizing the information encoded by smaller singular values, as evidenced by the rank-1 component distributions in Figure 3. 2.3 Bias Amplification and Error Growth in Block-Wise Quantization Block-Wise Quantization. To accommodate decreasing numerical precision in low-bit regimes, quantization granularity evolves from coarse tensor-wise scaling to channel-wise scaling, and ultimately to fine-grained block-wise scaling. This progression adapts the scaling factor to the local dynamic range, minimizing reconstruction error. We denote b-bit quantization function as Qb, transforming fullor half-precision matrix Rmn into its quantized form = Qb(A), with quantized matrices indicated by bar symbol (e.g., A). Due to the limited dynamic range of FP4 formats, block-wise quantization methods like MXFP4 [12] and NVFP4 [9] are standard, integrated as hardware-native formats. MXFP4 uses block size of 32 with an E8M0 scaling factor format, while NVFP4 employs block size of 16 with an E4M3 format, enabling finer-grained adaptation, denoted as QMX and QNV . 4 4 5 (bi) = round (cid:0) bi Bias in Quantization. To illustrate the quantization mechanism, we focus on MXFP4 as an example. Regardless of granularity, the core principle remains: group of values within block shares single scaling factor to minimize reconstruction error. For block Rt, where = 32 for MXFP4, the scaling factor is = max 2b11 , with = 4 being the bit width. Individual elements bi are (cid:1) s, mapping to the nearest representable value in the 4-bit range. quantized as QMX This mechanism biases towards larger magnitudes, as is dominated by the maximum absolute value, resulting in coarser quantization resolution = 2b1 for smaller entries. Consequently, smaller values often face inadequate resolution and are clipped to zero, introducing systematic bias rather than unbiased noise. As shown in Figure 4(A), which analyzes the first linear mapping matrix from the deepest feed-forward network layer of 1B GPT-2 model at 10k steps, MXFP4 quantization leads to significant loss of small-value information due to clipping, as evidenced by the comparison of value distributions before and after quantization. Wide Numerical Distribution Amplifies Quantization Bias. Wide numerical distributions, induced by anisotropy, amplify the bias in MXFP4 quantization. Larger spreads within block increase the scaling factor s, coarsening the quantization resolution . This inflates reconstruction error QMX 2 for non-extreme values and causes severe clipping of small-value intervals, disproportionately affecting spectral tails encoding rare signals. (B)2 Bias Interaction with Anisotropy. Anisotropic distributions allocate information unevenly: large singular components dominate high-value regions, while small singular components govern low-value intervals. Quantization bias disrupts learning in these low-magnitude directions by clipping small values, amplifying anisotropy. Figure 4(B) shows higher relative errors in smaller singular values, while Figure 4(C) indicates better directional preservation for larger singular values via absolute cosine similarity of left singular vectors. This bias destabilizes low-bit training by introducing heteroscedastic, input-dependent errors that violate uniform noise assumptions in stochastic optimization. Figure 4: Analysis of the first linear mapping matrix from the deepest feed-forward network layer of 1B GPT-2 model at 10k steps. (A) Comparison of value distributions before and after quantization, revealing significant loss of small-value information due to clipping to zero. (B) Relative error in singular values, showing larger singular values are less affected by quantization, while smaller ones exhibit greater changes. (C) Absolute cosine similarity of left singular vectors, indicating better directional preservation for larger singular values."
        },
        {
            "title": "3 Method",
            "content": "As the aforementioned analysis shows, low-bit quantization faces challenges due to the anisotropic parameter distribution, where large singular values dominate and bias matrix-level quantization, overshadowing information tied to smaller singular values. To address this, we propose Metis, framework integrating three components for stable low-bit quantization in large language models: 6 Spectral Decomposition with Random Embedding: Disentangles dominant and long-tail singular vectors using randomized SVD, efficiently decomposing broad distributions into quantization-friendly, narrow-range forms with low computational overhead. Adaptive Learning Rate: Adjusts learning rates in the spectral space to accelerate updates along moderate singular vectors, countering anisotropy trend and accelerating learning along longtail directions. Dual-Range Regularization: Constrains large and near-zero values to align parameter distributions with low-bit numerical ranges, narrowing numerical distributions and ensuring precise quantization. 3.1 Spectral Decomposition with Random Embedding Motivation: Spectral decomposition strategy decomposes the wide numerical distribution of weight matrix into independent, narrow distributions for separate quantization. The key motivation is that, despite wide variation in singular values, singular vectors exhibit scale-invariant, Gaussian-like distributions, with ranges approximately two orders of magnitude smaller than the entire matrix, as shown in Figure 5. This enables precise quantization. The anisotropic nature of allows decomposition of only dominant singular values and vectors, as the residual matrix has small, uniform singular values, maintaining narrow distribution without interference during quantization. Figure 5: Analysis of the first linear mapping matrix from the deepest feed-forward network layer of 1B GPT-2 model at 10k steps. The left panel illustrates that the broad distribution of the original parameter matrix results from the superposition of various singular components, with larger singular values predominantly influencing the high-value region and corresponding sub-distributions exhibiting wider spreads. The right panel demonstrates that, when singular values are extracted as scaling factors, the value distributions of all singular components except the largest are narrow and exhibit similar Gaussian-like shapes. Metis focuses on the low-bit quantization of General Matrix Multiplication (GeMM), an operation widely used in dense and self-attention layers of Transformers [16] and consuming over 95% of the computational workload in LLM training [18, 4]. Denote weight matrix as Rmn and an input activation matrix as Rlm, we aim to quantize the forward and backward propagation the GeMM operation = XW under low-bit formats, where Rln. We first decompose into two branches, one for learning dominant features and another for obtaining long-tail knowledge, to suppress the spread of parameter magnitudes. We employ low-rank SVD approximation of as the first branch for learning dominant features. Formally, low-rank SVD approximation of is written as ˆWk = (cid:80)k i=1 σiuiv Efficient Low-Rank Approximation via Randomized Embedding. To approximate the dominant subspace of an anisotropic weight matrix W, where few large singular values dominate, we use the randomized SVD algorithm though randomized projection. Gaussian random matrix Ω Rnk, with entries drawn from (0, 1) and min(m, n), projects onto low-dimensional subspace via = WΩ, sampling the column space. The dominance of large singular values ensures that , where and ˆWk Rmn. captures principal directions with high probability. QR decomposition of yields an orthonormal basis Rmk, approximating Ws column space. Projecting onto via CW produces reduced matrix, preserving the column space while transforming the row space orthogonally. An SVD on CW yields approximate singular values σi and vectors ui, vi for rank k, efficiently capturing the dominant subspace. Compared to full SVDs O(mnr) complexity, randomized SVD reduces this to O(mnk), making it scalable for large-scale LLMs. Note that ˆWk provides the best approximation of among all lowrank matrices based on the Eckart-Young-Mirsky Theorem. We will denote ˆWk in the matrix-based form, i.e., ˆWk = UkSkV where Uk Rmk, Vk Rnk, and Sk = diag(σ1, . . . , σk) Rkk. The second branch will concentrate on the learning of long-tail features, and we adopt the residual matrix WR = ˆWk to represent it. The decomposition of is then written as = ˆWk + WR = UkSkV + WR. Following the decomposition in Eq. 3, the forward GeMM computation can be written as = ˆWk + XWR = XUkSkV + XWR. (3) (4) The quantization function Qb is then applied separately to those matrices in Eq. 4 other than Sk, as those matrices have demonstrated generally unbiased and low-bit-friendly distributions. Specifically, the forward computation of under b-bit quantization, denoted as ˆY, is computed as ˆY = Qb(X)Qb(Uk)SkQb(V ) + Qb(X)Qb(WR) = XUkSkV + XWR. (5) Eq. 5 is applied to every GeMM operation during the forward propagation. Although quantized matrices are used in Eq. 5 for acceleration and memory efficiency, they are one-time running results, and we maintain high-precision copy (BF16 in this work) of parameter matrices in Eq. 4 for further quantization and gradient updates. In practice, we only perform the decompositions in Eq. 3 once for each weight matrix immediately after initialization. Besides, we empirically set = 0.1 r. In backward propagation, we quantize the GeMM operations associated with derivative computations of matrices in Eq. 4. Formally, denote the derivatives of the loss function with respect to (w.r.t) Rln, we need to compute the following derivatives for updating parameters: as = X , , and , , L . Similarly, we first decompose into SVD low-rank approximation Uk WR Vk Sk and the residual, which is = PjTjQ + DR (6) where Pj Rlj, Qj Rnj, Tj Rjj, and rank(D). The derivative X is computed as X = Y X = D(VkS = PjTjQ VkS + U ), + PjTjQ R + DRVkS + DRW . We then compute X under 4-bit quantization as ˆL = PjTjQ VkS + PjTjQ + DRVkS U + DRW R. Following similar pathway, we compute other derivatives under 4-bit quantization as follows: ˆL Uk ˆL Sk ˆL Vk ˆL WR = XPjTjQ VkS + XDRVkS , = XPjTjQ Vk + XDRVk, = k XPjTjQ + U XDR, = XPjTjQ + XDR. 8 (7) (8) (9) (10) (11) 3.2 Adaptive Spectral Learning Rate Motivation: In low-bit quantization regimes, uniformly treating all singular values introduces imbalance. Dominant spectral components with large singular values already absorb most of the gradient energy, while moderate ones often encoding semantically useful long-tail features are under-updated. At the same time, the smallest singular values frequently correspond to noise or spurious directions, and amplifying them may harm stability. To address this imbalance, we propose to selectively rescale only the top-k singular values, ensuring that dominant modes do not overwhelm the optimization while retaining sufficient representation of informative directions, where is the same value as the Section 3.1. Let {σ top-k spectrum, we replace them with rescaled version: i=1 denote the singular values sorted in descending of D. Instead of using the raw σi in the i}r σ = 2σ 1 + σ σ 1 , = 1, . . . , k, while keeping σ unchanged for > k. This inverse-normalized rescaling suppresses excessively large singular values relative to σ 1 and relatively enhances the contribution of intermediate ones, thereby flattening the update distribution across the top spectral modes. As result, the leading directions remain stable, while long-tail but informative directions receive proportionally more emphasis, yielding more balanced and quantization-friendly training dynamic. 3.3 Dual-Range Regularization Motivation: To further narrow numerical distributions, we propose dual-range regularization strategy. This technique enforces balanced parameter distribution that aligns with the numerical constraints of FP4 quantization, mitigating both overflow (large magnitudes) and underflow (near-zero values) while preserving the models representational capacity. Given parameter tensor W, we introduce regularization term R(W) that penalizes both excessively large and near-zero values: R(W) = λ1 2 + λ2 (cid:88) (cid:88) 1 2 + ϵ , where: λ1 controls the penalty on large magnitudes to prevent overflow beyond the quantization range [Rmax, Rmax]. λ2 penalizes near-zero values to avoid underflow and resolution loss in low-bit quantization. ϵ (e.g., 108) is smoothing term to ensure numerical stability when Wi 0. The combined regularization term is added to the training loss L: Ltotal = Ltask + R(W)."
        },
        {
            "title": "4 Experiments",
            "content": "This chapter presents an experimental evaluation of the proposed Metis framework for ultra-lowbit training. We validate its stability, efficiency, and effectiveness under FP8 and FP4 precision, comparing against full-precision FP32 and direct quantization baselines. Results demonstrate that Metis not only eliminates the convergence degradation common in low-bit settings but also preserves, or even improves, downstream task performance, establishing FP8 as sufficient and FP4 as feasible for large-scale model training. 4.1 Experiments setup In alignment with our central claims that FP8 quantization matches or surpasses FP32 performance and that FP4 quantization is feasible under Metis framework, we design experiments in two stages. First, we evaluate the performance of full W8A8G8 quantization (FP8E4M3) on large language models, assessing training loss and downstream task accuracy to confirm its ability to achieve or 9 exceed FP32 baselines. Subsequently, we explore the feasibility of full W4A4G4 quantization, investigating whether Metis can push the precision boundary further to FP4 while retaining stability and competitive performance. We adopt the GPT-2 architecture [11] with two model sizes: (1) 130M parameters (12 layers, embedding dimension 768) and (2) 1.1B parameters (24 layers, embedding dimension 2048). All models are trained on the DCLM dataset, with the 1.1B parameter model in FP4 configuration trained on approximately 88B tokens, while other configurations use subset of this dataset. For evaluation, we use the GLUE benchmark [17] for downstream tasks. We employ the AdamW [6] optimizer with an initial learning rate of 1 105, using 50 step linear warmup followed by cosine decay. Input sequences have fixed length of 1024 and are processed in batches of size 512. Weight decay is set to 1 102, and gradient norms are clipped at 8.0. FP8 Experiments. Due to the higher precision of FP8 compared to FP4, the spectral decomposition strategy is applied only to the forward pass, while backward propagation is quantized at the block level using FP8 by default. This setting enables us to decompose the wide distribution of parameters into narrower, quantization-friendly distributions while avoiding the computational cost of low-rank SVD decomposition in the backward pass. We implement two FP8 variants on GPT-2 (1.1B parameters): (1) Metis+FP8E4M3 (Full-rank SVD): forward-pass weight matrices are fully decomposed via SVD before quantization. (2) Metis (1% rank)+FP8E4M3: forward-pass weight matrices are approximated using only the top 1% of singular values/vectors before quantization. Both are compared against direct FP8E4M3 baseline without Metis. FP4 Experiments. Having established FP8 stability, we test whether Metis can stably train models under FP4 precision. We implement Metis with two FP4 formats, NVFP4 and MXFP4, using lowrank SVD approximation with rank set to 50% of the parameter rank in both forward and backward passes. Comparisons are made against (1) full-precision FP32 training and (2) direct FP4 baselines without Metis. All FP4 and FP8 experiments are conducted via simulation on NVIDIA H100 GPUs, with custom CUDA kernels for quantized operations in PyTorch 2.1. 4.2 Experimental results The efficacy of Metis is evaluated in two dimensions: (1) stability and convergence behavior during training, and (2) final performance of quantized models on downstream tasks. (a) (b) Figure 6: Training loss comparison for 1B-parameter GPT-2 under FP8 quantization. The direct FP8 baseline shows persistent loss gap relative to the full-precision FP32 baseline, while Metis based FP8 methods (Metis forward full rank SVD (a) and Metis forward 1% rank SVD (b)) closely track the FP32 loss trajectory throughout training, effectively eliminating the degradation observed in the direct FP8 case. For FP8 quantization, as shown in Figure 6, we observe clear performance gap between the direct FP8 baseline and the full-precision FP32 baseline: the FP8 baseline exhibits consistently higher training loss throughout the optimization trajectory. In contrast, both of our FP8 variants, Metis full SVD and Metis 1% rank SVD, track the FP32 baseline almost exactly, with their loss curves 10 remaining closely aligned across all training steps. This near-perfect alignment indicates that Metis effectively remove the degradation typically seen in direct FP8 training. (a) (b) Figure 7: For different GPT-2 : (a) 130M GPT-2 training loss curves. (b) 1B GPT-2 training loss curves. Direct FP4 quantization (especially MXFP4 baselines) suffers from training instability, exhibiting erratic loss fluctuations even failing to converge effectively. In stark contrast, Metis framework enables remarkably stable FP4 training (Figure 7). The loss curves for Metis closely track the smooth convergence trajectory of the full-precision FP32 baseline across both model sizes (130M and 1.1B parameters). Crucially, Metis models consistently achieve final training loss lower than direct FP4 methods. This final loss is closer to the FP32 baseline, demonstrating Metiss effectiveness in overcoming the inherent instability challenges of ultra-low-bit training. In the forward branch we parameterize UkSkV . The spectral anisotropy concentrates in the diagonal factor Sk, while Uk, Vk carry orientations with balanced row/column norms. As training progresses, magnitude growth is absorbed by Sk rather than the factors, so Uk and Vk do not become overly anisotropic. Empirical evidence is in Appendix A. 4.2.1 Model Performance on Downstream Tasks Table 1: Downstream task performance of GPT-2 (1.1B) under different FP8 quantization settings. Method FP Metis(full rank)+FP8E4M3 Metis(1%rank)+FP8E4M3 FP8E4M3 test loss 4.00 3.95 4.01 4.38 CoLA SST-2 MRPC MNLI QNLI (acc) (acc) (acc) (acc) (acc) RTE (acc) 68.9% 84.7% 81.2% 81.8% 82.3% 81.3% 68.3% 84.8% 81.6% 81.4% 83.0% 81.3% 68.0% 84.5% 81.3% 80.9% 82.6% 80.7% 68.5% 82.9% 79.1% 80.2% 81.7% 80.0% Table 2: Downstream task performance of GPT-2 (130M) under different FP4 quantization settings. Results for MXFP4 are omitted, as training failed to converge. Method test loss CoLA SST-2 MRPC MNLI QNLI (acc) (acc) (acc) (acc) (acc) RTE (acc) FP Metis+NVFP4 Metis+MXFP4 NVFP4 MXFP4 2.96 3.02 3.05 3.42 NaN 68.4% 86.2% 83.6% 82.9% 85.3% 82.0% 68.6% 85.7% 83.4% 82.3% 84.5% 81.5% 67.3% 86.0% 82.6% 82.5% 85.1% 82.1% 67.5% 85.2% 81.1% 81.8% 80.6% 80.7% - - - - - - Evaluations on the GLUE benchmark (Table 1, Table 2, and Table 3) reveal that Metis consistently bridges the performance gap introduced by aggressive low-bit quantization. 11 Table 3: Downstream task performance of GPT-2 (1.1B) under different FP4 quantization settings. Results for MXFP4 are omitted, as training failed to converge."
        },
        {
            "title": "Method",
            "content": "test loss CoLA SST-2 MRPC MNLI QNLI (acc) (acc) (acc) (acc) (acc) RTE (acc) FP Metis+NVFP4 Metis+MXFP4 NVFP4 MXFP4 3.29 3.36 3.37 3.82 7.54 67.9% 85.3% 81.4% 81.3% 83.6% 81.8% 68.6% 84.1% 80.7% 81.5% 83.8% 80.8% 67.2% 84.6% 81.0% 80.2% 83.0% 81.2% 67.5% 82.9% 77.8% 80.4% 82.1% 78.5% - - - - - - In the FP8E4M3 setting, both Metis (full rank)+FP8E4M3 and Metis (1% rank)+FP8E4M3 achieve test losses that are even lower than the FP32 baseline, while also outperforming the direct FP8E4M3 baseline. On GLUE tasks, accuracy improves slightly when singular values are split during forward computation. This indicates that FP8 training with Metis better preserves information from long-tail components, which are typically underrepresented in standard training, leading to more balanced feature learning and modest gains on tasks requiring fine-grained representations. In the FP4 setting, direct NVFP4 and MXFP4 baselines exhibit substantial degradation compared to FP32. In contrast, Metis dramatically outperforms these direct FP4 baselines, with average GLUE scores markedly higher and final performance closely matching the FP32 baseline. Notably, on finegrained tasks such as CoLA and MRPC, Metis even matches or slightly surpasses FP32 performance within statistical variance. This demonstrates Metiss ability to preserve, and occasionally enhance, model expressiveness under ultra-low-bit constraints. 4.2.2 Discussion on Training efficiency This section evaluates the computational overhead introduced by Metis in both the forward and backward passes. Following the same notation in Section 3.1, let the input activation matrix be Rlm, the weight matrix be Rmn, and the output be Rln. We represent the rank-k approximation of as (cid:99)Wk = UkSkV and the residual as WR = (cid:99)Wk. Forward pass. is decomposed into low-rank branch and residual branch: In the baseline, computing = XW requires O(lmn) operations. Under Metis, = XUkSkV + XWR. (12) The low-rank path XUkSkV costs O(lmk + lkn), while the residual path XWR remains O(lmn). When min(m, n), the extra cost from the low-rank path is marginal compared to the baseline. For completeness with the quantization notation of Section 3.1, if Q4 denotes the 4-bit quantizer (applied to factors other than Sk), the quantized forward output can be written as (cid:98)Y = Q4(X) Q4(Uk) Sk Q4(Vk) + Q4(X) Q4(WR), (13) which preserves the same asymptotic costs as in (12). Backward pass. Let = and are computed via DW and XD, each with complexity O(lmn). Rln denote the output gradient. In the baseline, gradients w.r.t. Metis introduces randomized low-rank decomposition of D: PjTjQ , min(l, n), (14) incurring O(ln log + lnj) to obtain the factors. Subsequent low-rank multiplications then cost O(lmj + mnj), which remain small for min(l, n). The residual branch continues to follow the O(lmn) baseline complexity. Training efficiency. As shown in Table 4, the additional computational overhead in Metis for both forward and backward passes is O(lmk) (with an effective rank of approximately 1% of 12 min(m, n)). This results in low additional cost compared to the baseline O(lmn), ensuring efficient training performance. In the FP8 configuration, the higher precision further optimizes the backward pass by eliminating the need for low-rank approximation, streamlining gradient computations without requiring additional decomposition or low-rank multiplications, thereby maintaining negligible computational overhead. Table 4: Computational Complexity of Metis."
        },
        {
            "title": "Forward",
            "content": "O(lmn)"
        },
        {
            "title": "Backward",
            "content": "O(lmn)"
        },
        {
            "title": "Metis",
            "content": "O(lmn + lkn) O(lmn + lkn) 4.3 Ablation Study We evaluate the contribution of each major component in Metis by systematically removing it from the full framework and retraining 1B-parameter GPT-2 on the DCLM corpus under FP4, using subset of the full 88B-token dataset. Each row in Table 5 corresponds to the complete system with one component removed. We report held-out test loss and dev accuracy on CoLA, SST-2, MRPC, and MNLI. Avg Acc is the unweighted mean over these four tasks. Table 5 summarizes outcomes. Backward decomposition is essential. Removing it destabilizes training (test loss rises to 7.50) and prevents reliable downstream evaluation, underscoring the role of gradient-path splitting for FP4 stability. Adaptive learning rate yields the largest consistent gain. Omitting it reduces Avg Acc by 0.56 points (76.77 vs. 77.33) and slightly worsens loss (4.27 vs. 4.25), with small drops across all tasks. Forward decomposition primarily benefits NLI-style generalization. Without it, Avg Acc drops by 0.36 points; MNLI falls by 1.7 (78.877.1), while MRPC is marginally higher. Dual-range regularization is mild but positive stabilizer. Removing it gives the worst loss among successful runs (4.32) and 0.18-point Avg Acc decrease; it slightly helps CoLA (+1.2) but hurts SST-2 (0.8) and MNLI (1.2). Takeaway. Metis benefits from all components: backward decomposition is necessary for stable FP4 training, while adaptive learning rate, forward decomposition, and dual-range regularization provide complementary accuracy gains, most notably on MNLI, yielding the best aggregate performance when combined. Table 5: Ablation of Metis components on 1B GPT-2 trained on DCLM in FP4. Each row removes one component from the full framework. Results for w/o backward decomposition are omitted, as training failed to converge. Values are dev accuracy (%) except Test loss. Avg Acc averages {CoLA, SST-2, MRPC, MNLI}. Setup Test loss CoLA SST-2 MRPC MNLI Avg Acc Metis w/o forward decomposition Metis w/o backward decomposition Metis w/o adaptive learning rate Metis w/o dual-range regularization Metis (full) 4.25 7.50 4.27 4.32 4.25 67.5 67.2 68.9 67.7 83.4 83.0 82. 83.4 79.9 78.7 79.5 79.4 77.1 78.2 77.6 78.8 76.97 76.77 77. 77."
        },
        {
            "title": "5 Related Works",
            "content": "Anisotropic. In LLMs, anisotropy in representation spaces has emerged as pervasive issue, where embeddings cluster in narrow cones or low-dimensional subspaces, reducing effective dimensionality and degrading downstream performance. Early work on static word embeddings, such as [8], showed that after removing the mean vector, representations concentrate energy in few directions, with much 13 of the variance captured by dominant principal components encoding common dataset information. This trend persists in contextual models like GPT-2, BERT, and ELMo, where [5] demonstrated extreme anisotropy in later layers, leading to near-unity cosine similarity between random words and collapse of directional diversity. The primary cause lies in token frequency imbalances, as highlighted by [21] and [10]. Rare tokens form large-norm outliers during pre-training, skewing the space and \"dragging down\" common representations, while high-frequency tokens cluster near the origin, amplifying the long-tail distribution of singular values. To address anisotropy, [13] introduced IsoScore for measuring uniformity and stable regularization techniques that adjust anisotropy levels, showing moderate anisotropy can enhance NLP tasks. Overall, these advances underscore the need for frequency-aware training and decomposition methods to harness LLMs full representational capacity. Block-level Quantization. NVIDIAs Blackwell natively supports block-scaled FP4 via two families, NVFP4 [9] and MXFP4 [12], which implement the microscaling idea: store elements in narrow FP4 type and attach higher-precision per-block scale to recover local dynamic range while containing bandwidth and compute cost. Concretely, NVFP4 encodes values in FP4 E2M1 and assigns an FP8 E4M3 scale to each block of 16 elements, enabling fractional scaling that better tracks real distributions. MXFP4 follows the OCP Microscaling specification, using power-of-two scales, typically with 32-element blocks, which simplifies hardware and dataflow at the cost of coarser adaptation. Outlier mitigation. Block-level microscaling alleviates but does not eliminate the core mismatch between FP4s limited dynamic range and the widespread and long-tailed statistics of LLM signals. Under shared block scale, few large-magnitude entries dominate the scale selection, compressing the numerous small-magnitude entries into only handful of quantization bins, often zero, and thus discarding disproportionate amount of information. Hence, algorithmic outlier mitigation is required. Prior work falls into three families: (1) Channel-wise re-parameterization. LLM activations often contain channel-localized, asymmetric outliers. Per-channel re-parameterization tackles this by balancing magnitudes before quantization: large channels are down-scaled and benign channels up-scaled so that activations and weights have comparable per-channel ranges under shared quantizer. SmoothQuant [20] implements this idea with calibrated per-channel scale that trades off activation and weight magnitudes using migration coefficient, then folds the inverse scale into the weights offline. Outlier Suppression+ [19] augments per-channel scaling with per-channel shift to correct inter-channel asymmetry and proves both operations can be migrated into subsequent modules without changing the function. To further strengthen extreme low-bit settings, OmniQuant [14] makes these equivalent transforms learnable and pairs them with learnable weight clipping. (2) Hadamard transformations. Hadamard transformations [15] apply an orthogonal rotation of channel coordinates using matrices with entries in {1}. Because = I, inserting and around linear layer preserves the function, while redistributing outliers across all channels so no single channel sets an excessively large quantization range. QuaRot [1] rotates hidden states and parameters to remove outliers and achieve end-to-end 4-bit quantization of weights, activations, and KV cache; QuIP [3] applies randomized Hadamard preprocessing to increase incoherence and attains state-of-the-art weight-only PTQ at 4 bits; HALO [2] strategically places Hadamard rotations in both forward and backward passes to stabilize low-precision training. However, these methods require inserting numerous Hadamard matrix multiplications in both forward and backward passes, incurring substantial computational overhead of O(mn log min(m, n)) even with fast WalshHadamard transform optimizations. Moreover, while Hadamard transformations can partially smooth outlier values, they fail to effectively narrow the overall wide numerical distribution, leaving the quantized representation highly susceptible to distribution shifts and precision loss. (3) Outlier separation. This family removes extreme values from the low-bit path and routes them through small higher-precision auxiliary branch, trading modest extra compute for large error reduction. Microsoft [18] proposes Outlier Clamping and Compensation: top-quantile activations are clamped to stabilize FP4, and the clipped residuals are recovered by sparse high-precision compensation matrix evaluated with sparse GEMM. complementary approach, SVDQuant [7], first shifts difficult activation mass into weights and then absorbs the outlier energy in high-precision low-rank branch, leaving the residual path safely quantized. 14 Per-channel scaling is fundamentally diagonal and However, gaps remain for fully FP4. calibration-dependent; residual extremes can still dictate shared block scale as precision shrinks, leaving small values over-compressed. Hadamard rotations better equalize statistics but require inserting transforms in both forward and backward passes to stabilize training, adding kernels and data-movement complexity without guaranteeing uniform variance across successive tensors. Outlier separation attains high accuracy only by retaining higher-precision computation, which departs from end-to-end FP4. Accordingly, we work in singular space, using randomized SVD to disentangle dominant and long-tail singular vectors, decomposing broad distributions into quantization-friendly, narrow-range forms with low computational overhead, enabling fully FP4 training without any auxiliary high-precision branches."
        },
        {
            "title": "6 Conclusion",
            "content": "This work identifies anisotropic parameter distributions as fundamental barrier to stable training of large language models under low-bit quantization. To address this, Metis, spectral-space training framework, is introduced to transform and stabilize parameter distributions, enabling robust optimization in ultra-low precision. Experiments show that Metis achieves FP8 performance on par with or surpassing FP32 baselines, and makes FP4 training feasible with comparable accuracy, establishing advanced low-bit quantization as new standard for efficient, high-fidelity LLM training. Ongoing efforts focus on scaling to larger models and hardware-native deployment, with the aim of broadening access to high-performance, energy-efficient LLMs."
        },
        {
            "title": "References",
            "content": "[1] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems, 37:100213 100240, 2024. [2] Saleh Ashkboos, Mahdi Nikdan, Soroush Tabesh, Roberto Castro, Torsten Hoefler, and Dan Alistarh. Halo: Hadamard-assisted lower-precision optimization for llms. arXiv preprint arXiv:2501.02625, 2025. [3] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36:43964429, 2023. [4] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in neural information processing systems, 35:3031830332, 2022. [5] Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512, 2019. [6] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [7] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024. [8] Jiaqi Mu, Suma Bhat, and Pramod Viswanath. All-but-the-top: Simple and effective postprocessing for word representations. arXiv preprint arXiv:1702.01417, 2017. [9] Nvidia. Introducing nvfp4 for efficient and accurate low-precision inference nvidia technical blog, 6 2025. [10] Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice DellOrletta. Outliers dimensions that disrupt transformers are driven by frequency. arXiv preprint arXiv:2205.11380, 2022. [11] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [12] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. [13] William Rudman and Carsten Eickhoff. Stable anisotropic regularization. arXiv preprint arXiv:2305.19358, 2023. [14] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023. [15] Ananda Theertha Suresh, Yu Felix, Sanjiv Kumar, and Brendan McMahan. Distributed mean estimation with limited communication. In International conference on machine learning, pages 33293337. PMLR, 2017. [16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [17] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. 16 [18] Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, and Peng Cheng. Optimizing large language model training using fp4 quantization. arXiv preprint arXiv:2501.17116, 2025. [19] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023. [20] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International conference on machine learning, pages 3808738099. PMLR, 2023. [21] Sangwon Yu, Jongyoon Song, Heeseung Kim, Seong-min Lee, Woo-Jong Ryu, and Sungroh Yoon. Rare tokens degenerate all tokens: Improving neural text generation via adaptive gradient gating for rare token embeddings. arXiv preprint arXiv:2109.03127, 2021."
        },
        {
            "title": "A Isotropy Trend in singular space",
            "content": "The left plot in Figure 8 illustrates the singular value distribution of the left and right singular vector matrices after decomposing the parameter matrix during the forward pass, tracked over the course of training. As training converges, the anisotropy of these matrices is significantly reduced compared to the original parameter matrix. The right plot visualizes the numerical distribution of the left and right singular matrices, which exhibits substantially narrower range than that of the original matrix. (a) (b) Figure 8: (a): Singular value distribution of left and right singular vector matrices during training, showing reduced anisotropy at convergence compared to the original parameter matrix. (b): Numerical distribution of the left and right singular matrices, demonstrating significantly narrower range than the original matrix."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Huawei",
        "Oxford Suzhou Centre for Advanced Research",
        "Shanghai Innovation Institute",
        "University of Bath"
    ]
}