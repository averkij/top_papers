{
    "paper_title": "RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization",
    "authors": [
        "Zeng Zhiyuan",
        "Jiashuo Liu",
        "Zhangyue Yin",
        "Ge Zhang",
        "Wenhao Huang",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for training large reasoning models, its training dynamics harbor a critical challenge: RL overfitting, where models gain training rewards but lose generalization. Our analysis reveals this is driven by policy over-specialization and catastrophic forgetting of diverse solutions generated during training. Standard optimization discards this valuable inter-step policy diversity. To address this, we introduce RLoop, a self-improving framework built on iterative policy initialization. RLoop transforms the standard training process into a virtuous cycle: it first uses RL to explore the solution space from a given policy, then filters the successful trajectories to create an expert dataset. This dataset is used via Rejection-sampling Fine-Tuning (RFT) to refine the initial policy, creating a superior starting point for the next iteration. This loop of exploration and exploitation via iterative re-initialization effectively converts transient policy variations into robust performance gains. Our experiments show RLoop mitigates forgetting and substantially improves generalization, boosting average accuracy by 9% and pass@32 by over 15% compared to vanilla RL."
        },
        {
            "title": "Start",
            "content": "RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization Zhiyuan Zeng 1 2 Jiashuo Liu 2 Zhangyue Yin 1 Ge Zhang 2 Wenhao Huang 2 Xipeng Qiu 1 3 1Fudan University 2M-A-P 3Shanghai Innovation Institute cengzy23@m.fudan.edu.cn gezhang@umich.edu rubio8741@gmail.com xpqiu@fudan.edu.cn Abstract While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for training large reasoning models, its training dynamics harbor critical challenge: RL overfitting, where models gain training rewards but lose generalization. Our analysis reveals this is driven by policy overspecialization and catastrophic forgetting of diverse solutions generated during training. Standard optimization discards this valuable inter-step policy diversity. To address this, we introduce RLoop, self-improving framework built on iterative policy initialization. RLoop transforms the standard training process into virtuous cycle: it first uses RL to explore the solution space from given policy, then filters the successful trajectories to create an expert dataset. This dataset is used via Rejection-sampling Fine-Tuning (RFT) to refine the initial policy, creating superior starting point for the next iteration. This loop of exploration and exploitation via iterative re-initialization effectively converts transient policy variations into robust performance gains. Our experiments show RLoop mitigates forgetting and substantially improves generalization, boosting average accuracy by 9% and pass@32 by over 15% compared to vanilla RL. 5 2 0 2 6 ] A . [ 1 5 8 2 4 0 . 1 1 5 2 : r 1. Introduction Reinforcement Learning (RL), particularly through policy gradient methods such as PPO and its variants, has emerged as cornerstone for aligning Large Language Models (LLMs) with complex human objectives. By enabling optimization for non-differentiable reward signals, RL has catalyzed significant advancements in diverse domains, including instruction following (Ouyang et al., 2022) and mathematical reasoning (DeepSeek-AI et al., 2025). However, our investigation into the application of RL for Corresponding authors 1 complex reasoning tasks reveals critical, yet previously under-explored, challenge: phenomenon we term RL overfitting, analogous to its supervised learning counterpart. As illustrated in Figure 1, we observe stark divergence between the training objective and true generalization performance. While the in-distribution reward signal exhibits steady increase throughout training (e.g., for over 700 steps), the models generalization capabilitiesmeasured by outof-distribution test accuracy and pass@k metricsstagnate or even degrade much earlier (e.g., around 140 steps). This divergence strongly suggests that the RL agent becomes overly specialized in exploiting high-reward trajectories within its known distribution, leading to model that is confident yet brittle when confronted with unseen problems. To dissect the underlying dynamics of this overfitting phenomenon, we conducted deeper empirical analysis of the RL training process. As shown in Figure 2b, we found that standard RL training suffers from catastrophic forgetting, especially in the later stages, where the model discards approximately 30% of the knowledge acquired during early training. This finding indicates that policies at different training steps are substantially distinct. Such inter-step policy diversity represents valuable asset for exploration, yet it is typically discarded in conventional training paradigms. While prior work has acknowledged the importance of trajectory diversity (Cui et al., 2025; Wang et al., 2025a), it has primarily focused on the diversity generated by single policy at fixed step, overlooking the rich diversity across different training checkpoints. Inspired by the potential of harnessing this inter-step diversity, we propose RLoop, self-improving framework centered around iterative policy initialization. Instead of single, monolithic training run, RLoop recasts the process as virtuous cycle where the policy is progressively refined and re-initialized. Each iteration consists of two phases: 1. Exploration Phase: Starting from the current policy θi, we run standard RL process not to find single optimal policy, but to generate diverse pool of solution trajectories. The significant policy shifts across RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization (a) Reward on Training set. (b) Accuracy on validation set. (c) Pass@32 on validation set. Figure 1: The reward, accuracy and pass@32 score of Qwen-2.5-math-7b trained with the DAPO algorithm evaluated on AIME-2024. RL steps act as powerful, built-in exploration mechanism. 2. Exploitation Phase: We curate the trajectories generated during exploration by filtering for successful outcomes. This expert dataset, Di expert, is then used to refine the initial policy θi via Supervised Fine-Tuning. The resulting improved policy, θi+1, serves as superior starting point for the subsequent exploration phase. The crucial step is that this consolidated policy θi+1 is not the final output, but serves as superior initial policy for the next Exploration Phase. RLoop thus establishes selfcontained improvement loop: RL explores possibilities from stable base, and RFT consolidates the findings into better base. This process of iterative policy initialization allows the model to systematically accumulate knowledge, turning the transient diversity from RL into robust, generalizable capabilities. Unlike prior works that rely on external expert data to bridge RL and supervised learning (Yan et al., 2025; Zhang et al., 2025b; Chen et al., 2025), RLoop bootstraps its own progress. To further stabilize this self-improvement, we incorporate an active learning strategy to ensure the model continually focuses on challenging problems. Our main contributions are summarized as follows: We empirically identify and characterize the RL overfitting phenomenon in LLMs, demonstrating that reward improvements do not necessarily translate to enhanced generalization. We reveal that this overfitting is linked to catastrophic forgetting and highlight the untapped potential of interstep policy diversity, valuable resource discarded by standard RL. We propose RLoop, an iterative self-improvement framework that effectively balances exploration and exploitation by alternating between RL for diverse solution generation and RFT for knowledge consolidation. Our experiments demonstrate that RLoop significantly outperforms vanilla RL on challenging math reasoning benchmarks, particularly in pass@k metrics. We further show that RLoop mitigates forgetting and make the RL training more stable. 2. Related Works Generalization of RLVR. The generalization capabilities of RLVR are subject of active research with conflicting findings. Several theoretical and empirical studies suggest that RL can lead to strong generalization (Chu et al., 2025; Zhang et al., 2025a; Anonymous, 2025), with some work even demonstrating its effectiveness with single questionanswer pair (Wang et al., 2025b). However, this optimistic view is challenged by other research. Yue et al. (2025) empirically found that while RLVR improves greedy-decoding accuracy, it can degrade pass@k performance, especially for large k. This suggests that standard RL may merely improve test-time efficiency rather than enhancing the models core ability to solve novel problems. Conversely, other studies (Liu et al., 2025b) report that RLVR can indeed improve pass@k scores on certain tasks. Our work contributes to this debate by identifying key dynamic: both accuracy and pass@k can degrade during training due to an overfittinglike phenomenon, which we aim to resolve. Efforts to improve RLVR generalization can be categorized into three main perspectives: 1. Data-centric Approaches: These methods focus on enriching the training data. For instance, Li et al. (2025a) and Liang et al. (2025) propose augmenting the question set to expose the model to wider range of states. Others focus on curriculum learning; Prakash & Buvanesh (2025) found that mixing simple and hard questions facilitates knowledge transfer, while Li et al. (2025b) advocate for increasing the rollout budget for more challenging problems, based on the finding that performance correlates with the number of unique problems solved. 2 RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization 2. Algorithm-centric Approaches: These methods modify the RL algorithm itself. prominent line of work explores the relationship between performance and policy entropy (Cui et al., 2025; Wang et al., 2025a). Wang et al. (2025a) propose masking gradients from low-entropy tokens to focus learning on more uncertain parts of the reasoning process. Similarly, Cui et al. (2025) introduce methods like Clip-Cov and KL-Cov to constrain gradients from high-variance tokens, thereby enhancing exploration. 3. Initialization-centric Approaches: Recognizing that strong starting point is crucial for RL, these approaches focus on creating superior initial policy. Works like Wang et al. (2025c) and Guha et al. (2025) achieve this by pre-training or fine-tuning models on large, high-quality, reasoning-intensive corpora before applying RL. it operates as self-improving, Our proposed RLoop framework aligns with the initialization-centric perspective but with critical distinction: iterative reinitialization loop. Unlike methods requiring extensive human effort for data curation, RLoop synthesizes its own expert data for re-initialization directly from the trajectories generated during the RL phase, creating fully autonomous improvement cycle. Combining RL and SFT. The RLoop framework, which iterates between RL and Rejection-sampling Fine-Tuning (RFT, form of SFT), belongs to broader class of methods that combine SFT and RL. The most common paradigm is simple pipeline where SFT provides the initial policy for subsequent RL phase (Ouyang et al., 2022). More intricate integrations have also been explored. Some methods aim to incorporate SFT data directly into the RL process, for instance, through off-policy learning (Yan et al., 2025). Others attempt to merge the SFT and RL objectives into single loss function for joint training (Zhang et al., 2025b; Yan et al., 2025; Chen et al., 2025). different approach, exemplified by Ma et al. (2025a), involves interleaving SFT steps within the RL training loop, using high-quality solutions discovered during RL to reinforce the policy. Addressing the catastrophic forgetting issues observed in such methods, Yuan et al. (2025) proposed constraining the SFT updates. Another line of work, including Chen et al. (2024) and Zhong et al. (2025), formulates the problem as latent variable model akin to Variational Autoencoder (VAE) (Kingma & Welling, 2022), where SFT updates reward model and RL improves the policy in an alternating fashion. Our RLoop framework is distinct from these prior works in two fundamental ways. First, it employs cyclical, macrolevel iteration between distinct RFT and RL phases, rather than fine-grained interleaving or joint loss. Second, and more importantly, RLoop is entirely self-contained, bootstrapping its SFT data from the RL agents own successful explorations. This eliminates the need for any external expert data, setting it apart from most hybrid SFT-RL methods. 3. Preliminary Study: Characterizing Policy"
        },
        {
            "title": "Dynamics in RLVR",
            "content": "As established in the introduction and illustrated in Figure 1, standard RLVR exhibits an overfitting-like behavior, where training rewards diverge from validation performance. To delve into the mechanisms behind this phenomenon, we conduct preliminary study analyzing three key metrics: the learning rate, the forgetting rate, and trajectory similarity. The learning and forgetting rates quantify the models ability to acquire new problem-solving capabilities and its tendency to lose previously acquired ones, respectively. Trajectory similarity measures the distributional shift in generated solutions across different training steps. 3.1. Learning and Forgetting Dynamics To understand the trade-offs during training, we analyze the policys ability to both learn new problems and forget old ones. We define the learning rate from checkpoint at step to later checkpoint at step (i < j) as the proportion of validation problems that the policy at step can solve, but the policy at step could not. Symmetrically, the forgetting rate is the proportion of problems that the policy at step could solve, but the policy at step fails to solve. The Learning Matrix (Figure 2a) reveals that the model continuously learns to solve new problems throughout training. The non-zero values in the upper triangle indicate that later policies acquire capabilities that earlier policies lacked. This demonstrates that continued training is not merely fitting to noise; the agent is genuinely expanding its problem-solving repertoire. However, this learning comes at cost, as shown by the Forgetting Matrix (Figure 2b). The matrix reveals significant level of forgetting, with rates frequently exceeding 10% and reaching as high as 35%, particularly between distant checkpoints. This observation empirically confirms that the RLVR is not only acquiring new skills but is also simultaneously discarding previously learned ones. The interplay between learning and forgetting explains the performance stagnation observed in RLVR. In the early stages of training, the learning rate surpasses the forgetting rate, leading to net increase in validation performance. As training progresses, the forgetting rate begins to catch up with or even exceed the learning rate. Therefore, the models performance on the validation set oscillates or de3 RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization (a) Learning Matrix (b) Forgetting Matrix (c) Similarity Matrix Figure 2: The value at (i, j) in the Learning Matrix represents the percentage of validation problems that the policy at step can solve but the policy at step cannot. Conversely, the value in the Forgetting Matrix represents problems solvable at step but not at step j. The value at (i, j) in the Similarity Matrix indicates the average n-gram similarity of trajectories between policies from step and step j. For all analyses, we sample 32 solutions for each question in the validation set. creases. This dynamic provides compelling explanation for why prolonged training does not necessarily lead to better generalization. 3.2. Similarity Matrix To complement the performance-based analysis of the forgetting matrix, we analyze the lexical similarity of the generated trajectories. This metric quantifies the textual consistency of solutions generated by policies at two distinct training steps, and j. The core idea is to measure the overlap of n-grams (specifically, bi-grams) between the sets of solutions generated at these two steps. The process involves calculating the pairwise similarity for all solution pairs using the Jaccard index, and then aggregating these scores into single, robust metric. The detailed mathematical formulation for this calculation is deferred to Appendix A. The resulting Similarity Matrix (Figure 2c) shows that the intra-step similarity (diagonal entries, typically >0.26) is substantially higher than the inter-step similarity (offdiagonal entries, typically <0.2). Moreover, the similarity systematically decreases as the distance between steps increases. Taken together, the Learning, Forgetting, and Similarity Matrices provide compelling evidence from two different perspectivesperformance and solution formthat policies at different RL training steps are remarkably diverse. This motivates our core idea: to explicitly collect and consolidate this valuable, yet typically discarded, diversity for more robust generalization. 4. Methodology 4.1. RLoop Framework To counteract the overfitting and instability identified in Figure 1, we introduce RLoop, an iterative training framework designed to harness the inter-step policy diversity. RLoop transforms the standard linear training process into cyclical one, explicitly alternating between an RL-based exploration phase and an RFT-based exploitation phase. The framework operates as an iterative loop, as detailed in Algorithm 1: 1. Exploration Phase (RL): Starting from base policy πθi , we execute standard RL training process for fixed number of steps. The primary goal of this phase is not to train the policy to convergence, but to leverage it as powerful search algorithm. The inherent stochasticity and policy drift across steps drive the model to explore diverse modes of the solution space. We collect trajectories from multiple intermediate checkpoints within this phase to create rich and varied dataset, Di RL = {τ1, . . . , τN }. 2. Exploitation Phase (RFT): In this phase, we distill and consolidate the knowledge discovered during exploration. First, we filter the collected trajectories using the reward signal, retaining only successful solutions to form an expert dataset: Di RL R(τ ) > 0}. We then use this curated dataset to finetune the initial policy πθi via Supervised Fine-Tuning (SFT). The resulting improved policy, πθi+1, becomes the starting point for the next iteration of the loop, thus creating self-improving cycle. expert = {τ Di The overall process is summarized in Algorithm 1. 4 RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization Algorithm 1 Iterative Policy Initialization Initialize: Start with base policy πθ0 (e.g., Qwen-2.5-7b-math). for = 0 to 1 do // Exploration Step Initialize RL policy from πθi. Run RL for NRL steps to generate set of trajectories Di // Exploitation Step Filter for successful trajectories: Di Initialize new policy from the same starting point πθi. Perform Supervised Fine-Tuning on this policy using Di θi+1 = arg maxθ expert = {τ Di log πθ(τ ) (cid:80) τ Di expert RL. RL R(τ ) > 0}. expert to obtain πθi+1. end for Return: The final refined policy πθI . 4.2. Active Learning for Focused Exploitation The efficacy of the RLoop framework arises from the complementary strengths of RL and RFT. closer look at their optimization dynamics reveals why RFT provides stable exploitation and, crucially, why this stability benefits from an active learning strategy. The policy gradients for RL (e.g., REINFORCE) and RFT differ fundamentally in their weighting schemes: θJRL(θ) = Eτ πθ [A(τ )θ log πθ(τ )] θJRFT(θ) = Eτ πθRL [R(τ )θ log πθ(τ )] (1) (2) RLs Relative Weighting: The advantage function A(τ ) measures trajectorys quality relative to the policys average performance. This makes RL effective at differentiating good from better but provides vanishing learning signal when all sampled trajectories are already successful (i.e., their advantages are near zero). RFTs Absolute Weighting: In contrast, RFT uses the absolute reward R(τ ) as weight (effectively 1 for success and 0 for failure). This provides stable, low-variance learning signal that reinforces every successful trajectory, regardless of the batchs overall performance. While RFTs stability is key advantage, it introduces potential inefficiency: RFT may over-invest capacity on problems the model already solves consistently. Since every successful trajectory receives an equal weight of 1, the model might spend excessive effort reinforcing its knowledge of easy problems. This observation directly motivates our use of active learning. To make the exploitation phase more efficient and targeted, we apply filter before the RFT step. We identify subset of problems that the current policy finds hard (e.g., those with low success rate across generated samples). The RFT update is then performed exclusively on successful trajectories from this hard subset. This active learning strategy ensures that the models capacity is focused on expanding its capabilities at the frontier of its knowledge, preventing redundant updates on mastered tasks and optimizing computational resource usage. 4.3. Theoretical Grounding: RFT as Importance-Weighted MLE The RFT phase is not merely heuristic; it can be theoretically grounded as form of policy improvement derived from Maximum Likelihood Estimation (MLE) with importance sampling. Ideally, we would want to train our policy πθ to match an unknown expert distribution p(τ ) that produces correct and generalizable solutions. This corresponds to maximizing the log-likelihood: LMLE(θ) = Eτ [log πθ(τ )]. (3) We cannot sample directly from p, but we can sample from the RL policy which can be seen as an approximation of p. Therefore, we use importance sampling to re-express this objective using trajectories sampled from our RL policy, πθRL: LMLE(θ) = Eτ πθRL (cid:20) p(τ ) πθRL(τ ) (cid:21) log πθ(τ ) . (4) The key challenge is the unknown importance weight w(τ ) = p(τ )/πθRL(τ ). However, we can approximate this weight using the reward signal R(τ ). Intuitively, trajectory τ with high reward is more likely to belong to the expert distribution than trajectory with low reward. For binary rewards (R(τ ) {0, 1}), this leads to simple and powerful approximation: w(τ ) = p(τ ) πθRL(τ ) R(τ ). (5) 5 RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization Table 1: The performance comparison between the base model (Qwen-2.5-7b-Math), RL and RLoop. Dataset Method Avg@32 Pass@8 Pass@16 Pass@"
        },
        {
            "title": "MinervaMath",
            "content": "Omini-Math"
        },
        {
            "title": "MATH",
            "content": "Avg"
        },
        {
            "title": "Base\nRL\nRLoop",
            "content": "Base RL RLoop 6.00 31.04 37.60 8.49 18.37 19.88 8.00 19.81 21.00 24.71 56.78 58.93 16.80 31.50 34. 43.65 50.96 58.77 25.78 27.64 29.95 26.38 27.70 31.74 67.32 66.54 75.50 40.78 43.21 49.00 46.09 56.97 66. 29.36 29.27 31.58 31.58 29.23 34.58 73.19 68.64 78.08 45.06 46.03 52.73 46.66 63.33 73.33 31.61 29.63 32. 36.20 30.00 37.00 76.00 70.20 80.00 47.62 48.29 55.73 By substituting this approximation into the importancesampled objective (Equation 4), we arrive at the RFT objective: LRFT(θ) = Eτ πθRL [R(τ ) log πθ(τ )] . (6) This objective is precisely the SFT loss applied to the rejectsampling dataset Dexpert, thus providing principled justification for our method. RLoop Setup For our proposed RLoop framework, the RFT phase utilizes trajectories cached during the preceding RL exploration phase. We implement an active learning strategy by filtering this data, retaining only successful trajectories from hard problemsdefined as those where the models success rate during the RL phase was below 10%. Each full iteration of the RLoop cycle consists of 200 RL training steps followed by one epoch of RFT on the curated dataset. 5. Experiments 5.1. Experiment Setting Datasets and Evaluation. For training, we employ the DAPO-17k dataset (Yu et al., 2025), which consists of 17,000 challenging mathematical problems. To ensure comprehensive assessment of generalization, we evaluate our models on suite of widely recognized benchmarks: AIME 2024, MinervaMath (Lewkowycz et al., 2022), OmniMath (Gao et al., 2025), and the MATH-500 test set (Hendrycks et al., 2021). RL Setup. Our base model for all reinforcement learnIn line with the ing experiments is Qwen-2.5-7b-Math. approach of R1-Zero (DeepSeek-AI et al., 2025), we apply RL directly to this base model with rule-based reward. We employ the DAPO algorithm (Yu et al., 2025) with group size of 16 and maximum generation length of 2048 tokens. For checkpoint selection, we use AIME 2024 as our validation set, selecting the model that achieves the best performance for final evaluation. 5.2. Main Results We conduct comprehensive comparison between our proposed RLoop framework and standard vanilla RL baseline (DAPO). The results are presented in Table 1. To ensure fair comparison of computational costs, the vanilla RL baseline was trained for 600 steps, while RLoop was run for three iterations, with each iteration comprising 200 RL steps. The computational overhead of the RFT phase is negligible relative to the RL phase, making the total training budgets for both methods comparable. As evidenced by the results, RLoop consistently and substantially outperforms the vanilla RL baseline across all evaluation benchmarks, in terms of both accuracy (Average@32) and Pass@k. The most significant gains are observed in the Pass@k scores, highlighting RLoops ability to generate more diverse set of correct solutions. As expected, the performance improvement on AIME 2024 is particularly pronounced, as it was used as the validation set for checkpoint selection. crucial observation is the detrimental effect of vanilla RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization Figure 3: The performance of Qwen-2.5-7b-Math trained with RLoop in different number of iterations, in terms of accuracy and pass@k score. ing knowledge, effectively mitigates this issue and achieves superior generalization. 5.3. Scalability Analysis In this section, we investigate the scalability of RLoop by analyzing its performance over an extended number of iterations and comparing its learning dynamic against vanilla RL. Scaling with More Iterations Figure 3 illustrates the performance of RLoop as function of the number of iterations. The results demonstrate clear positive scaling trend: performance on both accuracy (Avg@32) and pass@k metrics improves with additional iterations. This trend is particularly evident on the Omni-Math and MATH benchmarks. Notably, the improvement in pass@k scores is more pronounced than the gains in accuracy, suggesting that continued iterations primarily enhance the models ability to generate diverse set of correct solutions. Contrasting Scalability Dynamics To understand how RLoop utilizes computational budget differently from vanilla RL, we plot its performance against vanilla RL on continuous training step axis. As shown in Figure 4, each 200-step RL phase of RLoop iteration is juxtaposed with the corresponding training window of the vanilla RL baseline. The comparison reveals stark contrast. Vanilla RL (the red curve) exhibits classic overfitting: its performance on the validation set peaks around 300 steps and then steadily degrades, indicating that further training is detrimental. In contrast, RLoop effectively leverages the additional computational budget. While vanilla RLs performance deteriorates, RLoop continues to achieve new performance heights with each subsequent iteration. closer examination reveals fascinating dynamic within each RLoop iteration. The performance often rises before Figure 4: Compare the accuracy of vanilla RL an RLoop at different training steps. RL on the models pass@k performance. On three out of four benchmarks (MinervaMath, Omni-Math, and MATH), the pass@k scores of the RL-trained model are worse than those of the original base model, especially at larger values of like = 32. This aligns with findings from prior work (Yue et al., 2025), which posited that standard RL might fail to genuinely enhance the intrinsic reasoning capabilities of LLMs. However, our RLoop framework not only reverses this degradation but surpasses the base models performance by significant margin. This suggests that the performance drop is not an inherent flaw of using RL for reasoning, but rather byproduct of the standard, continuous training paradigm that leads to overfitting. Interestingly, vanilla RL shows performance gain on AIME 2024, in contrast to its degradation on other benchmarks. We hypothesize that this is due to closer distributional similarity between the AIME 2024 dataset and the DAPO-17k training set. This further supports our claim that vanilla RL tends to overfit to the training distribution, leading to diminished performance on out-of-distribution (OOD) tasks. RLoop, by cyclically exploring and consolidat7 RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization (a) Forgetting Rate. (b) N-gram similarity. (c) Entropy. Figure 5: (a): Analysis of RLoops mechanisms compared to vanilla RL. (a) Differential forgetting matrix (Vanilla RL Forgetting - RLoop Forgetting). Blue indicates RLoop forgets less. (b) N-gram similarity comparison, where lower values imply higher diversity. (c) Token-level policy entropy over training steps. plateauing or slightly fluctuating, mirroring the overfitting pattern of vanilla RL on micro-scale. However, the crucial difference is that each new iteration begins from superior starting point established by the RFT phase. This cyclical process allows RLoop to progressively climb to higher performance levels, escaping the terminal decline that plagues standard RL. 5.4. Why can RLoop Improve Generalization of RL? Having established RLoops superior performance in Table 1 and Figure 3, we now dissect the underlying mechanisms responsible for its improved generalization. Our analysis focuses on three key areas: catastrophic forgetting, trajectory diversity, and policy entropy. To facilitate direct comparison, we adopt the experimental setup from Section 5.3, aligning the i-th iteration of RLoop with the corresponding training window of the vanilla RL baseline (steps 200i to 200(i + 1)). Less Forgetting To quantify the difference in knowledge retention, we compute differential forgetting matrix, defined as the forgetting rate of vanilla RL minus that of RLoop. As shown in Figure 5a, blue cells indicate that RLoop forgets less (a positive outcome), while red cells indicate the opposite. The matrix is predominantly blue, providing strong visual evidence that RLoop generally suffers from less catastrophic forgetting than the standard RL baseline. deeper analysis reveals crucial distinction between intraiteration (within an RL phase) and inter-iteration (across RFT resets) forgetting. The forgetting rates within each 200-step RL phase of RLoop (the block-diagonal regions) are comparable to those of vanilla RL, exhibiting similar level of instability. However, the forgetting between iterations (the off-diagonal blocks) is substantially lower for RLoop. This indicates that the RFT phase is highly effective at consolidating knowledge and serving as stable anchor, preventing the long-term, catastrophic forgetting that plagues uninterrupted RL training. Better Trajectory Diversity We next examine trajectory diversity by comparing the n-gram similarity of generated solutions, metric inversely proportional to diversity. The process of estimating n-gram similarity is shown in Appendix A. Figure 5b shows that RLoop consistently maintains lower average n-gram similarity than vanilla RL throughout the training process. Since lower similarity corresponds to higher diversity, this result demonstrates that RLoop promotes more diverse set of generated solutions. This heightened diversity is key factor contributing to RLoops superior generalization and, in particular, its significantly improved pass@k performance. High Entropy Policy entropy is widely regarded as proxy for exploration in RL (Wang et al., 2025a; Cui et al., 2025; Liu et al., 2025b). We therefore compare the tokenlevel entropy of policies trained with RLoop and vanilla RL. As shown in Figure 5c, the entropy for both methods generally increases over time, and crucially, RLoop maintains policy entropy comparable to that of vanilla RL. It suggests that RLoops benefits of reduced forgetting and increased diversity are achieved without sacrificing policy exploration. 5.5. RLoop Improves Training Stability well-documented challenge in prolonged RL fine-tuning of LLMs is training instability, often manifesting as gradient explosion and catastrophic training collapse (Yao et al., 2025; He & Lab, 2025; Ma et al., 2025b; Liu et al., 2025a). Our experiments with vanilla RL confirm this issue; we 8 RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization (a) Gradient Norm of Vanilla RL (b) Gradient Norm of RLoop Figure 6: Comparison of gradient norm stability. (a) Vanilla RL exhibits explosive gradient growth after 600 steps, leading to training collapse. (b) RLoop maintains stable, low gradient norm throughout its iterative training process. observed training collapse around 750 steps, preceded by an uncontrolled surge in the gradient norm. We find that our RLoop framework inherently mitigates this instability. Figure 6 provides clear illustration of this effect. The gradient norm for the vanilla RL baseline (Figure 6a) remains manageable for approximately 600 steps before experiencing explosive growth, quickly exceeding 50.0 and causing the training process to fail. In stark contrast, the gradient norm for RLoop (Figure 6b) remains remarkably stable and bounded. Even after three full iterations, corresponding to total of 800 RL steps, the norm stays below 0.3, demonstrating the frameworks robustness against the instabilities that plague standard RL. The source of this stability lies in RLoops cyclical reset mechanism. Instead of single, prolonged optimization trajectory, RLoop performs series of shorter, bounded RL explorations. Crucially, each exploration phase begins from refreshed policy. This policy is not the potentially unstable endpoint of the previous RL phase, but rather new model created by fine-tuning the original, stable base model on small, high-quality dataset of expert trajectories. This periodic re-anchoring to stable base prevents the policy from drifting into volatile regions of the parameter space. In contrast, the vanilla RL process, after 750 steps (equivalent to approximately 45 epochs over the training data), is likely over-optimizing on fixed dataset, leading to the observed gradient explosion. 6. Conclusion In this work, we identified RL overfitting as critical challenge in RLVR, linking it to catastrophic forgetting and the under-utilization of inter-step policy diversity. To address this, we proposed RLoop, an iterative framework that transforms RLs instability into strength by cyclically alternating between an RL exploration phase to generate diverse solutions and an RFT exploitation phase to consolidate knowledge. Our experiments demonstrate that RLoop significantly outperforms vanilla RL, particularly in pass@k metrics, by mitigating long-term forgetting, enhancing solution diversity, and ensuring training stability. By reframing training instability as valuable source of exploration, our work offers robust and principled solution to current RL challenges and paves the way for more stable, generalizable, and powerful reasoning models."
        },
        {
            "title": "References",
            "content": "Anonymous. Generalization of RLVR using causal reaIn Submitted to The Fourteenth soning as testbed. International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=DZjbL9BuHs. under review. Chen, H., Feng, Y., Liu, Z., Yao, W., Prabhakar, A., Heinecke, S., Ho, R., Mui, P., Savarese, S., Xiong, C., and Wang, H. Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding. CoRR, abs/2411.04282, 2024. doi: 10.48550/ARXIV. 2411.04282. URL https://doi.org/10.48550/ arXiv.2411.04282. Chen, L., Han, X., Shen, L., Bai, J., and Wong, K. Beyond two-stage training: Cooperative SFT and RL for LLM reasoning. CoRR, abs/2509.06948, 2025. doi: 10.48550/ ARXIV.2509.06948. URL https://doi.org/10. 48550/arXiv.2509.06948. Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q. V., Levine, S., and Ma, Y. SFT 9 RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization memorizes, RL generalizes: comparative study of foundation model post-training. CoRR, abs/2501.17161, 2025. doi: 10.48550/ARXIV.2501.17161. URL https: //doi.org/10.48550/arXiv.2501.17161. and Schmidt, L. Openthoughts: Data recipes for reasoning models. CoRR, abs/2506.04178, 2025. doi: 10.48550/ARXIV.2506.04178. URL https://doi. org/10.48550/arXiv.2506.04178. Cui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., Li, H., Fan, Y., Chen, H., Chen, W., Liu, Z., Peng, H., Bai, L., Ouyang, W., Cheng, Y., Zhou, B., and Ding, N. The entropy mechanism of reinforcement learning for reasoning language models. CoRR, abs/2505.22617, 2025. doi: 10.48550/ARXIV.2505.22617. URL https: //doi.org/10.48550/arXiv.2505.22617. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., and Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. URL https: //doi.org/10.48550/arXiv.2501.12948. Gao, B., Song, F., Yang, Z., Cai, Z., Miao, Y., Dong, Q., Li, L., Ma, C., Chen, L., Xu, R., Tang, Z., Wang, B., Zan, D., Quan, S., Zhang, G., Sha, L., Zhang, Y., Ren, X., Liu, T., and Chang, B. Omni-math: universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https:// openreview.net/forum?id=yaqPf0KAlN. Guha, E. K., Marten, R., Keh, S., Raoof, N., Smyrnis, G., Bansal, H., Nezhurina, M., Mercat, J., Vu, T., Sprague, Z., Suvarna, A., Feuer, B., Chen, L., Khan, Z., Frankel, E., Grover, S., Choi, C., Muennighoff, N., Su, S., Zhao, W., Yang, J., Pimpalgaonkar, S., Sharma, K., Ji, C. C., Deng, Y., Pratt, S. M., Ramanujan, V., Saad-Falcon, J., Li, J., Dave, A., Albalak, A., Arora, K., Wulfe, B., Hegde, C., Durrett, G., Oh, S., Bansal, M., Gabriel, S., Grover, A., Chang, K., Shankar, V., Gokaslan, A., Merrill, M. A., Hashimoto, T., Choi, Y., Jitsev, J., Heckel, R., Sathiamoorthy, M., Dimakis, A. G., Defeating nondeterHe, H. and Lab, T. M. Thinking Machines minism in llm inference. Lab: Connectionism, 2025. 10.64434/tml. doi: 20250910. https://thinkingmachines.ai/blog/defeatingnondeterminism-in-llm-inference/. (eds.), Proceedings of Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with In Vanschoren, J. and Yeung, the MATH dataset. Information S. Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, URL https: virtual, 2021. December 2021, //datasets-benchmarks-proceedings. neurips.cc/paper/2021/hash/ be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2. html. the Neural Kingma, D. P. and Welling, M. Auto-encoding variational bayes, 2022. URL https://arxiv.org/ abs/1312.6114. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems 35: Annual Conference Information Processing Systems 2022, on Neural NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers. nips.cc/paper_files/paper/2022/hash/ 18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference. html. Li, J., Lu, H., Wen, K., Yang, Z., Gao, J., Lin, H., Wu, Y., and Zhang, J. Questa: Expanding reasoning capacity in llms via question augmentation. CoRR, abs/2507.13266, 2025a. doi: 10.48550/ARXIV.2507.13266. URL https: //doi.org/10.48550/arXiv.2507.13266. Li, Z., Chen, C., Yang, T., Ding, T., Sun, R., Zhang, G., Huang, W., and Luo, Z.-Q. Knapsack rl: Unlocking exploration of llms via optimizing budget allocation, 2025b. URL https://arxiv.org/abs/2509.25849. Liang, X., Li, Z., Gong, Y., Shen, Y., Wu, Y. N., Guo, Z., and Chen, W. Beyond pass@1: Selfplay with variational problem synthesis sustains RLVR. 10 RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization CoRR, abs/2508.14029, 2025. doi: 10.48550/ARXIV. 2508.14029. URL https://doi.org/10.48550/ arXiv.2508.14029. 2506.01939. URL https://doi.org/10.48550/ arXiv.2506.01939. Wang, Y., Yang, Q., Zeng, Z., Ren, L., Liu, L., Peng, B., Liu, J., Li, Y., Fu, Y., Wang, J., Liu, Q., and Shen, Cheng, H., He, X., Wang, K., Gao, J., Chen, W., Wang, Y. When speed kills stability: Demystifying rl S., Du, S. S., and Shen, Y. Reinforcement learning for collapse inference-training mismatch, reasoning in large language models with one training 2025a. URL https://yingru.notion.site/ example. CoRR, abs/2504.20571, 2025b. doi: 10.48550/ When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Inference-Training-Mismatch-271211a558b7808d8b12d403fd15edda. ARXIV.2504.20571. URL https://doi.org/10. 48550/arXiv.2504.20571. from the Liu, M., Diao, S., Lu, X., Hu, J., Dong, X., Choi, Y., Kautz, J., and Dong, Y. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. CoRR, abs/2505.24864, 2025b. doi: 10.48550/ARXIV. 2505.24864. URL https://doi.org/10.48550/ arXiv.2505.24864. Ma, L., Liang, H., Qiang, M., Tang, L., Ma, X., Wong, Z. H., Niu, J., Shen, C., He, R., Cui, B., and Zhang, W. Learning what reinforcement learning cant: Interleaved online fine-tuning for hardest questions. CoRR, abs/2506.07527, 2025a. doi: 10.48550/ARXIV.2506.07527. URL https: //doi.org/10.48550/arXiv.2506.07527. Ma, W., Zhang, H., Zhao, L., Song, Y., Wang, Y., Sui, Z., and Luo, F. Stabilizing moe reinforcement learning by aligning training and inference routers, 2025b. URL https://arxiv.org/abs/2510.11370. Wang, Z., Zhou, F., Li, X., and Liu, P. Octothinker: Mid-training incentivizes reinforcement learning scaling. CoRR, abs/2506.20512, 2025c. doi: 10.48550/ARXIV. 2506.20512. URL https://doi.org/10.48550/ arXiv.2506.20512. Yan, J., Li, Y., Hu, Z., Wang, Z., Cui, G., Qu, X., Cheng, Y., and Zhang, Y. Learning to reason under off-policy guidance. CoRR, abs/2504.14945, 2025. doi: 10.48550/ ARXIV.2504.14945. URL https://doi.org/10. 48550/arXiv.2504.14945. Yao, F., Liu, L., Zhang, D., Dong, C., Shang, J., and Gao, J. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https://fengyao. notion.site/off-policy-rl. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers. nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference. html. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV. 2503.14476. URL https://doi.org/10.48550/ arXiv.2503.14476. Yuan, X., Chen, X., Yu, T., Shi, D., Jin, C., Lee, W., and Mitra, S. Mitigating forgetting between supervised and reinforcement learning yields stronger reasoners, 2025. URL https://arxiv.org/abs/2510.04454. Prakash, J. and Buvanesh, A. What can you do https: when you have zero rewards during rl? //spiffy-airbus-472.notion.site/ What-Can-You-Do-When-You-Have-Zero-Rewards-During-RL-260429bdb7308024b6bdd3ed4f64c15f, 2025. Notion Blog. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? CoRR, abs/2504.13837, 2025. doi: 10.48550/ ARXIV.2504.13837. URL https://doi.org/10. 48550/arXiv.2504.13837. Wang, S., Yu, L., Gao, C., Zheng, C., Liu, S., Lu, R., Dang, K., Chen, X., Yang, J., Zhang, Z., Liu, Y., Yang, A., Zhao, A., Yue, Y., Song, S., Yu, B., Huang, G., and Lin, J. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for LLM reasoning. CoRR, abs/2506.01939, 2025a. doi: 10.48550/ARXIV. 11 Zhang, S., Liu, Q., Qin, G., Naumann, T., and Poon, H. Medrlvr: Emerging medical reasoning from 3b base model via reinforcement learning. CoRR, abs/2502.19655, 2025a. doi: 10.48550/ARXIV.2502.19655. URL https: //doi.org/10.48550/arXiv.2502.19655. RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization Zhang, W., Xie, Y., Sun, Y., Chen, Y., Wang, G., Li, Y., Ding, B., and Zhou, J. On-policy RL meets off-policy experts: Harmonizing supervised fine-tuning and reinforcement learning via dynamic weighting. CoRR, abs/2508.11408, 2025b. doi: 10.48550/ARXIV.2508.11408. URL https: //doi.org/10.48550/arXiv.2508.11408. Zhong, H., Yin, Y., Zhang, S., Xu, X., Liu, Y., Zuo, Y., Liu, Z., Liu, B., Zheng, S., Guo, H., Wang, L., Hong, M., and Wang, Z. Brite: Bootstrapping reinforced thinking process to enhance language model reasoning. CoRR, abs/2501.18858, 2025. doi: 10.48550/ARXIV. 2501.18858. URL https://doi.org/10.48550/ arXiv.2501.18858. A. Detailed Calculation of Trajectory"
        },
        {
            "title": "Similarity",
            "content": "This appendix provides the detailed methodology for computing the n-gram-based trajectory similarity score, as referenced in Section 3. To quantify the lexical consistency between different reasoning steps, we compute an n-gram-based similarity score. This metric evaluates the textual overlap between the set of solutions generated at two distinct steps, denoted as step and step j. For each input prompt, our model generates unique solutions. Consequently, for single prompt, we have two sets of solutions: Si = {si,1, si,2, . . . , si,N } for step i, and Sj = {sj,1, sj,2, . . . , sj,N } for step j. The calculation proceeds in two main stages: 1. Pairwise Solution Similarity via Jaccard Index First, we define the similarity between any pair of individual solutions, one from each step (si,a Si and sj,b Sj). This is based on their shared n-grams (we use bigrams, = 2, in our implementation). For each solution s, we generate set of its n-grams, denoted as Gn(s). The similarity between two solutions is then calculated using the Jaccard similarity coefficient: J(si,a, sj,b) = Gn(si,a) Gn(sj,b) Gn(si,a) Gn(sj,b) This value measures the proportion of shared n-grams relative to the total unique n-grams across both solutions. 2. Overall Step Similarity via Averaging To obtain single similarity score for given prompt, we compute the Jaccard similarity for all possible pairs of solutions between step and step j. The final similarity for that prompt, Simprompt(Si, Sj), is the arithmetic mean of these 2 pairwise scores: Simprompt(Si, Sj) = 1 2 (cid:88) (cid:88) a=1 b=1 J(si,a, sj,b) This aggregation provides robust measure of the overall similarity between the two sets of solutions. The final reported similarity between step and step is the average of these Simprompt scores across all prompts in the dataset."
        }
    ],
    "affiliations": [
        "Fudan University",
        "M-A-P",
        "Shanghai Innovation Institute"
    ]
}