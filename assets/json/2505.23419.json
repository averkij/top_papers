{
    "paper_title": "SWE-bench Goes Live!",
    "authors": [
        "Linghao Zhang",
        "Shilin He",
        "Chaoyun Zhang",
        "Yu Kang",
        "Bowen Li",
        "Chengxing Xie",
        "Junhao Wang",
        "Maoquan Wang",
        "Yufan Huang",
        "Shengyu Fu",
        "Elsie Nallipogu",
        "Qingwei Lin",
        "Yingnong Dang",
        "Saravan Rajmohan",
        "Dongmei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover a narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present \\textbf{SWE-bench-Live}, a \\textit{live-updatable} benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by a dedicated Docker image to ensure reproducible execution. Central to our benchmark is \\method, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate a range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing a fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings."
        },
        {
            "title": "Start",
            "content": "SWE-bench Goes Live! Linghao Zhang1 Shilin He1 Chaoyun Zhang1 Yu Kang1 Bowen Li2 Chengxing Xie2 Junhao Wang1 Maoquan Wang1 Yufan Huang1 Shengyu Fu1 Elsie Nallipogu1 Qingwei Lin1 Yingnong Dang1 1Microsoft 2Shanghai Artificial Intelligence Laboratory Saravan Rajmohan1 Dongmei Zhang1 5 2 0 2 9 2 ] . [ 1 9 1 4 3 2 . 5 0 5 2 : r Leaderboard GitHub HuggingFace"
        },
        {
            "title": "Abstract",
            "content": "The issue-resolving task, where model generates patches to fix real-world bugs, has emerged as critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present SWE-bench-Live3, live-updatable benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by dedicated Docker image to ensure reproducible execution. Central to our benchmark is REPOLAUNCH, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have fundamentally reshaped the landscape of software engineering [1], powering tools such as Cursor [2] and GitHub Copilot [3] that are now integral to modern development workflows. These models have transformed key stages of the software development lifecycleautomated code generation, bug detection, and issue resolutionleading to substantial gains in developer productivity. To systematically assess LLM capabilities across these tasks, variety of curated benchmarks have been developed, including HumanEval [4], MBPP [5], SWE-bench [6], DI-Bench [7], and OpenRCA [8]. These benchmarks are instrumental in identifying both the strengths and limitations of LLMs in diverse programming and maintenance settings. Among them, SWE-bench [6] and its variants, such as Multimodal SWE-bench [9] and Multi-SWEbench [10], have become standard for evaluating LLMs on the issue resolution task, where models Work done during the internship at Microsoft Shilin He is the corresponding author 3Homepage: Code: SWE-bench-Live, Dataset: https://huggingface.co/SWE-bench-Live https://swe-bench-live.github.io/, https://github.com/ Preprint. Figure 1: The automatic construction pipeline of SWE-bench-Live. are required to comprehend complex codebases, interact with execution environments, and generate patches that fix real-world issues. However, as LLMs evolve rapidly, existing benchmarks exhibit several critical limitations that undermine their continued utility: Table 1: Comparison with existing issue resolving benchmarks. Dataset Date #Instances #Repository Real/Synthetic Curation SWE-bench [6] Oct, 2023 SWE-bench-Verified [11] Aug, 2024 Dec, 2024 Apr, 2025 Apr, 2025 SWE-Gym [12] Multi-SWE-bench [10] SWE-smith [13] 2,294 500 2,438 1,632 50,000 SWE-bench-Live (Ours) April, 2025 1,319 (since 2024) 12 12 11 39 128 93 Real Real Real Real Synthetic Manual Manual Manual Manual Semi-manual Real Automatic 1. Staleness. SWE-bench and its derivatives have not been updated since their initial releases (Oct, 2023), making them static benchmarks. Because LLMs are trained on massive inscrutable corpora, these static datasets are at risk of data leakage, as they could have be been unpurposely included in model training data. This raises concerns about whether newer models are making truly generalizable progress or merely memorizing benchmark content, reducing the benchmarks effectiveness in distinguishing model capabilities. 2. Limited repository coverage. These benchmarks draw from small set of 12 repositories, limiting diversity in codebases, domains, and programming practices (see Table 1 for details). This narrow scope weakens the generalizability and robustness of evaluations. 3. Heavy reliance on manual effort. Constructing instances for SWE-bench-like task istances involves substantial human labor: identifying appropriate issue-resolution pairs, locating relevant tests, configuring runnable environments, composing test commands, and validating the full workflow.4 This process is resource-intensive and creates scalability bottlenecks. To address these challenges, we introduce SWE-bench-Live, live, automated, and scalable benchmark designed to evaluate LLMs on real-world issue resolution tasks. Unlike recent efforts such as LiveCodeBench [14], which focus on algorithmic programming problems, SWE-bench-Live is the first continuously updating benchmark tailored for complex, repository-level tasks that require multi-file reasoning, environment setup, and reproducible execution. Figure 1 presents the construction pipeline of SWE-bench-Live. At the heart of this framework lies REPOLAUNCH, fully automated system that eliminates manual bottlenecks by streamlining the entire processfrom issue mining to environment packaging. Specifically, REPOLAUNCH employs an agentic, end-to-end workflow to set up Docker-based environments by identifying relevant instruction files, selecting appropriate base images, installing dependencies, building the project, and validating its test suite. This automation enables continuous updates, broad repository coverage, and scalable dataset expansion. The current release of SWE-bench-Live includes 1,319 issue-resolution tasks sourced from real-world GitHub issues created since 2024, spanning 93 repositories. Compared to 4For instance, it take about one year for Multi-SWE-bench [10] to create 1,632 benchmark instances with 68 expert annotators. 2 existing benchmarks, this marks significant advancement in freshness, diversity, and scale (see Table 1). We evaluate three leading agent frameworks (i.e., OpenHands [15], SWE-Agent [16], and Agentless [17]) in combination with four state-of-the-art LLMs (namely, GPT-4.1, GPT-4o, Claude 3.7 Sonnet, and DeepSeek V3). Our results reveal substantial performance gap between SWE-benchLive and prior static benchmarks. For instance, the best-performing agentmodel combination, OpenHands with Claude 3.7 Sonnet, achieves resolved rate of only 19.25% on SWE-bench-Live. Even under controlled conditions with identical evaluation protocols, the same setup yields resolved rate of 43.20% on SWE-bench Verified, more than double its performance on SWE-bench-Live. Further breakdowns by repository origin and instance difficulty suggest that this discrepancy stems not only from benchmark familiarity but also from the greater diversity of SWE-bench-Live. These findings highlight the limitations of static, manually curated benchmarks and underscore the importance of dynamic, automatically updated testbeds for advancing robust and generalizable code agent systems. Our main contributions are summarized as follows: We introduce SWE-bench-Live, contamination-resistant, reproducible, and continuously updatable benchmark tailored to real-world issue resolution tasks. It reflects the dynamic nature of software development and offers broader repository coverage compared to prior benchmarks. We propose REPOLAUNCH, fully automated pipeline for benchmark construction that seamlessly integrates data curation, environment setup, and test validation into cohesive and scalable system. Through experimental evaluation, we observe the suboptimal performance of leading agent frameworks on SWE-bench-Live, highlighting significant opportunities for improvement on the contamination-free benchmark. Together, these contributions establish new and robust standard for evaluating the true capabilities of code LLMs and agent-based systems."
        },
        {
            "title": "2 Related Work",
            "content": "Coding Benchmarks. Early benchmarks for program synthesis and bug fixing focused on single-file, synthetic tasks such as HumanEval [4] and MBPP [5], which do not reflect the complexity of real repositories. To move closer to practice, SWE-bench [6] introduced the issue-resolving task, requiring model to generate validated patch for GitHub repositories issue. Numerous extensions have since appearedincluding Multimodal SWE-bench for JavaScript and UI screenshots [9], MultiSWE-bench for multiple languages such as Java and Rust [10]. Despite their impact, all of these datasets are static: they are collected once, cover at most few dozen repositories, and depend on labor-intensive environment construction. These yield two limitations. First, models can overfit to the fixed test set, inflating apparent progress. Second, public tasks may lead to data contamination, where benchmark instances leak into pre-training corpora [18, 19]. Recent live datasets such as LiveCodeBench [14] mitigate contamination by streaming algorithmic problems after their release dates, yet they do not address the harder repository-level setting that demands multi-file reasoning and execution inside faithful environment. SWE-bench-Live is the first open, continuously updating benchmark that fulfills these requirements. Coding Agents. On top of the above benchmarks, recent line of work has been working creating autonomous code agents that search, edit, and test large codebases. Representative systems include SWE-Agent [20], OpenHands [15], Agentless [17], and training frameworks that synthesize thousands of SWE-bench-like instances [21, 13, 22]. These agents report remarkable headline numbers, yet their evaluations rely almost exclusively on static offline datasets. As consequence, improvements may partially stem from memorisation of leaked solutions or configuration quirks, rather than genuine advances. SWE-bench-Live closes this gap by pushing agents to fix previously unseen, continuously arriving real-world bugs under fully reproducible Docker images, it reveals failure modes hidden by stale test suites and provides trustworthy yard-stick for code agents and LLMs."
        },
        {
            "title": "3 SWE-bench-Live",
            "content": "Targeting the issue resolution task on real-world GitHub repositories, SWE-bench serves as practical proxy for evaluating the coding capabilities of LLM-based systems. The issue resolving task is defined as follows: given code repository and an associated issue, an approach (e.g., LLM agent) is required to generate patch that resolves the issue and passes the test cases. While SWE-bench-Live adopts the same task definition as SWE-bench, it introduces novel, fully automated pipeline that enables scalable and continuously updatable benchmark construction. This automation allows for larger number of up-to-date instances and broader repository coverage. The initial release of SWE-bench-Live consists of 1,319 task instances created between January 2024 and April 2025, spanning 93 real-world repositories. Pipeline Overview. As shown in Figure 1, the construction of SWE-bench-Live follows threestage pipeline. First, starting from popular repositories, we identify GitHub issues that are resolved by pull request (PR). Next, we apply the proposed REPOLAUNCHan agentic approach that automatically sets up an Docker-based execution environment for each candidate instance. Finally, we perform multiple rounds of test execution for each instance to validate whether it consistently exhibits the expected issue-resolving testing behavior, and finalize the valid instances. Thanks to its fully automated pipeline, SWE-bench-Live can be maintained with minimal, ideally zero manual effort. We plan to update SWE-bench-Live on monthly basis, continually providing the community with an up-to-date evaluation dataset. This enables contamination-free, rigorous assessment of AI systems issue-resolving capabilities in constantly evolving real-world setting. 3.1 Task Formulation Figure 2: The issue-resolving task requires the model to generate patch that addresses given issue, with its correctness evaluated through test execution. Issue resolving is the task that introduced by SWE-bench [6] for benchmarking AI coding capabilities. In simple terms, it simulates the process of developer submitting pull request to address an issue. The formulation of the issue-resolving task is illustrated in Figure 2. Generating Patch. The task input includes the problem statement of the issue, which is the description written by the issue reporter, as well as snapshot of the codebase at the time the issue was filed (obtained by resetting to the base_commit). The model has access to full content of the codebase, after then it is tasked with generating patch that fixes the given issue, analogous to the file changes submitted in pull request. In practice, the expected output is in the .diff format. Evaluating Patch. Once patch is proposed by the model, we assess its correctness by applying it to the target codebase and executing the repositorys test suite. The output of the test execution are parsed using log parser function, which extracts the status of each individual test case. These results are then compared against the expected test case transitions pre-defined for the issue, specifically FAIL_TO_PASS and PASS_TO_PASS. FAIL_TO_PASS refers to test cases that originally failed prior to patch applicationtypically those introduced in the corresponding pull requestand are expected to pass if the proposed solution is correct. correct patch should successfully cause these failing tests to pass, without causing regressions in the already passing tests. 4 3.2 Raw IssuePR Crawling The first phase of the SWE-bench-Live pipeline involves collecting real-world issuePR pairs from popular open-source GitHub repositories. Repository Selection. We focus on Python repositories for the initial release of SWE-bench-Live, aligning with SWE-bench and other prior benchmarks, as Python is one of the most widely used languages in open-source development and well-supported by existing LLMs. The selection process consists of three filtering stages, designed to ensure both the quality and relevance of benchmark data: We first queried the GitHub API for repositories with over 1,000 stars and Python set as the primary language. The star threshold helps identify mature and actively maintained projects with large user base, while language filtering ensures consistency in model evaluation. This initial query yielded 8,577 repositories as of April 2025. To further ensure repository richness and task availability, we refined this set by requiring each repository to have more than 200 combined issues and pull requests, over 200 forks (as proxy for community engagement and adoption), and at least 60% of its codebase written in Python (to avoid polyglot or non-representative projects). This reduced the pool to 3,316 repositories. Finally, to comply with legal and ethical usage guidelines, we retained only repositories containing valid open-source license, resulting in final selection of 2,609 repositories suitable for benchmark construction. IssuePR Pair Extraction. From the selected repositories, we adopt the collection script from SWE-bench to extract issue and its associated PR. Meanwhile, the pull request must modify the repositorys test suitei.e., test patch, which will serve as the evaluation targets. We also incorporate improvements from SWE-Fixer [22], which introduces more robust heuristics to improve the effectiveness of issuePR pair identification and reduce reliance on the brittle string-matching method. To reduce the risk of data leakage, SWE-bench-Live prioritizes recency by including only issues created after January 2024 in our initial release. 3.3 REPOLAUNCH: Automated Execution Environment Setup critical requirement for test-based evaluation in issue-resolution benchmarks is the availability of faithful execution environmentone that reproduces the exact conditions under which bug was introduced. While issuePR pairs provide textual and code-level artifacts, without an execution environment, these cannot be tested or validated in reproducible manner. However, constructing such environments remains one of the most challenging and labor-intensive bottlenecks in dataset creation. Prior work such as SWE-bench [6] and SWE-Gym [12] rely entirely on manual setup, with SWE-Gym reporting over 200 hours of effort for relatively small dataset. This challenge is exacerbated by three key factors: (i) lack of standard build instructions across repositories, (ii) version drift and dependency incompatibilities due to time decay, and (iii) the need for fine-grained environments at the snapshot level, since different commits may require different setups even within the same repository. To address this scalability bottleneck, we introduce REPOLAUNCH, an LLM-powered agentic framework that automates the end-to-end construction of Dockerized execution environments for arbitrary repository snapshots. Unlike prior approaches that rely on scripted heuristics or manual curation, REPOLAUNCH simulates the trial-and-error reasoning process of human developers using an interactive, ReAct-style agent [23, 24]. Repository Snapshots and Environment Definition. Each benchmark instance is tied to repository snapshot defined by the base commit of an issue. An execution environment is considered valid if (i) the project builds from source and (ii) its test suite passes with zero or acceptable failures. This environment serves as the ground truth for verifying the correctness of model-generated patches. Automated Agentic Workflow. At the core of REPOLAUNCH lies an agentic workflow that emulates how human developers approach unfamiliar codebases: reading documentation, iteratively issuing commands, debugging errors, and refining their strategy based on feedback. Unlike static scripts or prompt-only approaches, our design instantiates multi-role, interactive agent framework grounded in the ReAct paradigm [23, 24]. This workflow is composed of two decoupled but coordinated agents: setup agent that incrementally builds the environment, and verification agent that checks for correctness. This separation of concerns provides modularity, simplifies control logic, and enables scalable error recovery.As shown in Figure 1, REPOLAUNCH proceeds through five key stages: Relevant Files Identification. The first step is to identify relevant files in the repository such as CI/CD pipelines and README files that are likely to contain useful information for setting up the environment (a detailed list is provided in the Appendix E). Base Image Selection. Given the full content of the relevant files, this step is to select suitable base Docker image based on the information provided in the repository. This involves correctly identifying the programming language and SDK version used in the repository (e.g., python:3.11). container is instantiated from the chosen image, and persistent bash session is launched. Interactive Environment Setup. The setup process is carried out by an agent whose goal is to successfully execute and pass all test cases in the repositorys test suite within the container. The agent interacts with the bash session by issuing commands and receiving feedback such as exit codes and outputs. It follows the ReAct design [23], iterating over Thought Action Observation [24, 25], mimicking developers reasoning and trial process. The agent can also search the web or query the issue tracker for troubleshooting. Verification Agent. Once the setup agent signals readiness or hits step limit, separate verification agent takes over. Its role is to determine the appropriate test command (e.g., pytest, tox, or custom script), execute it, and evaluate results. If test failures are observed, the feedback is passed back to the setup agent, which re-enters the ReAct loop to repair the environment. This design enables retryable, stateful, and verifiable setup. Finalization. Once all tests pass, the resulting container is committed as snapshot-specific Docker image. This ensures reproducibility and enables downstream usage for model evaluation. The agentic workflow in REPOLAUNCH is principled and modular design that goes beyond simple command execution. It separates setup and verification logic for maintainability, supports iterative self-correction to handle noisy or incomplete metadata, and mirrors real developer behavior for greater robustness. Combined with time-aware dependency control and snapshot-level granularity, this design enables the first fully automated, scalable benchmark pipeline for real-world issue resolution. Time Machine for Version Incompatibility Resolution. significant challenge in setting up environments for historical repository snapshots is dependency version drift, where unpinned dependencies are resolved to the latest package versions, leading to backward incompatibilities that frequently break the build. This is particularly problematic for older codebases that were never tested against modern package ecosystems. To address this, we introduce time-machine mechanism that constrains the dependency resolver to only consider package versions released prior to the snapshots base commit timestamp. Concretely, we modify the default pip index server to route through custom proxy that filters packages by release date. This simple yet effective solution mitigates the risk of future incompatibilities and substantially improves environment setup success rates across out-of-date repositories. We open-source REPOLAUNCH5 to benefit the broader community. While originally developed to automate the most labor-intensive stage of benchmark construction, namely environment setup, REPOLAUNCH significantly reduces the need for manual intervention, enabling scalable and reproducible benchmarking at scale. Beyond this primary use case, REPOLAUNCH also serves as practical tool for developers, offering fast and reliable way to set up execution environments for unfamiliar or legacy codebases. Its ability to reconstruct historical setups and resolve dependencies automatically makes it broadly useful in both research and real-world development workflows. 5https://github.com/microsoft/SWE-bench-Live 6 Figure 3: Temporal distribution of issue creation times in SWE-bench-Live. 3.4 Validating Task Instances To ensure the quality of the benchmark, each task instance is further validated to confirm that the associated PR effectively resolves the issue it is intended to fix. The validation is based on analyzing changes in the test suite results before and after applying the PRs patch. Specifically, we focuses on identifying two key behaviors in the test outcomes: FAIL_TO_PASS transitions: Tests that were initially failing (FAILED or ERROR) and later passing (PASSED) after the patch is applied. These yield that the patch addresses the issue effectively. PASS_TO_PASS transitions: Tests that were both passing before and after the patch is applied. These transitions demonstrate that the patch does not break unrelated functionality. To identify these transitions, the test results (as logs) are collected both before and after applying the PRs patch. By comparing individual test outcomes between the two runs, we determine how the patch affected specific tests. We designed framework-specific (e.g., tox, pytest) parsers to interpret test outputs reliably, as different testing tools may produce logs in various formats. For task instance to be included in the benchmark, it must exhibit at least one FAIL_TO_PASS transition. Instances lacking such transition are excluded because they do not demonstrate effective bug resolution. Additionally, to ensure reproducibility and avoid issues caused by test flakiness, the validation process is repeated multiple times. Only instances with consistent results across all runs are retained. This approach ensures that all task instances are grounded in evidence of real-world bug fixes and preserves stable behaviors, resulting in robust benchmark for evaluating automated bug-fixing solutions. 3.5 SWE-bench-Live Statistics The initial release of the SWE-bench-Live dataset consists of 1,319 task instances collected from real-world issues and pull requests across 93 open-source Python repositories. To ensure freshness and reduce the risk of data contamination from pretraining, we restrict the dataset to issues created between January 1, 2024, and April 20, 2025. As shown in Figure 3, the temporal distribution is generally uniform, indicating consistent coverage of issues over time. We plan to update the dataset on monthly basis to reflect the evolving software landscape and continuously provide new instances. Table 2 summarizes key statistics at both the repository and instance levels. At the repository level, projects vary in size, with an average of 85k lines of Python code and 423 files. At the instance level, we report metrics of the gold patchesincluding the number of edited files, hunks, and linesas heuristic indicators of task complexity. These statistics suggest that SWE-bench-Live tasks reflect realistic, non-trivial bug fixes that challenge code understanding, reasoning, and manipulation capabilities of LLMs. Additionally, we record the number of test cases that transition from failure to pass (F2P) and those that consistently pass (P2P), which form the basis of test-based evaluation. Repository Diversity. To ensure broad applicability, SWE-bench-Live includes repositories from diverse application domains. As shown in Figure 4, we manually categorized each repository based 7 Table 2: Statistics of SWE-bench-Live. *: Python code only. : Statistics of gold patches. Level #Item Average Median R a I Repositories LoC* Files* Instances Files Hunks Lines F2P test cases P2P test cases 85k 423 1319 3.3 9.0 102.6 5.4 2953.4 52k 222 2 3 24 1 1865 Figure 4: Repository distribution. on its primary functionalitysuch as AI/ML, DevOps, Web development, and others. This diversity helps evaluate LLMs across varied software stacks and bug types, enhancing the benchmarks representativeness of real-world usage scenarios. Lite Subset. To support lightweight experimentation, we construct lite subset of SWE-bench-Live, referred to as SWE-bench-Live-Lite, by sampling 50 instances per month from issues created between October 2024 and March 2025. This results in compact set of 300 instances that balances recency, diversity, and evaluation efficiency. Comparison with Existing Benchmarks. Table 1 compares SWE-bench-Live with several existing issue-resolution benchmarks. Unlike SWE-bench and its variants, which require extensive manual curation and cover limited set of repositories, SWE-bench-Live is the first to offer an automatically constructed, continuously updatable benchmark. It covers broader set of repositories (93 in total), while preserving the use of real issues and test-based evaluation. Compared to synthetic datasets like SWE-smith, which may not fully capture the complexity of human-written code and bugs, SWE-bench-Live maintains fidelity to real-world development workflows. Its unique combination of automation, realism, and diversity fills critical gap of the LLM evaluation for software engineering."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setups Agents and Models. To evaluate the effectiveness of our proposed SWE-bench-Live, we conduct experiments using three representative agent frameworks. These include the general-purpose coding agent OpenHands [15] (paired with the CodeAct agent [26]), as well as two agents specifically designed for issue-resolving tasks: SWE-Agent [16] and Agentless [17]. For OpenHands, we set maximum of 60 iterations per instance. For SWE-Agent, we limit the number of LLM calls to 100 per instance to maintain computational efficiency. For Agentless, we largely follow the original pipeline, which consists of two main stages: issue localization and patch generation. However, we omit the reranking stage based on regression testing, as supporting this step on SWE-bench-Live would require substantial infrastructure adaptation and is beyond the scope of this study. Consequently, both the localization and repair stages in our Agentless evaluation produce single sample without reranking. We test these agents using four recent state-of-the-art LLMs, covering both proprietary and open-source models: GPT-4o [27] (gpt-4o-2024-11-20), GPT-4.1 [28] (gpt-4.1-2025-04-14), Claude 3.7 Sonnet [29] (claude-3-7-sonnet-20250219), and DeepSeek V3 [30] (DeepSeek-V3-0324). Evaluation Protocols. Following the evaluation protocol of SWE-bench [6], we adopt the Resolved Rate (%) as our primary metric. This measures the proportion of issues successfully resolved by the agent across all task instances. We also report the Patch Apply Rate (%), which indicates the percentage of generated patches that are syntactically correct and can be successfully applied to the 8 Table 3: Performance on SWE-bench-Live-Lite. Models Resolved (%) Apply (%) Loc. Suc. (%) GPT-4o GPT-4.1 Claude 3.7 Sonnet DeepSeek V3 GPT-4o GPT-4.1 Claude 3.7 Sonnet DeepSeek V3 GPT-4o GPT-4.1 Claude 3.7 Sonnet DeepSeek V3 OpenHands 7.00 11.33 17.67 13.00 SWE-agent 10.00 16.33 17.67 15.33 Agentless 11.67 12.00 11.33 13.33 72.00 59.33 84.00 81.00 93.33 95.00 84.67 92.00 91.67 84.33 68.00 83. 30.33 28.67 48.00 38.33 40.33 47.33 46.33 44.00 37.67 39.00 30.00 40.67 Table 4: Performance of top-3 performing Agent / Model combinations on SWE-bench-Live. Agent / Model Subset Resolved (%) Apply (%) Loc. Suc. (%) OpenHands / Claude 3.7 Sonnet SWE-agent / GPT-4.1 SWE-agent / Claude 3.7 Sonnet Lite Full Lite Full Lite Full 17.67 19.25 16.33 18.57 17.67 17.13 84.00 85.89 95.00 94.54 84.67 89. 48.00 48.29 47.33 49.50 46.33 45.86 codebase without errors. Additionally, we measure the Localization Success Rate (%) at the file level. This reflects whether the set of files modified by the generated patch matches the gold patch.6 4.2 Performance on SWE-bench-Live We report the performance of all agentmodel combinations on SWE-bench-Live-Lite in Table 3. Meanwhile, Table 4 presents the results of the top three combinations selected based on Lite performance, evaluated on SWE-bench-Live-Full. We observe that the same methods achieve substantially higher scores on SWE-bench compared to their performance on SWE-bench-Live, despite both benchmarks targeting the same issue-resolving task with identical settings. For example, recent state-of-the-art agents and models report resolved rate exceeding 60% on the SWE-bench Verified subset7. In contrast, the highest resolved rate on SWE-bench-Live is only 19.25%. Considering that the experimental setups on the SWE-bench leaderboard often involve dramatically high rollout numbers or iteration efforts, we specifically re-ran the best performing combination, OpenHands with Claude 3.7 Sonnet, on the SWE-bench verified subset using the exact same setup as in our experiments. In Table 5, the resulting resolved rate reached 43.20%, more than twice the score achieved on SWE-bench-Live. This is particularly interesting phenomenon, as it highlights the challenges of constructing benchmark that can objectively measure an AI systems ability to resolve arbitrary and previously unseen issues. It also raises concerns about potential overfitting to SWE-bench. Similar phenomena are also observed in other existing 6We note that Localization Success Rate is proxy metric, as there may be multiple valid solutions to an issue that involve modifying different code files. 7https://www.swebench.com/ 9 Table 5: Performance of OpenHands / Claude 3.7 Sonnet across different test sets under an identical evaluation setup. Agent / Model Test Set Resolved (%) OpenHands / Claude 3.7 Sonnet SWE-bench Verified SWE-bench-Live-Lite SWE-bench-Live-Full 43.20 17.67 ( 25.53) 19.25 ( 23.95) Table 6: Performance of OpenHands / Claude 3.7 Sonnet on different repository-based splits of SWE-bench-Live-Full. Instances Avg. Repo Files Avg. Repo LoC Resolved (%) From SWE-bench Repos From Non-SWE-bench Repos 744 383 223k 68k 22.96 18.89 ( 4.07) issue-resolving datasets: the best-performing method in Multi-SWE-bench achieves resolved rate of only 19.32%, while the highest score reported in OmniGIRL is as low as 8.6%. To investigate this, we further categorize the instances in SWE-bench-Live based on their repository origin. Specifically, 216 instances are drawn from 8 repositories that were originally included in SWE-bench, which we refer to as From SWE-bench Repos. The remaining 1,103 instances are sourced from repositories not included in SWE-bench, denoted as From Non-SWE-bench Repos. As shown in Table 6, although the Non-SWE-bench repositories are generally simpler with fewer files and lower code volume, the best-performing agentmodel pair, OpenHands / Claude 3.7 Sonnet, achieves higher resolved rate of 22.96% on on SWE-bench repository instances, compared to just 18.89% on those from Non-SWE-bench repositories. This discrepancy supports the hypothesis that current agents may be overfit to, or implicitly optimized for, the SWE-bench dataset, further motivating the need for continuously updated, contamination-resistant benchmarks like SWE-bench-Live. 4.3 Performance vs. Creation Date To investigate whether the recency of an issue affects its difficulty, we analyze the resolved rate across different creation periods. As shown in Figure 5, SWE-bench-Live includes balanced distribution of instances across quarters from 2024Q1 to 2025Q1. The resolved rate, based on OpenHands with Claude 3.7 Sonnet on the full benchmark, remains relatively stable over time, fluctuating only modestly across quarters. While there is slight dip in resolved rate during 2024Q4, followed by recovery in 2025Q1, the trend does not indicate clear correlation between task recency and success rate. This suggests that newer issues are not inherently harder for current agents to solve, and that SWE-bench-Live maintains consistent level of challenge across time. These results reinforce the benchmarks ability to deliver steady and reliable evaluation signal, even as it continuously evolves with newly introduced instances. 4.4 Performance vs. Difficulty We approximate the difficulty of bugfixing instance along two complementary axes. Patch difficulty is captured by the scope of the gold fixthe number of files it touches and the total lines modifiedwhile repository difficulty is approximated by the overall size of the project in files and lines of code (LoC). Patch difficulty. Figure 6 visualises resolved rate as heat-map over patch scope. Success is high when the fix is local: single-file patch that changes fewer than five lines is solved almost one time in two (48%). Performance degrades quickly as either dimension grows. Once the patch edits three or more files, or spans more than one hundred lines, the success rate falls below ten per-cent; patches that touch seven or more files are never solved. The sharp drop beyond the one-file / few-lines corner highlights key limitation of current agents: they struggle to coordinate coherent edits across multiple files or to reason about large, intra-file changes. 10 Figure 5: Resolved rate in relation to the creation date of instances. (OpenHands / Claude 3.7 Sonnet on full set) Figure 6: Resolved rate in relation to the difficulty of instances. (OpenHands / Claude 3.7 Sonnet on full set) Figure 7: Resolved rate in relation to the number of files and lines of code of repository. Repository difficulty. Figure 7 plots resolved rate for every repository against its size (Python files on the x-axis, LoC on the y-axis). Each circle represents repository. As indicated in the legend, the color intensity corresponds to the resolved rate of instances within that repository, while the size of the circle reflects the number of instances associated with the repository. Red outlines denote the original SWE-bench repositories. clear negative trend emerges: repositories with fewer than one hundred files and under twentythousand LoC often yield success rates above 20%, whereas projects exceeding five-hundred files rarely exceed five per-cent. Nevertheless, notable variance remainssome small-to-mid-size projects are still hard to fix, likely due to atypical build systems or complex domain logicemphasising that size is an informative but imperfect proxy for difficulty. Together, the two figures show that difficulty increases along both local (patch) and global (repository) dimensions, and that current code agents falter once fixes spill beyond handful of lines or involve cross-file reasoning. Because SWE-bench-Live spans the full spectrum of these difficulty factorswhile continuously adding fresh, unseen instancesit provides stringent and up-to-date testbed for future advances in large-scale program repair."
        },
        {
            "title": "5 Conclusion",
            "content": "We present SWE-bench-Live, the first continuously updating benchmark for evaluating large language models on real-world issue resolution tasks at the repository level for fresh issue fixing. By addressing key limitations of prior benchmarks such as dataset staleness, limited repository diversity, and manual curation cost, SWE-bench-Live provides scalable, contamination resistant, and fully automated evaluation framework. At its core is REPOLAUNCH, an agent based pipeline that builds reproducible Docker environments and validates issue and pull request pairs through test execution, removing the need for manual intervention. Our empirical results across multiple agent and model combinations show that SWE-bench-Live presents significantly greater challenges than static datasets. The low resolution rates, especially on multi file patches and large codebases, highlight the limitations of current systems and the importance of live benchmarks in measuring true model generalization."
        },
        {
            "title": "References",
            "content": "[1] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie Zhang. Large language models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE), pages 3153. IEEE, 2023. [2] Cursor. Cursor the ai-powered code editor, 2025. Accessed: 2025-05-14. [3] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, Michel Desmarais, and Zhen Ming Jack Jiang. Github copilot ai pair programmer: Asset or liability? Journal of Systems and Software, 203:111734, 2023. [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [5] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [6] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. [7] Linghao Zhang, Junhao Wang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Jiaheng Wen, Chengxing Xie, Maoquan Wang, Yufan Huang, et al. Di-bench: Benchmarking large language models on dependency inference with testable repositories at scale. arXiv preprint arXiv:2501.13699, 2025. [8] Junjielong Xu, Qinan Zhang, Zhiqing Zhong, Shilin He, Chaoyun Zhang, Qingwei Lin, Dan Pei, Pinjia He, Dongmei Zhang, and Qi Zhang. Openrca: Can large language models locate the root cause of software failures? In The Thirteenth International Conference on Learning Representations. [9] John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, and Ofir Press. Swe-bench multimodal: Do ai systems generalize to visual software domains?, 2024. [10] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, et al. Multi-swe-bench: multilingual benchmark for issue resolving. arXiv preprint arXiv:2504.02605, 2025. [11] OpenAI. Introducing swe-bench verified, 2025. Accessed: 2025-05-05. [12] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. 12 [13] John Yang, Kilian Leret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents, 2025. [14] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [15] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024. [16] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [17] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. [18] Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, William Song, Tiffany Zhao, Pranav Raja, Charlotte Zhuang, Dylan Slack, et al. careful examination of large language model performance on grade school arithmetic. Advances in Neural Information Processing Systems, 37:4681946836, 2024. [19] Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large language models. arXiv preprint arXiv:2308.08493, 2023. [20] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [21] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym, 2024. [22] Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training open-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040, 2025. [23] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [24] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024. [25] Chaoyun Zhang, He Huang, Chiming Ni, Jian Mu, Si Qin, Shilin He, Lu Wang, Fangkai Yang, Pu Zhao, Chao Du, et al. Ufo2: The desktop agentos. arXiv preprint arXiv:2504.14603, 2025. [26] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024. [27] OpenAI. Hello gpt-4o, 2025. Accessed: 2025-05-14. [28] OpenAI. Introducing gpt-4.1 in the api, 2025. Accessed: 2025-05-14. [29] Anthropic. Claude 3.7 sonnet and claude code, 2025. Accessed: 2025-05-14. [30] DeepSeek-AI. Deepseek-v3 technical report, 2025."
        },
        {
            "title": "A Full Repositories List",
            "content": "Type Repository License #Instances #Files LoC deepset-ai/haystack instructlab/instructlab keras-team/keras kedro-org/kedro pytorch/torchtune jupyterlab/jupyter-ai run-llama/llama_deploy stanfordnlp/dspy projectmesa/mesa huggingface/smolagents theOehrly/Fast-F1 cyclotruc/gitingest modelcontextprotocol/python-sdk camel-ai/camel hiyouga/LLaMA-Factory feast-dev/feast openai/openai-agents-python huggingface/datasets stanford-crfm/helm freqtrade/freqtrade lss233/kirara-ai arviz-devs/arviz qubvel-org/segmentation_models.pytorch scikit-learn-contrib/category_encoders huggingface/open-r1 gptme/gptme conan-io/conan pylint-dev/pylint sphinx-doc/sphinx pdm-project/pdm beeware/briefcase bridgecrewio/checkov joke2k/faker python-attrs/attrs ipython/ipython koxudaxi/datamodel-code-generator tox-dev/tox dynaconf/dynaconf pypa/twine wemake-services/wemake-python-styleguide Delgan/loguru kubernetes-client/python olofk/fusesoc amoffat/sh facebookresearch/hydra home-assistant/supervisor FreeOpcUa/opcua-asyncio pytest-dev/pytest iterative/dvc reflex-dev/reflex sissbruecker/linkding Kozea/WeasyPrint python-telegram-bot/python-telegram-bot python-babel/babel falconry/falcon aiogram/aiogram privacyidea/privacyidea urllib3/urllib3 ag2ai/faststream encode/starlette scrapinghub/dateparser 14 Apache-2.0 Apache-2.0 Apache-2.0 Apache-2.0 BSD-3-Clause BSD-3-Clause MIT MIT Apache-2.0 Apache-2.0 MIT MIT MIT Apache-2.0 Apache-2.0 Apache-2.0 MIT Apache-2.0 Apache-2.0 GPL-3.0 AGPL-3.0 Apache-2.0 MIT BSD-3-Clause Apache-2.0 MIT MIT GPL-2.0 N/A MIT BSD-3-Clause Apache-2.0 MIT MIT BSD-3-Clause MIT MIT MIT Apache-2.0 MIT MIT Apache-2.0 BSD-2-Clause MIT MIT Apache-2.0 LGPL-3.0 MIT Apache-2. Apache-2.0 MIT BSD-3-Clause GPL-3.0 BSD-3-Clause Apache-2.0 MIT AGPL-3.0 MIT Apache-2.0 BSD-3-Clause BSD-3-Clause AI/ML DevOps Web 64 52 48 27 14 13 12 10 9 5 4 3 2 2 2 2 1 1 1 1 1 1 1 1 1 1 136 57 39 34 24 21 20 10 10 10 7 6 6 6 6 3 2 2 2 2 1 1 44 29 19 16 11 11 11 10 10 6 5 2 433 142 900 179 448 81 216 222 109 65 92 39 114 799 170 673 212 207 891 458 261 259 130 71 29 124 1056 2301 718 221 508 4551 754 52 293 599 225 463 34 396 168 783 45 5 439 541 168 260 554 376 193 144 464 75 262 861 483 81 1267 66 274 84.8k 28.2k 249.7k 40.4k 92.4k 9.0k 15.1k 30.6k 20.3k 21.4k 20.7k 4.7k 13.4k 130.1k 31.2k 103.3k 29.8k 69.6k 122.1k 130.1k 25.5k 50.6k 18.6k 12.9k 4.0k 23.3k 162.5k 116.8k 140.3k 32.3k 89.3k 234.9k 351.4k 18.6k 79.8k 60.3k 23.8k 55.1k 6.6k 52.5k 19.2k 267.2k 8.8k 7.4k 41.4k 82.3k 344.4k 99.5k 85.3k 89.8k 26.4k 70.0k 140.8k 23.1k 58.5k 69.8k 167.5k 31.3k 85.1k 17.2k 67.1k Type Repository License #Instances #Files Web pallets/flask scrapy-plugins/scrapy-splash psf/requests jpadilla/pyjwt slackapi/bolt-python pydata/xarray geopandas/geopandas reata/sqllineage patroni/patroni piskvorky/smart_open wireservice/csvkit jazzband/tablib Flexget/Flexget pvlib/pvlib-python python-control/python-control mikedh/trimesh PyPSA/PyPSA shapely/shapely pybamm-team/PyBaMM beancount/beancount sympy/sympy streamlink/streamlink beetbox/beets yt-dlp/yt-dlp jarun/buku matplotlib/matplotlib fonttools/fonttools pytransitions/transitions aws-cloudformation/cfn-lint icloud-photos-downloader/icloud_photos_downloader Database Scientific CLI Misc Cloud Desktop qtile/qtile pwr-Solaar/Solaar"
        },
        {
            "title": "B Dataset Fields",
            "content": "BSD-3-Clause BSD-3-Clause Apache-2.0 MIT MIT Apache-2.0 BSD-3-Clause MIT MIT MIT MIT MIT MIT BSD-3-Clause BSD-3-Clause MIT MIT BSD-3-Clause BSD-3-Clause GPL-2.0 N/A BSD-2-Clause MIT Unlicense GPL-3.0 N/A MIT MIT MIT-0 MIT MIT GPL-2.0 2 1 1 1 1 29 21 18 17 6 3 2 2 29 15 14 10 9 6 2 2 39 9 5 2 85 12 102 4 6 3 83 25 36 26 562 226 87 103 117 64 48 32 657 178 155 248 129 158 581 194 1574 510 193 1177 904 512 39 2422 73 405 94 LoC 17.8k 3.4k 11.2k 6.9k 60.8k 179.2k 47.3k 9.7k 45.9k 12.4k 6.6k 6.6k 108.6k 59.9k 70.7k 74.8k 32.3k 34.0k 113.4k 48.0k 760.5k 84.0k 69.3k 244.6k 7.1k 263.6k 192.6k 12.4k 160.2k 15.5k 81.6k 33.7k Table 7 provides detailed description of the fields included in the SWE-bench-Live dataset, along with how they are obtained during the curation process. Table 7: The required fields for typical issue-solving task instance. Fields marked with * are newly added in SWE-bench-Live compared to SWE-bench. Field base_commit patch test_patch problem_statement Type str str str str Description The commit on which the pull request is based, representing the repository state before the issue is resolved. Gold patch proposed by the pull request, in .diff format. Modifications to the test suite proposed by the pull request that are typically used to check whether the issue has been resolved. Issue description text, typically describing the bug or requested feature, used as the task problem statement. FAIL_TO_PASS PASS_TO_PASS *image_key *test_cmds *log_parser List[str] Test cases that are expected to successfully transition from failing to passing are used to evaluate the correctness of the patch. List[str] Test cases that are already passing prior to applying the gold patch. str correct patch shouldnt introduce regression failures in these tests. Instance-level docker image that provides an execution environment. List[str] The command(s) used to run the test suite is identified by the verify agent in REPOLAUNCH. It is required to enable detailed logging of each test items status (e.g., by using the pytest -rA option). The type of log parser required for the instanceby default, pytest. str"
        },
        {
            "title": "C Experimental Setup Details",
            "content": "In this section, we present additional details of the experimental setup to facilitate reproducibility. Hyperparameters used in the experiments. For OpenHands, we set maximum of 60 iterations per instance, with the LLM configured to use temperature of 0.0 and top-p value of 1.0 as default. For SWE-agent, we limit the number of LLM calls to 100 per instance, with the temperature set to 0.0 and top-p to 1.0. For Agentless, both the number of localization samples and repair samples are set to 1, corresponding to single rollout. The LLM temperature is set to 0.8 during the localization phase, as defined by the agents default, and 0.0 for all other phases. In our experiments, we omit the regression test-based reranking stage of Agentless, retaining only the localization and repair stages. The LLM calls within REPOLAUNCH are configured with temperature of 0.0. Random seed in subset splitting. The only stochastic component in this work arises during the sampling of the lite subset, where we set the random seed to 42. Computational resources. All LLM calls in this work are made through official APIs. The experiments involve parallel execution of multiple Docker containers for test execution. We conduct all the experiments on CPU server equipped with an Intel Xeon Gold 6338 @ 2.00GHz (128 cores) and 2TB of RAM."
        },
        {
            "title": "D Limitations",
            "content": "Randomness caused by LLMs: We use the LLMs as the core engine to conduct all the experiments, which might lead to potential randomness caused by different LLM calls. Since the experiments require extensive LLM calls while the overall budget is limited, we do not repeat the experiments for multiple times. To reduce the randomness, we control the execution environment to be the same and set the temperature and top_p to zero. Language limitation: Our benchmark SWE-bench-Live primarily focuses on the Python language only, which might be limited. Since our key contribution is to propose live benchmark with an automated and scalable method, we follow the same language choice as existing benchmarks like SWE-bench. In the future, we plan to extend SWE-bench-Live to multiple languages such as Java, Go, and etc."
        },
        {
            "title": "E Prompts in REPOLAUNCH",
            "content": "Prompt for Relevant Files Identification Given this repository structure: BEGIN REPOSITORY STRUCTURE {structure} END REPOSITORY STRUCTURE List the most relevant files for setting up development environment, including: 0. 1. 2. 3. 4. Format each file with its relative path (relative to project root) to be wrapped with tag <file> </file>, one per line. CI/CD configuration files README files Documentation Installation guides Development setup guides"
        },
        {
            "title": "Prompt for Setup Agent",
            "content": "You are developer. environment that is able to run the tests of the project. Your task is to install dependencies and set up - You start with an initial Docker container based on {base_image}. - You interact with Bash session inside this container. - Project files are located in /testbed within the container, and your current working directory of bash is already set to /testbed. - No need to clone the project again. to have development environment. The final objective is to successfully run the tests of the project. ### Attention: - For Python project, you should make sure the package is installed from source in the editable mode before running tests (for example pip install -e .) - For Python project, avoid use tox to run test if possible as it is designed specifically for CI. Read tox.ini file to find how to setup and run the test. You run in loop of Thought, Action, Observation. loop you should use Action to stop the loop. Use Thought to describe your thoughts about the question you have been asked. Use Action to run one of the actions available to you. Observation will be the result of running those actions. > Important Note: Action) pair. > Important Note: by the system. Your available actions are: {tools} Observation will be the result of running those actions. Do not reply **Observation**, it will be provided Each step, reply with only **one** (Thought, At the end of the the structure of the project, including files and Project Structure: directories. Related Files: the content of related files of the project that may help you understand the project. Thought: Action: Observation: the result of the action you should always think about what to do decide an action to take ... (this Thought/Action/Observation can repeat times) ... Thought: Action: Answer: think the setup should be fine stop the setup the final result Begin Project Structure: Related Files: {docs} {project_structure}"
        },
        {
            "title": "Prompt for Verify Agent",
            "content": "You are developer. for the given project is set up correctly. Docker environment for the project. You need to verify if it can successfully run the tests of the project. Your task is to verify whether the environment Your colleague has set up - You interact with Bash session inside this container. - The container is based on {base_image}. - The setup commands that your colleague has run are {setup_commands} - Project files are located in /testbed within the container, and your current working directory of bash is already set to /testbed. - Use the same test framework as your colleague, because that aligns with the setup stage. - Only test commands, skip linting/packaging/publishing commands. - Do not change the state of the environment, your task is to verify not to fix it. - You can tolerate few test cases failuresas long as most tests pass, its good enough. If you see issues, report it not fix it. ## Important Note: Your test command must output detailed pass/fail status for each test item. This is mandatory. For example, with pytest, use the -rA option to get output like: PASSED tests/test_resources.py::test_fetch_centromeres PASSED tests/test_vis.py::test_to_ucsc_colorstring Since we need to parse the test output to extract test item status mapping, **this requirement is mandatory**. that your test command does not produce such detailed output, you must adjust it accordingly. If you observed In summary, your goal is: Write the test commands that could output detailed pass/fail 1. status for each test item, you can iterate until it does. (this is mandatory, DO NOT ignore this requirement!!! to correctly identify the test commands to run the test suite of the project, and find way to output detailed pass/fail status) 2. correctly. If not, report any observed issues. setup is correct, report none issue. Run the test command to verify if the environment is set up If you think the This is your obligation"
        },
        {
            "title": "Prompt for Base Image Selection",
            "content": "Based on related file: {related_files} Please recommend suitable base Docker image. Consider: 1. 2. 3. The programming language and version requirements Common system dependencies Use official images when possible Select base image from the following candidate list: {candidate_images} Wrap the image name in block like <image>python:3.11</image> to indicate your choice."
        }
    ],
    "affiliations": [
        "Microsoft",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}