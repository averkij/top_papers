{
    "paper_title": "SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through a Cascaded Generative Pipeline",
    "authors": [
        "Helin Wang",
        "Jiarui Hai",
        "Dongchao Yang",
        "Chen Chen",
        "Kai Li",
        "Junyi Peng",
        "Thomas Thebaud",
        "Laureano Moro Velazquez",
        "Jesus Villalba",
        "Najim Dehak"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . e [ 1 4 1 3 9 1 . 5 0 5 2 : r SoloSpeech: Enhancing Intelligibility and Quality in Target Speech Extraction through Cascaded Generative Pipeline Helin Wang1 Jiarui Hai1 Dongchao Yang2 Chen Chen3 Kai Li Junyi Peng5 Thomas Thebaud1 Laureano Moro Velazquez1 Jesus Villalba1 Najim Dehak1 1Johns Hopkins University, 2The Chinese University of Hong Kong, 3Nanyang Technological University, 4Tsinghua University, 5Brno University of Technology hwang258@jhu.edu"
        },
        {
            "title": "Abstract",
            "content": "Target Speech Extraction (TSE) aims to isolate target speakers voice from mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features speaker-embedding-free target extractor that utilizes conditional information from the cue audios latent space, aligning it with the mixture audios latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios."
        },
        {
            "title": "Introduction",
            "content": "Humans possess the remarkable ability to focus on specific speaker amidst cacophony of sounds, including background noise, reverberation, and interfering voices. This capability is famously referred to as the cocktail-party effect [1]. For many years, researchers have endeavored to emulate this sophisticated auditory skill using algorithms. Target Speech Extraction (TSE) emerges as promising field, aiming to isolate desired speakers voice from complex mixture of multiple speakers, alongside any accompanying noise or reverberation [2]. TSE leverages cues, which serve as identifiers for the target speaker, to accurately extract their speech from mixture. Various types of cues have been investigated to address the TSE problem. These include spatial cues [3, 4], visual cues [5], concept cues [6], and audio cues [7, 8]. In this paper, we specifically focus on monaural TSE methods that utilize the most prevalent audio-based cues. significant challenge in TSE lies in distinguishing between target and non-target speakers, as their speech signals often share similar characteristics, complicating the extraction process [9]. To overcome this, many researchers have concentrated on discriminative model-based TSE, which directly maps the mixture signal to the target signal by optimizing signal-level metrics [10, 11, 12]. Both time-domain [13, 14, 15] and time-frequency (T-F) domain methodologies [16, 17] have Corresponding Author. Preprint. Under review. Figure 1: Overall pipeline of SoloSpeech. been investigated. While these discriminative approaches are highly effective, they can sometimes introduce artifacts and are often sensitive to discrepancies between training and testing conditions [18, 19]. In contrast, generative approaches model the joint distribution of the mixture speech, the target speech, and the cue speech. Various deep generative models have been explored for tasks such as speech enhancement, dereverberation, and separation [20, 21, 22]. These models have demonstrated the ability to exhibit greater robustness in unseen scenarios than discriminative models [20, 19]. They are capable of achieving high clarity in overlapping regions where multiple speakers converse simultaneously [23]. However, current generative approaches still result in separated speech with reduced audio quality and intelligibility, leading mainstream TSE solutions to continue relying on discriminative methods [24]. To consistently enhance the quality and intelligibility of generative models for TSE, we introduce SoloSpeech2, novel cascaded generative pipeline that comprises three key components: generative audio compressor, generative target extractor, and generative corrector. To the best of our knowledge, we are the first to introduce such pipeline for speech extraction and separation. The audio compressor utilizes T-F domain variational autoencoder (VAE) to transform audio waveforms into latent representations and vice versa. The target extractor employs latent diffusion model to derive the latent representation of the target signal. Notably, we propose speaker-embedding-free model for the target extractor, where the mixture audio and the cue audio (i.e. condition) are fused using cross-attention mechanism [25] in the same latent space, preventing potential mismatches. Furthermore, inspired by recent research on error correction with generative models [19, 26, 27], we propose T-F domain diffusion model as the corrector. This model addresses errors introduced by the target extractor and refines the audio quality, ensuring high quality and intelligibility in the extracted speech. We evaluated the proposed pipeline on the Libri2Mix noisy benchmark dataset [28] and demonstrated its significant superiority over existing methods in both TSE and Speech Separation (SS) tasks, the latter aiming to separate all speakers from the mixture audio. SoloSpeech achieves new state-of-the-art results in perceptual quality, naturalness, and intelligibility, underscoring its effectiveness and robustness in noisy acoustic scenarios. Moreover, experiments on three out-ofdomain datasets and two real-word datasets showed that SoloSpeech exhibits strong generalization capabilities on unseen conditions."
        },
        {
            "title": "2 SoloSpeech",
            "content": "In this section, we present cascaded generative pipeline that operates through sequence of compression, extraction, reconstruction, and correction stages, which is shown in Figure 1. 2.1 Overall pipeline Let xm R1Tm and xc R1Tc denote the mixture speech and the conditional speech (a.k.a. cue speech), respectively, where Tm and Tc represent their corresponding audio lengths. The task of TSE is to estimate the target speech signal R1Tm. The audio compressor transforms the speech signals into compact latent representations: zm RDLm for the mixture speech and zc RDLc for the conditional speech (see appendix 2.2 for details). Here, denotes the dimensionality of the latent features for each frame, while Lm and Lc represent the number of frames in the mixture and conditional latents, respectively. The target extractor (detailed in appendix 2.3) then predicts the latent features of the target speech, zy RDLm, 2See our audio demos at https://wanghelin1997.github.io/SoloSpeech-Demo/ and source code at https://github.com/WangHelin1997/SoloSpeech/. All resources are released under the CC BY-NC 4.0 license (Creative Commons Attribution-NonCommercial). Figure 2: The audio compressor architecture. from zm conditioned on zc. The estimated target speech signal, xr R1Tm , is reconstructed using the decoder of the audio compressor. Finally, we introduce corrector to refine the target speech, resulting in the refined signal Ë†y R1Tm . See appendix 2.4 for details."
        },
        {
            "title": "2.2 Audio compressor",
            "content": "The audio compressor operates on raw waveforms, compressing them into manageable sequence lengths. Current advanced audio compressors utilize time-domain VAEs with several convolutional blocks [29, 30, 31]. Inspired by the success of T-F domain modeling for mixture speech signals [32, 33], we propose novel T-F domain VAE as the audio compressor. As illustrated in Figure 2, the encoder first applies the Short-Time Fourier Transform (STFT) [34] to transform the input audio signal R1N into complex spectrum R2F , where denotes the audio length, the hop size of STFT, and the number of frequency bins. The real and imaginary components are concatenated along the first dimension. and the variance Ïƒx R1D Our VAE adopts TF-GridNet [32] as its backbone network, which comprises several TF-GridNet blocks. The encoder outputs latent representation of shape R12D , which is then split into two parts: the mean Âµx R1D . The latent representation is obtained by sampling from this distribution, denoted as (Âµx, Ïƒx) where R1D . The decoder mirrors the encoder structure, reconstructing the audio waveform using the inverse STFT (iSTFT). The VAE model is trained in generative and adversarial manner [30]. The loss functions include: (i) perceptually weighted multi-resolution STFT reconstruction loss [35]; (ii) an adversarial loss term with feature matching, utilizing five convolutional discriminators as in Encodec [36]; and (iii) KL divergence loss term. See appendix C.1 for details. 2.3 Target extractor The target extractor operates on the latent representations, aiming to extract the target latents zy from the mixture latents zm, guided by the conditional latents zc. As shown in Figure 3(a), we use modified diffusion scheduler and velocity prediction [37], which have been shown to improve the purity and overall performance of sound extraction [38]. Details of the diffusion process could be found in appendix B. The main network for the target extractor employs diffusion transformer with long skip connections (uDiT) [23], modified version of DiT [39], as detailed in Figure 3(b). These skip connections bridge shallow and deep DiT blocks, creating shortcuts for low-level features and streamlining the training of the entire velocity-prediction network. The input feature is concatenation of the current noisy latent zt and the mixture latent zm in the latent dimension (D). The time feature and the conditional feature are fused into each block. Unlike previous works that introduce speaker embeddings as conditional information [38, 23], our method jointly trains condition transformer to extract the conditional features from the conditional latents zc, which are then attended to by the diffusion transformer using cross-attention mechanism. This approach offers the following advantages: (i) it eliminates the need for extra data or labels to train speaker embedding network; (ii) it preserves the sequential information of the conditional features, such as local dynamics and temporal structures; and (iii) it prevents potential mismatching between VAE latents and other types of features representing the target speech. 3 (a): The architecture of the target extractor. (b): The architecture of Diffusion Transformer (c): The uDiT block Figure 3: The target extractor architecture and backbone. The condition transformer employs ViT-like backbone [40], consisting of several ViT blocks. Each ViT block contains two LayerNorm layers, Multi-Head Self-Attention (MHSA) layer, and MLP layers with SiLU activations [41]. The condition transformer generates sequence of conditional features, which are then attended to by the uDiT blocks. As shown in Figure 3(c), each uDiT block includes an MHSA module, Multi-Head Cross-Attention (MHCA) module, and an MLP module. The conditional features are fused by the MHCA module, and the embedded time features are integrated into the MHSA module and the MLP module using Adaptive LayerNorm (AdaLN) layers. SiLU serves as the activation function, and we incorporate rotary positional embeddings (RoPE) [42] for the position encoding in both MHCA and MHSA. Following the reverse process of diffusion models, the target extractor gradually reconstructs the target latents from random Gaussian noise, generating the estimated target latents zy. The speech signal is then obtained from the estimated target latents using the decoder of the audio compressor, resulting in the reference signal xr R1Tm. 2.4 Corrector Diffusion models have been shown to outperform discriminative models for non-additive corruption types or when evaluated under mismatched conditions [21]. However, they often yield worse results on reference-based metrics [43], particularly for latent diffusion models, as VAE reconstruction determines the upper bound of audio quality. In addition, diffusion models may produce vocalizing and breathing artifacts in adverse conditions [44]. Inspired by the success of incorporating generative models to refine signal quality [19], we propose T-F domain diffusion model-based corrector to address these issues. The corrector is designed to: (i) reduce artifacts caused by the front-end models; (ii) improve the signal quality from the audio compressors output by incorporating signal quality-based training objective functions; and (iii) correct errors introduced by the target extractor, such as speaker obfuscation and mispronunciation, thereby enhancing intelligibility. 2.4.1 Fast-GeCo Corrector Inspired by Fast-GeCo [19], we employ single-step diffusion model that allows us to directly optimize signal quality using the Scale-Invariant Signal-to-Noise Ratio (SI-SNR) loss function [45, 46]. As illustrated in Figure 4(a), Fast-GeCo initially trains multi-step diffusion model, which takes noised signal as input, conditioned on the reference signal, mixture signal, and timestep. The noised signal depends on both the timestep and the reference signal. Subsequently, Fast-GeCo accelerates the traditional multi-step diffusion process by distilling it into single-step operation, thereby producing the refined signal Ë†y R1Tm. Given the ground-truth target signal y, the 4 (a): Fast-GeCo corrector (b): SoloSpeech corrector Figure 4: Diagrams of the Fast-GeCo corrector and SoloSpeech corrector. signal-step corrector is optimized by: := 10 log (cid:13) (cid:13) (cid:13) Ë†y,yy y2 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13)Ë†y Ë†y,yy (cid:13) y2 (cid:13) 2 (cid:13) (cid:13) (1) where y2 = y, denotes the signal power. Scale invariance is maintained by normalizing Ë†y and to zero mean before computation. 2.4.2 SoloSpeech Corrector From our initial experiments, we observe that such corrector can improve signal quality and reduce artifacts from the target extractor. However, it may struggle when processing reference signals with speaker obfuscation or mispronunciation. This can result in weak speaker similarity between the target and extracted speech, as well as poor performance in Automatic Speech Recognition (ASR) tasks. We argue that this issue arises because the reference signal already contains sufficient information for the corrector to learn the refinement task, leading to inadequate utilization of the original mixture signal. Consequently, the corrector behaves more like Speech Enhancement (SE) tool rather than TSE tool. To address the above issues, in SoloSpeech, the refined signal is derived from the mixture signal rather than the reference signal, enabling the corrector to function similarly to TSE task. As illustrated in Figure 4(b), the corrector receives noised mixture signal as input. To encourage the corrector to leverage more information from the mixture signal and enhance the intelligibility of the refined signal, we propose simple yet effective augmentation approach for the reference signal. During the training phase, random continuous span of the reference signal is masked. We set the masking ratio as Î¸. This forces the model to recover the target signal conditioned on corrupted reference signal, thereby compelling it to utilize additional information from the mixture signal. Furthermore, we found that distilling from multi-step model does not significantly improve performance but greatly increases training costs. The performance gains are primarily due to the introduction of the signal quality loss in the single-step model. Therefore, we train the single-step corrector directly from scratch."
        },
        {
            "title": "3 Experiments",
            "content": "We conducted both in-domain and out-of-domain TSE experiments to evaluate the effectiveness and robustness of SoloSpeech. We also assessed SoloSpeech on the most related SS task alongside TSE. To examine the impact of each component in SoloSpeech, we performed ablation studies on TSE results involving different audio compressors, speaker conditions, and correctors. 3.1 Datasets To evaluate the performance of TSE, we trained models on the Libri2Mix 16k Hz dataset [28], which is simulated using WHAM! noise data [47] and Librispeech utterances [48]. The dataset comprises two training subsets with 212 hours (train-360) and 58 hours (train-100) of audio, respectively. We utilized the train-360 subset for training. The dev set, containing 11 hours of audio, was used for validation. We then evaluated our models on the test set, which includes 11 hours of audio. During inference, the cue speech is different utterance from the target speech by the same speaker, following 5 the same data preparation as in TD-SpeakerBeam [49]. This dataset is also utilized for two-speaker SS experiments. Although Libri2Mix includes different speakers and background noises in its training and test sets, the recording conditions and speaking styles remain highly similar. Following [19], we assessed the trained models on out-of-domain data. Utterances from the Wall Street Journal (WSJ) corpus [45] served as the speech source, while noise audio from WHAM!, MUSAN [50], and DEMAND [51] were used as noise sources for simulation. Each of these test sets has duration of 5 hours. Following the approach in [47], noise was introduced by sampling random SNR value from uniform distribution ranging from -6 to +3 dB. Additionally, we generated minimum-length version of the simulated data by removing any leading and trailing noise and truncating it to match the length of the shorter of the two speakers utterances. During inference, we selected random speech sample of the target speaker different from the target speech as the cue speech. We also evaluated the performance on two real-world datasets: CHiME-5 [52] and RealSEP [53]. CHiME-5 consists of real dinner party conversations recorded using distant microphones, featuring up to four overlapping speakers, expressive speech, non-verbal vocalizations (e.g., laughter, sigh), and background sound effects. RealSEP covers ten diverse real-world acoustic scenes, including music, sound effects, strong reverberation, and moving sound sources. Both datasets are highly challenging and contain conditions that are significantly different from those in Libri2Mix, demonstrating the robustness of our method in previously unseen scenarios. For each dataset, we randomly selected 100 mixtures (1020 seconds) and their corresponding 10-second cue audio samples. 3.2 Evaluation metrics To evaluate the performance of SoloSpeech, we conducted TSE experiments using both quality and intelligibility metrics. The perceptual metrics include: (i) Perceptual Evaluation of Speech Quality (PESQ) [54]; (ii) Extended Short-Time Objective Intelligibility (ESTOI) [55]; (iii) ScaleInvariant Signal-to-Noise Ratio (SI-SNR) [45]; and (iv) Deep Noise Suppression Mean Opinion Score (DNSMOS) [56]. The first three are intrusive speech perceptual quality metrics, while DNSMOS is non-intrusive metric that estimates the overall quality of the audio and naturalness. For the intelligibility metrics, we performed ASR on the speech files using the Whisper large-v3-turbo model3 [57] and calculated the Word Error Rate (WER) using the jiwer toolkit4. In addition, we measured the cosine similarity between the estimated speech and the ground truth using pretrained WavLM-based speaker verification network5 [58] to assess speaker similarity (SIM). For the real-world TSE evaluation, we followed the protocol in [59] and assessed both separation accuracy and perceptual quality through Mean Opinion Score (MOS) test conducted with 12 human raters recruited via Prolific6. Please refer to for detail. We report the mean scores along with 95% confidence intervals. For SS experiments, we evaluated performance using reference-based perceptual metrics, including PESQ improvement (PESQ-i), ESTOI improvement (ESTOI-i), and SI-SNR improvement (SI-SNR-i). Following [19, 60], we introduced reference-free metric for naturalness evaluation, Non-Intrusive Speech Quality Assessment (NISQA) [61]. 3.3 Model configurations For the audio compressor, the STFT utilizes window size of 640 and hop size of 320 (i.e., = 320), resulting in latent representations at 50 Hz. See appendix C.1 for details. The dimensionality is set to 128. During training, we clipped the audio into 100-frame chunks. The VAE was trained using the AdamW optimizer, with batch size of 16, base learning rate of 1.5 104, while the discriminators used learning rate of 3 104. The weight decay was set to 1 103, and training was conducted for 10 epochs. It took two days on one NVIDIA A100-80GB GPU. For the target extractor, we developed three different model sizes. See appendix C.2 for details. The Base model was trained with learning rate of 1 104, weight decay of 1 104, batch size of 32, and for 200 epochs. The training and inference steps were set to 1000 and 50, respectively, with 3https://huggingface.co/openai/whisper-large-v3-turbo 4https://github.com/jitsi/jiwer 5https://huggingface.co/microsoft/wavlm-base-plus-sv 6https://www.prolific.com/ 6 Table 1: Target speech extraction results on the Libri2Mix dataset. are the results of models reproduced by us. D/G indicates if the model is discriminative or generative. Bold for the best result and underline for the second-best result. Method D/G TD-SpeakerBeam [49] DPCCN [10] X-TF-GridNet [16] SUPERB-TSE [63] SSL-MHFA [64] USEF-TSE [17] DiscreteTSE [65] DDTSE [66] Diff-TSE [18] DPM-TSE [38] SoloAudio [23] SoloSpeech (ours) D G G Perceptual Quality ESTOI Naturalness SISNR DNSMOS WER Intelligibility PESQ 1.66 1.74 1.72 1.54 1.76 1.82 1.29 1.60 1.56 1.50 1.56 1.89 0.70 0.73 0.72 0.67 0.74 0.72 0.69 0.71 0.64 0.60 0.63 0. 9.21 9.30 9.85 8.17 10.60 10.17 39.95 7.60 8.35 1.56 2.64 11.12 3.14 3.58 3.42 3.19 3.22 3.48 - 3.74 3.53 3.20 3.71 3.76 0.21 - 0.19 0.24 0.17 0.17 - - 0.25 0.40 0.35 0. SIM 0.93 - 0.93 0.93 0.94 0.94 - - 0.89 0.89 0.93 0.96 the variance ranging from 8.5 104 to 1.2 102. During training, we applied data augmentation by randomly clipping the mixture and cue speech to lengths ranging from 3 seconds up to their original durations. The model was trained on one NVIDIA A100-80GB GPU for six days. During inference, we found that classifier-free guidance (CFG) [62] did not improve performance in TSE. Large guidance scales even tended to produce overly smoothed outputs, with reduced speech detail and degraded intelligibility. As result, we chose to exclude CFG in our pipeline. For the corrector, we employed the same network as Fast-GeCo [19] and set the reverse starting point Ï„ = 0.5, as detailed in appendix C.3. The STFT utilizes window size of 510 and hop size of 128. During training, we clipped the audio into 256-frame chunks. We used learning rate of 1 104, batch size of 16 and 50 epochs, which took three days on one NVIDIA A100-80GB GPU. 3.4 Comparisons with state-of-the-art methods To evaluate the efficiency of the proposed approach, we compared SoloSpeech with existing methods. We reproduced the results using the authors official settings. If the authors employed the same dataset as us, we report their metric values as presented in their original papers. See appendix for the details of the baseline methods. In-domain TSE Results. We trained and tested the models on the Libri2Mix dataset. As shown in Table 1, SoloSpeech consistently outperformed existing methods across all metrics, including perceptual quality, naturalness, and intelligibility, achieving 0.95 dB gain in SI-SNR. Compared to discriminative methods, SoloSpeech achieves significant improvements in DNSMOS, demonstrating its powerful ability to produce natural and high-quality audio. Furthermore, when compared with other generative approaches, SoloSpeech significantly outperforms them in perceptual and intelligibility metrics, highlighting its advantages. Higher values of DNSMOS and lower WER also indicate that our method produces fewer artifacts for both listeners and ASR systems. Out-of-domain TSE Results. We trained the models on the Libri2Mix dataset and tested them on three out-of-domain datasets without any fine-tuning. As shown in Table 2, SoloSpeech consistently outperforms existing methods in generalizing to unseen conditions. Specifically, SoloSpeech achieves SI-SNR gains of 16.7%, 16.1%, and 17.6% over the state-of-the-art discriminative method USEF-TSE on the WHAM!, MUSAN, and DEMAND datasets, respectively. Real-world TSE Results. We trained models on the Libri2Mix and tested them on two real-world datasets without any fine-tuning. As shown in Table 2, SoloSpeech significantly outperforms baseline methods in generalizing to real-world challenging scenarios. Remarkably, SoloSpeech is able to generalize to more challenging unseen conditions. For example, handling more overlapping speakers (up to 4 in CHiME-5 vs. 2 in Libri2Mix) and much longer audio durations (up to 20 seconds). Moreover, it demonstrates strong robustness to factors such as expressive speech, nonverbal vocalizations, reverberation, and background sound effects. 7 Table 2: Target speech extraction results on the out-of-domain data. We trained the models on the Libri2Mix dataset and then tested them on these datasets. DataSet WHAM! MUSAN DEMAND CHiME-5 (real-world) RealSEP (real-world) Method SISNR WER DNSMOS MOS USEF-TSE [17] SoloAudio [23] SoloSpeech (ours) USEF-TSE [17] SoloAudio [23] SoloSpeech (ours) USEF-TSE [17] SoloAudio [23] SoloSpeech (ours) USEF-TSE [17] SoloAudio [23] SoloSpeech (ours) USEF-TSE [17] SoloAudio [23] SoloSpeech (ours) 9.15 2.52 10.68 8.96 2.05 10.40 9.70 3.30 11.41 - - - - - - 0.23 0.36 0.18 0.25 0.38 0.20 0.19 0.34 0.17 - - - - - - 3.19 3.45 3.72 0.20 3.38 3.70 3.25 3.50 3.72 2.75 2.90 3.38 2.69 2.52 3.15 - - - - - - - - - 2.10 0.15 2.26 0.14 2.93 0.18 2.04 0.15 1.87 0.20 2.70 0.18 Table 3: Speech separation results on the Libri2Mix dataset. are the results of models reproduced by us. D/G indicates if the model is discriminative or generative. Bold for the best result and underline for the second-best result. Method D/G SepFormer [67] TF-Gridnet [32] SepReFormer [68] SPMamba [33] DiffSep [60] SpeechFlow [22] Fast-GeCo [19] SoloSpeech (ours) D G PESQ-i Perceptual Quality ESTOI-i SISNR-i 0.59 0.72 0.69 0.74 0.50 - 0.86 0.88 0.23 0.35 0.31 0.36 0.22 0.33 0.34 0. 10.58 11.32 12.00 12.25 8.90 10.46 12.98 13.92 Naturalness NISQA Intelligibility WER SIM 2.05 2.80 2.66 3.02 2.44 - 3.68 3.70 0.16 0.14 0.15 0.14 0.20 - - 0.13 0.92 0.94 0.95 0.95 0.89 - - 0. SS Results. We also conducted two-speaker speech separation experiments. Although SoloSpeech is trained to extract one speaker from the mixture audio, we trained another model to remove the target speaker and extract the other speaker. This approach allows for the separation of both speakers voices. Refer to appendix for details of this process. Results on the Libri2Mix dataset are presented in Table 3, where SoloSpeech outperforms all state-of-the-art methods. Specifically, SoloSpeech achieves 0.94 dB SI-SNR improvement compared to the previous best model, i.e., Fast-GeCo. 3.5 Ablation studies To gain deeper understanding of SoloSpeech, we compared each key component using the Libri2Mix dataset under completely fair setup, employing identical architectures and hyperparameters in the subsequent experiments. Comparison of audio compressors. We compared the proposed T-F Audio VAE with the state-ofthe-art Stable Audio VAE. We employed the VAEs as audio compressors within SoloSpeech and assessed their impact on the TSE task. As presented in Table 4, the newly proposed T-F Audio VAE significantly enhances TSE performance compared to the Stable Audio VAE. Comparison of speaker conditions. We compared different speaker conditions and reported the results in Table 4. Specifically, We evaluated speaker embedding7-based approach with AdaLN [23], considering both fixed and fine-tuned configurations; an SSL-based speaker network8 [64]; 7https://github.com/TaoRuijie/ECAPA-TDNN 8https://github.com/BUTSpeechFIT/wespeaker_ssl_public 8 Table 4: Impact of the modules. Unless specific statement, corrector is not applied. used fixed speaker embedding extracted from pretrained WavLM-based speaker verification model. finetuned the speaker embedding. used features from WavLM model. Module Method SISNR DNSMOS WER Compressor Speaker Condition Stable Audio VAE [30] T-F Audio VAE (ours) Speaker embedding [58] Speaker embedding [58] SSL-MHFA [64] Speaker network [49] Latent-space fusion (ours) Corrector USEF-TSE [17] USEF-TSE w/ SoloSpeech corrector SoloAudio [23] SoloAudio w/ SoloSpeech corrector SoloSpeech w/o corrector SoloSpeech w/ Fast-GeCo corrector [19] SoloSpeech (ours) 7.37 8. 6.25 5.77 5.85 5.32 8.10 10.17 11.06 2.64 6.58 8.10 10.30 11.12 3.69 3.76 3.71 3.65 3.69 3.69 3.76 3.48 3.55 3.71 3.72 3.76 3.76 3.76 0.25 0. 0.30 0.32 0.32 0.36 0.24 0.17 0.16 0.35 0.22 0.24 0.19 0.16 time-domain speaker network9 [49]; and our proposed VAE latent-domain speaker network. Our method does not require additional data to train speaker embeddings, yet achieves the best results among all methods. This superiority is attributed to our approachs ability to preserve the sequential information of the conditional features and prevent potential mismatching between VAE latents and other types of features representing the target speech. Impact of the corrector. We investigated the impact of the corrector and present the results in Table 4. Our proposed corrector significantly outperforms the previous work, Fast-GeCo, thereby substantially enhancing perceptual quality and intelligibility. The corrector contributes significantly to SI-SNR and WER gains, as we found that it can significantly improve signal quality and reduce artifacts in high frequencies, which may influence the ASR system. Furthermore, we applied the proposed corrector to two prior baselines, USEF-TSE (a discriminative method) and SoloAudio (a generative method), and reported the results in Table 4. The corrector consistently improved performance across all evaluation metrics. Notably, SoloSpeech still outperforms USEF-TSE even with the corrector applied, especially in DNSMOS scores, highlighting the advantage of generative models in producing more natural-sounding speech. Compared to SoloAudio with the same corrector, SoloSpeech achieves significantly better perceptual quality and intelligibility, demonstrating the effectiveness of our target extractor backbone. Additional ablation studies are provided in Appendix H, including the reconstruction quality of audio compressors (Appendix H.1), hyperparameter tuning of the audio compressor (Appendix H.2), the effect of masking ratios (Appendix H.3), audio durations (Appendix H.4), and model sizes (Appendix H.5). We also present comparison of computational costs in Appendix G."
        },
        {
            "title": "4 Conclusion and Discussions",
            "content": "We proposed cascaded generative pipeline that offers strong foundation for future research in generative TSE. Each component is validated for its effectiveness and designed with modularity and scalability in mind. Compared to previous state-of-the-art methods in both TSE and SS tasks, SoloSpeech achieves significantly higher perceptual quality, naturalness, and intelligibility. Furthermore, when evaluated on out-of-domain data, SoloSpeech demonstrates excellent generalization and robustness. In real-world evaluations, we observe that strong reverberation and moving sound sources, such as those in the RealSEP dataset, remain challenging, and we leave these issues to future work. We also plan to explore more efficient backbone architectures for each component. 9https://github.com/BUTSpeechFIT/speakerbeam"
        },
        {
            "title": "Impact Statement",
            "content": "TSE may pose privacy and security risks, such as unauthorized surveillance, voice spoofing, and misuse in creating misleading audio. To mitigate these risks, we have released source code and model checkpoints under restrictive non-commercial licenses. This work also opens up new opportunities and challenges for integrating watermarking techniques [69] into extracted outputs for traceability, developing deepfake detection tools [70] to flag manipulated or synthetic audio."
        },
        {
            "title": "References",
            "content": "[1] Adelbert Bronkhorst. The cocktail-party problem revisited: early processing and selection of multi-talker speech. Attention, Perception, & Psychophysics, 77(5):14651487, 2015. [2] Katerina ZmolÃ­kovÃ¡, Marc Delcroix, Tsubasa Ochiai, Keisuke Kinoshita, Jan CernockÃ½, and Dong Yu. Neural target speech extraction: An overview. IEEE Signal Process. Mag., 40(3):829, 2023. [3] Rongzhi Gu, Lianwu Chen, Shi-Xiong Zhang, Jimeng Zheng, Yong Xu, Meng Yu, Dan Su, Yuexian Zou, and Dong Yu. Neural spatial filter: Target speaker speech separation assisted with directional information. In Gernot Kubin and Zdravko Kacic, editors, 20th Annual Conference of the International Speech Communication Association, Interspeech 2019, Graz, Austria, September 15-19, 2019, pages 42904294. ISCA, 2019. [4] Aswin Shanmugam Subramanian, Chao Weng, Meng Yu, Shi-Xiong Zhang, Yong Xu, Shinji Watanabe, and Dong Yu. Far-field location guided target speech extraction using end-to-end speech recognition objectives. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pages 72997303. IEEE, 2020. [5] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: speaker-independent audio-visual model for speech separation. ACM Trans. Graph., 37(4):112, 2018. [6] Yasunori Ohishi, Marc Delcroix, Tsubasa Ochiai, Shoko Araki, Daiki Takeuchi, Daisuke Niizumi, Akisato Kimura, Noboru Harada, and Kunio Kashino. Conceptbeam: Concept driven target speech extraction. In JoÃ£o MagalhÃ£es, Alberto Del Bimbo, Shinichi Satoh, Nicu Sebe, Xavier Alameda-Pineda, Qin Jin, Vincent Oria, and Laura Toni, editors, MM 22: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14, 2022, pages 42524260. ACM, 2022. [7] Quan Wang, Hannah Muckenhirn, Kevin W. Wilson, Prashant Sridhar, Zelin Wu, John R. Hershey, Rif A. Saurous, Ron J. Weiss, Ye Jia, and Ignacio LÃ³pez-Moreno. Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking. In Gernot Kubin and Zdravko Kacic, editors, 20th Annual Conference of the International Speech Communication Association, Interspeech 2019, Graz, Austria, September 15-19, 2019, pages 27282732. ISCA, 2019. [8] Katerina ZmolÃ­kovÃ¡, Marc Delcroix, Keisuke Kinoshita, Tsubasa Ochiai, Tomohiro Nakatani, LukÃ¡s Burget, and Jan CernockÃ½. Speakerbeam: Speaker aware neural network for target speaker extraction in speech mixtures. IEEE J. Sel. Top. Signal Process., 13(4):800814, 2019. [9] Zifeng Zhao, Dongchao Yang, Rongzhi Gu, Haoran Zhang, and Yuexian Zou. Target confusion in end-to-end speaker extraction: Analysis and approaches. In Hanseok Ko and John H. L. Hansen, editors, 23rd Annual Conference of the International Speech Communication Association, Interspeech 2022, Incheon, Korea, September 18-22, 2022, pages 53335337. ISCA, 2022. [10] Jiangyu Han, Yanhua Long, LukÃ¡Å¡ Burget, and Jan Ë‡Cernock`y. Dpccn: Densely-connected pyramid complex convolutional network for robust speech separation and extraction. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 72927296. IEEE, 2022. [11] Helin Wang, Dongchao Yang, Chao Weng, Jianwei Yu, and Yuexian Zou. Improving target sound extraction with timestamp information. In Hanseok Ko and John H. L. Hansen, editors, 23rd Annual Conference of the International Speech Communication Association, Interspeech 2022, Incheon, Korea, September 18-22, 2022, pages 15261530. ISCA, 2022. [12] Shuai Wang, Ke Zhang, Shaoxiong Lin, Junjie Li, Xuefei Wang, Meng Ge, Jianwei Yu, Yanmin Qian, and Haizhou Li. Wesep: scalable and flexible toolkit towards generalizable target speaker extraction. CoRR, abs/2409.15799, 2024. [13] Chenglin Xu, Wei Rao, Eng Siong Chng, and Haizhou Li. Spex: Multi-scale time domain speaker extraction network. IEEE ACM Trans. Audio Speech Lang. Process., 28:13701384, 2020. [14] Meng Ge, Chenglin Xu, Longbiao Wang, Eng Siong Chng, Jianwu Dang, and Haizhou Li. Spex+: complete time domain speaker extraction network. In Helen Meng, Bo Xu, and Thomas Fang Zheng, editors, 21st Annual Conference of the International Speech Communication Association, Interspeech 2020, Virtual Event, Shanghai, China, October 25-29, 2020, pages 14061410. ISCA, 2020. [15] Kai Liu, Ziqing Du, Xucheng Wan, and Huan Zhou. X-SEPFORMER: end-to-end speaker extraction network with explicit optimization on speaker confusion. In IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-10, 2023, pages 15. IEEE, 2023. [16] Fengyuan Hao, Xiaodong Li, and Chengshi Zheng. X-tf-gridnet: time-frequency domain target speaker extraction network with adaptive speaker embedding fusion. Inf. Fusion, 112:102550, 2024. [17] Bang Zeng and Ming Li. USEF-TSE: universal speaker embedding free target speaker extraction. CoRR, abs/2409.02615, 2024. [18] Naoyuki Kamo, Marc Delcroix, and Tomohiro Nakatani. Target speech extraction with conditional diffusion model. In INTERSPEECH 2023, pages 176180, 2023. [19] Helin Wang, Jesus Villalba, Laureano Moro-Velazquez, Jiarui Hai, Thomas Thebaud, and Najim Dehak. Noise-robust speech separation with fast generative correction. arXiv preprint arXiv:2406.07461, 2024. [20] Julius Richter, Simon Welker, Jean-Marie Lemercier, Bunlong Lay, and Timo Gerkmann. Speech enhancement and dereverberation with diffusion-based generative models. IEEE ACM Trans. Audio Speech Lang. Process., 31:23512364, 2023. [21] Robin Scheibler, Youna Ji, Soo-Whan Chung, Jaeuk Byun, Soyeon Choe, and Min-Seok Choi. Diffusion-based generative speech source separation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [22] Alexander H. Liu, Matthew Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, and Wei-Ning Hsu. Generative pre-training for speech with flow matching. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [23] Helin Wang, Jiarui Hai, Yen-Ju Lu, Karan Thakkar, Mounya Elhilali, and Najim Dehak. Soloaudio: Target sound extraction with language-oriented audio diffusion transformer. CoRR, abs/2409.08425, 2024. [24] Shahar Lutati, Eliya Nachmani, and Lior Wolf. Separate and diffuse: Using pretrained diffusion model for improving source separation. CoRR, abs/2301.10752, 2023. [25] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 11 [26] Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr Zelasko, Chao Zhang, Yun-Nung Chen, Yu Tsao, Jagadeesh Balam, Boris Ginsburg, Sabato Marco Siniscalchi, Eng Siong Chng, Peter Bell, Catherine Lai, Shinji Watanabe, and Andreas Stolcke. Large language model based generative error correction: challenge and baselines for speech recognition, speaker tagging, and emotion recognition. CoRR, abs/2409.09785, 2024. [27] Zhaoxi Mu, Xinyu Yang, and Gang Wang. Sepalm: Audio language models are error correctors for robust speech separation. arXiv preprint arXiv:2505.03273, 2025. [28] Joris Cosentino, Manuel Pariente, Samuele Cornell, Antoine Deleforge, and Emmanuel Vincent. Librimix: An open-source dataset for generalizable speech separation. arXiv preprint arXiv:2005.11262, 2020. [29] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved RVQGAN. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [30] Zach Evans, Julian D. Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. CoRR, abs/2407.14358, 2024. [31] Jiarui Hai, Yong Xu, Hao Zhang, Chenxing Li, Helin Wang, Mounya Elhilali, and Dong Yu. Ezaudio: Enhancing text-to-audio generation with efficient diffusion transformer. arXiv preprint arXiv:2409.10819, 2024. [32] Zhong-Qiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeong-Yeol Kim, and Shinji Watanabe. Tf-gridnet: Integrating fulland sub-band modeling for speech separation. IEEE ACM Trans. Audio Speech Lang. Process., 31:32213236, 2023. [33] Kai Li and Guo Chen. Spmamba: State-space model is all you need in speech separation. CoRR, abs/2404.02063, 2024. [34] Donald S. Williamson, Yuxuan Wang, and DeLiang Wang. Complex ratio masking for monaural speech separation. IEEE ACM Trans. Audio Speech Lang. Process., 24(3):483492, 2016. [35] Christian Steinmetz and Joshua Reiss. auraloss: Audio focused loss functions in pytorch. In Digital music research network one-day workshop (DMRN+ 15), 2020. [36] Alexandre DÃ©fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. Trans. Mach. Learn. Res., 2023, 2023. [37] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2024, Waikoloa, HI, USA, January 3-8, 2024, pages 53925399. IEEE, 2024. [38] Jiarui Hai, Helin Wang, Dongchao Yang, Karan Thakkar, Najim Dehak, and Mounya Elhilali. Dpm-tse: diffusion probabilistic model for target sound extraction. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 11961200. IEEE, 2024. [39] William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 41724182. IEEE, 2023. [40] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [41] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:311, 2018. 12 [42] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [43] Masato Hirano, Kazuki Shimada, Yuichiro Koyama, Shusuke Takahashi, and Yuki Mitsufuji. Diffusion-based signal refiner for speech separation. CoRR, abs/2305.05857, 2023. [44] Jean-Marie Lemercier, Julius Richter, Simon Welker, and Timo Gerkmann. Storm: diffusionbased stochastic regeneration model for speech enhancement and dereverberation. IEEE ACM Trans. Audio Speech Lang. Process., 31:27242737, 2023. [45] Yusuf Ziya Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe, and John R. Hershey. Singlechannel multi-speaker separation using deep clustering. In Nelson Morgan, editor, 17th Annual Conference of the International Speech Communication Association, Interspeech 2016, San Francisco, CA, USA, September 8-12, 2016, pages 545549. ISCA, 2016. [46] Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation. IEEE ACM Trans. Audio Speech Lang. Process., 27(8):12561266, 2019. [47] Gordon Wichern, Joe Antognini, Michael Flynn, Licheng Richard Zhu, Emmett McQuinn, Dwight Crow, Ethan Manilow, and Jonathan Le Roux. Wham!: Extending speech separation to noisy environments. In Gernot Kubin and Zdravko Kacic, editors, 20th Annual Conference of the International Speech Communication Association, Interspeech 2019, Graz, Austria, September 15-19, 2019, pages 13681372. ISCA, 2019. [48] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015, pages 52065210. IEEE, 2015. [49] Marc Delcroix, Tsubasa Ochiai, Katerina Zmolikova, Keisuke Kinoshita, Naohiro Tawara, Tomohiro Nakatani, and Shoko Araki. Improving speaker discrimination of target speech extraction with time-domain speakerbeam. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 691695. IEEE, 2020. [50] David Snyder, Guoguo Chen, and Daniel Povey. MUSAN: music, speech, and noise corpus. CoRR, abs/1510.08484, 2015. [51] Elior Hadad, Florian Heese, Peter Vary, and Sharon Gannot. Multichannel audio database in various acoustic environments. In 14th International Workshop on Acoustic Signal Enhancement, IWAENC 2014, Juan-les-Pins, France, September 8-11, 2014, pages 313317. IEEE, 2014. [52] Jon Barker, Shinji Watanabe, Emmanuel Vincent, and Jan Trmal. The fifth chime speech separation and recognition challenge: Dataset, task and baselines. In B. Yegnanarayana, editor, 19th Annual Conference of the International Speech Communication Association, Interspeech 2018, Hyderabad, India, September 2-6, 2018, pages 15611565. ISCA, 2018. [53] Kai Li, Wendi Sang, Chang Zeng, Runxuan Yang, Guo Chen, and Xiaolin Hu. Sonicsim: customizable simulation platform for speech processing in moving sound source scenarios. CoRR, abs/2410.01481, 2024. [54] Antony W. Rix, John G. Beerends, Michael P. Hollier, and Andries P. Hekstra. Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs. In IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2001, 7-11 May, 2001, Salt Palace Convention Center, Salt Lake City, Utah, USA, Proceedings, pages 749752. IEEE, 2001. [55] Jesper Jensen and Cees H. Taal. An algorithm for predicting the intelligibility of speech masked by modulated noise maskers. IEEE ACM Trans. Audio Speech Lang. Process., 24(11):2009 2022, 2016. [56] Chandan K. A. Reddy, Vishak Gopal, and Ross Cutler. Dnsmos P.835: non-intrusive perceptual objective speech quality metric to evaluate noise suppressors. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022, pages 886890. IEEE, 2022. [57] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2849228518. PMLR, 2023. [58] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, and Furu Wei. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE J. Sel. Top. Signal Process., 16(6):15051518, 2022. [59] Ke Chen, Jiaqi Su, Taylor Berg-Kirkpatrick, Shlomo Dubnov, and Zeyu Jin. Improving generalization of speech separation in real-world scenarios: Strategies in simulation, optimization, and evaluation. CoRR, abs/2408.16126, 2024. [60] Robin Scheibler, Youna Ji, Soo-Whan Chung, Jaeuk Byun, Soyeon Choe, and Min-Seok Choi. Diffusion-based generative speech source separation. In IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-10, 2023, pages 15. IEEE, 2023. [61] Gabriel Mittag, Babak Naderi, Assmaa Chehadi, and Sebastian MÃ¶ller. NISQA: deep cnnself-attention model for multidimensional speech quality prediction with crowdsourced datasets. In Hynek Hermansky, Honza CernockÃ½, LukÃ¡s Burget, Lori Lamel, Odette Scharenborg, and Petr MotlÃ­cek, editors, 22nd Annual Conference of the International Speech Communication Association, Interspeech 2021, Brno, Czechia, August 30 - September 3, 2021, pages 21272131. ISCA, 2021. [62] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. [63] Junyi Peng, Marc Delcroix, Tsubasa Ochiai, OldË‡rich Plchot, Takanori Ashihara, Shoko Araki, and Jan Ë‡CernockÃ½. Probing self-supervised learning models with target speech extraction. In 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW), pages 535539, 2024. [64] Junyi Peng, Marc Delcroix, Tsubasa Ochiai, OldË‡rich Plchot, Shoko Araki, and Jan Ë‡CernockÃ½. Target speech extraction with pre-trained self-supervised learning models. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1042110425, 2024. [65] Linfeng Yu, Wangyou Zhang, Chenpeng Du, Leying Zhang, Zheng Liang, and Yanmin Qian. Generation-based target speech extraction with speech discretization and vocoder. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1261212616. IEEE, 2024. [66] Zhang, LY Yao Qian, Yu, Wang, Yang, Liu, Zhou, and Qian. Ddtse: Discriminative diffusion model for target speech extraction. In IEEE Spoken Language Technology Workshop, 2024. [67] Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, and Jianyuan Zhong. Attention is all you need in speech separation. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021, pages 2125. IEEE, 2021. 14 [68] Ui-Hyeop Shin, Sangyoun Lee, Taehan Kim, and Hyung-Min Park. Separate and reconstruct: Asymmetric encoder-decoder for speech separation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [69] Robin San Roman, Pierre Fernandez, Hady Elsahar, Alexandre DÃ©fossez, Teddy Furon, and Tuan Tran. Proactive detection of voice cloning with localized watermarking. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. [70] Yuankun Xie, Haonan Cheng, Yutian Wang, and Long Ye. Domain generalization via aggregation and separation for audio deepfake detection. IEEE Transactions on Information Forensics and Security, 19:344358, 2024. [71] Naoyuki Kamo, Marc Delcroix, and Tomohiro Nakatani. Target speech extraction with conditional diffusion model. In Naomi Harte, Julie Carson-Berndsen, and Gareth Jones, editors, 24th Annual Conference of the International Speech Communication Association, Interspeech 2023, Dublin, Ireland, August 20-24, 2023, pages 176180. ISCA, 2023. [72] Pin-Jui Ku, Alexander H. Liu, Roman Korostik, Sung-Feng Huang, Szu-Wei Fu, and Ante Jukic. Generative speech foundation model pretraining for high-quality speech extraction and restoration. CoRR, abs/2409.16117, 2024. [73] Hao Ma, Rujin Chen, Ruihao Jing, Xiao-Lei Zhang, Ju Liu, and Xuelong Li. Enhancing intelligibility for generative target speech extraction via joint optimization with target speaker ASR. CoRR, abs/2501.14477, 2025. [74] Katerina ZmolÃ­kovÃ¡, Marc Delcroix, Keisuke Kinoshita, Takuya Higuchi, Atsunori Ogawa, and Tomohiro Nakatani. Speaker-aware neural network based beamformer for speaker extraction in speech mixtures. In Francisco Lacerda, editor, 18th Annual Conference of the International Speech Communication Association, Interspeech 2017, Stockholm, Sweden, August 20-24, 2017, pages 26552659. ISCA, 2017. [75] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur. X-vectors: Robust DNN embeddings for speaker recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2018, Calgary, AB, Canada, April 15-20, 2018, pages 53295333. IEEE, 2018. [76] Ying Hu, Haitao Xu, Zhongcun Guo, Hao Huang, and Liang He. Smma-net: An audio cluebased target speaker extraction network with spectrogram matching and mutual attention. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024, pages 14961500. IEEE, 2024. [77] Xue Yang, Changchun Bao, Jing Zhou, and Xianhong Chen. Target speaker extraction by directly exploiting contextual information in the time-frequency domain. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024, pages 1047610480. IEEE, 2024. [78] Lei Yang, Wei Liu, Lufen Tan, Jaemo Yang, and Han-Gil Moon. Target speaker extraction with ultra-short reference speech by VE-VE framework. In IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-10, 2023, pages 15. IEEE, 2023. [79] Bang Zeng, Hongbin Suo, Yulong Wan, and Ming Li. Sef-net: Speaker embedding free target speaker extraction network. In Naomi Harte, Julie Carson-Berndsen, and Gareth Jones, editors, 24th Annual Conference of the International Speech Communication Association, Interspeech 2023, Dublin, Ireland, August 20-24, 2023, pages 34523456. ISCA, 2023. [80] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Generative models in TSE Recent works [71, 38] have introduced TSE systems utilizing diffusion probabilistic models, with similar methods being explored for SS [60]. SpeechFlow [22] proposed generative model pretrained on large-scale speech data using flow matching and masked conditions, and Ku, et al. (2024) [72] adapted this method for TSE. Additionally, Yu, et al. (2024) [65] introduced discrete token-based TSE approach by combining speech discretization with vocoder techniques and Ma, et al. (2025) [73] adopted the Whisper audio encoder to improve the intelligibility of generative TSE. These models are capable of generating audio samples with high naturalness and demonstrate good generalization to out-of-domain data. However, they often yield inferior results on reference-based metrics [43]. To further enhance perceptual quality, researchers have begun integrating generative models with In contrast, our proposed SoloSpeech employs fully discriminative models [44, 19, 24, 66]. generative approach, achieving superior perceptual quality, naturalness, and intelligibility. A.2 Conditional information in TSE Conditional information plays pivotal role in TSE by isolating the target speaker and attending to the mixture audio through various fusion layers, such as concatenation [5], multiplication [8], and factorized layers [74, 8]. Existing methods for extracting conditional information can be categorized into speaker embedding-based approaches and speaker embedding-free approaches. Speaker embeddingbased approaches include SpEx+ [14], which trained speaker embeddings from scratch using an auxiliary speaker identification task, X-SepFormer [15], built upon x-vectors [75], X-TF-GridNet [16], which utilized adaptive speaker embedding fusion. [63, 64] used pre-trained self-supervised learning (SSL) models to derive speaker embeddings. Conversely, speaker embedding-free approaches such as SMMA-Net [76] and CIENet [77] employed attention mechanisms to interact with the representations of the reference and mixed signals, VE-VE [78] used an RNN-based voice extractor to capture speaker characteristics, and SEF-Net [79] along with USEF-TSE [17] utilized cross-attention to implicitly leverage speaker information. Our proposed method adopts speaker embedding-free approach, eliminating the need for pre-trained model to extract fixed-length speaker embeddings or align the speaker embedding space with the target latent space. Compared to speaker embedding-based methods, our approach preserves local dynamics and temporal structures, which are crucial for guiding more effective speaker extraction."
        },
        {
            "title": "B Diffusion probabilistic model",
            "content": "Diffusion Probabilistic Models (DPMs) include forward and backward process. In the forward process, Gaussian noise is progressively added to the data following pre-defined variance schedule Î²1, . . . , Î²T . (x1:T x0) := (cid:89) t=1 (xt xt1) (xt xt1) := (cid:16) xt; (cid:112)1 Î²txt1, Î²tI (cid:17) The forward process allows sampling xt at an arbitrary timestep in closed form: (xt x0) := (cid:0)xt; Î±tx0, (1 Î±t) I(cid:1) Equivalently: xt := 1 Î±tÎ¾, where Î¾ (0, I) where Î±t := 1 Î²t and Î±t := (cid:81)t DPMs learn the reverse process to recover information incrementally. In this manner, DPMs can generate new data from random Gaussian noise. When Î²t is small, the reverse step is also Gaussian: Î±tx0 + s=1 Î±s. (2) (3) (4) (5) pÎ¸ (x0:T ) := (xT ) (cid:89) t=1 pÎ¸ (xt1 xt) (6) 16 pÎ¸ (xt1 xt) := (cid:16) xt1; Âµt, Î²tI (cid:17) where variance Î²t can be calculated using the forward process posteriors: Î²t := 1 Î±t1 1 Î±t Î²t Neural networks are typically used to predict noise Î¾, since: Î²t (cid:18) Âµt := xt (cid:19) Î¾ 1 Î±t 1 Î±t (7) (8) Following [37], we introduce modified noise schedule with zero terminal SNR, where the SNR can be calculated as: SNR(t) := Î±t 1 Î±t (9) Figure 5: Relations between vt, xt, x0 and Î¾. We adjust the noise schedule to enforce zero terminal SNR(T ) by keeping Î±T to zero, and linearly rescaling Î±t for intermediate [2, . . . , 1] respectively. Î±1 unchanged, changing When the SNR reaches zero at the terminal step, predicting noise Î¾ becomes meaningless because the input and output are identical. Consequently, the neural network is modified to predict velocity vt instead, as present in [80]: Î±tÎ¾ Î±tvt + vt := Î¾ = 1 Î±tx0 1 Î±txt According to (5) and (8), the backward process is performed by the following functions: Î±txt 1 Î±tvt x0 := Î±t1Î²t 1 Î±t Âµt := x0 + Î±t (1 Î±t1) 1 Î±t xt (10) (11) (12) (13) At the terminal timestep, the neural network trained to predict velocity estimates the mean of the data distribution under the given conditions. The diffusion sampler consistently initiates from the final timestep during inference."
        },
        {
            "title": "C Model configurations",
            "content": "C.1 Audio compressor The STFT and its inverse (iSTFT) utilize window size of 512 and hop size of 256, resulting in latent representations at 62.5 Hz. The dimensionality is set to 128. In the encoder, Conv2D layer is first applied to the complex-valued spectral input, with kernel size of 3 3, zero-padding size of 1 1, and 128 channels, followed by Group Normalization. Three TF-GridNet blocks are then applied. The embedding dimension for each T-F unit is 128, the kernel size for Unfold and Deconv1D layers is 1, the stride size for Unfold and Deconv1D layers is 1, the number of hidden units in bidirectional LSTMs (BLSTMs) in each direction is 256, the number of output channels in 17 the 1 1 Conv2D layers to obtain query and key tensors in the self-attention module is 512, and the number of heads in self-attention is 4. After that, we reshape the features and use Conv1D layer with channel size of 128. The PReLU function is used for all activations. The decoder is mirror of the encoder. The model has total of 49.3 million parameters. Loss Functions: We employed the following loss components: Reconstruction Loss: Based on perceptually weighted multi-resolution STFT [35] with window sizes of [1280, 640, 320, 160, 80, 40, 20] and hop sizes of [320, 160, 80, 40, 20, 10, 5]. Adversarial Loss with Feature Matching: Utilizing five convolutional discriminators as described in Encodec [36], this loss employs fixed mel bin size of 64, window sizes of [1280, 640, 320, 160, 80], and hop sizes of [320, 160, 80, 40, 20]. KL Divergence Loss: Down-weighted by 1 104. The final loss weightings are set to 1.0 for the multi-resolution STFT loss, 0.1 for the adversarial loss, and 5.0 for feature matching. C.2 Target extractor We developed three different model sizes for the target extractor: Small, Base, and Large. Unless otherwise specified, all results presented in this paper use the Base model. The performance of these three models can be found in Table H.5. In the Transformer column, the numbers denote the number of layers the model dimension, the number of heads, and the multiples of hidden size. Table 5: Details of the target extractor models. Model Params (M) MACs (G/s) Learning rate Diffusion Transformer Condition Transformer Small Base Large 50.6 200.8 474.1 1.2 4.8 11. 2 104 1 104 5 105 12, 384, 6, 2 12, 768, 12, 4 16, 1024, 16, 4 12, 192, 6, 2 12, 384, 6, 4 16, 512, 8, 4 C.3 Corrector Following [19], we employed the Noise Conditional Score Network (NCSN++) architecture as described in [20], which is based on multi-resolution U-Net structure operating on complex-valued STFT. The model has total of 65.6 million parameters. For detailed information about the network, please refer to [20]."
        },
        {
            "title": "D Subjective Evaluations",
            "content": "For real-world TSE evaluation, we followed the protocol from [59], assessing both separation accuracy and perceptual quality through Mean Opinion Score (MOS) test conducted with 12 human raters recruited via Prolific10. We randomly selected 100 test samples from the CHiME-5 and RealSEP datasets, ensuring that each sample was annotated by three raters. Each listener rated the separation quality of two isolated tracks individually on 15 scale (1 = Bad, 5 = Excellent), using the original mixture as reference. We visualize the annotation interfaces for TSE in Figure 6, 10https://www.prolific.com/ 18 Figure 6: Annotation UI for TSE."
        },
        {
            "title": "E Baselines",
            "content": "E.1 TSE baselines TD-SpeakerBeam [49]: time-domain discriminative model that investigates strategies for improving speaker discrimination capability. We utilized their open-source code, which is available at https://github.com/BUTSpeechFIT/speakerbeam. 19 DPCCN [10]: densely-connected pyramid complex convolutional network serving as time-frequency domain discriminative model. We used the results reported in [66]. SUPERB-TSE [63]: discriminative model based on large-scale pre-trained SSL models. We reached out to the authors to obtain the test results. SSL-MHFA [64]: discriminative model based on large-scale pre-trained SSL models with Adaptive Input Enhancer (AIE) and speaker encoder. The test results were requested from the authors. X-TF-GridNet [16]: time-frequency domain discriminative model using TF-GridNet as its backbone. It extracts speaker embeddings using U2-Net style network. We used their opensource code, which is available at https://github.com/HaoFengyuan/X-TF-GridNet. USEF-TSE [17]: speaker embedding-free discriminative model. We implemented the time-frequency model, i.e., USEF-TFGridNet, as described in the paper. The open-source code is available at https://github.com/ZBang/USEF-TSE. DiscreteTSE [65]: discrete token-based TSE approach that combines speech discretization and vocoder techniques. We used the results reported in the paper. DDTSE [66]: generative model that applies the same forward process as diffusion models and uses reconstruction loss similar to discriminative methods. We used the results reported in the paper. Diff-TSE [18]: time-frequency domain generative model based on conditional diffusion model. Since no open-source code is available, we reproduced the results based on the paper. DPM-TSE [38]: diffusion probabilistic model operating on the mel-spectrogram. We utilized pretrained WavLM-based speaker verification network as the speaker embedding network. The source code is available at https://github.com/haidog-yaqub/DPMTSE. SoloAudio [23]: latent diffusion model with time-domain audio VAE as the latent features. We utilized pretrained WavLM-based speaker verification network as the speaker embedding network. We utilized their open-source code, which is available at https: //github.com/WangHelin1997/SoloAudio. E.2 SS baselines SepFormer [67]: time-domain discriminative model featuring transformer-based backbones. We used the open-source code at https://github.com/speechbrain/ speechbrain/tree/develop/recipes/LibriMix. [32]: time-frequency several multi-path blocks with convolutional TF-GridNet posed of We used 5d4615f4264574b52c9458c2d1ddc52814400847/espnet2/enh/separator/ tfgridnet_separator.py. comlayers and BLSTMs. https://github.com/k5-inoue/espnet/blob/ discriminative model domain version the at SepReFormer [68]: time-domain discriminative model based on SepFormer, utilizing separation-and-reconstruction framework. The code is available at https://github.com/ dmlguq456/SepReformer. SPMamba [33]: time-frequency domain discriminative model that builds upon TFGridNet by replacing traditional BLSTM modules with bidirectional Mamba modules. We utilized their open-source code, which is available at https://github.com/JusperLee/ SPMamba. DiffSep [60]: Applies diffusion approach in the time-frequency domain. The open-source code is available at https://github.com/fakufaku/diffusion-separation. SpeechFlow [22]: generative model based on flow matching, followed by discriminative model that requires large-scale pre-training stage. We used the results reported in the paper. Fast-GeCo [19]: Utilizes generative corrector in the time-frequency domain, initialized with the SepFormer model. We used the results reported in the paper."
        },
        {
            "title": "F Speech Separation with SoloSpeech",
            "content": "Although SoloSpeech is trained to extract one speaker from the mixture audio, we trained target speech removal model to remove the target speaker and extract the other speaker. This approach allows for the separation of both speakers voices. As illustrated in Figure 7, the diffusion transformer in the target speech removal model receives four inputs: the noised latent, the mixture latent, the reference latent, and the current timestep. During training, the target latent is obtained from the excluded speakers speech, while the reference latent is derived from the target speakers speech. In inference, the reference latent is obtained from the output of the target extractor model. We used AdamW optimizer with learning rate of 1 104, weight decay of 1 104, batch size of 32, and for 100 epochs. The model was trained on one NVIDIA A100-80GB GPU for three days. Figure 7: Overview of the target speech removal model. The whole pipeline of two-speaker separation using SoloSpeech is illustrated in Figure 8. The target speech is first extracted and then removed, resulting in target speech and interference speech. Another naive manner to achieve speech separation is to simply reuse the original TSE model with the other speakers cue to extract the remaining speech. However, we find in practice that this approach does not produce high-quality complementary separation. We tried such approach that yielded an SI-SNR of 12.35 dB, below the 13.92 dB achieved by our separation method (in Table 3). Figure 8: Overview of two-speaker separation pipeline."
        },
        {
            "title": "G Computational Cost Analysis",
            "content": "We compare the computational cost of SoloSpeech with the state-of-the-art discriminative model (USEF-TSE) and the state-of-the-art generative model (SoloAudio), as summarized in Table 6. Both SoloSpeech and SoloAudio require considerably less training time than USEF-TSE due to the latentspace processing, enabling more practical large-scale training. While the real-time factor (RTF) of SoloSpeech is slightly higher, it remains suitable for offline or server-side deployment. 21 Since the primary motivation of this work is to achieve consistent improvements in the quality and intelligibility of TSE, particularly in terms of generalization, we argue that modest increase in inference time is reasonable trade-off. Furthermore, SoloSpeechs modular architecture (compressor, extractor, corrector) supports the substitution of each component with more lightweight alternatives, which we plan to explore in future work. Table 6: Comparison of computational cost on single NVIDIA A100-80GB GPU. Method Params (M) MACs (G/s) Training-time (hour per epoch) Real-time Factor (RTF) USEF-TSE SoloAudio SoloSpeech 19.7 249.1 315.7 129.0 354.0 418.5 2.80 0.92 1.05 0.34 0.66 0."
        },
        {
            "title": "H Ablation Studies",
            "content": "H.1 Comparison of audio compressors In the audio compressors reconstruction experiments, we utilized set of audio quality metrics: STFT distance, MEL distance (as implemented in [35] with default parameters), PESQ, ESTOI, and SI-SNR. As shown in Table 7, the T-F Audio VAE consistently outperforms the Stable Audio VAE across all metrics, demonstrating superior audio reconstruction quality. Table 7: Reconstruction results of different audio compressors on the Libri2Mix dataset. Data Model STFT-D Mel-D PESQ ESTOI SISNR Clean Single Noisy mixture Stable Audio VAE [30] T-F Audio VAE (ours) Stable Audio VAE [30] T-F Audio VAE (ours) 0.540 0.432 0.570 0.450 0.231 0.116 0.147 0.076 4.151 4. 4.327 4.529 0.984 0.995 0.984 0.995 17.094 18.379 14.575 15.571 H.2 Comparison of different settings of the audio compressor We conducted experiments to evaluate the reconstruction quality under different audio compressor settings, including variations in frame rate and latent dimension. Table 8 summarizes the impact of these configurations. Lower frame rates and smaller latent dimensions generally make it easier to train latent diffusion models. Ultimately, we select frame rate of 50 Hz and latent dimension of 128 as trade-off between training efficiency and reconstruction quality. Table 8: Impact of different settings for the audio compressor on the noisy mixture. Frame Rate Latent Dim STFT-D Mel-D PESQ ESTOI SISNR 25 Hz 50 Hz 100 Hz 64 128 64 64 128 0.596 0.522 0.510 0.450 0.454 0.440 0.182 0.133 0.128 0. 0.075 0.071 3.979 4.204 4.268 4.529 4.510 4.530 0.978 0.988 0.989 0. 0.995 0.995 14.105 14.996 15.080 15.571 15.499 15.688 H.3 Comparison of Masking Ratios We explored the influence of different masking ratios ranging from 0% to 50% during the training of the corrector. We measured SI-SNR for perceptual quality (higher values indicate better quality) and 22 WER for intelligibility (lower values indicate better performance). As shown in Figure 9, increasing the masking ratio initially improves overall performance but eventually degrades it, resulting in an optimal ratio of 30%. We argue that the masking ratio determines the learning dynamics of the corrector: small ratio causes the model to focus more on the reference audio, while high ratio leads the model to consider more of the mixture audio. Striking balance between these factors yields the best perceptual quality and intelligibility for the TSE task. Figure 9: Results of different masking ratios (Î¸) for the corrector. H.4 Comparison of audio durations SoloSpeech was trained on audio segments of varying lengths, ranging from 3 to 10 seconds. The model also supports longer input thanks to the Rotary Position Embeddings (RoPE) in the Transformer backbone. In our experiments on Libri2Mix, CHiME-5, and RealSEP, the input segments range from 3 to 20 seconds, and we observe no significant degradation in performance as the input length increases, which is shown in Table 10. This indicates that SoloSpeech is robust to variations in audio duration. Table 9: Comparison of different audio durations. Audio Length (s) SISNR DNSMOS WER 2.5-3.5 5.5-6.5 > 9 all 11.08 11.16 11.20 11.12 3.79 3.73 3.70 3.76 0.18 0.16 0.15 0. H.5 Comparison of different model sizes of the target extractor We present results for different model sizes of the target extractor in Table 10. Table 10: Comparison of different model sizes of the target extractor. No corrector is used. Model Size SISNR DNSMOS WER Small Base Large 7.68 8.10 8.40 3.71 3.76 3.77 0.26 0.24 0."
        },
        {
            "title": "I Visualization analysis",
            "content": "To intuitively showcase the extraction performance of SoloSpeech, we provide several visualization examples illustrated in Figure 10. The spectrograms below compare the inference results of 23 SoloSpeech and USEF-TSE applied to the same audio input, alongside the ground truth. All samples demonstrate that SoloSpeech achieves finer reconstruction, particularly at high frequencies, compared to USEF-TSE. In addition, SoloSpeech exhibits superior noise reduction and effectively prevents spectrum leakage, as evidenced in Samples IV and V. Details of these samples are as follows: Sample I: SoloSpeech achieves better noise reduction and reconstructs high frequencies more accurately. In contrast, USEF-TSE introduces artifacts at the beginning and end of the speech. Sample II: SoloSpeech provides superior reconstruction of high frequencies and better recovery of the speech onset. Sample III: SoloSpeech produces fewer artifacts, resulting in cleaner speech. Sample IV: SoloSpeech reconstructs high frequencies much better and introduces fewer artifacts in all the frequencies. Sample V: USEF-TSE introduces some artifacts at the beginning of the speech and results in more noise across all frequency bands. (a): Sample I: Ground truth (b): Sample I: SoloSpeech (c): Sample I: USEF-TSE (d): Sample II: Ground truth (e): Sample II: SoloSpeech (f): Sample II: USEF-TSE (g): Sample III: Ground truth (h): Sample III: SoloSpeech (i): Sample III: USEF-TSE (j): Sample IV: Ground truth (k): Sample IV: SoloSpeech (l): Sample IV: USEF-TSE (m): Sample V: Ground truth (n): Sample V: SoloSpeech (o): Sample V: USEF-TSE Figure 10: Comparison of the spectrograms of the ground truth, audio extracted by SoloSpeech and by USEF-TSE. 24 To analyze the impact of the corrector, we present three visualization examples in Figure 11. The corrector primarily functions by reducing background noise and artifacts in the high frequencies. Details of these samples are as follows: Sample I: The corrector reduces background noise artifacts and better reconstructs frequency details. Without the corrector, there is an obvious mispronunciation in the middle of the speech. Sample II: The corrector effectively reduces artifacts, especially affecting the last phoneme in this speech. Sample III: Using the corrector results in less noise production and clearer frequency bands. (a): Sample I: Ground truth (b): Sample I: w/ corrector (c): Sample I: w/o corrector (d): Sample II: Ground truth (e): Sample II: w/ corrector (f): Sample II: w/o corrector (g): Sample III: Ground truth (h): Sample III: w/ corrector (i): Sample III: w/o corrector Figure 11: Comparison of the spectrograms of the ground truth, audio extracted by SoloSpeech with corrector and by SoloSpeech without corrector."
        }
    ],
    "affiliations": [
        "Brno University of Technology",
        "Johns Hopkins University",
        "Nanyang Technological University",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}