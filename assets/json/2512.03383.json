{
    "paper_title": "UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs",
    "authors": [
        "Hung-Yueh Chiang",
        "Chi-Chih Chang",
        "Yu-Chen Lu",
        "Chien-Yu Lin",
        "Kai-Chiang Wu",
        "Mohamed S. Abdelfattah",
        "Diana Marculescu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deploying large language models (LLMs) on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 2 3 8 3 3 0 . 2 1 5 2 : r UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs Hung-Yueh Chiang1, Chi-Chih Chang2, Yu-Chen Lu3, Chien-Yu Lin4, Kai-Chiang Wu3, Mohamed S. Abdelfattah2, and Diana Marculescu1 1 Chandra Family Department of Electrical and Computer Engineering, The University of Texas at Austin 2Department of Electrical and Computer Engineering, Cornell University 3Department of Computer Science, National Yang Ming Chiao Tung University 4The Paul G. Allen School of Computer Science and Engineering, University of Washington # {hungyueh.chiang, dianam}@utexas.edu, {cc2869, mohamed}@cornell.edu, {yuchen.cs11, kcw}@cs.nycu.edu.tw, {cyulin}@cs.washington.edu Abstract Deploying large language models (LLMs) on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by on the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, unified post-training quantization and low-rank compression framework, with on-device configurable pruning rates for edge LLMs. UniQL is general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to cater to diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting that speeds up the computation by 20, quantization-aware singular value decomposition (SVD) decompositions to minimize the quantization errors, state-aware weight sorting for SSMs, and fused rotary embedding (RoPE) kernel for the pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in one-pass fashion, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models offer memory reduction of 45.7 and token throughput improvement of 2.73.4, maintaining accuracy within 5% of the original models at 15% pruning rates across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are released at: https://github.com/enyac-group/UniQL."
        },
        {
            "title": "1 Introduction",
            "content": "Numerous emerging applications, such as question answering on VR/AR glasses, are powered by large language models (LLMs). Yet, models with parameters on the order of billions (e.g., 10B) restrict the platforms and applications that can utilize them. Extensive research investigates quantization [Xiao et al., 2023, Lin et al., 2024a,b] and compression [Qinsi et al., 2025, Wang et al., 2025b, Lin et al., 2025, Wang et al., 2025c] for LLMs to lower memory and computing needs for deployment. However, the limited and shared resources (e.g., the unified memory architecture) on edge devices still pose huge challenges for model deployment. Since resources (e.g., memory) are dynamically managed by the operating system, the availability of the resources highly depends on the system workload. As result, the pre-compressed or pre-quantized language models with fixed model sizes may not run on device under high workload scenarios. # Corresponding authors. 1 Figure 1: (Proposed framework overview.) UniQL supports Transformers, SSMs, and hybrid models, enabling one-shot compression using single server-class GPU. The on-device pruning of the quantized model is feasible and configurable based on the current device workload. We present actual latency on Nano 8G in relation to accuracy for different pruning rates across three distinct models on the right. Circle sizes correspond to model sizes. Latency is measured using 512 prefilling tokens and 512 generated tokens on Nano. Re-compressing or re-quantizing the model to fit it into available memory is not practical due to the high computational costs, i.e., several hours on cloud GPUs [Lin et al., 2025, Frantar et al., 2023]. solution to address this issue is storing several model replicas at different compression rates. Nonetheless, producing pre-compressed replicas of different sizes is both timeand storage-consuming. Alternatively, employing elastic training [Cai et al., 2024, 2025] to pre-trained model enables the derivation of various sizes from the model. Yet, this approach requires availability of GPU resources and training on curated datasets to support flexible deployment for one specific type of model, e.g., Llama-3.1-8B, limiting the applicability. Our proposed work addresses this issue under the post-training setting when access to server-class GPUs and curated datasets is limited. As illustrated in Figure 1, our framework supports quantization and structured pruning, performing efficiently on one server GPU. Our objective is to support and design compression algorithms for various model architectures, including Transformers, State Space Models (SSMs), and hybrid models. Our pipeline is shown in Figure 2. We group the weights within the block, gather channel corrections from calibration dataset, and apply weight-sorting algorithms. Our multi-layer perceptron (MLP) weights are decomposed without any gradient information or expensive full matrix pseudo-inverse, yielding speedup of 20 compared to prior art [Lin et al., 2025]. For ùëäùë£ and ùëäùëú in self-attention layers, we develop quantization-aware singular value decomposition (SVD) of weights to minimize quantization errors. For SSMs and hybrids models, we find that SSM blocks are particularly sensitive to state matrices, and propose state-aware weight-sorting strategy to mitigate this. We then apply masked fine-tuning to the sorted model. In each fine-tuning step, global pruning rate ùëÉùë° is chosen randomly, masking the least ranked channels in the layers. The refined model is then quantized in low bit-width and deployed on the edge platform. The entire process is performed once in the cloud. For the deployed model, we prune the models according to specified global pruning rate, e.g., ùëÉ35 = 35%, on the edge device. Our contributions are summarized as follows: Our study explores broad spectrum of models, such as Transformers, SSMs, and hybrid, and introduces efficient pruning and quantization-friendly algorithms for these blocks. To the best of our knowledge, UniQL is the first post-training framework that systematically combines quantization and structured pruning for LLMs in one-shot fashion. We develop an integrated kernel to support the pruned RoPE, conducting comprehensive profiling to demonstrate 2.73.4 latency speedups for adaptive pruning on edge devices. 2 Figure 2: (The UniQL pipeline.) We devise pseudo-inverse-free, quantization-aware, and state-aware matrix decomposition methods for the grouped weights to obtain sorted weights (a). During fine-tuning, we sample global pruning rates, and masked out the weight channels (b). The refined patches are fused into the weights, followed by model quantization for deployment (c). Based on the system utilization, we perform on-device adaptive pruning of the quantized model (d)."
        },
        {
            "title": "2 Related work",
            "content": "Transformer compression. Prior work has aimed to reduce the size of Transformer-based LLMs for efficient deployment by utilizing low bit-width data types [Xiao et al., 2023, Lin et al., 2024b,a, Zhao et al., 2024, Ashkboos et al., 2024b, Liu et al., 2025], minimizing storage needs and optimizing hardware for low-bit computations. Unstructured [Frantar and Alistarh, 2023, Sun et al., 2024] and semi-structured pruning (e.g., N:M sparsity) [Li et al., 2023] for reducing model size by removing specific parameters while minimizing accuracy loss. Nonetheless, deploying such methods requires specialized hardware [Taka et al., 2025, Xia et al., 2023]. Structured pruning [Wang et al., 2025a, Lin et al., 2025, Ma et al., 2023, Ashkboos et al., 2024a] removes whole elements (e.g., channels and heads), enabling faster inference on standard hardware but potentially decreasing performance. Some studies focus on one-shot compression [Genzel et al., 2025, Wang et al., 2025c], flexible bit-width quantization [Park et al., 2024], and quantization with semi-structured sparsity [Mozaffari et al., 2025]. Wang et al. [2025c] propose solution for any size compression in FP16 models by iteratively adding SVD residual terms, but this method incurs substantial overhead, leading to higher latency compared to the original FP16 model. Our framework is systematically designed for quantization and on-device structured pruning with practical speedups. State Space Models are memory-efficient alternatives to Transformers. Recent studies [Xu et al., 2025, SSM compression. Chiang et al., 2025b,a] introduce low-bit quantization techniques for SSMs. Structured [Taghibakhshi et al., 2025, Mu√±oz et al., 2025] and unstructured pruning [Tuo and Wang, 2025, Shihab et al., 2025] strategies have been developed for SSMs. For example, Taghibakhshi et al. [2025] eliminate the SSM heads and restore performance through knowledge distillation training. Mu√±oz et al. [2025] explore block-wise (e.g., Mamba and Transformer blocks) and module-wise (e.g., SSM and self-attention modules) structured pruning methods. Some work explores the token pruning [Zhan et al., 2024] or dimension reduction [Chi et al., 2024] for vision SSMs. Our focus is on analyzing broader structured pruning and quantization framework for Transformers, SSMs, and hybrids, distinguishing it from previously mentioned approaches. Elastic training for LLMs. Elastic training aims to enable pre-trained LLM to dynamically adapt to varying deployment constraints such as memory, compute, and latency budgets. Flextron [Cai et al., 2024] and LLaMaFlex [Cai et al., 2025] introduce many-in-one architectures through pruning and weight sharing, allowing adaptive inference under dynamic resource constraints. Jet-Nemotron [Gu et al., 2025] further leverages post-training neural architecture search to generate compact LLM variants. These methods require GPU resources and training on curated datasets for flexible deployment tailored to particular model type and size, e.g., Llama-3.1-8B, thereby restricting their general applicability. In contrast, our work focuses on post-training on single server GPU for various architectures and supports on-device adaptation."
        },
        {
            "title": "3.2 Structured Weight Sorting\nOur objective is to enable adaptive on-device pruning by sorting the weights according to their importance scores, allowing\nthe device to prune the least significant columns. Inspired by recent studies [Lin et al., 2025, Koike-Akino et al., 2025], we\ngroup the weights and conduct joint decomposition, as shown in Figure 3. We co-design the pruning algorithm alongside\nquantization and fused kernels for Transformers, SSMs, and hybrids.",
            "content": "Multi-layer perceptron (MLP). The MLP includes up Wùë¢ Rùê∑h ùê∑int and down projections Wùëë Rùê∑int ùê∑h, with an optional gate projection Wùëî Rùê∑h ùê∑int. The formulation is defined as ùëìMLP(X) = (ùúé (cid:0)XWùëî(cid:1) XWùë¢)Wùëë . To derive Sùëö for sorting the weight matrices in the MLP layer, we collect the intermediate activation Xint = ùúé (cid:0)XWùëî(cid:1) XWùë¢ intXint Rùê∑int ùê∑int . We average the correlation matrix from the calibration set and calculate the channel correlation = over the calibration set, and compute the ridge leverage scores [McCurdy, 2018] defined by diag (cid:0)C(C + ùúÜùêº ) 1(cid:1). We set ridge lambda ùúÜ = 1 in our experiments. We use the scores and create column sorting matrix Sùëö Rùê∑int ùê∑int that reorders the output columns for Wùë¢ and Wùëî as Wùë¢Sùëö and WùëîSùëö, and the input rows of the Wùëë as ùëöWùëë , as shown in Figure 3 (a). We show the process in Algorithm 1. (1) Algorithm 1 Structured sorting for MLP. 1: Input: Up projection Wùë¢ Rùê∑h ùê∑int, gate projection Wùëî Rùê∑h ùê∑int , down matrix Wùê∑ Rùê∑int ùê∑h , and hidden states (cid:17) (cid:16) hWùë¢ , ùëñ = 1, ..., ùëÅ Xùëñ hWùëî ùëñ=1 Xùëñ (cid:205)ùëÅ Rùëá ùê∑h from ùëÅ calibration samples ùëñ = 1, ..., ùëÅ , and ridge intensity ùúÜ. Xùëñ 2: Xùëñ Xùëñ int = ùúé 3: = 1 int, Rùê∑int ùê∑int intXùëñ ùëÅ 4: ùë† diag (cid:0)C(C + ùúÜùêº ) 1(cid:1) , ùë† Rùê∑int 5: Sùëö Iùê∑int [:, argsort(ùë†)], Sùëö Rùê∑int ùê∑int 6: Pseudo-inverse-free (Ours) 7: return (Wùë¢, Wùëî, Wùëë ) (cid:0)Wùë¢Sùëö, WùëîSùëö, ùëöWùëë (cid:1) Average the correlation matrix over the samples Compute ridge leverage scores Get the sorting matrix based on the vector ùë† Output the structured sorted weights Our approach does not rely on time-consuming pseudo-inverse to sort the MLP weight matrices. Although the pseudo-inverse (i.e., Moore-Penrose inverse [Penrose, 1955]) provides theoretical bound in pruning errors [Lin et al., 2025], it exhibits three major drawbacks: (1) Pseudo-inverse has complexity of ùëÇ (ùëõ3) for ùëõ-size squared matrix. This is particularly time-consuming when computing the pseudoinverse of correlation matrices in MLP layers because ùê∑int is large number in most LLM designs, e.g.,, Llama-3-8B ùê∑int = 14336. (2) Pseudo-inverse computation requires high-precision FP64 to maintain numerical stability [Lin et al., 2025],which demands substantial memory usage for full-precision weights. (3) Matrix inverse breaks the equivalence of pruned weights, resulting in (W) (W), requiring recomputation for different pruning rates. Here, Rùê∑ ùê∑ . is submatrix of W, where Rùê∑ ùê∑ with ùê∑ > ùê∑ . represents the inverse matrix. We show the latency of pseudo-inverse computation for FP64 matrices on A6000 in Table 1. Table 1: (Pseudo-inverse.) Pseudoinverse latency for FP64 matrices on A6000 (in minutes). Matrix Size Lat. (min.) [1024, 1024] [4096, 4096] [8192, 8192] 0.02 0.57 4.24 [14336, 14336] 20.58 4 Figure 3: (Joint weight decomposition.) We visualize the group of sorted weights in MLP (a), MHSA (b), and Mamba (c) blocks. The group of weights for joint decomposition is shown in the same background color, e.g., Wùëû and Wùëò in the pink background, and other groups are distinguished by different colors. We devise different types of joint compression algorithms that are efficient and quantization-aware to support on-device pruning. Algorithm 2 Structured sorting key-query for MHSA with ùêª heads. 1: Input: MHSA query matrices Wùëû Rùê∑h (ùêª ùê∑hd ) , key matrices Wùëò Rùê∑h (ùêª ùê∑hd ) , hidden states Xùëñ Rùëá ùê∑h from ùëÅ calibration samples ùëñ = 1, ..., ùëÅ , and the function of rotary positional embedding ùúå (). Apply sorting to each head independently 4: hW hW ùëó ùëû) , ùëó ùëò ) , ùëó ùëû Rùê∑hd ùê∑hd ùëó ùëò Rùê∑hd ùê∑hd 2: for ùëó = 1, . . . , ùêª do ùëó ùëó (cid:205)ùëÅ ùëû = 1 ùëû)ùúå (Xùëñ ùëñ=1 ùúå (Xùëñ 3: hW ùëÅ ùëó ùëó (cid:205)ùëÅ ùëò = 1 ùëò )ùúå (Xùëñ ùëñ=1 ùúå (Xùëñ hW ùëÅ 1/2 1/2 ùëó ùëó , ùë† Rùê∑hd ùë† C ùëû ùëò Symmetric sorting for fused RoPE kernel (Ours) [ùë†1, ùë†2] ùë†, {ùë†1, ùë†2} Rùê∑hd/2 idxsym [argsort(ùë†1 + ùë†2), ùê∑dh/2 + argsort(ùë†1 + ùë†2)], Sùëûùëò Iùê∑dh [:, idxsym], Sùëûùëò Rùê∑dh ùê∑dh ùëó ùëó (W ùëûS ùëûùëò 11: end for 12: return (Wùëû, Wùëò ) ùëó ùëò ) (W ùëû, . . . , Wùêª 5: 6: 7: 8: 9: ùëó ùëû, , . . . , Wùêª ùëò ] ùëû ], [W1 ùëò [W1 ùëó ùëûùëò ) , ùëó ùëò 10: (cid:16) (cid:17) Query correlations Key correlations Calculate the norm score idxsym Rùê∑hd Split the norm score vector by half Get the symmetric sorted indices Get the sorting matrix based on the vector ùë† Concatenate the sorted heads Multi-head self-attention (MHSA). equivalent. The formulation of ùëñth head within the MHSA is provided as follows: For simplicity, we set the attention heads ùêªùë† and the key-value heads ùêªùëòùë£ as ùëìMHSA (X, ùëñ) = Softmax (cid:16) ùúå (cid:16) XWùëñ ùëû (cid:17) ùúå (cid:0)XWùëñ ùëò (cid:1) (cid:17) (cid:0)XWùëñ ùë£Wùëñ ùëú (cid:1) , (2) ùëû and Cùëñ ùëû and Wùëñ ùëò ùëò = Xùëñ , we obtain activations Xùëñ where ùúå () denotes RoPE [Su et al., 2021]. The weights in MHSA are divided into two groups: {Wùëñ ùëû, Wùëñ (cid:17), and compute the channel correlation Cùëñ For the Wùëñ ùëû = 1/2 Xùëñ Xùëñ Cùëñ , ùëû ùëò and averaged over the calibration sample. Since the embedding positions are broken by our structured sorting, we have to gather the corresponding indices for the rotary positional embeddings, i.e., sin and cos. Since RoPE is expressed as RoPE(X; ùúÉ ) = cos(ùúÉ ) + sin(ùúÉ ) R(X), where R(X) rotates by splitting into two components along the last axis: = [x1, x2] such that R(X) = [x2, x1], we split the dimension of ùë† by half such that [ùë†1, ùë†2] = ùë†, and apply sorting to ùë†1 + ùë†2, where {ùë†1, ùë†2} Rùê∑hd/2. As such, we construct the final sorting matrix Sùëûùëò Rùê∑hd ùê∑hd that sorts the output columns XWùëñ ùëò ùëò Rùê∑hd ùê∑hd . The sorting scores ùë† Rùê∑hd are calculated as ùë† = Cùëñ ùëò } and {Wùëñ (cid:17) and Xùëñ , where Cùëñ ùëû and Cùëñ ùëò = ùúå XWùëñ ùëû ùëû = ùúå ùë£, Wùëñ Xùëñ ùëò ùëú }. 1/ (cid:16) (cid:16) ùëû ùëò 5 Figure 4: (The fused kernel and SVD decomposition.) In the left illustration, gathering and slicing rotary positional embeddings by the index vector for ùëÑ and ùêæ are fused in one kernel to reduce memory access. The embeddings for the pruned head dimension ùê∑ are gathered from the index array Sùë†ùë¶ùëö in the fused kernel. On the right, we combine the hd diagonal matrix Œ£ with as the group shares quantization scaling factor to reduce the quantization errors. as WùëûSùëûùëò and Wùëò Sùëûùëò . Symmetric sorting is hardware-efficient since we only need to store and load half of the index vector into our fused RoPE kernel, as illustrated in Figure 4 (a). Algorithm 3 Structured sorting value-output for MHSA with ùêª heads. 1: Input: MHSA value matrices Wùë£ Rùê∑h (ùêª ùê∑hd ) , output matrices Wùëú R(ùêª ùê∑hd ) ùê∑h, hidden states Xùëñ Rùëá ùê∑h from ùëÅ calibration samples ùëñ = 1, ..., ùëÅ . 2: = Xùëñ Xùëñ , Rùê∑hd ùê∑hd Apply sorting to each head independently ùëó ùë£ ) SVD(C1/2W ùë£) ùëó ùëú ) ùë£ 3: for ùëó = 1, . . . , ùêª do (Uùë£, Œ£ùë£, 4: (U, Œ£, V) SVD(Œ£ùë£V 5: (W 6: 7: end for 8: return (Wùë£, Wùëú ) (cid:0)[W1 ùëó ùëú ) (C 1/2Uùë£UŒ£, V) ùë£, . . . , Wùêª ùëó ùë£, ùëâ ], [W1 ùëú, . . . , Wùêª ùëú ](cid:1) Quantization-aware SVD (Ours) Concatenate the sorted heads 1 1 ùë£Wùëñ 2 Wùëñ 2 Wùëñ ùë£Wùëñ ùë£)Wùëñ ùë£ and Wùëñ ùëú = SVD(C ùëú = Uùë£Œ£ùë£V For the Wùëñ ùëú , we perform an activation-scaled SVD decomposition [Wang et al., 2025a, Yuan et al., 2023]. With the input correlation matrix = XX, we follow Lin et al. [2025] to perform joint decomposition by two consecutive SVD ùëú ) = Uùë£UŒ£V. The SVD decomposition ranks operations: the eigenvectors by eigenvalues. To reduce quantization errors, we fuse the diagonal matrix Œ£ into Wùëñ ùë£, unlike prior art [Wang et al., 2025b, Lin et al., 2025]. We present the final sorted weights with quantization-aware SVD decomposition as Wùëñ ùëú = V, as shown in Figure 3 (b). Low-bit quantization (i.e., INT4) is sensitive to numerical distribution within the quantization group. We deconstruct the weight matrix = UŒ£V by merging the long-tailed eigenvalues Œ£ with U, such that = (UŒ£)V, where each column of is scaled by its eigenvalue ùúéùëñ . Thus, ùúéùëñ acts as the quantization scaling factor for the group without distorting the distributions, as depicted in Figure 4 (b). We show the details of our algorithms in Algorithm 2 and 3. This joint weight decomposition also supports Grouped-Query Attention (GQA) [Ainslie et al., 2023], as shown in Figure 3 (b) and Algorithm 6 and 7 in Appendix A. ùëú = Uùë£SVD(Œ£ùë£V 2 Uùë£UŒ£ and Wùëñ ùë£ = 1 ùë£Wùëñ 6 Mamba. The Mamba block encompasses five primary weight matrices. For simplicity, we decompose the entire computation and express the SSMs ùëñth head and ùëîth group as ùëìMamba(X, ùëñ, ùëî) = Norm(cid:16) ùúé (XWùëñ ùëî ùëî ùëß) SSM(cid:0)ŒîA, ùúô (XW ùê∂ ), Œîùúô (XW ùêµ), ùúô (XWùëñ ùë• )(cid:1)(cid:17) Wùëñ ùëú , (3) ùë•, Wùëñ ùëî ùëî ùê∂ } Rùê∑h ùê∑s, and the output weight Wùëñ ùëß } Rùê∑h ùê∑hd , and {W ùêµ, where {Wùëñ ùë• Rùê∑hd ùê∑h. Norm() and ùúô () denote normalization and 1D causal convolution layer fused with an activation, respectively. The SSM() function performs the ùë¶ùë° = ùê∂ùë°‚Ñéùë° for the ùëñth head and ùëîth group at each time step ùë° with linear recurrence computations ‚Ñéùë° = Œîùë°ùê¥ùë°‚Ñéùë° 1 + Œîùë° ùêµùë°ùë•ùë°, ùëî parameterized step size Œî [Dao and Gu, 2024]. ŒîA and ŒîBùëî = Œîùúô (XW ùêµ) are the matrix forms of Œîùë°ùê¥ùë° and Œîùë° ùêµùë° . is the matrix form of the SSM state ‚Ñéùë° . To perform the joint weight decomposition, we first break the computation of Mamba ùëî ùëî block into two sub-formulations: (1) SSM input mask M: ùëìM (X, ùëî) = Cùëî (ŒîBùëî) = ùúô (XW ùê∂ ) (Œîùúô (XW ùêµ)) and (2) SSM state : ùëìH (X, ùëî, ùëñ, ‚Ñé0) = ŒîAH (‚Ñé0) + ŒîBùëîXùëñ ùë• ) as the output of the causal ùúô convolution activation. with an initial state ‚Ñé0. We denote Xùëñ ùúô = ùúô (XWùëñ ùê∂ } that compute the SSM input mask M. For the SSM group ùëî with ùêª ùëî ùëî ùëî ùëö = ùêªùëö We first focus on sorting the weights {W ùêµ, ùê∫ùë† ùëî ùëî SSM heads in the group, we obtain the activations Bùëî = ùúô (cid:0)XW (cid:1) and Cùëî = ùúô (cid:0)XW (cid:1), where Bùëî and Cùëî Rùëá ùê∑s. ùê∂ ùêµ Unlike self-attention, Bùëî is discretized using the input-dependent variable Œîùëî Rùëá ùêª ùëî ùëö by broadcasted outer product (ŒîB)ùëî = Œîùëî Bùëî Rùêª ùëî ùëî ùëî ùê∂ = CùëîCùëî, ùêµ = (ŒîB)ùëî(ŒîB)ùëî and ùêµ Rùêª ùëî ùëî ùëî 1/2 where ŒîC , ùêµ) ùúè ùê∂ and averaged over the calibration samples. We use ùë† to construct the sorting matrix Sùêµùê∂ Rùê∑s ùê∑s that sorts the output ùëî ùëî ùëî ùëî ùê∂ Sùêµùê∂ , as shown in Figure 3 (c). ùêµSùêµùê∂ and ùê∂ as ùêµ and columns of ùê∂ Rùê∑s ùê∑s . The sorting scores ùë† Rùê∑s are calculated as ùë† = (cid:205)ùêª ùëî ùëö ùëá ùê∑s. As result, we compute the channel correlation ŒîC ùëö ùê∑s ùê∑s and ùúè=0 (ŒîC 1/2 ùëö ùëî ùëî Algorithm 4 Structured sorting B-C for Mamba with ùêªùëö heads and ùê∫ùë† SSM groups. 1: Input: matrix Wùêµ Rùê∑h (ùê∫ùë† ùê∑s ) and matrix Wùê∂ Rùê∑h (ùê∫ùë† ùê∑s ) , hidden states Xùëñ Rùëá ùê∑h and input-dependent step size Œîùëñ Rùëá (ùêªùëö/ùê∫ùë† ) from ùëÅ calibration samples ùëñ = 1, ..., ùëÅ , and the function of 1D causal convolution ùúô (). ùëî ùêµ R(ùêªùëö/ùê∫ùë† ) ùê∑s ùê∑s ùëö ùëá ùê∑s ùëî (cid:17) (cid:16) 4: 5: 6: 7: Xùëñ (cid:16) Xùëñ , Bùëñ,ùëî Rùëá ùê∑s , Cùëñ,ùëî Rùëá ùê∑s 2: for ùëó = 1, . . . , ùê∫ùë† do (cid:17) ùëî Bùëñ,ùëî = ùúô 3: hW ùêµ ùëî Cùëñ,ùëî = ùúô hW ùê∂ (ŒîB)ùëñ,ùëî = Œîùëñ,ùëî Bùëñ,ùëî, (ŒîB)ùëñ,ùëî Rùêª ùëî Average over the calibration samples ùëñ=1 (ŒîB)ùëñ,ùëî(ŒîB)ùëñ,ùëî, ŒîC (cid:205)ùëÅ ùêµ = 1 ŒîC ùëî ùëî ùëñ=1 Cùëñ,ùëî ùê∂ = 1 ùê∂ Rùê∑s ùê∑s Cùëñ,ùëî, Compute group correlations ùë† [0, . . . , 0] , ùë† Rùê∑hs for ùëò = 1, . . . , ùêªùëö Cùëò ùêµ = (ŒîC ùë† = ùë† + Cùëò ùêµ ùê∫ùë† do ùëî ùêµ)ùëò (ŒîC 1/2 ùëî ùêµ)ùëò ùëó ùê∂ ùëÅ (cid:205)ùëÅ , 1/2 ùëÅ 8: 9: 10: 11: 12: 13: 14: 15: end for ùëó ùëó ùêµùê∂ Rùê∑s ùê∑s ùêµùê∂ Iùê∑s [:, argsort(ùë†)], ùëó ùëó ùëó ùêµùê∂, (W ùê∂ ) (W 16: ùê∂ 17: end for 18: return (Wùêµ, Wùê∂ ) ùêµ, . . . , Wùê∫ùë† ùëó ùêµS (cid:16) ùëó ùêµ, [W1 ùëó ùêµùê∂ ) ùêµ ], [W1 Broadcasted outer product Initialize ùë† with zeros Calculate the norm score get the sorting matrix based on the vector ùë† (cid:17) Concatenate the sorted states ùê∂, . . . , ùê∫ùë† ùê∂ ] We propose state-aware method to compress the other group of weights {Wùëñ ùëú } by collecting the correlations ùëî from the SSM states ùëñ R(ùëá ùê∑s ) ùê∑hd, such that ùê∂ Rùê∑hd ùê∑hd and averaging over the calibration samples. The ridge leverage score diag (cid:0)CH (CH + ùúÜùêº ) 1(cid:1) is computed as the MLP layer, where we set ridge lambda ùúÜ = 1. We use the scores and design column sorting matrix Sùë† Rùê∑hd ùê∑hd that organizes the output columns for Wùëñ ùëßSùë† and Wùëñ ùëß and Wùëñ ùëú . Figure 3 (c) illustrates these sorted weights. ùëú are sorted accordingly by ùë• Sùë† . The input rows for Wùëñ = ùëñ ùëñ , ùë• as Wùëñ ùë•, Wùëñ ùëß, Wùëñ ùë† Wùëñ ùëî Algorithm 5 Structured sorting z-x-o for Mamba with ùêªùëö heads and ùê∫ùë† SSM groups. 1: Input: projection Wùë• Rùê∑h (ùêªùëö ùê∑hd ) , projection Wùëß Rùê∑h (ùêªùëö ùê∑hd ) , out matrix Wùëú R(ùêªùëö ùê∑hd ) ùê∑h, and ùëñ Rùêªùëö (ùëá ùê∑s ) ùê∑hd from ùëÅ calibration samples ùëñ = 1, ..., ùëÅ , and ridge intensity ùúÜ. ùëñ=1 ùëñ,ùëó ùëñ,ùëó, Rùê∑hd ùê∑hd, ùëñ = 1, ..., ùëÅ (cid:205)ùëÅ 2: for ùëó = 1, . . . , ùêªùëö do 3: 4: 5: State-aware (Ours) = 1 ùëÅ ùë† diag (cid:0)C(C + ùúÜùêº ) 1(cid:1) , ùë† Rùê∑hd ùëó ùë† Iùê∑hd [:, argsort(ùë†)], ùëó ùëó ùë†, (W ùëú ) (W 7: 8: end for 9: return (Wùëß, Wùë•, Wùëú ) ùëó ùë•, ùëó ùëß, [W1 ùëó ùë• ùëó ùëßS 6: (cid:16) ùëó ùë† Rùê∑hd ùê∑hd ùëó ùëó ùë†, ùë† ùëó ùëú ) Average over the samples compute ridge leverage scores get the sorting matrix based on the vector ùë† ùêªùëö ùëß, . . . , ùëß ], [W1 ùêªùëö ùë•, . . . , ùë• ], [W1 ùêªùëö ùëú, . . . , ùëú ] (cid:17) Concatenate the sorted heads"
        },
        {
            "title": "3.3 Masked LoRA Fine-tuning\nWe conduct a LoRA-based [Hu et al., 2022] recovery fine-tuning (FT) on the sorted model. Unlike previous work [Wang\net al., 2025b,a] that fine-tunes the pruned model, we fine-tune the un-pruned sorted model in one shot, as shown in\nFigure 2. We derive the layer-wise pruning rates ùëüùëô using Block Influence (BI) scores [Lin et al., 2025, Men et al., 2024]\nfor all pre-determined global pruning rates ùëÉ = [ùëÉ15, ùëÉ20, ...], such as ùëÉ15 = [ùëü ùëÉ15\nùêø ]. The BI score is defined as\n1\nùë† = 1 ‚àí E x‚ä§\nùëô yùëô\n, where the ùë•ùëô and ùë¶ùëô represent the input and output of the ùëô th layer, respectively. During fine-tuning, we\n‚à•xùëô ‚à•2 ‚à•yùëô ‚à•2\nrandomly draw a pruning rate ùëÉùë° ‚àº ùëÉ at time step ùë° to mask out the pruned channels. We follow the prior work [Wang\net al., 2025b] to perform instruction tuning on the Alpaca dataset [Taori et al., 2023] for five epochs. The entire fine-tuning\nprocess is conducted on a single cloud GPU. Our sorted model provides configurable pruning rates on the device after\none-shot masked fine-tuning. We note that our fine-tuning inherently supports downstream tasks for any application, such\nas summarization and question-answering datasets.",
            "content": ", ..., ùëü ùëÉ15 , ùëü ùëÉ"
        },
        {
            "title": "3.4 Quantization and On-device Adaptive Pruning\nWe quantize the fine-tuned full model to minimize the on-device storage needs. We employ group-wise uniform symmetric\nquantization, the most commonly supported method by hardware, to convert floating-point values into ùëÅ -bit discrete\nform. The quantization function for a group of weight ùëæ (ùëñ,ùëî) in column ùëñ is defined as",
            "content": "ùëæ (ùëñ,ùëî) = Clamp (cid:16) (cid:22) ùëæ (ùëñ,ùëî) ùë† (cid:25) , 2ùëÅ 1, 2ùëÅ 1 1(cid:17) , (4) (cid:12) (cid:12) (cid:12)ùëæ (ùëñ,ùëî) where ùë† = Max(cid:0)(cid:12) (cid:1)/(2ùëÅ 1 1) is the scaling factor (i.e., quantization step). For the quantization-aware SVD decomposition, we fuse the eigenvalues of the ùëñth column to the group of weight factor such that ùúéùëñùëæ (ùëñ,ùëî) . We set ùëÅ = 4 and group size 128 and adapt GPTQ [Frantar et al., 2023] to quantize our models. We fuse Hadamard matrices into the weights and apply 4-bit quantization to the embedding and output layers. The parameters of normalization layers are fused to the weights before applying quantization. After deploying the quantized model, we prune the channels on the device by reducing the intermediate dimension ùê∑int in the MLP layer, head dimension ùê∑hd in the MHSA layer, and both the state dimension ùê∑s and head dimension ùê∑hd in the Mamba layer. We keep the hidden state dimension ùê∑h the same across all pruned models. For INT4 weights, we unpack them online, prune channels, and repackage them into INT32 for the kernel."
        },
        {
            "title": "4.1 Setups\nModels and setups. We experiment with Transformers Llama-2-7B [Touvron et al., 2023], Llama-3.1-8B [Meta, 2024],\nQwen-2.5-7B [Hui et al., 2024], hybrid models Nemotron-H-8B [Blakeman et al., 2025], Bamba-9B-v2 [IBM, 2025], and the\nSSM model Mamba-2-8B [Dao and Gu, 2024]. ‚ãÑ, ‚Ä†, and ‚Ä° denote Transformers, hybrid and SSMs, respectively. FT, PTQ, and\nW-bit stand for fine-tune, post-training quantization, and the bit-width of weights. Prun. and R.size represent the pruning\nrate in percentage (%) and the reduction of model size (√ó), respectively.",
            "content": "8 Table 2: (Compared with structured pruning.) We compare all models in FP16. UniQL enables all compression rates in single pass. The symbols , , and denote Transformers, Mamba-Transformer hybrids, and Mamba models, respectively. Prun.% Prun. Method +FT Llama-2-7B Llama-3.1-8B Qwen-2.5-7B Bamba-v2-9B Nemotron-H-8B Mamba2-8B 0% - - - - MoDeGPT SVD-LLM UniQL (Ours) SVD-LLM UniQL (Ours) - - - MoDeGPT SVD-LLM UniQL (Ours) SVD-LLM UniQL (Ours) 15% 25% 68.8% 66.2% 56.3% 66.7% 64.7% 67.2% 63.4% 50.8% 63.7% 62.4% 64.9% 74.0% 72.4% 56.7% 70.5% 64.5% 71.9% 64.9% 45.8% 67.0% 59.5% 69.6% 72.4% 52.1% 62.6% 69.1% 69.5% 70.0% 40.8% 53.2% 62.1% 66.8% 65.8% 74.6% - - 70.9% - 72.9% - - 66.4% - 69.7% 76.0% - - 68.9% - 73.0% - - 60.6% - 67.3% 70.6% - - 65.6% - 66.4% - - 59.8% - 62.7% Table 3: (Compared with PTQ.) We benchmark all weight-only PTQ methods without fine-tuning and pruning. represents the FP16 embeddings and output layers as per the official implementation. denotes the GPTQ [Frantar et al., 2023] implemented on all models as an additional baseline. PTQ Method W-bit Llama-2-7B Llama-3.1-8B Qwen-2.5-7B Bamba-v2-9B Nemotron-H-8B Mamba2-8B FP 16 TRT-AWQ 4 TAO-HQQ 4 4 UniQL (Ours) GPTQ UniQL (Ours) 4 4 68.8% 68.1% 68.4% 68.2% 67.9% 67.8% 74.0% 71.9% 72.4% 72.9% 71.3% 72.3% 72.4% 70.3% 72.1% 72.0% 70.0% 71.0% 74.6% - - 74.8% 73.6% 73.8% 76.0% - - 74.9% 74.9% 74.8% 70.6% - - 69.3% 68.1% 69.3% Structured pruning baselines. We compare UniQL to cutting-edge model compression methods, MoDeGPT [Lin et al., 2025] and SVD-LLM [Wang et al., 2025b]. As MoDeGPT is not publicly available, we duplicate their method based on the paper and achieve similar accuracy to what was reported. We adapt SVD-LLM official implementation to experiment with Llama-2-7B, Llama-3.1-8B, and Qwen-2.5-7B for compression and quantize models. For MoDeGPT and SVD-LLM, we adhere to the hyper-parameters outlined in the papers. Post-training quantization baselines. We adopt AWQ [Lin et al., 2024a] in TensorRT-MO (TRT-AWQ) [NVIDIA, 2024, 2023] and HQQ [Badri and Shaji, 2023] in TorchAO [torchao, 2024] (TAO-HQQ) as our quantization baselines, both W4A16 libraries ready for PTQ. We evaluate UniQL in terms of model size, average accuracy on downstream tasks, and latency on A6000 and Nano 8G against TRT-AWQ and TAO-HQQ. In our experiments, we quantize the embedding and output (i.e., lm_head) layers to 4 bits, cutting memory usage in contrast to TRT-AWQ and TAO-HQQ which use FP16. We also adapt GPTQ [Frantar et al., 2023] for all models and also use 4-bit quantization on the embedding and output layers as an additional baseline. Datasets and evaluations. We evaluate UniQL on five zero-shot tasks with batch size of 16, including HellaSwag [Zellers et al., 2019], PIQA [Bisk et al., 2020], ARC [Clark et al., 2018], and WinoGrande [Sakaguchi et al., 2020] using LMEVAL [Gao et al., 2023]. The average of WinoGrande, PIQA, and ARC-easy (accuracy), and HellaSwag and ARC-challenge (length-normalized accuracy) is reported for experiments. More evaluations are placed in Appendix B. Implementations and environments. Our kernels are adapted from the 4-bit kernels [Frantar et al., 2024] and RoPE kernels [Hsu et al., 2025]. All computations are in BF16, except for correlation matrices for structured sorting and Cholesky decomposition for GPTQ are calculated in FP32. Detailed parameters are placed at Appendix G. The weight-sorting, masked fine-tuning, and quantization are computed on an A6000 GPU with 48GB memory in one-shot for enabling adaptive Table 4: (One-pass adaptive pruning.) We evaluate UniQL in one run across pruning rates. R.size stands for reduction of model size (). UniQL enables all pruning rates in single pass. The symbols , , and denote Transformers, MambaTransformer hybrids, and Mamba models, respectively. represents the FP16 embeddings and output layers as per the official implementation. We apply GPTQ [Frantar et al., 2023] on MoDeGPT [Lin et al., 2025] denoted as ."
        },
        {
            "title": "Method",
            "content": "one pass +FT"
        },
        {
            "title": "W\nbit",
            "content": "- - FP"
        },
        {
            "title": "16\nMoDeGPT‚ô≠ √ó ‚úì 4\n√ó ‚úì 4\nSVD-LLM √ó ‚úì 4‚àó\n√ó ‚úì 4‚àó",
            "content": "UniQL (Ours) 4 Prun. % 0% R.size () 0 15% 4.7 25% 4.7 15% 4.7 25% 4.7 0% 4 15% 4.7 25% 5.3 35% 6.1 Llama-2 7B Llama-3.1 8B Qwen-2.5 7B Bamba-v2 9B Nemotron-H 8B Mamba2 8B 68.8% 63.7% 60.3% 63.2% 59.1% 67.6% 65.6% 63.5% 61.0% 74.0% 64.2% 59.3% 60.6% 54.2% 73.6% 71.4% 67.7% 62.7% 72.4% 52.2% 48.4% 66.8% 64.6% 72.4% 68.1% 64.0% 58.1% 74.6% 76.0% 70.6% - - - - 75.1% 70.3% 67.4% 62.7% - - - - 73.3% 70.5% 64.7% 59.0% - - - - 69.3% 65.8% 61.8% 57.7% pruning on the device. We profile the latency on A6000 and Orin Nano 8GB, as our experimental cloud and edge platforms. We report the average latency of twenty profiles after five warmup runs."
        },
        {
            "title": "4.2 Zero-shot Downstream Tasks\nComparison with structured pruning. Table 2 compares structured pruning baselines against UniQL. Without\nfine-tuning, UniQL outperforms both MoDeGPT and SVD-LLM in most cases, achieving strong results such as 66.7%\non Llama-2-7B and 69.1% on Qwen-2.5-7B. With fine-tuning, UniQL further boosts performance, reaching 67.2% on\nLlama-2-7B and 70.0% on Nemotron-H-8B, surpassing SVD-LLM consistently. MoDeGPT suffers from the ill-conditioned\ncorrelation matrices C ‚àà Rùê∑int √óùê∑int and numerical instability when ùê∑int >> ùê∑h with limited calibration samples, resulting\nin large accuracy drops in Qwen-2.5-7B {ùê∑int, ùê∑h} = {18944, 3584}. SVD-LLM truncates numbers of the eigenvalues\nin the decomposed weight matrices according to the desired compression rates, requiring fine-tuning to recover the\nperformance. These results highlight UniQL‚Äôs effectiveness in preserving task performance while enabling efficient model\ncompression.",
            "content": "Comparison with post-training quantization. Table 6 presents the comparison of post-training quantization (PTQ) methods. Across all models, UniQL demonstrates highly competitive performance, matching or surpassing existing PTQ methods in several settings. For example, UniQL achieves 72.9% on Llama-3.1-8B with 4-bit layers and FP16 embedding/output layers. Notably, while TAO-HQQ slightly edges out UniQL on Llama-2-7B and Qwen-2.5-7B, UniQL is more general to different architectures, providing adaptive pruning features on-device. UniQL surpasses or equals GPTQ, the baseline we modify for all models. One-pass adaptive pruning. Table 4 evaluates the One-pass adaptive pruning under 4-bit quantization with fine-tuning of UniQL. We compare UniQL against SVD-LLM baselines, which follow similar compression process but support only single compression rate per run. Without any pruning, UniQL achieves competitive results, for example, 67.6% on Llama-2-7B and 73.6% on Llama-3.1-8B, while reducing the model size by 4 . As pruning ratios increase, UniQL maintains graceful degradation in accuracy; at 15% pruning, it still achieves 71.4% on Llama-3.1-8B and 70.5% on Nemotron-H-8B, outperforming SVD-LLM across all comparable settings. At higher compression (e.g., 35% pruning), UniQL still delivers reasonable performance, such as 62.7% on Llama-3.1-8B and 57.7% on Mamba2-8B. These results demonstrate UniQLs strong adaptability, generalizing to wide range of architectures."
        },
        {
            "title": "4.3 Compression Time\nIn Table 5, we compare the compression time of UniQL against MoDeGPT [Lin et al., 2025], noted for being training-\nfree, and SVD-LLM [Wang et al., 2025b], which involves fine-tuning, both state-of-the-art algorithms for Transformer",
            "content": "10 compression. Our matrix decomposition is 22 (0h19m vs. 7h03m) faster than MoDeGPT and 1.8 (0h19m vs. 0h35m) faster than SVD-LLM, as UniQL avoids pseudo-inverse and SVD decomposition for large MLP weight matrices. With masked fine-tuning (FT), UniQL remains quicker (6h59m) than both MoDeGPT (7h03m) and SVD-LLM (15h57m). MoDeGPT suffers from the high computation cost of performing pseudo-inverse on large weight matrices. SVD-LLM splits weights into two successive layers, and V, and carries out independent fine-tuning for each, leading to longer fine-tuning duration. Lastly, post-training quantization (PTQ) takes an extra forty minutes. Our compression algorithm is one-time ùëÇ (1) with respect to the number of compression rates, compared to ùëÇ (ùëõ) for MoDeGPT and SVD-LLM. Table 5: (Compression time.) The time is reported on an A6000 GPU. UniQL supports all compression rates in one shot. Table 6: (Model size.) The model size is reported in GB. Our 4-bit embedding/output layers yield smaller size than TRT-AWQ and TAO-HQQ. Method +FT +PTQ Llama-3.1-8B Mamba2-8B Method W-bit Prun. p% Llama-3.1-8B Qwen-2.5-7B Nemotron-8B MoDeGPT - SVD-LLM UniQL (Ours) - - - - - - - 7h03m 0h35m 16h25m 16h46m 0h19m 6h59m 7h43m - - - - 0h16m 7h18m 7h50m FP16 16 MoDeGPT 16 SVD-LLM 16 TRT-AWQ 4 TAO-HQQ 4 UniQL (Ours) 4 0% 15% 15% 0% 0% 0% 35% 16.0 GB 13.9 GB 14.1 GB 5.8 GB 5.7 GB 4.1 GB 2.8 GB 15.2 GB 13.2 GB 13.3 GB 5.6 GB 6.0 GB 3.9 GB 2.7 GB 16.2 GB 4.1 GB 2.9 GB"
        },
        {
            "title": "4.4 Model Size and Latency Profiling\nWe evaluate the model size and latency of UniQL and compare them with AWQ [Lin et al., 2024a] from TensorRT-MO‚àó\n[NVIDIA, 2024, 2023] (TRT-AWQ) and HQQ [Badri and Shaji, 2023] in TorchAO‚àó [torchao, 2024] (TAO-HQQ). Both libraries\nare weight-only (i.e., W4A16) quantization frameworks in production. We show the model size in Table 6. UniQL quantizes\nmodels in head-to-toe fashion, i.e., from embeddings, backbone layers, and output heads all at 4-bit, resulting in smaller\nmodel size (4.1GB vs. 5.7GB) compared to TRT-AWQ and TAO-HQQ with minimal accuracy drops. Our model enables\nall compression rates and on-device structured pruning, providing an elastic 3.9√ó‚Äì 5.7√ó memory reductions. We profile\nthe time-per-output-token (TPOT, i.e., generation) and time-to-last-token (TTLT, i.e., prefilling and generation) on A6000\nand Nano 8G, and show 2.7√ó‚Äì 3.4√ó throughput improvements in generation, outperforming TRT-AWQ and TAO-HQQ,\nas shown in Table 7 and Table 8. On the Nano 8G, our model is 1.7√ó faster than TAO-HQQ in TPOT. By pruning 35% of\nweights in our 4-bit model, our models generate 2.1√ó faster than TAO-HQQ.",
            "content": "Table 7: (Latency profiling on an A6000.) Reported TPOT and TTLT (1k+1k) are in ms. Method W-bit Prun. p% Llama-3.1-8B Nemotron-H-8B TPOT TTLT TPOT"
        },
        {
            "title": "TTLT",
            "content": "FP16 16 TRT-AWQ 4 TAO-HQQ 4 0% 0% 0% 11.2 11665.7 10.2 11639. 25.0 26653.8 24.4 25889.4 - - 8.2 6.8 - - 9095.7 6955. UniQL (Ours) 4 0% 35% 9.0 7.3 9944.6 8105."
        },
        {
            "title": "5 Ablation Study",
            "content": "Table 8: (Latency profiling on Nano 8G.) TPOT and TTLT (512+512) are shown in ms. (OOM: out-of-memory) Method W-bit Prun. p% Qwen-2.5-7B Mamba2-8B TPOT TTLT TPOT TTLT FP16 0% OOM OOM OOM OOM TAO-HQQ 4 0% 133.6 80770.2 - - UniQL (Ours) 4 0% 77.2 39795.3 35% 57.7 28185.8 81.6 41116.3 55.3 28508.1 We conduct an ablation study to demonstrate the effectiveness of each component of our framework. Additional ablation studies can be found at Appendix C. The embedding and output layers use FP16 according to the official implementation. Fused rotary positional embedding. We compare latency with and without our fused RoPE. Since the positions are broken by our structured sorting, we have to collect the corresponding indices from the rotary positional embeddings, where we fuse in kernel to minimize memory access. The fused RoPE kernel with index gathering yields 10% latency reduction (1.1 speedup) for Llama-3.1-8B in 4-bit models at 0% and 25% compression, as depicted in Table 9. Masked LoRA fine-tuning. We show that masked LoRA fine-tuning (FT) significantly benefits pruned models. As seen in Table 10, our method enhances accuracy by 2.6% (from 67.0% to 69.6%) and 3.7% (from 62.1% to 65.8%) for FP16 Llama-3.1-8B and Qwen-2.5-7B at 25% compression. For 4-bit models, it improves accuracy by 2.7% (from 65.0% to 67.7%) and 3.3% (from 60.7% to 64.0%) for Llama-3.1-8B and Qwen-2.5-7B at 25% compression. Quantization-aware decomposition. We show the quantization-aware SVD decomposition (QSVD) is key design to fill the performance gaps in Table 10. Low-bit quantization (i.e., INT4) is sensitive to the numerical distribution in the quantization group. We decompose the weight matrix = UŒ£V and combine the long-tailed eigenvalues Œ£ with U, resulting in = (UŒ£)V, where column of is multiplied by the corresponding eigenvalue ùúéùëñ . Thus, ùúéùëñ acts as the groups quantization scaling factor, as shown in Figure 4. We show this simple observation leads to significant performance gains 7.5% (from 60.2% to 67.7%) and 3% (from 61.0% to 64.0%) for 4-bit Llama-3.1-8B and Qwen-2.5-7B at the 25% pruning rate, respectively. Table 9: (The fused RoPE kernel.) We profile TPOT on A6000 and report in ms. Table 10: (Accuracy by Components.) We report the average accuracy for the models with different settings. W-bit 16 4 Prun. p% 0% 25% 0% 25% Fused RoPE - - - - Llama-3.1 8B 25.0 20.2 19.3 9.9 9.0 8.6 7.7 Qwen-2.5 7B 23.2 18.7 17.9 9.1 8.3 7.9 7.1 W-bit 16 Prun. p% 0% 25% 4 25% +FT +PTQ +QSVD - - - - - - - - - - - - Llama-3.1 8B 74.0% 67.0% 69.6% 55.2% 65.0% 60.2% 67.7% Qwen-2.5 7B 72.4% 62.1% 65.8% 56.1% 60.7% 61.0% 64.0%"
        },
        {
            "title": "6 Conclusion",
            "content": "We present UniQL, unified post-training compression framework that combines quantization and low-rank pruning to enable adaptive deployment of LLMs on edge. By supporting on-device configurable pruning and one-shot cloud compression pipeline, UniQL addresses the key challenges posed by dynamic workloads. Through structured weightsorting, quantization-aware decompositions, and fused rotary kernels, UniQL achieves substantial gains in memory and throughput across Transformers, SSMs, and hybrid models. Our results demonstrate that the compressed models can elastically adapt to runtime constraints."
        },
        {
            "title": "Impact Statement",
            "content": "The UniQL framework has the potential to make large language models more accessible by enabling their deployment on edge devices with limited resources. This could broaden the scope of applications beyond high-end servers, potentially benefiting settings such as education, accessibility tools, or low-resource regions. At the same time, the increased availability of compact models raises concerns about potential misuse, including privacy risks and the generation of harmful or misleading content on widely distributed devices. Reducing the computational and memory footprint may also lessen the environmental costs of running large models, though the overall impact depends on the scale of adoption and usage patterns. We emphasize that UniQL itself does not mitigate societal risks associated with language model outputs, and responsible deployment practices remain necessary. By releasing our code and models, we aim to facilitate further research on efficient adaptation while encouraging the community to carefully consider both the benefits and risks of enabling lightweight edge deployment."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the ONR Minerva program, NSF CCF Grant No. 2107085, iMAGiNE - the Intelligent Machine Engineering Consortium at UT Austin, UT Cockrell School of Engineering Doctoral Fellowships, NSF CAREER Grant No. 2339084, Nvidia research gift, and Taiwans NSTC Grant No. 111-2221-E-A49-148-MY3. References Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebr√≥n, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Saleh Ashkboos, Maximilian Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. In The Twelfth International Conference on Learning Representations, 2024a. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems, 37:100213100240, 2024b. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Hicham Badri and Appu Shaji. Half-quadratic quantization of large machine learning models, November 2023. URL https://mobiusml.github.io/hqq_blog/. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020. Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, et al. Nemotron-h: family of accurate and efficient hybrid mamba-transformer models. arXiv preprint arXiv:2504.03624, 2025. Ruisi Cai, Saurav Muralidharan, Greg Heinrich, Hongxu Yin, Zhangyang Wang, Jan Kautz, and Pavlo Molchanov. Flextron: Many-in-one flexible large language model. In International Conference on Machine Learning, pages 52985311. PMLR, 2024. Ruisi Cai, Saurav Muralidharan, Hongxu Yin, Zhangyang Wang, Jan Kautz, and Pavlo Molchanov. Llamaflex: Many-in-one llms via generalized pruning and weight sharing. In The Thirteenth International Conference on Learning Representations, 2025. TienYu Chi, Hung-Yueh Chiang, Chi-Chih Chang, Ning-Chi Huang, and Kai-Chiang Wu. mean ba: Visual state space models only need 1 hidden dimension. In Workshop on Machine Learning for Systems at Advances in Neural Information Processing Systems (NeurIPSW), 2024. 13 Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, Mohamed S. Abdelfattah, and Diana Marculescu. Quamba2: robust and scalable post-training quantization framework for selective state space models. In International Conference on Machine Learning (ICML), 2025a. Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, and Diana Marculescu. Quamba: post-training quantization recipe for selective state space models. In International Conference on Learning Representations (ICLR), 2025b. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, 2018. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In Forty-first International Conference on Machine Learning, 2024. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International conference on machine learning, pages 1032310337. PMLR, 2023. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. In International Conference on Learning Representations (ICLR), 2023. Elias Frantar, Roberto Castro, Jiale Chen, Torsten Hoefler, and Dan Alistarh. Marlin: Mixed-precision auto-regressive parallel inference on large language models. arXiv preprint arXiv:2408.11743, 2024. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 2023. URL https://zenodo.org/records/10256836. Martin Genzel, Patrick Putzky, Pengfei Zhao, Sebastian Schulze, Mattes Mollenhauer, Robert Seidel, Stefan Dietzel, and Thomas Wollmann. Compressing large language models to any size without re-computation. In ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models, 2025. Yuxian Gu, Qinghao Hu, Shang Yang, Haocheng Xi, Junyu Chen, Song Han, and Han Cai. Jet-nemotron: Efficient language model with post neural architecture search. arXiv preprint arXiv:2508.15884, 2025. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, Yanning Chen, and Zhipeng Wang. Liger-kernel: Efficient triton kernels for LLM training. In Championing Open-source DEvelopment in ML Workshop @ ICML25, 2025. URL https://openreview.net/forum?id=36SjAIT42G. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. International Conference on Learning Representations (ICLR), 1(2):3, 2022. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Bamba IBM. Bamba-9b-v2 - fast and powerful! https://huggingface.co/blog/ibm-ai-platform/bamba-9b-v2, 2025. Toshiaki Koike-Akino, Xiangyu Chen, Jing Liu, Ye Wang, Matthew Brand, et al. Latentllm: Attention-aware joint tensor compression. arXiv preprint arXiv:2505.18413, 2025. Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, and Zhanhui Kang. E-sparse: Boosting the large language model inference through entropy-based n: sparsity. arXiv preprint arXiv:2310.15929, 2023. Chi-Heng Lin, Shangqian Gao, James Seale Smith, Abhishek Patel, Shikhar Tuli, Yilin Shen, Hongxia Jin, and Yen-Chang Hsu. Modegpt: Modular decomposition for large language model compression. In International Conference on Learning Representations (ICLR), 2025. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6:87100, 2024a. Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024b. Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, In The Thirteenth Yuandong Tian, and Tijmen Blankevoort. Spinquant: Llm quantization with learned rotations. International Conference on Learning Representations, 2025. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. Shannon McCurdy. Ridge regression and provable deterministic ridge leverage score sampling. Advances in Neural Information Processing Systems, 31, 2018. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. AI Meta. Llama 3: Open foundation and instruction-tuned language models, 2024. Mohammad Mozaffari, Amir Yazdanbakhsh, and Maryam Mehri Dehnavi. Slim: One-shot quantization and sparsity with low-rank approximation for llm weight compression. In Forty-second International Conference on Machine Learning, 2025. Pablo Mu√±oz, Jinjie Yuan, and Nilesh Jain. Mamba-shedder: Post-transformer compression for efficient selective structured state space models. arXiv preprint arXiv:2501.17088, 2025. NVIDIA. TensorRT-LLM: High-performance inference for Large Language Models. https://github.com/NVIDIA/ TensorRT-LLM, 2023. Accessed: 2025-09-13. NVIDIA. TensorRT-Model-Optimizer: Quantization and Optimization Toolkit for LLMs. https://github.com/NVIDIA/ TensorRT-Model-Optimizer, 2024. Accessed: 2025-09-13. Yeonhong Park, Jake Hyun, Sanglyul Cho, Bonggeun Sim, and Jae Lee. Any-precision llm: Low-cost deployment of multiple, different-sized llms. In International Conference on Machine Learning, pages 3968239701. PMLR, 2024. Roger Penrose. generalized inverse for matrices. In Mathematical proceedings of the Cambridge philosophical society, volume 51, pages 406413. Cambridge University Press, 1955. Wang Qinsi, Jinghan Ke, Masayoshi Tomizuka, Kurt Keutzer, and Chenfeng Xu. Dobi-SVD: Differentiable SVD for LLM compression and some new perspectives. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=kws76i5XB8. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020. Ibne Farabi Shihab, Sanjeda Akter, and Anuj Sharma. Efficient unstructured pruning of mamba state-space models for resource-constrained environments. arXiv preprint arXiv:2505.08299, 2025. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. arxiv. arXiv preprint arXiv:2104.09864, 2021. Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024. Ali Taghibakhshi, Sharath Turuvekere Sreenivas, Saurav Muralidharan, Marcin Chochowski, Yashaswi Karnati, Raviraj Joshi, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Yoshi Suhara, Oluwatobi Olabiyi, et al. Efficient hybrid language model compression through group-aware ssm pruning. arXiv preprint arXiv:2504.11409, 2025. Endri Taka, Ning-Chi Huang, Chi-Chih Chang, Kai-Chiang Wu, Aman Arora, and Diana Marculescu. Systolic sparse tensor slices: Fpga building blocks for sparse and dense ai acceleration. In Proceedings of the 2025 ACM/SIGDA International Symposium on Field Programmable Gate Arrays, pages 159171, 2025. 15 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Stanford alpaca: an instruction-following llama model (2023), 2023. torchao. Torchao: Pytorch-native training-to-serving model optimization, oct 2024. URL https://github.com/pytorch/ ao. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Kaiwen Tuo and Huan Wang. Sparsessm: Efficient selective structured state space models can be pruned in one-shot. arXiv preprint arXiv:2506.09613, 2025. Xin Wang, Samiul Alam, Zhongwei Wan, Hui Shen, and Mi Zhang. Svd-llm v2: Optimizing singular value truncation for large language model compression. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 42874296, 2025a. Xin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. SVD-LLM: Truncation-aware singular value decomposition for large language model compression. In International Conference on Learning Representations (ICLR), 2025b. URL https://openreview.net/forum?id=LNYIUouhdt. Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, and Xipeng Qiu. Bitstack: Any-size compression of large language models in variable memory environments. In The Thirteenth International Conference on Learning Representations, 2025c. Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. Flash-llm: Enabling cost-effective and highly-efficient large generative model inference with unstructured sparsity. Proceedings of the VLDB Endowment, 17(2):211224, 2023. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International conference on machine learning, pages 3808738099. PMLR, 2023. Zukang Xu, Yuxuan Yue, Xing Hu, Dawei Yang, Zhihang Yuan, Zixu Jiang, Zhixuan Chen, Sifan Zhou, et al. Mambaquant: Quantizing the mamba family with variance aligned rotation methods. In The Thirteenth International Conference on Learning Representations, 2025. Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activation-aware singular value decomposition for compressing large language models. arXiv preprint arXiv:2312.05821, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Anna Korhonen, David R. Traum, and Llu√≠s M√†rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), 2019. Zheng Zhan, Zhenglun Kong, Yifan Gong, Yushu Wu, Zichong Meng, Hangyu Zheng, Xuan Shen, Stratis Ioannidis, Wei Niu, Pu Zhao, et al. Exploring token pruning in vision state space models. Advances in Neural Information Processing Systems, 37:5095250971, 2024. Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. In Proceedings of Machine Learning and Systems (MLSys), volume 6, pages 196209, 2024. URL https://proceedings.mlsys.org/paper_files/paper/ 2024/file/5edb57c05c81d04beb716ef1d542fe9e-Paper-Conference.pdf."
        },
        {
            "title": "A Detailed Structured Sorting Algorithms",
            "content": "A.1 Group-query Attention: Query-Key Algorithm 6 describes the structured query-key sorting for GQA, featuring ùêªùë† self-attention heads and ùêªùëòùë£ key-value heads, ùëó for key-value where ùêªùë† > ùêªùëòùë£. Firstly, we determine activations for ùëò ùëó ùëó head ùëó. The correlation is shared among group of self-attention heads. We then compute the channel correlation ùëû ùëò for the group of self-attention heads, and sum the norm scores of the group. The sorting matrix Sùëûùëò is obtained similarly to ùëó MHSA to enable RoPE. In GQA, Sùëûùëò sorts the output columns of self-attention heads as [W1 ùëûùëò ], and the ùëûS key-value head as ùëó and then compute the channel correlation ùëò ùêªùë† /ùêªùëòùë£ , . . . , ùëû ùëó ùëûùëò . ùëó ùëò ùëó ùëûùëò Algorithm 6 Structured sorting key-query for GQA with ùêªùë† heads and ùêªùëòùë£ key-value heads. 1: Input: MHSA query matrices Wùëû Rùê∑h (ùêªùë† ùê∑hd ) , key matrices Wùëò Rùê∑h (ùêªùëòùë£ ùê∑hd ) , hidden states Xùëñ Rùëá ùê∑h from ùëÅ calibration samples ùëñ = 1, ..., ùëÅ , and the function of rotary positional embedding ùúå (). ùëó ùëó 4: (cid:205)ùëÅ hW ùëò )ùúå (Xùëñ 2: for ùëó = 1, . . . , ùêªùëòùë£ do ùëò = 1 ùëñ=1 ùúå (Xùëñ 3: ùëÅ ùë† [0, . . . , 0] , ùë† Rùê∑hd for ùúÖ = 1, . . . , ùêªùë† (cid:205)ùëÅ ùëû = 1 CùúÖ ùëÅ ùë† = ùë† + CùúÖ ùëû ùêªùëòùë£ do ùëû )ùúå (Xùëñ ùëñ=1 ùúå (Xùëñ hWùúÖ 1/2 ùëó 1/2 , ùëò hW 6: 5: ùëó ùëò ) , ùëó ùëò Rùê∑hd hWùúÖ ùëû ) , CùúÖ ùëû Rùê∑hd 11: 10: end for [ùë†1, ùë†2] ùë†, {ùë†1, ùë†2} Rùê∑hd/2 idxsym [argsort(ùë†1 + ùë†2), ùê∑dh/2 + argsort(ùë†1 + ùë†2)], ùëó ùëûùëò Iùê∑dh [:, idxsym], ([W1 ], 13: end for 14: return (Wùëû, Wùëò ) ùëûùëò Rùê∑dh ùê∑dh ùëò ) ([W1 ùëûS ùêªùë† /ùêªùëòùë£ ùëû, . . . , ùëû ùêªùë† /ùêªùëòùë£ , . . . , ùëû ùëû, . . . , ùêªùëòùë£ , . . . , ùëò ùêªùë† ùëû ], [W1 ùëò [W1 ùëó ùëûùëò 12: (cid:16) (cid:17) ] ùëó ùëó 7: 8: 9: Key correlations Initialize ùë† with zeros Group query correlations Calculate the norm score idxsym Rùê∑hd Split the norm score vector by half Get the symmetric sorted indices get the sorting matrix based on the vector ùë† ùëó ùëûùëò ], ùëó ùëò ùëó ùëûùëò ) Concatenate the sorted heads A.2 Group-query Attention: Value-Output Algorithm 7 outlines structured value-output sorting for GQA. Since GQA has ùêªùë† self-attention heads and ùêªùëòùë£ key-value heads, where ùêªùë† > ùêªùëòùë£, single SVD decomposition is performed using the input correlation matrix CW ùë£. We also incorporate quantization-aware SVD for Wùëñ ùë£ is shared across attention heads such that ùë£ by integrating Œ£ùë£ with Uùë£ for ùëú for ùúÖ [1, . . . , ùêªùëòùë£]. ùëó ùë£ = Uùë£Œ£ùë£V ùëó ùë£. The ùë£ WùúÖ Algorithm 7 Structured sorting value-output for GQA with ùêªùë† heads and ùêªùëòùë£ key-value heads. 1: Input: MHSA value matrices Wùë£ Rùê∑h (ùêªùë† ùê∑hd ) , output matrices Wùëú R(ùêªùëòùë£ ùê∑hd ) ùê∑h, hidden states Xùëñ Rùëá ùê∑h ùë£ ) SVD(CW ùëó ùë£) Quantization-aware SVD from ùëÅ calibration samples ùëñ = 1, ..., ùëÅ . , Rùê∑hd ùê∑hd 5: 6: 2: = Xùëñ Xùëñ 3: for ùëó = 1, . . . , ùêªùëòùë£ do (Uùë£, Œ£ùë£, 4: ùëó ùë£ 1Uùë£Œ£ùë£ for ùúÖ = 1, . . . , ùêªùë† ùêªùëòùë£ do ùë£ WùúÖ ùëú ùëú WùúÖ end for 7: 8: 9: end for 10: return (Wùë£, Wùëú ) (cid:16) [W1 ùë£, . . . , ùêªùëòùë£ ùëâ ], [W1 ùëú, . . . , ùêªùë† ùëú ] (cid:17) Concatenate the sorted heads"
        },
        {
            "title": "B Broader Evaluation Results",
            "content": "B.1 Comparison with Additional Baselines we include more common baselines in Table 11, with all methods evaluated in FP16 to ensure fair and controlled setting. We follow the experimental setup used in MoDeGPT [Lin et al., 2025] and append our Llama-3.1-8B results at the 25% pruning rate to those reported in their work. The numbers, except for UniQL, are transcribed from MoDeGPT. The results show that UniQL consistently outperforms prior pruning-based methods at the 25% compression level, and UniQL-ft further boosts accuracy, achieving the strongest performance across all evaluated tasks. Table 11: (Comparison with Additional Baselines.) All models are evaluated in FP16 for fair and consistent comparison following the experimental setup used in MoDeGPT. Compress. % Method ARC-e ARC-c PIQA WinoG. HellaS. Average 0% 25%"
        },
        {
            "title": "Dense",
            "content": "77.69% 53.58% 80.63% 72.69% 79.16% 72.75% ShortGPT-Alpaca SliceGPT-Alpaca MoDeGPT-Alpaca UniQL UniQL-ft 38.13% 44.44% 67.05% 70.37% 76.05% 31.40% 29.27% 41.13% 46.33% 50.00% 60.94% 57.56% 75.52% 74.16% 76.55% 54.22% 58.48% 69.61% 71.82% 72.93% 31.52% 41.08% 66.49% 70.12% 73.37% 43.24% 46.17% 63.96% 66.56% 69.78% B.2 Evaluation on the MMLU Dataset We test Llama-3.1-8B, Qwen-2.5-7B, and Nemotron-H-8B, and report five-shot accuracy on the MMLU dataset [Hendrycks et al., 2020] with batch size of eight. The MMLU dataset is large multitasking dataset, covering 57 subjects of varying difficulty. Our pruned models maintain competitive accuracy on the challenging dataset and outperform MoDeGPT [Lin et al., 2025] and SVD-LLM [Wang et al., 2025b]. Nemotron-H, the Mamba-Transformer hybrid model, shows large accuracy drop, but recovered with our low-cost masked fine-tuning. We compare our method with AWQ [Lin et al., 2024a] implemented in the TensorRT framework [NVIDIA, 2023, 2024] (TRT-AWQ) and HQQ [Badri and Shaji, 2023] in TorchAO [torchao, 2024] (TAO-HQQ). Our 4-bit models perform comparably to these state-of-the-art PTQ frameworks while offering broader model support. Table 12: (Five-shot MMLU.) We compare UniQL against baselines under different settings. represent the FP16 embeddings and output layers as per the official implementation. denotes the GPTQ [Frantar et al., 2023] implemented on all models as an additional baseline. Method +FT +PTQ W-bit Prun. p% Llama-3.1-8B Qwen-2.5-7B Nemotron-H-8B FP16 - MoDeGPT - SVD-LLM - - UniQL (Ours) SVD-LLM UniQL (Ours) TRT-AWQ - TAO-HQQ - - GPTQ - UniQL (Ours) SVD-LLM UniQL (Ours) - - - - - - 16 16 16 16 16 4 4 4 4 4 4 0% 15% 15% 15% 15% 15% 0% 0% 0% 0% 15% 15% 65.6% 59.5% 28.4% 60.2% 41.5% 59.2% 63.0% 62.9% 61.5% 63.2% 34.8% 56.9% 18 74.2% 23.1% 49.9% 55.9% 61.1% 59.9% 72.5% 72.5% 70.5% 70.3% 56.3% 52.7% 67.6% - - 37.5% - 56.1% - - 64.0% 67.5% - 52.6% B.3 Evaluation on Coding Tasks We present the evaluation results on the MBPP+ [Austin et al., 2021, Gao et al., 2023] coding benchmark in Table 13. We apply UniQL to Llama-3.1-8B-Instruct and compare its performance against SVD-LLM [Wang et al., 2025b] and MoDeGPT [Lin et al., 2025] under various pruning ratios and bit-width configurations. The MBPP+ results obtained under batch size 1 and 0-shot settings following common practice. These results demonstrate UniQLs ability to maintain competitive performance compared to SVD-LLM and MoDeGPT while reducing model size. Table 13: (Evaluation results on the MBPP+ coding tasks.) We compare UniQL with existing compression baselines under different pruning ratios and bit-width settings. Results are reported on the MBPP+ (instruct) benchmark using batch size 1 and 0-shot evaluation. Method One-pass +FT W-bit Prun. % R.size () Llama-3.1-8B FP16 MoDeGPT SVD-LLM UniQL (Ours) 16 16 4 4 4 0% 15% 15% 0% 15% 25% 0 0.15 4.7 4 4.7 5.3 75.4% 42.3% 24.0% 64.8% 54.2% 33.8%"
        },
        {
            "title": "C Additional Ablation Studies",
            "content": "C.1 Ablation study on calibration sets Table 14 presents the performance of Llama-3.1-8B under different combinations of calibration sets at fixed 25% pruning rate. Following our setting, we report the average accuracy of five zero-shot downstream tasks. For each configuration, all hyperparameters and the number of calibration samples strictly follow the settings in Table 11 of our manuscript, ensuring controlled and consistent comparison. Using the Alpaca dataset [Taori et al., 2023] for all stages results in the best average accuracy. We follow MoDeGPT and use WikiText2 as the calibration set for pruning-ratio allocation to ensure fair comparison with prior work in all experiments. Table 14: (Ablation study on calibration sets.) We report results for Llama-3.1-8B at 25% pruning rate under different combinations of calibration sets used for weight-sorting, masked fine-tuning, and post-training quantization (PTQ). Prun. Ratio Alloc. Weight-Sort. Masked FT PTQ W-bit Prun. Rate Avg. Acc wikitext2 wikitext2 wikitext2 alpaca wikitext2 wikitext2 alpaca alpaca wikitext2 wikitext2 wikitext2 wikitext2 wikitext2 alpaca alpaca alpaca 16 4 4 4 4 0% 25% 25% 25% 25% 74.0% 60.8% 65.5% 67.7% 68.6% C.2 Ablation study on 3-bit UniQL Our framework supports post-training quantization with various bit-widths, including 8, 6, 4, and even 3 bits. To support this claim, we include additional experiments exploring 3-bit UniQL in Table 15. The 3-bit precision is simulated by FP16 only for proof of concept purposes. These results demonstrate stable performance trends as the precision decreases, highlighting UniQL applicable to various bit-widths. Notably, even the 3-bit variant retains competitive accuracy across multiple models, underscoring the effectiveness of our weight-sorting and recovery fine-tuning procedure. Overall, these results highlight the flexibility of UniQL and confirm that it remains reliable even in resource-constrained, on-device deployment scenarios. 19 Table 15: (Experimental results of 3-bit UniQL.)"
        },
        {
            "title": "One\npass",
            "content": "+FT"
        },
        {
            "title": "W\nbit",
            "content": "Prun. % Llama-2 7B Llama-3.1 8B Qwen-2.5 7B Bamba-v2 9B Nemotron-H 8B Mamba2 8B R.size () 0 FP16 MoDeGPT SVD-LLM 4 68.8% 16 16 15% 0.15 66.2% 63.2% 15% 4.7 0% UniQL (Ours) UniQL (Ours) 4 4 4 3 3 3 4 0% 15% 4.7 25% 5.3 5.3 0% 15% 6.2 25% 7.1 67.6% 65.6% 63.5% 62.8% 60.2% 58.9% Pareto-front Analysis 74.0% 72.4% 60.6% 73.6% 71.4% 67.7% 64.5% 63.5% 59.4% 72.4% 52.1% 66.8% 72.4% 68.1% 64.0% 67.4% 63.0% 58.7% 74.6% 75.1% 70.3% 67.4% 67.8% 64.4% 61.6% 76.0% 73.3% 70.5% 64.7% 71.3% 67.8% 62.1% 70.6% 69.3% 65.8% 61.8% 67.8% 64.0% 60.3% Figure 5 and 6 illustrate the Pareto-frontier trade-offs between accuracy and latency across diverse set of Transformer, hybrid, and state-space models on A6000 and Nano 8G, respectively. For the A6000, the latency is profiled with 1024 prefilling tokens and 1024 generated tokens. On Orin Nano, we report that the latency of the request, prefilled with 512 tokens, generates 512 new tokens in seconds (sec.). Each subplot groups models by: (a) pure Transformers, (b) hybrid and SSM-based models, and (c) the union of both. We compare UniQL (W4A16, starred markers), GPTQ [Frantar et al., 2023] (W4A16, circular markers), and FP16 baselines (squares), with circle sizes indicating memory consumption on A6000. For the Nano 8G, we use HQQ [Badri and Shaji, 2023] that is supported in the TorchAO framework [torchao, 2024] (TAO-HQQ) as our baseline. Across all architectures, UniQL consistently yields better latencyaccuracy trade-offs under the same memory budget, especially in the 24GB regime critical for edge deployment. For example, UniQL significantly improves latency for Qwen-2.5-7B and Mamba-2-8B while maintaining accuracy close to the FP16 baseline. Notably, UniQL achieves Pareto-dominant points for models like Llama-3.1-8B, outperforming GPTQ and TAO-HQQ in both latency and accuracy. Our analysis underscores UniQLs advantage for high-performance LLM inference under tight latency and memory constraints. Figure 5: (Pareto-front analysis on A6000.) We evaluate the trade-off between average accuracy (%) and time-to-lasttoken (sec.) for various LLMs under different quantization and pruning configurations. Circle, square, and star markers denote GPTQ (W4A16), FP16, and our proposed UniQL (W4A16), respectively. Marker size indicates memory footprint. 20 Figure 6: (Pareto-front analysis on Nano 8G.) We evaluate the trade-off between average accuracy (%) and time-to-lasttoken (sec.) for diverse LLMs with different quantization and pruning settings. Circle, square, and star markers represent TAO-HQQ (W4A16), FP16, and our UniQL (W4A16), respectively. Marker size reflects memory usage. Figure 7: (Energy efficiency analysis on A6000.) Nemotron-H incorporates SSM blocks to decrease KV cache memory requirements. UniQL continually offers superior energy efficiency for both Transformer and SSM architectures."
        },
        {
            "title": "E Energy Profiling",
            "content": "To assess the practical efficiency of our quantization and pruning strategies, we conduct energy profiling on an A6000 GPU and an Orin Nano 8G, both of which are representative of cloud and edge platforms. On Orin Nano, each request is prefilled with 512 tokens and generates 512 new tokens, where we record the total energy consumption in Joules-per-request (J/req.). As shown in Table 16, full-precision (FP16) models exceed the devices 8 GB memory limit, resulting in out-ofmemory (OOM) errors during inference. In contrast, quantized methods substantially reduce energy consumption while maintaining deployability. Without pruning, UniQL (W4A16) reduces the energy per request to 208.23 and 224.56 on Qwen-2.5-7B and Mamba2-8B, respectively. When combined with structured pruning (ùëù=35%), the energy further decreases to 143.12 and 153.64 J. 21 Table 16: (Energy profiling on Nano.) Joules-per-request (J/req.) is reported. Each request is prefilled with 512 tokens and 512 generated tokens. Lower is better (). Method W-bit FP16 TAO-HQQ 4 UniQL (Ours) 4 Prun. p% 0% 0% 0% 35% Qwen-2.5-7B Mamba2-8B J/req. OOM 381.13 208.23 143.12 J/req. OOM - 224.56 153.64 On cloud GPUs, we evaluate energy efficiency in terms of tokens-per-Gigawatt to align with the industrial computing power metric. In Figure 7, we visualize the energy efficiency of Transformer model (i.e., Llama-3.1-8B) and Hybrid model (i.e., Nemotron-H-8B) on an A6000 GPU with 48GB memory. Each request is prefilled with 1024 tokens and generates 1024 new tokens. Under different batch sizes, we report the total number of tokens for Gigawatt per second. Nemotron-H adopts SSM blocks to reduce the memory needs for KV cache. Also, UniQL consistently achieves higher throughput-per-energy across both Transformer-based and SSM-based architectures. This establishes UniQL as an effective deployment framework for both resource-constrained and energy-aware scenarios. Layer-wise Pruning Rates We adopt the approach from [Lin et al., 2025, Men et al., 2024] to determine layer-wise pruning rates ùëüùëô using Block Influence (BI) scores for specified global pruning rates. The BI score is given by ùë† = 1 ùëô yùëô , with ùë•ùëô and ùë¶ùëô as the input and xùëô 2 yùëô 2 output of the block (e.g., Transformer or Mamba block) at layer ùëô. Using the closed-form solution from Lin et al. [2025], we smooth the layer-wise pruning rate allocations to obtain ùëÉ = [ùëü ùëÉ ùêø ], such that ùëÉ = ùêøùëÉavg Softmax(s/ùúÄ) where ùë†ùëñ represents the importance score of layer ùëñ, and ùëÉavg denotes the target global sparsity. In our experiments, ùúÄ is set to 0.1, and we present the pruning rates for all models in Figure 8. Self-attention layers in hybrid models have low pruning rates, indicated by high BI scores in the Figure. In Bamba-9B-v2, layers 9, 18, and 27 are self-attention layers with lower pruning rates than nearby layers. Similarly, Nemotron-H-8B shows this pattern in layers 7, 18, 29, and 40. Pruning self-attention layers in hybrid models leads to significant accuracy drops. 2 , . . . , ùëü ùëÉ 1 , ùëü ùëÉ Figure 8: (Layer-wise pruning rates.) Hybrid model self-attention layers exhibit low pruning rates, as evidenced by the high BI scores in the figure."
        },
        {
            "title": "G Implementation Details",
            "content": "G.1 Calibration sets Table 17 lists the calibration set, number of samples, and the sequence length we use in our experiments. We collect BI scores and assign layer-specific pruning rates using 128 samples with sequence length of 2048 from wikitext-2 [Merity et al., 2017]. Various global pruning rates, such as ùëÉ15, ùëÉ25, and ùëÉ35, are computed using the same setting. With 128 samples and sequence length of 2048, we compute channel correlations from the Alpaca dataset [Taori et al., 2023], as detailed in [Lin et al., 2025]. Our masked LoRA fine-tuning is conducted on the Alpaca dataset with sequence length of 256 to reduce training memory usage. Lastly, we calibrate post-training quantization on the wikitext-2 dataset using 256 samples with sequence length of 2048. Table 17: (Calibration sets.) We show the detail configuration of the calibration set. Layer-wise prun. ratio alloc. Structured weight-sorting Masked LoRA fine-tuning Post-training quantization dataset wikitext2 alpaca alpaca wikitext2 #data 128 128 51800 256 seq. len. 2048 2048 256 Table 18: (Hyperparameters for Fine-tuning.) G.2 Hyper-parameters of Masked LoRA fine-tuning We list the hyper-parameters for our masked LoRA fine-tuning in Table 18. We follow the prior work to perform instruction tuning on the Alpaca dataset [Taori et al., 2023] for five epochs. Specifically, we adopt relatively small sequence length of 256 to reduce training cost, and set the LoRA rank ùëü = 8 with scaling factor ùõº = 16. warmup of 100 steps is applied with the AdamW optimizer, and microbatching is used to accommodate GPU memory limits. All models we experiment with are using the same hyperparameters, and we do not tune the parameters for the model and experiments. Hyperparameter Learning rate Batch size Micro batch size Optimizer LoRA rank (ùëü ) LoRA scaling (ùõº) LoRA dropout Warmup steps Max sequence length Training epochs Value 1 104 32 4 AdamW 8 16 0.05 100 256 5 G.3 Hadamard Transform Fusion To enable the flexibility of the model size, our framework does not apply Hadamard rotations to the pruned channels. Importantly, the hidden dimension, i.e.,, the dimension propagated across layers, remains unchanged. This design choice enables efficient on-device pruning for adaptive deployment, and avoid the mismatch shapes of the pre-fused Hadamard matrices after pruning the channels. We provide the detailed Hadamard fusion configurations for Transformer block in Table 19 and Mamba block in Table 20, where pruned channels are indicated with *. All other models follow the same Hadamard fusion pattern as these two examples. For Qwen-2.5-7B, we empirically find that applying Hadamard matrices degrades accuracy, so we remove all Hadamard matrices in our configuration. Table 20: (Hadamard matrix fusion for Mamba.) Operator Input Had. Output Had. z_proj x_proj B_proj C_proj out_proj Yes Yes Yes Yes No* No* No* No* No* Yes Table 19: (Hadamard matrix fusion for Transformers.) Input Had. Output Had. Operator q_proj k_proj v_proj o_proj up_proj gate_proj down_proj Yes Yes Yes No* Yes Yes No* No* No* No* Yes No* No* Yes"
        }
    ],
    "affiliations": [
        "Chandra Family Department of Electrical and Computer Engineering, The University of Texas at Austin",
        "Department of Computer Science, National Yang Ming Chiao Tung University",
        "Department of Electrical and Computer Engineering, Cornell University",
        "The Paul G. Allen School of Computer Science and Engineering, University of Washington"
    ]
}