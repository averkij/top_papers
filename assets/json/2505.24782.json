{
    "paper_title": "Context is Gold to find the Gold Passage: Evaluating and Training Contextual Document Embeddings",
    "authors": [
        "Max Conti",
        "Manuel Faysse",
        "Gautier Viaud",
        "Antoine Bosselut",
        "Céline Hudelot",
        "Pierre Colombo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A limitation of modern document retrieval embedding methods is that they typically encode passages (chunks) from the same documents independently, often overlooking crucial contextual information from the rest of the document that could greatly improve individual chunk representations. In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a benchmark designed to evaluate retrieval models on their ability to leverage document-wide context. Our results show that state-of-the-art embedding models struggle in retrieval scenarios where context is required. To address this limitation, we propose InSeNT (In-sequence Negative Training), a novel contrastive post-training approach which combined with late chunking pooling enhances contextual representation learning while preserving computational efficiency. Our method significantly improves retrieval quality on ConTEB without sacrificing base model performance. We further find chunks embedded with our method are more robust to suboptimal chunking strategies and larger retrieval corpus sizes. We open-source all artifacts at https://github.com/illuin-tech/contextual-embeddings."
        },
        {
            "title": "Start",
            "content": "Context is Gold to find the Gold Passage: Evaluating and Training Contextual Document Embeddings Max Conti1,4 Manuel Faysse* 1,3 Gautier Viaud1 Antoine Bosselut4 Céline Hudelot3 Pierre Colombo2,3 1Illuin Technology 2Equall.ai 3CentraleSupélec, Paris-Saclay 4EPFL Lausanne manuel.faysse@centralesupelec.fr 5 2 0 2 0 3 ] . [ 1 2 8 7 4 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "A limitation of modern document retrieval embedding methods is that they typically encode passages (chunks) from the same documents independently, often overlooking crucial contextual information from the rest of the document that could greatly improve individual chunk representations. In this work, we introduce ConTEB (Contextaware Text Embedding Benchmark), benchmark designed to evaluate retrieval models on their ability to leverage document-wide context. Our results show that state-of-the-art embedding models struggle in retrieval scenarios where context is required. To address this limitation, we propose InSeNT (In-sequence Negative Training), novel contrastive posttraining approach which combined with late chunking pooling enhances contextual representation learning while preserving computational efficiency. Our method significantly improves retrieval quality on ConTEB without sacrificing base model performance. We further find chunks embedded with our method are more robust to suboptimal chunking strategies and larger retrieval corpus sizes. We opensource all artifacts at https://github.com/ illuin-tech/contextual-embeddings."
        },
        {
            "title": "Introduction",
            "content": "The ability to rapidly process and query large-scale textual corpora is cornerstone of many industrial applications, ranging from the analysis of medical records and legal briefs to large-scale administrative archives. As these collections grow in size and complexity, advanced approaches to information retrieval (IR) particularly Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) have attracted widespread interest, yet, dealing with long documents remains an open challenge. While long context encoders have been recently developed (Zhang et al., 2024; Warner et al., 2024a; *Equal Contribution 1 Figure 1: Importance of Contextual Information: Starting from set of queries and mostly self-contained document paragraphs from the Football, we progressively reformulate paragraphs to remove information redundant with the rest of the document. This leads to sharp performance declines in standard retrieval approaches, but not in contextual retrieval approaches. Boizard et al., 2025) along with long context embedding models (Zhu et al., 2024), modern document retrieval pipelines typically segment lengthy documents into smaller chunks to optimize the granularity for efficient retrieval and readability of the retrieved content (Xu et al., 2024; Jiang et al., 2024). Traditionally, these chunks are then independently fed to an embedding model, and stored in vector database for efficient future query matching. By doing so, these systems remove strong semantic and conceptual links between the split passages, directly affecting the resulting representations. An example is illustrated in Figure 2: embedding the sentence \"He became emperor in 1804.\" without leveraging information about the person at hand (Napoléon) given in previous paragraphs will make matching queries related to Napoléon difficult. Recognizing the significant business value of incorporating broader contextual information into retrieval, major companies have explored leveraging large generative language models (LLMs) to mitigate this limitation. Some approaches attempt to circumvent retrieval altogether by feeding millions of tokens into the models context window at runtime (Gemini Team et al., 2024), while others reformulate individual passages by concatenating Figure 2: Training (Left). With respect to single query, each chunk inside batch plays different role, depending on its original document, and the positive chunk. Inference (Right). Traditional embedding methods (top) produce embeddings that do not include potentially essential contextual information. Contextualized embeddings (bottom) can integrate document-wide information in individual chunk representations, augmenting embedding relevance and improving downstream retrieval performance. them with document-level summaries and context (Anthropic, 2024). However, these methods are prohibitively expensive at scale when dealing with corpora comprising thousands of documents. Despite the critical importance of contextualized retrieval, standard benchmarks fail to capture this challenge. Evaluations traditionally focus on assessing the effectiveness of embedding models (Thakur et al., 2021; Muennighoff et al., 2022; Saad-Falcon et al., 2024), but they rely on datasets where document chunks are by design self-contained answer to the queries, which is largely idealized scenario in practice (Thakur et al., 2025). Consequently, benchmarks fail to highlight the limitations of current retrieval strategies in handling context-dependent passages. Worse, recent findings by Zhou et al. (2025) indicate that some widely-used benchmarks exhibit biases that favor standard context-agnostic retrieval methods. Companies such as Anthropic have acknowledged these issues and maintain proprietary contextual retrieval benchmarks that remain unavailable to the public1, underscoring the gap between academic evaluations and real-world industrial needs. Contribution 1: ConTEB. We introduce the Context-aware Text Embedding Benchmark, designed to assess the ability of retrieval systems to leverage information from the entire document when indexing and retrieving document chunks. ConTEB comprises both custom-designed tasks for fine-grained analysis, and practical retrieval evaluation settings spanning multiple document types, domains, and situations in which leveraging context is helpful to produce more meaningful chunk representations. We evaluate standard embedding methods on the benchmark and find they struggle when contextual awareness is required. Contribution 2: Efficient Contextual Training. Improving upon the Late Chunking method (Günther et al., 2024), we propose novel embedding post-training method that optimizes information propagation between same-document chunks at indexing time to ensure embeddings are better contextualized. Our method largely boosts performance on ConTEB, with minimal computational overhead. Through extensive ablations, we detail critical design choices and show our method improves displays increased robustness to sub-optimal chunking strategies and produces representations that scale better with corpus size. We open-source all project artifacts, including the benchmark, models and training data2."
        },
        {
            "title": "2 Problem Formulation & Related Work",
            "content": "2.1 Retrieval Frameworks In this paper, we consider the traditional retrieval framework where retrieval system given query q, searches corpus for relevant documents. Each document is scored based on its content by first embedding the text into vector space, and then computing similarity measure. The similarity between query and document is defined as sim(q, d) = (cid:0)ϕ(q), ϕ(d)(cid:1) 1https://www.anthropic.com/news/ contextual-retrieval 2https://github.com/illuin-tech/ contextual-embeddings 2 where ϕ maps text into an n-dimensional vector space and : RnRn is similarity function, such as cosine similarity or dot product. In applied settings, individual documents are often too long to be practical for retrieval purposes (Liu, 2022; Zhong et al., 2025a). Each document is thus divided into segments called chunks by partitioning function defined as P(d) = {c1, c2, . . . , cNd} In the standard retrieval setting, the score is computed solely based on chunk content: sim(q, c) = (cid:0)ϕ(q), ϕ(c)(cid:1) Additional information (priors) is however often available to the document embedding system. Typically, knowledge of the entire corpus D, or of structural metadata Mc such as neighboring document chunks obtained through P, can be leveraged by modified embedding function ϕ2, yielding the following similarity score: sim(q, c) = (cid:16) ϕ(q), ϕ2 (cid:0)c, Mc, D(cid:1)(cid:17) This work is centered on efficiently integrating priors about the entire document when embedding sub-document chunk. 2.2 Integrating Contextual Information Neural embedding models for passage-level text representation, popularized by SentenceBERT (Reimers and Gurevych, 2019), have enabled retrieval systems to move beyond lexical matching (Robertson et al., 1994). To include contextual information in these retrievers, previous works proposed methods that either operate offline during indexing, or online during querying when faced with user request. Indexing. The chunking strategy is crucial design choice and often aims to optimize chunk selfcontainment. Fixed-size approaches with overlaps preserve continuity, while structure-aware chunking respects natural text boundaries, such as paragraphs or sentences. Semantic chunking, by contrast, splits text into topic-aligned segments. These methods appear in frameworks such as LlamaIndex (Liu, 2022) and LangChain (Chase, 2022), but different queries may need different chunk sizes. Thus, dynamic chunking techniques have emerged to adapt segmentation on the fly (Zhong et al., 2025b; Qian et al., 2024). Beyond optimizing chunking, some indexing approaches enrich chunks with broader context by preprending LLMgenerated document summaries, contextual information or metadata Anthropic (2024); Poliakov and Shvai (2024). Similarly, Morris and Rush (2024) demonstrate that appending learned \"corpus\" embeddings to queries and documents can further improve retrieval. Other indexing-time techniques involve organizing chunks into higher-level data structures. For example, Edge et al. (2024) and Sarthi et al. (2024) cluster related chunks into semantic graphs or tree hierarchies. Querying. In contrast, query-time solutions rely on iterative or agentic loops to refine retrieval dynamically. LLMs can be used to iteratively update the query or request additional chunks based on partial results (Xiong et al., 2021; Trivedi et al., 2023), or even to run self-checks and seek extra context when needed (Asai et al., 2023). While these adaptive techniques can better address complex, multihop queries, they typically require much more computational resources during inference."
        },
        {
            "title": "3 ConTEB: Context-aware Text\nEmbedding Benchmark",
            "content": "3.1 Benchmark Design Existing benchmarks often rely on (or assume) selfcontained document chunks. This creates misleading perception that contextualization offers little to no benefit, which in practice is rarely the case. To address this gap, the ConTEB benchmark philosophy is to explicitly be composed of tasks in which leveraging document-wide context should lead to performance improvements. Our benchmark originates from two sources: new datasets specifically created for ConTEB, and repurposed academic datasets. We take special care in selecting data sources spanning from multiple domains, including realistic industrial scenarios. Why Context? Context can help resolve ambiguity, such as distinguishing between multiple meanings of word or resolving pronouns and entity references (co-reference resolution). It is also crucial when documents have structured format, like legal or scientific texts, where understanding table of content hierarchy is key. Concept. To isolate the importance of contextual cues and diminish other confounding factors, we construct three benchmark tasks to study contextualization in controlled experimental settings (AllenZhu, 2024). We also evaluate more practical retrieval settings at larger scale where we suspect 3 Dataset Queries Docs Tokens per Chunk Chunks per Document Context Utilization MLDR o NarrativeQA SQuAD t i D Football Geography Insurance Covid-QA ESG Reports NanoBEIR 100 8575 2067 1767 5283 120 1111 36 100 355 2067 224 530 1 115 30 56 723 170.5 154.5 19.1 86.1 113.6 80.7 153.9 205.5 199.4 15.4 4.9 8.5 3.6 4.3 60.0 29.1 123.4 Document-level reasoning Document-level reasoning Chunk not self-contained Co-reference resolution Co-reference resolution Structure understanding Chunk not self-contained Context disambiguation No context is needed Table 1: Merged ConTEB dataset details. Controlled datasets are highlighted in bold blue. NanoBEIR values are summed over the 13 datasets that compose it. contextualization to help, and in which we rely on organic, pre-existing query-document pairs. 3.2 Benchmark Construction Our generic benchmark curation pipeline is composed of three stages. We provide additional curation details in Appendix A. 1: Chunking. We select long documents spanning variety of domains and chunk them through structure-aware method3 (Rajpurkar et al., 2016; Möller et al., 2020; Koˇciský et al., 2017; Chen et al., 2024; Macé et al., 2025). 2: Pairing. We use manual answer span annotations (SQuAD, ESG) or synthetically label them with LLM (CovidQA, MLDR, NarrativeQA), to match queries with chunks obtained in Stage 1. This ensures queries are not solvable by design (Thakur et al., 2025). Alternatively, in our controlled experiment tasks, we generate queries pertaining to the chunks manually (Insurance) or synthetically using LLMs (Football, Geography). 3: Sabotage. The manually created questions in Insurance are designed to be ambiguous without prior knowledge of the document structure. This is manually verified in this phase. Going step further, in Football and Geography, we reformulate chunks with the help of LLM to remove explicit mentions of the original documents theme which all queries mention. We do so in all but the first chunks of each document, explicitly enforcing the need for context. In addition to our contextual scenarios, we use NanoBEIR (Thakur et al., 2021) to evaluate nonregression on standard non-contextualized embedding tasks. 3RecursiveCharacterSplitter with threshold of 1000 characters (Chase, 2022) By combining hard tasks in controlled environments, repurposed academic benchmarks, and realworld industrial queries, our benchmark provides comprehensive assessment of retrieval models in both standard and context-dependent retrieval scenarios. 3.3 Training Dataset Open training data is key factor to ensure fair comparison across methods and robust conclusion. In addition to our benchmark, we construct and release training dataset composed of query and document chunk pairs. It includes the training splits of MLDR and NarrativeQA, repurposed with our previously detailed pipeline. To increase the number of queries, we further use GPT-4o to generate relevant supplementary synthetic queries. We also concatenate SQuAD chunks from the same Wikipedia article, keeping track of the original question-passage associations. The full dataset contains 9881 unique long documents (3698 tokens on average), corresponding to total of 232587 chunks and 307241 queries (see subsection A.6). Scaling the dataset to more sources, through diverse synthetic augmentations and refinementbased augmentation methods (Lee et al., 2024; Wang et al., 2024) is left for future work. 3.4 Baselines Training-Free. We evaluate selection of off-the-shelf methods that are strong in their size categories such as standard singlevector embedding model based on ModernBERT (modernbert-embed-large (Warner et al., 2024b; Chaffin, 2025b)), its multi-vector ColBERT equivalent (Khattab and Zaharia, 2020; Chaffin, 2025a) and Okapi BM25 (Robertson et al., 1994), strong 4 lexical matching method. Additionally, we compare against various contextualization approaches. Specifically, we include Anthropics contextual retrieval approach (Anthropic, 2024)4, and evaluate Late Chunking (Günther et al., 2024) without specific fine-tuning using modernbert-embed-large. These methods cover standard practices with varying level of complexities and indexing budgets.5 Training-Based. fair evaluation, we For also fine-tune the sentence embedding method modernbert-embed-large on the training dataset with the same batch construction strategy as when training our main method, ensuring performance differences only stem from methodological design."
        },
        {
            "title": "4 Training Contextual Embedders",
            "content": "In this work, we leverage recent advances in longcontext embedding models (Zhang et al., 2024; Warner et al., 2024a) to improve upon existing approaches through novel training strategies. 4.1 Architecture Late Chunking. Late Chunking (Günther et al., 2024) (LC) is training-free token pooling technique designed to enable information propagation across same-document chunks. Formally, given document split into chunks {c1, . . . , cNd}, dense retrievers compute independent representations: ϕ(d) = [ϕ(c1), ϕ(c2), . . . , ϕ(cNd)] In Late Chunking, chunks are concatenated and the whole sequence representation is computed in single-forward pass: = ϕ(c1 c2 cNd) where = [h1, h2, . . . , hT ] consists of tokenlevel representations. We then apply average pooling within each original chunk to obtain chunkwise representations: ϕLC(ci) = 1 ci (cid:88) tci ht, {1, . . . , Nd} This allows each chunk representation to benefit from contextualization over the full document before aggregation. 4We use Qwen-2.5-7B-Instruct as the generative model which we serve on 80GB A100 GPU with vLLM and modernbert-embed-large as the embedding model 5We also evaluate RAPTOR (Sarthi et al., 2024) with Qwen-2.5-7B-Instruct and cde-small-v2 (Morris and Rush, 2024) but find them to be poorly adapted to our problem settings. Late Interaction. Late Interaction (LI) models (Khattab and Zaharia, 2020; Chen et al., 2024) are retrieval methods that do not pool token representations and instead store all token embeddings of each document. This approach boosts performance, especially on long-context retrieval tasks (Warner et al., 2024a; Zhu et al., 2024), at the expense of storage cost. In this work, we propose extending Late Chunking approaches to LI models by applying standard LC but simply forgoing the final pooling and storing token embeddings depending on their original chunk memberships. ϕLI (ci) = { ht : ci}, {1, . . . , Nd} (Chaffin, Setup. As the base single-vector embedfor our experiments, we use ding model modernbert-embed-large 2025b) (396M parameters), which is fine-tuned for tasks using the method from Nussretrieval baum et al. (2024). Respectively, we leverage GTE-ModernColBERT (Chaffin, 2025a) (149M parameters) for our late interaction experiments. Both models are based on ModernBERT (Warner et al., 2024a) which supports context length of up to 8,192 tokens, significantly surpassing the 512-token limit of traditional BERT models, and thereby enabling the processing of longer documents in memory efficient manner, which is critical to our method. 4.2 Learning Objective Late Chunking enables information \"leakage\" between chunks of the same document. While this training-free method showed promises, we construct learning objective to explicitly optimize contextual embedding models for this setting. Our aim is twofold: optimizing chunk representations to integrate relevant document-level information, all while ensuring they retain their specificity with respect to other same-document chunks, in order to prevent embedding collapse. Previous works (Karpukhin et al., 2020; Ni et al., 2021; Izacard et al., 2021; Li et al., 2023; Wang et al., 2022; Nussbaum et al., 2025) have relied on various learning objectives inspired by the contrastive learning literature (Schroff et al., 2015). natural choice is the InfoNCE objective (Oord et al., 2018), which samples \"negative\" embeddings from other documents of the same batch. In our approach, we combine it with an auxiliary in-sequence contrastive loss, where chunks originating from the same document as the positive 5 serve as hard negatives during training. Intuitively, training Late Chunking models contrastively with chunks from different documents encourages information propagation within each document and improves document identification. On the other hand, the contrastive term between same-document chunks ensures each chunk retains its specificity, and remains identifiable w.r.t. to its neighbors. This aspect is further motivated by the fact that in practice, queried corpora often contain negative documents stemming from the same source. Figure 2 illustrates chunk roles across training batch. Training Loss. To balance the contribution of in-sequence and in-batch negatives, we define the weighted InfoNCE loss as: = λseqLseq + (1 λseq)Lbatch (1) where λseq [0, 1]. Loss terms are defined as: (cid:34) Lseq = log exp (q k+/τ ) (cid:80) kiNseq exp (q ki/τ ) (cid:35) (cid:34) Lbatch = log exp (q k+/τ ) kj Nbatch{k+} exp (q kj/τ ) (cid:80) (cid:35) Here, denotes the query representation, and k+ is the gold chunk representation, which belongs to Nseq, the set of chunks from the same sequence as k+. Temperature τ > 0, and Nbatch is the set of all in-batch samples that do not belong to Nseq. This extends to late interaction models by replacing the dot product between query and chunk embeddings by ColBERTs MaxSim between the multiple query and document token embeddings. By tuning λseq, we can adjust the relative importance of in-sequence versus in-batch contrastive learning (Figure 3) resulting in our InSeNT method. 4.3 Model training Our training strategy (InSeNT) is designed to be lightweight and to occur on top of capable pretrained embedding models without degrading their capabilities. We use AdamW, cosine decay learning rate scheduler with 5% warm-up phase and learning rate of 5e 5 and train for 2 epochs on our training dataset. Batches are constructed by sampling 4 long documents per device, retrieving all corresponding chunks and concatenating them with separator token in between. As documents in our training set contain more than 20 chunks on average, which are themselves often linked to one or multiple queries, batch contains more than 100 query, positive, negatives triplets to learn on.6 single epoch takes less than 1 H100 GPU hour."
        },
        {
            "title": "5 Results",
            "content": "Document-wide context is essential. As seen in Table 2, methods leveraging contextual information widely outperform non-contextual methods across ConTEB tasks. These results highlight the critical role of context-aware embeddings in improving retrieval performance in such settings, whether through untrained late chunking approaches or expensive context-aware reformulation approaches. As expected, the gap is even more notable in ConTEBs controlled setting experiments. Improving contextual information propagation. Our results clearly show that InSeNT variants outperform their untrained counterpart (+14.6 nDCG@10 for ModernBERT, +11.5 for ModernColBERT). Importantly, this is not due to the nature of the training data itself; the non-contextual ModernBERT model trained on the same data (ModernBERT + Training) does not improve upon the untrained baseline. Furthermore, the tasks that display the biggest improvements are the controlled setting tasks Insurance, Football, that are explicitly designed to elicit information given in previous paragraphs, and that are out-of-domain w.r.t. our training set. Late Interaction. Interestingly, while LI models are good at long-context retrieving, they are poorly suited to out-of-the-box late chunking (-0.3 nDCG@10 w.r.t. ModernColBERT without LI). We posit that since token embeddings are never pooled, these models learn very local features and cannot leverage information from neighboring tokens. Once trained with our method, ModernColBERT+InSeNT displays large performance gains across the board (+11.5 nDCG@10 w.r.t. ModernColBERT + Late Chunking), showcasing an increased ability to leverage external context. Context can add noise. The CovidQA task sticks out from the rest as untrained late chunking approaches severely degrade performance. Qualitative analysis, as well as the strong performance of the non-contextualized ModernColBERT method, indicate that the query-chunk pairing are often very extractive and match on technical medical terms, 6In MB+Training, data is sampled the same way for fair evaluation but flattened in batch, corresponding to per-device batch sizes of more than 100. 6 In-Domain Out-Of-Domain Practical Settings Controlled Settings Non-Contextual M 69.4 78.4 83.5 78.7 85.4 78.5 84.1 Q 56.2 73.4 74.2 74.0 77.1 77.1 75.7 v r 74.7 77.9 80.4 77.3 77.7 75.8 80.7 - D I 53.7 61.7 78.2 55.2 60.7 40.0 75.5 o G 19.9 36.8 44.2 20. 34.8 31.7 44.4 a e 45.6 56.2 68.5 58.7 89.4 89.6 67.9 a n 0.0 12.4 16.1 13. 100.0 41.0 13.2 b F 12.2 19.1 30.2 22.9 53.9 54.6 31.3 r 41.5 52.0 59.4 50. 72.4 61.0 59.1 t ) / ( 4.29 17.83 14.99 16.44 1890.94 15.81 7.41 88.7 90.1 80.9 75.1 81.3 83.5 56.0 67. 43.1 48.3 63.9 64.6 90.7 89.8 100.0 45.9 75.6 70.6 15.26 7. E a 43.4 63.2 67.7 54.5 63.2 63.2 68.2 63.2 59.2 Non-Contextual Models BM25 ModernBERT Large ModernColBERT ModernBERT Large + Training Untrained Contextual Models Anthropic Contextual ModernBERT Large + Late Chunking ModernColBERT + Late Chunking Trained Contextual Models ModernBERT Large + InSeNT ModernColBERT + InSeNT Table 2: Evaluation (nDCG@10) of baseline models and our proposed method on ConTEB. Runtime is per-document indexing time in milliseconds; smaller is better, so the fastest model is bolded. task. When documents need to be disambiguated between one another (NanoBEIR, Geography), upweighting in-batch negatives seems optimal. On tasks where the challenge lies in locating information within given document (NarrativeQA, CovidQA), in-sequence negatives play large role, but still need to be combined to in-batch negatives. Striking the optimal trade-off is thus very use-case dependent, and we opt for λseq = 0.1 after tuning on the validation split of our training dataset. Efficiency-Performance. As shown in the Runtime column of Table 2, our approach is very capable on contextual tasks, yet does not add much computational overhead. In fact, we find slight indexing speed improvements, attributed to our approachs reduced need for padding in-batch sequences of different lengths. While Anthropic Contextual achieves sensibly similar performances on ConTEB, it relies on costly LLM-based summarization and chunk reformulation, that are hardly scalable to huge corpora (120x slower). Short-Context Performance. Careful hyperparameter tuning enables our best model to maintain strong performance on standard non-contextual benchmarks (NanoBEIR), demonstrating that longcontext optimization does not compromise shortcontext retrieval. Interestingly, LI models suffer from more degradation, which we posit is due to the original reliance on very local features modified through our training. Mixing in non-contextual \"replay\" data during training or merging models Importance of λseq: Figure 3: for ModernBERT-Large trained with varying λseq. Optimal values depend on the task, but integrating both in-sequence and in-batch negatives is crucial to performance. Results thus rendering context less useful. Our results show that naively applying late chunking in this setting adds noise and leads to notable performance drops (-21 nDCG@10), which are in large part recovered through our training method (+16 nDCG@10). λseq matters. The training objectives are to induce chunk representations to integrate document-level information (role of in-batch negatives) while maintaining their specificity with respect to other same document chunks (role of in-sequence negatives). By varying λseq from Equation 1, we weight the importance of both objectives. After training series of models with varying λseq, we see on Figure 3 that training with only insequence or in-batch negatives yields the worse results, and the optimal λseq varies depending on the 7 Figure 4: Contextualized models trained with InSeNT are more robust to aggressive chunking strategies that remove essential information from chunks (left), and scale better with corpus size and ambiguity (right). (Wang et al., 2025) should further enable preserving the original embedding models performances."
        },
        {
            "title": "6 Ablations",
            "content": "Robustness to chunking. We assess our methods robustness to poor chunking strategies using SQuAD annotations. Each originally self-contained chunk is split in multiple progressively smaller subchunks to while we keep track of the annotated answer span to identify the gold chunk. Eventually, these sub-chunks become too small to be self-contained and end up lacking sufficient information to be relevantly embedded on their own. Figure 4 (left) demonstrates that contextual embeddings greatly improves robustness w.r.t. suboptimal chunking. The model is able to elicit information from neighboring chunks to integrate contextual information within smaller sub-chunks, leading to much more uniform retrieval performance across wide range of chunk sizes. Robustness to corpus size. Common in the industry are templated documents that differ mostly by key aspect (year, company name) but contain otherwise very similar information. We study the dynamics of retrieval performance w.r.t. to the amount of similar documents in the corpus by computing scaling laws in which we iteratively vary the number of unique documents (composed of multiple chunks) in the corpus. We observe in Figure 4 (right) that contextual embeddings scale vastly differently than their independently embedded counterpart. Intuitively, the greater the amount of similar documents and chunks in the corpus, the harder it is for retrieval system to match the correct ones, but when embedding models are able to leverage external context, this effect is attenuated. Information Propagation. We experiment with concatenating semantically similar yet independent short chunks as \"artificial\" long documents. The resulting model is contextual as it uses late chunking, but exhibits performances in-line with non-contextual baselines (ModernBERT Large + Training). We posit training on arbitrarily concatenated chunks, which by design are not contextually linked, teaches the model not to use information from neighboring chunks. This highlight the necessity of sourcing organic long-context data during training to induce correct training dynamics. Details in Table 4 in Appendix C."
        },
        {
            "title": "7 Conclusions",
            "content": "In this work, we introduced ConTEB, benchmark designed to assess the effectiveness of retrieval models in leveraging document-wide contextual information. Our evaluation demonstrates that standard retrieval models struggle in contextdependent settings, while our proposed approach InSeNT, which combines late chunking and novel training methodology performs strongly on ConTEB without additional compute costs. Future Work. Scaling our approach with recent decoder models with extended context lengths (e.g., 1M+ tokens (Yang et al., 2025)) would enable embedding entire books or lengthy documents in single forward pass, potentially unlocking new capabilities for large-scale document retrieval. It would also be interesting to observe the impact of our method on retrieval confidence (Gisserot-Boukhlef et al., 2024). Finally, adapting our method to multimodal embedding pipelines that have less control over the chunking strategy could further enhance retrieval systems in industrial applications with visually rich contextual documents (Faysse et al., 2025; Ma et al., 2024)."
        },
        {
            "title": "Limitations",
            "content": "While our approach enhances retrieval performance in context-dependent settings, limitations persist. Context Length. Our method is applied to longcontext encoders that currently support sequences of up to 8k tokens. While we have shown we can extrapolate performance to sequences of up to 32k tokens, scaling this approach to handle 1M+ token contexts with decoder-based models would be an interesting research avenue and presents significant compute and memory challenges. Additionally, it requires rethinking the data construction processes to ensure longer documents are effectively leveraged. Data Generation. The creation of training and evaluation data relies on existing datasets and semisynthetic generation pipelines. However, fully automated and scalable method for generating highquality queries that effectively induce non-trivial context utilization remains an open challenge. Evaluation. While our model demonstrates strong cross-domain performance, further validation in real-world applications, various use cases, and multiple languages is necessary to further assess its robustness and generalizability."
        },
        {
            "title": "Ethical Considerations",
            "content": "Bias. As our method introduces novel way of leveraging document-wide context, the nature of information propagation between chunks remains uncertain. This may introduce biases that traditional embedding models do not encounter, necessitating further analysis. Ecological Impact. Our post-training approach is computationally efficient, with total training and evaluation runs requiring fewer than 100 GPU hours on H100 hardware. By providing costeffective alternative to LLM-dependent contextualization techniques, we aim to reduce the environmental footprint of large-scale retrieval systems. Social Impact. Improved retrieval capabilities can drive significant business benefits, particularly in industries that rely on processing extensive and structured documents, such as legal, medical, and financial sectors."
        },
        {
            "title": "References",
            "content": "Zeyuan Allen-Zhu. 2024. ICML 2024 Tutorial: Physics Project page: https:// of Language Models. physics.allen-zhu.com/. Anthropic. 2024. Introducing contextual retrieval. Accessed: 2025-02-10. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. Preprint, arXiv:2310.11511. Nicolas Boizard, Hippolyte Gisserot-Boukhlef, Duarte M. Alves, André Martins, Ayoub Hammal, Caio Corro, Céline Hudelot, Emmanuel Malherbe, Etienne Malaboeuf, Fanny Jourdan, Gabriel Hautreux, João Alves, Kevin El-Haddad, Manuel Faysse, Maxime Peyrard, Nuno M. Guerreiro, Patrick Fernandes, Ricardo Rei, and Pierre Colombo. 2025. Eurobert: Scaling multilingual encoders for european languages. Preprint, arXiv:2503.05500. Antoine Chaffin. 2025a. Gte-moderncolbert. Antoine Chaffin. 2025b. Modernbert-embed-large. Harrison Chase. 2022. LangChain. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, BGE M3Defu Lian, and Zheng Liu. 2024. Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through SelfKnowledge Distillation. arXiv preprint. Version Number: 3. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: graph rag approach to query-focused summarization. Preprint, arXiv:2404.16130. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2025. Colpali: Efficient document retrieval with vision language models. Preprint, arXiv:2407.01449. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, Soroosh Mariooryad, Yifan Ding, Xinyang Geng, Fred Alcober, Roy Frostig, Mark Omernick, Lexi Walker, Cosmin Paduraru, Christina Sorokin, and 1118 others. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Preprint, arXiv:2403.05530. Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Céline Hudelot, and Pierre Colombo. 2024. Towards trustworthy reranking: simple yet effective abstention mechanism. Preprint, arXiv:2402.12997. Michael Günther, Isabelle Mohr, Daniel James Williams, Bo Wang, and Han Xiao. 2024. Late chunking: Contextual chunk embeddings using long-context embedding models. Preprint, arXiv:2409.04701. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised Dense Information Retrieval with Contrastive Learning. arXiv preprint. Version Number: 4. 9 Ziyan Jiang, Xueguang Ma, and Wenhu Chen. 2024. Longrag: Enhancing retrieval-augmented generation with long-context llms. Preprint, arXiv:2406.15319. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. arXiv preprint. Version Number: 3. Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2017. The narrativeqa reading comprehension challenge. Preprint, arXiv:1712.07040. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. Preprint, arXiv:2405.17428. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for KnowledgeIntensive NLP Tasks. arXiv preprint. Version Number: 4. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. Preprint, arXiv:2308.03281. Jerry Liu. 2022. LlamaIndex. Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. 2024. Unifying multimodal retrieval via document screenshot embedding. Preprint, arXiv:2406.11251. Quentin Macé, António Loison, and Manuel Faysse. 2025. Vidore benchmark v2: Raising the bar for visual retrieval. Preprint, arXiv:2505.17166. Timo Möller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. 2020. COVID-QA: question answering dataset for COVID-19. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, Online. Association for Computational Linguistics. John X. Morris and Alexander M. Rush. 2024. Preprint, Contextual document embeddings. arXiv:2410.02525. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2021. Large dual encoders are generalizable retrievers. Preprint, arXiv:2112.07899. Zach Nussbaum, John X. Morris, Brandon Duderstadt, and Andriy Mulyar. 2024. Nomic embed: Training reproducible long context text embedder. Preprint, arXiv:2402.01613. Zach Nussbaum, John X. Morris, Brandon Duderstadt, and Andriy Mulyar. 2025. Nomic embed: Training reproducible long context text embedder. Preprint, arXiv:2402.01613. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. arXiv preprint. Version Number: 2. Mykhailo Poliakov and Nadiya Shvai. 2024. Multimeta-rag: Improving rag for multi-hop queries using database filtering with llm-extracted metadata. Preprint, arXiv:2406.13213. Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, and Zhicheng Dou. 2024. Grounding language model with chunking-free in-context retrieval. Preprint, arXiv:2402.09760. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence Embeddings using Siamese BERTNetworks. arXiv preprint. Version Number: 1. Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109 126. National Institute of Standards and Technology (NIST). Jon Saad-Falcon, Daniel Y. Fu, Simran Arora, Neel Guha, and Christopher Ré. 2024. Benchmarking and building long-context retrieval models with loco and m2-bert. Preprint, arXiv:2402.07440. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. 2024. Raptor: Recursive abstractive processing for tree-organized retrieval. Preprint, arXiv:2401.18059. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. MTEB: Massive Text Embedding Benchmark. arXiv preprint. Version Number: 3. Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. FaceNet: Unified Embedding for Face Recognition and Clustering. Publisher: arXiv Version Number: 3. 10 Nandan Thakur, Jimmy Lin, Sam Havens, Michael Carbin, Omar Khattab, and Andrew Drozdov. 2025. Freshstack: Building realistic benchmarks for evaluating retrieval on technical documents. Preprint, arXiv:2504.13128. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. arXiv preprint. Version Number: 4. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. Preprint, arXiv:2212.10509. Ke Wang, Nikolaos Dimitriadis, Alessandro Favero, Guillermo Ortiz-Jimenez, Francois Fleuret, and Pascal Frossard. 2025. Lines: Post-training layer scaling prevents forgetting and enhances model merging. Preprint, arXiv:2410.17146. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by WeaklySupervised Contrastive Pre-training. arXiv preprint. Version Number: 2. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Improving text embeddings with large language models. Preprint, arXiv:2401.00368. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. 2024a. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. Preprint, arXiv:2412.13663. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. 2024b. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. Preprint, arXiv:2412.13663. Wenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar Mehdad, Wen tau Yih, Sebastian Riedel, Douwe Kiela, and Barlas Oguz. 2021. Answering complex open-domain questions with multi-hop dense retrieval. Preprint, arXiv:2009.12756. Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. 2024. Retrieval meets long context large language models. Preprint, arXiv:2310.03025. An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, and 9 others. 2025. Qwen2.5-1m technical report. Preprint, arXiv:2501.15383. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, and 1 others. 2024. mgte: Generalized long-context text representation and reranking models for multilingual text retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 13931412. Zijie Zhong, Hanwen Liu, Xiaoya Cui, Xiaofan Zhang, and Zengchang Qin. 2025a. Mix-of-granularity: Optimize the chunking granularity for retrievalaugmented generation. Preprint, arXiv:2406.00456. Zijie Zhong, Hanwen Liu, Xiaoya Cui, Xiaofan Zhang, and Zengchang Qin. 2025b. Mix-of-granularity: Optimize the chunking granularity for retrievalaugmented generation. Preprint, arXiv:2406.00456. Yang Zhou, Hongyi Liu, Zhuoming Chen, Yuandong Tian, and Beidi Chen. 2025. Gsm-infinite: How do your llms behave over infinitely increasing context length and reasoning complexity? Preprint, arXiv:2502.05252. Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2024. Longembed: Extending embedding models for long context retrieval. Preprint, arXiv:2404.12096."
        },
        {
            "title": "A ConTEB Details",
            "content": "This appendix describes the data generation process employed in this project. The methodology varies based on the dataset source, but generally, long documents are segmented into smaller chunks. If preexisting queries are available, they are mapped to relevant chunks using either provided answer spans (e.g., SQuAD) or tagged using GPT-4o. In cases where queries are unavailable, large language model (LLM) generates them before associating them with the relevant text segments. This approach, illustrated in 5, is systematically applied across multiple datasets. A.1 Wiki-based Datasets Football and Geography are our two wiki-based datasets, focusing on the Sports and Geography domains. Wikipedia Data Extraction The pipeline first retrieves Wikipedia summaries for given person using the wikipediaapi library. The extracted summary is then split into paragraphs. 11 Figure 5: Benchmark creation process. Text Rephrasing Each paragraph from the Wikipedia summary undergoes rephrasing process to remove direct mentions of the persons name while maintaining the original context. The rephrased text replaces names with pronouns such as he or she. This transformation is performed using the GPT-4o model via the following prompt: Here is Wikipedia article: [Full Wikipedia Summary] Can you rephrase the following paragraph to remove all mention of the name of the person the article is about? You can leave other names as is and can replace the name with words such as he/she or other generic paraphrases. [Paragraph to be rephrased] Question Generation For each paragraph in the summary, the model generates three questions related to the person. The questions explicitly mention the persons name but do not include other named entities such as dates or proper nouns. The generation follows this structured prompt: Here is Wikipedia article: [Full Wikipedia Summary] Using specifically the following paragraph, can you ask 3 questions related to the person the article is about? Each question must mention the name of the person, but the question should not contain other named entities (dates, other proper nouns). Format the response as Python list of strings and do not output anything else. A.2 NarrativeQA, COVID-QA, MLDR NarrativeQA (literature), MLDR (encyclopedic) and Covid-QA (medical) consist of long documents, associated to existing sets of question-answer pairs. We chunk these documents, and use GPT-4o to annotate which chunk, among the gold document, best contains information needed to answer the query. Since chunking is done posteriori without considering the questions, chunks are not always self-contained and eliciting document-wide context can help build meaningful representations. Synthetic Query Generation: To extend MLDR for our training dataset, OpenAIs GPT4o model is prompted to generate 20-50 realistic queries per document, ensuring that each query aligns with the content of at least one chunk. This is on top of the queries that are already incuded in the dataset. Synthetic queries are included only in our training dataset. A.3 Insurance Insurance is composed of long document with insurance-related statistics for each country of the European Union. Countries are often not referred to in-text, but only once in the section title. Therefore, certain chunks require knowledge of their position within the document to be properly disambiguated from others. Questions are manually crafted to require structural understanding for accurate chunk matching. This process, in addition to manual verification of the contextuality quality, makes Insurance controlled dataset. Since questions are crafted after the chunking process, the annotation results directly from the manual question generation process. A.4 SQuAD [Paragraph be question generation] to used for SQuAD is an extractive QA dataset with questions associated to passages and annotated answer spans, that allow us to chunk individual passages into shorter sequences while preserving the original annotation. In particular, we do not use token skiplists at inference time, and use single document prefix for the whole document sequence. A.5 ESG Reports"
        },
        {
            "title": "C Additional Results",
            "content": "C.1 Training with concatenated short documents Results of training an InSeNT model with concatenated short document data (using the Nomic dataset) are available in Table 4. Short docs are clustered from the nomic-supervised dataset (Nussbaum et al., 2024) following Morris and Rush (2024). This approach did not yield promising results, proving that natively long documents are necessary to induce relevant in-sequence signal. C.2 Full ablation results on λseq We show the results of the different values for λseq on all our evaluation sets. C.3 Extending context beyond 8192 tokens ModernBERT was trained on documents of up to 8192 tokens (Warner et al., 2024a). Its Late Interaction counterpart, GTE-ModernColBERT, was exclusively fine-tuned on documents of no more than 300 tokens. However, its generalization capabilities to longer documents have been shown by its developers (Chaffin, 2025a), hinting at the fact that further research along those lines could be tried for both the bi-encoder and the LI variants. Based on these results, we tried two approaches to handle documents longer than 8192 tokens with ModernBERT (necessary for the ESG reports dataset): computing Late Chunking with context of max. 8192 tokens in an sliding window fashion (computing chunk embeddings in several forward passes of 8192 tokens, with 10 overlapping chunks between the various windows), and naively feeding the complete documents to the embedder. To our surprise, the latter worked better by large margin (43.1 on ESG as reported in 2, vs 25.4 for the sliding window approach), so we reported the results of this approach. Further studies could be led to better understand the dynamics underlying this extension. ESG Reports contains long documents from the fast-food industry, with manually annotated querypage pairs from the ViDoRe Benchmark v2 (Macé et al., 2025), originally thought for visual retrieving7. We convert all documents to text, chunk them, and re-annotate the resulting passages by hand, filtering out queries that relied solely on visual aspects (e.g., tables, graphs). A.6 Training Data Statistics Table 3 displays information about the training data. Our refined version of MLDR forms large part of the training corpus. We can see that the majority of chunks are used as positives at least once, ensuring that the model is not biased towards the position of the chunk in the sequence. MLDR NarrativeQA SQuAD Total Number of Docs Number of Chunks Number of Queries Number of Chunks per Doc % Chunks with associated Query Number of Tokens per Doc Number of Tokens per Query 8467 213001 211933 25.2 94.6% 3962.6 16.7 972 5219 27953 5.4 9881 442 232587 14367 307241 67355 32.5 23.5 81.9% 100.0% 94.61% 3698.2 4966.1 819.1 16.3 12.5 21.9 Table 3: Training Dataset Statistics"
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Sequence prefixes ModernBERT-based models are trained with query and document prefixes. We apply the same approach in our training and inference frameworks. After several tests, we opt for using single document prefix for the Late Chunking sequence, instead of adding document prefix at the beginning of each chunk inside the same sequence. We separate chunks with [SEP] tokens to let the model understand the concept of chunks during its token embedding computation. B.2 Late Interaction Models We leverage the pylate8 library for the Late Interaction implementation. For training LI models with InSeNT, we adapt the LI mechanisms to incorporate it with Late Chunking in our own codebase. 7https://huggingface.co/datasets/vidore/ restaurant_esg_reports_beir 8https://github.com/lightonai/pylate 13 MLDR SQuAD NarrativeQA Football Geography COVID-QA Insurance NanoBEIR Average Runtime (s) MB MB+InSeNT(Nomic) MB+Late Chunking Ours: MB+InSeNT 78.4 77.8 78.5 88. 73.4 76.0 77.1 80.9 77.9 76.2 75.8 81.3 19.1 26.2 54.6 63. 56.2 62.7 89.6 90.7 61.7 38.8 40.0 56.0 12.4 63.7 41.0 100. 63.2 59.9 63.2 60.4 55.3 60.2 65.0 77.8 40.0 36.3 36.3 36. Table 4: Evaluation (nDCG@10) of baseline models and our proposed method on ConTEB. We show MB+InSeNT(Nomic) behaves like non-contextual model after training on independant documents concatenated in single sequence. Figure 6: Evaluation results for varying λseq values. Left: ModernBERT-Large. Right: GTE-ModernColBERT. Trends vary across the datasets depending on their nature."
        }
    ],
    "affiliations": [
        "CentraleSupélec, Paris-Saclay",
        "EPFL Lausanne",
        "Equall.ai",
        "Illuin Technology"
    ]
}