{
    "paper_title": "X-Dancer: Expressive Music to Human Dance Video Generation",
    "authors": [
        "Zeyuan Chen",
        "Hongyi Xu",
        "Guoxian Song",
        "You Xie",
        "Chenxu Zhang",
        "Xin Chen",
        "Chao Wang",
        "Di Chang",
        "Linjie Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes."
        },
        {
            "title": "Start",
            "content": "X-Dancer: Expressive Music to Human Dance Video Generation Zeyuan Chen1,2 Hongyi Xu2 Guoxian Song2 You Xie2 Chenxu Zhang2 Xin Chen2 Chao Wang2 Di Chang2,3 1UC San Diego 2ByteDance Linjie Luo2 3University of Southern California 5 2 0 F 4 2 ] . [ 1 4 1 4 7 1 . 2 0 5 2 : r Figure 1. We present X-Dancer, unified transformer-diffusion framework for zero-shot, music-driven human image animation from single static image, capable of handling diverse body forms and appearances. Our method enables the synthesis of highly expressive and diverse full-body dance motions that are synchronized with music, including detailed movements at the head and hands, which are then seamlessly translated into vivid and lifelike dance videos. Code and model will be available for research purposes."
        },
        {
            "title": "Abstract",
            "content": "We present X-Dancer, novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from single static image. As its core, we introduce unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. 1. Introduction Dance is universal form of self-expression and social communication, deeply embedded in human behavior and culture. With the rise of social media platforms like TikTok and YouTube Shorts, people increasingly share self-expressive dance videos online. However, creating expressive choreography typically demands practice and even professional training. From computational aspect, generating realistic dance movements is challenging due to the freeform, personalized nature of dance and its alignment with musical rhythm and structure. In this work, we tackle the challenge of creating continuous, expressive and lifelike dance videos from single static image, driven solely by music track. This study addresses two key challenges in music-driven human image animation: (1) generating smooth, diverse full-body movements at finer scales that capture the complex, non-linear synchronization with music inputs, and (2) translating these generated body movements into highfidelity video outputs that maintain visual consistency with the reference image and ensure temporal smoothness. Prior approaches [32, 37] mainly focus on computationally generating 3D human poses, such as SMPL skeletons [26], from music inputs, utilizing diffusionor GPT-based frameworks. While these methods excel in producing highquality, clean motions, they are constrained by limited training datasets (primarily the multi-view AIST dataset [21, 38]), which lack diversity, and contain 3D body poses only (excluding head and hands motions). Expanding these datasets with widely available 2D monocular dance videos would require 3D human pose estimation, which is often error-prone and risks degraded motion quality and consistency. Moreover, we target at photorealistic dance video generation rather than 3D skeleton or mesh animations. With recent advances in diffusion models that have achieved unprecedented diversity and quality in image and video generation, numerous works have leveraged their generative capabilities to synthesize visually compelling videos by animating reference image with motion signals such as 2D skeletons [6, 15], dense poses [43], and depth maps [52]. Unlike motion transfer settings that derive motion from driving video, our goal is to generate motion signals that are consistent with the reference body shape and aligned with the input musical beats. Recently, few studies [35, 42] have attempted to synthesize visual outputs end-to-end from audio inputs using diffusion models. While these methods have advanced in realism and dynamic quality, they still struggle to capture long-range motion and audio context due to high computational demands. Moreover, these frameworks have primarily been validated on audiodriven portrait animations, leaving uncertain whether they can accommodate the complexities of full-body kinematics and rapid motion transitions. To this end, we propose X-Dancer, unified framework that integrates an autoregressive transformer model for generating extended dance sequences attuned to input music, coupled with diffusion model to produce high-resolution, lifelike videos. In contrast to prior methods focused on 3D music-to-dance motion generation, our approach models 2D human motion, leveraging widely accessible dance videos where 2D pose estimation is both reliable and scalable. For effective autoregressive motion generation, we develop multi-part tokenization scheme for per-frame 2D whole-body poses, incorporating detected keypoint confidences to capture multi-scale human motions with motion blur and various occlusions. Thereafter we train crossmodal transformer that auto-regressively predicts future Nframe pose tokens, paired with per-frame music features extracted with Jukebox [9] and Librosa [19]. Our design enables the model to capture broader diversity of expressive, music-aligned movements with enhanced scalability in both model complexity and data scale. We then leverage T2I diffusion model with temporal layers to animate the reference image by implicitly translating the generated confidence-aware motion tokens into spatial guidance via trainable motion token decoder. Specifically, it upsamples from learnable feature map into multi-scale spatial feature guidance, integrating motion tokens through AdaIN [16] during the upsampling process. By co-learning pose translation with temporal and appearance reference modules [5, 12], the diffusion backbone interprets motion tokens with temporal context and body shape reference, leading to better shape-disentangled pose control, smoother and more robust visual outputs even under low-confidence or jittering pose sequences. This design establishes an endto-end transformer-diffusion learning framework, merging the transformers strengths in long-context understanding and generation with the diffusion models high-quality visual synthesis. To the best of our knowledge, X-Dancer represents the first music-driven human image animation framework. Trained on curated dataset of 100K music-dance video clips, our method excels at generating diverse, expressive and detailed whole-body dance video animations attuned to input music, adaptable to both realistic and stylized human images across various body types. We comprehensively evaluate our model on challenging benchmarks, and XDancer outperforms state-of-the-art baselines both quantitatively and qualitatively. Additionally, we highlight its scalability and customization capabilities, showcasing scalability across different model and data scales, as well as fine-tuning to specific choreography. We summarize our contributions as follows, novel transformer-diffusion based music-to-dance human image animation, achieving state-of-the-art performance in terms of motion diversity, expressiveness, music alignment and video quality. cross-modal transformer model that captures longrange synchronized dance movements with music features, employing multi-scale tokenization scheme of whole-body 2D human poses with keypoint confidence. diffusion-based human image animation model that interprets temporal pose tokens and translates them into consistent high-resolution video outputs. Demonstration of captivating zero-shot music-driven human image animations, along with characterized choreography generation. 2. Related Work has further empowered prior methods [6, 15, 43, 52] to create realistic dance videos from reference image using motion signals such as 2D skeletons, dense poses, and depth maps. While these methods provide explicit control over body poses and allow flexible motion manipulation, they often struggle to preserve the original body shape and other identity-specific features, leading to noticeable identity leakage, particularly when significant differences exist between the target and source. Several methods [18, 35, 42] adopt an end-to-end pipeline for generating realistic talking human videos from audio by injecting audio features directly into the network via cross-attention layers. However, while these methods excel in capturing micro-expressions and lip movements, they fall short in handling dynamic, shape-preserving dance movements. X-Dancer uses diffusion backbone with AdaIN to animate the reference image in an end-to-end framework, enabling high-quality, shapeconsistent dance generation while effectively preserving the identity of the reference image. 2.1. Music to Dance Generation 3. Method Significant progress has been made in realistic human motion generation [7, 27, 32, 37, 48, 51] from various inputs, such as input speech [2, 11, 25, 30, 47] or text [17, 29, 34, 50]. However, the task of music-to-dance generation [1, 20, 23, 24, 28, 32, 37, 46] presents unique challenges: (1) ensuring the generated dance rhythmically aligns with the music, and (2) producing intricate motions with diverse styles and speeds. Due to data limitations, several 3D human pose datasets [2022, 28] have been proposed. The AIST++ dataset [21] is 3D music pose dataset containing 1,408 dance motions tailored to various music genres. FineDance [22], 3D dataset focusing on finegrained hand motions, includes 14 hours of dance data. Models like Bailando [32] and EDGE [37] have leveraged AIST++ for training. Bailando [32] pioneered using VQVAE for 3D pose encoding, followed by an Actor-Critic GPT model to generate body poses conditioned on music. EDGE [37] applied diffusion framework to predict human poses from noisy sequence, conditioning on music and employing long-form sampling for extended dance sequences. Despite these advancements, existing approaches trained on 3D datasets often generate dance poses with limited diversity and are primarily confined to 3D format. X-Dancer overcomes these data constraints by leveraging broad spectrum of 2D dance motions, aligning complex movements with musical beats, and utilizing widely available video data to enhance scalability. 2.2. Human Image Animation With the advancements in diffusion models [4, 12, 14], generating high-quality human videos has become feasible. The introduction of ControlNet [49] and PoseGuider [15] Given single portrait as the reference image IR and conditioning music sequence represented as Mt, our objective is to generate sequence of dance frames IMt, where = 1, . . . , denotes the frame index. The generated sequence IMt seeks to maintain the appearance and background context depicted in IR while present expressive dance movements in harmony with the provided musical rhythm and beats. As illustrated in Fig. 2, our model is trained in two transformer-based 2D human dance motion genstages: eration (Section. 3.2), and diffusion-based video synthesis (Section. 3.3) from the generated motion sequence. 3.1. Data Representations Generative Visual Representations. To generate human dance videos, we employ Latent Diffusion Models [31] that synthesize samples in the image latent space, facilitated by pretrained image auto-encoder. During training, latent representations of images are progressively corrupted with Gaussian noise ϵ, following the Denoising Diffusion Probabilistic Model (DDPM) framework [13, 33]. UNet-based denoising framework, enhanced with spatial and temporal attention blocks, is then trained to learn the reverse denoising process. For training, we apply human-centric cropping to the videos, yielding unified resolution of 896 512, that enInstead of dicompasses half-body to full-body dances. rectly capturing the intricate pixel-wise movements on music conditions, we first establish the correlation between music and human dance movement, which subsequently guides the final visual synthesis. Motion Modeling. In contrast to prior approaches that generate human motions in 3D space, we represent diverse Figure 2. Overview of X-Dancer. We propose cross-conditional transformer model to autoregressively generate 2D human poses synchronized with input music, followed by diffusion model that produces high-fidelity videos from single reference image IR. First, we develop multi-part compositional tokenization for 2D poses, encoding and quantizing each body part independently with keypoint confidence. These tokens are then merged into whole-body, confidence-aware pose using shared decoder. Next, we train GPT-based transformer to autoregressively predict future pose tokens with causal attention, conditioned on past poses and aligned music embeddings. For global music style and motion context, we incorporate the entire music sequence and sampled prior poses. With learnable motion decoder, we generate multi-scale spatial pose guidance upsampled from learned feature map, incorporating the generated motion tokens within temporal window (16 frames) using AdaIN. By co-training the motion decoder and temporal modules, our diffusion model is capable of synthesizing temporally smooth and high-fidelity video frames, while maintaining consistent appearance with the reference image with trained reference net. dance motions as 2D pose sequences. Compared to its 3D counterparts, 2D human dance motions are widely accessible from large collections of monocular videos, eliminating the need for complex multiview capture setups or labor-intensive 3D animations. Furthermore, the detection of 2D poses is significantly more reliable and robust. To enhance the realism and motion expressiveness, we model not only large body articulations but also finer details such as head movements and hand gestures, capturing subtle nuances in motion. Notably, we incorporate keypoint detection confidence into our pose representation, allowing the model to account for motion blur and occlusions. Each perframe whole-body pose with joints is thus represented as RJ3, where the last dimension encodes the keypoint confidence. Music Embedding. Inspired from [37], we utilize the pre-trained Jukebox model [9] to extract rich musical features, supplemented by rhythmic information with one-hot encoding of music beats using an audio processing toolbox Librosa [19]. We resample and synchronize these extracted embeddingsdenoted as 1:T for Librosato the video frame rate, ensuring per-frame alignment between the music and visual elements. 1:T for Jukebox and 3.2. Transformer-Based Music-to-Dance Motion"
        },
        {
            "title": "Generation",
            "content": "Given collection of monocular, single-person, musicsynchronized dance videos with paired 2D whole-body pose detections, we aim to model the intricate, non-linear correlation between skeletal dance motion and musical features. To achieve this, we first introduce compositional, confidence-aware VQ-VAE, which captures diverse and nuanced human dance poses across different body parts. Next, we leverage GPT-like transformer to autoregressively predict future pose tokens, modeling temporal motion transitions within the token space and aligning them with synchronized music embeddings. Compositional Confidence-Aware Human Pose Tokenization. Our approach builds on the standard VQ-VAE framework, trained in self-supervised manner. Given whole-body 2D poses with associated keypoint confidences p, an 1D convolutional encoder maps into latent pose embedding ze(p) = E(p), which is then quantized by mapping to its nearest representation zq(p) within learnable codebook, and finally the decoder reconstructs the pose ˆp from zq(p). However, as prior studies [32, 41, 45] suggest, the dependencies between spatial keypoints are complex, and vanilla VQ-VAE often struggles to capture subtle pose details, such as finger movements and head tilts, due to information loss during quantization and multi-frequency nature of pose variations across different body parts. To improve expressive coverage, we train independent 2D pose encoders Ej and learn = 5 separate codebooks for upper and lower half bodies, left and right hands, and head respectively, allowing the model to spatially decompose 2D whole-body pose variations across different frequency levels. By partitioning poses this way, distinct body-part codes can be flexibly combined, enhancing the range of expressiveness represented within each individual codebook. To capture part-wise spatial correlations and ensure information flow across body parts, we concatenate the quantized pose latents and feed them into shared decoder. The resulting embedding is then mapped to reconstructed keypoint coordinates through separate projection heads, enabling joint reconstruction while preserving nuanced partspecific details. We train encoder and decoder simultaneously with the compositional codebooks with the following loss function: (cid:88) (cid:88) sg (cid:2)zj e(p)(cid:3) zj q(p)2, (1) ˆpj pj2 + β LVQ = dual conditioning allows our model to produce both globally coherent and locally synchronized dance movements. We use = 64 frames for our autoregressive model training. To handle extended motion sequence generation with consistent motion styles and smooth transitions, we additionally incorporate cross-segment motion context into the transformer model. Specifically, we uniformly sample subset of 8 frames from the previous motion segment as the motion context, and append them after the global music context inputs. We denote the combined context as Fg. Since we model body parts independently, maintaining coherence in the assembled whole-body poses is essential to avoid asynchronous movements (e.g., upper and lower body moving in different directions). To address this, we leverage mutual information across multi-part motions, designing our model with cross-conditioning across body parts. Specifically, we employ GPT model to estimate the joint distributions of 1:T as follows, ϕ(C 1:B 1:T Fg) = (cid:89) (cid:89) (cid:89) ϕ(cid:0)cj k,tc1:B 1:K,<t, c<j 1:K,t, cj <k,t, t= j=1 k=1 t, t, Fg (cid:1) (2) j= j=1 where sg is stop gradient operation, and the second term is commitment loss with trade-off β. We utilize exponential moving average (EMA) [40] to update codebook vectors and remove the embedding loss (cid:80)B e(p) sg (cid:2)zj q(p)(cid:3) 2 for more stable training. j=1 zj Cross-Conditional Autoregressive Motion Modeling. After training the compositional quantized multi-part codebooks, 2D human poses detected in our training videos can be represented as sequences of codebook tokens via encoding and quantization. Specifically, each detected pose is mapped to sequence of corresponding codebook indices, structured as one-hot vectors indicating the nearest codebook entry for each element. We denote this as K,1), . . . , (cj 1:T = ((cj K,T )), where is the number of tokens per body part in each frame. 1,T , . . . , cj 1,1, . . . , cj With this quantized motion representation, we design temporal autoregressive model to predict the multinomial distributions of possible subsequent poses for each body part, conditioned on both the input music embeddings 1:T and 1:T and the preceding motion tokens. Our motion generation transformer is conditioned on the music inputs in two ways. First, we combine the Jukebox and Librosa music embeddings to form starting tokens, which serves as global music context, such as styles and genres, that informs the entire motion sequence generation. Second, inspired by [32], we concatenate the frame-wise projected music embeddings with the corresponding motion tokens as inputs to the transformer model, ensuring precise synchronization between the motion and music features. This We structure the cross-conditioning between body parts in two ways: (1) the current motion token is conditioned on all preceding motion information from all body parts, ensuring inter-part temporal coherence; (2) by ordering body parts as upper and lower body, head, and hands, we build the hierarchical dependencies from primary components (upper/lower body) to finer, high-frequency movements (head and hands). Since each body parts pose is represented with small set of tokens, we empirically observe that causal attention is sufficient to model the next-token distribution within each part. This modeling strategy preserves motion coherence of each body part as whole, producing expressive and plausible dance movements. Our GPT model is optimized through supervised training using cross-entropy loss on the next-token probabilities. Notably, because our pose tokenization includes associated keypoint confidences, the transformer also learns to model temporal variations in confidence, such as those caused by motion blur and occlusions, enabling it to capture more realistic motion distributions observed in videos. 3.3. Diffusion-Based Dance Video Synthesis With the generated motion token sequences from our trained transformer model, we employ diffusion model to synthesize high-resolution, realistic human dance videos, conditioned on the generated motions and given reference image. To achieve this, we leverage pretrained T2I diffusion backbone [31] and incorporate additional temporal modules [12] to improve cross-frame consistency and ensure temporal smoothness. For transferring the reference Figure 3. Qualitative Comparisons. Among all the methods, X-Dancer achieves the most expressive and high-fidelity human dance video synthesis, maintaining the highest consistency with both the reference human characteristics and the background scene. image context, reference network, as trainable copy of the backbone UNet, extracts reference features of identity appearance and background which are cross-queried by the self-attentions within the backbone UNet. Motion control is achieved through an addition module, often configured in ControlNet [49] or light-weight PoseGuider [15], which translates the motion conditions into 2D spatial guidance additive to the UNet features. To incorporate the generated motion tokens into human image animation, one approach is to utilize the trained VQVAE decoder to decode pose tokens into keypoint coordinates, which are then visualized as 2D skeleton maps for motion conditioning. These skeleton maps can provide motion guidance to the final diffusion synthesis through PoseGuider module. While effective to some extent, this approach introduces non-differentiable process due to the skeleton map visualization, which impedes end-toend training and often results in the loss of keypoint confidence information. Additionally, since the temporal module is trained on real video data where pose sequences are typically smooth, it may struggle with jittery or inconsistent motions generated by the transformer model at inference. In place of explicit translation of motion tokens into 2D skeleton maps as pose guider conditions, we introduce trainable motion decoder that implicitly and directly translates the 1D pose tokens into 2D spatial guidance, added towards the intermediate features of the denoising UNet. Starting from learnable 2D feature map, the decoder injects the 1D pose token sequences including the keypoint confidences with AdaIN layers [16], progressively upsampling this feature map into multiple scales aligned with the resolutions of the denoising UNet features. The motion decoder is trained alongside the temporal module within 16-frame sequence window, effectively decoding token sequences into continuous pose guidance that integrates temporal context from adjacent frames. Moreover, by incorporating reference image context during training, we observe empirically that the synthesized pose guidance retains minimal identity and body shape information compared to skeleton-based pose guiders, enabling generated pose tokens to adapt seamlessly to characters with varied body shapes and appearances. 4. Experiments 4.1. Implementation Details Dataset. Our model is trained on curated visual-audio dataset comprising 107,546 monocular recordings of human dance performances in both indoor and outdoor settings, with an average clip duration of 30 seconds. Each video is cropped to resolution of 896 512 and resampled to 30 fps, encompassing broad spectrum of halfto fullbody dances across diverse music genres and showcasing individuals with varied body shapes and appearances. Training. We train our full pipeline in three stages. First, we train multi-part pose VQ-VAE to encode and quantize 60 joint coordinates, with keypoint confidence scores, into 5-part pose tokens. The pose of each body part uses 6 tokens, each with unique 512-entry codebook of 6dimensional embeddings. This VQ-VAE is trained for 40k steps with batch size of 2048 at learning rate of 2104. Next, we train an autoregressive model for pose token prediction with 300k steps, initialized with pretrained GPT2 model weights, using batch size of 24 and learning rate of 1 104. The model is trained with 64-frame pose sequences with context window of 2224 tokens in total, enabling extended dance motion generation via sliding segments during inference. In the final diffusion stage, we fine-tune the denoising UNet and ReferenceNet with two randomly selected frames from video, followed by Table 1. Quantitative comparison on motion generation. Table 2. Quantitative comparison on visual synthesis. Metrics AIST++ Dataset / In-House Dataset FVD DIV BAS Metrics In-House Dataset FVD FID-VID ID-SIM Ground Truth Hallo [42] Bailando [32] EDGE [37] 509.58/129.75 548.81/249.12 621.22/534.02 639.46/303.36 34.10/29.67 28.66/28.98 22.34/24.05 24.87/27.29 0.24/0.22 0.16/0.20 0.19/0.19 0.26/0.24 609.08 Hallo [42] Bailando [32] + PG 583.26 613.81 EDGE [37] + PG 735.05 Our motion + PG 76.99 100.02 93.73 72.71 X-Dancer 531.52/238.22 25.61/28.08 0.23/0.21 X-Dancer 507.06 61.94 0.4870 0.3392 0.3034 0.4894 0.5317 Table 3. Ablation on our transformer model designs. FVD DIV BAS 265.73 w/o global music context w/o global motion context 247.54 sub-dataset + GPT-medium 402.63 332.93 sub-dataset + GPT-small 27.04 26.42 24.40 24.58 0.2142 0.2154 0.2112 0.2046 X-Dancer 238.22 28. 0.2182 co-training the motion guidance decoder and the temporal module with diffusion losses on consecutive 16 frames. We train the diffusion stage with 90k steps at learning rate of 1 105 and batch size of 16. All stages are trained with 8 A100 GPUs using the AdamW optimizer [44]. Inference. For inference, we initiate our autoregressive dance motion generation from pose tokens encoded from the reference image pose, maintaining skeleton consistency with the specified body shape. Extended dance sequences are generated in 64-frame sliding segments with 12-frame overlap, while 8 uniformly sampled frames from previous generated motions serve as global motion context. Using the full generated pose token sequences, we synthesize all video frames simultaneously, applying prompt traveling [36] to improve temporal smoothness. 4.2. Evaluations and Comparisons To the best of our knowledge, no existing approach addresses music-driven 2D video generation from single human image. For baseline comparisons, we adapted and combined established models to create two baselines. First, we adapted the audio-driven, diffusion-based portrait animation model Hallo [42], retraining it for our task by substituting its audio features with our music embeddings to animate human images via cross-attention modules. For the second baseline, we utilize 3D music-to-dance generation models like Bailando [32] and EDGE [37] for motion synthesis, projecting their outputs into 2D skeleton maps, which are then fed into diffusion model with pose guider for controlled image animation. We note that during our submission, the AIST dataset [38] was unavailable due to maintenance, and we were unable to train our model on the same dataset as Bailando [32] and EDGE [37]. We conduct separate evaluations of all models on the test set of AIST++ [21] (40 videos) as well as on test split of our curated music-video dataset (230 videos). Quantitative Evaluation. We numerically compare our method with baselines in terms of quality of both motion generation and video synthesis. Specifically, we calculate the Frechet Video Distance (FVD) [10, 39] between generated dance motions and ground-truth training sequences for assessment of motion fidelity. For motion diversity, we compute the distributional spread of generated pose features (DIV) [21, 32], for which the generated motion should aim to match the scores of the ground truth distribution rather than maximize their absolute values. To numerically evaluate the alignment between the music and generated dancing poses, we follow [21, 32] to measure the Beat Align Score (BAS) by the average temporal distance between each music beat and its closest dancing beat. These evaluations are conducted in 2D pose space, where we project the 3D skeleton motion of Bailando and EDGE into 2D and detect poses over synthesized videos from the retrained Hallo model. As shown in Tab. 1, our method surpasses all baselines in terms of motion fidelity (FVD), while achieving the second-best music beat alignment (BAS) and diversity (DIV). Notably, EDGE is trained on sequences from professional dancers, whereas our dataset comprises videos of everyday individuals, reflected in the beat alignment score of the ground-truth videos. Despite this, our method significantly outperforms Bailando trained on AIST++ and Hallo trained on our curated dataset for beat alignment. Hallo entangles motion generation and video synthesis, and achieves higher DIV score than X-Dancer primarily due to its extremely noisy video outputs which results in jittering and chaotic skeleton motions following pose projection. For evaluation of video synthesis fidelity, we measure the FVD and FID-VID [3] score between the ground-truth and generated dance videos. Additionally, we assess identity preservation using the ArcFace score [8], which measures the cosine similarity of face identity features (ID-SIM). All metrics are evaluated on our test video dataset. As an extra baseline, we replace the motion generator in EDGE and Bailando with our trained transformer model and generate the animation using pose guider. As shown in Tab. 2, ror per joint (0.5px vs. 0.83 px), with pronounced improvements in the hands (0.4px vs. 0.64px) and head (0.42px vs. 0.52px). This improvement translates into superior motion diversity, expressiveness, and control precision in the diffusion model, as shown through quantitative metrics and visual examples in our supplementary material. Next, we assess the impact of global music and motion context on motion generation. As presented in Tab. 3, both contexts contribute to producing more consistent, plausible, and musicsynchronized motions. We further analyze the benefits of 2D motion modeling and transformer-based autoregressive generation by scaling both model parameters and dataset size. Across all metrics, we observe significant performance gains (Tab. 3) as the number of training parameters increases from 117M (GPT-small) to 345M (GPT-medium) and data scale from 10k to 100k videos, underscoring the scalability potential of monocular dance video data with our architecture, sheding light on further performance improvements as they scale. In Tab. 2, we compare our tokenbased motion guidance with AdaIN against pose guider with explicitly decoded skeleton map. Our method demonstrates reduced motion jittering and enhanced temporal consistency due to the temporal motion context, and showcases superior identity and body shape preservation, as also visually shown in our supplementary video. 5. Conclusion We present X-Dancer, novel framework that unites an autoregressive transformer with diffusion model to generate high-quality, music-driven human dance videos from single reference image. Unlike prior works, X-Dancer models and generates dance movements in 2D space, harnessing widely accessible 2D poses from monocular dance videos to capture diverse, expressive whole-body motions. Our method achieves state-of-the-art results in video quality, motion diversity and expressiveness, providing scalable and adaptable solution for creating vivid, music-aligned dance videos across various human forms and styles. Limitations and Future Work. Our model is trained solely on curated real-human daily dance videos from internet, which can be noisy and the dance movements may not exhibit the precision found in professional dancer videos. Consequently, out-of-domain human images may lead to rendering artifacts, and the generated dance motions may occasionally lack alignment with music. While we designed our pipeline to be end-to-end trainable and scalable, we currently implement it in stages due to memory limitations. Future work will explore large-scale, multi-machine training to further enhance performance and efficiency. Ethics Statement. Our work aims to improve human image animation from technical perspective and is not intended for malicious use like fake videos. Therefore, synthesized videos should clearly indicate their artificial nature. Figure 4. Human image animation after finetuning motion transformer with 30 dance videos of Subject Three. our method achieves the highest visual quality and identity preservation, which we attribute to our disentangled design of motion generation and video synthesis (compared to Hallo) and the use of an implicit motion token decoder rather than an explicit pose guider. Qualitative Comparisons. We present visual comparisons between our method and the baselines in Fig. 3. For more dynamic comparisons and user study on perceptual quality, please refer to our supplementary material. The modified Hallo [42] represents an end-to-end diffusion pipeline that directly synthesizes the final video without intermediate motion generation. However, it exhibits noticeable artifacts, particularly in large articulated body motions, and often fails to preserve the human bodys articulation topology. Bilando [32] and EDGE [37] generate body motion in 3D space without considering the scene context or the human shape in the reference image. Despite post-processing for skeleton alignment in the pose guider input, these methods still struggle with significant identity and shape distortions, often producing unnatural interactions with the background scene. Furthermore, they do not model head and hand motions, leading to more rigid and less expressive dance movements compared to X-Dancer. Finetuning for Characterized Choreography. While our method operates as zero-shot pipeline, generalizing seamlessly to new reference images and music inputs, it can also be fine-tuned for characterized choreography using only few sample dance videos. This adaptability is challenging for 3D motion generation models like EDGE [37] and Bailando [32], which require extensive effort in creating 3D dance movements. As shown in Fig. 4 and our supplementary video, our method successfully captures and mimics the specific choreography after fine-tuning with only 30 dance videos from diverse performers, showcasing its efficiency and versatility in adapting to specific dance styles. 4.3. Ablation Study We conduct ablation studies by systematically removing individual design components from our full training pipeline. First, we assess the effectiveness of the multi-part VQVAE in capturing subtle and expressive human movements. Compared to single-part whole-body VQ-VAE, our approach achieves significantly lower L1 reconstruction er-"
        },
        {
            "title": "References",
            "content": "[1] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. ACM TOG, 2023. 3 [2] Tenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, and Libin Liu. Rhythmic gesticulator: Rhythm-aware co-speech gesture synthesis with hierarchical neural embeddings. ACM TOG, 2022. 3 [3] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Conditional gan with discriminative filter generation for text-to-video synthesis. In IJCAI, 2019. 7 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465, 2023. 2 [6] Di Chang, Yichun Shi, Quankai Gao, Hongyi Xu, Jessica Fu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. In Forty-first International Conference on Machine Learning, 2023. 2, 3 [7] Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang, Xiaobo Xia, and Tongliang Liu. Humanmac: Masked motion completion for human motion prediction. In ICCV, 2023. 3 [8] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, pages 46904699, 2019. 7 [9] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: generative model for music. arXiv preprint arXiv:2005.00341, 2020. 2, [10] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, JunYan Zhu, and Jia-Bin Huang. On the content bias in frechet video distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 7 [11] Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and Jitendra Malik. Learning individual styles of conversational gesture. In CVPR, 2019. 3 [12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 5 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020. 3 [14] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [15] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 2, 3, 6 [16] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 15011510, 2017. 2, [17] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. NeurIPS, 2023. 3 [18] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audio-driven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634, 2024. 3 [19] Yanghua Jin, Jiakai Zhang, Minjun Li, Yingtao Tian, Huachun Zhu, and Zhihao Fang. Towards the automatic anime characters creation with generative adversarial networks. arXiv preprint arXiv:1708.05509, 2017. 2, 4 [20] Nhat Le, Thang Pham, Tuong Do, Erman Tjiputra, Quang Tran, and Anh Nguyen. Music-driven group choreography. In CVPR, 2023. 3 [21] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In ICCV, 2021. 2, 3, 7 [22] Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Yansong Tang, and Xiu Li. Finedance: fine-grained choreography dataset for 3d full body dance generation. In ICCV, 2023. 3 [23] Ronghui Li, Yuqin Dai, Yachao Zhang, Jun Li, Jian Yang, Jie Guo, and Xiu Li. Exploring multi-modal control in musicdriven dance generation. In ICASSP, 2024. [24] Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, and Xiu Li. Lodge: coarse to fine diffusion network for long dance generation guided by the characteristic dance primitives. In CVPR, 2024. 3 [25] Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, and Bolei Zhou. Learning hierarchical cross-modal association for cospeech gesture generation. In CVPR, 2022. 3 [26] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. 2 [27] Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, and Gregory Rogez. Posegpt: Quantization-based 3d human motion generation and forecasting. In ECCV, 2022. 3 [28] Zhenye Luo, Min Ren, Xuecai Hu, Yongzhen Huang, and Li Yao. Popdg: Popular 3d dance generation with popdanceset. In CVPR, pages 2698426993, 2024. 3 [29] Mathis Petrovich, Michael Black, and Gul Varol. Temos: Generating diverse human motions from textual descriptions. In ECCV, 2022. [30] Shenhan Qian, Zhi Tu, Yihao Zhi, Wen Liu, and Shenghua Gao. Speech drives templates: Co-speech gesture synthesis with learned templates. In ICCV, 2021. 3 Generating holistic 3d human motion from speech. In CVPR, pages 469480, 2023. 4 [46] Canyu Zhang, Youbao Tang, Ning Zhang, Ruei-Sung Lin, Mei Han, Jing Xiao, and Song Wang. Bidirectional autoregessive diffusion model for dance generation. In CVPR, pages 687696, 2024. 3 [47] Chenxu Zhang, Chao Wang, Yifan Zhao, Shuo Cheng, Linjie Luo, and Xiaohu Guo. Dr2: Disentangled recurrent representation learning for data-efficient speech video synthesis. In WACV, 2024. 3 [48] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descripIn Proceedings of tions with discrete representations. the IEEE/CVF conference on computer vision and pattern recognition, pages 1473014740, 2023. 3 [49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 3, [50] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE TPAMI, 2024. 3 [51] Yihao Zhi, Xiaodong Cun, Xuelin Chen, Xi Shen, Wen Guo, Shaoli Huang, and Shenghua Gao. Livelyspeaker: Towards In ICCV, semantic-aware co-speech gesture generation. 2023. 3 [52] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. arXiv preprint arXiv:2403.14781, 2024. 2, 3 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3, 5 [32] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d dance generation by actor-critic gpt with choreographic memory. In CVPR, 2022. 2, 3, 4, 5, 7, 8 [33] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [34] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In ICLR, 2023. [35] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. arXiv preprint arXiv:2402.17485, 2024. 2, 3 [36] Jonathan Tseng, Rodrigo Castellon, and C. Karen Liu. Edge: Editable dance generation from music, 2022. 7 [37] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In CVPR, pages 448 458, 2023. 2, 3, 4, 7, 8 [38] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance informaIn Proceedings of the 20th International tion processing. Society for Music Information Retrieval Conference, ISMIR 2019, Delft, Netherlands, 2019. 2, 7 [39] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [40] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Neurips, 30, 2017. 5 [41] Yuan Wang, Zhao Wang, Junhao Gong, Di Huang, Tong He, Wanli Ouyang, Jile Jiao, Xuetao Feng, Qi Dou, Shixiang Tang, and Dan Xu. Holistic-motion2d: Scalable whole-body human motion generation in 2d space. arXiv preprint arXiv:, 2024. 4 [42] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. 2, 3, 7, 8 [43] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human imIn Proceedings of age animation using diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. 2, 3 [44] Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney. Adahessian: An adapIn protive second order optimizer for machine learning. ceedings of the AAAI conference on artificial intelligence, pages 1066510673, 2021. 7 [45] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael Black."
        }
    ],
    "affiliations": [
        "ByteDance",
        "UC San Diego",
        "University of Southern California"
    ]
}