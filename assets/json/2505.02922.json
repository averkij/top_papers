{
    "paper_title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference",
    "authors": [
        "Yaoqi Chen",
        "Jinkai Zhang",
        "Baotong Lu",
        "Qianxi Zhang",
        "Chengruidong Zhang",
        "Jingjia Luo",
        "Di Liu",
        "Huiqiang Jiang",
        "Qi Chen",
        "Jing Liu",
        "Bailu Ding",
        "Xiao Yan",
        "Jiawei Jiang",
        "Chen Chen",
        "Mingxing Zhang",
        "Yuqing Yang",
        "Fan Yang",
        "Mao Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 2 2 9 2 0 . 5 0 5 2 : r RetroInfer: Vector-Storage Approach for Scalable Long-Context LLM Inference Yaoqi Chen*, Jinkai Zhang*, Baotong Lu(cid:12), Qianxi Zhang, Chengruidong Zhang, Jingjia Luo*, Di Liu, Huiqiang Jiang, Qi Chen, Jing Liu, Bailu Ding, Xiao Yan, Jiawei Jiang, Chen Chen, Mingxing Zhang, Yuqing Yang, Fan Yang, Mao Yang Microsoft Research, University of Science and Technology of China, Wuhan University, Tsinghua University, Shanghai Jiao Tong University Abstract The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, novel system that reconceptualizes the keyvalue (KV) cache as vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an AttentionaWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising accuracy. Experiments on long-context benchmarks show up to 4.5 speedup over full attention within GPU memory limits and up to 10.5 over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy."
        },
        {
            "title": "1 Introduction\nTransformer-based LLMs have experienced rapid adoption\nand enabled a range of new applications. Recently, their con-\ntext windows have expanded dramatically to support use\ncases like multi-turn conversations [16], code repository un-\nderstanding [8], and large-scale data analysis [33]. Many pop-\nular LLMs now support context windows of 128K tokens [6,\n41, 46], and leading models such as Gemini 1.5 Pro [19] and\nLlama 4 [43] push this further to 10 million tokens.",
            "content": "However, scaling inference throughput with increasing context length presents significant challenges due to the growing demands on GPU resources. During inference, the memory consumption of LLMs key-value (KV) cache [47] grows linearly with the sequence length [31]. For instance, serving single 1M-token request with Llama3-8B-1048K in BF16 requires up to 125GB of memory. This far exceeds the Work performed during the internship at Microsoft. The paper is the result of an open-source project. (cid:12)Corresponding author: Baotong Lu (baotonglu@microsoft.com). 1 typical capacity of single GPU [44, 45], making it infeasible to serve large contexts or multiple requests efficiently. promising direction is to exploit inherent sparsity in attention [61], the major performance bottleneck of Transformer. Recent studies [13, 37, 60] have shown that small subset of tokens dominate the attention output for given query. Loading only these important tokens for GPU processing opens the opportunity to offload KV cache to highcapacity, slower CPU memory, thereby significantly reducing memory pressure and data transfer overhead. Yet identifying important tokens is challenging. These tokens are irregularly distributed across the context window and vary across model layers, attention heads [4], decoding queries and input tasks. The sparsity ratio (i.e., the proportion of negligible tokens) also fluctuates, making static or fixed policy insufficient. To realize the benefits of sparsity, there is pressing need for mechanism to dynamically identify the important tokens for each attention query. Moreover, even if the important tokens can be identified, the division of labor between GPU and CPU, as well as the associated memory transfers, must be managed cautiously to avoid GPU stalls or PCIe bottlenecks. In this paper, we propose to build KV Cache as Vector Storage System, new type of system to comprehensively address the challenges of long-context inference acceleration based on sparsity. Such systems follow two principles: (1) be attention-aware, and (2) separate the concern of attention accuracy from the system efficiency. We find that vector indexescommonly used in approximate nearest neighbor search (ANNS)have the potential to retrieve the important yet unpredictable tokens [27, 51, 55]. ANNS seeks the most similar vectors to given query vector using inner product, which closely aligns with how attention computes relevance (2). As such, we introduce an Attention-aWare VEctor index called wave index that retrieves important tokens with high accuracy and low latency. Drawing inspiration from the classic vector indexes [56], wave index introduces three novel techniques that are attention-aware. First, tripartite attention approximation, which logically partitions tokens into steady, retrieval, and estimation zones. It is powered by clustering-based vector index tailored for retrieving important tokens in attention. As we move from steady to estimation, attention becomes more approximate but cheaper to compute. Second, an accuracy-bound attention estimation mechanism, which approximates the contribution of less important but non-negligible tokens with guaranteed accuracy bounds. Lastly, segmented clustering that redesigns traditional vector index clustering for parallelism and low overhead, reducing the index construction and update cost. We design wave buffer to ensure the efficiency of the wave index and to maximize overall inference throughput. The wave buffer acts as the metadata and control plane, managing the physical locations of KV vectors and coordinating their movement between GPU and CPU memory, akin to paging system in an operating system. It also carefully orchestrates the sophisticated inference computation on index traversal, construction, and attention calculation across GPU and CPU, which differ significantly in architecture. By overlapping these tasks across different hardware components, the system improves throughput and avoids GPU stalls. As the wave index enhances the locality of token access by only loading important tokens, the wave buffer incorporates GPU-side cache of the KV cache. Data access is performed synchronously on the GPU, while cache updates, including the replacement policy, are handled asynchronously to reduce overhead and sustain high performance. We build RetroInfer, an retrieval-optimized inference system that integrates wave index and wave buffer, delivering both high throughput (tokens/s) and high accuracy for long-context LLM inference. We evaluate RetroInfer on three popular models across range of tasks included in advanced long-context benchmarks such as RULER [26], with varying context lengths and batch sizes. Our results demonstrate significant improvements in inference performance while maintaining accuracy. For the same retrieval budget (i.e., the number of KVs involved in attention), RetroInfer outperforms other sparse-attention baselines by 3.3425.37% in task accuracy and is the only solution to achieve accuracy comparable to full attention. RetroInfer also significantly improves inference throughput by efficiently extending the supported batch sizes and context lengths. Specifically, RetroInfer achieves up to 4.5 speedup over full attention when the context length fits in GPU memory, and up to 10.5 speedup over other sparse-attention inference systems when the KV cache is extended to CPU memory. We open source RetroInfer at https://github.com/microsoft/ RetrievalAttention. In the rest of paper, we first cover the background (2), and discuss the challenges and opportunities of employing vector indexes for KV Cache (3). Next, we dive into the design of RetroInfer (4). We then evaluate RetroInfer (5), discuss the related work (6), and finally conclude in 7. Figure 1. Transformer Architecture (with KV Cache). In attention computation of (c), deeper color indicates higher attention weight, corresponding to more important tokens. love systems are the input tokens (i.e., three tokens). 2 Background and Motivation 2.1 Transformer-based LLMs and Attention Transformers are the dominant architecture for large language models (LLMs), relying on multi-layer attention mechanism. As shown in Figure 1, given an input of 𝑛 tokens, each represented as high-dimensional vector, it is linearly transformed into three matrices: queries (𝑄), keys (𝐾), and values (𝑉 ). Within each layers attention module, the model relates each token to others using multiple attention heads in parallel. The outputs from all heads are then aggregated and passed to feed-forward neural network (FFN) layer. During inference, the transformer operates in two distinct phases: prefilling and decoding. In the prefilling phase, the model processes the input prompt as whole; in the decoding phase, it generates tokens one at time based on both the prompt and previously generated tokens. To accelerate decoding, modern LLMs use key-value cache (KV cache) [31, 47], which stores the key and value vectors of all previously seen tokens. This avoids redundant computation and significantly reduces decoding latency. However, the KV cache introduces substantial memory and bandwidth overhead, since its size and memory access grow linearly with both context length and batch size. As result, KV cache makes long-context inference increasingly constrained by memory capacity and bandwidth limitations. The attention output in decoding is computed as weighted sum over value vectors, where the weights are given by the softmax of the inner product between the current query 𝑞 and all cached keys: = aV = Softmax( qK𝑇 𝑑 )V = ( 𝑖=1..𝑛 𝑇 qK 𝑖 𝑑 𝑒 qK 𝑇 𝑗 𝑑 (cid:205)𝑗=1..𝑛 𝑒 V𝑖 ) (1)"
        },
        {
            "title": "KV cache has become so essential that it has influenced",
            "content": "2 model architecture changes in modern LLMs. Notably, Grouped Query Attention (GQA) [4] has become standard in recent models [1, 2, 20, 43, 50]. In GQA, query heads are divided into groups, with each group sharing key-value head, which reduces KV cache consumption proportional to the group size. GQA effectively reduces memory overhead while maintaining attention diversity. 2.2 Scalability Limits of Long-Context LLM Inference Long-context inference is increasingly important for applications [8, 16, 33] such as multi-turn conversations, document understanding, data analysis, and codebase navigation. However, achieving scalable performance for long-context inference is challenging. Although the KV cache reduces computation cost, the growing gap between GPU memory capacity and cache size significantly limits batch size and throughput. GPU Memory Capacity Limits. The size of the KV cache grows linearly with both the context length and the batch size, quickly draining GPU memory. For instance, an A100 GPU (80GB memory) can support maximum batch size of 4 at 128K context length for Llama-3-8B. Exceeding this limit results in out-of-memory (OOM) errors. Similarly, the maximum context length the GPU can support is 512K. 1 To support longer contexts, many systems offload the KV cache to the larger and more cost-effective CPU memory [32, 54]. However, this approach can degrade inference efficiency due to the high cost of data transfer over the PCIe interconnect. Memory Bandwidth Bottlenecks. Memory bandwidth is another key limiting factor. With KV cache, each query at decoding time linearly accesses all previous KV vectors. As the context length and batch size increase, the resulting access volume saturates memory bandwidth, limitation widely recognized in recent work [10, 52]. In our experiments on single A100 GPU, scaling the batch size beyond 3 (at 128K context) leads to marginal throughput improvement due to GPU memory bandwidth saturation. 2.3 Sparsity in Attention promising direction for efficient long-context inference is to leverage the sparsity inherent in the attention mechanism. This sparsity arises naturally from the structure of the attention equation (Equation 1) and the exponential nature of the softmax function. Intuitively, value vectors associated with high attention weights (i.e.,elements of in Figure 1) dominate the attention output (𝑜).2 That is, small subset of value vectors suffices to reconstruct the attention output, provided that the most important tokens can be accurately identified from the vast pool of KV vectors. Leveraging sparsity reduces memory access and helps alleviate the bandwidth bottleneck. Moreover, sparsity makes 1Unless specified, the analysis is done in Llama3-8B-1048K. Experiments on more models are available in Section 5. 2In this paper, we refer to qK𝑇 in attention as the attention scores, Softmax( qK𝑇 𝑑 ) as the attention weights. 3 Figure 2. Dynamic Sparsity in Attention. The distribution of top100 attention weights in 128K context varies across decoding steps. Figure 3. Variety of Attention Sparsity. (a) model layers and attention heads on fwe [26]; (b) decoding steps and tasks. it feasible to offload the KV cache to CPU memory to address the capacity limitation, as the inference system can selectively access subset of KV vectors over PCIe. Challenges to Leverage Sparsity. Identifying important tokens is challenging for three reasons. First, as shown in Figure 2, important tokens (i.e., those with high attention weights) are scattered across the context, making their positions unpredictable. Second, tokens that are important at one decoding step (i.e., one query vector) may not remain so in subsequent steps. As shown in Figure 2, the overlap of the top-100 KV vectors across three decoding steps is only 31%, highlighting token importance dynamics. Third, the sparsity exhibits high variability from two sources: the architecture of the model itself and the nature of the decoding query. As shown in Figure 3, attention distributions differ significantly across model layers and attention heads (Figure 3 (a)), potentially reflecting the varying semantic roles of heads and layers [34, 66]. Different queries within the same context or across different tasks exhibit substantially different sparsity ratios on the same attention head (Figure 3 (b)). Sparsity alone is not panacea. gap exists between sparsity ratio (i.e., negligible tokens percentage) and the bandwidth constraints imposed by the PCIe interconnect. GPU memory bandwidth (e.g., HBM) is about 60 higher than PCIe bandwidth on common GPUs (e.g., A100, H100, and H200). To hide PCIe transfer latency and prevent GPU stalls, the sparsity ratio must exceed 98% for the GPUs. Recent studies report lower sparsity ratios of 87.5% [60] and 90% [32], highlighting the need for careful system design that accounts for hardware constraints to achieve high throughput. Existing Long-Context Inference via Sparsity. Recent systems [13, 32, 34, 60, 67, 69] have explored the use of sparsity to facilitate long-context inference. However, they often fall short along two key axes: accuracy and efficiency. Some systems rely on fixed-position heuristics to discard KV vectors [34, 69], which often leads to significant loss due to static assumptions about token importance. Others estimate token importance by partitioning the KV cache into equal-sized chunks and using chunk representative vectors [60, 67]. While GPU-friendly, this coarse-grained approximation lowers the retrieval precision and model quality. Systems that retain all KV vectors in GPU memory [60] are constrained by limited GPU capacity, which restricts maximum context length and batch size. To alleviate this, InfiniGen [32] and MagicPIG [13] offload the KV cache to CPU memory and selectively fetch vectors, either via speculative prediction or locality-sensitive hashing (LSH). However, inaccurate selection can degrade quality, and throughput is bottlenecked by PCIe bandwidth or lower CPU compute power. 3 When ANNS Meets Sparse Attention The central research question we address is how to achieve both high accuracy and efficiency in long-context inference. We believe that approximate nearest neighbor search (ANNS) techniques offer promising path. Opportunities. One key observation is that Maximum Inner Product Search (MIPS), variant of nearest neighbor search (NNS), can be seamlessly applied to identify critical tokens in attention mechanisms [37]. This is due to the fact that high attention scores signify that their key vectors have substantial inner product with the query vector. ANNS aims to identify desired number (i.e., top-𝑘) of vectors most similar to given query in high-dimensional space, using similarity metric such as inner product. By carefully organizing vectors using vector index [40, 56], they greatly reduce the number of vectors to access during search while maintaining near-optimal results. well-designed vector index can retrieve high-quality key vectors efficiently, enabling fast and accurate attention computation with established ANNS techniques. Challenges. Beyond the accuracy challenges described earlier (2.3), efficiency challenges arise because incorporating ANNS introduces index traversal and selective data access into an inference system that is otherwise optimized for dense GPU execution. These demands require careful codesign of compute and memory across the hardware stack. First, sparse attention with ANNS introduces two types of data (i.e., index structures and KV vectors) and three types of computation (i.e., index traversal, index construction, and attention calculation). Mapping this design onto the hardware hierarchy introduces fundamental challenges due to the differences in GPU and CPU characteristics. GPUs offer highspeed computation and fast memory access, but are limited in memory capacity. CPUs, in contrast, provide abundant memory but lower computational throughput. This asymmetry raises key questions about where to place data and computation, and how to leverage parallelism and overlap to fully utilize both processors. reasonable starting point is to perform the final attention computation on the GPU while storing the high-volume KV Figure 4. Architecture of RetroInfer. Circles with numbers represent the steps of attention computation, while steps with the same number occur in parallel. Colored circles indicate vectors. Centroids (bold-edged circles) are stored in the meta index. vectors in CPU memory to exploit its capacity. Under this setup, efficient data movement becomes critical. Without careful orchestration, the GPU can be easily starved for data, especially under tight latency constraints. Second, the system should reduce index traversal and construction costs while preserving retrieval quality. Index traversal on GPUs is costly due to irregular memory accesses and fine-grained operations such as top-𝑘 selection. Index construction also introduces non-negligible overhead. These costs can be prohibitive under strict latency constraints. Finally, the control logic for managing index structures is often better suited for CPU execution. This necessitates memory management layer that runs on the CPU and coordinates efficient paging and data transfers to the GPU. These challenges are further complicated by the distinct programming models of CPUs and GPUs, requiring careful coordination to ensure high performance and low overhead. Our Approach. We advocate building the KV cache as vector storage system. Such system leverages vector-based index to identify important tokens with high quality and incorporates storage design that comprehensively addresses the accuracy and efficiency challenges discussed above."
        },
        {
            "title": "4 RetroInfer",
            "content": "We present RetroInfer, an inference system that builds the KV cache as vector storage system. We begin with highlevel overview, then describe the design of the wave index and wave buffer, and also discuss implementation. 4.1 Overview The design of RetroInfer follows two principles: (1) be attention-aware, and (2) separate the concern of attention accuracy from the system efficiency. We introduce an Attention-aWare VEctor index called wave index that retrieves the important KV vectors accurately and efficiently as needed. RetroInfer introduces wave buffer which manages memory across the GPU and 4 CPU to facilitate wave index, also in an attention-aware manner. Figure 4 shows the architecture of RetroInfer. To make judicious use of the GPU memory, the wave index employs cluster-based vector index design. Specifically, wave index partitions the KV vectors into clusters based on their similarity and stores the centroids of the clusters in meta index, as the representative of each cluster in GPU memory. To ensure the accuracy of attention outcome, each meta index entry contains additional information (i.e., (cid:205) 𝑣), which allows us to perform the precise attention computation for retrieved clusters while computing approximate attention for remaining clusters to cover varying sparsity ratios. All of the KV vectors are organized by contiguous KV blocks residing in CPU memory. We dive into the internals of wave index in Section 4.2. The wave buffer serves two purposes. First, it contains several buffers in GPU memory to accelerate inference throughput. These include block cache for KV vectors and an execution buffer, dedicated memory region that sequentially arranges needed KV vectors for attention computation. The content of the execution buffer is copied from the steady zone, block cache, and directly from the CPU memory KV blocks in case of cache miss (details in 4.3). Second, CPUresident buffer manager that manages the block cache and data movement between GPU and CPU memory. During decoding, RetroInfer computes the attention for each head in parallel, following the steps in Figure 4: 1 : The centroids are sorted according to the similarity to the query vector, determining subset of more critical clusters to retrieve for precise attention computation, and the clusters to perform estimation. 2 : The GPU performs attention estimation (2-G), and request is sent to the buffer manager to retrieve the needed clusters (2-C). 3 : The buffer manager ensures that the KV blocks are ready in the execution buffer through parallel data copying. 4 : The GPU computes the precise attention using the KV vectors in the execution buffer, while the attention estimation (2-G) result is merged. 4.2 Wave Index Wave index employs cluster-based vector index, specifically designing for the attention mechanism to ensure accurate and efficient attention computation. Index Organization. Wave index partitions the KV vectors in the context into clusters using spherical 𝑘-means [25], based on the similarity between key vectors. centroid is computed as the representative of each clusterthe average of key vectors of one cluster. We observe that the classic cluster-based vector index approach is synergistic with the attention mechanism, because critical key vectors for given query tend to be grouped within few clusters, while the critical clusters tend to be irregularly distributed across the context. As illustrated in Figure 5, although several tokens (i.e., 2nd, 4th, and 8th) are irregularly distributed, wave index groups them into the same cluster (i.e., yellow). Figure 5. Attention-aware Design of Wave Index. Wave index divides the context into segments (dotted boxes). Each vertical bar represents token at one position. In the first row, tokens with the same color belong to the same cluster, and similar colors (e.g., yellow and orange) indicate token similarity, which is based on key vector similarity. In the second row, the darker the bar, the more important the token to the query. For example, query of purple could be close to red vector (1st segment) and blue vector (2nd segment). When serving query vector, wave index sorts the centroids based on the inner product between the query vector and the centroids, as the inner product determines the attention score. Classic vector indexing and ANNS algorithms have addressed the complexity of high-dimensional vector spaces to effectively perform clustering and compute centroids [14, 25]. However, the goal of classic ANNS is to retrieve the top-𝑘 vectors by selecting candidates from clusters and performing pairwise comparisons to the vectors for further ranking, which is costly on GPU. In contrast, wave index retrieves all vectors from the critical clusters and incorporates them directly into the attention computation, avoiding the selection cost. Since the clusters are highly coherent, this design also improves the accuracy of attention computation by potentially including more tokens that are also important. To accelerate index construction, we introduce technique called segmented clustering, where the input sequence is divided into segments, and clustering is performed within each segment, instead of clustering over the entire input sequence as in classic vector index. For example, in Figure 5, vectors in the first segment (i.e., red, yellow, and orange ones) are clustered independently of vectors in the second segment. We describe its rationales and details later. For physical placement, the meta index, which requires only small memory footprint, resides in GPU memory. The meta index contains table of centroids, where each entry also includes the sum of the value vectors and the cluster size, as shown in Figure 4. The large volume of key and value vectors resides in CPU memory, organized into contiguous KV blocks. This block-based organization facilitates efficient data movement between GPU and CPU memory. Tripartite Attention Approximation. We introduce tripartite attention approximation to ensure the accuracy of attention computation with reduced computation cost by leveraging both the properties of the attention mechanism and the advantage of vector retrieval. The final KV vectors used for attention computation consist of three parts: steady zone, retrieval zone, and estimation zone, and we denote the attention output as 𝑜 0, 𝑜 1, and 𝑜 2 respectively. Steady, Retrieval, and Estimation Zones. Recent studies have revealed that range of tokens located at the beginning and end of the context are consistently important [69]. These tokens are categorized as steady zone in wave index, as shown in Figure 5. Therefore, tokens in the steady zone are directly included based on their positions, rather than being organized into the vector index. The KV vectors retrieved by the centroids and clusters are categorized as retrieval zone; these tokens are directly used for attention computation. For the GQA model, the centroid ranking is jointly determined by averaging the normalized inner products from different query heads in group. This approach enables the ranking of clusters based on their average importance and ensures that the retrieved clusters remain consistent within group, thereby maintaining low retrieval budget. Due to the high variety of sparsity ratios across model layers, attention heads, queries, and task types, determining the optimal value of how many clusters to include in the retrieval zone is challenging, if not entirely unattainable. Therefore, we introduce an accuracy-bounded attention estimation mechanism to compensate for the potential accuracy loss of non-retrieved clusters. Tokens in the non-retrieved clusters are categorized as the estimation zone. The attention estimation effectively handles sparsity variation without increasing data transfer over PCIe. Combining the three zones, we have 𝑝 KV vectors in the steady zone, and 𝑚 clusters in the retrieval and estimation zones. Among the 𝑚 clusters, we designate 𝑟 clusters in the retrieval zone, and the rest for estimation. The 𝑚 𝑟 clusters in the estimation zone significantly improve the robustness of the chosen number of clusters 𝑟 for retrieval in the face of varying sparsity ratios, thus improving attention accuracy. Intuitively, our tripartite attention approximation aligns with the inherent sparsity of attention. From 𝑜 0 to 𝑜 2, the attention weight computation becomes increasingly approximate and lightweight as the size of each zone increases, while determining the token positions for importance becomes more challenging, as illustrated in Figure 6. The clusters exhibit high coherence and strong alignment with attention scores, along with highly representative centroids (Figure 7), enabling robust distinction between the retrieval and the estimation zone. This design effectively handles variability in sparsity. Precise attention computation is especially critical for top-ranked centroids, which tend to have greater influence on the output. This zone division is reliable, allowing us to confidently retrieve important centroids while conservatively bounding those in the estimation zone to avoid including influential tokens. Accuracy-Bounded Attention Estimation. The key idea for accuracy-bounded attention estimation is to leverage the centroids to estimate the attention weights for all KV vectors Figure 6. Illustration of Computation Cost for Three Zones. In the steady zone, attention is computed precisely, so the computation cost is linear in the number of tokens (red lines slope). In the retrieval zone, we calculate attention for the retrieved vectors (whose number is significantly reduced due to sparsity). The computation cost is linear in the number of retrieved vectors (black line), while the overall cost is amortized over the total number of vectors (red line). In the estimation zone, the attention is estimated based on centroids, which incurs much lower computation cost (red line). within cluster with bounded accuracy, thereby avoiding the costly computation of attention for each individual KV vector, striking balance between accuracy and efficiency. Consider an attention head with 𝑝 KV vectors in the steady zone and 𝑚 centroids, denoted as 𝐶 R𝑚𝑑 , with corresponding cluster sizes 𝑠 R𝑚1. The attention weight for the KV vectors in the 𝑖-th cluster is estimated by: 𝑇 qC 𝑖 𝑑 𝑒 𝑇 𝑗 qK 𝑎𝑖 = , 𝑖 = 1..𝑚 (2) 𝑇 qC 𝑗 𝑑 ) (cid:205)𝑗=1..𝑝 𝑒 𝑑 + (cid:205)𝑗=1..𝑚 (𝑠 𝑗 𝑒 The accuracy of this estimation is assured because the centroid estimation acts as lower bound for the sum of exponential inner product values within cluster. For the 𝑖-th cluster with 𝑠 key vectors, the centroid is computed as the average of all key vectors in the cluster. By applying Jensens inequality [29], we derive the following bound, which becomes tighter when the key vectors within the cluster exhibit higher similarity, aligning with the objectives of clustering: C𝑖 = (cid:205)𝑗=1..𝑠𝑖 K𝑗 𝑠𝑖 , 𝑇 qC 𝑖 𝑑 𝑒 (cid:205)𝑗=1..𝑠𝑖 𝑠𝑖 qK 𝑇 𝑗 𝑑 𝑒 (3) To further reduce the overhead of accessing corresponding value vectors, we sum the value vectors of each cluster during the index construction and store cluster size and summed value vectors (𝑉 𝑆 R𝑚𝑑 ) in the meta index. This approach avoids individual value access during inference and is based on the following equality for the partial attention output 𝑜𝑖 derived from cluster 𝐶𝑖 and the attention output 𝑜2 for all non-retrieved clusters: oi = 𝑠𝑖 𝑗=1 ( 𝑎𝑖 V𝑗 ) = 𝑎𝑖 𝑠𝑖 𝑗=1 V𝑗 = 𝑎𝑖 𝑉 𝑆𝑖, o2 = 𝑚𝑟 𝑖= oi (4) As such, the time complexity of attention estimation for non-retrieved clusters is 𝑂 (𝑚 𝑟 ), which is substantially lower than accessing individual KV vectors, ensuring the efficiency of our approach, particularly for longer contexts. Figure 7 illustrates the accuracy bound of the estimation. Lightweight Index Construction. Index construction occurs during the prefilling phase, where the content of KV 6 Figure 7. Centroid Representativeness and Estimation Accuracy. The x-axis represents the ranking of centroids during index traversal. The ground truth (blue) shows that top-ranked centroids have higher cumulative attention scores, demonstrating the effectiveness of clustering. Attention estimation (green) follows the same trend of ground truth with tight bound. vectors is offloaded to CPU memory, and clustering is performed on the GPU to produce the meta index. The main challenges in index construction are reducing computation overhead to prevent slowing down the prefilling. While cluster-based vector index incurs lower index construction cost among vector index approaches, performing 𝑘-means across the entire input sequence is still expensive. To save construction cost, wave index introduces segmented clustering, performing 𝑘-means clustering within each segment independently, reducing the time complexity of clustering by factor of 𝑠, where 𝑠 is the number of segments. This design is based on the observation that key vectors exhibit coarse-grained spatial locality within the input sequence3. Although tokens in adjacent positions within sequence may not always be similar, key vectors within broader region exhibit strong spatial locality. As result, keys within single segment (around 8,000) tend to be similar to each other but dissimilar to those in other segments, thereby negating the need to include distant keys during clustering. As shown in Figure 5, tokens from the three clusters (i.e., yellow, red, and orange) are relatively similar. Though clustering results may vary across different contexts or attention heads, tokens in the first segment are unlikely to be similar to those in the second segment (e.g., green and blue tokens). Such coarse-grained spatial locality does not contradict the fact that important tokens (i.e., clusters) are usually scattered throughout the context (Figure 5). This is due to the fact that similarity-based clustering is conducted in highdimensional vector space and the query vector in attention behaves as special type of out-of-distribution query vector. [37] In this context, the key vectors similar to the query vector can possibly be distant from each other in the vector space [11]. We leverage existing literature [13] to ensure that the clustering effectively captures the attention importance. 4.3 Wave Buffer With the wave index, RetroInfer significantly reduces the number of KV vectors required per query, lowering PCIe 3We attribute the spatial locality to RoPE [58], which encodes positional information in key vectors. This effect is confirmed by observing significantly less locality when performing 𝑘-means clustering on Pre-RoPE versus Post-RoPE key vectors. 7 Figure 8. Design of Wave Buffer. The black arrows indicate the pointer relationship between the data structures. The red arrows indicates the possible three source of data copy to ensemble the execution buffer. Missed data is admitted into the block cache by copying from the execution buffer (blue arrow). traffic to less than 2% of that in full attention. However, the inference throughput could still suffer due to the limited PCIe bandwidth for long-context inference. Drawing on the classic wisdom of computer systems, the performance and capacity disparity between GPU memory and CPU memory, akin to the von Neumann bottleneck [62], necessitates the buffer caches in the fast, low-capacity GPU memory. Thanks to the high-quality retrieval provided by the wave index, adjacent decoding steps for neighboring query vectors show strong temporal locality, as only important tokens are loaded into GPU memory. Thus, caching is highly beneficial. The organization of the wave index, combined with the hardware characteristics, poses unique challenges in managing the memory used in RetroInfer to achieve better throughput. Specifically, what should be cached, how to manage data movement and the control plane of caching, and finally, how can all these movements be effectively parallelized? We address these challenges with the wave buffer, comprehensive memory management component that aligns well with the attention mechanism. Buffer Organization. Within GPU, wave buffer contains block cache for the KV vectors in the retrieval zone, small buffer for the steady zone, and an attention-layer-share execution buffer. The control plane of the wave buffer resides in CPU memory, with its data structures managed by the buffer manager running on CPU thread pool. Figure 8 shows the organization of wave buffer. KV Block Cache. We observe strong temporal locality of important tokens: neighboring decoding steps (i.e., query vectors) have high overlap of critical tokens due to topic continuity and syntactic proximity. This observation is supported by recent works [65, 70] and an empirical study that examined the cache hit ratio across two models and five tasks (5.4). With GPU cache size equal to 5% of the total KV vectors, the average cache hit ratio across tasks ranges from 0.79 to 0.94, indicating high cacheability of important KV vectors. However, realizing the benefits of this temporal locality in the KV Block Cache is challenging for two reasons. First, cache access and updates have an extremely low-latency requirement because the decoding step of each layer takes only few hundred microseconds. Thus, the software overhead of the complex cache management logic must be minimized to avoid curtailing the benefits of caching. Second, clusters are suitable to serve as the logical unit of the cache to integrate with the cluster-based index, while fixed-sized KV blocks are suitable to serve as the physical unit of the cache to ease data movement and memory management (e.g., fragmentation). Therefore, our cache management must bridge this gap between logical and physical units. Since the cache replacement logic is relatively complex and difficult to parallelize, we offload it to the CPU. This approach differs from classic caching systems, where the management logic is executed in faster hardware. We use clusters as the cache admission and replacement unit and blocks as the unit for space management. Note that cluster could be spread across multiple blocks; to admit missed cluster into the KV block cache, multiple clusters may be evicted until sufficient number of blocks are freed. We design cluster mapping table as layer of indirection. Each entry in the mapping table is cluster descriptor (Figure 8), mapping clusters addresses in GPU memory and CPU memory, similar in purpose to page table in operating systems. The key insight of the wave buffer is to decouple cache access on the critical path from the expensive cache update. This is facilitated by the hybrid GPU+CPU structureafter the lookup, the key-value vectors must be immediately ready for the GPU in the execution buffer, whereas the cache update can be performed asynchronously by the CPU, in parallel with the data copy and attention computation. Synchronous Cache Access. The mapping table associates each cluster ID with its block IDs in either GPU memory (i.e., cached) or CPU memory (i.e., missed). Upon obtaining the IDs of the clusters in the retrieval zone for query, the wave index sends these IDs to the CPU thread running the buffer manager, which looks up the mapping table to determine the clusters locations and issues the memory copy accordingly. Table lookup is accelerated using CPU multi-threading, without synchronization overhead, as this process is read-only. Asynchronous Cache Update. Following the mapping table lookup, the wave buffer immediately submits cache update request to the CPU thread pool for asynchronous execution. As result, cache updates are decoupled from cache access, allowing the wave buffer to issue memory copy operations via GPU threads while concurrently making cache replacement decisions and metadata updates (e.g., LRU lists) on the CPU. The replacement decision is then enacted by issuing request to the GPU to schedule copy from the execution buffer to the block cache, thereby admitting the data into GPU memory and completing the cache update. With the decoupled cache access and update, the latency of CPU involvement is fully overlapped with GPU execution, 8 which accelerates the overall process. Assemble the Execution Buffer. The execution buffer contains the final key-value vectors in the steady zone and the retrieval zone, arranged in contiguous memory. The sequential nature of the execution buffer makes them directly usable by FlashAttention to compute the partial attention output. This output is combined with the estimated attention output in the estimation zone to produce the final attention output. Three types of data copy may be needed to assemble the execution buffer, as shown in Figure 8: 1) from the steady zone (GPU to GPU), 2) from the KV block cache (GPU to GPU), and 3) from KV blocks (CPU to GPU). We have developed highly-optimized copy operator, which enables parallel execution of three types of data copying, each accelerated using numerous GPU threads (details in 4.5)."
        },
        {
            "title": "4.4 Constrain Prefilling Latency",
            "content": "Because prefilling is compute-intensive, special care is required to perform all buffer-related computation (e.g., updating the mapping table) alongside the lightweight index construction. We carefully constrain prefilling latency by fully utilizing GPUCPU parallelism. After obtaining the Q, K, and during prefilling, RetroInfer asynchronously offloads the KV vectors to CPU memory while the GPU performs segmented clustering. The clustering results are then sent to the CPU, allowing the wave buffer to construct its data structures, including the mapping table and metadata for cache replacement. With this parallelism, only segmented clustering remains on the critical path of the prefilling phase, and its latency is negligible (less than 5%, as shown in 5)."
        },
        {
            "title": "4.5 Implementation",
            "content": "We implement custom Triton kernel for segmented clustering in the wave index. The kernel first accepts KV vectors from single attention layer and executes 𝑘-means in parallel across attention heads and segments. It then computes the value sums and stores the results in the meta index. GPU-side memory copies for accessing and updating the KV block cache and execution buffer are complicated, as source and destination addresses are often non-contiguous and may involve both CPUGPU and GPUGPU transfers. We implement CUDA kernels (1,000 LoC) to support execution buffer transfers and block cache updates. The copy kernel skips over fragmented regions within blocks to mitigate fragmentation. Since attention heads may exhibit varying copy patterns due to differing cache hit ratios, number of threads are dynamically changed to maintain performance. Finally, attention in the estimation zone differs from the traditional mechanism, as cluster sizes must be used to calculate attention weights. We modify FlashAttention kernels to support weighted attention. The custom kernel also enables efficient merging of attention outputs from three zones."
        },
        {
            "title": "5 Evaluation",
            "content": "We thoroughly evaluate RetroInfer, comparing it with full attention and state-of-the-art sparse attention methods on different models and tasks. The results demonstrate that: RetroInfer outperforms sparse-attention baselines in model accuracy under the same retrieval budget, and is the only solution that matches full attention accuracy. RetroInfer achieves strong throughput scalability across varying context lengths and batch sizes, significantly outperforming the second-best baseline. The techniques incorporated into the wave buffer, such as GPU caching and asynchronous cache updating, effectively boost throughput by minimizing data transfer overhead and maximizing hardware resource utilization."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "We run experiments on VM server with an NVIDIA A100 GPU (80GB memory) and an AMD EPYC 7V12 64-core CPU (1900GB memory). The VM has 4 NUMA nodes, each with 12 physical cores. The GPU and CPU are linked via PCIe 4.0 (16), with unidirectional bandwidth of 32GB/s. The server runs Ubuntu 22.04 with CUDA 12.4 and PyTorch 2.4.0. Models. We evaluate RetroInfer on three leading opensource LLMs: Llama 3.1-8B [42], Qwen2.5-7B [50], and Llama38B-1048K [21]. Llama 3.1-8B (32 layers, 32 heads) and Qwen2.57B (28 layers, 28 heads) support up to 128K tokens. These models allow evaluation across architectures. All models adopt GQA, with group size 4 for Llama and 7 for Qwen. Llama3-8B-1048K, architecturally identical to Llama3.1-8B but with 1048K context window, demonstrates effectiveness under extreme long-context scenarios. Baselines. To exclude the effect of different request scheduling mechanisms [74], we explicitly control the batch sizes running on all methods to evaluate their throughput. vLLM [31] is not suitable for direct comparison because it includes internal request scheduling. Thus, we implement highlyoptimized full attention baseline that uses FlashInfer [73], which demonstrates comparable or even better throughput than vLLM. We also compare RetroInfer with three stateof-the-art sparse attention methods. (1) Quest [60]: GPUonly dynamic sparse attention method that employs chunkbased retrieval based on representative vectors. (2) Quest (offload): To accommodate larger batch sizes and context lengths, we implement an offloading version of Quest where all KV vectors are offloaded to CPU memory and only store the representative vectors on GPU. It selectively loads important chunks into GPU memory for attention computation. (3) MagicPIG [13]: GPU-CPU inference framework which offloads KV cache to CPU and employs LSH to sample relevant tokens. Hash table accesses and attention computation rely on CPU to reduce PCIe transfer. (4) InfiniGen [32]: another GPU-CPU system that offloads KV cache to CPU and speculatively prefetches important tokens for future layers. We use their open-sourced codes [28, 39, 49] and make necessary modifications to adapt to the model architectures. Parameters. Each sparse attention baseline uses the hyperparameters from its original paper 4 and RetroInfer uses 1 16 centroids of all tokens (avg. cluster size 16), with segments of 8K tokens and 10 iterations for 𝑘-means. For Quest, full attention is applied to the first two layers. For MagicPIG, full attention is applied to layer-0, 16 of Llama3-8B and Llama3.18B, and layer-0, 14 of Qwen2.5-7B. The retrieval budget is set as 1.8%5 across different sparse attention methods, all sparse layers and tasks, aligning with the budget used in [13] for fair comparison. We study the impact of different retrieval budgets in Section 5.2. For RetroInfer, the steady zone contains 4+64 tokens for initial and local windows while the estimation zone is set as 23% of total clusters so that three zones cover 25% of the context. RetroInfers wave buffer employs LRU policy and sets each page size as 2KB. The GPU cache size is set as 5% of all KV vectors. We use 24 logical threads in one NUMA node. MagicPIG is set using 48 logical threads from two NUMA nodes, following the setting in its original repository [39]. Benchmarks. We utilize three representative long-context benchmarks to evaluate models accuracy: (1) RULER [26]: comprehensive benchmark with 13 tasks spanning retrieval, multi-hop tracing, aggregation and question answering (QA) across 4K128K context length. We report the accuracy over 200 examples per task using both full and sparse attention baselines. (2) Needle-in-a-haystack (NIAH) [22]: it challenges LLMs to retrieve information (needle) from context (haystack) with different context lengths and different positions of needle in the context. We use NIAH to stress models accuracy up to 1 million tokens. (3) LongBench [7]: benchmark covering realistic long-context tasks. It composes of six major categories and twenty one different tasks, covering key long-text application scenarios such as singledocument QA, multi-document QA, summarization, fewshot learning, and code completion. The average length of most tasks ranging from 5k to 15k. We evaluate RetroInfer and other baselines inference efficiency with different batch sizes and context lengths. By default, we use Llama3-8B-1048K and NIAH to stress-test RetroInfer up to 1 million tokens. Additional models and tasks are evaluated in Section 5.3. 5.2 Inference Accuracy We first study the inference accuracy of all systems on different tasks, context lengths, and retrieval budgets. Different Tasks. Table 1 shows the model accuracy of 4Quest uses chunk size of 16; MagicPIG employs 10 hash functions and 150 hash tables per KV head; InfiniGen uses 32 partial channels. 5For RetroInfer, in the 128K context with 8192 clusters, we set 150 clusters in the retrieval zone, so the retrieval budget is 150 8192 1.8%. 9 Table 1. Model Accuracy (RULER, 128K Context). RetroInfer significantly outperforms other methods and maintains the same accuracy level as full attention on three models. Methods s1_niah s2_niah s3_niah mk1_niah mk2_niah mk3_niah mv_niah mq_niah fwe cwe qa_1 qa_2 vt Avg. Llama3.1-8B Quest MagicPIG InfiniGen RetroInfer Qwen2.5-7B Quest MagicPIG InfiniGen RetroInfer 100.00 100.00 99.00 100.00 100. 100.00 100.00 92.50 100.00 100.00 Llama3-8B-1048K 100.00 100.00 Quest 99.50 MagicPIG 100.00 InfiniGen RetroInfer 100.00 100.00 99.50 97.50 96.00 99.50 100.00 55.00 62.50 77.00 98.50 100.00 100.00 95.00 99.50 100.00 99.50 99.50 92.00 44.50 100. 100.00 90.00 37.50 53.50 99.50 100.00 100.00 94.00 98.50 100.00 99.00 98.00 98.50 95.00 99.00 92.50 49.50 54.50 70.00 93.00 98.50 99.00 88.50 97.00 97.50 88.50 71.00 73.50 39.00 83. 48.50 27.50 25.50 10.00 47.00 99.50 92.50 96.50 88.50 96.50 55.00 3.00 26.50 0.00 38.50 18.50 0.00 3.50 0.50 15.50 66.50 14.50 44.50 15.50 60.00 91.62 87.00 77.25 67.75 92. 67.00 48.00 42.00 48.00 65.25 96.00 98.00 74.50 91.75 92.75 97.25 53.67 0.05 71.00 39.50 88.80 75.68 95.38 55.17 0.15 68.50 41.00 84.90 69.47 94.62 53.00 0.05 65.50 37.00 86.50 69.30 82.88 46.50 0.40 68.00 37.50 75.50 57.93 97.38 64.00 0.10 70.50 40.50 89.00 74.95 88.00 64.00 41.20 44.50 36.50 85.20 68.15 53.75 63.50 34.00 38.00 30.00 84.90 51.86 46.00 65.33 38.40 40.50 33.50 80.50 47.86 58.50 58.83 29.60 42.00 33.50 87.50 51.46 84.50 58.67 42.25 46.00 38.50 87.20 67.37 98.75 75.50 0.75 64.50 47.00 79.80 78.98 99.00 67.33 3.00 63.00 44.50 83.70 74.19 76.88 73.67 0.40 63.00 44.00 75.90 71.26 91.88 63.83 0.80 62.50 45.50 80.00 71.94 98.50 73.00 0.75 64.00 46.50 78.20 77.52 Table 2. Model Accuracy (LongBench). RetroInfer consistently outperforms all baselines and maintains the same accuracy level as full attention on three models."
        },
        {
            "title": "TrQ RbP",
            "content": "LCC Avg. Llama3.1-8B Quest MagicPIG InfiniGen RetroInfer Qwen2.5-7B Quest MagicPIG InfiniGen RetroInfer 12.88 34.30 91.31 56.40 62.88 51.55 9.43 31.42 89.86 50.41 44.30 45.08 12.00 33.18 90.80 55.50 61.14 50.52 45.62 10.96 32.36 90.71 49.94 44.12 12.31 34.71 91.33 56.15 62.10 51.32 50.11 9.98 35.41 87.15 61.69 56.34 43.59 9.05 31.54 82.16 53.15 42.07 49.18 9.87 34.00 86.35 60.25 55.43 9.61 33.10 87.02 46.72 30.58 41.41 9.77 34.76 87.33 61.80 55.76 49."
        },
        {
            "title": "45.30\nLlama3-8B-1048K 14.25 35.25 87.06 44.74 45.18\n42.02\n11.22 30.35 86.94 44.84 36.74\nQuest\n44.06\n13.66 33.28 87.21 42.56 43.58\nMagicPIG\n12.15 30.81 85.39 38.84 29.41\nInfiniGen\n39.32\n15.17 34.66 86.74 45.58 45.66 45.56\nRetroInfer",
            "content": "various methods on the RULER benchmark with 128K context length. RetroInfer consistently achieves better accuracy than other methods on almost all tasks and matches the accuracy of full attention. For the average task accuracy, RetroInfer has only drop of 0.73%/0.78%/1.46% compared to full attention on Llama3.1-8B/Qwen2.5-7B/Llama3-8B1048K, respectively. Compared to the best-performing sparse attention baselines on each model (Quest/InfiniGen/Quest for Llama3.1-8B/Qwen2.5-7B/Llama3-8B-1048K), RetroInfer achieves 5.48%/15.51%/3.33% accuracy improvements, respectively. The accuracy improvements over other methods are even larger, up to 17.02%/19.51%/6.26% on three models. These results indicate that RetroInfer is able to effectively retrieve important tokens using wave index and cover the varying sparsity by attention estimation. In certain tasks Figure 9. Average Model Accuracy (RULER, 8K to 128K Context). RetroInfer always approaches full attention accuracy. Figure 10. Needle-in-a-haystack (Llama3-8B-1048K). RetroInfer handles all context length and depth. Figure 11. Accuracy vs. Retrieval Budgets. RetroInfer maintains almost the same accuracy as full attention even with small retrieval budget. Quest suffers from inaccurate token selection so it requires larger retrieval budget to achieve similar accuracy. (e.g., fwe on model Llama3.1-8B), RetroInfer even outperforms full attention, possibly because it helps models focus on important tokens and filter out distractions. To further evaluate the effectiveness of RetroInfer on more realistic tasks, we selected five tasks from the LongBench benchmark for extensive evaluation. It includes singledocument QA (Qasper), summarization (GovReport), fewshot learning (TriviaQA), code completion (RepoBench-P) and LCC. As shown in Table 2, RetroInfer consistently outperforms all baselines, with average scores at most 0.23% lower than full attention. Different Context Lengths. As shown in Figure 12 (a), on different context lengths ranging from 8K to 128K, RetroInfer on Llama3-8B-1048K consistently achieves the same accuracy as full attention and outperforms other baselines. Figure 9 presents the average model accuracy across context lengths from 8K to 128K on RULER on all models. RetroInfer maintains its advantage over others, showing the robustness of RetroInfers attention approximation. Figure 10 further demonstrates the accuracy of RetroInfer with context lengths up to 1M. RetroInfer achieves 100% accuracy on all contexts, demonstrating our systems capability to support million-token with accuracy. Different Retrieval Budgets. Furthermore, we evaluate the accuracy of RetroInfer and Quest using Qwen2.5 under varying retrieval budgets on the mk2_niah and cwe task. Figure 11 shows that RetroInfer maintains accuracy close to full attention, even with low retrieval budget. In contrast, Quest suffers degradation due to inaccurate token selection. 5.3 Inference Efficiency We evaluate decoding throughput across context lengths and batch sizes, followed by analysis on different tasks and models, and lastly examine prefilling latency. Throughput. Figure 12 (be) shows the decoding throughput of RetroInfer and other methods across four different context lengths ranging from 60K to 1024K. For the context length of 60K, 120K and 240K, RetroInfer outperforms full attention by 4.4, 4.4, and 4.5 respectively. When compared with Quest, Quest (offload), MagicPIG and InfiniGen, the speedups increase to 3.4, 18.3, 11.6, and 76.9, with an average across three context lengths. Combined with results in Figure 12 (a), RetroInfer achieves the best throughput while also outperforming other baselines in terms of model accuracy. The efficiency of RetroInfer stems from wave buffer, significantly extending the batch sizes while maintaining low data transfer over PCIe. When the batch size is small, full attention and Quest show comparable or slightly higher throughput than RetroInfer due to their fully GPU processing, but they cannot scale beyond GPU memory. Frequent PCIe data transfer in Quest (offload) easily exhausts the limited PCIe bandwidth, while the throughput of MagicPIG is constrained by limited CPU compute capacity, both of which restricts their throughput scalability to larger batch sizes. InfiniGen exhibits the lowest throughput because of its relatively expensive speculative operations and frequent data transfer over PCIe, limiting the overall scalability. At 1M context, full attention and Quest incur OOM. Although InfiniGen offloads the KV cache to the CPU, portion of the key cache is still retained on the GPU for speculation, which also incurs OOM issues when processing longer contexts. Therefore, we compare only with the remaining systems. The throughput advantages of RetroInfer over Quest (offload) and MagicPIG are 11.0 and 10.5 respectively. This demonstrates RetroInfers efficiency for supporting extremely long-context inference. Note that Quest 11 (offload) faces drop in throughput when the batch size reaches 4. This is because the KV of the dense layer can no longer fit in GPU memory, requiring the KV to be transferred from the CPU to the GPU during each decoding step, which results in slowdown. We also evaluate the decoding throughput on different tasks and models. As shown in Figure 13 (a), RetroInfer outperforms full attention by 4.4, 4.0, 4.6, and 3.4 on NIAH, fwe, qa1 and vt tasks, respectively. The throughput variation across tasks is due to differing cache hit ratios, as we analyzed later ( 5.4). The advantage over sparse attention systems still holds, with RetroInfer achieving 3.3, 3.0, 3.5, and 2.6 higher throughput than the best-performing baseline Quest on four tasks. Figure 13 (b) shows the decoding throughput of different models. RetroInfer outperforms full attention by 3.6, 2.9, and 4.4 on Llama3.1-8B, Qwen2.5-7B and Llama3-8B-1048K, respectively. Although Llama3.1-8B has the same model architecture as Llama38B-1048K, it has throughput difference because of different cache behaviors. For Qwen2.5-7B, with less memory consumption due to fewer KV heads and model layers, the advantages of RetroInfer are still evident, demonstrating its architectural generality. Prefilling Latency. We evaluate the prefilling latency across various context lengths, as illustrated in Figure 14. The prefilling latency of RetroInfer exceeds that of full attention by only 7%, 4%, and 2% at context lengths of 120K, 240K, and 480K, respectively. Index building is much faster than full attention and becomes negligible at longer context lengths. This reduction stems from the quadratic cost of full attention compared to the lower complexity of segmented clustering and asynchronous wave buffer construction. Systems like Quest have minimal overhead as they do not require index construction, while MagicPIG incurs higher prefill latency due to CPU-bound LSH table construction. 5.4 Micro Analysis We quantify the impact of individual design decisions of RetroInfer including different components of wave buffer, impact of cache sizes, effect of attention estimation and the trade-off behind segmented clustering. Finally, we study the latency breakdown of RetroInfer. Effect of Design Decisions in Wave Buffer. We assess the impact of two design choices in the wave buffer: GPU caching and asynchronous cache updates. The results are depicted in Figure 15. Without GPU caching (Base), throughput does not scale effectively with increasing batch sizes, primarily due to PCIe bandwidth constraints. Adding GPU caching substantially reduces data transfer, enabling more scalable throughput. Furthermore, the asynchronous update of the mapping table augments throughput even further by decoupling the heavyweight cache update process from the critical execution path, allowing it to fully overlap with GPU execution. Impact of Cache Sizes. We study the impact of cache sizes Figure 12. RULER Accuracy and Decoding throughput vs. Context Length and Batch Size (Llama3-8B-1048K). (a) RetroInfer matches full attention accuracy and outperforms baselines across different context lengths. (b)-(e) RetroInfer scales well with the batch size and achieves the best throughput on all context lengths. Figure 13. Decoding Throughput Across Tasks and Models. RetroInfer consistently outperforms other methods. Figure 14. Prefilling Latency vs. Context Lengths (batch = 1). The prefilling latency of RetroInfer is only slightly higher than full attention, and the overhead becomes negligible as the context length increases. Figure 15. Effect of Design Choices. Base offloads KV to CPU ( no GPU cache), exhibiting low throughput scalability; W/ GPU cache greatly improving throughput; W/ Async cache update further boosts the speed. Figure 16. Cache Hit Ratio vs. Cache Sizes. Increasing cache size from 1% to 5% yields substantial improvement in hit ratio; however, further increases beyond 5% result in marginal gains. on cache hit ratios across various models and tasks. We use LRU policy, which performs well due to enhanced temporal locality. Exploring other caching policies is left for future work. Figure 16 illustrates that as the cache size increases, the cache hit ratio improves significantly. However, once the cache size surpasses 5%, the growth rate slows down. Further increasing the cache size results in only marginal gains while substantially increasing GPU memory consumption, which in turn limits the maximum executable batch 12 Figure 17. (a) Effect of Attention Estimation on Accuracy. Estimation helps improve the model accuracy. (b) Index Build Time vs. Accuracy (128K context). The number next to the circle indicates the segment size. Segment size = 8K strikes optimal balance between index building time and clustering accuracy. size and hinders throughput scalability. Our extensive experiments reveal that this trend is consistent across different models and tasks, highlighting the ubiquity of temporal locality across models. Additionally, the cache hit ratios at the same cache size display slight variations across tasks. For example, on the Llama3-8B-1048K model, the NIAH task achieves higher cache hit ratio (0.93) compared to the vt task (0.86) and thus reduces the data transfer, resulting in better throughput as demonstrated in Figure 13. Impact of Attention Estimation. We quantify its effect by comparing RetroInfer with and without attention estimation. As depicted in Figure 17 (a), attention estimation enhances task accuracy across three models, delivering accuracy gains of up to 20%. Importantly, this improvement is achieved without compromising inference efficiency, as it can be fully overlapped with wave buffer accesses. Trade-off: Index Construction Time versus Accuracy. We study the impact of different segment sizes on both execution time and clustering accuracy of segmented 𝑘-means, using mv_niah task with 128K context length on Llama3-8B1048K. We use recall@100, which computes the overlap between top-100 retrieval results and ground truth, as the metric for measuring clustering accuracy. Figure 17 (b) shows the results. As the segment size decreases from 128K to 8K, the index recall drops by less than 1% benefiting from spatial locality while the index build time is reduced by 80%. However, further decreasing the segment size deteriorates the clustering quality without time reduction. This reveals trade-off point at which execution efficiency and recall are optimally balanced. We empirically set it to 8K as balanced run on memory-constrained devices. These are orthogonal to RetroInfer, which targets scenarios where the KV cache, not the weights, is the memory bottleneck. 7 Conclusion We presented RetroInfer, novel system design for highthroughput inference on long-context LLMs without compromising model accuracy. By reframing the KV cache as vector storage system, RetroInfer introduces co-designed software stack that integrates an attention-aware vector index (wave index) with runtime control plane (wave buffer) for heterogeneous memory systems. We decouple attention approximation from system efficiency and enable scalable execution across GPU and CPU. We demonstrate that RetroInfer not only achieves significant speedups over full and sparse attention baselines, but also preserves model accuracy. As we have shown through RetroInfer, KV cache is vector storage system. Table 3. Decoding Latency Breakdown (ms) (120K context, single layer). Purple components overlap with the subsequent."
        },
        {
            "title": "Estimation zone attention\nAccess cluster mapping table",
            "content": "1 32 0.1447 0.5132 0.1010 0.5398 0.2405 0.5902 0.0142 0.5176 Update cluster mapping table Data copy to execution buffer 0.1281 0.9008 Steady and retrieval zone attention 0.0777 0."
        },
        {
            "title": "Admit missed data",
            "content": "0.0572 0.1038 choice, based on extensive experimentation. Decoding Latency Breakdown. We evaluate the latency of each component of RetroInfer during the decoding phase with batch sizes of 1 and 32. As shown in Table 3, the time spent on estimation zone attention can be completely overlapped with the mapping table access. Similarly, the time for data copy to the execution buffer and the steady and retrieval zone attention can mask the time required for updating the cluster mapping table. This effectively leverages inherent parallelism of GPU and CPU. 6 Related Work LLM Serving Systems. Recent research has explored various strategies for LLM serving efficiency [3, 15, 31, 35, 53, 59, 74, 81, 82]. LoongServe [64] proposes an elastic sequence parallelism scheduling algorithm for long-context requests. Many works [12, 16, 17, 48, 75] also offload KV caches to CPU or disk to alleviate the memory capacity limit. These works focus on different aspects of long-context serving and are complementary to RetroInfer. Sparse Attention. To accelerate long-context inference, many works exploit attention sparsity. Static sparsity methods [34, 68, 69] maintain fixed patterns during decoding but compromise model accuracy. Many dynamic sparsity methods [13, 32, 60, 67, 72, 77, 79] heuristically select KV pairs per step. Recent works [23, 37] also explore leveraging vector indexes for KV cache retrieval. However, they are limited to prefix caching or offline inference scenarios due to the high cost of index construction. PyramidKV [9] and D2O [63] adapt retrieval budgets across layers but do not consider sparsity variation across queries and tasks. Tactic [83], concurrent effort, adjusts budgets via distribution fitting, though its accuracy and efficiency depend on fitting precision. Our work also focuses more on system-level design than on pure algorithmic improvements. Some sparse attention approaches [18, 76] require costly model retraining to learn dynamic sparsity. Other techniques include scalar KV cache quantization [24, 38, 78] and sparsity for accelerating prefilling [30], both of which are complementary to our work. LLM Weight Sparsity. Model weight compression and offloading techniques [5, 36, 54, 57, 71, 80] enable LLMs to"
        },
        {
            "title": "References",
            "content": "[1] 01-ai. Yi-6b-200k. https://huggingface.co/01-ai/Yi-6B-200K, 2024. Accessed: 2024-07-01. [2] 01-ai. Yi-9b-200k. https://huggingface.co/01-ai/Yi-9B-200K, 2024. Accessed: 2024-07-01. [3] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee. Taming Throughput-Latency tradeoff in LLM inference with Sarathi-Serve. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 117134, 2024. [4] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multiquery transformer models from multi-head checkpoints. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [5] Keivan Alizadeh, Seyed Iman Mirzadeh, Dmitry Belenko, Khatamifard, Minsik Cho, Carlo Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar. Llm in flash: Efficient large language model inference with limited memory. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1256212584, 2024. [6] Anthropic. Claude. https://www.anthropic.com/claude, 2025. [7] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: bilingual, multitask benchmark for long context understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, Bangkok, Thailand, 2024. Association for Computational Linguistics. [8] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675698, 2024. [9] Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, and Wen Xiao. Pyramidkv: Dynamic KV cache compression based on pyramidal information funneling. CoRR, abs/2406.02069, 2024. [10] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. [11] Meng Chen, Kai Zhang, Zhenying He, Yinan Jing, and X. Sean Wang. Roargraph: projected bipartite graph for efficient crossmodal approximate nearest neighbor search. Proc. VLDB Endow., 17(11):27352749, 2024. [12] Weijian Chen, Shuibing He, Haoyang Qu, Ruidong Zhang, Siling Yang, Ping Chen, Yi Zheng, Baoxing Huai, and Gang Chen. IMPRESS: An Importance-Informed Multi-Tier Prefix KV Storage System for Large Language Model Inference. In 23rd USENIX Conference on File and Storage Technologies (FAST 25), pages 187201, 2025. [13] Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, et al. Magicpig: Lsh sampling for efficient llm generation. ArXiv preprint, abs/2410.16179, 2024. [14] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library. 2024. [15] Yao Fu, Leyang Xue, Yeqi Huang, Andrei-Octavian Brabete, Dmitrii Ustiugov, Yuvraj Patel, and Luo Mai. Serverlessllm:low-latency serverless inference for large language models. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 135153, 2024. [16] Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo. Cost-Efficient large language model serving for multi-turn conversations with CachedAttention. In 2024 USENIX Annual Technical Conference (USENIX ATC 24), pages 111126, 2024. [17] Shiwei Gao, Youmin Chen, and Jiwu Shu. Fast state restoration in llm serving with hcache. In Proceedings of the Twentieth European Conference on Computer Systems, pages 128143, 2025. [18] Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai, Hayden Kwok-Hay So, Ting Cao, Fan Yang, et al. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. [19] Google. Gemini. https://gemini.google.com/app, 2025. [20] Gradient AI. Llama-3-8b-instruct-262k. https://huggingface.co/ gradientai/Llama-3-8B-Instruct-262k, 2024. Accessed: 2024-07-01. [21] gradientai. Llama-3-8b-instruct-gradient-1048k. https://huggingface. co/gradientai/Llama-3-8B-Instruct-Gradient-1048k, 2025. [22] Greg Kamradt. Needle in haystack - pressure testing llms. https:// github.com/gkamradt/LLMTest_NeedleInAHaystack, 2023. Accessed: 2024-08-12. [23] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, June Paik, Michael Mahoney, Kurt Keutzer, and Amir Gholami. Squeezed attention: Accelerating long context length llm inference. arXiv preprint arXiv:2411.09688, 2024. [24] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. Advances in Neural Information Processing Systems, 37:1270 1303, 2024. [25] Kurt Hornik, Ingo Feinerer, Martin Kober, and Christian Buchta. Spherical k-means clustering. Journal of statistical software, 50:122, 2012. [26] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. RULER: Whats the real context size of your long-context language models? In First Conference on Language Modeling, 2024. [27] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: In Proceedings of towards removing the curse of dimensionality. the thirtieth annual ACM symposium on Theory of computing, pages 604613, 1998. [28] InfiniGen. InfiniGen. InfiniGen Code. https://github.com/snu-comparch/ [29] Johan Ludwig William Valdemar Jensen. Sur les fonctions convexes et les inégalités entre les valeurs moyennes. Acta mathematica, 30(1):175193, 1906. [30] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, ChinYew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. [31] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. [32] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. InfiniGen: Efficient generative inference of large language models with dynamic KV cache management. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 155172, Santa Clara, CA, 2024. USENIX Association. [33] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as database interface? big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36:4233042357, 2023. [34] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. SnapKV: LLM knows what you are looking for before generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [35] Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, and Lili Qiu. Parrot: Efficient Serving of LLM-based Applications with Semantic Variable. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 929945, 2024. [36] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, WeiChen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. [37] Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, et al. Retrievalattention: Accelerating long-context llm inference via vector retrieval. ArXiv preprint, abs/2409.10516, 2024. [38] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [39] MagicPIG. MagicPIG Code. https://github.com/Infini-AI-Lab/ MagicPIG. [40] Yu Malkov and Dmitry Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4):824836, 2018. [41] Meta. Llama. https://www.llama.com/, 2025. [42] Meta. Llama-3.1-8b. https://huggingface.co/meta-llama/Llama-3.1-8B, 2025. [43] Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai.meta.com/blog/llama-4-multimodalintelligence/, 2025. [44] NVIDIA. Nvidia a100 tensor core gpu. https://www.nvidia.com/enus/data-center/a100/, 2020. [45] NVIDIA. Geforce rtx 4090. https://www.nvidia.com/en-us/geforce/ graphics-cards/40-series/rtx-4090/, 2022. [46] OpenAI. Chatgpt. https://chat.chatbotapp.ai/, 2025. [47] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5:606624, 2023. [48] Ruoyu Qin, Zheming Li, Weiran He, Jialei Cui, Feng Ren, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake: Trading more storage for less computationa kvcache-centric architecture for serving llm chatbot. In 23rd USENIX Conference on File and Storage Technologies (FAST 25). USENIX Association, Santa Clara, CA, pages 155170, 2025. [49] Quest. Quest Code. https://github.com/mit-han-lab/Quest. [50] Qwen. Qwen2.5-7b-instruct. https://huggingface.co/Qwen/Qwen2.57B-Instruct, 2024. [51] Parikshit Ram and Alexander G. Gray. Maximum inner-product search using cone trees. In Qiang Yang, Deepak Agarwal, and Jian Pei, editors, The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 12, Beijing, China, August 12-16, 2012, pages 931939. ACM, 2012. [52] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. In Forty-first International Conference on Machine Learning, 2024. [53] Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang Zhuo, Joseph Gonzalez, and Ion Stoica. Fairness in serving large language models. In 18th USENIX Symposium on Operating 15 Systems Design and Implementation (OSDI 24), pages 965988, 2024. [54] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: high-throughput generative inference of large language models with single gpu. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. [55] Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 23212329, 2014. [56] Sivic and Zisserman. Video google: text retrieval approach to object matching in videos. In Proceedings ninth IEEE international conference on computer vision, pages 14701477. IEEE, 2003. [57] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model serving with consumer-grade gpu. In Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles, pages 590606, 2024. [58] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [59] Biao Sun, Ziming Huang, Hanyu Zhao, Wencong Xiao, Xinyi Zhang, Yong Li, and Wei Lin. Llumnix: Dynamic scheduling for large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 173191, 2024. [60] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. QUEST: Query-aware sparsity for efficient long-context LLM inference. In Forty-first International Conference on Machine Learning, 2024. [61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008, 2017. [62] John Von Neumann. First draft of report on the edvac. IEEE Annals of the History of Computing, 15(4):2775, 1993. [63] Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, and Mi Zhang. D2o: Dynamic discriminative operations for efficient generative inference of large language models. arXiv preprint arXiv:2406.13035, 2024. [64] Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng Sun, Xuanzhe Liu, and Xin Jin. Loongserve: Efficiently serving long-context large language models with elastic sequence parallelism. In Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles, pages 640654, 2024. [65] Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Kun Fu, Zheng Wang, and Hui Xiong. Tokenselect: Efficient long-context inference and length extrapolation for llms via dynamic token-level KV cache selection. CoRR, abs/2411.02886, 2024. [66] Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically explains long-context factuality. In The Thirteenth International Conference on Learning Representations, 2025. [67] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. InfLLM: Training-free long-context extrapolation for LLMs with an efficient context memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. USENIX Association. [83] Kan Zhu, Tian Tang, Qinyu Xu, Yile Gu, Zhichen Zeng, Rohan Kadekodi, Liangyu Zhao, Ang Li, Arvind Krishnamurthy, and Baris Kasikci. Tactic: Adaptive sparse attention with clustering and distribution fitting for long-context llms. arXiv preprint arXiv:2502.12216, 2025. [68] Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. [69] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. [70] Fangyuan Xu, Tanya Goyal, and Eunsol Choi. Recycled attention: Efficient inference for long-context language models. arXiv preprint arXiv:2411.05787, 2024. [71] Zhenliang Xue, Yixin Song, Zeyu Mi, Xinrui Zheng, Yubin Xia, and Haibo Chen. Powerinfer-2: Fast large language model inference on smartphone. arXiv preprint arXiv:2406.06282, 2024. [72] Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, and Song Han. Lserve: Efficient long-sequence llm serving with unified sparse attention. arXiv preprint arXiv:2502.14866, 2025. [73] Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, et al. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. [74] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: distributed serving system for TransformerBased generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521538, 2022. [75] Lingfan Yu, Jinkun Lin, and Jinyang Li. Stateful large language model In Proceedings of the Twentieth European serving with pensieve. Conference on Computer Systems, pages 144158, 2025. [76] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. [77] Hailin Zhang, Xiaodong Ji, Yilin Chen, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Weipeng Chen, and Bin Cui. Pqcache: Product quantization-based kvcache for long context llm inference. ArXiv preprint, abs/2407.12820, 2024. [78] Tianyi Zhang, Jonah Yi, Zhaozhuo Xu, and Anshumali Shrivastava. Kv cache is 1 bit per channel: Efficient large language model inference with coupled quantization. Advances in Neural Information Processing Systems, 37:33043331, 2024. [79] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2O: heavyhitter oracle for efficient generative inference of large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [80] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196209, 2024. [81] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. SGLang: EfIn The ficient execution of structured language model programs. Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [82] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. DistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 193210, Santa Clara, CA, 2024."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Shanghai Jiao Tong University",
        "Tsinghua University",
        "University of Science and Technology of China",
        "Wuhan University"
    ]
}