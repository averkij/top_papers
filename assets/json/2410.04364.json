{
    "paper_title": "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide",
    "authors": [
        "Dohun Lee",
        "Bryan S Kim",
        "Geon Yeong Park",
        "Jong Chul Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, a novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as a guide during the early stages of inference, improving temporal quality by interpolating the guiding model's denoised samples into the sampling model's denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing a cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by utilizing the superior data prior of the guiding model through the proposed method. Project Page: https://dohunlee1.github.io/videoguide.github.io/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 2 4 6 3 4 0 . 0 1 4 2 : r Preprint. VIDEOGUIDE: IMPROVING VIDEO DIFFUSION MODELS WITHOUT TRAINING THROUGH TEACHERS GUIDE Dohun Lee, Bryan Kim, Geon Yeong Park, Jong Chul Ye KAIST : Equal Contribution {leedh7, bryanswkim, pky3436, jong.ye}@kaist.ac.kr Base Ours Base Ours \"A drone view of celebration with Christmas tree and fireworks\" \"A boat sailing in the middle of the ocean\" Base Ours Base Ours \"Slow motion footage of racing car\" \"A dog drinking water\" Figure 1: VideoGuide is novel framework for improving temporal consistency while preserving imaging quality, enabling high-quality video generation for diverse text prompts. By applying VideoGuide to underperforming base models, we can significantly improve temporal consistency with no additional training or fine-tuning. Best viewed with Acrobat Reader. Click each image to play the video clip."
        },
        {
            "title": "ABSTRACT",
            "content": "Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as guide during the early stages of inference, improving temporal quality by interpolating the guiding models denoised samples into the sampling models denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by 1 Preprint. utilizing the superior data prior of the guiding model through the proposed method. Project Page: this http URL"
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-to-image (T2I) diffusion models have greatly changed the way how visual content is created and distributed, enabling users to effortlessly generate creative images from detailed text descriptions. Now the AI community is looking deeper into the potential of T2I diffusion models, exploring their application to the higher dimensional field of video generation. Text-to-video (T2V) diffusion models aim to extend the capabilities of their image-based counterparts by generating coherent video sequences from text descriptions, handling both spatial and temporal dimensions simultaneously. However T2V diffusion models still show sub-standard performance regarding temporal consistency, and can lead to the generation of degraded samples. Poor temporal consistency is also the main challenge for variety of tasks, such as creation of personalized T2V models. Hence, recent work (Wu et al., 2023; Qiu et al., 2023; Ge et al., 2024) aims to enhance various aspects of temporal quality, but suffers from problems such as degraded quality, slow inference, etc. In this work, we attend to the clear absence of reliable method for refining the temporal quality of pretrained text-to-video (T2V) generation models, and propose novel framework for improved generation that does not require any training or fine-tuning. Specifically, we introduce VideoGuide, general framework that uses any pretrained video diffusion model as guide during early steps of reverse diffusion sampling. Choice of the pretrained VDM is flexible: it is either identical to the VDM used for inference, or it is freely selected from all existing VDMs. In any case, the VDM that acts as the guide provides consistent video trajectory by proceeding in its own denoising for small number of steps. The guiding models denoised sample is then integrated into the original denoising process to guide the sample towards direction with better temporal quality. Through interpolation, the sampling VDM is able to follow the temporal consistency of the guiding VDM to produce samples of enhanced quality. Such interpolation only needs to be involved in the first few steps of inference, but is strong enough to guide the entire denoising process towards more desirable results. Remarkably, interpolating information of the guiding models denoised sample has the effects of providing the base model better noise prior, even guiding the model to create samples that were previously unreachable. VideoGuide is versatile framework in that any pretrained video diffusion model can be used for distillation in plug-and-play fashion. By incorporating superior VDM as video guide, our framework can be used to boost underperforming VDMs into state-of-the-art quality. This is particularly useful when the relatively underperforming VDM possesses unique traits unavailable for the superior VDM. In particular, we show two representative cases of how VideoGuide can be applied, with AnimateDiff (Guo et al., 2024) and LaVie (Wang et al., 2023). In AnimateDiff, motion module is trained that can be interleaved into any pretrained T2I model. The scheme works for any personalized image diffusion model and grants easy application of controllable and extensible modules (Zhang et al., 2023; Guo et al., 2023), but not without consequences. Specifically, fixing the T2I weights limits interaction between the temporal module and generated spatial features, hence harming temporal consistency. Applying VideoGuide with an open-source state-of-the-art model without personalization capability (Chen et al., 2024) as the guiding model, we can greatly enhance the temporal quality of AnimateDiff. This allows us to combine the best of both worlds: personalization and controllability is provided by the base model, while temporal consistency is refined by the guiding model. Likewise, LaVie is multifaceted T2V model that offers various functions including interpolation and super-resolution in cascaded generation framework, but shows substandard temporal consistency. Using VideoGuide, we can upgrade its temporal consistency with an external model while maintaining its multiple functions. The synergistic effects that our framework can bring are not limited to these two cases but are, in fact, boundless. As powerful video diffusion models emerge, existing models will not become obsolete but actually improve through the guidance our method provides. Moreover, as VideoGuide can be applied solely during inference time, these benefits can be enjoyed with no cost at all. Our contributions can be summarized as follows: 1. We propose VideoGuide, novel framework for enhancing temporal consistency and motion smoothness while maintaining the imaging quality of the original VDM. 2 Preprint. Figure 2: Overall Pipeline. VideoGuide is framework for enhancing temporal quality without additional training, leveraging the capabilities of any pretrained VDM. Throughout the denoising process of the sampling VDM, the guiding VDM receives an intermediate latent zt and provides temporally consistent sample ztτ by proceeding in its own denoising for small number of steps τ . The sample ztτ is then denoised and interpolated with the denoised zt to produce fused latent t. Such interpolation only needs to take part in the first few steps of inference, and effectively guides samples towards direction of improved temporal consistency. To further ensure model flexibility in refining high-frequency areas for better image quality, the latent is passed through Low-Pass Filter (LPF). Overall, VideoGuide is straightforward addition to the original pipeline, yet it is powerful enough to significantly enhance temporal consistency without compromising imaging quality or motion smoothness. 2. We show how any existing VDM can be incorporated into our framework, enabling boosted performance of inadequate models along with newfound synergistic effects among models. 3. We provide evidence of prior distillation, in which the informative prior of guidance models can be utilized to create samples of improved text coherency."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "The Diffusion Model. Diffusion probabilistic models (Ho et al., 2020) have achieved great success as generative models. To address the significant computational cost that arises from operating in pixel space, Latent Diffusion Models (LDMs) (Rombach et al., 2021) learn the diffusion process in latent space. LDMs utilize an encoder-decoder framework where the encoder and the decoder are trained together to reconstruct the input data. This training aims to satisfy the relation = D(z0) = D(E(x)), where z0 is the latent representation of the corresponding clean pixel image x. Thus the forward diffusion process in latent space is defined as follows: zt = αtz0 + 1 αtϵ, (1) where αt is pre-determined noise scheduling coefficient, and ϵ (0, I) represents Gaussian noise sampled from standard normal distribution. The reverse diffusion process is directed by score-based neural network, denoted as the diffusion model ϵθ, which is trained using the denoising score matching framework (Ho et al., 2020; Song et al., 2021b). The training objective for this model is formulated as follows: min θ Et,ϵN (0,I)ϵ ϵθ(zt, t)2 2. (2) Following the formulation of DDIM (Song et al., 2021a), the reverse deterministic sampling from the posterior distribution p(zt1zt, z0) is given by: zt1 = αt1z0t + (cid:112)1 αt1ϵθ(zt, t) (3) 3 Preprint. zt z0t = 1 αtϵθ(zt, t) αt (4) where the denoised sample at timestep t, denoted as z0t, can be obtained using Tweedies formula. Classifier Free Guidance (CFG). In conditional diffusion models, classifier free guidance (Ho & Salimans, 2021) enhances quality of generated samples by increasing the conditional likelihood through weighted adjustment of the conditional distribution. Mathematically this is expressed as: ˆϵθ(zt, t) = ϵθ(zt, t, ϕ) + w[ϵθ(zt, t, c) ϵθ(zt, t, ϕ)] (5) where and ϕ refer to the text condition and null condition, respectively, and refers to the guidance scale used during reverse sampling. To apply classifier free guidance to Eq. (3) and Eq. (4), we substitute ϵθ(zt, t) with ˆϵθ(zt, t) in both. Recent work (Chung et al., 2024) points out that using high guidance scale (e.g., around 7.5) often results in issues such as abrupt changes and color saturation in the denoised sample z0t during the early timesteps of reverse sampling. To address these issues, CFG++ (Chung et al., 2024) introduces interpolation between the conditional estimate ϵθ(zt, t, c) and the unconditional estimate ϵθ(zt, t, ϕ) using lower guidance scale [0, 1]. Derived from score distillation sampling (SDS) (Poole et al., 2022), CFG++ replaces the renoising term ˆϵθ(zt, t) into ϵθ(zt, t, ϕ). In this case, Eq. (3) can be modified as below: (6) Our proposed interpolation scheme operates on denoised samples for early timesteps, during which maintaining high-quality denoised samples is essential. Thus, we utilize CFG++ throughout the early stages of denoising to achieve smooth interpolation. αt1z0t + (cid:112)1 αt1ϵθ(zt, t, ϕ) zt1 = Video Diffusion Model (VDM) & Consistent Video Generation. The Video Diffusion Model (VDM), originally proposed in Ho et al. (2022), operates the diffusion process in the video domain. Similar to LDMs, many recent VDMs (Xing et al., 2023; Chen et al., 2023; He et al., 2022) are trained in the latent space to reduce computational cost. In Latent VDMs (LVDMs), temporal layer is incorporated to facilitate frame interaction along the temporal axis during training. By modifying zt to z1:N in Eqs. (1)-(6), the diffusion model can be extended to the video domain. For simplicity, we will use the notation zt to represent the latent for video generation instead of z1:N . One of the main challenges in utilizing diffusion models for video generation lies in maintaining temporal consistency. In the video domain, PYoCo (Ge et al., 2024) introduces carefully designed progressive video noise prior to better leverage image diffusion models for video generation. However, PYoCo primarily focuses on the noise distribution during the training stage and requires extensive fine-tuning on video datasets. Recent work (Qiu et al., 2023; Jiaxi et al., 2023) also attempts to improve temporal consistency, but focuses more on long video generation and is not applicable to the basic 16 frame scenario. FreeInit (Wu et al., 2023) addresses the issue of video consistency by iterative refinement of the initial noise. This method aims to resolve the training-inference discrepancy in video diffusion models by reinitializing noise with spatio-temporal filter, ensuring the refined noise better aligns with the training distribution. While this approach enhances frame-to-frame consistency, it has significant drawback: repeated iteration leads to the loss of fine details and imaging quality degradation. Additionally, the iterative nature of the method induces high computational costs, prolonging the generation process. In VideoGuide, we aim to enhance video consistency without the aforementioned drawbacks. By integrating small number of guidance steps into the original reverse sampling process, we are able to avoid image degradation while significantly reducing inference time compared to prior work. Furthermore, our method can incorporate external diffusion models to facilitate more temporally consistent video generation. This makes our approach particularly effective for models that struggle with temporal consistency but demonstrate strong performance in other areas (e.g., customizable T2I-based video models (Guo et al., 2024))."
        },
        {
            "title": "3 VIDEOGUIDE",
            "content": "3.1 VIDEO CONSISTENCY ON DIFFUSION TRAJECTORY The DDIM formulation can be expressed as proximal optimization problem (Kim et al., 2024): αt1z + (cid:112)1 αt1ˆϵθ(zt, t) where = arg min zt1 = z0t2 2 (7) 4 Preprint. We extend this approach to the video domain by introducing novel regularization term specially crafted for enhancing temporal consistency. Specifically, for given video x1:N exists. Then, it would be desirable to set the optimization problem as follows: , suppose that temporally consistent latent of zr = E(x1:N r ) min z0t2 2 + λregR(z) where R(z) := zr2 2 (8) Unfortunately, it is infeasible to provide zr as the purpose of the VDM is to generate new unseen samples. Thus, we propose to use z0tτ as proxy of zr where τ is sufficient number of timesteps. This is because z0tτ is usually cleaner and temporally more consistent sample than z0t, so we want to utilize this property. Under this assumption, the highly complex problem of generating temporally consistent video samples is reduced to solving the simple optimization problem below: αt1z + (cid:112)1 αt1ˆϵθ(zt, t) z0t2 zt1 = where = min 2 + λregz z0tτ 2 2 (9) which is equivalent to zt1 = αt1 (cid:0)βz0t + (1 β)z0tτ (cid:1) + (cid:112)1 αt1ˆϵθ(zt, t), β = 1 1 + λreg (10) Accordingly, it suffices to use the interpolation of z0t and z0tτ as an estimate of the temporally consistent form of z0t. To further ensure model flexibility to refine high-frequency areas for better image quality, we employ low-pass filter inspired by previous work (Wu et al., 2023). Specifically, using low-pass filter and high-pass filter of cut-off frequency γ, denoted LPFγ and LPFγ respectively, we define the following update: zt1 = LPFγ(zt1) + HPF1γ(ϵ) where ϵ (0, I) (11) Replacement of high-frequency regions with random Gaussian noise enhances model capacity to infer corresponding high-frequency components, leading to denoised results of higher quality. 3.2 GUIDANCE WITH EXTERNAL VIDEO DIFFUSION MODELS The assumption zr z0tτ in Sec. 3.1 holds for any sample z0tτ with temporal consistency comparable to real-world sample. This brings us to realize that the sample z0tτ does not necessarily have to originate from the same base model. It is possible to plug in any denoised latent z0tτ from any video diffusion model, and the denoising process would be guided to follow the temporal consistency of the supplemented latent. Here, we demonstrate the steps required for utilizing denoised samples z(G) 0tτ of an external guidance model to enhance the performance of the base sampling model S. Renoising into the Guidance Domain. Different video diffusion models are trained on different noise schedules and distributions, and matching such discrepancies is mandatory process. When utilizing guiding model with conflicting factors, the intermediate latent zt of the sampling model must be transformed to align with the noise schedule and distribution of the guiding model. The transformation process can be defined as follows: z(G) = (cid:113) z(S) α(G) 0t + (cid:113) 1 α(G) ϵ, where ϵ (0, I) (12) where (S) denotes the components related to the base sampling model and (G) denotes the components related to the external guiding model. Specifically, z(S) at timestep t, and α(G) outcome z(G) to z(G) 0tτ . is derived from the noise schedule of the guiding diffusion model. The resulting can then be denoised with the guiding model for sufficient number of timesteps τ up 0t is the denoised sample from z(S) t Interpolation of Denoised Samples. Interpolating the denoised samples of the two models and can be expressed as below: αt1(βz(S) 0t + (1 β)z(G) 0tτ ) + (cid:112)1 αt1ˆϵ(S) θ (zt, t) z(S) t1 = (13) Preprint. Note that the only difference from Eq. (10) is the introduction of the z(G) z(S) 0tτ would be used. LPFγ can then be used on z(S) components: 0tτ term, where originally t1 as in Eq. (11) for replacing high-frequency t1 = LPFγ(z(S) z(S) t1) + HPF1γ(ϵ) where ϵ (0, I) (14) Synergistic Effects of External VDM Guidance. Utilizing high-performance open-source model (Chen et al., 2024) as the guiding diffusion model in our VideoGuide framework is shown to improve temporal consistency even while achieving faster convergence. Compared to the self-guided case, generating temporally coherent samples from superior model proves beneficial to the quality of the resulting samples, as illustrated in Sec. 4. Moreover, since interpolation occurs only during the early timesteps, the advantages of the sampling diffusion modelsuch as the personalized video generation and controllability of AnimateDiffare fully preserved. Accordingly, VideoGuide is versatile framework that can combine the best of both worlds: the sampling model and the guiding model. No additional training or fine-tuning is required for seeing such synergistic effects, allowing the user to freely select favored VDMs in plug-and-play fashion. 3.3 VIDEOGUIDE IN PRACTICE Early Timestep Interpolation. In VideoGuide novel interpolation technique is included in the inference process, and the equations above explain cases at specific timestep t. Theoretically this interpolation could be performed at every denoising timestep, but such iteration would both be computationally expensive and detrimental to the high-frequency components that emerge at later timesteps. Recent work (Wu et al., 2023) shows that providing informative low-frequency components at initialization time is sufficient for enhancing temporal consistency. Likewise, we find that applying our interpolation scheme at early timesteps is adequate for enforcing temporal consistency while allowing high-frequency regions to align more closely to the low-frequency structure. An extensive ablation study regarding the number of interpolation steps is given in Sec. 5.1 Prior Distillation. Each video diffusion model spans its own specific data distribution, causing sample generation to be restricted to the data prior the model has been trained on. Thus, if the data prior of model is substandard, the generation results of the model are also inherently substandard. This is especially noticeable when using personalized text-to-image (T2I) models such as Dreambooth or LoRA in AnimateDiff, in which substandard results that do not align with the given text prompt are frequently observed. Prior work (Ge et al., 2024) elaborates on the importance of data prior for VDMs, but the proposed solution involves extensive fine-tuning, making it impractical for simple use cases. On the other hand, VideoGuide comes as potential solution in such cases, where the interpolation between two models exhibit form of prior distillation. Through the guidance of generalized video diffusion model (e.g. Chen et al. (2024)) the base sampling model is able to refer to the denoised sample provided by the guidance model, and steer its sampling process towards relevant outcome. This allows for the effective generation of diverse objects, even while retaining the style of the original data domain. For the case of AnimateDiff, the approach allows for broader customization without the need for directly training the personalized T2I model on wider range of data. Extensive analysis concerning this issue is provided in Sec. 5.2."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Experimental Settings. In our experiments, we leverage multiple open-source Text-to-Video (T2V) diffusion models to explore the combined strengths of each. For the guiding diffusion model, we choose Videocrafter2 (Chen et al., 2024) due to its strong performance in temporal consistency, as measured by the VBench (Huang et al., 2024) benchmark. For sampling, we employ AnimateDiff (Guo et al., 2024) for flexible personalization of video content, and Lavie (Wang et al., 2023) to enhance video quality and increase frame count through super-resolution and interpolation techniques. This integration combines the temporal consistency of the guiding model with the advantages of the sampling model. All experiments were conducted using DDIM with 50 steps for sampling. For our experiments with AnimateDiff, we set = 5, β = 0.5, and τ = 10, and used the Butterworth filter with normalized frequency of 0.25 and filter order of = 4. Additional experimental details are provided in Appendix A. 6 Preprint. Method AnimateDiff (Guo et al., 2024) AnimateDiff + FreeInit (Wu et al., 2023) AnimateDiff + Ours (with AnimateDiff) AnimateDiff + Ours (with VideoCrafter2) LaVie (Wang et al., 2023) LaVie + FreeInit (Wu et al., 2023) LaVie + Ours (with Lavie) LaVie + Ours (with VideoCrafter2) Subject consistency () Background Consistency () Imaging Quality () Motion Smoothness () 0.9183 0.9487 0.9596 0.9614 0.9534 0.9625 0.9629 0. 0.9437 0.9604 0.9642 0.9664 0.9599 0.9643 0.9652 0.9643 0.6647 0.6173 0.6526 0.6671 0.6750 0.6533 0.6780 0.6796 0.9547 0.9705 0.9760 0.9772 0.9658 0.9757 0.9725 0. Table 1: Quantitative comparison of video generation. Bold: best, underline: second best. Evaluation Metrics. To validate the improvement in video consistency with our proposed method, we evaluate four key metrics: subject consistency, background consistency, imaging quality, and motion smoothness. For subject consistency evaluation, DINO (Caron et al., 2021) feature similarity between frames is measured to assess consistency of the subjects appearance throughout the video. Background consistency is evaluated using CLIP feature similarity between frames to evaluate overall scene consistency. Imaging quality is also key metric in that maintaining original image quality is essential for generation and enabling customization. Thus we evaluate image quality using the multiscale image quality transformer (MUSIQ) (Ke et al., 2021), which measures frame-wise low-level distortion such as noise, blur, and over-exposure. Additionally, to ensure smooth motion, we employ video interpolation model (Li et al., 2023) to assess consistency of motion across video frames. 4.1 COMPARISON RESULTS Qualitative results for various prompts and base models are shown in Fig. 3. Samples from the base model show impairment in temporal consistency, such as fluctuation in color or abrupt change in subject appearance. FreeInit moderately solves the problem of temporal consistency but at the cost of considerable degradation in imaging quality, such as smoothing out of textural details. In contrast, the proposed VideoGuide significantly enhances temporal consistency without loss of imaging quality or motion smoothness. Furthermore, VideoGuide solves sudden frame shifts frequently observed in LaVie by providing smooth frame transitions, explained in Appendix E. Detailed explanation of base models used and additional qualitative results are included in Appendix and Appendix E. In quantitative comparison, our method demonstrates superior performance over the base model, achieving improvements in both subject and background consistency. When using AnimateDiff as the base model, our approach shows best results for all key metrics. There is notable enhancement in temporal consistency compared to baselines, and such increase is not at the cost of imaging quality or motion smoothness. Our method is shown to actually improve both factors when VideoCrafter2 is used as the guiding model. small decrease in imaging quality can be observed for the self-guided case, but the difference is minimal compared to the notable decrease in imaging quality for FreeInit. When using LaVie as the base model, our approach still shows reliable increase in subject and background consistency. Note that increase is relatively smaller due to higher base consistency. Furthermore, our method successfully maintains imaging quality and improves motion smoothness. Such results conform with our original purpose to create method for improving temporal consistency while preserving imaging quality and motion smoothness. Additionally we conduct user study to prove the effectiveness of our approach regarding Text Alignment, Overall Quality, and Smooth And Dynamic Motion, further explained in Appendix C. Regarding computational efficiency, iterative initial noise refinement in prior work (Wu et al., 2023) requires performing DDIM sampling for multiple iterations, resulting in high computational cost. In contrast, our method only introduces small number of additional sampling steps. This difference leads to significant reduction in inference time, yielding 1.8 2.5 improvement in generation speed for AnimateDiff and 2.1 3.1 improvement for Lavie as shown in Tab. 2. Method FreeInit Ours (self-guided) Ours (VC-guided) AnimateDiff 51.88 21.02 29.22 LaVie 28.18 8.99 13.43 Table 2: Inference time for video generation(s). Both the self-guided case and the VideoCrafter2guided case show significant decrease in inference time. Bold: best, underline: second best. 7 Preprint. Figure 3: Qualitative Comparison. VideoGuide is applied on various base models for different text prompts. For each prompt, frames of generated samples from four different models are displayed: (i) Base model (first row); (ii) Base model with FreeInit (second row); (iii) Base model with VideoGuide (self-guided case) (third row); (iv) Base model with VideoGuide (external modelguided case) (fourth row). AD, VC, LV indicate guidance models of AnimateDiff, VideoCrafter-2.0, LaVie, respectively. Samples for the base model show substandard temporal consistency, especially regarding color fluctuation and subject appearance change. Applying FreeInit improves consistency but introduces degradation in imaging quality, such as smoothing out of textural details. In contrast, applying VideoGuide significantly enhances temporal consistency while preserving imaging quality, both for the self-guided case and the external model-guided case. 8 Preprint. Figure 4: (a) The interpolation process between denoised samples from the sampling model (S) and the guiding model (G) for high guidance scale = 7.5 is shown. (b) The interpolation process for low guidance scale = 0.8 is shown. Both interpolations are performed at = 980 and β = 0.7. Results indicate that with high guidance scale w, influence of the guiding diffusion model is significantly reduced due to color saturation. Interpolation Scale β Interpolation Step Number Guidance Step Number τ SC BC SC β = 0.9 0. 0.9599 = 1 0.9524 0.8 0.7 0. 0.5 0.9546 0.9609 0.9576 0.9628 0. 0.9649 0.9614 0.9664 2 3 5 0.9489 0.9546 0.9602 BC 0. 0.9588 0.9612 0.9645 SC BC τ = 0.9444 0.9558 3 5 7 0. 0.9611 0.9582 0.9641 0.9611 0.9658 0. 0.9664 10 0.9614 0.9664 Table 3: Ablation study regarding interpolation scale β, number of interpolation steps I, and number of guidance sampling steps τ . Subject consistency (SC) and background consistency (BC) is compared for various parameters. Bold: best, underline: second best."
        },
        {
            "title": "5 ANALYSIS",
            "content": "5.1 ABLATION STUDY Importance of Guidance Scale w. Recent study (Chung et al., 2024) demonstrates that employing high CFG scale (w > 1.0) in the early timesteps of diffusion sampling leads to off-manifold behavior. This phenomenon results in denoised samples exhibiting problems such as color saturation and abrupt transitions, which negatively affect the interpolation between samples during these timesteps. We solve this by applying lower guidance scale during the early stages of sampling, ensuring smoother interpolation between the denoised samples. As illustrated in Fig. 4 (a), when using high CFG scale (w = 7.5), the influence of the guiding diffusion model becomes minimal due to significant color saturation, making it difficult for the output of the guiding model to be reflected effectively. In contrast, as illustrated in Fig. 4 (b), lower CFG scale (w = 0.8) facilitates smoother interpolation between the sampling diffusion model and the guiding diffusion model. This highlights the importance of clean interpolation in our method, as improper guidance can lead to sub-optimal performance. Further analysis about CFG and CFG++ can be found in Appendix B. Parameter Selection. An analysis is performed to assess how varying parameters of the guiding diffusion model impacts temporal consistency. Specifically, we examine the effects of three factors: interpolation scale β, number of interpolation steps I, and number of guidance sampling steps τ . 9 Preprint. Figure 5: Prior Distillation Results. VideoGuide solves degraded performance regarding text coherency by enabling the utilization of superior data prior. Example results for certain ambiguous prompts are displayed. For each prompt, the same random seed is shared for both methods. AnimateDiff directs generation of beetle and jaguar towards car samples due to substandard data prior. Using VideoGuide, users can distill superior prior for correct generation. Temporal consistency is evaluated for both Subject Consistency (SC) and Background Consistency (BC). To secure efficient sampling time, we limit maximum values to τ = 10 and = 5. Our ablation studies prove that all three parameters are closely related to temporal consistency. Decrease in interpolation scale β, which is analogous to increase in the influence of the guiding diffusion model, leads to improved subject and background consistency. Note that the minimum value of β is constrained to 0.5 to mitigate the risk of distribution shift. Increasing the number of interpolation steps also leads to improvement in temporal consistency, which proves that our interpolation scheme is indeed effective. Furthermore, increasing the number of guidance sampling steps τ enhances consistency, indicating that blending intermediate latents with better-denoised versions enhances overall consistency as expected (i.e., z0tτ zr). Such ablation study highlights the trade-off between consistency improvement and computational efficiency, offering insight into optimal parameter settings for the guiding diffusion model. 5.2 PRIOR DISTILLATION Degraded performance due to substandard data prior is an issue only solvable through extra training. However VideoGuide provides workaround to this matter by enabling the utilization of superior data prior. Fig. 5 demonstrates example cases. For all instances, generated samples are guided towards result of better text coherence while maintaining the style of the original data domain. Additional examples of prior distillation are provided in Appendix E."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced VideoGuide, novel and versatile framework that enhances the temporal quality of pretrained text-to-video (T2V) diffusion models without the need for additional training or fine-tuning. Our approach provides temporally consistent samples to intermediate latents during the early stages of the denoising process, guiding the low frequency components of latents towards direction of high temporal consistency. The samples provided are not confined to the base model; any superior pretrained VDM can be selected for distillation. By doing so, we empower underperforming models with improved motion smoothness and temporal consistency while maintaining their unique traits and strengths, including personalization and controllability. We demonstrate the effectiveness of VideoGuide on various base models, and prove its ability to enhance temporal consistency without sacrifice of imaging quality or motion smoothness compared to prior methods. The potential of VideoGuide extends far beyond the cases discussed, as VideoGuide ensures that even existing models can remain relevant and competitive by leveraging the strengths of superior models. As video diffusion models continue to evolve, new and emerging VDMs will only enhance the pertinence of VideoGuide over time, broadening the scope of VDMs utilizable as video guide. 10 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. Cfg++: Manifoldconstrained classifier free guidance for diffusion models. arXiv preprint arXiv:2406.08070, 2024. Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models, 2024. URL https://arxiv.org/abs/2305.10474. Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. arXiv preprint arXiv:2311.16933, 2023. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. International Conference on Learning Representations, 2024. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv:2211.13221, 2022. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. URL https://openreview.net/ forum?id=qw8AKxfYbI. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models, 2022. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Gu Jiaxi, Wang Shicong, Zhao Haoyu, Lu Tianyi, Zhang Xing, Wu Zuxuan, Xu Songcen, Zhang Wei, Jiang Yu-Gang, and Xu Hang. Reuse and diffuse: Iterative denoising for text-to-video generation. arXiv preprint arXiv:2309.03549, 2023. Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 51485157, 2021. Jeongsol Kim, Geon Yeong Park, and Jong Chul Ye. Dreamsampler: Unifying diffusion sampling and score distillation for image manipulation. arXiv preprint arXiv:2403.11415, 2024. Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling, 2023. 11 Preprint. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models, 2021. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR, 2021a. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In 9th International Conference on Learning Representations, ICLR, 2021b. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, and Ziwei Liu. Freeinit: Bridging initialization gap in video diffusion models. arXiv preprint arXiv:2312.07537, 2023. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv:2310.12190, 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 12 Preprint."
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "A.1 PROMPT SELECTION In all experiments, we utilize 800 prompts from various categories in VBench (Huang et al., 2024) to evaluate the models ability to generate across diverse categories. A.2 HYPERPARAMETER SELECTION We employ classifier-free guidance (CFG) scale of 7.5 during inference for both base models (AnimateDiff, LaVie) and FreeInit-applied cases. During interpolation of the denoised samples, we apply CFG++ reverse sampling with guidance scale of = 0.8 in DDIM 50-step sampling. After completing the interpolation step, we revert to CFG reverse sampling with CFG scale of 7.5. In FreeInit, we use Butterworth filter with normalized frequency of 0.25, filter order = 4, and perform 5 iterations, as recommended in prior work. The same filter is applied in our experiments with FreeInit. For AnimateDiff, we configure the guiding model with parameters = 5, β = 0.5, and τ = 10. In the case of LaVie, we set = 3, β = 0.5, and τ = 10 to optimize inference speed. Additionally, the τ intervals are not uniformly spaced as in the standard 50-step DDIM sampling. To better leverage temporally consistent samples, we divide the remaining interval into 25 steps for reverse sampling during guidance steps. Also, we found that applying renoising to guidance sampling is more effective in improving consistency in the case of self-guidance. Therefore, we incorporated renoising during self-guidance in similar manner as when using external model for guidance. A.3 FIGURE EXPLANATION Base models used for Figure 3: (a) AnimateDiff with pretrained T2I model RealisticVision. (b) AnimateDiff with pretrained T2I model RealisticVision. (c) AnimateDiff with pretrained T2I model ToonYou. (d) AnimateDiff with pretrained T2I model FilmVelvia. (e) LaVie. (f) LaVie. Base model used for Figure 5: AnimateDiff with pretrained T2I model ToonYou. QUANTITATIVE ANALYSIS OF CFG AND CFG++ There may be concerns that the effectiveness of our method in improving consistency stems from the use of the CFG++ algorithm itself. To address this, we provide results for using CFG and CFG++ across the Base Model, Base Model + FreeInit, and Base Model + VideoGuide. The results demonstrate that CFG++ is particularly effective for interpolation. As shown in Tab. 4, metrics for Base and FreeInit decrease when CFG++ is used, and metrics improve only when CFG++ is applied to our interpolation scheme. This implies the significant positive impact on consistency of CFG++ within the proposed interpolation scheme, especially compared to CFG. Also, this supports the idea, as discussed earlier in Sec. 5.1, that smooth interpolation of denoised samples positively impacts model performance. Metrics Base FreeInit Ours CFG CFG++ CFG CFG++ CFG Interp. CFG++ Interp. Subject Consistency () 0. 0.9176 0.9487 0.9473 Background Consistency () 0.9437 0. 0.9604 0.9604 0.9598 0.9635 0.9614 0. Table 4: Comparison of consistency metrics between CFG and CFG++ in AnimateDiff. Results indicate that interpolating denoised samples with CFG++ has larger impact on improving both subject and background consistency. 13 Preprint."
        },
        {
            "title": "C USER STUDY",
            "content": "We conduct user study to evaluate generated video samples using three criteria: Text Alignment, Overall Quality, and Smooth And Dynamic Motion, with all metrics scored on 1 to 5 scale. total of 30 participants provided ratings for each metric, offering comprehensive feedback on the generated videos. Text Alignment Measures how well the video corresponds to the prompt, focusing on semantic coherence. Question: Do you think the videos reflect the given text condition well? (5: Strongly Agree / 4: Agree / 3: Neutral / 2: Disagree / 1: Strongly Disagree) Overall Quality Assesses the videos visual consistency, image degradation, and aesthetic appeal. Question: Do you think the videos overall quality is good? (rich detail, unchanging objects) (5: Strongly Agree / 4: Agree / 3: Neutral / 2: Disagree / 1: Strongly Disagree) Smooth And Dynamic Motion Evaluates the naturalness and fluidity of the motion in the video. Question: Do you think the videos overall motion is smooth and dynamic? (5: Strongly Agree / 4: Agree / 3: Neutral / 2: Disagree / 1: Strongly Disagree) Method Base Base + FreeInit Base + VideoGuide (Ours) Text Alignment Overall Quality Smooth And Dynamic Motion 3.72 3.97 4.36 2.84 3. 4.37 2.9 3.38 4.36 Table 5: User Study. Bold: best, underline: second best. Tab. 5 shows that our method surpasses the baseline and previous work in all evaluated aspects."
        },
        {
            "title": "D PSEUDO CODE",
            "content": "Pseudo codes regarding our algorithm are provided in the following page."
        },
        {
            "title": "E MORE QUALITATIVE EXAMPLES",
            "content": "Additional samples are provided in following pages: Supplemental examples of prior distillation. Qualitative comparison for various base models. Usage of VideoGuide to solve sudden frame shifts in LaVie samples. 14 Preprint. Algorithm 1 VideoGuide with Sampling Diffusion Model ˆϵθ(zt, t) = ϵθ(zt, t, ϕ) + λ[ϵθ(zt, t, c) ϵθ(zt, t, ϕ)] z0t = (zt zt = αtz0t + if < then Require: guidance scale λ [0, 1], guiding steps I, interpolation scale β, extra step τ 1: Initialize zT (0, I) 2: for = T, . . . , 1 do 3: 4: 5: 6: 7: 8: 9: 10: αtj1z0tj + (cid:112)1 αtj1ϵθ(ztj, j, ϕ) αt 1 αtϵ, where for = 0, . . . , τ do 1 αt ˆϵθ(zt, t))/ ϵ (0, I) ztj1 = end for 0t = β z0t + (1 β) z0tτ zt1 = zt1 = LPFγ(zt1) + HPFγ(ϵ), where 1 αt1ϵθ(zt, t, ϕ) αt1z 0t + ϵ (0, I) αt1z0t + 1 αt1ϵθ(zt, t, ϕ) else 11: 12: 13: 14: end if 15: 16: end for 17: Output: Final video z0 zt1 = Algorithm 2 VideoGuide with Guiding Diffusion Model Require: guidance scale λ [0, 1], guiding steps I, interpolation scale β, extra step τ , Guiding Model parameterized by ψ, noise schedule α(G) of 1: Initialize zT (0, I) 2: for = T, . . . , 1 do 3: 4: ˆϵθ(zt, t) = ϵθ(zt, t, ϕ) + λ[ϵθ(zt, t, c) ϵθ(zt, t, ϕ)] z0t = (zt z(G) = if < then 1 αt ˆϵθ(zt, t))/ (cid:113) 1 α(G) ϵ, where α(G) z0t + αt (cid:113) ϵ (0, I) (cid:113) 1 α(G) tjˆϵψ(z(G) (cid:113) for = 0, . . . , τ do 0tj = (z(G) z(G) tj (cid:113) z(G) tj1z(G) α(G) tj1 = end for 0t = β z0t + (1 β) z(G) zt1 = zt1 = LPFγ(zt1) + HPFγ(ϵ), where 0tj + αt1z 1 αt1ϵθ(zt, t, ϕ) 1 α(G) 0t + 0tτ (cid:113) tj, j)/ α(G) tj tj1ϵψ(z(G) tj, j, ϕ) ϵ (0, I) 5: 6: 7: 8: 9: 10: 11: else 12: 13: 14: 15: end if 16: 17: end for 18: Output: Final video z0 zt1 = αt1z0t + 1 αt1ϵθ(zt, t, ϕ) 15 Preprint. Figure 6: Prior Distillation. For each prompt, we share the same random seed for both methods. 16 Preprint. Figure 7: More Qualitative Results of VideoGuide on AnimateDiff (with RealisticVision). 17 Preprint. Figure 8: More Qualitative Results of VideoGuide on AnimateDiff (with RealisticVision). 18 Preprint. Figure 9: More Qualitative Results of VideoGuide on AnimateDiff (with ToonYou). 19 Preprint. Figure 10: More Qualitative Results of VideoGuide on AnimateDiff (with RCNZCartoon). 20 Preprint. Figure 11: More Qualitative Results of VideoGuide on AnimateDiff (with FilmVelvia). 21 Preprint. Figure 12: More Qualitative Results of VideoGuide on LaVie. 22 Preprint. Figure 13: More Qualitative Results of VideoGuide on LaVie. 23 Preprint. Figure 14: VideoGuide helps solve the issue of sudden frame shifts in LaVie samples. By integrating an external guiding model, VideoGuide provides smoother frame transitions to the base model. LV indicates that guidance model of LaVie is used (the self-guided case), and VC indicates that guidance model of VideoCrafter2 is used. Guidance given with the external model VideoCrafter2 solves sudden frame shift unsolvable by other methods."
        }
    ],
    "affiliations": [
        "KAIST"
    ]
}