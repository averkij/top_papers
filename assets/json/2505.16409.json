{
    "paper_title": "FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS",
    "authors": [
        "Chaeeun Kim",
        "Seungone Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in multi-step reasoning and calling search engines at appropriate steps. However, existing retrieval-augmented reasoning approaches rely on separate retrieval models, limiting the LRM's role in retrieval to deciding when to retrieve and how to query. This separation not only increases hardware and operational costs but also leads to errors in the retrieval process due to the representation bottleneck, a phenomenon where the retriever's embedding space is not expressive enough to meet the generator's requirements. To address this, we shift our perspective from sequence-to-sequence matching to locating the answer-containing paths within the corpus, and propose a novel framework called FREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables LRMs to retrieve relevant knowledge on their own by acting as both a generator and retriever. To achieve this, we introduce a variant of the MCTS algorithm specialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing Monte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus toward answer-containing regions. Our results on five open-domain QA benchmarks, including single-hop and multi-hop questions, show that FREESON achieves an average improvement of 14.4% in EM and F1 over four multi-step reasoning models with a separate retriever, and it also performs comparably to the strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 9 0 4 6 1 . 5 0 5 2 : r FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS Chaeeun Kim LBOX chaeeun@lbox.kr Seungone Kim Carnegie Mellon University seungone@cmu.edu"
        },
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in multi-step reasoning and calling search engines at appropriate steps. However, existing retrieval-augmented reasoning approaches rely on separate retrieval models, limiting the LRMs role in retrieval to deciding when to retrieve and how to query. This separation not only increases hardware and operational costs but also leads to errors in the retrieval process due to the representation bottleneck, phenomenon where the retrievers embedding space is not expressive enough to meet the generators requirements. To address this, we shift our perspective from sequence-to-sequence matching to locating the answer-containing paths within the corpus, and propose novel framework called FREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables LRMs to retrieve relevant knowledge on their own by acting as both generator and retriever. To achieve this, we introduce variant of the MCTS algorithm specialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing Monte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus toward answer-containing regions. Our results on five open-domain QA benchmarks, including single-hop and multi-hop questions, show that FREESON achieves an average improvement of 14.4% in EM and F1 over four multi-step reasoning models with separate retriever, and it also performs comparably to the strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA."
        },
        {
            "title": "Introduction",
            "content": "Retrieval-Augmented Reasoning (RAR) is widely used framework to reduce hallucinations and generate more factual responses by injecting external knowledge into the reasoning chain [1, 2, 3, 4, 5, 6, 7, 8, 9]. In such pipelines, external knowledge is crucial for guiding subsequent reasoning steps. However, conventional search enginestypically based on dual-encoder architecturesoften suffer from inherent limitations, failing to retrieve appropriate documents due to an representation bottleneck, where embedding vectors cannot sufficiently represent subtle distinctions between documents or their relevance to the question [10, 11, 12]. For example, given the query Where was the place of burial of John Tuchet, 6th Baron Audleys father?, E5base [13] (a state-of-the-art retriever) assigns higher similarity scores to John Tuchet, 8th Baron Audley and George Tuchet, 9th Baron Audley (both incorrect) than to John Tuchet, 6th Baron Audley (correct), retrieving an incorrect document in the first hop due to single-character difference, and consequently failing to reach the final answer document, James Tuchet, 5th Baron Audley. To address this issue, prior works have proposed better representation learning methods, or scaling up either the model size or the amount of training data to train retrieval models [14, 15, 13, 16, 17, 18]. However, fundamentally resolving the representation bottleneck remains challenging due to the nature of the architecture. In addition, maintaining two separate models introduces additional hardware Preprint. Under review. Figure 1: Overview of retrieval-augmented reasoning process. Left: FREESON performs both reasoning and retrieval using single generator model via token-level CT-MCTS. Right: conventional RAR methods compute similarity scores between query and document embeddings using separate retrieval model. FREESON fully leverages the LRMs reasoning over document structure, enabling precise access to relevant information from the corpus at inference time without relying on memorization. overhead and operational costs [19, 20]. In this paper, we revisit the conventional retrieval-augmented paradigm and pose question: Can single LRM autonomously identify the knowledge it needs from corpus while performing reasoning, without relying on separate retriever? To address this question, we shift our focus from sequence-to-sequence matching to locating the answer-containing paths within the corpus for retrieval tasks and propose novel framework called FREESON (Retriever-FREE Retrieval-Augmented ReaSONing). In this framework, single LRM functions as both generator and retriever, traversing the corpus to obtain information it needs. To implement this, we first introduce CT-MCTS (Corpus-Traversing Monte Carlo Tree Search), retrieval-specialized MCTS [21, 22, 23, 24, 25], which defines its search nodes at the token level, allowing each node to represent prefix of one or more tokens, where the prefix is constrained by predefined prefix-based index to ensure that the search follows only sequences that exist in the corpus ( 2.2). Implementing such retrieval-oriented MCTS introduces two key challenges: (1) the search operates at extremely fine granularity, with token-level nodes, making it difficult to capture meaningful semantics due to step-wise constraints; and (2) the model must obtain appropriate node value estimates to guide the search toward the desired location in the corpus that contains the answer. To address the first challenge, we increase node granularity while preserving single-token-level constraints, and incorporate stochastic beam search into the expansion process to better leverage the LRMs capability during path selection. For the second challenge, we train an on-policy value network to estimate answer-containment, using CT-MCTS rollouts in environments aligned with actual inference-time scenarios ( 4.4). We evaluate FREESON on five open-domain QA benchmarks comprising of single-hop and multihop questions. On average, FREESON achieves 14.4% improvement in EM and F1 compared to four reasoning models using separate retriever in their reasoning pipeline. It also performs on par with the strongest baseline, Search-R1, surpassing it by average 3% on PopQA and 2WikiMultihopQA ( 3.3). Our retrieval-specialized CT-MCTS plays key role in this performance. Flexible node granularity yieds 27.5% gain over single-token nodes ( 4.2), and multi-node expansion guided by the LRM improves performance by 13% over single-node expansion ( 4.3). FREESON does not require any training and is applicable to an arbitrary LMs when we could access the output logit values. Particularly, it is well suited for domain-specific applications that have unlabeled corpus, as it directly explores and reasons over the content without using any external search engine."
        },
        {
            "title": "2 Methodology",
            "content": "FREESON is model capable of functioning as both generator and retriever, built upon SearchR1 [5], retrieval-augmented reasoning model trained with PPO [26] and GRPO [27]. In this section, we detail how FREESON autonomously identifies and retrieves the knowledge it needs. See Figure 1 for an illustration of this process."
        },
        {
            "title": "2.1 Dynamically Adapted Search Space",
            "content": "When search is required, FREESON first generates query in the form of (subject: [subject], question: [question]), where [subject] is the entity or proper noun the question is mainly about, and [question] is the corresponding question. Then, based on the subject information, we adaptively construct prefix-based index we call CorpusTree that narrows the search space by limiting the range of possible next valid tokens. We then input the question using few-shot prompt that guides reasoning over document structure. The detailed prompt description is provided in Appendix D.2."
        },
        {
            "title": "2.2 CT-MCTS for Autonomous Retrieval",
            "content": "Given question and subject-specific CorpusTree, FREESON locates the appropriate information using CT-MCTS designed to reinforce paths that are likely to contain the correct answer. Basically, CT-MCTS operates in token-level search space where the LLMs probability distribution is dynamically masked by the CorpusTree, allowing only valid sequences found in the corpus. The CorpusTree is implemented using an FM-Index [28, 29] based on the Burrows-Wheeler Transform (BWT) [30], which enables efficient and compressed prefix-constrained search. In the following, we describe the key components of CT-MCTS. Selection. The first step in each simulation is to select node from the search tree for exploration. To do so, we employ the widely used selection function, UCT [21, 31]. Starting from the root node, we recursively select the child node CorpusTree(s) that maximizes the UCT score at the current node s, until leaf node is reached. (cid:34) = arg max Q(s, a) + λ (cid:115) (cid:35) log (cid:80) 1 + (s, a) (s, b) Here, Q(s, a) is the average value of taking action from node s, (s, a) is the number of times action has been selected from node s, and λ is scalar balancing exploration and exploitation. The action space is constrained by CorpusTree according to the expansion process described below. Granularity-aware multi-node expansion. The second step is to expand the selected node by determining the promising next search directions. Our expansion approach differs from conventional MCTS in two key ways. (1) Expanding Nodes Granularity: Each node in our search tree contains sequence of tokens (length G), rather than single token (e.g., 5th Baron Audley instead of 5th; see Figure 1). This allows the model to make more context-aware and semantically meaningful decisions at each step, while also enabling faster search by traversing multiple steps in single move. This allows the reasoning model to contribute more actively to the expansion process. Through this, we strengthen the effectiveness of CorpusTree-guided search while still adhering to token-level constraints for retrieval ( 4.2). (2) Multi-node expansion per simulation: Rather than expanding single node per simulation, we expand the candidate children based on the LRMs next-token probabilities (e.g., , 3rd baron and : john baron are expanded in single simulation; see Figure 1). This allows the model to better utilize the LRMs outputs during expansion, resulting in substantial performance gains ( 4.3). To enable the two features, we employ stochastic beam search decoding. Let As = CorpusTree(s) denote the set of valid next tokens for the current selected path s, and As = {a1, . . . , ak} As be the top-k tokens ranked by log-probability. Final candidates are sampled from As using multinomial sampling. For each candidate, we iteratively extend the sequence by sampling tokens until either 3 predefined per-node token limit is reached or no valid tokens remain (As = ), while maintaining the top-M paths ranked by cumulative log-probability at each step. Evaluating answer presence in search trajectories. For each newly expanded node, we perform rollout to evaluate its value using an answer presence-aware value network. The rollout follows greedy decoding process, guided by CorpusTree at each step, and continues until either the maximum sequence length is reached or no valid tokens remain. Once the rollout terminates, the resulting path sfinal is evaluated by the value network V, which takes as input prompt-style sequence consisting of the question and sfinal, and outputs value estimate. The detailed prompt format is provided in Appendix D.3. The value is computed as: ˆy = σ (R(pool(f (x)))) where (x) denotes the decoders final hidden states used as input to the value head, pool() extracts the hidden state of the last token, and σ is the sigmoid function. The output scalar ˆy [0, 1] serves as the value signal, which is used to update the statistics Q(s, a) and (s, a) during the backpropagation phase."
        },
        {
            "title": "2.3 Training the Value Network",
            "content": "To evaluate whether candidate path contains the information necessary to answer the question, we train value networks on rollouts from CT-MCTS and synthetic paths generated by LLM. In our experiments, evaluation is performed using the former. Each result is described in Section 4.4. On-policy training on CT-MCTS rollouts. In this on-policy approach, we directly leverage intermediate rollouts collected during CT-MCTS execution. At each expansion step, we pause and return to the original reasoning process, feeding the current candidate path and the question into the model. The model then generates an answer, which is compared to the ground-truth to assign soft value: 1.0 for full match, 0.8 for partial match, and 0.0 if there is no match. The value network is implemented by attaching classification head to the frozen backbone of the original reasoning model and trained using binary cross-entropy loss. Training is performed on approximately 15,000 such rollout-label pairs obtained from 400 PopQA [32] examples. Off-policy training on synthetic trajectories. In this off-policy setup, we generate diverse synthetic retrieval paths and corresponding value scores using GPT-4o on the 2WikiMultihopQA dataset [33]. To simulate realistic trajectories, we construct three paths per query, varying in length, relevance, and whether the final answer is entailed. Detailed prompt is in Appendix D.4. We input each query paired with its evidence sentences and generate three retrieval paths with corresponding value scores, resulting in total of 147,755 pathvalue pairs. We then train classification head on top of the frozen backbone of the original reasoning model."
        },
        {
            "title": "2.4 Document Selection From Retrieved Paths",
            "content": "After obtaining multiple paths through CT-MCTS, we must determine how to present the identified references for downstream reasoning. We consider three possible strategies for document selection: Direct Path: providing only the exact retrieved path spans. Window Expansion: extending retrieved paths with surrounding context windows. Complete Document: returning the complete documents from which the retrieved spans originate. In this work, we implement the Complete Document approach. While direct path or window expansion may offer compact references, they risk omitting potentially important information that lies outside the selected regions or fragmenting coherent explanations. By supplying complete documents, we alleviate potential information loss. Unlike dual-encoder models that always retrieve the predefined top-k documents based on similarity scores, FREESON retrieves only documents containing the search trajectories, reducing the possibility of including noisy or irrelevant information in the retrieved content (see 3. Document Selection of Figure 1). General QA Multi-hop QA Method PopQA TriviaQA HotpotQA 2WikiMultihopQA MuSiQue EM F1 EM F1 EM F1 EM F1 EM F1 Reasoning w/o retrieval Qwen2.5-7B 0.09 R1-Distill-Qwen-7B 0.07 0.11 0.10 0. 0.13 0.31 0.17 0.13 0.11 0. 0.15 0.19 0.18 0.23 0.20 0. 0.01 0.06 0.03 Retrieve-then-reasoning E5 + Qwen2.5-7B 0. 0.16 0.15 0.19 0.10 0.15 0. 0.23 0.02 0.05 Multi-step reasoning with separate retrievers FLARE Self-Ask Search-o1 Search-R1 0.20 0.21 0.13 0. 0.28 0.24 0.15 0.39 0.29 0. 0.36 0.54 0.41 0.45 0.43 0. 0.21 0.18 0.19 0.40 0.28 0. 0.25 0.53 0.27 0.22 0.09 0. 0.32 0.28 0.12 0.61 0.06 0. 0.03 0.12 0.14 0.09 0.10 0. Multi-step reasoning and autonomous retrieval FREESON (Ours) 0.39 0.43 0.51 0. 0.31 0.42 0.55 0.63 0.11 0. Table 1: Overall performance on single-hop and multi-hop QA. Bold indicates the best result, and underline indicates the second-best. All models in this table are based on 7B LMs. FREESON achieves its performance using single LRM without requiring additional retrieval hardware. Groundtruth labels in benchmarks constructed using existing retrieval engines may reflect their biases, potentially undervaluing FREESONs correct answers (see Appendix C)."
        },
        {
            "title": "3.1 Benchmarks",
            "content": "We evaluate the effectiveness of FREESON on five datasets of knowledge-intensive QA tasks, including single-hop and multi-hop questions. See Appendix 5 for detailed dataset and retrieval settings and Appendix for the prompts used in each model. We evaluate the performance using EM and F1 metrics. General QA: (1) POPQA [32], dataset constructed of factual questions centered on entities extracted from Wikipedia pages with high view counts. (2) TRIVIAQA [34], dataset containing complex and factoid questions collected from trivia websites and evidence passages from web documents. Multi-hop QA: (1) HOTPOTQA [35], the first multi-hop QA benchmark, which consists of questions that require reasoning over multiple Wikipedia paragraphs, and includes sentence-level supporting facts. (2) 2WIKIMULTIHOPQA [33], dataset where each question requires reasoning over two distinct Wikipedia pages corresponding to different entities, encouraging cross-page inference. (3) MUSIQUE [36], dataset containing 2-4 hop questions, requiring complex reasoning [37], curated to test compositional reasoning over multiple evidence sentences with reduced lexical overlap between questions and supporting contexts."
        },
        {
            "title": "3.2 Baselines",
            "content": "We evaluate our FREESON against range of baselines, including reasoning without retrieval that reasons only relying on its parametric knowledge, retrieve-then-reasoning that retrieves relevant documents first, then reasons about questions with the documents, and multi-step reasoning with separate retrievers that performs step-by-step reasoning along with retrieval, calling external search engines. Below are the methods used for multi-step reasoning with separate retriever. 5 (1) FLARE [1] generates reasoning steps and triggers retrieval when any token has low confidence, using look-ahead next step as the retrieval query. (2) SELF-ASK [2] employs scaffolded reasoning approach by generating sub-questions and corresponding intermediate answers to build the final answer. (3) SEARCH-O1 [4] performs reasoning with interleaved retrieval and uses separate Reason-inDocuments module when injecting retrieved documents into the reasoning chain to provide more accurate information. (4) SEARCH-R1 [5] is trained through reinforcement learning (e.g., PPO, GRPO) with retrieved-token masking to acquire the ability to interact with search engines during reasoning."
        },
        {
            "title": "3.3 Main Results",
            "content": "Table 1 presents FREESON performance on single-hop and multi-hop QA benchmarks. Comparison with Retrieve-then-reasoning. We observe that E5 + Qwen2.5-7B, which performs single retrieval step before generation, improves performance on PopQA, where most questions can be answered with single piece of evidence. This shows that even one-time retrieval can help in single-hop settings. However, on multi-hop QA, it does not bring meaningful gains, indicating that single-step retrieval is insufficient when multiple reasoning steps are required. In contrast, our method, FREESON, performs retrieval at each reasoning step and achieves 2 - 3 higher performance. This demonstrates the clear advantage of performing step-wise retrieval, aligned with each reasoning step when needed, for complex and multi-hop questions. Comparison with Multi-step reasoning with separate retrievers. Our primary focus is on how effectively retrieval-augmented reasoning can be performed using fully retriever-free approach. Our results show that FREESON achieves an average gain of +14.4% over four baseline models that use external retrievers during their multi-step reasoning. Specifically, it outperforms these baselines by +16.6% on PopQA, +13.5% on TriviaQA, +7.6% on HotpotQA, +28.4% on 2WikiMultihopQA, and +5.9% on MuSiQue. These results underscore that LRMs can obtain necessary knowledge without external retrieval models, by treating retrieval as path-finding process, rather than relying on conventional embedding-based similarity search. We have observed certain limitations in QA dataset annotations, which may lead to slight underestimation of FREESONs performance. As discussed in Appendix C, we observed that ground-truth answers in some QA datasets often align with expressions found in documents retrieved by systems like E5, which may introduce some bias during evaluation."
        },
        {
            "title": "4.1 Why CT-MCTS over other decoding strategies?",
            "content": "We compare three decoding algorithmsgreedy search, beam search, and CT-MCTSin Table 2. The results show consistent ranking under constrained decoding with prefix-based index for retrieval: CT-MCTS > beam search > greedy search. Constrained decoding is inherently dependent on previous decoding steps. Once the decoding path diverges in the wrong direction, it becomes impossible to recover. This makes greedy search particularly vulnerable in such environments. Beam search considers more candidates, but it remains deterministic and often suffers from early commitment, especially to the first token. PopQA 2WikiMultihopQA Decoding EM EM Greedy Search Beam Search CT-MCTS 0.17 0. 0.44 0.21 0.21 0.45 0.21 0. 0.54 F1 0.23 0.25 0.60 Table 2: Comparison of decoding strategies. CTMCTS flexibly explores, unlike deterministic methods that struggle to recover from early errors. 6 Figure 2: Impact of node granularity on system efficiency and performance (1 10 tokens per node). Left: Latency (s) sharply decreases with higher granularity, enabling faster search. Middle: As granularity increases, the retrieved path becomes longer (more informative), while the rollout count remains similarindicating more efficient search per computation. Right: Higher granularity leads to better performance. In contrast, CT-MCTS is better suited for retrieval-oriented decoding, as it enables more flexible exploration of how target information may be expressed in the corpus. Because CT-MCTS always starts from the root node and explores diverse paths from the very first token, it can more effectively follow the guidance of value network to construct an optimal retrieval trajectory."
        },
        {
            "title": "4.2 What node granularity is most effective for trajectory exploration?",
            "content": "To enable more context-aware decisions during CT-MCTS, we expand node granularity from single token to multiple tokens. Our findings highlight the importance of choosing granularity level that preserves semantic meaning while not hindering the fine-grained adjustments of MCTS. As shown in Figure 2, moderately coarse granularity (G = 6), corresponding to six-token nodes, achieved the best performance (F1 ). This level of granularity allows nodes to capture coherent semantic units, even if they do not correspond to complete linguistic phrases, while also providing significant speed improvements (Latency ) and longer retrieved paths (Avg Path Length ). These benefits arise from providing CT-MCTS with semantically richer units, which help overcome the limitations of token-level search under constrained decoding. In contrast, at finer granularities (e.g., = 1 and = 2), CT-MCTS takes considerably more time, while retrieving shorter and often uninformative paths. Performance also degrades, possibly due to the increased number of simulations needed to identify optimal paths and capture semantic relationships between nodesreflected in the notably high number of leaf nodes in Table 6, indicating scattered and uncertain search behavior. However, overly coarse granularity is not always better. As shown by the performance decline from = 6 to = 10, excessively long nodes could limit fine-grained exploration, ultimately degrading performance. These results underscore the importance of balancing semantic richness and controllability through appropriate node granularity."
        },
        {
            "title": "4.3 Can LLM-driven multi-node expansion boost performance?",
            "content": "To better leverage the LLMs reasoning ability over document expressions during expansion in CTMCTS, we explore expanding multiple candidate child nodes per simulation, selected via multinomial sampling over top-k predictions from the LLM. As shown in Table 3, setting = 2 achieves the best performance, improving EM and F1 scores by 13 and 14 points, respectively, compared to = 1. This improvement stems from the increased involvement of the LLM in selecting the node to expand, providing more informed guidance. However, under constrained decoding, the number of valid next-token candidates is inherently limited. As result, although expanding more candidates is initially beneficial, the performance quickly saturatesas many of the additional nodes are likely to be explored by later simulations anyway. From = 3 onward, no further gains are observed. These findings indicate that even modest increase in LLM involvement can meaningfully improve retrieval quality in CT-MCTS. 7 Performance Efficiency Expanded Node EM F1 Latency M=1 M=2 M=3 M= 0.41 0.54 0.54 0.54 0.46 0. 0.60 0.60 21s 31s 36s 44s Table 3: Effect of the number of expanded nodes (M ) per simulation. On 2WikiMultihopQA, performance improves significantly at =2 and quickly saturates due to constrained token space. PopQA TriviaQA 2WikiMultiHop Avg. Value Network EM F1 EM F1 EM LLM Synthetic Trajectory CS-MCTS Rollout 0. 0.44 0.41 0.45 0.48 0.51 0. 0.63 0.53 0.54 F1 0.59 0. EM F1 0.47 0.50 0.54 0.56 Table 4: Comparison between CT-MCTS and LLM-based value policy. On-policy CT-MCTS performs better by learning value estimates aligned with the inference-time environment."
        },
        {
            "title": "4.4 CT-MCTS on-policy vs. synthetic off-policy: which gives better value estimates?",
            "content": "Assigning an appropriate value to each explored node is crucial for guiding the search toward answercontaining paths. We compare two types of value models: (1) an on-policy model trained from our CT-MCTS rollouts, and (2) an off-policy model trained on LLM-generated samples. As shown in Table 4, value models trained on CT-MCTS rollouts outperform those trained on LLM-generated data. This is likely due to stronger alignment with the actual inference-time behavior of CT-MCTS, as the value model is trained directly on the actions the system would take during real search. Additionally, this on-policy approach is cost-efficient, as it eliminates the need for separate synthetic data generation. These results suggest that training the value network within the true CT-MCTS environment is more effective, and we adopt this strategy in our method. During training, we use an 80GB A100 GPU."
        },
        {
            "title": "5.1 Retrieval-Augmented Generation",
            "content": "Large Language Models (LLMs) have shown remarkable performance in natural language understanding and generation but struggle with hallucinations, particularly in domain-specific applications. To enable more reliable and up-to-date factual generation, Retrieval-Augmented Generation (RAG) addresses this limitation by retrieving relevant documents before generation, significantly improving reliability and factual accuracy [38, 39, 40, 41]. Early approaches such as RAG [39] follow retrievethen-generate framework, where retrieval is performed once before generation. Subsequent research has explored diverse approaches, focusing on what [42, 40], how [43], and when to retrieve [38]. This research direction gradually shifted toward integrating retrieval within the reasoning flow. SelfAsk [2] generates intermediate questions and retrieve information accordingly. FLARE [1] uses look-ahead generation as retrieval query, and Self-RAG [3] enables models to determine when to retrieve and evaluate content autonomously. More recently, with the emergence of Large Reasoning Models (LRMs) such as DeepSeek-R1 [44], OpenAI-o1 [45], and Qwen [46], these ideas have evolved into more sophisticated frameworks. Search-o1 [4] enhances LRMs with an agentic RAG mechanism that retrieves external knowledge when encountering uncertain information and analyzes documents before integration. Search-R1 [5] uses reinforcement learning to train LRMs to generate optimal search queries during reasoning. Methods like ReAct [7] directly interact with external tools. 8 FREESON (Ours) enables retrieval-augmented reasoning with single LRM that serves as both generator and retriever via inference-time retrieval algorithm, thereby eliminating the need for separate retrieval models and their hardware costs."
        },
        {
            "title": "5.2 LLMs with Monte Carlo Tree Search",
            "content": "Monte Carlo Tree Search (MCTS) [21] is search algorithm that iteratively builds search tree through four key steps: select node to explore, expansion of promising childeren nodes, evaluation through rollouts, and backpropagation of results to update node values. Recently, MCTS has been integrated with LLMs to enhance their reasoning capabilities. AlphaMath [23] leverages MCTS to improve mathematical reasoning by treating reasoning steps as nodes and using value model to evaluate partial solutions without requiring human annotations. ReST-MCTS* [25] integrates tree search with process reward guidance to infer per-step values from final answers, enabling higher-quality reasoning traces for self-training. PPO-MCTS [24] shows that using both the value network from PPO training and MCTS during inference improves text generation quality over using only the policy network. To the best of our knowledge, FREESON (Ours) is the first to adapt MCTS for retrieval tasks, introducing CT-MCTS, retrieval-specialized algorithm that enables single LRM to traverse the corpus with flexible multi-token node granularity while expanding multiple nodes per simulation and employing on-policy value estimation to locate answer-containing paths within corpus."
        },
        {
            "title": "5.3 Generative Information Retrieval",
            "content": "Generative Information Retrieval (GenIR) [47] refers to retrieval methods incorporating autoregressive language models into information retrieval problems. GenIR approaches are categorized by their document identifier format, the output form of generated content. These methods employ constrained decoding with prefix-tree structures to navigate predefined document identifiers. Some approaches generate document titles [48], while others sequentially generate document IDs [49, 10, 50, 51]. Methods like SEAL [29] directly generate document content spans, later enhanced by MINDER [52] and LTRGR [53] through pseudo-queries and pseudo-titles integration. Beyond static retrieval, dynamic environments have been explored in DSI++ [54], Corpusbrain [55], and DynamicIR [11]. Recent research introduced end-to-end LLM-driven architectures unifying all IR functions within single model by internalizing the corpus through self-supervised learning [56]. FREESON (Ours) is framework that unifies generator and retriever in single model without requiring additional training for memorization while fully leveraging the generators retrieval capability by traversing the corpus at inference time using retrieval-specialized MCTS. It is fundamentally based on generative information retrieval approach with content-generation document identifiers."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we revisit the conventional retrieval-augmented approach that relies on separate retrieval models and propose FREESON, method where single model can function as both the generator and retriever without further training for corpus memorization. To achieve this, we introduce retrieval-specific MCTS algorithm called CT-MCTS, which allows LMs to directly navigate the corpus toward answer-containing regions with more context-aware node selection and and increased LRM-involvement in MCTS exploration. Our experimental results show that this retriever-free retrieval-augmented reasoning approach achieves promising performance without the need for external search engines, showing an improvement of 14.4% over the average of four baselines across five benchmarks and 3% average gain over the strongest baseline model on two datasets. Our work highlights that LLMs can perform retrieval tasks by finding optimal paths within the corpus to answer questions."
        },
        {
            "title": "7 Limitations",
            "content": "Our method is particularly well-suited for scenarios with predefined corpus. In QA tasks where the corpus is not predefined, our method may be less effective compared to web-based retrieval 9 systems, which can flexibly access broader and more diverse range of information, even though. Although the development of large-scale corpora, such as MassiveDS-1.4T/140B [57], helps mitigate this limitation, efficiently handling them at scale remains challenge. Furthermore, our current inference-time algorithm is not explicitly optimized for reasoning over document structure. Incorporating reinforcement learning techniques such as PPO to optimize the LRMs traversal over retrieval candidates could enable more adaptive and retrieval-efficient behavior."
        },
        {
            "title": "References",
            "content": "[1] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation, 2023. [2] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2023. [3] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection, 2023. [4] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models, 2025. [5] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning, 2025. [6] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning, 2025. [7] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. [8] Zhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen, Yu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang, Weinan E, Linpeng Tang, and Wentao Zhang. Rare: Retrieval-augmented reasoning modeling, 2025. [9] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023. [10] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Allen Sun, Weiwei Deng, Qi Zhang, and Mao Yang. neural corpus indexer for document retrieval, 2023. [11] Chaeeun Kim, Soyoung Yoon, Hyunji Lee, Joel Jang, Sohee Yang, and Minjoon Seo. Exploring the practicality of generative retrieval on dynamic corpora, 2024. [12] Varun Magesh, Faiz Surani, Matthew Dahl, Mirac Suzgun, Christopher D. Manning, and Daniel E. Ho. Hallucination-free? assessing the reliability of leading ai legal research tools, 2024. [13] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024. [14] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning, 2022. [15] Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. Learning to retrieve passages without supervision, 2022. 10 [16] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models, 2024. [17] Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, and Iftekhar Naim. Gecko: Versatile text embeddings distilled from large language models, 2024. [18] Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen tau Yih, Pang Wei Koh, and Luke Zettlemoyer. Reasonir: Training retrievers for reasoning tasks, 2025. [19] Jintian Zhang, Cheng Peng, Mengshu Sun, Xiang Chen, Lei Liang, Zhiqiang Zhang, Jun Zhou, Huajun Chen, and Ningyu Zhang. Onegen: Efficient one-pass unified generation and retrieval for llms, 2024. [20] Benjamin Reichman and Larry Heck. Dense passage retrieval: Is it retrieving? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1354013553, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [21] Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In Proceedings of the 17th European Conference on Machine Learning, ECML06, page 282293, Berlin, Heidelberg, 2006. Springer-Verlag. [22] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, L. Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484489, 2016. [23] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision without process, 2024. [24] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Dont throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding, 2024. [25] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search, 2024. [26] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [28] P. Ferragina and G. Manzini. Opportunistic data structures with applications. In Proceedings of the 41st Annual Symposium on Foundations of Computer Science, FOCS 00, page 390, USA, 2000. IEEE Computer Society. [29] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen tau Yih, Sebastian Riedel, and Fabio Petroni. Autoregressive search engines: Generating substrings as document identifiers, 2022. [30] Giovanni Manzini. An analysis of the burrowswheeler transform. J. ACM, 48(3):407430, May 2001. [31] P. Auer, Paul Fischer, and N. Cesa-Bianchi. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(3):235256, 2002. 11 [32] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories, 2023. [33] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing In Donia Scott, multi-hop QA dataset for comprehensive evaluation of reasoning steps. Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. [34] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension, 2017. [35] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering, 2018. [36] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. [37] Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrieval-augmented generation, 2025. [38] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training, 2020. [39] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. [40] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. Improving language models by retrieving from trillions of tokens, 2022. [41] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models, 2022. [42] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models, 2020. [43] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models, 2023. [44] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting 12 Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [45] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yihen Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Lichao Sun, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Ninghao Liu, Bei Jiang, Linglong Kong, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, and Tianming Liu. Evaluation of openai o1: Opportunities and challenges of agi, 2024. [46] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [47] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. From matching to generation: survey on generative information retrieval, 2025. [48] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval, 2021. [49] Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. Transformer memory as differentiable search index, 2022. [50] Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, Tianxin Wei, and Hamed Zamani. Scalable and effective generative information retrieval, 2023. [51] Hansi Zeng, Chen Luo, and Hamed Zamani. Planning ahead in generative retrieval: Guiding autoregressive generation through simultaneous decoding, 2024. [52] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. Multiview identifiers enhanced generative retrieval, 2023. [53] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. Learning to rank in generative retrieval, 2023. [54] Sanket Vaibhav Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Jinfeng Rao, Marc Najork, Emma Strubell, and Donald Metzler. Dsi++: Updating transformer memory with new documents, 2023. 13 [55] Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yiqun Liu, Yixing Fan, and Xueqi Cheng. Corpusbrain: Pre-train generative retrieval model for knowledge-intensive language tasks. In Proceedings of the 31st ACM International Conference on Information &; Knowledge Management, CIKM 22, page 191200. ACM, October 2022. [56] Qiaoyu Tang, Jiawei Chen, Zhuoqun Li, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu, Hongyu Lin, Fei Huang, Ben He, Xianpei Han, Le Sun, and Yongbin Li. Self-retrieval: End-to-end information retrieval with one large language model, 2024. [57] Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Scaling retrieval-based language models with trillion-token datastore, 2024. 14 Dataset Statistics & Retrieval Settings Table 5 presents the statistics and retrieval settings of the five knowledge-intensive QA datasets used in our experiments. Settings PopQA [32] TriviaQA [34] HotPotQA [35] 2WikiMultihopQA [33] MuSiQue [36] Dataset statistics # Examples Gold answer count Retrieval settings Corpus Corpus size Retriever Top-k 500 Many 500 Single 500 Single 500 Single 500 Single Wikipedia-dpr Wikipedia-dpr Wikipedia-dpr Wikipedia-2wiki Wikipedia-dpr 21M E5 2 21M E5 3 21M E5 3 6M E5 3 21M E5 Table 5: Comparison of datasets and retrieval settings. As shown in # Examples, to reduce computational cost, we randomly sample up to 500 examples from each dataset. In Gold answer count, Many denotes multiple gold answers; Single denotes exactly one. 15 Node Granularity Latency () Avg Rollout Count Avg Path Length. EM () F1 () Exploration Efficiency Performance Exploration G=1 G=2 G=3 G=4 G=5 G=6 G= G=8 G=9 G=10 67s 60s 36s 34s 30s 28s 28s 27s 27s 27s 39.14 63.13 60.19 60.60 60.60 59.40 60. 60.00 61.10 60.23 11 42 84 99 109 130 136 154 0.30 0.48 0.54 0.52 0.52 0.55 0. 0.53 0.54 0.52 0.33 0.54 0. 0.58 0.59 0.63 0.60 0.60 0.61 0. 32 16 12 10 9 9 8 9 8 Table 6: Analysis of efficiency, performance, and exploration behavior under varying node granularity. Avg Rollout Counts for = 4 and = 5 are imputed with the mean (excluding = 1), as single-token nodes (G = 1) often fail to yield valid rollouts due to limited semantic context."
        },
        {
            "title": "B Analysis on Varying Node Granularity",
            "content": "Table 6 shows the efficiency, performance, and exploration behavior under varying node granularity. As granularity increases, latency decreases, while the average path length growsindicating that more informative reasoning paths are explored. Performance also improves with increasing granularity, peaking at = 6 and saturating thereafter. Interestingly, although the number of valid rollouts required during the CS-MCTS process remains theoretically similar or even lower, the length of the retrieved paths increases with granularity. This is because longer current nodes provide more contextual constraints, reducing the number of valid tokens available for rollout at the next step. In other words, increased granularity allows the model to extract more informative reasoning paths with comparable or even reduced computational cost. In the case of = 1, FREESON must not only search for answer-containing paths but also infer semantic relationships between adjacent nodes, leading to broader exploration across many nodes. higher number of explorations in this setting should not be interpreted as positive signal. 16 Limitations of Ground-truth Annotations in QA Benchmarks Although both systems generate semantically correct answers, only one may match the groundtruth due to differences in surface phrasing. For instance, E5 outputs water or music, whereas FREESON (ours) produces more precise alternatives such as fluid or operayet only the former is recognized as correct by the ground-truth annotations. Closer inspection reveals that the ground-truth answers often reflect the phrasing found in documents retrieved by E5. This suggests that the dataset construction process may be biased toward expressions favored by embedding-based retrieval systems, which could inadvertently advantage such models during evaluation. Below are the examples found in HotpotQA. #1. FREESON retrieves correct documents but ground-truth fails to recognize valid answers - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - * Question : What substance is always in trap to prevent the passage of anything from either direction , - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - * Ground - truth answer : water - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - even sewer gases ? < think > need to find the substance that is always in trap to prevent the passage of anything from either direction , even sewer gases . ll search for it . </ think > < search > ( subject : trap , question : What substance is always in trap to prevent the passage of anything from either direction , even sewer gases ) </ search > < information > For trap : Doc 0. trap ( plumbing ) trap ( plumbing ) in plumbing , trap is device shaped with bending pipe path to retain fluid to prevent sewer gases from entering buildings while allowing waste to pass through . in oil refineries , traps are used to prevent hydrocarbons and other dangerous gases from escaping through drains . ... ( omitted ) ... Doc 1. trap ( plumbing ) venting . certain drum - styled traps are also discouraged or banned . trap ( plumbing ) in plumbing , trap is device shaped with bending pipe path to retain fluid to prevent sewer gases from entering buildings while allowing waste to pass through . ... ( omitted ) ... </ information > < think > found out that in plumbing , trap is device shaped with bending pipe path to retain fluid to prevent sewer gases from entering buildings while allowing waste to pass through . Now need to find the substance that is always in trap to prevent the passage of anything from either direction . </ think > ... ( omitted ) ... - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - < answer > fluid </ answer > - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 17 #2. FREESON retrieves correct documents but ground-truth fails to recognize valid answers - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - * Question : What type of art does The Consul and Arlecchino have in common ? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - * Ground - truth answer : music - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - # First step < think > need to find the type of art that The Consul and Arlecchino have in common . ll search for it . </ think > < search > ( subject : The Consul , question : What type of art does The Consul have ) </ search > < information > For The Consul : Doc 0. the consul the consul the consul is an opera in three acts with music and libretto by gian carlo menotti , his first full - length opera . ... ( omitted ) ... Doc 1. the consul prize for music and also the 1950 new york drama critics circle award for best musical . ... ( omitted ) ... Doc 2. the consul the broadway cast , this time with rosemary kuhlmann as the secretary of the consulate . ... ( omitted ) ... noted the topicality of the opera by analogy to the real - life situations </ information > # Second step < think > found out that The Consul is an opera . Now need to find the type of art that Arlecchino has in common with The Consul . </ think > < search > ( subject : Arlecchino , question : What type of art does Arlecchino have ) </ search > < information > For Arlecchino : Doc 0. arlecchino ( opera ) of double - bill . the first british staged performance of arlecchino was in 1954 at glyndebourne . however , edward clark had produced concert version in london in 1939. the opera is in four movements ... ( omitted ) ... Doc 1. arlecchino ( opera ) arlecchino , oder die fenster ( harlequin , or the windows , is one - act opera with spoken dialog by ferruccio busoni . ... ( omitted ) ... Doc 2. arlecchino ( opera ) ( finale .) in procession the other characters , leandro and colombina , dottore and abbate , the donkey and the cart , the two constables , and finally arlecchino and annunziata , cross the stage and bow to the audience ... ( omitted ) ... </ information > < think > found out that Arlecchino is an opera . Now can compare the type of art that The Consul and Arlecchino have in common . Both The Consul and Arlecchino are operas . </ think > - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - < answer > opera </ answer > - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"
        },
        {
            "title": "D Prompt Template for FREESON",
            "content": "D.1 Reasoning prompt template"
        },
        {
            "title": "Reasoning process",
            "content": "Answer the given question . You must conduct reasoning inside < think > and </ think > every time you get new information . After reasoning , if you find you lack some knowledge , you can call search engine by : < search > ( subject : Help ! Help ! Police ! , question : Who is the director of the film Help ! Help ! Police !) </ search > This is the correct form for the query : Who is the director of the film Help ! Help ! Police ? It will return the searched results between < information > and </ information >. You can search as many times as you want . If you find no further external knowledge is needed , you can directly provide the answer inside < answer > and </ answer > , without detailed illustrations . For example : < answer > Beijing </ answer > Only respond to the final question . Your answer must reflect the end goal , not just part of the process . Question : { question } D.2 Retrieval prompt template"
        },
        {
            "title": "Retrieval prompts for reasoning over document structure",
            "content": "Given subject and question , generate word or phrase likely to appear in document that answers the question . : subject : Star Wars , question : who did Star Wars direct ? : Star Wars is directed by : subject : Alice , question : When was Alice born ? : Alice ( January 1 , 1970 December 12 , 2024) Now your tern : : { question } : 19 D.3 CT-MCTS value network prompt template for training and inference Training on-policy value networks and evaluation using them # training Answer the given question . you can call search engine using < search > and </ search >. It will return the top searched results between < information > and </ information >. Based on the provided information , provide the final answer inside < answer > and </ answer >. Question : { question } # inference Score from 0 to 1 how much the generated reference contains at least partial answer to the query . Query : { query_text } Generated reference : { rollout_text } Score : D.4 Prompt template for training value networks with synthetic rollouts Training off-policy value networks You are helping build dataset for reward model . Given : - user query - reference sentence that correctly answers it Your task : 1. Generate 3 diverse outputs that vary in : - Whether they contain the exact answer - Helpfulness in answering the query - Length and form ( sentence , phrase , or word ) 2. Include at least 1 short or fragment - style response . Each output should be dictionary with : - generated : the output - has_answer_score : 1 only if it contains the exact answer textually ( not paraphrased ) - sim_seq_score : float (0.0 -1.0) based on how well it answers the query Example : Query : What nationality is Aleksandr Stolper ? Reference : Aleksandr Borisovich Stolper (12 August 1907 , Dvinsk ( now Daugavpils ) - 12 January 1979 , Moscow ) was Russian / Soviet film director and screenwriter . Output : [{{\" generated \": \" Aleksandr Borisovich Stolper \" , \" has_answer_score \": 0 , \" sim_seq_score \": 0.7}} , {{\" generated \": \" Aleksandr Borisovich Stolper (12 August 1907 , Dvinsk ( now Daugavpils ) ) \" , \" has_answer_score \": 0 , \" sim_seq_score \": 0.85}} , {{\" generated \": \" Russian / Soviet film director and screenwriter .\" , \" has_answer_score \": 1 , \" sim_seq_score \": 0.7}}] Now do the same for : Query : { query } nReference : { reference } nOutput :"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "LBOX"
    ]
}