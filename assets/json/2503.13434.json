{
    "paper_title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing",
    "authors": [
        "Yaowei Li",
        "Lingen Li",
        "Zhaoyang Zhang",
        "Xiaoyu Li",
        "Guangzhi Wang",
        "Hongxiang Li",
        "Xiaodong Cun",
        "Ying Shan",
        "Yuexian Zou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Element-level visual manipulation is essential in digital content creation, but current diffusion-based methods lack the precision and flexibility of traditional tools. In this work, we introduce BlobCtrl, a framework that unifies element-level generation and editing using a probabilistic blob-based representation. By employing blobs as visual primitives, our approach effectively decouples and represents spatial location, semantic content, and identity information, enabling precise element-level manipulation. Our key contributions include: 1) a dual-branch diffusion architecture with hierarchical feature fusion for seamless foreground-background integration; 2) a self-supervised training paradigm with tailored data augmentation and score functions; and 3) controllable dropout strategies to balance fidelity and diversity. To support further research, we introduce BlobData for large-scale training and BlobBench for systematic evaluation. Experiments show that BlobCtrl excels in various element-level manipulation tasks while maintaining computational efficiency, offering a practical solution for precise and flexible visual content creation. Project page: https://liyaowei-stu.github.io/project/BlobCtrl/"
        },
        {
            "title": "Start",
            "content": "BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing Yaowei Li 1 Lingen Li 2 Zhaoyang Zhang 3 Xiaoyu Li 3 Guangzhi Wang 3 Hongxiang Li 1 Xiaodong Cun 3 Ying Shan 3 Yuexian Zou 1 * 5 2 0 2 7 1 ] . [ 1 4 3 4 3 1 . 3 0 5 2 : r Figure 1: Our proposed BlobCtrl framework enables comprehensive element-level control over both visual appearance and spatial layout, facilitating diverse manipulation operations including compositional generation, spatial transformation, element removal, content replacement and arbitrary combinations thereof (top). Through an iterative refinement process, BlobCtrl allows precise and fine-grained editing capabilities to achieve desired visual outcomes (bottom)."
        },
        {
            "title": "Abstract",
            "content": "Element-level visual manipulation is essential in digital content creation, but current diffusion1Peking University 2The Chinese University of Hong Kong Project lead: Zhaoyang Zhang 3ARC Lab, Tencent PCG. <zhaoyangzhang@link.cuhk.edu.hk>. * Corresponding author: Yuexian Zou <zouyx@pku.edu.cn>. based methods lack the precision and flexibility of traditional tools. In this work, we introduce BlobCtrl, framework that unifies element-level generation and editing using probabilistic blobbased representation. By employing blobs as visual primitives, our approach effectively decouples and represents spatial location, semantic content, and identity information, enabling precise 1 BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing element-level manipulation. Our key contributions include: 1) dual-branch diffusion architecture with hierarchical feature fusion for seamless foreground-background integration; 2) selfsupervised training paradigm with tailored data augmentation and score functions; and 3) controllable dropout strategies to balance fidelity and diversity. To support further research, we introduce BlobData for large-scale training and BlobBench for systematic evaluation. Experiments show that BlobCtrl excels in various element-level manipulation tasks while maintaining computational efficiency, offering practical solution for precise and flexible visual content creation. Project page: https://liyaowei-stu.github. io/project/BlobCtrl/. 1. Introduction Element-level image manipulation has long been goal in digital art, with tools like Adobe Photoshop(Adobe Inc., 19882023) enabling precise manipulation of visual elements. While recent AI models(Ramesh et al., 2022; Labs, 2023; Esser et al., 2024; Sheynin et al., 2024; Shi et al., 2024) excel in high-quality image synthesis, they often lack fine-grained control over individual elementsa key feature of traditional tools (Adobe Inc., 19882023; Serif Europe Ltd., 20152023). Advances like ControlNet(Zhang et al., 2023a) and IP-Adapter(Ye et al., 2023) have improved controllability but still do not support interactive, multi-round, element-based manipulation (e.g., composition, resizing, arrangement) crucial for creative workflows. Challenges include: 1) decoupling and representation of visual elements, 2) continuous layout control, 3) preserving appearance and identity, 4) maintaining visual harmony, and 5) scarcity of large-scale paired training data for end-to-end training. Current efforts in element-level manipulation focus on two approaches: generation and editing, each facing obstacles. Element-level generation(Li et al., 2023; Ye et al., 2023; Nie et al., 2024; Chen et al., 2023; Xiong et al., 2024; Parmar et al., 2025) uses grounding tokens (e.g., bounding boxes, ellipses) for spatial control and identity tokens like CLIP(Radford et al., 2021) and DINO (Caron et al., 2021) for appearance maintenance. These methods struggle with continuous layout control due to the discrete nature of grounding tokens and the high compression of identity tokens, which hinders detailed appearance preservation. Element-level editing (Zhang et al., 2023b; Avrahami et al., 2023; Shi et al., 2023; Alzayer et al., 2024; Mu et al., 2025) employs optimization, segmentation, clustering, and dragbased methods for attribute control. These approaches often lack flexibility and struggle with visual harmony, frequently relying on video data that introduces complexities like cam2 era movements, degrading performance and generalization. Detailed discussions of related research works are provided in Section 5. The essence of element-level visual modeling lies in the flexible decoupling and representation of location, semantics, and identity. BlobCtrl uses blobs as visual primitives to achieve this. Formally, blob is probabilistic twodimensional Gaussian distribution(Carson et al., 1999), and geometrically, it appears as an ellipse(Nie et al., 2024). Blob parameters precisely specify position, size, and orientation, while Gaussian smoothness ensures harmonious and continuous layout control. For visual identity, we use differentiable blob splatting(Epstein et al., 2022) combined with variational autoencoder (VAE) features(Kingma, 2013) to preserve appearance. Building on the probabilistic blob representation, we introduce dual-branch diffusion model: one branch for foreground elements and another for background elements. self-supervised training paradigm enhances generalization and efficiency, with specific strategies improving BlobCtrls robustness. To preserve foreground identities, we propose random data augmentation and an ID retention score function. Additionally, random dropout in the dual-branch structure allows flexible balancing of appearance fidelity and creative diversity during inference. These design choices make BlobCtrl an efficient, flexible solution for elementlevel generation and editing. To scale up our method and ensure comprehensive evaluation, we introduce new training dataset, BlobData, and benchmark, BlobBench. Extensive quantitative and qualitative results demonstrate BlobCtrls effectiveness in elementlevel generation (combining multiple subjects) and editing (moving, resizing, adding, deleting, and replacing elements). In nutshell, our main contributions include: We propose BlobCtrl, novel unified framework that first enables precise and flexible manipulation over visual elements through element-level generation and editing, while effectively preserving their intrinsic characteristics. We design an innovative dual-branch architecture with meticulously crafted training paradigms and strategies, achieving an optimal balance between maintaining appearance fidelity and enabling creative diversity in visual manipulation. We introduce BlobData, comprehensive large-scale dataset specifically curated for training element-level visual models, alongside BlobBench, rigorous evaluation benchmark for assessing element-level generation and editing capabilities. Through extensive experimentation, we demonstrate that BlobCtrl achieves superior performance compared to existing methods in both element-level generation and editBlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing ing tasks, while maintaining computational efficiency and practical applicability. lanobis, 1936) to the blob center is first computed: dM (xgrid, Q) = (xgrid µ)T Σ1(xgrid µ), (1) 2. Blob-Based Element-level Representation Why is the blob an effective element-level representation? As grounding token, blob precisely represents an objects position, size, and orientation. As Gaussian distribution, it offers more flexible and harmonious element-level expression than segmentation masks, which have strong shape constraints. In this section, we define the blob and explain its role as an element-level visual representation. 2.1. Blob Formula Fig. 2 visualizes blob. Geometrically, blob can be considered as an ellipse, parameterized by eτ = [Cx, Cy, a, b, θ], where Cx and Cy denote the coordinates of the ellipses center, and are the lengths of the minor and major axes, respectively, and θ [0, π] is the orientation angle of the ellipse. Statistically, blob is modeled as two-dimensional Gaussian distribution, parameterized by µ = [µx, µy] and Σ = (cid:20)σxx σxy σyx σyy (cid:21) , where µx and µy are the means of the Gaussian distribution, [σxx, σyy] are the variances corresponding to the and directions, respectively, and [σxy, σyx] are the covariances, indicating the correlation between and y. Figure 2: Blob Formula. blob can be represented in two equivalent forms: geometrically as an ellipse parameterized by center coordinates (Cx, Cy), axes lengths (a, b), and orientation θ; and statistically as 2D Gaussian distribution characterized by mean µ and covariance matrix Σ. The two forms are exactly equivalent and interchangeable. 2.2. Blob Opacity (cid:1)(cid:9)W,H , where xgrid (cid:8)(cid:0) w=1,h=1 is point on twodimensional grid map, and represents the 2D Gaussian distribution of the blob, characterized by its mean µ and covariance matrix Σ. The distance dM RH,W quantifies how far the point xgrid is from the center µ while taking into account the shape of the distribution defined by Σ. Then, the blob opacity can be calculated based on this distance: O(xgrid) = sigmoid(dM ), (2) which maps the distance dM to value between 0 and 1, effectively representing the opacity of the blob at the point xgrid. This ensures that points closer to the center of the blob have higher opacity, while those further away are more transparent, ensuring smooth and continuous transition. 2.3. Blob Composition and Splatting Blob composition implies the process of integrating multiple blobs through depth-aware alpha compositing (Porter & Duff, 1984; Nitzberg & Mumford, 1990), which effectively addresses occlusion and models inter-object relationships. Mathematically, blob composition is formulated as follows: Oi c(xgrid) = Oi(xgrid) (cid:89) j=i+1 (1 Oj (xg)) , (3) where is the total number of blobs to be composed, c(xgrid) Rh,w represents the composed opacity of the Oi i-th blob across spatial dimensions, and the product term accounts for the occlusion effects of subsequent blobs in the sequence. In our BlobCtrl, we compose the foreground and background element to obtain the foreground opacity Ofg and background opacity Obg, which serve as an elementlevel layout representation. Blob splatting (Epstein et al., 2022) refers to the ability to project features R1d into two-dimensional space via blob, creating spatially-aware features Rhwd. Formally, blob splatting is expressed as follows: = gsplatting(f , Oc) = (cid:88) i=0 Oi i, (4) Notably, the blob represented as Gaussian enables the calculation of opacity across spatial dimensions, leading to the concepts of blob splatting and blob composition. These concepts are crucial for achieving smooth rendering and seamless integration of visual elements in graphics. In particular, the squared Mahalanobis distance (Mahawhere denotes element-wise multiplication with broadcasting. In our BlobCtrl, we first use DINO V2 (Oquab et al., 2023) to encode the visual semantics of the foreground element. The semantic features ffg are then splatted with the foreground opacity Ofg, resulting in spatially-aware, element-level visual semantic feature Ffg. BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing Figure 3: Overview of BlobCtrl. Our framework consists of: (1) dual-branch architecture with foreground branch for element identity encoding and background branch for scene context preservation and harmonization. Both branches use concatenated inputs of noisy latents and reference conditions (Sec. 3.1). (2) self-supervised training paradigm for element-level manipulation through stochastic position generation and target reconstruction optimization. Through feature fusion between branches, our framework achieves precise control over elements while maintaining visual coherence. 3. Self-supervised Paradigm for BlobCtrl formally expressed as: As discussed in Sec. 2, blob-based representation offers continuous spatial control for flexible manipulation, seamless composition for harmonious integration, and spatial-aware splatting for visual semantics. Leveraging these advantages, we introduce self-supervised training paradigm to develop robust and versatile model for element-level visual generation and editing. 3.1. Model Architecture Based on the blob representation, we propose dual-branch diffusion model to handle foreground and background elements separately. As shown in Fig. 3, our model mainly consists of two key components: Foreground Branch. The foreground branch is designed to preserve the identity and appearance of foreground elements while enabling flexible layout control. As shown in Fig 3, we concatenate the noisy latent zt Rc,h,w with reference foreground conditions cfg R(c+1+d),h,w along the column dimension as input to the foreground branch. This column-wise concatenation strategy improves the models in-context learning capabilities, allowing it to more effectively grasp and retain the characteristics of elements. The reference foreground conditions cfg are constructed by concatenating three key components along the channel dimension: (1) the opacity map Ofg for layout information, (2) spatially-aware semantic features Ffg for identity preservation, and (3) VAE latent zfg for appearance encoding. To ensure dimensional compatibility for the column-wise concatenation between zt and cfg, we additionally concatenate Ofg and Ffg along the channel dimension of zt. The input construction process for the foreground branch can be inputfg = [cfg, [zt, Ofg, Ffg; axis = 0] ; axis = 2] , (5) cfg = [zfg, Ofg, Ffg; axis = 0] R(c+1+d),h,w, where [; axis = 0] and [; axis = 2] represent concatenation along the channel and column, respectively. (6) To process element-level foreground input, we use modified pre-trained diffusion backbone with cross-attention layers removed. This approach serves two purposes: the pretrained weights offer strong generative prior for effective foreground feature processing, and removing cross-attention layers ensures the model focuses solely on visual content without broader contextual influences. Background Branch. The background branch serves as diffusion backbone aims to preserve the original background while harmoniously integrating foreground elements into the scene. Similarly, we concatenate the noisy latent zt with reference background conditions cbg along the column dimension as input, as shown below: inputbg = [cbg, [zt, Obg; axis = 0] ; axis = 2] , (7) cbg = [zbg, Obg; axis = 0] R(c+1),h,w, where background branch lacks spatial-aware semantic features, as it tends to preserve information completely. (8) In element-level editing, the background is the masked image where both the original and target regions of the foreground element are masked out. For instance, when moving bird, the background has masks at both the birds initial and destination positions. The background branch uses complete diffusion backbone with cross-attention layers. To seamlessly integrate foreground and background elements, we employ hierarchical 4 BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing feature fusion, progressively injecting foreground features at multiple resolution levels in the background branch. We also use zero-initialization (Zhang et al., 2023a) for stable training. Feature fusion for the i-th block is formulated as: θ(zt, t, C) = ϵi ϵi θ(zt, t, C) + ω Z(ϵi θ(zt, t, )), (9) where and is the input conditions for the background and the foreground branch respectively, and ω is the weight for feature fusion. 3.2. Self-supervised Training While paired data of objects at different positions would be ideal for training, such data is scarce. Previous methods (Chen et al., 2023; Alzayer et al., 2024) rely on video data, but this introduces unwanted complexities that degrade model performance. Instead, we propose self-supervised training strategy, employing the idea that any image can be seen as the targer result of an element manipulation process. For each training image, we identify the target elements position and randomly generate blob at different location to simulate the source position. This mimics the manipulation process, as shown in Fig. 3, where toy appears to move from random left position to its actual right position. We optimize our model using noise-prediction score function during training: = Ez0,C,C,ϵN (0,I ) (cid:2)ϵ ϵθ(zt, t, C, )2 2 (10) where z0 is the latent of the source image and zt = αtx0 + 1 αtϵ is the noisy latent at time step t. This score function drives the model to fill foreground elements at the target layout, inpaint background elements at the original foreground position, and ensure harmonious integration of the entire scene. (cid:3) , propose an identity preservation score function. During training, we retain the diffusion models output layer in the foreground branch (discarded during inference) and apply score function that operates only within the foreground element region. Lid = Ez0,C,ϵN (0,I ) (cid:2)Mfg ϵ ϵθ(zt, t, )2 (cid:3) , (11) where Mfg is the foreground mask that indicates the foreground element region. This helps ensure accurate preservation of foreground element appearance while allowing flexibility in background integration. During training, the overall optimization objective is: Ltotal = + λidLid, (12) where λid is hyperparameter that controls the weight of identity preservation. We gradually decay λid from 1.0 to 0.6 during training, which encourages the model to focus more on scene harmonization in later training stages while still maintaining reasonable identity preservation. 3.4. Controllable Fidelity-Diversity Trade-off To achieve flexible control between appearance fidelity and creative diversity, we implement random dropout strategies during training. First, we randomly drop the weights of the foreground branch, allowing the model to adjust between freely generating foreground elements based on global textual information and strictly preserving given foreground identities. Second, we randomly drop both the semantic features to be splatted and the VAE features of foreground elements, enabling flexible control over the balance between semantics and appearance. Specifically, we apply: ω = ω I(ξ1 < pω) fg = Ffg I(ξ2 < pfeat) fg = zfg I(ξ3 < pvae) (13) 3.3. ID Preservation and Scene Harmonization Random Data Augmentation. To prevent the model from defaulting to simple copy-and-paste solution, we employ extensive data augmentation on foreground elements during training. This includes random transformations such as color jittering, scaling, rotation, erasing, and perspective changes. These augmentations serve two main purposes: they compel the model to harmoniously place foreground elements based on specified layouts and appearances, and the random erasing fosters robust inpainting capabilities for incomplete elements. This approach ensures the model learns to generate and manipulate elements flexibly and contextually, maintaining visual coherence with the background. Identity Preservation Score Function. To effectively decouple the foreground and background branchesensuring the foreground branch injects element-level information while the background branch integrates these elementswe where is indicator function, and ξ1, ξ2, ξ3 U(0, 1) are uniform random variables, pω, pfeat, pvae are the dropout probabilities for branch weights, semantic features, and VAE features respectively. 4. Experiments 4.1. Datasets, Benchmark and Metrics BlobData Curation. To train BlobCtrl, we construct BlobData (1.86M samples) sourced from BrushData, containing images, segmentation masks, fitted ellipse parameters (with derived 2D Gaussians), and descriptive texts. The dataset curation process involves: (1) Filtering source images to retain those with shorter sides over 480 pixels and valid instance segmentation masks. (2) Applying mask filtering criteria to preserve masks with area ratios between 0.01 and 0.9 of the total image area and excluding those at image 5 BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing Method Compose Move Resize Replace Remove CLIP-I DINO MSE CLIP-I DINO MSE CLIP-I DINO MSE CLIP-I DINO MSE CLIP-I DINO Anydoor (Chen et al., 2023) GliGen (Li et al., 2023) MagicFix (Alzayer et al., 2024) BlobCtrl (Ours) 86.7 70.7 80.5 88.3 81.2 57.8 78.6 86. 6.7 6.9 6.9 6.4 85.4 71.2 84.6 88.9 81.7 62.4 82.4 87. 6.8 7.1 6.7 6.3 83.3 78.2 83.7 86.5 83.7 69.4 85.2 89. 9.6 9.7 9.0 8.9 81.7 68.4 84.2 86.2 80.2 60.6 80.1 86. 9.7 9.6 9.4 9.0 39.5 40.2 43.6 35.3 13.6 15.3 23.1 8. Table 1: Quantitative comparison of identity preservation and grounding accuracy across various element-level manipulations. We evaluate using CLIP-I and DINO scores for identity preservation, and MSE for grounding accuracy. For removal operations, lower CLIP-I and DINO scores () are desired as they indicate more complete removal of target elements. Our method consistently outperforms existing approaches across all operations. Method PSNR SSIM LPIPS FID 4.2. Implementation Details. Anydoor (Chen et al., 2023) GliGen (Li et al., 2023) MagicFix (Chen et al., 2023) 32.0631 27.923 30.3958 0.7424 0.2414 0.7415 0.2394 0.6963 0.2277 145.2546 307.8219 194.0154 BlobCtrl (Ours) 32.1571 0.7507 0.2196 102.8094 Table 2: Comparison of image generation quality using standard metrics. Our method achieves superior performance across all metrics, demonstrating better generation quality and fewer artifacts. boundaries. (3) For the filtered masks, fitting ellipse parameters 1 and derive 2D Gaussian distributions. (4) Removing invalid samples, particularly those with covariance values below 1e-5. (5) Generating detailed image descriptions with InternVL-2.5 (Chen et al., 2024). BlobBench Curation. Existing evaluation benchmarks like DreamBooth(Ruiz et al., 2023), COCOE(Yang et al., 2023), COCO Val(Lin et al., 2014), and CreatiLayout(Zhang et al., 2024a) assess either grounding capability or identity preservation, but not both simultaneously. They also lack coverage of the full range of element-level manipulations, such as composition, movement, resizing, deletion, and replacement. To address these gaps, we introduce BlobBench, comprehensive benchmark with 100 curated images, evenly distributed across different element-level operations. Each image is annotated with ellipse parameters, foreground mask, and detailed text descriptions by experts. BlobBench includes both real-world and AI-generated images across diverse scenarios, such as indoor and outdoor scenes, animals, and landscapes, ensuring fair and effective evaluation. Evaluation Metrics. We evaluate BlobCtrl using both objective metrics and human assessment, including objective evaluation (identity preservation, grounding accuracy, generation quality and harmonization) and subjective evaluation. Please refer to the Appendix for details about these metrics. 1https://docs.opencv.org/4.x/de/d62/ tutorial_bounding_rotated_ellipses.html Training Details. BlobCtrl is built upon Stable Diffusion v1.5 (Rombach et al., 2022). During training, all images and annotations are resized to 512 512 pixels. We initialize both foreground and background branches with pretrained UNet weights. The foreground branch undergoes full finetuning with cross-attention layers removed, while the background branch is fine-tuned using LoRA (Hu et al., 2021) with rank=64. We employ the Adam optimizer (Kingma & Ba, 2014) with learning rate of 1e-5 and weight decay of 0.01. The model is trained on our curated BlobData dataset comprising 1.86M samples for 7 days using 24 NVIDIA V100 GPUs with batch size of 192. For controllable fidelity-diversity trade-off, we set dropout probabilities pω, pfeat, pvae to 0.1. The weight of identity preservation loss λid is gradually decayed from 1.0 to 0.6 during training. Additionally, to enable classifier-free guidance during inference, we set the caption dropout probability to 0.1. Evaluation Details. We evaluate BlobCtrl against three state-of-the-art methods on the BlobBench benchmark: GliGen(Li et al., 2023), bounding box-based text-to-image model; Anydoor(Chen et al., 2023), segmentation maskbased image-to-image model; and Magic Fixup (Chen et al., 2023), which specializes in harmonizing transformed regions. To systematically assess five fundamental elementlevel operations (composition, movement, resizing, replacement, and removal), we adapt the baselines with specific workflows. For Anydoor, we create clean background by teleporting background to foreground regions, then edit by teleporting foreground objects to target locations. For GliGen, we use BlobCtrl to remove elements for clean background, then apply bounding box constraints with text and image conditions. For Magic Fixup, we warp foreground elements using rigid transformations from editing operations, followed by scene harmonization. 4.3. Quantitative Evalution Comparison to State-of-the-Art Methods. As shown in Tab. 1 and Tab. 2, BlobCtrl demonstrates consistent and significant improvements over existing methods across all evaluation metrics: Identity Preservation: For tasks requiring identity preser6 BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing Figure 4: Visual comparison of element-level manipulation capabilities across different methods. We evaluate five fundamental operations: composition, movement, resizing, replacement and removal. Anydoor (Chen et al., 2023) struggles with precise identity preservation, GliGen (Li et al., 2023) fails to maintain any identity information, and Magic Fixup (Chen et al., 2023) produces results with poor visual harmonization. In contrast, BlobCtrl achieves superior results across all operations while maintaining both identity preservation and visual harmony. We recommend zooming in to examine the source images and element-level manipulation instructions in detail. vation (composition, movement, resizing, replacement), BlobCtrl achieves substantially higher average CLIP-I (87.48 vs. 84.28) and DINO (87.45 vs. 81.70) scores compared to the best baseline. For removal tasks, our method shows lower identity scores (average of CLIP-I and DINO scores) (21.95 vs. 26.55), indicating more thorough element elimination. Layout Control: BlobCtrl exhibits superior spatial control accuracy, reducing layout MSE by 8.11% relative to the previous best method. This validates the effectiveness of our probabilistic blob representation for precise element manipulation. Generation Quality: Our method sets new state-of-theart performance benchmarks across standard quality metrics: FID 102.8094, LPIPS 0.2196, PSNR 32.1571, and SSIM 0.7507. These results demonstrate BlobCtrls ability to generate high-fidelity outputs while maintaining global visual coherence. We attribute these substantial improvements to two key innovations: (1) the probabilistic blob representation that enables precise control over element attributes, and (2) our self-supervised training paradigm that effectively decouples and recombines visual elements identity, semantics and layout information, while eliminating performance degradation caused by unnecessary camera movements and other video-specific artifacts that plague previous methods. Human Evaluation. The subjective evaluation results reported in Tab. 3 demonstrate the superior performance of BlobCtrl across all assessment criteria. Quantitatively, our method establishes new state-of-the-art performance with significant margins over the previous best approach: an 87.2% preference rate in appearance fidelity compared to 82.5% for the previous best method, an 86.5% preference rate in layout accuracy versus 81.7%, and an 82.1% preference rate in visual harmony compared to 80.3%. These substantial improvements in human evaluation metrics in7 BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing dicate that BlobCtrl produces results that are more visually appealing and natural to human observers, making it better suited for real-world applications. Method Fidelity Layout Harmony Anydoor (Chen et al., 2023) GliGen (Li et al., 2023) MagicFix (Chen et al., 2023) 82.5% 51.2% 70.2% 81.7% 68.1% 73.1% 78.1% 80.3% 49.4% BlobCtrl (Ours) 87.2% 86.5% 82.1% Table 3: Human evaluation results comparing our method with baselines. Our method achieves consistently higher human preference scores across all metrics, demonstrating superior perceptual quality. 4.4. Qualitative Evalution Fig. 4 presents qualitative comparisons between BlobCtrl and state-of-the-art methods across various element-level manipulation scenarios. The results demonstrate several key advantages of our approach: Anydoor (Chen et al., 2023) struggles with accurate identity preservation during element manipulation and shows limitations in element-level removal, often leaving artifacts or incomplete modifications. While GliGen (Li et al., 2023) provides layout control capabilities, it fails to effectively preserve the visual appearance and identity of manipulated elements, resulting in inconsistent outputs. Magic Mixup (Alzayer et al., 2024) exhibits insufficient harmonization abilities, leading to visual inconsistencies between modified elements and their surroundings. In contrast, BlobCtrl demonstrates superior performance across all aspects - better generalization to diverse scenarios, more accurate identity preservation, and precise layout control while maintaining visual coherence. 4.5. Ablation Studies Analysis of Controllability and Flexibility As shown in Fig. 5, BlobCtrl achieves flexible control over the trade-off between identity preservation and diversity by adjusting the control timestep interval and control strength ω of the dualbranch fusion. When using only the background branch with text prompts, both identity preservation and layout accuracy suffer. Best results come from combining spatial-aware semantic features sfg and VAE features zvae. Ablation of Identity Preservation Score Function. We conduct an ablation study to analyze the effectiveness of our Identity Preservation Score Function. As shown in Fig. 6, under the same training steps, the model with Identity Preservation Score Function achieves significantly lower noise prediction loss (0.0235) compared to the model without it (0.0399), demonstrating faster convergence. To better understand how this score function affects the generation process, we visualize the denoising results using the preFigure 5: Flexible Control. Our dual-branch fusion mechanism enables flexible control over the trade-off between diversity and appearance preservation by adjusting the control timestep interval and fusion strength ω. Additionally, the feature dropout mechanisms provide more flexible interfaces for controlling the generation process. dicted noise from the foreground branch. The visualization reveals that the foreground branch effectively focuses on generating foreground content when guided by the Identity Preservation Score Function, validating our design choice of decoupling foreground and background element generation through this mechanism. Figure 6: Ablation of Identity Preservation Score Function. Training loss and denoising visualization for scaling deer, demonstrating how Identity Preservation Score Function enables faster convergence and effective foregroundbackground decoupling during element-level manipulation. 5. Related Work Element-level Generation. Contemporary element-level generation approaches can be categorized into two main paradigms: grounding-based and subject-driven methods. Grounding-based approaches, exemplified by GliGen and BlobGen (Li et al., 2023; Nie et al., 2024), employ bounding boxes and ellipses to achieve spatial control. However, these methods lack robust identity control mechanisms, resulting in significant content variations across different random initializations. While VisualComposer (Parmar et al., 2025) advances the field by incorporating multi-granular encoders for identity feature extraction, it struggles with precise layout control. Although Anydoor (Chen et al., 2023) and GroundingBooth (Xiong et al., 2024) demonstrate promising results 8 BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing in controlling both identity and layout, their heavy reliance on multi-view and video training data constrains their practical applicability and generalization capabilities. In the realm of subject-driven approaches, existing methods face significant limitations - they either demand computationally expensive test-time optimization (Gal et al., 2022; Ruiz et al., 2023; Gal et al.; Kumari et al., 2023) or depend extensively on multi-view datasets (Arar et al., 2023; Wei et al., 2023; Li et al., 2024a; Zhang et al., 2024b), which impedes their out-of-distribution generalization and spatial control capabilities. In contrast, our BlobCtrl presents an elegant solution by seamlessly integrating layout, semantic, and identity information through probabilistic blob representations and self-supervised training, thereby achieving flexible control over both appearance fidelity and creative diversity. Element-level Editing. Traditional image editing methods (Hertz et al., 2023; Brooks et al., 2023; Huang et al., 2024; Cao et al., 2023; Li et al., 2024b; Shi et al., 2024) rely on text prompts to introduce editing information, while element-level editing methods foucus on manipulating visual elements through operations like moving, resizing, replacing and removal. Continuous Layout Editing (Zhang et al., 2023b) decouples layout and appearance through testtime optimization but suffers from high computational costs. Magic Fixup (Alzayer et al., 2024) introduces two-stage pipeline consisting of transformation and harmonization steps, but relies on video data for training which can lead to degraded performance. Editable-element (Mu et al., 2025) proposes an element-level VAE approach but shows limited generalization ability due to its heavy dependence on large-scale paired training data. In contrast, our approach enables flexible element-level editing through self-supervised training, eliminating the need for explicitly paired editing data. 6. Discussion Conclusion. This work introduces BlobCtrl, unified framework that integrates element-level generation and editing using probabilistic blob-based representation. Blobs serve as visual primitives to encode spatial layout, semantics, and identity, allowing precise element manipulation. The dual-branch architecture with self-supervised training preserves foreground identities and maintains background harmony. Random data augmentation and dropout strategies offer flexible control between appearance fidelity and creative diversity. Extensive experiments on BlobBench demonstrate that BlobCtrl achieves state-of-the-art performance in element-level manipulation tasks. Limitations and Future Work. While BlobCtrl demonstrates strong capabilities in element-level manipulation, it currently only supports iterative single-element operations in single model forward pass. Fortunately, observersur blob-based representation inherently supports depth-aware composition, opening promising directions for future work."
        },
        {
            "title": "Impact Statement",
            "content": "Our work on element-level manipulation presents both opportunities and risks. While it enables more precise and flexible creative tools, there are potential concerns about misuse for creating misleading or harmful content. We advocate for responsible development and deployment of such technologies, with clear guidelines for ethical use and transparency about AI-generated content."
        },
        {
            "title": "References",
            "content": "Adobe Inc. Adobe photoshop, 19882023. https://www.adobe.com/products/ photoshop.html. Version 2023."
        },
        {
            "title": "URL",
            "content": "Alzayer, H., Xia, Z., Zhang, X., Shechtman, E., Huang, J.- B., and Gharbi, M. Magic fixup: Streamlining photo editing by watching dynamic videos. arXiv preprint arXiv:2403.13044, 2024. Arar, M., Gal, R., Atzmon, Y., Chechik, G., Cohen-Or, D., Shamir, A., and H. Bermano, A. Domain-agnostic tuningencoder for fast personalization of text-to-image models. In SIGGRAPH Asia 2023 Conference Papers, pp. 110, 2023. Avrahami, O., Aberman, K., Fried, O., Cohen-Or, D., and Lischinski, D. Break-a-scene: Extracting multiple conIn SIGGRAPH Asia 2023 cepts from single image. Conference Papers, pp. 112, 2023. Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., and Zheng, Y. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2256022570, October 2023. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Carson, C., Thomas, M., Belongie, S., Hellerstein, J. M., and Malik, J. Blobworld: system for region-based image indexing and retrieval. In Visual Information and Information Systems: Third International Conference, 9 BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing VISUAL99 Amsterdam, The Netherlands, June 24, 1999 Proceedings 3, pp. 509517. Springer, 1999. Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., and Zhao, H. Anydoor: Zero-shot object-level image customization. arXiv preprint, 2023. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Epstein, D., Park, T., Zhang, R., Shechtman, E., and Efros, A. A. Blobgan: Spatially disentangled scene representations. In European Conference on Computer Vision, pp. 616635. Springer, 2022. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. with decomposed dual-branch diffusion. In European Conference on Computer Vision, pp. 150168. Springer, 2024. Kingma, D. P. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2023. Kumari, N., Zhang, B., Zhang, R., Shechtman, E., and Zhu, J.-Y. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1931 1941, 2023. Labs, B. F. Flux. black-forest-labs/flux, 2023. https://github.com/ Li, D., Li, J., and Hoi, S. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024a. Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., and Lee, Y. J. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22511 22521, 2023. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., and Cohen-Or, D. Prompt-to-prompt image editing with cross-attention control. In ICLR, 2023. Li, Y., Bian, Y., Ju, X., Zhang, Z., Shan, Y., Zou, Y., and Xu, Q. Brushedit: All-in-one image inpainting and editing. arXiv preprint arXiv:2412.10316, 2024b. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. GANs trained by two time-scale update rule converge to local Nash equilibrium. Advances in Neural Information Processing Systems (NIPS), 30, 2017. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Huang, Y., Xie, L., Wang, X., Yuan, Z., Cun, X., Ge, Y., Zhou, J., Dong, C., Huang, R., Zhang, R., et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 83628371, 2024. Ju, X., Liu, X., Wang, X., Bian, Y., Shan, Y., and Xu, Q. Brushnet: plug-and-play image inpainting model Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740 755. Springer, 2014. Mahalanobis, P. On the generalized distance in statistics. National Institute of Science of India, 1936. Mu, J., Gharbi, M., Zhang, R., Shechtman, E., Vasconcelos, N., Wang, X., and Park, T. Editable image elements for controllable synthesis. In European Conference on Computer Vision, pp. 3956. Springer, 2025. Nie, W., Liu, S., Mardani, M., Liu, C., Eckart, B., and Vahdat, A. Compositional text-to-image generation with dense blob representations. In Forty-first International Conference on Machine Learning, 2024. 10 BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing Nitzberg, M. and Mumford, D. B. The 2.1-D sketch. IEEE Computer Society Press, 1990. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Howes, R., Huang, P.-Y., Xu, H., Sharma, V., Li, S.-W., Galuba, W., Rabbat, M., Assran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. Dinov2: Learning robust visual features without supervision, 2023. Parmar, G., Patashnik, O., Wang, K.-C., Ostashev, D., Narasimhan, S., Zhu, J.-Y., Cohen-Or, D., and Aberman, K. Object-level visual prompts for compositional image generation. arXiv preprint arXiv:2501.01424, 2025. Porter, T. and Duff, T. Compositing digital images. In Proceedings of the 11th annual conference on Computer graphics and interactive techniques, pp. 253259, 1984. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2250022510, 2023. Serif Europe Ltd. Affinity photo, 20152023. URL https: //affinity.serif.com/photo/. Version 2.0. Sheynin, S., Polyak, A., Singer, U., Kirstain, Y., Zohar, A., Ashual, O., Parikh, D., and Taigman, Y. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 88718879, 2024. Shi, Y., Xue, C., Pan, J., Zhang, W., Tan, V. Y., and Bai, S. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. arXiv preprint arXiv:2306.14435, 2023. Shi, Y., Wang, P., and Huang, W. Seededit: Align imarXiv preprint age re-generation to image editing. arXiv:2411.06686, 2024. Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. Wei, Y., Zhang, Y., Ji, Z., Bai, J., Zhang, L., and Zuo, W. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1594315953, 2023. Wikipedia contributors. Peak signal-to-noise ratio Wikipedia, the free encyclopedia, 2024. URL https://en.wikipedia.org/w/index.php? title=Peak_signal-to-noise_ratio& oldid=1210897995. 2024]. [Online; accessed 4-MarchXiong, Z., Xiong, W., Shi, J., Zhang, H., Song, Y., and Jacobs, N. Groundingbooth: Grounding text-to-image customization. arXiv preprint arXiv:2409.08520, 2024. Yang, B., Gu, S., Zhang, B., Zhang, T., Chen, X., Sun, X., Chen, D., and Wen, F. Paint by example: Exemplar-based In Proceedings image editing with diffusion models. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1838118391, 2023. Ye, H., Zhang, J., Liu, S., Han, X., and Yang, W. Ip-adapter: Text compatible image prompt adapter for text-to-image arXiv preprint arXiv:2308.06721, diffusion models. 2023. Zhang, H., Hong, D., Gao, T., Wang, Y., Shao, J., Wu, X., Wu, Z., and Jiang, Y.-G. Creatilayout: Siamese multimodal diffusion transformer for creative layout-to-image generation. arXiv preprint arXiv:2412.03859, 2024a. Zhang, L., Rao, A., and Agrawala, M. Adding conditional control to text-to-image diffusion models, 2023a. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 586595, 2018. Zhang, Y., Song, Y., Liu, J., Wang, R., Yu, J., Tang, H., Li, H., Tang, X., Hu, Y., Pan, H., et al. Ssr-encoder: Encoding selective subject representation for subject-driven In Proceedings of the IEEE/CVF Confergeneration. ence on Computer Vision and Pattern Recognition, pp. 80698078, 2024b. Zhang, Z., Huang, Z., and Liao, J. Continuous layout editIn Coming of single images with diffusion models. puter Graphics Forum, pp. e14966. Wiley Online Library, 2023b. 11 BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing A. BlobBench Overview and Evaluation Metrics BlobBench Overview As shown in Figure 7, BlobBench is comprehensive benchmark containing 100 curated images evenly distributed across different element-level operations (composition, movement, resizing, removal, and replacement). Each image is annotated with ellipse parameters, foreground masks, and text descriptions. The benchmark includes both real-world and AI-generated images across diverse scenarios like indoor/outdoor scenes, animals, and landscapes. Figure 7: Overview of the BlobBench. Evaluation Metrics For objective evaluation, we consider the following aspects: Identity Preservation. To evaluate element-level appearance preservation, we employ CLIP-I (Radford et al., 2021) and DINO (Caron et al., 2021) scores to measure the appearance similarity between generated and reference images. Grounding Accuracy. To evaluate layout control capability, we first extract masks from generated images using SAM (Kirillov et al., 2023) and fit ellipses or bounding box to these masks. We then compute the Mean Squared Error (MSE) between these fitted grounding annotations and the ground truth to quantify the accuracy of spatial control. Generation Quality and Harmonization. We adopt standard image quality metrics including FID (Heusel et al., 2017) for distribution similarity, PSNR (Wikipedia contributors, 2024) and SSIM (Wang et al., 2004) for pixel-level fidelity, and LPIPS (Zhang et al., 2018) for perceptual quality to evaluate both generation and editing results. For subjective evaluation, we conduct human study with 30 participants rating 20 sets of generated images on three aspects: fidelity, layout accuracy, and visual harmony. Each aspect is scored on scale of 1-5, with 5 being the highest quality. B. Mathematical Relationship Between Ellipses and 2D Gaussian Distributions 2D Gaussian distribution and an ellipse can be mathematically related through their covariance matrix and level sets. Here we explain their conversion: From Gaussian to Ellipse. 2D Gaussian distribution is formulated by its mean µ = (µx, µy) and covariance matrix Σ: Σ = (cid:20) σ2 ρσxσy (cid:21) ρσxσy σ2 The level sets of this distribution form ellipses. For given confidence level α, the corresponding ellipse equation is: where χ2 2(α) is the quantile function for chi-square distribution with 2 degrees of freedom. (x µ)T Σ1(x µ) = χ2 2(α) (14) (15) From Ellipse to Gaussian. Conversely, given an ellipse defined by its center (h, k), semi-major axis a, semi-minor axis b, and rotation angle θ, we can construct the corresponding Gaussian distribution: µ = (cid:19) (cid:18)h (16) BlobCtrl: Unified and Flexible Framework for Element-level Image Generation and Editing where R(θ) is the 2D rotation matrix: Σ = R(θ) (cid:20)a2 0 (cid:21) 0 b2 R(θ)T R(θ) = (cid:20)cos θ sin θ cos θ sin θ (cid:21) (17) (18) This mathematical relationship enables us to seamlessly transition between probabilistic blob representations and geometric ellipse controls in our framework. C. BlobData Curation BlobData is large-scale dataset containing 1.86M samples sourced from BrushData (Ju et al., 2024), featuring images, segmentation masks, fitted ellipse parameters with derived 2D Gaussians, and descriptive texts. As shown in Fig. 8, the BlobData curation process involves multiple steps: Image Filtering. We filter source images to: 1)Retain images with shorter sides exceeding 480 pixels; 2) Keep only images with valid instance segmentation masks; 3) Apply mask filtering to preserve masks with area ratios between 0.01-0.9 of total image area; 4) Exclude masks touching image boundaries. Parameter Extraction. 1) Fit ellipse parameters using OpenCVs ellipse fitting algorithm; 2)Derive corresponding 2D Gaussian distributions; 3) Remove invalid samples with covariance values below 1e-5. Annotation. We generate detailed image descriptions using InternVL-2.5, providing rich textual context for each sample in the dataset. Figure 8: The BlobData curation workflow."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "Peking University",
        "The Chinese University of Hong Kong"
    ]
}