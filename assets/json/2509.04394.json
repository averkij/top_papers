{
    "paper_title": "Transition Models: Rethinking the Generative Learning Objective",
    "authors": [
        "Zidong Wang",
        "Yiyuan Zhang",
        "Xiaoyu Yue",
        "Xiangyu Yue",
        "Yangguang Li",
        "Wanli Ouyang",
        "Lei Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A fundamental dilemma in generative modeling persists: iterative diffusion models achieve outstanding fidelity, but at a significant computational cost, while efficient few-step alternatives are constrained by a hard quality ceiling. This conflict between generation steps and output quality arises from restrictive training objectives that focus exclusively on either infinitesimal dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by introducing an exact, continuous-time dynamics equation that analytically defines state transitions across any finite time interval. This leads to a novel generative paradigm, Transition Models (TiM), which adapt to arbitrary-step transitions, seamlessly traversing the generative trajectory from single leaps to fine-grained refinement with more steps. Despite having only 865M parameters, TiM achieves state-of-the-art performance, surpassing leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across all evaluated step counts. Importantly, unlike previous few-step generators, TiM demonstrates monotonic quality improvement as the sampling budget increases. Additionally, when employing our native-resolution strategy, TiM delivers exceptional fidelity at resolutions up to 4096x4096."
        },
        {
            "title": "Start",
            "content": "Transition Models: Rethinking the Generative Learning Objective Zidong Wang1,2,*, Yiyuan Zhang1,2,,, Xiaoyu Yue2,3, Xiangyu Yue1, Yangguang Li1,, Wanli Ouyang1,2, Lei Bai2, 1MMLab, CUHK 2Shanghai AI Lab 3USYD {wangzd2022, yiyuanzhang.ai}@gmail.com, {xyyue, wlouyang}@ie.cuhk.edu.hk Code: https://github.com/WZDTHU/TiM 5 2 0 2 4 ] . [ 1 4 9 3 4 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "A fundamental dilemma in generative modeling persists: iterative diffusion models achieve outstanding fidelity, but at significant computational cost, while efficient few-step alternatives are constrained by hard quality ceiling. This conflict between generation steps and output quality arises from restrictive training objectives that focus exclusively on either infinitesimal dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by introducing an exact, continuous-time dynamics equation that analytically defines state transitions across any finite time interval t. This leads to novel generative paradigm, Transition Models (TiM), which adapt to arbitrary-step transitions, seamlessly traversing the generative trajectory from single leaps to fine-grained refinement with more steps. Despite having only 865M parameters, TiM achieves state-of-the-art performance, surpassing leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across all evaluated step counts. Importantly, unlike previous few-step generators, TiM demonstrates monotonic quality improvement as the sampling budget increases. Additionally, when employing our native-resolution strategy, TiM delivers exceptional fidelity at resolutions up to 4096 4096. 1. Introduction Diffusion models have emerged as the dominant paradigm in visual content generation, producing state-of-the-art results across various domains [9, 20, 35, 52, 54, 83]. They generate samples from noise via iterative denoising, process that can be formulated as numerical integration of either the reverse-time Stochastic Differential Equation (SDE) or the corresponding Probability-Flow Ordinary Differential Equation (PF-ODE), with related discrete-time solvers also widely used [46, 66, 69]. Despite its effectiveness, iterative denoising often entails large Number of *: Equal contribution. : Project lead. : Corresponding authors: bailei@pjlab.org.cn, liyangguang256@gmail.com. Figure 1. TiMs superior performance across different NFEs, resolutions, and aspect ratios. On the GenEval [27] benchmark, TiM outperforms Flux.1 models [5, 6] at different NFEs (top, 1024 1024), at higher resolutions (middle, 1024 1024 to 4096 4096), and diverse aspect ratios (bottom, 2 : 5 to 5 : 2). Function Evaluations (NFEs)approximately proportional to the number of integration stepsleading to increased inference latency and compute cost. In contrast, recent approaches reduce step counts by avoiding explicit multi-step integration. Consistency models [45, 67, 70] impose PF-ODE self-consistency across different noise levels, while distribution-distillation methods [42, 61, 62, 84, 93] train students to approximate teacher distributions with fewer denoising steps. Shortcut [21], FlowMap [7, 57], and MeanFlow [26, 53] learn the average (shortcut) velocity along the flow-matching trajectory via self-consistency objective. The principle is that single large step should approximate the integral of 1 all smaller, instantaneous steps. However, by averaging the entire trajectory, they irrevocably discard the fine-grained local dynamics necessary for high-fidelity refinement. This leads to performance saturationwhile effective for fewstep generation, it offers no gains from additional sampling budget. Moreover, despite these methods deliver strong few-step results; their computequality scaling is typically weaker than that of high-NFE diffusion models: quality gains plateau after only few steps, and asymptotic performance remains below traditional multi-step diffusion. Thus, the entire field converges on fundamental, yet flawed, compromise [26, 31, 44, 70, 84]: models either achieve high fidelity at the cost of computational efficiency (e.g., diffusion models), or they gain efficiency by sacrificing the very dynamics needed for high-fidelity refinement (e.g., few-step models). The root of this dilemma is not architectural, but learning objective. It stems from foundational choice in how these models are taught to generate, not the specific components they are built from. This trade-off is direct and inevitable consequence of the chosen granularity of supervision. On one hand, local training methods that model instantaneous dynamics (such as those consistent with PF-ODEs/SDEs [31, 44, 69]) achieve high accuracy with small step sizes (t) and scale well to manystep generation. However, their performance degrades significantly in few-step regimes. On the other hand, finitehorizon training, which learns direct mapping over fixed interval (flow-map and consistency models [7, 26, 70]), excels at few-step generation. Yet, these models see diminishing returns from additional intermediate steps unless specifically trained with complex, multi-interval objectives. This reveals persistent dilemma: objectives that model instantaneous dynamics and those that learn finite-interval mappings each entail inherent limitations. This motivates the question: What is an appropriate learning objective for generative models? We attempt to answer this question from the following perspectives: 1) Diffusion training [46, 47, 91] learns local PF-ODE field whose numerical integration is accurate only in the small-step limit 0. With large steps, the discretization error dominates; therefore, the objective should be flexible in terms of step sizes. 2) Few-step objectives supervise an endpoint map but do not learn compositional flow: without an approximate semigroup over time, extra steps change the path rather than refine it, causing schedule sensitivity and early saturation. Therefore, the objective requires consistency along the trajectory, where intermediate steps act as refinements along single trajectory, rather than deviations onto new ones, which makes the sampler insensitive to step schedules and enables steady quality improvements with more steps. Consequently, we argue that generative model should learn versatile denoising operator, parameterized by the desired interval t. By learning the transitions between any state xt to previous state xtt for an arbitrary t, the generative model is no longer approximating differential equation or statistic map. Instead, it is learning the solution manifold1 of the generative process itself. This formulation inherently unifies the local and finite-horizon perspectives, yielding sampler that is both powerful fewstep generator and precise, refinable integrator. Since the training objective is to learn the transitions between any state to previous state, thus, it is named as Transition Models (TiM), which parameterize state-to-state transitions along the PF-ODE trajectory for arbitrary time intervals. We validate TiMs effectiveness through extensive experiments on text-to-image and class-guided image generation. As shown in Figure 1, TiM shows superior performance across different NFEs, resolutions, and aspect ratios. On the GenEval [27] benchmark, our compact 865M parameter model, TiM, establishes new state-of-the-art. It achieves score of 0.67 with single function evaluation (1-NFE) and scales to 0.83 at 128-NFE, outperforming billion-scale industrial models including SD3.5-Large [20] (8B) and FLUX.1-Dev [5] (12B). 2. Related Work Diffusion and Consistency Models. Generative modeling has seen two dominant paradigms. Diffusion models [31, 35, 44] iteratively solve PF-ODE/SDE, achieving high quality but requiring many function evaluations (NFEs). In contrast, Consistency Models [70] learn direct mapping for few-step generation but suffer from performance saturation and complex training requirements (e.g., pre-training and stabilization [45, 67]). While recent methods like FlowMap [7, 57] and MeanFlow [26, 53] enable training CM-like models from scratch, they inherit the same limitation of stagnating quality with more steps. To break this impasse, we propose new learning principle: mastering state transitions over arbitrary time intervals. This transforms the model from brittle integrator or fixed-endpoint mapper into robust navigator on the data manifold, preserving few-step efficiency while supporting monotonic refinement by using more steps. Text-to-Image Generation with Few-steps. Efficient textto-image (T2I) sampling is currently dominated by distillation. These methods fall into two main camps: distribution distillation (e.g., SD-Turbo [61, 62], DMD [84, 85]), which matches the teachers output distribution, and trajectory distillation (e.g., LCM [48], PCM [77]), which mimics its generation path. Hybrid methods [55] combine both. 1solution manifold of PF-ODE is the high-dimensional geometric surface formed by the collection of all possible generative trajectories that lead from noise to data. 2 Figure 2. Illustration of Different Generative Paradigms. While conventional diffusion models learn the local vector field and few-step models learn fixed endpoint map (a single large step), our Transition Models (TiM) are trained to master arbitrary state-to-state transitions. This approach allows TiM to learn the entire solution manifold of the generative process, unifying the few-step and many-step regimes within single, powerful model. However, all these approaches are fundamentally limited: 1) they require large, pre-trained teacher, leading to complex and costly pipelines, and 2) they produce brittle, few-step-only models whose quality stagnates or degrades with more steps. We bypass these limitations entirely by introducing TiM, the first T2I generator trained from scratch that masters arbitrary-step sampling, delivering strong fewstep results that monotonically improve with more compute. 3. Transition Models In this section, we first analyze the limitations of PFODE supervision in diffusion models, which constrain the state transition to local, infinitesimal interval. To address the limitations, we generalize diffusions local state transition to an arbitrary-interval state transition, as illustrated in Fig. 2, from which we we derive novel mathematical identity that links the state xt, the interval t, and the network fθ. From this identity, we formulate training objective governing state evolution over any interval t, and further propose two theoretically motivated improvements for scalable and stable training. Finally, we present the architecture improvements for effective transition modeling. 3.1. Limitation of PF-ODE Supervision Given the noise distribution ε (0, I) and the data distribution pdata(x), diffusion models learn to map the noise distribution to the data distribution. Given time range [0, ], the forward process utilizes coefficients αt and σt, such that xt = αtx + σtε, which can be described by an SDE [69]: dxt = f(xt, t)dt + g(t)dw, (1) where is the standard Wiener process, f(xt, t) = αt xt αt is the drift coefficient and g(t) = 2σt σt 2 αt σ2 is the αt diffusion coefficient [35, 45, 69, 72]. Anderson [3] and Song et al. [69] have shown that the forward process can be reversed by solving the reverse-time SDE from or equivalently the probability flow ODE (PF-ODE)2: dxt dt = f(xt, t) 1 2 dσt dt where xt log pt(xt) = ε σt dαt dt + = g(t)2xt log pt(xt) ε, (2) denotes the score function. Thus, diffusion model can be parameterized as fθ(xt, t) = Fθ(xt, cnoise(t)), where θ denotes the parameters of the neural network and cnoise(t) is the time scaling function. The training objective can be given by: Ex,ε,t[w(t)d(fθ(xt, t) (ˆαtx + ˆσtε))], (3) where ˆαt and ˆσt are the coefficients of diffusion target, w(t) is weighting function, d(, ) is metric function such as the L2 loss d(x, y) = y2 2. Despite different transports 3 have instantiate coefficients αt and σt, the training objectives are equivalent to supervising the PF-ODE field 4. During sampling, diffusion models 2Song et al. [69] have shown that the PF-ODE trajectory has the same marginal probability as the reverse-time SDE: dxt = [f(xt, t) 1 2 g(t)2xt log pt(xt)]dt + g(t)dw. 3For convenience, we elaborate the coefficients αt, σt, ˆαt, and ˆσt of different diffusion transports in Tab. 4For example, in VE-SDE [69],with coefficients αt = 1, σt = t, the dt = ε, and the training objective is ε. In OT-FM [43], dt = ε x, which directly PF-ODE is: dxt with αt = 1 t, σt = t, the PF-ODE is: dxt matches the training objective. 3 solve this PF-ODE, integrated from = to = 0 using numerical solvers. To reduce discretization error and preserve the learned continuous-time dynamics, practical solvers [47, 66, 91] typically require small step size (i.e., 0) or many sub-steps per interval (i.e., high-order solvers), thus inducing huge NFEs. 3.2. State Transition The derivation begins with the general mathematical form for state transition between points (xt, xr) on PF-ODE trajectory, as given in Eq. (6). The central principle is to treat this form not as numerical approximation, but as an exact identity that must hold for any interval = r. It allows us to formulate general state transition dynamic (Eq. (8)) that is valid across any interval. Consequently, the models training objective is no longer constrained to approximating local solution of the PF-ODE. Instead, it is trained to learn the entire solution manifold of the generative process. By internalizing this global structure, the model inherently acquires the ability to perform inference over arbitrary step sizes, from large, single leaps to finegrained, iterative refinement. We illustrate our derivation process step-by-step as follows: State Transition. Given noisy state xt = αtx + σtε, diffusion model fθ(xt, t) is optimized towards the target ˆαtx + ˆσtε, leading to the x-prediction and ε-prediction: ˆx = ˆσtxt σtfθ(xt, t) ˆσtαt ˆαtσt , ˆε = αtfθ(xt, t) ˆαtxt ˆσtαt ˆαtσt . (4) Using the prediction ˆx and ˆε, arbitrary previous state xr (r < t) can be represented as: xr = αr ˆx + σr ˆε = (αr ˆσt σr ˆαt)xt + (σrαt αrσt)fθ(xt, t) ˆσtαt ˆαtσt . (5) This represents the general form of first-order state transition on the PF-ODE Trajectory. State Transition Identity. Different from the diffusion model, our transition model learns the state transition fθ(xt, t, r) = Fθ(xt, cnoise(t), cnoise(r)) between state xt and xr. By introducing fθ(xt, t, r) to Eq. (5), we obtain: xr = (αr ˆσt σr ˆαt)xt + (σrαt αrσt)fθ(xt, t, r) ˆσtαt ˆαtσt . (6) Here, we define At,r := αr ˆσtσr ˆαt , Bt,r := σrαtαrσt , ˆσtαt ˆαtσt ˆσtαt ˆαtσt and fθ,t,r := fθ(xt, t, r) for simplicity, then we differentiate both sides with respect to and rearranging yields: dxr dt = dt (At,rxt + Bt,rfθ,t,r) = xt dAt,r dt + At,r dxt dt = fθ,t,r dBt,r dt Bt,r dfθ,t,r dt , (7) 4 which can be further simplified as follows (detailed in Appx. A.1): d(Bt,r ( ˆαtx + ˆσtε fθ,t,r)) dt = 0 = ( ˆαtx + ˆσtε fθ,t,r (cid:125) (cid:123)(cid:122) (cid:124) PF-ODE supervision ) dBt,r dt + Bt,r d( ˆαtx + ˆσtε fθ,t,r) dt (cid:123)(cid:122) time-slope matching (cid:125) (cid:124) = 0. (8) We denote Equation (8) as the State Transition Identity, product-derivative invariant. The State Transition Identity, dt (Bt,r h(t)) = 0, where h(t) = ˆαtx + ˆσtε fθ,t,r is the instantaneous residual, imposes powerful two-fold constraint on the generative model fθ. Implicit Trajectory Consistency: The identity dictates that the weighted residual Bt,rh(t) must be constant for any starting time leading to the same target xr. This directly enforces path consistency: the direct map (t r) must be equivalent to any composition of intermediate steps, such as (t s) (s r). This property (Eq. (8) ), absent in standard consistency models, is the core mechanism that makes TiM robust to sampling schedules and enables monotonic refinement. dt Bt,r)h(t) + Bt,r( Time-Slope Matching: Unpacking the product rule reveals that ( dt h(t)) = 0. Unlike conventional diffusion training, which only minimizes the residuals value (h(t) 0), our objective forces the model to also minimize the residuals temporal derivative ( dt h(t) 0). This higher-order supervision compels the model to learn smoother solution manifold, preserving coherence during large-step sampling and ensuring stable refinement with smaller steps. Derived from State Transition Identity (Eq. (8)), we obtain the learning target ˆf : ˆf = ˆαtx + ˆσtε + (cid:18) ˆαt dt + dˆσt dt ε dfθ,t,r dt (cid:19) , (9) Bt,r dBt,r dt where θ indicates the fixed network parameter θ and dfθ,t,r dt is the time derivative of the network. 3.3. Scalability and Stability in TiM Training Remark 1: Making TiM Training Scalable. dfθ ,t,r dt critical challenge in implementing our training target (Eq. (9)) is the computation of the networks time deriva- . Prior work, such as MeanFlow [26, 53, 57] tive, and sCM [45], relies on the Jacobian-Vector Product (JVP) for this task. However, JVP presents fundamental roadblock to scalability. It is not only compute-intensive but, more cripplingly, its reliance on backward-mode automatic differentiation is incompatible with essential training optimizations, including FlashAttention [17] and distributed frameworks of FSDP [89]. This incompatibility has effectively rendered JVP-based methods impractical for training billion-parameter foundation models. Method Operator FLOPs Latency Training FID Throughput Memory NFE=1 NFE=8 NFE=50 (G) 48.29 24.14 (ms) 213.14 110.08 (/s) 1.80 2.40 JVP DDE (GiB) 14.89 15. 49.75 49.91 26.22 26.09 18.11 17.99 Table 1. Derivative Calculation Comparison. We utilize TiMB/4 model for latency, throughput, and memory measurement, with batch size of 256 on NVIDIA-A100 GPU using BF16 precision. We break this barrier with the Differential Derivation Equation (DDE), principled and highly efficient finitedifference approximation: dfθ,t,r dt fθ (xt+ϵ, + ϵ, r) fθ(xtϵ, ϵ, r) 2ϵ . (10) As shown in Tab. 1, DDE is not only 2 faster than JVP but, crucially, its forward-pass-only structure is natively compatible with FSDP. This compatibility transforms previously unscalable training process into one ready for largescale deployment, making TiM the first model of its kind practical for from-scratch, billion-parameter pre-training 5. Remark 2: Making TiM Training Stable. In addition to scalability, key challenge in training with arbitrary intervals is managing gradient variance. For example, transitions over very large intervals (t t) are easier to make loss spikes. To mitigate this, we introduce loss weighting scheme that prioritizes short-interval transitions, which are more frequent and provide more stable learning signal. The weighting function, w(t, r), is composition of time-warping function τ () and kernel function k(, ): w(t, r) = k(τ (t), τ (r)). (11) Here, τ () is monotonic function that re-parameterizes the time axis. For our final model, we use tangent space transformation, which effectively stretches the time domain, yielding the specific weighting: w(t, r) = (σdata + tan(t) tan(r)) 1 2 , (12) where σdata is the standard deviation of the clean data 6. Learning Objective. Our theoretical framework culminates in scalable and stable learning objective. We train the network fθ to predict the dynamic target ˆf in Eq. (9). To manage gradient variance and ensure stable convergence, this is weighted by the interval function w(t, r) from Eq. (11). This results in the final TiM objective: Ex,ε,t,r (cid:104) w(t, r) (cid:16) fθ(xt, t, r) ˆf (cid:17)(cid:105) . (13) 5We provide detailed analysis of DDE in the Appendix Tab. 10 6In the Appendix, we conduct an in-depth comparison of alternative weighting schemes is provided in Tab. 12. 5 This objective generalizes the standard PF-ODE supervision to arbitrary state-to-state transitions. The practical implementation, enabled by our efficient DDE calculation, is detailed in Algorithm 1. We summarize the specific parameterizations for various transport choices in Tab. 7. 3.4. Improved Architectures We conduct series of experiments to explore architectural modifications based on DiT [52] for effective state transition learning in Tab. 4. We illustrate the exact architectures in the Appendix A. Decoupled Time and Interval Embeddings. To enable the model to distinguish between the absolute time and the transition interval t, we introduce decoupled embedding strategy. We employ two independent time encoders, ϕt and ϕt, to parameterize these two quantities. Their outputs are summed to form the final time-conditioning vector: Et,t = ϕt(t) + ϕt(t). (14) This time embedding is then integrated with task-specific conditioning as follows: For class-guided generation, the class embedding Ec is added to the time embedding, and the resulting sum, Et,t + Ec, modulates the AdaLN layers of the model. For text-to-image generation, the conditioning pathways are separated. The time embedding Et,t solely modulates the AdaLN layers, while textual features from the prompt are injected via dedicated cross-attention mechanisms. Interval-Aware Attention. We assume that the optimal way to model spatial dependencies is conditional on the transition interval t. large step (t t) may require global, coarse-grained restructuring, while small step (t 0) demands fine-grained, local refinement. Standard self-attention, which is agnostic to this context, is inappropriate for this task. We therefore introduce the IntervalAware Attention, mechanism that infuses the transition intervals magnitude directly into the query, key, and value computations. Specifically, we project both the spatial tokens and the interval embedding Et into shared representational space before the attention calculation: = zWq + bq + EtW q, = zWk + bk + EtW k, = zWv + bv + EtW v. (15) Here, (Wq, Wk, Wv) are the primary projection matrices for the spatial tokens, while (W v) are dedicated projection matrices that modulate the attention based on the interval embedding. k, q, Model Param. NFE Overall Single Obj. Two Obj. Counting Colors Position Attr. Binding Autoregressive Models Emu3-Gen [79] GPT-4o [1] - - - - Multi-step Diffusion Models SD2.1 [56] SD-XL [54] Seedream2.0 [28] SD3.5-Medium [20] SD3.5-Large [20] SANA-1.5 [81] FLUX.1-Dev [5] 865M 100 100 2.6B - - 100 2B 128 8B 40 4.6B 128 12B Few-step Distilled Diffusion Models 8 SDXL-LCM [48] 8 SDXL-Turbo [62] 8 Hyper-SDXL [55] 8 SANA-Sprint [16] 8 SD3.5-Turbo [61] 1 FLUX.1-Schnell [5] 2.6B 2.6B 2.6B 1.6B 8B 12B Transition Models TiM 865M 1 8 128 0.54 0.84 0.50 0.55 0.84 0.63 0.69 0.81 0.65 0.40 0.50 0.46 0.72 0.66 0.68 0.67 0.76 0.83 0.98 0. 0.98 0.98 1.0 0.98 0.99 0.99 0.98 0.97 0.99 0.92 1.0 0.99 0.99 0.98 0.99 1.0 0.71 0.92 0.51 0.74 0.98 0.78 0.89 0.93 0.79 0.50 0.75 0.58 0.88 0.81 0. 0.75 0.87 0.91 0.34 0.85 0.44 0.39 0.91 0.50 0.67 0.86 0.69 0.12 0.07 0.26 0.56 0.62 0.63 0.52 0.61 0.73 0.81 0. 0.85 0.85 0.94 0.81 0.81 0.84 0.76 0.67 0.89 0.78 0.87 0.79 0.78 0.80 0.88 0.91 0.17 0.75 0.07 0.15 0.47 0.24 0.24 0.59 0.21 0.09 0.11 0.11 0.56 0.25 0. 0.54 0.63 0.73 0.21 0.61 0.17 0.23 0.75 0.52 0.56 0.65 0.48 0.07 0.20 0.15 0.47 0.48 0.53 0.44 0.61 0.71 Table 2. System-level quality comparison of TiM and SOTA methods on GenEval benchmark. In the table, 1-NFE denotes single sampling step; 8-NFE corresponds to four sampling steps with CFG, and other multi-NFE follow the same convention. Compared with multi-step diffusion models and few-step distilled models, TiM offers any-step generation, delivering strong few-step performance and exhibiting consistent, stable improvements as NFE increases. Model Param. NFE MJHQ30K DPGBench FID CLIP Overall Global Entity Attribute Relation Other PixArt-α [12] PixArt-Σ [14] SDXL [54] Playground v2.5 [39] Hunyuan-DiT [41] SD3.5-Medium [20] SD3.5-Turbo [61] SD3.5-Large [20] FLUX.1-Schnell [6] FLUX.1-dev [5] 610M 100 610M 100 100 2.6B 100 2.6B 100 1.5B 100 2B 8 8B 32 8B 8 12B 32 12B TiM 865M 1 8 32 6.14 6.15 6.63 6.09 6.54 11.92 11.97 14.68 7.94 9.19 6.68 5.28 5. 27.55 28.26 29.03 29.13 28.19 27.83 27.35 27.88 28.14 27.27 24.80 26.10 26.31 71.11 80.54 74.65 75.47 78.87 84.08 79.03 83.21 84.94 83.32 74.93 81.30 82.71 74.97 86.89 83.27 83.06 84.59 87.90 80.12 84.27 86.62 81.46 82.98 82.01 82. 79.32 82.89 82.43 82.59 80.59 91.01 86.13 88.99 90.82 90.02 83.64 88.31 89.40 78.60 88.94 80.91 81.20 88.01 88.83 84.73 87.35 88.35 87.50 83.54 87.81 88.48 82.57 86.59 86.76 84.08 74.36 80.70 91.86 93.28 93.45 92.72 91.99 93.37 93. 76.96 87.68 80.41 83.50 86.41 88.68 78.29 80.35 82.00 82.39 63.20 70.80 79.20 Table 3. System-level quality comparison on MJHQ30K and DPGBench benchmarks. 4. Experiments 4.1. Setup We use SD-VAE [56] for ImageNet-256 256 experiments and DC-AE [13] for text-to-image (T2I) experiments. Model architecture follows DiT [52], except the modifica6 tions in Sec. 3.4. For T2I generation, we use 33M images from public datasets [2, 11, 15, 18, 33, 6365]. We train the T2I model with 865M parameters using the nativeresolution training strategies for about 30 days using 16 NVIDIA-A100 GPUs. Gemma3-1B-it [74] is utilized as text encoder. See more details in Appx. C.2. We report the Method Training Objective (a) Baseline (SiT-B/4 [49]) (b) TiM-B/4 (w/ JVP) (c) TiM-B/4 (w/ DDE) Architecture (d) Vanilla Architecture (e) + Decoupled Time Embedding (De-TE) (f) + Interval-Aware Attention (IA-Attn) (g) + De-TE + IA-Attn Training Strategy (on top of (g)) (h) + Time-weighting NFE=1 NFE=8 NFE=50 309.5 49.75 49.91 56.22 49.91 48.38 48.30 77.26 26.22 26.09 28.75 26.09 26.10 25.05 20.35 18.11 17. 20.37 17.99 17.85 17.43 47.46 24.62 17.10 Table 4. Ablation studies of Transition Models on the standard ImageNet-256 benchmark (FID). We analyze the effect of training objectives, architecture, and training strategies. Method NFE=1 NFE=8 NFE=32 NFE=128 SD3.5-Turbo [61] FLUX.1-Schnell [6] SD3.5-Large [20] FLUX.1-Dev [5] TiM 0.50 0.68 0.00 0.00 0.67 0.66 0.67 0.50 0. 0.76 0.70 0.63 0.69 0.64 0.80 0.70 0.58 0.70 0.65 0.83 Table 5. Benchmarking generation quality across NFEs on the GenEval benchmark (score). We compare single TiM model against diffusion models (i.e., SD3.5-Large and FLUX.1-Dev) and distilled models (i.e., SD3.5-Turbo and FLUX.1-Schnell). Number of Function Evaluations (NFE) to quantify sampling steps. When classifier-free guidance (CFG) is used, NFE doubles, because each step requires two model evaluations: one conditioned and one unconditioned. We provide the T2I experiments and ablation experiments on ImageNet256 256 below and more results on class-guided image generation in Appxs. D.1 and D.2. Native-Resolution Training. Previous methods [25, 28, 80] have shown the success of native-resolution training on resolution generalization; thus, we adopt this strategy for text-to-image generation, which preserves the original image resolution and aspect ratio information to the greatest extent possible. Given the wide resolution range, we increase noise for higher-resolution images and decrease it for lower-resolution ones. Following Esser et al. [20], we therefore apply resolution-dependent timestep shifting. Please see more details in Appx. C.2. Sampling. Since TiM learns the arbitrary state transition on the diffusion trajectory, it supports arbitrary-step sampling when producing images. Given set of timesteps = {ti}0 i=N where tN = T, t0 = 0, we obtain the next state xtn1 given the current state xtn based on Eq. (5), as illustrated in Algorithm 2. 4.2. Text-to-Image Generation 3, 5 and 6). It achieves an SOTA FID of 5.25 on MJHQ30K while resolving the core speed-quality trade-off. On GenEval, TiMs 1-NFE performance surpasses 8-NFE distilled models (e.g., SDXL-Turbo), while its 128-NFE quality rivals closed-source models. This unique scalability starkly contrasts with competitors like SD3.5-Large, which collapse at few steps, and FLUX.1-Schnell, which degrades at many steps. TiM alone shows monotonic quality improvement with NFE. This efficiency is further proven on DPGBench, where 8-NFE TiM outperforms 100-NFE baselines like SDXL. Finally, TiM demonstrates superior generalization across diverse resolutions and aspect ratios, validating its fundamentally more robust design. 4.3. Ablation Studies We conduct series of ablation studies to validate our design choices, building from standard diffusion baseline (SiT-B/4 [49] here) to our final TiM configuration. We use 131M parameter model trained on ImageNet-256 256 for 80 epochs and report FID at 1, 8, and 50 NFEs, corresponding to single-step, few-step, and multi-step generation7. The results are summarized in Table 4. Transition Objective. As shown in Table 4 (a vs. c), switching from the standard SiT objective to our TiM objective delivers dramatic improvement in few-step performance, reducing the 1-NFE FID by over 6 (309.5 49.91) while maintaining strong many-step quality. This confirms that learning arbitrary transitions is critical for few-step generation. Furthermore, our proposed DDE method (c) achieves this performance while being far more scalable than JVP (b), making large-scale training practical. Architectural Contributions. We next analyze the impact of our architectural innovations on top of vanilla TiM baseline (d). Both the Decoupled Time Embedding (e) and Interval-Aware Attention (f) individually provide substantial gains across all sampling steps. Crucially, combining them (g) yields the best performance, lowering the 8-NFE FID from 33.08 to 29.21. This demonstrates that enabling the model to explicitly reason about both absolute time and the transition interval is complementary and essential for optimal performance. Training Strategy. Building on our best architecture (g), we apply our proposed interval weighting scheme. This final step provides consistent boost across the board (h), further refining the model and achieving our best FID scores of 47.46 / 24.62 / 17.10. 5. Conclusion and Limitations This paper introduces the Transition Models (TiM), novel generative model that learns to navigate the entire generative trajectory with unprecedented flexibility. The success TiM establishes new state-of-the-art in performance, efficiency, and flexibility across diverse benchmarks (Tabs. 2, 71-NFE: single sampling step; 8-NFE: 4 sampling steps with CFG; 50NFE: 25 sampling steps with CFG. 7 Method NFE 2 : 5 9 : 16 Aspect Ratio 3 : 2 2 : 3 16 : 9 5 : 1280 1536 Resolution 2560 2048 3072 4096 SD3.5-Turbo [61] FLUX.1-Schnell [6] TiM SD3.5-Large [20] FLUX.1-Dev [5] TiM 8 8 8 32 32 32 0.57 0.55 0.25 0.48 0.66 0.53 0.61 0. 0.48 0.59 0.67 0.60 0.63 0.63 0.60 0.62 0.72 0.58 0.62 0.64 0.57 0.60 0.72 0.30 0.59 0. 0.16 0.59 0.62 0.57 0.56 0.57 0.64 0.61 0.64 0.70 0.63 0.62 0.75 0.58 0.61 0.58 0.69 0.46 0.49 0.49 0. 0.14 0.48 0.27 0.62 0.45 0.59 0.39 0.53 Table 6. Benchmarking resolution generation capabilities on GenEval Benchmark. For aspect ratio generalization, the exact resolutions are: {1024 2560, 1024 1856, 1024 1536, 1536 1024, 1856 1024, 2560 1024}. : when GenEval score falls below 0.10, we interpret it as evidence that the model fails to generalize to that resolution. Figure 3. Qualitative Analysis between TiM and existing methods under different NFEs. TiM delivers superior fidelity and text alignment across all NFEs. In contrast, multi-step diffusion and few-step distilled models exhibit pronounced stepquality trade-offs: SDXL, SD3.5-Large, and FLUX.1-Dev fail to generate images at low NFEs, while SDXL-Turbo, SD3.5-Turbo, and FLUX.1-Schnell produce over-saturated outputs at high NFEs. of our compact 865M model in outperforming multi-billion parameter giants is not just new state-of-the-art; it is testament to more efficient and powerful paradigm. By achieving monotonic quality improvement from one step to many, and scaling to ultra-high resolutions, TiM demonstrates that unified model is not only possible but superior. We believe this work paves the way for new generation of foundation models that are at once efficient, scalable, and promising in their creative potential. Limitations. Although TiM delivers significant contribution to the fundamental generative models, ensuring content safety and controllability remains an open challenge, and model fidelity can degrade in scenarios requiring fine-grained detail, such as rendering text and hands. We also observe occasional artifacts at high resolutions (e.g., 3072 4096), likely attributable to biases in the underlying autoencoder."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 6 [2] adebyollin. Megalith-huggingface. https://huggingface.co/datasets/madebyollin/megalith-10m. 6 [3] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313 326, 1982. 3 [4] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. 23 [5] black-forest labs. Flux.1-dev. https://huggingface. co/black-forest-labs/FLUX.1-dev, . 1, 2, 6, 7, 8 [6] black-forest labs. https : / / huggingface . co / black - forest - labs / FLUX.1-schnell, . 1, 6, 7, 8 Flux.1-schnell. [7] Nicholas Matthew Boffi, Michael Samuel Albergo, and Eric Vanden-Eijnden. Flow map matching with stochastic interpolants: mathematical framework for consistency models. Transactions on Machine Learning Research, 2025. 1, 2 [8] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 22, 23 [9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. Accessed: 2024-5-1. 1 [10] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. [11] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text preIn CVPR, training to recognize long-tail visual concepts. 2021. 6 [12] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 6 [13] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. 6, 19, 21 [14] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. 6 9 [15] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. [16] Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Song Han, and Enze Xie. Sana-sprint: One-step diffusion with continuous-time consistency distillation. arXiv preprint arXiv:2503.09641, 2025. 6 [17] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 4 [18] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021. 6 [19] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021. 21, 23 [20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. 2024. 1, 2, 6, 7, 8, 20 [21] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter arXiv Abbeel. One step diffusion via shortcut models. preprint arXiv:2410.12557, 2024. 1, 17, 19, 22 [22] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. 22, 23 [23] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 22 [24] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. [25] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. 7 [26] Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. 1, 2, 4, 17, 19, 22 [27] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 1, 2 [28] Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. 6, 7 [29] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision transformers for image generation. arXiv preprint arXiv:2312.02139, 2023. 23 [30] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. 21 [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2, 16, 20 [32] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. 2023. 22, 23 [33] jackyhate. text-to-image-2m. https://huggingface.co/datasets/jackyhate/text-to-image2M. 6 [34] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1012410134, 2023. [35] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. NeurIPS, 2022. 1, 2, 3, 15, 16, 17, 20 [36] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the In Proceedings of training dynamics of diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. 16, 17, 20, 23 [37] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. 17, 19 [38] T. Kynkaanniemi, T. Karras, S. Laine, and Lehtinen, J.and Aila. Improved precision and recall metric for assessing generative models. NeurIPS, 2019. 21 [39] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. 6 [40] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 22 [41] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 6 [42] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. 1 [43] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3, 13, 16, 19, 20 10 [44] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 2, 16, 20 [45] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. 1, 2, 3, 4, 13, 15, 16, 17, 20 [46] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:5775 5787, 2022. 1, [47] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pages 122, 2025. 2, 4 [48] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2, 6 [49] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 7, 22, 23 [50] C. Nash, J. Menick, S. Dieleman, and P. Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. 21 [51] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 4555, 2025. 22 [52] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 1, 5, 6, 22, [53] Yansong Peng, Kai Zhu, Yu Liu, Pingyu Wu, Hebei Li, Xiaoyan Sun, and Feng Wu. Flow-anchored consistency models. arXiv preprint arXiv:2507.03738, 2025. 1, 2, 4, 17 [54] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 6 [55] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. Advances in Neural Information Processing Systems, 37:117340117362, 2025. 2, 6 [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 6, 21, 22 [57] Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your flow: Scaling continuous-time flow map distillation. arXiv preprint arXiv:2506.14603, 2025. 1, 2, 4 [58] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. RadImproved techniques for training gans. ford, and Chen. NeurIPS, 2016. [59] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, 2022. 23 [60] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 22 [61] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion In SIGGRAPH Asia 2024 Conference Papers, distillation. pages 111, 2024. 1, 2, 6, 7, 8 [62] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2024. 1, 2, 6 [63] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 6 [64] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. 2018. [65] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From pixels to prose: large dataset of dense image captions. arXiv preprint arXiv:2406.10328, 2024. [66] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 1, 4 and Stefano Ermon. arXiv preprint [67] Yang Song and Prafulla Dhariwal. Improved techniques for In The Twelfth International training consistency models. Conference on Learning Representations. 1, 2, 17, 22 [68] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. NeurIPS, 2019. 16 [69] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1, 2, 3, 13, 14, 15, 16, [70] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, pages 3221132252. PMLR, 2023. 1, 2, 17, 18 [71] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 22 [72] Peng Sun, Yi Jiang, and Tao Lin. Unified continuous generative models. arXiv preprint arXiv:2505.07447, 2025. 3, 13, 15 [73] Zhicong Tang, Jianmin Bao, Dong Chen, and Baining Guo. arXiv Diffusion models without classifier-free guidance. preprint arXiv:2502.12154, 2025. 20 [74] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, arXiv preprint et al. arXiv:2503.19786, 2025. 6 Gemma 3 technical report. [75] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. 22, 23 [76] Fu-Yun Wang, Zhengyang Geng, and Hongsheng Li. Stable consistency tuning: Understanding and improving consistency models. arXiv preprint arXiv:2410.18958, 2024. 23 [77] Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in Neural Information Processing Systems, 37:8395184009, 2025. 2, 17, 19 [78] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Exploring dcn-like architecture for fast image generation with arbitrary resolution. Advances in Neural Information Processing Systems, 37:8795987977, 2024. 22, [79] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 6 [80] Zidong Wang, Lei Bai, Xiangyu Yue, Wanli Ouyang, and Yiyuan Zhang. Native-resolution image synthesis. arXiv preprint arXiv:2506.03131, 2025. 7 [81] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. 6 [82] Wanghan Xu, Xiaoyu Yue, Zidong Wang, Yao Teng, Wenlong Zhang, Xihui Liu, Luping Zhou, Wanli Ouyang, and Lei Bai. Exploring representation-aligned latent space for better generation. arXiv preprint arXiv:2502.00359, 2025. 22 [83] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1 [84] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. 1, 2 [85] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in Neural Information Processing Systems, 37: 4745547487, 2025. [86] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 22, 23 [87] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 22, 23 [88] Xiaoyu Yue, Zidong Wang, Zeyu Lu, Shuyang Sun, Meng Wei, Wanli Ouyang, Lei Bai, and Luping Zhou. Diffusion models need visual priors for image generation. arXiv preprint arXiv:2410.08531, 2024. 22 [89] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, ChienChin Huang, Min Xu, Less Wright, Hamid Shojanazeri, experiMyle Ott, Sam Shleifer, et al. ences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. 4, 20 Pytorch fsdp: [90] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. 2023. 23 [91] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpmsolver-v3: Improved diffusion ode solver with empirical model statistics. Advances in Neural Information Processing Systems, 36:5550255542, 2023. 2, 4 [92] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025. 22 [93] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024."
        },
        {
            "title": "Appendix",
            "content": "We include additional derivations, experimental details, and results in the appendix. In Appx. A, we provide detailed formula derivation of the State Transition Identity, training, and sampling algorithms. In Appx. B, we discuss TiMs relationships with existing methods, including diffusion models, consistency models, and other training approaches. In Appx. C, we provide the implementation details of text-to-image generation, including native-resolution training, resolution-dependent timestep shifting, model-guidance training, and from-scratch training. In Appx. D, we provide additional ablation results and results on class-guided image generation. In Appx. E, we provide more qualitative results of TiM. A. Transition Model Framework In this section, we first provide the derivation of the TiM identity equation Eq. (8). Then we provide the training and sampling algorithms. Finally, we provide systematic analysis of the connections with other existing methods. A.1. TiM Identity Equation Derivation We demonstrate the derivation from Eq. (7) to the TiM identity equation Eq. (8). We start from the detailed expansion of each term of Eq. (7). Firstly, we have: xt = αtx + σtε, dαt dxt dt dt + = dσt dt ε, (16) (17) where dxt dt αr ˆσtσr ˆαt ˆσtαt ˆαtσt is the PF-ODE of diffusion and Eq. (17) has already been proved in previous works [43, 45, 69, 72]. For At,r = , Bt,r = σrαtαrσt ˆσtαt ˆαtσt , we have: dAt,r dt dBt,r dt = = dAt,r dαt dBt,r dαt dαt dt dαt dt + + dAt,r dσt dBt,r dσt dσt dt dσt dt + + dAt,r dˆαt dBt,r dˆαt dˆαt dt dˆαt dt + + dAt,r dˆσt dBt,r dˆσt dˆσt dt dˆσt dt (18) (19) We use Ct,r = ˆσtαt ˆαtσt for simplicity, which is the denominator of At,r and Bt,r. For Eq. (18) and Eq. (19), each term is calculated as: dAt,r dαt dAt,r dσt dAt,r ˆαt dAt,r dˆσt At,r Ct,r = (σr ˆαtαr ˆσt)ˆσt = (αr ˆσtσr ˆαt) ˆαt = (αrσtσrαt)σt = (αtσrαrσt)αt (ˆσtαt ˆαtσt)2 = ˆσt (ˆσtαt ˆαtσt)2 = ˆαt (ˆσtαt ˆαtσt)2 = ˆσt (ˆσtαt ˆαtσt)2 = ˆαt At,r Ct,r Bt,r Ct,r Bt,r Ct,r ; dBt,r dαt dBt,r dσt dBt,r ˆαt dBt,r dˆσt Substituting Eq. (20) into Eq. (18) and Eq. (19), we have: At,r Ct,r = (αr ˆσtσr ˆαt)σt = (σr ˆαtαr ˆσt)αt = (σrαtαrσt)σt = (αrσtσrαt)αt (ˆσtαt ˆαtσt)2 = σt (ˆσtαt ˆαtσt)2 = αt (ˆσtαt ˆαtσt)2 = σt (ˆσtαt ˆαtσt)2 = αt Bt,r Ct,r At,r Ct,r Bt,r Ct,r . (20) dAt,r dt dBt,r dt = ˆσt At,r Ct,r = σt At,r Ct,r dαt dt dαt dt + ˆαt At,r Ct,r αt At,r Ct,r dσt dt dσt dt ˆσt Bt,r Ct,r + σt Bt,r Ct,r dˆαt dt dˆαt dt + ˆαt Bt,r Ct,r αt Bt,r Ct,r dˆσt dt dˆσt dt . , (21) (22) There exists some symmetry between the above two equations, which is the key to our TiM identity. Combining Eqs. (16), (17) and (21), we have: xt dAt,r dt + At,r dxt dt = (At,r dαt dt + αt dAt,r dt )x + (At,r dσt dt + σt dAt,r dt )ε. (23) 13 The coefficient of in the above equation can be decomposed as: At,r dαt dt + αt dAt,r dt =(At,r + αt dAt,r dαt ) dαt dt + αt dAt,r dσt dσt dt + αt = ˆαtσt = ˆαt = ˆαt At,r Ct,r dBt,r dt dBt,r dt dαt dt + ˆαtαt At,r Ct,r dσt dt ˆσtαt + (ˆαtσt ˆσtαt) Bt,r Ct,r ˆαt dt Bt,r dˆαt dt . Similarly, the coefficient of ε in the Eq. (23) can be decomposed as: dAt,r dˆαt Bt,r Ct,r dˆαt dt dˆαt dt + αt + ˆαtαt dAt,r dˆσt Bt,r Ct,r dˆσt dt dˆσt dt + σt dAt,r dˆαt dˆαt dt + σt dAt,r dˆσt dˆσt dt dˆσt dt + ˆσtαt At,r Ct,r dˆσt dt ˆσtσt dˆσt dt + ˆαtσt Bt,r Ct,r dˆσt dt At,r dσt dt dAt,r dαt + σt dAt,r dt dαt dt + (At,r + σt dAt,r dσt ) dAt,r dσt =σt = ˆσtσt = ˆσt = ˆσt At,r Ct,r dBt,r dt dBt,r dt + (ˆαtσt ˆσtαt) Bt,r dt dˆσt dt Bt,r dˆσt dt . dσt dt Bt,r Ct,r Substituting Eqs. (23) to (25) into Eq. (7), we have: dxt dt + fθ,t,r dBt,r dt + Bt,r dfθ,t,r dt = 0. xt + At,r dAt,r dt dBt,r dt (ˆαt Bt,r )x + (ˆσt (ˆαtx + ˆσtε fθ,t,r) + Bt,r( dBt,r dt dˆαt dt + dfθ,t,r dt dˆσt dt d(ˆαtx + ˆσtε fθ,t,r) dt ) = 0 = 0 dˆαt dt dBt,r dt dBt,r dt (ˆαtx + ˆσtε fθ,t,r) + Bt,r Bt,r dˆσt dt )ε + fθ,t,r dBt,r dt + Bt,r dfθ,t,r dt = 0. (24) (25) (26) d(Bt,r (ˆαtx + ˆσtε fθ,t,r)) dt = 0. This is the TiM identity equation in Eq. (8), the proof is completed. A.2. TiM Training Algorithm We provide the detailed training algorithm of TiM in Algorithm 1. It is noteworthy that the TiM models are entirely trained from scratch. A.3. TiM Sampling Algorithms We provide the TiM sampling algorithm in Algorithm 2. For multi-step sampling, we can further incorporate stochasticity into the sampling process for improved diversity. In multi-step scenarios, the TiM sampling is similar to the diffusion sampling process, but with new condition for the next step. Therefore, we can construct stochastic sampling from the SDE (stochastic differential equation) diffusion process. Given xt = αt + σtε,Song et al. [69] has shown that the SDE forward and reverse are: forward : dxt = f(xt, t) + g(t)dw, reverse : dxt = [f(xt, t) 1 2 g(t)2xt log pt(xt)]dt + g(t)dw. (27) Algorithm 1 Training Algorithm of Diffusion Transition Models (TiM). Input: dataset with standard deviation σd, model fθ, diffusion parameterization {αt, σt, ˆαt, ˆσt}, weighting wt, learning rate η, time distribution , constant ϵ, constant c. Init: Iters 0 repeat 2ϵ (fθ (xt+ϵ, + ϵ, r) fθ (xtϵ, ϵ, r)) xd D, = cdata(xd), ε (0, I), < , xt αtx + σtε Bt,r (σrαt αrσt)/(ˆσtαt ˆαtσt) dfθ dt = 1 ˆf ˆαtx + ˆσtε + ( ˆαt + dˆσt dt L(θ) fθ ˆf 2 2 + Lcos(fθ, ˆf ) L(θ) wt L(θ)/(L(θ) + c) θ θ ηθL(θ) Iters Iters + 1 dt ) Bt,r/ dBt,r dt ε dfθ dt until convergence Algorithm 2 Piecewise Sampling Algorithm of Diffusion Transition Models (TiM). DDE Calculation TiM Target αti+1 ˆσti σti+1 ˆαti ˆσti αti ˆαti σti i=N where tN = Tmax, t0 = Input: sampling step , miximum timestep Tmax, model fθ, diffusion parameterization {αt, σt, ˆαt, ˆσt}, stochasticity ratio ρ. Init: data xN (0, I), timesteps = {ti}0 for = to 1 do xti+1 = if ρ > 0 then: ˆε εi (0, I) dt = ti ti+1 xti+1 xti+1 ρ(αtiσ ti σti) ˆεdt (cid:112)2ρ(αtiσ ˆαti ˆσti αti ˆαti σti αti+1 ˆσti αti ˆαti σti αti ˆσti αti ˆαti σti fθ(xti, ti, t0) (xti, ti, ti+1) tiσti)εi ti α σti+1αti α ti xti + xti dt σti end if xi = xti+1 end for Previous works[35, 45, 69, 72] has provided the explicit form of f(xt, t), g(t) and xt log pt(xt): f(xt, t) = αt αt xt, g(t) = 2σt sigmat αt αt σ2 , xt log pt(xt) = ε σt , where αt and σt represent the derivation of αt and σt respectively. For PF-ODE, it is defined as: vt = dxt dt = f(xt, t) 1 2 g(t)2xt log pt(xt) = αtx + σtε. For reverse-SDE, it is defined as: dxt = [f(xt, t) g(t)2xt log pt(xt)]dt + g(t)dw = [f(xt, t) (cid:124) 1 2 g(t)2xt log pt(xt)] (cid:125) (cid:123)(cid:122) PF-ODE Term 1 dt (cid:124) g(t)2xt log pt(xt)dt + g(t)dw (cid:125) (cid:123)(cid:122) Stochastic Term (28) (29) (30) = vtdt + [ σt (cid:114) σt]εdt + 2σt σt 2 αt αt αt αt σ2 dw. In the TiM sampling, we can take the stochastic term in the above equation to enhance diversity. To balance the stochast ). ticity and stability, we incorporate scaling factor s(t) = ραt, leading to scaled = ραtg(t) = 2ρ(αtσt σt 2 αtσ2 Therefore, the stochastic term is: ρ[αt σt αtσt]εdt + (cid:112)2ρ(αtσt σt 2 αtσ2 )dw. Transport Diffusion Parameterization Transition Parameterization cnoise(t) = αt = σt = ˆαt = ˆσt = OT-FM [43, 44] TrigFlow [45] EDM [35, 36] VP-SDE [31, 69] VE-SDE [68, 69] 1 4 ln(t) (T 1)t ln( 2 t) 1 cos(t) 1 t2+σ2 1 β2 +1 1 sin(t) t2+σ2 βt β2 +1 1 sin(t) t2+σ2 0 σd 1 cos(t) σd t2+σ2 1 1 ˆαt dt = dˆσt dt = 0 0 Bt,r = dBt,r dt = 1 cos(t) sin(t) Eq. (36) Eq. (35) sin(r t) cos(r t) Eq. (37) Eq. (38) 0 0 0 0 βrβt β2 +1 dβt dt 1 β2 +1 1 2 βdt2+βmint 1, Table 7. Transition parameterization for different diffusion transports. For VP-SDE, is set to 1000, and βt = where βd = 19.9 and βmin = 0.1 by default. We provide the details of timestep sampling in Tab. 8. Song et al. [69] has shown that VP-SDE is equivalent to DDPM [31] while VE-SDE is equivalent to score matching [68], so we adopt their notations for uniformity. For EDM, its TiM parameterization is too complex; we provide them in Eqs. (35) to (38). (cid:113) 1 B. Connections with Existing Methods In this section, we highlight the connection between TiM and other existing methods. We first demonstrate the properties of TiM compared with diffusion models. Then we demonstrate the connections of TiM with other training strategies. Transport Noise Level Timestep Time Range Time Scaling OT-FM Trigflow EDM VP VE std) std) std) ln(σ) (Pmean, 2 ln(σ) (Pmean, 2 ln(σ) (Pmean, 2 σ U(ϵt, 1) σ U(ϵt, 1) = σ 1+σ = arctan( σ σd = σ = σ = σmax( σ min σ2 max ) )σ [0, 1] [0, π 2 ] [0, +) [ϵt, 1] [σmin, σmax] cnoise(t) = cnoise(t) = cnoise(t) = 1 4 ln(t) cnoise(t) = (T 1)t cnoise(t) = ln( 1 2 t) Table 8. Time distribution of diffusion diffusion transports. B.1. Connections with Diffusion Models TiM generalizes the standard diffusion models. As complement to Tab. 7, we elucidate the time distribution of different diffusion transports in Tab. 8. Our TiMs share these parameters with diffusion models, but learn different objective. We show that the TiM training objective Eq. (13) generalizes the standard diffusion objective Eq. (3). Specifically, the TiM identity equation reduces to the diffusion identity equation in the limit as r. Recall that Bt,r = σrαtαrσt , the training ˆσtαt ˆαtσt target of TiM when becomes: (cid:32) lim tr ˆf = lim tr ˆαtx + ˆσtε + = ˆαtx + ˆσtε + lim tr = ˆαtx + ˆσtε + lim tr = ˆαtx + ˆσtε + lim tr = ˆαtx + ˆσtε. (cid:18) dˆαt dt Bt,r dBt,r dt (cid:32) (cid:18) dˆαt Bt,r dBt,r dt dt (cid:32) σrαtαrσt ˆσtαt ˆαtσt dBt,r dt (cid:32) + + dˆσt dt dˆσt dt ε ε dfθ,t,r dt dfθ,t,r dt (cid:19)(cid:33) (cid:19)(cid:33) (cid:18) dˆαt dt + dˆσt dt ε dfθ,t,r dt (cid:19)(cid:33) (cid:18) dˆαt dt 0 dBt,r dt + dˆσt dt ε dfθ,t,r dt (cid:19)(cid:33) (31) The above target is the diffusion target. This target lacks the modeling of state transitions from state to state, thus limiting the arbitrary-step generation capabilities of diffusion models. 16 (32) (33) (34) (35) (36) (37) (38) EDM parametrization. EDM [35, 36] parameterizes the diffusion model as: Dθ(x + tε, t) = σ2 t2 + σ2 (x + tε) + σd (cid:112)t2 + σ2 Fθ (cid:32) + tε (cid:112)t2 + σ2 , 1 4 (cid:33) ln(t) . It adopts the x-prediction in its training and use time weighting w(t) = t2+σ2 t2σ2 , leading to training objective as: L(θ) = t2 + σ2 t2σ2 Dθ(x + tε, t) x2 2 = = t2 + σ2 t2σ2 (cid:32) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Fθ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) σ2 t2 + σ2 (x + tε) + σd (cid:112)t2 + σ2 Fθ (cid:32) + tε (cid:112)t2 + σ2 , 1 4 (cid:33) ln(t) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) + tε (cid:112)t2 + σ2 (cid:33) ln(t) ( , 1 4 (cid:112)t2 + σ2 σd σd (cid:112)t2 + σ ε) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 Therefore, let cnoise(t) = 1 following coefficients: 4 ln(t), the original EDM parameterization can be unified into our parameterization with the αt = 1 (cid:112)t2 + σ2 , σt = (cid:112)t2 + σ , ˆαt = (cid:112)t2 + σ2 σd ˆσt = σd (cid:112)t2 + σ2 Therefore, the TiM parameterization is defined as: = t2 σd(t2 + σ ) 3 2 + 1 (cid:112)t2 + σ2 , σd dˆαt dt dˆσt dt = Bt,r = , tσd ) 3 (t2 + σ2 (t r)σ2 (t2 + σ3 , (cid:112)t2 + σ2 )(cid:112)r2 + σ2 t(t r)(t2 + σ3 ) 2t(t r)(t2 + σ2 (t2 + σ3 )2(cid:112)t2 + σ2 dBt,r dt = σ2 ) + (t2 + σ2 (cid:112)r2 + σ2 )(t2 + σ3 ) . B.2. Connections to Other training Methods In this section, we discuss the connections of TiM with other training strategies, including continuous-time consistency models [45, 70], consistency trajectory models [37], phased consistency models [77], Shortcut models [21], and MeanFlow models [26, 53]. Continuous-time consistency models. The TiM objective Eq. (13) generalizes the continuous-time consistency models. Specifically, the CTM objective reduces to the continuous-time CM objective when = 0. For TiM, let = 0 and d(x, y) = y2 2, the training objective becomes: θEx,ε,t (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) fθ(xt, t, 0) (ˆαtx + ˆσtε + (cid:18) dˆαt dt Bt,0 dBt,0 dt + dˆσt dt ε dfθ,t,0 dt =Ex,ε,t (cid:34) [θfθ,t,0]T (cid:32) fθ,t,0 ˆαtx ˆσtε + (cid:18) dfθ,t,0 dt dˆαt dt Bt,0 dBt,0 dt dˆσt dt (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:19)(cid:33)(cid:35) ε . (39) Continuous-time consistency models [45, 67, 70] are trained to map the noisy input xt directly to the clean data in one or few steps. Given model Fθ, the consistency models are formulated as: Dθ(xt, t) = cskip(t)xt + cout(t)Fθ(cin(t)xt, cnoise(t)). (40) Using the parameters αt, σt, ˆαt, and ˆσt, consistency parameterization corresponds to the transition from xt to x0: Dθ(xt, t) = ˆσtxt σtFθ(cin(t)xt, cnoise(t)) ˆσtαt ˆαtσt , (41) where ˆσt ˆσtαt ˆαtσt = At,0 and σt ˆσtαt ˆαtσt When using loss function d(x, y) = y2 = Bt,0 correspond to TiM parameterizations. 2, [70] show that the gradient of continuous-time consistency models is: θExt,t (cid:20) DT θ (xt, t) (cid:20) [Bt,0f cm =θExt,t (cid:20) Bt,0[θf cm =Ext,t (cid:21) dDθ (xt, t) dt θ (xt, t)]T dDθ(xt, t) dt θ (xt, t)]T dDθ(xt, t) (cid:21) (cid:21) (42) =Ext,t (cid:20) Bt,0θ[f cm θ (xt, t)]T xt + At,0 xt dt + dBt,0 dt cm θ + Bt,0 (cid:19)(cid:21) , df cm θ dt dt (cid:18) dAt,0 dt where cm dt + σt dαt θ (xt, t) = Fθ(cin(t)xt, cnoise(t))) represents the network in consistency models. As xt = αtx + σtε, dxt dt ε, we have: dt = dAt,0 dt dAt,0 dt = xt + At,0 xt dt + dBt,0 dt (αtx + σtε) + At,0( cm θ + Bt,0 dσt dt dαt dt + df cm θ dt z) + dBt,0 dt =(αt dAt,0 dt + At,0 dαt dt )x + (σt dAt,0 dt + At, dσt dt Based on Eqs. (24) and (25), we have: df cm θ dt cm θ + Bt,0 dBt,0 dt )z + cm θ + Bt, df cm θ dt . αt dAt,0 dt + At,0 dαt dt = ˆαt dBt,0 dt Bt,0 dˆαt dt , σt dAt,0 dt + At,0 dσt dt = ˆσt dBt,0 dt Bt, dˆσt dt . Therefore, we have: dAt,0 dt xt + At,0 + xt dt dˆαt dt df cm θ dt dBt,0 dt dˆσt dt cm θ + Bt,0 dBt,0 dt ) + =Bt,0( df cm θ dt (f cm θ ˆαtx ˆσtz). Substituting the above equation into Eq. (42), the gradient of continuous-time consistency models is: (cid:20) DT θ (xt, t) θExt,t (cid:20) Bt,0[θfθ]T =Ext,t (cid:34) =Ext,t (Bt,0 dBt,0 dt (cid:21) dDθ (xt, t) dt (cid:18) Bt,0( df cm θ dt (cid:32) dˆαt dt dˆσt dt ) + dBt,0 dt (f cm θ ˆαtx ˆσtz) )[θfθ]T cm θ ˆαtx ˆσtz + Bt,0 dBt,0 dt ( df cm θ dt dˆαt dt dˆσt dt ) (cid:19)(cid:21) , (cid:33)(cid:35) , (43) (44) (45) (46) Note that TiM network fθ(xt, t, 0) corresponds to the network cm θ (xt, t) in consistency models. The only difference between Eq. (46) and Eq. (39) is term (Bt, dBt,0 dt ), which can be bridged by weighting function. Consistency trajectory models, phased consistency models, and shortcut models. These models learn to transition from one state to another state in discrete manner, while our TiM generalizes this to the continuous-time domain. The core of our method is the TiM identity equation Eq. (8), which determines the function for the transition between two arbitrary states. 18 For consistency trajectory models (CTM) [37] and phased consistency models (PCM) [77], they targets at intermediate state xr, where 0 tn1, thus leading to the identity equation: Ψ(xtn , fθ(xtn , tn, r), r) = Ψ(xtn1, fθ(xtn1 , tn1, r), (47) where 0 < t1 < < tn < < tN = represents the discrete timesteps, and Ψ is an ODE solver to obtain the state at timemstep r. It is noteworthy that PCM splits the entire trajectory into several segments and learns this identity on each segment independently. Shortcut models [21] adopts the OT-flow-matching [43] as the transport, the ODE solver is: Ψ(xt, fθ(xt, t, r), r) = xt (t r)fθ(xt, t, r). The original identity equation is: (t r)fθ(xt, t, r) = (t s)fθ(xt, t, s) + (s r)fθ(xs, s, r). This identity equation of shortcut models can be rearranged as: (t r)fθ(xt, t, r) = (t s)fθ(xt, t, s) + (s r)fθ(xs, s, r) =xt (t r)fθ(xt, t, r) = xt (t s)fθ(xt, t, s) (s r)fθ(xs, s, r) =xt (t r)fθ(xt, t, r) = xs (s r)fθ(xs, s, r) =Ψ(xt, fθ(xt, t, r), r) = Ψ(xs, fθ(xs, s, r), r), (48) where = t+r 2 . Based on Eq. (47) and Eq. (48), when tn1 = tn+r 2 , CTMs are equivalent to shortcut models. MeanFlow models. We show that the TiM Eq. (8) generalizes the MeanFlow [26]. In particular, the training objective of TiM reduces to the MeanFlow objective in the OT-FM [43] transport setting. As in Tab. 7, OT-FM uses the parameterization {αt = 1t, σt = t, ˆαt = 1, ˆσt = 1}, leading to the TiM parameterization {Bt,r = t, dBt,r dt = 1}. Therefore, the TiM training objective becomes: Ex,ε,t (cid:20) (cid:18) (cid:18) fθ(xt, t, r) (t r) dfθ,t,r dt (cid:19)(cid:19)(cid:21) . (49) This corresponds to the training objective of MeanFlow. C. Implementation Details C.1. Model Architecture We illustrate the model architecture in Fig. 4. As we incorporate decoupled time embedding and interval-aware attention designs into DiT architecture, we use LoRA-AdaLN to avoid increasing model size. Specifically, given attention hidden size D, LoRA rank is set as = 1 3 D, such as = 1152 and = 384 in XL-models. For text-to-image generation, we incorporate caption features through CrossAttention mechanism, as in Fig. 5. C.2. Text-to-Image Training Details Native-Resolution Training. We adopt the VAE-specific native resolution training for text-to-image generation. As we use DC-AE [13] with 32 downsampling scale, an image with shape is resized to shape (32 32 ). For example, an image with shape 1025 513 is resized to 1024 512, , preserving resolution and aspect ratio as much as possible. Images of the same size are grouped into resolution buckets for batching. 32 ) (32 We set the base batch size as 16 on single GPU for 1024 1024 resolution bucket, then for resolution bucket, the minimal batch size is = 16HW 10241024 . For instance, the 512 512 resolution bucket holds the minimal batch size as = 64, while the 2048 2048-resolution bucket holds the minimal batch size as = 4. The maximum resolution is 4096 4096 with = 1. For data parallelism, each device processes distinct buckets with their corresponding batch sizes, maintaining similar token budget. Resolution-Dependent Timestep Shifting. Sampling from single timestep distribution is suboptimal across resolutions ranging from less than 256 256 to 4096 4096 pixels. Intuitively, higher-resolution images require stronger corruption (more noise) to destroy the signal, while lower-resolution images require less noise. Given an image with = H1 W1 19 Figure 4. TiM Model Architecture. Figure 5. TiM T2I block. pixels and its high-resolution counterpart with = H2 W2 pixels, Esser et al. [20] provides an equation to map the the timestep tn to tm: tm = (cid:112) 1 + ((cid:112) tn 1)tn . (50) In our practice, we set the base pixel number as = 1024 1024, and apply this mapping to all sampled timesteps. Model-Guidance Training. Tang et al. [73] propose model-guidance training target to improve sampling fidelity. We adopt this approach for text-to-image training. Under our formulation, the target becomes: ˆf = ˆαtx + ˆσtε uncond θ,t,t ) + (cid:20) dˆαt dt Bt,r dBt,r dt + dˆσt dt ε (cid:21) dfθ,t,r dt + (ω 1)(f cond θ,t,t, (51) where ω denotes the Classifier-Free Guidance (CFG) scale, θ is the Exponential Moving Average (EMA) of θ, cond uncond θ,t,t respectively represent the conditional and unconditional outputs. θ,t,t and From-Scratch Training. The TiM-T2I model contains 865M parameters with the patch size of 1. We train from scratch for about 30 days across 16 NVIDIA-A100 GPUs with constant learning rate of 4 104, using PyTorch-FSDP [89] and half-precision (torch.bfloat16) for memory efficiency. Following Tang et al. [73], we use model-guidance target Eq. (51) with CFG scale ω = 1.75 after 100K iterations. D. Additional Results Transport NFE=1 NFE=8 NFE=50 OT-FM [43, 44] TrigFlow [45] EDM [35, 36] VP-SDE [31, 69] 49.91 67.32 53.64 78.98 26.09 25.14 37.01 37. 17.99 18.28 24.06 35.72 Table 9. Ablation Studies on different transports. 20 Method JVP DDE DDE DDE DDE DDE DDE DDE DDE DDE ϵ n.a. 0.0001 0.0002 0.0005 0.001 0.002 0.005 0.01 0.02 0.05 Speed 1-step 4-step 50-step = = 0 1-step 4-step 50-step 1.8 iter/s 49.75 26.22 2.4 iter/s 2.4 iter/s 2.4 iter/s 2.4 iter/s 2.4 iter/s 2.4 iter/s 2.4 iter/s 2.4 iter/s 2.4 iter/s 111.25 80.14 67.09 48.83 49.07 49.91 50.05 49.72 49.90 23.34 23.83 24.33 24.73 25.54 26.09 26.53 26.67 27. 18.11 18.38 17.58 16.93 17.03 17.59 17.99 18.33 18.33 18.79 0% 0% 52.08 10% 53.46 0% 10% 10% 51.74 20% 10% 50.98 30% 10% 50.09 40% 10% 49.88 50% 10% 47.46 60% 10% 48.29 70% 10% 48.44 80% 10% 48.26 31.52 32.49 29.75 28.41 27.01 26.42 24.62 24.55 24.05 22.88 24.85 25.74 22.56 20.74 19.20 18.54 17.10 16.58 16.32 15.34 Table 10. The impacts of DDE ϵ on generation performance. Table 11. Timestep sampling comparison."
        },
        {
            "title": "Weighting",
            "content": "transform(t) = transform(t) = t/(1 t) NFE=1 NFE=8 NFE=50 NFE=1 NFE=8 NFE=50 NFE=1 NFE=8 NFE=50 transform(t) = tan(t) (a) Reciprocal (b) SMS (c) Sqrt (d) Square 48.25 49.01 48.24 48.55 25.29 25.76 25.82 25. 17.42 17.75 17.87 17.11 56.65 72.56 49.93 70.83 24.33 25.15 24.73 25.79 16.58 17.01 16.85 17.80 49.65 48.93 47.46 48.86 25.22 25.23 24.62 24. 17.39 17.19 17.10 17.15 Table 12. Ablation studies on different time weighting functions. D.1. Additional Ablations We provide additional ablation results in this section. Transport Comparison. We conduct ablation studies on different transports in Tab. 9. We find that different transports affect the convergence speed, where OT-FM and TrigFlow perform best, EDM is slightly worse, and VP-SDE performs the worst. Thus, we adopt OT-FM in all experiments. Differential Derivation Equation Calculation. As we incorporate small quantity ϵ into Eq. (10) to calculate the time derivative of network. We systematically evaluate the impact of ϵ on numerical accuracy in Tab. 10and observe that ϵ [0.001, 0.01] yields high precision. For training stability, we adopt ϵ = 0.005 in all experiments. Timestep Sampling. Using the TiM-B/4 model, we observe improved performance when portion of timesteps is fixed to = r, as in Tab. 11. The best results are obtained with 50% of samples using = (diffusion training) and 10% using = 0 (consistency training). Time Weighting. Using the TiM-B/4 model, we provide systematic analysis of time weighting as in Eq. (11). We study three types of transformations: (1) transform(t) = t, (2) transform(t) = 1t , (3) transform(t) = tan(t); and four types of weighting functions: (1) Reciprocal: fn(t, r) = 1 +(tr)2 , σ2 1 (3) SQRT: fn(t, r) = (σd+tr)2 , where σd is the standard deviation of clean data (σd = 1 in our dataset). Empirically, the combination w(t) = w(t, r) = achieves the best performance, slightly surpassing the best results reported in Tab. 4. σd+tr , (2) Soft-Min-SNR (SMS): fn(t, r) = 1 σd+tr , (4) Square: fn(t, r) = σd+tan(t)tan(r) 1 D.2. Class-Guided Image Generation We provide the results of class-guided image generation in this section. Setup. We use SD-VAE [56] for ImageNet-256 256 and DC-AE [13] for ImageNet-512 512, with patch sizes of 2 and 1, respectively. We train an XL-model with 664M parameters for 750K iterations with batch size of 512 (300 epochs), using constant learning rate of 2 104 and AdamW optimizer. We report FID [30], sFID [50], Inception Score (IS) [58], Precision and Recall [38] using ADM evaluation suite [19]."
        },
        {
            "title": "NFE",
            "content": "FID sFID IS Prec. Rec. Generative Adversarial Networks BigGAN [8] StyleGAN-XL [60] GigaGAN [34] - - - 112M 166M 569M 1 1 1 2 Masked and Autoregressive Models Mask-GIT [10] MagViT-v2 [86] LlamaGen-XL [71] LlamaGen-XXL [71] LlamaGen-3B [71] VAR [75] MAR [40] RandAR-XL [51] RandAR-XXL [51] 555 1080 300 300 300 350 800 300 300 - 307M 775M 1.4B 3.1B 2.0B 943M 775M 1.4B - - - - - - - - - Multi-step Diffusion Models 170 LDM-4-G [56] 800 SimpleDiffusion [32] Flag-DiT-3B [22] 200 Large-DiT-3B [22] 340 1300 MDT [23] 700 MDTv2 [24] 1400 DiT-XL [52] 1400 SiT-XL [49] 400 FlowDCN-XL [78] 800 SiT-REPA-XL [87] 300 DoD-XL [88] 400 SiT-RealS-XL [82] Few-step Consistency Models 395M 250 2 250 2 2B 250 2 4.23B 250 2 4.23B 676M 250 2 676M 250 2 675M 250 2 675M 250 2 675M 250 2 675M 250 2 613M 250 2 675M 250 2 6.95 2.30 3.45 6.18 1.78 2.62 2.34 2.18 1.80 1.55 2.22 2.15 3.60 2.44 1.96 2.10 1.79 1.63 2.27 2.06 2.00 1.42 1.73 1. MeanFlow-XL [26] iCT-XL [67] Shortcut-XL [21] 1000 - 250 675M 675M 675M IMM-XL [92] 3840 675M Any-step Transition Models TiM-XL 300 664M 1 1 2 1 2 4 2 1 2 2 2 4 2 1 1 1 2 2 2 4 2 250 2 3.43 20.30 10.60 7.80 7.77 3.99 2. 3.26 7.11 6.14 3.61 2.62 1.65 7.36 4.02 - - - 5.59 5.97 5.97 - - - - 5.12 - 4.43 4.52 4.57 4.45 4.60 4.49 4.37 4.70 5.14 4.45 - - - - - - - 171.4 265.12 225. 182.1 319.4 244.08 253.90 263.3 365.4 303.7 314.21 321.97 247.67 256.3 284.8 304.36 283.01 311.73 278.24 277.50 263.16 305.7 304.31 268.54 - - - - - - - 0.87 0.78 0.84 - - 0.81 0.81 0.81 0.83 0.81 0.80 0.79 0.87 - 0.82 0.82 0.81 0.79 0.83 0.83 0.82 0.80 0.79 0. - - - - - - - 0.28 0.53 0.61 - - 0.58 0.59 0.58 0.57 0.62 0.60 0.62 0.48 0.61 0.60 0.61 0.65 0.57 0.59 0.58 0.65 0.64 0.60 - - - - - - - 4.37 4.97 6.21 6.74 5.57 5.02 210.33 140.39 151.79 189.99 203.41 248.12 0.75 0.71 0.74 0.79 0.79 0.79 0.59 0.63 0.59 0.58 0.60 0.63 Table 13. Performance comparison on ImageNet-256 256 class-guided generation. : Flag-DiT-3B and Large-DiT-3B actually have 4.23 billion parameters, where 3B means the parameters of all transformer blocks. : means using model-guidance in the training, therefore eliminating the usage of CFG. Performance Analysis. We provide the results on ImageNet-256 256 and ImageNet-512 512 in Tabs. 13 and 14 respectively. Across both ImageNet-256256 and ImageNet-512512, TiM-XL demonstrates strong performance-efficiency"
        },
        {
            "title": "NFE",
            "content": "FID sFID IS Prec. Rec. Generative Adversarial Networks BigGAN [8] StyleGAN-XL [59] - - 160M 168M 1 1 2 Masked and Autoregressive Models VAR [75] MAGVITv2 [86] 1080 350 307M 2.3B - - Multi-step Diffusion Models 800 SimpleDiffusion [32] - DiffiT [29] 800 MaskDiT [90] 368 Large-DiT-3B [22] 400 ADM-G,U [19] 400 U-ViT-H/2 [4] 600 DiT-XL [52] 1468 EDM2-L [36] 1048 EDM2-XL [36] 734 EDM2-XXL [36] 600 SiT-XL [49] - FlowDCN-XL [78] 200 SiT-REPA-XL [87] 2B 250 2 561M 250 2 250 2 - 250 2 4.23B 774M 250 2 501M 250 2 675M 250 2 778M 64 2 64 2 1.1B 64 2 1.5B 675M 250 2 675M 250 2 675M 250 2 Few-step Consistency Models 1273 sCT-L [76] 778M sCT-XL [76] 1117 1.1B sCT-XXL [76] 762 1.5B Any-step Transition Models TiM-XL 300 664M 1 2 1 2 1 2 1 1 2 2 2 4 2 250 2 7.5 2.41 2.63 1. 3.02 2.67 2.50 2.52 3.85 4.05 3.04 1.87 1.80 1.73 2.62 2.44 2.08 5.15 4.65 4.33 3.73 4.29 3.76 5.07 4.79 4.01 2.55 1.69 - 4.06 152.8 267.75 - 0. - 0.52 - - 303.2 324.3 - - - - 5.10 5.01 5.86 6.44 5.02 - - - 4.18 4.53 4.19 - - - - - - 248.7 252.12 256.27 303.70 221.72 263.79 240.82 - - - 252.21 252.8 274.6 - - - - - - - 0.83 0.83 0.82 0.84 0.84 0.84 - - - 0.84 0.84 0.83 - - - - - - - - - 0.55 0.56 0.57 0.53 0.48 0.54 - - - 0.57 0.54 0. - - - - - - 4.29 4.36 4.22 4.24 4.66 160.06 171.73 171.51 207.07 247.52 0.79 0.82 0.83 0.83 0.82 0.59 0.57 0.58 0.57 0.62 Table 14. Performance comparison on ImageNet-512 512 class-guided generation. trade-offs: at low NFE (1 to 4 2), it can compete with few-step consistency models, achieving comparable FID with fewer training epochs and smaller model size. When increasing NFEs, TiM-XL matches or surpasses many multi-step diffusion models in FID, despite training for only 300 epochs. Notably, TiM demonstrates remarkable generation quality and shows stable gains as NFE increases. E. Qualitative Results We provide the qualitative results in Fig. 6. 23 Figure 6. High-resolution and multi-aspect generations from TiM (128 NFEs). TiM attains up to 4096 4096 resolution and reliably handles multiple aspect ratios, including 1024 4096 and 2560 1024."
        }
    ],
    "affiliations": [
        "MMLab, CUHK",
        "Shanghai AI Lab",
        "USYD"
    ]
}