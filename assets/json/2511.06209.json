{
    "paper_title": "Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads",
    "authors": [
        "Jingwei Ni",
        "Ekaterina Fadeeva",
        "Tianyi Wu",
        "Mubashara Akhtar",
        "Jiaheng Zhang",
        "Elliott Ash",
        "Markus Leippold",
        "Timothy Baldwin",
        "See-Kiong Ng",
        "Artem Shelmanov",
        "Mrinmaya Sachan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 9 0 2 6 0 . 1 1 5 2 : r Preprint REASONING WITH CONFIDENCE: EFFICIENT VERIFICATION OF LLM REASONING STEPS VIA UNCERTAINTY HEADS Jingwei Ni1 , Ekaterina Fadeeva1 , Tianyi Wu2, Mubashara Akhtar1, Jiaheng Zhang2, Elliott Ash1, Markus Leippold4 Timothy Baldwin3 5,See-Kiong Ng1, Artem Shelmanov3, Mrinmaya Sachan1 1ETH Zurich, 2National University of Singapore, 3MBZUAI, 4University of Zurich, 5The University of Melbourne"
        },
        {
            "title": "ABSTRACT",
            "content": "Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose lightweight alternative for steplevel reasoning verification based on data-driven uncertainty estimation. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of the frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even exceed the performance of PRMs that are up to 810 larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering promising direction towards scalable and generalizable introspective LLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Chain-of-thought (CoT) prompting has proven highly effective in eliciting the reasoning capabilities of large language models (LLMs) for solving complex tasks (Wei et al., 2022). Recent post-training approaches further enhance this ability through reinforcement learning, rewarding models for generating responses that conform to the CoT pattern and yielding correct final answers (DeepSeek-AI, 2025; Yang et al., 2025b; Abdin et al., 2025). However, the emphasis on verifying only final answers raises concerns about the reliability of intermediate reasoning steps (Lightman et al., 2023; Zheng et al., 2025; Barez et al., 2025). Flawed steps can propagate and systematically distort conclusions (Fu et al., 2025b), while in some cases LLMs may still produce the correct answer despite erroneous intermediate steps (Arcuschin et al., 2025). Such issues undermine trust in CoT-based methods, particularly in high-stakes domains like medicine (Amann et al., 2020) and law (Fan et al., 2025). The contemporary approach to supervise models for step-level correctness is through process reward models (PRMs: Lightman et al. (2023); Zhang et al. (2025c)). However, PRM-based supervision faces key drawbacks. First, it requires expensive monte-carlo rollouts for training data annotation and deploying an additional LLM supervisor, adding substantial computational overhead. Second, this approach fails in tasks like mathematical proofs (Azerbayev et al., 2023) and planning (Zheng et al., 2024), where step-level correctness cannot be inferred from the final result alone. Equal contribution. Correspond to {jingni,efadeeva,msachan}@ethz.ch, tianyi wu@u.nus.edu, artem.shelmanov@mbzuai.ac.ae 1 Preprint Figure 1: Process reward models (top) in comparison to uncertainty quantification heads (bottom). Another promising line of work for assessing the correctness of reasoning steps leverages uncertainty quantification (UQ) methods (Gal et al., 2016; Malinin & Gales, 2021). Unlike PRMs, which rely on external verification, UQ assumes that models outputs and internal states provide information about the reliability of its generations. To date, only relatively simple unsupervised UQ methods have been investigated for LLM reasoning (Fu et al., 2025b; Yan et al., 2025). These approaches are primarily based on consistency-based heuristics such as majority voting and rely on simplistic uncertainty estimates without leveraging any model internals. While computationally lightweight, they consistently fail to capture subtle reasoning flaws and lag significantly behind PRMs in performance. In this work, we ask: Can the cost-efficiency of UQ methods be combined with the performance of PRMs? Recent research in UQ shows that data-driven approaches substantially outperform unsupervised ones in detecting factual hallucinations (Azaria & Mitchell, 2023; Chuang et al., 2024; Shelmanov et al., 2025). Building on this insight, we introduce lightweight data-driven uncertaintybased verifier for step-level reasoning. Specifically, we adapt the UHead framework (Shelmanov et al., 2025) to train uncertainty quantification heads on top of frozen LLMs for reasoning step verification. Unlike PRMs, which rely on expensive rollouts and training of large supervisor LLM, UHeads exploit internal LLM states (attention and token probabilities) as features, making them far more efficient (see Figure 1): UHeads trained in our work require less than 10M parameters. Training labels are generated automatically, either by using powerful external LLM (e.g., DeepSeek R1) as judge or by relying on the original LLM itself. This enables scalable supervised or selfsupervised training without the need for verifiable final answers, human annotations, or costly Monte Carlo rollouts. Extensive in-domain (ID) and out-of-domain (OOD) evaluation highlights UHeads advantages in three key aspects: (1) Generalizable Performance: UHeads achieve competitive or superior performance compared to PRM baselines in step-level verification and test-time scaling, including bestof-N selection and step-level correctness supervision, especially when dealing with OOD reasoning tasks (e.g., reasoning-intensive QA and planning); (2) Computational Efficiency: UHeads substantially outperform PRMs that are 150 larger, while remaining competitive with PRMs up to 800 larger; and (3) Training Data Efficiency: unlike PRMs, which often rely on proprietary datasets (He et al., 2024b), costly human annotations (Lightman et al., 2023), or consensus-filtering pipelines involving both Monte Carlo rollouts and LLM judgments (Zhang et al., 2025c), UHeads can be trained on data annotated automatically in self-supervised manner. Our contributions are the following: 2 Preprint We introduce lightweight approach to verifying step-level correctness in LLM-generated reasoning by extending the UHead framework. Our 9.8M-parameter UHead achieves competitive or superior performance compared to 7B-scale PRMs across diverse reasoning tasks and settings, while being significantly more efficient at inference time in terms of memory and compute. We develop an efficient and scalable data annotation pipeline that eliminates the need for human-labeled data, verifiable final answers, or costly Monte Carlo rollouts. We further show that high performance in reasoning verification can be achieved using annotations derived either from external verification models (e.g., DeepSeek R1) or from fully self-supervised process. We show that our approach achieves strong generalization across domains: single UHead trained only on mathematical tasks generalizes effectively to planning and general knowledge QA. Unlike PRMs, which often overfit to domain-specific reasoning patterns, our lightweight UHead verifier leverages internal LLM uncertainty signals, enabling robust reasoning verification without requiring domain adaptation."
        },
        {
            "title": "2 BACKGROUND",
            "content": "2.1 TEST-TIME SCALING State-of-the-art LLMs solve complex tasks by generating CoT reasoning traces (Guo et al., 2025). Given an input question x, models produce sequence of reasoning steps = {r1, r2, . . . , rT } followed by final answer = g(r, x). Formally, an LLM parameterized by θ models the conditional distribution: Pθ(y, x) = Pθ(rt x, r<t) Pθ(y x, r), t=1 where rt denotes the t-th reasoning step and r<t are the preceding steps. Although CoT reasoning improves final task performance, locally incorrect steps rt can propagate and cause wrong answers. Test-time scaling (TTS) algorithms (Yao et al., 2023; Snell et al., 2024) aim to further improve the performance of LLMs without retraining the entire model, by allocating additional computation or using more sophisticated inference strategies. By leveraging multiple candidate solutions and verification signals, TTS facilitates selecting correct answer. Common approaches include online and offline best-of-N (BoN) sampling. Other techniques employ sophisticated search and sampling techniques based on beam search and backtracking, such as the tree of thoughts (Yao et al., 2023). , r(2) in each step, we generate continuations Online best-of-N sampling assumes that {r(1) 1t ). In the greedy version of online best-of-N , only the most promising option is expanded further. After reaching the final step , the answer is obtained from the greedily derived chain: }, and each of them is evaluated online using quality function Qonline(r(j) , . . . , r(N ) r = arg max j{1,...,N } Qonline(r(j) 1t ) , = {r 1 , 2 , ..., T }, = g(x, r). Offline best-of-N sampling assumes that we sample not one, but reasoning chains {r(1), r(2), . . . , r(N )}. Each chain r(j) is evaluated using scoring function Qoffline(r(j)) (see Section 2.2), and the final answer is derived from the best-scoring chain: = arg max j{1,...,N } Qoffline(r(j)) , = g(x, r). 2.2 PROCESS REWARD MODELS PRMs are prominent approach for quality assessment in both online and offline test-time scaling (Lightman et al., 2023; Luo et al., 2024; Wang et al., 2024b). PRMs are critics designed to estimate the quality of each partial reasoning state r(j) 1t ) that reflects the likelihood of the chain eventually leading to correct solution. PRMs are typically implemented by separate LLM trained to evaluate the plausibility and correctness of intermediate reasoning steps. The annotation for training PRMs comes from various sources, including crowdsourcing, self-consistency checking, and synthetic data generation pipelines. 1t by assigning reward RPRM(r(j) 3 Preprint In online BoN, we simply use the reward from PRM as step-wise quality function. In offline BoN, the score for the complete chain r(j) can be obtained via temporal aggregation of process rewards, e.g., minimum step score (Zhang et al., 2025c): Qonline(r(j) 1t ) = RPRM (r(j) 1t ) , Qoffline(r(j)) = min 1tT (j) RPRM(r(j) 1t ) . Although PRMs have proven to be highly effective even in boosting the performance of larger LLMs, they come with several limitations: they are relatively large models, typically containing 1.5B 8B parameters, which leads to additional computational and memory overhead during inference. Moreover, PRMs are often domain-specific, being trained for tasks such as mathematical reasoning, and exhibit limited generalization to unseen domains. 2.3 UNCERTAINTY QUANTIFICATION UQ is conceptual and methodological framework that bridges Bayesian and informationtheoretical approaches to modeling (Maddox et al., 2019) with various practical tasks such as selective classification (Geifman & El-Yaniv, 2017), out-of-distribution detection (Mukhoti et al., 2023), and adversarial attack detection (Lee et al., 2018). The core assumption behind UQ is that ML models can produce not only predictions but also signals indicative of their reliability. In practice, it is common to use various heuristics and approximations, which can be of various natures. However, unlike external verification methods, UQ primarily leverages internal model capabilities to provide these estimates, such as logits, multiple sampled predictions, hidden states, and attention weights (see Figure 1). UQ for LLMs poses unique challenges beyond standard text classification, making it difficult to establish an effective universal unsupervised solution (Zhang et al., 2023; Kuhn et al., 2023a; Duan et al., 2024; Fadeeva et al., 2024). These limitations have motivated the exploration of datadriven approaches, which have consistently demonstrated superior empirical performance (Azaria & Mitchell, 2023; He et al., 2024a; Chuang et al., 2024)."
        },
        {
            "title": "QUANTIFICATION HEADS",
            "content": "Method. UQ offers an efficient alternative to PRMs for assessing the quality of reasoning steps during online test-time scaling. While PRMs leverage generated sequences, UQ methods leverage internal capabilities of LLMs. Instead of training large reward models, it is possible to use lightweight UQ methods or train lightweight data-driven uncertainty estimators: Qonline(r(j) ) = 1 (r(j) r(j) <t , x), Qoffline(r(j)) = min 1tT (j) (1 (r(j) r(j) <t , x)) . We adopt state-of-the-art data-driven method for uncertainty quantification (UQ) uncertainty quantification heads (UHeads) (Shelmanov et al., 2025). UHeads are lightweight supervised auxiliary modules for LLMs, designed to estimate uncertainty and detect hallucinations. Implemented as compact transformer-based classifiers, they leverage the internal states of the base LLM without requiring fine-tuning or modification of the model or its outputs. Unlike PRMs, which are domainspecific (e.g., math proofs or planning) and incur significant additional inference costs, UHeads are plug-and-play modules that generalize across tasks, domains, and languages, making them practical for reasoning step assessment. While PRMs rely solely on generated tokens, UHead leverages richer per-token features extracted from the base LLMs internal states, specifically, attention weights to the 1-3 preceding tokens and the logits of the top-K candidate generations. Its architecture is transformer-based: each per-token feature vector is first linearly projected and then processed through stack of Transformer blocks to capture intra-step dependencies. The resulting token representations are mean-pooled across the reasoning step to obtain step-level vector, which is passed through two-layer classification head with dropout and GeLU activation to produce the final class logits. The architecture of UHead consists of three main components: 4 Preprint 1. Feature projection layer: linear transformation that maps the input features: h0 = Linearproj(F (x, r1t1, rt)). 2. Transformer layers: stack of Transformer blocks: hl = Transformerl(hl1), = 1, . . . , L. 3. Aggregation of token-level features into step-level vector: hr = 1 rt hL τ rt 4. Two-layer classification head: applies two linear layers with non-linearity and dropout: = Linear2(σ(Dropout(Linear1(hL)))), where σ() is an activation function (GeLU) and are the class logits. Originally developed to score atomic factual claims (Fadeeva et al., 2024), here we extend UHeads to score reasoning steps in PRM-style setup. Constructing training dataset for UHead. To construct the training data for UHead, we use 10.8K problems (prompts) from the PRM800K training dataset. This dataset is an established resource for training and evaluating LLM reasoning capabilities. As most PRMs are already trained on LLM generations derived from it, it enables fair comparison (Wang et al., 2024a; Zhang et al., 2025c). We prompted reasoning LLMs to generate each step of the CoT solution on separate, selfcontained line, enabling accurate correctness grading. The full prompt is provided in Appendix C.1. Qwen3-8B reliably follows this prompt and produces high-quality CoT steps. For each of the 10.8K math problems, we generate 3 reasoning chains, resulting in 32K data samples. For all reasoning chain generations, we use nucleus sampling with maximum of 256 new tokens, top-k = 50, top-p = 0.95, and temperature of 1.0. Then, we annotated step-level correctness of each CoT step using an LLM as judge. The judge LLM was provided with the question, the target LLMs CoT steps and final answer, and the groundtruth answer; and then prompted to assess the correctness of each step. The full annotation prompts are given in Appendix C.2. We consider two judging approaches: (1) an external verifier, where larger LLM evaluates the steps, and (2) self-supervised setting, where the same reasoning LLM annotates its own generations. Following Zheng et al. (2025), who showed that general reasoning models perform well at step-level correctness annotation, we adopt DeepSeek-R1 as the external verifier. Other details to reproduce the training data can be found in D.1."
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "4.1 LLMS FOR REASONING, UHEAD TRAINING DETAILS, EVALUATION DATASETS LLMs for reasoning. We conduct experiments with two state-of-the-art LLMs, Qwen3-8B (Yang et al., 2025a) and Phi-4 (Abdin et al., 2025). For these models, we prepared training datasets for reasoning-step verification and trained UHeads on them. UHead training settings. We follow Shelmanov et al. (2025)s recipe to train UHeads with frozen underlying LLM. For all experiments with different training data and underlying LLMs, we use the same hyperparameter settings, which are detailed in D.2. Evaluation datasets span three domains: mathematical reasoning (in-domain), planning (OOD), and general knowledge QA (OOD). We select datasets that demand non-trivial reasoning and include reasoning steps that are unambiguously verifiable, enabling reliable evaluation by both LLMs and human annotators. The details for test datasets are presented in E. 4.2 BASELINES We benchmark against broad set of baselines, covering PRMs of different sizes and UQ methods. 5 Preprint Small PRMs (1.5B) include 2 PRMs fine-tuned from Qwen2.5-Math-1.5B: Skywork-PRM-1.5B (He et al., 2024b) and H4-Qwen2.5-PRM-1.5B-0.2 (HuggingFaceH4, 2025). Large PRMs (78B) include (1) Math-Shepherd-PRM-7B (Wang et al., 2024b), which determines the process labels for each step by estimating the empirical probability of reaching the correct final answer (MC estimation); (2) RLHFlow-PRM-8B-DeepSeek/Mistral models (Xiong et al., 2024) trained with MC-estimated labels from DeepSeek/Mistral rollouts; (3) Universal-PRM-7B (AURORA, 2025) trained using ensemble prompting and reverse verification; (4) Qwen2.5-Math-7B-PRM800k trained on the PRM800k dataset (Lightman et al., 2023); and (5) Qwen2.5-Math-PRM-7B (Zhang et al., 2025c), which combines MC estimation with LLM-as-a-Judge consensus and currently achieves the best result on ProcessBench compared to PRMs of similar scale and computation (Zhao et al., 2025). UQ methods evaluated in our experiments fall into two categories, differing in computational cost: (1) lightweight scores that use only single generation: Maximum Sequence Probability, Mean Token Entropy, Perplexity (Fadeeva et al., 2023), P(True) (Kadavath et al., 2022), CCP (Fadeeva et al., 2024), and Self-Certainty (Kang et al., 2025), which show significant advantages in reasoning chain selection (Fu et al., 2025b); and (2) sampling-based scores: Semantic Entropy, Lexical Similarity (Kuhn et al., 2023b), and Degree Matrix (Lin et al., 2024). To compute the latter, we draw = 10 alternative steps per step position, so they are much less computationally efficient. All methods are implemented using the LM-Polygraph framework (Fadeeva et al., 2023; Vashurin et al., 2025). 4.3 EVALUATION SETTINGS We conduct experiments to compare and evaluate PRMs and UQ methods in three scenarios: (1) step-level correctness prediction; (2) offline best-of-N selection; and (3) online best-of-N selection. Table 1: Accuracy of the stepDeepSeek-R1-based level pipeline evaluation relative to human labels. Acc. # Steps Dataset (1) Step-level correctness prediction. For each question in the test set, we generate CoT trace and evaluate step-level correctness using DeepSeek-R1. To ensure accurate judgements, the model is provided with full metadata, such as the ground-truth answer, reasoning steps, and supporting evidence (see prompt in C.2). The resulting correctness labels serve as the reference for evaluating the step-level quality scores from our baselines. To validate the reliability of our evaluation pipeline, we evaluate it against (1) the human annotations from random subset of PRM800k and (2) manually annotated set of 1000 steps spanning QA, planning, and ProofNet tasks (see for annotation details). DeepSeek-R1 achieves 95% Acc. on PRM800k and 90% on other datasets (see Table 1). (2) Offline best-of-N . In the Best-of-N (BoN) setting, we generate reasoning chains per problem (N =10 for the math and QA datasets, =5 for the planning datasets, temperature=1.0). We compute the quality score for each step, and the chain with the highest score is selected for deriving the final answer. Correctness of the solution is measured using accuracy. For GSM8K, we use exact match against the gold-standard final answer. For other datasets, where final answers may be open-ended or structurally complex, we use DeepSeek-R1 to assign binary correctness labels (1 for correct, 0 for incorrect) based on both the problem statement and the reference solution. The grading prompt is provided in Appendix C.2. PRM800K ProofNet Trips Meetings Calendar StrQA SciQA 17,067 193 102 101 102 313 245 95.29 87.05 93.06 95.70 91.09 95.85 99.28 (3) Online best-of-N selects the best candidate at each step during the generation process. At each step, the system produces candidate steps, scores them, and expands the best option. This process repeats until an end-of-sequence token is reached, after which the chain is evaluated using the same metrics as in the offline BoN. This setting evaluates how well PRMs and UQ methods can guide the LLM reasoning trajectory. For computational efficiency, we pick =5 and use generation temperature of 1.5 to ensure the diversity of candidate steps. 6 Preprint Table 2: PR-AUC for detecting incorrect reasoning steps (Qweb3-8B). Best scores are shown in bold. Other competitive scores show clear advantages are underlined. # Sample indicates the number of training samples; each sample corresponds to reasoning trajectory with step-level labels. Method # Sample Math (ID) Planning (OOD) MATH GSM8k ProofNet Trips Meetings Calendar QA (OOD) StrQA SciQA Average ID OOD Overall Random MaxProb MaxEntropy Perplexity Self-Certainty CCP P(True) Semantic Entropy Lexical Similarity Degree Matrix Skywork-PRM-1.5B H4-Qwen2.5-PRM-1.5B-0.2 Math-Shepherd-PRM-7B RLHFlow-PRM-Deepseek-8B RLHFlow-PRM-Mistral-8B Universal-PRM-Qwen2.5-Math-7B Qwen2.5-Math-7B-PRM800k Qwen2.5-Math-PRM-7B - - - - - - - - - - Unk 369K 440K 253K 273K 690K 265K 860K UHead Self-anno (Ours) UHead DeepSeek-anno (Ours) 32K 32K Unsupervised Uncertainty Quantification (UQ) .173 .221 .212 .205 .213 .250 .164 .257 .250 . .283 .259 .380 .289 .233 .534 .586 .531 .529 .465 .061 .106 .122 .099 .101 .090 .059 .116 .119 .089 .153 .194 .185 .176 .155 .168 .172 .173 .170 .147 .524 .578 .545 .519 .516 .584 .535 .565 .569 . .588 .655 .618 .572 .643 .645 .608 .610 .603 .597 PRMs 150 Larger than UHeads .412 .171 .147 .159 .433 .597 .532 . PRMs 750 to 810 Larger than UHeads .405 .540 .537 .624 .613 .702 .147 .136 .118 .329 .301 .310 .662 .583 .523 .730 .708 .711 Uncertainty Heads (UHeads) .594 . .260 .243 .735 .740 .660 .579 .555 .753 .768 .757 .779 .802 .486 .483 .443 .418 .482 .452 .490 .492 .490 .484 .502 . .657 .504 .499 .691 .727 .745 .779 .786 .116 .114 .119 .110 .120 .119 .126 .111 .120 .107 .254 .213 .284 .390 .349 .328 .362 .334 .394 . .125 .259 .227 .228 .243 .235 .263 .265 .254 .258 .129 .174 .173 .160 .156 .169 .132 .182 .180 .154 .368 .418 .390 .369 .401 .407 .404 .409 .407 .396 .408 .228 .281 .196 .426 . .415 .518 .415 .330 .404 .429 .311 .322 .296 .496 .500 .514 .536 .515 .468 .566 .594 .595 .404 .361 .461 .441 .618 . .278 .326 .309 .291 .309 .318 .302 .324 .322 .305 .371 .344 .451 .442 .404 .540 .559 .565 .559 ."
        },
        {
            "title": "5 RESULTS AND ANALYSIS",
            "content": "Step-level correctness prediction results for Qwen-3 are presented in Table 2 and for Phi-4 in Table 7 in Appendix A. We conclude that unsupervised UQ methods in many cases provide valuable signals for detecting reasoning errors. Even lightweight single-sample techniques such as MSP and MaxEntropy demonstrate slight improvements over the random baseline in the majority of datasets, with the exception of calendar planning. Sampling-based methods such as Semantic Entropy usually perform slightly better, but have similar issues on calendar planning. Small PRMs demonstrate notable improvements over unsupervised UQ methods for mathematical problems and QA, but for OOD planning datasets, they are not better than UQ. Best large PRMs considered in our work, such as Qwen2.5-Math-7B-PRM800k and Qwen2.5-Math-PRM-7B, substantially outperform all unsupervised UQ methods and smaller PRMs on all tasks. Despite using 750-810 fewer parameters than PRMs, both UHead variants perform on par with, or even surpass the best PRMs. For ID mathematical datasets, UHeads substantially outperform all other PRM baselines except the two strongest PRMs based on Qwen2.5-Math. On MATH, UHead trained on self-supervised annotation is on par with Qwen2.5-Math-PRM-7B. On GSM8K, UHead trained on DeepSeek annotation is on par with Qwen2.5-Math-7B-PRM800k. For ProofNet, UHead falls behind the strongest Qwen-based PRMs, but substantially outperforms all others. It is not surprising that parameter-heavy PRMs achieve the best results on ID mathematical datasets, as they tend to strongly overfit to this domain. In contrast, UHead, with far fewer parameters, avoids such domain-specific overfitting. This becomes evident on planning tasks, where UHead consistently outperforms the strongest PRMs across all datasets. The best performance is obtained by UHead trained on DeepSeek annotations, though even the self-supervised variant surpasses all PRMs. For QA datasets, the picture is more mixed. On StrategyQA, UHead again emerges as the top step-verification method. However, on ScienceQA, RLHFlow-PRM-DeepSeek makes clear leap forward, outperforming other techniques, including Qwen-based PRMs. Still, UHead performs on par with Qwen-based PRMs, underscoring its solid performance in this setting. In summary, UHead lags behind the strongest PRMs on average for ID tasks but outperforms them for OOD tasks. Notably, the average performance of the self-supervised UHead is comparable to that of the externally supervised variant, highlighting its ability to provide an efficient self-supervised Preprint Table 3: Offline best-of-N decoding accuracy across datasets (Qwen-3 8B). For datasets with verifiable final answer, we also provide majority voting. Method # Sample Math (ID) Planning (OOD) MATH GSM8k ProofNet Trips Meetings Calendar QA (OOD) StrQA SciQA Average ID OOD Overall Qwen3-8B pass@1 (Lower Bound) Qwen3-8B pass@N (Upper Bound) Qwen3-14B pass@1 Majority Voting Min Number of Steps MaxProb MaxEntropy Perplexity Skywork-PRM-1.5B H4-Qwen2.5-PRM-1.5B-0. Math-Shepherd-PRM-7B RLHFlow-PRM-Deepseek-Data RLHFlow-PRM-Mistral-Data Universal-PRM-Qwen2.5-Math-7B Qwen2.5-Math-7B-PRM800k Qwen2.5-Math-PRM-7B UHead Self-anno (Ours) UHead DeepSeek-anno (Ours) - - - - - - - - Unk 369K 440K 253K 273K 690K 263K 860K 32K 32K Pass@N or Larger LLM 95.6 99.2 97.6 74.1 97.8 76.0 8.1 36.2 38.7 5.5 16.0 5. Unsupervised Uncertainty Quantification (UQ) 97.6 95.1 96.2 96.1 96.4 73.9 76.0 74.1 74.1 23.4 20.6 6.9 6.9 7.0 5.5 4.0 3.5 PRMs 150 Larger than UHeads 97.6 95.1 76.5 71.4 6.9 15.6 6.0 4.5 PRMs 750 to 810 Larger than UHeads 95.5 96.4 96.3 97.5 97.3 97. 72.8 71.7 71.7 76.0 74.4 76.0 9.1 8.7 8.4 9.7 5.9 7.2 Uncertainty Heads (UHeads) 97.5 97.8 73.6 76.5 9.4 17. 4.0 3.5 3.5 4.0 6.0 5.5 6.5 7.0 92.4 99.3 93.4 91.0 92.4 92.0 92.7 94.4 91.7 93.0 92.7 93.7 95.7 92.7 93. 94.4 92.7 23.5 60.5 40.5 22.5 31.0 29.5 27.5 23.0 22.0 30.0 25.5 30.0 24.0 27.5 26.5 31.0 26. 86.8 98.3 91.9 86.6 85.9 84.1 86.6 85.6 86.6 84.6 87.3 87.6 87.8 87.8 87.1 88.1 88.6 88.6 92.7 99.3 95. 92.5 96.3 96.0 95.6 94.5 87.4 98.8 89.0 86.7 88.2 87.4 87.7 43.3 56.1 54.3 47.0 47.4 44.5 43.6 96.3 94. 89.5 86.1 43.8 44.3 95.8 94.1 94.1 97.1 96.9 96.9 87.1 86.9 87.2 89.7 88.1 89.2 45.2 43.9 44.8 44.5 44.7 44.8 97.1 96. 88.5 89.0 46.5 47.1 59.8 72.1 67.3 61.9 62.7 60.6 60.1 60.9 59.9 60.9 60.0 60.7 61.5 61.0 61. 62.3 62.8 Table 4: Online best-of-N decoding accuracy across datasets (Qwen3-8B). Method # Sample Math (ID) Planning (OOD) MATH GSM8k ProofNet Trips Meetings Calendar QA (OOD) StrQA SciQA Average OOD Overall ID Math-Shepherd-PRM-7B RLHFlow-PRM-Deepseek-Data RLHFlow-PRM-Mistral-Data Universal-PRM-Qwen2.5-Math-7B Qwen2.5-Math-7B-PRM800k Qwen2.5-Math-PRM-7B 440K 253K 273K 690K 263K 860K UHead Self-anno (Ours) UHead DeepSeek-anno (Ours) 32K 32K PRMs 750 to 810 Larger than UHeads 72.9 74.0 74.1 70.4 76.4 74.0 74.2 74.8 93.7 94.4 94.4 91.6 91.3 92. 44.7 48.2 46.9 43.8 48.0 55.9 5.3 5.1 7.5 9.1 5.1 5.0 7.7 8.3 10.6 6.5 11.5 6.6 Uncertainty Heads (UHeads) 92.9 93.7 48.0 51. 9.5 12.2 10.1 13.7 27.5 28.1 29.6 28.5 24.5 26.0 29.0 24.6 65.0 66.8 65.0 60.4 63.8 57.6 69.6 69. 63.4 63.2 62.4 61.0 63.2 63.8 70.43 72.20 71.80 68.60 71.90 74.20 33.78 34.30 35.02 33.10 33.62 31.80 47.53 48.51 48.81 46.41 47.98 47.70 59.8 58.8 71.70 73. 35.60 35.84 49.14 49.90 solution for reasoning step verification, particularly valuable in OOD settings. Similar results are observed for Phi-4. Offline best-of-N results for Qwen-3 are presented in Table 3 (Phi-4 results are in Table 8, A). Both UHead types achieve the best offline BoN performance on several benchmarks, including GSM8K, ProofNet, Meeting/Calendar Planning, StrategyQA, and ScienceQA, achieving on par or superior performance to the best PRMs. On MATH and Trip Planning, UHeads rank second-best. Notably, UHead-based BoN selection enables Qwen3-8B to outperform its larger counterpart Qwen3-14B on multiple benchmarks (MATH, GSM8K, ProofNet, Meeting Planning, and ScienceQA). As in the step-level setting, UHead exhibits strong generalization. While the best PRMs (UniversalPRM-7B, Qwen2.5-MATH-7B-PRM, and Qwen2.5-Math-PRM-7B) reach parity with UHead on in-domain datasets MATH and GSM8K, they often fall behind on OOD tasks, such as planning and StrategyQA. Furthermore, UHead provides stable gains across tasks of varying difficulty, from relatively simple datasets (MATH, GSM8K, ScienceQA) to complex planning benchmarks. Online best-of-N evaluation results are presented in Table 2. While PRMs remain highly competitive on ID tasks like MATH and GSM8K, UHead trained on Deepseek-R1 annotations has obtained the runner-up accuracy across all three ID datasets. More impressively, UHeads demonstrate superior generalization to OOD. UHead DeepSeek-anno secures the highest overall and OOD average accuracy, outperforming all PRMs by considerable margin. By excelling across both ID and OOD 8 Preprint Figure 2: Top row: PR-AUC of UHeads with increasing training set size (x-axis). Bottom row: scaling training data either by adding new unique questions or by sampling additional trajectories. One dataset per domain is shown; other tasks are covered in A. tasks with drastically fewer resources, UHeads proves to be powerful substitute for PRMs as test-time reasoning guidance tool. Combining PRM with UHead. PRMs and UHeads capture complementary aspects of reasoning quality: PRMs rely on textual cues from the generated rationale, while UHeads leverage internal uncertainty signals from the LLM itself. To explore whether these two perspectives can be synergistic, we train logistic regression model that takes both the PRM score and the UHead score as input to predict step-level correctness labels. The model is trained on random subset of 200 questions from the training set. Table 5 reports the step-level PR-AUC, showing that this combination yields additional improvements. These findings suggest that integrating uncertainty-based and PRM-based signals is promising direction for future work. Table 5: Step-level PR-AUC for the experiment combining PRMs with UHead (Qwen-3 8B). Impact of data quantity and diversity. In practice with budget constraint, it is important to understand the how data quantity and diversity influence UHead performance Figure 2 (top) shows that larger training sets, more questions and sampled reasoning trajectories, consistently improve UHead performance on both ID and OOD tasks. In practice, however, high-quality questions are often scarce, making it difficult to scale by prompt expansion alone. An alternative is to sample multiple reasoning trajectories per question. The bottom row of Figure 2 compares scaling strategies, demonstrating that trajectory sampling is an effective way to boost UHead performance. PRM1 (Qwen2.5-Math-7B-PRM800k) PRM2 (Qwen2.5-Math-7B) UHead DeepSeek-anno UHead + PRM1 UHead + PRM2 .301 .310 .260 .613 .702 .594 .586 .531 . .674 .710 .318 .327 .613 .573 ProofNet GSM8k Method MATH To examine the effect of data diversity, we train UHeads on two subsets of 2K questions from the training set: one highly similar and one highly diverse. Questions are embedded using Qwen3Embedding-8B (Zhang et al., 2025b). The similar subset is formed by selecting the 2K nearest neighbors to the median embedding, while the diverse subset is constructed via farthest-first traversal (Gonzalez, 1985). Results in Table 9 show that data diversity benefits UHead performance. In Figure 3, we present the best-of-N performance on GSM8k and ScienceQA for Scaling . different values 1 10. On GSM8k, performance improves with larger for almost all methods, with both UHead variants consistently achieving the best results across most settings, outperforming the PRM-based baselines. On ScienceQA, accuracy also increases as grows, but the differences between methods are smaller, and all approaches perform comparably. 9 Preprint"
        },
        {
            "title": "6 RELATED WORK",
            "content": "Process reward models. Research in PRMs has advanced by scaling and refining step-level annotations: from manual labeling (Uesato et al., 2022; Lightman et al., 2023), to MC-based automatic labeling (Wang et al., 2024a; Luo et al., 2024), and consensus methods combining LLM-as-a-Judge and MC estimation (Zhang et al., 2025c; Zhao et al., 2025). Generative PRMs further extend steplevel judgment with long CoT (Xiong et al., 2025; Zhao et al., 2025). In contrast, our approach focuses on cost-efficient step-level verification via LLM internal states. Uncertainty quantification methods recently have been employed for test-time scaling and improving reasoning performance (Mo & Xin, 2024; Yin et al., 2024; Zhang et al., 2025a; Fu et al., 2025a; Yan et al., 2025; Kang et al., 2025). However, so far, only weak unsupervised UQ methods have been used. In this work, we propose data-driven UQ techniques that achieve much better performance. Formal verification has recently been used to verify LLM reasoning steps (Zhou et al., 2024; Hu et al., 2025; Liu et al., 2025; Zhou & Zhang, 2025). However, these methods often require specialized autoformalization data for training and are limited to narrow domains (e.g., math proofs). In contrast, we focus on UQ methods and demonstrate their ability to generalize across domains."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduced UHead, lightweight step-level verifier that reads an LLMs own internal states to detect incorrect reasoning. UHead can be trained fully self-supervised without human labels, verifiable final answers, or Monte-Carlo rollouts. Across math, planning, and QA, UHead delivers strong inand out-of-domain results and is competitive with, or better than, far larger PRMs, making it practical building block for resource-efficient reasoning systems. Beyond replacing PRMs, UHead complements them: combining PRM scores with UHeads score consistently improves performance, indicating the two signals capture complementary aspects of reasoning quality. This synergy suggests promising path toward strong hybrid verifiers that marry introspective uncertainty with process rewards. Looking ahead, our findings pave the way for more efficient test-time scaling and self-verification for LLMs in reasoning tasks. 10 Preprint"
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "We use publicly available datasets (MATH, GSM8K, ProofNet, ScienceQA, and StrategyQA) which have no data privacy issues. All artifacts we use are under licenses allowing research usage. Human annotations were conducted by the authors of this paper. We do not identify any other ethical risks associated with this study."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We will fully open-source our trained UHeads, code, prompts, human annotations, and processed datasets to ensure full reproducibility. For all training, evaluation, and sampling, we fix random seeds to 1 or 42 (see specified in scripts). One major challenge of reproducing exact numbers in our tables from scratch is the use of API-based DeepSeek-R1. API-based LLMs are known to be inherently non-deterministic even if fixing prompts and temperature. To address this, we provide all DeepSeek-R1 annotations used in training and evaluation, allowing others to faithfully reproduce our results. If reproducing from scratch, our codebase also guarantees to reproduce similar trends and observations, even if there are slight differences in exact numbers. Code and data: https: //anonymous.4open.science/r/uhead-86C1/README.md"
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, et al. Phi-4-reasoning technical report. arXiv preprint arXiv:2504.21318, 2025. S. Agarwal et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Julia Amann, Alessandro Blasimme, Effy Vayena, Dietmar Frey, Vince I. Madai, and Precise4Q consortium. Explainability for artificial intelligence in healthcare: multidisciplinary perspective. BMC Medical Informatics and Decision Making, 20(1):310, 2020. doi: 10.1186/ s12911-020-01332-6. Published 30 November, 2020. Ivan Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, and Arthur Conmy. Chain-of-thought reasoning in the wild is not always faithful, 2025. URL https: //arxiv.org/abs/2503.08679. AURORA. Aurora: Automated training framework of universal process reward models via ensemble prompting and reverse verification, 2025. URL https://auroraprm.github.io/. Amos Azaria and Tom Mitchell. The internal state of an LLM knows when its lying. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 967976, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.68. URL https://aclanthology. org/2023.findings-emnlp.68/. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics, 2023. URL https://arxiv.org/abs/2302.12433. Fazl Barez, Tung-Yu Wu, Ivan Arcuschin, Michael Lan, Vincent Wang, Noah Siegel, Nicolas Collignon, Clement Neo, Isabelle Lee, Alasdair Paren, Adel Bibi, Robert Trager, Damiano Fornasiere, John Yan, Yanai Elazar, and Yoshua Bengio. Chain-of-thought is not explainability. Preprint, July 2025. Preprint; under review. Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and James R. Glass. Lookback lens: Detecting and mitigating contextual hallucinations in large language models In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), using only attention maps. Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 14191436, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.84. URL https://aclanthology.org/2024. emnlp-main.84/. 11 Preprint Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 50505063, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.276. URL https: //aclanthology.org/2024.acl-long.276. Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, and Artem Shelmanov. LM-polygraph: Uncertainty estimation for language models. In Yansong Feng and Els Lefever (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 446461, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-demo.41. URL https://aclanthology.org/2023.emnlp-demo.41/. Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, et al. Fact-checking the output of large language models via token-level uncertainty quantification. In Findings of the Association for Computational Linguistics ACL 2024, pp. 93679385, 2024. Yu Fan, Jingwei Ni, Jakob Merane, Etienne Salimbeni, Yang Tian, Yoan Hermstruwer, Yinya Huang, Mubashara Akhtar, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, and Joel Niklaus. Lexam: Benchmarking legal reasoning on 340 law exams, 2025. URL https://arxiv.org/abs/2505. 12864. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025a. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence, 2025b. URL https://arxiv.org/abs/2508.15260. Yarin Gal et al. Uncertainty in deep learning, 2016. Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 48784887, 2017. URL https://proceedings.neurips.cc/ paper/2017/hash/4a8423d5e91fda00bb7e46540e2b0cf1-Abstract.html. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use Laptop? Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021. Teofilo F. Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical Computer Science, 38:293306, 1985. doi: https://doi.org/10. 1016/0304-3975(85)90224-5. URL https://www.sciencedirect.com/science/ article/pii/0304397585902245. ISSN 0304-3975. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. 12 Preprint Jinwen He, Yujia Gong, Zijin Lin, Chengan Wei, Yue Zhao, and Kai Chen. LLM factoscope: Uncovering LLMs factual discernment through measuring inner states. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1021810230, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.608. URL https://aclanthology.org/ 2024.findings-acl.608/. Jujie He, Tianwen Wei, Rui Yan, Jiacai Liu, Chaojie Wang, Yimeng Gan, Shiwen Tu, Chris Yuhao Liu, Liang Zeng, Xiaokun Wang, Boyang Wang, Yongcong Li, Fuxiang Zhang, Jiacheng Xu, Bo An, Yang Liu, and Yahui Zhou. Skywork-o1 open series. https://huggingface.co/ Skywork, November 2024b. URL https://huggingface.co/Skywork. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. Xiaolin Hu, Qinghua Zhou, Bogdan Grechuk, and Ivan Y. Tyukin. Stepproof: Step-by-step verification of natural language mathematical proofs, 2025. URL https://arxiv.org/abs/ 2506.10558. HuggingFaceH4. Qwen2.5-math-1.5b-instruct-prm-0.2. Hugging Face Model Hub, URL https://huggingface.co/HuggingFaceH4/Qwen2.5-Math-1. 2025. 5B-Instruct-PRM-0.2. Fine-tuned version of Qwen/Qwen2.5-Math-1.5B-Instruct on the prm800k-trl-dedup dataset, trained with PRM and TRL. Safetensors format. arXiv:2211.14275. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know, 2022. URL https://arxiv.org/ abs/2207.05221. Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language models via self-certainty. In 2nd AI for Math Workshop @ ICML 2025, 2025. URL https: //openreview.net/forum?id=nddwJseiiy. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023a. URL https://openreview.net/forum?id=VD-AYtP0dve. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id= VD-AYtP0dve. Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. simple unified framework for In Advances in Neural Indetecting out-of-distribution samples and adversarial attacks. formation Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada, volume 31, pp. 71677177, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ abdeb6f575ac5c6676b747bca8d09cc2-Abstract.html. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with confidence: Uncertainty quantification for black-box large language models, 2024. URL https://openreview.net/ forum?id=XJiN1VkgA0. Preprint Chengwu Liu, Ye Yuan, Yichun Yin, Yan Xu, Xin Xu, Zaoyu Chen, Yasheng Wang, Lifeng Shang, Qun Liu, and Ming Zhang. Safe: Enhancing mathematical reasoning in large language models via retrospective step-aware formal verification, 2025. URL https://arxiv.org/abs/2506. 04592. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision, 2024. URL https://arxiv.org/ abs/2406.06592. Wesley Maddox, Pavel Izmailov, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. simple baseline for bayesian uncertainty in deep learning. Advances in neural information processing systems, 32, 2019. Andrey Malinin and Mark J. F. Gales. Uncertainty estimation in autoregressive structured prediction. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https://openreview.net/forum?id=jN5y-zb5Q7m. Shentong Mo and Miao Xin. Tree of uncertain thoughts reasoning for large language models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1274212746. IEEE, 2024. Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deep deterministic uncertainty: new simple baseline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2438424394, 2023. Artem Shelmanov, Ekaterina Fadeeva, Akim Tsvigun, Ivan Tsvigun, Zhuohan Xie, Igor Kiselev, Nico Daheim, Caiqi Zhang, Artem Vazhentsev, Mrinmaya Sachan, Preslav Nakov, and Timothy Baldwin. head to predict and head to question: Pre-trained uncertainty quantification heads for hallucination detection in llm outputs, 2025. URL https://arxiv.org/abs/2505. 08200. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Akim Tsvigun, Daniil Vasilev, Rui Xing, Abdelrahman Boda Sadallah, Kirill Grishchenkov, Sergey Petrakov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, and Artem Shelmanov. Benchmarking uncertainty quantification methods for large language models with lm-polygraph. Transactions of the Association for Computational Linguistics, 2025. Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024a. URL https://arxiv.org/abs/2312.08935. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/ v1/2024.acl-long.510. URL https://aclanthology.org/2024.acl-long.510/. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 14 Preprint Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm, 2024. Wei Xiong, Wenting Zhao, Weizhe Yuan, Olga Golovneva, Tong Zhang, Jason Weston, and Sainbayar Sukhbaatar. Stepwiser: Stepwise generative judges for wiser reasoning, 2025. URL https://arxiv.org/abs/2508.19229. Hang Yan, Fangzhi Xu, Rongman Xu, Yifei Li, Jian Zhang, Haoran Luo, Xiaobao Wu, Luu Anh Tuan, Haiteng Zhao, Qika Lin, et al. Mur: Momentum uncertainty guided reasoning for large language models. arXiv preprint arXiv:2507.14958, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025b. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Xiaonan Li, Junqi Dai, Qinyuan Cheng, Xuanjing Huang, and Xipeng Qiu. Reasoning in flux: Enhancing large language models reasoning through uncertainty-aware adaptive guidance. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 24012416, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.131. URL https://aclanthology.org/2024.acl-long.131/. Jinghan Zhang, Xiting Wang, Fengran Mo, Yeyang Zhou, Wanfu Gao, and Kunpeng Liu. Entropybased exploration conduction for multi-step reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 38953906, Vienna, Austria, July 2025a. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.201. URL https://aclanthology.org/2025.findings-acl.201/. Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, and Luoyi Fu. Enhancing uncertainty-based hallucination detection with stronger focus. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 915932, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.58. URL https://aclanthology.org/2023.emnlp-main.58/. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025b. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning, 2025c. URL https://arxiv.org/abs/2501.07301. Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, and Bowen Zhou. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025. 15 Preprint Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning, 2025. URL https://arxiv.org/abs/2412.06559. Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, and Denny Zhou. Natural plan: Benchmarking llms on natural language planning, 2024. URL https://arxiv.org/abs/2406.04520. Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, and Yuhuai Wu. Dont trust: Verify grounding llm quantitative reasoning with autoformalization, 2024. URL https://arxiv.org/abs/2403.18120. Kuo Zhou and Lu Zhang. Step-wise formal verification for llm-based mathematical problem solving, 2025. URL https://arxiv.org/abs/2505.20869. 16 Preprint"
        },
        {
            "title": "A ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "To verify our framework works for models of different size, family, and post-training, we conduct step-level correctness and offline BoN evaluation on Phi-4 UHead trained on Qwen3-8B annotated training data. The step-level correctness prediction and Offline BoN results are presented in Table 7 and Table 8 correspondingly. For budget reason, we do not perform DeepSeek-R1 annotation for Phi-4 training data. On step-level correctness, Qwen3-8B-annotated UHead achieves the best performance in Meeting and Calendar Planning, and the best average performance on OOD tasks. On StrategyQA and overall average, it ranks the second. For offline BoN, the Qwen3-8Bannotated UHead outperforms two strong Qwen2.5-Math PRMs on MATH and matches the strongest PRMs on GSM8K. On ProofNet and StrategyQA, it also outperforms some much larger PRMs. Figure 3: Best-of-N performance on GSM8k and ScienceQA for different values (Qwen3-8B). Figure 4: Top row: PR-AUC of UHeads with increasing training set size (x-axis). Bottom row: scaling training data either by adding new unique questions or by sampling additional trajectories. One dataset per domain is shown. Table 6: Beam-Search decoding accuracy across datasets (Qwen3-8B). Method # Sample Math (ID) Planning (OOD) QA (OOD) Average MATH GSM8k ProofNet Trips Meetings Calendar StrQA SciQA ID OOD Overall Qwen2.5-Math-7B-PRM800k Qwen2.5-Math-PRM-7B 263K 860K UHead Self-anno (Ours) UHead DeepSeek-anno (Ours) 32K 32K 78. 76.0 75.6 PRMs 750 to 810 Larger than UHeads 88.6 87.8 Uncertainty Heads (UHeads) 87.8 89.0 Preprint Table 7: PR-AUC for detecting incorrect reasoning steps for Phi-4. Best scores are shown in bold, and other competitive scores are underlined. # Sample indicates the number of training samples, where each sample corresponds to reasoning trajectory with step-level annotations. Qwen2.5Math-PRM-7Bs training data is filtered from an 860K-sample dataset; the exact size after filtering is not specified in their paper. Method # Sample Math (ID) Planning (OOD) QA (OOD) Average MATH GSM8k ProofNet Trips Meetings Calendar StrQA SciQA ID Avg. OOD Avg. Overall Random MaxProb MaxEntropy Perplexity Skywork-PRM-1.5B H4-Qwen2.5-PRM-1.5B-0. Math-Shepherd-PRM-7B RLHFlow-PRM-Deepseek-8B RLHFlow-PRM-Mistral-8B Universal-PRM-Qwen2.5-Math-7B Qwen2.5-Math-7B-PRM800k Qwen2.5-Math-PRM-7B - - - - Unk 369K 440K 253K 273K 690K 265K 860K .106 .127 .112 .117 .219 . .248 .200 .141 .485 .474 .427 Unsupervised Uncertainty Quantification (UQ) .038 .084 .079 .066 .181 .061 .082 .123 .107 .099 .552 .618 .585 . .463 .548 .533 .508 PRMs 150 Larger than UHeads .185 .105 .408 .534 .467 .476 PRMs 750 to 810 Larger than UHeads .188 .263 .195 .213 .406 .377 .188 .109 .093 .263 .238 .240 .747 .558 .462 .741 .825 .791 .584 .455 .424 .559 .599 .594 Uncertainty Heads (UHeads) .324 .380 .362 . .327 .434 .489 .343 .309 .497 .568 .555 .172 .252 .248 .228 .237 .212 .249 .315 .261 .321 .355 .333 .086 .158 .135 . .415 .116 .327 .440 .311 .252 .329 .310 .075 .111 .099 .094 .195 .113 .208 .191 .143 .320 .373 .348 .319 .391 .373 . .371 .354 .479 .422 .353 .474 .535 .517 .228 .286 .270 .255 .305 .264 .378 .335 .274 .416 .474 .453 UHead Qwen3-8B-anno (Ours) 32K .404 .340 .155 .756 . .592 .347 .347 .300 .538 . Table 8: Offline Best-of-N decoding accuracy across datasets for Phi-4 model. For datasets with verifiable final answer, we also provide Majority Voting. We use max uncertainty aggregation over steps. Method # Sample Math (In-domain) MATH GSM8k ProofNet Reasoning QA StrQA SciQA Pass@N Phi-4 pass@1 (Lower Bound) Phi-4 pass@N (Upper Bound) - - 90.5 98.2 97.9 100. Unsupervised Uncertainty Quantification (UQ) Majority Voting Min Number of Steps MaxProb MaxEntropy Perplexity - - - - - 91.1 92.3 94.1 92.9 100. 100. 98.9 98.4 98.9 PRMs 150 Larger than UHeads Skywork-PRM-1.5B H4-Qwen2.5-PRM-1.5B-0.2 Unk 369K 95.9 93.5 99.5 100. PRMs 750 to 810 Larger than UHeads Math-Shepherd-PRM-7B RLHFlow-PRM-Deepseek-Data RLHFlow-PRM-Mistral-Data Universal-PRM-Qwen2.5-Math-7B Qwen2.5-Math-7B-PRM800k Qwen2.5-Math-PRM-7B 440K 253K 273K 690K 263K 860K 92.9 94.1 92.3 96.4 93.5 93. 98.4 98.9 98.4 100. 100. 100. Uncertainty Heads (UHeads) 86.8 97.2 91.0 88.2 87.5 87.5 95.7 89.2 95.2 89.6 90.3 93.5 92.4 94. 93.0 99.1 93.2 93.9 94.1 93.0 93.4 92.5 91.8 93.2 94.8 94.1 92.5 93.9 94.1 95.7 99.8 94.6 97.1 98.0 98.0 98. 96.6 96.4 96.6 98.0 97.3 96.8 97.5 97.5 UHead Qwen3-8B-anno (Ours) 32K 95.9 100. 93.1 93.0 95.7 Table 9: PR-AUC performance of UHeads trained on the most Diverse/Indiverse 2K questions (3 trajectories sampled for each question). Data diversity benefits the overall performance of UHead. Method # Sample Math (ID) Planning (OOD) QA (OOD) Average MATH GSM8k ProofNet Trips Meetings Calendar StrQA SciQA ID Avg. OOD Avg. Overall Avg. UHead DeepSeek-anno Indiverse UHead DeepSeek-anno Diverse 6K 6K .308 .409 .549 .575 .205 .180 .626 . .687 .793 .685 .792 .377 .271 .251 .325 .354 .388 .525 . .461 .507 18 Preprint"
        },
        {
            "title": "B DISCUSSION AND LIMITATIONS",
            "content": "5 shows that UHead performance increases with larger training data and benefits from data diversity. Curves in Figure 2 and Figure 4, although show diminishing marginal gains, do not seem to reach the top for tasks like StrategyQA. Therefore, it seems possible to further unleash the potential of UHead with further data scaling sampling more reasoning trajectories per question or adding new questions beyond the PRM800K training set. In this work, we do not involve questions outside the PRM800K training set to establish fair comparison with PRMs trained on data derived from this set (e.g., Qwen2.5-Math-7B-PRM800K). Due to the high cost of DeepSeek-R1 (annotation of 32K reasoning trajectories cost >2000 USD), we also do not annotate more reasoning trajectories per question. We leave this promising scaling to future work, including the integration of diverse, high-quality questions outside the math domain. To save annotation cost, future work may also substitute API-based LLM with recent small but capable reasoning models, for example, GPT-OSS (Agarwal et al., 2025). Another limitation regarding the UHead framework is that it needs to be trained on the internal signal of target LLMs it supervises. Once trained, UHead cannot be directly applied to another LLM, since it depends on model-specific internal states. By contrast, PRMs can supervise any LLM because they only operate on output tokens. However, UHeads are highly efficient in parameter size and training data, making their training relatively inexpensive. Once trained, UHeads can significantly save inference costs compared to PRMs. In practice, applications may focus on small number of widely used models. If UHeads for these models are shared online (e.g., on HuggingFace), practitioners could readily download and apply them without training from scratch. Future work may also investigate fine-tuning UHeads to adapt to customized or fine-tuned versions of target LLMs."
        },
        {
            "title": "C PROMPTS",
            "content": "C.1 LLM PROMPT We use domain-agnostic, format-enforcing prompt to elicit structured step-by-step reasoning, detailed in Figure 5. C.2 ANNOTATION PROMPTS For step-level annotation, we use the 2-stage prompts shown in Figure 6, while chain-level correctness annotations are obtained using the prompt in Figure 7. 19 Preprint <im_start>user You will be presented with <Question>. Before providing the [Answer ], you should first think step-by-step carefully. Your response format: <start of response> Reasoning Steps: - Step 1: [Your first reasoning step] - Step 2: [Your second reasoning step] - Step 3: [Next step, and so on...] ... - Step N: [Final reasoning step] <Answer>: [Your final answer] <end of response> Strict Requirements: - DO NOT include any text outside the specified format. - Each reasoning step MUST be written on **single line only**: NO line breaks, bullet points, or substeps within step. - Each step should express one precise and **self-contained** logical operation, deduction, calculation, or fact application. - Steps MUST provide explicit result of the step or concrete reasoning outcomes. Avoid vague explanations or meta-descriptions of the reasoning process. - For example: - Good: \"- Step 1: Multiply 5 by 4, which equals 20.\" - Bad: \"- Step 1: Multiply 5 by 4.\" (no result of the step or concrete reasoning outcome) - Continue writing steps until the problem is solved. - Violating ANY requirement above is NOT acceptable. Now answer: <Question>: {q}<im_end> <im_start>assistant <think> </think> Reasoning Steps: Figure 5: Prompt template used to elicit structured step-by-step reasoning from the model. 20 Preprint Step-Level Annotation Stage 1 Prompt: You are given problem, ground-truth solution, and step-bystep student solution. Your task is to analyze each step in the students solution to determine whether it is both logically correct and relevant. Instructions: - Carefully examine each student step for logical errors or unnecessary/redundant reasoning. - If all steps are correct and they lead to the same final answer as the ground-truth solution, conclude that there are no errors . - If any step contains an error that would prevent the student from reaching the correct solution, identify and report those specific steps with an explanation. PROBLEM: {problem} GROUND-TRUTH SOLUTION: {answer} STUDENTS SOLUTION STEPS: {steps} Now, please evaluate whether the students steps are correct and logical. Step-Level Annotation Stage 2 Prompt (Postprocessing): You are given: - problem - students step-by-step solution (as Python list of string steps) - An assessment of students solution Your task: Output single Python list where each element is: - 1 if the corresponding step is correct - 0 if the step is incorrect Important: - Output only the list, nothing else. - The list must have the same length as the number of steps. PROBLEM: {problem} STUDENTS SOLUTION STEPS: {steps} ASSESSMENT OF STUDENT SOLUTION STEPS: {reply} OUTPUT LIST: Figure 6: Two-step prompting procedure for step-level correctness annotation. The first stage evaluates the solution and identifies flaws, while the second converts this into binary correctness labels. 21 Preprint You will be given <Problem> and its proposed <Solution>. Your task is to assess whether the solution is **correct** or ** incorrect**. Respond using the **exact format** below, do not include any text outside this template. Output format: <start of response> Solution comments: ... your comments on the solution, explaining reasoning, pointing out any errors or confirming correctness ... <Grade>: (CorrectIncorrect) <end of response> <Problem>: {problem} <Solution>: {solution} Figure 7: Prompt used for annotating chain-level correctness by evaluating the full reasoning trace. 22 Preprint"
        },
        {
            "title": "D TRAINING DETAILS",
            "content": "D.1 TRAINING DATA ANNOTATION DETAILS In our experiments, we employ two LLM judges for the annotation of UHead training data. For DeepSeek-R1, we follow the officially recommended inference setting, using temperature of 0.6. Similarly, when using Qwen3-8B as training data annotator, we also follow the recommended inference hyperparameter, using temperature of 0.7, top of 20, and top of 0.95. We access Qwen3-8B through vLLM local deployment, and access DeepSeek-R1 through the DeepSeek API. The annotation prompts are detailed in Figure 6. We do not set max tokens to restrict the length of reasoning chains. D.2 HYPERPARAMETER DETAILS For all experiments on Phi-4 and Qwen3-8B, we use the same set of hyperparameters. We use learning rate of 5e-4, batch size of 128, and positive class weight of 3. For UHeads, we use one layer of transformer encoder with hidden size of 512 and 16 attention heads. All training continues for 5 epochs. We use 10% of the training data and 200-sample set of GSM8K, ScienceQA, and StrategyQA for validation and best checkpoint selection. D.3 COMPUTATION DETAILS All experiments are carried out on cluster nodes with 4 GH200 GPUs and another cluster node with 2 H100 GPUs. 23 Preprint"
        },
        {
            "title": "E TEST DATASET DETAILS",
            "content": "The mathematical domain includes MATH (high school and competition-style math problems; Hendrycks et al., 2021), GSM8K (grade-school math word problems; Cobbe et al., 2021), and ProofNet (natural language proofs of undergraduate-level math problems; Azerbayev et al., 2023). Planning domain includes NaturalPlan (Zheng et al., 2024) (spans three real-world tasks trip planning, meeting planning, and calendar scheduling). The QA domain includes ScienceQA without multi-modal context (Lu et al., 2022) (covers 26 science subjects from elementary to high school) and StrategyQA (Geva et al., 2021) general knowledge QA benchmark that requires implicit multi-step reasoning. Due to compute limitations, we evaluate on subsets of the test sets. Table 10 presents the statistics of the test datasets used for the step-level benchmark. Importantly, we rely on DeepSeek-R1 for step-level evaluation, which is an expensive API-based LLM. We thus take subset of test sets for this evaluation. While only having few hundred questions, there is substantial number of reasoning steps, which allows us to draw representative insights. Table 10: Test dataset statistics used for step-level benchmark. Dataset Model #Questions #Steps Mean #Steps %Correct Steps Answer Mean Len (Tokens) MATH GSM8k ProofNet Trips Meetings Calendar StrategyQA ScienceQA Qwen3-8B Phi-4 Qwen3-8B Phi-4 Qwen3-8B Phi-4 Qwen3-8B Phi-4 Qwen3-8B Phi-4 Qwen3-8B PhiQwen3-8B Phi-4 Qwen3-8B Phi-4 200 200 200 200 186 186 320 200 200 200 200 500 500 500 500 1203 2296 1056 1211 2227 5492 4486 2569 3350 1616 2172 2865 3288 2621 6.2 11.5 5.1 6.4 6.5 12.0 17.2 14.0 12.8 16.75 8.1 10. 5.73 6.6 5.24 6.8 85.5% 89.2% 94.0% 95.7% 81.9% 91.1% 46.9% 44.87% 40.9% 52.3% 52.5% 66.9% 81.1% 82.6% 88.7% 91.9% 204 417 139 236 515 641 584 739 584 365 428 121 182 110 24 Preprint"
        },
        {
            "title": "F DATASET EXAMPLES",
            "content": "Dataset Question Answer GSM8k ProofNet MATH Trips Meetings Calendar Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. did Natalia sell altogether in April and May? How many clips If is rational (r 0) and is irrational, prove that + is irrational. - Step 1: Natalia sold 48 clips in April. - Step 2: Natalia sold half as many clips in May, which is 48 2 = 24. - Step 3: Add them: 48 + 24 = 72. <Answer>: 72 - Step 1: Assume for contradiction that + is rational. - Step 2: Then = (r + x) is rational. - Step 3: This contradicts being irrational. - Step 4: Therefore + is irrational. <Answer>: + is irrational Between what two consecutive integers is Answer in the form and n. 50 located? 30+ You plan to visit 3 European cities for 14 days. Stay in Istanbul for 5 days, Tallinn for 4 days, Zurich for 7 days. From day 1-7, there is show in Zurich. Direct flights exist: Istanbul--Tallinn, Zurich--Tallinn, Zurich--Istanbul. Find valid trip plan. Marina District You arrive at Financial District at 9:00AM. Richard: 3:30-5:30PM (need 90 min). Andrew: 5:00-10:00PM (need 120 min). Travel: FD-Alamo 17min, Marina-Alamo 15min. Plan schedule to meet both. FD-Marina 15min, Alamo Square 30 5.477. - Step 1: 50 7.071. - Step 2: - Step 3: Sum 12.548. - Step 4: Lies between 12 and 13. <Answer>: 12 and 13 - Step 1: Start in Zurich for days 1-7 (show). - Step 2: Fly Zurich Tallinn, stay days 8-11. - Step 3: Fly Tallinn Istanbul, stay days 12-14. <Answer>: Day 1-7: Zurich, Day 8-11: Tallinn, Day 12-14: Istanbul - Step 1: Leave FD 2:45PM, arrive Marina 3:00PM. - Step 2: Meet Richard 3:30-5:00PM. - Step 3: Leave Marina 5:15PM, arrive Alamo 5:30PM. - Step 4: Meet Andrew 5:30-7:30PM. <Answer>: FD Marina Alamo schedule works Schedule 30min meeting for Laura and Paul on Monday between 9:00-17:00. Laura busy: 11:30-12:30, 14:30-15:00, 16:00-16:30. Paul busy: 11:00-14:30, 15:00-17:00. Paul prefers not after 9:30. 9:30-10:00, - Step 1: Laura free 9:00-11:30. - Step 2: Paul free 9:00-9:30 and before 9:30 only. - Step 3: Intersection is 9:00-9:30. <Answer>: Meeting 9:00-9:30 StrategyQA Did Donatello use smartphone? ScienceQA Which word is not like the others? A. horse B. goat C. squirrel D. leg - Step 1: Donatello lived 1386-1466. - Step 2: Smartphones invented 21st century (iPhone 2007). - Step 3: Impossible for him to have used one. <Answer>: No, he did not - Step 1: Horse, goat, squirrel are animals. - Step 2: Leg is body part. - Step 3: Outlier is D. <Answer>: D. leg Table 11: Examples of question-answer pairs from each dataset, ordered by domain: Math (GSM8k, ProofNet, MATH), Planning (Trips, Meetings, Calendar), and QA (StrategyQA, ScienceQA). Table 11 presents examples of questions and LLM answers from the training datasets. 25 Preprint"
        },
        {
            "title": "G HUMAN VALIDATION DETAILS",
            "content": "We first randomly shuffle the DeepSeek-R1 annotated test set. The human annotation then starts from the first row of the shuffled dataset. Human annotators are not provided with DeepSeek-R1s rationales (i.e., contents within the <think> tag) and must independently assess the correctness of each reasoning step, following the step-level correctness definition in Figure 6. The annotation difficulty varies by tasks. ScienceQA is the easiest, which mostly requires common sense and usually takes 1 to 2 minutes to annotate one step. StrategyQA requires extensive factchecking against Wikipedia, but since the dataset provides relevant facts for grading (available to both human and LLM annotators),annotation usually takes around 2 minutes per step. ProofNet and three planning datasets are the most demanding, often requiring more than 5 minutes per step due to the reasoning length and question complexity. Regardless of dataset difficulty, annotators are requested to annotate at least 100 steps from each dataset, with the option to annotate more depending on their availability."
        },
        {
            "title": "H THE USAGE OF LLMS",
            "content": "In this paper, we mainly use LLMs as objects of study. We also use DeepSeek-R1 and Qwen3-8B as training data annotators, as detailed in 3 and C.1. We also use DeepSeek-R1 as judge to grade answer correctness, as detailed in 4. DeepSeek-R1s accuracy in these tasks is manually validated through human annotation (see Table 1). In coding and writing, we use LLM assistants (e.g., ChatGPT) to identify grammar errors and debug. Such usage is under careful human supervision."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "MBZUAI",
        "National University of Singapore",
        "The University of Melbourne",
        "University of Zurich"
    ]
}