{
    "paper_title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention",
    "authors": [
        "Yiwen Chen",
        "Zhihao Li",
        "Yikai Wang",
        "Hu Zhang",
        "Qin Li",
        "Chi Zhang",
        "Guosheng Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with fine-grained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose Ultra3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate a coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, a geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7x speed-up in latent generation. To support this mechanism, we construct a scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference."
        },
        {
            "title": "Start",
            "content": "ULTRA3D: EFFICIENT AND HIGH-FIDELITY 3D GENERATION WITH PART ATTENTION Yiwen Chen1,2, Zhihao Li1,2, Yikai Wang3, Hu Zhang2, Qin Li2,4, Chi Zhang5, Guosheng Lin1 1Nanyang Technological University 4School of Artificial Intelligence, Beijing Normal University 5Westlake University https://buaacyw.github.io/ultra3d/ 3Tsinghua University 2Math Magic 5 2 0 2 J 3 2 ] . [ 1 5 4 7 7 1 . 7 0 5 2 : r Figure 1: Image-to-3D Generation Results of ULTRA3D. ULTRA3D delivers high-quality 3D meshes with fine-grained geometric details while maintaining efficient generation. Please zoom in to view detailed geometry."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with finegrained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose ULTRA3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7 speed-up in latent generation. To support this mechanism, we construct scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that ULTRA3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference. Work done during research internship at Math Magic. Corresponding Authors."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in generative modeling (Poole et al., 2023; Liu et al., 2023; Hong et al., 2023; Zhang et al., 2023; Xiang et al., 2025) have significantly expanded the frontier of 3D content creation. These models enable the generation of high-resolution and structurally consistent 3D assets, supporting wide array of applications in gaming, augmented and virtual reality (AR/VR), digital content creation, and robotics. The growing demand for scalable and controllable 3D generation pipelines in these domains has spurred extensive research into expressive representations and generative models (Zhang et al., 2023; Xiang et al., 2025) that can produce high-fidelity 3D content. notable advancement in this direction is the introduction of sparse voxel-based representations (Ren et al., 2024; Xiang et al., 2025). Sparse voxel-based representations have emerged as powerful 3D representations due to their ability to capture fine-grained geometry (Ren et al., 2024; Xiang et al., 2025; Ye et al., 2025; He et al., 2025b; Wu et al., 2025; Li et al., 2025b). In this paradigm, 3D object is encoded as sparse voxel grid, where each active voxel is associated with latent feature vector. This design enables two complementary benefits: (1) the voxel grid provides coarse yet globally consistent structural layout; and (2) the per-voxel latent features support localized surface modeling with fine granularity. Together, these attributes facilitate high-quality 3D generation with fine-grained geometry quality (Xiang et al., 2025). As result, sparse voxels have been adopted by series of state-of-the-art 3D generation frameworks (Xiang et al., 2025; Ye et al., 2025; Wu et al., 2025; He et al., 2025b; Li et al., 2025b), becoming mainstream solution for 3D modeling. However, the benefits of this expressive representation come at the cost of computational efficiency. Most existing frameworks adopt two-stage pipeline introduced by Xiang et al. (2025): first, predicting the coordinates of active voxels, followed by generating per-voxel latent vectors. Both stages are typically implemented using Diffusion Transformer (DiT) (Peebles & Xie, 2023). As resolution increases, the number of tokens processed in each stage grows significantly, leading to substantial memory and computation overhead due to the quadratic complexity of attention computation (Vaswani et al., 2017). Consequently, current approaches are often constrained to low resolutions and limited output quality. Built upon the success of sparse voxel generation frameworks (Xiang et al., 2025), we aim to develop an efficient 3D generation pipeline that overcomes their efficiency limitations and supports highresolution modeling. We observe that the two stages in the pipeline serve distinct purposes: the first stage constructs coarse object layout, while the second stage refines fine surface details. In the first stage, prior methods typically compress sparse voxel coordinates into dense feature grid at one-fourth the resolution for DiT modeling. Although this reduces the number of tokens, the computation remains costly at high resolutions. This is because directly predicting tens of thousands of coordinates is inherently complex and computationally intensive. Therefore, we depart from direct prediction and instead propose using more efficient methods to first generate coarse mesh, which is then voxelized to produce the sparse voxel coordinates. Notably, since the coarse mesh is only expected to convey the overall structure rather than fine surface details, it can be efficiently represented using compact 3D representation. Specifically, we adopt VecSet (Zhang et al., 2023), an efficient 3D representation that encodes each 3D asset into small set of latent tokens, for generating sparse voxel coordinates. While VecSet is less expressive for finegrained geometry than sparse voxels, its compactness enables highly efficient generation, which is sufficient for producing the coarse object layout without compromising final generation quality. In practice, this reduces generation time from several minutes to just few seconds for producing 128-resolution sparse voxels. To refine the coarse outputs from the first stage with rich and precise latent vectors, previous methods typically adopt DiT architecture with expensive full attention (Xiang et al., 2025; Ye et al., 2025; Li et al., 2025b). However, we observe that the sparse voxels predicted from the first stage already captures the overall object structure, making global attention often redundant and inefficient for local detail refinement. Motivated by this, we introduce Part Attention, geometry-aligned, localized attention mechanism tailored for sparse voxels. It leverages voxel-level part annotations to restrict attention computation within each part group, thereby significantly enhancing efficiency by avoiding unnecessary interactions across unrelated regions. 2 Figure 2: Expeiments on different attention mechanisms. Each color denotes an attention group, within which attention is computed independently. All other settings remain unchanged, with only the attention mechanism being replaced. 3D Window Attention partitions the object space into 8 fixed regions by splitting at the center along each axis. This fixed partitioning often misaligns with semantic boundaries, leading to degraded quality and style inconsistencies. Compared to the widely used window attention in LLMs (Beltagy et al., 2020; Zaheer et al., 2021), Part Attention is better suited for 3D sparse voxels. As shown in Fig. 2, fixed window attention struggles with the irregular nature of sparse voxels, often leading to inconsistent styles and degraded performance. In contrast, Part Attention partitions tokens according to the objects parts and geometric layout, respecting semantic boundaries and object structure, thereby preserving consistency and high-quality output. Our experiments demonstrate that Part Attention achieves up to 6.7 speed-up without compromising generation quality. To support Part Attention, we construct an efficient, large-scale part annotation pipeline. Given the limited availability of part-labeled 3D datasets (Yang et al., 2024), we adopt PartField (Liu et al., 2025) as our base segmentation model to convert raw mesh data into sparse voxels with part annotations. To ensure data quality, we further apply series of lightweight filtering strategies. Our pipeline takes only few seconds to process each mesh with high annotation quality, making it practical for large-scale dataset annotation. Our experiments show that our design accelerates the generation pipeline by 3.3 without compromising quality. Furthermore, ULTRA3D achieves state-of-the-art performance in both visual fidelity and user preference. Our main contributions are summarized as follows: We present ULTRA3D, an efficient high-resolution 3D generation framework that first produces sparse voxels via the compact VecSet representation (Zhang et al., 2023) and then refines them through per-voxel latent generation (Xiang et al., 2025), enabling both speed and fidelity. We introduce Part Attention, an efficient localized attention mechanism tailored for sparse voxels that performs attention computation independently within each part group. By preserving geometric continuity, it achieves up to 6.7 speed-up without compromising quality. We develop scalable part annotation pipeline that efficiently converts raw meshes into part-labeled sparse voxels, enabling high-quality annotations at scale. Our experiments demonstrate that ULTRA3D achieves state-of-the-art performance in both visual quality and user preference, while achieving up to 3.3 speed-up over baseline methods without compromising generation quality."
        },
        {
            "title": "2 RELATED WORK",
            "content": "The field of 3D object generation has witnessed rapid progress in recent years (Liu et al., 2023; Poole et al., 2023; Wang et al., 2023a; Liu et al., 2024a; Wang et al., 2023c; Chan et al., 2023; Liang et al., 2024; Lin et al., 2023; Yi et al., 2024; Chen et al., 2023; Long et al., 2023; Shi et al., 2024; Liu et al., 2024b; Wang & Shi, 2023; Wu et al., 2024a; Li et al., 2024b; Tang et al., 2024c; Hong et al., 2023; Tang et al., 2024a; Xu et al., 2024b; Wang et al., 2023b; Li et al., 2024a; Zhang et al., 2024a; Zou et al., 2024; Wei et al., 2024; Xu et al., 2024a; Hong et al., 2024; Tochilkin et al., 2024), with 3 several promising directions emerging for scalable 3D object generation (Zhang et al., 2023; 2024b; Xiang et al., 2025; Xiong et al., 2025; Nash et al., 2020; Siddiqui et al., 2024; Chen et al., 2024b). We broadly categorize current methods into three main categories."
        },
        {
            "title": "2.1 VECTOR SET-BASED OBJECT GENERATION",
            "content": "3DShape2Vecset (Zhang et al., 2023) introduces pipeline that uses VAE to compress 3D shapes into compact latent space named Vector Set (VecSet), and subsequently trains diffusion model in this latent space. Follow-up works (Zhang et al., 2024b; Zhao et al., 2024; Wu et al., 2024b; Li et al., 2024c; Lan et al., 2024; Hong et al., 2024; Li et al., 2025a; Chen et al., 2025; Zhao et al., 2025b; Hunyuan3D et al., 2025; Lai et al., 2025; Yang et al., 2025; Lin et al., 2025; Tang et al., 2025) have demonstrated that this pipeline is highly scalable and capable of producing high-resolution meshes from large-scale datasets. Due to the compact nature of the VecSet representation (typically only few thousand tokens), both training and inference are computationally efficient. Yang et al. (2025); Lin et al. (2025); Tang et al. (2025) have shown the flexibility of this representation in integrating part information, enabling simultaneous generation of 3D object and part-label. These methods can be combined with our work, as they can provide high-quality part labels for our Part Attention mechanism. While VecSet is flexible and efficient, it falls short in modeling fine-grained 3D surface details when compared to sparse voxel-based methods (Xiang et al., 2025; He et al., 2025b; Li et al., 2025b). 2.2 SPARSE VOXEL-BASED OBJECT GENERATION Trellis (Xiang et al., 2025) introduces novel 3D representation known as the structured latent, which encodes 3D assets as 3D sparse voxels augmented with latent vectors to capture fine-grained surface details. The generation of this representation typically follows two-stage pipeline. In the first stage, the voxel coordinates are compressed into low-resolution continuous feature grid using lightweight 3D convolutional VAE, and then Diffusion Transformer (DiT) (Peebles & Xie, 2023) is trained to generate this grid from noise. In the second stage, another DiT generates the corresponding latent features, conditioned on these voxel coordinates. Both stages are trained independently using the conditional flow matching objective (Lipman et al., 2023). Subsequent studies (Ye et al., 2025; He et al., 2025a; Wu et al., 2025; Li et al., 2025b) demonstrate that sparse voxel excels at modeling extremely fine-grained 3D geometry, significantly outperforming prior representations in terms of geometric fidelity. While sparse voxel provides superior fidelity, it suffers from significant computational overhead. As the resolution increases, the number of sparse voxel tokens can exceed 20K, leading to expensive attention computation. 2.3 AUTOREGRESSIVE MESH GENERATION MeshGPT (Siddiqui et al., 2024) introduced vertex-by-vertex autoregressive generation approach for meshes, producing outputs that closely resemble those created by human artistsmaking them particularly valuable in applications such as gaming and digital content creation. Building on this direction, MeshAnything (Chen et al., 2024b) proposed scalable shape-to-mesh training setting focused on topology generation. Subsequent developments (Chen et al., 2024a; Tang et al., 2024b; Chen et al., 2024c; Weng et al., 2024; Hao et al., 2024; Wang et al., 2024; Zhao et al., 2025a), demonstrated that with large-scale training data, such models can generate highly detailed meshes with thousands of faces. This line of work stands out for its ability to mimic artist-created meshes, though it also faces challenges related to high computational cost due to the large number of autoregressive tokens."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "3.1 SPARSE VOXEL-BASED REPRESENTATIONS Introduced by Xiang et al. (2025), sparse voxel-based representations (Xiang et al., 2025; Ye et al., 2025; He et al., 2025b; Li et al., 2025b) compress 3D object into set of latent features distributed over sparse voxel grid. Formally, shape is encoded as collection of tuples {(zi, pi)}L i=1, where pi {0, . . . , 1}3 denotes the coordinate of an active voxel in cubic grid of resolution , and 4 Figure 3: Pipeline Overview. We introduce ULTRA3D, an efficient and high-quality 3D generation framework that first generates sparse voxel layout via VecSet and then refines it by generating pervoxel latent. The core of ULTRA3D is Part Attention, an efficient localized attention mechanism that performs attention computation independently within each part group. Besides, when the input condition is an image, each part group performs cross attention only with the image tokens onto which its voxel tokens are projected. zi RC is the latent vector associated with that voxel. This formulation preserves both geometric fidelity and spatial locality, making it well-suited for high-resolution surface modeling. The generation of this representation typically follows two-stage pipeline (Xiang et al., 2025). In the first stage, the coordinates {pi}L i=1 are compressed into low-resolution continuous feature grid RDDDC using lightweight 3D convolutional VAE. DiT (Peebles & Xie, 2023) is trained to generate from noise. i=1 generates the corresponding latent features {zi}L i=1. Both stages are trained independently using the conditional flow matching (CFM) objective (Lipman et al., 2023). In the second stage, another DiT conditioned on {pi}L 3.2 VECTOR SET REPRESENTATION The Vector Set (VecSet) (Zhang et al., 2023) representation is compact and permutation-invariant representation for 3D object modeling, designed to facilitate scalable generative pipelines. VecSet encodes 3D object as an unordered set of latent vectors, where each latent vector encodes localized 3D shape and semantic attributes. With only few thousand vectors per shape, VecSet supports fast diffusion-based generation, scaling well to high-resolution 3D synthesis (Zhang et al., 2023; 2024b; Zhao et al., 2025b). While sparse voxels offer strong expressiveness and state-of-the-art surface fidelity (Xiang et al., 2025; He et al., 2025b; Li et al., 2025b), their large token count at high resolutions poses significant challenges for efficient training and inference. In contrast, the compactness of VecSet (Zhang et al., 2023) enables fast generation with only few thousand latent tokens per shape, but its lack of explicit spatial structure limits its ability to capture fine-grained geometry, making it less suitable for high-resolution surface modeling."
        },
        {
            "title": "4 METHOD",
            "content": "We present ULTRA3D, an efficient 3D generation framework that supports high-resolution and highfidelity modeling. It first produces sparse voxels using the compact VecSet representation (Zhang et al., 2023), and then refines them through per-voxel latent generation (Xiang et al., 2025), achieving both speed and fidelity. At the core of ULTRA3D is Part Attention, an efficient attention mechanism designed to accelerate sprase voxel generation by performing attention computation independently within each part group 5 (see Sec. 4.1). We describe the overall generation pipeline in Sec. 4.2, and introduce our scalable part annotation pipeline in Sec. 4.3."
        },
        {
            "title": "4.1 PART ATTENTION",
            "content": "In the widely used sparse voxel generation pipeline (Xiang et al., 2025), the primary computational bottleneck lies in generating the local latents {zi}L i=1. For instance, generating 1024-resolution mesh typically requires performing attention over sparse voxel at resolution 64 or 128. In these cases, the average number of active voxels reaches approximately 20K and 60K, respectively. This makes attention computation prohibitively expensive due to its quadratic scaling with respect to token count. Moreover, as the mesh resolution increases, the corresponding sparse voxel becomes more detailed and effectively captures the global shape and topology of the object. Consequently, the role of sparse voxel latent generation shifts toward refining local surface geometry rather than modeling the overall structure. These observations suggest that full global attention across all tokens is both unnecessary and inefficient. straightforward solution is to introduce more efficient attention mechanisms, such as window attention, as explored in the context of large language models (Beltagy et al., 2020). However, as illustrated in Fig. 2, these methods cannot be directly applied to our case. These attention strategies often adopt certain fixed pattern and leads to attention boundaries that misalign with the semantic layout of the object, leading to inconsistent styles and degraded performance. We find that the fundamental reason is the structural complexity of 3D assets: unlike text sequences, 3D sparse voxel cannot be easily divided into semantically meaningful local blocks using fixed partitioning schemes. The above observations motivate the introduction of new 3D attention design, Part Attention. Part Attention performs grouping based on externally provided part information. Specifically, for each active voxel pi, we assign part index ai {1, . . . , K}, where denotes the number of part groups. Based on this grouping, we apply attention computations that are restricted within each part. Part Attention respects semantic boundaries and object structure, thereby preserving consistency and high-quality output. We next illustrate the designs of Part Self Attention and Part Cross Attention. Part Self Attention. During self-attention computation, each token only attends to other tokens that belong to the same part. Let Attn(i, j) denote the attention mask value from the token located at pi to the token at pj. The attention is masked as: Attn(i, j) = 0 if ai = aj. (1) This masking strategy enforces part-level locality by restricting attention to within each part group, enabling more structured and efficient attention computation. Under the assumption that each part group contains similar number of tokens, the computation cost of attention is reduced by nearly factor of K. Part Cross Attention. In the image-to-3D setting, cross-attention between the 3D sparse voxel and the image features becomes increasingly expensive at high resolutions. To reduce this cost, we also leverage the part index ai {1, . . . , K} to constrain attention across modalities. Our goal is to allow each 3D voxel to only interact with 2D image regions that correspond to the same part. To implement this, we project each 3D part group onto the condition image using externally provided camera parameters, and assign the part index to the pixels onto which the 3D voxels are projected. Since multiple 3D parts may project onto the same pixel, each 2D image token is associated with set of part indices. Let Attn(i, j) denote the attention mask value from the voxel token located at pi to the image token indexed by j, and let Aj denote the set of part indices assigned to the j-th image token. The attention is masked as: Attn(i, j) = 0 if ai / Aj, (2) 6 Figure 4: Impact of Resolution on Generation Quality. We compare results under different configurations, where 512 64 denotes mesh resolution of 512 and sparse voxel resolution In previous works, to reduce computational cost in the second stage, the sparse voxels of 64. are typically downsampled by half before attention computation in the DiT, then upsampled afterwardannotated as Downsample in the figure. As shown, both the mesh resolution and the sparse voxel resolution used during attention computation significantly impact the final quality. However, due to efficiency constraints, prior methods were limited to lower resolutions. In contrast, our efficient framework supports higher sparse voxel resolutions, making high-quality generation feasible. ensuring that each voxel token only attends to pixels aligned with its own part group. This masking reduces the overall cross-attention cost while preserving semantic consistency between 3D and 2D tokens. In ULTRA3D, the part labels required by Part Attention are provided by PartField (Liu et al., 2025)during both training and inference. For camera information, we use ground-truth cameras during training and obtain estimated cameras from the VecSet decoder during inference. 4.2 ULTRA3D Following Xiang et al. (2025), our generation pipeline is divided into two stages: the first stage generates sparse voxel coordinates that capture the overall object structure, and the second stage refines these coordinates by producing high-quality per-voxel latent features for final mesh reconstruction. VecSet-based Sparse Voxel Generation. In our initial experiments, we adopted the sparse voxel generation architecture from Trellis (Xiang et al., 2025), as outlined in Sec. 3.1. However, we found that this design struggles to balance quality and efficiency at high resolutions. As shown in Fig. 4, when aiming to generate meshes at resolution of 1024, high-quality results typically require sparse voxels at 128 resolution. Under Trellis (Xiang et al., 2025) formulation, this corresponds to applying diffusion-based generation over 323 dense gridequivalent to full attention over 32K tokenswhich incurs prohibitive computational cost during both training and inference. Alternatively, downsampling to 163 grid reduces the token count but leads to significant drop in generation quality, making it inadequate for high-fidelity outputs. Faced with this dilemma, we turn to VecSet (Zhang et al., 2023) for its compact representation and generation efficiency, making it suitable choice for modeling coarse object structures at scale. As the vector set typically contains only few thousand tokens, its computational cost is significantly lower than that of sparse voxel-based methods. Specifically, we first use it to generate 512-resolution mesh, which is then voxelized into sparse voxel. Since the sparse voxel typically has resolution of 64 or 128, the surface fidelity limitations of VecSet-based models are largely mitigated and have minimal impact on downstream quality. As illustrated in Sec. 3.2, our implementation of VecSet (Zhang et al., 2023) generation pipeline follows Zhao et al. (2025b). The shape VAE encodes point clouds sampled from mesh surfaces into fixed-length vector set, and decodes them into Sign Distance Function (SDF). In addition, since Part Cross Attention requires camera information, we also encode the camera parameters into the latent vector. Sparse Latent Generation. Despite the efficiency of VecSet (Zhang et al., 2023), its inability to capture fine surface geometry necessitates subsequent refinement stage using sparse voxelFigure 5: Robustness of Part Annotation. Although our method is trained using data with exactly 8 part groups, we find it to be robust to variations in part annotation. Varying the number of part groups has little impact on generation quality, suggesting that increasing the number of annotated part groups can further accelerate computation without compromising performance. based method. We follow Trellis (Xiang et al., 2025) and adopt 3D sparse VAE, along with its corresponding flow-matching DiT, to generate per-voxel latents {zi}L i=1. We adopt the SparconvVAE introduced by Li et al. (2025b) as our 3D sparse VAE. It compresses 3D shapes into sparse voxel with latents {(zi, pi)}L i=1 as illustrated in Sec. 3.1. For DiT modeling, we replace most attention blocks with Part Attention to improve efficiency. small number of full attention layers are retained to align styles across part groups. However, since full attention at high resolutions is computationally expensive, we introduce residual block that performs full attention at lower resolution. Specifically, as shown in Fig. 3, we downsample the sparse voxels, apply full attention, and then upsample the features to fuse them backenabling efficient cross-part communication at low cost. In practice, we stack one full-attention block followed by three Part Attention blocks repeatedly to build the DiT architecture. The part labels are provided by an external part annotation model (Liu et al., 2025) during both training and inference. Note that recent advances in VecSet-based part-aware 3D generation (Yang et al., 2025; Lin et al., 2025; Tang et al., 2025) offer an alternative for directly generating sparse voxels with part annotations, which can be integrated into our framework. It is worth noting that, for annotation efficiency, we use fixed 8-part grouping during training. However, this does not limit the models flexibility at inference time, as it can accept inputs with varying numbers of part groups. As shown in Fig. 5, our model consistently produces high-quality outputs even when the number of part groups differs from that used during training. 4.3 SPARSE VOXEL PART ANNOTATION In this section, we illustrate our data preprocessing pipeline for producing part annotations required by Part Attention. While existing large-scale datasets (Chang et al., 2015; Deitke et al., 2023b;a) provide wealth of 3D assets, most of them lack part-level annotations. Therefore, we construct part annotation pipeline that efficiently transforms raw meshes into sparse voxel with part annotations. 8 Figure 6: Percentile for Filtering Metrics of Part Annotation. We apply two metrics to filter poorly segmented samples: (1) the sum of squared voxel ratios, which identifies imbalanced part distributions, and (2) neighborhood inconsistency, which measures the proportion of voxels whose neighbors have different part labels. Lower values on both metrics indicate better segmentation quality. As shown in the plot, most samples exhibit stable and low values across both metrics, indicating high annotation quality. Since our goal is to annotate millions of 3D assets, methods that require several minutes per shape are impractical. We therefore adopt PartField (Liu et al., 2025), an efficient and high-quality part segmentation model. PartField is feedforward network that takes point-sampled 3D shape as input and predicts feature field in the form of triplane, which is subsequently clustered to obtain part-level segmentation. Its fast inference speed and reasonably accurate predictions make it wellsuited for large-scale preprocessing. To process raw mesh into sparse voxels with part annotations, we first uniformly sample point clouds from the mesh surface and input them into PartField (Liu et al., 2025) to obtain triplane feature field. We then query per-point features of the sampled point clouds from the triplane and voxelize the raw mesh. For each voxel, we average the features of the sampled points within it to produce sparse voxel with part-aware voxel features. Finally, we apply Agglomerative Clustering to segment the voxels into parts based on these features (Liu et al., 2025). key challenge during clustering is determining the appropriate number of part groups. Since Part Attention benefits from part segmentation that aligns with geometric structures, the ideal number of clusters can vary across different objects. Using too many clusters results in over-segmentation with noisy and fragmented parts, while too few clusters limit the computational advantage of Part Attention. Although one could design heuristics to adaptively determine the optimal number of clusters per object, we found this significantly increases the preprocessing cost. As practical compromise, we apply fixed number of clusters to all meshes and empirically set the number to 8. This setting yields reasonable part segmentations for the vast majority of samples. After clustering, we apply two filtering criteria to discard samples with suboptimal segmentations. First, we heuristically assume that samples dominated by single part group tend to exhibit poor segmentation quality. To identify such cases, we compute the voxel ratio of each part group, square these ratios, and sum them as measure of imbalance. higher score indicates more uneven part distribution. Samples exceeding fixed threshold are removed to eliminate those with overly dominant part groups. Second, well-segmented shapes typically exhibit strong spatial consistency, where most voxels are surrounded by neighbors that share the same part label. To quantify this, we define metric called neighborhood inconsistencythe proportion of voxels whose neighbors have different part labels. Samples with low full-neighborhood consistency are discarded, as they often reflect fragmented or noisy segmentations. With filtering included, the full preprocessing pipeline is highly efficient, processing each mesh in just 2 seconds on an A800 GPU. As shown in Fig. 6, the majority of processed samples exhibit high-quality segmentations, indicating the reliability of our annotation pipeline. 9 Figure 7: Comparison with Prior Methods. Our method produces higher fidelity and richer surface details. As highlighted in the red boxes, our results align more closely with the input image compared to other methods. During inference, we first pass the input image through our sparse voxel generation pipeline to obtain coarse mesh. We then apply the same part annotation procedure to obtain part-labeled sparse voxel, which is required by the Part Attention."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 IMPLEMENTATION DETAILS ULTRA3D adopts sparse voxel resolution of 128 and produces output meshes at resolution of 1024. Unless otherwise specified, all results reported in this paper are generated with this setting. Our 3D sparse VAE adopts the settings from Li et al. (2025b). The VecSet-based generator follows the configuration of Zhao et al. (2025b), while our part-aware DiT is built upon Xiang et al. (2025), incorporating the modifications described in Sec. 4.2, and scaling up the model size to 1.8B parameters. 10 Table 1: User Study. Each table reports selection rates from user studies conducted on image3D mesh pairs. Participants were asked to choose the result that best matches the image and exhibits the highest quality. (a) Comparison with Other Methods Model Direct3D-S2 Commercial Model Select. 7.2% 24.3%"
        },
        {
            "title": "Ours",
            "content": "68.5% (b) Full Attention vs. Part Attention Model Ours-Full Ours No Pref. (c) 3D Window vs. Part Attention Model Ours-Naive"
        },
        {
            "title": "Ours",
            "content": "No Pref. Select. 12.4% 8.9% 76.7% Select. 2.1% 63.7% 34.2% All models are trained on private 3D dataset, annotated using the part annotation pipeline detailed in Sec. 4.3. Following the procedure described in the method, we exclude samples where the sum of squared part ratios exceeds 25% or the neighborhood inconsistency exceeds 25%. The SparConvVAE (Li et al., 2025b) is trained for 2 days on 32 A800 GPUs; the VecSet-DiT (Zhang et al., 2023; Peebles & Xie, 2023) is trained for 15 days on 128 GPUs; and the part-aware DiT is trained for 15 days on 256 GPUs with total batch size of 256. For training the part-aware DiT, we use the AdamW (Kingma & Ba, 2014) optimizer with learning rate of 1e-4 and no weight decay. An exponential moving average (EMA) with rate of 0.9999 is applied. The unconditional guidance probability is set to 0.1, and the timestep sampling schedule follows Logit-Normal distribution with mean of 0 and standard deviation of 1. The model is conditioned on DINOv2 (Oquab et al., 2023) image embeddings. At inference time, we sample with classifier-free guidance (Ho & Salimans, 2021) scale of 3.5 over 25 steps. 5.2 QUALITATIVE EXPERIMENTS Comparison with Concurrent Methods. As shown in Fig. 7, ULTRA3D consistently outperforms prior methods in visual quality, demonstrating notable improvements in geometric detail and surface accuracy. These gains highlight the effectiveness of our framework and the benefits of our efficient and high resolution design. Ablation of Part Attention. As shown in Fig. 2, Part Attention achieves quality comparable to full global attention, while 3D window attention results in significantly degraded geometry. This is because 3D window attention relies on fixed partitions, which often misalign with object semantics and lead to fragmented attention computation. In contrast, our Part Attention respects semantic boundaries and preserves geometric continuity, enabling both accurate surface modeling and efficient computation. Note that both the Full Attention and 3D Window Attention models are finetuned from the Part Attention checkpoints for sufficient training steps. 5.3 QUANTITATIVE EXPERIMENTS User Study. We conducted user studies with 36 participants to evaluate the effectiveness of ULTRA3D and the proposed Part Attention. Each participant was presented with set of image3D mesh pairs and asked to choose the result that best matched the image in terms of overall quality and fidelity. Three sets of comparisons were conducted: (a) Comparison with Other Methods: 20 imagemesh pairs comparing ULTRA3D with Direct3D-S2 and the commercial model A. (b) Full Attention vs. Part Attention: 10 imagemesh pairs comparing ULTRA3D with variant where Part Attention is replaced by Full Attention. (c) 3D Window vs. Part Attention: 10 imagemesh pairs comparing ULTRA3D with variant where Part Attention is replaced by 3D Window Attention. 11 Table 2: Efficiency Comparison. Part Attention significantly accelerate computation over full attention. Substituting most DiT layers with Part Attention yields substantial speed-up in both training and inference. All attention blocks are implemented with FlashAttention-2 (Dao, 2023)."
        },
        {
            "title": "Part Cross Attention DiT Training DiT Inference",
            "content": "Speedup Rate 6.7 4.1 3.1 3.3 As shown in Tab.1, ULTRA3D achieves the highest user preference in (a), with 68.5% of selectionssubstantially outperforming existing state-of-the-art methods. In (b), Part Attention attains comparable preference to full attention, confirming its efficiency advantage without compromising quality. In (c), users clearly favored Part Attention over 3D window attention (63.7% vs. 2.1%), suggesting that our design better maintains geometric continuity and semantic alignment. Acceleration Benefits of Part Attention. We evaluate the acceleration benefits of Part Attention under the unified use of FlashAttention-2. As shown in Tab. 2, both Part Self Attention and Cross Attention significantly reduce computation cost. Consequently, the overall training and inference pipeline is also accelerated. Depending on the number of active voxels, the full attention baseline often requires over 15 minutes to generate single meshan impractical costwhereas our pipeline averages just 4 minutes per sample."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we aim to address the computational bottlenecks of sparse voxel-based 3D generation. We identify the inefficiency of traditional two-stage pipelines and propose more scalable alternative by first generating coarse mesh using VecSet and then voxelizing it into sparse voxels for refinement. To further alleviate the cost of full attention in the second stage, we introduce Part Attentiona geometry-aware attention mechanism that restricts computation to semantically coherent part groups. This design preserves detail while significantly reducing computation. To support this, we construct large-scale, efficient part annotation pipeline, enabling high-quality part annotation at scale. Through extensive experiments, we demonstrate that our framework ULTRA3D achieves state-of-the-art visual quality with substantially improved efficiency in both training and inference, making high-resolution 3D generation more tractable and practical."
        },
        {
            "title": "REFERENCES",
            "content": "Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3d-aware diffusion models. In IEEE/CVF ICCV, pp. 41944206, 2023. Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Billzb Wang, Jingyi Yu, Gang Yu, et al. Meshxl: Neural coordinate field for generative 3d foundation models. Advances in Neural Information Processing Systems (NeurIPS), 2024a. Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, and Chi Zhang. Meshanything: Artist-created mesh generation with autoregressive transformers, 2024b. URL https://arxiv.org/abs/2406. 10163. Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization, 2024c. URL https://arxiv.org/abs/2408.02555. 12 Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using gaussian splatting. arXiv preprint arXiv:2309.16585, 2023. Zilong Chen, Yikai Wang, Wenqiang Sun, Feng Wang, Yiwen Chen, and Huaping Liu. Meshgen: Generating pbr textured mesh with render-enhanced auto-encoder and generative data augmentation, 2025. URL https://arxiv.org/abs/2505.04656. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023a. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023b. Zekun Hao, David W. Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like 3d mesh generation at scale, 2024. URL https://arxiv.org/abs/2412.09548. Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, and Yangguang Li. Sparseflex: High-resolution and arbitrary-topology 3d shape modeling, 2025a. URL https://arxiv.org/abs/2503.21732. Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, and Yangguang Li. Sparseflex: High-resolution and arbitrary-topology 3d shape modeling. arXiv preprint arXiv:2503.21732, 2025b. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Tengfei Wang, Liang Pan, Dahua Lin, and Ziwei Liu. 3dtopia: Large text-to-3d generation model with hybrid diffusion priors. CoRR, abs/2403.02234, 2024. Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, Qingxiang Lin, Zeqiang Lai, Xianghui Yang, Huiwen Shi, Zibo Zhao, Bowen Zhang, Hongyu Yan, Lifu Wang, Sicong Liu, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Dongyuan Guo, Junlin Yu, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Shida Wei, Chao Zhang, Yonghao Tan, Yifu Sun, Lin Niu, Shirui Huang, Bojian Zheng, Shu Liu, Shilin Chen, Xiang Yuan, Xiaofeng Yang, Kai Liu, Jianchen Zhu, Peng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Jingwei Huang, and Chunchao Guo. Hunyuan3d 2.1: From images to high-fidelity 3d assets with production-ready pbr material, 2025. URL https://arxiv.org/abs/2506.15442. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, Sheng Zhang, Xin Huang, Di Luo, Fan Yang, Fang Yang, Lifu Wang, Sicong Liu, Yixuan Tang, Yulin Cai, Zebin He, Tian Liu, Yuhong Liu, Jie Jiang, Linus, Jingwei Huang, and Chunchao Guo. Hunyuan3d 2.5: Towards high-fidelity 3d assets generation with ultimate details, 2025. URL https://arxiv.org/abs/2506.16504. Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, and Chen Change Loy. Ln3diff: Scalable latent neural fields diffusion for speedy 3d generation. In ECCV, 2024. 13 Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. In ICLR, 2024a. Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wenhan Luo, Ping Tan, Wenping Wang, Qifeng Liu, and Yike Guo. Era3d: Highresolution multiview diffusion using efficient row-wise attention. CoRR, abs/2405.11616, 2024b. Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024c. Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape synthesis using large-scale rectified flow models. arXiv preprint arXiv:2502.06608, 2025a. Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, and Bihan Wen. Sparc3d: Sparse representation and construction for high-resolution 3d shapes modeling, 2025b. URL https://arxiv.org/ abs/2505.14521. Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. In IEEE/CVF CVPR, pp. 65176526, 2024. Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In IEEE CVPR, pp. 300309, 2023. Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compositional latent diffusion transformers, 2025. URL https://arxiv.org/abs/2506.05573. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In International Conference on Learning Representations (ICLR), 2023. Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. In NeurIPS, 2024a. Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. Partfield: Learning 3d feature fields for part segmentation and beyond. arXiv preprint arXiv:2504.11451, 2025. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. In ICLR, Syncdreamer: Generating multiview-consistent images from single-view image. 2024b. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008, 2023. Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive generative model of 3d meshes. In International Conference on Machine Learning (ICML), 2020. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pp. 4195 4205, 2023. 14 Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/ pdf?id=FjNys5c7VyY. Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In ICLR, 2024. Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoderonly transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In ECCV, 2024a. Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation, 2024b. URL https: //arxiv.org/abs/2409.18114. Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative In International Conference on Learning gaussian splatting for efficient 3d content creation. Representations (ICLR), 2024c. Jiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang Zeng, Ming-Yu Liu, and Tsung-Yi Lin. Efficient part-level 3d object generation via dual volume packing, 2025. URL https://arxiv.org/abs/2506.09980. Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, In Advances in Neural InforŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. mation Processing Systems (NeurIPS), 2017. Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In IEEE CVPR, pp. 12619 12629, 2023a. Peng Wang and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation. CoRR, abs/2312.02201, 2023. Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Zhang. Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024, 2023b. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023c. Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui Zeng. Llama-mesh: Unifying 3d mesh generation with language models, 2024. URL https://arxiv.org/abs/2411.09595. Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for high-quality mesh. arXiv preprint arXiv:2404.12385, 2024. 15 Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, Tong Zhang, Shenghua Gao, and C. L. Philip Chen. Scaling mesh generation via compressive tokenization, 2024. URL https://arxiv.org/abs/2411. 07025. Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. CoRR, abs/2405.20343, 2024a. Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024b. Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Xun Cao, Philip Torr, and Yao Yao. Direct3d-s2: Gigascale 3d generation made easy with spatial sparse attention, 2025. URL https://arxiv.org/abs/2505.17412. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation, 2025. URL https://arxiv.org/abs/2412.01506. Bojun Xiong, Si-Tong Wei, Xin-Yang Zheng, Yan-Pei Cao, Zhouhui Lian, and Peng-Shuai Wang. Octfusion: Octree-based diffusion models for 3d shape generation, 2025. URL https: //arxiv.org/abs/2408.14732. Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024a. Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. In International Conference on Learning Representations (ICLR), 2024b. Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184, 2024. Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, and Xihui Liu. Holopart: Generative 3d part amodal segmentation, 2025. URL https: //arxiv.org/abs/2504.07943. Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3dgen: High-fidelity 3d geometry generation from images via normal bridging. arXiv preprint arXiv:2503.22236, 2025. Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In IEEE CVPR, pp. 67966807, 2024. Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences, 2021. URL https://arxiv.org/abs/2007.14062. Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In ECCV, 2024a. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024b. 16 Ruowen Zhao, Junliang Ye, Zhengyi Wang, Guangce Liu, Yiwen Chen, Yikai Wang, and Jun Zhu. Deepmesh: Auto-regressive artist-mesh creation with reinforcement learning, 2025a. URL https://arxiv.org/abs/2503.15265. Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, 36, 2024. Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, Huiwen Shi, Sicong Liu, Junta Wu, Yihang Lian, Fan Yang, Ruining Tang, Zebin He, Xinzhou Wang, Jian Liu, Xuhui Zuo, Zhuo Chen, Biwen Lei, Haohan Weng, Jing Xu, Yiling Zhu, Xinhai Liu, Lixin Xu, Changrong Hu, Shaoxiong Yang, Song Zhang, Yang Liu, Tianyu Huang, Lifu Wang, Jihong Zhang, Meng Chen, Liang Dong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Chao Zhang, Yonghao Tan, Jie Xiao, Yangyu Tao, Jianchen Zhu, Jinbao Xue, Kai Liu, Chongqing Zhao, Xinming Wu, Zhichao Hu, Lei Qin, Jianbing Peng, Zhan Li, Minghui Chen, Xipeng Zhang, Lin Niu, Paige Wang, Yingkai Wang, Haozhao Kuang, Zhongyi Fan, Xu Zheng, Weihao Zhuang, YingPing He, Tian Liu, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Jingwei Huang, and Chunchao Guo. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation, 2025b. URL https://arxiv.org/abs/2501.12202. Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1032410335, 2024."
        }
    ],
    "affiliations": [
        "Math Magic",
        "Nanyang Technological University",
        "School of Artificial Intelligence, Beijing Normal University",
        "Tsinghua University",
        "Westlake University"
    ]
}