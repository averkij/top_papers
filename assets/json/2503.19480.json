{
    "paper_title": "GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers",
    "authors": [
        "Shijie Ma",
        "Yuying Ge",
        "Teng Wang",
        "Yuxin Guo",
        "Yixiao Ge",
        "Ying Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The synergy between generative and discriminative models receives growing attention. While discriminative Contrastive Language-Image Pre-Training (CLIP) excels in high-level semantics, it struggles with perceiving fine-grained visual details. Generally, to enhance representations, generative models take CLIP's visual features as conditions for reconstruction. However, the underlying principle remains underexplored. In this work, we empirically found that visually perfect generations are not always optimal for representation enhancement. The essence lies in effectively extracting fine-grained knowledge from generative models while mitigating irrelevant information. To explore critical factors, we delve into three aspects: (1) Conditioning mechanisms: We found that even a small number of local tokens can drastically reduce the difficulty of reconstruction, leading to collapsed training. We thus conclude that utilizing only global visual tokens as conditions is the most effective strategy. (2) Denoising configurations: We observed that end-to-end training introduces extraneous information. To address this, we propose a two-stage training strategy to prioritize learning useful visual knowledge. Additionally, we demonstrate that lightweight denoisers can yield remarkable improvements. (3) Generation paradigms: We explore both continuous and discrete denoisers with desirable outcomes, validating the versatility of our method. Through our in-depth explorations, we have finally arrived at an effective method, namely GenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark, e.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into multimodal large language models for better vision-centric performance. All the models and codes are made publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 8 4 9 1 . 3 0 5 2 : r GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers Shijie Ma1,2, Yuying Ge1,(cid:66), Teng Wang1, Yuxin Guo1,2, Yixiao Ge1, Ying Shan1 2Institute of Automation, CAS 1ARC Lab, Tencent PCG https://mashijie1028.github.io/GenHancer Figure 1. Perfect generation (reconstruction) does not always yield desirable visual representations. (a) Pipeline of fine-grained visual enhancements, where generative models take visual tokens as conditions and perform reconstruction. (b) Experiments across four dimensions, i.e., training iterations, denoiser size, ratio of local tokens as conditions, and whether to use pre-trained denoisers. We measure generation (CLIP score ) and visual representations (MMVP-VLM ) performance. As the results demonstrate, although increasing the number of training iterations, adding more denoiser blocks, using larger ratio of local tokens as conditions, and employing pre-trained denoisers lead to better generation results, the performance of visual representations does not always improve. Best viewed zoomed in."
        },
        {
            "title": "Abstract",
            "content": "The synergy between generative and discriminative models receives growing attention. While discriminative Contrastive Language-Image Pre-Training (CLIP) excels in high-level semantics, it struggles with perceiving finegrained visual details. Generally, to enhance representations, generative models take CLIPs visual features as conditions for reconstruction. However, the underlying principle remains underexplored. In this work, we empirically found that visually perfect generations are not always optimal for representation enhancement. The essence lies in effectively extracting fine-grained knowledge from generative models while mitigating irrelevant information. To explore critical factors, we delve into three aspects: (1) Conditioning mechanisms: We found that even small number of local tokens can drastically reduce the difficulty of reconstruction, leading to collapsed training. We thus conclude that utilizing only global visual tokens as conditions is the most effective strategy. (2) Denoising configurations: We observed that end-to-end training introduces extraneous information. To address this, we propose two-stage training strategy to prioritize learning useful visual knowledge. Additionally, we demonstrate that lightweight denoisers can yield remarkable improvements. (3) Generation paradigms: We explore both continuous and discrete denoisers with desirable outcomes, validating the versatility of our method. Through our in-depth explorations, we have finally arrived at an effective method, namely GenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark, e.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into multimodal large language models for better vision-centric performance. All the models and codes are made publicly available. 1. Introduction Generative and discriminative models have evolved rapidly in recent years [3, 27, 38, 50]. Both of them exhibit complementary strengths, where generative models like diffusion models [14, 34, 51] and rectified flow [9, 26] capture lowlevel visual details, while discriminative models like Contrastive Language-Image Pre-Training (CLIP) [33, 54] and DINO [31] excel in high-level semantics. This complementary nature enables synergistic relationship between them. 1 ing irrelevant information. Furthermore, generative models can be divided into continuous [26, 29] and discrete [8] ones, with different denoising objectives, which should also be considered. Consequently, we conduct in-depth explorations from three key aspects: conditioning mechanisms, denoising configurations, and generation paradigms. Key Point #1: Which part of the visual information should generative models focus on? As in Fig. 1 (a), generative models take visual tokens of discriminative models as conditions. The choice of different tokens significantly In this regard, we find only the impacts the outcomes. global token (i.e., class token) could yield desirable visual enhancements. We attribute this to the fact that class token alone helps maximize mutual information between visual representations and generative models, while local tokens bring about information leakage and drastically reduce the tasks difficulty, resulting in collapsed learning. Key Point #2: How to design denoising configurations to transfer useful information for visual representations? The structure of the denoiser could determine the enhancement effects. Additionally, before training CLIP, it is essential to mitigate irrelevant information. Therefore, we investigate the influence of different sizes of the denosier and training stages. In this paper, we propose GenHancer, two-stage post-training method for visual enhancements. In the first stage, we pre-train the projector and denoiser while freezing the ViT, learning basic reconstruction abilities and mitigating irrelevant information. In the second stage, we fine-tune CLIP ViT to enhance its fine-grained visual representations. Meanwhile, we empirically found that lightweight denoiser is sufficient to achieve remarkable results, which is more efficient yet stronger, as in Fig. 2. Key Point #3: Do two types of denoisers share common enhancing principle for visual representations? For both continuous and discrete denoisers, we present tailormade designs, including denoiser and conditioning structure. Moreover, we reveal that previous Key Points #1, #2 apply to both types, indicating the versatility of our method. Our contributions are summarized as follows: We conduct an in-depth study on visual representation enhancements with generative models and make the innovative discovery that perfect reconstruction and pre-trained models are not necessary. This leads us to explore three key aspects: conditioning mechanisms, denoising configurations, and the generation paradigms. We propose GenHancer, two-stage post-training method with only lightweight denoisers for visual enhancements, which uses only the class token as the conditional input to perform self-supervised reconstruction. Our method is applicable to both continuous and discrete denoisers. Comprehensive vision-centric evaluations show that our enhanced CLIP significantly outperforms prior methods that rely on pre-trained heavy denoisers, as in Fig. 2. Figure 2. Comparison with prior method [46]. (a) We only need lightweight denoiser, but (b) achieve stronger performance than DIVA [46], which relies on pre-trained heavy generative models. Pioneering work [53] has shown that discriminative models can facilitate the training of generative models through feature alignment. Conversely, generative models can also enhance discriminative models by improving their ability to understand fine-grained visual patterns, e.g., orientation, color and quantity. This enhancement is particularly pertinent for models like CLIP, which have inherent visual shortcomings [41] that could also limit Multimodal Large Language Models (MLLMs) [28, 40] in vision-centric tasks. Recent works [16, 45, 46] have attempted to enhance CLIP ViT by using the visual features of ViT [7] as conditional inputs for generative models. These models perform self-supervised reconstruction to compel the discriminative model to capture fine-grained visual details, as illustrated in Fig. 1 (a). While these approaches demonstrate the potential of enhancing representations through generative models, they often rely on pre-trained heavy denoisers and do not explore the underlying principle. To enable generative models to enhance visual representations, natural question arises: Do we need perfect generative model to achieve this enhancement? To address this question, we conducted preliminary experiments from several dimensions, including #training iterations, the ratio of local tokens as conditions, the size of denoisers, and whether to use pre-trained generative model (denoiser), as in Fig. 1 (b). The answer is that perfect generation (reconstruction) does not always yield desirable visual representations. For example, in Fig. 1 (iii), introducing more local tokens as conditions can significantly improve reconstruction, while the visual enhancement will be drastically degraded. In Fig. 1 (iv), although the pre-trained denoiser exhibits better reconstruction, its representations are weaker. This leads us to further investigate the key points for generative models to effectively enhance visual representations. We argue that generative models simultaneously contain useful knowledge, like visual patterns and details, as well as irrelevant information, like the gap between CLIP ViTs feature space and generative models condition space. To effectively enhance representations, our general philosophy is that discriminative models should prioritize learning useful knowledge from generative models while circumvent2 2. Related Works MLLMs and Vision Encoders. Currently, MLLMs predominantly employ CLIP [33] for visual encoding. Tong et al. [41] identified several failure patterns in CLIP, which hinder the fine-grained visual understanding of MLLMs. To overcome this issue, early efforts [18, 40, 41] employed an ensemble of visual experts to combat the visual shortcomings. More recently, ROSS [45] leverages intrinsic visual activations and incorporates self-supervised visual reconstruction loss during training MLLMs. Complementarily, DIVA [46] proposes to enhance CLIPs fine-grained abilities through diffusion feedback. Similar to [46], we independently enhance CLIPs internal representations, which not only strengthens CLIP as vision-language retriever but also enables the enhanced CLIP to be seamlessly integrated into MLLMs in plug-and-play manner for better fine-grained vision-centric performance. Enhancing Visual Representations with Diffusion Models. Early works [36, 39, 56] utilize generative models as data augmenters. Another line of works [5, 16, 47] leverages self-supervised reconstruction tasks with diffusion models, which helps models grasp visual details and learn fine-grained representations. Similarly, DIVA [46] takes CLIPs features as conditional inputs to the diffusion model [34], addressing its visual shortcomings through reconstruction. In summary, prior arts predominantly rely on diffusion models [11, 34], whereas we apply our method to both continuous and discrete generative models. Vision-Centric Benchmarks. Canonical evaluations of MLLMs focus on fundamental multimodal Q&A capabilities across various domains, e.g., general perception and cognition [10], text and characters [37], scientific fields [30], and potential hallucinations [13, 25] in MLLMs. However, these benchmarks could not effectively assess models fine-grained visual perception abilities, such as object color, quantity, orientation, and viewpoint. To solve this issue, Tong et al. [41] systematically explore the failure modes of CLIP and propose challenging MMVP benchmark with 9 visual patterns. CV-Bench [40] further expands with 2,600 vision-centric VQA questions, covering dimensions like spatial relationships, count, depth, and distance of both 2D and 3D domains. Besides, NaturalBench [24] curates natural adversarial samples that are easy for humans but MLLMs struggle with. In this paper, we employ these vision-centric benchmarks to comprehensively evaluate models fine-grained visual abilities. 3. Preliminaries of Generative Models In principle, generative models can be divided into continuous and discrete ones. For continuous generative models, we focus on the recently popular rectified flow [26, 29], while discrete generative models are conventionally built upon pre-trained codebooks [8, 43] for discrete modeling. Rectified Flow (RF). Most generative models explicitly or implicitly learn mapping from basic distribution, e.g., Gaussian distribution (0, I), to target distribution, typically the real data distribution pdata. The core idea of RF is to learn an Ordinary Differential Equation (ODE) dZt = u(Zt, t)dt that follows straight path from π0 to π1. Here u(Zt, t) is time-conditional velocity field. This could be achieved by solving least squares regression E(cid:2)(X1 X0) u(Xt, t)2(cid:3)dt, where problem: minu Xt = tX1+(1t)X0. In practice, we use ϕ to parameterize u, and is basically sampled from the uniform distribution U(0, 1). The learning objective of RF is: (cid:82) 1 0 LRF = Et,x0,x1 (cid:13) (cid:13) (cid:13)(x1 x0) uϕ (cid:0)tx1 + (1 t)x0, t(cid:1)(cid:13) 2 (cid:13) (cid:13) 2 , where U(0, 1), x0 (0, I), x1 pdata. (1) Discrete Generative Models. For discrete modeling, one should first learn discrete codebook, where images are represented by their corresponding indices. For example, VQ-GAN [8] employs some schemes [12, 55] to learn discrete codebook of perceptually rich representations. Subsequently, given indices s<i of image x, the discrete generative model pϕ learns to predict the categorical distribution of the next index si via the cross-entropy objective: LCE = Expdata log (cid:89) i= pϕ(sis<i), (2) where denotes the sequence length of sample. pϕ could be any form of model capable of modeling discrete distributions, e.g., PixelCNNs [42] and Transformers [8, 44]. Conditional Generation. To achieve conditional generation, one could incorporate the condition c, e.g., class labels or text prompts, into the parameterized model in Eq. (1) and Eq. (2) as uϕ(xt, t, c) and pϕ(sis<i, c), respectively. 4. Method 4.1. Overview and Formulation Overview. We propose two-stage post-training method, namely GenHancer, to enhance CLIP ViTs fine-grained representations, as in Fig. 3 (a). To capture key information from generative models, we delve into three aspects: First, the choice of visual tokens for condition determines the difficulty of the reconstruction task, which is crucial for enhancement (Sec. 4.2). Second, we introduce denoising configurations, which enable ViT to capture useful knowledge while mitigating irrelevant information (Sec. 4.3). Third, we present tailored design for both continuous and discrete generative models (Sec. 4.4), also shown in Fig. 3 (b), (c). 3 Figure 3. The two-stage post-training framework for visual enhancements. (a) Overall training pipeline. (b) Continuous generative model as the denoiser. We employ lightweight FLUX-like DiT [22] (but with fewer blocks) and employ regression loss of flow matching. (c) Discrete generative model as the denoiser. We choose lightweight Perceiver [17] and employ cross-entropy loss to predict masked tokens. Notations. Here, two types of generative models are uniformly represented as gϕ parameterized by ϕ. Let vθ denote CLIPs visual encoder with parameters θ, whose features are connected to gϕ as conditions through projector hω, i.e., hω vθ(x). The input sample is x, which becomes (cid:101)x in the denoising space, e.g., VAE [19] and VQ-GAN [8] for continuous and discrete denoisers, respectively. Repurposing Conditional Generation to Self-supervised Reconstruction. Generative models can capture lowlevel details. To transfer this capability to vθ, we replace the original condition with the visual feature vθ(x). By reconstructing the visual inputs, vθ learns to grasp low-level visual details and is enhanced with fine-grained representations. In this sense, we transform the original conditional generation into self-supervised reconstruction task. The learning objectives for continuous Lc and discrete generative models Ld can be re-written as Eq. (3) and Eq. (4): (cid:102)xt, t, hω vθ(x)(cid:1)(cid:13) 2 2, (cid:13) (cid:13) (cid:13)((cid:102)x1 (cid:102)x0) gϕ Lc = Et,x,(cid:101)x0,(cid:101)x1 (cid:0) where U(0, 1), (cid:102)xt = t(cid:102)x1 + (1 t)(cid:102)x0, Ld = Ex log (cid:89) i=1 (cid:0)sis<i, hω vθ(x)(cid:1). gϕ (3) (4) Here, hω vθ(x) serves as the conditional input of gϕ. Formulation. Let and denote random variables of features of gϕ and vθ. I() and H() denote mutual information and entropy. Then we have the following theorem: Theorem 1. When gϕ is fixed, self-supervised reconstruction is equivalent to maximizing the mutual information I(V ; G) between and G. The knowledge learned by vθ from gϕ can be interpreted as the increase in I(V ; G). 4 Proof. The mutual information could be written as: I(V ; G) = H(G) H(GV ). Through reconstruction in Eq. (3) or Eq. (4), by conditioning on , is trained to approximate the distribution of G. Consequently, H(GV ) decreases during training. While H(G) is fixed, the decrease in H(GV ) leads to the increase in I(V ; G). From the results in Fig. 1 (b)(i), the reconstruction improves as training progresses, which corresponds to an increasing I(V ; G). However, visual representations might decrease. In light of this, for the enhancement of visual representations, the knowledge in can be decomposed into useful knowledge G1 (e.g., basic semantics, visual patterns) and irrelevant information G2 like the gap between feature space of vθ and condition space of gϕ. In this regard, to effectively enhance visual representations, our underlying philosophy is: The visual encoder should learn to capture useful knowledge from generative models as much as possible, i.e., max I(V ; G1), while avoiding irrelevant information, i.e., min I(V ; G2). This equals to applying regularization on to prevent overfitting to G2: max I(V ; G1)λI(V ; G2) max I(V ; G1)+λd(V ; V0), (5) V0 is the initial visual model and d() is distance metric. 4.2. Conditional Visual Tokens The choice of conditional visual tokens is crucial for visual enhancement. If too many tokens are fed to the generative model, the reconstruction becomes excessively easy. The reason is that local tokens directly correspond to image areas with information leakage. In this case, I(V ; G1) in Eq. (5) becomes small and vθ fails to grasp useful information from gϕ. To ensure remarkable I(V ; G1), we argue that the number of local tokens should be carefully controlled. Our experiments show that even small number of local tokens, though achieving good reconstruction quality, can still cause marginal visual enhancement, as in Fig. 1 (iii). As result, we propose that the visual condition features should exclusively comprise only the class token [CLS]. This strategy applies to both continuous and discrete models, as validated in Fig. 5 of Sec. 5.3. 4.3. Denoising Configurations To effectively enhance visual representations, we aim to maximize I(V ; G1) while suppressing I(V ; G2) in Eq. (5). In this regard, our explorations are three-fold: training stages, timestamp sampling of the continuous denoiser, and the update strategy for vθ. Two Stage Training. An important source of G2 is the gap between the feature space of vθ and the conditions of gϕ, which is irrelevant to representation learning and could degrade the performance. Furthermore, since gϕ is lightweight and randomly initialized, it could introduce potential noise to vθ at the beginning. Consequently, we propose two-stage training pipeline. At Stage-1, we train the denoiser gϕ and the projector hω while freezing vθ, in which gϕ acquires basic generative capabilities for visual enhancements and hω learns to bridge the space gap, thereby reducing I(V ; G2). In Stage-2, we focus on enlarging I(V ; G1) and train vθ to improve fine-grained representations. Moreover, we empirically found that as long as Stage-1 is performed sufficiently, the impact of whether the denoiser and projector are trained in Stage-2 is negligible. Low Rank Adaption (LoRA) of vθ. The pre-trained visual encoder vθ possesses strong global semantics, i.e., V0, which should be maintained when incorporating finegrained perception. To prevent vθ from overfitting during reconstruction, we update vθ using LoRA [15], which implicitly constrains d(V, V0) in Eq. (5). Timestamp Sampling. For continuous models like RF, timestamp sampling is of vital importance. Conventionally, RF [29] is trained to predict velocity across timestamps uniformly in [0, 1]. Considering xt = tx1 + (1 t)x0, prior works [9] uncover that the velocity target at intermediate timestamps, i.e., 0.5, is more challenging. In our case, sampling intermediate timestamps more frequently could increase the difficulty of the reconstruction task, thus effectively amplifying I(V ; G1) and allowing the visual encoder vθ to effectively acquire useful fine-grained knowlIn this regard, we propose scaled Logitedge from G1. Normal sampling for timestamps, as shown below: = sigmoid(s ε), where ε (0, 1). (6) ε is sampled from the normal distribution, Here, sigmoid(x) = 1+exp(x) , and > 0 is the scale hyperparameter that controls the extent to which sampling is 1 focused on the intermediate timestamps. Smaller results in more frequent sampling around 0.5. The diagrams of distributions in various are illustrated in the Appendix. 4.4. Generation Paradigms For both types of generative models, we need to design architectures for denoisers and implementation of the conditioning mechanism. Notably, our denoiser is lightweight and randomly initialized, without pre-trained weights of heavy denoisers like Stable Diffusion [34] in [46]. Continuous Generative Models. We choose RF as the continuous denoiser, which is modeled in the latent space of pre-trained VAE [19]. The structure is inherited from FLUX.1-dev [22], consisting of Multimodal Diffusion Transformer (MM-DiT) [9, 32] blocks and 2n singlestream DiT (Single-DiT) blocks, as shown in Fig. 3 (b). By default, we set = 2, which is very efficient with 1/10 parameters of the original FLUX.1-dev denoiser. Similar to DiT [9, 32], the condition of visual tokens ([CLS] of vθ) is introduced through the modulation mechanism via adaptive layernorm [1, 32]. The learning objective is the regression of flow matching in Eq. (3). Discrete Generative Models. Here, we choose Perceiver [17] as the discrete denoiser, building upon off-theshelf VQ-GANs codebook [8]. We first mask certain proportion of input tokens. The condition of visual features is introduced via cross-attention module, as depicted in Fig. 3 (c). Specifically, we set the query as the unmasked tokens s<i, while the key and value are the concatenation of the unmasked tokens and [CLS] of vθ. They are collectively fed to the Perceiver with cross-entropy loss to predict the masked token indices si, as in Eq. (4). 5. Experiments 5.1. Experimental Setup Implementation Details. For continuous generative models, we choose RF, whose structure is similar to FLUX.1-dev [22], but with only 2 MM-DiT and 4 Single-DiT blocks ( 10% of the parameters). The discrete denoiser is parameterized by 6-layer Perceiver to predict the masked tokens indexed VQ-GANs codebook [8]. Similar to [49], the mask ratio is randomly sampled from 50% to 90%. For both generative models, we only take the [CLS] token of CLIP ViT as the conditional input while dropping other local tokens to prevent information leakage. We choose the scale factor in Eq. (6) as 1 by default. Training Details. Our training process consists of two stages, each involving one epoch on the CC3M [35] dataset. We choose AdamW as the optimizer, with learning rate of 1e-4 and 1e-5 for Stage-1 and Stage-2, respectively. At Stage-2, we optimize the visual encoder using LoRA with rank of 16. We employ global batch size of 256. Table 1. Performance of various CLIP backbones in MMVP-VLM benchmark. Here, we report our results using the continuous denoiser. The enhanced CLIP consistently outperforms prior methods across various visual patterns. The visual patterns are symbolized as: : Orientation and Direction, (cid:219): Presence of Specific Features, L: State and Condition, (cid:29): Quantity and Count, ,: Positional and Relational Context, h: Color and Appearance, (cid:212): Structural and Physical Characteristics, k: Texts, (cid:130): Viewpoint and Perspective. CLIP Backbone #Params (M) Resolution Method (cid:219) (cid:29) , (cid:212) (cid:130) Average OpenAI ViT-L-14 427. OpenAI ViT-L-14 427.9 MetaCLIP ViT-L-14 427.6 MetaCLIP ViT-H-14 986. SigLIP ViT-SO-14 877.4 SigLIP ViT-SO-14 878.0 2242 2242 2242 2242 3842 Original 13.3 13.3 20.0 20.0 13.3 53.3 20.0 + DIVA 13.3 20.0 40.0 + Ours 13.3 20.0 53.3 46.7 20.0 13.3 6. 6.7 13.3 33.3 33.3 20.0 6.7 73.3 46.7 20.0 40.0 31.9 (+6.0) 19.3 25.9 0.0 Original 20.0 33.3 20.0 40.0 20.0 + DIVA 26.7 20.0 33.3 13.3 13.3 46.7 26.7 20.0 33.3 20.0 + Ours 6.7 6.7 6.7 6.7 6.7 33.3 40.0 20.0 25. 73.3 53.3 26.7 26.7 29.6 (+4.4) Original 13.3 + DIVA 6.7 + Ours 6.7 6.7 66.7 60.0 6.7 0.0 33.3 46.7 20.0 13.3 26.7 66.7 20.0 20.0 40. 6.7 23.7 27.4 13.3 20.0 53.3 13.3 26.7 80.0 33.3 13.3 33.3 31.9 (+4.5) 6.7 Original 53.3 26.7 13.3 33.3 + DIVA 13.3 20.0 53.3 33.3 13.3 66.7 33.3 13.3 40.0 + Ours 20.0 20.0 66.7 26.7 26.7 66.7 33.3 20.0 53.3 37.0 (+5.1) 13.3 60.0 13.3 25.2 31.9 6.7 Original 26.7 20.0 53.3 40.0 20.0 66.7 40.0 20.0 53.3 + DIVA 13.3 26.7 60.0 46.7 13.3 73.3 53.3 26.7 53.3 + Ours 20.0 20.0 66.7 60.0 20.0 86.7 40.0 13.0 53.3 42.2 (+1.5) 37.8 40. Original 20.0 26.7 60.0 33.3 13.3 66.7 33.3 26.7 53.3 + DIVA 26.7 33.3 53.3 26.7 13.3 80.0 40.0 26.7 46.7 + Ours 26.7 20.0 66.7 33.3 13.3 86.7 40.0 26.7 46.7 40.0 (+1.5) 37.0 38.5 Comparative Baseline. Similar to [46], our method GenHancer independently enhances CLIP via post-tuning. When equipped with our enhanced CLIP and trained with original recipes, MLLMs could perform better on visioncentric benchmarks. In this regard, GenHancer could be viewed as plug-and-play vision-enhancement method for MLLMs. We primarily compare with DIVA [46]. Evaluation Protocol. Following [46], we perform visual enhancements on six CLIP backbones, including OpenAICLIP ViT-L @224/@336 [33], MetaCLIP@224 ViTL/H [50] and SigLIP-SO-14 @224/@384 [54]. We use MMVP-VLM [41] to evaluate fine-grained perception abilities. Subsequently, we follow the official training recipes of LLaVA-1.5 [28] to train MLLMs with our enhanced CLIP ViT. The resulting MLLMs are comprehensively evaluated on vision-centric benchmarks like MMVP-MLLM [41], CV-Bench [40] and NaturalBench [24], as well as multimodal understanding benchmarks, including POPE [25] ScienceQA [30] and HallusionBench [13]. 5.2. Comparative Results Our method significantly enhances CLIPs fine-grained visual perception abilities. We evaluate CLIP models on the challenging MMVP-VLM benchmark [41], which contains 9 fine-grained visual patterns for comprehensive vision-centric evaluation. As in Table 1, our method with only lightweight denoiser, surpasses the previous method [46] that employed heavy pre-trained denoiser across multiple CLIP backbones, with variations in resolution and parameters. For example, our method outperforms Figure 4. Qualitative results. Although DIVA achieves better reconstructions of input images, it fails to perceive fine-grained visual details between tongue out and without tongue out. DIVA by 6.0% and 4.5% on OpenAICLIP and MetaCLIP, respectively. Besides, CLIPs visual shortcomings are effectively addressed after post-training, e.g., we improved MetaCLIPs color perception (h) from 46.7% to 80.0%, and enhanced its viewpoint understanding ((cid:130)) by 20%. Qualitative Evaluations. We present two cases in Fig. 4. Although DIVA achieves better reconstructions, our method correctly retrieves images for given texts, while DIVA fails. This further emphasizes that better reconstruction does not necessarily lead to better representations. Plug-and-play vision-centric enhancements for MLLMs. Our method independently enhances CLIP ViT with fine-grained representations. Considering that existing MLLMs [2, 27, 28] predominantly use CLIP ViT as the visual encoder, we replace the original CLIP with the enhanced CLIP as plug-and-play module and integrate it into MLLMs to explore the impact of the enhanced visual representations on MLLMs final performance. For fair Table 2. Comprehensive evaluation of MLLMs (LLaVA-1.5 [28]), including vision-centric and conventional MLLM benchmarks. We use official DIVA CLIP checkpoints [46] to reproduce the results. Similar to [23], we select the choice with the highest likelihood as MLLMs prediction. Hallusion: HallusionBench [13]. SciQA: ScienceQA [30]. Bold and underline indicate the best and the second best. Vision-Centric Benchmarks Conventional MLLM Benchmarks LLM CLIP MMVPMLLM [41] NaturalBench [24] CV-Bench 2D [40] CV-Bench POPE [25] Acc Q-Acc I-Acc G-Acc ADE20K COCO 3D [40] Vicuna-7B Vicuna-13B Original DIVA Ours Original DIVA Ours 24.7 31.3 30.7 30.7 35.3 36.7 76.4 75.3 77. 76.3 76.0 77.2 53.6 51.7 55.6 52.9 52.7 55.3 56.4 56.1 59.1 55.1 56.0 58.7 17.6 22.3 24. 13.8 16.8 22.9 49.6 51.3 52.9 52.6 53.2 55.3 60.9 63.4 63.6 63.3 64.3 64.3 58.7 60.2 63. 65.0 65.8 66.4 rand pop adv 87.3 86.1 84.2 87.9 87.0 84.6 88.1 86.7 84.6 87.1 86.2 84.5 88.1 87.4 84.8 87.8 87.0 84.9 SciQAIMG [30] Hallusion Avg. [13] 66.8 66.3 66.5 71.6 71.8 72.3 27.6 28.6 28.4 24.5 25.2 26.4 Table 3. Performance of zero-shot classification and retrieval that require global semantics. We report the results of original and post-tuned OpenAICLIP@224. Method Classification Retrieval-Image@5 Retrieval-Text@5 IN-1K C100 SUN397 Cars Flickr30k COCO Flickr30k COCO Original 75.5 75.6 Ours 76.1 76.1 67.5 67. 77.7 77.6 87.2 87.3 61.1 61.2 97.4 97.2 79.2 79.4 comparisons, we adopt the same training setup as LLaVA1.5 [28], i.e., training data and stages, to train MLLMs. For DIVA [46], we adopt the official CLIP checkpoints. We conduct comprehensive evaluation of the MLLMs on multiple vision-centric benchmarks, including MMVPMLLM [41], CV-Bench [40] and NaturalBench [24], as well as some general multimodal understanding benchmarks. Results in Table 2 show that visual enhancement of CLIP is effectively transferred to MLLMs, resulting in significant improvements across vision-centric benchmarks. For instance, compared to the original CLIP in Vicuna7B MLLM, we achieved 6.0% and 4.5% improvements on MMVP-MLLM and CV-Bench 3D, respectively. Visual enhancements do not hurt CLIPs original global semantics. CLIP has inherently strong global semantics in classification-based tasks. To explore how finegrained enhancements affect this ability, we evaluate zeroshot classification on datasets like ImageNet-1K [6], CIFAR100 [21], Stanford Cars [20], and SUN397 [48] and zero-shot cross-modal retrieval tasks on Flick30k [52] and COCO [4]. Table 3 reveals that the performance difference is minimal (< 0.3%) across various settings, which means that our method could enhance CLIPs fine-grained understanding without forgetting its global semantics. 5.3. Key Explorations and Ablations Key Point #1: Selecting Conditional Visual Tokens. As in Sec. 4.2, selecting conditional visual tokens is critical for enhancing representations. We conduct experiments by choosing the class token and different proportions of local tokens, i.e., [CLS] + n% [LOCAL]. As displayed in Fig. 5, even very small ratio (10%) leads to signifiFigure 5. Performance of CLIP across various conditional visual tokens on MMVP-VLM, i.e., [CLS] + n% [LOCAL]. Figure 6. Comparison of CLIP with end-to-end and the proposed two-stage training on MMVP-VLM. Here, Cont. and Disc. denote continuous and discrete denoisers. O: OpenAICLIP. S: SigLIP. cant performance degradation, which suggests that local tokens carry substantial signals for reconstruction, making the task too easy with information leakage. Consequently, this prevents the visual encoder from effectively learning finegrained details and brings about limited I(V ; G1). The conclusion applies to both types of gϕ. Therefore, we propose to choose only the class token as the condition. Key Point #2.1: Two-Stage Training. As elaborated in Sec. 4.3, in Stage-1 of the two-stage training scheme, the projector learns to bridge the gap between the feature space of the visual encoder and the condition space of the denoiser, which serves as irrelevant information G2. Ablations comparing end-to-end with the proposed two-stage training are illustrated in Fig. 6. End-to-end training consistently exhibits performance drop of over 5% across various settings. This indicates that our two-stage training is crucial in preventing interference from G2. Key Point #2.2: Timestamp Sampling for Continuous Denoisers. Timestamp sampling of continuous denoisers 7 Table 4. Comparison of timestamp sampling in continuous denoisers on MMVP-VLM. O: OpenAICLIP. M: MetaCLIP. Distribution Uniform Logit-Normal Scale O@224 O@336 M@224 N/A 21.5 22.2 23.7 0.1 0.5 1.0 5.0 10.0 27.4 28.2 31.9 24.5 20. 25.9 28.9 29.6 25.9 20.0 26.7 29.6 31.9 25.9 21.5 Table 5. Performance on SigLIP@224 across different sizes of lightweight continuous and discrete denoisers. Continuous Discrete #DiT Blocks (MM+Single) 1+2 2+4 3+6 4+ MMVP-VLM 41.5 42.2 42.2 41.5 #Perceiver Layers 2 4 8 MMVP-VLM 41.5 43.7 45.2 43.7 is also pivotal for vθ to learn the fine-grained knowledge from gϕ, i.e., I(V ; G1). We compare our proposed scaled Logit-Normal sampling with standard uniform sampling, as shown in Table 4. Compared to uniform sampling, ours favors sampling closer to the middle (t = 0.5), i.e., in the middle of two distributions xt = tx1 + (1 t)x0, making denoising more challenging and more beneficial for enhancing I(V ; G1). For example, our proposed distribution outperforms uniform sampling by 10.4%, 7.4% and 8.2% on three CLIP backbones in Table 4. Additionally, when the scale is too small (e.g., = 0.1, sampling too around 0.5) or too large (e.g., = 10, sampling close to 0 or 1), the lack of diversity in can lead to suboptimal results due to the lack of diversity. In this work, we set = 1 by default. Key Point #2.3: Sizes of lightweight denoisers. We further explore the impact of the size of lightweight denoisers. For the continuous RF, we consider the number of blocks in MM-DiT and Single DiT. We consider the number of layers for Perceiver. Table 5 demonstrates that the denoiser could perform remarkably well with relatively small size, indicating the efficiency of our lightweight denoisers. Key Point #3: Continuous and Discrete Denoisers. Table 6 demonstrates the performance with continuous and discrete denoisers. Both of them surpass previous work [46] on various backbones. For example, the discrete denoiser obtains 4.5% performance gain on SigLIP@224 [54]. In summary, our method is general and applies to both continuous and discrete models. It is efficient with lightweight denoisers but strong enough to outperform prior arts [46]. Notably, previous Key Points #1#2 are consistently applicable to both continuous and discrete denoisers, further highlighting the versatility of our method. 5.4. Further Analysis Why are improvements on SigLIP relatively small? In Table 1, we observe that the improvement on SigLIP is relatively smaller compared to OpenAICLIP and MetaCLIP. 8 Table 6. Performance of our method with our continuous and discrete denoisers on MMVP-VLM (average of all visual patterns). Bold and underline indicate the best and the second best. Method OpenAI@224 SigLIP@224 SigLIP@384 DIVA Continuous Discrete 25.9 31.9 28.9 40.7 42.2 45.2 38.5 40.0 40.7 Table 7. Efficiency comparison of our lightweight RF denoiser with pre-trained FLUX.1-dev. Denoiser Efficiency MMVP-VLM #Params Memory Time/100 iters OpenAI Meta-H Pre-trained Lightweight 11.90B 37.33G 1.31B 13.07G 198.57s 20.55s 32.6 31. 37.1 37.1 Specifically, the performance gain over the original SigLIP is 3.7%, less than that for others, i.e., > 10%. Unlike the other two backbones [33, 50], SigLIP [54] does not explicitly train distinct class token. In practice, we extract the pooler output of SigLIP as the condition for the denoiser, which is obtained by aggregating all local tokens through attention and linear layers. We attribute the relatively small improvement on SigLIP to the indirect leakage of local information through the pooler output, which hinders the enhancement of I(V ; G1). This is consistent with the discussion in Sec. 4.2 and the results in Fig. 5. Efficiency analysis compared with pre-trained FLUX. We provide comparison between our lightweight RF (n = 2) and original FLUX.1-dev [22] across the following dimensions: #params of denoisers, per-device GPU memory and training time of 100 iterations. To ensure fair comparisons, we fix per-device batch size of 2. As Table 7 shows, our lightweight denoiser is much more efficient than the pre-trained heavy one. Specifically, our lightweight denoiser has approximately 1/10 of the parameters, occupies about 1/3 of the memory, and is 10 times faster in training, while the final performance remains comparable. 6. Conclusive Remarks In this paper, we delve into the underlying principles of how generative models enhance visual representations. We innovatively uncover that the perfect generation does not always yield optimal representations. The pivot is to learn useful knowledge from the generative model while mitigating irrelevant information. Our key findings lie in three aspects. (1) Conditioning mechanism. We found that local tokens could make the reconstruction task too easy, while class token alone as the condition makes the reconstruction task meaningful and significantly enhances visual representations. (2) Denoising configurations. We propose novel two-stage post-training method to enable vision encoders committed to learning fine-grained knowledge while (3) Our model design enalleviating irrelevant content. ables both continuous and discrete denoisers to effectively enhance visual representations. Vision-centric evaluations demonstrate that our method with lightweight denoisers can significantly outperform previous methods relying on heavy pre-trained generative models. We hope this work will inspire further in-depth explorations into the synergy between generative and discriminative models, as well as the relationship between generation and understanding tasks."
        },
        {
            "title": "References",
            "content": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 5 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 6 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1:8, 2024. 1 [4] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 7 [5] Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. Deconstructing denoising diffusion models for self-supervised In The Thirteenth International Conference on learning. Learning Representations, 2025. 3 [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 2 [8] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2, 3, 4, 5 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 5 [10] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 3 [11] Michael Fuest, Pingchuan Ma, Ming Gui, Johannes Schusterbauer, Vincent Tao Hu, and Bjorn Ommer. Diffusion models and representation learning: survey. arXiv preprint arXiv:2407.00783, 2024. 3 [12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [13] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual ilIn Proceedings of lusion in large vision-language models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. 3, 6, 7 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [15] Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 5 [16] Drew Hudson, Daniel Zoran, Mateusz Malinowski, Andrew Lampinen, Andrew Jaegle, James McClelland, Loic Matthey, Felix Hill, and Alexander Lerchner. Soda: Bottleneck diffusion models for representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2311523127, 2024. 2, 3 [17] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 46514664. PMLR, 2021. 4, 5 [18] Oguzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, and Federico Tombari. Brave: Broadening the visual encoding of vision-language models. In European Conference on Computer Vision, pages 113 132. Springer, 2024. [19] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 4, 5 [20] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554561, 2013. 7 [21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 7 [22] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 4, 5, 8 [23] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 7 [24] Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, 9 Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating vision-language models on natural adversarial samples. Advances in Neural Information Processing Systems, 37:1704417068, 2024. 3, 6, 7 [25] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in In Proceedings of the 2023 large vision-language models. Conference on Empirical Methods in Natural Language Processing, pages 292305, 2023. 3, 6, [26] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 1, 2, 3 [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, 6 [28] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2, 6, 7 [29] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. 2, 3, 5 [30] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 3, 6, 7 [31] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. 1 [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 3, 6, 8, 12 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 3, 5, 12 [35] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018. 5, 12 [36] Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, and Clinton Fookes. Diversity is definitely needed: Improving model-agnostic zero-shot classification via stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 769778, 2023. 3 [37] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [38] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2025. 1 [39] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic images from text-toimage models make strong visual representation learners. Advances in Neural Information Processing Systems, 36: 4838248402, 2023. 3 [40] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. 2, 3, 6, 7 [41] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 2, 3, 6, 7 [42] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. 3 [43] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 3 [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [45] Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. In The Thirteenth International Conference on Learning Representations, 2025. 2, 3, [46] Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, and Xinlong Wang. Diffusion feedback helps CLIP see better. In The Thirteenth International Conference on Learning Representations, 2025. 2, 3, 5, 6, 7, 8, 12 [47] Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, and Christoph Feichtenhofer. Diffusion models as masked autoencoders. In Proceedings of the IEEE/CVF In10 ternational Conference on Computer Vision, pages 16284 16294, 2023. 3 [48] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 34853492. IEEE, 2010. 7 [49] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In The Thirteenth International Conference on Learning Representations, 2025. [50] Hu Xu, Saining Xie, Xiaoqing Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. DemystiIn The Twelfth International Conference fying CLIP data. on Learning Representations, 2024. 1, 6, 8 [51] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and MingHsuan Yang. Diffusion models: comprehensive survey of methods and applications. ACM Computing Surveys, 56(4): 139, 2023. 1 [52] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the association for computational linguistics, 2:6778, 2014. 7 [53] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In The Thirteenth International Conference on Learning Representations, 2025. 2 [54] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 1, 6, 8 [55] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [56] Liming Zhao, Kecheng Zheng, Yun Zheng, Deli Zhao, and Jingren Zhou. Rleg: Vision-language representation learnIn Intering with diffusion-based embedding generation. national Conference on Machine Learning, pages 42247 42258. PMLR, 2023. 3 11 GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers"
        },
        {
            "title": "Overview",
            "content": "In this appendix, we provide additional descriptions of the following contents: Relationship with prior works in Appendix A, including some discussions about the differences. More training details of hyperparameters in Appendix B. Diagrams of various timestamp sampling distributions in Appendix C. Additional experimental results in Appendix D. Additional qualitative results and cases of the enhanced CLIP (Appendix E) and MLLMs with our enhanced CLIP (Appendix F). We also attach algorithms of our two-stage training with continuous and dicrete denoisers in Appendix G. A. Relationship with Prior Works In this paper, we propose two-stage post-training method to enhance discriminative models fine-grained visual representations. For discriminative models, we primarily choose CLIP [33], considering its wide range of applications. Specifically, CLIP is inherently vision-language model, capable of image-text retrieval and matching. Additionally, CLIP ViT is widely employed as visual encoder in Multimodal Large Language Models (MLLMs). Note that our approach follows post-training paradigm, where we enhance the fine-grained capabilities of pre-trained CLIP ViT, while preserving its original global semantics. Comparison with DIVA [46]. DIVA is pioneer work and proposes to enhance visual representations of CLIP ViT through diffusion feedback. It independently enhances CLIP ViTs visual representations with the guidance of pretrained stable diffusion [34]. Similar to DIVA, our work focuses on enhancing CLIP ViTs internal visual representations. The enhanced CLIP itself could be more competent vision-language model with better image-text retrieval performance. Furthermore, the enhanced CLIP ViT serves as plug-and-play module and could be seamlessly plugged into MLLMs. When using the same training recipes but with the enhanced vision encoder, MLLMs could be more capable on several vision-centric benchmarks, with better fine-grained perception of visual details and overcoming visual shortcomings brought about by the original CLIP. Different from DIVA, we delve into the underlying principles of how generative models enhance vision models from various orthogonal dimensions. Notably, we only employ lightweight denoisers without pre-trained weights of heavy generative models. Our method is efficient yet stronger than DIVA. We also provide several key insights about how to enhance visual representations, i.e., conditioning mechanisms and training configurations. We further explore the implementation of both continuous and discrete generative models. When equipped with corresponding tailor-made designs, both continuous and discrete denoisers outperform DIVA. Comparison with ROSS [45]. Ross is pioneering work that explores the intrinsic signals in vision modality and proposes to append vision-centric self-supervision into the training of MLLMs. The core difference between ROSS and our method is that, ROSS is directly oriented to training better MLLMs. In most cases, ROSS freezes CLIP ViT and enhances the vision-centric performance of MLLMs through the parameters of LLMs. In contrast, our method is directly oriented to enhance CLIP ViTs visual representations. Our method is more general, and the resulting enhanced CLIP could be plugged into various MLLMs. In summary, we independently enhance CLIP ViT, which could be merged into MLLMs for further enhancements, while ROSS directly enhances MLLMs with the ViT frozen. B. More Training Details Our training process consists of two stages, each involving one epoch on the CC3M [35] dataset. We choose AdamW as the optimizer, with learning rate of 1e-4 and 1e-5 for Stage-1 and Stage-2, respectively. At Stage-2, we optimize the visual encoder using LoRA with rank of 16. We train the model on 8 GPUs with per-device batch size of 16, and the gradient accumulation steps are set as 2, resulting in global batch size of 256. We plug LoRA to CLIP ViT, with rank of 16, and an α of 16, and we also employ dropout with ratio of 0.1 within LoRA. C. Diagrams of Timestamp Sampling The scaled Logit-Normal timestamp sampling is as follows: = sigmoid(s ε), where ε (0, 1). (7) We provide some illustrative diagrams to show the distribution of several candidate distributions, as shown in Fig. 7. In our scaled Logit-Normal sampling, as decreases, the distribution becomes more focused on sampling around the middle (t = 0.5). Conversely, as increases, the distribution becomes more biased towards sampling at the extremes, i.e., = 0 or 1. 12 Figure 7. Probability density function of different distributions. Figure 8. The effect of LoRA on several CLIP backbones. Figure 9. The performance of whether to update the denoiser and the projector in Stage-2. D. More Experimental Results The effect of LoRA. In Stage-2, we apply LoRA to the visual model. The reason is that direct training on the visual model causes rapid updates, which can easily damage the models high-level semantics and lead to overfitting. By using LoRA, the model can be trained on larger variety of samples, allowing it to learn more generalizable and finegrained representations. We conduct experiments on several CLIP backbones, and compare the performance with direct training and LoRA training, as shown in Fig. 8. The performance with LoRA for visual encoder consistently outperforms the cases of direct training. Whether to update the denoiser and projector in Stage2. In the main text, we argue that in Stage-1, the visual encoder should be fixed, and we train the denoiser and projector. In this way, the projector could learn to bridge the gap between the feature spaces, which serves as the irreleTable 8. nAICLIP@224. Performance of various mask ratios on OpeMask Ratio (%) 50 70 75 80 85 90 random (50-90) MMVP-VLM 28.1 27.4 28.9 27.4 26.7 25.9 25. 28.9 vant information G2 for visual enhancements. In Stage-2, we begin to train CLIP ViT to enhance its visual representations. We empirically found that whether the denoiser and projector are updated in Stage-2 has marginal impacts on the final results, as long as stage-1 training is sufficient. The results are shown in Fig. 9. Performance with various mask ratios. In the discrete denoiser, we apply masking mechanisms. Here, we provide experimental results across various mask ratios of OpenAICLIP@224, as shown in Table 8. 13 Figure 10. Qualitative results of CLIP on MMVP-VLM benchmark. The enhanced CLIP overcomes original visual shortcomings in finegrained details. E. Qualitative Results of CLIPs We provide further qualitative results of the original CLIP and our enhanced CLIP, as shown in Fig. 10. The enhanced CLIP overcomes original visual shortcomings in including color, quantity, structural fine-grained details, characteristics and state. F. Qualitative Results of MLLMs We provide qualitative results of LLaVA-1.5 with original CLIP ViT and our enhanced CLIP ViT, as shown in Fig 11. Our enhanced visual model could further boost MLLMs fine-grained visual perception abilities. G. Algorithms For clearer and more thorough understanding of our method, we attach the algorithm details of two-stage posttraining with continuous and discrete denoisers in Algorithm 1 and Algorithm 2, respectively. Figure 11. Qualitative results of MLLMs on MMVP-MLLM benchmark. When equipped with our enhanced CLIP, MLLMs produce better vision-centric performance. 15 Algorithm 1 Two-stage Visual Enhancements with Continuous Lightweight Denoiser Input: Lightweight and random-initialized denoiser gϕ(), with lightweight FLUX-like architecture (MM-DiT + Single-DiT). Input: Pre-trained CLIP ViT vθ() for fine-grained visual representation enhancements. Input: Random initialized projector hω() to bridge the feature space of vθ and condition space of gϕ. Input: The scale hyperparameter in the proposed scaled Logit-Normal sampling. Input: Pre-trained VAE vae() to provide latent space for generative modeling. Input: Image-only training dataset without annotations. 1: # =================================== Stage-1 ================================== 2: for in do 3: 4: 5: 6: 7: Prepare input data for generative modeling in latent space: (cid:102)x1 = vae(x) and (cid:102)x0 (0, I). Interpolating in the feature space: (cid:102)xt = t(cid:102)x1 + (1 t)(cid:102)x0. Visual encoding as conditions for denoisers: hω vθ(x). Timestamp sampling via scaled Logit-Normal distributions: Denoising regression objective (flow matching): ε (0, 1) then = sigmoid(s ε). # only update gϕ and hω. (cid:102)xt, t, hω vθ(x)(cid:1)(cid:13) (cid:0) (cid:13) (cid:13) (cid:13)((cid:102)x1 (cid:102)x0) gϕ 2 2 . 8: end for arg min ϕ,ω Et,x,(cid:101)x0,(cid:101)x 9: # =================================== Stage-2 ================================== 10: Plug LoRA upon vθ. 11: for in do 12: 13: 14: 15: 16: Prepare input data for generative modeling in latent space: (cid:102)x1 = vae(x) and (cid:102)x0 (0, I). Interpolating in the feature space: (cid:102)xt = t(cid:102)x1 + (1 t)(cid:102)x0. Visual encoding as conditions for denoisers: hω vθ(x). Timestamp sampling via scaled Logit-Normal distributions: Denoising regression objective (flow matching): ε (0, 1) then = sigmoid(s ε). Optional: gϕ and hω. # update vθ. arg min θ Et,x,(cid:101)x0,(cid:101)x1 (cid:13) (cid:13)((cid:102)x1 (cid:102)x0) gϕ (cid:102)xt, t, hω vθ(x)(cid:1)(cid:13) (cid:0) (cid:13) 2 2 . 17: end for Output: The enhanced visual model θ with stronger fine-grained representations. 16 Algorithm 2 Two-stage Visual Enhancements with Discrete Lightweight Denoiser Input: Lightweight and random-initialized denoiser gϕ(), instantiated with lightweight Perceiver. Input: Pre-trained CLIP ViT vθ() for fine-grained visual representation enhancements. Input: Random initialized projector hω() to bridge the feature space of vθ and condition space of gϕ. Input: Mask ratio for discrete modeling. Input: Pre-trained VQ-GAN vq-gan() to discrete indices for generative modeling. Input: Image-only training dataset without annotations. 1: # =================================== Stage-1 ================================== 2: for in do 3: 4: 5: Obtain latent embeddings and corresponding discrete indices of input data in VQ-GANs codebook: Masking xs tokens with ratio to obtain masked part (cid:101)xmask, smask and unmasked part (cid:101)xunmask, sunmask. Visual encoding and obtain conditions via cross-attention for denoisers: (cid:101)x, = vq-gan(x). = (cid:101)xunmask, K, = concat(cid:0) cω,θ = cross-attn(Q, K, ). (cid:101)xunmask; hω vθ(x)(cid:1), 6: Denoising cross-entropy objective (masked index prediction): # only update gϕ and hω. 7: end for arg min ϕ,ω Ex log (cid:89) i=1 (cid:0)smasksunmask, cω,θ (cid:1). gϕ 8: # =================================== Stage-2 ================================== 9: Plug LoRA upon vθ. 10: for in do 11: 12: 13: Obtain latent embeddings and corresponding discrete indices of input data in VQ-GANs codebook: Masking xs tokens with ratio to obtain masked part (cid:101)xmask, smask and unmasked part (cid:101)xunmask, sunmask. Visual encoding and obtain conditions via cross-attention for denoisers: (cid:101)x, = vq-gan(x). = (cid:101)xunmask, K, = concat(cid:0) cω,θ = cross-attn(Q, K, ). (cid:101)xunmask; hω vθ(x)(cid:1), 14: Denoising cross-entropy objective (masked index prediction): # update vθ. Optional: gϕ and hω. arg min θ Ex log (cid:89) i=1 (cid:0)smasksunmask, cω,θ (cid:1). gϕ 15: end for Output: The enhanced visual model θ with stronger fine-grained representations."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "Institute of Automation, CAS"
    ]
}