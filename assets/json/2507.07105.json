{
    "paper_title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
    "authors": [
        "Yushen Zuo",
        "Qi Zheng",
        "Mingyang Wu",
        "Xinrui Jiang",
        "Renjie Li",
        "Jian Wang",
        "Yide Zhang",
        "Gengchen Mai",
        "Lihong V. Wang",
        "James Zou",
        "Xiaoyu Wang",
        "Ming-Hsuan Yang",
        "Zhengzhong Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: https://4kagent.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 5 0 1 7 0 . 7 0 5 2 : r Agentic Any Image to 4K Super-Resolution 4KAgent: Yushen Zuo1, Qi Zheng1, Mingyang Wu1, Xinrui Jiang2, Renjie Li1, Jian Wang3, Yide Zhang4, Gengchen Mai5, Lihong V. Wang6, James Zou2, Xiaoyu Wang7, Ming-Hsuan Yang8, Zhengzhong Tu1 1Texas A&M University 2Stanford University 3Snap Inc. 6California Institute of Technology 7Topaz Labs 5UT Austin 4CU Boulder 8UC Merced Corresponding Author: tzz@tamu.edu. Equal contributions. Project Website: 4kagent.github.io Figure 1: We present 4KAgent, an agentic image super-resolution generalist designed to universally upscale any image to 4K, regardless of input type, degradation level, or domain. That is, 4KAgent effectively restores diverse imagery, spanning from natural scenes, severely degraded captures (e.g., old photos), human/pet portraits, AI-generated content (AIGC), as well as specialized scientific imaging domains, such as remote sensing, fluorescence microscopy, pathology, and various medical modalities like X-ray, ultrasound, and funduscopyall without the need for any re-training or domain-specific adaptation. Preprint. Under review."
        },
        {
            "title": "Abstract",
            "content": "We present 4KAgent, unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256 256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, module that customizes the 4KAgent pipeline based on bespoke use cases; (2) Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make tailored restoration plan; and (3) Restoration Agent, which executes the plan, following recursive execution-reflection paradigm, guided by quality-driven mixture-ofexpert policy to select the optimal output for each step. Additionally, 4KAgent embeds specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing total of 26 diverse benchmarks, setting new state-of-the-art on broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: https://4kagent.github.io."
        },
        {
            "title": "2.2 Perception Agent",
            "content": "."
        },
        {
            "title": "2.3 Restoration Agent .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.5 Profile Module",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.1 Profile .",
            "content": ". . ."
        },
        {
            "title": "3.3 Prompts .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 5 5 6 7 8 8 9 9 3.4 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.3 Multiple-Degradation Image Restoration . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "11 11 11 14"
        },
        {
            "title": "5.4 Face Restoration .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6.1 Large Scale Factor (16×) Image Super-Resolution . .",
            "content": ". . . . . . . . . . . . . . . . 6.2 Joint Restoration & 4K Upscaling . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "10.1 Applications .",
            "content": ". ."
        },
        {
            "title": "10.2 Broader Impacts .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "11.3 LLM Agents .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "12 Concluding Remarks",
            "content": "18 18 20 21 22 24 30 32 35 38 40 43 43 44 44 45 45"
        },
        {
            "title": "Introduction",
            "content": "Image super-resolution (SR) is fundamental task in computer vision that aims to reconstruct highresolution (HR) images from their low-resolution (LR) counterparts [45, 46, 96, 166, 192, 197, 198, 203, 227, 262]. It serves as bedrock for various low-level vision tasks [117, 183, 241, 252], including deblurring [33, 179], dehazing [63, 104], deraining [78, 163], and low-light enhancement [59, 207]. Beyond its classical role in computational photography and imaging, SR techniques significantly influence numerous domains, such as biomedical imaging [55, 167], remote sensing [64, 91, 169], surveillance [254], and embodied artificial intelligence applications [60, 74, 168]. Traditional SR methods [45, 198] typically assume known synthetic degradation during training, which limits their generalization to real-world captures that suffer from complex, heterogeneous, and unpredictable degradations [197]. Recent research has increasingly shifted to more practical real-world super-resolution (RealSR) task [21, 211], which attempts to explicitly address diverse and unknown degradations in naturally captured photoand video-graphs. RealSR requires models not only to handle multiple combined degradations effectively but also to exhibit strong adaptability and generalization across varied scenarios [184, 233]. Many effective solutions have been proposed to solve the RealSR problem, via simulating complex real-world degradations [197, 251], leveraging the powerful generative prior of pre-trained diffusion models [80, 142, 176, 211, 212], enabling robust restoration under unknown conditions. Inspired by the advanced planning and reasoning capabilities of large language models (LLMs) [48, 71, 208, 232], agentic restoration frameworks [22, 269] have emerged as powerful tool that can adaptively handle multiple degradations through sequential planning and dynamic restoration strategies. Despite their successes in certain scenarios, existing performant generative approaches [176, 211] can only handle limited degradation ranges, e.g., up to 4 upscaling, failing to recover extremely low-quality images with highly complex and diverse degradations in the wild. Moreover, SR specialist models are known to generalize poorly to out-of-distribution domains [23], let alone when applied on different scaling factor. This is mainly due to heavy reliance on supervised learning on synthetic image pairs that cannot fully simulate the complex real-world image degradations, not to mention other domains, ranging from AI-generated imagery, scientific computing, to biomedical images. Last but not least, practically, users often demand highly specific workflows, e.g., only denoising, 4K upscaling, or prioritizing high fidelity over perceptual quality, and one-size-fits-all system that can flexibly adapt to satisfy diverse requirements and application scenarios is in pressing need. To fill this gap, we present 4KAgent, the first-of-its-kind agentic framework for generic, flexible, and interpretable super-resolution of any image to 4K. As illustrated in Fig. 1, 4KAgent is capable of upscaling any low resolution image (e.g., 0.065 megapixels) to 4K4K, (i.e., 16.7 megapixels) by 16 upscaling factors1 (6.1). It also sets new state-of-the-art (SoTA) on classical image super-resolution (5.1), real-world image super-resolution (5.2), face restoration (5.4), and multiple-degradation image restoration (5.3) benchmarks, in terms of perceptual quality. We also show that 4KAgent enjoys broader applications in broad low-level tasks, such as joint restoration & 4K upscaling (6.2), and AI-generated content 4K upscaling (7.1). Lastly, thanks to the mixture-of-experts and profile design, 4KAgent demonstrates larger impact on interdisciplinary areas such as scientific superresolution (6), including ❶ Satellite image super-resolution (8.1), ❷ fluorescence microscopy super-resolution (8.2), and ❸ medical image super-resolution ((8.3,8.4)). Our contributions are as follows: [Framework] We present 4KAgent, the first AI agent framework for universal any-image-to4K upscaling, capable of handling all image categories, ranging from classical and realistic degradations, extreme low-quality inputs, AI-generated imagery, to scientific imaging tasks such as remote sensing, microscopy, and biomedical inputs. [System Design] We design multi-agent system in 4KAgent, the Perception Agent employs large vision-language models (VLMs) to analyze the content and distortion in the image and provide the restoration plan for the restoration agent to execute. The Restoration Agent, which sets up an executionreflectionrollback procedure for recursive restoration and upscaling. [Q-MoE & Face Restoration pipeline] In each restoration step of the restoration plan, we propose Quality-Driven Mixture-of-Expert (Q-MoE) policy in execution and reflection to select the optimal image. We further develop face restoration pipeline to enhance faces in images. 14KAgent can actually achieve arbitrarily large-scale super-resolution if applied recursively [166]. 4 Figure 2: 4KAgent system overview. [Profile Module] To expand the applicability of 4KAgent, we propose Profile Module to bring the availability to customize the system for different restoration tasks. 4KAgent can adapt to different restoration tasks without extra training. [DIV4K-50 Dataset] To evaluate 4K super-resolution performances, we build the DIV4K-50 dataset as challenging testset to upscale low-quality (LQ) image in 256 256 resolution with multiple degradations to high-quality (HQ) 4K image in 4096 4096 resolution. [Experiments] Extensive experimental results demonstrate the superiority of 4KAgent as generalist 4K upscaling agent: 4KAgent sets new state-of-the-art on variety of real-world image super-resolution benchmarks, multiple-degradation restoration benchmarks, face restoration, 4K upscaling task, and various scientific imaging tasks, including satellite image super-resolution, fluorescence microscopic imaging, X-ray radiography, and bio-medical imaging super-resolution."
        },
        {
            "title": "2.1 System Overview",
            "content": "We introduce 4KAgent, multi-agent framework designed to upscale any real-world image to 4K resolution. Fig. 2 illustrates the overall workflow of our proposed 4KAgent, which decomposes the restoration pipeline into collection of specialized agents. The Perception Agent analyzes degradations (noise, blur, etc.), extracts semantic/structural cues, and schedules restoration plan containing sequence of operators (denoising, deblurring, super-resolution, etc.). The Restoration Agent follows the restoration plan using our proposed Quality-driven Mixture-of-Experts (Q-MoE) to pick the best output from multiple restoration tools. The Rollback mechanism will be activated if the quality of the restored image falls below threshold. Additionally, dedicated Face Restoration Pipeline further enhances facial regions by triggering expert face restoration models. user-configurable Profile Module allows users to customize the system (e.g., prioritize fidelity or perceptual quality), enabling robust, high-quality 4K SR across diverse content and degradation types."
        },
        {
            "title": "2.2 Perception Agent",
            "content": "The Perception Agent is designed as four-stage analytical module that bridges low-level image quality assessment with high-level reasoning. Its core function is to extract robust, holistic understanding of the input image in terms of both semantic content and low-level degradations, and to create restoration plan that will guide the subsequent restoration process. 5 Image Analyzer. Perception agent invokes suite of expert Image Quality Assessment (IQA) tools that evaluate the input image across multiple quality dimensions QI = (Q1, Q2, ...). Specifically, we adopt CLIPIQA [191], TOPIQ [20], MUSIQ [82], and NIQE [255] as the IQA metrics. These metrics represent perceptual quality from diverse aspects (due to their different model designs and training data), which will be employed as context for the next step of degradation reasoning. Degradation Reasoning. Perception agent leverages VLM MR to reason over the obtained IQA metrics. Specifically, by incroporating the input image I, IQA metrics QI , the VLM MR will predict the degradation list DI from the input image, which will correspond to an initial restoration agenda . Meanwhile, MD also analyzes the content in the image and outputs the corresponding image descriptions CI (i.e., captioning). The whole process can be express as CI , DI , = MR(I, QI ). Upscaling Factor Configuration. 4KAgent is able to automatically determine and apply an appropriate super-resolution scale to reach 4K resolution. Given an input image with height HI and width WI , the scale factor is calculated by (cid:16) = min { {2, 4, 8, 16} max(HI , WI ) 4000} {16} (cid:17) , (1) After obtaining the initialized agenda from MD, 4KAgent will calculate the scale factor and append super-resolution task(s) (e.g., when = 16, append two super-resolution (4) task) into to obtain the final agenda AI and update DI correspondingly. Under this setting, 4KAgent is able to upscale any image (resolution larger than 250 250) to 4K resolution in single process. Task Planning. After obtaining the degradation list DI present in the input image and the restoration agenda AI , the perception agent employs an LLM / VLM MP to provide the restoration plan. Specifically, by coporating image descriptions CI , degradation list DI , restoration experience E, and input image itself (available when using VLM as MP ), MP outputs an initial restoration plan PI = MP (CI , DI , AI , E, I), which contains sequence of restoration tasks."
        },
        {
            "title": "2.3 Restoration Agent",
            "content": "Building upon the task plan PI provided by the Perception Agent, the Restoration Agent executes an iterative process, each stage of which tightly couples restoration and evaluation using an executionreflection-rollback triplet. Within this agent, we propose quality-driven mixture-of-experts (Q-MoE) policy, both in execution and reflection, to select the optimal image for each restoration step. We also employ rollback mechanism to adjust the restoration plan if necessary. Execution. Guided by the task plan PI , this agent executes the restoration step by step. In each restoration step, the input image will go through all tools in the toolbox, which contains number of advanced restoration models (detailed in Sec. 3.2) corresponding to each individual restoration task. In 4KAgent, we have curated 9 different restoration tasks that are useful to enhance picture quality: Brightening, Defocus Deblurring, Motion Deblurring, Dehazing, Denoising, Deraining, JPEG Compression Artifact Removal, Super Resolution, and Face Restoration. Specifically, for step in the restoration plan, it produces multiple restoration results {Ti(Ik1), = 1 } (Ti is i-th tool in the toolbox, is the number of tools in the toolbox) based on the input image Ik1. Reflection. After obtaining restoration results {Ti(Ik1), = 1 }, the Restoration Agent will select the optimal image based on their quality. To evaluate the quality of image Ti(Ik1), we compute the image quality score by combining the preference model HPSv2 [214] and no-reference IQA metrics. Specifically, we use HPSv2 to assess the human preference of the resulting image Ti(Ik1) based on the image content description CI . For no-reference IQA metrics, we employ NIQE [146], MANIQA [228], MUSIQ [82], and CLIPIQA [191] to calculate weighted sum as its no-reference quality score: Qs(Ti(Ik1)) = H(Ti(Ik1, CI )) + Qnr(Ti(Ik1))/4, where denotes HPSv2, Qnr(Ti(Ik1)) = wNIQE (1 QNIQE/10) + (cid:88) jΩ wj Qj. (2) (3) Ω = {MUSIQ, MANIQA, CLIPIQA}. After obtaining the quality score of each result image, the final result of this restoration step is obtained by the highest quality score: Ik = arg max Qs (T1(Ik1), T2(Ik1), ..., TN (Ik1)). (4) 6 The combination of execution and reflection can be viewed as Mixture-of-Expert (MoE) system, which we refer to as quality-driven mixture-of-expert (Q-MoE): the input image is processed through each expert (execution), and the Reflection function selects the optimal image among all. Rollback. Following previous AI Agent systems [69, 111, 158, 263, 269], we also design rollback mechanism in the 4KAgent system. Specifically, if the quality score of Ik after step in the initial restoration plan PI is lower than threshold η, i.e., Qs(Ik) η, the restoration step will be seen as failure step and 4KAgent will generate failure message SI . Then the system will employ the Perception Agent to adjust the subsequent plan based on the degradation list DI , the remaining restoration tasks AR of the restoration agenda AI , restoration experience E, and failure message FI : adj = MP (DI , AR , E, SI ). After that, the system will assign different restoration task in this step. If all subsequent restoration tasks assigned to this step lead to rollback, then 4KAgent will take compromise policy and go back to the original plan to execute subsequent restoration tasks."
        },
        {
            "title": "2.4 Face Restoration Pipeline",
            "content": "Human face regions are often the most visually sensitive and semantically important components in an image. However, conventional super-resolution methods struggle to maintain identity consistency, natural skin textures, and perceptual quality when restoring faces, especially in heavily degraded portraits. To address this, 4KAgent incorporates dedicated Face Restoration pipeline, which is selectively triggered within the restoration workflow. The Face Restoration pipeline is embedded as submodule in 4KAgent and will only be invoked after the super-resolution restoration step, ensuring that face quality refinement is seamlessly integrated into the iterative restoration loop. The overall framework of the face restore pipeline in the 4KAgent system is shown in Fig. 3. First, 4KAgent will detect and crop faces in the input image {F , = 1 L} (L is the number of faces in the image I). Then, if super-resolution is in the restoration plan and the resulting image Ik of super-resolution step does not trigger the rollback mechanism, 4KAgent will detect and crop faces in the resulting image {F , = 1 L} (L is Ik the number of faces in the image Ik). If = L, then for each face in Ik, different advanced face restoration methods are applied, yielding restored faces {T 0 is an identical function, and is the number of face restoration tools in the toolbox. is face restoration tool in the toolbox, Figure 3: Face restoration pipeline overview. ), = 0 }. Here, i (F Ik Likewise, we also conduct Q-MoE policy here: 4KAgent selects the best face based on the quality score Qf not only considers the face quality, but also the identity preservation: (F Ik . The quality score Qf (T ) + wIQA (Qnr(T )) = wIP IP(T ))/4 + QCF(T (F Ik (F Ik (F Ik ), ))), Qf (5) where = 1 L. IP calculates the cosine similarity of face features, extracted using ArcFace [42]. CF indicates CLIB-FIQA [154], which is an advanced face IQA metric. 4KAgent combines the no-reference quality score used in the reflection stage and the CLIB-FIQA score to assess the face quality. After obtaining quality score Qf , 4KAgent select the best face (T 1 (F Ik ), ..., Nf 0 (F Ik (F Ik ), )). out:"
        },
        {
            "title": "F l",
            "content": "out = arg max"
        },
        {
            "title": "Qf\ns",
            "content": "(6) 4KAgent will paste out back to the original image Ik, then proceeding to the next step."
        },
        {
            "title": "2.5 Profile Module",
            "content": "To enhance the flexibility and applicability of our 4KAgent system, we develop the Profile Module, enabling dynamic customization for diverse image restoration scenarios, per users needs. Specifically, the Profile Module acts like system prompt for LLM applications, allowing fine-grained control through the following seven configuration parameters: 7 1. Perception Agent: Specifies the choice of LLM / VLM employed by the Perception Agent. [Default: Llama-vision] 2. Upscale to 4K: Determines whether to upscale to 4K resolution. [Default: True] 3. Scale Factor: Explicitly defines the upscale factor for the entire pipeline. (Default: 4, Options: [2,4,8,16]). This parameter overrides Upscale to 4K when specified. 4. Restore Option: Explicitly sets the restoration task(s) to be applied. If set to None, restoration task(s) are determined automatically by the Perception Agent. (Default: None) 5. Face Restore: Toggles activation of the dedicated face restore pipeline. (Default: True) 6. Brightening: Controls the activation of image brightening, which may cause color shifts in restored images. Provided as [Optional] to maintain image color fidelity. (Default: False) 7. Restore Preference: Defines whether to prioritize higher perceptual quality or higher fidelity in image restoration. (Options: [Perception, Fidelity], Default: Perception). Here we respect the perception-distortion tradeoff [14, 265], deeming models that optimize for distortion metrics (e.g., PSNR, SSIM [204]) as Fidelity models while methods trained for perceptual quality (e.g., NIQE [146], MUSIQ [82]) as Perception models. The Profile Module offers exceptional configurability, enabling seamless adaptation to wide range of restoration tasks without requiring model retraining or domain-specific fine-tuning. To the best of our knowledge, 4KAgent is first-of-its-kind framework that enjoys unprecedented robustness and generalizability: each distinct restoration scenario can be addressed by simply selecting an appropriate configuration profile, thanks to which 4KAgent consistently achieves excellent performance across variety of challenging restoration domainsincluding natural images, portraits, AI-generated images (AIGC), medical imaging, scientific microscopy, and remote sensing imagesall without additional training or adaptation. Comprehensive details on predefined profiles in 4KAgent and their naming conventions are further elaborated in Sec. 3."
        },
        {
            "title": "3.1 Profile",
            "content": "4KAgent is highly flexible based on the profile setting. Users can easily customize 4KAgent by pre-selecting profile defined in 4KAgent. We pre-define profile examples in Tab. 1, which cover most use cases and include all the modes used in our experiments. This feature offers easy and intuitive customization for new, unseen scenarios identified by the customers."
        },
        {
            "title": "Profile Nickname Perception Agent",
            "content": "Upscale to 4K Scale Factor Restore Option Face Restore Brightening Restore Preference Table 1: Pre-defined Profiles in 4KAgent. Gen4K-P DepictQA [236] Gen4K-F DepictQA [236]"
        },
        {
            "title": "True",
            "content": "Aer4K-P Llama-3.2-Vision [6] True Aer4K-F Llama-3.2-Vision [6] True ExpSR-s4-P Llama-3.2-Vision [6] False ExpSR-s4-F Llama-3.2-Vision [6] False ExpSR-s2-F Llama-3.2-Vision [6] False ExpSR-s8-F Llama-3.2-Vision [6] False GenSR-s4-P DepictQA [236] GenMIR-P DepictQA [236]"
        },
        {
            "title": "False",
            "content": "ExpSRFR-s4-P Llama-3.2-Vision [6] False GenSRFR-s4-P DepictQA [236]"
        },
        {
            "title": "False",
            "content": "4 4 2 8 4 4 4 super-resolution False super-resolution False super-resolution False super-resolution False"
        },
        {
            "title": "False",
            "content": "super-resolution True"
        },
        {
            "title": "Perception",
            "content": "Profile naming convention: We combine restoration type, restoration task, and restoration preference to construct the profile name. For example, Gen indicates General image, 4K indicates Upscale to 4K on, and indicates to restore the image with high Perceptual quality. Aer indicates Aerial image, Exp corresponds to Explicit setting, indicating that the profile has explicitly set the restoration 8 task (e.g., SR, which indicates Super-Resolution). MIR indicates Multiple Image Restoration. FR indicates Face Restoration. s4 indicates to upscale the image by scale factor of 4. 4KAgent supports various VLMs or LLMs in the Perception Agent, enabling effective analysis of image content and degradation. Specifically, users can select either DepictQA [236] or Llama-3.2Vision (11B) [6] as available options, but can also be extended to other more recent VLM models, e.g., Qwen2.5-VL [10]. For the VLMs or LLMs to schedule the restoration plan, users can choose from GPT-4 [4], or Llama-3.2-Vision. This is configured by the Perception Agent in the profile module. For example, when it is set to Llama-3.2-Vision, the Llama-3.2-Vision model serves as the core engine to perceive image content and degradation, and then schedules the restoration plan PI . As DepictQA is fine-tuned for image quality assessment (IQA), when it is set as the VLM in the perception agent, 4KAgent will use Llama-3.2-Vision to obtain the image description CI and use GPT-4 [4] to schedule the restoration plan."
        },
        {
            "title": "3.2 Model Zoo",
            "content": "The 4KAgent system supports nine distinct image restoration models in the toolbox: ❶ Brightening, ❷ Defocus Deblurring, ❸ Motion Deblurring, ❹ Dehazing, ❺ Denoising, ❻ Deraining, ❼ JPEG Compression Artifact Removal (JPEG CAR), ❽ Super Resolution, and ❾ Face Restoration. For each of these tasks, we integrate advanced state-of-the-art methods into our comprehensive restoration toolbox. Detailed correspondences between restoration tasks and their respective methods are presented below, where QF denotes the Quality Factor and BQF indicates methods that are blind to the Quality Factor in the JPEG CAR task."
        },
        {
            "title": "Super Resolution",
            "content": "CLAHE [271] Constant Shift (C=40) DiffPlugin [129] FourierDiff [134] Gamma Correction (γ = 2/3) MAXIM [183]"
        },
        {
            "title": "Denoising",
            "content": "MAXIM [183] MPRNet [243] NAFNet [25] Restormer [241] X-Restormer [28] SwinIR [117] ConvIR [39] DiffPlugin [129] DRBNet [165] IFAN [98] LaKDNet [164] Restormer [241]"
        },
        {
            "title": "Motion Deblurring",
            "content": "EVSSM [88] LaKDNet [164] MAXIM [183] MPRNet [243] NAFNet [25] Restormer [241] X-Restormer [28] DehazeFormer [172] DiffPlugin [129] MAXIM [183] RIDCP [213] X-Restormer [28]"
        },
        {
            "title": "Deraining",
            "content": "DiffPlugin [129] MAXIM [183] MPRNet [243] Restormer [241] X-Restormer [28]"
        },
        {
            "title": "Face Restoration",
            "content": "GFPGAN [196] CodeFormer [266] DifFace [239] DiffBIR [121] DRCT [68] HAT-L [29] HAT-GAN [29] HMA [34] OSEDiff [211] PiSA-SR [176] SwinIR [117] SwinIR (Real-ISR) [117] SwinFIR [248] X-Restormer [28]"
        },
        {
            "title": "JPEG CAR",
            "content": "FBCNN [77] (QF=5) FBCNN [77] (QF=90) FBCNN [77] (BQF) SwinIR [117] (QF=40) As previously mentioned in Sec. 2.5, users can tailor the model by adjusting the Restore Preference setting, which prioritizes either perceptual quality or fidelity. We achieve this by partitioning our toolbox methods into perception-oriented and fidelity-oriented categories. For example, the SuperResolution tools in the toolbox are split into: 1. Fidelity-based: HAT-L [29], X-Restormer [28], SwinFIR [248], HMA [34], DRCT [68] 2. Perception-based: DiffBIR [121], HAT-GAN [29], OSEDiff [211], PiSA-SR [176], SwinIR (Real-ISR) [117] Therefore, when Restore Preference is set to Perception, 4KAgent will only use the Perceptionbased methods to restore the image, efficiently meeting the users restoration requirements. We develop Fast4K mode for 4KAgent. Specifically, when the size of the input image at the current step of the restoration plan exceeds predefined threshold st, 4KAgent automatically excludes methods with long inference times from the toolbox, such as DiffBIR (a 50-step diffusion-based method) in the super-resolution toolbox. Users can adjust st to control the running time of 4KAgent."
        },
        {
            "title": "3.3 Prompts",
            "content": "In 4KAgent, we enable the VLM / LLM to perceive image degradations and formulate restoration plan via customized system prompts. In this section, we present the details of these prompts in 9 4KAgent. When the Perception Agent in the profile module selects DepictQA, we use the same prompt as in AgenticIR [269] for DepictQA to assess the image degradations, and GPT-4 to construct the restoration plan. When setting Llama-3.2-Vision in Perception Agent, we design tailored prompts for degradation reasoning and planning, as shown below, where {} represents slots to fill according to the context, and the content inside comes from external input. For the restoration experience in 4KAgent, we employ the restoration experience from AgenticIR. Prompt for Llama-Vision in Degradation Reasoning You are an expert tasked with image quality assessment (IQA) and well-versed in popular IQA metrics, including CLIPIQA+, TOPIQ_NR, MUSIQ, and NIQE. Note that for NIQE, lower score indicates better image quality, whereas for the other metrics, higher scores generally reflect better quality. Heres an image to restore, along with its corresponding quality scores evaluated using the aforementioned IQA metrics. First, please describe the content and style of the input image, the description must not contain its image quality. the metric scores and your prior visual knowledge, with respect to the following two degradations: noise, motion blur, defocus blur, Images may suffer from one haze, rain, jpeg compression artifact. or more of these degradations. **Do not output any explanations or comments.** **Strictly return only JSON object** containing degradation types and image content/style description. the JSON object should be: Information about the input image: ({iqa_result} corresponds to QI in Sec. 2.2.) The keys in degradations and image_description. Second, please assess the image based on both IQA metrics: {iqa_result}. Prompt for Llama-Vision in Planning (Rollback) The key in the JSON object should be:"
        },
        {
            "title": "The final output should be",
            "content": "Information about the input image: Your goal is to make plan (the order of You are an expert in image restoration. Given an image of low quality, your task is to guide the user to utilize various tools to enhance its quality. The input image requires list of restoration tasks. the tasks) based on the task list. formatted as JSON object containing the restoration plan (the correct order of the tasks). plan. Its description is: {image_description} (CI ), It suffer from degradations {degradations} (DI ), The list of restoration tasks: {tasks} (AI / AR For your information, based on past trials, we have the following experience in making restoration plan: {experience} (E). on this experience, please give the correct order of the tasks in The restoration plan must be permutation the restoration plan. of {tasks} in the order you determine. now, we found the result is unsatisfactory if {failed_tries} (SI ) is conducted first. first place.) **Do not output any explanations or comments.** **Strictly return only JSON object** containing plan. in the JSON object should be: Remember not to arrange {failed_tries} in the (Besides, in attempts just"
        },
        {
            "title": "The keys",
            "content": "plan."
        },
        {
            "title": "Based",
            "content": "I ), 3."
        },
        {
            "title": "Implementation Details",
            "content": "Computing Resource. As multi-agent system, 4KAgent supports multi-GPU deployment. Specifically, 4KAgent assigns different agents (Perception Agent, Restoration Agent) on different GPUs to conserve memory. Most of our experiments were conducted using two NVIDIA RTX 4090 GPUs. Hyper-parameters. Hyperparameters in 4KAgent reside in the Restoration Agent, namely the weights used to compute the execution quality scores Qs and Qf in execution, as well as the quality threshold η in rollback. Specifically, in 4KAgent, we set wNIQE = 1.0, wMUSIQ = 0.01, wMANIQA = 1.0, wCLIPIQA = 1.0 for Qs, wIP = 0.001, wIQA = 1.0 for Qf , and η = 0.5 for rollback."
        },
        {
            "title": "4 Experiment Overview",
            "content": "We evaluate 4KAgent on variety of complex degradation and super-resolution (SR) tasks, demonstrate its flexible profile-driven modes for different restoration requirements, validate its generalization to multiple image domains, and quantify the contributions of each core component via ablation studies. Specifically, we test 4KAgent on wide range of 11 image SR tasks on 26 benchmarks. The summary of datasets used in experiments is shown in Tab. 2, which can be classified as natural degraded images (5,6), AI-generated images (7), and scientific images (8). Then, we perform an ablation study on Q-MoE policy and Face restoration pipeline in 4KAgent with runtime analysis (9). First, we evaluate 4KAgent on natural image restoration / super-resolution tasks under general settings, including classical image super-resolution (4) (5.1), real-world image super-resolution (4) (5.2), multiple-degradation image restoration (5.3), and face restoration (4) (5.4). Next, we assess its performance in more challenging scenarios, such as large scale factor super-resolution (16) (6.1) and joint restoration with 4K upscaling (6.2). Finally, we extend 4KAgent to diverse domains by testing its capabilities on AIGC images (7.1) and scientific imagery (8), including remote sensing (8.1), microscopy (8.2), pathology (8.3), and medical images (8.4). To comprehensively evaluate the performance of 4KAgent, we disable the Fast4K mode in all our experiments."
        },
        {
            "title": "5.1 Classical Image Super-Resolution",
            "content": "Settings. In this experiment, we follow the standard super-resolution (SR) evaluation protocol [117, 261], and assess the performance of 4KAgent on widely-used benchmark datasets, including Set5 [13], Set14 [244], B100 [137], Urban100 [70], and Manga109 [139]. In addition to PSNR and SSIM [204], we adopt range of quality metrics for more comprehensive evaluation, including LPIPS [257], DISTS [43], FID [66], NIQE [146], CLIPIQA [191], MUSIQ [82], and MANIQA-pipal [228]. Specifically, PSNR and SSIM are computed on the channel in the YCbCr space. They are used to measure the fidelity of images. LPIPS and DISTS are computed in the RGB space, are used to measure the perceptual quality of images. FID is used to evaluate the distance of distributions between the ground truth and the restored images. NIQE, CLIPIQA, MUSIQ, and MANIQA-pipal are used to evaluate the perceptual quality of images without reference images. Thanks to the high flexibility of our 4KAgent, which is governed by configurable profiles, we customize the 4KAgent in this experiment using three specific profiles: ExpSR-s4-F, ExpSR-s4-P, and GenSR-s4-P. For comparison, we employ state-of-theart fidelity-based methods (e.g., SwinIR [117], X-Restormer [28], HAT-L [29]) and perception-based methods (e.g., DiffBIR [121], OSEDiff [211], PiSA-SR [176]). In addition, we include AgenticIR [269] in this experiment for agentic system comparison. Table 2: Dataset summary in 4KAgent experiments. Task Dataset #Test Images Classical SR (5.1) Real-World SR (5.2) Set5 [13] Set14 [244] B100 [137] Urban100 [70] Manga109 [139] RealSR [17] DrealSR [209] Multiple-Degradation IR (5.3) MiO-Group [269] MiO-Group [269] MiO-Group [269] Face Restoration (5.4) WebPhoto-Test [196] Large Scale Factor SR (6.1) RealSRSet [251] Joint Image Restoration + DIV4K-50 (Ours) 4K Upscaling (6.2) AI-Generated GenAIBench-4K [101] Content 4K SR (7.1) DiffusionDB-4K [206] Remote Sensing SR (8.1) DIOR [109] AID [218] DOTA [217] WorldStrat [37]"
        },
        {
            "title": "Fluorescence Microscopy",
            "content": "SR-CACO-2 [12] Image SR (8.2) Pathology Image SR (8.3) bcSR [76] Chest X-ray 2017 [84] Chest X-ray 14 [195] Medical Image SR (8.4) US-Case [173] MMUS1K [152] DRIVE [174] 11 14 100 100 109 93 640 400 400 407 20 100 100 135 154 183 300 200 624 880 111 20 Quantitative Comparison It should be noted that, once the user sets the Restore Option to super-resolution in the profile, the 4KAgent system can be seen as quality-driven Mixture-ofExpert system for image super-resolution. In this mode, the system sequentially invokes every super-resolution tool in its toolbox based on the Restore Option setting in the profile, then selects the best result based on the quality score Qs. Accordingly, we group 4KAgent with ExpSR-s4-F and ExpSR-s4-P profile to Fidelity based method and Perception based method. Table 3: Quantitative comparison on classical image super-resolution benchmarks (Set5, Set14, B100). The top three performances of each metric are marked in bold, underline, italic respectively. For Agentic systems, we only bold the best performance."
        },
        {
            "title": "Dataset Method",
            "content": "PSNR SSIM LPIPS DISTS FID NIQE CLIPIQA MUSIQ MANIQA Set5 Set B100 Fidelity based method SwinIR [117] X-Restormer [28] DRCT [68] HAT-L [29] HMA [34] 4KAgent (ExpSR-s4-F) Perception based method SwinIR (Real-ISR) [117] DiffBIR [121] OSEDiff [211] PiSA-SR [176] 4KAgent (ExpSR-s4-P) Agentic System AgenticIR [269] 4KAgent (GenSR-s4-P) Fidelity based method SwinIR [117] X-Restormer [28] DRCT [68] HAT-L [29] HMA [34] 4KAgent (ExpSR-s4-F) Perception based method SwinIR (Real-ISR) [117] DiffBIR [121] OSEDiff [211] PiSA-SR [176] 4KAgent (ExpSR-s4-P) Agentic System AgenticIR [269] 4KAgent (GenSR-s4-P) Fidelity based method SwinIR [117] X-Restormer [28] DRCT [68] HAT-L [29] HMA [34] 4KAgent (ExpSR-s4-F) Perception based method SwinIR (Real-ISR) [117] DiffBIR [121] OSEDiff [211] PiSA-SR [176] 4KAgent (ExpSR-s4-P) Agentic System AgenticIR [269] 4KAgent (GenSR-s4-P) 32.92 33.15 33.26 33.29 33.39 33.34 28.48 26.41 26.21 27.56 26. 0.9044 0.9057 0.9067 0.9082 0.9089 0.9081 0.8446 0.7510 0.8063 0.8189 0.7899 0.1669 0.1636 0.1616 0.1582 0.1587 0.1589 0.1632 0.2059 0.1583 0.1318 0.1591 0.1567 0.1564 0.1526 0.1542 0.1535 0.1549 0.1590 0.1888 0.1647 0.1516 0. 57.37 60.24 52.25 56.95 54.61 56.62 63.58 72.79 67.50 62.94 70.63 23.68 26.25 0.6711 0.7672 0.2737 0.1785 0.2190 0. 124.96 89.02 29.09 29.16 29.57 29.46 29.51 29.43 25.91 24.73 24.30 24.76 24.76 0.7950 0.7963 0.8009 0.8014 0.8019 0.7989 0.7187 0.6349 0.6663 0.6716 0.6471 0.2671 0.2659 0.2617 0.2565 0.2567 0. 0.2244 0.2338 0.2389 0.1993 0.2158 0.1574 0.1557 0.1524 0.1516 0.1510 0.1528 0.1508 0.1545 0.1524 0.1343 0.1467 70.49 69.86 67.84 66.61 69.41 67.83 96.19 100.51 101.03 89.91 101.99 21.98 23. 0.6064 0.6340 0.2807 0.2484 0.1812 0.1749 129.29 125.29 27.92 27.99 28.10 28.08 28.12 28.09 25.42 24.99 24.35 25.00 24. 0.7489 0.7508 0.7535 0.7547 0.7559 0.7540 0.6711 0.6156 0.6495 0.6520 0.6294 0.3548 0.3521 0.3480 0.3440 0.3442 0.3453 0.2500 0.2719 0.2408 0.2111 0.2387 0.2005 0.1972 0.1947 0.1952 0.1953 0.1950 0.1699 0.1666 0.1634 0.1471 0. 94.57 90.52 87.76 89.52 88.46 88.89 92.65 84.99 73.23 61.82 73.64 22.51 23.64 0.5853 0.6246 0.3078 0.2572 0.1907 0. 102.92 78.80 7.24 7.07 6.94 7.11 7.11 6.90 7.46 6.06 5.78 5.87 5.79 6.59 6.72 6.19 6.22 6.09 6.11 6.25 5.95 4.45 4.34 4.61 4.16 4. 4.58 4.29 6.27 6.21 6.06 6.20 6.17 6.02 4.00 3.92 4.08 4.04 3.86 4.08 3.93 0.6179 0.6368 0.6406 0.6389 0.6338 0.6294 0.7072 0.8405 0.7973 0.8086 0. 0.7750 0.7396 0.5252 0.5332 0.5362 0.5267 0.5278 0.5315 0.6506 0.7553 0.7264 0.7643 0.7740 0.7449 0.7604 0.5373 0.5427 0.5499 0.5477 0.5534 0.5516 0.6322 0.7483 0.7422 0.7384 0. 0.7474 0.7354 59.98 60.09 60.21 60.44 60.39 60.02 62.43 70.23 68.76 69.87 69.93 71.88 70.39 63.10 62.91 63.12 63.23 63.00 63.45 66.82 72.97 70.02 71.81 72. 72.48 73.64 57.71 57.91 58.78 58.71 59.11 59.12 62.78 68.23 68.54 68.47 69.42 68.36 69.44 0.6095 0.6169 0.6100 0.6212 0.6241 0.6177 0.6153 0.6767 0.6698 0.6904 0. 0.7079 0.6811 0.5891 0.5925 0.5932 0.5986 0.6012 0.5970 0.6054 0.6869 0.6674 0.7015 0.6956 0.6804 0.7061 0.5860 0.5935 0.5895 0.5991 0.6043 0.5994 0.6085 0.6750 0.6725 0.6829 0. 0.6752 0.6844 Experimental results are shown in Tabs. 3 and 4. For the commonly used fidelity metrics PSNR and SSIM in the classical image SR task, 4KAgent with ExpSR-s4-F profile shows competitive 12 Table 4: Quantitative comparison on classical image super-resolution benchmarks (Urban100 and Manga109). The top three performances of each metric are marked in bold, underline, italic respectively. For Agentic systems, we only bold the best performance."
        },
        {
            "title": "Method",
            "content": "PSNR SSIM LPIPS DISTS FID NIQE CLIPIQA MUSIQ MANIQA Urban100 Manga109 Fidelity based method SwinIR [117] X-Restormer [28] DRCT [68] HAT-L [29] HMA [34] 4KAgent (ExpSR-s4-F) Perception based method SwinIR (Real-ISR) [117] DiffBIR [121] OSEDiff [211] PiSA-SR [176] 4KAgent (ExpSR-s4-P) Agentic System AgenticIR [269] 4KAgent (GenSR-s4-P) Fidelity based method SwinIR [117] X-Restormer [28] DRCT [68] HAT-L [29] HMA [34] 4KAgent (ExpSR-s4-F) Perception based method SwinIR (Real-ISR) [117] DiffBIR [121] OSEDiff [211] PiSA-SR [176] 4KAgent (ExpSR-s4-P) Agentic System AgenticIR [269] 4KAgent (GenSR-s4-P) 27.45 27.64 28.78 28.58 28.69 28. 23.24 22.51 21.88 22.36 22.56 0.8254 0.8288 0.8492 0.8495 0.8511 0.8479 0.7184 0.6397 0.6572 0.6704 0.6582 0.1840 0.1805 0.1623 0.1598 0.1583 0.1599 0.1908 0.2011 0.2185 0.1823 0.1955 0.1533 0.1504 0.1388 0.1411 0.1405 0. 0.1365 0.1395 0.1479 0.1297 0.1378 3.58 3.65 2.92 2.87 2.93 2.97 25.36 26.10 38.13 28.51 25.55 22.03 22.27 0.6615 0.6545 0.2147 0. 0.1507 0.1444 31.09 32.29 32.05 32.40 32.84 33.08 33.20 32.87 26.29 23.57 23.74 24.02 23.76 0.9260 0.9279 0.9307 0.9334 0.9344 0.9316 0.8553 0.7297 0.7980 0.8119 0. 0.0926 0.0909 0.0889 0.0845 0.0835 0.0860 0.1367 0.1923 0.1703 0.1450 0.1776 0.0761 0.0748 0.0685 0.0684 0.0682 0.0683 0.0948 0.1275 0.1181 0.1161 0.1231 1.88 1.88 1.49 1.48 1.47 1.48 24.59 30.11 41.54 34.11 33. 23.70 23.12 0.7550 0.7556 0.1862 0.1834 0.1246 0.1264 34.01 34.58 5.50 5.61 5.45 5.55 5.61 5. 4.29 4.79 4.67 4.43 4.53 4.65 4.43 5.32 5.48 5.08 5.26 5.24 4.95 4.30 4.55 4.78 4.35 4.32 4.38 4.23 0.5003 0.4953 0.5271 0.5054 0.5084 0. 0.6169 0.7185 0.6593 0.6814 0.7092 0.6790 0.7001 0.6385 0.6325 0.6362 0.6160 0.6208 0.6329 0.6316 0.7804 0.6874 0.7277 0.7678 0.7450 0.7652 70.00 70.00 70.48 70.62 70.75 70. 71.99 73.10 72.35 72.93 73.65 73.10 73.57 70.32 70.05 69.77 69.76 69.92 69.99 70.28 74.51 72.51 74.76 75.08 73.98 75.02 0.6693 0.6746 0.6778 0.6866 0.6893 0. 0.6578 0.6956 0.6822 0.7020 0.6981 0.6873 0.6961 0.6117 0.6123 0.6087 0.6145 0.6196 0.6125 0.5868 0.6787 0.6538 0.6779 0.6801 0.6597 0.6797 performance compared to state-of-the-art fidelity-based methods, ranking among the top three on Set5, B100, Urban100, and Manga109 datasets. For perception-based methods, we focus more on perceptual metrics such as NIQE, CLIPIQA, MUSIQ, and MANIQA. By simply switching the profile from ExpSR-s4-F to ExpSR-s4-P, 4KAgent achieves strong performance among state-ofthe-art perception-based methods, ranking among the top two across most metrics on all classical SR benchmarks. For comparison across agentic systems, 4KAgent outperforms AgenticIR in most metrics on classical SR benchmarks, especially on the Set14 and B100 datasets. Qualitative Comparison For visual comparison, we select two leading fidelity-based methods (X-Restormer, HAT-L), two perception-based methods (SwinIR (Real-ISR), DiffBIR), as well as one agentic system (AgenticIR) as baselines. For 4KAgent, we present images under GenSR-s4-P profile for comprehensive comparison. Visual comparisons in Fig. 4 reveal that fidelity-based methods tend to produce overly smooth or blurred details (e.g., HAT-L), even trained with real-world image SR setting (e.g., SwinIR (Real-ISR)), which is visually unpleasant. Diffusion-based method (e.g., DiffBIR) generates rich but unrealistic details. AgenticIR performs well in detail generation but still lacks realism and exhibits noticeable color shifts. 4KAgent delivers both richer and more accurate details than these methods. For instance, it faithfully reproduces the fine stripes on tree bark in the top row and the intricate structure of antlers in the bottom row. Discussions In the context of classical image super-resolution (SR), fidelity-based methods prioritize reconstruction accuracy, measured by PSNR and SSIM, resulting in outputs that often appear overly smooth or blurred. In contrast, perception-based methods optimize for high perceptual quality, reflected in metrics like NIQE, CLIPIQA, MUSIQ, and MANIQA, though often at the expense of 13 Figure 4: Visual comparisons on classical image SR task. (Please zoom in to see details.) fidelity. For example, diffusion-based approaches (e.g., DiffBIR) may hallucinate rich but unrealistic textures. AgenticIR, while capable of generating sharper details, sometimes introduces color shifts or artifacts that undermine visual plausibility. 4KAgent offers configurable flexibility through its profile system, allowing it to operate either as fidelity-based system (ExpSR-s4-F) or as perception-based system (ExpSR-s4-P). Quantitatively, 4KAgent delivers competitive PSNR and SSIM scores under the fidelity-based profile, and achieves leading performance in perceptual metrics (NIQE, CLIPIQA, MUSIQ, MANIQA) under the perception-based profile. Qualitatively, 4KAgent consistently produces images with rich, realistic details. The flexibility of 4KAgent allows it to strike superior balance: it can be easily tuned for maximum visual fidelity or for maximum perceptual appeal without extra training or adaptation, which avoids the common drawbacks of existing SR systems."
        },
        {
            "title": "5.2 Real-World Image Super-Resolution",
            "content": "Settings. In this experiment, we follow prior real-world image super-resolution (SR) methods [176, 192, 201, 211] and adopt widely used real-world image SR datasets (RealSR [17], DrealSR [209]) for evaluation. Following the common practice in recent works [192, 201, 211], the real-world data are center-cropped from these datasets with size 128 128 for low-quality (LQ) images and 512 512 for high-quality (HQ) images. In this experiment, we customize 4KAgent with two different profiles: ExpSR-s4-P and GenSR-s4-P. We compare 4KAgent with state-of-the-art real-world image SR methods, including ResShift [240], StableSR [192], DiffBIR [121], PASD [229], SeeSR [212], SinSR [201], OSEDiff [211], and PiSA-SR [176]. In addition, we employ AgenticIR in this experiment for agentic system comparison. The evaluation is conducted using the same metrics as those employed in the Classical Image SR experiment (5.1). Quantitative Comparison Experiment results on real-world image super-resolution datasets are shown in Tab. 5. For the real-world image super-resolution task, we concern more about perceptual metrics, such as NIQE, CLIPIQA, MUSIQ, and MANIQA. Real-world image SR methods have achieved promising results on these metrics. AgenticIR, which contains the DiffBIR in its toolbox, outperforms DiffBIR in most perceptual metrics, proving that agentic systems have better potential in solving the real-world SR problem. 4KAgent goes step further and outperforms AgenticIR in most metrics, achieving better perceptual quality with better fidelity (PSNR and SSIM), regardless of profile setting. In addition, 4KAgent sets new state-of-the-art performance on perceptual metrics. Qualitative Comparison For visual comparison, we select four leading real-world image superresolution methods (StableSR, DiffBIR, SinSR, OSEDiff) as well as one agentic system (AgenticIR) as baselines. The visual results are presented in Fig. 5. While previous methods are able to recover rich details from the LQ image, their results often lack realism and fidelity. For example, in the top 14 row, OSEDiff reconstructs clothing that appears more like jackets, whereas the HQ reference image shows down jackets. 4KAgent produces sharper and more realistic details, such as the texture of the down jacket in the top row and the clarity of the number 27 in the bottom row. Table 5: Quantitative comparison on real-world image super-resolution benchmarks (RealSR and DrealSR). The top three performances of each metric are marked in bold, underline, italic respectively."
        },
        {
            "title": "Dataset Method",
            "content": "PSNR SSIM LPIPS DISTS FID NIQE CLIPIQA MUSIQ MANIQA RealSR"
        },
        {
            "title": "DrealSR",
            "content": "ResShift [240] StableSR [192] DiffBIR [121] PASD [229] SeeSR [212] SinSR [201] OSEDiff [211] PiSA-SR [176] AgenticIR [269] 4KAgent (ExpSR-s4-P) 4KAgent (GenSR-s4-P) ResShift [240] StableSR [192] DiffBIR [121] PASD [229] SeeSR [212] SinSR [201] OSEDiff [211] PiSA-SR [176] AgenticIR [269] 4KAgent (ExpSR-s4-P) 4KAgent (GenSR-s4-P) 26.31 24.69 24.88 25.22 25.33 26.30 25.15 25.50 22.45 24.60 22.55 28.45 28.04 26.84 27.48 28.26 28.41 27.92 28.31 23.06 26.00 23.11 0.7411 0.7052 0.6673 0.6809 0.7273 0.7354 0.7341 0.7417 0.6447 0.6839 0.6557 0.7632 0.7460 0.6660 0.7051 0.7698 0.7495 0.7835 0.7804 0.6145 0.6535 0. 0.3489 0.3091 0.3567 0.3392 0.2985 0.3212 0.2921 0.2672 0.3745 0.3253 0.3509 0.4073 0.3354 0.4446 0.3854 0.3197 0.3741 0.2968 0.2960 0.4775 0.4257 0.4579 0.2498 0.2167 0.2290 0.2259 0.2213 0.2346 0.2128 0.2044 0.2503 0.2292 0.2468 0.2700 0.2287 0.2706 0.2535 0.2306 0.2488 0.2165 0.2169 0.2973 0.2717 0.2866 142.81 127.20 124.56 123.08 125.66 137.05 123.50 124.09 140.38 127.64 134.63 175.92 147.03 167.38 157.36 149.86 177.05 135.29 130.61 182.02 170.19 178. 7.27 5.76 5.63 5.18 5.38 6.31 5.65 5.50 5.81 5.09 4.78 8.28 6.51 6.02 5.57 6.52 7.02 6.49 6.20 6.11 5.51 4.65 0.5450 0.6195 0.6412 0.6502 0.6594 0.6204 0.6693 0.6702 0.6506 0.7078 0.6666 0.5259 0.6171 0.6292 0.6714 0.6672 0.6367 0.6963 0.6970 0.6542 0.7167 0.7092 58.10 65.42 64.66 68.74 69.37 60.41 69.09 70.15 65.87 70.97 71.77 49.86 58.50 60.68 64.55 64.84 55.34 64.65 66.11 63.59 67.72 69. 0.5305 0.6211 0.6231 0.6461 0.6439 0.5389 0.6339 0.6560 0.6210 0.6602 0.6564 0.4573 0.5602 0.5902 0.6130 0.6026 0.4898 0.5899 0.6156 0.5927 0.6397 0.6219 Figure 5: Visual comparisons on real-world image SR task. (Please zoom in to see details) Discussions The real-world image super-resolution task is more challenging than the classical image super-resolution task as it contains more complex distortions than the synthetic downsample, which can also be seen from the comparison of the LQ image and HQ image in the dataset. Under this challenging setting, agentic systems prove their advantage by analyzing the distortion and restoring the image properly. 4KAgent further proves its superiority by consistently outperforming AgenticIR 15 in most quantitative metrics. In particular, 4KAgent sets new state-of-the-art performance for no-reference perceptual metrics, demonstrating that its design effectively elevates perceived realism. Qualitatively, these gains translate into visibly sharper and more believable details. By dynamically leveraging multiple SR experts and selecting the optimal result, 4KAgent shows its superiority in the challenging real-world image super-resolution task."
        },
        {
            "title": "5.3 Multiple-Degradation Image Restoration",
            "content": "Settings. We follow the experimental protocol of AgenticIR, evaluating 4KAgent on the Group A, B, and test sets, which together contain 1,440 LQ images generated by applying 16 combinations of degradations to images from the MiO100 dataset [90]. In this experiment, we configure 4KAgent with the GenMIR-P profile. We compare 4KAgent with several leading all-in-one models, including AirNet [105], PromptIR [160], MiOIR [89], DA-CLIP [133], InstructIR [36], AutoDIR [80], as well as agentic systems: AgenticIR [269] and MAIR [79]. For evaluation, we adopt comprehensive set of metrics, including PSNR, SSIM, LPIPS, MANIQA, CLIPIQA, and MUSIQ. Quantitative Comparison Experimental results are shown in Tab. 6. In the multiple-degradation image restoration (IR) task, agentic systems once again prove their superiority, outperforming allin-one methods in all metrics. Among the agentic systems, 4KAgent performs the best, achieving new state-of-the-art performance on PSNR, MANIQA, CLIPIQA, and MUSIQ. Specifically, for no-reference perceptual metrics (MANIQA, CLIPIQA, MUSIQ), 4KAgent outperforms all compared methods by noticeable margin (e.g., 4.2 lead of MUSIQ on Group C). For SSIM and LPIPS metrics, 4KAgent remains competitive, ranking among the top two on Group and Group subsets. Table 6: Quantitative comparison of multiple-degradation image restoration tasks on three subsets (Group A, B, and C) from the MiO100 dataset. The top three performances of each metric are marked in bold, underline, italic respectively. Degradations Method PSNR SSIM LPIPS MANIQA CLIPIQA MUSIQ"
        },
        {
            "title": "Group C",
            "content": "AirNet [105] PromptIR [160] MiOIR [89] DA-CLIP [133] InstructIR [36] AutoDIR [80] AgenticIR [269] MAIR [79] 4KAgent (GenMIR-P) AirNet [105] PromptIR [160] MiOIR [89] DA-CLIP [133] InstructIR [36] AutoDIR [80] AgenticIR [269] MAIR [79] 4KAgent (GenMIR-P) AirNet [105] PromptIR [160] MiOIR [89] DA-CLIP [133] InstructIR [36] AutoDIR [80] AgenticIR [269] MAIR [79] 4KAgent (GenMIR-P) 19.13 20.06 20.84 19.58 18.03 19.64 21.04 21.02 21.48 19.31 20.47 20.56 18.56 18.34 19.90 20.55 20.92 20.95 17.95 18.51 15.63 18.53 17.09 18.61 18.82 19.42 19. 0.6019 0.6088 0.6558 0.6032 0.5751 0.6286 0.6818 0.6715 0.6720 0.6567 0.6704 0.6905 0.5946 0.6235 0.6643 0.7009 0.7004 0.6727 0.5145 0.5166 0.4896 0.5320 0.5135 0.5443 0.5474 0.5544 0.5629 0.4283 0.4127 0.3715 0.4266 0.4429 0.3967 0.3148 0.2963 0.3019 0.3670 0.3370 0.3243 0.4405 0.4072 0.3542 0.3072 0.2788 0.3017 0.5782 0.5756 0.5376 0.5335 0.5582 0.5019 0.4493 0.4142 0. 0.2581 0.2633 0.2451 0.2418 0.2660 0.2500 0.3071 0.3330 0.3748 0.2882 0.2893 0.2638 0.2435 0.3022 0.2534 0.3204 0.3544 0.3734 0.1854 0.1906 0.1717 0.1916 0.1732 0.2045 0.2698 0.2798 0.3545 0.3930 0.4013 0.3933 0.4139 0.3528 0.3767 0.4474 0.4751 0.5544 0.4274 0.4289 0.4330 0.4154 0.3790 0.3986 0.4648 0.5084 0.5505 0.3113 0.3104 0.2891 0.3476 0.2537 0.2939 0.3948 0.4239 0. 42.46 42.62 47.82 42.51 45.77 47.01 56.88 59.19 63.19 47.88 48.10 51.87 43.70 50.94 49.64 57.57 60.98 62.69 30.12 29.71 37.95 33.87 33.69 37.86 48.68 51.36 55.56 Qualitative Comparison For visual comparison, we select two leading all-in-one methods (DACLIP, AutoDIR) as well as an agentic system (AgenticIR) as baselines. Visual comparisons are shown in Fig. 6, all-in-one methods performs limited under this setting, especially when restoring complex distortions, such as rain drops. AgenticIR achieves promising results, proving the potential 16 of agentic systems in dealing with complex distortion tasks. 4KAgent goes step further, generating images with high-grained details and more consistent with the high-quality (HQ) reference image. For instance, the natural stripes on the tree trunks and the fine leaf textures, the intricate waterfall ripples and mountain contours on the left, and the skin of the lizard on the right. These results demonstrate the superiority of 4KAgent under complex distortions. Figure 6: Visual comparisons on multiple-degradation IR task. (Please zoom in to see details) Discussions Concluding from experiment results, agentic systems have shown their superiority in the multi-degradation image restoration tasks, where each low-quality (LQ) image is affected by 2 3 types of distortions. Under this challenging setting, 4KAgent exhibits clear advantage in handling complex, multi-degraded inputs, outperforming both conventional all-in-one methods and previous agentic systems. Quantitatively, 4KAgent achieves state-of-the-art results across multiple metrics, including PSNR, MANIQA, CLIPIQA, and MUSIQ, highlighting its strong capability in enhancing both fidelity and perceptual quality. Qualitatively, this metric superiority translates into more faithful restoration of fine-grained patterns and textures, even under severe and heterogeneous distortions."
        },
        {
            "title": "5.4 Face Restoration",
            "content": "Settings. In this section, we evaluate 4KAgent on the real-world face restoration benchmark, the WebPhoto-test [196] dataset, which contains 407 low-quality face images collected from the Internet. As face restoration pipeline is module subsequent to super-resolution task in 4KAgent, we first downsample the images by factor of 4 to generate low-quality (LQ) images. In this experiment, we configure 4KAgent with the GenSRFR-s4-P profile. We compare 4KAgent with state-of-the-art face restoration methods, including CodeFormer [266], GFPGAN [196], and DifFace [239], as well as an agentic system AgenticIR [269]. For face restoration methods, we set the scaling factor to 4. As there are no high-quality (HQ) references, we evaluate performance with four no-reference perceptual metrics (NIQE, CLIPIQA, MUSIQ, and MANIQApipal) and two advanced face-specific IQA metrics (CLIB-FIQA [154] and DSL-FIQA [27]). Quantitative Comparison Experimental results are shown in Tab. 7. AgenticIR performs worse than previous face restoration methods in terms of general perceptual metrics and face IQA metrics. 4KAgent outperforms AgenticIR on every metric by clear margin (e.g., 19.67 lead on MUSIQ). Moreover, 4KAgent achieves the best scores on general no-reference perceptual metrics and delivers competitive results on face IQA metrics, ranking second in both CLIB-FIQA and DSL-FIQA. Table 7: Quantitative comparison on face restoration benchmark (WebPhoto-Test). The top three performances of each metric are marked in bold, underline, italic respectively."
        },
        {
            "title": "Method",
            "content": "NIQE CLIPIQA MUSIQ MANIQA CLIB-FIQA DSL-FIQA WebPhoto-Test GFPGAN [196] CodeFormer [266] DifFace [239] AgenticIR [269] 4KAgent (GenSRFR-s4-P) 5.12 4.58 4.20 6.85 4.15 74.21 73.87 65.31 56.25 75.92 0.6379 0.6415 0.5891 0.5465 0. 0.6590 0.6840 0.6511 0.5978 0.6671 0.7732 0.7435 0.6189 0.5289 0.7683 0.6792 0.6884 0.5831 0.5731 0.7077 17 Qualitative Comparison Visual comparisons are shown in Fig. 7. Compared with other methods, 4KAgent demonstrates clear advantage in restoring realistic facial details, such as fine hair strands and natural skin textures. Moreover, it achieves superior restoration performance in non-facial regions, such as the wall and leaves in the first row and the logo on the hat in the second row. By consistently delivering high-quality restoration in both facial and non-facial areas, 4KAgent produces more visually pleasing and perceptually balanced results overall. Figure 7: Visual comparisons on face restoration task. (Please zoom in to see details) Discussions In the face restoration scenario, 4KAgent effectively addresses both facial and contextual degradations. Quantitatively, 4KAgent achieves the best performance on general no-reference perceptual metrics and delivers competitive scores on face IQA metrics, demonstrating the superiority of its system design and face Q-MoE policy. Qualitatively, this translates into more natural and richly detailed facial features, such as individual hair strands and realistic skin texture, while also enhancing background elements, producing overall more visually pleasing outputs. Among agentic systems, AgenticIR applies uniform processing pipeline without dedicated face restoration module, which limits its performance on face restoration tasks. Benefit from the face restoration pipeline and profile design, 4KAgent can be tailored as face restoration expert, achieving superior results. We further present how these two designs enhance the face restoration ability of 4KAgent in the ablation study."
        },
        {
            "title": "6 Experiment Part II: 16× Natural Image Super-Resolution",
            "content": "Experiment Part (5) demonstrates the flexibility and superiority of 4KAgent on general image restoration and super-resolution tasks. In this section, we evaluate 4KAgent on more challenging restoration tasks. First, we assess its performance on 16 real-world image super-resolution task. Next, we introduce new image restoration dataset, DIV4K-50, designed to restore 256256 resolution images with multiple distortions and upscale them to 40964096 resolution, and we evaluate our 4KAgent on this new benchmark."
        },
        {
            "title": "6.1 Large Scale Factor (16×) Image Super-Resolution",
            "content": "In this experiment, we explore challenging setting, 16 real-world image upscaling. For Settings. the dataset in this experiment, we adopt the RealSRSet dataset [251], which consists of 20 real-world low-quality images for real-world image super-resolution task. We configure 4KAgent with the Gen4K-P profile for this experiment. Based on the original resolution of images in this dataset, 4KAgent will upscale each image with scale factor of 16. We compare 4KAgent against previous methods, including HAT-L [29], DiffBIR [121], OSEDiff [211], and PiSA-SR [176], under two distinct settings: (1) 4 4: first upscale the low-quality image by scale factor of 4, then upscale the upscaled images to obtain the 16 upscaled images. (2) 16: directly upscale the low-quality image by scale factor of 16. We also extend AgenticIR [269] for larger scale-factor (16) image super-resolution for agentic system comparison. As there are no corresponding high-quality (HQ) reference images in the RealSRSet dataset, we evaluate the result images on four no-reference perceptual metrics: NIQE, CLIPIQA, MUSIQ, and MANIQA-pipal. 18 Quantitative Comparison Experiment results are shown in Tab. 8. As the largest scale factor of the pre-trained model in HAT-L is 4, we apply the 4 4 setting for HAT-L for 16 upscaling. Fidelity-based method struggles to deliver satisfactory performance in perceptual metrics under this setting. Recent perceptual-based real-world image super-resolution methods perform well in these metrics, even with the 16 setting. For example, DiffBIR with 16 setting achieves the best NIQE and MANIQA scores. Among agentic systems, 4KAgent outperforms AgenticIR on every metric by clear margin (e.g., 6.13 lead on MUSIQ). In addition, 4KAgent achieves the best performance on MUSIQ and the second-best performance on NIQE. For CLIPIQA and MANIQA metrics, 4KAgent also delivers competitive performance, ranking among the top three across all methods. Table 8: Quantitative comparison on RealSRSet dataset under 16 upscaling. The top three performances of each metric are marked in bold, underline, italic respectively."
        },
        {
            "title": "Method",
            "content": "NIQE CLIPIQA MUSIQ MANIQA RealSRSet HAT-L [29] (4 4) DiffBIR [121] (4 4) DiffBIR [121] (16) OSEDiff [211] (4 4) OSEDiff [211] (16) PiSA-SR [176] (4 4) PiSA-SR [176] (16) AgenticIR [269] 4KAgent (Gen4K-P) 10.59 3.63 2.80 5.40 4.66 5.70 4.88 4.86 3.53 0.3885 0.7867 0.7583 0.7665 0.6483 0.7883 0.6384 0.6775 0. 25.06 44.86 47.54 48.42 35.33 48.20 35.90 44.71 50.84 0.3060 0.6076 0.6099 0.5362 0.4581 0.5464 0.4128 0.5236 0.5913 Qualitative Comparison For visual comparison, we select three representative methods to benchmark against 4KAgent: (1) HAT-L (4 4): As representative fidelity-based method, we investigate its performance under large-scale upscaling setting. (2) DiffBIR (16): As shown in Tab. 8, DiffBIR with 16 setting achieves the best performance on NIQE and MANIQA. Therefore, we include it to assess its visual quality. (3) AgenticIR: Selected for agentic system comparison. Visual comparisons are shown in Fig. 8. HAT-L (4 4) shows limited enhancement over the low-quality input, leading to notably blurred textures. DiffBIR (16) produces visually rich but often unrealistic hallucinations, and in some cases even alters the semantic content of the scene (e.g., the first row), which is visually unappealing. AgenticIR generates visually plausible results but lacks sufficient sharpness and fine-grained details. 4KAgent generates high-grained and realistic details: the rock and grass textures in the first row, and the hair strands, eyebrow patterns, and naturally expressive eyes in the second and third rows are all much more faithfully restored with high-grained details. Figure 8: Visual comparisons on RealSRSet dataset (16 upscaling). (Please zoom in to see details) In the challenging 16 upscaling scenario, 4KAgent delivers competitive quantitative Discussions results alongside high-grained and realistic qualitative results, compared to other methods. In addition, traditional fidelity-oriented methods such as HAT-L struggle to recover fine details and instead produce overly smoothed and blurred results. This highlights the limitation of fidelity-driven pipelines under extreme magnification levels. Therefore, for such high-scale upscaling tasks, it is essential to configure 4KAgent with perception-oriented profile (e.g., setting Restore Option to Perception in the profile module) to better prioritize realistic texture synthesis. As image resolution approaches 4K and beyond, existing no-reference perceptual metrics may become misaligned with human judgment of visual quality. This discrepancy underscores the need for developing new no-reference perceptual metrics specifically designed for ultra-high-resolution images. 6.2 Joint Restoration & 4K Upscaling Settings. In this section, we bring 4KAgent to the most challenging setting: Joint multiple image restoration and 4K upscaling. As there are no previous methods and dataset target at this setting, we construct new evaluation dataset, DIV4K-50, constructed from the Aesthetic-4K dataset [249] to rigorously test end-to-end restoration and ultra-high-scale SR. Specifically, we select 50 images from the Aesthetic-4K dataset based on its content, then center-crop each image to 40964096 as the high-quality (HQ) ground truth. Then we downsample HQ images to 256256 and randomly apply combinations of defocus blur, motion blur, additive Gaussian noise, and JPEG compression to generate the corresponding low-quality (LQ) images. In this experiment, we also configure 4KAgent with the Gen4K-P profile. Comparing methods and experimental settings are the same as in Sec. 6.1. Quantitative Comparison Quantitative comparisons are shown in Tab. 9. Similar to the experiment result in Sec. 6.1, real-world image super-resolution methods perform competitively on perceptual metrics under this challenging setting. For example, DiffBIR achieves the best score on NIQE and CLIPIQA metrics. For agentic systems, 4KAgent outperforms AgenticIR on every metric. Additionally, 4KAgent achieves the best performance on MUSIQ and MANIQA metrics, and the second-best performance on NIQE and CLIPIQA metrics. Table 9: Quantitative comparison on DIV4K-50 dataset. The top three performances of each metric are marked in bold, underline, italic respectively. Dataset Method NIQE CLIPIQA MUSIQ MANIQA DIV4K-50 HAT-L [29] (4 4) DiffBIR [121] (4 4) DiffBIR [121] (16) OSEDiff [211] (4 4) OSEDiff [211] (16) PiSA-SR [176] (4 4) PiSA-SR [176] (16) AgenticIR [269] 4KAgent (Gen4K-P) 11.86 3.36 2.65 4.88 8.37 5.01 9.30 5.13 3.15 0.4699 0.7588 0.7078 0.7201 0.5680 0.7141 0.5549 0.5614 0.7585 22.82 37.17 38.59 39.88 25.07 38.22 24.51 39.55 44.16 0.3270 0.5916 0.5858 0.5482 0.4210 0.5364 0.3861 0.4814 0.5928 Qualitative Comparison As shown in Sec. 6.1, directly upscaling images with the 16 setting often produces visually rich but unrealistic artifacts. To ensure fair and meaningful qualitative comparison in this experiment, we select previous methods with the 4 4 setting, along with AgenticIR, as baselines. Qualitative comparisons are shown in Fig. 9. Real-world image superresolution methods generally recover more details than fidelity-based method. However, their outputs still exhibit noticeable distortions. For example, the generated patches from OSEDiff and PiSA-SR in the middle row retain visible JPEG compression artifacts, which degrade the overall visual quality. While DiffBIR achieves the most favorable visual results among these methods, its outputs still suffer from either blurring or unrealistic artifacts. AgenticIR performs competitively but tends to produce insufficiently sharp details. 4KAgent consistently reconstructs finer and more natural details. Notable examples include the facial features in the top row, the bears fur in the middle row, and the intricate coral textures in the bottom row, highlighting the superiority of our method. Discussions The DIV4K-50 benchmark presents an extremely challenging setting, where fully recovering low-quality 256256 inputs to match the 40964096 ground-truth images is virtually 20 Figure 9: Visual comparisons on DIV4K-50 dataset. (Please zoom in to see details) unattainable. Therefore, our focus shifts towards generating richly detailed and visually authentic textures. Existing real-world image super-resolution methods struggle to fully correct the compounded degradations present in challenging scenarios. While these methods achieve competitive scores on perceptual metrics, their outputs tend to suffer from unrealistic hallucinated textures. Prior agentic systems, despite their effectiveness in handling multiple degradations, show limitations in maintaining sufficient sharpness when upscaling to 4K resolutions. 4KAgent demonstrates its capability to simultaneously address multiple degradations and extreme scaling factors, effectively reconstructing natural, fine-grained details with high visual fidelity."
        },
        {
            "title": "7 Experiment Part III: AI-Generated Content (AIGC) 4K Super-Resolution",
            "content": "Text-to-visual models have ushered in new era of high-quality image synthesis in AI-generated content. While existing models exhibit impressive capabilities in interpreting and following complex user instructions, their limitation to relatively low-resolution outputs (e.g., 1024 1024) poses significant challenges for applications requiring ultra-high visual fidelity, such as digital content 21 creation and cinematic production. Scaling diffusion models for high-resolution generation entails computational overhead and access to large-scale high-resolution training data. As practical alternative, pre-trained diffusion models can be repurposed for ultra-high-resolution image generation."
        },
        {
            "title": "7.1 AI-Generated Content (AIGC) 4K Super-Resolution",
            "content": "In this section, we investigate super-resolution in AIGC scenarios by comparing direct 4K generation with 1K upscaling using our 4KAgent. Settings. As no prior method and dataset targets this setting, we sample 200 prompts from two standard AIGC benchmarks [101, 206]. For each prompt, 1K-resolution images were generated using several representative text-to-image models, including Flux.1-dev [93], Stable Diffusion 3 (SD3) [49], PixArt-Σ [24], SANA [224], and GPT-4o [72]. In parallel, we employed native 4K-capable models such as SANA [224] and Diffusion-4K [249] to directly synthesize 4K-resolution outputs. Due to stringent safety protocols, GPT-4o yielded only 39 valid 1K-resolution images from DiffusionDB prompts. We refer to the resulting datasets as GenAIBench-4K and DiffusionDB-4K. We use the ExpSR-s4-P profile in 4KAgent here. To assess perceptual quality, we employ noreference perceptual metrics. However, we observe that these metrics particularly MUSIQ are not tailored for evaluating ultra-high-resolution images, likely due to their inability to capture finegrained details through multi-scale architecture. To mitigate this limitation, we introduce MUSIQ-P, patch-applied variant that computes MUSIQ scores over non-overlapping 512 512 patches and averages them, thereby improving sensitivity to localized artifacts in ultra-high-resolution content. Quantitative Comparison. Tab. 10 presents the quantitative results across three strategies: (1) native 4K generation, (2) 1K-resolution generation, and (3) 1K-resolution images upscaled by 4KAgent. On GenAIBench-4K, the SANA-1K + 4KAgent pipeline achieves NIQE score of 3.03 and CLIPIQA of 0.7050, significantly outperforming SANA-4K (NIQE 4.02, CLIPIQA 0.6172). Similarly, PixArt-Σ + 4KAgent obtains the best NIQE score (2.76) among all methods, while SD3-Medium + 4KAgent achieves the best CLIPIQA (0.7169) and MANIQA (0.5155) scores. On DiffusionDB-4K, several models such as SANA-1K, GPT-4o, PixArt-Σ, and SD3-Medium, when upscaled with 4KAgent, achieve significantly lower NIQE and higher CLIPIQA scores. Although MUSIQ-P scores for the upscaled images show slight decrease relative to their 1K counterparts, the difference remains marginal, suggesting limited perceptual degradation during upscaling. Table 10: Comprehensive quantitative comparison of AIGC 4 Super-Resolution. The top three performances of each metric are marked in bold, underline, italic respectively. MUSIQ-P indicates patch-applied variant of the MUSIQ metric for evaluating ultra-high-resolution (4K) images."
        },
        {
            "title": "Model",
            "content": "SANA-4K [224] Diffusion-4K [249] SANA-1K [224] + 4KAgent GPT4o [72] + 4KAgent FLUX.1-dev [93] + 4KAgent PixArt-Σ [24] + 4KAgent SD3-Medium [49] + 4KAgent GenAIBench-4K [101] DiffusionDB-4K [206] NIQE CLIPIQA MUSIQ-P MANIQA NIQE CLIPIQA MUSIQ-P MANIQA 4.02 6.38 4.18 3.03 5.69 3.56 6.18 2.98 4.12 2.76 5.03 2.99 0.6172 0.5049 0.7147 0.7050 0.6607 0.7016 0.6768 0.7078 0.6960 0.7077 0.6922 0. 47.93 35.07 66.30 57.97 64.43 58.28 61.02 58.19 63.74 56.71 64.68 60.22 0.3673 0.3535 0.4814 0.4735 0.4997 0.4976 0.5018 0.5034 0.4415 0.4699 0.4767 0.5155 3.74 6.55 3.80 3.04 5.13 3.40 5.33 3.04 3.66 2.88 4.38 2. 0.6005 0.5056 0.6910 0.7082 0.6275 0.6867 0.7509 0.7440 0.6892 0.7092 0.6667 0.7066 48.66 35.87 67.99 60.48 62.53 56.67 69.69 60.88 66.54 58.85 65.99 59.35 0.3425 0.3404 0.5104 0.4715 0.4398 0.4711 0.5835 0.5056 0.4386 0.4659 0.4413 0. To further assess semantic and aesthetic fidelity, we report PickScore [87] in Tab. 11, which quantitatively captures diversity and human-aligned visual quality. On GenAIBench-4K, models enhanced with 4KAgent outperform their native 4K counterparts. On DiffusionDB-4K, the performance gap is smaller, which may be attributed to the datasets richer and more descriptive prompt content. 22 Table 11: Comparison of PickScore-Based [87] Quantitative Evaluation Between Native 4K and 4KAgentUpscaled 1K Models."
        },
        {
            "title": "Dataset",
            "content": "Avg. Prompt Length SANA-4K SANA-1K + 4KAgent Diffusion-4K Flux.1-dev + 4KAgent GenAIBench-4K [101] DiffusionDB-4K [206] 12.13 25.29 0.4482 0. 0.5518 0.5107 0.2389 0.2406 0.7611 0.7594 Qualitative Comparison Fig. 10 shows qualitative results of applying 4KAgent to various base models under identical prompts. Across different models, 4KAgent consistently enhances visual fidelity and preserves fine-grained details. As shown in Fig. 11, images upscaled from SANA-1K using 4KAgent exhibit richer textures and stronger aesthetic alignment than those generated natively at 4K resolution by SANA-4K. Figure 10: Visual comparison between native 4K image generation and 1K image generation methods with 4KAgent, using identical prompts. 4K images from Diffusion-4K and SANA-4K are displayed on the left, while the corresponding outputs enhanced by 4KAgent are shown on the right. Discussions The application of our 4KAgent in AIGC scenarios leads to substantial improvements in image quality when upscaling 1K-resolution images to 4K. First, when applied to 1K-resolution inputs, 4KAgent consistently achieves notable gains across multiple quantitative benchmarks, enabling more detailed and accurate reconstructions in the resulting 4K outputs. As traditional metrics are not specifically tuned for ultra-high-resolution images, we adopted the adaptive MUSIQ-P metric to enable perceptually-focused evaluation. The results indicate that 4K-upscaled images achieve perceptual quality scores comparable to their original 1K counterparts. Second, 4KAgent demonstrates strong capability in synthesizing high-fidelity visual details and intricate textures. However, as with 23 Figure 11: Visual comparison of aesthetic preference alignment between SANA-4K and SANA1K+4KAgent using identical prompts sampled from GenAIBench-4K. SANA-1K+4KAgent yields superior aesthetic alignment and richer high-resolution details, highlighted in the zoomed-in patches. many traditional super-resolution methods, this training-free framework occasionally introduces unintended bokeh-like artifacts, particularly in blurred background regions. Given 4KAgents modular and scalable design, we believe integrating task-specific profile configurations and perceptual alignment strategies could further reduce artifacts and improve robustness in diverse AIGC applications."
        },
        {
            "title": "8.1 Remote Sensing Image Super-Resolution",
            "content": "High-resolution satellite imagery is foundational for wide spectrum of remote sensing tasks, including urban planning, environmental monitoring, and disaster response [64, 169]. However, due to cost, bandwidth, and sensing constraints, acquiring such high-resolution imagery globally at very high frequency remains very expensive or even impractical. Recent advances in deep learning-based super-resolution have provided promising alternative by reconstructing high-fidelity imagery from lower-resolution observations [91]. In this section, we evaluate 4KAgent against state-of-the-art baselines on diverse set of real-world satellite image super-resolution datasets. Settings. We evaluate our models on four benchmark datasets covering varied land-use patterns and sensing characteristics: AID [218] is large-scale dataset constructed to benchmark aerial scene classification methods. It includes over 10,000 high-resolution aerial images across 30 scene categories, such as airports, industrial areas, and farmlands. Each image has resolution of 600600 with spatial resolution of 0.5-8 m/pixel. Images exhibit high intra-class diversity and low inter-class variation, making them well-suited for evaluating generalization in SR tasks. DIOR [109] is comprehensive object detection benchmark in the remote sensing domain, containing 23,463 images and 192,472 annotated object instances across 20 categories. The resolution of images is 800800, and the spatial resolutions range from 0.5m to 30m. These images exhibit high diversity in resolution, imaging conditions, and object scale. DOTA [217] consists of 2,806 ultra-high-resolution aerial images collected from various sensors. It features over 188,000 labeled object instances with arbitrary orientations. Each image is in resolution about 40004000. The combination of large-scale, fine-grained annotations and high inter-scene variability makes DOTA particularly valuable for evaluating perceptual fidelity. WorldStrat [37] is unique dataset designed for real-world satellite image super-resolution tasks, with globally stratified land use coverage across 10,000 km2. It pairs 10541054 pixel highresolution (SPOT 6/7, 1.5 m/pixel) and temporally matched low-resolution (Sentinel-2, 10 m/pixel) 24 imagery for thousands of regions worldwide. Importantly, unlike synthetic degradation benchmarks, WorldStrat contains real cross-sensor-captured low-resolution (LR) and high-resolution (HR) image pairs, introducing natural misalignment and color mismatches due to different sensor characteristics. From each dataset, we select 100-200 representative scenes. Following [222, 223], for AID, DIOR, and DOTA, HR images are downsampled using bicubic interpolation to generate corresponding LR inputs. For WorldStrat, we adopt the datasets official pre-processing pipeline, selecting LR images that are temporally closest to each HR acquisition. Notably, due to the different sensors used for LR and HR captures, RGB content may exhibit significant variation, posing realistic challenge for super-resolution. To test the generalization ability of our models, we evaluate on spectrum of resolution scales: 1) 4 SR (128512) on DIOR and DOTA datasets; 2) 4 SR (160640) on AID and WorldStrat datasets; 3) 4 SR (5122048) for high-res DOTA scenes; 4) 16 SR (e.g., 2564096) for DOTA scenes. We evaluate 4KAgent with the ExpSR-s4-F profile and ExpSR-s4-P profile in 4 super resolution for Fidelity and Perception preference, respectively. Then we evaluate 4KAgent with the Aer4KF profile and Aer4K-P profile in 16 super resolution for Fidelity and Perception preference, respectively. We benchmark 4KAgent against the following categories of SR models: 1) Expert aerial SR models: HAUNet [193], TransENet [100]; 2) Fidelity-based SR models: HAT-L [29], PiSA-SRPSNR [176], and SwinIR [117]; 3) Perception-based SR Models: DiffBIR [121], OSEDiff [211], HAT-GAN [29], PiSA-SR [176], and SwinIR (Real-ISR) [117]. For PiSA-SR-PSNR, we set the pixel guidance factor λpix = 1.0 and semantic guidance factor λsem = 0 for PiSA-SR [176] in inference. Additionally, we include AgenticIR for agentic system comparison. Together, these diverse datasets and models enable comprehensive evaluation of 4KAgent in terms of both pixel fidelity and perceptual quality, across synthetic and real-world degradation settings. Quantitative Comparison We report the comparison results on AID (4x SR, 160640), DIOR (4 SR, 128512), DOTA (4 SR, 128512), WorldStrat (4 SR, 160640), DOTA (4 SR, 5122048), and DOTA (16 4K SR) in Tabs. 12 to 17, respectively. Across all six benchmark settings, 4KAgent with Fidelity preference consistently demonstrates superior performance in terms of pixel-level reconstruction. It ranks within the top three PSNR in four out of six tasks, and ranks within the top three in SSIM across all synthetic scenarios. This confirms its ability to preserve structural details across scales and domains. Importantly, 4KAgent also consistently outperforms AgenticIR by large margin across all fidelity metrics and tasks, highlighting its effectiveness. In terms of perceptual quality, 4KAgent with Perception preference achieves top performance in perceptual quality assessment metrics across multiple datasets. Notably, on the WorldStrat, 4KAgent (ExpSR-s4-P) ranks first in MUSIQ, MANIQA, and CLIPIQA. These results indicate that 4KAgent not only produces photorealistic outputs but also maintains robustness in real-world, sensor-misaligned scenarios. Compared to AgenticIR, 4KAgent with Perception preference shows clear gains across all perceptual IQA metrics, reaffirming the value of 4KAgent in balancing realism and structure across diverse and challenging remote sensing settings. Table 12: 4 performance comparison of evaluated models on the AID dataset (160640). The top three performances of each metric are marked in bold, underline, italic respectively."
        },
        {
            "title": "Model",
            "content": "PSNR SSIM LPIPS DISTS FID NIQE MUSIQ MANIQA CLIPIQA Fidelity-based SR Perception-based SR SwinIR [117] HAT-L [29] PiSA-SR-PSNR [176] 28.4887 0.7422 27.1630 0.6835 27.9079 0.7251 SwinIR (Real-ISR) [117] 26.5090 0.6700 25.7860 0.6643 HAT-GAN [29] 24.8343 0.5554 DiffBIR [121] 25.2220 0.6164 OSEDiff [211] 24.5971 0.5903 PiSA-SR [176] 0.4355 0.4635 0.4273 0.3344 0.3522 0.4466 0.3497 0.3541 0.2473 0.2330 0.4273 0.1928 0.2137 0.2374 0.3497 0. 186.1843 7.4295 126.9911 7.3586 144.3109 7.2371 129.3879 3.8690 140.2364 4.8258 130.6386 4.8871 91.5957 3.6661 115.8287 3.4859 50.1222 36.4299 41.6239 60.6544 55.4862 65.9636 63.9855 66."
        },
        {
            "title": "Expert Aerial SR",
            "content": "HAUNet [193] TransENet [100] 28.5136 0.7146 28.0317 0.6983 0.4327 0.4179 0.2083 0.2109 122.1656 7.2497 125.9495 6.7700 35.5021 35."
        },
        {
            "title": "Agentic System",
            "content": "AgenticIR [269] 4KAgent (ExpSR-s4-F) 4KAgent (ExpSR-s4-P) 21.3431 0.5147 28.5481 0.7157 24.4212 0.5354 0.4600 0.4436 0.4696 0.2539 0.2263 0.2566 149.7191 4.5325 127.4411 7.5916 139.7775 4.8915 67.0257 37.5713 68. 0.3108 0.4149 0.4024 0.5641 0.5618 0.6342 0.6219 0.6555 0.4162 0.3776 0.6283 0.3774 0.6439 0.2563 0.4111 0.1831 0.5205 0.3556 0.7302 0.6074 0. 0.1706 0.1162 0.6693 0.4322 0.6897 25 Table 13: 4 performance comparison of evaluated models on the DIOR dataset (128512). The top three performances of each metric are marked in bold, underline, italic respectively."
        },
        {
            "title": "Model",
            "content": "PSNR SSIM LPIPS DISTS FID NIQE MUSIQ MANIQA CLIPIQA Fidelity-based SR Perception-based SR SwinIR [117] HAT-L [29] PiSA-SR-PSNR [176] 27.8751 0.7257 27.7355 0.6962 27.4176 0.7118 SwinIR (Real-ISR) [117] 26.4708 0.6698 26.8015 0.6848 HAT-GAN [29] 24.9254 0.5742 DiffBIR [121] 25.0470 0.6207 OSEDiff [211] 24.4078 0.5932 PiSA-SR [176] 0.4474 0.4586 0.4378 0.3391 0.3398 0.4201 0.3506 0.3534 0.2488 0.2195 0.2202 0.1983 0.2073 0.2317 0.1875 0. 223.4653 7.1247 134.4649 7.1142 167.7604 6.9455 144.3900 3.8921 149.8121 4.8459 146.2642 4.9198 127.7888 3.6641 129.2724 3.5111 51.6481 37.1784 42.5087 60.6319 55.8046 66.4572 65.3934 67.6365 Expert Aerial SR HAUNet [193] TransENet [100] 27.8221 0.6992 27.3002 0.6824 0.4527 0.4391 0.2100 0.2113 128.8770 6.9586 129.4893 6.4986 35.5885 34.7713 Agentic System AgenticIR [269] 4KAgent (ExpSR-s4-F) 4KAgent (ExpSR-s4-P) 22.4811 0.5654 27.6761 0.7062 24.4893 0.5795 0.4668 0.4309 0.4374 0.2388 0.2250 0.2471 169.2341 4.7446 146.5618 7.2555 160.6006 4.6522 63.1399 37.5543 68. 0.3122 0.4151 0.4095 0.5552 0.5592 0.6315 0.6245 0.6571 0.4089 0.3750 0.5938 0.3811 0.6358 0.2441 0.4194 0.1881 0.5091 0.3392 0.7078 0.5976 0. 0.1572 0.0984 0.6252 0.4368 0.6456 Table 14: 4 performance comparison of evaluated models on the DOTA dataset (128512). The top three performances of each metric are marked in bold, underline, italic respectively. Type Model PSNR SSIM LPIPS DISTS FID NIQE MUSIQ MANIQA CLIPIQA Fidelity-based SR Perception-based SR HAT-L [29] PiSA-SR-PSNR [176] SwinIR [117] 33.0720 0.8656 28.9623 0.7999 30.5969 0. 25.8326 0.6489 DiffBIR [121] 26.3616 0.7156 OSEDiff [211] 28.6557 0.7869 HAT-GAN [29] PiSA-SR [176] 25.8447 0.6921 SwinIR (Real-ISR) [117] 28.9000 0.7883 0.2448 0.3415 0.3275 0.3724 0.3324 0.2751 0.3220 0."
        },
        {
            "title": "Expert Aerial SR",
            "content": "HAUNet [193] TransENet [100] 32.8286 0.8627 30.7214 0.8176 0.2480 0."
        },
        {
            "title": "Agentic System",
            "content": "AgenticIR [269] 4KAgent (ExpSR-s4-F) 4KAgent (ExpSR-s4-P) 19.9655 0.5973 31.3589 0.8478 24.9224 0.6427 0.4227 0.2853 0.3884 0.1471 0.2093 0.2215 0.2340 0.2133 0.1818 0.2081 0.1769 0.1428 0. 0.2620 0.1776 0.2555 58.0105 6.6527 133.7664 7.7350 143.2866 7.5391 115.1440 6.0906 126.4670 5.4257 115.1743 5.7245 112.1042 4.9062 110.4552 4.7712 51.7547 47.4847 54.3111 64.9539 64.1220 57.0159 66.3901 59.7489 57.3008 68. 6.5917 6.2878 50.7492 42.8957 137.2777 6.3126 7.0808 88.0366 131.0346 6.1609 65.5596 50.6815 67.0355 0.5858 0.4878 0.4526 0.6535 0.6278 0.5929 0.6676 0. 0.5711 0.4856 0.6375 0.5515 0.6701 0.3725 0.3094 0.2700 0.6772 0.6736 0.3691 0.6855 0.4519 0.3824 0.3441 0.6198 0.3799 0. Table 15: 4 performance comparison of evaluated models on the WorldStrat dataset (160640). The top three performances of each metric are marked in bold, underline, italic respectively. Type Model PSNR SSIM LPIPS DISTS FID NIQE MUSIQ MANIQA CLIPIQA Fidelity-based SR Perception-based SR HAT-L [29] PiSA-SR-PSNR [176] SwinIR [117] 21.2238 0.6480 24.4312 0.7271 18.0937 0.6136 20.6485 0.5150 DiffBIR [121] 25.9716 0.6316 OSEDiff [211] 27.0796 0.7241 HAT-GAN [29] PiSA-SR [176] 23.9304 0.6179 SwinIR (Real-ISR) [117] 27.9062 0.7120 0.3468 0.3312 0. 0.6781 0.4460 0.3199 0.4581 0.3473 0.2108 0.2283 0.2279 0.3712 0.2562 0.1978 0.2748 0.2074 145.3798 9.0026 142.8870 10.5327 8.2607 180.4954 7.5514 227.6764 8.6342 176.2589 137.4441 9.8474 7.2214 170.0426 149.4428 10.1237 30.6655 29.6462 27. 53.5666 46.5092 30.3741 48.6414 32."
        },
        {
            "title": "Expert Aerial SR",
            "content": "HAUNet [193] TransENet [100] 26.1895 0.7143 24.4879 0.6943 0.3141 0.3270 0.1976 0.2106 128.2747 10.6318 7.7765 133.6959 28.4401 27."
        },
        {
            "title": "Agentic System",
            "content": "AgenticIR [269] 4KAgent (ExpSR-s4-F) 4KAgent (ExpSR-s4-P) 19.5883 0.5188 22.3529 0.6470 20.1510 0.5379 0.6716 0.3702 0.6363 0.3686 0.2324 0.3664 224.8042 166.2731 223.4866 8.7079 9.5364 8. 54.0649 34.3698 56.8421 0.2844 0.2994 0.2104 0.5475 0.4988 0.3587 0.5152 0.3497 0.3129 0.3152 0.5166 0.3011 0.5547 0.2339 0.2509 0. 0.6075 0.5096 0.2623 0.5010 0.3060 0.2701 0.2246 0.5402 0.2875 0.6236 Qualitative Comparison Figs. 12 to 16 present comprehensive visual comparison of all evaluated models across the tested datasets of 4 AID, 4 DIOR, 4 DOTA, 4 WorldStrat, and 16 DOTA. Firstly, 4KAgent with the perception preference consistently delivers superior perceptual quality on low-resolution SR datasets, as demonstrated in Figs. 12 to 15. In contrast, 4KAgent with the fidelity preference excels on high-resolution 4K SR datasets, producing the most faithful reconstructions 26 Table 16: 4 performance comparison of evaluated models on the DOTA dataset (5122048). The top three performances of each metric are marked in bold, underline, italic respectively."
        },
        {
            "title": "Model",
            "content": "PSNR SSIM LPIPS DISTS FID NIQE MUSIQ MANIQA CLIPIQA Fidelity-based SR Perception-based SR HAT-L [29] PiSA-SR-PSNR [176] SwinIR [117] 38.4856 0.9101 31.5336 0.8519 33.7463 0. 26.3032 0.6490 DiffBIR [121] 28.5768 0.7567 OSEDiff [211] 31.2735 0.8408 HAT-GAN [29] 27.6765 0.7303 PiSA-SR [176] SwinIR (Real-ISR) [117] 31.6933 0.8423 0.1956 0.3183 0.2990 0.4035 0.3632 0.2720 0.3499 0.2564 Expert Aerial SR HAUNet [193] TransENet [100] 38.2237 0.9075 35.9776 0. 0.2002 0.2431 Agentic System AgenticIR [269] 4KAgent (ExpSR-s4-F) 4KAgent (ExpSR-s4-P) 21.4719 0.7284 36.7655 0.9017 28.4281 0.7513 0.4157 0.2343 0.3440 0.1007 0.1797 0. 0.1953 0.2127 0.1527 0.2175 0.1453 0.0974 0.1267 0.2167 0.1283 0.2181 6.8076 0.2722 40.7879 7.0981 15.6964 6.7591 76.3522 4.0360 70.4222 4.1286 47.0606 4.6791 54.5844 3.8036 37.3714 4.1055 39.5562 38.6277 43. 57.2133 51.4468 47.6011 51.7604 48.3376 0.2984 0.2137 6.6907 6.3886 38.8776 34.8775 77.7286 4.7902 11.0017 7.0361 41.1425 3.9267 49.5913 38.7286 52. 0.5006 0.4226 0.4200 0.6265 0.5871 0.5508 0.6135 0.5561 0.4926 0.4345 0.5496 0.4738 0.6264 0.3408 0.2444 0.2847 0.7306 0.6907 0.3551 0.6353 0. 0.3471 0.2942 0.4853 0.3493 0.6608 Table 17: 16 performance comparison of evaluated models on the DOTA dataset (4K resolution). The top three performances of each metric are marked in bold, underline, italic respectively. Type Model PSNR SSIM LPIPS DISTS FID NIQE MUSIQ MANIQA CLIPIQA Fidelity-based SR Perception-based SR HAT-L [29] PiSA-SR-PSNR [176] SwinIR [117] 23.9586 0.6362 22.6265 0.5994 22.9425 0. 21.4093 0.4612 DiffBIR [121] 22.0602 0.5544 OSEDiff [211] 21.7525 0.5901 HAT-GAN [29] PiSA-SR [176] 22.1022 0.5761 SwinIR (Real-ISR) [117] 21.6770 0.5731 0.6471 0.7279 0.6860 0.5595 0.5450 0.5590 0.5552 0."
        },
        {
            "title": "Expert Aerial SR",
            "content": "HAUNet [193] TransENet [100] 23.6649 0.6268 22.9690 0.5992 0.6922 0."
        },
        {
            "title": "Agentic System",
            "content": "AgenticIR [269] 4KAgent (Aer4K-F) 4KAgent (Aer4K-P) 17.8736 0.4675 23.4348 0.6255 21.9826 0.5515 0.5928 0.6520 0.5525 0.3219 0.3368 0.3815 0.2214 0.2647 0.2668 0.2517 0.2431 0.3304 0. 0.2451 0.3312 0.2415 82.7644 9.0807 110.5112 9.2583 162.1128 9.7613 114.8595 3.4046 107.3622 4.1667 139.2047 5.4465 100.3336 4.2723 129.7745 3.7377 31.5394 24.0154 37.0268 57.5771 52.5278 45.6411 48.0151 50.5413 86.2487 97. 9.0018 7.6931 26.2489 21.3092 135.6437 3.8950 105.6710 9.0064 112.2518 3.7230 54.3685 33.6645 55.7730 0.2565 0.2066 0.2792 0.5030 0.4430 0.2791 0.3194 0. 0.2207 0.1903 0.4301 0.2725 0.5175 0.3062 0.1766 0.2814 0.7588 0.7287 0.3448 0.5972 0.4885 0.2567 0.1765 0.6551 0.3314 0. in Fig. 16. Secondly, 4KAgent exhibits clear advantage in reconstructing fine structures such as lines and patterns, as evident in Figs. 13 and 14. Finally, in the challenging cross-sensor super-resolution scenario of WorldStrat, where LR and HR images originate from different sensors. 4KAgent with the perception preference still maintains promising visual performance, as shown in Fig. 15. This demonstrates the robustness of 4KAgent across both resolution scales and sensor domains. Discussions These results across fidelity and perception metrics, combined with qualitative visual comparisons, provide several key insights into the advantages of 4KAgent. First, the consistent top-tier performance of 4KAgent with fidelity-based profile across wide range of datasets and scaling factors suggests that the agents analytical pipeline and adaptive control provide more precise reconstruction than traditional feedforward models. Unlike conventional SR networks that are typically optimized for either low-level fidelity or high-level realism, 4KAgents architecture decouples these objectives through specialized profiles, allowing it to excel in both domains without compromise. Second, the perceptual strength of 4KAgent is reflected not only in numerical scores but also in its sharper textures, reduced artifacts, and more semantically coherent outputs in qualitative results, demonstrating the value of integrating agentic reasoning with perceptual priors, especially in real-world datasets like WorldStrat. Finally, the margin by which both 4KAgent variants outperform AgenticIR, demonstrating the superiority of 4KAgent in terms of agentic system. Our contributions in the design of specialized profiles, adaptive modulation, and perceptual alignment mechanisms are crucial to bridging the gap between task generality and SR specialization. Together, these findings indicate that agentic architectures, when properly aligned with SR objectives, enable scalable, generalizable, and controllable super-resolution across diverse remote sensing domains. 27 Figure 12: Visual comparison on AID dataset (160640). Figure 13: Visual comparison on DIOR dataset (128512). Figure 14: Visual comparison on DOTA dataset (128512). Figure 15: Visual comparison on WorldStrat dataset (160640). 29 Figure 16: Visual comparison on DOTA dataset (4K upscaling)."
        },
        {
            "title": "8.2 Fluorescence Microscopic Image Super-Resolution",
            "content": "Confocal fluorescence microscopy is one of the most accessible and widely used techniques for studying cellular and subcellular structures [67, 230]. It builds sharp image by using either single pinhole to scan point-by-point or an array of pinholes on spinning disk to scan multiple points simultaneously to reject out-of-focus light, offering molecular specificity and 3D sectioning capabilities. However, it is constrained by diffraction-limited resolution down to 200 nm under visible light [181]. Meanwhile, high-intensity illumination required for improved resolution leads to photobleaching and phototoxicity, limiting live-cell imaging duration and data throughput [182]. 30 Deep learning-based single-image super-resolution (SISR) methods have shown great promise in recovering high-frequency details from lower-resolution inputs in biological microscopy, overcoming some limitations of hardware-based SR techniques [136], despite the scarcity of large, publicly available fluorescence microscopy datasets. To extend the evaluation of our 4KAgent on this scientific application of different modalities, we conducted experiments on representative dataset against baselines from major SISR families. Settings. We evaluate 4KAgent on SR-CACO-2 benchmark dataset [12], which contains 2,200 unique images of the Caco-2 human epithelial cell line, labeled with three distinct fluorescent markers: Survivin (CELL0), E-cadherin / Tubulin (CELL1), and Histone H2B (CELL2), at 2 (256512), 4 (128512), and 8 (64512) scales. To generate high-resolution images, each tile was scanned with 10241024 pixel resolution, and 8 scans were captured and then averaged together to reduce noise. Meanwhile, low-resolution images were captured directly by the microscope at three different scales without averaging. The full dataset contains 9,937 patches for each cell extracted from scanning confocal volumes, with tiles 9, 10, 14, 20 used as the test set. In our experiments, we randomly sampled 100 patches from each marker category in the test set at three super-resolution scales. We evaluate 4KAgent with the ExpSR-s2-F profile, ExpSR-s4-F profile, and ExpSR-s8-F profile, considering the demands and requirements of the microscopy image super-resolution task. We benchmark 4KAgent against 15 representative SISR models, broadly spanning pre-upsampling, post-upsampling, iterative up-and-down sampling and progressive upsampling SR methods. Each model has been trained on the SR-CACO-2 training set before deployment. Encompassing wide spectrum of upsampling strategies, this rigorous benchmark ensures comprehensive comparison between our 4KAgent and other specialized and general-purpose SR methods, assessing 4KAgents blind inference performance on the novel microscopy data domain. Quantitative Comparison All quantitative results are in Tab. 18. PSNR, SSIM, and NRMSE were selected as criteria. We noticed that the background of cell constituted significant proportion of an image, as was also reported in the original SR-CACO-2 benchmark. Because including the noninformative dark background in evaluation can lead to inflated and biased performance metrics, we adopted the masking strategy described in [12] to define our Regions-of-Interest (ROIs) and calculated performance metrics based only on these areas. Across different scales and cell types, 4KAgent with Fidelity preference consistently achieves top performance in pixel-level reconstruction in ROI. The superior result on PSNR, SSIM, and NRMSE confirms 4KAgents effectiveness in reconstructing fine fluorescence-labeled structures and low-level pixel fidelity, under various downsampling conditions. Furthermore, when compared with ENLCN, one of the most competitive methods, our 4KAgent with fidelity mode consistently exhibits clear advantage in all numerical metrics, underscoring its ability to handle the blind super-resolution task for real-world microscopy data. Qualitative Comparison Representative qualitative results for the highly challenging 8 superresolution task are shown in Fig. 17. At such high magnification, where information loss is severe, the ability to reconstruct distinct biological structures for each of the three cellular markers becomes critical test for any SISR method. Our visual analysis reveals clear performance differences. For the inherently dim and sparse CELL0 Survivin marker, 4KAgents reconstruction is markedly clearer and closer to the ground truth. It successfully restores the faint midbody structure with higher fidelity than top-performing baselines like ENLCN and ACT, which struggle to resolve this signal from the background. This superior performance is also evident for CELL1, where 4KAgent delineates the membrane and cytoskeletal framework with sharp, continuous lines. In contrast, the outputs from most other methods appear noticeably blurry, failing to preserve the cells essential structural integrity. In the case of the bright nuclear marker CELL2, the diffuse nature of the chromatin structure means even the ground truth image itself lacks hard, well-defined edges. In this difficult context, 4KAgent reconstructs complex, high-frequency textural pattern that is visually competitive with the other methods. While the intricate nature of the target makes absolute fidelity hard to judge, our method effectively generates detailed result on par with other models, even under zero-shot blind inference setting. Discussions Our experiments show that 4KAgent delivers leading performance on the challenging SR-CACO-2 dataset, with quantitative metrics and qualitative results surpassing those of the evaluated specialist models. The superior performance of 4KAgent underscores its strong applicability to 31 Table 18: Performance comparison of evaluated models on the selected sr-caco-2 test set on ROI only, i.e., cells. The top three performances of each metric are marked in bold, underline, and italic. SISR Methods Bicubic Pre-upsampling SR SRCNN [45] VDSR [86] DRRN [177] MemNet [178] Post-upsampling SR NLSN [143] DFCAN [162] SwinIR [117] ENLCN [215] GRL [112] ACT [234] Omni-SR [190] Iterative up-and-down sampling SR DBPN [61] SRFBN [115] Progressive upsampling SR ProSR [199] MS-LapSRN [95] Agentic System 4KAgent (ExpSR-sN-F) (N [2,4,8]) PSNR CELL0 CELL1 CELL2 Mean NRMSE CELL0 CELL1 CELL2 Mean SSIM CELL0 CELL1 CELL2 Mean Scale X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X8 X2 X4 X2 X4 X8 X2 X4 X8 x2 x4 x8 34.93 35.08 32.01 37.04 35.39 32.52 37.50 36.18 32.03 37.35 36.00 31.92 35.69 34.61 32.00 37.57 36.39 32.56 37.21 35.92 31.25 24.55 35.93 31.34 37.59 36.30 32.69 31.28 35.76 28.14 37.24 36.17 32.74 37.35 35.86 30.44 37.44 36.22 32.39 36.02 35.66 32. 36.92 36.11 32.13 32.73 30.91 30.67 39.92 41.25 38.93 32.61 31.99 28.77 34.47 32.73 29.16 34.29 32.52 28.80 34.23 32.50 28.31 33.40 32.48 28.76 34.31 32.75 29.13 34.20 32.49 28.15 34.48 32.66 28.43 34.41 32.74 29.28 34.54 32.81 28.93 34.66 32.76 29.13 34.19 32.53 28.21 34.54 32.85 28.89 33.49 32.49 29. 34.66 32.71 29.43 32.49 31.36 27.64 36.95 36.86 33.66 30.24 30.43 26.27 33.03 31.48 26.53 33.00 31.44 26.42 33.08 31.43 26.39 30.81 30.26 26.52 33.14 31.68 26.30 32.74 31.29 25.45 33.08 31.57 25.86 33.15 31.63 26.31 32.81 31.48 26.22 33.14 31.56 26.39 33.02 31.49 25.32 33.02 31.64 26.36 31.38 31.05 26. 32.80 31.61 26.36 28.34 30.69 24.68 33.93 35.07 31.99 32.59 32.50 29.02 34.85 33.20 29.41 34.93 33.38 29.08 34.89 33.31 28.88 33.30 32.45 29.09 35.01 33.61 29.33 34.72 33.23 28.28 30.71 33.39 28.54 35.05 33.56 29.43 32.88 33.35 27.76 35.01 33.50 29.42 34.85 33.29 27.99 35.00 33.57 29.21 33.63 33.07 29. 34.79 33.48 29.31 31.19 30.99 27.66 36.94 37.73 34.86 0.0887 0.0681 0.0661 0.0743 0.0793 0.0690 0.0641 0.0708 0.1311 0.1071 0.1240 0.1207 0.7899 0.7826 0.7038 0.7588 0.8411 0.7998 0.7718 0.8042 0.7280 0.6677 0.6808 0.6922 0.0610 0.0516 0.0471 0.0532 0.0704 0.0610 0.0563 0.0626 0.1074 0.0924 0.1143 0.1047 0.0602 0.0554 0.0479 0.0545 0.0663 0.0638 0.0571 0.0624 0.1307 0.1062 0.1237 0.1202 0.0609 0.0555 0.0475 0.0546 0.0678 0.0637 0.0570 0.0628 0.1310 0.1096 0.1230 0.1212 0.0776 0.0557 0.0607 0.0647 0.0808 0.0610 0.0654 0.0690 0.1272 0.0993 0.1183 0.1149 0.0588 0.0527 0.0465 0.0527 0.0630 0.0600 0.0548 0.0593 0.1147 0.0939 0.1205 0.1097 0.0614 0.0561 0.0493 0.0556 0.0684 0.0653 0.0582 0.0640 0.1344 0.1079 0.1276 0.1233 0.2349 0.0527 0.0473 0.1116 0.0673 0.0618 0.0559 0.0617 0.1314 0.1035 0.1230 0.1193 0.0574 0.0518 0.0462 0.0518 0.0638 0.0606 0.0553 0.0599 0.1108 0.0921 0.1205 0.1078 0.1088 0.0522 0.0492 0.0701 0.0678 0.0581 0.0565 0.0608 0.1555 0.0953 0.1104 0.1204 0.0619 0.0496 0.0459 0.0525 0.0652 0.0590 0.0554 0.0599 0.1064 0.0915 0.1152 0.1044 0.0597 0.0548 0.0475 0.0540 0.0680 0.0635 0.0563 0.0626 0.1418 0.1075 0.1265 0. 0.8733 0.8566 0.8283 0.8527 0.8707 0.8193 0.8104 0.8335 0.8117 0.7219 0.7207 0.7514 0.8921 0.8608 0.8385 0.8638 0.8777 0.8218 0.8180 0.8392 0.7291 0.6712 0.6877 0.6960 0.8917 0.8597 0.8386 0.8634 0.8772 0.8216 0.8167 0.8385 0.7290 0.6583 0.6860 0.6911 0.8295 0.8280 0.7759 0.8111 0.8465 0.8067 0.7651 0.8061 0.7528 0.6972 0.7102 0.7201 0.8911 0.8563 0.8375 0.8616 0.8754 0.8179 0.8131 0.8355 0.7909 0.7092 0.6989 0.7330 0.8899 0.8603 0.8375 0.8626 0.8770 0.8223 0.8174 0.8389 0.7447 0.6749 0.6841 0.7012 0.3785 0.8626 0.8385 0.6932 0.8772 0.8198 0.8161 0.8377 0.7516 0.6838 0.6923 0.7092 0.8876 0.8569 0.8340 0.8595 0.8766 0.8196 0.8148 0.8370 0.7998 0.7109 0.6984 0.7364 0.8043 0.8625 0.8337 0.8335 0.8774 0.8133 0.8144 0.8350 0.7296 0.7110 0.7197 0.7201 0.8890 0.8604 0.8288 0.8594 0.8761 0.8134 0.8065 0.8320 0.8083 0.7128 0.7063 0.7425 0.8896 0.8562 0.8370 0.8609 0.8737 0.8165 0.8117 0.8340 0.7231 0.6673 0.6808 0.6904 0.0588 0.0512 0.0470 0.0523 0.0638 0.0591 0.0548 0.0593 0.1103 0.0946 0.1157 0.1069 0.0767 0.0611 0.0576 0.0651 0.0729 0.0636 0.0592 0.0653 0.1243 0.1019 0.1181 0.1148 0.8872 0.8601 0.8339 0.8604 0.8745 0.8216 0.8091 0.8351 0.8060 0.7084 0.7149 0.7431 0.8319 0.8205 0.7592 0.8038 0.8589 0.8131 0.7921 0.8214 0.7553 0.6869 0.7081 0.7168 0.0621 0.0494 0.0485 0.0533 0.0656 0.0614 0.0556 0.0609 0.1268 0.0909 0.1224 0.1134 0.1014 0.0593 0.0805 0.0804 0.1118 0.0672 0.0611 0.0801 0.1206 0.1056 0.1305 0.1189 0.8879 0.8577 0.8385 0.8614 0.8771 0.8217 0.8164 0.8384 0.7504 0.7200 0.6919 0.7208 0.7957 0.8177 0.7735 0.7956 0.8124 0.7820 0.7858 0.7934 0.7829 0.6902 0.6649 0. 0.0508 0.0337 0.0426 0.0424 0.0389 0.0318 0.0366 0.0358 0.0532 0.0483 0.0602 0.0539 0.9321 0.9105 0.8745 0.9057 0.9555 0.9314 0.9089 0.9319 0.9378 0.9033 0.8929 0.9113 fluorescence microscopy SISR. First, it showcases strong zero-shot generalization, achieving highly competitive super-resolution performance on microscopy data, and could be further strengthened by adapting more domain-specific tools. Second, 4KAgent exhibits impressive cross-domain transferability, successfully adapting methods originally optimized for natural scenes to the distinct characteristics of fluorescence microscopy images. Third, the agent-based architecture enables the flexible and modular integration of existing models without requiring expensive retraining or model modification. Beyond immediate applications to super-resolution, the modularity and domain-agnostic nature of 4KAgent also suggest its broad potential for other real-world biomedical imaging domains where data scarcity or retraining costs are limiting factors."
        },
        {
            "title": "8.3 Pathology Image Super-Resolution",
            "content": "Pathology imagesparticularly whole-slide images (WSIs) and their extracted patchesplay critical role in digital diagnostics and disease detection. Typically, glass slides containing tissue sections stained with hematoxylin and eosin are digitized using high-speed scanners at resolutions 32 Figure 17: Visualization of fluorescence microscopy image SR on SR-CACO-2 dataset (64512). approaching 0.25 µm per pixel, resulting in gigapixel-scale images characterized by distinct color profiles and high-frequency textures unique to cellular structures. However, the substantial costs and data storage requirements associated with ultra-high-resolution scanning have led many workflows to rely on computational upscaling from lower-resolution acquisitions. This task is challenging, as pathology images possess specialized characteristics that pose significant difficulties for conventional single-image super-resolution (SISR) methods originally optimized for natural scenes. To address this challenge, we evaluate our 4KAgent on the bcSR dataset [76], comparing it against several established techniques to assess its effectiveness in this specialized domain. Settings. Our evaluation is conducted on the bcSR benchmark dataset curated for pathology image super-resolution. The bcSR dataset was derived from the larger CAMELYON [124] dataset, which contains whole-slide images (WSIs) of H&E-stained breast cancer sentinel lymph node sections. To create bcSR, the authors first sampled representative 10241024 patches from the original WSIs. Subsequently, filtering process was applied to remove patches with large blank areas and to select for images with high color channel variance, ensuring the dataset was rich in informative and challenging tissue structures. The final bcSR dataset consists of 1,200 unique images, which are split into 1,000-image training set and 200-image test set. Following the standard protocol established by the bcSR benchmark, the high-resolution ground truth images were downsampled using bicubic interpolation to generate the low-resolution inputs. We evaluated 4KAgent using the ExpSR-s4-F and ExpSR-s8-F profiles for the 4 and 8 tasks, respectively, prioritizing pixel fidelity which is critical for preserving fine diagnostic details. Performance was measured using the PSNR and SSIM metrics. Quantitative Comparison Quantitative results for the pathology image super-resolution task are summarized in Tab. 19. Across both 4 and 8 upsampling tasks, our 4KAgent achieves the highest SSIM score among all evaluated methods. While CARN, model specifically designed for this pathology dataset, attains marginally higher Peak PSNR, 4KAgents superior SSIM is more indicative of its ability to accurately preserve the complex tissue morphology and textures that are 33 essential for pathological diagnosis, which is more critical for the reliability of features used for clinical assessment. This demonstrates the effectiveness of 4KAgent in recovering diagnostically relevant details from downsampled pathology patches, with performance comparable or superior to other specialized, fully-trained models. Table 19: Quantitative comparison on Pathology dataset. The top three performances of each metric are marked in bold, underline, italic respectively."
        },
        {
            "title": "Method",
            "content": "4 8 Bicubic SRCNN [45] SRGAN [96] EDSR [119] RDN [262] RCAN [261] SWD-Net [32] CARN [76] PSNR SSIM PSNR SSIM 27.019 27.475 28.606 29.830 29.913 29.916 29.853 29.964 0.6659 0.7329 0.7719 0.8058 0.8074 0.8085 0.8000 0.8408 22.475 22.489 23.729 24.366 24.392 24.404 24.465 24.479 0.2776 0.3624 0.5580 0.5715 0.5711 0.5749 0.5755 0.5763 4KAgent (ExpSR-sN-F) (N [4,8]) 29.746 0.8602 24.300 0.5826 Qualitative Comparison Fig. 18 presents qualitative comparison for 4 super-resolution on representative patches from the bcSR test set. While both methods significantly improve upon the heavily blurred low-resolution inputs, closer inspection of the ROIs reveals that our training-free 4KAgent consistently produces results that are on par with, and often superior to, the fully-trained, domain-specific CARN model. This superior performance is particularly evident in challenging cases. In image 1010, 4KAgent successfully delineates individual cell boundaries and restores the heterogeneous texture of the tissue architecture. In contrast, CARNs output suffers from loss of sharpness and definition, resulting in noisier and blurrier cell regions and poorly defined tissue architecture, while also introducing slight color deviation. Similarly, in image 1175, 4KAgent accurately reconstructs the intricate internal structures, preserving the sharp outlines of the nuclei and cytoplasm. CARNs output, conversely, suffers from loss of sharpness and inaccurate detail, while also exhibiting subtle grid-like artifacts. Across all examples, 4KAgent consistently generates nuclei with sharper boundaries and more distinct internal textures, along with clearer cell membranes, demonstrating higher fidelity to the ground truth. These visual improvements also directly correlate with 4KAgents higher SSIM scores, confirming its enhanced ability to preserve the structural integrity of the tissue. The accurate recovery of such finegrained morphological details is critical for potential downstream clinical applications. High-fidelity reconstructions like those from 4KAgent can enable more reliable automated analysis, such as precise nuclei segmentation for cell counting, classification of cellular atypia, and grading of cancerous tissue, thereby highlighting the potential value of our approach in digital pathology workflows. Discussions The combined quantitative and qualitative results underscore the significant potential of 4KAgent for pathology image super-resolution. Although not leading in PSNR, 4KAgents superior SSIM scores demonstrate more accurate reconstruction of high-frequency textures and tissue morphology, which are paramount for pathological interpretation. Furthermore, because 4KAgent is not trained on specific pathology dataset, it is less susceptible to overfitting to the characteristics of single data source. This provides significant advantage when performing SR on real-world pathology images acquired by different scanners and staining protocols. Additionally, 4KAgents agentic framework allows for flexible expansion of its tool profile to better adapt to pathology imaging modalities. Its leading performance on the bcSR dataset validates the potential of this agentic approach as robust and generalizable solution for biomedical imaging. The ability to generate reconstructions with high structural fidelity has direct implications for critical downstream applications in computational pathology. By restoring sharper nuclei, clearer cell membranes, and more intelligible tissue architecture, 4KAgent provides more reliable input for automated analysis pipelines. This can enhance the accuracy of tasks such as nuclei segmentation, cell counting, and the classification of cancerous tissue, ultimately making it more robust and practically useful tool for AI-assisted biomedical diagnostics. 34 Figure 18: Visual comparison of pathology image super-resolution on bcSR dataset (2561024)."
        },
        {
            "title": "8.4 Medical Image Super-Resolution: X-Ray, Ultrasound, and Fundoscopy",
            "content": "In this chapter, we shift our focus to the super-resolution of clinical diagnostic imaging modalities, where the primary goal is to enhance anatomical and pathological details for improved diagnostic accuracy while minimizing patient burden, such as reducing exposure to ionizing radiation in X-ray imaging [185]. Although often grouped together, these modalities can be fundamentally diverse, operating on different physical principles: from X-rays utilizing ionizing radiation to ultrasound relying on acoustic waves. This diversity gives rise to unique image characteristics and modalityspecific challenges, such as maintaining pathological invariance in chest X-rays or avoiding the generation of pseudo-structures in ultrasound images. The prevailing approach in medical image SR has been the development of highly specialized models, each trained on specific datasets for single modality [238]. The rise of foundation models has led to more powerful specialist systems, such as those tailored for single modality like CT or dermatology [85, 155], though few pioneering works have begun to explore more universal solutions [120]. significant drawback of such specialized paradigm is poor generalization across different datasets and modalities, which creates major bottleneck for practical clinical deployment and motivates our evaluation of 4KAgents performance on these challenges. To this end, this chapter evaluates 4KAgents performance across several distinct and clinically important modalities. Settings. We evaluate 4KAgent with the ExpSR-s4-F profile across three medical imaging modalities with distinct imaging principles: X-ray, ultrasound, and fundoscopy, benchmarking against their respective baselines. Benchmark datasets of each imaging modality are summarized as follows: 35 X-ray. Chest X-ray 2017 [84] and Chest X-ray 14 [195]. Specifically, Chest X-ray 2017 is dataset of 5,856 pediatric images from Guangzhou Women and Childrens Medical Centre, split into 5,232 images for training and 624 images for testing. Chest X-ray 14 contains 112,120 frontal-view X-rays of 30,805 patients with 14 disease labels mined from radiology reports. Among them, 880 images additionally contain expert-annotated bounding boxes. Following the settings in [225], we evaluate on the Chest X-ray 2017 test set and the 880 annotated data in Chest X-ray 14. Ultrasound Image. US-Case [173] and MMUS1K [152]. The US-Case collection comprises over 7,000 sonographic images spanning organs such as the liver, heart, and mediastinum. Adopting the selection protocol from [152], we reused the subset of 111 images in the test set for benchmarking, excluding 11 scans whose small field of view limited their diagnostic value. MMUS1K features 1,023 anonymized multi-organ ultrasound scans, including bladder, gallbladder, thyroid, kidney, etc., sourced from Shanghai Tenth Peoples Hospital. All images meet minimum resolution of 448600 px and were cleansed of watermarks and blurring artifacts via LabelImg. The test set with label numbers from 0801 to 0900 was used for evaluation. Fundoscopy Image. DRIVE [174] consists of 40 color fundus images from diabetic retinopathy screening program in the Netherlands collected by Canon CR5 non-mydriatic 3CCD camera. The dataset is equally divided into 20 for training and 20 for testing. As all images are in 584565 resolution, following the setup in [5], the original high-resolution (HR) images were resized to 512512 and before being further utilized to generate LR pairs. To consist with baseline methods for each dataset, the LR input images were generated by downsampling HR images via bicubic interpolation. For X-ray images, we use SSIM, FSIM [256], and MSIM [205] as metrics, while PSNR and SSIM are used for Ultrasound and Fundoscopy images. Table 20: Quantitative comparison on X-ray datasets. The top three performances of each metric are marked in bold, underline, italic respectively."
        },
        {
            "title": "Method",
            "content": "Chest X-ray 2017 Chest X-ray 14 SSIM FSIM MSIM SSIM FSIM MSIM Nearest-Neighbor Interpolation [226] CTF [253] ESPCN [170] FSRCNN [47] LapSRN [94] SRGAN [96] GAN-CIRCLE [235] SNSRGAN [225] 0.637 0.615 0. 0.756 0.897 0.893 0.821 0.897 0. 4KAgent (ExpSR-s4-F) 0.933 0.672 0.663 0.933 0. 0.943 0.942 0.896 0.947 0.981 0. 0.668 0.644 0.954 0.804 0.953 0. 0.868 0.923 0.983 0.987 0.701 0. 0.917 0.795 0.917 0.915 0.844 0. 0.925 0.960 0.724 0.698 0.955 0. 0.959 0.956 0.903 0.969 0.995 0. 0.713 0.681 0.943 0.815 0.953 0. 0.897 0.945 0.986 0.993 Quantitative Comparison X-ray Quantitative results are summarized in Tab. 20. Ultrasound Quantitative results are summarized in Tab. 21. Fundoscopy Quantitative results are summarized in Tab. 22, which collectively demonstrate 4KAgents consistently superior performance across all three distinct medical imaging modalities. On the X-ray datasets, 4KAgent with Fidelity profile surpasses the specialized SNSRGAN [225] model across all structure-focused metrics. For Ultrasound imaging, it also achieves significant performance leap, boosting the PSNR on the MMUS1K dataset by nearly 3 dB over the previous state-of-the-art, M2Trans [152]. Similarly, on the DRIVE Fundoscopy dataset, 4KAgent again sets new performance benchmark, improving the PSNR from 37.72 to 41.52 and the SSIM from 0.91 to 0.95. This consistent outperformance across modalities, from the need for pathological invariance in X-rays to the clarity of fine vessels in fundoscopy, highlights the effectiveness and robustness of 4KAgent for diverse medical SR tasks. Qualitative Comparison Representative qualitative results for X-Ray SR, ultrasound SR, and fundoscopy SR are shown in Figs. 19 to 21. The visual outcomes generally align with our quantitative findings and suggest the potential benefits of 4KAgent for clinical imaging applications. For X-ray super-resolution, where maintaining pathological invariance is important, 4KAgent produces reconstructions with improved delineation of lung parenchyma and clearer visibility of rib cage contours. It appears to achieve this clarity while reducing some of the oversmoothing artifacts occasionally seen in other SR methods, thus helping to preserve diagnostic details that are crucial for identifying pulmonary abnormalities with greater confidence. In the ultrasound comparisons, 4KAgent shows notable advantage. On the US-CASE example, it restores clearer tissue boundaries and more internal detail compared to the blurrier reconstruction from the M2Trans baseline. Similarly, for the MMUS1K image, 4KAgent appears to reduce speckle noise while enhancing anatomical definition, whereas the baseline result is affected by some noise and artifacts. In both cases, 4KAgent generates echogenic patterns that more closely resemble the ground truth, improving overall image fidelity. Table 21: Quantitative comparison on Ultrasound dataset. The top three performances of each metric are marked in bold, underline, italic respectively."
        },
        {
            "title": "Method",
            "content": "US-CASE MMUS1K PSNR SSIM PSNR SSIM Bicubic 28.90 0. 28.24 0.7817 EDSR [119] 30.82 0.8497 30. 0.8326 SwinIR [117] 28.50 0.7834 27.66 0. ELAN [258] 31.02 0.8464 30.40 0.8309 ESRT [131] 30.84 0.8374 30.25 0.8235 HAT [30] 28. 0.7812 28.08 0.7582 M2Trans [152] 31.32 0. 30.68 0.8392 4KAgent (ExpSR-s4-F) 33.27 0.8895 33. 0.8678 Table 22: Quantitative comparison on Fundoscopy dataset. The top three performances of each metric are marked in bold, underline, italic respectively. Dataset Method PSNR SSIM"
        },
        {
            "title": "DRIVE",
            "content": "SRGAN [96] Ahmad et al. [5] 25.20 34.22 37.72 4KAgent (ExpSR-s4-F) 41.52 0.86 0.88 0.91 0.95 The fundoscopy results demonstrate 4KAgents effectiveness in restoring details from degraded inputs. Compared to the LR image, 4KAgents reconstruction of the retinal vascular network shows clear improvement. The method produces sharper and more continuous vessels, resolving many of the fine micro-vessels and bifurcation points that are obscured in the LR version. The resulting image more closely resembles the HR ground truth, suggesting its potential to aid in retinopathy screening from lower-resolution captures without sacrificing critical diagnostic details. Figure 19: Visual comparison of X-Ray image SR on Chest X-ray 2017 and Chest X-ray 14 dataset. Discussions From both quantitative and qualitative perspectives, our evaluation suggests that 4KAgent is capable system for cross-domain super-resolution across diverse clinical imaging modalities, showing competitive performance on X-ray, ultrasound, and fundoscopy datasets. This result is notable, as the prevailing approach often involves developing specialized models for each modality, paradigm that can be limited by poor generalization across different scanners and protocols. By not relying on domain-specific training, 4KAgents agentic framework offers flexible alternative, 37 Figure 20: Visual comparison of Ultrasound image SR on US-CASE and MMUS1K dataset. Figure 21: Visual comparison of fundoscopy image SR on DRIVE dataset (128512). adaptively deploying its tools to address the unique challenges of each image, from enhancing the clarity of lung markings in chest radiographs to defining subtle echogenic interfaces in ultrasound and resolving fine vascular networks in fundoscopy. The ability to generate reconstructions with improved structural detail may have implications for downstream applications. For example, improved sharpness in retinal vessels could aid in retinopathy screening; clearer ultrasound images help with tissue boundary delineation for segmentation; and more detailed X-rays could enhance the visibility of subtle pulmonary abnormalities. Ultimately, the performance of our agentic approach indicates its significant potential for robust deployment across more real-world clinical workflows and imaging modalities, driven by its adaptability and inherent extensibility for incorporating more specialized medical profiles."
        },
        {
            "title": "9 Ablation Studies",
            "content": "In this section, we conduct ablation studies on core components in 4KAgent system: (1) Q-MoE policy and (2) Face restoration pipeline. Then, we perform running time analysis of 4KAgent. Q-MoE policy. To assess the contribution of our Q-MoE mechanism during execution and reflection, we perform an ablation study in which Q-MoE is replaced by the DFS strategy from AgenticIR 38 [269], denoting this variant as 4KAgent (DFS). Experiments are conducted on the MiO-100 Group dataset under the multiple-degradation image restoration setting. As shown in Tab. 23, integrating Q-MoE leads to substantial improvements in perceptual quality. Specifically, metrics such as LPIPS, MANIQA, CLIPIQA, and MUSIQ exhibit significant gains, with minimal impact on fidelity metrics like PSNR and SSIM. Furthermore, the visual comparisons presented in Fig. 22 provide additional evidence, showing that 4KAgent equipped with Q-MoE generates noticeably sharper and more realistic details compared to the DFS-based variant. Table 23: Ablation study on Q-MoE policy. The better performance is marked in bold."
        },
        {
            "title": "Method",
            "content": "PSNR SSIM LPIPS MANIQA CLIPIQA MUSIQ MiO100 - Group 4KAgent (DFS) 4KAgent (Q-MoE) 19.81 19.77 0.5785 0.5629 0.4381 0. 0.3286 0.3545 0.4854 0.5233 54.03 55.56 Figure 22: Visual comparisons for ablation study on Q-MoE. Face restoration pipeline. To evaluate the impact of our Face Restoration Pipeline, we conduct an ablation study on the WebPhoto-test dataset using three profiles: ExpSR-s4-P, ExpSRFR-s4-P, and GenSRFR-s4-P. Experiment result and the difference among these profiles are shown in Tab. 24. Enabling the face restoration module (i.e., switching profile from ExpSR-s4-P to ExpSRFR-s4-P and GenSRFR-s4-P) yields higher face IQA scores (CLIB-FIQA and DSL-FIQA). Moreover, when setting the Restore Option to None rather than super-resolution, we observe further improvements across both generic image IQA metrics (NIQE and MUSIQ) and face IQA metrics. Table 24: Ablation study on face restoration pipeline on WebPhoto-Test dataset. The best and second-best performances are marked in bold and underline respectively. Method Restore Option Face Restore NIQE CLIPIQA MUSIQ MANIQA CLIB-FIQA DSL-FIQA 4KAgent (ExpSR-s4-P) 4KAgent (ExpSRFR-s4-P) 4KAgent (GenSRFR-s4-P) super-resolution super-resolution None"
        },
        {
            "title": "False\nTrue\nTrue",
            "content": "5.11 4.53 4.15 0.7119 0.6600 0.7077 73.62 72.89 75.92 0.6601 0.6405 0.6576 0.6415 0.6602 0.6671 0.7194 0.7237 0. Visual comparisons are shown in Fig. 23. 4KAgent with GenSRFR-s4-P profile produces the finest facial details (hair details, harmony of facial area and background area). This trend indicates that WebPhoto-test images suffer from complex, mixed degradations and 4KAgent benefits from integrating multiple restoration tasks with Q-MoE driven selection to achieve superior visual quality. Figure 23: Visual comparisons for ablation study on face restoration pipeline. 39 Running Time Analysis. The inference time of 4KAgent varies depending on the selected profile, the quality of the input image, and the length of the restoration plan. In this section, we analyze the inference time of 4KAgent using NVIDIA RTX 4090 GPUs. Specifically, we report the fastest and slowest cases observed in our experiments. The fastest case involves super-resolving images (4) from the B100 dataset using the ExpSR-s4-F profile. The slowest case corresponds to jointly restoring and upscaling low-quality images from the DIV4K-50 dataset to 4K resolution under the Gen4K-P profile. The inference times for these two cases are summarized in Tab. 25. Table 25: Inference time of 4KAgent (fastest and slowest cases on our experiments)."
        },
        {
            "title": "Benchmark Length of Plan",
            "content": "Inference Time (s) ExpSR-s4-F Super-resolution (4) 120 80 480 320 B100 1.0 0. 50.96 2.01 Gen4K-P Joint restoration + 4K Upscaling 256 256 4096 4096 DIV4K-50 3.4 0.6 1551.76 230. As 4KAgent currently executes its tools sequentially, there is substantial potential for acceleration, for example, by running independent restoration tools in parallel at each step."
        },
        {
            "title": "10.1 Applications",
            "content": "High-resolution On-Demand Media Streaming 4KAgent offers significant potential for enabling network operators, such as YouTube, Netflix, Instagram, Amazon Prime Video, TikTok, Kwai, Snap, Twitch, to name few, to deliver 4K-quality video services from much lower-bitrate streams. For example, edge-based SR can upscale 1K stream to 4K at the users device [245], allowing providers to store and transmit mostly lower-resolution content (e.g., 1K) which can then be upscaled to 4K quality on the end-users device using edge-based processing. This approach dramatically cuts storage and bandwidth costs (and even energy use) compared to naïvely streaming native 4K [106]. Technologies like NVIDIAs Deep Learning Super Sampling (DLSS) [1] demonstrate the feasibility and usability of real-time super-resolution on GPU chips. Integrating such real-time upscaling into adaptive streaming protocols could also improve user experience by minimizing disruptive quality shifts often associated with variable network conditions, ensuring viewers consistently receive high-resolution playback on capable displays. Video Conferencing and Telepresence Network bandwidth constraints and limitations inherent in typical webcams or smartphone cameras often necessitate transmitting video streams at resolutions lower than 4K. Implementing SR algorithms, such as 4KAgent, on the receivers end can effectively upscale these lower-resolution feeds. This process restores fine-grained details in facial features or gestures that might otherwise be lost, thereby enhancing the perceived visual quality and potentially aiding communication cues like lip-reading or the interpretation of subtle expressions [110, 149, 220]. Consequently, even devices with modest camera capabilities can deliver an experience approximating 4K quality to the viewer, without requiring increased upload bandwidth from the sender. This democratization of high-resolution video conferencing can improve remote collaboration, making it more accessible and effective for users constrained by network limitations or hardware capabilities. Surveillance and Security Image SR technologies like 4KAgent (with fidelity-based profile) offer significant value in enhancing footage from law enforcement operations, particularly from body-worn cameras and dashcams. These devices often capture video at resolutions like 720p or 1080p with wide fields of view, resulting in low-detail imagery, especially in challenging conditions such as low light [107]. Faces or license plates captured at distance may span only few dozen pixels, far below recommended thresholds for identification (e.g., 9060 pixels per face for courtroom evidence [2]). The quality is often further compromised by heavy compression and sensor limitations, introducing noise and motion blur. Modern SR approaches, particularly blind methods that model complex real-world degradations, can effectively mitigate these issues and restore detail in practical bodycam footage. By enhancing critical regions (faces, license plates) in police videos, SR can improve both human and automated identification, while preserving the veracity required for judicial use. 40 Similarly, public surveillance systems, including city-wide CCTV networks, border security cameras, and transit hub monitoring, face comparable challenges related to resolution and image quality. Fixed cameras covering wide areas often render persons or objects of interest with very low pixel counts, with quality impacted by distance, illumination, camera motion, and aggressive compression techniques employed to manage bandwidth and storage [150]. SR provides means to enhance detail retroactively without costly hardware upgrades. Field studies have also reported the effectiveness of SR. For example, National Institute of Justice study [3] showed that multi-resolution SR could reconstruct identifiable features from extremely low-resolution facial images comparable to those from real-world security cameras. Overall, SR can act as force multiplier for legacy surveillance infrastructure, enhancing situational awareness and forensic capabilities. However, the enhanced capability for identification also raises potential privacy concerns, which will be discussed in Sec. 10.3. Gaming and Entertainment SR techniques are extensively utilized in the entertainment sector to enhance visual quality while sustaining high frame rates, particularly in demanding gaming, VR, and AR applications. prominent example is NVIDIAs DLSS, suite of AI-powered neural rendering techniques that upscale lower-resolution frames to higher target resolutions, as high as 4K. DLSS can significantly improve performance, often more than doubling GPU throughput and leading to substantial frame rate increasesfor instance, one report indicated boost of up to approximately 360% (e.g., from 8 to 36.8 fps on an RTX 2060 at 4K). Successive iterations like DLSS 3 with Frame Generation and DLSS 3.5 with Ray Reconstruction have introduced further advancements by using AI to generate additional frames or improve ray-traced effects. VR, AR, and XR This need for efficient, high-quality rendering extends critically to the domain of spatial intelligence and computing, as seen in advanced devices like the Apple Vision Pro, which aims to deliver experiences with more pixels than 4K TV per eye. While such platforms boast high native display resolutions, SR techniques could play crucial role in rendering complex mixed-reality scenes or high-fidelity passthrough video efficiently, maintaining visual clarity without overwhelming the processing capabilities. Similarly, as smart glasses like the Ray-Ban Meta Wayfarer evolve and potentially incorporate more advanced display capabilities for augmented reality overlays, SR will be key to delivering crisp digital information without excessive battery drain. Broader XR initiatives, such as Googles development of Android XR, also stand to benefit from robust SR solutions to enable diverse ecosystem of devices to achieve compelling visual experiences. For all these platforms, from gaming consoles to sophisticated XR headsets and smart glasses, the ability of SR systems like 4KAgent to adaptively enhance visual quality from various inputs will be paramount in balancing immersive, high-resolution experiences with practical performance and power constraints. AI-Generated Content (AIGC) Production Industry Photographers, digital artists, and filmmakers increasingly leverage SR tools to enlarge, restore, and enhance the quality of both conventional (e.g., old photos, archival digital footage) and even AI-generated images and video footage. We have demonstrated in Sec. 7.1 that 4KAgent is capable of synthesizing high-fideltiy details in generated media, coinciding with recent trend that generates high-resolution advertisement for KFC2, leveraging outputs from generative video models such as Googles Veo [188], Luma AIs Dream Machine [132], and OpenAIs Sora [153], further enhanced using Topaz Labs Video Upscaler to achieve higher resolutions (e.g., 4K) and professional quality suitable for broader use. SR techniques are crucial for bridging this gap towards generating ultra-high-resolution content, enabling creators to enhance these AI-generated visuals. For instance, images generated for concept art, marketing materials, or virtual environments can be significantly improved in detail and clarity through SR, making them suitable for 4K displays or large-format printing. Similarly, generative video content, which might be created at lower resolutions to manage computational costs, can be upscaled using specialized tools like Topaz Video AI to achieve crisper, higher-resolution results (e.g., 4K) ready for distribution or integration into larger productions. State-of-the-art SR methods, including GAN-based approaches, can synthesize photorealistic details, effectively transforming AIGC outputs into polished, professional-grade assets. The ability of robust and adaptive SR solutions like 4KAgent to handle the diverse and sometimes unpredictable nature of AIGC makes them particularly valuable for ensuring that AI-driven creative endeavors can meet high-quality benchmarks. 2https://x.com/Wesley_Kibande/status/1908091178723029193 41 Scientific Imaging High-resolution imagery is crucial across numerous scientific disciplines, particularly when native sensor capabilities are constrained. In remote sensing, deep super-resolution (SR) methods significantly enhance spatial details of satellite imagery, facilitating accurate land-use classification and environmental monitoring [151, 169]. For instance, self-supervised SR techniques trained on sequences of satellite images yield sharper and less noisy results compared to raw captures, substantially improving downstream geospatial analysis [151]. Microscopy and biomedical imaging similarly benefit from SR, particularly through novel quantum imaging techniques. Recent advancements by Zhang et al. and He et al. leverage quantum entanglement to achieve unprecedented imaging resolution, demonstrating quantum microscopy at the Heisenberg limit and significantly enhancing cellular and sub-cellular visualization [65, 260]. Additionally, computational SR methods applied to microscopy, like content-aware restoration for fluorescence images [210], complement these quantum techniques by computationally reconstructing detailed 3D biological structures from limited optical inputs. Thus, versatile and advanced SR frameworks such as 4KAgent, coupled with emerging quantum imaging methods, can revolutionize scientific research by providing richer, more precise imagery across multiple imaging modalities. Medical Image Applications In medical imaging, SR facilitates detailed diagnostics by transforming low-dose or rapidly acquired imaging scans into high-fidelity medical images. Techniques employing deep learning-based SR on modalities such as Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) have shown promise in generating accurate, high-resolution images from suboptimal inputs, thus reducing patient radiation exposure and acquisition time without sacrificing diagnostic quality [31, 264]. For instance, generative adversarial networks (GANs) and transformerbased SR approaches like TransMRI demonstrate substantial improvements in enhancing anatomical details critical for diagnostic accuracy [51]. Consequently, methods like 4KAgent, which provide universal super-resolution capabilities, can significantly impact clinical diagnostics by offering highly detailed and diagnostically reliable imagery from resource-efficient imaging procedures. Embodied AI and Robotics Embodied AI systems, including robotics platforms, leverage SR to enhance visual perception, critical for tasks such as navigation, object manipulation, and human-robot interaction. Robotic visual systems frequently face limitations in sensor resolution and onboard processing capacity, challenges that SR methods can effectively address. Recent studies indicate that integrating SR into robotic vision pipelines notably improves object detection and localization, particularly for distant or small-scale objects critical in dynamic environments [138]. Furthermore, real-time lightweight SR models tailored for robotic platforms have been developed, improving perception accuracy and enabling robots to perform complex tasks efficiently, such as precise grasping and navigation through cluttered or visually challenging scenarios [7, 75]. Consequently, robust SR algorithms substantially advance robotic autonomy and operational effectiveness. Autonomous Vehicles, Drones, and Intelligent Transportation Systems (ITS) Autonomous vehicles, aerial drones, and intelligent transportation systems increasingly depend on high-quality visual inputs for safe and efficient operation. SR technologies significantly boost the capability of onboard cameras to discern critical details from lower-resolution captures under real-world constraints. For instance, SR-enhanced imagery improves the detection accuracy of pedestrian, vehicle, and traffic sign recognition systems, crucial for autonomous navigation and safety-critical decisionmaking [125, 147, 180]. Research demonstrates that training object detection models with superresolved images significantly enhances their effectiveness in challenging scenarios, including poor visibility or long-range object detection from drones or vehicle-mounted cameras [147]. Furthermore, ITS can employ SR-enhanced camera networks for robust and precise traffic monitoring and anomaly detection, improving urban mobility management and public safety. GIScience and GeoAI Geographic Information Science (GIScience) heavily utilizes spatially explicit, high-resolution data for mapping, analysis, and policy-making. Deep SR has recently emerged as powerful method for enhancing geospatial imagery resolution, substantially improving the interpretability and accuracy of satellite-derived GIS datasets. For example, SR methods significantly improve the extraction accuracy of buildings, roads, vegetation, and other geographic features from mediumto low-resolution imagery, providing cost-effective alternative to deploying expensive high-resolution satellite sensors [161, 169]. Additionally, high-quality SR outputs are vital for applications such as precision agriculture and disaster response planning, demonstrating the broad utility of universal SR frameworks like 4KAgent within GIScience research and applications."
        },
        {
            "title": "10.2 Broader Impacts",
            "content": "Economic Impacts. SR technologies, exemplified by 4KAgent, drive significant economic advantages by enhancing operational efficiency, creating new markets, and supporting environmental sustainability. By reducing bandwidth, storage, and infrastructure costs associated with high-resolution content delivery, SR solutions enable economical, high-quality media dissemination over constrained networks, benefiting digital platforms and smaller businesses [114, 246]. Companies such as BytePlus and Maxar Intelligence have successfully leveraged SR technologies to open new markets in healthcare diagnostics, geospatial intelligence, and media restoration [16, 140]. Additionally, by minimizing data storage and transmission demands, SR contributes meaningfully to the environmental goals of reduced energy consumption and lower carbon emissions [99]. Accessibility. SR substantially promotes digital equity by enabling high-quality visual content access for users in regions with limited bandwidth or resource constraints without necessitating advanced infrastructure or expensive devices [159]. Particularly in education and healthcare, SR supports remote learning and telemedicine by delivering clearer instructional and diagnostic imagery, significantly benefiting underserved communities [15]. Moreover, SR-integrated assistive technologies offer improved accessibility for individuals with visual impairments by enhancing image clarity and text readability, facilitating greater inclusion and interaction within the digital sphere [118, 189]. Vertical Impacts to Industry. Demand for real-time, high-quality SR capabilities stimulates technological progress across various industries, including media, robotics, autonomous systems, and scientific visualization. SR advancement drives innovation in specialized neural hardware, edge computing solutions, and embedded AI, spurring the development of powerful and efficient Neural Processing Units (NPUs) in consumer devices [18, 97]. Additionally, SR techniques significantly enhance autonomous vehicle, drone, and robotic platform capabilities by improving object detection accuracy, scene understanding, and decision-making reliability, particularly in challenging operational environments [147, 169]. These impacts extend to scientific instrumentation, such as microscopy and geospatial imaging, where SR enables unprecedented detail and precision [151, 210]."
        },
        {
            "title": "10.3 Limitations and Potential Negative Societal Impacts",
            "content": "Efficiency and Computational Cost. SR methods, particularly deep learning-based and agentic frameworks like 4KAgent, often require substantial computational resources for training and inference. High-resolution SR models usually rely on resource-intensive GPU or TPU clusters, imposing significant energy consumption [171]. Even optimized inference can become computationally burdensome at the edge, potentially limiting deployment on low-powered or mobile devices unless significant model compression and acceleration techniques are applied [97, 259]. Balancing performance and efficiency remains critical open challenge, particularly for real-time applications in resource-constrained environments. Bias, Fairness, and Model Drift. Data-driven SR models inherit biases from their training datasets, potentially leading to uneven quality across different image categories, demographics, or scenarios [54, 141]. Such biases might systematically disadvantage specific groups, for instance, by inadequately resolving images related to underrepresented populations or environments. Model drift over timewhere the model gradually becomes less accurate due to changing real-world distributionsalso poses serious issue for practical deployment, requiring continuous monitoring and recalibration to ensure fairness and reliability [52, 130]. Ethical Issues and Privacy. Enhanced imaging capabilities enabled by SR, particularly in surveillance contexts, can amplify privacy risks. The ability to recover detailed features such as faces or license plates from previously anonymized or low-resolution imagery might lead to unauthorized or unethical identification of individuals [62]. This capability necessitates clear regulatory guidelines and ethical oversight to avoid misuse [113]. Concerns are especially pronounced in contexts of law enforcement, border surveillance, and public monitoring, where SR technologies must be carefully governed to prevent potential violations of civil liberties and personal privacy [38]. Failure Modes in High-stake Settings. The adoption of SR techniques in high-stakes environments, including medical diagnostics, autonomous vehicles, and security monitoring, introduces 43 risks associated with model hallucinations or misleading image reconstructions [35]. SR models, particularly generative approaches, may produce plausible yet incorrect details absent from the original low-resolution inputs, potentially leading to erroneous interpretations or decisions [8]. In clinical settings, for example, SR-generated artifacts could result in incorrect diagnoses or overlooked medical conditions, underscoring the importance of rigorous validation and transparency regarding model uncertainty and reliability [83]."
        },
        {
            "title": "11 Related Works",
            "content": "11.1 Image Super-Resolution Deep learning has significantly advanced the field of single-image Super-Resolution (SR). The seminal work, SRCNN [45], introduced convolutional net for SR, with primary focus on optimizing the Mean Squared Error between the super-resolved and high-resolution images. Following this, numerous studies have enhanced reconstruction accuracy by improving network architectures, including residual and dense connections [86, 119, 262], attention mechanisms [19, 41, 261], and multi-scale networks [53, 108]. While these methods perform well in modeling the posterior distribution of the training data, they inevitably suffer from the issue of overly smooth visual results [96, 145, 221]. In recent years, significant efforts have been made to develop generative model-based SISR techniques that produce more visually appealing results. These include autoregressive models [40, 144, 187], GAN-based models [21, 96, 103, 116, 197, 251], and diffusion-based models [121, 166, 192, 201, 211, 212, 229]. SRGAN [96], as pioneering GAN-based SR model, assumes image degradation through bicubic downsampling and generates photo-realistic images. BSRGAN [251] and Real-ESRGAN [197] achieve promising real-world SR results by using randomly shuffled degradation and higher-order degradation. SwinIR [117] replaces the CNN-based generator network with visual transformers, leading to more stable training and more realistic textures. Additionally, SeD [103] introduces semantic-aware discriminator to capture fine-grained distributions by incorporating image semantics as condition. Recent diffusion-based models have focused on fine-tuning the Stable Diffusion model for reconstructing high-quality images, using low-quality images as control signals. Notably, StableSR [192] fine-tunes time-aware encoder and employs feature warping to balance fidelity and perceptual quality. SeeSR [212] introduces degradation-robust, tag-style text prompts to enhance the semantic awareness of the Real-ISR model. Furthermore, recent studies on diffusion-based models, such as SinSR [201], OSEDiff [211], PiSA-SR [176], and GuideSR [9] achieve one-step image super-resolution. 11."
        },
        {
            "title": "Image Restoration",
            "content": "Recent advances in deep learning have led to remarkable progress in blind image restoration tasks, including denoising, deblurring, deraining, dehazing, and removal of JPEG compression artifacts. Early works such as ARCNN [44] demonstrated the potential of compact convolutional neural networks, particularly in the context of image denoising. Since then, broad range of sophisticated network architectures and training strategies have been developed to further enhance restoration performance. These include the use of residual blocks [128, 250, 252], attention mechanisms [25, 57, 183, 237, 242], and Transformer-based designs [186, 202, 241, 270]; as well as generative paradigms such as GANs [11, 26, 56, 58, 73, 126, 145, 156] and diffusion models [50, 80, 81, 121, 200, 216]. Notably, general-purpose restoration models like Uformer [202], MAXIM [183], Restormer [241], and NAFNet [25] have demonstrated strong performance across diverse restoration tasks, often trained independently for each specific degradation type. However, such single-degradation methods often struggle in real-world scenarios where multiple types of degradations co-exist. This limitation has sparked growing interest in the emerging field of All-in-One image restoration, which aims to build unified models capable of handling wide range of degradations with single network [105, 135, 157, 186, 231, 247]. For instance, AirNet [105] introduces degradation classifier trained via contrastive learning to guide restoration, while ADMS [157] employs multi-type degradation classifier to dynamically select Adaptive Discriminant filters, enabling degradation-specific parameter modulation within the restoration network."
        },
        {
            "title": "11.3 LLM Agents",
            "content": "Advancements in LLM-based frameworks have enabled more structured reasoning and agent designs, particularly for complex multimodal tasks. Initial efforts emphasized improving reasoning capabilities through refined prompting strategies and modular architecture. Chain-of-Thought (CoT) prompting [208] introduced stepwise reasoning, facilitating decomposition and interpretability across diverse tasks. ReAct [232] combined reasoning with tool interaction by interleaving thought traces and external actions, supporting more adaptive behavior. Extending this direction, CoALA [175] formalized components such as memory, reasoning, and control within cognitive architecture, offering modular design space for building general-purpose language agents. These developments established basis for domain-specific agent systems with integrated reasoning pipelines. Building on these foundations, application-driven LLM agents have been developed to incorporate tool use and dynamic decision-making within specialized domains. In vision tasks, MMCTAgent [92], VideoAgent [194], and ReAgent-V [268] implement planning and evaluation pipelines for image and video analysis, incorporating external modules for retrieval and verification. In the medical domain, agents such as MedCoT [127], CLINICR [148], and MMedAgent-RL [219] employ hierarchical reasoning frameworks to address clinical questions, integrating structured logic and domain-specific knowledge to enhance interpretability and decision quality. Similarly, LLM-based agents have also emerged as promising paradigm for tackling complex image restoration tasks involving multiple degradations. RestoreAgent [22] pioneered the use of MLLMs for autonomous task identification, model selection, and execution planning. AgenticIR [269] introduced five-stage human-inspired workflowPerception, Scheduling, Execution, Reflection, and Reschedulingaugmented with self-exploration to build IR-specific experience. MAIR [79] advanced this by employing multi-agent system guided by real-world degradation priors, improving both efficiency and scalability. HybridAgent [102] proposed hybrid interaction scheme with fast and slow agents, along with mixed-distortion removal strategy to mitigate error propagation. QAgent [267] further introduces quality-driven chain-of-thought framework, leveraging no-reference IQA metrics to guide greedy restoration without costly rollbacks. These works demonstrate the growing potential of combining general-purpose language intelligence with visual tools for robust, adaptive image restoration. More recently, JarvisIR [122] and JarvisArt [123] leveraged intelligent agent workflows to perform task-oriented image restoration and creative photo retouching."
        },
        {
            "title": "12 Concluding Remarks",
            "content": "In this paper, we introduced 4KAgent, versatile agentic image super-resolution generalist model designed to universally upscale images of diverse types and degradation levels to 4K resolution. By leveraging advanced multi-expert integration, adaptive decision-making, and specialized tools for perception and fidelity optimization, 4KAgent significantly enhances restoration quality across various challenging domains, including severely degraded images, natural scenes, portraits, AIgenerated content, and specialized scientific modalities such as remote sensing, microscopy, and medical imaging. Our extensive evaluations on standard benchmarks and specialized datasets confirm that 4KAgent consistently outperforms existing state-of-the-art approaches, especially in complex scenarios where conventional super-resolution methods fall short. This robust performance, achieved without domain-specific retraining, demonstrates the models unique generalizability and practical utility for generic deployment in both consumer, commercial, and scientific applications. Future Work Looking ahead, we have identified several promising directions that can further enhance the capabilities and applicability of 4KAgent, enabling broader use cases. First, we will optimize the efficiency of 4KAgent by designing more accurate distortion-perception models and refining the execution-reflection-rollback mechanism to achieve faster and higher-quality image restoration. Second, we will prioritize enhancing the safety and robustness of 4KAgent to mitigate risks such as privacy breaches and the generation of harmful imagery. Lastly, we will continuously expand the toolbox of 4KAgent by integrating additional domain-specific restoration methods and developing targeted restoration profiles, thus significantly improving performance and user experience across specialized imaging applications."
        },
        {
            "title": "References",
            "content": "[1] NVIDIA DLSS 4 Supreme Speed. Superior Visuals. Powered by AI. https://www.nvidia.com/ en-us/geforce/technologies/dlss/. 40 [2] Video quality in public safety (VQiPS) workshop report. https://www.nist.gov/system/ files/documents/2016/10/06/final_vqipsworkshopreport_092912.pdf. 40 [3] Ramzi Abiantun, Felix Juefei-Xu, Utsav Prabhu, and Marios Savvides. Ssr2: Sparse signal recovery for single-image super-resolution on faces with extreme low resolutions. Pattern Recognition, 90:308324, 2019. 41 [4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 9 [5] Waqar Ahmad, Hazrat Ali, Zubair Shah, and Shoaib Azmat. new generative adversarial network for medical images super resolution. Scientific Reports, 12(1):9533, 2022. 36, 37 [6] Meta AI. Llama 3.2-vision. https://huggingface.co/meta-llama/Llama-3.2-11BVision-Instruct, 2024. 8, 9 [7] Simone Angarano, Francesco Salvetti, Mauro Martini, and Marcello Chiaberge. Generative adversarial super-resolution at the edge with knowledge distillation. Engineering Applications of Artificial Intelligence, 123:106407, 2023. 42 [8] Vegard Antun, Francesco Renna, Clarice Poon, Ben Adcock, and Anders Hansen. On instabilities of deep learning in image reconstruction and the potential costs of ai. Proceedings of the National Academy of Sciences, 117(48):3008830095, 2020. 44 [9] Aditya Arora, Zhengzhong Tu, Yufei Wang, Ruizheng Bai, Jian Wang, and Sizhuo Ma. Guidesr: Rethinking guidance for one-step high-fidelity diffusion-based super-resolution. arXiv preprint arXiv:2505.00687, 2025. [10] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 9 [11] David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba. Semantic photo manipulation with generative image prior. arXiv preprint arXiv:2005.07727, 2020. 44 [12] Soufiane Belharbi, Mara Whitford, Phuong Hoang, Shakeeb Murtaza, Luke McCaffrey, and Eric Granger. Sr-caco-2: dataset for confocal fluorescence microscopy image super-resolution. Advances in Neural Information Processing Systems, 37:5994859983, 2024. 11, 31 [13] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. 2012. [14] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff."
        },
        {
            "title": "In Proceedings of the IEEE",
            "content": "conference on computer vision and pattern recognition, pages 62286237, 2018. 8 [15] BytePlus. Business growth through superior technology. https://www.byteplus.com, 2025. 43 [16] BytePlus. Unleashing the power of super resolution: game-changer for visual content. https: //www.byteplus.com/en/topic/96403, 2025. 43 [17] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: new benchmark and new model. In Proceedings of the IEEE/CVF international conference on computer vision, pages 30863095, 2019. 11, 14 [18] Jung-Woo Chang, Keon-Woo Kang, and Suk-Ju Kang. An energy-efficient fpga-based deconvolutional neural networks accelerator for single image super-resolution. IEEE Transactions on Circuits and Systems for Video Technology, 30(1):281295, 2018. [19] Chaofeng Chen, Dihong Gong, Hao Wang, Zhifeng Li, and Kwan-Yee Wong. Learning spatial attention for face super-resolution. IEEE Transactions on Image Processing, 30:12191231, 2020. 44 [20] Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Topiq: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 2024. 6 46 [21] Chaofeng Chen, Xinyu Shi, Yipeng Qin, Xiaoming Li, Xiaoguang Han, Tao Yang, and Shihui Guo. Realworld blind super-resolution via feature matching with implicit high-resolution priors. In Proceedings of the 30th ACM International Conference on Multimedia, pages 13291338, 2022. 4, 44 [22] Haoyu Chen, Wenbo Li, Jinjin Gu, Jingjing Ren, Sixiang Chen, Tian Ye, Renjing Pei, Kaiwen Zhou, Fenglong Song, and Lei Zhu. Restoreagent: Autonomous image restoration agent via multimodal large language models. arXiv preprint arXiv:2407.18035, 2024. 4, [23] Haoyu Chen, Wenbo Li, Jinjin Gu, Jingjing Ren, Haoze Sun, Xueyi Zou, Zhensong Zhang, Youliang Yan, and Lei Zhu. Low-res leads the way: Improving generalization for super-resolution by self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2585725867, 2024. 4 [24] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. 22 [25] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image restoration. In European conference on computer vision, pages 1733. Springer, 2022. 9, 44 [26] Shengjie Chen, Shuo Chen, Zhenhua Guo, and Yushen Zuo. Low-resolution palmprint image denoising by generative adversarial networks. Neurocomputing, 358:275284, 2019. [27] Wei-Ting Chen, Gurunandan Krishnan, Qiang Gao, Sy-Yen Kuo, Sizhou Ma, and Jian Wang. Dsl-fiqa: Assessing facial image quality via dual-set degradation learning and landmark-guided transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29312941, 2024. 17 [28] Xiangyu Chen, Zheyuan Li, Yuandong Pu, Yihao Liu, Jiantao Zhou, Yu Qiao, and Chao Dong. comparative study of image restoration networks for general backbone network design. In European Conference on Computer Vision, pages 7491. Springer, 2024. 9, 11, 12, 13 [29] Xiangyu Chen, Xintao Wang, Wenlong Zhang, Xiangtao Kong, Yu Qiao, Jiantao Zhou, and Chao Dong. Hat: Hybrid attention transformer for image restoration. arXiv preprint arXiv:2309.05239, 2023. 9, 11, 12, 13, 18, 19, 20, 25, 26, 27 [30] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Activating more pixels in image super-resolution transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2236722377, 2023. 37 [31] Yuhua Chen, Yibin Xie, Zhengwei Zhou, Feng Shi, Anthony Christodoulou, and Debiao Li. Brain mri super resolution using 3d deep densely connected neural networks. In 2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018), pages 739742. IEEE, 2018. 42 [32] Zhen Chen, Xiaoqing Guo, Chen Yang, Bulat Ibragimov, and Yixuan Yuan. Joint spatial-wavelet dualstream network for super-resolution. In Medical Image Computing and Computer Assisted Intervention MICCAI 2020: 23rd International Conference, Lima, Peru, October 48, 2020, Proceedings, Part 23, pages 184193. Springer, 2020. [33] Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won Jung, and Sung-Jea Ko. Rethinking coarse-to-fine approach in single image deblurring. In Proceedings of the IEEE/CVF international conference on computer vision, pages 46414650, 2021. 4 [34] Shu-Chuan Chu, Zhi-Chao Dou, Jeng-Shyang Pan, Shaowei Weng, and Junbao Li. Hmanet: Hybrid multi-axis aggregation network for image super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62576266, 2024. 9, 12, 13 [35] Joseph Paul Cohen, Margaux Luck, and Sina Honari. Distribution matching losses can hallucinate features in medical image translation. In Medical Image Computing and Computer Assisted InterventionMICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I, pages 529536. Springer, 2018. 44 [36] Marcos Conde, Gregor Geigle, and Radu Timofte. Instructir: High-quality image restoration following human instructions. In European Conference on Computer Vision, pages 121. Springer, 2024. 16 [37] Julien Cornebise, Ivan Oršolic, and Freddie Kalaitzis. Open high-resolution satellite imagery: The worldstrat datasetwith application to super-resolution. Advances in Neural Information Processing Systems, 35:2597925991, 2022. 11, 24 [38] Kate Crawford. Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence. Yale University Press, 2021. 43 [39] Yuning Cui, Wenqi Ren, Xiaochun Cao, and Alois Knoll. Revitalizing convolutional network for image restoration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 9 [40] Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In Proceedings of the IEEE international conference on computer vision, pages 54395448, 2017. [41] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1106511074, 2019. 44 [42] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2019. 7 [43] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture similarity. IEEE transactions on pattern analysis and machine intelligence, 44(5):25672581, 2020. 11 [44] Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang. Compression artifacts reduction by deep convolutional network. In Proceedings of the IEEE international conference on computer vision, pages 576584, 2015. 44 [45] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning deep convolutional network for image super-resolution. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV 13, pages 184199. Springer, 2014. 4, 32, 34, 44 [46] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):295307, 2015. [47] Chao Dong, Chen Change Loy, and Xiaoou Tang. Accelerating the super-resolution convolutional neural network. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 391407. Springer, 2016. 36 [48] Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, et al. Agent ai: Surveying the horizons of multimodal interaction. arXiv preprint arXiv:2401.03568, 2024. 4 [49] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 22 [50] Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai. Generative diffusion prior for unified image restoration and enhancement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99359946, 2023. 44 [51] Chun-Mei Feng, Yunlu Yan, Huazhu Fu, Li Chen, and Yong Xu. Task transformer network for joint mri reconstruction and super-resolution. In Medical Image Computing and Computer Assisted Intervention MICCAI 2021: 24th International Conference, Strasbourg, France, September 27October 1, 2021, Proceedings, Part VI 24, pages 307317. Springer, 2021. 42 [52] João Gama, Indre Žliobaite, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. survey on concept drift adaptation. ACM computing surveys (CSUR), 46(4):137, 2014. 43 [53] Shangqi Gao and Xiahai Zhuang. Multi-scale deep neural networks for real image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, 2019. 44 [54] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):8692, 2021. 43 [55] Hayit Greenspan. Super-resolution in medical imaging. The computer journal, 52(1):4363, 2009. 4 [56] Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code gan prior. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30123021, 2020. 44 [57] Shuhang Gu, Yawei Li, Luc Van Gool, and Radu Timofte. Self-guided network for fast image denoising. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 25112520, 2019. 44 [58] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. Advances in neural information processing systems, 30, 2017. 44 [59] Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map estimation. IEEE Transactions on image processing, 26(2):982993, 2016. 4 [60] Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita. Task-driven super resolution: Object detection in low-resolution images. In Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 812, 2021, Proceedings, Part 28, pages 387395. Springer, 2021. [61] Muhammad Haris, Gregory Shakhnarovich, and Norimichi Ukita. Deep back-projection networks for super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 16641673, 2018. 32 [62] Woodrow Hartzog. Privacys blueprint: The battle to control the design of new technologies. Harvard University Press, 2018. 43 [63] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze removal using dark channel prior. IEEE transactions on pattern analysis and machine intelligence, 33(12):23412353, 2010. 4 [64] Yutong He, Dingjie Wang, Nicholas Lai, William Zhang, Chenlin Meng, Marshall Burke, David Lobell, and Stefano Ermon. Spatial-temporal super-resolution of satellite imagery via conditional pixel synthesis. Advances in Neural Information Processing Systems, 34:2790327915, 2021. 4, [65] Zhe He, Yide Zhang, Xin Tong, Lei Li, and Lihong Wang. Quantum microscopy of cells at the heisenberg limit. Nature Communications, 14(1):2441, 2023. 42 [66] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 11 [67] Shane Hickey, Ben Ung, Christie Bader, Robert Brooks, Joanna Lazniewska, Ian RD Johnson, Alexandra Sorvina, Jessica Logan, Carmela Martini, Courtney Moore, et al. Fluorescence microscopyan outline of hardware, biological handling, and fluorophore considerations. Cells, 11(1):35, 2021. 30 [68] Chih-Chung Hsu, Chia-Ming Lee, and Yi-Shiuan Chou. Drct: Saving image super-resolution away from information bottleneck. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61336142, 2024. 9, 12, 13 [69] Minda Hu, Tianqing Fang, Jianshu Zhang, Junyu Ma, Zhisong Zhang, Jingyan Zhou, Hongming Zhang, Haitao Mi, Dong Yu, and Irwin King. Webcot: Enhancing web agent reasoning by reconstructing chain-of-thought in reflection, branching, and rollback. arXiv preprint arXiv:2505.20013, 2025. [70] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 51975206, 2015. 11 [71] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: survey. arXiv preprint arXiv:2402.02716, 2024. 4 [72] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 22 [73] Shady Abu Hussein, Tom Tirer, and Raja Giryes. Image-adaptive gan based reconstruction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 31213129, 2020. 44 [74] Md Jahidul Islam, Sadman Sakib Enan, Peigen Luo, and Junaed Sattar. Underwater image super-resolution using deep residual multipliers. In 2020 IEEE international conference on robotics and automation (ICRA), pages 900906. IEEE, 2020. [75] Md Jahidul Islam, Peigen Luo, and Junaed Sattar. Simultaneous enhancement and super-resolution of underwater imagery for improved visual perception. In 16th Robotics: Science and Systems, RSS 2020. MIT Press Journals, 2020. 42 49 [76] Feng Jia, Lei Tan, Guang Wang, Cheng Jia, and Zhi Chen. super-resolution network using channel attention retention for pathology images. PeerJ Computer Science, 9:e1196, 2023. Published 2023 Jan 17. 11, 33, 34 [77] Jiaxi Jiang, Kai Zhang, and Radu Timofte. Towards flexible blind jpeg artifacts removal. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 49975006, 2021. 9 [78] Kui Jiang, Zhongyuan Wang, Peng Yi, Chen Chen, Baojin Huang, Yimin Luo, Jiayi Ma, and Junjun Jiang. Multi-scale progressive fusion network for single image deraining. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83468355, 2020. [79] Xu Jiang, Gehui Li, Bin Chen, and Jian Zhang. Multi-agent image restoration. arXiv preprint arXiv:2503.09403, 2025. 16, 45 [80] Yitong Jiang, Zhaoyang Zhang, Tianfan Xue, and Jinwei Gu. Autodir: Automatic all-in-one image restoration with latent diffusion. In European Conference on Computer Vision, pages 340359. Springer, 2024. 4, 16, 44 [81] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:2359323606, 2022. 44 [82] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 6, 8, [83] Christopher Kelly, Alan Karthikesalingam, Mustafa Suleyman, Greg Corrado, and Dominic King. Key challenges for delivering clinical impact with artificial intelligence. BMC medicine, 17:19, 2019. 44 [84] Daniel Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. cell, 172(5):11221131, 2018. 11, 36 [85] Chanwoo Kim, Soham U. Gadgil, Alex J. DeGrave, Jesutofunmi A. Omiye, Zhuo Ran Cai, Roxana Daneshjou, and Su-In Lee. Transparent medical image AI via an imagetext foundation model grounded in medical literature. Nature Medicine, 30:11541165, 2024. 35 [86] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 16461654, 2016. 32, 44 [87] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. 22, [88] Lingshun Kong, Jiangxin Dong, Ming-Hsuan Yang, and Jinshan Pan. Efficient visual state space model for image deblurring. arXiv preprint arXiv:2405.14343, 2024. 9 [89] Xiangtao Kong, Chao Dong, and Lei Zhang. Towards effective multiple-in-one image restoration: sequential and prompt learning strategy. arXiv preprint arXiv:2401.03379, 2024. 16 [90] Xiangtao Kong, Jinjin Gu, Yihao Liu, Wenlong Zhang, Xiangyu Chen, Yu Qiao, and Chao Dong. preliminary exploration towards general image restoration. arXiv preprint arXiv:2408.15143, 2024. 16 [91] Pawel Kowaleczko, Tomasz Tarasiewicz, Maciej Ziaja, Daniel Kostrzewa, Jakub Nalepa, Przemyslaw Rokita, and Michal Kawulok. real-world benchmark for sentinel-2 multi-image super-resolution. Scientific Data, 10(1):644, 2023. 4, [92] Somnath Kumar, Yash Gadhia, Tanuja Ganu, and Akshay Nambi. Mmctagent: Multi-modal critical thinking agent framework for complex visual reasoning. arXiv preprint arXiv:2405.18358, 2024. 45 [93] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 22 [94] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang. Deep laplacian pyramid networks for fast and accurate super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 624632, 2017. 36 [95] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang. Fast and accurate image superresolution with deep laplacian pyramid networks. IEEE transactions on pattern analysis and machine intelligence, 41(11):25992613, 2018. 32 50 [96] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image superresolution using generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 46814690, 2017. 4, 34, 36, 37, [97] Juhyoung Lee, Jinsu Lee, and Hoi-Jun Yoo. Srnpu: An energy-efficient cnn-based super-resolution processor with tile-based selective super-resolution in mobile devices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 10(3):320334, 2020. 43 [98] Junyong Lee, Hyeongseok Son, Jaesung Rim, Sunghyun Cho, and Seungyong Lee. Iterative filter adaptive network for single image defocus deblurring. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 20342042, 2021. 9 [99] Royson Lee, Stylianos Venieris, Lukasz Dudziak, Sourav Bhattacharya, and Nicholas Lane. Mobisr: In The 25th annual Efficient on-device super-resolution through heterogeneous mobile processors. international conference on mobile computing and networking, pages 116, 2019. 43 [100] Sen Lei, Zhenwei Shi, and Wenjing Mo. Transformer-based multistage enhancement for remote sensing image super-resolution. IEEE Transactions on Geoscience and Remote Sensing, 60:111, 2022. 25, 26, 27 [101] Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, et al. Genai-bench: Evaluating and improving compositional text-to-visual generation. arXiv preprint arXiv:2406.13743, 2024. 11, 22, [102] Bingchen Li, Xin Li, Yiting Lu, and Zhibo Chen. Hybrid agents for image restoration. arXiv preprint arXiv:2503.10120, 2025. 45 [103] Bingchen Li, Xin Li, Hanxin Zhu, Yeying Jin, Ruoyu Feng, Zhizheng Zhang, and Zhibo Chen. Sed: Semantic-aware discriminator for image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2578425795, 2024. 44 [104] Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking single-image dehazing and beyond. IEEE Transactions on Image Processing, 28(1):492 505, 2018. 4 [105] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jiancheng Lv, and Xi Peng. All-in-one image restoration for unknown corruption. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1745217462, 2022. 16, 44 [106] Gen Li, Jie Ji, Minghai Qin, Wei Niu, Bin Ren, Fatemeh Afghah, Linke Guo, and Xiaolong Ma. Towards high-quality and efficient video super-resolution via spatial-temporal data overfitting. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1025910269. IEEE, 2023. [107] Jinlong Li, Baolu Li, Zhengzhong Tu, Xinyu Liu, Qing Guo, Felix Juefei-Xu, Runsheng Xu, and Hongkai Yu. Light the night: multi-condition diffusion framework for unpaired low-light enhancement in autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1520515215, 2024. 40 [108] Juncheng Li, Faming Fang, Kangfu Mei, and Guixu Zhang. Multi-scale residual network for image super-resolution. In Proceedings of the European conference on computer vision (ECCV), pages 517532, 2018. 44 [109] Ke Li, Gang Wan, Gong Cheng, Liqiu Meng, and Junwei Han. Object detection in optical remote sensing images: survey and new benchmark. ISPRS journal of photogrammetry and remote sensing, 159:296307, 2020. 11, 24 [110] Xin Li, Kun Yuan, Bingchen Li, Fengbin Guan, Yizhen Shao, Zihao Yu, Xijun Wang, Yiting Lu, Wei Luo, Suhang Yao, et al. Ntire 2025 challenge on short-form ugc video quality assessment and enhancement: Methods and results. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 10921103, 2025. 40 [111] Xingzuo Li, Kehai Chen, Yunfei Long, Xuefeng Bai, Yong Xu, and Min Zhang. Generator-assistant stepwise rollback framework for large language model agent. arXiv preprint arXiv:2503.02519, 2025. 7 [112] Yawei Li, Yuchen Fan, Xiaoyu Xiang, Denis Demandolx, Rakesh Ranjan, Radu Timofte, and Luc Van Gool. Efficient and explicit modelling of image hierarchies for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1827818289, 2023. 51 [113] Yiming Li, Shuo Shao, Yu He, Junfeng Guo, Tianwei Zhang, Zhan Qin, Pin-Yu Chen, Michael Backes, Philip Torr, Dacheng Tao, and Kui Ren. Rethinking data protection in the (generative) artificial intelligence era. arXiv preprint arXiv:2507.03034, 2025. 43 [114] Yinxiao Li, Pengchong Jin, Feng Yang, Ce Liu, Ming-Hsuan Yang, and Peyman Milanfar. Comisr: Compression-informed video super-resolution. In Proceedings of the IEEE/CVF international conference on computer vision, pages 25432552, 2021. 43 [115] Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwanggil Jeon, and Wei Wu. Feedback network for image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 38673876, 2019. 32 [116] Jie Liang, Hui Zeng, and Lei Zhang. Efficient and degradation-adaptive network for real-world image super-resolution. In European Conference on Computer Vision, pages 574591. Springer, 2022. 44 [117] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 18331844, 2021. 4, 9, 11, 12, 13, 25, 26, 27, 32, 37, [118] Lighthouse Guild. High tech for low vision: How technology is changing the world for people with vision loss. https://lighthouseguild.org/news/high-tech-for-low-vision-howtechnology-is-changing-the-world-for-people-with-vision-loss/, 2025. 43 [119] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 136144, 2017. 34, 37, 44 [120] Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, et al. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. arXiv preprint arXiv:2502.09838, 2025. 35 [121] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu, Yu Qiao, Wanli Ouyang, and Chao Dong. Diffbir: Toward blind image restoration with generative diffusion prior. In European Conference on Computer Vision, pages 430448. Springer, 2024. 9, 11, 12, 13, 14, 15, 18, 19, 20, 25, 26, 27, 44 [122] Yunlong Lin, Zixu Lin, Haoyu Chen, Panwang Pan, Chenxin Li, Sixiang Chen, Kairun Wen, Yeying Jin, Wenbo Li, and Xinghao Ding. Jarvisir: Elevating autonomous driving perception with intelligent image restoration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2236922380, 2025. 45 [123] Yunlong Lin, Zixu Lin, Kunjie Lin, Jinbin Bai, Panwang Pan, Chenxin Li, Haoyu Chen, Zhongdao Wang, Xinghao Ding, Wenbo Li, et al. Jarvisart: Liberating human artistic creativity via an intelligent photo retouching agent. arXiv preprint arXiv:2506.17612, 2025. [124] G. Litjens, P. Bandi, B. Ehteshami Bejnordi, O. Geessink, M. Balkenhol, P. Bult, A. Halilovic, M. Hermsen, R. van de Loo, R. Vogels, Q.F. Manson, N. Stathonikos, A. Baidoshvili, P. van Diest, C. Wauters, M. van Dijk, and J. van der Laak. 1399 H&E-stained sentinel lymph node sections of breast cancer patients: the CAMELYON dataset. GigaScience, 7(6):giy065, June 2018. 33 [125] Jiaming Liu, Zihao Liu, Xuan Huang, Ruoxi Zhu, Qi Zheng, Zhijian Hao, Tao Liu, Jun Tao, and Yibo Fan. Auto-isp: An efficient real-time automatic hyperparameter optimization framework for isp hardware system. In Proceedings of the 61st ACM/IEEE Design Automation Conference, pages 16, 2024. 42 [126] Jiaming Liu, Qi Zheng, Zihao Liu, Yilian Zhong, Peiye Liu, Tao Liu, Shusong Xu, Yanheng Lu, Sicheng Li, Dimin Niu, et al. Frequency-biased synergistic design for image compression and compensation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1282012829, 2025. 44 [127] Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, and Zuozhu Liu. Medcot: Medical chain of thought via hierarchical expert. arXiv preprint arXiv:2412.13736, 2024. 45 [128] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki Okatani. Dual residual networks leveraging the potential of paired operations for image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 70077016, 2019. [129] Yuhao Liu, Zhanghan Ke, Fang Liu, Nanxuan Zhao, and Rynson WH Lau. Diff-plugin: Revitalizing details for diffusion-based low-level tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 41974208, 2024. 9 52 [130] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. Learning under concept drift: review. IEEE transactions on knowledge and data engineering, 31(12):23462363, 2018. 43 [131] Zhisheng Lu, Juncheng Li, Hong Liu, Chaoyan Huang, Linlin Zhang, and Tieyong Zeng. Transformer for single image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 457466, 2022. 37 [132] Luma. Dream-machine. https://lumalabs.ai/dream-machine, 2024. 41 [133] Ziwei Luo, Fredrik Gustafsson, Zheng Zhao, Jens Sjölund, and Thomas Schön. Controlling vision-language models for multi-task image restoration. arXiv preprint arXiv:2310.01018, 2023. 16 [134] Xiaoqian Lv, Shengping Zhang, Chenyang Wang, Yichen Zheng, Bineng Zhong, Chongyi Li, and Liqiang Nie. Fourier priors-guided diffusion for zero-shot joint low-light enhancement and deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 25378 25388, 2024. 9 [135] Jiaqi Ma, Tianheng Cheng, Guoli Wang, Qian Zhang, Xinggang Wang, and Lefei Zhang. Prores: Exploring degradation-aware visual prompt for universal image restoration. arXiv preprint arXiv:2306.13653, 2023. 44 [136] Varun Mannam, Yide Zhang, Xiaotong Yuan, and Scott Howard. Deep learning-based super-resolution fluorescence microscopy on small datasets. In Single Molecule Spectroscopy and Superresolution Imaging XIV, volume 11650, pages 6068. SPIE, 2021. 31 [137] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings eighth IEEE international conference on computer vision. ICCV 2001, volume 2, pages 416423. IEEE, 2001. 11 [138] Daniel Enrique Martinez, Waiman Meinhold, John Oshinski, Ai-Ping Hu, and Jun Ueda. Super resolution for improved positioning of an mri-guided spinal cellular injection robot. Journal of Medical Robotics Research, 6(01n02):2140002, 2021. [139] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. Multimedia tools and applications, 76:2181121838, 2017. 11 [140] Maxar Intelligence. legion satellite supercharging-maxar-intelligences-imagery-basemaps-with-worldviewlegion-satellite-imagery, 2025. 43 Supercharging maxar intelligences imagery basemaps with worldview https://blog.maxar.com/earth-intelligence/2025/ imagery. [141] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6):135, 2021. [142] Kangfu Mei, Hossein Talebi, Mojtaba Ardakani, Vishal Patel, Peyman Milanfar, and Mauricio Delbracio. The power of context: How multimodality improves image super-resolution. arXiv preprint arXiv:2503.14503, 2025. 4 [143] Yiqun Mei, Yuchen Fan, and Yuqian Zhou. Image super-resolution with non-local sparse attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35173526, 2021. 32 [144] Jacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. arXiv preprint arXiv:1812.01608, 2018. 44 [145] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised photo upsampling via latent space exploration of generative models. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 24372445, 2020. 44 [146] Anish Mittal, Rajiv Soundararajan, and Alan Bovik. Making completely blind image quality analyzer. IEEE Signal processing letters, 20(3):209212, 2012. 6, 8, 11 [147] Yogendra Rao Musunuri, Oh-Seol Kwon, and Sun-Yuan Kung. Srodnet: Object detection network based on super resolution for autonomous vehicles. Remote Sensing, 14(24):6270, 2022. 42, 43 [148] Saeel Sandeep Nachane, Ojas Gramopadhye, Prateek Chanda, Ganesh Ramakrishnan, Kshitij Sharad Jadhav, Yatin Nandwani, Dinesh Raghu, and Sachindra Joshi. Few shot chain-of-thought driven reasoning to prompt llms for open ended medical question answering. arXiv preprint arXiv:2403.04890, 2024. 45 53 [149] Babak Naderi, Ross Cutler, Juhee Cho, Nabakumar Khongbantabam, and Dejan Ivkovic. Icme 2025 grand challenge on video super-resolution for video conferencing. arXiv preprint arXiv:2506.12269, 2025. [150] Valfride Nascimento, Rayson Laroca, Jorge de Lambert, William Robson Schwartz, and David Menotti. Super-resolution of license plate images using attention modules and sub-pixel convolution layers. Computers & Graphics, 113:6976, 2023. 41 [151] Ngoc Long Nguyen, Jérémy Anger, Axel Davy, Pablo Arias, and Gabriele Facciolo. Self-supervised multiimage super-resolution for push-frame satellite images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11211131, 2021. 42, 43 [152] Zhangkai Ni, Runyu Xiao, Wenhan Yang, Hanli Wang, Zhihua Wang, Lihua Xiang, and Liping Sun. M2trans: Multi-modal regularized coarse-to-fine transformer for ultrasound image super-resolution. IEEE Journal of Biomedical and Health Informatics, pages 112, 2024. 11, 36, 37 [153] OpenAI, Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. https://openai.com/research/video-generationmodels-as-world-simulators, 2024. 41 [154] Fu-Zhao Ou, Chongyi Li, Shiqi Wang, and Sam Kwong. Clib-fiqa: face image quality assessment with confidence calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16941704, 2024. 7, 17 [155] Suraj Pai, Ibrahim Hadzic, Dennis Bontempi, Keno Bressem, Benjamin Kann, Andriy Fedorov, Raymond Mak, and Hugo JWL Aerts. Vision foundation models for computed tomography. arXiv preprint arXiv:2501.09001, 2025. [156] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):74747489, 2021. 44 [157] Dongwon Park, Byung Hyun Lee, and Se Young Chun. All-in-one image restoration for unknown degradations using adaptive discriminative filters for specific degradations. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 58155824. IEEE, 2023. 44 [158] Shishir Patil, Tianjun Zhang, Vivian Fang, Roy Huang, Aaron Hao, Martin Casado, Joseph Gonzalez, Raluca Ada Popa, Ion Stoica, et al. Goex: Perspectives and designs towards runtime for autonomous llm applications. arXiv preprint arXiv:2404.06921, 2024. 7 [159] Leonardo Peroni and Sergey Gorinsky. An end-to-end pipeline perspective on video streaming in best-effort networks: survey and tutorial. ACM Computing Surveys, 2024. 43 [160] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Shahbaz Khan. Promptir: Prompting for all-in-one image restoration. Advances in Neural Information Processing Systems, 36:7127571293, 2023. [161] Darren Pouliot, Rasim Latifovic, Jon Pasher, and Jason Duffe. Landsat super-resolution enhancement using convolution neural networks and sentinel-2 for training. Remote Sensing, 10(3):394, 2018. 42 [162] Chang Qiao, Di Li, Yuting Guo, Chong Liu, Tao Jiang, Qionghai Dai, and Dong Li. Evaluation and development of deep neural networks for image super-resolution in optical microscopy. Nature methods, 18(2):194202, 2021. 32 [163] Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, and Deyu Meng. Progressive image deraining networks: better and simpler baseline. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 39373946, 2019. 4 [164] Lingyan Ruan, Mojtaba Bemana, Hans-peter Seidel, Karol Myszkowski, and Bin Chen. Revisiting image deblurring with an efficient convnet. arXiv preprint arXiv:2302.02234, 2023. [165] Lingyan Ruan, Bin Chen, Jizhou Li, and Miuling Lam. Learning to deblur using light field generated and real defocus images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1630416313, 2022. 9 [166] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE transactions on pattern analysis and machine intelligence, 45(4):47134726, 2022. 4, 44 54 [167] Lothar Schermelleh, Alexia Ferrand, Thomas Huser, Christian Eggeling, Markus Sauer, Oliver Biehlmaier, and Gregor PC Drummen. Super-resolution microscopy demystified. Nature cell biology, 21(1):7284, 2019. 4 [168] Tixiao Shan, Jinkun Wang, Fanfei Chen, Paul Szenher, and Brendan Englot. Simulation-based lidar super-resolution for ground vehicles. Robotics and Autonomous Systems, 134:103647, 2020. 4 [169] Jacob Shermeyer and Adam Van Etten. The effects of super-resolution on object detection performance in satellite imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019. 4, 24, 42, [170] Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient subpixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 18741883, 2016. 36 [171] Dehua Song, Yunhe Wang, Hanting Chen, Chang Xu, Chunjing Xu, and DaCheng Tao. Addersr: Towards energy efficient image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1564815657, 2021. 43 [172] Yuda Song, Zhuqing He, Hui Qian, and Xin Du. Vision transformers for single image dehazing. IEEE Transactions on Image Processing, 32:19271941, 2023. 9 [173] FUJIFILM Healthcare Europe & SonoSkills. US-CASE: Ultrasound Cases Dataset. http:// www.ultrasoundcases.info/Cases-Home.aspx, 2025. 11, [174] J. J. Staal, M. D. Abràmoff, M. Niemeijer, M. A. Viergever, and B. van Ginneken. Ridge-based vessel segmentation in color images of the retina. IEEE Transactions on Medical Imaging, 23(4):501509, 2004. 11, 36 [175] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas Griffiths. Cognitive architectures for language agents. Transactions on Machine Learning Research, 2023. 45 [176] Lingchen Sun, Rongyuan Wu, Zhiyuan Ma, Shuaizheng Liu, Qiaosi Yi, and Lei Zhang. Pixel-level and semantic-level adjustable super-resolution: dual-lora approach. arXiv preprint arXiv:2412.03017, 2024. 4, 9, 11, 12, 13, 14, 15, 18, 19, 20, 25, 26, 27, 44 [177] Ying Tai, Jian Yang, and Xiaoming Liu. Image super-resolution via deep recursive residual network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 31473155, 2017. 32 [178] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Memnet: persistent memory network for image restoration. In Proceedings of the IEEE international conference on computer vision, pages 45394547, 2017. [179] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia. Scale-recurrent network for deep image deblurring. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 81748182, 2018. 4 [180] Muhammed Telçeken, Devrim Akgun, Sezgin Kacar, and Bunyamin Bingol. new approach for super resolution object detection using an image slicing algorithm and the segment anything model. Sensors, 24(14):4526, 2024. 42 [181] Jennifer Thorley, Jeremy Pike, and Joshua Rappoport. Super-resolution microscopy: comparison of commercially available options. In Fluorescence microscopy, pages 199212. Elsevier, 2014. 30 [182] Kalina Tosheva, Yue Yuan, Pedro Matos Pereira, Siân Culley, and Ricardo Henriques. Between life and death: strategies to reduce phototoxicity in super-resolution microscopy. Journal of Physics D: Applied Physics, 53(16):163001, 2020. 30 [183] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxim: Multi-axis mlp for image processing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57695780, 2022. 4, 9, [184] Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan Bovik. Ugc-vqa: Benchmarking blind video quality assessment for user generated content. IEEE Transactions on Image Processing, 30:44494464, 2021. 4 55 [185] Sabina Umirzakova, Shabir Ahmad, Latif Khan, and Taegkeun Whangbo. Medical image superresolution for smart healthcare applications: comprehensive survey. Information Fusion, 103:102075, 2024. 35 [186] Jeya Maria Jose Valanarasu, Rajeev Yasarla, and Vishal Patel. Transweather: Transformer-based restoration of images degraded by adverse weather conditions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 23532363, 2022. 44 [187] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. 44 [188] Veo-Team, Agrim Gupta, Ali Razavi, Andeep Toor, Ankush Gupta, Dumitru Erhan, Eleni Shaw, Eric Lau, Frank Belletti, Gabe Barth-Maron, Gregory Shaw, Hakan Erdogan, Hakim Sidahmed, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Irina Blok, Jeff Donahue, José Lezama, Kory Mathewson, Kurtis David, Matthieu Kim Lorrain, Marc van Zee, Medhini Narasimhan, Miaosen Wang, Mohammad Babaeizadeh, Nelly Papalampidi, Nick Pezzotti, Nilpa Jha, Parker Barnes, Pieter-Jan Kindermans, Rachel Hornung, Ruben Villegas, Ryan Poplin, Salah Zaiem, Sander Dieleman, Sayna Ebrahimi, Scott Wisdom, Serena Zhang, Shlomi Fruchter, Signe Nørly, Weizhe Hua, Xinchen Yan, Yuqing Du, and Yutian Chen. Veo 2. https://deepmind.google/technologies/veo/veo-2, 2024. [189] W3C. Research - low vision accessibility task force. https://www.w3.org/WAI/GL/lowvision-a11y-tf/wiki/Research, 2025. 43 [190] Hang Wang, Xuanhong Chen, Bingbing Ni, Yutian Liu, and Jinfan Liu. Omni aggregation networks for lightweight image super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2237822387, 2023. 32 [191] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 25552563, 2023. 6, 11 [192] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. International Journal of Computer Vision, 132(12):59295949, 2024. 4, 14, 15, 44 [193] Jiarui Wang, Binglu Wang, Xiaoxu Wang, Yongqiang Zhao, and Teng Long. Hybrid attention-based u-shaped network for remote sensing image super-resolution. IEEE Transactions on Geoscience and Remote Sensing, 61:115, 2023. 25, 26, [194] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pages 5876. Springer, 2024. 45 [195] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 20972106, 2017. 11, 36 [196] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. Towards real-world blind face restoration with generative facial prior. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91689178, 2021. 9, 11, 17 [197] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In Proceedings of the IEEE/CVF international conference on computer vision, pages 19051914, 2021. 4, 44 [198] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In Proceedings of the European conference on computer vision (ECCV) workshops, 2018. 4 [199] Yifan Wang, Federico Perazzi, Brian McWilliams, Alexander Sorkine-Hornung, Olga Sorkine-Hornung, and Christopher Schroers. fully progressive approach to single-image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 864873, 2018. [200] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion nullspace model. arXiv preprint arXiv:2212.00490, 2022. 44 56 [201] Yufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu, Yu Qiao, Alex Kot, and Bihan Wen. Sinsr: diffusion-based image super-resolution in single step. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2579625805, 2024. 14, 15, 44 [202] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. In Proceedings of the IEEE/CVF Uformer: general u-shaped transformer for image restoration. conference on computer vision and pattern recognition, pages 1768317693, 2022. [203] Zhihao Wang, Jian Chen, and Steven CH Hoi. Deep learning for image super-resolution: survey. IEEE transactions on pattern analysis and machine intelligence, 43(10):33653387, 2020. 4 [204] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 8, 11 [205] Zhou Wang, Eero Simoncelli, and Alan Bovik. Multiscale structural similarity for image quality In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, assessment. volume 2, pages 13981402. Ieee, 2003. 36 [206] Zijie Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. Diffusiondb: large-scale prompt gallery dataset for text-to-image generative models. arXiv preprint arXiv:2210.14896, 2022. 11, 22, [207] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, 2018. 4 [208] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 4, 45 [209] Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng Zuo, and Liang Lin. Component divide-and-conquer for real-world image super-resolution. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VIII 16, pages 101117. Springer, 2020. 11, 14 [210] Martin Weigert, Uwe Schmidt, Tobias Boothe, Andreas Müller, Alexandr Dibrov, Akanksha Jain, Benjamin Wilhelm, Deborah Schmidt, Coleman Broaddus, Siân Culley, et al. Content-aware image restoration: pushing the limits of fluorescence microscopy. Nature methods, 15(12):10901097, 2018. 42, 43 [211] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei Zhang. One-step effective diffusion network for real-world image super-resolution. Advances in Neural Information Processing Systems, 37:9252992553, 2024. 4, 9, 11, 12, 13, 14, 15, 18, 19, 20, 25, 26, 27, [212] Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang, Shuai Li, and Lei Zhang. Seesr: Towards semantics-aware real-world image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2545625467, 2024. 4, 14, 15, 44 [213] Rui-Qi Wu, Zheng-Peng Duan, Chun-Le Guo, Zhi Chai, and Chongyi Li. Ridcp: Revitalizing real image dehazing via high-quality codebook priors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2228222291, 2023. 9 [214] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 6 [215] Bin Xia, Yucheng Hang, Yapeng Tian, Wenming Yang, Qingmin Liao, and Jie Zhou. Efficient non-local contrastive attention for image super-resolution. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 27592767, 2022. 32 [216] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. Diffir: Efficient diffusion model for image restoration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1309513105, 2023. 44 [217] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. Dota: large-scale dataset for object detection in aerial images. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 39743983, 2018. 11, 57 [218] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang Bai, Yanfei Zhong, Liangpei Zhang, and Xiaoqiang Lu. Aid: benchmark data set for performance evaluation of aerial scene classification. IEEE Transactions on Geoscience and Remote Sensing, 55(7):39653981, 2017. 11, 24 [219] Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Shujie Liu, Yan Lu, et al. Mmedagent-rl: Optimizing multi-agent collaboration for multimodal medical reasoning. arXiv preprint arXiv:2506.00555, 2025. 45 [220] Jun Xiao, Xinyang Jiang, Ningxin Zheng, Huan Yang, Yifan Yang, Yuqing Yang, Dongsheng Li, and Kin-Man Lam. Online video super-resolution with convolutional kernel bypass grafts. IEEE Transactions on Multimedia, 25:89728987, 2023. 40 [221] Jun Xiao, Tianshan Liu, Rui Zhao, and Kin-Man Lam. Balanced distortion and perception in single-image super-resolution based on optimal transport in wavelet domain. Neurocomputing, 464:408420, 2021. 44 [222] Yi Xiao, Qiangqiang Yuan, Kui Jiang, Yuzeng Chen, Qiang Zhang, and Chia-Wen Lin. Frequency-assisted mamba for remote sensing image super-resolution. IEEE Transactions on Multimedia, 2024. [223] Yi Xiao, Qiangqiang Yuan, Kui Jiang, Jiang He, Xianyu Jin, and Liangpei Zhang. Ediffsr: An efficient diffusion probabilistic model for remote sensing image super-resolution. IEEE Transactions on Geoscience and Remote Sensing, 62:114, 2023. 25 [224] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. 22 [225] Liming Xu, Xianhua Zeng, Zhiwei Huang, Weisheng Li, and He Zhang. Low-dose chest x-ray image superresolution using generative adversarial nets with spectral normalization. Biomedical Signal Processing and Control, 55:101600, 2020. 36 [226] Feng Yang, Yue-Min Zhu, Jian-Hua Luo, Marc Robini, Jie Liu, and Pierre Croisille. comparative study of different level interpolations for improving spatial resolution in diffusion tensor imaging. IEEE Journal of Biomedical and Health Informatics, 18(4):13171327, 2014. 36 [227] Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution via sparse representation. IEEE transactions on image processing, 19(11):28612873, 2010. [228] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11911200, 2022. 6, 11 [229] Tao Yang, Rongyuan Wu, Peiran Ren, Xuansong Xie, and Lei Zhang. Pixel-aware stable diffusion for realistic image super-resolution and personalized stylization. In European Conference on Computer Vision, pages 7491. Springer, 2024. 14, 15, 44 [230] Tianjie Yang, Yaoru Luo, Wei Ji, and Ge Yang. Advancing biological super-resolution microscopy through deep learning: brief review. Biophysics Reports, 7(4):253, 2021. 30 [231] Mingde Yao, Ruikang Xu, Yuanshen Guan, Jie Huang, and Zhiwei Xiong. Neural degradation representation learning for all-in-one image restoration. IEEE Transactions on Image Processing, 2024. 44 [232] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. 4, [233] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti Ghadiyaram, and Alan Bovik. From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35753585, 2020. 4 [234] Jinsu Yoo, Taehoon Kim, Sihaeng Lee, Seung Hwan Kim, Honglak Lee, and Tae Hyun Kim. Enriched cnn-transformer feature aggregation networks for super-resolution. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 49564965, 2023. 32 [235] Chenyu You, Guang Li, Yi Zhang, Xiaoliu Zhang, Hongming Shan, Mengzhou Li, Shenghong Ju, Zhen Zhao, Zhuiyang Zhang, Wenxiang Cong, et al. Ct super-resolution gan constrained by the identical, residual, and cycle learning ensemble (gan-circle). IEEE transactions on medical imaging, 39(1):188203, 2019. 36 58 [236] Zhiyuan You, Zheyuan Li, Jinjin Gu, Zhenfei Yin, Tianfan Xue, and Chao Dong. Depicting beyond scores: Advancing image quality assessment through multi-modal language models. In European Conference on Computer Vision, pages 259276. Springer, 2024. 8, 9 [237] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Free-form image inpainting with gated convolution. In Proceedings of the IEEE/CVF international conference on computer vision, pages 44714480, 2019. [238] Miao Yu, Zhenghua Xu, and Thomas Lukasiewicz. general survey on medical image super-resolution via deep learning. Computers in Biology and Medicine, 193:110345, Jul 2025. 35 [239] Zongsheng Yue and Chen Change Loy. Difface: Blind face restoration with diffused error contraction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 9, 17 [240] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: Efficient diffusion model for image superresolution by residual shifting. Advances in Neural Information Processing Systems, 36:1329413307, 2023. 14, 15 [241] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57285739, 2022. 4, 9, [242] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Learning enriched features for real image restoration and enhancement. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXV 16, pages 492511. Springer, 2020. 44 [243] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1482114831, 2021. 9 [244] Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In International conference on curves and surfaces, pages 711730. Springer, 2010. 11 [245] Zheng Zhan, Yifan Gong, Pu Zhao, Geng Yuan, Wei Niu, Yushu Wu, Tianyun Zhang, Malith Jayaweera, David Kaeli, Bin Ren, et al. Achieving on-mobile real-time super-resolution with neural architecture and pruning search. In Proceedings of the IEEE/CVF international conference on computer vision, pages 48214831, 2021. 40 [246] Aoyang Zhang, Qing Li, Ying Chen, Xiaoteng Ma, Longhao Zou, Yong Jiang, Zhimin Xu, and GabrielMiro Muntean. Video super-resolution and cachingan edge-assisted adaptive video streaming solution. IEEE Transactions on Broadcasting, 67(4):799812, 2021. [247] Cheng Zhang, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang. All-in-one multi-degradation image restoration network via hierarchical degradation representation. In Proceedings of the 31st ACM international conference on multimedia, pages 22852293, 2023. 44 [248] Dafeng Zhang, Feiyu Huang, Shizhuo Liu, Xiaobing Wang, and Zhezhu Jin. Swinfir: Revisiting the swinir with fast fourier convolution and improved training for image super-resolution. arXiv preprint arXiv:2208.11247, 2022. 9 [249] Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, and Di Huang. Diffusion-4k: Ultra-high-resolution image synthesis with latent diffusion models. arXiv preprint arXiv:2503.18352, 2025. 20, 22 [250] Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, and Radu Timofte. Plug-and-play image restoration with deep denoiser prior. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):63606376, 2021. 44 [251] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind image super-resolution. In Proceedings of the IEEE/CVF international conference on computer vision, pages 47914800, 2021. 4, 11, 18, [252] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):3142 3155, 2017. 4, 44 [253] Kaibing Zhang, Dacheng Tao, Xinbo Gao, Xuelong Li, and Jie Li. Coarse-to-fine learning for singleimage super-resolution. IEEE transactions on neural networks and learning systems, 28(5):11091122, 2016. 36 59 [254] Liangpei Zhang, Hongyan Zhang, Huanfeng Shen, and Pingxiang Li. super-resolution reconstruction algorithm for surveillance images. Signal Processing, 90(3):848859, 2010. 4 [255] Lin Zhang, Lei Zhang, and Alan Bovik. feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing, 24(8):25792591, 2015. 6 [256] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. Fsim: feature similarity index for image quality assessment. IEEE transactions on Image Processing, 20(8):23782386, 2011. 36 [257] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 11 [258] Xindong Zhang, Hui Zeng, Shi Guo, and Lei Zhang. Efficient long-range attention network for image super-resolution. In European conference on computer vision, pages 649667. Springer, 2022. 37 [259] Xindong Zhang, Hui Zeng, and Lei Zhang. Edge-oriented convolution block for real-time super resolution on mobile devices. In Proceedings of the 29th ACM international conference on multimedia, pages 40344043, 2021. [260] Yide Zhang, Zhe He, Xin Tong, David Garrett, Rui Cao, and Lihong Wang. Quantum imaging of biological organisms through spatial and polarization entanglement. Science Advances, 10(10):eadk1495, 2024. 42 [261] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Proceedings of the European conference on computer vision (ECCV), pages 286301, 2018. 11, 34, 44 [262] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24722481, 2018. 4, 34, 44 [263] Zhisong Zhang, Tianqing Fang, Kaixin Ma, Wenhao Yu, Hongming Zhang, Haitao Mi, and Dong Yu. Enhancing web agents with explicit rollback mechanisms. arXiv preprint arXiv:2504.11788, 2025. 7 [264] Xiaole Zhao, Yulun Zhang, Tao Zhang, and Xueming Zou. Channel splitting network for single mr image super-resolution. IEEE transactions on image processing, 28(11):56495662, 2019. 42 [265] Qi Zheng, Yibo Fan, Leilei Huang, Tianyu Zhu, Jiaming Liu, Zhijian Hao, Shuo Xing, Chia-Ju Chen, Xiongkuo Min, Alan Bovik, et al. Video quality assessment: comprehensive survey. arXiv preprint arXiv:2412.04508, 2024. 8 [266] Shangchen Zhou, Kelvin Chan, Chongyi Li, and Chen Change Loy. Towards robust blind face restoration with codebook lookup transformer. Advances in Neural Information Processing Systems, 35:3059930611, 2022. 9, 17 [267] Yingjie Zhou, Jiezhang Cao, Zicheng Zhang, Farong Wen, Yanwei Jiang, Jun Jia, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai. Q-agent: Quality-driven chain-of-thought image restoration agent through robust multimodal large language model. arXiv preprint arXiv:2504.07148, 2025. 45 [268] Yiyang Zhou, Yangfan He, Yaofeng Su, Siwei Han, Joel Jang, Gedas Bertasius, Mohit Bansal, and Huaxiu Yao. Reagent-v: reward-driven multi-agent framework for video understanding. arXiv preprint arXiv:2506.01300, 2025. 45 [269] Kaiwen Zhu, Jinjin Gu, Zhiyuan You, Yu Qiao, and Chao Dong. An intelligent agentic system for complex image restoration problems. arXiv preprint arXiv:2410.17809, 2024. 4, 7, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 25, 26, 27, 39, [270] Ruoxi Zhu, Zhengzhong Tu, Jiaming Liu, Alan Bovik, and Yibo Fan. Mwformer: Multi-weather image restoration using degradation-aware transformers. IEEE Transactions on Image Processing, 2024. 44 [271] Karel Zuiderveld et al. Contrast limited adaptive histogram equalization. Graphics gems, 4(1):474485, 1994."
        }
    ],
    "affiliations": [
        "CU Boulder",
        "California Institute of Technology",
        "Snap Inc.",
        "Stanford University",
        "Texas A&M University",
        "Topaz Labs",
        "UC Merced",
        "UT Austin"
    ]
}