{
    "paper_title": "Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models",
    "authors": [
        "Jianshu Zeng",
        "Yuxuan Liu",
        "Yutong Feng",
        "Chenxuan Miao",
        "Zixiang Gao",
        "Jiwang Qu",
        "Jianzhang Zhang",
        "Bin Wang",
        "Kun Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video relighting is a challenging yet valuable task, aiming to replace the background in videos while correspondingly adjusting the lighting in the foreground with harmonious blending. During translation, it is essential to preserve the original properties of the foreground, e.g., albedo, and propagate consistent relighting among temporal frames. In this paper, we propose Lumen, an end-to-end video relighting framework developed on large-scale video generative models, receiving flexible textual description for instructing the control of lighting and background. Considering the scarcity of high-qualified paired videos with the same foreground in various lighting conditions, we construct a large-scale dataset with a mixture of realistic and synthetic videos. For the synthetic domain, benefiting from the abundant 3D assets in the community, we leverage advanced 3D rendering engine to curate video pairs in diverse environments. For the realistic domain, we adapt a HDR-based lighting simulation to complement the lack of paired in-the-wild videos. Powered by the aforementioned dataset, we design a joint training curriculum to effectively unleash the strengths of each domain, i.e., the physical consistency in synthetic videos, and the generalized domain distribution in realistic videos. To implement this, we inject a domain-aware adapter into the model to decouple the learning of relighting and domain appearance distribution. We construct a comprehensive benchmark to evaluate Lumen together with existing methods, from the perspectives of foreground preservation and video consistency assessment. Experimental results demonstrate that Lumen effectively edit the input into cinematic relighted videos with consistent lighting and strict foreground preservation. Our project page: https://lumen-relight.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 5 4 9 2 1 . 8 0 5 2 : r Lumen: Consistent Video Relighting and"
        },
        {
            "title": "Harmonious Background Replacement\nwith Video Generative Models",
            "content": "Jianshu Zeng1,3, Yuxuan Liu2, Yutong Feng2, Chenxuan Miao4, Zixiang Gao1, Jiwang Qu5, Jianzhang Zhang5, Bin Wang2, Kun Yuan1 1Peking University, 2Kunbyte AI, 3University of Chinese Academy of Sciences, 4Zhejiang University, 5Hangzhou Normal University {zengjianshu.ai,liuyuxuanuestc,fengyutong.fyt,weiyuchoumou526}@gmail.com, 2401210062@stu.pku.edu.cn, 2024112013037@stu.hznu.edu.cn, zjzhang@hznu.edu.cn, binwang393@gmail.com, kunyuan@pku.edu.cn Equal Contribution. Project Leader. Corresponding Author. Figure 1: Video relighting results of Lumen, where the left sequence shows sampled key-frames of input video and the right sequence shows relighted video. Lumen achieves harmonious video relighting with consistent foreground preservation for various characters, scenarios and domains. Our project page: https://lumen-relight.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Video relighting is challenging yet valuable task, aiming to replace the background in videos while correspondingly adjusting the lighting in the foreground with harmonious blending. During translation, it is essential to preserve the original properties of the foreground, e.g., albedo, and propagate consistent relighting among temporal frames. While previous research mainly relies on 3D simulation, recent works leverage the generalization ability of diffusion generative models to achieve learnable relighting of images. In this paper, we propose Lumen, an end-to-end video relighting framework developed on large-scale video generative models, receiving flexible textual description for instructing the control of lighting Preprint. Under review. and background. Considering the scarcity of high-qualified paired videos with the same foreground in various lighting conditions, we construct large-scale dataset with mixture of realistic and synthetic videos. For the synthetic domain, benefiting from the abundant 3D assets in the community, we leverage advanced 3D rendering engine to curate video pairs in diverse environments. For the realistic domain, we adapt HDR-based lighting simulation to complement the lack of paired in-the-wild videos. Powered by the aforementioned dataset, we design joint training curriculum to effectively unleash the strengths of each domain, i.e., the physical consistency in synthetic videos, and the generalized domain distribution in realistic videos. To implement this, we inject domain-aware adapter into the model to decouple the learning of relighting and domain appearance distribution. We construct comprehensive benchmark to evaluate Lumen together with existing methods, from the perspectives of foreground preservation and video consistency assessment. Experimental results demonstrate that Lumen effectively edit the input into cinematic relighted videos with consistent lighting and strict foreground preservation."
        },
        {
            "title": "Introduction",
            "content": "Video relighting with harmonious background replacement has attracted tremendous efforts [32, 19, 55, 33, 58, 17, 60], due to its widespread application in various scenarios, e.g., filmmaking and e-commerce. Given the input video with foreground segmentation mask and the input condition, the objectives of video relighting can be formulated as two-fold: 1. relighting the foreground while preserving the foreground with its intrinsic attributes, e.g., albedo and texture. 2. replacing the background while harmonizing the lighting of foreground and background based on the input condition, e.g., text description or background image. Traditional methods generally address this task with 3D-based physical simulations, such as spherical harmonics and light transport simulation. More advanced studies introduce learning-based frameworks for editing the illumination of images [45, 59, 37, 54, 29] and videos [55, 33, 13], controlled by HDR map or Spherical Harmonic (SH) lighting. Inspired by the recent success of large-scale generative models [43, 40, 31], it has been explored to unify the relighting task with end-to-end diffusion-based generation [56, 60, 17]. These methods leverage the generalization capabilities of diffusion models to achieve flexible lighting control with textual descriptions or background images, and can be widely-applied on in-the-wild data for broader real-world applications. Despite the aforementioned advancements, existing works mainly concentrate on the image domain. When confronting with the video input, it would be more challenging to propagate the relighting modification across all frames with temporal consistency. Extension to the video relighting is restricted by the scarcity of large-scale dataset containing high-qualified and foreground-aligned pairs of videos. We summarize all the necessary constitution of such dataset as: 1. video pairs with physically-aligned foreground and distinguished background conditions, 2. corresponding textual descriptions of each video, especially on the background and lighting conditions, 3. binary mask videos that segment the foreground. In this paper, we construct large-scale video relighting dataset to complement the scarcity, consisting of paired video samples from two domains: For the synthetic domain, powered by 3D rendering framework like the Unreal Engine [16], we could obtain videos with fixed foreground subject and animation in various environments, which strictly follow the physical rules to preserve the foreground. However, models directly optimized on the 3D pairs suffer from the domain-gap problem, and fail to generalize to real videos. For the realistic domain, inspired by the random degradation strategy in IC-Light [56], we prepare artificial paired videos by extracting the video normals [5] and simulating various lighting with random HDR maps. The weakness of the realistic pairs is that the foreground-preservation is not guaranteed, and there lack dynamics of light and shadow effects between videos. Equipped with the above dataset, we presents Lumen, video relighting model developed on largescale video generative models [30, 48, 47] with hyper-realistic aesthetics and consistent temporal motions for video generation. To effectively leverage the constructed dataset and benefit the strengths from each domain, we propose multi-domain joint training curriculum in Lumen. In detail, we inject style adapter into the model, serving to generate 3D-style videos with empty control signal. Then 2 the model is trained with mixed sampling of the realistic and synthetic data, where we activate the adapter only for the forward process of synthetic data. During the inference, the adapter is deactivated to avoid introducing artificial quality into the edited video. Throughout this way, Lumen decouples the style distribution of multiple domains and achieves natural output. We present comprehensive benchmark for evaluating the video relighting performance, consisting of both realistic and synthetic videos as input. Besides existing metrics regarding the similarity measurement and quality assessment [27], we propose new evaluation method, termed intrinsic consistency, to measure the similarity after transferring both videos into uniform illumination. Extensive experiments demonstrate the effectiveness of Lumen compared with existing methods, showing consistent video relighting and foreground preservation among video frames."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Video Generative Models The success of diffusion model [23] on image generation has inspired the extension to video generation [24]. Pioneer works like Make-A-Video [44] design cascaded architecture to handle larger resolution generation. Inspired by the latent diffusion model [6] on images, extensive researches achieve end-to-end video generation with convolutional networks for text-to-video [49, 18, 3] and image-to-video [21, 11] generation. More recently, Sora [8] demonstrates the potential of scaling diffusion transformers (DiT) [38] to achieve significant advancement on the aesthetics, dynamics and smoothness of generated videos. Following Sora, tremendous efforts explore the variants of DiT with various text encoders to achieve better generation quality and prompt following [53, 30, 48, 2]. The advancement of fundamental video generative models lays foundation to diverse downstream tasks, e.g., controllable video generation and video editing. 2.2 Image Relighting Image relighting aims to modify the lighting of objects in static image under different illumination conditions. Early approaches primarily rely on physical illumination models [4] or deep learningbased methods with specific lighting representations [45, 59]. Recently, the success of diffusion models has led to significant advancements in image relighting quality. [23, 9, 28, 41, 56]. Some works mainly focus on the portrait relighting [22, 41] or object relighting [52, 28], and they are limited to specific scenarios and lack the ability to control the lighting and background through text. Beyond that, some works explore the use of large-scale synthetic data to train diffusion models for text-guided and image-guided image relighting. Text2Relight [9] leverages generative diffusion model with auxiliary task augmentation to enable text-guided portrait relighting from large-scale synthetic data. IC-Light [56] introduces light transport consistency loss and trains on comprehensive dataset of rendered and augmented real-world images. These advancements in image relighting provide essential foundations for extending the task to the temporal domain. 2.3 Video Relighting Video relighting aims to modify the lighting of the dynamic foreground object in video under different background conditions. Current methods on portrait video relighting [12, 20, 35, 50] often rely on the lighting source or the HDR image as the input, which limits the flexible application of the model. Besides, the background in the generated videos is usually fixed, which leads to unnaturalness of the whole video. More recently, inspired by image relighting advancements, some works have attempted to adapt image relighting models to video tasks for more flexible control and better results. Light-A-Video [60] adapts the IC-Light by introducing Consistent Light Attention and Progressive Light Fusion strategies, while its train-free approach limits overall quality. RelightVid [17] inflates pre-trained 2D image relighting model into 3D U-Net with temporal attention layers, but struggles with foreground preservation and lighting consistency due to the limited quality of in-the-wild data and the inherent limitations of image relighting models. Video relighting remains challenging due to the need for high-quality paired videos with different lighting conditions, and models that can effectively learn the lighting harmonization while maintaining the foreground and background consistency. 3 Figure 2: The data preparation and examples of two domains. (a) The 3D rendered data combines various environments, characters and animations to form paired videos with aligned foreground. (b) The realistic videos are transformed into uniform-lit appearance and rendered with HDR-based relighting."
        },
        {
            "title": "3 Method",
            "content": "3.1 Dataset Construction In this section, we present the preparation pipeline for the paired video relighting dataset, which can be divided into the 3D rendered synthetic data, and the realistic in-the-wild data. The dataset targets to serve diverse range of lighting conditions and backgrounds to facilitate the training of video relighting. 2 illustrates the pipeline and example pairs of our dataset. 3D Rendered Data. The rapid development of computer graphics enables us to render cinematic videos with precise control on the animation and environment. Specifically, we leverage the advanced 3D rendering engine, i.e., the Unreal Engine 5 (UE5) [16], to render video relighting pairs based on diverse 3D assets in the community. The data preparation can be summarized as follows: 3D Assets Collection: We first collect the necessary assets for rendering, including environments, characters (human or non-human objects), and animations from open-sourced websites, e.g., Fab [14], MetaHuman [15], Mixamo [36] and other publicly available resources. The environments contain various lifelike indoor and outdoor scenarios with diverse lighting and background conditions, which are then split into small scenes for rendering. The characters are generally equipped with sets of animations to simulate different subject motions. Random Shot Generation: To obtain valid shot focusing on the foreground, we first randomly sample character and assign an animation towards it. The animations can be divided into in-place animation and movement across the scene (e.g., walking). The camera is set to be permanently oriented towards the character, while also moving in pre-defined camera trajectories, such as zoom-in and zoom out. By combing different characters, animations and camera movements, we could obtain large scale of valid shots in diverse distribution. Another notable strength of rendered data is that we can automatically obtain the foreground mask video by setting specific texture to the character in the rendering engine. It avoids to use segmentation parsers to detect the foreground, which could generate unstable masks for varying reasons. Paired Video Rendering: To formulate paired relighting videos, we adjust the background conditions for each combination of character, animation and camera movement. The background is modified by either setting the character in various scenes, or modulating the lighting configurations with different light colors and intensities. Therefore, we could obtain group of videos with aligned foreground and diverse backgrounds in each group. Finally, by combining 15 environments with 100 selected scenes, 20 characters, 20 animations and 10 camera movements, we construct the synthetic dataset with 20, 000 videos in resolution of 1920 1080 and frame length of 90 (corresponding to 6 seconds). Since the videos are in groups, we could sample at least 100, 000 training pairs via randomly pick two videos inside each group. 4 Figure 3: The framework of Lumen, which is developed on video generative model in DiT architecture. The model consumes the concatenation of noisy tokens and the masked input video. An adapter module is injected into the backbone to decouple the style distribution in 3D paired videos. In-the-wild Data To mitigate the domain-gap of 3D rendered data, we further prepare realistic videos with simulated relighting effects in the following steps: Data Collection: We start by gathering high-qualified videos from open sourced websites (e.g., Pexels [39]) or internal datasets, with shorter spatial side greater than 720 and temporal length greater than 2 seconds. The videos are then filtered with single character clearly visible in the foreground. The final collection contains roughly 100, 000 videos for training. HDR-based Lighting Simulation: Inspired by the image degradation proposed in IC-Light [56], we design strategy of video lighting simulation to prepare relighted video (degraded) from the original in-the-wild video. During training, the degraded video serves as input control, while the original video serves as ground truth output. Considering that only the foreground of input video is consumed by the model, we ignore the visual content in background and concentrate on the lighting adjustment. In brief, we firstly convert the original video into uniform-lit appearance inspired by LuminaBrush [46], then modify the lighting of converted video based on random HDR maps, which can be formulated as follows: 1. The uniform-lit video indicates those captured in the environments with uniformly assigned light sources in white color. We first select set of videos under the sufficient lighting environments from our real-world video dataset as uniform-lit videos. Given the collect uniform-lit videos as ground truth, the model inputs are prepared with the next step. 2. To simulate lighting adjustment in videos, we first use video normal extractor [5] to obtain temporally consistent normal maps. Then we synthesize random environment maps, and conduct HDR rendering on videos based on the extracted normals. Specifically, we randomly sample point set as the light sources in the environment. For each point P, we assign random light color, denoted as Ip = (r, g, b). Then for any point in the environment map, we assume the light color in as the assembling of all light points with cosine light falloff: Iv = min(ΣpP max(Ip cos θv, p, 0), 255), (1) where v, denotes the radian distance between and in the spherical coordinate. We then render the relighted video by applying the environment map on the original video, where the environment map is fixed across all frames. 3. Based on the above lighting simulation, we first apply this method on the uniform-lit videos, and use the paired videos to finetune uniform-lit restorer. Then for all the collected videos, we extract their uniform-lit appearance with the restorer, and conduct the HDR-based relighting to obtain the final degraded videos for training. 3.2 Lumen: Video Relighting Model In this section, we present the model architecture of Lumen, together with Multi-domain Joint training Curriculum on the aforementioned relighting video pairs. Model Architecture. Considering the end-to-end generation manner, the fundamental capability of backbone model is critical to the quality of relighted videos. In this paper, we adopt the most recent video generative model, i.e., Wan2.1 [48], due to its remarkable performance with high fidelity and realistic details. The model is based on diffusion transformer (DiT) architecture [38]. The overall architecture of Lumen is illustrated in 3. Suppose the training video pair is (Vsrc, Vtar), representing the source video and target video, respectively, and the foreground mask video is Mf g. During training, we first encode the videos into latent space [43] with the 3D-VAE encoder E(), and encode the corresponding description of target video into text tokens, noted as . Following the flow-matching process [34] adopted in Wan2.1, we sample timestep [0, 1] and standard Gaussian noise ϵ. Then the final input of model is: = concat([E(Vtar) (1 t) + ϵ t]; E(Vsrc Mf g)), (2) where concat() denotes channel-wise concatenation, and denotes point-wide multiplication. The input projection layer of DiT is correspondingly extended to consume the additional channels in the concatenated input. Then the video tokens are projected and fed into transformer blocks together with the text tokens , which guide the model to generate harmonious background and lighting for the video. Suppose the model is FΘ() with trainable parameters Θ, then the final optimization target is: min Θ Eϵ FΘ(X, T, t)2 2 (3) Multi-domain Joint Training. Given the paired videos from synthetic and realistic domains, naive baseline solution is to directly optimize the model with mixed sampling of all data. To better investigate the effects of the two domain data, we first conduct exploratory experiments with models separately trained on each domain. Experiments suggest that: 1. the model trained on the 3D domain achieves the light harmonization between foreground and background but may generated characters with plastic effects (closer to rendering data); 2. model trained on the realistic data maintains the intrinsic consistency of foreground, which, however, may be lack of lighting effects and disharmonious with the background. To better leverage the strengths of each domain, we design two-stage training framework. Based on the observation on separate-domain experiments, the synthetic data is mainly restricted by the domain-gap problem. Ignoring the domain distribution, model benefit from rendered data with diverse lighting and shadow effects following physical rules. To address this problem, inspired by the Still-Moving [10], we introduce style adapter (implemented as LoRA [26]) to decouple the style distribution of rendered videos. The model is trained in the following two stages: For the first stage, we individually optimize the style adapter on the rendered data. We set the source video as all-zero map, i.e., Vsrc = 0. The input text is prepended with prefix, This is video rendered with 3D unreal engine, to distinguish the video style with better convergence. Through this stage, the adapter learns to generate videos in the 3D rendering style according to text descriptions. For the second stage, we mix the 3D rendered data and in-the-wild data in ratio of 1:1 sampling. We fully finetune the DiT model while freeze the style adapter in this stage. During training, the adapter is activated only in the forward process of 3D data, and is ignored for realistic data. Such switchable process enables the model to learn the mapping between rendered pairs, while the adapter serves to absorb information related to domain distribution. Then during inference, we remove the adapter from the finetuned DiT model, thus it achieves natural output with better foreground alignment. 3.3 Benchmark and Metrics To fully evaluate the relighting performance of Lumen, especially from quantitative aspect, we construct comprehensive video relighting benchmark with diverse range of lighting and background, and design metrics to evaluate the model from multiple perspectives. 6 Benchmark Constitution. The benchmark contains data in two domains, targeting different evaluation principles: 1. Paired 3D videos: We split 100 pairs with diverse characters and scenes from the synthetic dataset for evaluation. Since there are strictly aligned pairs, we could evaluate with metrics of similarity measurement, e.g., PSNR [25], SSIM [51] and LPIPS [57]. 2. Paired realistic videos: Though it is hard to obtain realistic pairs with ground-truth alignment, we construct pseudo-pairs with stricter alignment in the following steps: (1) Following the same architecture of Lumen, we train an additional first-frame conditioned relighting model, which requires the first frame of target video as input. The frame is injected in extended input channel of the model. (2) Given set of realistic videos, we use an in-house image relighting model to transfer their first frames. The relighted frame then guide the model to generate complete relighted video, forming the pseudo pairs. (3) We manually filter the pairs based on the criteria of foreground preservation, background quality and lighting harmonization. The final subset contains 100 video pairs. 3. Unpaired realistic videos: We further select 100 high-quality videos with diverse lighting conditions and backgrounds for direct relighting test. Towards this subset, we do not measure similarity in pairs, but evaluate the general quality of output videos based on metrics from the widely-adopted V-Bench [27], such as temporal consistency and video aesthetics. Videos in this subset can be further classified into three categories: 70 videos containing only one character in close-up view, 15 videos with one character in relatively far view, and 15 videos containing multiple characters. Intrinsic Consistency Evaluation. Besides the existing metrics in the above evaluation subsets, we further propose new evaluation principle, termed intrinsic consistency, capable of assessing the foreground preservation without ground-truth pairs. Intrinsic consistency concentrate on the fixed attributes of subjects that are not influenced by the environments, such as albedo and texture. The measurement is based on the uniform-lit restorer, denoted as U(), in the data preparation pipeline. Given the source video Vsrc, foreground mask Mf and generated video ˆVtar, we measure the consistency of their foregrounds by sim(U(Vsrc) Mf g, U( ˆVtar) Mf g), (4) where sim(, ) can be implemented as any similarity measuring metrics like PSNR, SSIM, and LPIPS."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Implementation Details. We adopt variant of Wan2.1, i.e., the controllable generative model Wan2.1-Fun-Control [47], as our backbone. We train the model on 4 GPUs with batch size as 1 and gradient accumulation steps as 2, i.e., the actual batch size is 8. The model is trained on the NVIDIA H200 GPUs. The learning rate is set to 15 when fully fine-tuning the main DiT blocks, and 14 when training the LoRA module. The LoRA rank and α is set to be 16 and 16, and we inject LoRA into the input projection layer, QKV projection layer, output layer in attention and linear layers in the feed-forward network in all blocks. The training steps are set to 40,000 for the first stage and 80,000 for the second. We use RMBG-2.0 [7] to extract the foreground mask, and Qwen2.5-VL-7B-Instruct [1] to caption the videos. Evaluation Metrics. We use the PSNR [25], SSIM [51], LPIPS [57] to calculate the similarity between two videos, and use CLIP-T Score [42] to evaluate the alignment between the text prompt and the generated video. Several metrics in Vbench [27] (e.g., subject consistency, background consistency, motion smoothness, temporal flickering, etc.) are used to evaluate the overall quality of the generated video. The Intrinsic Consistency metric (Eq. 4) is used to evaluate the foreground preservation of the relighted videos when there is no available corresponding ground truth. Compared Methods. We compare with existing methods supporting text-guided relighting and background replacement. IC-Light [56] is representative method for diffusion-based end-to-end image relighting. We adapt it to video relighting by applying it frame by frame. Light-A-Video [60] 7 Figure 4: Qualitative Comparison: We qualitatively compare Lumen with existing methods on the constructed benchmark. The text prompts of the four cases from upper to lower are: (1) man in dimly lit room with blue lighting, neon lighting; (2) man in the room, natural lighting, Wong Kar-wai style; (3) man in room, natural lighting; (4) woman clapping hands, natural lighting, warm atmosphere. Table 1: Comparison with existing methods on paired synthetic/realistic videos. Best and second are in bold and underline. Method PSNR SSIM LPIPS CLIP-T VBench Subject Background Motion Temporal Consistency Consistency Smoothness Flickering IC-Light 21.03 Light-A-Video 22.34 22.39 Lumen (Ours) 0.8856 0.1033 0.9008 0.0951 0.8985 0.0741 0.2838 0.2793 0.3395 0.8524 0.9564 0.9575 On the 3D paired videos On the realistic paired videos IC-Light 18.96 Light-A-Video 19.41 23.06 Lumen (Ours) 0.8060 0.1712 0.8329 0.1717 0.8620 0.1083 0.3080 0.2978 0.3214 0.9257 0.9811 0.9808 0.8964 0.9439 0.9279 0.9428 0.9636 0. 0.9284 0.9832 0.9885 0.9636 0.9933 0.9943 0.9164 0.9886 0.9648 0.9579 0.9901 0.9905 extend IC-Light to video relighting with training-free adaption, which designs Consistent Light Attention (CLA) module and Progressive Light Fusion (PLF) strategy to achieve stable video relighting results. The training-based RelightVid [17] is not included since its text-guided relighting is not open-sourced. 4.2 Performance on Video Relighting Qualitative Evaluation Figure 4 shows the qualitative results of Lumen and existing methods, which shows, Light-A-Video and IC-Light get poor results on the foreground preservation and background quality, while ours achieves better results on foreground preservation, background quality, and lighting harmonization. Quantitative Evaluation The evaluation is conducted on paired synthetic videos, paired realistic videos and unpaired realistic videos. Paired Videos Evaluation. We compare Lumen with existing methods on the paired 3D videos and paired realistic videos in our constructed benchmark. The quantitative results are shown in Table 1. When testing on paired videos, the similarity metrics (PSNR, SSIM, LPIPS) are calculated between the generated video and the ground truth video of the foreground region, indicating the foreground preservation of the generated video. The CLIP-T and VBench metrics are used to evaluate the text alignment and overall quality. 8 Table 2: Comparison with existing methods on unpaired realistic videos. Intrinsic Consistency VBench Method CLIP-T PSNR SSIM LPIPS IC-Light Light-A-Video Lumen (Ours) 18.42 20.16 23.55 0.8336 0.8800 0.9052 0.1210 0.1141 0.0650 0.1887 0.1857 0.2342 Subj. Backg. Cons. Cons. 0.9667 0.9882 0. 0.9536 0.9676 0.9673 Mot. Smo. 0.9751 0.9941 0.9953 Tempo. Flic. 0.9704 0.9910 0.9916 Unpaired Videos Evaluation. We compare Lumen with existing methods on unpaired realistic videos. The quantitative results are shown in Table 2. When testing on unpaired videos, there is no ground truth available, so we use the proposed Intrinsic Consistency method to extract the uniform-lit of the foreground of the source video and the generated video, and then calculate the similarity between them, as formulated in Eq. 4. Results on both paired videos and unpaired videos show that Lumen outperforms existing methods in most metrics, indicating that our method can achieve better foreground preservation, background quality and lighting harmonization. User Study. We conduct user study to evaluate the quality of the generated videos on the unpaired realistic videos from the following three aspects: 1. Foreground Preservation: The consistency of the intrinsic attributes of the foreground subjects; 2. Background Quality: The realism, richness and consistency of the background; 3. Lighting Harmonization: The lighting consistency and harmonization between the foreground and background; About 10 participants are invited to evaluate the results from the above three aspects, with score {1, 2, 3} indicating bad, medium and good, respectively. The averaged and normalized scores are shown in Table 3. The results show that Lumen achieves the best scores on all three aspects by large margin, indicating that our method can achieve remarkable performance on these three aspects. Table 3: The user study results on unpaired realistic videos. User Study Method Foreground Preservation Background Quality Lighting Harmonization Average IC-Light Light-A-Video Lumen (Ours) 0.7567 0.8100 0.9133 0.7867 0.7433 0. 0.8300 0.8567 0.9533 0.7911 0.8033 0.9311 4.3 Ablation Studies We conduct ablation studies to evaluate the effectiveness of our Multi-domain Joint Training curriculum. When training, we first train LoRA branch on 3D rendered data, and then mix the 3D data and augmented in-the-wild data for joint training via data-aware domain distillation method. Thus, we compare the results of the following four training methods: 1. Only 3D Data: Fully fine-tuning the main DiT blocks with only 3D rendered data; 2. Only Real Data: Fully fine-tuning the main DiT blocks with only augmented in-the-wild data; 3. Mixed Data w/o Adapter: Fully fine-tuning the main DiT blocks with 3D rendered data and augmented in-the-wild data; 4. Mixed Data with Adapter: Our Multi-domain Joint training curriculum with domain-aware adapter. The qualitative ablation results are shown in Figure 5. From the visiualization comparison, the results of Only-3D-data have better lighting effects under the given background environment, but the foreground preservation is poor. Results of Only-real-data may get unsatisfying lighting consistency between the foreground and background. Simply mixing the 3D data and real data for training can not achieve good results. Our multi-domain joint training curriculum with domain-aware adapter achieves the best results on both foreground preservation and lighting harmonization. The quantitative ablation results are shown in Table 4. Using realistic data enhances the prompt following and video quality due to its generalized distribution compared with 3D data. Directly mixing the two domains shows further improvement on prompt following, but also slightly decreases 9 Figure 5: Qualitative Ablation Study: Results of Only-3D-data have better lighting effects but poor foreground preservation. Results of Only-real-data may get unsatisfying lighting consistency. Lumen (Mixed Data with Adapter) get best results on both foreground preservation and lighting harmonization. Table 4: Ablation study of Multi-domain Joint training method on unpaired realistic videos. Best and second best results are in bold and underline. Method CLIP-T Subj. Cons. Backg. Cons. Mot. Smo. Tempo. Flic. VBench Only 3D Data Only Real Data Mixed w/o Adapter Mixed w/ Adapter 0.2260 0.2323 0.2377 0.2342 0.9879 0.9916 0.9909 0.9909 0.9695 0.9670 0.9667 0.9673 0.9936 0.9946 0.9950 0. 0.9877 0.9907 0.9885 0.9916 the quality. Our final solution achieves balanced improvement on prompt following and video quality. We note that quantitative metrics could not fully reflect the relighting performance, thus it is better to combine the results together with visualization for analysis."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present Lumen, video relighting model based on large-scale video generative models, supporting text guidance on the lighting and background replacement. To address the problem of lacking paired relighting videos, we construct multi-domain dataset containing both 3D rendered and augmented realistic videos. We adapt the pretrained video generative model by extending its input with the foreground of input video. To fully leverage the constructed dataset, we design domain mixed training curriculum, where style adapter module is injected to help decoupling the domain distribution of rendered data. Furthermore, we present comprehensive benchmark with paired/unpaired videos in synthetic/realistic domain, together with new metrics examining the foreground preservation results. Both qualitative and quantitative comparison demonstrate the effectiveness of Lumen compared with existing methods, where Lumen generates cinematic videos with consistent foreground preservation and temporal relighting. 10 Figure 6: Video relighting results of Lumen 11 Figure 7: Video relighting results of Lumen"
        },
        {
            "title": "References",
            "content": "[1] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] F. Bao, C. Xiang, G. Yue, G. He, H. Zhu, K. Zheng, M. Zhao, S. Liu, Y. Wang, and J. Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. [3] O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss, S. Zada, A. Ephrat, J. Hur, G. Liu, A. Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [4] J. T. Barron and J. Malik. Shape, illumination, and reflectance from shading, 2020. URL https://arxiv.org/abs/2010.03592. [5] Y. Bin, W. Hu, H. Wang, X. Chen, and B. Wang. Normalcrafter: Learning temporally consistent normals from video diffusion priors, 2025. URL https://arxiv.org/abs/2504.11427. [6] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [7] BRIA. Bria background removal v2.0 model card, 2025. URL https://github.com/ ai-anchorite/BRIA-RMBG-2.0. [8] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1:8, 2024. [9] J. Cha, M. Ren, K. K. Singh, H. Zhang, Y. Hold-Geoffroy, S. Yoon, H. Jung, J. S. Yoon, and S. Baek. Text2relight: Creative portrait relighting with text guidance. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 19801988, 2025. [10] H. Chefer, S. Zada, R. Paiss, A. Ephrat, O. Tov, M. Rubinstein, L. Wolf, T. Dekel, T. Michaeli, and I. Mosseri. Still-moving: Customized video generation without customized video data. ACM Transactions on Graphics (TOG), 43(6):111, 2024. [11] X. Chen, Z. Liu, M. Chen, Y. Feng, Y. Liu, Y. Shen, and H. Zhao. Livephoto: Real image animation with text-guided motion control. In European Conference on Computer Vision, pages 475491. Springer, 2024. [12] J. M. Choi, M. Christman, and R. Sengupta. Personalized video relighting with an at-home light stage, 2024. URL https://arxiv.org/abs/2311.08843. [13] J. M. Choi, M. Christman, and R. Sengupta. Personalized video relighting with an at-home light stage. In European Conference on Computer Vision, pages 394410. Springer, 2024. [14] Epic. Fab, 2025. URL https://www.fab.com/. [15] Epic. Matahuman., 2025. URL https://www.unrealengine.com/en-US/metahuman. [16] Epic. Unreal engine 5, 2025. URL https://www.unrealengine.com/. [17] Y. Fang, Z. Sun, S. Zhang, T. Wu, Y. Xu, P. Zhang, J. Wang, G. Wetzstein, and D. Lin. Relightvid: Temporal-consistent diffusion model for video relighting, 2025. URL https: //arxiv.org/abs/2501.16330. [18] R. Girdhar, M. Singh, A. Brown, Q. Duval, S. Azadi, S. S. Rambhatla, A. Shah, X. Yin, D. Parikh, and I. Misra. Factorizing text-to-video generation by explicit image conditioning. In European Conference on Computer Vision, pages 205224. Springer, 2024. [19] K. Guo, P. Lincoln, P. Davidson, J. Busch, X. Yu, M. Whalen, G. Harvey, S. Orts-Escolano, R. Pandey, J. Dourgarian, et al. The relightables: Volumetric performance capture of humans with realistic relighting. ACM Transactions on Graphics (ToG), 38(6):119, 2019. [20] M. Guo, G. Xing, and Y. Liu. High-fidelity relightable monocular portrait animation with lighting-controllable video diffusion model, 2025. URL https://arxiv.org/abs/2502. 19894. [21] Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [22] M. He, P. Clausen, A. L. Tasel, L. Ma, O. Pilarski, W. Xian, L. Rikker, X. Yu, R. Burgert, N. Yu, and P. Debevec. Diffrelight: Diffusion-based facial performance relighting. In SIGGRAPH Asia 2024 Conference Papers, SA 24, page 112. ACM, Dec. 2024. doi: 10.1145/3680528.3687644. URL http://dx.doi.org/10.1145/3680528.3687644. [23] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [24] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [25] A. Hore and D. Ziou. Image quality metrics: Psnr vs. ssim. In ICPR, 2010. [26] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [27] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [28] H. Jin, Y. Li, F. Luan, Y. Xiangli, S. Bi, K. Zhang, Z. Xu, J. Sun, and N. Snavely. Neural gaffer: Relighting any object via diffusion, 2024. URL https://arxiv.org/abs/2406.07520. [29] H. Kim, M. Jang, W. Yoon, J. Lee, D. Na, and S. Woo. Switchlight: Co-design of physics-driven architecture and pre-training framework for human portrait relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2509625106, 2024. [30] W. Kong, Q. Tian, Z. Zhang, R. Min, Z. Dai, J. Zhou, J. Xiong, X. Li, B. Wu, J. Zhang, K. Wu, Q. Lin, J. Yuan, Y. Long, A. Wang, A. Wang, C. Li, D. Huang, F. Yang, H. Tan, H. Wang, J. Song, J. Bai, J. Wu, J. Xue, J. Wang, K. Wang, M. Liu, P. Li, S. Li, W. Wang, W. Yu, X. Deng, Y. Li, Y. Chen, Y. Cui, Y. Peng, Z. Yu, Z. He, Z. Xu, Z. Zhou, Z. Xu, Y. Tao, Q. Lu, S. Liu, D. Zhou, H. Wang, Y. Yang, D. Wang, Y. Liu, J. Jiang, and C. Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. URL https: //arxiv.org/abs/2412.03603. [31] B. F. Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [32] G. Li, Y. Liu, and Q. Dai. Free-viewpoint video relighting from multi-view sequence under general illumination. Machine vision and applications, 25:17371746, 2014. [33] M.-H. Lin, M. Reddy, G. Berger, M. Sarkis, F. Porikli, and N. Bi. Edgerelight360: Textconditioned 360-degree hdr image generation for real-time on-device video portrait relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 831840, 2024. [34] X. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [35] Y. Mei, M. He, L. Ma, J. Philip, W. Xian, D. M. George, X. Yu, G. Dedic, A. L. Tasel, N. Yu, V. M. Patel, and P. Debevec. Lux post facto: Learning portrait performance relighting with conditional video diffusion and hybrid dataset, 2025. URL https://arxiv.org/abs/2503. 14485. [36] Mixamo. Mixamo., 2025. URL https://www.mixamo.com/. [37] R. Pandey, S. Orts-Escolano, C. Legendre, C. Haene, S. Bouaziz, C. Rhemann, P. E. Debevec, and S. R. Fanello. Total relighting: learning to relight portraits for background replacement. ACM Trans. Graph., 40(4):431, 2021. [38] W. Peebles and S. Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [39] Pexels. Pexels, 2025. URL https://www.pexels.com. [40] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 14 [41] P. Ponglertnapakorn, N. Tritrong, and S. Suwajanakorn. Difareli++: Diffusion face relighting with consistent cast shadows, 2025. URL https://arxiv.org/abs/2304.09479. [42] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. [43] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [44] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [45] T. Sun, J. T. Barron, Y.-T. Tsai, Z. Xu, X. Yu, G. Fyffe, C. Rhemann, J. Busch, P. E. Debevec, and R. Ramamoorthi. Single image portrait relighting. ACM Trans. Graph., 38(4):791, 2019. [46] L. Team. Luminabrush: Illumination drawing tools for text-to-image diffusion models, 2024. [47] VideoX-Fun. Videox-fun, 2025. URL https://github.com/aigc-apps/VideoX-Fun. [48] A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, J. Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [49] J. Wang, H. Yuan, D. Chen, Y. Zhang, X. Wang, and S. Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. [50] J. Wang, J. Liu, X. Sun, K. K. Singh, Z. Shu, H. Zhang, J. Yang, N. Zhao, T. Y. Wang, S. S. Chen, U. Neumann, and J. S. Yoon. Comprehensive relighting: Generalizable and consistent monocular human relighting and harmonization, 2025. URL https://arxiv.org/abs/2504.03011. [51] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: From error visibility to structural similarity. TIP, 2004. [52] Y. Yang, H. A. Sial, R. Baldrich, and M. Vanrell. Relighting from single image: Datasets and deep intrinsic-based architecture, 2024. URL https://arxiv.org/abs/2409.18770. [53] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng, D. Yin, Y. Zhang, W. Wang, Y. Cheng, B. Xu, X. Gu, Y. Dong, and J. Tang. Cogvideox: Text-to-video diffusion models with an expert transformer, 2025. URL https://arxiv.org/ abs/2408.06072. [54] Y.-Y. Yeh, K. Nagano, S. Khamis, J. Kautz, M.-Y. Liu, and T.-C. Wang. Learning to relight portrait images via virtual light stage and synthetic-to-real adaptation. ACM Transactions on Graphics (TOG), 41(6):121, 2022. [55] L. Zhang, Q. Zhang, M. Wu, J. Yu, and L. Xu. Neural video portrait relighting in real-time via consistency modeling. In Proceedings of the IEEE/CVF international conference on computer vision, pages 802812, 2021. [56] L. Zhang, A. Rao, and M. Agrawala. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In The Thirteenth International Conference on Learning Representations, 2025. [57] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [58] Y. Zhang, D. Zheng, B. Gong, J. Chen, M. Yang, W. Dong, and C. Xu. Lumisculpt: consistency lighting control network for video generation. arXiv preprint arXiv:2410.22979, 2024. [59] H. Zhou, S. Hadap, K. Sunkavalli, and D. W. Jacobs. Deep single-image portrait relighting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 71947202, 2019. [60] Y. Zhou, J. Bu, P. Ling, P. Zhang, T. Wu, Q. Huang, J. Li, X. Dong, Y. Zang, Y. Cao, A. Rao, J. Wang, and L. Niu. Light-a-video: Training-free video relighting via progressive light fusion, 2025. URL https://arxiv.org/abs/2502.08590."
        }
    ],
    "affiliations": [
        "Hangzhou Normal University",
        "Kunbyte AI",
        "Peking University",
        "University of Chinese Academy of Sciences",
        "Zhejiang University"
    ]
}