{
    "paper_title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
    "authors": [
        "Baorui Ma",
        "Jiahui Yang",
        "Donglin Di",
        "Xuancheng Zhang",
        "Jianxun Cui",
        "Hao Li",
        "Yan Xie",
        "Wei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research."
        },
        {
            "title": "Start",
            "content": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources Baorui Ma, Jiahui Yang, Donglin Di, Xuancheng Zhang, Jianxun Cui, Hao Li, Xie Yan, Wei Chen Li Auto Inc Abstract Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, cameradependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or taskspecific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as universal interface that decouples spatial reasoning from sensor and camera biases. Using 20M imagedepth pairs spanning reconstructed, captured, and rendered 3D data across 10,000+ camera models, we demonstratefor the first timea clear scaling trend in the metric depth track. The pretrained model excels at promptdriven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing new path toward scalable and efficient real-world metric perception. We opensource MetricAnything at https://metric-anything.github.io/metric-anything-io/ to support community research. 6 2 0 2 9 ] . [ 1 4 5 0 2 2 . 1 0 6 2 : r Figure 1: Overview of Metric Anything. (I) We aggregate diverse open-source 3D data into per-pixel metric depth maps, forming 20M imagedepth dataset captured by over 10,000 cameras across heterogeneous scenes. (II) Sparse Metric Prompts, generated by randomly masking depth maps, provide minimal interface that decouples spatial reasoning from sensor and camera biases, enabling metric depth learning from noisy, heterogeneous sources. (III) The pretrained model and its distilled prompt-free student generalize robustly across multiple downstream tasks, revealing clear scaling trend and establishing solid foundation for versatile, data-driven metric perception. Equal contribution. Corresponding author. Correspondence to {mabaorui2014@gmail.com}. Project leader."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work"
        },
        {
            "title": "2.1 Monocular Depth Estimation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.2 Metric Depth Estimation with Sparse Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3 Method"
        },
        {
            "title": "3.1 Multi-Source Data Collection .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.3 Prompt-Free Model Distillation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiment"
        },
        {
            "title": "4.1.1 Zero-Shot Depth Super-Resolution and Completion.\n4.1.2 Radar-Camera Depth Estimation.",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Prompt-Free Downstream Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 Monocular Depth Estimation. 4.2.2 Recovering Camera Intrinsics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.3 Zero-shot Boundaries Accuracy Measure . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.4 Multi-view Metric 3D Reconstruction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.5 VLA Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Spatial Understanding of MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.6 . . . 5 Ablation Study 5.1 Scaling up Data. . . . . 5.2 Network Architecture. 5.3 Runtime. . . . . . . . . . . . . . . . . . . . . 5.4 Test Time Resolution Scaling . 5.5 Training Objectives . 5.6 Prompt Setting . 5.7 Balance Weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Generalizability to Unseen Sensors, Scenarios, and Extreme Environmental Conditions 6.1 Generalization across Sensor Configurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Robustness under Environmental Degradation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Generalization to Unseen Visual Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Training Details 7.1 Training and Test Set Split. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Training Details of Pre-trained Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Training Details of Distilled Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Training Details of Vision-Language-Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.5 Training Details of Vision-Language-Action Model. . . . . . . . . . . . . . . . . . . . . . . . . . 7.6 Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Limitations 9 Conclusion 2 3 4 4 5 5 6 7 8 8 8 10 10 14 16 16 17 18 20 20 20 21 21 22 23 24 24 25 30 30 30 30 30 32 33"
        },
        {
            "title": "Introduction",
            "content": "Vision foundation models have achieved remarkable progress through scalingtraining larger models on everexpanding datasets to unlock emergent capabilities and robust generalization [23, 70, 47, 77, 79]. As AI systems increasingly interact with the physical world through robotics, AR, and autonomous driving, foundation models must go beyond 2D perception to perceive the 3D world. Monocular depth estimation (MDE) serves as fundamental bridge, providing depth cues essential for physical interaction. While relative depth estimation has demonstrated successful scaling through synthetic data and large-scale pseudolabel distillation [124, 125], metric depth estimation has not exhibited similar scaling trends in previous works, as illustrated in Fig. 2a. Unlike relative depths ordinal relationships, metric depth requires learning absolute, physically meaningful distancesa fundamentally harder problem compounded by data-related challenges. The core bottleneck lies in the heterogeneity of metric depth data sources. Real-world 3D annotations come from diverse sensors (LiDAR, RGB-D, stereo cameras) or reconstruction algorithms (SfM, MVS, SLAM), each introducing distinct noise patterns, hardware-specific artifacts, and camera-dependent biases. This heterogeneity creates three critical issues: (i) significant domain gaps between sources, (ii) noisy supervision from sensor misalignment and algorithmic matching failures, and (iii) metric ambiguity due to varying camera intrinsics. Together, these factors prevent the data soup scaling strategy that has proven successful in other domains, limiting current metric depth methods to careful curation of small, clean datasets. Recent approaches have attempted to address these challenges through prompt-based methods [60, 114, 140, 64, 106], using sparse depth points or simulated LiDAR cues to guide depth prediction. However, these methods remain limited in scopethey typically focus on specific prompt-driven downstream tasks with small datasets and rely on complex, hand-crafted prompt construction pipelines that introduce strong human priors. This task-specific engineering limits both scalability and generalization. In this work, we present fundamentally different perspective: rather than engineering better prompts for specific tasks, we demonstrate that simple, scalable prompt-based pretraining paradigm can unlock the metric depth perception capabilities with large-scale heterogeneous 3D data. In other words, we are not aiming to construct stronger model for particular prompt-driven task (e.g., depth completion or super-resolution). We posit simple yet effective pre-training paradigm to demonstrate new direction, which proves that with effective mitigation of data scarcity issues, scaling trends can similarly occur in the metric estimation track, just as in NLP and 2D vision. Our key insight is that sparse prompts can serve as universal interface to decouple spatial understanding from sensor-specific biases, enabling effective learning from diverse, noisy sources without complex engineering. To realize this vision, we introduce Metric Anything, minimalist pretraining paradigm tailored for metric depth estimation, which generates Sparse Metric Prompts by randomly masking portions of depth maps. This design intentionally avoids task-specific architectures, complex prompt construction rules, or strong 3D inductive biases, making specialized network design unnecessary. Instead, it enables the model to naturally learn structural and metric understanding directly from data. To support scaling, we assemble approximately 20M imagedepth pairs across three categories: reconstructed 3D data (SfM/SLAM/MVS), captured 3D data (LiDAR/ToF/RGB-D), and rendered 3D data. All samples are aligned with metric annotations; for raw point clouds, we obtain per-pixel depths by projection using known calibration. The collection spans over 10,000 camera models and diverse environments, enabling unified pretraining across heterogeneous 3D sources. The pretrained model, with its simple architecture and pretext pretraining objective, excels in prompt-based tasks such as depth completion, super-resolution, and radar-camera depth estimation. Leveraging dedicated distillation process, we further create prompt-free student model that achieves state-of-the-art performance across diverse downstream tasks, including monocular depth estimation, camera calibration, multi-view metric 3D reconstruction, and Vision-Language-Action (VLA) planning. We further demonstrate that leveraging the pretrained ViT from our Metric Anything as visual encoder substantially enhances the spatial reasoning capabilities of Multimodal Large Language Models. Remarkably, without any task-specific design, both the pretrained and distilled models consistently achieve state-of-the-art performance across these tasks. Our findings align with The Bitter Lesson [98]: general-purpose, data-driven methods systematically outperform hand-crafted designs. By demonstrating consistent performance gains with increasing scale (see Fig. 2a) and robust generalization (Fig. 2b) across ten downstream tasks, we establish prompt-based pretraining as scalable path toward general-purpose metric depth perception. Our contributions are: Metric Anything: minimalist prompt-based pretraining paradigm that employs Sparse Metric Prompt to decouple spatial understanding from sensor biases, enabling scalable learning from heterogeneous 3D sources. 3 (a) Scaling trend. Larger training dataset yields consistently higher zero-shot δ1 accuracy. (b) Task performance. Radar plot over downstream tasks; Larger area indicates better performance. Figure 2: Scaling and Generalization. MetricAnything exhibits clear scaling trend and strong overall downstream performance. Demonstrated Scaling: Aggregation of 20M diverse depth-image pairs reveals stable scaling trends in metric depth estimation, previously unseen in this domain. Universal Generalization: Both pretrained and distilled models achieve state-of-the-art performance across ten downstream tasks without task-specific engineering."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Monocular Depth Estimation Relative depth estimation. Over the past decade, relative depth has emerged as the dominant formulation because it can learn from heterogeneous and weakly supervised signalspairwise orderings [15, 16], stereo/MVS cues [32, 57], and pseudo-depth [82, 124]yielding broad data coverage and strong cross-domain generalization. Early CNNs treated depth as regression [25, 24, 50]. Subsequent work introduced adaptive discretizations and per-pixel classification [27, 6, 7] to better handle non-uniform distributions and ambiguity, but these methods often coupled models to dataset-specific depth ranges, limiting transfer. key advance was large-scale, multi-dataset training with scale-and-shift-invariant objectives: MegaDepth [57] highlighted the benefits of diverse training data, MiDaS [82] established mixing heterogeneous datasets with scale-and-shift-invariant losses, and transformer-based architectures brought global context beyond CNN locality constraints [23, 81]. Depth Anything further demonstrated practical scaling route for relative depth by combining synthetic supervision that is noise-free and highly consistent with massive pseudo-labell distillation (62 million images), producing strong generalization [124, 125]. Replicating this success for metric depth remains an open challenge. Prior works [38, 127, 8, 10, 110, 75, 74] have recognized that metric-aware perception is inherently more challenging than relative depth perception and requires substantially more data for effective learning. However, metric depth data, originating from diverse algorithms or hardware configurations, exhibits various types of noise and heterogeneous patterns, further impeding the benefits obtainable from scaling. Unlike prior approaches, our prompt-based pretraining paradigm explicitly addresses this inherent noise and heterogeneity, establishing viable scaling path for robust metric depth estimation. Metric Depth Estimation. Metric depth estimation has progressed from domain-constrained, fixed-intrinsics settings with limited RGB-D/LiDAR supervision [25, 126, 35] to camera-aware, open-domain formulations that couple depth with intrinsics or canonical camera transforms [8, 127, 38, 75, 10, 110]. Representative strategies include attaching domain-specific metric heads to relative-depth backbones (ZoeDepth) [8], learning canonical camera normalization for zero-shot transfer (Metric3D and its scaled variants) [127, 38], predicting canonical inverse depth with high-resolution ViTs (DepthPro) [10], and directly regressing metric 3D points with camera4 conditioned features (UniDepth) [75]. MoGe-2 further decouples metric scale from affine-invariant point maps [110, 109]. While MoGe-2 seeks gains via complex, non-learnable pipelines (e.g., edge detection and depth refinement) to exploit large-scale real data. In this work, we discard hand-crafted processing and propose pretraining paradigm from data-driven perspective that learns robust representation of metric depth from noisy and heterogeneous 3D sources, enabling the model to convert coarse data into scalable source of accurate pseudolabels. 2.2 Metric Depth Estimation with Sparse Priors This line of work predicts dense metric depth from RGB with sparse cues such as LiDAR, RGB-D, or radar [71, 132, 139, 19, 18, 100]. Representative approaches include: Marigold-DC, which repurposes diffusion model for guided denoising with dynamic scale and shift to achieve zero-shot densification [106]; OMNI-DC, which predicts multi-scale depth gradients and integrates them to reduce long-range drift, augmented with Laplacian loss for ambiguous regions and scale normalization for cross-dataset transfer [140]; and methods like PriorDA, which align metric priors with relative depth and refine structure under metric and geometric constraints using pixellevel alignment and distance-aware reweighting [114]. Other efforts explore foundation model prompting, such as PromptDA, which injects low-resolution noisy depth via multi-scale fusion decoder but relies on heavy data pipelines involving LiDAR noise simulation, 3D reconstruction hole filling, and mixed supervision [60]. Sensorspecific designs have also emerged, e.g., TacoDepth, which fuses radar and camera data in single stage using graph-based radar structure extraction and pyramid fusion [113]. Despite these advances, most methods remain confined to single prompt type (e.g., for depth completion or super-resolution) or single sensor modality. While PriorDA attempts to handle more diverse prompts, it still focuses on hand-designed prompt simulationsuch as creating missing areas, downsampling GT, or adding outliers and blurand remains limited to small-scale training datasets. In contrast, our work shows that minimizing human prior design in prompts and instead learning from naturally heterogeneous data offers more general and scalable approach. Through simple pretext tasks and data scaling, our model learns essential and robust spatial understanding, achieving state-of-the-art performance across diverse downstream tasksincluding multi-sensor fusion, monocular depth estimation, camera calibration, multiview 3D reconstruction, and VLA policywithout being confined to any single sub-task."
        },
        {
            "title": "3 Method",
            "content": "We aim to develop general-purpose MDE foundation model that achieves both fine structural fidelity and accurate metric perception, enabling robust generalization across diverse tasks. To this end, we first aggregate 20Mscale imagedepth dataset (Sec. 3.1) from open-source 3D datasets. We introduce sparse metric prompt-based pretraining paradigm that supports scalable learning from heterogeneous sources (Sec. 3.2). Finally, the pretrained knowledge is distilled into prompt-free student model for broader applicability (Sec. 3.3). An overview is shown in Fig. 1. 3.1 Multi-Source Data Collection To enable scalable pretraining, we aggregate open-source 3D datasets with metric annotations and standardize all inputs as metric depth maps with validity masks: {G, M}, RHW , {0, 1}HW , (1) where denotes the metric depth along the camera z-axis, and indicates valid measurements (M(p) = 1 for valid pixels and M(p) = 0 otherwise). Reconstructed 3D Data. We collect depth maps from open-source datasets [4, 40, 103, 116, 122, 5, 20, 107] produced by wide range of reconstruction techniques, such as SfM, SLAM, MVS, binocular/multi-view stereo (e.g., plane-sweeping, PatchMatch), and dense temporal stereo. These depth maps are represented as per-pixel metric depth with validity mask M, as defined above. Such maps often contain artifacts and missing regions, especially in weakly textured areas, specular or metallic surfaces, thin structures, sharp boundaries, repetitive patterns, depth discontinuities, occlusions, or under motion blur and dynamic scenes. These errors typically arise from incorrect or lost matches during reconstruction. Captured 3D Data. We also incorporate point clouds captured by LiDAR, ToF, and RGB-D sensors [30, 120, 37, 129, 78, 117, 14, 95, 97, 1, 34, 21]. Given the camera intrinsics and sensor-to-camera pose Tcs = [R t], depth maps {G, M} are obtained by projecting each sensor-frame point Xs onto the corresponding camera plane. Sensor-derived depth is often noisy and sparse due to limitations in sensor resolution and the influence of 5 Figure 3: Percentile Depth Range Comparison from Seven Datasets (Real-world vs. Our Pseudo Labels). environmental factors such as severe weather. Additional sources of error include beam divergence and sampling gaps (affecting thin structures), weak or anomalous returns on specular, metallic, or transparent/semi-transparent surfaces, multi-path interference, mixed pixels at sharp boundaries, out-of-range distances, occlusions, and motioninduced time offsets. These factors result in invalid measurements, indicated by M(p) = 0 for affected pixels. Rendered 3D Data. To complement the real-world data, we also include small portion of depth maps rendered from virtual engines [118, 26, 84, 28, 108, 112, 2, 56, 33, 136, 33, 86]. Although these synthetic depth maps exhibit limited visual realism and diversity, they are completely noise-free, preserve fine structural details, and display sharp geometry, thereby providing valuable geometric supervision. In total, this purposefully aggregated dataset comprises approximately 20 million imagedepth pairs, captured by over 10,000 distinct camera models and spanning wide variety of real and synthetic scenes. 3.2 Pretraining via Sparse Metric Prompt Problem Formulation. Given monocular image RHW 3 and corresponding sparse metric prompt, = {(xi, yi, di)}N where each triplet (xi, yi, di) represents pixel coordinate (xi, yi) and its associated metric depth value di. The objective of pretraining is to learn function fθ, parameterized by θ, that predicts dense metric depth map from the input image I, conditioned on the prompt : i=1, (2) = fθ(I, ), RHW . Here, prompt provides metric depth for small subset of pixels, offering limited geometric cues. The model fθ learns to propagate these sparse metric constraints across the image, yielding complete, geometrically consistent, and metrically accurate depth map. Since our data sources inevitably contain inherent noise and varying patterns of incompleteness, fθ is trained to identify and correct potential errors within the sparse prompt . (3) Prompt Preparation. From each source in {G, M}, we randomly sample [2,000, 40,000] valid pixels (around 1% of all image pixels) from its depth map to construct sparse metric prompt and its corresponding mask . Due to diverse patterns, sparsity, and incompleteness across {G}, the sparsely sampled prompt inevitably inherits heterogeneous distributions and irregular structures, which challenge models to effectively handle diverse distributions. To mitigate this, we first apply prompt preparation step that maps all prompts into shared intermediate domain. Following PriorDA [114], we apply pre-trained depth prediction model [10] to obtain prior depth map Pd. Then, inspired by See3D [68], we perform Pixel-wise Depth Scale Alignment (PDSA) and Global Metric Depth Recovery (GMDR) to fill the missing regions in under the guidance of Pd. The final input prompt is constructed by concatenating the PDSA-refined map, the GMDR-corrected map, and the prompt mask along the channel dimension. This yields unified and regularized representation of shape 3, avoiding issues with irregular prompt network design and training efficiency, while also easing the handling of diverse prompts. 6 Figure 4: Skip-Connection in ViT-DPT Architecture. Prompt Injection. Although this preparation process can maintain metric consistency between Pd and Prompt through pixel-wise and global alignment, its parameter-free (non-learning) nature remains sensitive to sampling patterns and noise propagated from prompt . To further address this, we employ prompt-injection mechanism that allows the model to correct noisy prompts and generate accurate, dense depth predictions. Following prior works, several strategies can be used for prompt injection, including adaptive layer normalization (AdaLN) [72], cross-attention [105, 85], ControlNet-style conditioning [130], conditioned DPT heads [60], and conditioning layers parallel to the RGB input [114]. Among these, we opt for conditioned DPT head, as it introduces only about 5% extra parameters while maintaining efficiency through lightweight interpolation and shallow convolutional layers. This design allocates most parameters to the foundational depth backbone, allowing the main network pathway to correct noise and structural inconsistencies, rather than relying on the injected condition layers. Importantly, we make no bespoke modifications to the backbone; instead, we reuse general, efficient, and widely validated architecture [10]. The only difference is that we merge the patch encoder and image encoder into single shared ViT to achieve balance between accuracy and efficiency, as detailed in our ablation studies. This emphasizes our deliberate shift away from hand-crafted priors and manually designed prompts, relying entirely on data-driven learning. By isolating conditioning from architectural changes, we can cleanly assess the effectiveness of the learned prompts and rigorously study the scaling behavior. Pre-training Objective. Following DepthPro [10], we use mean absolute error (MAE) and scale-and-shift invariant mean absolute gradient error (SSI-MAGE) losses for synthetic data. For real-world data, we adopt robust MAE loss that drops top-n-largest-loss regions per image during training to mitigate noise sensitivity, where is set as 20%. The overall training objective is formulated as: Ltotal = αLM AE + βLSSIM AGE. (4) 3.3 Prompt-Free Model Distillation Distillation Process. To validate the benefits of large-scale pretraining and transfer them to scenarios where prompts are unavailable, we distill the pre-trained (teacher) model into prompt-free student that performs dense depth prediction solely from RGB inputs. Specifically, we leverage the trained teacher model to generate high-fidelity pseudo depth maps conditioned on sparse prompt for all real-world images in the collected dataset via simple feed-forward process, effectively transforming sparse and noisy prompts into dense, high-quality supervision signals (i.e., pseudo labels). Unlike conventional RGB-D or LiDAR sensorswhose effective sensing range is constrained by hardware to only few to tens of metersand many depth-estimation algorithms that struggle to estimate distant backgrounds, our teachergenerated pseudo labels span diverse environments and cover both near and far distances. These teacher labels train prompt-free monocular depth estimation model, preserving the teachers metric understanding without prompts. Student Selection. straightforward approach is to directly reuse the pre-training network architecture as the student backbone while removing the prompt layers. However, in our preliminary experiments, this approach resulted in suboptimal performance when trained on the teacher labels, despite many prior works [10, 110, 38, 8] successfully training similar ViTDPT configurations from scratch. 7 We hypothesize two main causes for the suboptimality. First, our teacher model generates reliable pseudo labels across both near and far regions, unlike most real-world datasets biased toward short-range depths (see Fig. 3 for comparison). This wider depth distribution exposes limitations of conventional depth losses: direct depth supervision (L1/L2) blurs fine geometric details, while inverse-depth loss decays too rapidly with distance, losing effective supervision for distant regions. Consequently, these standard losses are suboptimal for supervising our high-fidelity, large-range pseudo labels. Second, typical ViT encoders with DPT-head decoders use U-Net-style skip connections, injecting shallow features into deeper layers and propagating deep features upward. While this stabilizes training under noisy supervision, since the low-level cues in the ViT encoder (e.g., textures and colors) are more consistent and easier to learn. The low-level feature is connected via skip connection to the DPT head near the output, which mitigates conflicts between the output and the noisy supervision, thereby smoothing gradient fluctuations. However, it can underutilize high-level semantic cues essential for precise depth. In our setting, pseudo labels are generated by unified model, exhibit minimal domain gap, and have been refined to correct most noisy. As result, we can reduce reliance on shallow-to-deep feature injection and explore more aggressive network designs that fully exploit the rich semantic cues provided by deep block of the ViT encoder. Improved Student. Drawing on these observations and analyses, we retain the multi-scale fusion mechanism proposed by [10] while introducing two key modifications. First, we design distance-balanced inverse-depth loss that preserves fine-grained sensitivity in near regions while extending effective supervision to long-distance areas. Depth values in log-space are defined as: Dlog = 1 ln(x)/ln(C), (5) where is hyperparameter that controls the trade-off between long-range and short-range supervision. Second, leveraging the high-fidelity teacher labels, we invert the conventional skip-connection scheme between the ViT encoder and DPT head: injecting deep, high-level ViT features into the deeper decoding layers near the output, while shallow, low-level features feed into the shallower decoder layers, as shown in Fig. 4. This inversion emphasizes semantic reasoning at the final prediction stage, fully exploiting the rich semantic cues embedded in the teacher-generated pseudo labels, which already exhibit low noise and high structural consistency. In this way, we experimentally demonstrate that the student model achieves stable, prompt-free metric depth estimation, effectively distilling the metric perception capability from the pre-trained model."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Prompt-Based Downstream Task 4.1.1 Zero-Shot Depth Super-Resolution and Completion. We study depth super-resolution and completion in zero-shot setting, where our pretrained model directly takes sparse or low-resolution depth maps as prompts, without any task-specific finetuning. We compare two categories of baselines: (a) post-aligned MDE, including DepthAnything V2 [125] and DepthPro [10]; and (b) priorbased MDE, including LingBot-Depth [99], [60], PriorDA [114], DepthLab [64], Omni-DC [140], and MarigoldDC [106]. Following PriorDA [114], we construct four prompt typesLiDAR-like sparse scans, extremely sparse samples (100 points), and depth maps downsampled by 8 and 16. All prompts are directly fed into our pretrained model for zero-shot inference on unseen datasets, including NYUv2 [91], ETH3D [89], and KITTI [31], covering indoor, outdoor, and open-world scenarios. As shown in Tab. 1 and Fig. 5, our pretrained model demonstrates strong zero-shot performance across all prompt types and datasets, consistently outperforming both post-aligned and prior-based baselines. Unlike previous approaches [114, 140] that generate synthetic prompts (e.g., LiDAR simulation or noisy downsampling) to approximate test-time conditions, our model is trained only once with simple, sparsely sampled prompts and operates fully zero-shot manner, without any task-specific design or prompt alignment. This enables superior generalization across diverse prompt densities, spatial layouts, and scene domains. 8 Figure 5: Visualization of Depth SR and Completion. Our method better recovers missing regions with improved structure. Table 1: Zero-shot Depth Super-Resolution / Completion (AbsRel %). NYUv2 ETH-3D KITTI Method 8 16 LiDAR Extreme 8 16 LiDAR Extreme 8 16 LiDAR Extreme DAv2 [125] DepthPro [10] Omni-DC [140] Marigold-DC DepthLab [64] PromptDA [60] PriorDA [114] LingBot-Depth [99] Ours-Pretrain 4.77 4.48 1.57 1.83 2.60 1.61 1.73 2.23 1.53 5.13 4.83 3.11 3.32 3.73 1.75 2.79 3.05 1.86 4.85 4.47 2.12 1.90 4.30 17.59 2.01 2. 1.70 4.77 4.41 2.63 2.13 6.30 16.96 2.01 2.03 2.08 6.27 5.88 1.86 2.33 2.60 1.80 2.06 1.18 1.46 7.38 6.79 4.09 4.75 4.50 2.56 3.91 1. 2.03 7.41 5.31 1.88 2.27 6.40 18.86 1.90 13.19 0.87 6.51 6.51 1.98 2.03 8.01 18.18 1.61 4.12 0.84 9.54 6.76 4.05 5.17 17.17 3.92 4.54 4. 11.22 9.16 8.35 9.47 22.90 4.95 8.20 7.13 2.34 3.53 8.86 6.05 5.27 6.88 37.17 21.96 4.81 6.55 3.53 9.25 6.19 4.17 5.62 40.29 21.39 3.76 3. 3.36 4.1.2 Radar-Camera Depth Estimation. Radar sensing has gained increasing attention in autonomous driving and robotics for its low cost, all-weather capability, and long effective range. The task of radarcamera depth estimation aims to recover dense metric depth by fusing RGB frames with mmWave Radar. However, Radar point clouds are extremely sparse and noisyover thousand times sparser than LiDAR, and often misaligned spatiotemporally with other onboard sensors, leading to substantial modality gap from the sparse metric prompts used in our pretraining. To ensure fair evaluation, our pretraining datasets intentionally exclude all Radar data to avoid information leakage. We therefore use this task to rigorously assess the transferability of our pretrained model to an unseen sensor. We conduct two experiments on the nuScenes dataset [12] using the standard split [113, 67, 54, 62, 66, 96, 93]: (1) training the teacher model from scratch using Radar signals as prompts and LiDAR maps as ground truth; and (2) finetuning the pretrained teacher model under the same supervision. For comparison, we include several 9 Table 2: Radar-Camera Depth Estimation. Comparisons on the nuScenes [12] dataset (in millimeters) Type Method 0-50m 0-70m 0-80m MAE RMSE MAE RMSE MAE RMSE Independent Plug-in Metric Anything-Pretrain Lin et al. [62] RC-PDA [67] R4Dyn [29] DORN [65] Singh et al. [93] CaFNet [96] Li et al. [55] TacoDepth [113] RadarCam-Depth [54] TacoDepth [113] From-Scratch Finetune 2034.9 2225.0 1926.6 1727.7 1674.2 1524.5 1423. 1286.1 1046.8 1335.4 651.4 4316.5 4156.5 4124.8 3746.8 3674.5 3567.3 3275.8 2964.3 2487.5 2958.8 2084.4 2294.7 3326.1 2380.6 2073.2 2010.3 1822.9 1712. 1587.9 1347.1 1622.9 863.6 5338.2 6700.6 5252.7 4590.7 4493.1 4303.6 3960.5 3662.5 3152.8 3788.1 2771.6 2371.0 3713.6 2467.7 2179.3 2109.8 1927.0 1833. 1689.7 1492.4 2101.7 934.5 5623.0 7692.8 6434.0 5554.3 4898.7 4765.6 4609.6 4150.2 3948.0 3324.8 4033.2 3057.5 Radarimage fusion baselines, including Lin et al. [62], RC-PDA [67], R4Dyn [29], DORN [65] , Singh et al. [93], CaFNet [96], Li et al. [55], RadarCam-Depth [54] and TacoDepth [113]. As shown in Tab. 2, finetuning our pretrained model with Radar prompt achieves state-of-the-art performancenearly doubling the accuracy of its from-scratch counterpart and surpassing all prior fusion methods. These results demonstrate that our pretraining paradigm scales effectively: training with randomly sampled sparse prompts allows the model to learn versatile metric representations, enabling seamless adaptation to new and unseen sensing modalities such as mmWave Radar. In contrast to previous works that focus on training and testing with single sensors, our experimental results suggest that future research should pay more attention to general pretraining paradigms. 4.2 Prompt-Free Downstream Tasks 4.2.1 Monocular Depth Estimation. Metric prediction can be broadly divided into two tracks: (1) Monocular metric depth map estimation and (2) Monocular geometry estimation. The first track aims to predict dense point map where each pixel encodes the metric distance from the camera origin to the visible surface point along its viewing ray. The second track is monocular geometry estimation, which focuses on predicting per-pixel 3D point map in the camera coordinate system. Monocular metric depth map. For metric depth map prediction, we design dedicated student network (Sec.3.3) and train it from scratch, denoted as Student-DepthMap in Tab. 3 and Tab. 4. For comparison, we evaluate our method against wide range of recent approaches, including Depth Anything [124], Depth Anything V2 [125], Metric3D [127], Metric3D V2 [38], PatchFusion [58], UniDepth [75], ZoeDepth [8], and DepthPro [10]. We adopt the δ1 metric, which measures the proportion of inlier pixels based on relative error thresholds, and report results on six datasets Booster [80], Middlebury [88], Sun-RGBD [94], ETH3D [90], NuScenes [12], and Sintel [11]. These datasets cover indoor/outdoor scenes, cartoon or game scenes, street views and extreme situations such as severe weather and lighting conditions. Notably, none of them are used for training, validation, or hyper-parameter tuning all results are evaluated under strict zero-shot protocol. Comprehensive evaluation metrics, including AbsRel [49], Log10 error, δ2/δ3 thresholds, and RMSE are provided in the Tab. 4. Quantitative results are presented Fig. 6 and Fig. 7. To ensure strict zero-shot protocol, we exclude any modeldataset pair if the evaluation set was used during model training. Performance is measured using standard depth estimation metrics: AbsRel, log10, and threshold accuracy scores (δ1, δ2, δ3). As reported in Tab. 3 and Tab. 4, our method exhibits remarkable robustness and consistency across diverse domains: Indoor & Complex Geometry: On Sun-RGBD and Middlebury, which feature cluttered indoor scenes and high-resolution details respectively, our method achieves the best performance (Rank 1), significantly outperforming Metric3D v2 and Depth Pro (e.g., AbsRel 0.085 vs. 0.156 on Sun-RGBD). This indicates our models superior capability in resolving intricate local geometry in constrained spaces. 10 Table 3: Monocular depth estimation performance of our Student-DepthMap against prior methods, measured by δ1 accuracy (%, ). Method ETH3D Booster NuScenes Sun-RGBD Sintel Middlebury Rank DepthAnything [124] DAV2 [125] Metric3D [127] Metric3D-v2 [38] PatchFusion [58] UniDepth [75] ZoeDepth [8] DepthPro [10] Student-DepthMap 9.3 36.3 34.2 87.7 51.8 25.3 34.2 41.5 79.9 52.3 59.5 4.7 39.4 22.6 27.6 21.6 46.6 59.5 35.4 17.7 64.4 82.6 20.4 83.6 28.1 49. 88.1 85.0 72.4 16.9 75.6 53.6 95.8 85.7 89.0 97.7 6.9 5.9 17.3 38.3 14.0 16.5 7.8 40.0 27.7 39.3 37.2 13.6 29.9 49.9 31.9 53.8 60. 65.8 6.00 6.17 6.83 4.17 6.00 5.00 5.83 3.17 1.50 Table 4: Monocular Depth Estimation on Diverse Unseen Datasets. Zero-shot evaluation across six benchmarks covering indoor, outdoor, driving, and synthetic domains. Our method consistently ranks 1st or 2nd across all six datasets. Method AbsRel Log10 ETH3D [90] DepthAnything [124] DepthAnything v2 [125] Metric3D [127] Metric3D v2 [38] PatchFusion [58] UniDepth [75] ZoeDepth [8] Depth Pro [10] Ours-Student-Depthmap 1.682 0.370 0.859 0.124 0.256 0.457 0.500 0.327 0.147 0.380 0.173 0.240 0.053 0.106 0.186 0.176 0. 0.064 Method AbsRel Log10 Sintel [11] DepthAnything [124] DepthAnything v2 [125] Metric3D [127] Metric3D v2 [38] PatchFusion [58] UniDepth [75] ZeroDepth [36] ZoeDepth [8] Depth Pro [10] Ours-Student-Depthmap 3.973 2.226 1.733 0.370 0.617 0.869 0.703 0.946 0.508 0.792 0.559 0.494 0.387 0.216 0.391 0.301 0.491 0.392 0.230 0.227 Middlebury [88] Method AbsRel Log10 DepthAnything [124] DepthAnything v2 [125] Metric3D [127] Metric3D v2 [38] PatchFusion [58] UniDepth [75] ZeroDepth [36] ZoeDepth [8] Depth Pro [10] Ours-Student-Depthmap 0.273 0.262 1.251 0.450 0.250 0.324 0.377 0.214 0.251 0.200 0.149 0.141 0.305 0.152 0.108 0.127 0.179 0.115 0. 0.082 δ2 19.78 64.66 49.29 99.55 88.38 57.67 64.45 61.31 δ3 31.06 86.26 57.57 99.90 97.31 81.48 81.43 71.23 97. 99.90 δ2 15.42 18.70 32.38 62.92 35.52 35.72 25.63 22.70 59.25 δ3 27.28 33.82 44.79 76.87 51.44 57.26 37.08 44.97 71.14 50. 70.57 δ2 69.62 72.07 37.53 73.32 87.17 80.05 67.06 77.68 93.17 δ3 86.06 90.55 58.73 88.61 98.15 99.62 78.95 90.86 96.40 96. 99.93 Method AbsRel Log10 nuScenes [12] DepthAnything [124] DepthAnything v2 [125] Metric3D [127] Metric3D v2 [38] PatchFusion [58] UniDepth [75] ZeroDepth [36] ZoeDepth [8] Depth Pro [10] Ours-Student-Depthmap 0.453 0.614 0.422 0.197 0.392 0.138 0.237 0.498 0.287 0.152 0.151 0.326 0.132 0.080 0.226 0.060 0.121 0.182 0.164 0.063 Method AbsRel Log10 Sun-RGBD [94] DepthAnything [124] DepthAnything v2 [125] Metric3D [127] Metric3D v2 [38] PatchFusion [58] UniDepth [75] ZoeDepth [8] Depth Pro [10] Ours-Student-Depthmap 0.114 0.182 1.712 0.156 0.466 0.087 0.123 0.113 0.085 0.053 0.070 0.382 0.076 0.961 0.037 0.053 0. 0.033 Method AbsRel Log10 Booster [80] DepthAnything [124] DepthAnything v2 [125] Metric3D [127] Metric3D v2 [38] PatchFusion [58] UniDepth [75] ZoeDepth [8] Depth Pro [10] Ours-Student-Depthmap 0.317 0.315 1.332 0.417 0.719 0.500 0.610 0.336 0.282 0.114 0.110 0.346 0.140 0.213 0.166 0.195 0.118 0.100 δ2 73.88 31.84 77.22 93.25 48.74 93.01 82.60 64.95 73. δ3 90.30 47.27 83.61 95.74 76.04 96.42 89.91 82.70 84.25 96.56 98.26 δ2 98.81 97.65 27.00 96.35 60.15 99.33 97.95 98. δ3 99.77 99.46 34.12 99.55 60.65 99.80 99.51 99.55 99.31 99.65 δ2 79.62 76.24 13.07 75.78 49.39 60.90 52.66 79. δ3 95.23 94.28 33.98 92.83 72.89 89.21 75.51 96.52 84.11 96.83 Outdoor & Driving: On nuScenes and ETH3D, our method remains highly competitive, ranking 2nd in absolute error metrics while often achieving the highest accuracy in stricter thresholds (δ3). However, since these GT datasets inherently contain noise, we consider δ3 metric to be more reasonable and Figure 6: Zero-shot Visual Comparisons on Challenging Test Samples. Our model robustly captures details of thin structures and in scenes with difficult lighting where competitors often fail. robust. This suggests that our model effectively handles large-scale depth variations typical of outdoor environments. Robustness on Unconventional Data: On Booster, dataset known for challenging lighting and textures, our method outperforms all baselines (AbsRel 0.282), highlighting its resilience to domain shifts. Even on the synthetic Sintel dataset, where domain gaps are significant, we maintain strong performance (2nd best in Log10), demonstrating that our learned representations generalize well beyond photorealistic domains. 12 Figure 7: Qualitative Comparison of Monocular Depth Estimation. Compared with MoGe2 and UniDepthv2, our distilled model produces more detailed and geometrically plausible predictions for both depth maps and point maps. Overall, while some baselines excel in specific niches, our method delivers the most balanced and consistently high performance across the full spectrum of test scenarios. Monocular metric point map. For metric point map prediction, we leverage pseudo-labels generated by our pretrained model to fine-tune recent state-of-the-art methods such as MoGe-2 [110], denoted as Student-PointMap. This design allows us to assess the generality and precision of our pseudo-labels under different training paradigms whether training from scratch or fine-tuning, and regardless of whether the output head predicts depth maps or 3D point maps. 13 Figure 8: Qualitative Comparison of Point Maps. The red arrows indicate the GT distance, the yellow arrows indicate the distance from predicted point map. Monocular 3D geometry estimation aims to recover per-pixel 3D point map in the camera coordinate system. In this setting, we leverage pseudo-labels generated by our pre-trained model to fine-tune recent state-of-the-art frameworks such as MoGe-2 [110], denoted as Student-PointMap. This design enables comprehensive evaluation of the generality and precision of our pseudo-labels across different training paradigmsincluding training from scratch versus fine-tuning, and varying output representations (depth maps or 3D point maps). As shown in Fig. 8, Fig. 9, Tab. 5 and Tab. 6, our distillation approach consistently achieves state-of-the-art performance, demonstrating its robustness to differences in prediction heads and network initialization. We adopt the GIANTLARGE and DA3MONO-LARGE variants from the official DepthAnything3 [59] checkpoints, which represent the largest and most powerful models that support monocular metric depth estimation. However, the performances of GIANT-LARGE and DA3MONO-LARGE are not particularly satisfactory in our setting. We conjecture that the capability of DepthAnything3 [59] still depends on inferring matching relationships across multiple views, making accurate metric scale recovery particularly challenging in complex monocular settings. Additionally, Depth Anything3 [59] does not support inference when the cameras intrinsic parameters are unknown, which limits its applicability. In contrast, we evaluated its performance under both known and unknown intrinsic parameter conditions. 4.2.2 Recovering Camera Intrinsics. Furthermore, we utilize the point map predicted by our finetuned model (Student-PointMap ) to infer the intrinsic parameters of the camera from straightforward optimization. Throughout our experiments, we assume unit aspect ratio and that the principal point is approximately centered in the image; therefore, the only unknown intrinsic parameter is the focal length of the first camera, denoted . We estimate by minimizing weighted 14 Table 5: Monocular Geometry Estimation without Camera Intrinsics. Our Student-PointMap model achieves the best average rank (1.88), demonstrating superior robustness under unknown camera parameters. Method ZoeDepth [8] MASt3R [52] DAV1 [124] DAV2 [125] UniDepth V1 [75] UniDepth V2 [74] Depth Pro [10] MoGe-2 [110] DAV3-Metric-Large [59] DAV3-Nested-G-L [59] KITTI [31] ETH3D [90] iBims-1 [48] DIODE [104] AbsRel 17.0 56.7 11.6 10.6 4.69 8.58 23.5 18.1 11.8 83.2 δ1 AbsRel 85.4 9.84 94.5 88.6 98.4 95.4 38.3 62.9 83.0 0.00 57.1 47.2 40.2 36.1 56.9 20.7 38.5 10.4 11.0 80.7 δ1 AbsRel 33.7 20.1 24.0 36.3 14.9 69.5 32.8 90.8 90.1 0. 17.4 18.7 12.9 11.1 23.8 9.52 15.9 13.6 27.6 39.1 δ1 AbsRel 67.2 61.5 81.8 91.7 57.6 93.2 81.5 83.0 43.5 20.9 39.3 54.9 58.0 41.2 17.1 43.0 31.9 17.5 24.2 51.9 δ1 AbsRel 29.3 19.0 16.2 22.1 71.9 51.8 37.7 66.4 53.7 26.4 32.70 44.38 30.68 24.75 25.62 20.45 27.45 14.90 18.66 63.73 Avg. δ1 53.90 27.61 54.13 59.68 60.70 77.48 47.58 75.78 67.59 11. Rank 7.00 9.13 6.88 4.88 5.50 3.63 6.75 4.13 5.88 10.38 Ours-Student-Pointmap 3.22 97.7 10.8 90. 11.3 86.6 13.9 79.8 9.81 88. 1.88 Table 6: Monocular Geometry Estimation with Provided Camera Intrinsics. When ground-truth intrinsics are provided, our Student-PointMap model significantly outperforms the baseline metric depth estimation models. Method Metric3D V2 [38] UniDepth V1 [75] UniDepth V2 [74] MoGe-2 [110] KITTI [31] ETH3D [90] iBims-1 [48] AbsRel 5.25 4.43 5.98 8.64 δ1 AbsRel 98.0 98.5 97.7 93.7 11.8 44.5 15.0 10.5 δ1 AbsRel 88.8 26.7 85.2 92.2 9.96 22.6 7.71 9. DIODE [104] δ1 1.98 63.5 67.1 77.1 δ1 Rel 94.1 49.1 21.0 60.5 95.5 41.0 16.2 92.4 AbsRel 19.03 23.13 17.42 11.32 Avg. δ1 70.72 62.30 86.38 88.85 Rank 3.38 3.75 3.13 2.88 Ours-Student-Pointmap 5.47 99.6 9.75 92.4 9.47 92. 14.1 74.3 9.69 89.56 1.88 reprojection error, = arg min (cid:88) (cid:88) i=0 j= (cid:12) (cid:12) (cid:12) (cid:12) (i, j) (Xi,j,0, Xi,j,1) Xi,j,2 (cid:12) (cid:12) (cid:12) (cid:12) , (6) where and are the pixel coordinates expressed relative to the image center, and Xi,j,k denotes the k-th component of the point map at location (i, j). The optimization in (6) is efficiently solved with few iterations of Weiszfeld-type algorithm [115], yielding the estimated focal length . We compare our model to optimizated methods using point map [75, 111, 128] and learning-based camera calibration [43, 138]. As shown in Tab. 7, we achieve the best average accuracy in terms of mean error and median error in degrees. Figure 9: Qualitative Comparison of Depth Map. Compared with MoGe2 and UniDepthv2, our StudentPointMap model, which is finetuned from MoGe2 using pseudo-labels predicted by the our pretrained model, achieves more stable and accurate depth estimation. 15 Table 7: Evaluation Results for Camera Calibration in Degrees."
        },
        {
            "title": "Method",
            "content": "Perspective [43] WildCam [138] Depth Pro [10] LeReS [128] DUSt3R [111] UniDepth [75] MoGe-2 [110] Student-PointMap ETH3D [90] iBims-1 [48] Mean Med. Mean Med. Mean Average Med. Rank 13.6 7.70 7.18 8.26 5.77 10.7 4.66 2.50 11.9 5.81 6.34 7.19 3.60 9.96 3.04 1. 10.6 9.48 4.12 18.4 3.83 11.9 8.17 3.74 9.30 9.08 2.58 17.5 2.53 5.96 6.77 3. 12.1 8.59 5.65 13.33 4.80 11.3 6.42 3.12 10.6 7.45 4.46 12.35 3.07 7.96 4.91 2. 7.25 5.00 3.50 7.00 2.25 6.25 3.25 1.50 4.2.3 Zero-shot Boundaries Accuracy Measure We evaluate the sharpness of the predicted geometry by our Student-DepthMap using two synthetic benchmarks, Spring [69] and Sintel [11], together with the real-world iBims-1 [48] dataset. As reported in Tab. 8, our method achieves the best average accuracy boundaris compared to DepthPro [10] and MoGe-2 [110]. 4.2.4 Multi-view Metric 3D Reconstruction. To validate the accuracy and cross-view consistency of our monocular depth predictions without prompt, we compare with Map Anything [45], recent state-of-the-art method for multi-view metric 3D reconstruction method. We use the officially released checkpoint without any additional post-processing or finetuning. Our evaluation takes the multi-view images together with the per-frame monocular depth maps predicted by our prompt-free model as input. No cross-frame correction or post-processing optimization techniques (e.g., Bundle Adjustment) is applied, the 3D metric reconstruction results were obtained through single feed-forward. We report results on the ETH3D [90] and ScanNet [21] test sets, as show in Tab. 9. Compared with the baseline, our approach achieves superior performance, particularly in metric-scale estimation, as show in Fig. 10. The results show that our proposed pretraining paradigm can transfer its capabilities to prompt-free student model via distillation. In promptfree setting, our model exhibits strong cross-view-consistent depth prediction without relying on any additional conditions. It delivers virtually cost-free improvements in multi-view 3D metric reconstruction accuracy, relying solely on powerful monocular priors rather than multi-view matching-based inference. Table 8: Zero-shot Depth Boundary (F1 score [10]) on Multiple Benchmarks."
        },
        {
            "title": "Method",
            "content": "Relative DepthAnything [124] DepthAnything v2 [125] Marigold [44] Absolute DPT [81] Metric3D [127] Metric3D v2 [38] ZoeDepth [8] PatchFusion [58] UniDepth [75] Depth Pro [10] UniDepth-v2 [74] MoGe-v2 [110] Student-DepthMap Sintel F1 Spring F1 iBims F1 Rank 0.261 0.228 0.068 0.181 0.037 0.321 0.027 0.312 0.316 0.409 0.344 0.282 0.382 0.0109 0.0610 - - - 0.0723 0.0043 - 0.0017 0.1100 0.0737 0.0890 0.1635 0.127 0.111 0.149 0.113 0.055 0.096 0.035 0.134 0.039 0.176 0.138 0.194 0.179 7.33 8.00 7.50 9.00 11.50 6.33 11.33 6.00 8.67 2.00 4.00 3.67 1.67 Figure 10: Auxiliary monocular depth inputs improve performance of MapAnything. The red arrows indicate the GT distance, the yellow arrows indicate the distance from 3D reconstruction. . Table 9: Integrating our Metric Anything Student module into the MapAnything baseline yields significant performance gains on multi-view metric 3D reconstruction (AbsRel, δ1 in %). Method MapAnything [45] MapAnything + Ours ETH3D [90] Scannet [21] Avg AbsRel 20.43 18.98 δ1 69.07 73.94 AbsRel 37.61 5. δ1 61.78 99.41 AbsRel 29.02 12.44 δ1 65.42 86. 4.2.5 VLA Planning We distill our Metric Anythings capability into Vision-Language-Action (VLA) models for action planning. Results (Tab. 10) demonstrate SoTA performance. 3D spatial perception is essential for manipulation in the physical world. Prior work [9] attempt to input depth maps from depth sensors or offline depth-estimation models, along with RGB observations, into vision-language-action (VLA) policies model to improve performance. However, such approaches still require additional hardware or depth predictors at test time, which increases deployment complexity on real robots. Following MolmoACT [51], we distill the depth-perception capability of our trained prompt-free model into the VLA model by supervising it to predict depth tokens, rather than consuming depth Figure 11: Enhancing VLA Planning with Metric Anything. We distill the depth-perception capability of Metric Anything into the VLA model by supervising it to predict metric-aware depth tokens. 17 Method TraceVLA [137] Octo-Base [102] OpenVLA [46] SpatialVLA [76] CoT-VLA [135] NORA-AC [41] WorldVLA [13] π0-FAST [73] ThinkAct [39] Baseline-DAV2 [125] Ours Table 10: LIBERO benchmark success rates by task category. Object Spatial Goal 84.6 78.9 84.7 88.2 87.5 85.6 87.6 96.4 88.3 87.0 88. 85.2 85.7 88.4 89.9 91.6 89.4 96.2 96.8 91.4 95.4 94.4 75.1 84.6 79.2 78.6 87.6 80.0 83.4 88.6 87.1 87.6 88.8 Long 54.1 51.1 53.7 55.5 69.0 63.0 60.0 60.2 70. 77.2 78.8 Avg 74.8 75.1 76.5 78.1 83.9 79.5 79.1 85.5 84.4 86.6 87.7 inputs. We adopt Depth Anything V2 [125] as baseline and report success rates on four tasks from the LIBERO benchmark [63]. As shown in Tab. 10, our model achieves more accurate spatial understanding of both the environment and the target objects than Depth Anything V2, yielding the best average success rate. Moreover, it achieves significant improvements over models without depth perception or input, indicating that distilling depth perception into VLA policies is promising avenue for enhancing manipulation performance, even without depth inputs during either training or inference. An overview of the framework is provided in Fig. 11 4.2.6 Spatial Understanding of MLLMs We further evaluate how Metric Anything enhances the 3D spatial reasoning ability of foundation VLMs. Metric Anything encodes rich metric 3D information, and we use its pretrained ViT encoder as spatial perception backbone to provide 3D-aware features for vision-language model. Concretely, we extract 3D feature tokens from the ViT encoder of Metric Anything and fuse them with the 2D visual tokens of the VLM via cross-attention module, where the 2D tokens serve as queries and the 3D tokens as keys and values. In this way, the visual stream of the VLM is explicitly conditioned on the metric 3D prior learned by Metric Anything. The fused tokens are then passed through two linear layers to obtain the final fused representation, which is used as the new visual token input to the language model. Following VLM-3R [9], we adopt LLaVA-Next-Video-7B as the base VLM and conduct supervised fine-tuning on 200K general question-answer pairs, and 4,225 embodied route planning instances released by VLM-3R [9]. Table 11: Evaluation Results on VSI-Bench [123]. For Spatial-MLLM and Qwen2.5VL-series [3], we use 16 frames as input. For other open-source methods and GPT-4o [42], we follow the setting of VSI-Bench to set frame numbers (ranging from 8 to 32 frames). For Gemini-1.5 Pro [101], it samples video frames at 1 FPS. Bold denote the best-performing open-source models. Methods Proprietary Models GPT-4o [42] Gemini-1.5 Pro [101] Open-source Models InternVL2-40B [17] LongVILA-8B [121] VILA-1.5-40B [61] LongVA-7B [131] LLaVA-OneVision-72B [53] LLaVA-Video-72B [134] Qwen2.5VL-3B [3] Qwen2.5VL-7B [3] Qwen2.5VL-72B [3] Spatial-MLLM-4B [119] Ours Numerical Question Multiple-Choice Question Obj. Cnt. Abs. Dist. Obj. Size Room Size Rel. Dist. Rel. Dir. Route Plan Appr. Order 46.2 56.2 34.9 29.1 22.4 38.0 43.5 48.9 24.3 40.9 25.1 65.3 70.0 5.3 30.9 26.9 9.1 24.8 16.6 23.9 22.8 24.7 14.8 29.3 34.8 51. 43.8 64.1 46.5 16.7 48.7 38.9 57.6 57.4 31.7 43.4 54.5 63.1 67.5 38.2 43.6 31.8 0.0 22.7 22.2 37.5 35.3 22.6 10.7 38.8 45.1 65.0 18 37.0 51. 42.1 29.6 40.5 33.1 42.5 42.4 38.3 38.6 38.2 41.3 66.2 41.3 46.3 32.2 30.7 25.7 43.3 39.9 36.7 41.6 38.5 37.0 46.2 76.8 31.5 36.0 34.0 32.5 31.5 25.4 32.5 35.0 26.3 33.0 34.0 33.5 40.2 28.5 34. 39.6 25.5 32.9 15.7 44.6 48.6 21.2 29.8 28.9 46.3 29.0 Avg. Rank 34.0 45.4 36.0 21.6 31.2 29.2 40.2 40.9 30.6 33.0 37.0 48.4 58.3 8 3 7 13 10 12 5 4 11 9 6 2 Figure 12: Enhancing 3D Spatial Reasoning with Frozen ViT from Metric Anything . We evaluate our approach on the VIS Benchmark, covering video question-answering tasks like estimating object size, objects distances, appearance order, route planning, and room size. Compared to mainstream large models, our method demonstrates robust and superior performance in 3D spatial understanding. 19 Figure 13: Enhancing 3D Spatial Reasoning in MLLMs. We enhance VLM capabilities by employing the frozen, pretrained ViT from Metric Anything as the visual encoder, thereby preserving its strong spatial understanding during fine-tuning. We report the evaluation results on VSI-Bench [123] in the Tab.11. For Spatial-MLLM and Qwen2.5VL-series [3], we use 16 frames as input. For other open-source methods and GPT-4o [42], we follow the setting of VSI-Bench to set frame numbers (ranging from 8 to 32 frames). For Gemini-1.5 Pro [101], it samples video frames at 1 FPS. As shown in Fig.12, we demonstrate various video question-answer categories within the spatial reasoning benchmark (VIS Benchmark), including object size, inter-object distances, object appearance order, route planning, and room size. Quantitative results and case studies demonstrate that the features extracted by our pretrained model encode rich and accurate spatial understanding, and can substantially enhance the spatial reasoning capabilities of existing VLMs. Since spatial understanding is core competence for tasks such as embodied intelligence, navigation and path planning, and 3D real-world manipulation, we believe that our proposed pretraining paradigm has the potential to serve as fundamental visual representation backbone that raises the performance ceiling of these tasks. An overview of the framework is provided in Fig. 13."
        },
        {
            "title": "5 Ablation Study",
            "content": "5.1 Scaling up Data. We investigate the impact of training data by ablating different proportions of our training dataset. Metric Anything-Pretrain is trained with 5%, 10%, 20%, 40%, 80%, and 100% of the training set, and its depth superresolution on 8 downsample performance is evaluated on KITTI, achieving AbsRel values of 5.22, 4.25, 3.98, 3.02, 2.63, 2.34, respectively. The results show that while our performance on small-scale datasets lags behind PromptDA, which uses sophisticated rules to simulate low-resolution prompts, our model achieves the best performance as data scales up due to increasing prompt diversity from varied data sources. By minimizing design biases in prompt, our paradigm offers the most robust generalization to downstream tasks without task-specific pre-design. The data proportion ablation for student is shown in Fig. 2a. 5.2 Network Architecture. Excluding the benefits of data scaling, we evaluate the effectiveness of our ViT-encoderDPT-head skip-connection design by comparing it with classic U-Netstyle skip-connection architecture [81]. For both architectures, we 20 Table 12: Network architecture ablation on NuScenes and ETH3D. NuScenes [12] ETH3D [90]"
        },
        {
            "title": "Lable Source",
            "content": "Real-world"
        },
        {
            "title": "Pseudo",
            "content": "Real-world"
        },
        {
            "title": "Pseudo",
            "content": "Unet-sytle Ours Abs 0.213 0.235 RMSE 9.631 9.662 Abs 0.187 0.125 RMSE 7.222 6.267 Abs 0.327 0.334 RMSE 5.353 5.490 Abs 0.269 0.182 RMSE 1.44 1.898 Table 13: Single-frame inference latency under FP32 (ms). Method Para. Native Res. FLOPs@HD DepthPro [10] Ours-PreTrain Student-PointMap Student-DepthMap 952M 993M 326M 877M 1536 1536 1536 1536 840 840 1536 1536 8848.5G 19992.6G 4015.7G 21516.4G Latency (ms) @FP32 VGA 246.0 285.2 29.4 278.8 HD 246. 285.2 369.8 278.5 4K 246.0 285.3 5043.9 278.8 train under two regimes: (1) on noisy, real-world data drawn from multiple sources, and (2) on high-accuracy, domain-aligned pseudo labels produced by our pre-trained model. Results in Tab. 12 show that classic U-Net skip connections perform well on heterogeneous data but fail to fully leverage ViTs semantic representations when trained with consistent pseudo labels. Our Inverse Skip-Connection design better realizes the potential of pseudo-label distillation. Additionally, even with identical architectures, training with our pseudo labels improves performance, demonstrating our pre-trained models strong spatial understanding. 5.3 Runtime. To assess the latency of our pretrained model and prompt-free variants in comparison to baselines, we evaluated all approaches at three image resolutionsVGA (640480), HD (1920 1080) and 4K (4032 3024), and recorded the per-image runtime w/wo prompts. All models were run under FP32 precision in the same environment measured on an H200 GPU. We also report parameter counts and FLOPs for comparison. As summarized in Tab. 13, compared to Depth Pro, which shares the most similar in network architecture to ours, our pretrained model does not exhibit noticeable increase in inference time despite introducing an additional prompt branch, owing to our design that unifies the image encoder and patch encoder into shared ViT. We further provide two prompt-free variants, including Student-DepthMap and Student-PointMap. Due to their substantially different parameter counts, their inference latencies vary accordingly. Additional results, including boundary evaluation, hyperparameters, training details and loss ablations are provided in the Appendix. 5.4 Test Time Resolution Scaling Our metric depth estimation model exhibits remarkable capability termed test-time resolution scaling.\" This allows the model to process input images at resolutions significantly higher than those encountered during training, which results in progressively refined depth predictions. As illustrated in Fig. 14, we present depth maps generated at 1, 3, and 9 the base resolution. The results show that increasing the resolution at test time recovers finer structures and high-frequency details, such as thin object boundaries and texture-rich areas. This demonstrates the models potential for high-resolution depth sensing without the need for fine-tuning. 5.5 Training Objectives To evaluate the impact of different training objectives, we trained our model using losses defined on depth, logdepth, inverse-depth, and our proposed distance-balanced inverse-depth loss (Eq. 5 in the main manuscript). Quantitative results in Tab. 14 show that our proposed loss achieves performance comparable to the standard inversedepth loss at close range, while exhibiting superior performance at longer distances. As the depth range increases, the advantage of our method becomes more pronounced. Considering the depth distribution characteristics shown in Fig. 3 in the main manuscript, and given that our pseudo-labels cover significantly broader depth range, these results validate the design of our proposed loss function for large-range depth estimation. Figure 14: Test-time Resolution Scaling. Qualitative results of depth estimation on an example image at 1, 3, and 9 the base input resolution. Higher resolutions recover finer details. Table 14: Ablation Study for Different Training Objectives under Varying Depth Ranges. DIODE [104] AbsRel metric is reported. DIODE [104] AbsRel 0-10m 0.574 0.556 0.467 0.465 10-20m 10-30m 30-40m 40-50m 0.582 0.562 0.493 0.480 0.599 0.577 0.565 0.489 0.622 0.594 0.581 0. 0.645 0.612 0.592 0.537 >50m 0.689 0.663 0.632 0.589 Training objective Depth [38] Log-depth [8] Inverse-depth [10] Ours 5.6 Prompt Setting We analyze the impact of sparse metric-depth prompt density by varying the number of sampled pixels . Specifically, for each image, we randomly sample 500, 1,000, 2,000, 4,000, 8,000, 16,000, 32,000, or 64,000 valid pixels from the depth map to construct sparse metric prompts. As reported in Tab.15 (left), we present the performance on the Hypersim test set. The results show that as increases, accuracy gains gradually diminish while computational complexity increases. To balance accuracy and efficiency, we randomly sample [2,000, 40,000] valid pixels during training and inference. Notably, although we did not explicitly train for extremely sparse prompts (e.g., = 100), the benefits of data scaling and the diversity of our collected data enable our pretrained model to maintain state-of-the-art performance, as shown in Tab.1 of the main manuscript. Table 15: Two Ablation Studies: (left) number of prompt points on Hypersim; (right) balance coefficient on DIODE(AbsRel ). Points 500 1000 2000 4000 8000 16000 32000 50 100 200 400 AbsRel Time (ms) 0.043 0.041 0.038 0.036 0.034 0.033 0.032 0.031 224 299 284 308 274 243 266 040 0.441 0.452 0.454 0.456 0.465 >40 0.635 0.611 0.565 0.562 0.559 22 Figure 15: Analysis of Our Proposed Distance-Balanced Loss. Left: The training loss for different values. Right: The loss function curves. (Eq. 5 in the main manuscript ). Figure 16: Robustness in Night-Time Driving. We deployed test vehicle to evaluate performance under lowlight conditions. As scene brightness drops, visual signals deteriorate and object details fade. Despite this severe degradation, our model maintains remarkably robust. 5.7 Balance Weights We explore the effect of the balance weight in Eq.14 in the main manuscript. We try different candidates including {50, 100, 200, 400, 600} , and report the results in Tab.15 (right). As shown in Fig.15, if the weight is too large, e.g., 600, it tends to encourage the network to learn depth at distant regions while neglecting details in nearby areas. Conversely, when the weight is too small, the network tends to focus on near-field geometric details at the expense of supervision for distant regions. We therefore set this hyperparameter to = 400, which provides reasonable trade-off. 23 Figure 17: Sensor Configuration for Real-World Generalization Evaluation. Our real-world test vehicle is equipped with three cameras (front, left-front, right-front) and 128-beam solid-state LiDAR. Due to the LiDARs limited vertical field of view(pitch angle limitation), its captured point cloud does not fully cover the cameras combined frustums, leaving large image regions without metric depth cues."
        },
        {
            "title": "Conditions",
            "content": "6.1 Generalization across Sensor Configurations This subsection assesses the models generalization capability to variations in sensor hardware configuration and data characteristics. We deployed test vehicle equipped with sensor suite that differed from the training set in both type and spatial arrangement. The setup consisted of three cameras providing front, right-front, and left-front views, coupled with 128-beam solid-state LiDAR for forward scene perception (see Fig. 17). The collected real-world data exhibits two key challenges: 1) minor calibration inaccuracies and asynchronous sampling rateswith cameras operating at 24 Hz and LiDAR at 10 Hzintroduced spatiotemporal misalignments between sensor modalities; 2) the LiDARs field of view did not fully cover the lateral areas captured by the sidefacing cameras. We deliberately avoided additional post-processing techniques, such as motion compensation, to rigorously evaluate the models inherent robustness under these realistic imperfections. The models performance on two critical tasks is visualized in Fig. 18: depth completion for the lateral blind spots (left-front and right-front views) and super-resolution for the front view. Together, these results demonstrate that our model can faithfully recover the scenes metric depth even when presented with imperfect, real-world data from an unseen sensor configuration. 6.2 Robustness under Environmental Degradation This subsection examines the models robustness under conditions where environmental interference degrades perceptual signals. Two typical scenarios of signal degradation were considered: Night-time driving: Night-time environments introduce multiple challenges including significantly reduced signal-to-noise ratios, loss of texture and color information, over-saturation from artificial light sources, and high-contrast shadows. These factors substantially impact the reliability of vision-based perception systems. Rainy/Foggy weather conditions: LiDAR sensors suffer from reflectivity issues that produce anomalous signals or artifacts. This scenario tests whether our model can rely on visual signals to generate reasonable predictions when LiDAR inputs are corrupted. 24 Figure 18: Generalization to Real-World Sensor Configurations. We deployed test vehicle to evaluate in-thewild depth super-resolution and completion performance of our pre-trained model without any fine-tuning. As shown in Fig. 19 and Fig. 16, our model maintains reliable depth estimation in both scenarios, demonstrating strong robustness against environmental degradation. The supplementary video further shows the stability of long-term temporal predictions in our real-world application. 6.3 Generalization to Unseen Visual Domains This subsection evaluates the models zero-shot generalization on monocular depth estimation across visual domains absent from training. Tests were conducted without prompt guidance on three challenging scenarios: panoramic images from spherical projections, fisheye images with extreme distortions, and diverse in-the-wild Figure 19: Robustness in Adverse Weather. In the real-world deployment, we used test vehicle to evaluate our pre-trained model for depth super-resolution and completion in rainy and foggy weather conditions without finetuning. These adverse conditions significantly affect scene reflectance, causing the LiDAR to produce numerous artifacts or completely occlude critical objects. For example, the degraded data can lead to flat ground surfaces being misinterpreted as uneven or crucial obstacles like pillars being missed. However, our model robustly ignores these erroneous inputs and generates accurate depth predictions based on visual cues, thereby demonstrating the complementary strengths of the two sensing modalities. scenes including cartoons, grayscale images, and artistic renderings. Qualitative results (Fig. 20, Fig. 21, Fig. 22, and Fig. 23) confirm accurate metric depth estimation throughout. This robust performance across domains previously unrepresented in training data substantiates our claim of achieving Metric Anything generalization. 26 Figure 20: Generalization to Unseen Visual Domains. Depth prediction results on fisheye images, an unseen domain characterized by severe radial distortion. The model was applied in zero-shot setting without fine-tuning. Figure 21: Generalization to Unseen Visual Domains. Depth prediction visualization for diverse in-the-wild images. Figure 22: Generalization to Unseen Visual Domains. Visualizing depth predictions on panoramic images, an unseen domain during training. Our model successfully handles such extreme distortion and novel viewpoints. 28 Figure 23: Generalization to Unseen Visual Domains. Additional visualizations of depth predictions on diverse in-the-wild images."
        },
        {
            "title": "7 Training Details",
            "content": "7.1 Training and Test Set Split. Across all experiments, results are reported in zero-shot manner, meaning that the training and test sets are sourced from completely distinct datasets with no shared origin. The only exception is ScanNet [21], where we utilize its training set for model training and evaluate on its test set in the multi-view 3D metric reconstruction experiments, following the exact protocol of Map Anything [45]. This usage of ScanNet for training is fair to other methods, as it is also used for training in Map Anything. 7.2 Training Details of Pre-trained Model. We train our model on 144 H200 GPUs for 100k steps, using 10k-step warm-up, peak learning rate of 1106 for the ViT backbone, and 1105 for the DPT head and the prompt layer. To incorporate the prompt, we introduce the required convolutional kernels and biases with zero initialization. 7.3 Training Details of Distilled Model. We distill the pre-trained model into prompt-free student, supervised by pseudo-labels generated through pretrained model inference. To validate the generality of our pre-trained model, we design two training paradigms: (1) From-scratch training, where we initialize the ViT backbone from DINOv3 ViT-H+/16 [92] and randomly initialize the DPT head. We train our model on 144 H200 GPUs for 200k steps, using 5k-step warm-up with peak learning rate of 2106 for the ViT backbone and 2105 for the DPT head. (2) Fine-tuning state-of-the-art methods: We initialize all network parameters from MoGo-2 [110]. We train our model on 80 H200 GPUs for 10k steps, employing 1k-step warm-up with peak learning rate of 2.5106 for the ViT backbone and 2.5105 for the DPT head. For all experiments mentioned above, including pre-training and distillation, we supervise the network using the original resolution of depth maps from the datasets. Specifically, inputs are uniformly resized to 1536 1536, and the network outputs are produced at the same resolution. However, for loss computation during training supervision, we resize the network outputs to match the original dataset resolution for comparison with ground truth. To enhance training efficiency, we utilize FlashAttention [22] alongside DeepSpeed with ZeRO Stage-2 optimizer [83] and BF16 precision. 7.4 Training Details of Vision-Language-Model. Our VLM is initialized from LLaVa-NeXT-Video-7B [133] and equipped with two visual encoders, SigLIP based image encoder and our pretrained ViT of Metric Anything. The features of Metric Anything are first mapped to 1152 dimensional space using lightweight adapter. We then fuse the SigLIP features and the adapted Metric Anything features with cross attention module, and apply projection layer to match the multimodal embedding dimension of the backbone before feeding the fused representation into the language model. The projection layer is initialized from the pretrained LLaVA-NeXT model, while the adapter and cross attention module are initialized from scratch. For training, we adopt the dataset configuration introduced in VLM-3R [9] and fine-tune our VLM for 2K steps with per-device batch size of 4, using 10 nodes with 8 GPUs per node. During training, the LLM backbone, the vision adapter, the cross-attention fusion modules, and the projection layer are all kept trainable, while all remaining vision encoders are frozen. We use AdamW with learning rate of 1e-5 for the trainable modules, cosine learning rate schedule with 0.03 warmup ratio, and gradient clipping with threshold of 0.5. 7.5 Training Details of Vision-Language-Action Model. For each frame, we first extract depth map using our distilled depth estimation model. The depth maps are then normalized using the 5th and 95th percentiles and subsequently converted into depth tokens using VQ-VAE, which are used as an extra ground-truth signal to supervise the VLA model in predicting depth tokens, while simultaneously predicting the action token.. We apply parameter-efficient fine-tuning via LoRA, attaching lowrank adaptation modules to all Transformer layers in the model. The model is trained on 10 nodes with 8 GPUs per node, using global batch size of 160. We use AdamW with learning rate of 5e-4 and cosine learning rate schedule. Evaluation Metrics 30 Depth Metrics. For the quantitative evaluation of depth estimation, we follow the standard metrics from prior works. Let di and ˆdi denote the ground truth depth and the predicted depth for pixel i, respectively. The evaluation is performed over all valid pixels. Our reported metrics are defined as follows: Threshold Accuracy (δi): The percentage of pixels where the ratio of predicted and ground truth depth falls within certain threshold: % of pixels s.t. max (cid:33) (cid:32) ˆdi di , di ˆdi < δ, where δ {1.25, 1.252, 1.253}. Absolute Relative Error (AbsRel): Root Mean Squared Error (RMSE): Mean Absolute Error (MAE): Log10 Error:"
        },
        {
            "title": "1\nN",
            "content": "(cid:88) ˆdi di di (cid:115) 1 (cid:88) ( ˆdi di) 1 (cid:88) ˆdi di 1 (cid:88) log10( ˆdi) log10(di) These metrics are used throughout our experiments. Specifically: δ1: Reported in Sec 4 (Tab. 3, Tab. 9) for zero-shot monocular depth estimation evaluation. AbsRel: Reported in Sec 4 (Tab. 1, Tab. 9) for metric depth accuracy evaluation. RMSE, MAE: Reported in Sec 4 (Tab. 2) for radar-camera depth estimation evaluation. Log10, δ2, δ3: Used in comprehensive evaluation tables in the appendix for additional depth accuracy assessment. Boundary Metrics. Following DepthPro [10], we evaluate boundary sharpness using depth-based and maskbased metrics. For depth maps, we define occluding contours based on pairwise depth ratios between neighboring pixels. Let i, be the locations of two neighboring pixels. We define an occluding contour cd derived from depth map as: cd(i, j) = (cid:20) d(j) d(i) (cid:18) > 1 + (cid:19)(cid:21) , 100 where [] is the Iverson bracket, indicating the presence of an occluding contour if the depth differs by more than t%. For all pairs of neighboring pixels, we compute precision (P ) and recall (R) as: P(t) = (cid:80) i,jN (i) cd(i, j) ˆd(i, j) i,jN (i) cd(i, j) (cid:80) , (cid:80) R(t) = i,jN (i) cd(i, j) ˆd(i, j) i,jN (i) ˆd(i, j) (cid:80) , where (i) denotes the set of neighboring pixels of i, cd and ˆd are occluding contours from predicted and groundtruth depth maps, respectively. The Boundary F1 score is computed as: F1(t) = 2 P(t) R(t) P(t) + R(t) . 31 (7) Camera Intrinsics Metrics. For focal length estimation evaluation, we report the mean and median angular errors in degrees. Given the ground-truth focal length fgt (in pixels) and image width w, the horizontal field of view (FOV) is computed as: The angular error in degrees is: Error = FOVpred FOVgt FOV = 2 arctan (cid:19) (cid:18) 2fgt . 180 π , where FOVpred and FOVgt are predicted and ground-truth FOV values, respectively. FOV estimation results are reported in Sec 4 (Tab. 7) for camera calibration evaluation. 7.6 Loss Functions Pre-train Model Losses. The pre-train model is trained to predict dense metric depth from monocular images conditioned on sparse metric prompts. We operate in the inverse depth space C. For real-world metric datasets, we adopt Robust MAE loss that discards the top 20% of pixels with the largest errors per image to mitigate the influence of noisy ground truth: LMAE ( ˆC, C) = 1 ˆCi Ci, (cid:88) iS (8) where is the set of pixels after removing the top 20% largest errors, and = S. For synthetic datasets without metric scale, we additionally apply the Scale-and-Shift-Invariant Mean Absolute Gradient Error (SSI-MAGE) loss [10]. First, we normalize predictions and ground truth via mean absolute deviation from the median to achieve scale and shift invariance: = ˆC = median(C) MAD(C) ˆC median( ˆC) MAD( ˆC) , , (9) (10) where MAD(C) = median(C median(C)) is the median absolute deviation. Then, we compute the multi-scale gradient loss. Let denote the Scharr gradient operator [87]. The multi-scale derivative loss over scales is defined as: L,p,M (C, ˆC) = 1 (cid:88) 1 Nj Nj (cid:88) C ˆC p, (11) where and ˆC are the inverse depth maps at scale (obtained by blurring and downsampling by factor of 2 per scale), and Nj represents the number of valid pixels at scale j. The SSI-MAGE loss is defined as: LSSI -MAGE ( ˆC, C) = LS,1,6( ˆC, C), (12) where LS,1,6 denotes the multi-scale gradient loss with Scharr operator (S), L1 norm (p = 1), and 6 scales (M = 6). The overall training objective for the teacher model is: Ltotal = αLMAE + βLSSI -MAGE , where α and β are weighting coefficients, we set α = 15 and β = 5. For synthetic datasets, both terms are applied; for real-world datasets, only the robust MAE loss is used. This loss formulation is described in Sec 3.2 of the main manuscript. (13) Distill Model Losses. The distill model is trained on pseudo labels generated by the teacher model. To handle the wide depth range (from near to far distances) in the pseudo labels (as shown in Fig. 3), we design DistanceBalanced Inverse-Depth Loss. The loss operates in log-space representation that preserves fine-grained sensitivity in near regions while extending effective supervision to long-distance areas. The transformed depth value is defined as (Eq. 5 in the main manuscript): Dlog = 1 ln(D)/ln(C), where is the metric depth and = 400 is hyperparameter controlling the trade-off between long-range and short-range supervision. The distance-balanced loss is then computed as: LStudent ( ˆD, D) = 1 (cid:88) Dlog( ˆDi) Dlog(Di), (14) 32 where ˆD and are predicted and ground-truth metric depth maps, respectively. This loss function addresses the limitation of standard inverse-depth loss, which decays too rapidly with distance, by providing more balanced supervision across the full depth range covered by our teacher-generated pseudo labels. Additionally, we apply the SSI-MAGE loss (Eq. 12) to the log-space transformed depth maps, combining it with our distance-balanced supervision to further enhance boundary sharpness and geometric detail preservation. The overall student training objective is: LStudentTotal = γLStudent + δLSSI -MAGE (Dlog( ˆD), Dlog(D)), (15) where γ and δ are weighting coefficients, we set γ = 10 and δ = 2. The design rationale and experimental validation are presented in Sec 3.3 of the main paper and Tab. 14."
        },
        {
            "title": "8 Limitations",
            "content": "Our work maintains the central projection camera assumption and has not been extended to specialized camera models (e.g., non-central or non-pinhole configurations). In terms of model scalability, while our data-centric scaling strategy demonstrates strong empirical gains, the scalability of the model architecture itself remains unexplored. Expanding the models architecture could potentially enhance its capability for depth perception in more complex and diverse scenarios."
        },
        {
            "title": "9 Conclusion",
            "content": "We present Metric Anything, scalable pretraining framework for metric depth estimation that learns from diverse, noisy 3D sources without task-specific architectures or manually engineered prompts. Using Sparse Metric Prompts to separate spatial reasoning from sensor and camera biases, our approach effectively leverages heterogeneous data. Experiments reveal, for the first time, clear scaling effect in the metric depth trick. Both the pretrained model and its distilled prompt-free student achieve state-of-the-art results across wide range of downstream tasks. These results indicate more efficient general-purpose solutions for real-world depth perception."
        },
        {
            "title": "References",
            "content": "[1] Manuel López Antequera, Pau Gargallo, Markus Hofinger, Samuel Rota Bulo, Yubin Kuang, and Peter Kontschieder. Mapillary planet-scale depth dataset. In European Conference on Computer Vision, pages 589604. Springer, 2020. [2] Baidu Apollo. Apollo synthetic dataset, 2019. Accessed: 2025-6-12. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. ArXiv, abs/2502.13923, 2025. [4] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. [5] Zuria Bauer, Francisco Gomez-Donoso, Edmanuel Cruz, Sergio Orts-Escolano, and Miguel Cazorla. Uasol, large-scale high-resolution outdoor stereo dataset. Scientific data, 6(1):162, 2019. [6] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 40094018, 2021. [7] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Localbins: Improving depth estimation by learning local distributions. In European Conference on Computer Vision, pages 480496. Springer, 2022. [8] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth, 2023. [9] Vineet Bhat, Yu-Hsiang Lan, Prashanth Krishnamurthy, Ramesh Karri, and Farshad Khorrami. 3d cavla: Leveraging depth and 3d context to generalize vision language action models for unseen tasks. arXiv preprint arXiv:2505.05800, 2025. [10] Aleksei Bochkovskii, Amaël Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, In International and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. Conference on Learning Representations, 2025. [11] Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael Black. naturalistic open source movie for optical flow evaluation. In European conference on computer vision, pages 611625. Springer, 2012. [12] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1162111631, 2020. [13] Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, arXiv preprint Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv:2506.21539, 2025. [14] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. International Conference on 3D Vision (3DV), 2017. [15] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-image depth perception in the wild. Advances in neural information processing systems, 29, 2016. [16] Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, and Jia Deng. Oasis: largescale dataset for single image 3d in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [18] Xinjing Cheng, Peng Wang, Chenye Guan, and Ruigang Yang. Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion. In AAAI, 2020. [19] Xinjing Cheng, Peng Wang, and Ruigang Yang. Learning depth with convolutional spatial propagation network. TPAMI, 2019. [20] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. [21] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. [22] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. [24] David Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with common multiscale convolutional architecture. In Proceedings of the IEEE international conference on computer vision, pages 26502658, 2015. [25] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multiscale deep network. Advances in neural information processing systems, 27, 2014. [26] Michael Fonder and Marc Van Droogenbroeck. Mid-air: multi-modal dataset for extremely low altitude In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition drone flights. workshops, pages 00, 2019. [27] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 20022011, 2018. [28] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking analysis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 43404349, 2016. [29] Stefano Gasperini, Patrick Koch, Vinzenz Dallabetta, Nassir Navab, Benjamin Busam, and Federico Tombari. R4dyn: Exploring radar for self-supervised monocular depth estimation of dynamic scenes. In 2021 International Conference on 3D Vision (3DV), pages 751760. IEEE, 2021. 34 [30] Mathias Gehrig, Willem Aarents, Daniel Gehrig, and Davide Scaramuzza. Dsec: stereo event camera dataset for driving scenarios. IEEE Robotics and Automation Letters, 6(3):49474954, 2021. [31] Geiger, Lenz, and Urtasun. Are we ready for autonomous driving. The KITTI vision benchmark suite. InCVPR, 2:5, 2012. [32] Clément Godard, Oisin Mac Aodha, Michael Firman, and Gabriel Brostow. Digging into self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38283838, 2019. [33] Jose Gómez, Manuel Silva, Antonio Seoane, Agnès Borrás, Mario Noriega, Germán Ros, Jose IglesiasGuitian, and Antonio López. All for one, and one for all: Urbansyn dataset, the third musketeer of synthetic driving scenes. Neurocomputing, 637:130038, 2025. [34] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for selfsupervised monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24852494, 2020. [35] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares, Ambrus, , and Adrien Gaidon. Towards zero-shot scaleaware monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 92339243, 2023. [36] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares, Ambrus, , and Adrien Gaidon. Towards zero-shot scaleaware monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 92339243, 2023. [37] John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir Iglovikov, and Peter Ondruska. One thousand and one hours: Self-driving motion prediction dataset. arXiv preprint arXiv:2006.14480, 2020. [38] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [39] Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. Thinkact: Vision-language-action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025. [40] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28212830, 2018. [41] Chia-Yu Hung, Qi Sun, Pengfei Hong, Amir Zadeh, Chuan Li, Tan, Navonil Majumder, Soujanya Poria, et al. Nora: small open-sourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854, 2025. [42] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [43] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Blackburn-Matzen, Matthew In Proceedings of Sticha, and David Fouhey. Perspective fields for single image camera calibration. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1730717316, 2023. [44] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. In Proceedings of the Repurposing diffusion-based image generators for monocular depth estimation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [45] Nikhil Keetha, Norman Müller, Johannes Schönberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, Jonathon Luiten, Manuel Lopez-Antequera, Samuel Rota Bulò, Christian Richardt, Deva Ramanan, Sebastian Scherer, and Peter Kontschieder. MapAnything: Universal feed-forward metric 3D reconstruction, 2025. arXiv preprint arXiv:2509.13414. [46] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [47] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, In Proceedings of the Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. IEEE/CVF international conference on computer vision, pages 40154026, 2023. 35 [48] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based singleimage depth estimation methods. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 00, 2018. [49] Lubor Ladicky, Jianbo Shi, and Marc Pollefeys. Pulling things out of perspective. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8996, 2014. [50] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In 2016 Fourth international conference on 3D vision (3DV), pages 239248. IEEE, 2016. [51] Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, et al. Molmoact: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917, 2025. [52] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [53] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [54] Han Li, Yukai Ma, Yaqing Gu, Kewei Hu, Yong Liu, and Xingxing Zuo. Radarcam-depth: Radar-camera fusion for depth estimation with learned metric scale. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1066510672. IEEE, 2024. [55] Huadong Li, Minhao Jing, Wang Jin, Shichao Dong, Jiajun Liang, Haoqiang Fan, and Renhe Ji. Sparse beats dense: Rethinking supervision in radar-camera depth completion. In European Conference on Computer Vision, pages 127143. Springer, 2024. [56] Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, and Bo Dai. Matrixcity: In Proceedings of the IEEE/CVF large-scale city dataset for city-scale neural rendering and beyond. International Conference on Computer Vision, pages 32053215, 2023. [57] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 20412050, 2018. [58] Zhenyu Li, Shariq Farooq Bhat, and Peter Wonka. Patchfusion: An end-to-end tile-based framework for high-resolution monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1001610025, 2024. [59] Haotong Lin, Sili Chen, Jun Hao Liew, Donny Y. Chen, Zhenyu Li, Guang Shi, Jiashi Feng, and Bingyi Kang. Depth anything 3: Recovering the visual space from any views. arXiv preprint arXiv:2511.10647, 2025. [60] Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, and Bingyi Kang. Prompting depth anything for 4k resolution accurate metric depth estimation. In CVPR, 2025. [61] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2667926689, 2023. [62] Juan-Ting Lin, Dengxin Dai, and Luc Van Gool. Depth estimation from monocular images and sparse radar data. in 2020 ieee. In RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1023310240, 2020. [63] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. [64] Zhiheng Liu, Ka Leong Cheng, Qiuyu Wang, Shuzhe Wang, Hao Ouyang, Bin Tan, Kai Zhu, Yujun Shen, Qifeng Chen, and Ping Luo. Depthlab: From partial to complete. arXiv preprint arXiv:2412.18153, 2024. [65] Chen-Chou Lo and Patrick Vandewalle. Depth estimation from monocular images and sparse radar using deep ordinal regression network. In 2021 IEEE International Conference on Image Processing (ICIP), pages 33433347. IEEE, 2021. [66] Chen-Chou Lo and Patrick Vandewalle. Rcdpt: Radar-camera fusion dense prediction transformer. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 36 [67] Yunfei Long, Daniel Morris, Xiaoming Liu, Marcos Castro, Punarjay Chakravarty, and Praveen Narayanan. Radar-camera pixel depth association for depth completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1250712516, 2021. [68] Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 20162029, 2025. [69] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andrés Bruhn. Spring: highresolution high-detail dataset and benchmark for scene flow, optical flow and stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49814991, 2023. [70] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [71] Jin-Hwi Park, Chanhwi Jeong, Junoh Lee, and Hae-Gon Jeon. Depth prompting for sensor-agnostic depth estimation. In CVPR, 2024. [72] William Peebles and Saining Xie. Scalable diffusion models with transformers."
        },
        {
            "title": "In Proceedings of the",
            "content": "IEEE/CVF international conference on computer vision, pages 41954205, 2023. [73] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. [74] Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool. Unidepthv2: Universal monocular metric depth estimation made simpler. arXiv preprint arXiv:2502.20110, 2025. [75] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1010610116, 2024. [76] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. [77] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [78] Santhosh Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXiv preprint arXiv:2109.08238, 2021. [79] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [80] Pierluigi Zama Ramirez, Alex Costanzino, Fabio Tosi, Matteo Poggi, Samuele Salti, Stefano Mattoccia, and Luigi Di Stefano. Booster: benchmark for depth from images of specular and transparent surfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(1):85102, 2023. [81] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. [82] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. [83] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. [84] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10912 10922, 2021. 37 [85] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [86] German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio Lopez. The synthia dataset: large collection of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32343243, 2016. [87] Hanno Scharr, Stefan Körkel, and Bernd Jähne. Numerische Isotropieoptimierung von FIR-Filtern mittels Querglättung. In DAGM-Symposium, 1997. [88] Daniel Scharstein, Heiko Hirschmüller, York Kitajima, Greg Krathwohl, Nera Nešic, Xi Wang, and Porter Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In German conference on pattern recognition, pages 3142. Springer, 2014. [89] Thomas Schöps, Johannes L. Schönberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with high-resolution images and multi-camera videos. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [90] Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with high-resolution images and multi-camera videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3260 3269, 2017. [91] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In European conference on computer vision, pages 746760. Springer, 2012. [92] Oriane Siméoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil arXiv preprint Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. arXiv:2508.10104, 2025. [93] Akash Deep Singh, Yunhao Ba, Ankur Sarker, Howard Zhang, Achuta Kadambi, Stefano Soatto, Mani In Srivastava, and Alex Wong. Depth estimation from camera image and mmwave radar point cloud. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 92759285, 2023. [94] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. Sun rgb-d: rgb-d scene understanding benchIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages mark suite. 567576, 2015. [95] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. [96] Huawei Sun, Hao Feng, Julius Ott, Lorenzo Servadei, and Robert Wille. Cafnet: confidence-driven framework for radar camera depth estimation. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 27342740. IEEE, 2024. [97] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, open dataset. pages 24462454, 2020. [98] Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. [99] Bin Tan, Changjiang Sun, Xiage Qin, Hanat Adai, Zelin Fu, Tianxiang Zhou, Han Zhang, Yinghao Xu, Xing Zhu, Yujun Shen, and Nan Xue. Masked depth modeling for spatial perception. arXiv preprint arXiv:2601.17895, 2026. [100] Jie Tang, Fei-Peng Tian, Boshi An, Jian Li, and Ping Tan. Bilateral propagation network for depth completion. In CVPR, 2024. [101] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [102] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. 38 [103] Fabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger. Smd-nets: Stereo mixture density networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 89428952, 2021. [104] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. [105] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [106] Massimiliano Viola, Kevin Qu, Nando Metzger, Bingxin Ke, Alexander Becker, Konrad Schindler, and Anton Obukhov. Marigold-dc: Zero-shot monocular depth completion with guided diffusion, 2024. [107] Kaixuan Wang and Shaojie Shen. Flow-motion and depth network for monocular stereo and beyond. IEEE Robotics and Automation Letters, 5(2):33073314, 2020. [108] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, and Xiaowen Chu. Irs: large naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation. arXiv preprint arXiv:1912.09678, 2019. [109] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training superIn Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52615271, vision. 2025. [110] Ruicheng Wang, Sicheng Xu, Yue Dong, Yu Deng, Jianfeng Xiang, Zelong Lv, Guangzhong Sun, Xin Tong, and Jiaolong Yang. Moge-2: Accurate monocular geometry with metric scale and sharp details, 2025. [111] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: GeometIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern ric 3d vision made easy. Recognition, pages 2069720709, 2024. [112] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 49094916. IEEE, 2020. [113] Yiran Wang, Jiaqi Li, Chaoyi Hong, Ruibo Li, Liusheng Sun, Xiao Song, Zhe Wang, Zhiguo Cao, and In Guosheng Lin. Tacodepth: Towards efficient radar-camera depth estimation with one-stage fusion. Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1052310533, 2025. [114] Zehan Wang, Siyu Chen, Lihe Yang, Jialei Wang, Ziang Zhang, Hengshuang Zhao, and Zhou Zhao. Depth anything with any prior, 2025. [115] Endre Weiszfeld. Sur le point pour lequel la somme des distances de points donnés est minimum. Tohoku Mathematical Journal, First Series, 43:355386, 1937. [116] Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, and Stan Birchfield. Foundationstereo: Zero-shot stereo matching. CVPR, 2025. [117] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, et al. Argoverse 2: Next generation datasets for self-driving perception and forecasting. arXiv preprint arXiv:2301.00493, 2023. [118] Magnus Wrenninge and Jonas Unger. Synscapes: photorealistic synthetic dataset for street scene parsing. arXiv preprint arXiv:1810.08705, 2018. [119] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. ArXiv, abs/2505.23747, 2025. [120] Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang, Xiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun, Kun Jiang, et al. Pandaset: Advanced sensor suite dataset for autonomous driving. In 2021 IEEE international intelligent transportation systems conference (ITSC), pages 30953101. IEEE, 2021. [121] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos. ArXiv, abs/2408.10188, 2024. 39 [122] Guorun Yang, Xiao Song, Chaoqin Huang, Zhidong Deng, Jianping Shi, and Bolei Zhou. Drivingstereo: large-scale dataset for stereo matching in autonomous driving scenarios. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 899908, 2019. [123] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Fei-Fei Li, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. ArXiv, abs/2412.14171, 2024. [124] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. [125] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. [126] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 56845693, 2019. [127] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 90439053, 2023. [128] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 204213, 2021. [129] Amir Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. In Proceedings of the IEEE conference on computer Taskonomy: Disentangling task transfer learning. vision and pattern recognition, pages 37123722, 2018. [130] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. [131] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. ArXiv, abs/2406.16852, 2024. [132] Youmin Zhang, Xianda Guo, Matteo Poggi, Zheng Zhu, Guan Huang, and Stefano Mattoccia. Completionformer: Depth completion with convolutions and vision transformers. In CVPR, 2023. [133] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024. [134] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. ArXiv, abs/2410.02713, 2024. [135] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17021713, 2025. [136] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou. Structured3d: large photorealistic dataset for structured 3d modeling. In European Conference on Computer Vision, pages 519535. Springer, 2020. [137] Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. [138] Shengjie Zhu, Abhinav Kumar, Masa Hu, and Xiaoming Liu. Tame wild camera: In-the-wild monocular camera calibration. Advances in Neural Information Processing Systems, 36:4513745149, 2023. [139] Yiming Zuo and Jia Deng. Ogni-dc: Robust depth completion with optimization-guided neural iterations. In ECCV, 2024. [140] Yiming Zuo, Willow Yang, Zeyu Ma, and Jia Deng. Omni-dc: Highly robust depth completion with multiresolution depth integration. ICCV, 2025."
        }
    ],
    "affiliations": [
        "Li Auto Inc"
    ]
}