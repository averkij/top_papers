{
    "paper_title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning",
    "authors": [
        "Ming Chen",
        "Sheng Tang",
        "Rong-Xi Tan",
        "Ziniu Li",
        "Jiacheng Chen",
        "Ke Xue",
        "Chao Qian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 3 3 5 6 0 . 2 1 5 2 : r Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Ming Chen * 1 2 Sheng Tang * 1 2 Rong-Xi Tan * 1 2 Ziniu Li 3 Jiacheng Chen 4 Ke Xue 1 2 Chao Qian"
        },
        {
            "title": "Abstract",
            "content": "Decoding-based regression, which reformulates regression as sequence generation task, has emerged as promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as robust and accurate paradigm for general-purpose numerical prediction. 1. Introduction Regression, the task of predicting continuous targets from input representations, stands as fundamental role of machine learning (Bishop, 2006; Van Breugel & Van Der Schaar, *Equal contribution 1National Key Laboratory for Novel Software Technology, Nanjing University 2School of Artificial Intelligence, Nanjing University 3School of Data Science, The Chinese University of Hong Kong, Shenzhen 4Department of Computer Science and Engineering, The Chinese University of Hong Kong. Correspondence to: Ke Xue <xuek@lamda.nju.edu.cn>, Chao Qian <qianc@lamda.nju.edu.cn>. Preprint. Working in progress. 2024), with wide applications across critical domains ranging from scientific discovery (Hu et al., 2024) to industrial scenarios (He et al., 2025). Traditional regression methods, including Gaussian Processes (Rasmussen & Williams, 2006) and tree-based models (Chen & Guestrin, 2016; Prokhorenkova et al., 2018), excel due to their robustness and interpretability (Sahakyan et al., 2021). However, with the advent of the deep learning era and the increasing complexity of data, there has been paradigm shift towards deep-learning (DL) based regressors (Jiang et al., 2025; Ye et al., 2024). These methods leverage the power of representation learning to map high-dimensional inputs into latent spaces, subsequently modeling the target value through specialized regression heads. For DL-based regressors, there have been some design philosophies of regression heads to map latent representations to continuous targets. The most common approach, the pointwise head, projects representations directly to scalar but often fails to capture the uncertainty or the complex multimodality of the target distribution (Lakshminarayanan et al., 2017). To address this, parametric distribution heads model outputs as predefined distributions (e.g., Gaussian), yet they rely on rigid assumptions that may not hold in real-world scenarios (Imani & White, 2018). Alternatively, the Riemann head (or histogram head) discretizes the continuous output into finite bins, converting regression into classification (Bellemare et al., 2017; Imani & White, 2018), showing great robustness (Imani et al., 2024) and performance (Muller et al., 2022). However, these methods primarily operate on structured data, limiting their ability to perform regression on the vast and diverse spectrum of unstructured data (e.g., text or code). This limitation has motivated recent studies to leverage Large Language Models (LLMs) for universal regression (Vacareanu et al., 2024; Song et al., 2024; Tchuindjo & Khattab, 2025). key development in this line of work is decoding-based regression (Song & Bahri, 2025), which reformulates regression as discrete sequence generation task and can be trained over large amounts of regression data (x, y) represented as text. As illustrated in Figure 1, this approach reformulates regression as next-token prediction task by tokenizing continuous values (e.g., via base-B Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning sequence-level numerical magnitude. In this paper, we propose Generative Reinforced Regressor (GenRe2) to bridge this gap. We reformulate decodingbased regression as Markov Decision Process (MDP), allowing us to optimize the model using policy gradient methods (Sutton et al., 1999). Unlike previous approaches, GenRe2 utilizes sequence-level reward signal, which is computed only after the full numerical sequence is generated and detokenized, to directly guide the model towards minimizing the true regression error (e.g., MSE). We explore efficient REINFORCE-style (Williams, 1992) algorithms, including ReMax (Li et al., 2024) and GRPO (Shao et al., 2024) to finetune the CE-trained models, and validate GenRe2 across two distinct domains: tabular regression on the TALENT benchmark (Liu et al., 2025a; Ye et al., 2024) and code metric regression (Akhauri et al., 2025b) using RLM (Song et al., 2024; Akhauri et al., 2025a;b). Experimental results demonstrate that GenRe2 consistently outperforms the pointwise and Riemann baselines, and stateof-the-art token-level improvements for decoding-based regressor, clearly showing the benefits of GenRe2 based on sequence-level reward. Our findings reveal that (1) Equipped with GenRe2, the decoding-based paradigm generally outperforms traditional designs (e.g., pointwise and Riemann heads); (2) Sequencelevel supervision is significant for decoding-based regression to bridge the gap between regression and the token-level objectives; (3) While RL may sharpen the output distribution, it significantly enhances the sampling efficiency and precision, making generative decoding-based models as highly competitive paradigm for numerical prediction. 2. Background Traditional DL-based regressors typically employ pointwise head (predicting scalar) or Riemann head (predicting binned histogram distribution) (Imani & White, 2018), where the Riemann head has better robustness and performance in many applications and is widely used (Farebrother et al., 2024; Hollmann et al., 2025). detailed overview of these methods is provided in Appendix A.1. Recently, Song & Bahri (2025) proposed decoding-based regression by reformulating regression as discrete sequence generation task, calling for paradigm shift to generative regression. Specifically, target scalar value is transformed into sequence of discrete tokens = {t1, t2, , tK}. Then, an autoregressive decoder head is trained to predict the tokens sequentially. Given the input representation ϕ(x), it models the conditional probability distribution pθ(yx) as (cid:89)K pθ(yx) = pθ(tk ϕ(x), T<k), k= where T<k denotes the tokens generated before step k. Given dataset = {(xi, yi)}N i=1, we first tokenize each 2 Figure 1. Illustration of decoding-based regression. The input passes through an encoder to produce the representation ϕ(x), which is then processed by decoder. The model performs multiple sampling trials to generate several discrete token sequences (e.g., the binary representation <1><1><0>). These sequences are individually detokenized into corresponding scalar values (shown in the stacked layers as ˆy1 = 6, ˆy2 = 5, ˆy3 = 7). Finally, these scalar values are combined via an aggregation strategy (e.g., median) to produce the final prediction ˆy = 6. expansion). Unlike traditional scalar regressors, decodingbased regression not only can handle unstructured raw data, but also leverages the strong sequential modeling capabilities of Transformers to capture complex distributions (Song et al., 2024). Furthermore, the generative approach of decoding-based regression mitigates the susceptibility to reward hacking often seen in scalar or histogram baselines (Chen et al., 2024; Yu et al., 2025), producing more robust and calibrated predictions, which align with the recent observations from generative reward models (Mahan et al., 2024; Zhang et al., 2025c). The concept of decodingbased regression gives rise to Regression Language Model (RLM) (Song et al., 2024), which demonstrates great potential in diverse applications ranging from industrial prediction (Akhauri et al., 2025a;b) to black-box optimization (Nguyen et al., 2024; Tan et al., 2025). However, despite its promise, the potential of decodingbased regression remains unlocked. The critical barrier lies in the misalignment between the widely used CrossEntropy (CE) loss and the numerical nature of the regression task (Lukasik et al., 2025). CE treats tokens as independent categories, ignoring their ordinal value and the entire magnitude of the detokenized number. While recent works have attempted to mitigate this via token-level distance penalties, e.g., NTL (Zausinger et al., 2025) and DIST2 (Chung et al., 2025), fundamental limitation remains: these methods operate locally on individual tokens and overlook the cumulative error over the entire sequence (Selvam, 2025), which can lead to catastrophic outcomes in the original numerical space (Song et al., 2024; Song & Bahri, 2025). Thus, there is an urgent need for method that is inherently aware of Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning target into corresponding token sequence , then the decoder head is trained to predict the next token by minimizing the standard Cross-Entropy (CE) loss: L(θ) = E(x,T )D (cid:20)(cid:88)K k=1 log pθ (tk ϕ(x), T<k) . (cid:21) the form of natural language, eliminating the need of feature engineering, which has been successfully applied to industrial scenarios (Akhauri et al., 2025a), code metric prediction (Akhauri et al., 2025b), and black-box optimization (Nguyen et al., 2024; Tan et al., 2025). For inference, we generate candidate solutions via sampling (e.g., temperature sampling) and return the aggregation of these solutions. Here, the aggregation strategy can be various, such as mean() or median(), and different aggregations lead to Bayes-optimal solutions for different regression metrics (Lukasik et al., 2024). The tokenization of decoding-based regression is important. Following (Song & Bahri, 2025), we briefly introduce two common tokenization strategies (Detailed description can be founded in Appendix B): Normalized Tokenization: The normalization tokenization first scales target value to fixed interval (e.g. [0, 1]), then represents the scaled value as base-B expansion (e.g., 0.6 as <1><1><0> with = 2). While effective, it relies on the access to the global minimum and maximum and is highly sensitive to outliers (Yeo & Johnson, 2000; Song et al., 2024). Scientific Notation Tokenization: Scientific notation tokenization methods (e.g., P10 (Charton, 2022) or IEEE (IEEE, 2019)) do not normalize the target, representing numbers using sign, mantissa, and exponent components (e.g., P10 represents 1.23 as <+><1><2><3><E-2>). This tokenization supports wider range of values but can be prone to yield hallucinations in unbounded generation (Song et al., 2024). regression decoding-based Intuitively, generalizes histogram-based regression (e.g., Riemann head) into multi-step binning paradigm, where tokenization defines the structure and the autoregressive decoding sequentially refines predictions (Song & Bahri, 2025). Notably, this approach offers two clear advantages: (1) It integrates seamlessly with LLMs, thereby enabling universal regression (Song et al., 2024) on free-formed inputs (Akhauri et al., 2025a) while leveraging rich priors (Akhauri et al., 2025b; Dong et al., 2025); (2) It improves calibration. As noted in the reward model community (Liu et al., 2025c; Li et al., 2025a), sequential generative scoring yields more robust predictions (Chiang et al., 2025) and better mitigates reward hacking compared to scalar or histogram baselines (Chen et al., 2024; Yu et al., 2025). 3. Method In this section, we present our proposed method, GenRe2, which leverages RL to address the sequence-level challenge of decoding-based regression with policy gradient. In Section 3.1, we first discuss the limitations of previous token-level decoding-based regression methods, showing emergent need for sequence-level supervisions and motivating us to solve it via RL. In Section 3.2, we formulate the decoding-based regression task as Markov Decision Process (MDP) (Puterman, 2014), which serves as the foundation of GenRe2. In Section 3.3, we discuss several reward design strategy to guide the model towards better regression performance. Finally, we present and visualize some training dynamics of GenRe2 in Section 3.4. 3.1. Limitations of Previous Token-level Methods Standard decoding-based regression typically relies on CE. However, the potential of CE-trained decoding-based regression remains locked. Lukasik et al. (2025); Selvam (2025) theoretically showed that CE is not well-aligned with regression, as it treats digits as individual categories and ignores the numerical continuity. While recent improvements like NTL (Zausinger et al., 2025) and DIST2 (Chung et al., 2025) introduce distance penalties, they still operate locally on individual tokens. As illustrated in the left part of Figure 2, the token-level losses overlook the global magnitude of the detokenized number. However, the true regression error is determined by the holistic value of the generated sequence, showing misalignment with the token-level supervisions. To bridge this gap, we reformulate the task via RL, optimizing the decoder using policy gradients with full-sequence rewards (Figure 2, right). This approach is motivated by recent successes in RL for LLM (Li et al., 2025b), where policy gradient methods (Sutton, 1988; Sutton et al., 1999) effectively align LLMs responses with sequence-level, nondifferentiable objectives, such as human preference (Christiano et al., 2017; Ouyang et al., 2022) or verifiable correctness (Guo et al., 2025; Wang et al., 2025). We provide detailed overview of RL for LLMs in Appendix A.2. Next, we will introduce the RL formulation of decoding-based regression, establishing the foundation of GenRe2. Decoding-based regression has been applied to many downstream scenarios (Nguyen et al., 2025; Kim et al., 2025), one representative of which is Regression Language Model (RLM) (Song et al., 2024). RLM directly regresses in 3.2. Problem Formulation In this section, we formalize the generation of numerical sequence (i.e., the primary goal of decoding-based Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning with zero rewards to all intermediate generation steps. The specific design of r(ϕ(x), a0:K1) can be flexible, which we will elaborate in Section 3.3. Initial State Distribution ρ: The distribution ρ is deterministic, with the initial state s0 corresponding to the input feature ϕ(x) and an empty sequence. Horizon : is the maximum length of the generated sequence, i.e., = K. Within this framework, the learning objective of GenRe2 is to maximize the expected return: (πθ) = E(x,y)Dtrain Eτ πθ (cid:20)(cid:88)K1 k=0 (cid:21) r(sk, ak) , (1) where πθ is the policy parameterized by θ, and τ = (s0, a0, , sK) denotes trajectory sampled from πθ. By formulating the decoding regression task into policy optimization problem, we employ the policy gradient method (Sutton et al., 1999) to optimize πθ by ascending the gradient of the expected return: θJ (πθ) = E(x,y)Dtrain Eτ πθ (cid:20)(cid:88)K k=0 (cid:21) θ log πθ(ak sk)Aπθ (sk, ak) , (2) where Aπθ (st, at) is the advantage function estimating the relative value of action ak in state sk. Given the deterministic state transitions in the MDP, simple policy gradient methods like REINFORCE (Williams, 1992) are efficient. In this work, we employ two prevalent REINFORCE-style algorithms, ReMax (Li et al., 2024) and GRPO (Shao et al., 2024), details of which are provided in Appendix C. 3.3. Reward Design for GenRe The reward function is designed to guide the model towards the final regression metrics. Upon completing an episode, the generated sequence τ is detokenized into its original prediction ˆy = Detokenize(τ ). Given bijective mapping ψ, we can define the terminal reward via the distancebased metrics in the target space, e.g., via the negative Mean Squared Error (MSE): R(τ ) = (ψ(ˆy) ψ(y))2, (3) where is the ground-truth target 1. The mapping ψ can be chosen flexibly, e.g., identity or normalization. This reward function is calculated on sequence level, and thus inherently numerically aware on the target space, which is property 1Given the inherent quantization error by discrete tokenization (Selvam, 2025), one could round the target to the nearest tokenization bin to calculate the metrics. However, we omit this detail for simplicity. Figure 2. Comparison between local token-level training and global sequence-level update. Left (existing methods): The model is trained at each token [t1, . . . , tn] with local loss (e.g., CE) that focuses solely on individual tokens. Right (ours): The model generates full sequence and detokenizes it into prediction ˆy. global reward (i.e., negative MSE) against the ground truth is then backpropagated to update the model parameters. regression) as an MDP. Specifically, taking the generation of 6 (the sequence representation is <1><1><0>) as an example, for an input representation ϕ(x), the MDP = (S, A, P, r, ρ, ) can be written by: State S: state sk is defined by the input feature and the generated token sequence, i.e., sk = (ϕ(x), T<k), where T<k = (t0, . . . , tk1). For instance, an intermediate state at = 2 is s2 = (ϕ(x), <1><1>). Action A: The action space is defined over the token vocabulary V, where an action ak is the selection of the next token tk V. For instance, given the state s2, the model can sample the next token <0> or <1> to proceed towards completing the sequence. Transition : The state transitions (sk+1 sk, a) are deterministic. Appending selected token ak = tk to the current state sk always leads to unique next state sk+1 = (sk, ak) = (ϕ(x), {T<k, tk}), which is also an important characteristic of RL formulation in LLM (Li et al., 2024; 2025b). Continuing the example from state s2, if the model samples the action <0>, the state transitions to sequence <1><1><0> (decoding to 6); conversely, if the action <1> is sampled, the state updates to <1><1><1> (decoding to 7). Reward r: The reward function assigns reward values to state-action pairs. Since we have to access signals from the detokenized numerical value only after the entire sequence is generated, the reward function is defined by: (cid:40) r(sk, ak) = 0 r(ϕ(x), a0:K1) otherwise. if = 1 Consistent with the formulation of RL in LLM (Li et al., 2025b; Zhang et al., 2025b), this reward design is sparse 4 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning being ignored in previous token-level objectives (Zausinger It correctly assigns et al., 2025; Chung et al., 2025). relative higher reward to numerically close prediction (e.g., 101 for target of 100) compared to numerically distant one (e.g., 200), even if both differ by single token. This directly forces the model to learn the principles of numerical magnitude and proximity. We will discuss different settings of ψ according to different problem natures in Section 4. Notably, compared to Reinforcement Learning with Verifiable Reward (RLVR) research in LLM (Li et al., 2025b; Zhang et al., 2025b), which takes sparse reward, e.g., {1, +1}, the reward in GenRe2 is dense one, where different generated sequences receive different rewards. 3.4. Training Dynamics We follow the settings of (Song & Bahri, 2025) to examine the feasibility of GenRe2. Specifically, we instantiate the encoder ϕ as Multi-Layer Perception (MLP), and the autoregressive decoder as standard Transformer decoder with normalized tokenization. Here we use the negative MSE on the normalized space as the reward. The RL training pipeline is implemented under the accelerate framework (Gugger et al., 2022) with deepspeed (Rasley et al., 2020) ZeRO stage 2 (Rajbhandari et al., 2020). Analogous to the common practice of performing RL after SFT in LLM post-training, we initiate RL using the CE-trained checkpoint that achieved the minimum validation loss. Reward dynamics. We run GenRe2 on the TALENT benchmark (Liu et al., 2025a; Ye et al., 2024), expanding over 100 regression tasks. In the two top sub-figures of Figure 3, we present the training and validation reward dynamics of GenRe2 combined with ReMax and GRPO, where the rewards of individual tasks are normalized to [0, 1]. It can be observed that the rewards increase steadily and result in stable convergence, showing that our method is robust across diverse tasks. analyze representative furdydataset, We performance Regression performance dynamics. regression ther the of GenRe2 namics on Kaggle bike sharing demand challange (FanaeeT & Gama, 2014). Rather than focusing only on the final metrics, e.g., the coefficient of determination (R2), we consider the Wasserstein-1 distance to measure the distance between the output distribution (i.e., the histogram distribution of the generated candidates) and the target. Formally, assume the output distribution lies in group of supports {zi}k i=1, then we can calculate the Wasserstein-1 distance as: W1 = (cid:80)k i=1 pi zi ytrue, where ytrue represents the ground-truth target. The Wasserstein-1 distance quantifies the distance of towards ytrue, where lower distance indicates better regression performance. As shown in the i=1 with probabilities {pi}k Figure 3. Training dynamics of GenRe2. Top row: Normalized reward dynamics for GenRe2 combined with ReMax (left) and GRPO (right) on 100 TALENT regression tasks, where the reward is normalized to [0, 1] with respect to each task. Bottom row: Visualization of regression performance dynamics on Kaggle bike sharing demand challange (Fanaee-T & Gama, 2014), comparing GenRe2 with NTL-WAS (Zausinger et al., 2025) and DIST2 (Chung et al., 2025) on test R2 score (left, higher is better) and test Wasserstein-1 distance (right, lower is better). two bottom sub-figures in Figure 3, compared to other token-level methods, GenRe2 achieves both significantly lower W1 distance and better performance, demonstrating better alignment with the aim of regression. This clearly shows the advantage of focusing on the global structure and numerical magnitude on sequence-level. 4. Experiments In this section, we empirically compare GenRe2 with variety of baseline methods on two representative decodingbased regression tasks. In Section 4.1, we evaluate GenRe2 on tabular regression tasks, while in Section 4.2, we conduct experiments on the recently proposed Regression Language Model (RLM) to address code-to-metric regression. We show the performance of different methods and conduct case studies to analyze algorithmic behavior. Finally, we deliver some empirical discussions to understand the superior performance of RL in Section 4.3. 4.1. Tabular Regression We examine the ability of GenRe2 to perform tabular regression on TALENT benchmark (Liu et al., 2025a; Ye et al., 2024), popular benchmark for tabular data containing 100 regression datasets. Following the practice of conducting RL after SFT in LLM post-training, we start RL from the CEpretrained checkpoints. In Section 4.1.1, we introduce our experimental settings. Then we present the results to show the superiority of GenRe2 in Section 4.1.2. We also examine the robustness of GenRe2 across multiple tokenization Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Table 1. RMSE, R2 and Rank Correlation results over 5 random seeds on 100 TALENT regression tasks. The best and runner-up results are bolded and underlined, respectively. For the decoding-based methods, Median and Mean denote the aggregation strategy used to derive the final prediction from the generated candidates. Rows shaded in gray indicate our proposed methods. Head Pointwise Riemann Decoder Method / / RMSE R2 Rank Corr. Median Mean Median Mean Median Mean 0.55630.0035 0.54350.0004 0.57080.0262 0.61700.0008 0.72890.0046 0.77090. Base Model +NTL-WAS (Zausinger et al., 2025) +NTL-MSE (Zausinger et al., 2025) +DIST2 (Chung et al., 2025) 0.54840.0004 0.54780.0006 0.54780.0012 0.54570.0019 0.53270.0004 0.53070.0007 0.53200.0013 0.58100.0017 0.61240.0004 0.61320.0009 0.60980.0044 0.60960.0057 0.63680.0005 0.63910.0009 0.63430.0049 0.46780.0090 0.77050.0007 0.77120.0013 0.77210.0011 0.77340. 0.76700.0011 0.76890.0003 0.76860.0003 0.73340.0018 +GenRe2-ReMax (Ours) +GenRe2-GRPO (Ours) 0.51900.0014 0.53200.0020 0.51510.0012 0.52710.0019 0.64590.0020 0.62480.0062 0.65080.0017 0.63160. 0.77850.0011 0.77850.0011 0.77280.0017 0.77370.0016 settings, and give explanations for the different performance when combined with ReMax and GRPO. 4.1.1. EXPERIMENTAL SETUP Compared methods. We mainly consider two categories of methods: (1) Baselines with different regression heads (i.e., pointwise head and Riemann head); (2) Decoding-based regression methods, including two NTL variants (NTL-WAS and NTL-MSE) (Zausinger et al., 2025) and DIST2 (Chung et al., 2025). Here, NTL and DIST2 are improvement methods for decoding-based regression with token-level loss. Details of the compared baselines can be found in Appendix D.1. Following (Song & Bahri, 2025), we instantiate the encoder ϕ as an MLP, and the decoder is standard Transformer decoder. Implementation details. Following the common protocol in tabular research (Liu et al., 2025a; Ye et al., 2024), we standardize the input using z-score transformation. We train the pointwise baseline with MSE loss, and the Riemann baseline with the unbounded variants suggested by (Muller et al., 2022; 2023). For other decoding-based baselines and GenRe2, we first train the base model from scratch using CE loss for 200 epochs, followed by fine-tuning the best validation checkpoint for 100 epochs using the respective strategies. We use the normalized tokenization to prevent outliers (Song & Bahri, 2025) by default, with the digit base = 2 and output sequence length = 8. Other details, including objective normalization and model optimization, can be found in Appendix D.3 and D.4. We will also consider different tokenization settings in Section 4.1.2. RL details. We set the rollout budget = 16 in the experiments. The reward function is the negative MSE in Eq. (3). Before calculating the reward, we first transform the detokenized number to its original space, and then set the mapping ψ as z-score standardization ψ(y) = yµy σy where µy and σy represent the mean and standard derivation of in the training set, respectively. We perform this transformation for fair comparison to the pointwise and Riemann baselines, which also conduct z-score on target scores. We employ two RL methods (i.e., ReMax (Li et al., 2024) and GRPO (Shao et al., 2024)) to finetune the pretrained checkpoint using AdamW optimizer (Loshchilov & Hutter, 2019) with an initial learning rate of 5 105, and report results of the checkpoint that achieves the best validation reward. Evaluation. We compare all methods on suite of regression metrics, including RMSE, R2, and Spearmans Rank Correlation. For decoding-based methods, following (Akhauri et al., 2025a), we directly sample from the models output distribution with temperature 1.0 to generate = 128 candidate solutions, and aggregate them via both mean and median. 4.1.2. RESULTS AND ANALYSES Main results. In Table 1, we report the main results of our tabular regression experiments, where our method GenRe2 is appended with the name of the employed RL backbone. We can observe that: (1) The base model of the decoder head method consistently outperforms the pointwise baseline, showing competitive performance against the Riemann baseline; (2) All of the token-level methods, NTL-WAS, NTL-MSE, and DIST2 do not consistently improve the performance of the base model after finetuning, with slight improvement on some metrics; (3) Our proposed methods, GenRe2-ReMax and GenRe2-GRPO, instead significantly improve the performance of the base model, where GenRe2ReMax achieves the best overall performance on all metrics, and GenRe2-GRPO performs best on rank correlation and is runner-up on RMSE. After finetuned by our methods, the decoding-based methods consistently outperform the pointwise and Riemann baselines, demonstrating superiority for regression modeling. Different tokenization settings. To examine the robustness of our method, we vary the digit bases of the normalized tokenization from 2 to 10. The results are illustrated in Figure 4, where: (1) GenRe2-ReMax consistently achieves the highest R2 scores across all digits bases, showing great robustness and improvements against token-level methods; (2) In contrast, GenRe2-GRPO exhibits high sensitivity to this 6 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning 2025a), an important downstream application of decodingbased regression, to perform code metric regression, following (Akhauri et al., 2025b). Specifically, we finetune the pretrained checkpoints provided by (Akhauri et al., 2025b) 2 on two datasets collected by (Akhauri et al., 2025b) 3 4: APPS Leetcode (Hendrycks et al., 2021), which primarily involves predicting peak memory usage for high-level Python code, and the objectives include computational latency and memory usage; Triton Kernel Latency (Paliskara & Saroufim, 2025), which focuses on estimating the execution latency of PyTorch programs for low-level Triton GPU kernels. 4.2.1. EXPERIMENTAL SETUP Model architecture. The pretrained model provided by (Akhauri et al., 2025b) is an encoder-decoder model, where the encoder is pretrained T5Gemma (Zhang et al., 2025a) encoder and the decoder is standard Transformer decoder trained from scratch with the IEEE tokenizer (IEEE, 2019) with digit base = 10, exponent length = 3, and mantissa length = 5. Since Akhauri et al. (2025b) trained the model with the encoder frozen, we also freeze the encoder and finetune the decoder with respective strategies. Compared methods. We consider decoding-based baselines for finetuning the given checkpoint, including finetuning by CE, NTL-WAS, NTL-MSE (Zausinger et al., 2025), and DIST2 (Chung et al., 2025). Training & evaluation. We randomly split the datasets (i.e., APPS Leetcode or Triton Kernel Latency) into training, validation, and test sets with proportions of 8:1:1, and finetune the model for 20 epochs using AdamW optimizer (Loshchilov & Hutter, 2019) with learning rate of 1106. We then evaluate the tuned model that achieves the best validation loss / reward, taking the median of = 64 generated samples for evaluation, following (Akhauri et al., 2025b). RL details. We set the rollout budget = 4 in this experiment. Before calculating the reward, we set the mapping ψ as quantile transformation towards standard Gaussian distribution. The number of quantiles is adaptively set to clip(Ntrain/30, 10, 1000), where Ntrain stands for the training set size. We use the quantile transformation instead of z-score standardization to mitigate the impact of outliers on the reward. As shown in Figure 12 in Appendix E.3, the 2https://huggingface.co/akhauriyash/ RLM-GemmaS-Code-v 3https://huggingface.co/datasets/ akhauriyash/Code-Regression 4We exclude the CodeNet dataset (Puri et al., 2021) currently due to its large scale. Figure 4. Average R2 over 100 TALENT regression tasks of different methods under varying normalized tokenization digit bases. Table 2. Ablation study on the three key differences between GenRe2-GRPO and GenRe2-ReMax, averaging over 5 random seeds across 100 TALENT regression tasks. The best and runnerup results are bolded and underlined, respectively. Rows shaded in gray indicate experiments with Reward Standardization (Rew. Std.) enabled. IS Clip denotes Importance Sampling Clipping. Method GenRe2-ReMax GenRe2-GRPO + Greedy Base. IS Clip Rew. Std. Greedy Variant Components IS Clip Rew. Std. Baseline RMSE Metrics R2 Rank Corr. Greedy Mean Greedy Mean Mean Greedy 0.5464 0.6108 0.5634 0.5629 0.5637 0.5478 0.5472 0.5872 0.5876 0.5854 0.6089 0.6095 0. 0.7717 0.7725 0.7717 0.7836 0.7840 hyperparameter, and its performance degrades drastically as the digit base increases, even underperforming the base model at base 10. To better understand this performance gap, we next conduct case study to analyze these two RL methods. Additionally, we provide the ablation on different tokenizers in Appendix E.1. Ablation on GenRe2-GRPO components. To analyze the performance gap, we ablate the different components of GRPO and ReMax at digit base 10, where GenRe2-GRPO performs significantly worse than GenRe2-ReMax in Figure 4. We note that GRPO differs from ReMax in three perspectives: (1) clipping important sampling ratio; (2) dividing reward by its standard deviation; and (3) using the mean reward as the baseline value while ReMax uses greedy baseline. As shown in Table 2, reward standardization is the primary cause of GenRe2-GRPOs degradation. We hypothesize this degradation results from biased gradient estimation from reward standardization (Liu et al., 2025b), which hampers calibrated prediction (Bereket & Leskovec, 2025). However, as recent works also demonstrate the effectiveness of reward standardization in training stability, which can be viewed as an adaptive learning rate (Ge et al., 2025; Li et al., 2025b), we leave the discussion of this component as future work. 4.2. RLM for Code Metric Regression In this subsection, we conduct experiments on Regression Language Model (RLM) (Song et al., 2024; Akhauri et al., Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Table 3. Results for code metric regression on APPS Leetcode and Triton Kernel Latency datasets comparing RMSE, R2 and Rank Correlation. Due to high training overhead, we train single model and report results as the average over 5 random inference seeds. The best and runner-up results are bolded and underlined, respectively. The row shaded in gray indicates our proposed method. Model Base Model RMSE APPS Leetcode R2 Rank Corr. RMSE Triton Kernel Latency R2 Rank Corr. 0.4930.000 0.0090. 0.9350.000 1.0950.000 -0.0030.000 0.5360.003 +CE +NTL-WAS (Zausinger et al., 2025) +NTL-MSE (Zausinger et al., 2025) +DIST2 (Chung et al., 2025) +GenRe2-ReMax (Ours) 0.4951.2810-6 0.4952.2010-7 0.4954.6410-7 0.4951.3710-6 0.4745.4110-6 -0.0025.1910-6 -0.0028.8910-7 -0.0021.8810-6 -0.0025.5610-6 0.0832.1010-5 0.9130.001 0.9040.001 0.8670.002 0.9020.002 16.371.719 23.991.625 33.321.795 560.452.74 -224.847.81 -481.664.20 -928.9101.2 -2.64 5.06104 0.9677.3410-5 1.0948.4410-7 -0.0011.5410-6 0.5550.001 0.5390.010 0.5100.008 0.5400.006 0.5980. objective distribution under z-score retains heavy tails and extreme values, while the quantile normalization suppresses outliers to yield well-behaved Gaussian distribution. Additionally, we clip the reward by minimum negative MSE of 50 by: R(τ ) = max (cid:0)(ψ(ˆy) ψ(y))2, 50(cid:1). We use GenRe2-ReMax (Li et al., 2024), the best-performing method in Section 4.1, as our RL backbone. 4.2.2. RESULTS Table 3 summaries the results of different regression metrics on the two datasets. GenRe2-ReMax achieves superior performance across all metrics, showing steady improvements against the base model. Notably, no individual tokenlevel technique outperforms the base model after datasetspecific finetuning, which is also reported by (Akhauri et al., 2025b). We hypothesize this is form of catastrophic forgetting, where specifically finetuning on the subset may negatively affect the general regression ability. Instead, GenRe2 can mitigate the forgetting compared to other token-level baselines, which aligns with the observation in LLM posttraining that RL forgets less than SFT (Zhu et al., 2025a; Shenfeld et al., 2025; Chen et al., 2025a). 4.3. Understanding the Effectiveness of RL for Decoding-based Regression In this subsection, we conduct illustrative experiments on tabular regression tasks to understand the effectiveness of RL in decoding-based regression. In RLVR for LLM, Yue et al. (2025) showed that RL-tuned models do not exceed the potential of the base model. They found that under the standard implementation, RL often reduces the models reasoning capacity by observing that the base model often outperforms the RL-tuned model on pass@k at large k, widely adopted metric for RLVR (Chen et al., 2021; Yu, 2025) measuring the probability of obtaining at least one correct solution in independent samples. But RL significantly improves sampling efficiency by boosting pass@1 (Yue et al., 2025; Matsutani et al., 2025), thus showing great capability in real-world application. Figure 5. Metric dynamics across 100 TALENT regression tasks. The left sub-figure displays the average best R2@k, while the right one shows the average mean (dashed) and median (solid) R2@k. However, standard regression metrics derived from aggregation (e.g., mean or median) just reflect the expected utility but mask the capability boundary (i.e., the potential to generate precise solution), which is different from pass@k. Therefore, to disentangle these factors and probe the theoretical limit of the models capacity, we analyze the best@k metric under an oracle selection setting. Specifically, for given feature ϕ(xi) in the test set, the model generates predictions {ˆy1 } and selects the closest one to the ground truth yi: , . . . , ˆyk ˆybest = arg minˆy{ˆy1 ,...,ˆyk } yi ˆy. Then, the best@k metrics can be calculated using the collection of all ˆybest values. We plot the best R2@k with varying in the left subfigure of Figure 5. Consistent with the observations in RLVR (Yue et al., 2025; Zhu et al., 2025b), we find that the GenRe2-tuned models significantly improve best R2@1 but surpassed by the base model as increases. While all token-level finetuning methods maintain performance at large k, GenRe2 implicitly suppresses exploration of the output space, thereby lowering the capability boundary. However, this lower variance allows GenRe2 to generate better solutions in single trial (i.e., better best@1), thus achieving better mean and median regression performance shown in the right sub-figure of Figure 5. To better understand the effectiveness of GenRe2, we visualize the evolution of the models output distribution in Figure 6. In (a), the average entropy drops by over 50% 8 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning 1. Extending to generative reward models and verifiers. Decoding-based regression is relevant to Generative Reward Models (GRMs), which also score the inputs in an end-to-end manner. While current works have introduced RL to GRMs (Liu et al., 2025c; Mahdavi et al., 2025; Shi & Jin, 2025), they primarily rely on sparse ranking signals based on the final solution without analyzing the intermediate procedure. Notably, the recently proposed DeepSeek-Math-V2 (Shao et al., 2025) introduces regression-like rewards for verifier RL training. It remains to be studied whether GenRe2 can be effectively extended to enhance the performance of RL-trained regression-based verifiers. 2. Robust uncertainty calibration. Our experiments in Section 4.3, together with prior works in RLVR (Zhu et al., 2025b; Yue et al., 2025; Matsutani et al., 2025) indicate that while RL is effective, it tends to over-sharpen the output distribution, which leads to uncalibrated prediction. However, this harms the uncertainty estimation capability delivered from pretraining (Song & Bahri, 2025; Bereket & Leskovec, 2025), which is important for response verification (Chen et al., 2025b) and real-world usage (Nguyen & Grover, 2022; Damani et al., 2025; Nguyen et al., 2024). Thus, an urgent need still exists for developing more robust and calibrated generative decoding-based regressors under the dynamics of RL-based post-training. 3. Understanding the mechanism of RL update. Recent works have identified the sparse weight update dynamics of RLVR (Shao & Wu, 2025; Mukherjee et al., 2025; Zhu et al., 2025a), which motivates for geometry-aware, parameter-efficient RL algorithm design. Although RL shows consistent improvements in decoding-based regression, the underlying mechanisms remain underexplored. It is worth studying further how RL changes the parameter and its association with regression metrics. 4. Better RL algorithms. Although RLVR algorithms (i.e., ReMax and GRPO used in this paper) show good capabilities, our analysis of the best@k metric in Section 4.3 suggests that current algorithms have not fully explored the search space for decoding-based regression. Thus, techniques like entropy regularization (Cui et al., 2025), improving sampling efficiency (Karan & Du, 2025), and negative samples reinforcement (Zhu et al., 2025b) can be further explored. 5. Combination with modern tabular regression structures. In this paper, we mainly use MLP as the encoder for tabular regression. The generalization of decodingbased regression upon other prevalent tabular model structures (Yan et al., 2023; Chen et al., 2023; Gorishniy et al., 2021; Ye et al., 2025) and tabular foundation models (Zhang et al., 2025e; Hollmann et al., 2025; Grinsztajn et al., 2025; Zhang et al., 2025d) is worth further studied. Figure 6. Impact of GenRe2 finetuning on output distribution. (a) GenRe2 significantly reduces entropy during training, transforming the initial high-entropy distribution into sharper, lowentropy distribution that is more accurate (visualized on the Kaggle bike sharing demand challange (Fanaee-T & Gama, 2014) task). (b) Visualization of the test Wasserstein-1 distance (lower is better) across 100 regression datasets of TALENT benchmark, where GenRe2-ReMax (red) consistently achieves lower distances compared to the base model (blue), demonstrating better approximation towards the ground truth target. during training, transforming initial high-entropy, biased distributions into sharper, more accurate predictions. Additionally, Figure 6 (b) shows that GenRe2-ReMax consistently achieves lower test Wasserstein-1 distances towards the target value than the base model on most tasks, demonstrating better approximation of the ground truth target. 5. Discussion Conclusion. In this paper, we emphasize the significance of decoding-based regression. We challenge the practice of training the model via token-level loss, and propose GenRe2 to address the limitations of prior methods by utilizing Reinforcement Learning. Experimental results on tabular regression and code metric regression show the superiority and generalization of GenRe2, demonstrating the effectiveness of sequence-level reward signals overall token-level supervisions. Future works. Based on our experimental results and analyses, there are many worthwhile directions for future exploration. Here we highlight some promising avenues for future research: 9 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning"
        },
        {
            "title": "References",
            "content": "Ahmadian, A., Cremer, C., Galle, M., Fadaee, M., Kreutzer, J., Pietquin, O., Ustun, A., and Hooker, S. Back to basics: Revisiting REINFORCE-style optimization for learning from human feedback in LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL24), pp. 1224812267, Bangkok, Thailand, 2024. Akhauri, Y., Lewandowski, B., Lin, C.-H., Reyes, A. N., Forbes, G. C., Wongpanich, A., Yang, B., Abdelfattah, M. S., Perel, S., and Song, X. Performance prediction for large systems via text-to-text regression. arXiv:2506.21718, 2025a. Akhauri, Y., Song, X., Wongpanich, A., Lewandowski, B., and Abdelfattah, M. S. Regression language models for code. arXiv:2509.26476, 2025b. Bai, L., Cai, Z., Cao, Y., Cao, M., Cao, W., Chen, C., Chen, H., Chen, K., Chen, P., Chen, Y., et al. Intern-S1: scientific multimodal foundation model. arXiv:2508.15763, 2025. Bellemare, M. G., Dabney, W., and Munos, R. distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning (ICML17), pp. 449458, Sydney, Australia, 2017. Bereket, M. and Leskovec, J. Uncalibrated reasoning: GRPO induces overconfidence for stochastic outcomes. arXiv:2508.11800, 2025. Bishop, C. M. Mixture Density Networks. Aston University, 1994. Bishop, C. M. Pattern Recognition and Machine Learning. Springer-Verlag, Berlin, Heidelberg, 2006. Charton, F. Linear algebra with Transformers. Transactions on Machine Learning Research, 2022. Chen, H., Razin, N., Narasimhan, K., and Chen, D. Retaining by doing: The role of on-policy data in mitigating forgetting. arXiv:2510.18874, 2025a. Chen, J., Yan, J., Chen, Q., Chen, D. Z., Wu, J., and Sun, J. ExcelFormer: neural network surpassing GBDTs on tabular data. arXiv:2301.02819, 2023. Chen, J., Cheng, Q., Yu, F., Wan, H., Zhang, Y., Zheng, S., Yao, J., Zhang, Q., He, H., Luo, Y., Zhao, Y., Wang, F., Sheng, L., Xie, C., Zuo, Y., Li, Y., Zeng, W., Wu, Y., Huang, R., Zhou, D., Chen, K., Qiao, Y., Bai, L., Cheng, Y., Ding, N., Zhou, B., Ye, P., and Cui, G. P1: Mastering physics Olympiads with reinforcement learning. arXiv:2511.13612, 2025b. Chen, L., Zhu, C., Chen, J., Soselia, D., Zhou, T., Goldstein, T., Huang, H., Shoeybi, M., and Catanzaro, B. ODIN: Disentangled reward mitigates hacking in RLHF. In Proceedings of the 41st International Conference on Machine Learning (ICML24), pp. 79357952, Vienna, Austria, 2024. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., et al. Evaluating large language models trained on code. arXiv:2107.03374, 2021. Chen, T. and Guestrin, C. XGBoost: scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD16), pp. 785794, San Francisco, CA, 2016. Chen, Z., Qin, X., Wu, Y., Ling, Y., Ye, Q., Zhao, W. X., and Shi, G. Pass@k training for adaptively balancing exploration and exploitation of large reasoning models. arXiv:2508.10751, 2025c. Chiang, C.-H., Lee, H.-y., and Lukasik, M. TRACT: Regression-aware fine-tuning meets chain-of-thought reasoning for LLM-as-a-judge. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL25), pp. 29342952, Vienna, Austria, 2025. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems 31 (NeurIPS17), pp. 43024310, Long Beach, CA, 2017. Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q. V., Levine, S., and Ma, Y. SFT memorizes, RL generalizes: comparative study of foundation model post-training. arXiv:2501.17161, 2025. Chung, J., Kim, S., Jo, Y., Park, J., Min, D., and Yu, Y. Teaching metric distance to autoregressive language models. arXiv:2503.02379, 2025. Cui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., Li, H.-S., Fan, Y., Chen, H., Chen, W., Liu, Z., Peng, H., Bai, L., Ouyang, W., Cheng, Y., Zhou, B., and Ding, N. The entropy mechanism of reinforcement learning for reasoning language models. arXiv:2505.22617, 2025. Damani, M., Puri, I., Slocum, S., Shenfeld, I., Choshen, L., Kim, Y., and Andreas, J. Beyond binary rewards: Training LMs to reason about their uncertainty. arXiv:2507.16806, 2025. Dong, H., Zhang, P., Lu, M., Shen, Y., and Ke, G. MachineLearningLM: Scaling many-shot in-context learning via continued pretraining. arXiv:2509.06806, 2025. 10 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. Model alignment as prospect theoretic optimization. In Proceedings of the 41st International Conference on Machine Learning (ICML24), pp. 1263412651, Vienna, Austria, 2024. Fanaee-T, H. and Gama, J. Event labeling combining ensemble detectors and background knowledge. Progress in Artificial Intelligence, 2:113127, 2014. Farebrother, J., Orbay, J., Vuong, Q., Taıga, A. A., Chebotar, Y., Xiao, T., Irpan, A., Levine, S., Castro, P. S., Faust, A., Kumar, A., and Agarwal, R. Stop regressing: Training value functions via classification for scalable deep RL. In Proceedings of the 41st International Conference on Machine Learning (ICML24), pp. 1304913071, Vienna, Austria, 2024. Garnelo, M., Rosenbaum, D., Maddison, C., Ramalho, T., Saxton, D., Shanahan, M., Teh, Y. W., Rezende, D., and Eslami, S. M. A. Conditional neural processes. In Proceedings of the 35th International Conference on Machine Learning (ICML18), pp. 17041713, Stockholm, Sweden, 2018a. Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D. J., Eslami, S., and Teh, Y. W. Neural processes. arXiv:1807.01622, 2018b. Ge, C., Yin, H., Liang, H., and Zhang, J. Why GRPO needs normalization: local-curvature perspective on adaptive gradients. In NeurIPS 2025 Workshop on Efficient Reasoning, San Diego, CA, 2025. Gorishniy, Y., Rubachev, I., Khrulkov, V., and Babenko, A. Revisiting deep learning models for tabular data. In Advances in Neural Information Processing Systems 35 (NeurIPS21), pp. 1893218943, Virtual, 2021. Grinsztajn, L., Floge, K., Key, O., Birkel, F., Jund, P., Roof, B., Jager, B., Safaric, D., Alessi, S., Hayler, A., et al. TabPFN-2.5: Advancing the state of the art in tabular foundation models. arXiv:2511.08667, 2025. Gugger, S., Debut, L., Wolf, T., Schmid, P., Mueller, Z., Mangrulkar, S., Sun, M., and Bossan, B. Accelerate: Training and inference at scale made simple, efficient and adaptable., 2022. URL https://github.com/ huggingface/accelerate. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu, R., Zhang, R., Ma, S., Bi, X., et al. DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature, 645(8081):633638, 2025. Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J. Measuring coding challenge competence with APPS. In Advances in Neural Information Processing Systems 35 (NeurIPS21), Datasets and Benchmarks Track, Virtual, 2021. Hollmann, N., Muller, S., Purucker, L., Krishnakumar, A., Korfer, M., Hoo, S. B., Schirrmeister, R. T., and Hutter, F. Accurate predictions on small data with tabular foundation model. Nature, 637(8045):319326, 2025. Hu, H., Qian, C., Xue, K., Jorgensen, R. G., Keiluweit, M., Liang, C., Zhu, X., Chen, J., Sun, Y., Ni, H., Ding, J., Huang, W., Mao, J., Tan, R.-X., Zhou, J., Crowther, T. W., Zhou, Z.-H., Zhang, J., and Liang, Y. Reducing the uncertainty in estimating soil microbial-derived carbon storage. Proceedings of the National Academy of Sciences, 121 (35):e2401916121, 2024. Hu, J., Liu, J. K., Xu, H., and Shen, W. REINFORCE++: An efficient RLHF algorithm with robustness to both prompt and reward models. arXiv:2501.03262, 2025. IEEE. IEEE standard for floating-point arithmetic. IEEE Std 754-2019 (Revision of IEEE 754-2008), pp. 184, 2019. Imani, E. and White, M. Improving regression performance with distributional losses. In Proceedings of the 35th International Conference on Machine Learning (ICML18), pp. 21572166, Stockholm, Sweden, 2018. Imani, E., Luedemann, K., Scholnick-Hughes, S., Elelimy, E., and White, M. Investigating the histogram loss in regression. arXiv:2402.13425, 2024. Jiang, J.-P., Liu, S.-Y., Cai, H.-R., Zhou, Q., and Ye, H.-J. Representation learning for tabular data: comprehensive survey. arXiv:2504.16109, 2025. Karan, A. and Du, Y. Reasoning with sampling: Your base model is smarter than you think. arXiv:2510.14901, 2025. Kim, J., Chung, H., and Kim, B.-H. CapeLLM: Supportfree category-agnostic pose estimation with multimodal large language models. In Proceedings of the 20th International Conference on Computer Vision (ICCV25), Honolulu, HI, 2025. Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems 31 (NeurIPS17), pp. 64056416, Long Beach, CA, 2017. He, P., Cao, Z., Di, H., Shen, G., and Zhou, S. Application of machine learning in caisson inclination prediction: Model performance comparison and interpretability analysis. Transportation Geotechnics, 55:101654, 2025. Li, Y.-C., Xu, T., Yu, Y., Zhang, X., Chen, X.-H., Ling, Z., Chao, N., Yuan, L., and Zhou, Z.-H. Generalist reward models: Found inside large language models. arXiv:2506.23235, 2025a. 11 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Li, Z., Xu, T., Zhang, Y., Lin, Z., Yu, Y., Sun, R., and Luo, Z.-Q. ReMax: simple, effective, and efficient reinforcement learning method for aligning large language models. In Proceedings of the 41st International Conference on Machine Learning (ICML24), pp. 2912829163, Vienna, Austria, 2024. Li, Z., Wang, P., Xu, T., Ding, T., Sun, R., and Yu, Y. Review of reinforcement learning for large language models: Formulations, algorithms, and opportunities, 2025b. URL http://www.liziniu.org/docs/ RL4LLM_Survey.pdf. Liu, S.-Y., Cai, H.-R., Zhou, Q.-L., and Ye, H.-J. TALENT: tabular analytics and learning toolbox. Journal of Machine Learning Research, 2025a. Meng, Y., Xia, M., and Chen, D. SimPO: Simple preference optimization with reference-free reward. In Advances in Neural Information Processing Systems 38 (NeurIPS24), pp. 124198124235, Vancouver, Canada, 2024. Mukherjee, S., Yuan, L., Hakkani-Tur, D., and Peng, H. Reinforcement learning finetunes small subnetworks in large language models. In Advances in Neural Information Processing Systems 39 (NeurIPS25), San Diego, CA, 2025. Muller, S., Hollmann, N., Arango, S. P., Grabocka, J., and Hutter, F. Transformers can do Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations (ICLR22), Virtual, 2022. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding R1-Zero-like training: critical perspective. In Proceedings of the 2nd Conference on Language Modeling (COLM25), Montreal, Canada, 2025b. Muller, S., Feurer, M., Hollmann, N., and Hutter, F. PFNs4BO: In-context learning for Bayesian optimization. In Proceedings of the 40th International Conference on Machine Learning (ICML23), pp. 2544425470, Honolulu, HI, 2023. Liu, Z., Wang, P., Xu, R., Ma, S., Ruan, C., Li, P., Liu, Y., and Wu, Y. Inference-time scaling for generalist reward modeling. arXiv:2504.02495, 2025c. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In Proceedings of the 7th International Conference on Learning Representations (ICLR19), New Orleans, LA, 2019. Lukasik, M., Narasimhan, H., Menon, A. K., Yu, F., and Kumar, S. Regression aware inference with LLMs. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP24), pp. 1366713678, Miami, FL, 2024. Lukasik, M., Meng, Z., Narasimhan, H., Chang, Y.-W., Menon, A. K., Yu, F., and Kumar, S. Better autoregressive regression with LLMs via regression-aware fine-tuning. In Proceedings of the 13th International Conference on Learning Representations (ICLR25), Singapore, 2025. Mahan, D., Van Phung, D., Rafailov, R., Blagden, C., Lile, N., Castricato, L., Franken, J.-P., Finn, C., and Albalak, A. Generative reward models. arXiv:2410.12832, 2024. Mahdavi, S., Kisacanin, B., Toshniwal, S., Du, W., Moshkov, I., Armstrong, G., Liao, R., Thrampoulidis, C., and Gitman, I. Scaling generative verifiers for natural language mathematical proof verification and selection. arXiv:2511.13027, 2025. Matsutani, K., Takashiro, S., Minegishi, G., Kojima, T., Iwasawa, Y., and Matsuo, Y. RL squeezes, SFT expands: comparative study of reasoning LLMs. arXiv:2509.21128, 2025. Nguyen, T. and Grover, A. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling. In Proceedings of the 39th International Conference on Machine Learning (ICML22), pp. 1656916594, Baltimore, MD, 2022. Nguyen, T., Zhang, Q., Yang, B., Lee, C., Bornschein, J., Miao, Y., Perel, S., Chen, Y., and Song, X. Language model embeddings can be sufficient for Bayesian optimization. arXiv:2410.10190, 2024. Nguyen, T., Koneru, A., Li, S., and Grover, A. PhysiX: foundation model for physics simulations. arXiv:2506.17774, 2025. OpenAI. GPT-4 technical report. arXiv:2303.08774, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 36 (NeurIPS22), pp. 2773027744, New Orleans, LA, 2022. Paliskara, S. and Saroufim, M. KernelBook, 5 2025. https://huggingface.co/datasets/ URL GPUMODE/KernelBook. Peng, R., Ren, Y., Yu, Z., Liu, W., and Wen, Y. SimKO: Simple Pass@K policy optimization. arXiv:2510.14807, 2025. 12 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., and Gulin, A. CatBoost: Unbiased boosting with categorical features. In Advances in Neural Information Processing Systems 32 (NeurIPS18), pp. 66396649, Montreal, Canada, 2018. Puri, R., Kung, D. S., Janssen, G., Zhang, W., Domeniconi, G., Zolotov, V., Dolby, J., Chen, J., Choudhury, M. R., Decker, L., Thost, V., Buratti, L., Pujar, S., Ramji, S., Finkler, U., Malaika, S., and Reiss, F. CodeNet: largescale AI for code dataset for learning diversity of coding tasks. In Advances in Neural Information Processing Systems 35 (NeurIPS21), Datasets and Benchmarks Track, Virtual, 2021. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 2014. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 37 (NeurIPS23), pp. 5372853741, New Orleans, LA, 2023. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. ZeRO: Memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC20), pp. 116, Atlanta, GA, 2020. Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. DeepSpeed: System optimizations for enabling training of extreme-scale models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD20), pp. 35053506, Virtual, 2020. Rasmussen, C. E. and Williams, C. K. I. Gaussian Processes for Machine Learning. The MIT Press, Cambridge, MA, 2006. Sahakyan, M., Aung, Z., and Rahwan, T. Explainable artificial intelligence for tabular data: survey. IEEE Access, 9:135392135422, 2021. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv:1707.06347, 2017. Selvam, K. P. Why large language models fail at precision regression, 2025. URL https://karthick. ai/blog/2025/LLM-Regression/. Shao, J. and Wu, J. Who reasons in the large language models? In Advances in Neural Information Processing Systems 39 (NeurIPS25), San Diego, CA, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J.-M., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv:2402.03300, 2024. Shao, Z., Luo, Y., Lu, C., Ren, Z., Hu, J., Ye, T., Gou, Z., Ma, S., and Zhang, X. DeepSeekMathself-verifiable mathematical V2: reaURL https://github.com/ soning, 2025. deepseek-ai/DeepSeek-Math-V2/blob/ main/DeepSeekMath_V2.pdf."
        },
        {
            "title": "Towards",
            "content": "Shenfeld, I., Pari, J., and Agrawal, P. RLs razor: Why online reinforcement learning forgets less. arXiv:2509.04259, 2025. Shi, W. and Jin, X. Heimdall: Test-time scaling on the generative verification. arXiv:2504.10337, 2025. Song, X. and Bahri, D. Decoding-based regression. Transactions on Machine Learning Research, 2025. Song, X., Li, O., Lee, C., Yang, B., Peng, D., Perel, S., and Chen, Y. OmniPred: Language models as universal regressors. Transactions on Machine Learning Research, 2024. Sutton, R. S. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):944, 1988. Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. MIT press, 2018. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12 (NeurIPS99), pp. 10571063, Denver, CO, 1999. Tan, R.-X., Chen, M., Xue, K., Wang, Y., Wang, Y., Fu, S., and Qian, C. Towards universal offline black-box optimization via learning language model embeddings. In Proceedings of the 42nd International Conference on Machine Learning (ICML25), pp. 5849958544, Vancouver, Canada, 2025. Tarasov, D., Brilliantov, K., and Kharlapenko, D. Is value functions estimation with classification plug-and-play for offline reinforcement learning? Transactions on Machine Learning Research, 2024. Tchuindjo, D. and Khattab, O. Reasoning-intensive regression. arXiv:2508.21762, 2025. Vacareanu, R., Negru, V. A., Suciu, V., and Surdeanu, M. From words to numbers: Your large language model is secretly capable regressor when given in-context examples. In Proceedings of the 1st Conference on Language Modeling (COLM24), Philadelphia, PA, 2024. 13 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Van Breugel, B. and Van Der Schaar, M. Position: Why tabular foundation models should be research priority. In Proceedings of the 41st International Conference on Machine Learning (ICML24), pp. 4897648993, Vienna, Austria, 2024. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems 31 (NeurIPS17), pp. 60006010, Long Beach, CA, 2017. Wang, P., Liu, T.-S., Wang, C., Wang, Y., Yan, S., Jia, C.- X., Liu, X.-H., Chen, X.-W., Xu, J.-C., Li, Z., and Yu, Y. survey on large language models for mathematical reasoning. arXiv:2506.08446, 2025. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Yan, J., Chen, J., Wu, Y., Chen, D. Z., and Wu, J. T2GFormer: Organizing tabular features into relation graphs promotes heterogeneous feature interaction. In Proceedings of the 37th AAAI Conference on Artificial Intelligence (AAAI23), pp. 1072010728, Washington, DC, 2023. Ye, H.-J., Liu, S.-Y., Cai, H.-R., Zhou, Q.-L., and Zhan, D.-C. closer look at deep learning methods on tabular datasets. arXiv:2407.00956, 2024. Ye, H.-J., Yin, H.-H., Zhan, D.-C., and Chao, W.-L. Revisiting nearest neighbor for tabular data: deep tabular baseline two decades later. In Proceedings of the 13th International Conference on Learning Representations (ICLR25), Singapore, 2025. Yeo, I. and Johnson, R. A. new family of power transformations to improve normality or symmetry. Biometrika, 87(4):954959, 2000. Yu, Y. Pass@k metric for RLVR: diagnostic tool of exploration, but not an objective. arXiv:2511.16231, 2025. Yu, Y., Chen, Z., Zhang, A., Tan, L., Zhu, C., Pang, R. Y., Qian, Y., Wang, X., Gururangan, S., Zhang, C., Kambadur, M., Mahajan, D., and Hou, R. Self-generated critiques boost reward modeling for language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL25), pp. 1149911514, Albuquerque, New Mexico, 2025. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model? In Advances in Neural Information Processing Systems 39 (NeurIPS25), San Diego, CA, 2025. Zausinger, J., Pennig, L., Kozina, A., Sdahl, S., Sikora, J., Dendorfer, A., Kuznetsov, T., Hagog, M., Wiedemann, N., Chlodny, K., Limbach, V., Ketteler, A., Prein, T., Singh, V. M., Danziger, M., and Born, J. Regress, dont guess: regression-like loss on number tokens for language models. In Proceedings of the 42nd International Conference on Machine Learning (ICML25), pp. 7399574017, Vancouver, Canada, 2025. Zhang, B., Moiseev, F., Ainslie, J., Suganthan, P., Ma, M., Bhupatiraju, S., Lebron, F., Firat, O., Joulin, A., and Dong, Z. Improving the quality-efficiency trade-off via adaptation. arXiv:2504.06225, 2025a. Encoder-decoder Gemma: Zhang, K., Zuo, Y., He, B., Sun, Y., Liu, R., Jiang, C., Fan, Y., Tian, K., Jia, G., Li, P., Fu, Y., Lv, X., Zhang, Y., Zeng, S., Qu, S., Li, H.-S., Wang, S., Wang, Y., Long, X.-D., Liu, F., Xu, X., Ma, J., Zhu, X., Hua, E., Liu, Y., Li, Z., yong Chen, H., Qu, X., Li, Y., Chen, W., Yuan, Z., Gao, J., Li, D., Ma, Z., Cui, G., Liu, Z., Qi, B., Ding, N., and Zhou, B. survey of reinforcement learning for large reasoning models. arXiv:2509.08827, 2025b. Zhang, L., Hosseini, A., Bansal, H., Kazemi, M., Kumar, A., and Agarwal, R. Generative verifiers: Reward modeling In Proceedings of the 13th as next-token prediction. International Conference on Learning Representations (ICLR25), Singapore, 2025c. Zhang, X., Maddix, D. C., Yin, J., Erickson, N., Ansari, A. F., Han, B., Zhang, S., Akoglu, L., Faloutsos, C., Mahoney, M. W., et al. Mitra: Mixed synthetic priors for enhancing tabular foundation models. In Advances in Neural Information Processing Systems 39 (NeurIPS25), San Diego, CA, 2025d. Zhang, X., Ren, G., Yu, H., Yuan, H., Wang, H., Li, J., Wu, J., Mo, L., Mao, L., Hao, M., et al. LimiX: Unleashing structured-data modeling capability for generalist intelligence. arXiv:2509.03505, 2025e. Zhou, Z.-H. and Feng, J. Deep forest. National Science Review, 6(1):7486, 2019. Zhu, H., Zhang, Z., Huang, H., Su, D., Liu, Z., Zhao, J., Fedorov, I., Pirsiavash, H., Sha, Z., Lee, J., et al. The path not taken: RLVR provably learns off the principals. In NeurIPS 2025 Workshop on Efficient Reasoning, San Diego, CA, 2025a. Zhu, X., Xia, M., Wei, Z., Chen, W.-L., Chen, D., and Meng, Y. The surprising effectiveness of negative reinforcement in LLM reasoning. In Advances in Neural Information Processing Systems 39 (NeurIPS25), San Diego, CA, 2025b. 14 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning A. Additional Backgrounds A.1. Regression i=1 sampled from an unknown ground-truth function : R, regression Given training dataset Dtrain = {(xi, yi)}N aims to learn model from Dtrain that accurately predicts the output for unseen inputs. The quality of the learned model is evaluated on hold-out test set, Dtest, by measuring its predictive ability with regression-based metrics, e.g., Root Mean Squared Error (RMSE). Traditional regression methods involve statistical techniques like Gaussian Processes (Rasmussen & Williams, 2006) and tree-based methods (Chen & Guestrin, 2016; Zhou & Feng, 2019). Recent works have focused on deep learning (DL)-based methods (Gorishniy et al., 2021; Jiang et al., 2025), which train deep neural networks to leverage the power of representation learning for regression, demonstrating great superiority and scalability (Gorishniy et al., 2021; Ye et al., 2024). Specifically, DL-based methods train neural networks to map the input to high-dimensional representation ϕ(x), and subsequently models the probability distribution of the target value pθ(y ϕ(x)) via regression head, parameterized by θ. There are several design philosophies for the regression head, including pointwise head, parametric distribution head, and Riemann head. The pointwise head maps ϕ(x) to scalar prediction, which is the most commonly used regression head. However, the pointwise head fails to capture both the uncertainty (Lakshminarayanan et al., 2017) and the complex multimodality of the target distribution (Bishop, 1994). To address this, the parametric distribution head instead models output as predefined distribution (e.g., Gaussian) and predicts its parameters (e.g., the mean value and the standard variance) (Garnelo et al., 2018a;b; Nguyen & Grover, 2022). The Riemann head, also called histogram head, instead converts the regression problem into classification by discretizing the continuous output into finite bins (Imani & White, 2018). The learned model predicts the probability of each bin, from which the output value is derived using weighted sum. Though sensible to hyperparameters (Tarasov et al., 2024), the Riemann head has been shown to improve the models robustness and performance (Imani & White, 2018; Imani et al., 2024), with successful application to reinforcement learning (RL) (Bellemare et al., 2017; Farebrother et al., 2024) and tabular foundation models (Hollmann et al., 2025; Grinsztajn et al., 2025; Zhang et al., 2025e). A.2. RL for LLM Reinforcement Learning (RL) has become pivotal post-training technique for LLMs (Li et al., 2025b), popularized by RLHF for alignment (Christiano et al., 2017; Ouyang et al., 2022; OpenAI, 2023), and extended to domains with verifiable rewards like mathematical (Guo et al., 2025; Wang et al., 2025) and scientific reasoning (Bai et al., 2025; Chen et al., 2025b). RL approaches are primarily categorized into offline preference optimization (Rafailov et al., 2023; Meng et al., 2024; Ethayarajh et al., 2024) and online policy gradient methods (Schulman et al., 2017; Williams, 1992). While early online methods relied on actor-critic algorithms like PPO (Schulman et al., 2017), recent works leverage the deterministic transitions of LLMs to adopt lightweight REINFORCE-based methods without value model (Williams, 1992). Notably, ReMax (Li et al., 2024) and GRPO (Shao et al., 2024) reduce variance using greedy and multi-sample mean baselines, respectively, with the latter showing power in DeepSeek-R1 (Guo et al., 2025). Further variants enhance scalability and stability through reducing variance (Hu et al., 2025) and estimation biases (Ahmadian et al., 2024; Liu et al., 2025b), and employing regularization techniques such as entropy (Cui et al., 2025) and reward shaping (Chen et al., 2025c; Peng et al., 2025). Compared to Supervised Finetuning (SFT), RL demonstrates superior generalization (Chu et al., 2025; Matsutani et al., 2025; Zhu et al., 2025a) and mitigated forgetting (Shenfeld et al., 2025; Chen et al., 2025a). B. Description of Tokenizations Here we provide detailed descriptions of the two common tokenization strategies introduced in the main text: Normalized Tokenization: The normalization tokenization first scales target value to fixed interval (e.g. [0, 1]). Then, this method represents the scaled value as base-B expansion. For instance, when choosing = 2 and mantissa length of = 3, the scaled number 0.6 is tokenized as <1><1><0>. For prediction, we need rescale the detokenized number to its original space. However, normalized tokenization relies on the access to the global minimum and maximum. One could set ymin and ymax in accordance with the training dataset, but this method is highly sensitive to outliers (Yeo & Johnson, 2000). Besides, it is unsuitable for multi-task regression, where different tasks may have different objectives, as globally linear scaling to [0, 1] can cause precision loss (Song et al., 2024). 15 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Scientific Notation Tokenization: Unlike normalized approaches, scientific notation tokenization methods do not normalize the target value. Instead, they represent numbers using sign, mantissa, and exponent components. We describe two specific implementations below: P10 Tokenization (Charton, 2022): P10 Tokenization is an unnormalized tokenization method that represents numbers in format similar to scientific notation. It breaks down scalar into three components: sign token, mantissa part with tokens, and an exponent token. For example, with mantissa length of = 3, the number 1.23 is tokenized as <+><1><2><3><E-2>. IEEE Tokenization (IEEE, 2019): IEEE Tokenization is another unnormalized tokenization scheme that directly represents target value by generalizing the IEEE-754 floating-point standard into base-B format. It tokenizes number into sequence representing its sign, exponent and mantissa. For instance, with base = 10, an exponent length of = 3, and mantissa length of = 4, the number 1012 1.234 is tokenized as <+><-><0><1><2><1><2><3><4>. C. Policy Gradient Methods To optimize the objective in Eq. (1), we employ the policy gradient method (Sutton et al., 1999) to optimize πθ by ascending the gradient of the expected return: θJ (θ) = E(x,y)Dtrain Eτ πθ (cid:34)K1 (cid:88) (cid:35) θ log πθ(ak sk)Aπθ (sk, ak) , (4) k= where Aπθ (sk, ak) is the advantage function estimating the relative value of action ak in state sk. As the expectation Eτ πθ is intractable, one practical solution is to approximate it via Monte Carlo sampling. Given the deterministic state transitions in the MDP, simple policy gradient methods like REINFORCE (Williams, 1992) are efficient. To reduce variance, REINFORCE subtracts baseline in the advantage function: Aπθ (sk, ak) = R(τ ) b(ϕ(x)), where τ = (s0, a0, , sK) denotes trajectory sampled from πθ, R(τ ) = (cid:80)K1 k=0 r(sk, ak) is the expected return of the trajectory τ , and b(ϕ(x)) is the baseline value related to the input ϕ(x), which is to be designed. Crucially, this subtraction maintains an unbiased estimator (Williams, 1992), forming the foundation of online policy gradient methods (Sutton & Barto, 2018). In this paper, we employ two prevalent algorithms, ReMax (Li et al., 2024) and GRPO (Shao et al., 2024), which can be viewed as REINFORCE variants with distinct advantage formulations. ReMax reduces variance efficiently by setting the baseline as the reward of greedy decoding sequence: Aπθ ReMax(τ ) = R(τ ) r(ϕ(x), ˆa0:K1), where ˆak arg max πθ( ϕ(x), ˆa0:k1). GRPO, on the other hand, computes the advantage by normalizing rewards relative to group of sampled trajectories {τ i}G i=1: Aπθ GRPO(τ i) = R(τ i) meanj (cid:8)R(τ j)(cid:9) stdj {R(τ j)} + ϵ , where ϵ is small positive constant. Additionally, GRPO further stabilizes training by incorporating importance sampling and clipping mechanisms into its final objective, which is defined as: (θ) =Eτ πθ (cid:34) 1 (cid:88) i=1 min (cid:8)IS(θ)Aπθ GRPO(τ i), clip(IS(θ), 1 ε, 1 + ε)Aπθ GRPO(τ i)(cid:9) (cid:35) , where IS(θ) = πθ (τ iϕ(x)) hyperparameter controlling the clipping range. πθold (τ iϕ(x)) denotes the importance sampling ratio between the current and reference policies, and ε is 16 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning D. Experimental Settings D.1. Baseline Details In this section, we provide detailed implementations of the baselines compared in our experiments. Consistent with the main text, we categorize these methods into two groups: (1) Baselines with different regression heads, specifically the Pointwise head and the Riemann head; and (2) Decoding-based regression methods, which incorporate token-level loss improvements including NTL variants (NTL-MSE, NTL-WAS) and DIST2. D.1.1. POINTWISE HEAD The Pointwise head represents the standard regression approach. It projects the latent representation ϕ(x) directly to scalar prediction ˆy via linear layer. The model is optimized by minimizing the Mean Squared Error (MSE) loss between the predicted value and the ground truth: LMSE ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (yi ˆyi)2 , where denotes the number of samples in the batch, yi and ˆyi denote the true value and the predicted value, respectively. D.1.2. RIEMANN HEAD Following (Muller et al., 2022; 2023), we implement the Riemann head by combining the infinite support architecture with the histogram loss objective (Imani & White, 2018). This approach models the regression target as probability distribution rather than single scalar, allowing for better handling of uncertainty and outliers. Infinite Support Architecture. We partition the target space of into central finite range and two infinite tails to handle potential outliers. The central range [ymin, ymax] is divided into uniform bins, each with width = (ymax ymin)/K. Additionally, we define left tail region for < ymin and right tail region for ymax. The neural network outputs probability vector = [o0, . . . , oK+1], representing the probability mass assigned to each component. The full predictive Probability Density Function (PDF), denoted as qθ(y x), is defined piecewise: (cid:19) (cid:125) if < ymin if [ymin + (k 1)w, ymin + kw) , {1, 2, . . . , K} qθ(y x) = o0 σtail (cid:124) ϕHN (cid:18) ymin σtail (cid:123)(cid:122) Left Tail (Half-Normal) ok (cid:124)(cid:123)(cid:122)(cid:125) Central Bins (Uniform) oK+1 σtail (cid:124) ϕHN (cid:123)(cid:122) Right Tail (Half-Normal) (cid:18) ymax σtail if ymax (cid:19) (cid:125) where ϕHN () is the PDF of the standard Half-Normal distribution, and σtail is fixed scale parameter (set to 0.5) controlling the decay rate in the tail regions. Histogram Loss. To train the model, we construct smoothed target distribution. Given ground truth scalar ygt, we model the target distribution p(y) as Gaussian centered at ygt with standard deviation σ = 0.75w, truncated to the central range: p(y) (y; ygt, σ2) I[ymin, ymax]. We then discretize this continuous target by integrating p(y) over each bins interval to obtain the target probability mass pk = (cid:82) rk p(y)dy, where [lk, rk] denotes the interval of the k-th central bin. The model is lk optimized by minimizing the CE loss between the target mass vector and the predicted mass vector o: = K+1 (cid:88) k= pk log(ok). Inference. During inference, we obtain the final scalar prediction ˆy by calculating the expected value of the predicted 17 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning distribution qθ(yx). This is computed as the weighted sum of the centroids of all components: ˆy = Eyqθ [y] = K+1 (cid:88) k=0 ok ck, where ck is the centroid of the k-th component. For central bins, ck is the midpoint of the interval; for the tail regions, ck is the expectation of the shifted Half-Normal distribution. D.1.3. NUMBER TOKEN LOSS (NTL) Number Token Loss (NTL) (Zausinger et al., 2025) is an auxiliary regression objective designed to improve numerical predictability of autoregressive language models. Unlike standard CE, which treats numbers as independent nominal tokens, NTL penalizes the numerical distance between the predicted distribution and the ground truth of each numeric token. We implement the two primary variants proposed by the authors: NTL-MSE. This variant treats the models output as continuous expectation. It minimizes the MSE between the numerical value of the ground truth token and the expected numerical value derived from the predicted probability distribution. Let (j) denote the numerical value of token j, ωt be ground truth numeric token at step t, and be the set of indices for number tokens: LNTLMSE = (ωt) 1 (cid:88) t=1 (cid:88) jN 2 pj (j) , where pj tokens in the sequence. denotes the predicted probability assigned to token at step t, , and represents the total number of numeric NTL-WAS. To address potential optimization issues in MSE (e.g., non-unique minima), NTL-WAS minimizes the Wasserstein-1 distance. For one-hot ground truth distribution, this simplifies to the expected absolute difference: LNTLWAS = 1 K (cid:88) (cid:88) t=1 jN pj (ωt) (j) . Implementation Details. The model is optimized using joint objective: = LCE + λ LNTL, where we set the hyperparameter λ to 0.3. The auxiliary loss is computed exclusively on numerical tokens, while non-numerical tokens are masked out. D.1.4. DIST2 LOSS DIST2 Loss (Chung et al., 2025) introduces distance-aware framework that integrates metric relationships directly into the target distribution of discrete autoregressive models. DIST2 constructs soft, categorical target distribution pd based on the inherent distance metric between the ground truth token ω and vocabulary tokens j. The target distribution is modeled as discretized exponential family distribution, where tokens closer to the ground truth in the metric space are assigned higher probabilities. This is controlled by temperature parameter : pd(j ω) = exp(d(j, ω)/T ) jN exp (d (j, ω) /T ) (cid:80) , where we set = 1.0 and the distance metric as Euclidean distance. The objective minimizes the KL divergence between this distance-aware target distribution and the models predicted distribution pθ: LDIST2 = (cid:88) (cid:88) t= jN pd (j ωt) log pd (j ωt) pθ (j ω<t) , where denotes the sequence length, ωt represents the ground truth token at step t, and is the set of indices corresponding to number tokens in the vocabulary. Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning In this work, we adopt joint training strategy that combines the standard CE loss (LCE) with the DIST2. The final optimization objective is formulated as: = LCE + λ LDIST2. Following the default configuration of the original paper, we set the weighting hyperparameter λ to 0.1. The auxiliary loss is computed exclusively on numerical tokens, while non-numerical tokens are masked out. D.2. Model Architecture Consistent with the decoding-based regression paradigm, all models employed in this work utilize an encoder-decoder framework. The specific architectural choices are tailored to the input modalities of the respective tasks. D.2.1. TABULAR REGRESSION Decoding-based Regressor. Following (Song & Bahri, 2025), we utilize hybrid architecture composed of an MLP encoder and Transformer decoder to handle numerical feature inputs. Encoder: The encoder is implemented as Multi-Layer Perceptron (MLP) to project continuous input features into the latent space. It consists of three hidden layers, each with dimensionality of 1024 and Rectified Linear Unit (ReLU) activation functions. The input layer dynamically adjusts to the dimensionality of the feature vector x, while the final linear layer projects the representation to model dimension of = 256. Decoder: The decoder follows standard Transformer architecture (Vaswani et al., 2017) to autoregressively generate the target token sequence. The model dimension is set to = 256 to align with the encoders output. The network comprises stack of 3 decoder layers with multi-head attention (the number of head is set to 4), balancing computational efficiency with modeling capacity. Tokenizer Configuration: For the decoding-based heads, we apply specific tokenization settings. For the P10 (Charton, 2022) and IEEE (IEEE, 2019) tokenizers, the default configuration preserves 4 decimal places for precision, with an exponent length (order) of 10. Baseline Regressors. We configure the baseline regression heads as follows: Pointwise Head (MLP): To ensure fair comparison, we scale the Pointwise regression baseline to have parameter count comparable to the decoding-based model. Specifically, we implement it as large MLP consisting of 3 hidden layers, each with dimensionality of 2048 and ReLU activations. Such baseline setting ensures that the pointwise baselines have more parameter than the decoding-based regressors. Riemann Head: For the Riemann head baseline, we discretize the target space into = 256 bins. The support range for the bins is set to [3, 3] (applied to the normalized targets), with infinite tails handling values outside this range. The MLP encoder setting follows the same setting of pointwise head. D.2.2. CODE METRIC REGRESSION We utilize the pretrained model provided by (Akhauri et al., 2025b) 5, which is an encoder-decoder model. The encoder of the pretrained model is pretrained T5Gemma (Zhang et al., 2025a) encoder and the decoder is standard Transformer decoder trained from scratch with the IEEE tokenizer (IEEE, 2019), configured with digit base = 10, exponent length = 3, and mantissa length = 5. Since Akhauri et al. (2025b) trained the model with the encoder frozen, we also freeze the encoder and finetune the decoder with respective strategies. D.3. Data Processing We apply specific data processing strategies according to the input modality and the regression head employed. All statistics used for normalization are computed exclusively from the training set to prevent data leakage. Input Processing. The preprocessing of input features depends on the task type: 5https://huggingface.co/akhauriyash/RLM-GemmaS-Code-v 19 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Tabular Regression: Since the inputs are numerical vectors, we apply standard z-score normalization to enhance numerical stability: µx σx , where µx and σx denote the coordinate-wise mean and standard deviation of the training inputs, respectively. Code Metric Regression: The inputs for these tasks are raw textual code. Consequently, we feed the text directly into the encoder without applying any additional numerical normalization. Target Processing. The processing of target values is determined by the specific regression head and tokenization scheme: Non-Decoder Heads (Pointwise & Riemann): For these baselines, we standardize the targets using z-score normalization: µy σy , where µy and σy represent the mean and standard deviation of the training targets, respectively. Decoder Heads: For decoding-based regression, the strategy varies by tokenization scheme: P10 and IEEE Tokenization: These schemes are designed to represent the raw numbers directly via scientific notation. Therefore, we do not apply any normalization and train the model to regress the raw target values. Normalized Tokenization: This scheme requires targets to be bounded within fixed interval. We adopt two-stage scaling strategy: targets are first standardized via z-score, followed by Min-Max scaling. The transformation is defined as: = D.4. Implementation Details µy σy , min(y) max(y) min(y) . Training Hyperparameters. Optimization is performed using the AdamW optimizer. The specific configurations for each domain are as follows: Tabular Regression: For decoding-based methods, we use batch size of 128 and an initial learning rate of 1 105. The learning rate follows cosine annealing schedule with 100 warmup steps and minimum decay ratio of 0.1. The Base Model pretraining (CE) is conducted for 200 epochs, while the proposed Policy Gradient optimization runs for 100 epochs. For the baseline regression heads (MLP), we utilize the default training framework and hyperparameters provided by the TALENT benchmark. Code Metric Regression: We employ batch size of 16 with lower initial learning rate of 1 106 to preserve the pre-trained knowledge of the backbone. The model is fine-tuned for total of 20 epochs. Model Selection. To ensure optimal performance and fair comparison, we employ different checkpoint selection strategies based on the training objective. For all standard regression models (including baselines and the pretrained base model), we select the checkpoint that achieves the lowest validation loss. Conversely, for GenRe2, we select the checkpoint that yields the highest mean rewards on the validation set. Inference and Rollout Settings. During the reinforcement learning phase (rollout), we employ stochastic sampling to encourage exploration. Rollout: We set the sampling temperature to 1.0. The number of samples generated per input is set to 16 for Tabular Regression and 4 for Code-to-Metric Regression. Evaluation: For final inference on the test set, we maintain the temperature at 1.0 and aggregate predictions using the median of the generated candidates. The sampling budget is increased to 128 samples for Tabular Regression and 64 samples for Code-to-Metric Regression to ensure robust estimation. Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning E. Additional Experiment E.1. Ablation on Tokenizer Table 4. Ablation study on different output tokenization schemes comparing R2 and Rank Correlation. The results are reported as the average over 5 random seeds across 100 TALENT regression tasks. The best results are bolded. Rows shaded in gray highlight the ReMax results for direct comparison against CE. Tokenization Metric Method Aggregation Strategy Median Median + Filter Norm. P10 IEEE R2 Rank Corr. R2 Rank Corr. R2 Rank Corr. Base GenRe2-ReMax Base GenRe2-ReMax Base GenRe2-ReMax Base GenRe2-ReMax Base GenRe2-ReMax Base GenRe2-ReMax 0.6124 0.6459 0.7705 0.7785 -2.46 109 0.6057 0.7630 0.7862 -2.95 1017 -1.49 1017 0.7652 0.7769 / / / / 0.5874 0.6123 0.7630 0.7862 0.5947 0.6179 0.7652 0.7769 Mean 0.6368 0. 0.7670 0.7728 -6.75 1024 -4.20 1025 0.7161 0.7675 -2.76 1017 -1.52 1017 0.7389 0.7604 Mean + Filter / / / / 0.6102 0.6251 0.7605 0.7692 0.6199 0.6307 0.7619 0.7701 We additionally evaluate the performance of GenRe2-ReMax across different output tokenization schemes, i.e., P10 (Charton, 2022) and IEEE floating-point representations (IEEE, 2019). Given that P10 and IEEE tokenization could yield outlier predictions due to the models hallucinations (Song et al., 2024; Song & Bahri, 2025), we also include an outlier filtering strategy for the generated candidates. From the R2 and rank correlation results in Table 4, we find that GenRe2, based on ReMax, consistently outperforms the base model under different output tokenizations, except for the mean R2 on P10, showing the robustness of GenRe2-ReMax. Besides, it is worth mentioned that tokenization with unlimited output range, e.g., P10 or IEEE, is easier to produce outlier values, resulting in poor R2. However, such tokenization schemes remain higher rank correlation, implicitly capturing the relationship between numbers. We also observe that GenRe2-ReMax mitigates outliers in most cases, even obtaining positive median R2 for P10, but it cannot eliminate the hallucinations. Reducing the hallucinations for unbounded tokenization is still crucial future work for decoding-based regression (Song et al., 2024; Song & Bahri, 2025). E.2. Robustness of GenRe2-ReMax Across Different Tokenizer Bases We further investigate the impact of the tokenizers base parameter on model ranking within the TALENT benchmark. As shown in Figures 7 to 11, GenRe2-ReMax consistently achieves the highest R2 score on the majority of the 100 datasets, regardless of the base selected. Specifically, GenRe2-ReMax maintains dominant position, securing the best performance across all configurations. This analysis confirms that GenRe2-ReMax performs consistently well across different tokenizer settings, showing that it does not require specific tuning of the base parameter to achieve best results. E.3. Visualization of Target Normalization for Code Metric Regression In this subsection, we visualize the target distribution under different normalization strategies mentioned in Section 4.2.1. As shown in Figures 12 and 13, we can observe that both on the APPS Leetcode and the Triton Kernel Latency dataset, the z-score standardization exhibits sharp distribution and is prone to outliers, while the quantile normalization based on Gaussian delivers smooth one. 21 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Figure 7. The proportion of models achieving the best R2. The length of each bar represents the proportion of the 100 datasets (in which given method achieved the highest R2) on the TALENT benchmark. Note that all models utilized normalized tokenizer with base=2. Figure 8. The proportion of models achieving the best R2. The length of each bar represents the proportion of the 100 datasets (in which given method achieved the highest R2) on the TALENT benchmark. Note that all models utilized normalized tokenizer with base=4. 22 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Figure 9. The proportion of models achieving the best R2. The length of each bar represents the proportion of the 100 datasets (in which given method achieved the highest R2) on the TALENT benchmark. Note that all models utilized normalized tokenizer with base=6. Figure 10. The proportion of models achieving the best R2. The length of each bar represents the proportion of the 100 datasets (in which given method achieved the highest R2) on the TALENT benchmark. Note that all models utilized normalized tokenizer with base=8. Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Figure 11. The proportion of models achieving the best R2. The length of each bar represents the proportion of the 100 datasets (in which given method achieved the highest R2) on the TALENT benchmark. Note that all models utilized normalized tokenizer with base=10. Figure 12. Comparison of target value distributions on the APPS Leetcode Dataset across training and validation sets. Top row: Z-score standardization results in distributions with heavy tails and extreme outliers in both the training (left) and validation (right) splits. Bottom row: In contrast, quantile normalization effectively transforms the target values into well-formed standard normal distribution consistently across both subsets. 24 Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning Figure 13. Comparison of target value distributions on the Triton Kernel Latency Dataset across training and validation sets. Top row: Z-score standardization results in distributions with heavy tails and extreme outliers in both the training (left) and validation (right) splits. Bottom row: In contrast, quantile normalization effectively transforms the target values into well-formed standard normal distribution consistently across both subsets."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering, The Chinese University of Hong Kong",
        "National Key Laboratory for Novel Software Technology, Nanjing University",
        "School of Artificial Intelligence, Nanjing University",
        "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
    ]
}